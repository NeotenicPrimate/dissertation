# %run './model/multi_corpus.py'
# %run './model/ergm_functions.py'
# %run './constants.py'

import multiprocessing as mp
import polars as pl
from itertools import combinations
# mp.set_start_method('forkserver')
import math

import networkx as nx

import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pymc as pm
import scipy as sp

from itertools import product
from scipy.special import comb
from sklearn.preprocessing import MinMaxScaler, StandardScaler

import umap.umap_ as umap

from sklearn.preprocessing import StandardScaler

import os


from scipy.spatial.distance import cosine
from scipy.stats import norm

import pytensor.tensor as pt

from IPython.display import display

import seaborn as sns
sns.set(rc = {'figure.figsize': (15, 8)})

# RANDOM_SEED = 8927
# rng = np.random.default_rng(RANDOM_SEED)
az.style.use("arviz-darkgrid")





corpora = co_occurence_graphs(n_edges=100)
Gs = {field_name: corpus['G'] for (field_name, corpus) in corpora.items()}
Dfs = {field_name: corpus['Df'] for (field_name, corpus) in corpora.items()}


import networkx as nx
import numpy as np

# Create a synthetic network
G = Gs['Sociology']

# Compute the average clustering coefficient
avg_clustering = nx.average_clustering(G)

# Compute degree centralization
degrees = np.array(list(dict(G.degree()).values()))
max_possible_degree = len(G.nodes) - 1
centralization = np.sum(max_possible_degree - degrees) / (len(G.nodes) - 1) / (len(G.nodes) - 2)

print("Degree Coefficient:", avg_clustering)
print("Average Clustering Coefficient:", avg_clustering)
print("Degree Centralization:", centralization)

import pymc3 as pm

with pm.Model() as ergm:
    # Define the priors on the parameters
    theta_degree = pm.Normal('theta_degree', mu=0, sigma=1)
    theta_clustering = pm.Normal('theta_clustering', mu=0, sigma=1)
    theta_centralization = pm.Normal('theta_centralization', mu=0, sigma=1)
    
    # Define the likelihood
    p = pm.math.exp(theta_degree * degrees + theta_clustering * avg_clustering + theta_centralization * centralization)
    likelihood = pm.Poisson('likelihood', mu=p, observed=len(G.edges))
    
    # Perform MCMC sampling to estimate the parameters
    trace = pm.sample(2000, tune=1000)

# You can then examine the trace plots and summary statistics to assess the fit of the model.
# pm.traceplot(trace)
# pm.summary(trace)



pm.traceplot(trace)


pm.summary(trace)





tables = []
dist = {}

for (field_name, d) in corpora.items():

    G = d['G']
    df = d['Df']

    print(field_name)

    observed = nx.to_numpy_array(G, weight=None)

    βs_xs = {
        'Density': delta_edges(G),
        # 'Triangles': delta_triangles(G),
        # 'Stars': delta_star(G, 3),
        # 'Betweenness': delta_betweenness(G),
        # 'Closeness': delta_closeness(G),
        # 'Eigenvector': delta_eigenvector(G),
        # 'Centralization': delta_centralization(G),
        # 'Clustering': delta_clustering(G),
        # 'Transitivity': delta_transitivity(G),
        # 'Cliques': delta_cliques(G),
        # 'Components': delta_components(G),
        # 'Gini': delta_g ini(G),
        # 'Louvain': delta_louvain(G),
        # 'Geodesic': delta_geodesic(G),
    }

    dist[field_name] = βs_xs

    with pm.Model() as model:

        βs = []
        xs = []
        for β_name, x in βs_xs.items():
            β = pm.Normal(β_name, sigma=1, initval=None)
            βs.append(β)
            xs.append(x)

        μ = sum(β * x for β, x in zip(βs, xs))

        likelihood = pm.math.sigmoid(μ)

        pm.Bernoulli(name='logit', p=likelihood, observed=observed)

        trace = pm.sample(
            tune=1000,
            draws=2000,
            chains=4,
            # init = 'adapt_diag',
            cores=4,
            # step=pm.Metropolis(),
            step=pm.NUTS(),
            # random_seed=12345,
        )

        trace.to_netcdf(os.path.join(os.path.join(OUTPUT_PATH, f'co_occurence_traces'), f'{field_name}.nc'))






def p_value_stars(p_value):
    match p_value:
        case _ if p_value <= 0.001:
            stars = '***'
        case _ if p_value <= 0.01:
            stars =  '**'
        case _ if p_value <= 0.05:
            stars =  '*'  
        case _:
            stars = ' '
    return stars


traces_path = os.path.join(OUTPUT_PATH, f'co_occurence_traces')

co_citation_coef = []
co_citation_se = []

for file in os.listdir(traces_path):

    field_name = file.split('.')[0]
    trace = az.from_netcdf(os.path.join(traces_path, file))

    summary = az.summary(trace, kind="stats")
    summary.index.name = 'beta'
    df = pl.from_pandas(summary, include_index=True)
    df = df.with_columns(pl.col('beta').str.replace('_', ' '))

    df = (
        df
        .with_columns((pl.col('mean') / pl.col('sd')).round(4).alias('z_score'))
        .with_columns((pl.col('z_score').abs().apply(norm.sf).round(4).alias('p_value')))
        .with_columns(pl.col('p_value').apply(p_value_stars).alias('significance'))
    )

    df_coef = (
        df.select(
            pl.col('beta'),
            pl.concat_str(
                [
                    pl.col('mean'),
                    pl.col('significance')
                ],
                # sep=''
            ).alias(field_name),
        )
    )

    df_se = (
        df.select(
            pl.col('beta'),
            pl.concat_str(
                [
                    pl.lit('('),
                    pl.col('sd'),
                    pl.lit(')'),
                ]
            ).alias(field_name),
        )
    )

    co_citation_coef.append(df_coef)
    co_citation_se.append(df_se)

iter_dfs = iter(co_citation_coef)
co_citation_df_coef = next(iter_dfs)
for df in iter_dfs:
    co_citation_df_coef = co_citation_df_coef.join(df, on='beta', how='inner')

iter_dfs = iter(co_citation_se)
co_citation_df_se = next(iter_dfs)
for df in iter_dfs:
    co_citation_df_se = co_citation_df_se.join(df, on='beta', how='inner')

columns = sorted(co_citation_df_coef.select(pl.all().exclude('beta')).columns)


coef_df = co_citation_df_coef.select(pl.concat_str(pl.all(), sep=' & ').alias('all_fields'))
se_df = co_citation_df_se.select(pl.concat_str(pl.all().exclude('beta').alias('all_fields'), sep=' & '))

coef_df = coef_df['all_fields'].to_list()
se_df = se_df['all_fields'].to_list()

row_str = ''
for i in range(len(se_df)):
    row_str += f'{coef_df[i]} \\\ \n & {se_df[i]} \\\ \n \\addlinespace[0.5em] \n'

alignments = ''.join(['c']*len(columns)*2)

new_columns = []
for col in columns:
    if ' & ' in col:
        first, second = col.split(' & ')
        s = f'\\begin{{tabular}}{{cc}} {first} \& \\\ {second} \\end{{tabular}}'
        new_columns.append(s)
    elif ' ' in col:
        first, second = col.split(' ')
        s = f'\\begin{{tabular}}{{cc}} {first} \\\ {second} \\end{{tabular}}'
        new_columns.append(s)
    else:
        s = f'\\begin{{tabular}}{{cc}} {col} \\end{{tabular}}'
        new_columns.append(s)
new_columns = ' \n& '.join(new_columns)


table_str = f"""
\\begin{{tabular}}{{l*{{{len(columns)*2}}}{{c}}}}
\\toprule
\\addlinespace[0.7em]
& {new_columns} \\\ 
\\midrule
\\midrule
\\addlinespace[0.5em]
{row_str}
\\bottomrule
\end{{tabular}}
"""

with open(os.path.join(LATEX_TABLE_PATH, 'co_occurrence_ergm_model.tex'), "w+") as file:
    file.write(table_str)


pm.model_to_graphviz(model)


dfs = []

for field_name, features in dist.items():
    df = pl.DataFrame()
    for feature_name, mat in features.items():
        df = df.with_columns(
            pl.Series(
                feature_name,
                mat.reshape(np.multiply(*mat.shape))
            )
        )
    df = df.with_columns(
        pl.Series(
            'Field',
            np.full((len(df)), field_name)
        )
    )
    dfs.append(df)
df = pl.concat(dfs)





cols = df.select(pl.all().exclude('Field')).columns
n_cols = len(cols)

fig, axs = plt.subplots(n_cols, figsize=(16, 10*n_cols))
for col, ax in zip(cols, axs):
    sns.stripplot(data=df.to_pandas(), x=col, y="Field", ax=ax)
    ax.spines['top'].set_color('k')
    ax.spines['top'].set_linewidth(1)
    ax.spines['bottom'].set_color('k')
    ax.spines['bottom'].set_linewidth(1)
    ax.spines['right'].set_color('k')
    ax.spines['right'].set_linewidth(1)
    ax.spines['left'].set_color('k')
    ax.spines['left'].set_linewidth(1)

plt.plot()





df.to_pandas().corr()


# sns.set(rc={'figure.figsize':(16, 12)})
# sns.set_style('darkgrid', {'axes.linewidth': 1, 'axes.edgecolor': 'black', 'axes.spines.left': True, 'axes.spines.bottom': True, 'axes.spines.right': True, 'axes.spines.top': True})

pp = sns.pairplot(df.to_pandas(), vars=None, hue='Field', corner=True, grid_kws={'layout_pad': 0.5}) # , corner=True , grid_kws={"despine": False}

pp.map_lower(plt.scatter, alpha = 0.6, linewidth=1, edgecolor='k')
# g.map_lower(sns.kdeplot, hue=None, levels=5, color=".2")
# pp.map_diag(plt.kdeplot, alpha=1)
# pp.map_lower(sns.scatterplot)
pp.map_upper(sns.kdeplot, shade=True)

# for ax in pp.axes.flat:
#     if ax:
#         ax.spines['top'].set_color('k')
#         ax.spines['top'].set_linewidth(1)
#         ax.spines['bottom'].set_color('k')
#         ax.spines['bottom'].set_linewidth(1)
#         ax.spines['right'].set_color('k')
#         ax.spines['right'].set_linewidth(1)
#         ax.spines['left'].set_color('k')
#         ax.spines['left'].set_linewidth(1)

plt.show()


# sns.set_style('darkgrid', {'axes.linewidth': 1, 'axes.edgecolor':'black'})

jp = sns.jointplot(data=df, x="Eigenvector", y="Transitivity", hue="Field", height=15, space=0)
jp.plot_marginals(sns.kdeplot)

jp.ax_marg_x.set_facecolor('w')
jp.ax_marg_y.set_facecolor('w')

ax.spines['top'].set_color('k')
ax.spines['top'].set_linewidth(1)
ax.spines['bottom'].set_color('k')
ax.spines['bottom'].set_linewidth(1)
ax.spines['right'].set_color('k')
ax.spines['right'].set_linewidth(1)
ax.spines['left'].set_color('k')
ax.spines['left'].set_linewidth(1)

plt.show()





traces_path = os.path.join(OUTPUT_PATH, f'co_occurence_traces')

co_occurence_coef = []
co_occurence_se = []

for file in os.listdir(traces_path):

    field_name = file.split('.')[0]
    trace = az.from_netcdf(os.path.join(traces_path, file))

    summary = az.summary(trace, kind="stats")
    summary.index.name = 'beta'
    df = pl.from_pandas(summary, include_index=True)
    df = df.with_columns(pl.col('beta').str.replace('_', ' '))

    df = (
        df
        .with_columns((pl.col('mean') / pl.col('sd')).round(4).alias('z_score'))
        .with_columns((pl.col('z_score').abs().apply(norm.sf).round(4).alias('p_value')))
    )

    df_coef = (
        df.select(
            pl.col('beta'),
            pl.when(pl.col('p_value') > 0.05).then(0).otherwise(pl.col('mean')).alias(field_name)
        )
    )

    co_occurence_coef.append(df_coef)

iter_dfs = iter(co_occurence_coef)
co_occurence_df_coef = next(iter_dfs)
for df in iter_dfs:
    co_occurence_df_coef = co_occurence_df_coef.join(df, on='beta', how='inner')

columns = co_occurence_df_coef['beta'].to_list()
vec_df = co_occurence_df_coef.select(pl.all().exclude('beta')).transpose(include_header=True, header_name='Field', column_names=columns)
field_names = vec_df['Field'].to_list()
vecs = vec_df.select(pl.all().exclude('Field')).to_numpy()

scaled_data = StandardScaler().fit_transform(vecs)





reducer = umap.UMAP(random_state=42)
embs = reducer.fit_transform(scaled_data)

sns.set()

fig, ax = plt.subplots(1, 1, figsize=(12, 8))

plt.scatter(embs[:, 0], embs[:, 1], alpha=1, edgecolor='k')

# ax.set_title(f'{field_name.capitalize()}', fontweight='semibold', fontsize=20)

for field_name, (x, y) in zip(field_names, embs):
    plt.text(x, y+0.05, field_name)


ax.spines['top'].set_color('k')
ax.spines['top'].set_linewidth(1)
ax.spines['bottom'].set_color('k')
ax.spines['bottom'].set_linewidth(1)
ax.spines['right'].set_color('k')
ax.spines['right'].set_linewidth(1)
ax.spines['left'].set_color('k')
ax.spines['left'].set_linewidth(1)

# fig.tight_layout()
# plt.savefig(f'{OBSIDIAN_IMG_PATH}/co_citation_umap_node2vec.png')
plt.plot()








axes_arr = az.plot_trace(trace, divergences=None, lines={'Centralization': 2})
for ax in axes_arr.flatten():
    ax.spines['top'].set_color('k')
    ax.spines['top'].set_linewidth(1)
    ax.spines['bottom'].set_color('k')
    ax.spines['bottom'].set_linewidth(1)
    ax.spines['right'].set_color('k')
    ax.spines['right'].set_linewidth(1)
    ax.spines['left'].set_color('k')
    ax.spines['left'].set_linewidth(1)
plt.draw()


axes_arr = az.plot_forest(trace, combined=True, hdi_prob=0.95, r_hat=False, figsize=(9, 7))
for ax in axes_arr.flatten():
    ax.spines['top'].set_color('k')
    ax.spines['top'].set_linewidth(1)
    ax.spines['bottom'].set_color('k')
    ax.spines['bottom'].set_linewidth(1)
    ax.spines['right'].set_color('k')
    ax.spines['right'].set_linewidth(1)
    ax.spines['left'].set_color('k')
    ax.spines['left'].set_linewidth(1)
plt.show()


ax = az.plot_energy(trace)
ax.spines['top'].set_color('k')
ax.spines['top'].set_linewidth(1)
ax.spines['bottom'].set_color('k')
ax.spines['bottom'].set_linewidth(1)
ax.spines['right'].set_color('k')
ax.spines['right'].set_linewidth(1)
ax.spines['left'].set_color('k')
ax.spines['left'].set_linewidth(1)
plt.draw()


az.plot_posterior(trace)
plt.show()


# Goodness of fit
# The observed data looks like draws from the posterior distribution

with model:
    ppc = pm.sample_posterior_predictive(trace)


az.plot_ppc(ppc)
plt.show()


az.plot_ppc(ppc, kind='cumulative')
plt.show()





inference_data = az.convert_to_inference_data(trace)
inference_data


pooled_loo = az.loo(inference_data)

pooled_loo


pm.logp(β_density, 1)


pm.draw(β_density, 10)







