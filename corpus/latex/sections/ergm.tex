Exponential Random Graph Models (ERGMs) are commonly employed to model and test various hypotheses about the structural characteristics of social networks. This approach allows for the investigation of both local and global properties of a network and enables the identification of significant factors that contribute to its formation. By examining network properties such as the presence of triangles or the average shortest path between nodes, we can determine the extent to which these features played a role in shaping the network's structure.

ERGMs rely on the fundamental theoretical assumption of dependence, which posits that the presence of certain relationships between documents can impact the formation, persistence, or prevention of other relationships. To illustrate, when an article, let's say article A, cites both articles B and C, it is more probable that articles B and C will also cite each other.

In their general form, ERGMs are written as \citep{hunter2008}:

$$
P(Y=y) = \frac{exp[\beta_1 s_1(g) + \beta_2 s_2(g) + ... + \beta_p s_p(g)]}{\sum\limits_{g'} exp[\beta_1 s_1(g') + \beta_2 s_2(g') + ... + \beta_p s_p(g')]}
$$

The model specifies that the probability of a network (on the left-hand side) is a function of weighted sum of sufficient statistics of the observed graph, over the weighted sum of sufficient statistics summed over all possible graphs with $n$ nodes, where:

\begin{itemize}
    \item Y is the random variable for the state of the network (with realization y)
    \item $s_k(g)$ is a $k$th model statistics (ERGM term) for the network $g$
    \item $\beta_k$ is the coefficients the sufficient statistic $k$
\end{itemize}

The denominator of the probability distribution is a normalizing constant that ensures that the distribution sums up to one. Computing this constant requires summing over the space of all possible networks on n nodes, but the number of possible configurations grows exponentially with the number of nodes, specifically $2^{(n(n-1)/2)}$ for undirected graphs and $2^{(n(n-1))}$ for directed graphs. Therefore, exact computation of this sum is infeasible. To address this, Markov Chain Monte Carlo Maximum Likelihood Estimation (MCMC MLE) methods are commonly used to generate samples and estimate the values of the parameters that maximize the likelihood of the observed network. These estimates capture the strength and direction of the effects of different network statistics on the likelihood of observing the network.

The above formula can be rewritten in terms of the covariate vector $\theta$:

$$
logit(Y_{ij} = 1 | y_{ij}^c) = \theta' \delta(y_{ij})
$$

Where:
\begin{itemize}
\item $y_{ij}^c$ is the complement of $y_{ij}$, i.e. all dyads in the network other than $y_{ij}$
\item $y_{ij}^+$ as the same network as $y$ except that $y_{ij} = 1$
\item $y_{ij}^-$ as the same network as $y$ except that $y_{ij} = 0$
\item $\delta(y_{ij})$ is given by $g(y_{ij}^+) - g(y_{ij}^-)$ which measures how the sufficient 
statistic $g(y)$ changes if the $(i, j)$th edge is "toggled" on or off.
\end{itemize}

To summarize, for each of the sufficient statistics, a matrix $g_p(y)$ of size $n \times n$ is created. Each entry $g_{ij}$ represents the effect that the presence of an edge between nodes $i$ and $j$ has on the corresponding network statistic, while holding the rest of the network constant.

To help us interpret the coefficents, let us take an example. If only edges and triangles are included in the model, the conditional log-odds of two documents being co-cited or two terms co-occurring, keeping the rest of the network constant, is:

$$
\theta_{edges} \times \text{change in the number of ties} + \theta_{triangles} \times \text{change in number of triangles}
$$

The total number of ties in the network always increases by 1 with the addition of any new tie, therefore, for an edge that creates no triangles, the conditional log-odds is simply $\theta_{edges}$. If the addition of the edge creates one triangle the conditional log-odds becomes $\theta_{edges} + 1 \times \theta_{triangles}$. If the addition of the edge creates one triangle the conditional log-odds becomes $\theta_{edges} + 2 \times \theta_{triangles}$. Etc. Note that the edge covariate is always included and is treated as an intercept in the model.

To obtain the corresponding probability, the expit (or inverse logit) of $\theta$ is taken:

$$
P(e) = \exp(\theta)/(1 + exp(\theta))
$$

% SLIDES

% In their general form, ERGMs are written as \citep{hunter2008}:

% $$
% P(Y=y) = \frac{exp[\beta_1 s_1(g) + \beta_2 s_2(g) + ... + \beta_p s_p(g)]}{\sum\limits_{g'} exp[\beta_1 s_1(g') + \beta_2 s_2(g') + ... + \beta_p s_p(g')]}
% $$

% \begin{itemize}
%     \item Y is the random variable for the state of the network (with realization y)
%     \item $s_k(g)$ is a $k$th model statistics (ERGM term) for the network $g$
%     \item $\beta_k$ is the coefficients the sufficient statistic $k$
% \end{itemize}

% Computing the denominator requires summing over the space of all possible networks on $n$ nodes, but the number of possible configurations grows exponentially with the number of nodes, specifically $2^{(n(n-1)/2)}$ for undirected graphs and $2^{(n(n-1))}$ for directed graphs.

% Because of this, Markov Chain Monte Carlo Maximum Likelihood Estimation (MCMC MLE) is used to estimate the denominator. It samples from the space of all possible networks producing a representative sample and estimate of the true denominator. 

% $$
% logit(Y_{ij} = 1 | y_{ij}^c) = \theta' \delta(y_{ij})
% $$

% Where:
% \begin{itemize}
%     \item $y_{ij}^c$ is the complement of $y_{ij}$, i.e. all dyads in the network other than $y_{ij}$
%     \item $\theta'$ is the vector of coefficients for the sufficient statistics
%     \item $y_{ij}^+$ as the same network as $y$ except that $y_{ij} = 1$
%     \item $y_{ij}^-$ as the same network as $y$ except that $y_{ij} = 0$
%     \item $\delta(y_{ij})$ is given by $g(y_{ij}^+) - g(y_{ij}^-)$ which measures how the sufficient 
%     statistic $g(y)$ changes if the $(i, j)$th edge is "toggled" on or off.
% \end{itemize}

% What are the coefficients that maximize the likelihood of observing the graph? 

% To obtain the probability of observing an edge, the expit (or inverse logit) of $\theta$ is taken:

% $$
% P(e) = \frac{\exp(\theta)}{(1 + exp(\theta))}
% $$


