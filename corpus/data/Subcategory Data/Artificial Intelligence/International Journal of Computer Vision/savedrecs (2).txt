PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
J	Sundaramoorthi, G; Yezzi, A; Mennucci, AC; Sapiro, G				Sundaramoorthi, Ganesh; Yezzi, Anthony; Mennucci, Andrea C.; Sapiro, Guillermo			New Possibilities with Sobolev Active Contours	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Active contours; Gradient flows; Sobolev norm; Global flows; Shape optimization; Shape priors; Ill-posed flows	TRACKING; PRIORS; FLOWS	Recently, the Sobolev metric was introduced to define gradient flows of various geometric active contour energies. It was shown that the Sobolev metric outperforms the traditional metric for the same energy in many cases such as for tracking where the coarse scale changes of the contour are important. Some interesting properties of Sobolev gradient flows include that they stabilize certain unstable traditional flows, and the order of the evolution PDEs are reduced when compared with traditional gradient flows of the same energies. In this paper, we explore new possibilities for active contours made possible by Sobolev metrics. The Sobolev method allows one to implement new energy-based active contour models that were not otherwise considered because the traditional minimizing method render them ill-posed or numerically infeasible. In particular, we exploit the stabilizing and the order reducing properties of Sobolev gradients to implement the gradient descent of these new energies. We give examples of this class of energies, which include some simple geometric priors and new edge-based energies. We also show that these energies can be quite useful for segmentation and tracking. We also show that the gradient flows using the traditional metric are either ill-posed or numerically difficult to implement, and then show that the flows can be implemented in a stable and numerically feasible manner using the Sobolev gradient.	[Sundaramoorthi, Ganesh; Yezzi, Anthony] Georgia Inst Technol, Sch Elect Engn, Atlanta, GA 30332 USA; [Mennucci, Andrea C.] Scuola Normale Super Pisa, Pisa, Italy; [Sapiro, Guillermo] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN USA	University System of Georgia; Georgia Institute of Technology; Scuola Normale Superiore di Pisa; University of Minnesota System; University of Minnesota Twin Cities	Sundaramoorthi, G (corresponding author), Georgia Inst Technol, Sch Elect Engn, Atlanta, GA 30332 USA.	ganeshs@ece.gatech.edu	Mennucci, Andrea/B-6335-2012	Mennucci, Andrea/0000-0002-4302-3275	NSF [CCR-0133736]; NIH/NINDS [R01-NS-037747]; Airforce MURI; ONR; NGA; ARO; DARPA; McKnight Foundation	NSF(National Science Foundation (NSF)); NIH/NINDS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS)); Airforce MURI(MURI); ONR(Office of Naval Research); NGA; ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); McKnight Foundation	Sundaramoorthi and Yezzi were supported by NSF CCR-0133736, NIH/NINDS R01-NS-037747, and Airforce MURI; Sapiro was partially supported by NSF, ONR, NGA, ARO, DARPA, and the McKnight Foundation.	Boykov Y.Y., 2001, ICCV, V1, P105, DOI DOI 10.1109/ICCV.2001.937505; Brook A, 2005, LECT NOTES COMPUT SC, V3459, P456; BRUCKSTEIN AM, 1990, COMPUT VISION GRAPH, V49, P283, DOI 10.1016/0734-189X(90)90105-5; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Charpiat G, 2007, INT J COMPUT VISION, V73, P325, DOI 10.1007/s11263-006-9966-2; Charpiat G., 2005, ICCV; Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985; COHEN L, 1996, C COMP VIS PATT REC, P666; Cremers D, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P137, DOI 10.1109/VLSM.2001.938892; Cremers D., 2003, IEEE 2 INT WORKSH VA, P169; Delingette H, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P43, DOI 10.1109/VLSM.2001.938880; Droske M, 2004, INTERFACE FREE BOUND, V6, P361, DOI 10.4171/IFB/105; Eckstein Ilya, 2007, P 5 EUR S GEOM PROC, P183; Foulonneau A, 2006, IEEE T PATTERN ANAL, V28, P1352, DOI 10.1109/TPAMI.2006.154; Fua P., 1990, Machine Vision and Applications, V3, P45, DOI 10.1007/BF01211451; GUYADER CL, 2007, SELF REPELLING SNAKE; HORN BKP, 1983, ACM T MATH SOFTWARE, V9, P441, DOI 10.1145/356056.356061; JACKSON J, 2004, IEEE C DEC CONTR; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; Kim J, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P797, DOI 10.1109/ICIP.2002.1039092; Kolmogorov V, 2005, IEEE I CONF COMP VIS, P564; Leventon ME, 2000, PROC CVPR IEEE, P316, DOI 10.1109/CVPR.2000.855835; Ma TY, 1999, IEEE T IMAGE PROCESS, V8, P1549, DOI 10.1109/83.799883; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; Mansouri AR, 2004, IEEE T IMAGE PROCESS, V13, P853, DOI 10.1109/TIP.2004.826128; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Mio W, 2004, Q APPL MATH, V62, P359, DOI 10.1090/qam/2054604; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; MUMFORD D., 1985, P IEEE C COMP VIS PA; Nain D, 2004, LECT NOTES COMPUT SC, V3216, P51; NEUBERGER JW, 1997, LECT NOTES MATH, V1670; Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758; Paragios N, 2002, INT J COMPUT VISION, V46, P223, DOI 10.1023/A:1014080923068; POLDEN A, 1996, THESIS U TUBINGEN GE; RAVIV TR, 2004, P EUR C COMP VIS; ROCHERY M, 2003, IEEE WORKSH VLSM; RONFARD R, 1994, INT J COMPUT VISION, V13, P229, DOI 10.1007/BF01427153; ROUSSON M, 2002, P EUR C COMP VIS, V2, P78; Rudin W., 1973, FUNCTIONAL ANAL; SAPIRO G, 1995, IEEE T PATTERN ANAL, V17, P67, DOI 10.1109/34.368150; Schoenemann T., 2007, IEEE INT C COMP VIS; Sundaramoorthi G, 2005, IEEE I CONF COMP VIS, P1276; Sundaramoorthi G, 2005, LECT NOTES COMPUT SC, V3752, P109; SUNDARAMOORTHI G, 2006, CVPR, P674; Sundaramoorthi G, 2008, IEEE T PATTERN ANAL, V30, P851, DOI 10.1109/TPAMI.2007.70751; Sundaramoorthi G, 2007, INT J COMPUT VISION, V73, P345, DOI 10.1007/s11263-006-0635-2; Tsai A, 2001, PROC CVPR IEEE, P463; Yezzi A, 2005, IEEE I CONF COMP VIS, P913; Yezzi A. J., 2005, ARXIVMATHDG0412454; Yezzi A.  Jr., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P898, DOI 10.1109/ICCV.1999.790317; ZHU SC, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P416, DOI 10.1109/ICCV.1995.466909	54	34	35	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2009	84	2					113	129		10.1007/s11263-008-0133-9	http://dx.doi.org/10.1007/s11263-008-0133-9			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	451NO					2022-12-18	WOS:000266477100001
J	Fazekas, S; Amiaz, T; Chetverikov, D; Kiryati, N				Fazekas, Sandor; Amiaz, Tomer; Chetverikov, Dmitry; Kiryati, Nahum			Dynamic Texture Detection Based on Motion Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dynamic texture; Motion analysis; Optical flow; Level set; Real-time processing; Brightness conservation	OPTICAL-FLOW; VISUAL-MOTION; SEGMENTATION; RECOGNITION; FRAMEWORK; REGIONS	Motion estimation is usually based on the brightness constancy assumption. This assumption holds well for rigid objects with a Lambertian surface, but it is less appropriate for fluid and gaseous materials. For these materials an alternative assumption is required. This work examines three possible alternatives: gradient constancy, color constancy and brightness conservation (under this assumption the brightness of an object can diffuse to its neighborhood). Brightness conservation and color constancy are found to be adequate models. We propose a method for detecting regions of dynamic texture in image sequences. Accurate segmentation into regions of static and dynamic texture is achieved using a level set scheme. The level set function separates each image into regions that obey brightness constancy and regions that obey the alternative assumption. We show that the method can be simplified to obtain a less robust but fast algorithm, capable of real-time performance. Experimental results demonstrate accurate segmentation by the full level set scheme, as well as by the simplified method. The experiments included challenging image sequences, in which color or geometry cues by themselves would be insufficient.	[Amiaz, Tomer; Kiryati, Nahum] Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel; [Fazekas, Sandor; Chetverikov, Dmitry] Comp & Automat Res Inst, Budapest, Hungary	Tel Aviv University; Eotvos Lorand Research Network; Hungarian Academy of Sciences; Hungarian Institute for Computer Science & Control	Amiaz, T (corresponding author), Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel.	tomrevam@yahoo.com		Kiryati, Nahum/0000-0003-1436-2275	MUSCLE: Multimedia Understanding through Semantics, Computation and Learning; EC 6th Framework IST Program; A.M.N. Foundation	MUSCLE: Multimedia Understanding through Semantics, Computation and Learning; EC 6th Framework IST Program(European Commission); A.M.N. Foundation	This research was supported partly by MUSCLE: Multimedia Understanding through Semantics, Computation and Learning, a European Network of Excellence funded by the EC 6th Framework IST Program. At Tel Aviv University this research was supported also by the A.M.N. Foundation.	Amiaz T, 2006, INT J COMPUT VISION, V68, P111, DOI 10.1007/s11263-005-6206-0; Amiaz T, 2007, LECT NOTES COMPUT SC, V4485, P848; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; Bereziat D, 2000, PROC CVPR IEEE, P487, DOI 10.1109/CVPR.2000.854890; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; Bouthemy P, 1998, INT C PATT RECOG, P905, DOI 10.1109/ICPR.1998.711298; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Brox T, 2006, LECT NOTES COMPUT SC, V3951, P471; Bruce, 1996, VISUAL PERCEPTION; Burnham K.P., 2002, MODEL SELECTION MULT, V2nd edn, DOI DOI 10.1007/B97636; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chetverikov D, 2005, ADV SOFT COMP, P17; CORPETTI T, 2000, INT S FLOW VIS, P1; Cremers D, 2005, INT J COMPUT VISION, V62, P249, DOI 10.1007/s11263-005-4882-4; Cuzol A, 2005, LECT NOTES COMPUT SC, V3459, P254; Cuzol A, 2007, INT J COMPUT VISION, V75, P329, DOI 10.1007/s11263-007-0037-0; DERVIEUX A, 1979, LECT NOTES MATH, V771, P145; Doretto G, 2004, LECT NOTES COMPUT SC, V3022, P591; Doretto G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1236; Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132; Fablet R, 2003, IEEE T PATTERN ANAL, V25, P1619, DOI 10.1109/TPAMI.2003.1251155; Fazekas S., 2005, INT WORKSH TEXT AN S, P37; FAZEKAS S, 2007, DYNAMIC TEXTURE DETE; Fazekas S, 2007, SIGNAL PROCESS-IMAGE, V22, P680, DOI 10.1016/j.image.2007.05.013; field, 2021, OIL GAS STUDIES, P25, DOI [DOI 10.1007/978-3-540-24673-2, 10.31660/0445-0108-2021-3-25-36, DOI 10.31660/0445-0108-2021-3-25-36]; FUJITA K, 2003, INT WORKSH TEXT AN S, P31; GALUN M, 2005, P C COMP VIS PATT RE, V1, P256; Golland P, 1997, COMPUT VIS IMAGE UND, V68, P346, DOI 10.1006/cviu.1997.0553; HILDRETH EC, 1987, ANNU REV NEUROSCI, V10, P477, DOI 10.1146/annurev.ne.10.030187.002401; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Lu Zongqing, 2005, WACVMOTION, P241; Lucas Bruce D, 1981, P 7 INT JOINT C ART, DOI DOI 10.1042/CS0730285; Mileva Y, 2007, LECT NOTES COMPUT SC, V4713, P152; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; MURRAY DW, 1987, IEEE T PATTERN ANAL, V9, P220, DOI 10.1109/TPAMI.1987.4767896; NELSON RC, 1992, CVGIP-IMAG UNDERSTAN, V56, P78, DOI 10.1016/1049-9660(92)90087-J; Nir T, 2008, INT J COMPUT VISION, V76, P205, DOI 10.1007/s11263-007-0051-2; OHTA N, 1989, P IEEE INT C IM PROC, P801; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Otsuka K, 1998, INT C PATT RECOG, P1047, DOI 10.1109/ICPR.1998.711871; Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y; Paragios N, 2005, COMPUT VIS IMAGE UND, V97, P259, DOI 10.1016/j.cviu.2003.04.001; Peh CH, 2002, IEEE T IMAGE PROCESS, V11, P1179, DOI 10.1109/TIP.2002.804265; Peteri R, 2005, LECT NOTES COMPUT SC, V3523, P223; Peteri R., 2006, DYNTEX COMPREHENSIVE; Saisan P, 2001, PROC CVPR IEEE, P58; SCHNORR C, 1984, P INT C PATT REC, P661; Schoenemann T, 2006, LECT NOTES COMPUT SC, V4174, P455; SCHUNCK BG, 1984, P INT C PATT REC, V1, P20; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shi JB, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1154, DOI 10.1109/ICCV.1998.710861; Smith JR, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P437; Soatto S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P439, DOI 10.1109/ICCV.2001.937658; SONG S, 1991, IEEE T MED IMAGING, V1, P462; Sundaramoorthi G, 2007, LECT NOTES COMPUT SC, V4485, P153; Szummer M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P823, DOI 10.1109/ICIP.1996.560871; Todorovic D, 1996, PERCEPTION, V25, P1235, DOI 10.1068/p251235; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; WILDES R, 2000, LNCS, V1843, P768; Wu Peng, 2001, LNCS, V2124, P21; Yuan L, 2004, LECT NOTES COMPUT SC, V3022, P603; ZHENG HY, 1995, IEEE T IMAGE PROCESS, V4, P1223, DOI 10.1109/83.413167; ZHONG J, 2002, TEMPORAL TEXTURE REC	68	34	35	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2009	82	1					48	63		10.1007/s11263-008-0184-y	http://dx.doi.org/10.1007/s11263-008-0184-y			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	402AH					2022-12-18	WOS:000262986100003
J	Knossow, D; Ronfard, R; Horaud, R				Knossow, David; Ronfard, Remi; Horaud, Radu			Human motion tracking with a kinematic parameterization of extremal contours	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						articulated motion representation; human-body tracking; zero-reference kinematics; developable surfaces; extremal contours; chamfer distance; chamfer matching; multiple-camera motion capture	SHAPE-FROM-SILHOUETTE; TIME PART; ACQUISITION; POSE	This paper addresses the problem of human motion tracking from multiple image sequences. The human body is described by five articulated mechanical chains and human body-parts are described by volumetric primitives with curved surfaces. If such a surface is observed with a camera, an extremal contour appears in the image whenever the surface turns smoothly away from the viewer. We describe a method that recovers human motion through a kinematic parameterization of these extremal contours. The method exploits the fact that the observed image motion of these contours is a function of both the rigid displacement of the surface and of the relative position and orientation between the viewer and the curved surface. First, we describe a parameterization of an extremal-contour point velocity for the case of developable surfaces. Second, we use the zero-reference kinematic representation and we derive an explicit formula that links extremal contour velocities to the angular velocities associated with the kinematic model. Third, we show how the chamfer-distance may be used to measure the discrepancy between predicted extremal contours and observed image contours; moreover we show how the chamfer distance can be used as a differentiable multi-valued function and how the tracker based on this distance can be cast into a continuous non-linear optimization framework. Fourth, we describe implementation issues associated with a practical human-body tracker that may use an arbitrary number of cameras. One great methodological and practical advantage of our method is that it relies neither on model-to-image, nor on image-to-image point matches. In practice we model people with 5 kinematic chains, 19 volumetric primitives, and 54 degrees of freedom; We observe silhouettes in images gathered with several synchronized and calibrated cameras. The tracker has been successfully applied to several complex motions gathered at 30 frames/second.	[Knossow, David; Ronfard, Remi; Horaud, Radu] INRIA Rhone Alpes, F-38330 Montbonnot St Martin, France		Horaud, R (corresponding author), INRIA Rhone Alpes, 655 Ave Europe, F-38330 Montbonnot St Martin, France.	radu.horaud@inrialpes.fr	Horaud, Radu/AAR-5982-2021; Ronfard, Rémi P/AAW-6761-2021	Horaud, Radu/0000-0001-5232-024X; Ronfard, Rémi P/0000-0003-4830-5690				Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; Balan A. O., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P349; BARROW HG, 1981, ARTIF INTELL, V17, P75, DOI 10.1016/0004-3702(81)90021-7; BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0; Bregler C, 2004, INT J COMPUT VISION, V56, P179, DOI 10.1023/B:VISI.0000011203.00237.9b; Cheung KM, 2005, INT J COMPUT VISION, V63, P225, DOI 10.1007/s11263-005-6879-4; Cheung KM, 2005, INT J COMPUT VISION, V62, P221, DOI 10.1007/s11263-005-4881-5; David P, 2004, INT J COMPUT VISION, V59, P259, DOI 10.1023/B:VISI.0000025800.10423.1f; Delamarre Q, 2001, COMPUT VIS IMAGE UND, V81, P328, DOI 10.1006/cviu.2000.0892; DEUTSCHER J, 2000, COMPUTER VISION PATT, P2126; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; DRUMMOND T, 2001, ICCV, P315; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Forsyth DA, 2005, FOUND TRENDS COMPUT, V1, P77, DOI 10.1561/0600000005; Forsyth David A, 2012, COMPUTER VISION MODE; Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131; GAVRILA D, 1999, ICCV, P87; Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716; Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056; Gleicher M, 2002, COMP ANIM CONF PROC, P75, DOI 10.1109/CA.2002.1017510; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; Kakadiaris I, 2000, IEEE T PATTERN ANAL, V22, P1453, DOI 10.1109/34.895978; Kehl R, 2006, COMPUT VIS IMAGE UND, V104, P190, DOI 10.1016/j.cviu.2006.07.010; Knossow D, 2006, LECT NOTES COMPUT SC, V3851, P664; Koenderink J., 1990, SOLID SHAPE; Kreyszig E., 1991, DIFFERENTIAL GEOMETR; Martin F, 2002, INT J ROBOT RES, V21, P97, DOI 10.1177/027836402760475324; McCarthy J.M., 1990, INTRO THEORETICAL KI; Mikic I, 2003, INT J COMPUT VISION, V53, P199, DOI 10.1023/A:1023012723347; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Mooring BW, 1991, FUNDAMENTALS MANIPUL; Murray R. M., 1994, MATH INTRO ROBOTIC M, V1; Plankers R, 2003, IEEE T PATTERN ANAL, V25, P1182, DOI 10.1109/TPAMI.2003.1227995; RONFARD R, 2002, P 7 EUR C COMP VIS C, V4, P700; SIGAL L, 2006, CS0608 BROWN U DEP C; Sim DG, 1999, IEEE T IMAGE PROCESS, V8, P425, DOI 10.1109/83.748897; Sminchisescu C, 2005, INT J COMPUT VISION, V61, P81, DOI 10.1023/B:VISI.0000042935.43630.46; SMINCHISESCU C, 2003, IEEE INT C COMP VIS, V1, P69; Song Y, 2003, IEEE T PATTERN ANAL, V25, P814, DOI 10.1109/TPAMI.2003.1206511; Toyama K, 2002, INT J COMPUT VISION, V48, P9, DOI 10.1023/A:1014899027014	40	34	35	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2008	79	3					247	269		10.1007/s11263-007-0116-2	http://dx.doi.org/10.1007/s11263-007-0116-2			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	310KW		Green Submitted			2022-12-18	WOS:000256529500003
J	Segonne, F				Segonne, Florent			Active contours under topology control - Genus preserving level sets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						geometric deformable model; genus preservation; topology control; topological constraint; level set method; digital topology; active contours; simple points	GEOMETRICALLY ACCURATE; CORTEX SEGMENTATION; MOTION ESTIMATION; CEREBRAL-CORTEX; RECONSTRUCTION; STEREO; SHAPE; SURFACES	We present a novel framework to exert topology control over a level set evolution. Level set methods offer several advantages over parametric active contours, in particular automated topological changes. In some applications, where some a priori knowledge of the target topology is available, topological changes may not be desirable. This is typically the case in biomedical image segmentation, where the topology of the target shape is prescribed by anatomical knowledge. However, topologically constrained evolutions often generate topological barriers that lead to large geometric inconsistencies. We introduce a topologically controlled level set framework that greatly alleviates this problem. Unlike existing work, our method allows connected components to merge, split or vanish under some specific conditions that ensure that the genus of the initial active contour (i.e. its number of handles) is preserved. We demonstrate the strength of our method on a wide range of numerical experiments and illustrate its performance on the segmentation of cortical surfaces and blood vessels.	ENS INRIA ENPC, Certis Lab, Paris, France	Ecole des Ponts ParisTech	Segonne, F (corresponding author), ENS INRIA ENPC, Certis Lab, Paris, France.	florent.segonne@m4x.org						ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; Bardinet E, 1998, COMPUT VIS IMAGE UND, V71, P39, DOI 10.1006/cviu.1997.0595; Bertrand G, 1996, PATTERN RECOGN LETT, V17, P115, DOI 10.1016/0167-8655(95)00100-X; BERTRAND G, 1994, PATTERN RECOGN LETT, V15, P1003, DOI 10.1016/0167-8655(94)90032-9; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Dale AM, 1999, NEUROIMAGE, V9, P179, DOI 10.1006/nimg.1998.0395; Davatzikos C, 1996, IEEE T MED IMAGING, V15, P785, DOI 10.1109/42.544496; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; DUAN Y, 2004, EUR C COMP VIS, V3, P238; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; Fischl B, 2001, IEEE T MED IMAGING, V20, P70, DOI 10.1109/42.906426; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; Goldenberg R, 2002, IEEE T MED IMAGING, V21, P1544, DOI 10.1109/TMI.2002.806594; Goldluecke B, 2004, PROC CVPR IEEE, P350; GUSKOV I, 2001, GRAPH P, V1, P19; Han X, 2003, IEEE T PATTERN ANAL, V25, P755, DOI 10.1109/TPAMI.2003.1201824; Han X, 2002, IEEE T MED IMAGING, V21, P109, DOI 10.1109/42.993130; Hatcher A., 2005, ALGEBRAIC TOPOLOGY; Jin HL, 2003, PROC CVPR IEEE, P171; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kriegeskorte N, 2001, NEUROIMAGE, V14, P329, DOI 10.1006/nimg.2001.0831; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; MacDonald D, 2000, NEUROIMAGE, V12, P340, DOI 10.1006/nimg.1999.0534; Mangin J.-F., 1995, Journal of Mathematical Imaging and Vision, V5, P297, DOI 10.1007/BF01250286; METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Paragios N, 2005, COMPUT VIS IMAGE UND, V97, P259, DOI 10.1016/j.cviu.2003.04.001; Pons JP, 2007, INT J COMPUT VISION, V72, P179, DOI 10.1007/s11263-006-8671-5; PONS JP, 2005, THESIS ECOLE NATL PO; SEGONNE F, 2005, THESIS MIT; SEGONNE F, 2005, WORKSH COMP VIS BIOM, P135; Segonne F, 2007, IEEE T MED IMAGING, V26, P518, DOI 10.1109/TMI.2006.887364; Shattuck DW, 2001, IEEE T MED IMAGING, V20, P1167, DOI 10.1109/42.963819; TAUBIN G, 1994, IEEE T PATTERN ANAL, V16, P287, DOI 10.1109/34.276128; Xu CY, 1999, IEEE T MED IMAGING, V18, P467, DOI 10.1109/42.781013; Yezzi AJ, 2003, INT J COMPUT VISION, V53, P153, DOI 10.1023/A:1023048024042; Zhao HK, 2000, COMPUT VIS IMAGE UND, V80, P295, DOI 10.1006/cviu.2000.0875	37	34	35	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2008	79	2					107	117		10.1007/s11263-007-0102-8	http://dx.doi.org/10.1007/s11263-007-0102-8			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	302ZV					2022-12-18	WOS:000256009700001
J	Helmke, U; Huper, K; Lee, PM; Moore, J				Helmke, Uwe; Huper, Knut; Lee, Pei Yean; Moore, John			Essential matrix estimation using Gauss-Newton iterations on a manifold	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						essential matrix; stereo vision; Riemannian manifold; Newton's method; local quadratic convergence	MOTION ESTIMATION	A novel approach for essential matrix estimation is presented, this being a key task in stereo vision processing. We estimate the essential matrix from point correspondences between a stereo image pair, assuming that the internal camera parameters are known. The set of essential matrices forms a smooth manifold, and a suitable cost function can be defined on this manifold such that its minimum is the desired essential matrix. We seek a computationally efficient optimization scheme towards meeting the demands of on-line processing of video images. Our work extends and improves the earlier research by Ma et al., who proposed an intrinsic Riemannian Newton method for essential matrix computations. In contrast to Ma et al., we propose three Gauss-Newton type algorithms that have improved convergence properties and reduced computational cost. The first one is based on a novel intrinsic Newton method, using the normal Riemannian metric on the manifold consisting of all essential matrices. The other two methods are Newton-like methods, that are more efficient from a numerical point of view. Local quadratic convergence of the algorithms is shown, based on a careful analysis of the underlying geometry of the problem.	Univ Wurzburg, Dept Math, D-97074 Wurzburg, Germany; Natl ICT Australia Ltd, Canberra, ACT 2601, Australia; Australian Natl Univ, Dept Informat Engn, RSISE, Canberra, ACT 0200, Australia	University of Wurzburg; NICTA; Australian National University	Helmke, U (corresponding author), Univ Wurzburg, Dept Math, D-97074 Wurzburg, Germany.	helmke@mathematik.uni-wuerzburg.de; knut.hueper@nicta.com.au; peiyean.lee@nicta.com.au; john.moore@anu.edu.au						Hartley R., 2003, MULTIPLE VIEW GEOMET; Helmke U, 1994, OPTIMIZATION DYNAMIC; HELMKE U, 2004, P MTNS LEUV; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; HUANG TS, 1994, P IEEE, V82, P252, DOI 10.1109/5.265351; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P1310, DOI 10.1109/34.41368; Lang S., 1999, FUNDAMENTALS DIFFERE; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Ma Y, 2001, INT J COMPUT VISION, V44, P219, DOI 10.1023/A:1012276232049; MAHONY R, 1994, THESIS AUSTR NATL U; SMITH ST, 1994, HAMILTONIAN GRADIENT, P113; Soatto S, 1996, IEEE T AUTOMAT CONTR, V41, P393, DOI 10.1109/9.486640	12	34	38	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2007	74	2					117	136		10.1007/s11263-006-0005-0	http://dx.doi.org/10.1007/s11263-006-0005-0			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	167ZH		Green Submitted			2022-12-18	WOS:000246490900002
J	Xiong, YG; Quek, F				Xiong, Yingen; Quek, Francis			Hand motion gesture frequency properties and multimodal discourse analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						hand gesture; hand motion; gesticulatory oscillation; frequency ridges; multimodal discourse analysis; speech analysis		Gesture and speech are co-expressive and complementary channels of a single human language system. While speech carries the major load of symbolic presentation, gesture provides the imagistic content. We investigate the role of oscillatory/cyclical hand motions in 'carrying' this image content. We present our work on the extraction of hand motion oscillation frequencies of gestures that accompany speech. The key challenges are that such motions are characterized by non-stationary oscillations, and multiple frequencies may be simultaneously extant. Also, the duration of the oscillations may be extended over very few cycles. We apply the windowed Fourier transform and wavelet transform to detect and extract gesticulatory oscillations. We tested these against synthetic signals (stationary and non-stationary) and real data sequences of gesticulatory hand movements in natural discourse. Our results show that both filters functioned well for the synthetic signals. For the real data, the wavelet bandpass filter bank is better for detecting and extracting hand gesture oscillations. We relate the hand motion oscillatory gestures detected by wavelet analysis to speech in natural conversation and apply to multimodal language analysis. We demonstrate the ability of our algorithm to extract gesticulatory oscillations and show how oscillatory gestures reveal portions of the multimodal discourse structure.	Virginia Polytech Inst & State Univ, Dept Comp Sci, Ctr Human Comp Interact, Blacksburg, VA 24061 USA	Virginia Polytechnic Institute & State University	Xiong, YG (corresponding author), Virginia Polytech Inst & State Univ, Dept Comp Sci, Ctr Human Comp Interact, 621 McBryde Hall,MC 0106, Blacksburg, VA 24061 USA.	yxiong@cs.vt.edu						BOERSMA P, 1996, 132 I PHOT SCI U AMS; BRYLL R, 2003, THESIS WRIGHT STATE; Carmona RA, 1999, IEEE T SIGNAL PROCES, V47, P480, DOI 10.1109/78.740131; CASCIA ML, 2000, PAMI, V22, P322; COHEN CJ, 1997, 1997 IEEE INT C SYST, V5, P4513; Cutler R, 2000, IEEE T PATTERN ANAL, V22, P781, DOI 10.1109/34.868681; Davis J, 2000, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2000.855878; FUJIYOSHI H, 1998, IEEE WORKSH APPL COM; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; GOUPILLAUD P, 1984, GEOEXPLORATION, V23, P85, DOI 10.1016/0016-7142(84)90025-5; Hall P., 1992, J COMPUT GRAPH STAT, V1, P197, DOI 10.2307/1390716; HEISELE B, 1998, P INT C PATT REC BRI, P2; Liu CJ, 1998, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.1998.698700; Mallat S., 1999, WAVELET TOUR SIGNAL; MCCULLOUGH KE, 1992, ANN M BELG LING SOC; McNeill D., 1992, HAND MIND WHAT GESTU; McNeill D., 2000, LANGUAGE GESTURE, P312, DOI DOI 10.1017/CBO9780511620850.019; MCNEILL D, 2000, B JAPANESE COGNITIVE, V7; MCNEILL D, 2001, ORALITE GESTUALITE O, P474; NAKATANI CH, 1995, TR2195 HARV U; Niyogi S. A., 1994, Proceedings of the 1994 IEEE Workshop on Motion of Non-Rigid and Articulated Objects (Cat. No.94TH0671-8), P64, DOI 10.1109/MNRAO.1994.346253; NIYOGI SA, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P469, DOI 10.1109/CVPR.1994.323868; PEDELTY LL, 1987, THESIS U CHICAGO; Plotnik AM, 2002, OCEANS 2002 MTS/IEEE CONFERENCE & EXHIBITION, VOLS 1-4, CONFERENCE PROCEEDINGS, P1575, DOI 10.1109/OCEANS.2002.1191870; Polana R, 1997, INT J COMPUT VISION, V23, P261, DOI 10.1023/A:1007975200487; Quek F., 2002, ACM Transactions on Computer-Human Interaction, V9, P171, DOI 10.1145/568513.568514; QUEK F, 2003, ICCV NICE FRANCE, P13; QUEK F, 1999, ICCV 99 WKSP RATFG R, P119; QUEK F, 1998, ACCV, V2, P591; Quek F. K. H., 1994, Virtual Reality Software and Technology. Proceedings of the VRST '94 Conference, P17; Quek FKH, 1996, IEEE MULTIMEDIA, V3, P36, DOI 10.1109/93.556459; Rao R.M., 1998, WAVELET TRANSFORMS I; Seitz SM, 1997, INT J COMPUT VISION, V25, P231, DOI 10.1023/A:1007928103394; TSAI PS, 1994, PATTERN RECOGN, V27, P1591, DOI 10.1016/0031-3203(94)90079-5; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109	35	34	36	2	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2006	69	3					353	371		10.1007/s11263-006-8112-5	http://dx.doi.org/10.1007/s11263-006-8112-5			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	067YM					2022-12-18	WOS:000239338700006
J	Chen, HT; Liu, TL; Fuh, CS				Chen, HT; Liu, TL; Fuh, CS			Tone reproduction: A perspective from luminance-driven perceptual grouping	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							HIGH-DYNAMIC-RANGE; LIGHTNESS; RETINEX; DISPLAY	We address the tone reproduction problem by integrating the local adaptation effect with the consistency in global contrast impression. Many previous works on tone reproduction have focused on investigating the local adaptation mechanism of human eyes to compress high-dynamic-range (HDR) luminance into a displayable range. Nevertheless, while the realization of local adaptation properties is not theoretically defined, exaggerating such effects often leads to unnatural visual impression of global contrast. We propose to perceptually decompose the luminance into a small number of regions that sequentially encode the overall impression of an HDR image. A piecewise tone mapping can then be constructed to region-wise perform HDR compressions, using local mappings constrained by the estimated global perception. Indeed, in our approach, the region information is used not only to practically approximate the local properties of luminance, but more importantly to retain the global impression. Besides, it is worth mentioning that the proposed algorithm is efficient, and mostly does not require excessive parameter fine-tuning. Our experimental results and comparisons indicate that the described framework gives a good balance in both preserving local details and maintaining global perceptual impression of HDR scenes.	Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan; Natl Taiwan Univ, Dept CSIE, Taipei 106, Taiwan	Academia Sinica - Taiwan; National Taiwan University	Chen, HT (corresponding author), Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.	pras@iis.sinica.edu.tw; liutyng@iis.sinica.edu.tw; fuh@csie.ntu.edu.tw		Fuh, Chiou-Shann/0000-0002-6174-2556				Adelson E. H., 2000, NEW COGNITIVE NEUROS, V339, P339, DOI DOI 10.1068/P230869; Aggarwal M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P2, DOI 10.1109/ICCV.2001.937492; ALLRED SR, 2004, APGV 04 P 1 S APPL P, P161; ASHIKHMIN M, 2002, 13 EUR WORKSH REND P, P145; Barlow HB, 1961, CODING SENSORY MESSA; Bishop, 1995, NEURAL NETWORKS PATT; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; CHIU K, 1993, GRAPH INTER, P245; DEBEVEC PE, 1997, SIGGRAPH 97, P369, DOI DOI 10.1145/258734.258884; DiCarlo JM, 2000, PROC SPIE, V3965, P392, DOI 10.1117/12.385456; Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574; FATTAL R, 2002, SIGGRAPH 02, P249; FERWERDA JA, 1996, SIGGRAPH 96, P249; Gilchrist A, 1999, PSYCHOL REV, V106, P795, DOI 10.1037/0033-295X.106.4.795; Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272; Kang S. B., 2003, SIGGRAPH; Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998; KRAWCZYK G, 2004, P S APPL PERCEPTION, P172; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233; Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857; Palmer S.E., 1999, VISION SCI PHOTONS P; PATTANAIK SN, 1998, SIGGRAPH 98 C P, P287; Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Rubner Yossi, 2001, PERCEPTUAL METRICS I; Schechner YY, 2003, INT J COMPUT VISION, V53, P245, DOI 10.1023/A:1023082924255; Schlick C., 1994, PHOTOREALISTIC RENDE, P7; STOKES M, 1996, STANDART DEFAULT COL; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554; Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544; Tumblin J, 1999, ACM T GRAPHIC, V18, P56, DOI 10.1145/300776.300783; WARD G, 1994, GRAPHICS GEMS, V4, P412; WARD G, 1991, GRAPHICS GEMS, V2, P80; Yee YH, 2003, VISUAL COMPUT, V19, P457, DOI 10.1007/s00371-003-0211-5	36	34	40	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2005	65	1-2					73	96		10.1007/s11263-005-3846-z	http://dx.doi.org/10.1007/s11263-005-3846-z			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	994NU		Green Submitted			2022-12-18	WOS:000234039300004
J	Mrazek, P; Weickert, J; Steidl, G				Mrazek, P; Weickert, J; Steidl, G			Diffusion-inspired shrinkage functions and stability results for wavelet denoising	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	4th International Conference on Scale Space Methods in Computer Vision	JUN 10-12, 2003	ISLE SKYE, SCOTLAND	British Machine Vis Assoc, Kings Coll London, IT Univ Copenhagen		image denoising; wavelet shrinkage; diffusion filtering; finite differences; stability	NOISE REMOVAL; ENHANCEMENT; EQUIVALENCE; SPACE	We study the connections between discrete one-dimensional schemes for nonlinear diffusion and shift-invariant Haar wavelet shrinkage. We show that one step of a (stabilised) explicit discretisation of nonlinear diffusion can be expressed in terms of wavelet shrinkage on a single spatial level. This equivalence allows a fruitful exchange of ideas between the two fields. In this paper we derive new wavelet shrinkage functions from existing diffusivity functions, and identify some previously used shrinkage functions as corresponding to well known diffusivities. We demonstrate experimentally that some of the diffusion-inspired shrinkage functions are among the best for translation-invariant multiscale wavelet denoising. Moreover, by transferring stability notions from diffusion filtering to wavelet shrinkage, we derive conditions on the shrinkage function that ensure that shift invariant single-level Haar wavelet shrinkage is maximum-minimum stable, monotonicity preserving, and variation diminishing.	Univ Saarland, Fac Math & Comp Sci, Math Image Anal Grp, D-66041 Saarbrucken, Germany	Saarland University	Mrazek, P (corresponding author), Univ Saarland, Fac Math & Comp Sci, Math Image Anal Grp, Bldg 27, D-66041 Saarbrucken, Germany.	mrazek@mia.uni-saarland.de; weickert@mia.uni-saarland.de; steidl@math.uni-mannheim.de						ACAR R, 1994, INVERSE PROBL, V10, P1217, DOI 10.1088/0266-5611/10/6/003; Andre E, 2001, KNOWL-BASED SYST, V14, P3, DOI 10.1016/S0950-7051(00)00096-4; BAO Y, 2001, COMPUTATIONAL IMAGIN, V19, pCH6; Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192; Brox T, 2003, LECT NOTES COMPUT SC, V2695, P86; Candes EJ, 2002, SIGNAL PROCESS, V82, P1519, DOI 10.1016/S0165-1684(02)00300-6; Chambolle A, 2001, IEEE T IMAGE PROCESS, V10, P993, DOI 10.1109/83.931093; Chambolle A, 1998, IEEE T IMAGE PROCESS, V7, P319, DOI 10.1109/83.661182; CHAN TF, 2000, P 7 INT C IM PROC VA; CHARBONNIER P, 1994, IEEE IMAGE PROC, P168; Cohen A, 1999, AM J MATH, V121, P587; Coifman RR, 2000, APPL COMPUT HARMON A, V9, P1, DOI 10.1006/acha.2000.0299; Coifman RR, 2001, SIAM J NUMER ANAL, V39, P480, DOI 10.1137/S0036142999362031; Crank J., 1975, MATH DIFFUSION, V2nd; Donoho D.L., 1995, WAVELETS STAT, V103, P125; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Durand S, 2003, SIAM J SCI COMPUT, V24, P1754, DOI 10.1137/S1064827501397792; Gao HY, 1997, STAT SINICA, V7, P855; Gao HY, 1998, J COMPUT GRAPH STAT, V7, P469; Gilboa G, 2002, IEEE T IMAGE PROCESS, V11, P689, DOI 10.1109/TIP.2002.800883; GLASHOFF K, 1980, NUMER MATH, V35, P343, DOI 10.1007/BF01396416; Holschneider M., 1990, WAVELETS, p286?297, DOI DOI 10.1007/978-3-642-75988-8_28; Hummel R. A., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P204; Iijima T., 1962, BULL ELECT LAB, V26, P368; Kawohl B, 1998, MATH ANN, V311, P107, DOI 10.1007/s002080050179; Keeling SL, 2002, INVERSE PROBL, V18, P175, DOI 10.1088/0266-5611/18/1/312; Kichenassamy S, 1997, SIAM J APPL MATH, V57, P1328, DOI 10.1137/S003613999529558X; Malgouyres F, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P57, DOI 10.1109/VLSM.2001.938882; MALGOUYRES F, 2002, INVERSE PROBL, V2, P1; Mallat S., 1999, WAVELET TOUR SIGNAL, DOI 10.1016/B978-012466606-1/50008-8; Mrazek P, 2003, LECT NOTES COMPUT SC, V2781, P156; Mrazek P, 2003, LECT NOTES COMPUT SC, V2695, P101; MRAZEK P, 2003, P 8 COMP VIS WINT WO, P61; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Schoenberg I, 1930, MATH Z, V32, P321, DOI 10.1007/BF01194637; Smolka B., 2002, Pattern Recognition. 24th DAGM Symposium. Proceedings (Lecture Notes in Computr Science Vol.2449), P314; Starck JL, 2003, IEEE T IMAGE PROCESS, V12, P706, DOI 10.1109/TIP.2003.813140; Steidl G., 2002, Pattern Recognition. 24th DAGM Symposium. Proceedings (Lecture Notes in Computr Science Vol.2449), P198; Steidl G, 2004, SIAM J NUMER ANAL, V42, P686, DOI 10.1137/S0036142903422429; Sturm C, 1836, J MATH PURE APPL, V1, P373; Varga RS., 1999, MATRIX ITERATIVE ANA; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; Weickert J., 1997, ADV COMPUTER VISION, P1	46	34	35	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2005	64	2-3					171	186		10.1007/s11263-005-1842-y	http://dx.doi.org/10.1007/s11263-005-1842-y			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	961YC		Green Submitted			2022-12-18	WOS:000231696700007
J	Qian, G; Chellappa, R				Qian, G; Chellappa, R			Structure from motion using sequential Monte Carlo methods	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; sequential Monte Carlo methods; video analysis	RECOVERING 3-D MOTION; INHERENT AMBIGUITIES; MAXIMUM-LIKELIHOOD	In this paper, the structure from motion (SfM) problem is addressed using sequential Monte Carlo methods. A new SfM algorithm based on random sampling is derived to estimate the posterior distributions of camera motion and scene structure for the perspective projection camera model. Experimental results show that challenging issues in solving the SfM problem, due to erroneous feature tracking, feature occlusion, motion/structure ambiguity, mixed-domain sequences, mismatched features, and independently moving objects, can be well modeled and effectively addressed using the proposed method.	Univ Maryland, Ctr Automat Res, Dept Elect & Comp Engn, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Qian, G (corresponding author), Arizona State Univ, Arts Media & Engn Program, Tempe, AZ 85287 USA.	gang.qian@asu.edu; rama@cfar.umd.edu	Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/AAV-8690-2020					ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; AZARBAYEJANI A, 1995, IEEE T PATTERN ANAL, V17, P562, DOI 10.1109/34.387503; BROIDA TJ, 1990, IEEE T AERO ELEC SYS, V26, P639, DOI 10.1109/7.55557; Chiuso A, 2002, IEEE T PATTERN ANAL, V24, P523, DOI 10.1109/34.993559; Daniilidis K., 1993, IEEE C COMP VIS PATT, P188; DELLAERT F, 2000, IEEE COMPUTER VISION; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Doucet A., 2001, SEQUENTIAL MONTE CAR; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FORSYTH D, 1999, INT C COMP VIS ICCV, P660; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HUANG TS, 1994, P IEEE, V82, P252, DOI 10.1109/5.265351; ISARD M, 1996, P EUR C COMP VIS CAM, V1, P343; Jebara T, 1999, IEEE SIGNAL PROC MAG, V16, P66, DOI 10.1109/79.768574; JERIAN CP, 1991, IEEE T SYST MAN CYB, V21, P572, DOI 10.1109/21.97478; Kitagawa Genshiro, 2021, J COMPUT GRAPH STAT, V5, P1, DOI [DOI 10.2307/1390750, 10.2307/1390750]; KONG A, 1994, J AM STAT ASSOC, V89, P278, DOI 10.2307/2291224; Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Oliensis J., 2000, CRITIQUE STRUCTURE M; Qian G, 2001, J OPT SOC AM A, V18, P2982, DOI 10.1364/JOSAA.18.002982; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Soatto S, 1998, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.1998.698621; Soatto S, 1998, IEEE T PATTERN ANAL, V20, P933, DOI 10.1109/34.713360; Tian TY, 1996, PROC CVPR IEEE, P315, DOI 10.1109/CVPR.1996.517091; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; WU TH, 1995, INT J COMPUT VISION, V15, P77, DOI 10.1007/BF01450850; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903; ZHANG Z, 1996, 2927 INRIA FRENCH NA	31	34	34	3	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2004	59	1					5	31		10.1023/B:VISI.0000020669.68126.4b	http://dx.doi.org/10.1023/B:VISI.0000020669.68126.4b			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816HI		Green Submitted			2022-12-18	WOS:000221100600001
J	Alter, T; Basri, R				Alter, T; Basri, R			Extracting salient curves from images: An analysis of the Saliency Network	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CONTOURS; ENERGY	The Saliency Network proposed by Shashua and Ullman (1988) is a well-known approach to the problem of extracting salient curves from images while performing gap completion. This paper analyzes the Saliency Network. The Saliency Network is attractive for several reasons. First, the network generally prefers long and smooth curves over short or wiggly ones. While computing saliencies, the network also fills in gaps with smooth completions and tolerates noise. Finally, the network is locally connected, and its size is proportional to the size of the image. Nevertheless, our analysis reveals certain weaknesses with the method. In particular, we show cases in which the most salient element does not lie on the perceptually most salient curve. Furthermore, in some cases the saliency measure changes its preferences when curves are scaled uniformly. Also, we show that for certain fragmented curves the measure prefers large gaps over a few small gaps of the same total size. In addition, we analyze the time complexity required by the method. We show that the number of steps required for convergence in serial implementations is quadratic in the size of the network, and in parallel implementations is linear in the size of the network. We discuss problems due to coarse sampling of the range of possible orientations. Finally, we consider the possibility of using the Saliency Network for grouping. We show that the Saliency Network recovers the most salient curve efficiently, but it has problems with identifying any salient curve other than the most salient one.	MIT, AI Lab, Cambridge, MA 02139 USA; Weizmann Inst Sci, Dept Appl Math, IL-76100 Rehovot, Israel	Massachusetts Institute of Technology (MIT); Weizmann Institute of Science	Alter, T (corresponding author), MIT, AI Lab, 545 Technol Sq, Cambridge, MA 02139 USA.	tda@ai.mit.edu; ronen@wisdom.weizmann.ac.il						ALTER TD, 1996, P IEEE C COMP VIS PA; AMINI AA, 1990, IEEE T PATTERN ANAL, V12, P855, DOI 10.1109/34.57681; BRADY M, 1980, P 1 ANN C ART INT; BRADY M, 1981, 666 MIT AI; BRUCKSTEIN AM, 1990, COMPUT VISION GRAPH, V49, P283, DOI 10.1016/0734-189X(90)90105-5; ELDER J, 1993, TR932 MCGILL RES CTR; FINKEL LH, 1992, NEURAL COMPUT, V4, P901, DOI 10.1162/neco.1992.4.6.901; FREEMAN WT, 1992, 190 MIT MED LAB; GEIGER D, 1995, IEEE T PATTERN ANAL, V17, P294, DOI 10.1109/34.368194; GROSSBERG S, 1987, COMPUT VISION GRAPH, V37, P116, DOI 10.1016/S0734-189X(87)80015-4; GUY G, 1993, P DARPA IM UND WORKS, P881; HEITGER F, 1993, P 4 INT C COMP VIS, P32; HERAULT L, 1993, IEEE T PATTERN ANAL, V15, P899, DOI 10.1109/34.232076; HORN BKP, 1983, ACM T MATH SOFTWARE, V9, P441, DOI 10.1145/356056.356061; JACOBS DW, 1993, 1416 MIT ART INT LAB; Kass Michael, 1988, INT J COMPUTER VISIO, V988; MARTELLI A, 1976, COMM ACM, V19; MOHAN R, 1988, P DARPA IMAG UND WOR, P512; MOHAN R, 1992, T PATERN ANAL MACHIN, V14; MONTANARI U, 1971, COMM ACM, V14; Mumford D., 1994, ALGEBRAIC GEOMETRY I, V5681, P491, DOI DOI 10.1007/978-1-4612-2628-4_31; PARENT P, 1989, T PATTERN ANAL MACHI, V11; PAVLIDIS T, 1990, IEEE T PATTERN ANAL, V12, P225, DOI 10.1109/34.49050; RUTKOWSKI WS, 1979, COMPUT VISION GRAPH, V9, P89, DOI 10.1016/0146-664X(79)90086-8; Sha'ashua A., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P321, DOI 10.1109/CCV.1988.590008; SHAASHUA A, 1988, CSS8818 WEIZM I SCI; SHAASHUA A, 1990, ADV NEURAL INFORMATI, V3, P335; SUBIRANAVILANOV.J, 1991, 1137 MIT AI; SUBIRANAVILANOV.J, 1992, 1318 MIT AI; ULLMAN S, 1976, BIOL CYBERN, V25, P1; WEISS I, 1988, COMPUT VISION GRAPH, V41, P80, DOI 10.1016/0734-189X(88)90118-1; WILLIAMS LR, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P408, DOI 10.1109/ICCV.1995.466910; WILLIAMS LR, IN PRESS COMPUTER VI; WILLIAMS LR, 1994, P IEEE C COMP VIS PA; [No title captured]	35	34	39	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1998	27	1					51	69		10.1023/A:1007953729443	http://dx.doi.org/10.1023/A:1007953729443			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZH371					2022-12-18	WOS:000073101800003
J	Bhat, DN; Nayar, SK				Bhat, DN; Nayar, SK			Stereo and specular reflection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						stereo; correspondence; specular reflection; image matching; depth estimation		The problem of accurate depth estimation using stereo in the presence of specular reflection is addressed. Specular reflection, a fundamental and ubiquitous reflection mechanism, is viewpoint dependent and can cause large intensity differences at corresponding points, resulting in significant depth errors. We analyze the physics of specular reflection and the geometry of stereopsis which lead to a relationship between stereo vergence, surface roughness, and the likelihood of a correct match. Given a lower bound on surface roughness, an optimal binocular stereo configuration can be determined which maximizes precision in depth estimation despite specular reflection. However, surface roughness is difficult to estimate in unstructured environments. Therefore, trinocular configurations, independent of surface roughness are determined such that at each scene point visible to all sensors, at least one stereo pair can produce correct depth. We have developed a simple algorithm to reconstruct depth from the multiple stereo pairs.	Columbia Univ, Dept Comp Sci, New York, NY 10027 USA	Columbia University	Bhat, DN (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.							BARNARD ST, 1982, ACM COMPUT SURV, V14, P553, DOI DOI 10.1145/356893.356896; BHAT D, 1994, CUCS03094; Bhat DN, 1996, PROC CVPR IEEE, P351, DOI 10.1109/CVPR.1996.517096; BLAKE A, 1985, P INT JOINT C ART IN, P973; BRELSTAFF G, 1988, P IEEE INT C COMP VI, P297; CHING WS, 1993, P IEEE COMP SOC INT, P384; DHOND UR, 1991, INT J COMPUT VISION, V6, P39, DOI 10.1007/BF00127125; GENNERT MA, 1988, P INT C COMP VIS, P139; Ito M., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P9; MATTHIES L, 1992, INT J COMPUT VISION, V8, P71, DOI 10.1007/BF00126401; Nayar S. K., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P583, DOI 10.1109/CVPR.1993.341071; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; Oren M., 1994, P ECCV, P269, DOI DOI 10.1007/BFB0028360; Rodriguez J. J., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P153, DOI 10.1109/CVPR.1988.196229; Smith G. B., 1986, Proceedings AAAI-86: Fifth National Conference on Artificial Intelligence, P689; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; VERRI A, 1986, J OPT SOC AM A, V3, P297, DOI 10.1364/JOSAA.3.000297; WOLFF LB, 1994, P EUR C COMP VIS, P247; YASHIDA M, 1986, P 8 INT C PATT REC, P27	20	34	38	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1998	26	2					91	106		10.1023/A:1007940725322	http://dx.doi.org/10.1023/A:1007940725322			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZC477					2022-12-18	WOS:000072583200002
J	Liu, HC; Hong, TH; Herman, M; Chellappa, R				Liu, HC; Hong, TH; Herman, M; Chellappa, R			A general motion model and spatio-temporal filters for computing optical flow	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hermite polynomial; motion estimation; evaluation	RECOVERING 3-D MOTION; INHERENT AMBIGUITIES; IMAGE VELOCITY; FIELD; SEGMENTATION; OPTIMIZATION; COMPUTATION; PERCEPTION	Traditional optical flow algorithms assume local image translational motion and apply simple image filtering techniques. Recent studies have taken two separate approaches toward improving the accuracy of computed flow: the application of spatio-temporal filtering schemes and the use of advanced motion models such as the affine model. Each has achieved some improvement over traditional algorithms in specialized situations but the computation of accurate optical flow for general motion has been elusive. In this paper, we exploit the interdependency between these two approaches and propose a unified approach. The general motion model we adopt characterizes arbitrary 3-D steady motion. Under perspective projection, we derive an image motion equation that describes the spatio-temporal relation of gray-scale intensity in an image sequence, thus making the utilization of 3-D filtering possible. However, to accommodate this motion model, we need to extend the filter design to derive additional motion constraint equations. Using Hermite polynomials, we design differentiation filters, whose orthogonality and Gaussian derivative properties insure numerical stability; a recursive relation facilitates application of the general nonlinear motion model while separability promotes efficiency. The resulting algorithm produces accurate optical flow and other useful motion parameters. It is evaluated quantitatively using the scheme established by Barren et al. (1994) and qualitatively with real images.	NIST, DIV INTELLIGENT SYST, GAITHERSBURG, MD 20899 USA; UNIV MARYLAND, CTR AUTOMAT RES, DEPT ELECT ENGN, COLLEGE PK, MD 20742 USA	National Institute of Standards & Technology (NIST) - USA; University System of Maryland; University of Maryland College Park			Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/AAJ-1504-2020					ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; ALOIMONOS Y, 1994, INT J COMPUT VISION, V13, P33, DOI 10.1007/BF01420794; ANANDAN P, 1987, 8721 COINS TR U MASS; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BERGEN JR, 1992, IEEE T PATTERN ANAL, V14, P886, DOI 10.1109/34.161348; BOBER M, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P947, DOI 10.1109/CVPR.1994.323931; BURT PJ, 1991, OCT IEEE WORKSH VIS, P187; BUXTON BF, 1983, PROC R SOC SER B-BIO, V218, P27, DOI 10.1098/rspb.1983.0024; CAMPANI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P90, DOI 10.1016/1049-9660(92)90088-K; CHOU WS, 1993, PATTERN RECOGN, V26, P351, DOI 10.1016/0031-3203(93)90043-V; COOMBS D, 1995, P IEEE INT C COMP VI; DUNCAN J, 1989, IEEE T PATTERN ANAL, V14, P346; Fang J.-Q., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P253; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; Gibson JJ., 1966, SENSES CONSIDERED PE; GIROSI F, 1989, MAR P IEEE WORKSH VI, P116; GIUSSIN R, 1991, P IEEE WORKSH VIS MO, P146; GRZYWACZ NM, 1990, PROC R SOC SER B-BIO, V239, P129, DOI 10.1098/rspb.1990.0012; GUPTA N, 1993, THESIS U MARYLAND; Haralick R. M., 1983, P IMAGE UNDERSTANDIN, P84; HARTLEY R, 1985, PATTERN RECOGN LETT, V3, P253, DOI 10.1016/0167-8655(85)90005-4; HASHIMOTO M, 1987, COMPUT VISION GRAPH, V39, P28, DOI 10.1016/S0734-189X(87)80201-3; HEEGER DJ, 1987, INT J COMPUT VISION, V1, P279, DOI 10.1007/BF00133568; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN RA, 1991, TOPICS MATRIX ANAL, P382; KOENDERINK JJ, 1987, BIOL CYBERN, V55, P367, DOI 10.1007/BF00318371; LIU H, 1995, P IEEE INT S COMP VI, P587; LIU H, 1993, 5333 NISTIR; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; MURRAY DW, 1987, IEEE T PATTERN ANAL, V9, P220, DOI 10.1109/TPAMI.1987.4767896; MUTCH KM, 1985, IEEE T PATTERN ANAL, V7, P133, DOI 10.1109/TPAMI.1985.4767638; NAGEL HH, 1983, COMPUT VISION GRAPH, V21, P85, DOI 10.1016/S0734-189X(83)80030-9; NAGEL HH, 1992, ARTIFICIAL BIOL VISI, P193; NAGEL HH, 1983, AUG P INT JOINT C AR, P945; Negahdaripour S., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P132, DOI 10.1109/WVM.1991.212778; NELSON RC, 1989, IEEE T PATTERN ANAL, V11, P1102, DOI 10.1109/34.42840; PRAZDNY K, 1980, BIOL CYBERN, V36, P87, DOI 10.1007/BF00361077; PRAZDNY K, 1981, P IEEE C PATT REC IM, P82; SINGH A, 1990, P INT C COMP VIS OS, P169; SRINIVASAN MV, 1990, BIOL CYBERN, V63, P421, DOI 10.1007/BF00199574; Stewart G., 1973, INTRO MATRIX COMPUTA; THOMPSON WB, 1985, IEEE T PATTERN ANAL, V7, P374, DOI 10.1109/TPAMI.1985.4767677; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; Wang J. Y. A., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P361, DOI 10.1109/CVPR.1993.341105; Waxman A. M., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P717, DOI 10.1109/CVPR.1988.196313; WEBER J, 1995, INT J COMPUT VISION, V14, P67, DOI 10.1007/BF01421489; WEBER J, 1993, P 4 INT C COMP VIS B; WERKHOVEN P, 1990, BIOL CYBERN, V63, P185, DOI 10.1007/BF00195857; Young G.-S., 1992, Proceedings. IEEE Workshop on Applications of Computer Vision (Cat. No.92TH0446-5), P100, DOI 10.1109/ACV.1992.240322; YOUNG GS, 1993, P SOC PHOTO-OPT INS, V2056, P31, DOI 10.1117/12.150227; YOUNG GS, 1993, THESIS U MARYLAND; YOUNG GSJ, 1990, IEEE T PATTERN ANAL, V12, P735, DOI 10.1109/34.57666; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903; Young R. A., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P564; Zheng Q, 1993, IEEE T IMAGE PROCESS, V2, P311, DOI 10.1109/83.236535	59	34	37	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1997	22	2					141	172		10.1023/A:1007988028861	http://dx.doi.org/10.1023/A:1007988028861			32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XD967					2022-12-18	WOS:A1997XD96700003
J	Sinclair, D; Blake, A				Sinclair, D; Blake, A			Quantitative planar region detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							MOTION	This paper presents a means of segmenting planar regions from two views of a scene using point correspondences. The initial selection of groups of coplanar points is performed on the basis of conservation of two five point projective invariants (groups for which this invariant is conserved are assumed to be coplanar). The five point correspondences are used to estimate a projectivity which is used to predict the change in position of other points assuming they lie on the same plane as the original four. The variance in any points new position is then used to define a distance threshold between actual and predicted position which is used as a coplanarity test to find extended planar regions. If two distinct planar regions can be found then a novel motion direction estimator suggests itself. The projection of the line of intersection of two planes in an image may also be recovered. An analytical error model is derived which relates image uncertainty in a corner's position to genuine perpendicular height of a point above a given plane in the world. The model may be used for example to predict the performance of given stereo ground plane prediction system or a monocular drivable region detection system on and AGV. The model may also be used in reverse to determine the camera resolution required if a vehicle in motion is to resolve obstacles of a given height a given distance from it.	UNIV OXFORD,DEPT ENGN SCI,OXFORD OX1 3PJ,ENGLAND	University of Oxford	Sinclair, D (corresponding author), AALBORG UNIV,INST ELECT SYST,LAB IMAGE ANAL,FR BAJERSVEJ 7,BLDG D1,DK-9220 AALBORG E,DENMARK.							COEHLO C, 1991, DARPA IMAGE UNDERSTA, P273; FAUGERAS D, 1988, INT J COMPUTER VISIO, V4, P225; FORSYTH DA, 1990, 1ST P EUR C COMP VIS, P427; HARRIS CG, 1987, 3RD ALV VIS C, P189; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; KANATANI K, 1991, GEOMETRIC COMPUTATIO; LONGUETHIGGINS HC, 1984, PROC R SOC SER B-BIO, V223, P165, DOI 10.1098/rspb.1984.0088; LONGUETHIGGINS HC, 1986, P R SOC LOND, P399; Mundy J., 1992, GEOMETRIC INVARIANCE; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; SMITH S, 1992, THESIS U OXFORD; SPRINGER CE, 1964, GEOMETRY ANAL PROJEC, V1; TSAI RY, 1981, IEEE T ACOUST SPEECH, V29, P1147, DOI 10.1109/TASSP.1981.1163710	14	34	35	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1996	18	1					77	91		10.1007/BF00126141	http://dx.doi.org/10.1007/BF00126141			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	UJ409					2022-12-18	WOS:A1996UJ40900005
J	VENKATESWAR, V; CHELLAPPA, R				VENKATESWAR, V; CHELLAPPA, R			HIERARCHICAL STEREO AND MOTION CORRESPONDENCE USING FEATURE GROUPINGS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							IMAGES; RECOGNITION; SYSTEM	Hierarchical feature based stereo matching and motion correspondence algorithms are presented. The hierarchy consists of lines, vertices, edges and surfaces. Matching starts at the highest level of the hierarchy (surfaces) and proceeds to the lowest (lines). Higher level features are easier to match, because they are fewer in number and more distinct in form. These matches then constrain the matches at lower levels. Perceptual and structural relations are used to group matches into islands of certainty. A Truth Maintenance System (TMS) is used to enforce grouping constraints and eliminate inconsistent match groupings. The TMS is also used to carry out belief revisions necessitiated by additions, deletions and confirmations of feature and match hypotheses.	UNIV MARYLAND, CTR AUTOMAT RES, DEPT ELECT ENGN, COLLEGE PK, MD 20742 USA; UNIV MARYLAND, INST ADV COMP STUDIES, COLLEGE PK, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	VENKATESWAR, V (corresponding author), TEXAS INSTRUMENTS INC, POB 655474, DALLAS, TX 75265 USA.		Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/AAJ-1504-2020					ARNOLD RD, 1980, SPIE, V238, P281; AYACHE N, 1987, INT J COMPUT VISION, V1, P107, DOI 10.1007/BF00123161; AYACHE N, 1988, P INT C PATTERN RECO; BAKER H, 1982, P IMAGE UNDERSTANDIN, P215; Ballard D.H., 1982, COMPUTER VISION; Beveridge J. R., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P18, DOI 10.1109/ICPR.1990.118058; Beveridge J. R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P432, DOI 10.1109/CVPR.1992.223154; BHATNAGAR RK, 1988, UNCERTAINTY ARTIFICI, P3; BODINGTON RM, 1990, P EUROPEAN C COMPUTE, P542; Bolles R. C., 1982, INT J ROBOT RES, V1, P57; BOWEN JB, 1988, IMAGE VISION COMPUT, V6, P12; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chandrashekhar S., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P2, DOI 10.1109/WVM.1991.212795; Chung R. C., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P50, DOI 10.1109/CVPR.1991.139660; Crowley J. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P658, DOI 10.1109/CCV.1988.590047; DEKLEER J, 1986, ARTIF INTELL, V28, P127, DOI 10.1016/0004-3702(86)90080-9; DEKLEER J, 1986, ARTIF INTELL, V28, P197, DOI 10.1016/0004-3702(86)90082-2; DERICHE R, 1990, 1ST P EUR C COMP VIS, P259; DOYLE J, 1979, ARTIF INTELL, V12, P231, DOI 10.1016/0004-3702(79)90008-0; Faugeras O. D., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P25; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; Grimson W. E. L., 1990, OBJECT RECOGNITION C; GRIMSON WEL, 1981, IMAGES SURFACES; HERMAN M, 1986, ARTIF INTELL, V30, P289, DOI 10.1016/0004-3702(86)90002-0; HOFF W, 1989, IEEE T PATTERN ANAL, V11, P121, DOI 10.1109/34.16709; HORAUD R, 1989, IEEE T PATTERN ANAL, V11, P1168, DOI 10.1109/34.42855; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; HWANG VSS, 1986, COMPUT VISION GRAPH, V36, P321, DOI 10.1016/0734-189X(86)90081-2; LASKEY KB, 1989, ARTIF INTELL, V41, P65, DOI 10.1016/0004-3702(89)90078-7; LIM HS, 1988, P DARPA IM UND WORKS, P794; LIM HS, 1988, P DARPA IM UND WORKS, P809; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; MACKWORTH AK, 1977, ARTIF INTELL, V8, P99, DOI 10.1016/0004-3702(77)90007-8; MAYHEW JEW, 1981, ARTIF INTELL, V17, P349, DOI 10.1016/0004-3702(81)90029-1; MEDIONI G, 1985, COMPUT VISION GRAPH, V31, P2, DOI 10.1016/S0734-189X(85)80073-6; MOHAN R, 1989, IEEE T PATTERN ANAL, V11, P1121, DOI 10.1109/34.42852; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; PRICE KE, 1985, IEEE T PATTERN ANAL, V7, P617, DOI 10.1109/TPAMI.1985.4767709; PROVAN G, 1987, P AAAI 87 SEATTLE, P173; PROVAN GM, 1988, 4TH P IEEE C ART INT, P230; Sawhney H. S., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P418, DOI 10.1109/CVPR.1992.223156; Sawhney H. S., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P179, DOI 10.1109/CVPR.1991.139684; STALLMAN RM, 1977, ARTIF INTELL, V9, P135, DOI 10.1016/0004-3702(77)90029-7; VENKATESWAR V, 1992, IEEE T PATTERN ANAL, V14, P1111, DOI 10.1109/34.166627; VENKATESWAR V, 1991, CARTR567 U MARYL CTR; Waltz D., 1975, PSYCHOL COMPUTER VIS, P19; Williams L. R., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P441, DOI 10.1109/CCV.1988.590021; 1987, ART REFERENCE MANUAL	49	34	37	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	1995	15	3					245	269		10.1007/BF01451743	http://dx.doi.org/10.1007/BF01451743			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RM520					2022-12-18	WOS:A1995RM52000004
J	Pavllo, D; Feichtenhofer, C; Auli, M; Grangier, D				Pavllo, Dario; Feichtenhofer, Christoph; Auli, Michael; Grangier, David			Modeling Human Motion with Quaternion-Based Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human motion modeling; Quaternion; Deep learning; Neural networks; Motion generation	ANIMATION	Previous work on predicting or generating 3D human pose sequences regresses either joint rotations or joint positions. The former strategy is prone to error accumulation along the kinematic chain, as well as discontinuities when using Euler angles or exponential maps as parameterizations. The latter requires re-projection onto skeleton constraints to avoid bone stretching and invalid configurations. This work addresses both limitations. QuaterNet represents rotations with quaternions and our loss function performs forward kinematics on a skeleton to penalize absolute position errors instead of angle errors. We investigate both recurrent and convolutional architectures and evaluate on short-term prediction and long-term generation. For the latter, our approach is qualitatively judged as realistic as recent neural strategies from the graphics literature. Our experiments compare quaternions to Euler angles as well as exponential maps and show that only a very short context is required to make reliable future predictions. Finally, we show that the standard evaluation protocol for Human3.6M produces high variance results and we propose a simple solution.	[Pavllo, Dario] Swiss Fed Inst Technol, Zurich, Switzerland; [Feichtenhofer, Christoph; Auli, Michael] Facebook AI Res, Menlo Pk, CA 94025 USA; [Grangier, David] Google Brain, Mountain View, CA USA	Swiss Federal Institutes of Technology Domain; ETH Zurich; Facebook Inc; Google Incorporated	Auli, M (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.	michaelauli@fb.com						Akhter I., 2015, 2015 IEEE C COMP VIS; Arikan O., 2003, ACM T GRAPHICS SIGGR; Badler Norman I., 1993, SIMULATING HUMANS CO; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Bengio Y, 2001, ADV NEUR IN, V13, P932; Butepage J., 2017, C COMP VIS PATT REC; Butepage J, 2018, IEEE INT CONF ROBOT, P4563; Byravan Arunkumar, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P173, DOI 10.1109/ICRA.2017.7989023; Chao Y. W., 2017, C COMP VIS PATT REC; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chung J., 2014, ARXIV14123555; CMU, 2003, CMU GRAPH LAB MOT CA; Collobert R., 2016, WAV2LETTER END TO EN; Cootes T, 2000, PRACT APPROACH SER, P223; Dai JS, 2015, MECH MACH THEORY, V92, P144, DOI 10.1016/j.mechmachtheory.2015.03.004; Dauphin Y. N., 2017, P ICLR; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Dunn F., 2010, 3D MATH PRIMER GRAPH; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Forsyth DA, 2005, FOUND TRENDS COMPUT, V1, P77, DOI 10.1561/0600000005; Fragkiadaki K., 2015, C VIS PATT REC CVPR; Gaudet C.J., 2018, 2018 INT JOINT C NEU, P1; Gehring J, 2017, PR MACH LEARN RES, V70; Ghosh P, 2017, INT CONF 3D VISION, P458, DOI 10.1109/3DV.2017.00059; Gong WJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16121966; Gopalakrishnan A., 2018, ARXIV180903036; Grassia F.S., 1998, J GRAPH TOOLS, V3, P29, DOI [10.1080/10867651.1998.10487493, DOI 10.1080/10867651.1998.10487493]; Gu C., 2018, COMPUTER VISION PATT; Gui Liang-Yan, 2018, EUR C COMP VIS ECCV; Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Herda L, 2005, COMPUT VIS IMAGE UND, V99, P189, DOI 10.1016/j.cviu.2005.01.005; Hinton, 2016, ARXIV PREPRINT ARXIV; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663; Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975; Holden Daniel, 2015, PROCEEDING SA 15 SIG, P1, DOI [10.1145/2820903.2820918, DOI 10.1145/2820903.2820918]; Huynh DQ, 2009, J MATH IMAGING VIS, V35, P155, DOI 10.1007/s10851-009-0161-2; Ionescu C., 2011, INT C COMP VIS ICCV; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573; Kiasari M.A., 2018, ARXIV PREPRINT ARXIV; Kingma D.P, P 3 INT C LEARNING R; Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15; Kolen J.F., 2001, GRADIENT FLOW RECURR, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lan T., 2014, EUR C COMP VIS ECCV; LaValle SM, 2006, PLANNING ALGORITHMS, P150; Lehrmann A. M., 2014, C COMP VIS PATT REC; Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446; Li Z., 2018, INT C LEARN REPR ICL; Lin Xiao, 2018, ARXIV180410652; Liu CK, 2005, ACM T GRAPHIC, V24, P1071, DOI 10.1145/1073204.1073314; Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50; Luc P, 2017, IEEE I CONF COMP VIS, P648, DOI 10.1109/ICCV.2017.77; Luc Pauline, 2018, ARXIV180311496; Martinez J., 2017, C VIS PATT REC CVPR; Mathieu Michael, 2016, ICLR; McCarthy J.M., 1990, INTRO THEORETICAL KI; Menache A., 1999, UNDERSTANDING MOTION; Muller M., 2007, CG20072 U BONN, P6; Multon F, 1999, J VISUAL COMP ANIMAT, V10, P39, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<39::AID-VIS195>3.0.CO;2-2; Oberweger Markus, 2015, ARXIV150206807; Ofli F., 2013, P IEEE WORKSH APPL C; Oord A.V.D., 2016, SSW; Parameswaran V., 2004, C COMP VIS PATT REC; Parcollet T., 2018, ARXIV180604418; Parcollet T, 2018, INTERSPEECH, P22; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794; Pavllo Dario, 2018, BRIT MACH VIS C BMVC, P299; Pavlovic V, 2001, ADV NEUR IN, V13, P981; Pervin E., 1983, C COMP VIS PATT REC; Radwan I, 2013, IEEE I CONF COMP VIS, P1888, DOI 10.1109/ICCV.2013.237; Ranzato M., 2015, INT C LEARN REPR ICL; Shlizerman E, 2018, PROC CVPR IEEE, P7574, DOI 10.1109/CVPR.2018.00790; Shoemaker K., 1985, Computer Graphics, V19, P245, DOI 10.1145/325165.325242; Stoer J, 2013, INTRO NUMERICAL ANAL; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tanco L. M., 2000, WORKSH HUM MOT HUMO; Taylor G.W., 2006, ADV NEURAL INFORM PR, V19, P1345; Toyer S., 2017, INT C DIG IM COMP TE; Treuille A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239458; Tripathi B. K., 2017, INT J INTELLIGENT EN, V10, P205, DOI DOI 10.22266/ijies2017.0831.22; Van Den Bergen G., 2010, GAME PHYS PEARLS; van den Oord A, 2016, PR MACH LEARN RES, V48; Villegas R, 2018, PROC CVPR IEEE, P8639, DOI 10.1109/CVPR.2018.00901; Villegas R, 2017, PR MACH LEARN RES, V70; Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361; Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167; Wang Zhiyong, 2018, ARXIV180608666; Wiseman Sam, 2016, ARXIV160602960; Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999; Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137; Zhou X., 2016, ARXIV160606854; Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17; Zhou Yi, 2018, INT C LEARN REPR; Zhu XY, 2018, LECT NOTES COMPUT SC, V11212, P645, DOI 10.1007/978-3-030-01237-3_39	100	33	33	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		855	872		10.1007/s11263-019-01245-6	http://dx.doi.org/10.1007/s11263-019-01245-6			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Green Submitted			2022-12-18	WOS:000525393600007
J	Wang, SF; Fowlkes, CC				Wang, Shaofei; Fowlkes, Charless C.			Learning Optimal Parameters for Multi-target Tracking with Contextual Interactions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-target tracking; Data association; Network-flow; Structured prediction		We describe an end-to-end framework for learning parameters of min-cost flow multi-target tracking problem with quadratic trajectory interactions including suppression of overlapping tracks and contextual cues about co-occurrence of different objects. Our approach utilizes structured prediction with a tracking-specific loss function to learn the complete set of model parameters. In this learning framework, we evaluate two different approaches to finding an optimal set of tracks under a quadratic model objective, one based on an linear program (LP) relaxation and the other based on novel greedy variants of dynamic programming that handle pairwise interactions. We find the greedy algorithms achieve almost equivalent accuracy to the LP relaxation while being up to 10 faster than a commercial LP solver. We evaluate trained models on three challenging benchmarks. Surprisingly, we find that with proper parameter learning, our simple data association model without explicit appearance/motion reasoning is able to achieve comparable or better accuracy than many state-of-the-art methods that use far more complex motion features or appearance affinity metric learning.	[Wang, Shaofei; Fowlkes, Charless C.] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA	University of California System; University of California Irvine	Wang, SF (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.	sfwang0928@gmail.com; fowlkes@ics.uci.edu			US National Science Foundation [IIS-1253538, DBI-1053036]; Direct For Computer & Info Scie & Enginr [1253538] Funding Source: National Science Foundation	US National Science Foundation(National Science Foundation (NSF)); Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This work was supported by the US National Science Foundation through Awards IIS-1253538 and DBI-1053036.	Ahuja R. K., 1993, NETWORK FLOWS THEORY; Bae S.-H., 2014, P CVPR; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Brau E., 2013, P ICCV; Brendel W., 2011, P CVPR; Butt A.A., 2013, P CVPR; Chari V., 2015, P CVPR; Choi W., 2012, P ECCV; Choi W., 2015, P ICCV; Dehghan A., 2015, P CVPR; Desai C., 2009, P ICCV; Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Finley T., 2008, P ICML; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A., 2012, P CVPR; Geiger A, 2014, IEEE T PATTERN ANAL, V36, P1012, DOI 10.1109/TPAMI.2013.185; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Joulin A., 2014, P ECCV; Kim C., 2015, P ICCV; Kim S., 2013, P ACCV; Lacoste-Julien S., 2006, P HLTNAACL; Leal-Taixe L., 2014, P CVPR; Lenz P., 2015, P ICCV; Li Y., 2009, P CVPR; Liu Ce, 2009, THESIS, P2; Lou X., 2011, P NIPS; Milan A., 2015, ARXIV150401942CS; Milan A., 2013, P CVPR; Milan A., 2015, P CVPR; Milan A., 2012, P CVPR; Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103; Pirsiavash H., 2011, P CVPR; Segal A.V., 2013, P ICCV; Solera F., 2015, P ICCV; Szummer M., 2008, ECCV; Tang S., 2013, P ICCV; Tang S., 2015, P CVPR; Taskar B., 2003, P NIPS; Wang L., 2014, P CVPR; Wang S., 2015, P BMVC; Wang X, 2013, P ICCV; Wu Z., 2012, P CVPR; Xiang Y., 2015, P ICCV; Yang B., 2012, P CVPR; Yoon J. H., 2015, P WACV; Zaied A. N. H., 2014, INT COMPUT APPL, V101, P28; Zhang L., 2008, P CVPR	49	33	36	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2017	122	3			SI		484	501		10.1007/s11263-016-0960-z	http://dx.doi.org/10.1007/s11263-016-0960-z			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ES7OH		Green Submitted			2022-12-18	WOS:000399739200006
J	Tian, YH; Li, J; Yu, S; Huang, TJ				Tian, Yonghong; Li, Jia; Yu, Shui; Huang, Tiejun			Learning Complementary Saliency Priors for Foreground Object Segmentation in Complex Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Foreground object segmentation; Visual saliency; Complementary saliency map; Graph cuts	ENERGY MINIMIZATION; EXTRACTION; ATTENTION; MODEL	Object segmentation is widely recognized as one of the most challenging problems in computer vision. One major problem of existing methods is that most of them are vulnerable to the cluttered background. Moreover, human intervention is often required to specify foreground/background priors, which restricts the usage of object segmentation in real-world scenario. To address these problems, we propose a novel approach to learn complementary saliency priors for foreground object segmentation in complex scenes. Different from existing saliency-based segmentation approaches, we propose to learn two complementary saliency maps that reveal the most reliable foreground and background regions. Given such priors, foreground object segmentation is formulated as a binary pixel labelling problem that can be efficiently solved using graph cuts. As such, the confident saliency priors can be utilized to extract the most salient objects and reduce the distraction of cluttered background. Extensive experiments show that our approach outperforms 16 state-of-the-artmethods remarkably on three public image benchmarks.	[Tian, Yonghong; Li, Jia; Huang, Tiejun] Peking Univ, Sch EE & CS, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China; [Yu, Shui] Deakin Univ, Sch Informat Technol, Burwood, Vic 3125, Australia	Peking University; Deakin University	Tian, YH (corresponding author), Peking Univ, Sch EE & CS, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.	yhtian@pku.edu.cn; jia.li@pku.edu.cn	Li, Jia/AAB-6431-2019	Li, Jia/0000-0002-4346-8696; Yu, Shui/0000-0003-4485-6743	Chinese National Natural Science Foundation [61035001, 61370113, 61390515]; Supervisor Award Funding for Excellent Doctoral Dissertation of Beijing [20128000103]	Chinese National Natural Science Foundation(National Natural Science Foundation of China (NSFC)); Supervisor Award Funding for Excellent Doctoral Dissertation of Beijing	This work was supported in part by grants from the Chinese National Natural Science Foundation under contract No. 61035001, No. 61370113, and No. 61390515, and the Supervisor Award Funding for Excellent Doctoral Dissertation of Beijing (No. 20128000103).	Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66; Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Borenstein E, 2004, LECT NOTES COMPUT SC, V3023, P315; Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30; Boyd S, 2004, CONVEX OPTIMIZATION; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Everingham M, 2009, PASCAL VISUAL OBJECT; Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929; Gopalakrishnan V, 2009, PROC CVPR IEEE, P1698, DOI 10.1109/CVPRW.2009.5206767; Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Hou X., 2007, IEEE C COMP VIS PATT; Hua G, 2006, IEEE T PATTERN ANAL, V28, P1701, DOI 10.1109/TPAMI.2006.209; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jiang H., 2011, P BRIT MACH VIS C, P1; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Lempitsky V, 2009, IEEE I CONF COMP VIS, P277, DOI 10.1109/ICCV.2009.5459262; Li CM, 2005, PROC CVPR IEEE, P430; Li FX, 2010, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2010.5539839; Li J, 2013, IEEE SIGNAL PROC LET, V20, P845, DOI 10.1109/LSP.2013.2268868; Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6; Liu GC, 2010, IEEE T PATTERN ANAL, V32, P910, DOI 10.1109/TPAMI.2009.40; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047; Ma Y.-F, 2003, P 11 ACM INT C MULTI, P374; Mehrani P., 2010, BMVC, P1; Movahedi V., 2010, IEEE WORKSH PERC ORG; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rother C., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91; Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15; Tseng PH, 2009, J VISION, V9, DOI 10.1167/9.7.4; Vicente S., 2008, P 2008 IEEE C COMP V, P1, DOI DOI 10.1109/CVPR.2008.4587440; Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001; Winn J, 2005, IEEE I CONF COMP VIS, P756; Yu H., 2010, P INT C MULT, P891; Yu S., 2002, NEURAL INFORM PROCES, V14, P1383; Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32; Zhao L, 2005, IEEE I CONF COMP VIS, P454	43	33	36	1	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2015	111	2					153	170		10.1007/s11263-014-0737-1	http://dx.doi.org/10.1007/s11263-014-0737-1			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AZ6RD					2022-12-18	WOS:000348345500002
J	Charles, J; Pfister, T; Everingham, M; Zisserman, A				Charles, James; Pfister, Tomas; Everingham, Mark; Zisserman, Andrew			Automatic and Efficient Human Pose Estimation for Sign Language Videos	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sign language; Human pose estimation; Co-segmentation; Random forest	RECOGNITION	We present a fully automatic arm and hand tracker that detects joint positions over continuous sign language video sequences of more than an hour in length. To achieve this, we make contributions in four areas: (i) we show that the overlaid signer can be separated from the background TV broadcast using co-segmentation over all frames with a layered model; (ii) we show that joint positions (shoulders, elbows, wrists) can be predicted per-frame using a random forest regressor given only this segmentation and a colour model; (iii) we show that the random forest can be trained from an existing semi-automatic, but computationally expensive, tracker; and, (iv) introduce an evaluator to assess whether the predicted joint positions are correct for each frame. The method is applied to 20 signing footage videos with changing background, challenging imaging conditions, and for different signers. Our framework outperforms the state-of-the-art long term tracker by Buehler et al. (International Journal of Computer Vision 95:180-197, 2011), does not require the manual annotation of that work, and, after automatic initialisation, performs tracking in real-time. We also achieve superior joint localisation results to those obtained using the pose estimation method of Yang and Ramanan (Proceedings of the IEEE conference on computer vision and pattern recognition, 2011).	[Charles, James; Everingham, Mark] Univ Leeds, Sch Comp, Leeds, W Yorkshire, England; [Pfister, Tomas; Zisserman, Andrew] Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England	University of Leeds; University of Oxford	Pfister, T (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	j.charles@leeds.ac.uk; tp@robots.ox.ac.uk; m.everingham@leeds.ac.uk; az@robots.ox.ac.uk			Engineering and Physical Sciences Research Council (EPSRC); EPSRC [EP/I01229X/1, EP/I012001/1] Funding Source: UKRI; Engineering and Physical Sciences Research Council [EP/I012001/1, EP/I01229X/1] Funding Source: researchfish	Engineering and Physical Sciences Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We are grateful to Lubor Ladicky for discussions, and to Patrick Buehler for his very generous help. Funding is provided by the Engineering and Physical Sciences Research Council (EPSRC) grant Learning to Recognise Dynamic Visual Content from Broadcast Footage.	Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545; Andriluka M, 2012, INT J COMPUT VISION, V99, P259, DOI 10.1007/s11263-011-0498-z; Apostoloff N. E., 2007, P BRIT MACH VIS C; Benfold B., 2008, P BRIT MACH VIS C; BOSCH A, 2007, P INT C COMP VIS; Bowden R., 2004, P EUR C COMP VIS; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Buehler P., 2009, P IEEE C COMP VIS PA; Buehler P, 2011, INT J COMPUT VISION, V95, P180, DOI 10.1007/s11263-011-0480-9; Buehler Patrick, 2010, WORKSH REPR PROC SIG; Chai Y., 2012, EUR C COMP VIS; Chai Y., 2011, P INT C COMP VIS; Charles J., 2013, P BRIT MACH VIS C; Chunli W., 2002, GESTURE SIGN LANGUAG; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Cooper H., 2007, WORKSH HUM COMP INT; Cooper H., 2009, P IEEE C COMP VIS PA; Cootes TF, 2012, P EUR C COMP VIS; Criminisi A., 2011, INT C MED IM COMP CO; Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1561/0600000035, 10.1501/0000000035]; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dantone M, 2012, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2012.6247976; Dreuw P., 2006, P IEEE C AUT FAC GES; Dreuw P., 2012, TRENDS TOPICS COMPUT, P286; Eichner M., 2012, INVENT MATH, P1; Eichner M., 2009, P BRIT MACH VIS C; Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0; Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458; Farhadi A., 2007, P IEEE C COMP VIS PA; Farhadi A., 2006, P IEEE C COMP VIS PA; Felzenszwalb P., 2008, PROCEEDINGS OF THE I; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; FELZENSZWALB PF, 2010, PROC CVPR IEEE, P2241, DOI DOI 10.1109/CVPR.2010.5539906; Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468; Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755; Geremia E, 2011, NEUROIMAGE, V57, P378, DOI 10.1016/j.neuroimage.2011.03.080; Girshick R., 2011, P INT C COMP VIS; Hochbaum D., 2009, P INT C COMP VIS; Jammalamadaka N., 2012, P EUR C COMP VIS; Johnson S., 2009, IEEE INT WORKSH MACH; JOJIC N, 2001, P IEEE C COMP VIS PA; Joulin A., 2010, P IEEE C COMP VIS PA; KADIR T, 2004, P BRIT MACH VIS C; Kadir T., 2004, P EUR C COMP VIS; Kontschieder P, 2012, ADV NEURAL INFORM PR; Kumar MP, 2008, INT J COMPUT VISION, V76, P301, DOI 10.1007/s11263-007-0064-x; Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188; Liu C., 2012, P EUR C COMP VIS; MAREE R, 2005, P IEEE C COMP VIS PA; Moeslund TB, 2011, VISUAL ANAL HUMANS L, V1st; Nowozin S., 2011, P INT C COMP VIS; Ozuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23; Ong Eng-Jon, 2004, P INT C AUT FAC GEST; Pfister T., 2013, P BRIT MACH VIS C; Pfister T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.4; RAMANAN D, 2006, ADV NEURAL INFORM PR; Ramanan D, 2007, IEEE T PATTERN ANAL, V29, P65, DOI 10.1109/TPAMI.2007.250600; Rother C., 2006, P IEEE C COMP VIS PA; Rother C., 2004, P ACM SIGGRAPH C COM; Santner J., 2010, P IEEE C COMP VIS PA; Sapp B., 2010, P IEEE C COMP VIS PA; Sapp B., 2011, P IEEE C COMP VIS PA; SHARP T, 2008, P EUR C COMP VIS; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Sivic J., 2006, P BRIT MACH VIS C; Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811; Sun M, 2012, P IEEE C COMP VIS PA; Szeliski R., 2000, P IEEE C COMP VIS PA; Taylor J, 2012, P IEEE C COMP VIS PA; Tran D., 2010, P EUR C COMP VIS; Vogler C., 1998, P INT C COMP VIS; Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741; Yin P., 2007, P IEEE C COMP VIS PA; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zisserman A, 2012, IEEE T PATTERN ANAL, V34, P2081, DOI 10.1109/TPAMI.2012.204	75	33	34	0	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2014	110	1			SI		70	90		10.1007/s11263-013-0672-6	http://dx.doi.org/10.1007/s11263-013-0672-6			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AQ0BZ					2022-12-18	WOS:000342448000007
J	Provenzi, E; Caselles, V				Provenzi, Edoardo; Caselles, Vicent			A Wavelet Perspective on Variational Perceptually-Inspired Color Enhancement	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Local contrast enhancement; Wavelets; Color image processing; Variational methods	IMAGE-CONTRAST ENHANCEMENT; RETINEX THEORY; ALGORITHM; GRAY	The issue of perceptually-inspired correction of color and contrast in digital images has been recently analyzed with the help of variational principles. These techniques allowed building a general framework in which the action of many already existing algorithms can be more easily understood and compared in terms of intensification of local contrast and control of dispersion around the average intensity value. In this paper we analyze this issue from the dual perspective of wavelet theory, showing that it is possible to build energy functionals of wavelet coefficients that lead to a multilevel perceptually-inspired color correction. By computing the Euler-Lagrange equations associated to the wavelet-based functionals we were able to find an analytical formula for the modification of wavelet detail coefficients that overcomes the problem of an ad-hoc selection based on empirical considerations. Besides these theoretical results, the wavelet perspective provides the computational advantage of generating much faster algorithms in comparison with the spatial variational framework.	[Provenzi, Edoardo] Telecom ParisTech, LTCI, CNRS, F-75013 Paris, France; [Caselles, Vicent] Univ Pompeu Fabra, Dept Tecnol Informacio & Comunicac, Barcelona 08018, Spain	Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Cite; Pompeu Fabra University	Provenzi, E (corresponding author), Telecom ParisTech, LTCI, CNRS, 46 Rue Barrault, F-75013 Paris, France.	edoardo.provenzi@telecom-paristech.fr; vicent.caselles@upf.edu			MICINN [MTM2009-08171]; GRC [2009 SGR 773]; Generalitat de Catalunya; "ICREA Academia" prize for excellence in Research; ERC Advanced Grant INPAINTING [319899]	MICINN(Ministry of Science and Innovation, Spain (MICINN)Spanish GovernmentEuropean Commission); GRC(Science Foundation Ireland); Generalitat de Catalunya(Generalitat de Catalunya); "ICREA Academia" prize for excellence in Research(ICREA); ERC Advanced Grant INPAINTING	The authors acknowledge partial support by MICINN Project, reference MTM2009-08171, and by GRC reference 2009 SGR 773 funded by the Generalitat de Catalunya. Edoardo Provenzi also acknowledges FUI Project CEDCA. V. Caselles also acknowledges partial support by "ICREA Academia" prize for excellence in Research funded by the Generalitat de Catalunya and by the ERC Advanced Grant INPAINTING (Grant Agreement No.: 319899).	Ambrosio L., 2005, LECT MATH; Bertalmio M, 2007, IEEE T IMAGE PROCESS, V16, P1058, DOI 10.1109/TIP.2007.891777; Bertalmio M, 2009, INT J COMPUT VISION, V83, P101, DOI 10.1007/s11263-009-0221-5; Bradley AP, 1999, IEEE T IMAGE PROCESS, V8, P717, DOI 10.1109/83.760338; Cho D., 2011, IMAGE CONTRAST EHNAN, P3421; Ciarlet P.G., 1989, INTRO NUMERICAL LINE, DOI [10.1017/9781139171984, DOI 10.1017/9781139171984]; DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952; Daubechies I., 1992, 10 LECT WAVELETS, DOI [10.1137/1.9781611970104.ch1, DOI 10.1137/1.9781611970104.CH1]; Di Zenzo S, 1986, COMPUTER VISION GRAP, V33; Gonzales R., 2002, DIGITAL IMAGE PROCES; Held S, 2010, IEEE T IMAGE PROCESS, V19, P653, DOI 10.1109/TIP.2009.2036713; LAINE AF, 1994, IEEE T MED IMAGING, V13, P725, DOI 10.1109/42.363095; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108; Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271; Loza A, 2010, IEEE IMAGE PROC, P3553, DOI 10.1109/ICIP.2010.5651173; LU J, 1994, P SOC PHOTO-OPT INS, V2242, P711, DOI 10.1117/12.170070; Mallat S., 1999, WAVELET TOUR SIGNAL, DOI 10.1016/B978-012466606-1/50008-8; Michelson Albert A., 1927, STUDIES OPTICS; Palma-Amestoy R, 2009, IEEE T PATTERN ANAL, V31, P458, DOI 10.1109/TPAMI.2008.86; PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032; Peli E, 1997, VISION RES, V37, P3217, DOI 10.1016/S0042-6989(96)00262-3; Provenzi E, 2005, J OPT SOC AM A, V22, P2613, DOI 10.1364/JOSAA.22.002613; Provenzi E, 2008, IEEE T PATTERN ANAL, V30, P1757, DOI 10.1109/TPAMI.2007.70827; Provenzi E, 2007, IEEE T IMAGE PROCESS, V16, P162, DOI 10.1109/TIP.2006.884946; Pu T, 2000, OPT ENG, V39, P2075, DOI 10.1117/1.1303728; Rizzi A, 2003, P SOC PHOTO-OPT INS, V5008, P24, DOI 10.1117/12.472017; Rizzi A, 2003, PATTERN RECOGN LETT, V24, P1663, DOI 10.1016/S0167-8655(02)00323-9; Sapiro G, 1997, J DIFFER EQUATIONS, V135, P238, DOI 10.1006/jdeq.1996.3237; Scheunders P, 2002, IEEE T IMAGE PROCESS, V11, P568, DOI 10.1109/TIP.2002.1006403; Shapley R., 1984, PROG RETIN RES, V3, P263, DOI [DOI 10.1016/0278-4327(84)90011-7, 10.1016/0278-4327(84)90011-7]; Starck JL, 2003, IEEE T IMAGE PROCESS, V12, P706, DOI 10.1109/TIP.2003.813140; Strang G., 1996, WAVELETS FILTER BANK; Tang JS, 2009, IEEE J-STSP, V3, P74, DOI 10.1109/JSTSP.2008.2011108; Velde K. V., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P584, DOI 10.1109/ICIP.1999.817182; Weber E. H., 1834, PULSU RESORPTIONE AU	36	33	34	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2014	106	2					153	171		10.1007/s11263-013-0651-y	http://dx.doi.org/10.1007/s11263-013-0651-y			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	290UR					2022-12-18	WOS:000329784300003
J	Varadarajan, J; Emonet, R; Odobez, JM				Varadarajan, Jagannadan; Emonet, Remi; Odobez, Jean-Marc			A Sequential Topic Model for Mining Recurrent Activities from Long Term Video Logs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised; Latent sequential patterns; Topic models; PLSA; Video surveillance; Activity analysis		This paper introduces a novel probabilistic activity modeling approach that mines recurrent sequential patterns called motifs from documents given as word time count matrices (e.g., videos). In this model, documents are represented as a mixture of sequential activity patterns (our motifs) where the mixing weights are defined by the motif starting time occurrences. The novelties are multi fold. First, unlike previous approaches where topics modeled only the co-occurrence of words at a given time instant, our motifs model the co-occurrence and temporal order in which the words occur within a temporal window. Second, unlike traditional Dynamic Bayesian networks (DBN), our model accounts for the important case where activities occur concurrently in the video (but not necessarily in synchrony), i.e., the advent of activity motifs can overlap. The learning of the motifs in these difficult situations is made possible thanks to the introduction of latent variables representing the activity starting times, enabling us to implicitly align the occurrences of the same pattern during the joint inference of the motifs and their starting times. As a third novelty, we propose a general method that favors the recovery of sparse distributions, a highly desirable property in many topic model applications, by adding simple regularization constraints on the searched distributions to the data likelihood optimization criteria. We substantiate our claims with experiments on synthetic data to demonstrate the algorithm behavior, and on four video datasets with significant variations in their activity content obtained from static cameras. We observe that using low-level motion features from videos, our algorithm is able to capture sequential patterns that implicitly represent typical trajectories of scene objects.	[Varadarajan, Jagannadan; Emonet, Remi; Odobez, Jean-Marc] Idiap Res Inst, Martigny, Switzerland; [Varadarajan, Jagannadan; Emonet, Remi; Odobez, Jean-Marc] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Varadarajan, J (corresponding author), Idiap Res Inst, Martigny, Switzerland.	vjagann@idiap.ch; remonet@idiap.ch; odobez@idiap.ch		Emonet, Remi/0000-0002-1870-1329	Swiss National Science Foundation [FNS-198]; European Union [248907]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); European Union(European Commission)	Authors gratefully acknowledge the support of the Swiss National Science Foundation (Project: FNS-198, HAI) and of the European Union through its 7th framework program (Integrated project VANAHEIM (248907, and Network of Excellence PASCAL2).	Blei D.M., 2006, ICML, V25-29, P113; Blei DM, 2007, ANN APPL STAT, V1, P17, DOI 10.1214/07-AOAS114; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boiman O, 2007, INT J COMPUT VISION, V74, P17, DOI 10.1007/s11263-006-0009-9; Bradshaw D., 2008, P NEUR INF PROC SYST, P22; Chen S., 1998, PROC DARPA BROADCAST, V8, P127; Chien JT, 2008, IEEE T AUDIO SPEECH, V16, P198, DOI 10.1109/TASL.2007.909452; Emonet R, 2011, PROC CVPR IEEE; Fablet R, 2000, IEEE INT C PATT REC, V4; Faruquie T. A, 2009, BRIT MACH VIS C LOND; Girolami M., 2003, P 26 ANN INT ACM SIG, P433, DOI [10.1145/860435.860537, DOI 10.1145/860435.860537]; Gohr A., 2009, P 2009 SIAM INT C DA, P859; Gruber A, 2007, INT C ART INT STAT S; Hervieu A, 2008, IEEE T CIRC SYST VID, V18, P1533, DOI 10.1109/TCSVT.2008.2005609; Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950; Hospedales T, 2012, INT J COMPUT VISION, V98, P303, DOI 10.1007/s11263-011-0510-7; Hospedales T, 2009, IEEE I CONF COMP VIS, P1165, DOI 10.1109/ICCV.2009.5459342; Hoyer P.O., 2005, J MACHINE LEARNING R, V5, P1457; Keogh E., 2001, Knowledge and Information Systems, V3, P263, DOI 10.1007/PL00011669; Kuettel D, 2010, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2010.5539869; Le Besnerais G, 1999, IEEE T INFORM THEORY, V45, P1565, DOI 10.1109/18.771159; LI J, 2009, IEEE INT WORKSH VIS; Li J., 2008, BMVC, V3231, P3232; Luvison B, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P506; Makris D, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P183, DOI 10.1109/AVSS.2003.1217920; Mueen Abdullah, 2009, Proc SIAM Int Conf Data Min, V2009, P473, DOI 10.1137/1.9781611972795.41; Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4; Quelhas P, 2007, IEEE T PATTERN ANAL, V29, P1575, DOI 10.1109/TPAMI.2007.1155; Saleemi I, 2010, PROC CVPR IEEE, P2069, DOI 10.1109/CVPR.2010.5539884; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677; Tanaka Y, 2005, MACH LEARN, V58, P269, DOI 10.1007/s10994-005-5829-2; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Tommasi C., 1991, CMUCS91132; Tritschler A, 1999, 6 EUR C SPEECH COMM; Varadarajan J, 2010, BRIT MACH VIS C AB, P1171; Varadarajan J, 2010, WORKSH PRACT APPL SP; Varadarajan J, 2012, IEEE C COMP VIS PATT; Varadarajan J, 2009, IEEE INT WORKSH VIS; Vasconcelos N, 2000, IEEE T IMAGE PROCESS, V9, P3, DOI 10.1109/83.817595; Wang C., 2009, ADV NEURAL INFORM PR, V22, P1982; Wang C, 2008, C UNC ART INT; Wang X., 2004, EUR C COMP VIS, V14, P234; Wang X, 2006, PHIL ACM C KNOWL DIS; Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87; Wang Y, 2009, IEEE T PATTERN ANAL, V31, P1762, DOI 10.1109/TPAMI.2009.43; Williamson S, 2009, NIPS WORKSH APPL TOP; Xiang T, 2008, IEEE T PATTERN ANAL, V30, P893, DOI 10.1109/TPAMI.2007.70731; Yang Y, 2009, IEEE I CONF COMP VIS, P1669, DOI 10.1109/ICCV.2009.5459376; Yao J, 2007, PROC CVPR IEEE, P3718; Yi Z, 2010, P SIAM DAT MIN SDM C; Zhang D, 2004, ACM INT C MULT WORKS; Zhong H, 2004, PROC CVPR IEEE, P819	55	33	35	0	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2013	103	1					100	126		10.1007/s11263-012-0596-6	http://dx.doi.org/10.1007/s11263-012-0596-6			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	137CV		Green Submitted			2022-12-18	WOS:000318413500005
J	Wetzstein, G; Ihrke, I; Heidrich, W				Wetzstein, Gordon; Ihrke, Ivo; Heidrich, Wolfgang			On Plenoptic Multiplexing and Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computational photography; Optical multiplexing; Plenoptic function; Light fields	RESOLUTION; FIELD	Photography has been striving to capture an ever increasing amount of visual information in a single image. Digital sensors, however, are limited to recording a small subset of the desired information at each pixel. A common approach to overcoming the limitations of sensing hardware is the optical multiplexing of high-dimensional data into a photograph. While this is a well-studied topic for imaging with color filter arrays, we develop a mathematical framework that generalizes multiplexed imaging to all dimensions of the plenoptic function. This framework unifies a wide variety of existing approaches to analyze and reconstruct multiplexed data in either the spatial or the frequency domain. We demonstrate many practical applications of our framework including high-quality light field reconstruction, the first comparative noise analysis of light field attenuation masks, and an analysis of aliasing in multiplexing applications.	[Wetzstein, Gordon] MIT, Media Lab, Cambridge, MA 02139 USA; [Ihrke, Ivo] Univ Saarland, Saarbrucken, Germany; [Heidrich, Wolfgang] Univ British Columbia, Vancouver, BC V5Z 1M9, Canada	Massachusetts Institute of Technology (MIT); Saarland University; University of British Columbia	Wetzstein, G (corresponding author), MIT, Media Lab, Cambridge, MA 02139 USA.	gordonw@media.mit.edu; ihrke@mmci.uni-saarland.de; heidrich@cs.ubc.ca		Wetzstein, Gordon/0000-0002-9243-6885	Dolby Canada; UBC; NSERC; DARPA SCENICC; Dolby Research Chair in Computer Science at UBC; Humboldt Foundation, Germany	Dolby Canada; UBC; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); DARPA SCENICC; Dolby Research Chair in Computer Science at UBC; Humboldt Foundation, Germany(Alexander von Humboldt Foundation)	We thank Dolby Canada for their support and the anonymous reviewers for their insightful feedback. Gordon Wetzstein was supported by a UBC Four Year Fellowship, an NSERC Postdoctoral Fellowship, and the DARPA SCENICC program. Wolfgang Heidrich was supported under the Dolby Research Chair in Computer Science at UBC. Ivo Ihrke was supported by a Feodor-Lynen Fellowship of the Humboldt Foundation, Germany.	ADELSON EH, 1992, IEEE T PATTERN ANAL, V14, P99, DOI 10.1109/34.121783; Aggarwal M, 2004, INT J COMPUT VISION, V58, P7, DOI 10.1023/B:VISI.0000016144.56397.1a; AGRAWAL A, 2010, P EUR, P1; Agrawal A., 2010, P IEEE CVPR, P374; Alleysson D, 2005, IEEE T IMAGE PROCESS, V14, P439, DOI 10.1109/TIP.2004.841200; Bando Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409087; Bayer B. E., 1976, U.S. Patent, Patent No. [3,971,065, 3971065, 3 971 065]; Ben-Ezra M, 2005, IEEE T PATTERN ANAL, V27, P977, DOI 10.1109/TPAMI.2005.129; Bub G, 2010, NAT METHODS, V7, P209, DOI [10.1038/NMETH.1429, 10.1038/nmeth.1429]; Compton J., 2007, COLOR FILTER ARRAY 2; Debevec P. E., 1997, ACM T GRAPHIC, P369; Georgiev T, 2008, LECT NOTES COMPUT SC, V5304, P224, DOI 10.1007/978-3-540-88690-7_17; Georgiev Todor, 2010, IEEE INT C COMP PHOT, P1; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; GOTTESMAN SR, 1989, APPL OPTICS, V28, P4344, DOI 10.1364/AO.28.004344; GREIVENKAMP JE, 1990, APPL OPTICS, V29, P676, DOI 10.1364/AO.29.000676; Gupta M, 2010, LECT NOTES COMPUT SC, V6311, P100, DOI 10.1007/978-3-642-15549-9_8; Horstmeyer R., 2009, 2009 IEEE INT C COMP, P1, DOI DOI 10.1109/ICCPHOT.2009.5559016; Ives F.E., 1903, US Patent, Patent No. [725,567, 725567]; Landy M.S., 1991, COMPUTATIONAL MODELS; Lanman D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409084; Levin A., 2010, P IEEE CVPR, P1; Levin A, 2008, LECT NOTES COMPUT SC, V5305, P88, DOI 10.1007/978-3-540-88693-8_7; Levin A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531403; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Levoy M, 2006, ACM T GRAPHIC, V25, P924, DOI 10.1145/1141911.1141976; Lippmann G, 1908, CR HEBD ACAD SCI, V146, P446; Lumsdaine A., 2009, 2009 IEEE INT C COMP, P1, DOI [10.1109/ICCPHOT.2009.5559008, DOI 10.1109/ICCPHOT.2009.5559008]; Mateos J, 2009, IEEE IMAGE PROC, P129, DOI 10.1109/ICIP.2009.5414169; McGuire M, 2007, IEEE COMPUT GRAPH, V27, P32, DOI 10.1109/MCG.2007.45; Mitsunaga T., P 1999 IEEE COMP SOC, P374; Narasimhan SG, 2005, IEEE T PATTERN ANAL, V27, P518, DOI 10.1109/TPAMI.2005.76; Ng R, 2005, ACM T GRAPHIC, V24, P735, DOI 10.1145/1073204.1073256; Reddy D., 2011, P IEEE CVPR, P1; SAJADI B., 2011, ACM T GRAPHICS SIGGR, V30, P1; Schechner YY, 2007, IEEE T PATTERN ANAL, V29, P1339, DOI 10.1109/TPAMI.2007.1151; Schechner YY, 2003, INT J COMPUT VISION, V53, P245, DOI 10.1023/A:1023082924255; Takhar D, 2006, PROC SPIE, V6065, DOI 10.1117/12.659602; VEERARAGHAVAN A., 2008, P IEEE CVPR, P1; Veeraraghavan A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239520; Veeraraghavan A, 2011, IEEE T PATTERN ANAL, V33, P671, DOI 10.1109/TPAMI.2010.87; Wang SZ, 2004, COMPUT GRAPH FORUM, V23, P441, DOI 10.1111/j.1467-8659.2004.00775.x; Wetzstein G., 2010, P IEEE CVPR, P1; Wetzstein G., 2011, 2011 IEEE INT C COMP, P1; Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259; Wuttig A, 2005, APPL OPTICS, V44, P2710, DOI 10.1364/AO.44.002710; Yasuma F., 2010, IEEE T IM P DYNAMICS, P99; Zhang L., 2008, SPIE C VIS COMM IM P	49	33	47	0	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	2					384	400		10.1007/s11263-012-0585-9	http://dx.doi.org/10.1007/s11263-012-0585-9			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	081BW		Green Submitted			2022-12-18	WOS:000314291600009
J	Valgaerts, L; Bruhn, A; Mainberger, M; Weickert, J				Valgaerts, Levi; Bruhn, Andres; Mainberger, Markus; Weickert, Joachim			Dense versus Sparse Approaches for Estimating the Fundamental Matrix	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Optical flow; Fundamental matrix; Performance evaluation; 3D reconstruction	OPTICAL-FLOW; MOTION; COMPUTATION; ALGORITHM; GEOMETRY; SEGMENTATION; PERFORMANCE; STEREO; CAMERA; MODEL	There are two main strategies for solving correspondence problems in computer vision: sparse local feature based approaches and dense global energy based methods. While sparse feature based methods are often used for estimating the fundamental matrix by matching a small set of sophistically optimised interest points, dense energy based methods mark the state of the art in optical flow computation. The goal of our paper is to show that this separation into different application domains is unnecessary and can be bridged in a natural way. As a first contribution we present a new application of dense optical flow for estimating the fundamental matrix. Comparing our results with those obtained by feature based techniques we identify cases in which dense methods have advantages over sparse approaches. Motivated by these promising results we propose, as a second contribution, a new variational model that recovers the fundamental matrix and the optical flow simultaneously as the minimisers of a single energy functional. In experiments we show that our coupled approach is able to further improve the estimates of both the fundamental matrix and the optical flow. Our results prove that dense variational methods can be a serious alternative even in classical application domains of sparse feature based approaches.	[Valgaerts, Levi; Bruhn, Andres] Univ Saarland, MMCI Cluster Excellence, Vis & Image Proc Grp, D-66041 Saarbrucken, Germany; [Mainberger, Markus; Weickert, Joachim] Univ Saarland, Fac Math & Comp Sci, Math Image Anal Grp, D-66041 Saarbrucken, Germany	Saarland University; Saarland University	Valgaerts, L (corresponding author), Univ Saarland, MMCI Cluster Excellence, Vis & Image Proc Grp, Campus E1-1, D-66041 Saarbrucken, Germany.	valgaerts@mmci.uni-saarland.de; bruhn@mmci.uni-saarland.de; mainberger@mia.uni-saarland.de; weickert@mia.uni-saarland.de			Deutsche Forschungsgemeinschaft [WE 2602/6-1]; Cluster of Excellence "Multimodal Computing and Interaction"	Deutsche Forschungsgemeinschaft(German Research Foundation (DFG)); Cluster of Excellence "Multimodal Computing and Interaction"	We gratefully acknowledge partial funding by the Deutsche Forschungsgemeinschaft under grant WE 2602/6-1 and the Cluster of Excellence "Multimodal Computing and Interaction". We thank Henning Zimmer for providing his optical flow algorithm and Pascal Gwosdek for his help.	Alvarez L, 2002, LECT NOTES COMPUT SC, V2350, P721; Alvarez L, 2002, J VIS COMMUN IMAGE R, V13, P3, DOI 10.1006/jvci.2001.0482; Alvarez L., 1999, P 16 C EC DIF APL LA, P1349; Amiaz T, 2007, PATTERN RECOGN, V40, P2496, DOI 10.1016/j.patcog.2006.09.011; [Anonymous], P VIS MOD VIS WORKSH; Baker S., 2007, P 2007 IEEE INT C CO; Baker S., 2009, MSRTR2009179; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Ben-Ari R., 2007, P 11 INT C COMP VIS; Bitton D., 2009, CIS200918 TECHN ISR; Black M. J., 1991, P 1991 IEEE COMP SOC, P292; Brooks MJ, 1997, J OPT SOC AM A, V14, P2670, DOI 10.1364/JOSAA.14.002670; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Brox T, 2009, PROC CVPR IEEE, P41, DOI 10.1109/CVPRW.2009.5206697; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; Bruhn A, 2006, INT J COMPUT VISION, V70, P257, DOI 10.1007/s11263-006-6616-7; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chum O, 2005, PROC CVPR IEEE, P772; Chum O., 2004, P ACCV, V2, P812; Faugeras O. D., 1992, Computer Vision - ECCV '92. Second European Conference on Computer Vision Proceedings, P563; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fitzgibbon AW, 2001, PROC CVPR IEEE, P125; Forstner<spacing Wolfgang, 1987, ISPRS INT C FAST PRO, P2; Frahm J.-M., 2006, P COMP VIS PATT REC, V1, P453, DOI DOI 10.1109/CVPR.2006.235; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Gwosdek P., 2010, P 2010 ECCV WORKSH C; Hanna K. J., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P156, DOI 10.1109/WVM.1991.212812; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Huber P., 1981, ROBUST STAT; Kanatani K, 2000, J ELECTRON IMAGING, V9, P194, DOI 10.1117/1.482739; Kim YH, 2005, IMAGE VISION COMPUT, V23, P365, DOI 10.1016/j.imavis.2004.05.010; Klaus A, 2006, INT C PATT RECOG, P15; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82; Lei C., 2006, P 2006 IEEE COMP SOC, V2, P2378; Levenberg K., 1944, Q APPL MATH, V2, P164, DOI 10.1090/qam/10666; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; Mainberger M, 2008, LECT NOTES COMPUT SC, V5112, P630, DOI 10.1007/978-3-540-69812-8_62; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Memin E, 2002, INT J COMPUT VISION, V46, P129, DOI 10.1023/A:1013539930159; Memin E, 1998, IEEE T IMAGE PROCESS, V7, P703, DOI 10.1109/83.668027; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mileva Y, 2007, LECT NOTES COMPUT SC, V4713, P152; NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833; Nir T, 2008, INT J COMPUT VISION, V76, P205, DOI 10.1007/s11263-007-0051-2; Ohta N, 1995, IEICE T INF SYST, VE78D, P1559; Proesmans M., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P295; Raguram R, 2008, LECT NOTES COMPUT SC, V5303, P500, DOI 10.1007/978-3-540-88688-4_37; ROTH S, 2005, P IEEE INT C COMP VI, V1, P42; Rousseeuw P. J., 1987, ROBUST REGRESSION OU; Saragih J., 2007, P 2007 IEEE COMP SOC; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Schlesinger D, 2004, LECT NOTES COMPUT SC, V3175, P440; Schnorr C., 1994, Proceedings of the 12th IAPR International Conference on Pattern Recognition (Cat. No.94CH3440-5), P661, DOI 10.1109/ICPR.1994.576391; Seitz S., 2006, 2006 IEEE COMP SOC C, V1, P519, DOI [10.1109/CVPR.2006.19, DOI 10.1109/CVPR.2006.19]; Sheikh Y., 2007, P 2007 IEEE COMP SOC; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Slesareva N, 2005, LECT NOTES COMPUT SC, V3663, P33; Steinbrucker F., 2009, P 12 INT C COMP VIS; Stewart CV, 1999, SIAM REV, V41, P513, DOI 10.1137/S0036144598345802; Strecha C, 2004, LECT NOTES COMPUT SC, V3247, P71; Strecha C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1194; Strecha C., 2008, P 2008 IEEE COMP SOC; Sun DQ, 2008, LECT NOTES COMPUT SC, V5304, P83; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; Valgaerts L, 2008, LECT NOTES COMPUT SC, V5096, P314, DOI 10.1007/978-3-540-69321-5_32; Valgaerts L, 2010, LECT NOTES COMPUT SC, V6314, P568, DOI 10.1007/978-3-642-15561-1_41; VIEVILLE T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P750, DOI 10.1109/ICCV.1995.466863; WANG H, 1994, IEEE IMAGE PROC, P919, DOI 10.1109/ICIP.1994.413446; Wedel A., 2008, P IM VIS COMP NZ; Wedel A., 2009, P 12 INT C COMP VIS; Weickert J, 2001, INT J COMPUT VISION, V45, P245, DOI 10.1023/A:1013614317973; WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074; WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779; Werlberger M, 2010, PROC CVPR IEEE, P2464, DOI 10.1109/CVPR.2010.5539945; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zach C., 2007, P 9 INT C COMP VIS; Zeisl B., 2009, P 2009 BRIT MACH VIS; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561; Zimmer H., 2011, INT J COMPUT VISION, V93, P638; Zimmer H, 2009, LECT NOTES COMPUT SC, V5681, P207, DOI 10.1007/978-3-642-03641-5_16	93	33	37	1	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2012	96	2					212	234		10.1007/s11263-011-0466-7	http://dx.doi.org/10.1007/s11263-011-0466-7			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	876AW		Green Submitted			2022-12-18	WOS:000299080200005
J	Buehler, P; Everingham, M; Huttenlocher, DP; Zisserman, A				Buehler, Patrick; Everingham, Mark; Huttenlocher, Daniel P.; Zisserman, Andrew			Upper Body Detection and Tracking in Extended Signing Sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person tracking; Human pose estimation; Sign language; Gesture recognition		The goal of this work is to detect and track the articulated pose of a human in signing videos of more than one hour in length. In particular we wish to accurately localise hands and arms, despite fast motion and a cluttered and changing background. We cast the problem as inference in a generative model of the image, and propose a complete model which accounts for self-occlusion of the arms. Under this model, limb detection is expensive due to the very large number of possible configurations each part can assume. We make the following contributions to reduce this cost: (i) efficient sampling from a pictorial structure proposal distribution to obtain reasonable configurations; (ii) identifying a large number of frames where configurations can be correctly inferred, and exploiting temporal tracking elsewhere. Results are reported for signing footage with challenging image conditions and for different signers. We show that the method is able to identify the true arm and hand locations with high reliability. The results exceed the state-of-the-art for the length and stability of continuous limb tracking.	[Everingham, Mark] Univ Leeds, Sch Comp, Leeds, W Yorkshire, England; [Buehler, Patrick; Zisserman, Andrew] Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England; [Huttenlocher, Daniel P.] Cornell Univ, Dept Comp Sci, Ithaca, NY USA	University of Leeds; University of Oxford; Cornell University	Everingham, M (corresponding author), Univ Leeds, Sch Comp, Leeds, W Yorkshire, England.	patrick@robots.ox.ac.uk; M.Everingham@leeds.ac.uk			Engineering and Physical Sciences Research Council; Microsoft; Royal Academy of Engineering; ERC [228180]	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Microsoft(Microsoft); Royal Academy of Engineering(Royal Academy of Engineering - UK); ERC(European Research Council (ERC)European Commission)	We are grateful for financial support from the Engineering and Physical Sciences Research Council, Microsoft, the Royal Academy of Engineering, and ERC grant VisRec No. 228180.	Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; ANDRILUKA M, 2009, P IEEE C COMP VIS PA; BUCHANAN A, 2006, P IEEE CSCONF COMP V, V1, P626; Buehler P., 2009, P IEEE C COMP VIS PA; Buehler Patrick, 2008, P BRIT MACH VIS C; Cooper H, 2007, LECT NOTES COMPUT SC, V4796, P88; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Eichner M., 2009, P BRIT MACH VIS C; Farhadi A., 2007, 2007 IEEE C COMP VIS, P1; FELZENSZWALB P, 2000, P IEEE C COMP VIS PA, V2, P2066; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468; FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602; FLECK MM, 1996, LNCS, V1065, P591; Fossati A., 2007, CVPR, P1; JIANG H, 2009, P INT C COMP VIS; Johnson S., 2009, IEEE INT WORKSH MACH; KADIR T, 2004, P BRIT MACH VIS C; Kumar M. P., 2004, P BRIT MACH VIS C; Kumar M. P., 2009, P INT C COMP VIS; KUMAR MP, 2005, P INT C COMP VIS; LAN X, 2005, P INT C COMP VIS, V1; Lee MW, 2006, IEEE T PATTERN ANAL, V28, P905, DOI 10.1109/TPAMI.2006.110; LIN Z, 2007, ICCV WORKSH INT COMP; MICILOTTA A, 2005, REAL TIME UPPER BODY; NAVARATNAM R, 2005, P BRIT MACH VIS C, P479; Ong EJ, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P889; Ramanan D, 2005, PROC CVPR IEEE, P271; RAMANAN D, 2006, ADV NEURAL INFORM PR; SHEIKH Y, 2008, P INT C AUT FAC GEST; SIDDIQUI M, 2007, LECT NOTES COMPUTER, V4814; Sigal L., 2006, PROC IEEE C COMPUT V, P2041; Sivic J., 2006, P BRIT MACH VIS C; Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811; TRAN D, 2007, ADV NEURAL INFORM PR; VIOLA P, 2002, INT J COMPUT VISION, V1, P137; Wang Y., 2008, P EUR C COMP VIS	37	33	35	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2011	95	2					180	197		10.1007/s11263-011-0480-9	http://dx.doi.org/10.1007/s11263-011-0480-9			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	815YN					2022-12-18	WOS:000294566000005
J	Roman-Rangel, E; Pallan, C; Odobez, JM; Gatica-Perez, D				Roman-Rangel, Edgar; Pallan, Carlos; Odobez, Jean-Marc; Gatica-Perez, Daniel			Analyzing Ancient Maya Glyph Collections with Contextual Shape Descriptors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cultural heritage; Maya civilization; Archeology; Epigraphy; Image retrieval; Visual similarity; Shape descriptor; Histogram of orientation	RETRIEVAL	This paper presents an original approach for shape-based analysis of ancient Maya hieroglyphs based on an interdisciplinary collaboration between computer vision and archeology. Our work is guided by realistic needs of archaeologists and scholars who critically need support for search and retrieval tasks in large Maya imagery collections. Our paper has three main contributions. First, we introduce an overview of our interdisciplinary approach towards the improvement of the documentation, analysis, and preservation of Maya pictographic data. Second, we present an objective evaluation of the performance of two state-of-the-art shape-based contextual descriptors (Shape Context and Generalized Shape Context) in retrieval tasks, using two datasets of syllabic Maya glyphs. Based on the identification of their limitations, we propose a new shape descriptor named Histogram of Orientation Shape Context (HOOSC), which is more robust and suitable for description of Maya hieroglyphs. Third, we present what to our knowledge constitutes the first automatic analysis of visual variability of syllabic glyphs along historical periods and across geographic regions of the ancient Maya world via the HOOSC descriptor. Overall, our approach is promising, as it improves performance on the retrieval task, has been successfully validated under an epigraphic viewpoint, and has the potential of offering both novel insights in archeology and practical solutions for real daily scholar needs.	[Roman-Rangel, Edgar; Odobez, Jean-Marc; Gatica-Perez, Daniel] Idiap Res Inst, CH-1920 Martigny, Switzerland; [Roman-Rangel, Edgar; Odobez, Jean-Marc; Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Pallan, Carlos] Natl Anthropol & Hist Inst Mexico INAH, Mexico City, DF, Mexico	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Roman-Rangel, E (corresponding author), Idiap Res Inst, CH-1920 Martigny, Switzerland.	eroman@idiap.ch; pallan.carlos@gmail.com; odobez@idiap.ch; gatica@idiap.ch	Roman-Rangel, Edgar/AAL-7489-2021	Roman-Rangel, Edgar/0000-0002-0590-1698	Swiss National Science Foundation [200021-116702]; INAH	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); INAH	We thank the support of the Swiss National Science Foundation thorough the CODICES project (grant 200021-116702), and of INAH through the AJIMAYA project.	[Anonymous], 1958, J SOCI T AM RICANIST; [Anonymous], 2000, CHRONICLE MAYA KINGS; Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Belongie S, 2001, ADV NEUR IN, V13, P831; BOUJEMAA N, 2002, P ACM MM MIR PAR; Duan HG, 2003, IEEE T IMAGE PROCESS, V12, P365, DOI 10.1109/TIP.2003.809010; Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144; Frome A., 2007, P INT C COMP VIS ICC; GRAHAM I, 1975, INTRO CORP CORP MAYA; GRUBE NK, 1989, THESIS U HAMBURG; Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24; JIANG T, 2009, P COMP VIS PATT REC; Justeson John S., 1985, FOREIGN IMPACT LOWLA, V53; Lacadena A, 1995, THESIS U COMPLUTENSE; Lee Y. J., 2009, P COMP VIS PATT REC; Lewis PH, 2004, IEEE T IMAGE PROCESS, V13, P302, DOI 10.1109/TIP.2003.821346; Looper, 2003, NEW CATALOG MAYA HIE; LU C, 2009, P INT C COMP VIS ICC; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220; OPELT A, 2006, EUR C COMP VIS GRAZ; PROSKOURIAKOFF T, 1960, AM ANTIQUITY, V25, P454, DOI 10.2307/276633; QUELHAS P, 2005, P INT C COMP VIS BEI; ROMANRANGEL E, 2009, P IEEE WORKSH EH DIG; Sharer R. J., 1996, DAILY LIFE MAYA CIVI; SHOTTON J, 2005, P INT C COMP VIS BEI; SIVIC J, 2003, P INT C COMP VIS NIC; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; Stuart David, 2005, SOURCEBOOK 29 MAYA H; Taube Karl A., 1992, MAJOR GODS ANCIENT Y; Thompson J., 1962, CATALOG MAYA HIEROGL; Willamowski J., 2004, P ICPR WORKSH LEARN; Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008; ZHU Q, 2008, P EUR C COMP VIS MAR	35	33	38	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2011	94	1					101	117		10.1007/s11263-010-0387-x	http://dx.doi.org/10.1007/s11263-010-0387-x			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)	Computer Science	760JM		Green Published			2022-12-18	WOS:000290320600008
J	Chen, T; Vemuri, BC; Rangarajan, A; Eisenschenk, SJ				Chen, Ting; Vemuri, Baba C.; Rangarajan, Anand; Eisenschenk, Stephan J.			Group-Wise Point-Set Registration Using a Novel CDF-Based Havrda-Charvat Divergence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Group-wise registration; Point-set; Thin plate spline; Information theory; Havrda-Charvat divergence	ATLAS; ALGORITHM; MIXTURE	This paper presents a novel and robust technique for group-wise registration of point sets with unknown correspondence. We begin by defining a Havrda-Charvat (HC) entropy valid for cumulative distribution functions (CDFs) which we dub the HC Cumulative Residual Entropy (HC-CRE). Based on this definition, we propose a new measure called the CDF-HC divergence which is used to quantify the dis-similarity between CDFs estimated from each point-set in the given population of point sets. This CDF-HC divergence generalizes the CDF Jensen-Shannon (CDF-JS) divergence introduced earlier in the literature, but is much simpler in implementation and computationally more efficient. A closed-form formula for the analytic gradient of the cost function with respect to the non-rigid registration parameters has been derived, which is conducive for efficient quasi-Newton optimization. Our CDF-HC algorithm is especially useful for unbiased point-set atlas construction and can do so without the need to establish correspondences. Mathematical analysis and experimental results indicate that this CDF-HC registration algorithm outperforms the previous group-wise point-set registration algorithms in terms of efficiency, accuracy and robustness.	[Chen, Ting; Vemuri, Baba C.; Rangarajan, Anand] Univ Florida, Dept CISE, Gainesville, FL 32601 USA; [Eisenschenk, Stephan J.] Univ Florida, Dept Neurol, Gainesville, FL 32601 USA	State University System of Florida; University of Florida; State University System of Florida; University of Florida	Rangarajan, A (corresponding author), Univ Florida, Dept CISE, Gainesville, FL 32601 USA.	anand@cise.ufl.edu	Chen, Ting/E-1559-2013; Xuan, Ying/B-4004-2014; Rangarajan, Anand/A-8652-2009	Rangarajan, Anand/0000-0001-8695-8436	NIH [RO1 NS046812]; NSF [0307712]; NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE [R01NS046812] Funding Source: NIH RePORTER	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS))	This research was in part funded by the NIH grant RO1 NS046812 and NSF grant NSF 0307712.	BAIRD HS, 1985, MODEL BASED IMAGE MA; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Blake A., 1998, ACTIVE CONTOURS APPL; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Chui H, 2004, IEEE T PATTERN ANAL, V26, P160, DOI 10.1109/TPAMI.2004.1262178; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Cootes TF, 1999, IMAGE VISION COMPUT, V17, P567, DOI 10.1016/S0262-8856(98)00175-9; Duta N, 2001, IEEE T PATTERN ANAL, V23, P433, DOI 10.1109/34.922703; FELLER W, 1948, ANN MATH STAT, V19, P177, DOI 10.1214/aoms/1177730243; Glaunes J, 2004, PROC CVPR IEEE, P712; GOSSET E, 1987, ASTRON ASTROPHYS, V188, P258; Guo HY, 2005, LECT NOTES COMPUT SC, V3750, P984; Havrda J., 1967, KYBERNETIKA, V3, P30; Jian B, 2005, IEEE I CONF COMP VIS, P1246; Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333; Lorenzen P, 2006, MED IMAGE ANAL, V10, P440, DOI 10.1016/j.media.2005.03.002; McLachlan G.J., 1988, MIXTURE MODELS INFER, V38; Nocedal J., 1999, SPRINGER SERIES OPER; PEACOCK JA, 1983, MON NOT R ASTRON SOC, V202, P615, DOI 10.1093/mnras/202.3.615; Rohlfing T, 2001, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P187, DOI 10.1109/MMBIA.2001.991733; ROHR K, 2001, LANDMARK BASED IMAGE; SABUNCU MR, 2007, P MICCAI WORKSH STAT, V10, P47; Sebastian TB, 2000, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P70, DOI 10.1109/MMBIA.2000.852362; TSIN Y, 2004, LNCS, V3024, P558; TWINING CJ, 2006, P MED IMAGE UNDERSTA, V2, P226; Wahba G., 1990, SPLINE MODELS OBSERV; Wang F, 2003, LECT NOTES COMPUT SC, V2732, P388; WANG F, 2006, P IEEE C COMP VIS PA, P1283; Wang F, 2008, IEEE T PATTERN ANAL, V30, P2011, DOI 10.1109/TPAMI.2007.70829; Wang Y, 2002, IEEE T IMAGE PROCESS, V11, P868, DOI 10.1109/TIP.2002.801120	30	33	33	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	1					111	124		10.1007/s11263-009-0261-x	http://dx.doi.org/10.1007/s11263-009-0261-x			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	534MZ	20221321	Green Accepted, Green Submitted			2022-12-18	WOS:000272903100006
J	Bronstein, AM; Bronstein, MM; Kimmel, R				Bronstein, Alexander M.; Bronstein, Michael M.; Kimmel, Ron			Topology-Invariant Similarity of Nonrigid Shapes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape similarity; Isometry; Topological noise; Gromov-Hausdorff distance; Generalized MDS; GMDS; Iterative closest point	POINT; REGISTRATION; SIGNATURES; FRAMEWORK; DISTANCES; SURFACES; HEAR	This paper explores the problem of similarity criteria between nonrigid shapes. Broadly speaking, such criteria are divided into intrinsic and extrinsic, the first referring to the metric structure of the object and the latter to how it is laid out in the Euclidean space. Both criteria have their advantages and disadvantages: extrinsic similarity is sensitive to nonrigid deformations, while intrinsic similarity is sensitive to topological noise. In this paper, we approach the problem from the perspective of metric geometry. We show that by unifying the extrinsic and intrinsic similarity criteria, it is possible to obtain a stronger topology-invariant similarity, suitable for comparing deformed shapes with different topology. We construct this new joint criterion as a tradeoff between the extrinsic and intrinsic similarity and use it as a set-valued distance. Numerical results demonstrate the efficiency of our approach in cases where using either extrinsic or intrinsic criteria alone would fail.	[Bronstein, Alexander M.; Bronstein, Michael M.; Kimmel, Ron] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Bronstein, AM (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.	bronstein@ieee.org	Bronstein, Michael/G-5415-2010	Bronstein, Michael/0000-0002-1262-7252	Israel Science Foundation [623/08]; United States-Israel Binational Science Foundation [2004274]	Israel Science Foundation(Israel Science Foundation); United States-Israel Binational Science Foundation(US-Israel Binational Science Foundation)	This research was supported by the Israel Science Foundation grant No. 623/08 and by the United States-Israel Binational Science Foundation grant No. 2004274.	Amberg B., 2007, P CVPR; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Berger M., 2002, PANORAMIC VIEW RIEMA; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Borg I., 1997, MODERN MULTIDIMENSIO; Bronstein AM, 2008, INT J COMPUT VISION, V78, P67, DOI 10.1007/s11263-007-0078-4; Bronstein AM, 2007, IEEE T VIS COMPUT GR, V13, P902, DOI 10.1109/TVCG.2007.1041; Bronstein AM, 2007, IEEE T IMAGE PROCESS, V16, P188, DOI 10.1109/TIP.2006.884940; Bronstein AM, 2006, LECT NOTES COMPUT SC, V3953, P396, DOI 10.1007/11744078_31; Bronstein AM, 2006, SIAM J SCI COMPUT, V28, P1812, DOI 10.1137/050639296; Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1; Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103; Bronstein AM, 2005, INT J COMPUT VISION, V64, P5, DOI 10.1007/s11263-005-1085-y; BRONSTEIN AM, 2007, P 11 IEEE INT C COMP, P1; BRONSTEIN AM, 2008, P WORKSH NONR SHAP A; BRONSTEIN AM, 2008, INT J COMPUTER VISIO; Bronstein MM, 2006, NUMER LINEAR ALGEBR, V13, P149, DOI 10.1002/nla.475; BRUCKSTEIN AM, 1992, INT J COMPUT VISION, V7, P271, DOI 10.1007/BF00126396; Burago D., 2001, GRADUATE STUDIES MAT, V33, DOI 10.1090/gsm/033; CHEN Y, 1991, P C ROB AUT; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Chung F., 1997, AM MATH SOC, DOI 10.1090/cbms/092; Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102; COOPER J, 1995, COMBINATORICA, V15, P319, DOI 10.1007/BF01299739; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100; ECKSTEIN I, 2007, P S GEOM PROC; Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902; Elad A, 2002, MATH VISUAL, P77; GELFAND N, 2005, P S GEOM PROC SGP; GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5; GORDON C, 1992, B AM MATH SOC, V27, P134, DOI 10.1090/S0273-0979-1992-00289-6; Groemer H., 1996, GEOMETRIC APPL FOURI; GROMOV M., 1981, TEXTES MATH, V1; Grossmann R, 2002, IEEE T PATTERN ANAL, V24, P433, DOI 10.1109/34.993552; HAHNEL D, 2003, P IJCAI; HOCHBAUM DS, 1985, MATH OPER RES, V10, P180, DOI 10.1287/moor.10.2.180; HOCHBAUM DS, 1985, MATH OPERATIONS RES, V10; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Jacobs DW, 2000, IEEE T PATTERN ANAL, V22, P583, DOI 10.1109/34.862197; KAC M, 1966, AM MATH MON, V73, P1, DOI 10.2307/2313748; Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9; Kazhdan M., 2003, P S GEOM PROCESS, P156; KILIAN M, 2007, P SIGGRAPH, V26; Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431; Latecki LJ, 2000, IEEE T PATTERN ANAL, V22, P1185, DOI 10.1109/34.879802; Latecki LJ, 2005, IMAGE VISION COMPUT, V23, P227, DOI 10.1016/j.imavis.2004.06.015; LEOPOLDSEDER S, 2003, D2 TREE HIERARCHICAL; Levy B, 2006, INT C SHAP MOD APPL; Ling H., 2005, P ICCV; LING H, 2005, P CVPR; Litke N., 2005, IMAGE PROCESSING APP; Memoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y; MITRA NJ, 2004, P EUR S GEOM PROC, P23; Mohar B., 1991, GRAPH THEORY COMBINA, V2, P12; Novoselov VV, 2003, GENE EXPR PATTERNS, V3, P225, DOI 10.1016/S1567-133X(02)00077-7; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5; RAVIV D, 2007, P WORKSH NONR REG TR; Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011; Rosman G, 2008, COMPUT IMAGING VIS, V36, P243; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Rustamov Raif M, 2007, P 5 EUR S GEOM PROC, P225, DOI DOI 10.2312/SGP/SGP07/225-233; SALUKWADZE ME, 1979, VECTOR VALUED OPTIMI; SALZMANN M, 2007, T PAMI, V29; SCHWARTZ EL, 1989, IEEE T PATTERN ANAL, V11, P1005, DOI 10.1109/34.35506; SHUM H, 1995, 3D SHAPE SIMILARITY; TAL A, 2001, P EUR WORKSH MULT; Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502; TEAGUE M, 1979, J OPT SOC AM, V70, P920; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749; Walter J. A., 2002, P 8 ACM SIGKDD INT C, P123; WEBER O, 2008, P ACM T GRA IN PRESS; YU M, 2003, P CVPR, V2; ZHANG C, 2001, P IEEE ICIP, V3; Zhang C., 2001, ACM MULTIMEDIA, P615; ZHANG Z, 2002, CSE02019 PENNS STAT; ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149; Zigelman G, 2002, IEEE T VIS COMPUT GR, V8, P198, DOI 10.1109/2945.998671	80	33	35	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2009	81	3					281	301		10.1007/s11263-008-0172-2	http://dx.doi.org/10.1007/s11263-008-0172-2			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	394GK		Green Submitted			2022-12-18	WOS:000262433800004
J	Cuzol, A; Hellier, P; Memin, E				Cuzol, Anne; Hellier, Pierre; Memin, Etienne			A low dimensional fluid motion estimator	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						fluid motion; optical flow; parametric model; radial basis functions	IMAGE REGISTRATION; DENSE ESTIMATION; OPTICAL-FLOW; MEAN SHIFT; FIELDS; COMPUTATION	In this paper we propose a new motion estimator for image sequences depicting fluid flows. The proposed estimator is based on the Helmholtz decomposition of vector fields. This decomposition consists in representing the velocity field as a sum of a divergence free component and a vorticity free component. The objective is to provide a low-dimensional parametric representation of optical flows by depicting them as deformations generated by a reduced number of vortex and source particles. Both components are approximated using a discretization of the vorticity and divergence maps through regularized Dirac measures. The resulting so called irrotational and solenoidal fields consist of linear combinations of basis functions obtained through a convolution product of the Green kernel gradient and the vorticity map or the divergence map respectively. The coefficient values and the basis function parameters are obtained by minimization of a functional relying on an integrated version of mass conservation principle of fluid mechanics. Results are provided on synthetic examples and real world sequences.	Univ Rennes 1, IRISA, F-35042 Rennes, France	Universite de Rennes	Cuzol, A (corresponding author), Univ Rennes 1, IRISA, Campus Beaulieu, F-35042 Rennes, France.	acuzol@irisa.fr; phellier@irisa.fr; memin@irisa.fr						Alvarez L, 2000, INT J COMPUT VISION, V39, P41, DOI 10.1023/A:1008170101536; AMINI A, 1994, P EUR C COMP VIS, P125; Bartoli A., 2004, P 15 BRIT MACH VIS C, V2, P899; Bereziat D, 2000, PROC CVPR IEEE, P487, DOI 10.1109/CVPR.2000.854890; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; BROX T, 2004, P 8 EUR C COMP VIS, V4, P25; Burt P.J., 1984, MULTIRESOLUTION IMAG, P6; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; Chorin A. J., 1979, MATH INTRO FLUID MEC; Chorin AJ, 1973, J FLUID MECH, V57, P785, DOI 10.1017/S0022112073002016; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761; Corpetti T, 2003, J MATH IMAGING VIS, V19, P175, DOI 10.1023/A:1026352203836; Corpetti T, 2002, IEEE T PATTERN ANAL, V24, P365, DOI 10.1109/34.990137; Cottet G.-H., 2000, VORTEX METHODS THEOR; Cuzol A, 2005, LECT NOTES COMPUT SC, V3565, P456; CUZOL A, 2005, 5 INT C SCAL SPAC PD; CUZOL A, 2005, P INT C COMP VIS ICC; Fitzpatrick J. M., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P78; FITZPATRICK JM, 1988, COMPUT VISION GRAPH, V44, P155, DOI 10.1016/S0734-189X(88)80003-3; FORD RM, 1994, CVGIP-GRAPH MODEL IM, V56, P75, DOI 10.1006/cgip.1994.1007; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Gilbert JC, 1992, SIAM J OPTIMIZ, V2, P21, DOI 10.1137/0802003; Hellier P, 2001, IEEE T MED IMAGING, V20, P388, DOI 10.1109/42.925292; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; KOHLBERGER T, 2003, INT C SCAL SPAC THEO; Lamballais E, 2002, J TURBUL, V3, DOI 10.1088/1468-5248/3/1/028; Larsen R, 1998, IEEE T GEOSCI REMOTE, V36, P256, DOI 10.1109/36.655334; LEONARD A, 1980, J COMP PHYS, V37; Lester H, 1999, PATTERN RECOGN, V32, P129, DOI 10.1016/S0031-3203(98)00095-8; Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664; Maintz J B, 1998, Med Image Anal, V2, P1, DOI 10.1016/S1361-8415(01)80026-8; Memin E., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P620, DOI 10.1109/ICCV.1999.791282; Memin E, 1998, IEEE T IMAGE PROCESS, V7, P703, DOI 10.1109/83.668027; NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833; NOMURA A, 1991, PATTERN RECOGN LETT, V12, P183, DOI 10.1016/0167-8655(91)90048-Q; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; SONG SM, 1991, IEEE T MED IMAGING, V10, P295, DOI 10.1109/42.97579; Thirion J P, 1998, Med Image Anal, V2, P243, DOI 10.1016/S1361-8415(98)80022-4; Toga AW, 2001, IMAGE VISION COMPUT, V19, P3, DOI 10.1016/S0262-8856(00)00055-X; Weickert J, 2001, J MATH IMAGING VIS, V14, P245, DOI 10.1023/A:1011286029287; Wildes RP, 1997, PROC CVPR IEEE, P969, DOI 10.1109/CVPR.1997.609445; YUAN J, 2005, 5 INT C SCAL SPAC PD; Zhou L, 2000, PROC CVPR IEEE, P744, DOI 10.1109/CVPR.2000.854949; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9; [No title captured]	46	33	35	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2007	75	3					329	349		10.1007/s11263-007-0037-0	http://dx.doi.org/10.1007/s11263-007-0037-0			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	211QS		Green Submitted			2022-12-18	WOS:000249539000002
J	Riklin-Raviv, T; Kiryati, N; Sochen, N				Riklin-Raviv, Tammy; Kiryati, Nahum; Sochen, Nir			Prior-based segmentation and shape registration in the presence of perspective distortion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						prior-based segmentation; registration; projective transformation; homography; level-sets; variational methods	ACTIVE CONTOURS	Challenging object detection and segmentation tasks can be facilitated by the availability of a reference object. However, accounting for possible transformations between the different object views, as part of the segmentation process, remains difficult. Recent statistical methods address this problem by using comprehensive training data. Other techniques can only accommodate similarity transformations. We suggest a novel variational approach to prior-based segmentation, using a single reference object, that accounts for planar projective transformation. Generalizing the Chan-Vese level set framework, we introduce a novel shape-similarity measure and embed the projective homography between the prior shape and the image to segment within a region-based segmentation functional. The proposed algorithm detects the object of interest, extracts its boundaries, and concurrently carries out the registration to the prior shape. We demonstrate prior-based segmentation on a variety of images and verify the accuracy of the recovered transformation parameters.	Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel; Tel Aviv Univ, Dept Appl Math, IL-69978 Tel Aviv, Israel	Tel Aviv University; Tel Aviv University	Kiryati, N (corresponding author), Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel.		Raviv, Tammy Riklin/A-3462-2013	Kiryati, Nahum/0000-0003-1436-2275				Aubert G., 2002, MATH PROBLEMS IMAGE; BINFORD TO, 1971, P IEEE C SYSTEMS CON; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985; COLEMAN TF, 1994, OPTIMIZATION TOOLBOX; COOTES TF, 1998, P EUR C COMP VIS, V2, P484; Cremers D, 2006, INT J COMPUT VISION, V66, P67, DOI 10.1007/s11263-005-3676-z; Cremers D, 2003, LECT NOTES COMPUT SC, V2695, P388; Cremers D, 2003, PATTERN RECOGN, V36, P1929, DOI 10.1016/S0031-3203(03)00056-6; CREMERS D, 2003, WORKSH VAR GEOM LEV, P169; DUCI A, 2002, P EUR C COMPUTER VIS, V3, P48; FAGUERAS O, 1993, 3 DIMENSIONAL COMPUT; FAGUERAS O, 2001, GEOMETRY MULTIPLE IM; Forsyth David A, 2012, COMPUTER VISION MODE; HARALICK RM, 1993, COMPUTER ROBOT VISIO, V2, P593; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; HUANG X, 2004, MICCAI, V1, P60; IRANI M, 1999, VISION ALGORITHMS TH; Jain AK, 1998, SIGNAL PROCESS, V71, P109, DOI 10.1016/S0165-1684(98)00139-X; JAIN R, 1995, MACHINE VISION, P482; Jurie F, 2001, PROC CVPR IEEE, P791; Leventon ME, 2000, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P4, DOI 10.1109/MMBIA.2000.852354; Leventon ME, 2000, PROC CVPR IEEE, P316, DOI 10.1109/CVPR.2000.855835; LEVINE MD, 1985, VISION MAN MACHINE, P46; MA Y, 0OVITATION 3D VISION; Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information; Miller MI, 2001, INT J COMPUT VISION, V41, P61, DOI 10.1023/A:1011161132514; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; RAO KG, 1996, COMPUTER VISION GRAP, P185; RIKLINRAVIV T, 2004, P EUR C COMP VIS, V4, P50; RIKLINRAVIV T, 2005, SEGMENTATION EXAMPLE; ROUSSON M, 2002, P EUR C COMP VIS, P78; Springer C. E., 1964, GEOMETRY ANAL PROJEC; STOCKMAN GC, 1977, COMMUN ACM, V20, P820, DOI 10.1145/359863.359882; TORN A, 1989, LECT NOTES COMPUT SC, V350, P1; Tsai A, 2003, IEEE T MED IMAGING, V22, P137, DOI 10.1109/TMI.2002.808355; Veltkamp RC, 2001, ADV PTRN RECOGNIT, P87; Veltkamp RC, 2000, LECT NOTES COMPUT SC, V1929, P467; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; WEISSTEIN ED, EULER ANGLES MATHWOR	43	33	35	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2007	72	3					309	328		10.1007/s11263-006-9042-y	http://dx.doi.org/10.1007/s11263-006-9042-y			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	133MW		Green Submitted			2022-12-18	WOS:000244018000005
J	Veit, T; Cao, F; Bouthemy, P				Veit, T; Cao, F; Bouthemy, P			An a contrario decision framework for region-based motion detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion detection; number of false alarms; a contrario detection	IMAGE SEQUENCES; LEVEL SETS; SEGMENTATION; PRINCIPLE	The aim of motion detection is to decide whether a given part of an image belongs to a moving object or to the static background. This paper proposes an automatic decision rule for the detection of moving regions. The proposed framework is derived from a perceptual grouping principle, namely the Helmholtz principle. This principle basically states that perceptually relevant events are perceived because they deviate from a model of complete randomness. Detections are then said to be performed a contrario: moving regions appear as low probability events in a model corresponding to the absence of moving objects in the scene. A careful design of the events considered under the hypothesis of absence of moving objects results in a general and robust motion detection algorithm. No posterior parameter tuning is necessary. Furthermore, a confidence level is attached to each detected region.	Inst Natl Rech Informat & Automat, IRISA, F-35042 Rennes, France		Veit, T (corresponding author), Inst Natl Rech Informat & Automat, IRISA, Campus Beaulieu, F-35042 Rennes, France.	thomas.veit@irisa.fr; frederic.cao@irisa.fr; patrick.bouthemy@irisa.fr		Veit, Thomas/0000-0003-0484-3809				AACH T, 1995, SIGNAL PROCESS-IMAGE, V7, P147, DOI 10.1016/0923-5965(95)00003-F; Almansa A, 2003, IEEE T PATTERN ANAL, V25, P502, DOI 10.1109/TPAMI.2003.1190575; [Anonymous], 1985, PERCEPTUAL ORG VISUA; Attneave F., 1954, PSYCHOL REV; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; Cao F., 2004, Computing and Visualization in Science, V7, P3, DOI 10.1007/s00791-004-0123-6; Caselles V, 1999, INT J COMPUT VISION, V33, P5, DOI 10.1023/A:1008144113494; CSURKA G, 1999, 7 INT C COMP VIS KER, P566; Desolneux A, 2003, IEEE T PATTERN ANAL, V25, P508, DOI 10.1109/TPAMI.2003.1190576; Desolneux A, 2001, J MATH IMAGING VIS, V14, P271, DOI 10.1023/A:1011290230196; Desolneux A, 2000, INT J COMPUT VISION, V40, P7, DOI 10.1023/A:1026593302236; DESOLNEUX A, 2005, LECT NOTES MATH; FABLET R, 1999, IEEE INT C IM PROC I; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; HSU YZ, 1984, COMPUT VISION GRAPH, V26, P73, DOI 10.1016/0734-189X(84)90131-2; KANIZSA G, 1996, GRAMMAIRE VOIR; KONRAD J, 2000, HDB IMAGE VIDEO PROC; Mansouri AR, 2003, IEEE T IMAGE PROCESS, V12, P201, DOI 10.1109/TIP.2002.807582; Moscheni F, 1998, IEEE T PATTERN ANAL, V20, P897, DOI 10.1109/34.713358; MUSE P, 2003, IEEE INT C IM PROC B; Odobez J., 1997, VIDEO DATA COMPRESSI, P283, DOI [10.1007/978-1-4615-6239-9_8., DOI 10.1007/978-1-4615-6239-9_8]; ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029; Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758; Rosin PL, 2002, COMPUT VIS IMAGE UND, V86, P79, DOI 10.1006/cviu.2002.0960; STEWART CV, 1995, IEEE T PATTERN ANAL, V17, P925, DOI 10.1109/34.464558; VEIT T, 2004, 5313 INRIA; VEIT T, 2004, IEEE C COMP VIS PATT	27	33	34	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2006	68	2					163	178		10.1007/s11263-006-6661-2	http://dx.doi.org/10.1007/s11263-006-6661-2			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	055CK					2022-12-18	WOS:000238427400004
J	Wang, X; He, L; Wee, W				Wang, X; He, L; Wee, W			Deformable contour method: A constrained optimization approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						deformable contour method; constrained optimization; image segmentation; level set	BOUNDARY DETECTION; SEGMENTATION; MODEL; IMAGES; MINIMUM; SNAKES; BRAIN	In this paper, a class of deformable contour methods using a constrained optimization approach of minimizing a contour energy function satisfying an interior homogeneity constraint is proposed. The class is defined by any positive potential function describing the contour interior characterization. An evolutionary strategy is used to derive the algorithm. A similarity threshold T-v can be used to determine the interior size and shape of the contour. Sensitivity and significance of T-v and sigma (a spreadness measure) are also discussed and shown. Experiments on noisy images and the convergence to a minimum energy gap contour are included. The developed method has been applied to a variety of medical images from CT abdominal section, MRI image slices of brain, brain tumor, a pig heart ultrasound image sequence to visual blood cell images. As the results show, the algorithm can be adapted to a broad range of medical images containing objects with vague, complex and/or irregular shape boundary, inhomogeneous and noisy interior, and contour with small gaps.	Univ Cincinnati, Elect & Comp Engn & Comp Sci Dept, Cincinnati, OH 45220 USA	University System of Ohio; University of Cincinnati	Wang, X (corresponding author), Univ Cincinnati, Elect & Comp Engn & Comp Sci Dept, Cincinnati, OH 45220 USA.	xun_wang_2001@yahoo.com; leihe_ga@yahoo.com; wwee@ececs.uc.edu						Atkins MS, 1998, IEEE T MED IMAGING, V17, P98, DOI 10.1109/42.668699; BUCK T, 1997, IEEE T EVOLUTIONARY, V1, P3; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chakraborty A, 1999, IEEE T PATTERN ANAL, V21, P12, DOI 10.1109/34.745730; Chalana V, 1996, IEEE T MED IMAGING, V15, P290, DOI 10.1109/42.500138; Cham TJ, 1999, IEEE T PATTERN ANAL, V21, P49, DOI 10.1109/34.745733; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; CHIOU GI, 1995, IEEE T IMAGE PROCESS, V4, P1407, DOI 10.1109/83.465105; COHEN JS, 1991, PHARMACOL THERAPEUT, V52, P211, DOI 10.1016/0163-7258(91)90009-B; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675; FENSTER S, 1998, ICCV, P420; Fok YL, 1996, IEEE T MED IMAGING, V15, P353, DOI 10.1109/42.500144; FRIEDLAND NS, 1992, IEEE T PATTERN ANAL, V14, P770, DOI 10.1109/34.142912; Grzeszczuk RP, 1997, IEEE T PATTERN ANAL, V19, P1100, DOI 10.1109/34.625111; Jermyn IH, 2001, IEEE T PATTERN ANAL, V23, P1075, DOI 10.1109/34.954599; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; LUNDERVOLD A, 1995, IEEE T MED IMAGING, V14, P339, DOI 10.1109/42.387715; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; McInerney T, 1999, IEEE T MED IMAGING, V18, P840, DOI 10.1109/42.811261; Pien HH, 1997, INT J PATTERN RECOGN, V11, P1233, DOI 10.1142/S0218001497000573; RANGANATH S, 1995, IEEE T MED IMAGING, V14, P328, DOI 10.1109/42.387714; Samson C, 2000, INT J COMPUT VISION, V40, P187, DOI 10.1023/A:1008183109594; SETHIAN JA, 1996, P NAT ACAD SCI, V93, P4; Siddiqi K, 1998, IEEE T IMAGE PROCESS, V7, P433, DOI 10.1109/83.661193; STOVIK G, 1994, IEEE T PAMI, V16, P976; TOENNIES KD, 1994, P SOC PHOTO-OPT INS, V2167, P18, DOI 10.1117/12.175066; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; Yezzi A, 1997, IEEE T MED IMAGING, V16, P199, DOI 10.1109/42.563665; ZHONG M, 1997, THESIS U CINCINNATI; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343; Zhu Y, 1997, IEEE T MED IMAGING, V16, P55, DOI 10.1109/42.552055	32	33	34	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2004	59	1					87	108		10.1023/B:VISI.0000020672.14006.ad	http://dx.doi.org/10.1023/B:VISI.0000020672.14006.ad			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816HI					2022-12-18	WOS:000221100600004
J	Muller, H; Pun, T; Squire, D				Muller, H; Pun, T; Squire, D			Learning from user behavior in image retrieval: Application of market basket analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						content-based image retrieval; market basket analysis; learning from user interaction	PERFORMANCE	This article describes an approach to learn feature weights for content-based image retrieval (CBIR) from user interaction log files. These usage log files are analyzed for images marked together by a user in the same query step. The problem is somewhat similar to one of the traditional data mining problems, the market basket analysis problem, where items bought together in a supermarket are analyzed. This paper outlines similarities and differences between the two fields and explains how to use the interaction data for deriving a better feature weighting Experiments with existing log files are done and a significant improvement in performance is reached with a feature weighting, calculated from the information contained in the log files. Even with several steps of relevance feedback the results remain much better than without the learning, which means that not only information from feedback is taken into account earlier, but a better quality of retrieval is reached in all steps.	Univ Geneva, Comp Vis Grp, CH-1211 Geneva 4, Switzerland; Monash Univ, Melbourne, Vic 3004, Australia	University of Geneva; Monash University	Muller, H (corresponding author), Univ Geneva, Comp Vis Grp, 24 Rue Gen Dufour, CH-1211 Geneva 4, Switzerland.	Henning.Mueller@cui.unige.ch; David.Squire@csse.monash.edu.au	Squire, David/A-1298-2008	Muller, Henning/0000-0001-6800-9878; Squire, David/0000-0001-6738-8271				Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072; Agrawal R., 1994, P 20 INT C VER LARG, P487; Berendt B, 2000, VLDB J, V9, P56, DOI 10.1007/s007780050083; Brin S., 1997, SIGMOD Record, V26, P265, DOI [10.1145/253262.253325, 10.1145/253262.253327]; Cox IJ, 1996, PROCEEDINGS OF THE THIRD FORUM ON RESEARCH AND TECHNOLOGY ADVANCES IN DIGITAL LIBRARIES (ADL '96), P66, DOI 10.1109/ADL.1996.502517; HAN J, 2000, P ANN INT ACM SIGMOD; HARMAN D, 1992, SIGIR 92 : PROCEEDINGS OF THE FIFTEENTH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1; Harman D., 1998, P 7 TEXT RETR C TREC, P1; Hipp Jochen, 2000, ACM SIGKDD EXPLORATI, V2, P58, DOI DOI 10.1145/360402.360421; JERMAIN C, 2001, P INT C COMP INT MOD, P199; LEE CS, 1999, SPIE P, V3246, P294; LI B, 2001, P 2 INT C MULT EXP I, P1168; LI M, 2001, P 3 INT WORKSH MULT, P42; Ma WY, 1997, P SOC PHOTO-OPT INS, V3016, P496, DOI 10.1117/12.274547; Mannila H., 1996, P 2 INT C KNOWL DISC, P146; MINKA T, 1996, THESIS MIT MEDIA LAB; Muller H, 2000, INT C PATT RECOG, P1043, DOI 10.1109/ICPR.2000.905650; Muller H, 2001, PATTERN RECOGN LETT, V22, P593, DOI 10.1016/S0167-8655(00)00118-5; MULLER H, 2000, ACM SIGKDD INT C KNO, P67; Muller L, 1999, P SOC PHOTO-OPT INS, V3846, P328; MULLER W, 2000, SPIE PHOT E VOIC VID, P961; PANCHANATHAN S, 1999, SPIE P, V3846; Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; SALTON G, 1990, J AM SOC INFORM SCI, V41, P288, DOI 10.1002/(SICI)1097-4571(199006)41:4<288::AID-ASI8>3.0.CO;2-H; SAVASERE A, 1995, P 22 INT C VER LARG; Shekhar S., 2001, P 7 INT S SPAT TEMP; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; Smith J.R., 1996, 4 ACM INT MULT C EXH; Smith JR, 1998, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES - PROCEEDINGS, P112, DOI 10.1109/IVL.1998.694520; Squire DM, 2000, PATTERN RECOGN LETT, V21, P1193, DOI 10.1016/S0167-8655(00)00081-7; *U WASH, 1999, ANN GROUNDTR DAT; Vasconcelos N, 2000, LECT NOTES COMPUT SC, V1842, P33; Worring M, 2000, LECT NOTES COMPUT SC, V1929, P26; Wu KL, 1998, IBM SYST J, V37, P89, DOI 10.1147/sj.371.0089; Zaki MJ., 1997, 3 INT C KDD DATA MIN	36	33	34	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN-FEB	2004	56	1-2			SI		65	77		10.1023/B:VISI.0000004832.02269.45	http://dx.doi.org/10.1023/B:VISI.0000004832.02269.45			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	745MJ		Green Accepted			2022-12-18	WOS:000186692200006
J	Guo, CE; Zhu, SC; Wu, YN				Guo, CE; Zhu, SC; Wu, YN			Modeling visual patterns by integrating descriptive and generative methods	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						descriptive models; generative models; Gibbs point processes; Markov chain Monte Carlo; Markov random fields; minimax entropy learning; perceptual organization; texton models; visual learning	CHAIN MONTE-CARLO; TEXTURE; COMPUTATION	This paper presents a class of statistical models that integrate two statistical modeling paradigms in the literature: ( I) Descriptive methods, such as Markov random fields and minimax entropy learning (Zhu, S. C., Wu, Y. N., and Mumford, D. 1997. Neural Computation, 9( 8)), and (II) Generative methods, such as principal component analysis, independent component analysis ( Bell, A. J. and Sejnowski, T. J. 1997. Vision Research, 37: 3327-3338), transformed component analysis ( Frey, B. and Jojic, N. 1999. ICCV), wavelet coding ( Mallat, S. and Zhang, Z. 1993. IEEE Trans. on Signal Processing, 41: 3397-3415; Chen, S., Donoho, D., and Saunders, M. A. 1999. Journal on Scientific Computing, 20( 1): 33-61), and sparse coding (Olshausen, B. A. and Field, D. J. 1996. Nature, 381: 607-609; Lewicki, M. S. and Olshausen, B. A. 1999. JOSA, A. 16( 7): 1587-1601). In this paper, we demonstrate the integrated framework by constructing a class of hierarchical models for texton patterns ( the term "texton" was coined by psychologist Julesz in the early 80s). At the bottom level of the model, we assume that an observed texture image is generated by multiple hidden "texton maps", and textons on each map are translated, scaled, stretched, and oriented versions of a window function, like mini-templates or wavelet bases. The texton maps generate the observed image by occlusion or linear superposition. This bottom level of the model is generative in nature. At the top level of the model, the spatial arrangements of the textons in the texton maps are characterized by minimax entropy principle, which leads to embellished versions of Gibbs point process models (Stoyan, D., Kendall, W. S., and Mecke, J. 1985. Stochastic Geometry and its Applications). The top level of the model is descriptive in nature. We demonstrate the integrated model by a set of experiments.	Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA; Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles	Guo, CE (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.	cguo@stat.ucla.edu; sczhu@stat.ucla.edu; ywu@stat.ucla.edu						AHUJA N, 1989, CVGIP, V48; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; BERGEN JR, 1988, NATURE, V333, P363, DOI 10.1038/333363a0; Chandler D., 1987, INTRO MODERN STAT ME; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; COVER TM, 1994, ELEMENTS INFORMATION; DEBONET JS, 1997, ADV NEURAL INFORMATI, V10; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Duda R.O., 2000, PATTERN CLASSIFICATI; Efros A. A., 2001, SIGGRAPH; Frey Brendan J, 1999, ICCV; FRIDMAN A, 2000, THESIS BROWN U; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GILKS WR, 1997, MARKOV CHAIN MONTE C, pCH6; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; GU MG, 1998, STOCHASTIC APPROXIMA; HEEGER DJ, 1995, SIGGRAPHS; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0; Koffka K., 1935, PRINCIPLES GESTALT P; Lee AB, 2001, INT J COMPUT VISION, V41, P35, DOI 10.1023/A:1011109015675; LEUNG T, 1999, P 7 ICCV CORF GREEC; LEUNG T, 1996, P 4 ICCV CAMBR UK; Lewicki MS, 1999, J OPT SOC AM A, V16, P1587, DOI 10.1364/JOSAA.16.001587; MALIK J, 1990, J OPTICAL SOC AM A, V7; Malik J., 1999, ICCV; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Marr D., VISION; MILLER E, 2002, THESIS MIT; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; ROWEIS S, 1999, NEURAL COMPUTATION, V11; STEVENS KA, 1978, BIOL CYBERN, V29, P19, DOI 10.1007/BF00365232; STOYAN D, 1985, STOCHASTIC GEOMETRY; Tanner M.A., 1996, TOOLS STAT INFERENCE; Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239; WU YN, 2002, ECCV; WU YN, 1999, ICCV; XU YQ, 2000, TR200032 MSR; Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1170, DOI 10.1109/34.809110; Zhu SC, 2000, IEEE T PATTERN ANAL, V22, P554, DOI 10.1109/34.862195; Zhu SC, 1997, NEURAL COMPUT, V9, P1627, DOI 10.1162/neco.1997.9.8.1627; ZHU SC, 2002, ECCV; ZHU SC, 2001, P 3 INT WORKSH PERC	44	33	47	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2003	53	1					5	29		10.1023/A:1023023207396	http://dx.doi.org/10.1023/A:1023023207396			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	668CV					2022-12-18	WOS:000182273500001
J	Heimes, F; Nagel, HH				Heimes, F; Nagel, HH			Towards active machine-vision-based driver assistance for urban areas	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						active driver assistance; model based object tracking; Kalman filter; binocular vision	ROAD VEHICLE; TRACKING; RECOGNITION	Currently available driver assistance systems (i) warn the driver based on vehicle state sensors (e.g., door open, outside temperature near or below the freezing point), (ii) offer route guidance information (navigation systems based on GPS and digital road maps), or-in some critical situations-(iii) even actively influence vehicle handling under carefully delimited conditions (anti-blocking-system, electronic-stability-program). This contribution reports about investigations to combine passive GPS- and map-based route guidance with model-based machine vision in order to automatically assess or even execute driving maneuvers in inner-city traffic situations. Information provided by todays route guidance systems is treated as a generic description of lane structure. The schematic description of lane structures extractable from commercially available standard digital maps is automatically instantiated by a machine vision approach which interprets video image sequences recorded by cameras from within a driving vehicle. The resulting model of the lane structure in front of the vehicle is subsequently exploited in order to control vehicle maneuvers in real-time as a proof of principal system competences. The machine-vision-based execution of driving maneuvers can at any time be overridden by the driver.	Fraunhofer Inst Informat & Data Proc, D-76131 Karlsruhe, Germany; Univ Karlsruhe, Fak Informat, Inst Algorithmen & Kognit Syst, D-76128 Karlsruhe, Germany	Fraunhofer Gesellschaft; Helmholtz Association; Karlsruhe Institute of Technology	Heimes, F (corresponding author), Fraunhofer Inst Informat & Data Proc, Fraunhoferstr 1, D-76131 Karlsruhe, Germany.	Dr-Frank.Heimes@epost.de; nagel@ira.uka.de						Bar-Shalom Y., 1988, TRACKING DATA ASS; BATAVIA PH, 1998, INT VEH 98 S OCT 28, P5; BATAVIA PH, 2000, CARNEGIE MELLON AHS; Behringer R, 1998, IEEE T ROBOTIC AUTOM, V14, P810, DOI 10.1109/70.720356; Bertozzi M, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P39, DOI 10.1109/IVS.2000.898315; Bertozzi M, 1998, IMAGE VISION COMPUT, V16, P585, DOI 10.1016/S0262-8856(97)00093-0; Bertozzi M, 1997, IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P1010, DOI 10.1109/ITSC.1997.660612; BERTOZZI M, 1998, IEEE INT VEH S, P305; BERTOZZI M, 1998, IVS 1998, P505; Dellaert F, 1998, IEEE INT CONF ROBOT, P1889, DOI 10.1109/ROBOT.1998.680587; Dickmanns ED, 1998, ARTIF INTELL, V103, P49, DOI 10.1016/S0004-3702(98)00071-X; ENKELMANN W, 1997, ENTWICKLUNG SYSTEMEN; Enkelmann W, 1998, IEEE INT C INT VEH, P35; Franke U, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P273, DOI 10.1109/IVS.2000.898354; GAVRILA DM, 1999, P IEEE INT C COMP VI, P87, DOI DOI 10.1109/ICCV.1999.791202; Gengenbach V., 1995, Proceedings of the Intelligent Vehicles '95. Symposium (Cat. No.95TH8132), P512, DOI 10.1109/IVS.1995.528334; *GEOM, 1999, MAPS 101 TOP MAPS BA; GILLES L, 1999, SYSTEM PARTNERS AUTO, V101, P52; Goldbeck J, 2000, IMAGE VISION COMPUT, V18, P425, DOI 10.1016/S0262-8856(99)00037-2; GORZIG S, 1998, IEEE C INTELLIGENT T, P545; GREGOR R, 2000, IVS 2000, P140; Grewal M, 1993, KALMAN FILTERING THE; Haag M, 1999, INT J COMPUT VISION, V35, P295, DOI 10.1023/A:1008112528134; Haag M, 2000, IMAGE VISION COMPUT, V18, P137, DOI 10.1016/S0262-8856(99)00021-9; Heimes F, 1998, ENG APPL ARTIF INTEL, V11, P215, DOI 10.1016/S0952-1976(97)00077-8; Heimes F, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P498, DOI 10.1109/IVS.2000.898392; Heimes F, 1998, MATH COMPUT MODEL, V27, P189, DOI 10.1016/S0895-7177(98)00059-4; HEIMES F, 2000, THESIS U KARLSRUHE; *ITSC, 2000, P IEEE INT TRANSP SY; *ITSC, 1997, IEEE C INT TRANSP SY; *IVS, 1993, P 1993 IEEE INT C IN; *IVS, 2000, P IEEE INT VEH S 200; *IVS, 1998, P 1998 IEEE INT C IN; Kluge KC, 1997, IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P723, DOI 10.1109/ITSC.1997.660563; KORN AF, 1988, IEEE T PATTERN ANAL, V10, P610, DOI 10.1109/34.6770; KREUCHER C, 1999, IEEE T ROBOTIC AUTOM, V15, P572; Kreucher C., 1998, P IEEE INT C INT VEH P IEEE INT C INT VEH, V1, P17; Kruger W, 1999, MACH VISION APPL, V11, P203, DOI 10.1007/s001380050103; Lutzeler M, 1998, P IEEE INT VEH S 98, P341; LUTZELER M, 2000, IVS 2000, P302; Masaki I., 1992, VISION BASED VEHICLE; Maurer M, 1997, IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P578, DOI 10.1109/ITSC.1997.660538; MUCK K, 2000, LECT NOTES COMPUTER, V1843, P411; NAGEL HH, 1995, MATH COMPUT MODEL, V22, P185, DOI 10.1016/0895-7177(95)00133-M; Nagel HH, 2000, IMAGE VISION COMPUT, V18, P435, DOI 10.1016/S0262-8856(99)00038-4; NAGEL HH, 1994, ARTIF INTELL, V8, P97; NAGESHA K, 1991, RAPID COMMUN MASS SP, V5, P15, DOI 10.1002/rcm.1290050105; Nigro JM, 2000, 2000 IEEE INTELLIGENT TRANSPORTATION SYSTEMS PROCEEDINGS, P71, DOI 10.1109/ITSC.2000.881020; Ozguner U, 1997, IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P502, DOI 10.1109/ITSC.1997.660525; Paetzold F, 2000, IMAGE VISION COMPUT, V18, P377, DOI 10.1016/S0262-8856(99)00033-5; PAETZOLD F, 1998, IEEE C INTELLIGENT T, P87; Pellkofer M, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P296, DOI 10.1109/IVS.2000.898358; Pomerleau D, 1997, IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P906, DOI 10.1109/ITSC.1997.660594; Risack R, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P356, DOI 10.1109/IVS.2000.898369; Rombaut M, 2000, 2000 IEEE INTELLIGENT TRANSPORTATION SYSTEMS PROCEEDINGS, P77, DOI 10.1109/ITSC.2000.881021; SIEDERSBERGER KH, 2000, IVS 2000, P146; STRUCK G, 1993, IEEE S INT VEH IEEE, P461; Thorpe C, 1997, IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P496, DOI 10.1109/ITSC.1997.660524; Thorpe C.E, 1990, VISION NAVIGATION; WINTERHAGEN J, 1999, ATZ AUTOMOBILTECHNIS, V101, P820; Zomotor A., 1987, FAHRWERKTECHNIK FAHR, V1st	61	33	35	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2002	50	1					5	34		10.1023/A:1020272819017	http://dx.doi.org/10.1023/A:1020272819017			30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	593GW					2022-12-18	WOS:000177983500001
J	Cooper, MC				Cooper, MC			The tractability of segmentation and scene analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						segmentation; scene analysis; computational complexity; NP-completeness	IMAGE SEGMENTATION; RECOGNITION; COMPLEXITY; PROJECTIONS; OBJECTS	One of the fundamental problems in computer vision is the segmentation of an image into semantically meaningful regions, based only on image characteristics. A single segmentation can be determined using a Linear number of evaluations of a uniformity predicate. However, minimising the number of regions is shown to be an NP-complete problem. We also show that the variational approach to segmentation, based on minimising a criterion combining the overall variance of regions and the number of regions, also gives rise to an NP-complete problem. When a library of object models is available, segmenting the image becomes a problem of scene analysis. A sufficient condition for the reconstruction of a 3D scene from a 2D image to be solvable in polynomial time is that the scene contains no cycles of mutually occluding objects and that no range information can be deduced from the image. It is known that relaxing the no cycles condition renders the problem NP-complete. We show that relaxing the no range information condition also produces an NP-complete problem.	Univ Toulouse 3, Inst Rech Informat Toulouse, F-31062 Toulouse, France	Universite de Toulouse; Universite Federale Toulouse Midi-Pyrenees (ComUE); Universite Toulouse III - Paul Sabatier; Institut National Polytechnique de Toulouse; Universite Toulouse 1 Capitole; Universite de Toulouse - Jean Jaures; Centre National de la Recherche Scientifique (CNRS)	Cooper, MC (corresponding author), Univ Toulouse 3, Inst Rech Informat Toulouse, 118 Route Narbonne, F-31062 Toulouse, France.		Cooper, Martin/AAE-8777-2020; Cooper, Martin/AAV-1705-2021	Cooper, Martin/0000-0003-4853-053X; Cooper, Martin/0000-0003-4853-053X				Aho AV, 1974, DESIGN ANAL COMPUTER; ANSARI N, 1990, IEEE T PATTERN ANAL, V12, P470, DOI 10.1109/34.55107; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BEAULIEU JM, 1989, IEEE T PATTERN ANAL, V11, P150, DOI 10.1109/34.16711; BORISENKO VI, 1987, AUTOMAT REM CONTR+, V48, P837; CASS TA, 1992, P EUR C COMP VIS, P834; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; COOPER M, 1992, VISUAL OCCLUSION INT; COOPER MC, 1988, PATTERN RECOGN LETT, V7, P259, DOI 10.1016/0167-8655(88)90111-0; COOPER MC, 1994, ARTIF INTELL, V65, P347, DOI 10.1016/0004-3702(94)90021-3; Cooper MC, 1997, IMAGE VISION COMPUT, V15, P263, DOI 10.1016/S0262-8856(96)01135-3; Dendris N. D., 1994, Journal of Mathematical Imaging and Vision, V4, P375, DOI 10.1007/BF01262403; EADES P, 1993, PATTERN RECOGN LETT, V14, P715, DOI 10.1016/0167-8655(93)90140-9; FU KS, 1981, PATTERN RECOGN, V13, P3, DOI 10.1016/0031-3203(81)90028-5; Garey M.R., 1979, COMPUTERS INTRACTABI; GUPTA L, 1990, PATTERN RECOGN, V23, P1103, DOI 10.1016/0031-3203(90)90006-7; HARALICK RM, 1985, COMPUT VISION GRAPH, V29, P100, DOI 10.1016/S0734-189X(85)90153-7; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; KIROUSIS LM, 1990, IEEE T PATTERN ANAL, V12, P123, DOI 10.1109/34.44400; KIROUSIS LM, 1988, J COMPUT SYST SCI, V37, P14, DOI 10.1016/0022-0000(88)90043-8; KIROUSIS LM, 1993, ARTIF INTELL, V64, P147, DOI 10.1016/0004-3702(93)90063-H; KUBE PR, 1991, BEHAV BRAIN SCI, V4, P768; LANDAN Y, 1988, P IEEE COMP SOC C CO, P335; LIM YW, 1990, PATTERN RECOGN, V23, P935, DOI 10.1016/0031-3203(90)90103-R; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; NEVEU CF, 1986, COMPUT VISION GRAPH, V34, P52, DOI 10.1016/0734-189X(86)90047-2; NITZBERG M, 1991, LECT NOTES COMPUTER, V662; PAL NR, 1993, PATTERN RECOGN, V26, P1277, DOI 10.1016/0031-3203(93)90135-J; Papadimitriou CH., 1993, COMPUT COMPLEX; PARODI P, 1994, ARTIF INTELL, V70, P239, DOI 10.1016/0004-3702(94)90107-4; Pavlidas T., 1977, STRUCTURAL PATTERN R; Sugihara K., 1986, MACHINE INTERPRETATI; TSOTSOS JK, 1990, BEHAV BRAIN SCI, V13, P423, DOI 10.1017/S0140525X00079577; ULLMANN JR, 1992, IEEE T PATTERN ANAL, V14, P485, DOI 10.1109/34.126808	35	33	34	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1998	30	1					27	42		10.1023/A:1008013412628	http://dx.doi.org/10.1023/A:1008013412628			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	152CC					2022-12-18	WOS:000077757100002
J	Brandt, JW				Brandt, JW			Improved accuracy in gradient-based optical flow estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						optical flow	COMPUTATION	Optical flow estimation by means of first derivatives can produce surprisingly accurate and dense optical flow fields. In particular, recent empirical evidence suggests that the method that is based on local optimization of first-order constancy constraints is among the most accurate and reliable methods available. Nevertheless, a systematic investigation of the effects of the various parameters for this algorithm is still lacking. This paper reports such an investigation. Performance is assessed in terms of flow-field accuracy, density, and resolution. The investigation yields new information regarding pre-filter, differentiator, least-squares neighborhood, and reliability test selection. Several changes to previously-employed parameter settings result in significant overall performance improvements, while they simultaneously reduce the computational cost of the estimator.			Brandt, JW (corresponding author), SILICON GRAPH COMP SYST, ADV SYST DIV, 2011 N SHORELINE BLVD, MT VIEW, CA 94043 USA.							Adelson E. H., 1986, Proceedings of the Workshop on Motion: Representation and Analysis (Cat. No.86CH2322-6), P151; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BARRON JL, 1993, TR299 U W ONT DEP CO; BRANDT JW, 1994, P 1 IEEE INT C IM PR, V2, P775; BRANDT JW, 1994, IAPR WORKSH MACH VIS, P464; BRANDT JW, 1994, P 28 ANN AS C SIGN S, P721; CAFFORIO C, 1982, SIGNAL PROCESS, V4, P45, DOI 10.1016/0165-1684(82)90038-X; CAFFORIO C, 1979, SIGNAL PROCESS, V1, P133, DOI 10.1016/0165-1684(79)90015-X; CAMPANI M, 1990, P 3 IEEE INT C COMP, P2; ENKELMANN W, 1988, COMPUT VISION GRAPH, V43, P150, DOI 10.1016/0734-189X(88)90059-X; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; HEEGER DJ, 1987, J OPT SOC AM A, V4, P1455, DOI 10.1364/JOSAA.4.001455; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; LUCAS B, 1985, P 7 INT JOINT C ART, P981; Lucas B D, 1981, P 7 INT JOINT C ARTI, P674; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; NAGEL HH, 1987, ARTIF INTELL, V33, P299, DOI 10.1016/0004-3702(87)90041-5; NOMURA A, 1993, TIME VARYING IMAGE P, V3, P343; OTTE M, 1994, LECTURE NOTES COMPUT, V800, P51; Simoncelli E, 1993, THESIS MIT; Simoncelli E. P., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P310, DOI 10.1109/CVPR.1991.139707; Simoncelli E. P., 1994, P 1 IEEE INT C IM PR, V1, P790; TISTARELLI M, 1994, LECT NOTES COMPUTER, V800, P61; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; VERRI A, 1990, J OPT SOC AM A, V7, P912, DOI 10.1364/JOSAA.7.000912; WEBER J, 1995, INT J COMPUT VISION, V14, P67, DOI 10.1007/BF01421489; Weber J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P12, DOI 10.1109/ICCV.1993.378240	28	33	33	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1997	25	1					5	22		10.1023/A:1007987001439	http://dx.doi.org/10.1023/A:1007987001439			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YB326					2022-12-18	WOS:A1997YB32600001
J	PAUWELS, EJ; MOONS, T; VANGOOL, LJ; KEMPENAERS, P; OOSTERLINCK, A				PAUWELS, EJ; MOONS, T; VANGOOL, LJ; KEMPENAERS, P; OOSTERLINCK, A			RECOGNITION OF PLANAR SHAPES UNDER AFFINE DISTORTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								Methods for the recognition of planar shapes from arbitrary viewpoints are described. The adopted model of projection is orthographic. The invariant descriptions derived for this group are one-dimensional shape signatures comparable to the well-known curvature as a function of arc length description of Euclidean geometry. Since the use of such differential invariants in the affine case would lead to unacceptably high orders of derivatives, affine invariant descriptions based on semi-differential invariants are proposed as an alternative. A systematic discussion of different types of these invariants is given. The usefulness and viability of this methodology is demonstrated on a database containing more than 40 objects.			PAUWELS, EJ (corresponding author), KATHOLIEKE UNIV LEUVEN,ESAT MI2,KARD MERCIERLAAN 94,B-3001 LOUVAIN,BELGIUM.							BARTELS HR, 1987, INTRO SPLINES USE CO; BRUCKSTEIN AM, 1990, DIFFERENTIAL INVARIA; COSTA M, 1989, SPIE, V1095, P515; CYGANSKI D, 1987, 1ST P INT C COMP VIS, P496; KEMPENAERS P, 1991, VISUAL FORM, P323; LAMDAN Y, 1988, APR P IEEE INT C ROB, P1407; MANNAERT H, 1990, P INT JOINT C NEURAL, P405; MOONS T, 1995, INT J COMPUT VISION, V14, P49; Olver P.J., 1986, GRADUATE TEXTS MATH, V107; PAUWELS EJ, 1994, KULESATMI29405 KATH; ROTHWELL CA, 1991, P BMVC, P62; SAGLE AA, 1973, PURE APPLIED MATH, V51; Van Gool L., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P454, DOI 10.1109/CVPR.1991.139735; VANGOOL L, 1992, GEOMETRIC INVARIANCE, P293; [No title captured]	15	33	33	0	4	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1995	14	1					49	65		10.1007/BF01421488	http://dx.doi.org/10.1007/BF01421488			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QG479					2022-12-18	WOS:A1995QG47900003
J	Sundermeyer, M; Marton, ZC; Durner, M; Triebel, R				Sundermeyer, Martin; Marton, Zoltan-Csaba; Durner, Maximilian; Triebel, Rudolph			Augmented Autoencoders: Implicit 3D Orientation Learning for 6D Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						6D object detection; Pose estimation; Domain randomization; Autoencoder; Synthetic data; Symmetries	REPRESENTATIONS; REGISTRATION	We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results. Our code is available here https://github.com/DLR-RM/AugmentedAutoencoder.	[Sundermeyer, Martin; Marton, Zoltan-Csaba; Durner, Maximilian; Triebel, Rudolph] German Aerosp Ctr DLR, D-82234 Wessling, Germany; [Triebel, Rudolph] Tech Univ Munich, D-80333 Munich, Germany	Helmholtz Association; German Aerospace Centre (DLR); Technical University of Munich	Sundermeyer, M (corresponding author), German Aerosp Ctr DLR, D-82234 Wessling, Germany.	martin.sundermeyer@dlr.de; zoltan.marton@dlr.de; maximilian.durner@dlr.de; rudolph.triebel@dlr.de	Triebel, Rudolph/ABG-8692-2020	Triebel, Rudolph/0000-0002-7975-036X	German Aerospace Center (DLR); Robert Bosch GmbH	German Aerospace Center (DLR)(Helmholtz AssociationGerman Aerospace Centre (DLR)); Robert Bosch GmbH	Funding was provided by German Aerospace Center (DLR) and Robert Bosch GmbH.	[Anonymous], 2017, ARXIV170907857; Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; Drost Bertram, 2010, 2010 IEEE COMP SOC C, DOI DOI 10.1109/CVPR.2010.5540108; Everingham M., 2012, PASCAL VISUAL OBJECT; Glorot X., 2010, PROC MACH LEARN RES, P249; HINTERSTOISSER S, 2008, P IEEE C COMP VIS PA, P1; Hinterstoisser S., 2017, ARXIV171010710; Hinterstoisser S, 2016, LECT NOTES COMPUT SC, V9907, P834, DOI 10.1007/978-3-319-46487-9_51; Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206; Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326; Hinterstoisser Stefan, 2012, P AS C COMP VIS, P2, DOI DOI 10.1007/978-3-642-37331-2_42; HODA T, 2017, IEEE WINT C APPL COM; Hodan T., 2017, SIXD CHALLENGE 2017; Hodan T, 2019, IEEE IMAGE PROC, P66, DOI 10.1109/ICIP.2019.8803821; Hodan T, 2018, LECT NOTES COMPUT SC, V11214, P19, DOI 10.1007/978-3-030-01249-6_2; Hodan T, 2016, LECT NOTES COMPUT SC, V9915, P606, DOI 10.1007/978-3-319-49409-8_52; Howard A.G., 2017, MOBILENETS EFFICIENT; Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169; Kehl W, 2016, LECT NOTES COMPUT SC, V9907, P205, DOI 10.1007/978-3-319-46487-9_13; Kingma D.P, P 3 INT C LEARNING R; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Mahendran S, 2017, IEEE COMPUT SOC CONF, P494, DOI 10.1109/CVPRW.2017.73; Manhardt F, 2018, LECT NOTES COMPUT SC, V11218, P833, DOI 10.1007/978-3-030-01264-9_49; Mitash C, 2017, IEEE INT C INT ROBOT, P545; Movshovitz-Attias Y, 2016, LECT NOTES COMPUT SC, V9915, P202, DOI 10.1007/978-3-319-49409-8_18; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Saxena A, 2009, INT CONF ACOUST SPEE, P713, DOI 10.1109/ICASSP.2009.4959683; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43; Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Tremblay Jonathan, 2018, CORL; Ulrich M, 2009, IEEE INT CONF ROBOT, P2090; VIDAL J, 2018, ARXIV180208516; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930; Wu Zifeng, 2016, ARXIV160506885; Xiang Yu, 2018, POSECNN CONVOLUTIONA, V5, P8, DOI [10.15607/RSS.2018.XIV.019, DOI 10.15607/RSS.2018.XIV.019]; Zakharov S., 2019, ARXIV190211020; ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149; 2017, ADV COMPUT VIS PATT, P1	52	32	33	2	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		714	729		10.1007/s11263-019-01243-8	http://dx.doi.org/10.1007/s11263-019-01243-8		OCT 2019	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KU1MV					2022-12-18	WOS:000492170700002
J	Kuhn, A; Hirschmuller, H; Scharstein, D; Mayer, H				Kuhn, Andreas; Hirschmueller, Heiko; Scharstein, Daniel; Mayer, Helmut			A TV Prior for High-Quality Scalable Multi-View Stereo Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-View Stereo; 3D Modeling; Scalable 3D Surface Reconstruction		We present a scalable multi-view stereo method able to reconstruct accurate 3D models from hundreds of high-resolution input images. Local fusion of disparity maps obtained with semi-global matching enables the reconstruction of large scenes that do not fit into main memory. Since disparity maps may vary widely in quality and resolution, careful modeling of the 3D errors is crucial. We derive a sound stereo error model based on disparity uncertainty, which can vary spatially from tenths to several pixels. We introduce a feature based on total variation that allows pixel-wise classification of disparities into different error classes. For each class, we learn a disparity error distribution from ground-truth data using expectation maximization. We present a novel method for stochastic fusion of data with varying quality by adapting a multi-resolution volumetric fusion process that uses our error classes as a prior and models surface probabilities via an octree of voxels. Conflicts during surface extraction are resolved using visibility constraints and preference for voxels at higher resolutions. Experimental results on several challenging large-scale datasets demonstrate that our method yields improved performance both qualitatively and quantitatively.	[Kuhn, Andreas; Mayer, Helmut] Bundeswehr Univ Munich, Neubiberg, Germany; [Kuhn, Andreas; Hirschmueller, Heiko] German Aerosp Ctr, Munich, Germany; [Hirschmueller, Heiko] Roboception GmbH, Munich, Germany; [Scharstein, Daniel] Middlebury Coll, Middlebury, VT 05753 USA	Bundeswehr University Munich; Helmholtz Association; German Aerospace Centre (DLR)	Kuhn, A (corresponding author), Bundeswehr Univ Munich, Neubiberg, Germany.; Kuhn, A (corresponding author), German Aerosp Ctr, Munich, Germany.	andreas.kuhn@unibw.de; heiko.hirschmueller@roboception.de; schar@middlebury.edu; helmut.mayer@unibw.de		Mayer, Helmut/0000-0002-9439-2695				Bailer C., 2012, ECCV; Bao Sid Yingze, 2013, CVPR; Bodenmuller T., 2009, THESIS; Curless B., 1996, SIGGRAPH; Frahm J.-M., 2010, ECCV; Fuhrmann S., 2014, SIGGRAPH; Fuhrmann S., 2011, SIGGRAPH ASIA; Furukawa R, 2007, ACCV; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; GOESELE M, 2006, CVPR; Goesele Michael, 2007, ICCV; Hane Christian, 2013, CVPR; Hernandez C., 2007, CVPR; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hirschmuller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221; Vu HH, 2012, IEEE T PATTERN ANAL, V34, P889, DOI 10.1109/TPAMI.2011.172; Hu X., 2012, 3DIMPVT; Kazhdan M., 2006, EUROGRAPHICS; Kazhdan M., 2007, EUROGRAPHICS; Kolev K, 2009, INT J COMPUT VISION, V84, P80, DOI 10.1007/s11263-009-0233-1; Kuhn A., 2014, 3DV; Kuhn A., 2015, ICCV WORKSH ICCVW; Kuhn A., 2013, GCPR; Kuhn A., 2014, THESIS; Mayer H, 2011, 15 INT WORKSH THEOR; Merrell P., 2007, CVPR; Molton N, 2000, INT J COMPUT VISION, V39, P5, DOI 10.1023/A:1008191416557; Mucke Patrick, 2011, VMV; Newcombe R. a, 2011, ISMAR; Ochs Peter, 2013, CVPR; Pathak K., 2007, IROS; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Sagawa R, 2005, IEEE T PATTERN ANAL, V27, P392, DOI 10.1109/TPAMI.2005.46; Scharstein D., 2007, CVPR; Scharstein D., 2014, GCPR; Schroers C., 2012, DAGM; Seitz Steven M, 2006, CVPR; Sinha Sudipta N., 2014, CVPR; Steinbrucker F., 2013, ICCV; Strecha C., 2008, CVPR; Thrun S, 2003, AUTON ROBOT, V15, P111, DOI 10.1023/A:1025584807625; Vogiatzis G, 2011, IMAGE VISION COMPUT, V29, P434, DOI 10.1016/j.imavis.2011.01.006; Wei Jian, 2014, BMVC; Wheeler M., 1998, ICCV; Woodford O., 2012, ECCV; WU C, 2011, CVPR; Wu C., 2013, 3DV; Xiong Yalin, 1997, CVPR; Zach C., 2008, 3DPVT; Zach C., 2007, ICCV	50	32	34	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2017	124	1					2	17		10.1007/s11263-016-0946-x	http://dx.doi.org/10.1007/s11263-016-0946-x			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FA2JD					2022-12-18	WOS:000405265900001
J	Verma, Y; Jawahar, CV				Verma, Yashaswi; Jawahar, C. V.			Image Annotation by Propagating Labels from Semantic Neighbourhoods	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image annotation; Nearest neighbour; Metric learning; Cross-media analysis	RELEVANCE; MODEL; GRAPH	Automatic image annotation aims at predicting a set of semantic labels for an image. Because of large annotation vocabulary, there exist large variations in the number of images corresponding to different labels ("class-imbalance"). Additionally, due to the limitations of human annotation, several images are not annotated with all the relevant labels ("incomplete-labelling"). These two issues affect the performance of most of the existing image annotation models. In this work, we propose 2-pass k-nearest neighbour (2PKNN) algorithm. It is a two-step variant of the classical k-nearest neighbour algorithm, that tries to address these issues in the image annotation task. The first step of 2PKNN uses "image-to-label" similarities, while the second step uses "image-to-image" similarities, thus combining the benefits of both. We also propose a metric learning framework over 2PKNN. This is done in a large margin set-up by generalizing a well-known (single-label) classification metric learning algorithm for multi-label data. In addition to the features provided by Guillaumin et al. (2009) that are used by almost all the recent image annotation methods, we benchmark using new features that include features extracted from a generic convolutional neural network model and those computed using modern encoding techniques. We also learn linear and kernelized cross-modal embeddings over different feature combinations to reduce semantic gap between visual features and textual labels. Extensive evaluations on four image annotation datasets (Corel-5K, ESP-Game, IAPR-TC12 and MIRFlickr-25K) demonstrate that our method achieves promising results, and establishes a new state-of-the-art on the prevailing image annotation datasets.	[Verma, Yashaswi; Jawahar, C. V.] IIIT, Ctr Visual Informat Technol, Hyderabad, Andhra Pradesh, India	International Institute of Information Technology Hyderabad	Verma, Y (corresponding author), IIIT, Ctr Visual Informat Technol, Hyderabad, Andhra Pradesh, India.	yashaswi.verma@research.iiit.ac.in; jawahar@iiit.ac.in	Verma, Yashaswi/GXF-3950-2022	Verma, Yashaswi/0000-0003-2317-2641; Jawahar, C. V./0000-0001-6767-7057	Microsoft Research India PhD fellowship	Microsoft Research India PhD fellowship(Microsoft)	We thank Prof. Raghavan Manmatha for sharing the Corel-5K dataset, and the anonymous reviewers for their helpful comments. Yashaswi Verma is partially supported by Microsoft Research India PhD fellowship 2013.	Anderson C., 2006, LONG TAIL WHY FUTURE; Ballan L., 2014, P ICMR; Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61; Chen  M., 2013, P ICML; Donahue J., 2014, P ICML; Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97; Feng SL, 2004, PROC CVPR IEEE, P1002; Fu H, 2012, LECT NOTES COMPUT SC, V7577, P86, DOI 10.1007/978-3-642-33783-3_7; Grubinger M., 2007, THESIS; Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266; Gupta A, 2012, P AAAI; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Huiskes M. J., 2008, MIR; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Jeon J., 2003, P ACM SIGIR C RES DE, P119; Jin R, 2009, PROC CVPR IEEE, P896, DOI 10.1109/CVPRW.2009.5206684; Kalayeh M. M., 2014, P CVPR; Lavrenko V., 2003, NIPS; Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598; Liu J, 2009, PATTERN RECOGN, V42, P218, DOI 10.1016/j.patcog.2008.04.012; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24; Makadia A, 2010, INT J COMPUT VISION, V90, P88, DOI 10.1007/s11263-010-0338-6; Metzler D, 2004, LECT NOTES COMPUT SC, V3115, P42; Moran S, 2014, INT J MULTIMED INF R, V3, P209, DOI 10.1007/s13735-014-0063-y; Moran S, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.1; Mori Y., 1999, MISRM 99 1 INT WORKS; Murthy V. N., 2014, P ICMR; Nakayama H., 2011, THESIS; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; van de Weijer J, 2006, LECT NOTES COMPUT SC, V3952, P334; Verbeek J., 2010, MIR; Verma Y., 2013, P BMVC; Verma Y, 2012, LECT NOTES COMPUT SC, V7574, P836, DOI 10.1007/978-3-642-33712-3_60; Von Ahn Luis, 2004, P SIGCHI C HUM FACT, P319, DOI DOI 10.1145/985692.985733; Wang C., 2009, P CVPR; Wang H, 2011, PROC CVPR IEEE, P793, DOI 10.1109/CVPR.2011.5995379; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Xiang Y, 2009, PROC CVPR IEEE, P1153, DOI 10.1109/CVPRW.2009.5206518; Yavlinsky A, 2005, LECT NOTES COMPUT SC, V3568, P507; Zhang ST, 2010, PROC CVPR IEEE, P3312, DOI 10.1109/CVPR.2010.5540036	44	32	35	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	1					126	148		10.1007/s11263-016-0927-0	http://dx.doi.org/10.1007/s11263-016-0927-0			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EI3HN					2022-12-18	WOS:000392380900006
J	Funes-Mora, KA; Odobez, JM				Funes-Mora, Kenneth A.; Odobez, Jean-Marc			Gaze Estimation in the 3D Space Using RGB-D Sensors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Gaze estimation; Appearance based methods; RGB-D cameras; Head-pose invariance; Person invariance		We address the problem of 3D gaze estimation within a 3D environment from remote sensors, which is highly valuable for applications in human-human and human-robot interactions. To the contrary of most previous works, which are limited to screen gazing applications, we propose to leverage the depth data of RGB-D cameras to perform an accurate head pose tracking, acquire head pose invariance through a 3D rectification process that renders head pose dependent eye images into a canonical viewpoint, and computes the line-of-sight in the 3D space. To address the low resolution issue of the eye image resulting from the use of remote sensors, we rely on the appearance based gaze estimation paradigm, which has demonstrated robustness against this factor. In this context, we do a comparative study of recent appearance based strategies within our framework, study the generalization of these methods to unseen individual, and propose a cross-user eye image alignment technique relying on the direct registration of gaze-synchronized eye images. We demonstrate the validity of our approach through extensive gaze estimation experiments on a public dataset as well as a gaze coding task applied to natural job interviews.	[Funes-Mora, Kenneth A.; Odobez, Jean-Marc] Idiap Res Inst, Martigny, Switzerland; [Funes-Mora, Kenneth A.; Odobez, Jean-Marc] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Funes-Mora, KA (corresponding author), Idiap Res Inst, Martigny, Switzerland.; Funes-Mora, KA (corresponding author), Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.	kenneth.funes@idiap.ch; jean-marc.odobez@idiap.ch						Amberg B, 2007, IEEE I CONF COMP VIS, P1326; Amberg B, 2008, IEEE INT CONF AUTOMA, P667; Baltrusaitis T., 2012, COMPUTER VISION PATT; Baluja S, 1994, NONINTRUSIVE GAZE TR; Barron J. T., 2013, UCBEECS2013117; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Choi DH, 2007, IEEE INT SYMP CIRC S, P3948, DOI 10.1109/ISCAS.2007.378664; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dantone M, 2012, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2012.6247976; Egger B., 2014, GERM C PATT REC; Fanelli G., 2011, S GERM ASS PATT REC; Funes Mora K. A., 2013, INT C MULT INT; Funes Mora K.A., 2014, P S EYE TRACK RES AP, P255; Guestrin ED, 2006, IEEE T BIO-MED ENG, V53, P1124, DOI 10.1109/TBME.2005.863952; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30; Herrera CD, 2012, IEEE T PATTERN ANAL, V34, P2058, DOI 10.1109/TPAMI.2012.125; Kanade T., 2004, P 11 WORLD C INT TRA, P1; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Li Dongheng, 2005, COMP VIS PATT REC 20, V3, P79; Li JF, 2014, IEEE COMPUT SOC CONF, P606, DOI 10.1109/CVPRW.2014.93; Low K.L.., 2004, LINEAR LEAST SQUARES, P1; Lu F., 2011, INT C COMP VIS ICCV; Lu F, 2014, IEEE T PATTERN ANAL, V36, P2033, DOI 10.1109/TPAMI.2014.2313123; Lu F, 2014, IMAGE VISION COMPUT, V32, P169, DOI 10.1016/j.imavis.2014.01.005; Martinez F, 2012, IEEE IMAGE PROC, P1961, DOI 10.1109/ICIP.2012.6467271; Mora K. A. F., 2012, P IEEE COMP SOC C CO, P25, DOI DOI 10.1109/CVPRW.2012.6239182; Mora KAF, 2013, IEEE IMAGE PROC, P2787, DOI 10.1109/ICIP.2013.6738574; Moriyama T, 2004, IEEE SYS MAN CYBERN, P629; Murphy-Chutorian E., 2008, IEEE T PATTERN ANAL; Nguyen L. S., 2014, IEEE T MULTIMEDIA; Noris B, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P611; Noris B, 2011, COMPUT VIS IMAGE UND, V115, P476, DOI 10.1016/j.cviu.2010.11.013; Oertel C., 2014, INT C MULT INT UND M; Paysan P., 2009, P ADV VID SIGN BAS S; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Schneider T., 2014, INT C PATT REC ICPR; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88; Sugano Y., 2014, COMPUTER VISION PATT; Sugano Y, 2008, LECT NOTES COMPUT SC, V5304, P656, DOI 10.1007/978-3-540-88690-7_49; Tan KH, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P191, DOI 10.1109/ACV.2002.1182180; Timm F, 2011, VISAPP 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, P125; Valenti R, 2012, IEEE T PATTERN ANAL, V34, P1785, DOI 10.1109/TPAMI.2011.251; Viola Paul, 2001, IJCV; Weise T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964972; Williams O., 2006, P IEEE C CVPR, V1, P230, DOI DOI 10.1109/CVPR.2006.285; Xiong XH, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1113, DOI 10.1145/2638728.2641694; Yamazoe H, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P245, DOI 10.1145/1344471.1344527; YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169	50	32	35	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2016	118	2			SI		194	216		10.1007/s11263-015-0863-4	http://dx.doi.org/10.1007/s11263-015-0863-4			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DO0OE					2022-12-18	WOS:000377477400006
J	Fouhey, DF; Delaitre, V; Gupta, A; Efros, AA; Laptev, I; Sivic, J				Fouhey, David F.; Delaitre, Vincent; Gupta, Abhinav; Efros, Alexei A.; Laptev, Ivan; Sivic, Josef			People Watching: Human Actions as a Cue for Single View Geometry	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene understanding; Action recognition; 3D reconstruction		We present an approach which exploits the coupling between human actions and scene geometry to use human pose as a cue for single-view 3D scene understanding. Our method builds upon recent advances in still-image pose estimation to extract functional and geometric constraints on the scene. These constraints are then used to improve single-view 3D scene understanding approaches. The proposed method is validated on monocular time-lapse sequences from YouTube and still images of indoor scenes gathered from the Internet. We demonstrate that observing people performing different actions can significantly improve estimates of 3D scene geometry.	[Fouhey, David F.; Gupta, Abhinav; Efros, Alexei A.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Delaitre, Vincent; Laptev, Ivan; Sivic, Josef] Ecole Normale Super, INRIA, CNRS, WILLOW Project,Dept Informat,UMR 8548, F-75013 Paris, France	Carnegie Mellon University; Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Fouhey, DF (corresponding author), Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	dfouhey@cs.cmu.edu		Efros, Alexei A./0000-0001-5720-8070	NSF [IIS-1320083]; NDSEG; ONR-MURI [N00014 1010934]; MSR-INRIA laboratory; EIT-ICT labs; Google; ERC Activia; Quaero Programme - OSEO	NSF(National Science Foundation (NSF)); NDSEG; ONR-MURI(MURIOffice of Naval Research); MSR-INRIA laboratory; EIT-ICT labs; Google(Google Incorporated); ERC Activia; Quaero Programme - OSEO	This work was supported by NSF Graduate Research and NDSEG Fellowships to DF, and by ONR-MURI N00014 1010934, NSF IIS-1320083, the MSR-INRIA laboratory, the EIT-ICT labs, Google, ERC Activia, and the Quaero Programme, funded by OSEO.	Andriluka M., 2010, CVPR; Andriluka M., 2009, CVPR; [Anonymous], 2011, CVPR; [Anonymous], 2011, CVPR; Barinova O., 2010, ECCV; Bourdev L., 2009, ICCV; Choi Wongun, 2013, CVPR; Coughlan J., 2000, NIPS; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Del Pero L, 2011, CVPR; Del Pero L, 2012, CVPR; Delaitre V., 2011, NIPS; Delaitre V., 2012, ECCV; Desai C., 2010, SMICV CVPR; EFRON B, 1987, J AM STAT ASSOC, V82, P171, DOI 10.2307/2289144; Felzenszwalb P., 2008, CVPR; Flint A., 2011, ICCV; Fouhey D. F., 2012, ECCV; Fouhey D. F., 2013, ICCV; Gall Juergen, 2011, CVPR; Gibson J., 1979, ECOLOGICAL APPROACH; Grabner H., 2011, CVPR; Guan L., 2007, CVPR; Gupta A., 2008, CVPR; Gupta A., 2007, CVPR; Gupta A., 2010, ECCV; Gupta A., 2011, CVPR; Hartley R., 2004, ROBOTICA; Hedau V., 2009, ICCV; Hedau Varsha, 2010, ECCV; Hoiem D., 2008, IJCV; Hoiem Derek, 2005, CVPR; Jiang Y., 2013, CVPR; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; Karsch K., 2012, ECCV; Kjellstrom H., 2008, ECCV; Krahnstoever N., 2005, CVPR; Lee D., 2009, ICCV; Lee D.C., 2010, ADV NEURAL INFORM PR; Park D., 2011, ICCV; Payet N., 2011, CVPR; Prest A., 2011, PAMI; Ramakrishna V., 2013, CVPR; Rother C., 2002, IVC; Rother D., 2007, CVPR; Saxena A., 2008, TPAMI; Schodl A., 2001, CVPR; Schwing A., 2012, ECCV; Schwing A.G., 2013, P ICCV; Taylor C.J., 2000, CVPR; Turek MW, 2010, ECCV; Wang H., 2010, ECCV; Xiao J., 2012, P ADV NEURAL INFORM, V25; Yao B., 2011, P ICML; Yu S. X., 2008, 6 WORKSH PERC ORG CO	55	32	33	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2014	110	3			SI		259	274		10.1007/s11263-014-0710-z	http://dx.doi.org/10.1007/s11263-014-0710-z			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AT2HL		Green Submitted			2022-12-18	WOS:000344754500003
J	Asteriadis, S; Karpouzis, K; Kollias, S				Asteriadis, Stylianos; Karpouzis, Kostas; Kollias, Stefanos			Visual Focus of Attention in Non-calibrated Environments using Gaze Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Head pose estimation; User attention estimation; Facial feature tracking; Facial feature detection; Face tracking; Eye Gaze estimation; Human computer interaction	HEAD POSE; FACE DETECTION; TRACKING; EYE	Estimating the focus of attention of a person highly depends on her/his gaze directionality. Here, we propose a new method for estimating visual focus of attention using head rotation, as well as fuzzy fusion of head rotation and eye gaze estimates, in a fully automatic manner, without the need for any special hardware or a priori knowledge regarding the user, the environment or the setup. Instead, we propose a system aimed at functioning under unpretending conditions, only with the usage of simple hardware, like a normal web-camera. Our system is aimed at functioning in a human-computer interaction environment, considering a person is facing a monitor with a camera adjusted on top. To this aim, we propose in this paper two novel techniques, based on local and appearance information, estimating head rotation, and we adaptively fuse them in a common framework. The system is able to recognize head rotational movement, under translational movements of the user towards any direction, without any knowledge or a-priori estimate of the user's distance from the camera or camera intrinsic parameters.	[Asteriadis, Stylianos; Karpouzis, Kostas; Kollias, Stefanos] Natl Tech Univ Athens, Image Video & Multimedia Syst Lab, Athens 15780, Greece	National Technical University of Athens	Asteriadis, S (corresponding author), Natl Tech Univ Athens, Image Video & Multimedia Syst Lab, 9 Iroon Polytechniou Str, Athens 15780, Greece.	stiast@image.ntua.gr; kkarpou@image.ntua.gr; stefanos@cs.ntua.gr	Kollias, Stefanos/ACY-7285-2022; Karpouzis, Kostas/AAQ-8018-2020; Asteriadis, Stylianos/O-2140-2016	Karpouzis, Kostas/0000-0002-4615-6751; Asteriadis, Stylianos/0000-0002-4298-6870; Kollias, Stefanos/0000-0003-2899-0598	FP7 ICT European project SIREN [258453]	FP7 ICT European project SIREN	This research was supported by the FP7 ICT European project SIREN (project no: 258453). We would also like to thank all participants of the HPEG dataset for their helpful participation.	Aggarwal G, 2005, LECT NOTES COMPUT SC, V3776, P515; Ahlberg J, 2002, EURASIP J APPL SIG P, V2002, P566, DOI 10.1155/S1110865702203078; Asteriadis S., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P414, DOI 10.1109/ICCVW.2011.6130271; Asteriadis S., 2009, P INT WORKSH AFF AW; Asteriadis S, 2007, VISAPP 2007: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOLUME IU/MTSV, P247; Asteriadis S, 2009, MULTIMED TOOLS APPL, V41, P469, DOI 10.1007/s11042-008-0240-1; Asteriadis S, 2009, PATTERN RECOGN, V42, P1388, DOI 10.1016/j.patcog.2009.01.009; Ba SO, 2011, IEEE T PATTERN ANAL, V33, P101, DOI 10.1109/TPAMI.2010.69; Begley S, 2008, LECT NOTES COMPUT SC, V5359, P692, DOI 10.1007/978-3-540-89646-3_68; CASCIA ML, 2000, IEEE T PATTERN ANAL, V22, P322; Chiu S.L., 1994, J INTELL FUZZY SYST, V2, P267, DOI DOI 10.3233/IFS-1994-2306; Cootes T. F., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P227, DOI 10.1109/AFGR.2000.840639; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; De La TORRE F., 2008, P 8 IEEE INT C AUT F, P1, DOI DOI 10.1109/AFGR.2008.4813372; Dornaika F, 2008, INT J COMPUT VISION, V76, P257, DOI 10.1007/s11263-007-0059-7; Fathi A, 2007, IEEE I CONF COMP VIS, P1917; GEE A, 1994, IMAGE VISION COMPUT, V12, P639, DOI 10.1016/0262-8856(94)90039-6; Gourier N., 2004, INT WORKSH VIS OBS D; Haralick RM., 1992, COMPUTER ROBOT VISIO; Horprasert T, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P242, DOI 10.1109/AFGR.1996.557271; Horvitz E. J., 1988, International Journal of Approximate Reasoning, V2, P247, DOI 10.1016/0888-613X(88)90120-X; JANG JSR, 1993, IEEE T SYST MAN CYB, V23, P665, DOI 10.1109/21.256541; Jensen F.V., 1996, INTRO BAYESIAN NETWO; Jesorsky O, 2001, LECT NOTES COMPUT SC, V2091, P90; Ji Q, 2002, REAL-TIME IMAGING, V8, P357, DOI 10.1006/rtim.2002.0279; Kourkoutis LG, 2007, PROC INT C TOOLS ART, P536, DOI 10.1109/ICTAI.2007.114; Kovac J., 2003, IEEE INT C COMP TOOL, V2; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lecun Y., 1989, GEN NETWORK DESIGN S; LeCun Y., 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130; LeCun Yann, 1998, EFFICIENT BACKPROP; Lefevre S, 2009, IEEE INT CON MULTI, P298, DOI 10.1109/ICME.2009.5202494; LIU F, 2003, P IEEE INT WORKSH AN; Ma BP, 2008, IEEE T SYST MAN CY B, V38, P1501, DOI 10.1109/TSMCB.2008.928231; MAGEE JJ, 2008, IEEE T SYST MAN CYB, V38, P1; Messer K, 2003, LECT NOTES COMPUT SC, V2688, P964; Morency LP, 2010, IMAGE VISION COMPUT, V28, P754, DOI 10.1016/j.imavis.2009.08.004; Morency LP, 2003, PROC CVPR IEEE, P803; Murphy-Chutorian E., 2007, INT TRANSP SYST C 20, P709, DOI DOI 10.1109/ITSC.2007.4357803; Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106; Osadchy M, 2007, J MACH LEARN RES, V8, P1197; Peters C, 2010, J MULTIMODAL USER IN, V3, P119, DOI 10.1007/s12193-009-0029-1; Sarle W. S., 1995, Computing Science and Statistics. Vol.27. Proceedings of the 27th Symposium on the Interface. Statistics and Manufacturing with Subthemes in Environmental Statistics, Graphics and Imaging, P352; Shaker N, 2011, LECT NOTES COMPUT SC, V6975, P547, DOI 10.1007/978-3-642-24571-8_68; Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154; Stiefelhagen R., 2004, POINT 04 WORKSH ICPR; Sung J, 2008, INT J COMPUT VISION, V80, P260, DOI 10.1007/s11263-007-0125-1; TAKAGI T, 1985, IEEE T SYST MAN CYB, V15, P116, DOI 10.1109/TSMC.1985.6313399; Tan KH, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P191, DOI 10.1109/ACV.2002.1182180; TOYAMA K, 2000, P 4 AS C COMP VIS AC; Valenti R, 2012, IEEE T IMAGE PROCESS, V21, P802, DOI 10.1109/TIP.2011.2162740; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Voit M., 2010, P ICMI MLMI; Wang JG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P136; Weidenbacher U, 2006, LECT NOTES ARTIF INT, V4021, P9; Xiao J, 2003, INT J IMAG SYST TECH, V13, P85, DOI 10.1002/ima.10048; [No title captured]	57	32	34	0	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2014	107	3					293	316		10.1007/s11263-013-0691-3	http://dx.doi.org/10.1007/s11263-013-0691-3			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AD6IU					2022-12-18	WOS:000333362600005
J	Nieuwenhuis, C; Toppe, E; Cremers, D				Nieuwenhuis, Claudia; Toeppe, Eno; Cremers, Daniel			A Survey and Comparison of Discrete and Continuous Multi-label Optimization Approaches for the Potts Model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-label; Survey; Comparison; Optimization; Markov random fields; Partial differential equations	ENERGY MINIMIZATION; ALGORITHMS; SEGMENTATION	We present a survey and a comparison of a variety of algorithms that have been proposed over the years to minimize multi-label optimization problems based on the Potts model. Discrete approaches based on Markov Random Fields as well as continuous optimization approaches based on partial differential equations can be applied to the task. In contrast to the case of binary labeling, the multi-label problem is known to be NP hard and thus one can only expect near-optimal solutions. In this paper, we carry out a theoretical comparison and an experimental analysis of existing approaches with respect to accuracy, optimality and runtime, aimed at bringing out the advantages and short-comings of the respective algorithms. Systematic quantitative comparison is done on the Graz interactive image segmentation benchmark. This paper thereby generalizes a previous experimental comparison (Klodt et al. 2008) from the binary to the multi-label case.	[Nieuwenhuis, Claudia; Toeppe, Eno; Cremers, Daniel] Tech Univ Munich, Dept Comp Sci, Munich, Germany; [Nieuwenhuis, Claudia; Toeppe, Eno; Cremers, Daniel] Tech Univ Munich, Dept Math, Munich, Germany	Technical University of Munich; Technical University of Munich	Nieuwenhuis, C (corresponding author), Tech Univ Munich, Dept Comp Sci, Munich, Germany.	claudia.nieuwenhuis@in.tum.de						Alahari K, 2010, IEEE T PATTERN ANAL, V32, P1846, DOI 10.1109/TPAMI.2009.194; Batra D., 2011, INT C COMP VIS PATT; BESAG J, 1986, J R STAT SOC B, V48, P259; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chambolle A., 2008, TR200805 U BONN; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286; Cremers D, 2006, INT J COMPUT VISION, V66, P67, DOI 10.1007/s11263-005-3676-z; Cremers D, 2004, LECT NOTES COMPUT SC, V2034, P74; Cremers D, 2007, INT J COMPUT VISION, V72, P195, DOI 10.1007/s11263-006-8711-1; Felzenszwalb P., 2010, INT C COMP VIS PATT; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GOLDSCHLAGER LM, 1982, THEOR COMPUT SCI, V21, P105, DOI 10.1016/0304-3975(82)90092-5; GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Kleinberg J, 2002, J ACM, V49, P616, DOI 10.1145/585265.585268; Klodt M., 2008, EUR C COMP VIS MARS; Kolev K., 2010, EUR C COMP VIS CRET; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Komodakis N, 2005, IEEE I CONF COMP VIS, P1018; Komodakis N., 2007, INT C COMP VIS PATT; Lellmann J, 2009, IEEE I CONF COMP VIS, P646, DOI 10.1109/ICCV.2009.5459176; Lellmann J., 2011, INT C EN MIN METH CO; Lellmann J, 2009, LECT NOTES COMPUT SC, V5567, P150, DOI 10.1007/978-3-642-02256-2_13; Lempitsky V, 2007, IEEE I CONF COMP VIS, P620; Liu X., 2010, IEEE T PATTERN ANAL, V32, P1317; Michelot C., 1986, J OPTIMIZATION THEOR, V50, P189; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Nieuwenhuis C., 2012, IEEE T PATTERN ANAL; Nieuwenhuis C., 2011, INT C EN MIN METH CO; Osokin A, 2011, PROC CVPR IEEE; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pock T, 2009, IEEE INT C COMP VIS; Pock T, 2011, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2011.6126441; Pock T, 2010, SIAM J IMAGING SCI, V3, P1122, DOI 10.1137/090757617; Santner J., 2010, THESIS U GRAZ GRAZ; SCHLESINGER M, 1976, KIBERNETIKA, V4, P113; Strekalovskiy E., 2011, IEEE INT C COMP VIS; Szeliski R, 2006, LECT NOTES COMPUT SC, V3952, P16; Tsai A, 2001, PROC CVPR IEEE, P463; Veksler O, 2009, INT C EN MIN METH CO; Veksler O., 2007, INT C COMP VIS PATT; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Zach C., 2008, P VIS MOD VIS WORKSH; Zach C., 2012, INT C COMP VIS PATT; Zach C., 2009, INT C COMP VIS PATT	48	32	33	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2013	104	3			SI		223	240		10.1007/s11263-013-0619-y	http://dx.doi.org/10.1007/s11263-013-0619-y			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	190VT		Green Submitted			2022-12-18	WOS:000322371100002
J	Tao, MW; Johnson, MK; Paris, S				Tao, Michael W.; Johnson, Micah K.; Paris, Sylvain			Error-Tolerant Image Compositing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Gradient-domain compositing; Visual masking		Gradient-domain compositing is an essential tool in computer vision and its applications, e.g., seamless cloning, panorama stitching, shadow removal, scene completion and reshuffling. While easy to implement, these gradient-domain techniques often generate bleeding artifacts where the composited image regions do not match. One option is to modify the region boundary to minimize such mismatches. However, this option may not always be sufficient or applicable, e.g., the user or algorithm may not allow the selection to be altered. We propose a new approach to gradient-domain compositing that is robust to inaccuracies and prevents color bleeding without changing the boundary location. Our approach improves standard gradient-domain compositing in two ways. First, we define the boundary gradients such that the produced gradient field is nearly integrable. Second, we control the integration process to concentrate residuals where they are less conspicuous. We show that our approach can be formulated as a standard least-squares problem that can be solved with a sparse linear system akin to the classical Poisson equation. We demonstrate results on a variety of scenes. The visual quality and run-time complexity compares favorably to other approaches.	[Tao, Michael W.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Johnson, Micah K.] MIT, Cambridge, MA 02139 USA; [Paris, Sylvain] Adobe Syst Inc, Cambridge, MA USA	University of California System; University of California Berkeley; Massachusetts Institute of Technology (MIT); Adobe Systems Inc.	Tao, MW (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	mtao@berkeley.edu; kimo@csail.mit.edu; sparis@adobe.com			National Science Foundation [0739255, 0924968]	National Science Foundation(National Science Foundation (NSF))	The authors thank Todor Georgiev for the link with the Poisson equation, Kavita Bala and George Drettakis for their discussion about visual masking, Aseem Agarwala and Bill Freeman for their help with the paper, Tim Cho and Biliana Kaneva for helping with the validation, Medhat H. Ibrahim for the image of the Egyption pyramids, Adobe Systems, Inc. for supporting Micah K. Johnson's research, and Ravi Ramamoorthi for supporting Michael Tao's work. This material is based upon work supported by the National Science Foundation under Grant Nos. 0739255 and 0924968.	Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718; Agarwala A, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276495, 10.1145/1239451.1239545]; AGRAWAL A, 2006, P EUR C COMP VIS; Agrawala M., 2005, P ICCV WORKSH TEXT A; Aubert G., 2002, APPL MATH SCI; Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935; Blake A., 2003, ACM T GRAPHICS, V22; Chellappa R, 2009, P C COMP VIS PATT RE; Cho TS, 2010, IEEE T PATTERN ANAL, V32, P1489, DOI 10.1109/TPAMI.2009.133; Curless B., 2009, ACM T GRAPHICS; DRETTAKIS G., 2007, RENDERING TECHNIQUES; Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666; Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373; Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18; Finlayson GD, 2009, INT J COMPUT VISION, V85, P35, DOI 10.1007/s11263-009-0243-z; Freeman W. T, 2008, P IEEE WORKSH INT VI; Georgiev T, 2006, P EUR C COMP VIS; Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239454, 10.1145/1276377.1276381]; Levin Anat, 2006, P EUR C COMP VIS; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]; Ramanarayanan G, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360659; Shum H. Y, 2006, ACM T GRAPHICS, V25; Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185; Vangorp P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276473, 10.1145/1239451.1239528]; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Zisserman A., 2009, P BRIT MACH VIS C	28	32	35	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2013	103	2			SI		178	189		10.1007/s11263-012-0579-7	http://dx.doi.org/10.1007/s11263-012-0579-7			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	150JC		Green Submitted			2022-12-18	WOS:000319385400002
J	Ross, DA; Tarlow, D; Zemel, RS				Ross, David A.; Tarlow, Daniel; Zemel, Richard S.			Learning Articulated Structure and Motion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Graphical models; Non-rigid motion	MULTIBODY FACTORIZATION; SHAPE	Humans demonstrate a remarkable ability to parse complicated motion sequences into their constituent structures and motions. We investigate this problem, attempting to learn the structure of one or more articulated objects, given a time series of two-dimensional feature positions. We model the observed sequence in terms of "stick figure" objects, under the assumption that the relative joint angles between sticks can change over time, but their lengths and connectivities are fixed. The problem is formulated as a single probabilistic model that includes multiple sub-components: associating the features with particular sticks, determining the proper number of sticks, and finding which sticks are physically joined. We test the algorithm on challenging datasets of 2D projections of optical human motion capture and feature trajectories from real videos.	[Ross, David A.; Tarlow, Daniel; Zemel, Richard S.] Univ Toronto, Toronto, ON M5S 3G4, Canada	University of Toronto	Ross, DA (corresponding author), Univ Toronto, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.	dross@cs.toronto.edu; dtarlow@cs.toronto.edu; zemel@cs.toronto.edu			NSF [EIA-0196217]; Natural Science and Engineering Research Council of Canada; Canadian Institute for Advanced Research; Microsoft/LiveLabs-University	NSF(National Science Foundation (NSF)); Natural Science and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canadian Institute for Advanced Research(Canadian Institute for Advanced Research (CIFAR)); Microsoft/LiveLabs-University	The motion capture data used in this project was provided by the Biomotion Lab, Queen's University, Canada, and the CarnegieMellon University Motion Capture Database http://mocap.cs.cmu.edu/ (created with funding from NSF EIA-0196217). We would like to acknowledge funding from the Natural Science and Engineering Research Council of Canada, the Canadian Institute for Advanced Research, and a Microsoft/LiveLabs-University Support Agreement.	ABDELMALEK K, 2004, VSR0402 U IOW COLL E; Bray M, 2006, LECT NOTES COMPUT SC, V3952, P642; Costeira J, 1996, IMAGE UNDERSTANDING WORKSHOP, 1996 PROCEEDINGS, VOLS I AND II, P1013; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; CULVERHOUSE P, 2003, BRIT MACHINE VISION, P639; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Gear CW, 1998, INT J COMPUT VISION, V29, P133, DOI 10.1023/A:1008026310903; GHAHRAMANI Z, 1996, CRGT961 U TOR; Ghahramani Z., 1996, CRGT962 U TOR; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Gruber A, 2004, PROC CVPR IEEE, P707; GRUBER A, 2003, ADV NEURAL INFORM PR; Hartley R., 2003, MULTIPLE VIEW GEOMET; Herda L, 2001, HUM MOVEMENT SCI, V20, P313, DOI 10.1016/S0167-9457(01)00050-1; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Kirk A. G., 2005, P IEEE C COMP VIS PA; Neal R. M., 1998, LEARNING GRAPHICAL M; Ng A.Y., 2002, ADV NEURAL INFORM PR; ROSS DA, 2008, THESIS U TORONTO ONT; ROSS DA, 2008, P 10 EUR C COMP VIS; ROSS DA, 2007, WORKSH DYN VIS ICCV; Ross DA, 2006, J MACH LEARN RES, V7, P2369; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Silaghi MC, 1998, LECT NOTES ARTIF INT, V1537, P26; Sminchisescu C, 2003, INT J ROBOT RES, V22, P371, DOI 10.1177/0278364903022006003; Song Y, 2001, PROC CVPR IEEE, P771; Song Y, 2003, IEEE T PATTERN ANAL, V25, P814, DOI 10.1109/TPAMI.2003.1206511; TAYCHER L, 2002, ADV NEURAL INFORM PR, P1311; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TRESADERN P, 2005, COMP VIS PATT REC 20, V2, P1110; Viklands, 2006, THESIS UMEA U UMEA S; WEISS Y, 1999, P INT C COMP VIS ICC; YAN J, 2005, WORKSH DYN VIS ICCV; YAN J, 2006, P IEEE C COMP VIS PA; Yan J., 2005, P IEEE C COMP VIS PA; YAN J, 2006, P COMP VIS ECCV 20 3; Yan JY, 2008, IEEE T PATTERN ANAL, V30, P865, DOI 10.1109/TPAMI.2007.70739	40	32	33	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN 10	2010	88	2			SI		214	237		10.1007/s11263-010-0325-y	http://dx.doi.org/10.1007/s11263-010-0325-y			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	573YX					2022-12-18	WOS:000275955400005
J	Sand, P; Teller, S				Sand, Peter; Teller, Seth			Particle video: Long-range motion estimation using point trajectories	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	24th Conference on Computer Vision and Pattern Recognition	JUN 17-22, 2006	New York, NY			video motion estimation; optical flow; feature tracking	OPTICAL-FLOW; COMPUTATION; SEGMENTATION	This paper describes a new approach to motion estimation in video. We represent video motion using a set of particles. Each particle is an image point sample with a long-duration trajectory and other properties. To optimize particle trajectories we measure appearance consistency along the particle trajectories and distortion between the particles. The resulting motion representation is useful for a variety of applications and cannot be directly obtained using existing methods such as optical flow or feature tracking. We demonstrate the algorithm on challenging real-world videos that include complex scene geometry, multiple types of occlusion, regions with low texture, and non-rigid deformations.	[Sand, Peter; Teller, Seth] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Sand, P (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	sand@csail.mit.edu	Sand, Peter/AAG-9606-2019					Alvarez L, 2002, LECT NOTES COMPUT SC, V2350, P721; AMIAZ T, 2005, P ICIP 2005 GEN IT S, V3, P1264; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Baker Simon, 2007, ICCV; Barrett R., 1994, TEMPLATES SOLUTION L; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141; BERGEN JR, 1992, EUR C COMP VIS, P237; Black M. J., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P296, DOI 10.1109/CVPR.1991.139705; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; BLACK MJ, 1994, ECCV, P138; Brandt Tobias, 2001, Curr Treat Options Neurol, V3, P463, DOI 10.1007/s11940-001-0034-5; BROX T, 2004, P 8 EUR C COMP VIS, V4, P25; CHIN TM, 1994, IEEE T IMAGE PROCESS, V3, P773, DOI 10.1109/83.336247; Elad M, 1998, J VIS COMMUN IMAGE R, V9, P119, DOI 10.1006/jvci.1998.0382; Fusiello A, 1999, PATTERN ANAL APPL, V2, P312, DOI 10.1007/s100440050039; GOLDMAN DB, 2007, UWCSE20070401; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Haussecker HW, 2001, IEEE T PATTERN ANAL, V23, P661, DOI 10.1109/34.927465; Irani M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P626, DOI 10.1109/ICCV.1999.791283; Lischinski D., 1994, GRAPHICS GEMS, P47; Liu C, 2005, ACM T GRAPHIC, V24, P519, DOI 10.1145/1073204.1073223; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; MURRAY DW, 1987, IEEE T PATTERN ANAL, V9, P220, DOI 10.1109/TPAMI.1987.4767896; Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y; Sand P., 2006, CVPR, P2195; SAND P, 2006, THESIS MIT; Sawhney HS, 2001, COMP GRAPH, P451, DOI 10.1145/383259.383312; Shi JB, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1154, DOI 10.1109/ICCV.1998.710861; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Silva C, 2001, ROBOT AUTON SYST, V35, P153, DOI 10.1016/S0921-8890(01)00119-1; Strecha C, 2004, LECT NOTES COMPUT SC, V3247, P71; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Thompson WB, 1998, INT J COMPUT VISION, V30, P163, DOI 10.1023/A:1008026031844; WEICKERT J, 2004, INT WORKSH COMP VIS, P1; Xiao JJ, 2006, LECT NOTES COMPUT SC, V3951, P211; Zitnick CL, 2005, IEEE I CONF COMP VIS, P1308	37	32	32	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2008	80	1					72	91		10.1007/s11263-008-0136-6	http://dx.doi.org/10.1007/s11263-008-0136-6			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	344CB		Green Submitted			2022-12-18	WOS:000258901900006
J	Torsello, A; Robles-Kelly, A; Hancock, ER				Torsello, Andrea; Robles-Kelly, Antonio; Hancock, Edwin R.			Discovering shape classes using tree edit-distance and pairwise clustering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shock trees; shape recognition; pairwise clustering; edit-distance	GRAPHS; ALGORITHM; DYNAMICS	This paper describes work aimed at the unsupervised learning of shape-classes from shock trees. We commence by considering how to compute the edit distance between weighted trees. We show how to transform the tree edit distance problem into a series of maximum weight clique problems, and show how to use relaxation labeling to find an approximate solution. This allows us to compute a set of pairwise distances between graph-structures. We show how the edit distances can be used to compute a matrix of pairwise affinities using x(2) statistics. We present a maximum likelihood method for clustering the graphs by iteratively updating the elements of the affinity matrix. This involves interleaved steps for updating the affinity matrix using an eigendecomposition method and updating the cluster membership indicators. We illustrate the new tree clustering framework on shock-graphs extracted from the silhouettes of 2D shapes.	Univ Ca Foscari Venice, Dipartimento Informat, I-30172 Venice, Italy; Australian Natl Univ, NICTA, Canberra, ACT 0200, Australia; Univ York, Dept Comp Sci, York Y01 5DD, N Yorkshire, England	Universita Ca Foscari Venezia; Australian National University; University of York - UK	Torsello, A (corresponding author), Univ Ca Foscari Venice, Dipartimento Informat, Via Torino 155, I-30172 Venice, Italy.	torsello@dsi.unive.it; erh@cs.york.ac.uk	Robles-Kelly, Antonio/A-2459-2009; Torsello, Andrea/K-6352-2016; Hancock, Edwin R/C-6071-2008; Hancock, Edwin/N-7548-2019	Robles-Kelly, Antonio/0000-0002-2465-5971; Torsello, Andrea/0000-0001-9189-4924; Hancock, Edwin R/0000-0003-4496-2028; Hancock, Edwin/0000-0003-4496-2028				ARCELLI C, 1992, PATTERN RECOGN LETT, V13, P237, DOI 10.1016/0167-8655(92)90074-A; Barrow H. G., 1976, Information Processing Letters, V4, P83, DOI 10.1016/0020-0190(76)90049-1; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; Bomze IM, 2000, IEEE T NEURAL NETWOR, V11, P1228, DOI 10.1109/72.883403; Bouix S, 2000, LECT NOTES COMPUT SC, V1842, P603; Bowyer K. W., 1990, International Journal of Imaging Systems and Technology, V2, P315, DOI 10.1002/ima.1850020407; Bridle J. S., 1990, PROC 2 INT C NEURAL, P211, DOI [10.5555/2969830, DOI 10.5555/2969830]; Bunke H, 1999, PATTERN RECOGN LETT, V20, P1271, DOI 10.1016/S0167-8655(99)00094-X; Bunke H, 2000, PATTERN RECOGN LETT, V21, P163, DOI 10.1016/S0167-8655(99)00143-9; Cox T.F., 1994, MULTIDIMENSIONAL SCA; Cyr CM, 2004, INT J COMPUT VISION, V57, P5, DOI 10.1023/B:VISI.0000013088.59081.4c; DENTON T, 2004, P INT C PATT REC ICP; DENTON T, 2004, IEEE COMP SOC C COMP, V2, P550; DICKMAN S, 1992, ANN ONCOL, V3, P2, DOI 10.1093/oxfordjournals.annonc.a058061; ESHERA MA, 1986, IEEE T PATTERN ANAL, V8, P604, DOI 10.1109/TPAMI.1986.4767835; Gibbons LE, 1997, MATH OPER RES, V22, P754, DOI 10.1287/moor.22.3.754; Giblin P. J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P385, DOI 10.1109/ICCV.1999.791246; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; KLEIN P, 1999, ACM SIAM S DISC E AL; Lozano MA, 2003, LECT NOTES COMPUT SC, V2683, P52; Luo B, 2003, PATTERN RECOGN, V36, P2213, DOI 10.1016/S0031-3203(03)00084-0; MOTZKIN TS, 1965, CANADIAN J MATH, V17, P533, DOI 10.4153/CJM-1965-053-6; OGNIEWICZ R, 1994, ASPECTS VISUAL FORM, P430; Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI; Pavan M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P362; Pavan M, 2003, PROC CVPR IEEE, P145; Pelillo M, 1999, NEURAL COMPUT, V11, P1933, DOI 10.1162/089976699300016034; Pelillo M, 1999, IEEE T PATTERN ANAL, V21, P1105, DOI 10.1109/34.809105; Pelillo M, 1997, J MATH IMAGING VIS, V7, P309, DOI 10.1023/A:1008255111261; Pelillo M, 2006, NEURAL COMPUT, V18, P1215, DOI 10.1162/neco.2006.18.5.1215; PERONA P, 1998, ECCV 98, V1, P655; REYNER SW, 1977, SIAM J COMPUT, V6, P730, DOI 10.1137/0206053; Rizzi S, 1998, PATTERN RECOGN LETT, V19, P1293, DOI 10.1016/S0167-8655(98)00110-X; Sarkar S, 1998, COMPUT VIS IMAGE UND, V71, P110, DOI 10.1006/cviu.1997.0637; Sebastian T, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P755, DOI 10.1109/ICCV.2001.937602; Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924; SEGEN J, 1988, P 5 INT C MACH LEARN, P29; SENGUPTA K, 1995, IEEE T PATTERN ANAL, V17, P321, DOI 10.1109/34.385984; SENGUPTA K, 1998, COMPUTER VISION IMAG, V70; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; SHOKOUFANDEH A, 1999, P IEEE C COMP VIS PA; Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703; Siddiqi K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P828, DOI 10.1109/ICCV.1999.790307; Tirthapura S, 1998, P SOC PHOTO-OPT INS, V3527, P25, DOI 10.1117/12.325825; Torsello A, 2006, IEEE T PATTERN ANAL, V28, P954, DOI 10.1109/TPAMI.2006.125; Torsello A, 2005, IEEE T PATTERN ANAL, V27, P1087, DOI 10.1109/TPAMI.2005.146; Torsello A, 2004, COMPUT VIS IMAGE UND, V95, P1, DOI 10.1016/j.cviu.2004.03.006; Torsello A, 2003, PATTERN RECOGN LETT, V24, P1089, DOI 10.1016/S0167-8655(02)00255-6; TSAI WH, 1979, IEEE T SYST MAN CYB, V9, P757, DOI 10.1109/TSMC.1979.4310127	52	32	32	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2007	72	3					259	285		10.1007/s11263-006-8929-y	http://dx.doi.org/10.1007/s11263-006-8929-y			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	133MW		Green Submitted			2022-12-18	WOS:000244018000003
J	Bresson, X; Vandergheynst, P; Thiran, JP				Bresson, Xavier; Vandergheynst, Pierre; Thiran, Jean-Philippe			Multiscale active contours	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	5th International Conference on Scale-Space and PDE Methods in Computer Vision	APR 07-09, 2005	Hofgeismar, GERMANY	German Pattern Recognit Soc		active contour; scale space; multiscale segmentation; PDE; Polyakov action; Riemannian manifolds; gradient vector flow	SCALE-SPACE; ANISOTROPIC DIFFUSION; ALGORITHMS; VISION; IMAGES; SHAPE	We propose a new multiscale image segmentation model, based on the active contour/snake model and the Polyakov action. The concept of scale, general issue in physics and signal processing, is introduced in the active contour model, which is a well-known image segmentation model that consists of evolving a contour in images toward the boundaries of objects. The Polyakov action, introduced in image processing by Sochen-Kimmel-Malladi in Sochen et al. (1998), provides an efficient mathematical framework to define a multiscale segmentation model because it generalizes the concept of harmonic maps embedded in higher-dimensional Riemannian manifolds such as multiscale images. Our multiscale segmentation model, unlike classical multiscale segmentations which work scale by scale to speed up the segmentation process, uses all scales simultaneously, i.e. the whole scale space, to introduce the geometry of multiscale images in the segmentation process. The extracted multiscale structures will be useful to efficiently improve the robustness and the performance of standard shape analysis techniques such as shape recognition and shape registration. Another advantage of our method is to use not only the Gaussian scale space but also many other multiscale spaces such as the Perona-Malik scale space, the curvature scale space or the Beltrami scale space. Finally, this multiscale segmentation technique is coupled with a multiscale edge detecting function based on the gradient vector flow model, which is able to extract convex and concave object boundaries independent of the initial condition. We apply our multiscale segmentation model on a synthetic image and a medical image.	Ecole Polytech Fed Lausanne, Swiss Fed Inst Technol, Signal Proc Inst, STI,ITS, CH-1015 Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Bresson, X (corresponding author), Ecole Polytech Fed Lausanne, Swiss Fed Inst Technol, Signal Proc Inst, STI,ITS, Stn 11, CH-1015 Lausanne, Switzerland.	Xavier.Bresson@epfl.ch; Pierre.Vandergheynst@epfl.ch; JP.Thiran@epfl.ch		Thiran, Jean-Philippe/0000-0003-2938-9657				ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; ALVAREZ L, 1993, ARCH RATIONAL MECH A, V123, P3; [Anonymous], 1988, EYE BRAIN VISION; [Anonymous], 2007, J INT COMP L, V321; AUBERT G, 2001, MATH PROBLEMS IMAGE, P147; BRESSON X, 2005, P 5 INT C SCAL SPAC, P167; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Eberly D., 1994, COMP IMAG VIS, V1, P371; Eberly D.H., 1994, THESIS U N CAROLINA; FLORACK LMJ, 1993, THESIS U UTRECHT; HUBEL DH, 1979, BRAIN MECH VISION, V241, P45; KICHENASSAMY S, 1996, ARCH RATIONAL MECH, P375; Kimmel R, 2000, INT J COMPUT VISION, V39, P111, DOI 10.1023/A:1008171026419; Kreyszig E., 1991, DIFFERENTIAL GEOMETR; LEROY B, 1996, P 12 INT C AN OPT SY, P58; LINDEBERG T, 1994, SCAPE SPACE THEORY C; MORSE BS, 1994, THESIS U N CAROLINA; Osher S, 2003, GEOMETRIC LEVEL SET METHODS IN IMAGING, VISION AND GRAPHICS, P3, DOI 10.1007/0-387-21810-6_1; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Pizer SM, 1998, COMPUT VIS IMAGE UND, V69, P55, DOI 10.1006/cviu.1997.0563; Romeny B.M., 1994, GEOMETRY DRIVEN DIFF; ROMERY BMT, 1997, FRONT END VISION MUT; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; SAPIRO G, 1993, INT J COMPUT VISION, V11, P25, DOI 10.1007/BF01420591; Schnabel JA, 1999, IMAGE VISION COMPUT, V17, P419, DOI 10.1016/S0262-8856(98)00127-9; Sethian J. A., 1999, LEVEL SET METHODS FA; Sochen N, 1998, IEEE T IMAGE PROCESS, V7, P310, DOI 10.1109/83.661181; SOCHEN N, 1996, LBNL39243; Weickert J, 1999, J MATH IMAGING VIS, V10, P237, DOI 10.1023/A:1008344623873; Witkin A.P., 1983, INT JOINT C ART INT, P1019; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; Zeki S., 1993, VISION BRAIN	36	32	35	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2006	70	3					197	211		10.1007/s11263-006-7462-3	http://dx.doi.org/10.1007/s11263-006-7462-3			15	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	091VL		Green Submitted			2022-12-18	WOS:000241056300002
J	Dong, JY; Chantler, M				Dong, JY; Chantler, M			Capture and synthesis of 3D surface texture	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						surface texture; texture synthesis; psychophysical experiment	IMAGES	We present and compare five approaches for capturing, synthesising and relighting real 3D surface textures. Unlike 2D texture synthesis techniques they allow the captured textures to be relit using illumination conditions that differ from those of the original. We adapted a texture quilting method due to Efros and combined this with five different relighting representations, comprising: a set of three photometric images; surface Gradient and albedo maps; polynomial texture maps; and two eigen based representations using 3 and 6 base images. We used twelve real textures to perform quantitative tests on the relighting methods in isolation. We developed a qualitative test for the assessment of the complete synthesis systems. Ten observers were asked to rank the images obtained from the five methods using five real textures. Statistical tests were applied to the rankings. The six-base-image eigen method produced the best quantitative relighting results and in particular was better able to cope with specular surfaces. However, in the qualitative tests there were no significant performance differences detected between it and the other two top performers. Our conclusion is therefore that the cheaper gradient and three-base-image eigen methods should be used in preference, especially where the surfaces are Lambertian or near Lambertian.	Ocean Univ China, Dept Comp Sci, Qingdao 266071, Shandong Prov, Peoples R China; Heriot Watt Univ, Sch Math & Comp Sci, Edinburgh EH14 4AS, Midlothian, Scotland	Ocean University of China; Heriot Watt University	Dong, JY (corresponding author), Ocean Univ China, Dept Comp Sci, 23 E Hong Kong Rd, Qingdao 266071, Shandong Prov, Peoples R China.	dongjunyu@ouc.edu.cn; m.j.chantler@hw.ac.uk		Chantler, Mike/0000-0002-8381-1751; Dong, Junyu/0000-0001-7012-2087				Ashikhmin M., 2001, P 2001 S INT 3D GRAP, P217, DOI DOI 10.1145/364338.364405; BARJOSEPH Z, 1999, THESIS HEBREW U JURU; Basri R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P383, DOI 10.1109/ICCV.2001.937651; BLINN JF, 1978, THESIS U UTAH; Cook R., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293; Copeland AC, 2001, OPT ENG, V40, P2655, DOI 10.1117/1.1412851; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; DANA KJ, 1999, P IEEE WORKSH INT AP, P46; Daniel W. W., 1990, APPL NONPARAMETRIC S, V2nd; De Benet J. S., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P361, DOI 10.1145/258734.258882; DONG J, 2003, THESIS HERIOTWATT U; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Epstein J. L., 1995, FAMILY SCH CONNECTIO, P108; HEEGER DJ, 1995, P IEEE INT C IM PROC, V3, P648; Hochberg Y., 1987, MULTIPLE COMP PROCED; Liu XG, 2001, COMP GRAPH, P97; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; Malzbender T, 2001, COMP GRAPH, P519, DOI 10.1145/383259.383320; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; Nishino K, 2001, IEEE T PATTERN ANAL, V23, P1257, DOI 10.1109/34.969116; Ramamoorthi R, 2002, IEEE T PATTERN ANAL, V24, P1322, DOI 10.1109/TPAMI.2002.1039204; RUSHMEIER H, 1997, P 8 EUR REND WORKSH, P35; Shashua A, 1992, THESIS MIT; Simoncelli EP, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P62, DOI 10.1109/ICIP.1998.723417; Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634; Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009; WOODHAM RJ, 1981, ARTIF INTELL, V17, P117, DOI 10.1016/0004-3702(81)90022-9; XU YQ, 2001, P 2 INT WORKSH STAT; Zalesny A, 2001, PROC CVPR IEEE, P615; ZALESNY A, 2000, LECT NOTES COMPUTER, V2018, P123; Zhang ZY, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1041, DOI 10.1109/ICCV.1998.710845; Zhu SC, 2000, IEEE T PATTERN ANAL, V22, P554, DOI 10.1109/34.862195	33	32	35	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-MAY	2005	62	1-2					177	194						18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	866XV					2022-12-18	WOS:000224807600011
J	Kaminski, JY; Shashua, A				Kaminski, JY; Shashua, A			Multiple view geometry of general algebraic curves	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; algebraic curves; epipolar geometry; Kruppa's equations; multiple-view geometry; 3D reconstruction		We introduce a number of new results in the context of multi-view geometry from general algebraic curves. We start with the recovery of camera geometry from matching Curves. We first show how one can compute, without any knowledge on the camera, the homography induced by a single planar curve. Then we continue with the derivation of the extended Kruppa's equations which are responsible for describing the epipolar constraint of two projections of a general algebraic curve. As part of the derivation of those constraints we address the issue of dimension analysis and as a result establish the minimal number of algebraic curves required for a solution of the epipolar geometry as a function of their degree and genus. We then establish new results on the reconstruction of general algebraic curves from multiple views. We address three different representations of curves: (i) the regular point representation in which we show that the reconstruction from two views of a curve of degree d admits two solutions, one of degree d and the other of degree d(d-1). Moreover using this representation, we address the problem of homography recovery for planar curves, (ii) dual space representation (tangents) for which we derive a lower bound for the number of views necessary for reconstruction as a function of the curve degree and genus, and (iii) a new representation (to computer vision) based on the set of lines meeting the curve which does not require any curve fitting in image space, for which we also derive lower bounds for the number of views necessary for reconstruction as a function of curve degree alone.	Bar Ilan Univ, Dept Math & Comp Sci, Ramat Gan, Israel; Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel	Bar Ilan University; Hebrew University of Jerusalem	Kaminski, JY (corresponding author), Bar Ilan Univ, Dept Math & Comp Sci, Ramat Gan, Israel.	kaminsj@math.biu.ac.il						ALLGOWER EL, 1990, COMPUTATIONAL MATH, V13; [Anonymous], 1998, USING ALGEBRAIC GEOM; Astrom K, 1999, IEEE T PATTERN ANAL, V21, P114, DOI 10.1109/34.748821; BARNABEI M, 1985, J ALGEBRA, V96, P120, DOI 10.1016/0021-8693(85)90043-2; BERTHILSON R, 1999, IEEE T PATTERN ANAL, V21; Cox D., 1997, UNDERGRADUATE TEXTS, DOI 10.1007/978-3-662-41154-4; CROSS, 1998, QUADRIOC RECONSTRUCT; Eisenbud D, 1995, GRADUATE TEXTS MATH; Eisenbud David, 2000, GRADUATE TEXTS MATH, V197; Faugeras O., 1997, 3225 INRIA; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FAUGERE JC, 1998, L1P6; FAUGERE JC, NEW EFFICIENT ALGORI; FORSYTH D, RECOGNIZING ALGEBRAI; Fulton W., 1969, ALGEBRAIC CURVES; Greuel G.-M., 2002, SINGULAR INTRO COMMU; HARRIS J., 1992, ALGEBRAIC GEOMETRY 1; HARRIS J, PRINCIPLE ALGEBRAIC; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hartshorne R., 1977, ALGEBRAIC GEOM; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; KAHL F, 1998, P INT C COMP VIS; KAMINSKI JY, 2001, P INT C COMP VIS; KAMINSKI JY, 2001, THESIS HEBREW U JERU; KAMINSKI JY, 2000, P EUR C COMP VIS; LANG S, ALGEBRA; LUONG QT, 1994, P EUR C COMP VIS; MA SD, 1994, P INT C PATT REC; MA SD, 1996, P INT C PATT REC; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; MOURRAIN B, 2002, P 3 INT WORKSH SYMB, P42; PAPADOPOULO T, 1996, P EUR C COMP VIS; PAPADOPOULO T, 1995, 2765 INRIA; Quan L, 1996, IEEE T PATTERN ANAL, V18, P151, DOI 10.1109/34.481540; REYSSAT E, 1989, QUELQUES ASPECETS SU; SCHMID C, 1998, P EUR C COMP VIS; SEGAL D, 2000, P EUR C COMP VIS; Shashua A, 1996, IEEE T PATTERN ANAL, V18, P873, DOI 10.1109/34.537342; Shashua A, 1997, INT J COMPUT VISION, V23, P185, DOI 10.1023/A:1007962930529; Sturmfels B., 2002, CBMS REG C SER MATH, DOI DOI 10.1090/CBMS/097; Walker R.J., 1950, ALGEBRAIC CURVES; [No title captured]	43	32	32	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	2004	56	3					195	219		10.1023/B:VISI.0000011204.89453.4d	http://dx.doi.org/10.1023/B:VISI.0000011204.89453.4d			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	761RT					2022-12-18	WOS:000187927800003
J	Schrater, PR; Kersten, D				Schrater, PR; Kersten, D			How optimal depth cue integration depends on the task	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						task; data fusion; cue integration; optimal estimation; Bayes nets; Bayesian inference; depth estimation; depth from shadows; depth from image size	PERCEPTION; SHADOWS; MOTION	Bayesian parameter estimation can be used to generate statistically optimal solutions to the problem of cue integration. However, the complexity and dimensionality of these solutions is frequently prohibitive. In this paper, we show how the complexity and performance characteristics of the optimal estimator for a task depend strongly on the detailed formulation of the task, including the choice of representation for the scene variables. In particular, some representations lead to simpler inference algorithms than others. We illustrate the problem of cue integration for the perception of depth from two highly disparate cues, cast shadow position and image size, and show how the complexity and performance of the depth estimators depends on the specific representation (choice) of depth parameter. From the analysis we predict human performance on a simple depth discrimination task from the optimal cue integration in each depth representation. We find that the cue-integration strategy used by human subjects can be described as near-optimal using a particular choice of depth representation.	Univ Minnesota, Dept Psychol, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Schrater, PR (corresponding author), Univ Minnesota, Dept Psychol, N218 Elliott Hall,75 E River Rd, Minneapolis, MN 55455 USA.							Blake A., 1996, PERCEPTION BAYESIAN, P287; Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393; Clark J.J., 1990, DATA FUSION SENSORY; CUTTING JE, 1996, PERCEPTION SPACE MOT, P69; Edwards AWF, 1992, LIKELIHOOD; GOODALE MA, 1994, CURR BIOL, V4, P604, DOI 10.1016/S0960-9822(00)00132-9; Jensen F.V., 1996, INTRO BAYESIAN NETWO; Kersten D, 1996, NATURE, V379, P31, DOI 10.1038/379031a0; Kersten D, 1997, PERCEPTION, V26, P171, DOI 10.1068/p260171; KNILL DC, 1991, NATURE, V351, P228, DOI 10.1038/351228a0; LANDY MS, 1995, VISION RES, V35, P389, DOI 10.1016/0042-6989(94)00176-M; LAWSON S, 1998, INVESTIGATIVE OPTHAM, V39, P827; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rao C. R, 1973, LINEAR STAT INFERENC; STEVENS SS, 1957, PSYCHOL REV, V64, P153, DOI 10.1037/h0046162; Tanner M.A., 1996, TOOLS STAT INFERENCE; Yuille A. L., 1996, PERCEPTION BAYESIAN, P123, DOI DOI 10.1017/CBO9780511984037.006	18	32	32	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2000	40	1					73	91						19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	384KV					2022-12-18	WOS:000165942300005
J	Fermuller, C; Aloimonos, Y				Fermuller, C; Aloimonos, Y			Ambiguity in structure from motion: Sphere versus plane	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						error analysis; normal flow; negative depth; spherical and planar imaging surfaces	3-DIMENSIONAL MOTION; VISUAL-MOTION; NORMAL FLOW; PERCEPTION; EGOMOTION; SURFACES; MODELS; IMAGE	If 3D rigid motion can be correctly estimated from image sequences, the structure of the scene can be correctly derived using the equations for image formation. However, an error in the estimation of 3D motion will result in the computation of a distorted version of the scene structure. Of computational interest are these regions in space where the distortions are such that the depths become negative, because in order for the scene to be visible it has to lie in front of the image, and thus the corresponding depth estimates have to be positive. The stability analysis for the structure from motion problem presented in this paper investigates the optimal relationship between the errors in the estimated translational and rotational parameters of a rigid motion that results in the estimation of a minimum number of negative depth values. The input used is the value of the flow along some direction, which is more general than optic flow or correspondence. For a planar retina it is shown that the optimal configuration is achieved when the projections of the translational and rotational errors on the image plane are perpendicular. Furthermore, the projection of the actual and the estimated translation lie on a line through the center. For a spherical retina, given a rotational error, the optimal translation is the correct one; given a translational error, the optimal rotational error depends both in direction and value on the actual and estimated translation as well as the scene in view. The proofs, besides illuminating the confounding of translation and rotation in structure from motion, have an important application to ecological optics. The same analysis provides a computational explanation of why it is easier to estimate self-motion in the case of a spherical retina and why shape can be estimated easily in the case of a planar retina, thus suggesting that nature's design of compound eyes (or panoramic vision) for flying systems and camera-type eyes for primates (and other systems that perform manipulation) is optimal.	Univ Maryland, Inst Adv Comp Studies, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA; Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Fermuller, C (corresponding author), Univ Maryland, Inst Adv Comp Studies, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA.	fer@cfar.umd.edu; yiannis@cfar.umd.edu	Aloimonos, Yiannis/AAI-2969-2020	Aloimonos, Yiannis/0000-0002-8152-4281				ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; ALOIMONOS Y, 1994, INT J COMPUT VISION, V13, P33, DOI 10.1007/BF01420794; ASTROM K, 1996, THESIS LUND I TECHNO; Brodsky T, 1998, INT J COMPUT VISION, V26, P5, DOI 10.1023/A:1007928406666; BRUSS AR, 1983, COMPUT VISION GRAPH, V21, P3, DOI 10.1016/S0734-189X(83)80026-7; CHEONG L, 1998, IN PRESS COMPUTER VI; DANIILIDIS K, 1992, THESIS U KARLSRUHE G; Daniilidis Kostas, 1997, VISUAL NAVIGATION BI; FAUGERAS O, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P951, DOI 10.1109/ICCV.1995.466832; Faugeras O., 1992, 3 DIMENSIONAL COMPUT; FAUGERAS OD, 1992, P 2 EUR C COMP VIS S, P563; Fermuller C, 1997, INT J COMPUT VISION, V21, P223, DOI 10.1023/A:1007951901001; FERMULLER C, 1995, INT J COMPUT VISION, V15, P7, DOI 10.1007/BF01450848; Fermuller C, 1997, BIOL CYBERN, V77, P323, DOI 10.1007/s004220050393; FERMULLER C, 1995, SCIENCE, V270, P1973, DOI 10.1126/science.270.5244.1973; FERMULLER C, 1998, P EUR C COMP VIS FRE; HARTLEY RI, 1994, IEEE T PATTERN ANAL, V16, P1036, DOI 10.1109/34.329005; HOFMANN W, 1953, DTSCH GEODATISCHE C; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; HORN BKP, 1987, INT J COMPUT VISION, V1, P259, DOI 10.1007/BF00127824; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; JEPSON AD, 1990, RBCVTR9036 U TOR; KOENDERINK JJ, 1994, PATTERN RECOGN LETT, V15, P439, DOI 10.1016/0167-8655(94)90134-1; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; KOENDERINK JJ, 1995, IMAGE VISION COMPUT, V13, P321, DOI 10.1016/0262-8856(95)99719-H; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; MAYBANK S, 1993, THEORY RECONSTRUCTIO; MAYBANK SJ, 1986, IMAGE VISION COMPUT, V4, P38, DOI 10.1016/0262-8856(86)90006-5; MAYBANK SJ, 1987, THESIS U LONDON ENGL; NEGAHDARIPOUR S, 1986, THESIS MIT CAMBRIDGE; NELSON RC, 1988, BIOL CYBERN, V58, P261, DOI 10.1007/BF00364131; POGGIO T, 1973, KYBERNETIK, V13, P223, DOI 10.1007/BF00274887; REICHARDT W, 1987, J COMP PHYSIOL A, V161, P533, DOI 10.1007/BF00603660; Reichardt W., 1961, SENSORY COMMUNICATIO, P303; SINCLAIR D, 1994, INT J COMPUT VISION, V13, P57, DOI 10.1007/BF01420795; Slama CC., 1980, MANUAL PHOTOGRAMMETR, V4th edn; SPETSAKIS M, 1994, CVGIP-IMAG UNDERSTAN, V60, P300, DOI 10.1006/ciun.1994.1059; SPETSAKIS M, 1990, P DARPA IM UND WORKS, P271; Spetsakis M. E., 1989, Proceedings. Workshop on Visual Motion (IEEE Cat. No.89CH2716-9), P229, DOI 10.1109/WVM.1989.47114; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; SPETSAKIS ME, 1988, P 2 INT C COMP VIS, P449; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; VANSANTEN JPH, 1984, J OPT SOC AM A, V1, P451, DOI 10.1364/JOSAA.1.000451; WAXMAN AM, 1985, INT J ROBOT RES, V4, P95, DOI 10.1177/027836498500400307	47	32	32	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN-JUL	1998	28	2					137	154		10.1023/A:1008063000586	http://dx.doi.org/10.1023/A:1008063000586			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	103BB					2022-12-18	WOS:000074959800003
J	Weng, J; Ahuja, N; Huang, TS				Weng, J; Ahuja, N; Huang, TS			Learning recognition and segmentation using the Cresceptron	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						visual learning; face recognition; face detection; object recognition; object segmentation; feature selection; feature extraction; shape representation; self-organization; associative memory	3-D OBJECT RECOGNITION; NEURAL NETWORK MODEL; PATTERN-RECOGNITION; MECHANISM; VISION; NEOCOGNITRON	This paper presents a framework called Cresceptron for view-based learning, recognition and segmentation. Specifically, it recognizes and segments image patterns that are similar to those learned, using a stochastic distortion model and view-based interpolation, allowing other view points that are moderately different from those used in learning. The learning phase is interactive. The user trains the system using a collection of training images. For each training image, the user manually draws a polygon outlining the region of interest and types in the label of its class. Then, from the directional edges of each of the segmented regions, the Cresceptron uses a hierarchical self-organization scheme to grow a sparsely connected network automatically, adaptively and incrementally during the learning phase. At each level, the system detects new image structures that need to be learned and assigns a new neural plane for each new feature. The network grows by creating new nodes and connections which memorize the new image structures and their context as they are detected. Thus, the structure of the network is a function of the training exemplars. The Cresceptron incorporates both individual learning and class learning; with the former, each training example is treated as a different individual while with the latter, each example is a sample of a class. In the performance phase, segmentation and recognition are tightly coupled. No foreground extraction is necessary, which is achieved by backtracking the response of the network down the hierarchy to the image parts contributing to recognition. Several stochastic shape distortion models are analyzed to show why multilevel matching such as that in the Cresceptron can deal with more general stochastic distortions that a single-level matching scheme cannot. The system is demonstrated using images from broadcast television and other video segments to learn faces and other objects, and then later to locate and to recognize similar, but possibly distorted, views of the same objects.	UNIV ILLINOIS, BECKMAN INST, URBANA, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Weng, J (corresponding author), MICHIGAN STATE UNIV, DEPT COMP SCI, E LANSING, MI 48824 USA.							Anderson J.R., 1980, COGNITIVE PSYCHOL IT; [Anonymous], 1985, PERCEPTUAL ORG VISUA; ARMAN F, 1991, JUN IEEE WORKSH DIR, P124; Bichsel M., 1991, THESIS SWISS FEDERAL; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; CARBONETTO S, 1982, CURR TOP DEV BIOL, V17, P33; CAREW TJ, 1989, TRENDS NEUROSCI, V12, P389, DOI 10.1016/0166-2236(89)90078-7; Carey S., 1985, CONCEPTUAL CHANGE CH; CHEN CH, 1989, IEEE T SYST MAN CYB, V19, P1535, DOI 10.1109/21.44070; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; COVER TM, METHODOLOGIES PATTER, P111; DESMOND N L, 1988, P265; DREHER B, 1973, J PHYSIOL-LONDON, V234, P95, DOI 10.1113/jphysiol.1973.sp010336; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; Fu K. S., 1968, SEQUENTIAL METHODS P, V240, P241; FUKUSHIMA K, 1983, IEEE T SYST MAN CYB, V13, P826, DOI 10.1109/TSMC.1983.6313076; FUKUSHIMA K, 1975, BIOL CYBERN, V20, P121, DOI 10.1007/BF00342633; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Gool L. V., 1991, P IEEE C COMP VIS PA, P454; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; GUTH L, 1975, EXPT NEUROLOGY, V48; HANSEN C, 1989, IEEE T PATTERN ANAL, V11, P1181, DOI 10.1109/34.42856; HEBB DO, 1949, ORG BEHAVIOR; HIGHLEYMAN WH, 1962, P IRE, V50, P1501, DOI 10.1109/JRPROC.1962.288194; Huang T.S., 1993, P 4 INT C COMP VIS B, P121; HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085; HUBEL DH, 1988, EYE BRAIN VISION, P22; IKEUCHI K, 1988, P IEEE, V76, P1016, DOI 10.1109/5.5972; Jain A. K., 1989, FUNDAMENTALS DIGITAL; Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6; JAIN AK, 1988, IEEE T PATTERN ANAL, V10, P783, DOI 10.1109/34.9102; KANDEL ER, 1982, SCIENCE, V218, P433, DOI 10.1126/science.6289442; KEEHN DG, 1965, IEEE T INFORM THEORY, V11, P126, DOI 10.1109/TIT.1965.1053726; Kohonen T., 1988, SELF ORG ASS MEMORY; KOLERS PA, 1985, J EXP PSYCHOL HUMAN, V11, P726, DOI 10.1037/0096-1523.11.6.726; Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995; LEVYSCHOEN A, 1981, EYE MOVEMENTS COGNIT, P299; Lippmann R. P., 1988, Computer Architecture News, V16, P7, DOI [10.1109/MASSP.1987.1165576, 10.1145/44571.44572]; LOFTSGAARDEN DO, 1965, ANN MATH STAT, V36, P1049, DOI 10.1214/aoms/1177700079; Martinez J.L., 1991, LEARNING MEMORY BIOL; Michalski R. S., 1986, Proceedings AAAI-86: Fifth National Conference on Artificial Intelligence, P1041; NAZIR T A, 1990, Spatial Vision, V5, P81, DOI 10.1163/156856890X00011; Olshen R., 1984, CLASSIFICATION REGRE; PAVLIDIS T, 1992, PATTERN RECOGN LETT, V13, P221, DOI 10.1016/0167-8655(92)90072-8; Pavlidis T., 1977, STRUCTURAL PATTERN R; POGGIO T, 1990, NATURE, V343, P263, DOI 10.1038/343263a0; Pomerleau D.A., 1989, ALVINN AUTONOMOUS LA; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; RAKIC P, 1988, SCIENCE, V241, P170, DOI 10.1126/science.3291116; RAMACHANDRAN VS, 1990, PERCEPTUAL WORLD, P127; Rowley HA, 1995, CMUCS95158; Rumelhart D. E., 1988, PARALLEL DISTRIBUTED; Sacks O, 1993, NEW YORKER, P59; Sato H., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P695, DOI 10.1109/CVPR.1992.223198; SHATZ CJ, 1992, SCI AM, V267, P61, DOI 10.1038/scientificamerican0992-60; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; SUNG K, 1994, 1521 AI; SWETS D, 1995, P INT C IM PROC WASH, P22; THOMPSON P, 1980, PERCEPTION, V9, P483, DOI 10.1068/p090483; TREISMAN AM, 1983, PHYSICAL BIOL PROCES; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; WEISS I, 1993, INT J COMPUT VISION, V10, P207, DOI 10.1007/BF01539536; Weng J., 1992, IJCNN International Joint Conference on Neural Networks (Cat. No.92CH3114-6), P576, DOI 10.1109/IJCNN.1992.287150; WENG J, 1993, P WORLD C NEUR NETW, V4, P149; WENG J, 1996, EARLY VISUAL LEARNIN; WILSON HR, 1979, VISION RES, V19, P19, DOI 10.1016/0042-6989(79)90117-2; WILSON HR, 1977, VISION RES, V17, P1177, DOI 10.1016/0042-6989(77)90152-3; YANG GZ, 1994, PATTERN RECOGN, V27, P53, DOI 10.1016/0031-3203(94)90017-5; Yarbus A. L., 1967, EYE MOVEMENTS VISION, P171; [No title captured]; [No title captured]	72	32	34	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1997	25	2					109	143		10.1023/A:1007967800668	http://dx.doi.org/10.1023/A:1007967800668			35	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YG836					2022-12-18	WOS:A1997YG83600002
J	KUTULAKOS, KN; DYER, CR				KUTULAKOS, KN; DYER, CR			RECOVERING SHAPE BY PURPOSIVE VIEWPOINT ADJUSTMENT	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							INTERPRETING LINE DRAWINGS; GENERALIZED CYLINDERS; ANIMATE VISION; SINGLE VIEW; CONTOURS; SURFACES; OBJECT	We present an approach for recovering surface shape from the occluding contour using an active (i.e., moving) observer. It is based on a relation between the geometries of a surface in a scene and its occluding contour: If the viewing direction of the observer is along a principal direction for a surface point whose projection is on the contour, surface shape (i.e., curvature) at the surface point can be recovered from the contour. Unlike previous approaches for recovering shape from the occluding contour, we use an observer that purposefully changes viewpoint in order to achieve a well-defined geometric relationship with respect to a 3-D shape prior to its recognition. We show that there is a simple and efficient viewing strategy that allows the observer to align the viewing direction with one of the two principal directions for a point on the surface. This strategy depends on only curvature measurements on the occluding contour and therefore demonstrates that recovering quantitative shape information from the contour does not require knowledge of the velocities or accelerations of the observer. Experimental results demonstrate that our method can be easily implemented and can provide reliable shape information from the occluding contour.			KUTULAKOS, KN (corresponding author), UNIV WISCONSIN, DEPT COMP SCI, 1210 W DAYTON ST, MADISON, WI 53706 USA.							ALOIMONOS J, 1987, 1ST P INT C COMP VIS, P35; ALOIMONOS Y, 1990, 10TH P INT C PATT RE, P346; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; Ballard D. H., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P524, DOI 10.1109/CCV.1988.590033; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BALLARD DH, 1989, IMAGE VISION COMPUT, V7, P3, DOI 10.1016/0262-8856(89)90013-9; BALLARD DH, 1992, CVGIP-IMAG UNDERSTAN, V56, P3, DOI 10.1016/1049-9660(92)90081-D; BALLARD DH, 1989, P INT JOINT C ART IN, P1635; BARROW HG, 1981, ARTIF INTELL, V17, P75, DOI 10.1016/0004-3702(81)90021-7; BLAKE A, 1993, INT J COMPUT VISION, V11, P127, DOI 10.1007/BF01469225; BRADY M, 1984, IEEE T PATTERN ANAL, V6, P288, DOI 10.1109/TPAMI.1984.4767521; BRADY M, 1985, COMPUT VISION GRAPH, V32, P1, DOI 10.1016/0734-189X(85)90001-5; BROOKS RA, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P692, DOI 10.1109/ROBOT.1989.100065; BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; Conte SD., 1972, ELEMENTARY NUMERICAL; COOMBS D, 1993, INT J COMPUT VISION, V11, P147, DOI 10.1007/BF01469226; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; GIBLIN P, 1987, 1ST P INT C COMP VIS, P136; Grosso E., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P395, DOI 10.1109/ICCV.1993.378188; HAGER G, 1987, P WORKSHOP SPATIAL R, P313; HORAUD R, 1987, 1ST P INT C COMP VIS, P374; Horn B., 1986, ROBOT VISION, P1; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321; KRIEGMAN DJ, 1983, COMMUNICATION; KROTKOV E, 1987, INT J COMPUT VISION, V1, P223, DOI 10.1007/BF00127822; KROTKOV E, 1993, INT J COMPUT VISION, V11, P187, DOI 10.1007/BF01469228; KUTULAKOS KM, 1993, IN PRESS P C COMPUT; LEYTON M, 1988, ARTIF INTELL, V34, P213, DOI 10.1016/0004-3702(88)90039-2; LUPINAR F, 1988, 2ND P INT C COMP VIS, P414; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; MARR D, 1978, TECHNOL REV, V81, P28; NELSON RC, 1989, IEEE T PATTERN ANAL, V11, P1102, DOI 10.1109/34.42840; PAPANIKOLOPOULOS NP, 1993, IEEE T ROBOTIC AUTOM, V9, P14, DOI 10.1109/70.210792; PONCE J, 1989, IEEE T PATTERN ANAL, V11, P951, DOI 10.1109/34.35498; PONCE J, 1987, INT J COMPUT VISION, V1, P195, DOI 10.1007/BF00127820; RICHARDS W, 1988, NATURAL COMPUTATION, P125; RICHETIN M, 1991, IEEE T PATTERN ANAL, V13, P185, DOI 10.1109/34.67647; RIMEY RD, 1994, INT J COMPUT VISION, V12, P173, DOI 10.1007/BF01421202; STEVENS KA, 1981, ARTIF INTELL, V17, P47, DOI 10.1016/0004-3702(81)90020-5; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; WIXSON LE, 1994, INT J COMPUT VISION, V12, P209, DOI 10.1007/BF01421203	44	32	34	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1994	12	2-3					113	136		10.1007/BF01421200	http://dx.doi.org/10.1007/BF01421200			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NL558		Green Submitted			2022-12-18	WOS:A1994NL55800001
J	SCHNORR, C				SCHNORR, C			COMPUTATION OF DISCONTINUOUS OPTICAL-FLOW BY DOMAIN DECOMPOSITION AND SHAPE OPTIMIZATION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								A well-known method for the reconstruction of motion fields from noisy image data is to determine flow fields by the minimization of a quadratic functional. The first approach of this class has been proposed by Horn and Schunck (1981). A drawback of such approaches is that an explicit representation of the discontinuities of the motion field is lacking and that, in general, the resulting flow fields approximate the motion fields only badly at the corresponding locations in the image plane. In this article, we discuss the possibility to improve the results by hypothesizing the qualitative structure of the motion field in terms of certain parameters. We decompose the image plane into disjoint sets, restrict the domain of definition of die functionals to these sets, and use the hypotheses to deform and to move the boundaries of the sets within the image plane. We discuss the range of applicability of this new technique and illustrate the algorithm by numerical examples. This article is a revised and extended version of Schnorr (1990).	UNIV HAMBURG,FACHBEREICH INFORMAT,AB KOGNIT SYST,W-2000 HAMBURG 50,GERMANY	University of Hamburg								GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; HAUG EJ, 1981, OPTIMIZATION DISTRIB, V2; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HUTCHINSON J, 1988, IEEE COMPUT, V21, P52; Lions J.L, 1971, OPTIMAL CONTROL SYST; Murat F., 1976, LECT NOTES COMPUT SC, V41, P54; MURRAY DW, 1987, IEEE T PATTERN ANAL, V9, P220, DOI 10.1109/TPAMI.1987.4767896; NAGEL HH, 1989, IEEE T PATTERN ANAL, V11, P13, DOI 10.1109/34.23110; NAGEL HH, 1987, ARTIF INTELL, V33, P299, DOI 10.1016/0004-3702(87)90041-5; Pironneau O., 1983, OPTIMAL SHAPE DESIGN; SCHNORR C, 1991, INT J COMPUT VISION, V6, P25, DOI 10.1007/BF00127124; SCHNORR C, 1991, THESIS U KARLSRUHE T; SCHNORR C, 1990, 1990 BRIT MACH VIS C, P109; SIMON J, 1980, NUMER FUNC ANAL OPT, V2, P649, DOI 10.1080/01630563.1980.10120631; TERZOPOULOS D, 1988, IEEE T PATTERN ANAL, V10, P417, DOI 10.1109/34.3908; VERRI A, 1987, 1ST P INT C COMP VIS, P171; YUILLE AL, 1990, LECT NOTES COMPUT SC, V427, P73	18	32	32	1	8	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1992	8	2					153	165		10.1007/BF00127172	http://dx.doi.org/10.1007/BF00127172			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JM824					2022-12-18	WOS:A1992JM82400004
J	Wu, AC; Zheng, WS; Gong, SG; Lai, JH				Wu, Ancong; Zheng, Wei-Shi; Gong, Shaogang; Lai, Jianhuang			RGB-IR Person Re-identification by Cross-Modality Similarity Preservation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Cross-modality model; RGB-infrared matching	FACE	Person re-identification (Re-ID) is an important problem in video surveillance for matching pedestrian images across non-overlapping camera views. Currently, most works focus on RGB-based Re-ID. However, RGB images are not well suited to a dark environment; consequently, infrared (IR) imaging becomes necessary for indoor scenes with low lighting and 24-h outdoor scene surveillance systems. In such scenarios, matching needs to be performed between RGB images and IR images, which exhibit different visual characteristics; this cross-modality matching problem is more challenging than RGB-based Re-ID due to the lack of visible colour information in IR images. To address this challenge, we study the RGB-IR cross-modality Re-ID (RGB-IR Re-ID) problem. Rather than applying existing cross-modality matching models that operate under the assumption of identical data distributions between training and testing sets to handle the discrepancy between RGB and IR modalities for Re-ID, we cast learning shared knowledge for cross-modality matching as the problem of cross-modality similarity preservation. We exploit same-modality similarity as the constraint to guide the learning of cross-modality similarity along with the alleviation of modality-specific information, and finally propose a Focal Modality-Aware Similarity-Preserving Loss. To further assist the feature extractor in extracting shared knowledge, we design a modality-gated node as a universal representation of both modality-specific and shared structures for constructing a structure-learnable feature extractor called Modality-Gated Extractor. For validation, we construct a new multi-modality Re-ID dataset, called SYSU-MM01, to enable wider study of this problem. Extensive experiments on this SYSU-MM01 dataset show the effectiveness of our method. Download link of dataset: https://github.com/wuancong/SYSU-MM01.	[Wu, Ancong; Zheng, Wei-Shi; Lai, Jianhuang] Sun Yat Sen Univ, Guangzhou, Peoples R China; [Zheng, Wei-Shi] Pengcheng Lab, Shenzhen, Peoples R China; [Zheng, Wei-Shi] Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou, Peoples R China; [Lai, Jianhuang] Guangdong Prov Key Lab Informat Secur, Guangzhou, Peoples R China; [Gong, Shaogang] Queen Mary Univ London, London, England	Sun Yat Sen University; University of London; Queen Mary University London	Zheng, WS (corresponding author), Sun Yat Sen Univ, Guangzhou, Peoples R China.; Zheng, WS (corresponding author), Pengcheng Lab, Shenzhen, Peoples R China.; Zheng, WS (corresponding author), Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou, Peoples R China.	wuancong@gmail.com; wszheng@ieee.org; s.gong@qmul.ac.uk; stsljh@mail.sysu.edu.cn		Wu, Ancong/0000-0002-7969-3190	National Key Research and Development Program of China [2016YFB1001002]; NSFC [U1911401, U1811461, U1611461]; Guangdong Province Science and Technology Innovation Leading Talents [2016TX03X157]; Guangdong Project [2018B030312002]; Guangzhou Research Project [201902010037]; Research Projects of Zhejiang Lab [2019KD0AB03]	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Guangdong Province Science and Technology Innovation Leading Talents; Guangdong Project; Guangzhou Research Project; Research Projects of Zhejiang Lab	This work was supported partially by the National Key Research and Development Program of China (2016YFB1001002), NSFC(U1911401, U1811461, U1611461), Guangdong Province Science and Technology Innovation Leading Talents (2016TX03X157), Guangdong Project (No. 2018B030312002), and Guangzhou Research Project (201902010037), and Research Projects of Zhejiang Lab (No. 2019KD0AB03). The corresponding author and principal investigator for this paper is Wei-Shi Zheng.	Ahmed E., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299016; Andrew Galen, 2013, ICML; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bak S, 2017, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR.2017.171; Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321; Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902; Chen DP, 2015, PROC CVPR IEEE, P1565, DOI 10.1109/CVPR.2015.7298764; Chen JX, 2017, PROC CVPR IEEE, P5330, DOI 10.1109/CVPR.2017.566; Chen YC, 2017, IEEE T CIRC SYST VID, V27, P1661, DOI 10.1109/TCSVT.2016.2515309; Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805; Chen YA, 2015, IEEE IJCNN; Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68; DAI P, 2018, INT JOINT C ART INT; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902; Feng W, 2021, INT J COMPUT VISION, V129, P619, DOI 10.1007/s11263-020-01388-x; Gray D., 2007, IEEE INT WORKSH PERF, V3, P1; Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21; Gretton A, 2012, J MACH LEARN RES, V13, P723; Guo CC, 2014, INT C PATT RECOG, P3540, DOI 10.1109/ICPR.2014.609; Haque A, 2016, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2016.138; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HE R, 2017, LEARNING INVARIANT D; He R, 2019, IEEE T PATTERN ANAL, V41, P1761, DOI 10.1109/TPAMI.2018.2842770; Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Jing XY, 2015, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2015.7298669; Kai Jungling, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P448, DOI 10.1109/AVSS.2010.75; Kan MN, 2016, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2016.524; Karanam S, 2015, IEEE I CONF COMP VIS, P4516, DOI 10.1109/ICCV.2015.513; Kodirov E, 2016, LECT NOTES COMPUT SC, V9905, P178, DOI 10.1007/978-3-319-46448-0_11; Kostinger Martin, 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247939; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kviatkovsky I, 2013, IEEE T PATTERN ANAL, V35, P1622, DOI 10.1109/TPAMI.2012.246; Lei Z, 2009, PROC CVPR IEEE, P1123, DOI 10.1109/CVPRW.2009.5206860; Li MX, 2018, LECT NOTES COMPUT SC, V11208, P772, DOI 10.1007/978-3-030-01225-0_45; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li X, 2015, IEEE I CONF COMP VIS, P3765, DOI 10.1109/ICCV.2015.429; Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463; Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin DH, 2006, LECT NOTES COMPUT SC, V3954, P13; Lin L, 2017, IEEE T PATTERN ANAL, V39, P1089, DOI 10.1109/TPAMI.2016.2567386; Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011; Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055; Liu CX, 2012, LECT NOTES COMPUT SC, V7583, P391, DOI 10.1007/978-3-642-33863-2_39; Lu KK, 2018, INT C PATT RECOG, P489, DOI 10.1109/ICPR.2018.8545190; Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152; NGUYEN DT, 2017, SENSORS BASEL, V17; Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794; Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426; Prosser B. J., 2010, PROC BRIT MACH VIS C, P6, DOI DOI 10.5244/C.24.21; RASIWASIA N, 2010, ACM MM, P251; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sharma A., 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247923; Shi ZY, 2015, PROC CVPR IEEE, P4184, DOI 10.1109/CVPR.2015.7299046; Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129; Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Van der Maaten L., 2008, J MACH LEARN RES, V9, P2579; Wang W, 2016, VLDB J, V25, P79, DOI 10.1007/s00778-015-0391-4; Wang XJ, 2016, IEEE T CIRC SYST VID, V26, P1447, DOI 10.1109/TCSVT.2015.2450331; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575; Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201; Wu BT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3946; Wu SH, 2019, INT J PAVEMENT ENG, V20, P33, DOI 10.1080/10298436.2016.1248204; Wu ZY, 2015, IEEE T PATTERN ANAL, V37, P1095, DOI 10.1109/TPAMI.2014.2360373; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1; Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509; Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35; YE M, 2018, INT JOINT C ART INT; You JJ, 2016, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2016.150; Yu HX, 2020, IEEE T PATTERN ANAL, V42, P956, DOI 10.1109/TPAMI.2018.2886878; Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113; ZHANG D, 2014, LARGE SCALE SUPERVIS; Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139; Zhang LH, 2015, INT CONF SOFTW ENG, P931, DOI 10.1109/ICSESS.2015.7339207; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zhao R, 2017, IEEE T PATTERN ANAL, V39, P356, DOI 10.1109/TPAMI.2016.2544310; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531; Zheng WS, 2016, IEEE T PATTERN ANAL, V38, P591, DOI 10.1109/TPAMI.2015.2453984; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138; Zheng Wei-Shi, 2009, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.23.23; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Zhun, 2017, PROC CVPR IEEE, P1318, DOI DOI 10.1109/CVPR.2017.389; Zhu F, 2014, ACM INT C CONFERENCE, P1479; Zhu JY, 2014, IEEE T INF FOREN SEC, V9, P501, DOI 10.1109/TIFS.2014.2299977; Zhu XT, 2018, IEEE T IMAGE PROCESS, V27, P2286, DOI 10.1109/TIP.2017.2740564; Zhuo ZH, 2012, INT CONF SIGN PROCES, P31, DOI 10.1109/ICoSP.2012.6491661	100	31	33	5	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1765	1785		10.1007/s11263-019-01290-1	http://dx.doi.org/10.1007/s11263-019-01290-1		FEB 2020	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN					2022-12-18	WOS:000515808300004
J	Wen, YD; Zhang, KP; Li, ZF; Qiao, Y				Wen, Yandong; Zhang, Kaipeng; Li, Zhifeng; Qiao, Yu			A Comprehensive Study on Center Loss for Deep Face Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Convolutional neural networks; Face recognition; Discriminative feature learning; Center loss		Deep convolutional neural networks (CNNs) trained with the softmax loss have achieved remarkable successes in a number of close-set recognition problems, e.g. object recognition, action recognition, etc. Unlike these close-set tasks, face recognition is an open-set problem where the testing classes (persons) are usually different from those in training. This paper addresses the open-set property of face recognition by developing the center loss. Specifically, the center loss simultaneously learns a center for each class, and penalizes the distances between the deep features of the face images and their corresponding class centers. Training with the center loss enables CNNs to extract the deep features with two desirable properties: inter-class separability and intra-class compactness. In addition, we extend the center loss in two aspects. First, we adopt parameter sharing between the softmax loss and the center loss, to reduce the extra parameters introduced by centers. Second, we generalize the concept of center from a single point to a region in embedding space, which further allows us to account for intra-class variations. The advanced center loss significantly enhances the discriminative power of deep features. Experimental results show that our method achieves high accuracies on several important face recognition benchmarks, including Labeled Faces in the Wild, YouTube Faces, IJB-A Janus, and MegaFace Challenging 1.	[Wen, Yandong; Zhang, Kaipeng] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Comp Vis & Pattern Recognit, Shenzhen, Peoples R China; [Li, Zhifeng] Tencent AI Lab, Shenzhen, Peoples R China; [Qiao, Yu] Chinese Acad Sci, SIAT SenseTime Joint Lab Shenzhen Inst Adv Techno, Shenzhen, Peoples R China	Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; Tencent; Chinese Academy of Sciences	Qiao, Y (corresponding author), Chinese Acad Sci, SIAT SenseTime Joint Lab Shenzhen Inst Adv Techno, Shenzhen, Peoples R China.	yandongw@andrew.cmu.edu; kpzhang@cmlab.csie.ntu.edu.tw; michaelzi@tencent.com; yu.qiao@siat.ac.cn	Qiao, Yu/ABD-5787-2021	Wen, Yandong/0000-0001-6330-7438	National Natural Science Foundation of China [U1613211, 61633021]; Shenzhen Research Program [JCYJ20170818164704758, JCYJ2015092516300 5055, ZDSYS201605101739178]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Research Program	This work was supported in part by National Natural Science Foundation of China (U1613211, 61633021) and Shenzhen Research Program (JCYJ20170818164704758, JCYJ2015092516300 5055, ZDSYS201605101739178).	AHONEN T, 1998, TPAMI, V28, P2037, DOI DOI 10.1109/TPAMI.2006.244; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Bredin H, 2017, INT CONF ACOUST SPEE, P5430, DOI 10.1109/ICASSP.2017.7953194; Burges, 1998, MNIST DATABASE HANDW; Cao Q., 2017, VGGFACE2 DATASET REC; Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992; Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41; Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389; Chen J, 2016, MOB INF SYST, V2016, DOI 10.1155/2016/5894752; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Chu WQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1561; Crosswhite N, 2017, IEEE INT CONF AUTOMA, P1, DOI 10.1109/FG.2017.11; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Duan YQ, 2017, IEEE T IMAGE PROCESS, V26, P3636, DOI 10.1109/TIP.2017.2704661; Glorot X., 2010, PROC MACH LEARN RES, P249; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Huang Gary B, 2014, 14003 U MASS AMH DEP, V14, P1; Huang GB, 2007, 07 UMASS TR; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jin H., 2017, ARXIV170503332; Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu J., 2015, ARXIV150607310; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Liu Y., 2017, ARXIV171000870; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu JW, 2015, IEEE T PATTERN ANAL, V37, P2041, DOI 10.1109/TPAMI.2015.2408359; Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523; Mika S., 1999, NEURAL NETWORKS SIGN, V1999, P41, DOI DOI 10.1109/NNSP.1999.788121; Miller D, 2015, ARXIV150502108; Nagi J, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P27, DOI 10.1109/ICMLA.2012.14; Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068; Parkhi Omkar M., 2015, BRIT MACH VIS C; Prince SJD, 2007, IEEE I CONF COMP VIS, P1751; Ranjan R., 2017, ARXIV PREPRINT ARXIV; Rao YM, 2017, IEEE I CONF COMP VIS, P3801, DOI 10.1109/ICCV.2017.408; Rippel O, 2015, ARXIV PREPRINT ARXIV, P1; Sankaranarayanan S., 2016, P IEEE INT C BIOMETR, P1; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Sohn Kihyuk, 2017, P IEEE INT C COMP VI, P3210; Sohn Kihyuk, 2016, NEURIPS, DOI DOI 10.5555/3157096.3157304; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tadmor O., 2016, ADV NEURAL INFORM PR, P1388; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tang Yichuan, 2013, ARXIV13060239; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; Vinyals Oriol, 2012, ADV NEURAL INFORM PR, P2825; Wang D., 2015, ARXIV150707242; Wang Feng, 2017, ARXIV170406369; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang XG, 2004, IEEE T PATTERN ANAL, V26, P1222, DOI 10.1109/TPAMI.2004.57; Wang Y., 2018, ARXIV181007599; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; WEN YD, 2016, PROC CVPR IEEE, P4893, DOI DOI 10.1109/CVPR.2016.529; Wisniewksi G, 2017, INTERSPEECH, P3582, DOI 10.21437/Interspeech.2017-1067; Wolf L., 2011, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2011.5995566; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Wu WL, 2017, IEEE I CONF COMP VIS, P3792, DOI 10.1109/ICCV.2017.407; Yang J., 2016, ARXIV160305474; Yang X, 2017, PROC CVPR IEEE, P4342, DOI 10.1109/CVPR.2017.462; Yao JF, 2017, LECT NOTES COMPUT SC, V10635, P405, DOI 10.1007/978-3-319-70096-0_42; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhou B., 2014, CORR, V1412, P6856; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	79	31	34	3	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		668	683		10.1007/s11263-018-01142-4	http://dx.doi.org/10.1007/s11263-018-01142-4			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD					2022-12-18	WOS:000468525900008
J	Jiang, CFF; Qi, SY; Zhu, YX; Huang, SY; Lin, J; Yu, LF; Terzopoulos, D; Zhu, SC				Jiang, Chenfanfu; Qi, Siyuan; Zhu, Yixin; Huang, Siyuan; Lin, Jenny; Yu, Lap-Fai; Terzopoulos, Demetri; Zhu, Song-Chun			Configurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image grammar; Scene synthesis; Photorealistic rendering; Normal estimation; Depth prediction; Benchmarks		We propose a systematic learning-based approach to the generation of massive quantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D images thereof, with associated ground truth information, for the purposes of training, benchmarking, and diagnosing learning-based computer vision and robotics algorithms. In particular, we devise a learning-based pipeline of algorithms capable of automatically generating and rendering a potentially infinite variety of indoor scenes by using a stochastic grammar, represented as an attributed Spatial And-Or Graph, in conjunction with state-of-the-art physics-based rendering. Our pipeline is capable of synthesizing scene layouts with high diversity, and it is configurable inasmuch as it enables the precise customization and control of important attributes of the generated scenes. It renders photorealistic RGB images of the generated scenes while automatically synthesizing detailed, per-pixel ground truth data, including visible surface depth and normal, object identity, and material information (detailed to object parts), as well as environments (e.g., illuminations and camera viewpoints). We demonstrate the value of our synthesized dataset, by improving performance in certain machine-learning-based scene understanding tasks-depth and surface normal prediction, semantic segmentation, reconstruction, etc.-and by providing benchmarks for and diagnostics of trained models by modifying object attributes and scene properties in a controllable manner.	[Jiang, Chenfanfu] Univ Penn, SIG Ctr Comp Graph, Philadelphia, PA 19104 USA; [Qi, Siyuan; Zhu, Yixin; Huang, Siyuan; Lin, Jenny; Zhu, Song-Chun] Univ Calif Los Angeles, UCLA Ctr Vis Cognit Learning & Auton, Los Angeles, CA 90095 USA; [Yu, Lap-Fai] Univ Massachusetts, Graph & Virtual Environm Lab, Boston, MA 02125 USA; [Terzopoulos, Demetri] Univ Calif Los Angeles, UCLA Comp Graph & Vis Lab, Los Angeles, CA USA	University of Pennsylvania; University of California System; University of California Los Angeles; University of Massachusetts System; University of Massachusetts Boston; University of California System; University of California Los Angeles	Zhu, YX (corresponding author), Univ Calif Los Angeles, UCLA Ctr Vis Cognit Learning & Auton, Los Angeles, CA 90095 USA.	cffjiang@seas.upenn.edu; syqi@ucla.edu; yixin.zhu@ucla.edu; huangsiyuan@ucla.edu; jh.lin@ucla.edu; craigyu@cs.umb.edu; dt@cs.ucla.edu; sczhu@stat.ucla.edu			DARPA XAI [N66001-17-2-4029]; ONR MURI [N00014-16-1-2007]; DoD CDMRP AMRAA [W81XWH-15-1-0147]	DARPA XAI; ONR MURI(MURIOffice of Naval Research); DoD CDMRP AMRAA	Support for the research reported herein was provided by DARPA XAI Grant N66001-17-2-4029, ONR MURI Grant N00014-16-1-2007, and DoD CDMRP AMRAA Grant W81XWH-15-1-0147.	Aldous D. J., 1985, ECOLE DETE PROBABILI, P1, DOI DOI 10.1007/BFB0099421; Backhaus W. G. K., 1998, COLOR VISION PERSPEC; Bansal A., 2016, C COMP VIS PATT REC; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Bartell F. O., 1980, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V257, P154; Bell S., 2014, ACM T GRAPHIC, V33, P98; Bell S, 2015, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR.2015.7298970; Bell S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462002; Ben-David Shai, 2007, NEURIPS, P7; Bickel S, 2009, J MACH LEARN RES, V10, P2137; Blitzer J., 2006, P 2006 C EMP METH NA, P120, DOI DOI 10.3115/1610075.1610094; Bowdish J., 2012, C COMP VIS PATT REC; Carreira-Perpinan Miguel A, 2005, AISTATS, P33; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Choi W, 2015, INT J COMPUT VISION, V112, P204, DOI 10.1007/s11263-014-0779-4; Cortes C., 2008, INT C ALG LEARN THEO; Csurka Gabriela, 2017, ARXIV170205374; Daume III H., 2009, C UNC ART INT UAI; Daume III Hal, 2007, P 45 ANN M ASS COMP, P256, DOI DOI 10.48550/ARXIV.0907.1815; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Du Y., 2016, EUR C COMP VIS ECCV; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eigen David, 2014, NEURIPS; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Fanello SR, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601223; Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154; Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929; Fouhey DF, 2013, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2013.421; Fridman A, 2003, P NATL ACAD SCI USA, V100, P8092, DOI 10.1073/pnas.0731829100; Gaidon A., 2017, C COMP VIS PATT REC; Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470; Ganin Yaroslav, 2015, ICML; Ghezelghieh MF, 2016, INT CONF 3D VISION, P685, DOI 10.1109/3DV.2016.75; Grabner H., 2011, C COMP VIS PATT REC; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Gretton A, 2009, NEURAL INF PROCESS S, P131; Gupta A., 2010, ADV NEURAL INFORM PR; Gupta A., 2011, C COMP VIS PATT REC; Handa A., 2016, C COMP VIS PATT REC; Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054; Handa Ankur, 2016, INT C ROB AUT ICRA; Hara K, 2005, IEEE T PATTERN ANAL, V27, P493, DOI 10.1109/TPAMI.2005.82; Harchaoui Z., 2005, ADV NEURAL INFORM PR; Hattori H., 2015, C COMP VIS PATT REC; He K., 2015, INT C COMPV VIS ICCV; Heckman James J, 1977, 172 NAT BUR EC RES; Hedau V., 2009, INT C COMP VIS ICCV; Heess Nicolas, 2017, ABS170702286 CORR, P3; Hermans T., 2011, INT C ROB AUT ICRA; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232; Huang QX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766890; Kohli Y. Z. M. B. P., 2016, ARXIV160304922; Koppula H. S., 2014, EUR C COMP VIS ECCV; Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335; Kulkarni T. D., 2015, C COMP VIS PATT REC; Kulkarni TD, 2015, ADV NEUR IN, V28; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lazebnik S., 2015, INT C COMP VIS ICCV; Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872; Liang W., 2016, INT JOINT C ART INT; Lin  J., 2016, SIGGRAPH ASIA 2016 V, P11, DOI DOI 10.1145/2992138.2992144; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu F., 2015, C COMP VIS PATT REC; Liu X., 2014, C COMP VIS PATT REC; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Lopez AM, 2017, ADV COMPUT VIS PATT, P243, DOI 10.1007/978-3-319-58347-1_13; Lu Y., 2016, AAAI C ART INT AAAI; Mansinghka V., 2013, ADV NEURAL INFORM PR; Mansour Y., 2009, ANN C LEARN THEOR CO; Marin J., 2010, C COMP VIS PATT REC; Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982; Mian A., 2015, C COMP VIS PATT REC; Movshovitz-Attias Y., 2014, BRIT MACH VIS C BMVC; Movshovitz-Attias Y., 2016, EUR C COMP VIS ECCV; Myers A., 2014, WORKSH VIS MEETS COG; Nishino K., 2001, INT C COMP VIS ICCV; Nishino K., 2016, T PATTERN ANAL MACHI, V38, P2321; Nishino K., 2009, INT C COMP VIS ICCV; Nishino K., 2014, C COMP VIS PATT REC; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Oxholm G., 2016, T PATTERN ANAL MACHI, V38, P2321; Pearl J., 2009, CAUSALITY, DOI DOI 10.1017/CBO9780511803161; Peng X., 2015, C COMP VIS PATT REC; Pharr M., 2004, PHYS BASED RENDERING; Pishchulin L., 2011, C COMP VIS PATT REC; Pishchulin L., 2012, C COMP VIS PATT REC; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Qiu W, 2016, THESIS; Qureshi F, 2008, P IEEE, V96, P1640, DOI 10.1109/JPROC.2008.928932; Rabie T. F., 1995, INT C COMP VIS ICCV; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Rivlin E, 2006, BRIT MACH VIS C BMVC; Rogez G, 2016, ADV NEUR IN, V29; Romero J., 2015, GERM C PATT REC; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Roy A, 2016, LECT NOTES COMPUT SC, V9908, P186, DOI 10.1007/978-3-319-46493-0_12; Sato I., 2003, T PATTERN ANAL MACHI, V25, P1218; Saxena A., 2013, C COMP VIS PATT REC; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Sharma G., 2002, DIGITAL COLOR IMAGIN; Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381; Silberman Nathan, 2012, EUR C COMP VIS, DOI 10.1007/978-3-642-33715-4_54; Song S., 2014, C COMP VIS PATT REC; Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41; STARK L, 1991, IEEE T PATTERN ANAL, V13, P1097, DOI 10.1109/34.99242; Stark M., 2010, P BRIT MACH VIS C, P1, DOI [10.5244/C.24.106, DOI 10.5244/C.24.106]; Su H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601159; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sun B., 2014, BMVC, V1, P3; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; VALBERG A, 2007, LIGHT VISION COLOR; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163; Wang X., 2015, C COMP VIS PATT REC; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Wang ZY, 2017, ADV NEUR IN, V30; Weinberger K., 2009, ANN INT C MACH LEARN, P1113, DOI DOI 10.1145/1553374.1553516; Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; WQiu A, 2016, ECCV WORKSH, P2, DOI DOI 10.1007/978-3-319-49409-8_75; Wu J., 2016, THESIS; Wu J., 2015, ADV NEURAL INFORM PR; Xiao J., 2012, ADV NEURAL INFORM PR, P755; Xie J., 2016, ARXIV160909408CSSTAT; Xie JY, 2016, PR MACH LEARN RES, V48; Xue Y, 2007, J MACH LEARN RES, V8, P35; Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535; Yeh YT, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185552; Yu K., 2005, INT C MACH LEARN ICM; Yu LF, 2015, IEEE I CONF COMP VIS, P711, DOI 10.1109/ICCV.2015.88; Yu LF, 2016, IEEE T VIS COMPUT GR, V22, P1138, DOI 10.1109/TVCG.2015.2417575; Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981; Zhang H., 2015, C COMP VIS PATT REC; Zhang Yuting, 2017, CVPR; Zhao YB, 2013, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2013.401; Zheng B., 2013, C COMP VIS PATT REC; Zheng B, 2015, INT J COMPUT VISION, V112, P221, DOI 10.1007/s11263-014-0795-4; Zhou T., 2016, C COMP VIS PATT REC; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhu S. C., 2007, STOCHASTIC GRAMMAR I; Zhu YX, 2016, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2016.415; Zhu YX, 2015, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2015.7298903; Zhu YK, 2014, LECT NOTES COMPUT SC, V8690, P408, DOI 10.1007/978-3-319-10605-2_27	155	31	31	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2018	126	9			SI		920	941		10.1007/s11263-018-1103-5	http://dx.doi.org/10.1007/s11263-018-1103-5			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GQ3HQ		Green Submitted			2022-12-18	WOS:000441553300003
J	Schonborn, S; Egger, B; Morel-Forster, A; Vetter, T				Schonborn, Sandro; Egger, Bernhard; Morel-Forster, Andreas; Vetter, Thomas			Markov Chain Monte Carlo for Automated Face Image Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face image analysis; Markov chain Monte Carlo; Model fitting; Morphable Model; Generative models; Top-down and bottom-up integration	SHAPE; INFERENCE; MODEL	We present a novel fully probabilistic method to interpret a single face image with the 3D Morphable Model. The new method is based on Bayesian inference and makes use of unreliable image-based information. Rather than searching a single optimal solution, we infer the posterior distribution of the model parameters given the target image. The method is a stochastic sampling algorithm with a propose-and-verify architecture based on the Metropolis-Hastings algorithm. The stochastic method can robustly integrate unreliable information and therefore does not rely on feed-forward initialization. The integrative concept is based on two ideas, a separation of proposal moves and their verification with the model (Data-Driven Markov Chain Monte Carlo), and filtering with the Metropolis acceptance rule. It does not need gradients and is less prone to local optima than standard fitters. We also introduce a new collective likelihood which models the average difference between the model and the target image rather than individual pixel differences. The average value shows a natural tendency towards a normal distribution, even when the individual pixel-wise difference is not Gaussian. We employ the new fitting method to calculate posterior models of 3D face reconstructions from single real-world images. A direct application of the algorithm with the 3D Morphable Model leads us to a fully automatic face recognition system with competitive performance on the Multi-PIE database without any database adaptation.	[Schonborn, Sandro; Egger, Bernhard; Morel-Forster, Andreas; Vetter, Thomas] Univ Basel, Dept Math & Comp Sci, Spiegelgasse 1, CH-4051 Basel, Switzerland	University of Basel	Schonborn, S (corresponding author), Univ Basel, Dept Math & Comp Sci, Spiegelgasse 1, CH-4051 Basel, Switzerland.	sandro.schoenborn@unibas.ch	Egger, Bernhard/AAE-5389-2019	Egger, Bernhard/0000-0002-4736-2397; Schonborn, Sandro/0000-0003-4316-8519				Albrecht T, 2013, MED IMAGE ANAL, V17, P959, DOI 10.1016/j.media.2013.05.010; Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; De Bernardinis F., 2004, MONTE CARLO STAT MET; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Eckhardt M, 2009, INT J PATTERN RECOGN, V23, P379, DOI 10.1142/S0218001409007247; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb P.F., 2012, THEORY COMPUT, V8, P415, DOI DOI 10.4086/TOC.2012.V008A019; Gilks WR, 1996, MARKOV CHAIN MONTE C, V2; Gonick L., 1993, CARTOON GUIDE STAT; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Huang Gary B., 2007, 0749 U MASS, P7; Jampani V, 2015, COMPUT VIS IMAGE UND, V136, P32, DOI 10.1016/j.cviu.2015.03.002; KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390; Koestinger M., 2011, ICCV WORKSH, DOI [10.1109/ICCVW.2011.6130513, DOI 10.1109/ICCVW.2011.6130513]; Liu C, 2002, LECT NOTES COMPUT SC, V2351, P687; Liu JS, 2000, J AM STAT ASSOC, V95, P121, DOI 10.2307/2669532; Lohit S, 2015, IEEE COMPUT SOC CONF; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luthi M., 2012, INSIGHT J, V1, P1; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247; Rauschert I, 2012, LECT NOTES COMPUT SC, V7576, P704, DOI 10.1007/978-3-642-33715-4_51; Romdhani S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P59; ROMDHANI S, 2005, PROC CVPR IEEE, P986, DOI DOI 10.1109/CVPR.2005.145; Sambridge M, 2002, REV GEOPHYS, V40, DOI 10.1029/2000RG000089; Schonborn S, 2015, COMPUT VIS IMAGE UND, V136, P117, DOI 10.1016/j.cviu.2015.01.008; Schonborn S, 2013, LECT NOTES COMPUT SC, V8142, P101, DOI 10.1007/978-3-642-40602-7_11; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Wojek C, 2010, LECT NOTES COMPUT SC, V6314, P467, DOI 10.1007/978-3-642-15561-1_34; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Zhu XY, 2015, IEEE INT CONF AUTOMA; Zivanov J, 2013, INT CONF BIOMETR	44	31	31	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2017	123	2					160	183		10.1007/s11263-016-0967-5	http://dx.doi.org/10.1007/s11263-016-0967-5			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EU2OM		Green Accepted			2022-12-18	WOS:000400868800002
J	Larsson, V; Olsson, C				Larsson, Viktor; Olsson, Carl			Convex Low Rank Approximation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Low rank approximation; Convex relaxation; Convex envelope; Structure from motion	MATRIX COMPLETION; FACTORIZATION; MOTION; REGULARIZATION; ALGORITHM; SHAPE	Low rank approximation is an important tool in many applications. Given an observed matrix with elements corrupted by Gaussian noise it is possible to find the best approximating matrix of a given rank through singular value decomposition. However, due to the non-convexity of the formulation it is not possible to incorporate any additional knowledge of the sought matrix without resorting to heuristic optimization techniques. In this paper we propose a convex formulation that is more flexible in that it can be combined with any other convex constraints and penalty functions. The formulation uses the so called convex envelope, which is the provably best possible convex relaxation. We show that for a general class of problems the envelope can be efficiently computed and may in some cases even have a closed form expression. We test the algorithm on a number of real and synthetic data sets and show state-of-the-art results.	[Larsson, Viktor; Olsson, Carl] Lund Univ, Lund, Sweden	Lund University	Larsson, V (corresponding author), Lund Univ, Lund, Sweden.	viktorl@maths.lth.se; calle@maths.lth.se			Swedish Research Council [2012-4213]; Crafoord Foundation	Swedish Research Council(Swedish Research CouncilEuropean Commission); Crafoord Foundation	This work has been funded by the Swedish Research Council (Grant No. 2012-4213) and the Crafoord Foundation. This paper has benefited from discussions with Erik Bylow and Fredrik Kahl.	Andersson F, 2014, IEEE T SIGNAL PROCES, V62, P5761, DOI 10.1109/TSP.2014.2358961; Angst R., 2011, INT C COMP VIS; [Anonymous], 2016, MOSEK OPTIMIZATION T; Aquiar P. M. Q., 2008, IEEE C COMP VIS PATT; Argyriou A., 2012, ADV NEURAL INFORM PR; Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Buchanan AM, 2005, PROC CVPR IEEE, P316; Cabral R, 2013, INT C COMP VIS; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Eckart C, 1936, PSYCHOMETRIKA, V1, P211, DOI 10.1007/BF02288367; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Eriksson A, 2015, PROC CVPR IEEE, P3349, DOI 10.1109/CVPR.2015.7298956; Eriksson A, 2012, IEEE T PATTERN ANAL, V34, P1681, DOI 10.1109/TPAMI.2012.116; Favaro P, 2011, PROC CVPR IEEE, P1801, DOI 10.1109/CVPR.2011.5995365; Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730; Garg R., 2013, IEEE C COMP VIS PATT; Garg R, 2013, INT J COMPUT VISION, V104, P286, DOI 10.1007/s11263-012-0607-7; Gillis N, 2011, SIAM J MATRIX ANAL A, V32, P1149, DOI 10.1137/110820361; Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271; Jacobs D., 1997, IEEE C COMP VIS PATT; Jojic V., 2011, INT C ART INT STAT; Ke QF, 2005, PROC CVPR IEEE, P739; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Lai H., 2014, EUR C COMP VIS, V8690; Larsson V., 2014, EUR C COMP VIS; Larsson V., 2015, INT C EN MIN METH CO; Lin Z., 2010, ARXIV10095055, DOI DOI 10.1016/J.JSB.2012.10.010; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; McDonald A. M., 2014, PROC 27 INT C NEURAL, P3644; Okatani T., 2011, P INT C COMP VIS; Okatani T, 2007, INT J COMPUT VISION, V72, P329, DOI 10.1007/s11263-006-9785-5; Olsen SI, 2008, J MATH IMAGING VIS, V31, P233, DOI 10.1007/s10851-007-0060-3; Olsson C., 2009, SCAND C IM AN; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Rockafellar R.T., 1997, CONVEX ANAL; Strelow D., 2012, IEEE C COMP VIS PATT; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Wang S., 2013, IJCAI, P1764; Wiberg T., 2013, P 2 S COMP STAT; Yan JY, 2008, IEEE T PATTERN ANAL, V30, P865, DOI 10.1109/TPAMI.2007.70739; Zheng YQ, 2012, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2012.6247828; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	45	31	35	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2016	120	2					194	214		10.1007/s11263-016-0904-7	http://dx.doi.org/10.1007/s11263-016-0904-7			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DU3EF					2022-12-18	WOS:000382092900005
J	Hexner, J; Hagege, RR				Hexner, Jonathan; Hagege, Rami R.			2D-3D Pose Estimation of Heterogeneous Objects Using a Region Based Approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Local region-based 3D pose estimation; Three-dimensional pose estimation	SEGMENTATION; TRACKING	Recently, region based methods for estimating the 3D pose of an object from a 2D image have gained increasing popularity. They do not require prior knowledge of the object's texture, making them particularity attractive when the object's texture is unknown a priori. Region based methods estimate the 3D pose of an object by finding the pose which maximizes the image segmentation in to foreground and background regions. Typically the foreground and background regions are described using global appearance models, and an energy function measuring their fit quality is optimized with respect to the pose parameters. Applying a region based approach on standard 2D-3D pose estimation databases shows its performance is strongly dependent on the scene complexity. In simple scenes, where the statistical properties of the foreground and background do not spatially vary, it performs well. However, in more complex scenes, where the statistical properties of the foreground or background vary, the performance strongly degrades. The global appearance models used to segment the image do not sufficiently capture the spatial variation. Inspired by ideas from local active contours, we propose a framework for simultaneous image segmentation and pose estimation using multiple local appearance models. The local appearance models are capable of capturing spatial variation in statistical properties, where global appearance models are limited. We derive an energy function, measuring the image segmentation, using multiple local regions and optimize it with respect to the pose parameters. Our experiments show a substantially higher probability of estimating the correct pose for heterogeneous objects, whereas for homogeneous objects there is minor improvement.	[Hexner, Jonathan; Hagege, Rami R.] Ben Gurion Univ Negev, IL-84105 Beer Sheva, Israel	Ben Gurion University	Hexner, J (corresponding author), Ben Gurion Univ Negev, IL-84105 Beer Sheva, Israel.	jonathan.hexner@gmail.com; hagege@ee.bgu.ac.il		Hexner, Jonathan/0000-0001-9970-0143				Arie-Nachimson M, 2009, IEEE I CONF COMP VIS, P1341, DOI 10.1109/ICCV.2009.5459310; Bibby C, 2008, LECT NOTES COMPUT SC, V5303, P831, DOI 10.1007/978-3-540-88688-4_61; Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35; Brox T, 2010, IEEE T PATTERN ANAL, V32, P402, DOI 10.1109/TPAMI.2009.32; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Dambreville S, 2010, SIAM J IMAGING SCI, V3, P110, DOI 10.1137/080741653; Dame A, 2013, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2013.170; Harris C., 1990, P BRIT MECH VIS C; Hinterstoisser S, 2012, LECT NOTES COMPUTER, V7724, P548; Lankton S, 2008, IEEE T IMAGE PROCESS, V17, P2029, DOI 10.1109/TIP.2008.2004611; Leibe B., 2004, EUROPEAN C COMPUTER, P17; Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; Ma Y., 2003, INVITATION 3D VISION; MathWorks, 2014, PAR COMP TOOLB R2014; Prisacariu VA, 2012, INT J COMPUT VISION, V98, P335, DOI 10.1007/s11263-011-0514-3; Prisacariu VA, 2013, INT SYM MIX AUGMENT, P89, DOI 10.1109/ISMAR.2013.6671768; Rematas K, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCV.2011.6126455; Rosenhahn B, 2007, INT J COMPUT VISION, V73, P243, DOI [10.1007/s11263-006-9965-3, 10.1007/S11263-006-9965-3]; Savarese S, 2007, IEEE I CONF COMP VIS, P1245; Schmaltz C., 2009, LECT NOTES COMPUTER; Schmaltz C, 2007, LECT NOTES COMPUT SC, V4478, P56; Tan D. J., 2014, IEEE C COMP VIS PATT	24	31	35	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2016	118	1					95	112		10.1007/s11263-015-0873-2	http://dx.doi.org/10.1007/s11263-015-0873-2			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DL6ZT					2022-12-18	WOS:000375789300005
J	Mathias, M; Martinovic, A; Van Gool, L				Mathias, Markus; Martinovic, Andelo; Van Gool, Luc			ATLAS: A Three-Layered Approach to Facade Parsing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic segmentation; Facade parsing; Procedural modeling	SEGMENTATION; OPTIMIZATION	We propose a novel approach for semantic segmentation of building facades. Our system consists of three distinct layers, representing different levels of abstraction in facade images: segments, objects and architectural elements. In the first layer, the facade is segmented into regions, each of which is assigned a probability distribution over semantic classes. We evaluate different state-of-the-art segmentation and classification strategies to obtain the initial probabilistic semantic labeling. In the second layer, we investigate the performance of different object detectors and show the benefit of using such detectors to improve our initial labeling. The generic approaches of the first two layers are then specialized for the task of facade labeling in the third layer. There, we incorporate additional meta-knowledge in the form of weak architectural principles, which enforces architectural plausibility and consistency on the final reconstruction. Rigorous tests performed on two existing datasets of building facades demonstrate that we outperform the current state of the art, even when using outputs from lower layers of the pipeline. Finally, we demonstrate how the output of the highest layer can be used to create a procedural building reconstruction.	[Mathias, Markus; Martinovic, Andelo; Van Gool, Luc] Katholieke Univ Leuven, Kasteelpk Arenberg 10,Box 2441, B-3001 Heverlee, Belgium	KU Leuven	Mathias, M (corresponding author), Katholieke Univ Leuven, Kasteelpk Arenberg 10,Box 2441, B-3001 Heverlee, Belgium.	markus.mathias@esat.kuleuven.be; andelo.martinovic@esat.kuleuven.be; luc.vangool@esat.kuleuven.be			EC [231809]; ERC [273940]	EC(European CommissionEuropean Commission Joint Research Centre); ERC(European Research Council (ERC)European Commission)	This paper is supported by EC FP7 Integrated Project 3D-COFORM, Grant No. 231809, and ERC Advanced Grant VarCity, Grant No. 273940.	Achanta R., 2010, TECHNICAL REPORT; Alegre O., 2004, INT WORKSH VIS TECHN, P1; Aliaga DG, 2007, IEEE T VIS COMPUT GR, V13, P786, DOI 10.1109/TVCG.2007.1024; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Beale M., 1993, NEURAL NETWORK TOOLB; Belic F, 2013, IEEE SYMP COMP COMMU; Benenson R, 2013, PROC CVPR IEEE, P3666, DOI 10.1109/CVPR.2013.470; Bokeloh M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778841; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cohen-Or D., 2013, ACM T GRAPHIC, V32, P1, DOI DOI 10.1145/2461912.24619231305.68292; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Dai D., 2012, EUR C COMP VIS; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deep K, 2009, APPL MATH COMPUT, V212, P505, DOI 10.1016/j.amc.2009.02.044; Dick AR, 2004, INT J COMPUT VISION, V60, P111, DOI 10.1023/B:VISI.0000029665.07652.61; Dollar P., 2009, BRIT MACH VIS C; Esri, 2013, CITYENGINE; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; FELZENSZWALB PF, 2010, PROC CVPR IEEE, P2241, DOI DOI 10.1109/CVPR.2010.5539906; Fletcher Roger, 1987, PRACTICAL METHODS OP, DOI 10.1002/9781118723203; Floros G., 2011, P BRIT MACH VIS C BM, P1; Frohlich B, 2013, MACH VISION APPL, V24, P1043, DOI 10.1007/s00138-012-0480-y; Frohlich B., 2012, 11 AS C COMP VIS DAE, P218; Gilks W. R., 1995, MARKOV CHAIN MONTE C, DOI 10.1201/b14835; Girshick RB, 2012, DISCRIMINATIVELY TRA; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; Han F, 2009, IEEE T PATTERN ANAL, V31, P59, DOI 10.1109/TPAMI.2008.55; Han T., 2012, AS C COMP VIS, P552; Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4; Holland J. H, 1992, ADAPTATION NATURAL A; Houben S, 2013, IEEE INT C INTELL TR, P7, DOI 10.1109/ITSC.2013.6728595; Karp Richard M., 1972, COMPLEXITY COMPUTER, P85, DOI [10.1007/978-1-4684-2001-2_9, DOI 10.1007/978-1-4684-2001-2_9]; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Korah T, 2008, LECT NOTES COMPUT SC, V5302, P359, DOI 10.1007/978-3-540-88682-2_28; Korc F., 2009, TRIGGP200901; Kumar MP, 2010, PROC CVPR IEEE, P3217, DOI 10.1109/CVPR.2010.5540072; Kumar S, 2005, LECT NOTES COMPUT SC, V3757, P153, DOI 10.1007/11585978_11; Ladick`y Lubor, 2010, ECCV, DOI DOI 10.1007/978-3-642-15561-1; Leibe B., 2007, IEEE C COMP VIS PATT; Leibe B, 2006, LECT NOTES COMPUT SC, V4170, P508; Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131; Martinovic A, 2012, LECT NOTES COMPUT SC, V7578, P416, DOI 10.1007/978-3-642-33786-4_31; Mathias M., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P304, DOI 10.1109/3DIMPVT.2011.45; Mathias M., 2011, ISPRS INT WORKSH 3D; Mathias M, 2013, IEEE IJCNN; Mathworks, 2014, GLOB OPT TOOLB DOC; Muller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]; Musialski P., 2012, STATE ART REPORTS, P1; Nowozin S, 2010, LECT NOTES COMPUT SC, V6316, P98, DOI 10.1007/978-3-642-15567-3_8; Ok D, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P128, DOI 10.1109/3DIMPVT.2012.25; Recky M., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P358, DOI 10.1109/3DIMPVT.2011.52; Riemenschneider H, 2012, PROC CVPR IEEE, P1640, DOI 10.1109/CVPR.2012.6247857; Ripperda N, 2006, LECT NOTES COMPUT SC, V4174, P750; Russell S. J., 1996, ARTIFICIAL INTELLIGE; Shechtman E, 2007, PROC CVPR IEEE, P1744; Shen CH, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024218; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Simon L, 2012, PROC CVPR IEEE, P518, DOI 10.1109/CVPR.2012.6247716; Socher R., 2011, P 28 INT C INT C MAC, P129; Szummer M, 2008, LECT NOTES COMPUT SC, V5303, P582, DOI 10.1007/978-3-540-88688-4_43; Taskar Ben, 2005, P 22 INT C MACH LEAR, P896; Teboul O., 2010, ECOLE CENTRALE PARIS; TEBOUL O, 2010, PROC CVPR IEEE, P3105, DOI DOI 10.1109/CVPR.2010.5540068; Teboul O, 2013, IEEE T PATTERN ANAL, V35, P1744, DOI 10.1109/TPAMI.2012.252; Tighe J, 2013, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2013.386; Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39; Van den Bergh Michael, 2012, EUR C COMP VIS; Weissenberg J., 2013, IEEE C COMP VIS PATT; Wendel A, 2010, LECT NOTES COMPUT SC, V6376, P51; Wojek C, 2008, LECT NOTES COMPUT SC, V5305, P733, DOI 10.1007/978-3-540-88693-8_54; Wu F., 2013, CORR; Xiao J., 2009, SIGGRAPH, V28; Xiao J., 2008, SIGGRAPH ASIA; Yang C, 2012, PROC CVPR IEEE, P1720, DOI 10.1109/CVPR.2012.6247867; Yang MY, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCVW.2011.6130243; Yang MY, 2011, LECT NOTES COMPUT SC, V6952, P209, DOI 10.1007/978-3-642-24393-6_18; Zhao P, 2010, PROC CVPR IEEE, P342, DOI 10.1109/CVPR.2010.5540192	83	31	30	3	24	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2016	118	1					22	48		10.1007/s11263-015-0868-z	http://dx.doi.org/10.1007/s11263-015-0868-z			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DL6ZT		Green Published			2022-12-18	WOS:000375789300002
J	Ladicky, L; Russell, C; Kohli, P; Torr, PHS				Ladicky, L'ubor; Russell, Chris; Kohli, Pushmeet; Torr, Philip H. S.			Inference Methods for CRFs with Co-occurrence Statistics	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Conditional random fields; Object class segmentation; Optimization	ENERGY MINIMIZATION; ALGORITHMS; MODEL	The Markov and Conditional random fields (CRFs) used in computer vision typically model only local interactions between variables, as this is generally thought to be the only case that is computationally tractable. In this paper we consider a class of global potentials defined over all variables in the CRF. We show how they can be readily optimised using standard graph cut algorithms at little extra expense compared to a standard pairwise field. This result can be directly used for the problem of class based image segmentation which has seen increasing recent interest within computer vision. Here the aim is to assign a label to each pixel of a given image from a set of possible object classes. Typically these methods use random fields to model local interactions between pixels or super-pixels. One of the cues that helps recognition is global object co-occurrence statistics, a measure of which classes (such as chair or motorbike) are likely to occur in the same image together. There have been several approaches proposed to exploit this property, but all of them suffer from different limitations and typically carry a high computational cost, preventing their application on large images. We find that the new model we propose produces a significant improvement in the labelling compared to just using a pairwise model and that this improvement increases as the number of labels increases.	[Ladicky, L'ubor] Univ Oxford, Oxford, England; [Russell, Chris] Univ London, Queen Mary Coll, London, England; [Kohli, Pushmeet] Microsoft Res, Cambridge, England; [Torr, Philip H. S.] Oxford Brookes Univ, Oxford OX3 0BP, England	University of Oxford; University of London; Queen Mary University London; Microsoft; Oxford Brookes University	Ladicky, L (corresponding author), Univ Oxford, Oxford, England.	lubor@robots.ox.ac.uk; chrisr@eecs.qmul.ac.uk; pkohli@microsoft.com; philiptorr@brookes.ac.uk		Russell, Chris/0000-0003-1665-1759	EPSRC; HMGCC; IST Programme of the European Community, under the PASCAL2 Network of Excellence [IST-2007-216886]; Royal Society Wolfson Research Merit Award; Engineering and Physical Sciences Research Council [EP/I001107/1] Funding Source: researchfish; EPSRC [EP/I001107/1] Funding Source: UKRI	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); HMGCC; IST Programme of the European Community, under the PASCAL2 Network of Excellence; Royal Society Wolfson Research Merit Award(Royal Society of London); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This study was supported by EPSRC research grants, HMGCC, the IST Programme of the European Community, under the PASCAL2 Network of Excellence, IST-2007-216886. P. H. S. Torr is in receipt of Royal Society Wolfson Research Merit Award.	Belongie S, 2007, INT C COMP VIS RIO D; Benson HY, 2007, COMPUT OPTIM APPL, V38, P371, DOI 10.1007/s10589-007-9048-6; Borenstein E., 2006, CVPR, V1, P969, DOI DOI 10.1109/CVPR.2006.276; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Delong A., 2010, IEEE C COMP VIS PATT; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Galleguillos C, 2008, PROC CVPR IEEE, P3552; Heitz G, 2008, EUR C COMP VIS MARS; Hoiem D., 2007, IEEE C COMP VIS PATT; Jurie F., 2008, IEEE C COMP VIS PAT; Kleinberg J, 2002, J ACM, V49, P616, DOI 10.1145/585265.585268; Kohli P., 2008, IEEE C COMP VIS PATT; Koller D., 2009, INT C COMP VIS KYOT; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kolmogorov V, 2006, LECT NOTES COMPUT SC, V3952, P1; KOMODAKIS N, 2007, IEEE C COMP VIS PATT; Kumar MP, 2011, J MACH LEARN RES, V12, P31; Ladicky L, 2010, LECT NOTES COMPUT SC, V6314, P424, DOI 10.1007/978-3-642-15561-1_31; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Malik J, 2005, UCBCSD051382 EECS DE; Narasimhan M., 2005, UAI, P404; Perronnin F., 2008, BRIT MACH VIS CONFER; Rother C, 2005, PROC CVPR IEEE, P589; SCHLESINGER M, 1976, KIBERNETIKA, V4, P113; Scholkopf B., 2001, ADOPTIVE COMPUTATION; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Szeliski R., 2006, EUR C COMP VIS; Torr P. H. S., 2010, UNCERTAINTY ARTIFICI; Torr P. H. S., 2012, BRIT MACH VIS C; Torr P. H. S., 2008, INT C MACH LEARN NEW; Torr PHS, 1998, PHILOS T R SOC A, V356, P1321, DOI 10.1098/rsta.1998.0224; Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273; Toyoda T, 2008, IEEE T PATTERN ANAL, V30, P1483, DOI 10.1109/TPAMI.2008.105; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Willsky A., 2002, MAP ESTIMATION VIA A; YANG L, 2007, IEEE C COMP VIS PATT; Zisserman A., 2006, IEEE C COMP VIS PAT	43	31	32	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2013	103	2			SI		213	225		10.1007/s11263-012-0583-y	http://dx.doi.org/10.1007/s11263-012-0583-y			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	150JC					2022-12-18	WOS:000319385400004
J	Ke, Y; Sukthankar, R; Hebert, M				Ke, Yan; Sukthankar, Rahul; Hebert, Martial			Volumetric Features for Video Event Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; Video event detection		Real-world actions occur often in crowded, dynamic environments. This poses a difficult challenge for current approaches to video event detection because it is difficult to segment the actor from the background due to distracting motion from other objects in the scene. We propose a technique for event recognition in crowded videos that reliably identifies actions in the presence of partial occlusion and background clutter. Our approach is based on three key ideas: (1) we efficiently match the volumetric representation of an event against oversegmented spatio-temporal video volumes; (2) we augment our shape-based features using flow; (3) rather than treating an event template as an atomic entity, we separately match by parts (both in space and time), enabling robustness against occlusions and actor variability. Our experiments on human actions, such as picking up a dropped object or waving in a crowd show reliable detection with few false positives.	[Ke, Yan; Sukthankar, Rahul; Hebert, Martial] Carnegie Mellon, Sch Comp Sci, Pittsburgh, PA USA; [Sukthankar, Rahul] Intel Labs Pittsburgh, Pittsburgh, PA USA	Carnegie Mellon University; Intel Corporation	Ke, Y (corresponding author), Carnegie Mellon, Sch Comp Sci, Pittsburgh, PA USA.	yke@cs.cmu.edu; rahuls@cs.cmu.edu; hebert@cs.cmu.edu			NSF [IIS-0534962]	NSF(National Science Foundation (NSF))	This work was supported by NSF Grant IIS-0534962.	Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744; ANKERST M, 1999, P INT S ADV SPAT DAT; ARAMBEL P, 2004, P SPIE, V5429; *ASIN, ASINB00004Z73V P PAN; ASLAM JA, 2005, P INT ACM SIGIR C RE; BELL W, 1999, DETECTION LONG TERM; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BLANK M, 2005, P ICCV; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; BOIMAN O, 2006, NIPS; BUCKLEY C, 2000, P INT ACM SIGIR C RE; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; COUR T, 2007, P CVPR; CYR CM, 2001, P ICCV; DEMENTHON D, 2006, MULTIMEDIA TOOLS APP, V30; DEMENTHON D, 2002, STAT METH VID PROC W; DOLLAR P, 2005, IEEE VS PETS WORKSH; Efros Alexei A, 2003, P ICCV; Fei-Fei L., 2006, IEEE T PATTERN ANAL, V28; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; FISCHLER MA, 1973, IEEE T COMPUTERS, V22; FUNKHOUSER T, 2003, ACM T GRAPHICS; Gorelick L, 2006, IEEE T PATTERN ANAL, V28, P1991, DOI 10.1109/TPAMI.2006.253; HAMID R, 2005, P UAI; HONGENG S, 2001, P ICCV; JHUANG H, 2007, P ICCV; JIANG H, 2006, P CVPR; Kazhdan M., 2003, Symposium on Geometry Processing, P156; KE Y, 2005, P ICCV; KE Y, 2007, WORKSH VIS SURV; KE Y, 2007, P ICCV; Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378; LAPTEV I, 2007, P ICCV; Laptev Ivan, 2008, P CVPR; LEIBE B, 2007, P ICCV; Leung Y, 2000, IEEE T PATTERN ANAL, V22, P1396, DOI 10.1109/34.895974; Ling H, 2007, IEEE T PATTERN ANAL, V29, P840, DOI 10.1109/TPAMI.2007.1058; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; MEDIONI G, 2001, IEEE T PATTERN ANAL; MORI G, 2005, P ICCV; Niebles Juan Carlos, 2006, P BMVC; ODOBEZ JM, 1995, J VISUAL COMMUNICATI, V6; Ramanan D., 2003, NIPS; Ramanan D, 2007, IEEE T PATTERN ANAL, V29, P65, DOI 10.1109/TPAMI.2007.250600; Ramanan D, 2006, IEEE T PATTERN ANAL, V28, P1319, DOI 10.1109/TPAMI.2006.155; SALI E, 1999, P BMVC; Schuldt C., 2004, P ICPR; Shah M., 2005, P CVPR; Shechtman E, 2007, P CVPR; SHECHTMAN E, 2005, P CVPR; SHECHTMAN E, 2007, IEEE T PATTERN ANAL, V29; Sheikh Y., 2005, P ICCV; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; SRINIVASAN P, 2007, P CVPR; *SRO SPORTS ENT, WIMBL 2000 SEM AG VS; VASWANI N, 2003, P CVPR; VEERARAGHAVAN A, 2006, P CVPR; VEIT PBT, 2004, P CVPR; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang FL, 2003, CHINESE LAW GOV, V36, P3, DOI 10.2753/CLG0009-460936043; WANG J, 2005, ACM SIGGRAPH; WANG J, 2004, P ECCV; WEBER M, 2000, P ECCV; WEINLAND D, 2006, P CVPR; Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013; ZHU G, 2006, P ICPR; ZHU G, 2006, ECCV WORKSH HCI	71	31	36	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2010	88	3					339	362		10.1007/s11263-009-0308-z	http://dx.doi.org/10.1007/s11263-009-0308-z			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	580EL		Green Submitted			2022-12-18	WOS:000276429900001
J	Larlus, D; Verbeek, J; Jurie, F				Larlus, Diane; Verbeek, Jakob; Jurie, Frederic			Category Level Object Segmentation by Combining Bag-of-Words Models with Dirichlet Processes and Random Fields	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object recognition; Segmentation; Random fields		This paper addresses the problem of accurately segmenting instances of object classes in images without any human interaction. Our model combines a bag-of-words recognition component with spatial regularization based on a random field and a Dirichlet process mixture. Bag-of-words models successfully predict the presence of an object within an image; however, they can not accurately locate object boundaries. Random Fields take into account the spatial layout of images and provide local spatial regularization. Yet, as they use local coupling between image labels, they fail to capture larger scale structures needed for object recognition. These components are combined with a Dirichlet process mixture. It models images as a composition of regions, each representing a single object instance. Gibbs sampling is used for parameter estimations and object segmentation. Our model successfully segments object category instances, despite cluttered backgrounds and large variations in appearance and viewpoints. The strengths and limitations of our model are shown through extensive experimental evaluations. First, we evaluate the result of two methods to build visual vocabularies. Second, we show how to combine strong labeling (segmented images) with weak labeling (images annotated with bounding boxes), in order to limit the labeling effort needed to learn the model. Third, we study the effect of different initializations. We present results on four image databases, including the challenging PASCAL VOC 2007 data set on which we obtain state-of-the art results.	[Larlus, Diane] Tech Univ Darmstadt, INP Grenoble, Multimodal Interact Syst, D-64289 Darmstadt, Germany; [Verbeek, Jakob] INRIA Rhone Alpes, F-38334 Montbonnot St Martin, St Ismier, France; [Jurie, Frederic] Univ Caen, UFR Sci, GREYC, F-14032 Caen, France	Technical University of Darmstadt; Universite de Caen Normandie	Larlus, D (corresponding author), Tech Univ Darmstadt, INP Grenoble, Multimodal Interact Syst, Hochschulstr 10, D-64289 Darmstadt, Germany.	larlus@cs.tu-darmstadt.de; jakob.verbeek@inria.fr; frederic.jurie@unicaen.fr						[Anonymous], 2007, PASCAL VISUAL OBJECT; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; BORENSTEIN E, 2006, IEEE C COMP VIS PATT; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Breiman L., 2017, CLASSIFICATION REGRE; Cao LL, 2007, IEEE I CONF COMP VIS, P1080; Csurka G., 2008, BRIT MACH VIS C, P221; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; Fergus R, 2005, IEEE I CONF COMP VIS, P1816; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950; Jurie F, 2005, IEEE I CONF COMP VIS, P604; Kumar M., 2005, IEEE C COMP VIS PATT; Kumar S, 2006, INT J COMPUT VISION, V68, P179, DOI 10.1007/s11263-006-7007-9; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; LARLUS D, 2008, IEEE C COMP VIS PATT; LARLUS D, 2006, BRIT MACH VIS C; Leibe B., 2003, BRIT MACH VIS C; LEVIN A, 2006, EUR C COMP VIS; Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Moosmann F, 2008, IEEE T PATTERN ANAL, V30, P1632, DOI 10.1109/TPAMI.2007.70822; Neal RM, 1998, 9815 U TOR DEP STAT; ORBANZ P, 2006, EUR C COMP VIS; PANTOFARU C, 2008, EUR C COMP VIS; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; SHOTTON S, 2008, IEEE C COMP VIS PATT; Storkey AJ, 2003, IEEE T PATTERN ANAL, V25, P859, DOI 10.1109/TPAMI.2003.1206515; Sudderth EB, 2008, INT J COMPUT VISION, V77, P291, DOI 10.1007/s11263-007-0069-5; van de Weijer J., 2006, EUR C COMP VIS; Verbeek J., 2008, ADV NEURAL INFORM PR; Verbeek J, 2007, PROC CVPR IEEE, P982; Winn J, 2005, IEEE I CONF COMP VIS, P1800; WINN J, 2006, IEEE INT C COMP VIS	37	31	33	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN 10	2010	88	2			SI		238	253		10.1007/s11263-009-0245-x	http://dx.doi.org/10.1007/s11263-009-0245-x			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	573YX		Green Submitted			2022-12-18	WOS:000275955400006
J	Kwon, H; Nasrabadi, NM				Kwon, Heesung; Nasrabadi, Nasser M.			Kernel spectral matched filter for hyperspectral imagery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	3rd Workshop on Object Tracking and Classification Beyond the Visible Spectrum	JUN 22, 2006	New York, NY	IEEE		matched filter; hyperspectral; kernel; nonlinear detection; target detection	DETECTOR	In this paper a kernel-based nonlinear spectral matched filter is introduced for target detection in hyperspectral imagery, which is implemented by using the ideas in kernel-based learning theory. A spectral matched filter is defined in a feature space of high dimensionality, which is implicitly generated by a nonlinear mapping associated with a kernel function. A kernel version of the matched filter is derived by expressing the spectral matched filter in terms of the vector dot products form and replacing each dot product with a kernel function using the so called kernel trick property of the Mercer kernels. The proposed kernel spectral matched filter is equivalent to a nonlinear matched filter in the original input space, which is capable of generating nonlinear decision boundaries. The kernel version of the linear spectral matched filter is implemented and simulation results on hyperspectral imagery show that the kernel spectral matched filter outperforms the conventional linear matched filter.	USA, Res Lab, ATTN, AMSRD,ARL,SE, Adelphi, MD 20783 USA	United States Department of Defense; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL)	Kwon, H (corresponding author), USA, Res Lab, ATTN, AMSRD,ARL,SE, 2800 Powder Mill Rd, Adelphi, MD 20783 USA.	hkwon@arl.army.mil; nnasraba@arl.army.mil						[Anonymous], 2002, LEARNING KERNELS; Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980; CAPON J, 1969, P IEEE, V57, P1408, DOI 10.1109/PROC.1969.7278; Chang C.-I., 2003, HYPERSPECTRAL IMAGIN, DOI 10.1007/978-1-4419-9170-6; Harsanyi J.C, 1993, THESIS U MARYLAND BA; Johnson D.H., 1993, ARRAY SIGNAL PROCESS; Kraut S, 1999, IEEE T SIGNAL PROCES, V47, P2538, DOI 10.1109/78.782198; Kraut S, 2001, IEEE T SIGNAL PROCES, V49, P1, DOI 10.1109/78.890324; Kwon H, 2005, IEEE T GEOSCI REMOTE, V43, P388, DOI 10.1109/TGRS.2004.841487; Kwon H, 2004, IEEE IJCNN, P717, DOI 10.1109/IJCNN.2004.1380005; Manolakis D, 2000, PROC SPIE, V4049, P2, DOI 10.1117/12.410332; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; ROBEY FC, 1992, IEEE T AERO ELEC SYS, V28, P208, DOI 10.1109/7.135446; Ruiz A, 2001, IEEE T NEURAL NETWOR, V12, P16, DOI 10.1109/72.896793; Scharf LL, 1991, STAT SIGNAL PROCESSI; Scholkopf N., 1999, NEURAL COMPUT, P1299; STRANG G, 1986, LINEAR ALGEBRA ITS A; Van Veen B. D., 1988, IEEE ASSP Magazine, V5, P4, DOI 10.1109/53.665	19	31	35	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2007	71	2					127	141		10.1007/s11263-006-6689-3	http://dx.doi.org/10.1007/s11263-006-6689-3			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	098DN					2022-12-18	WOS:000241501300002
J	Feddern, C; Weickert, J; Burgeth, B; Welk, M				Feddern, Christian; Weickert, Joachim; Burgeth, Bernhard; Welk, Martin			Curvature-driven PDE methods for matrix-valued images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	2nd IEEE Workshop on Variational, Geometric and Level Set Methods held in Conjunction with the IEEE International Conference on Computer Vision	OCT, 2003	Nice, FRANCE	IEEE, French Natl Inst Res Comp Sci & Control, Siemens Corp Res, Imaging & Visualizat Dept		DT-MRI; denoising; segmentation; edge detection; structure tensor; mean curvature motion; self-snakes; active contours	EDGE-DETECTION; HEAT-EQUATION; DIFFUSION; TEXTURE; REGULARIZATION; SEGMENTATION; ENHANCEMENT; COLOR; FLOW	Matrix-valued data sets arise in a number of applications including diffusion tensor magnetic resonance imaging (DT-MRI) and physical measurements of anisotropic behaviour. Consequently. there arises the need to filter and segment such tensor fields. In order to detect edge-like structures in tensor fields, we first generalise Di Zenzo's concept of a structure tensor for vector-valued images to tensor-valued data. This structure tensor allows us to extend scalar-valued mean curvature motion and self-snakes to the tensor setting. We present both two-dimensional and three-dimensional formulations, and we prove that these filters maintain positive semidefiniteness if the initial matrix data are positive semidefinite. We give an interpretation of tensorial mean curvature motion as a process for which the Corresponding curve evolution of each generalised level line is the gradient descent of its total length. Moreover, we propose a geodesic active contour model for segmenting tensor fields and interpret it as a minimiser of a suitable energy functional with a metric induced by the tensor image. Since tensorial active contours incorporate information from all channels, they give a contour representation that is highly robust under noise. Experiments oil three-dimensional DT-MRI data and an indefinite tensor field from fluid dynamics show that the proposed methods inherit the essential properties of their scalar-valued counterparts.	Univ Saarland, Math Image Anal Grp, Fac Math & Comp Sci, D-66041 Saarbrucken, Germany	Saarland University	Weickert, J (corresponding author), Univ Saarland, Math Image Anal Grp, Fac Math & Comp Sci, Bldg E11, D-66041 Saarbrucken, Germany.	weickert@mia.uni-saarland.de						ALVAREZ L, 1992, SIAM J NUMER ANAL, V29, P845, DOI 10.1137/0729052; Ben-Shahar O, 2003, IEEE T PATTERN ANAL, V25, P401, DOI 10.1109/TPAMI.2003.1190568; BIGUN J, 1991, IEEE T PATTERN ANAL, V13, P775, DOI 10.1109/34.85668; Brox T, 2003, LECT NOTES COMPUT SC, V2756, P353; BROX T, 2002, LECT NOTES COMPUTER, V2449, P446; Burgeth B, 2004, LECT NOTES COMPUT SC, V2034, P155; Campbell JSW, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING, PROCEEDINGS, P505, DOI 10.1109/ISBI.2002.1029305; Caselles V, 2000, J MATH IMAGING VIS, V12, P109, DOI 10.1023/A:1008310305351; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; CHAMBOLLE A, 1994, IEEE IMAGE PROC, P16, DOI 10.1109/ICIP.1994.413266; Chung DH, 2000, IEEE SIGNAL PROC LET, V7, P241, DOI 10.1109/97.863143; COULON O, 2001, LECT NOTES COMPUTER, V2082, P92; CUMANI A, 1991, CVGIP-GRAPH MODEL IM, V53, P40, DOI 10.1016/1049-9652(91)90018-F; DERVIEUX A, 1979, LECT NOTES MATH, V771, P145; DIZENZO S, 1986, COMPUT VISION GRAPH, V33, P116, DOI 10.1016/0734-189X(86)90223-9; FEDDERN C, 2003, P 2 IEEE WORKSH GEOM, P65; GAGE M, 1986, J DIFFER GEOM, V23, P69; GAGE M, 1986, CONT MATH, V23, P51; GERIG G, 1992, IEEE T MED IMAGING, V11, P221, DOI 10.1109/42.141646; Granlund G.H., 1995, SIGNAL PROCESSING CO; GRAYSON MA, 1987, J DIFFER GEOM, V26, P285; GUICHARD F, 1997, IMA C SERIES, V0063, P00525; HAHN K, 2001, LECT NOTES COMPUTER, V2208, P195; HUISKEN G, 1984, J DIFFER GEOM, V20, P237; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; Kimia BB, 1996, COMPUT VIS IMAGE UND, V64, P305, DOI 10.1006/cviu.1996.0062; Kimmel R, 2002, J VIS COMMUN IMAGE R, V13, P238, DOI 10.1006/jvci.2001.0501; Kimmel R., 2003, NUMERICAL GEOMETRY I; KRAMER HP, 1975, PATTERN RECOGN, V7, P53, DOI 10.1016/0031-3203(75)90013-8; KRIVA Z., 2000, P ALGORITMY 2000 C S, P174; MALLADI R, 1993, P SOC PHOTO-OPT INS, V2031, P246, DOI 10.1117/12.146630; Medioni G., 2000, COMPUTATIONAL FRAMEW; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053; Osher S., 2002, APPL MATH SCI, V153; Osher S., 2003, GEOMETRIC LEVEL SET; Parker GJM, 2000, J MAGN RESON IMAGING, V11, P702, DOI 10.1002/1522-2586(200006)11:6<702::AID-JMRI18>3.0.CO;2-A; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Pierard G E, 1996, Skin Res Technol, V2, P3, DOI 10.1111/j.1600-0846.1996.tb00047.x; Poupon C, 1998, LECT NOTES COMPUT SC, V1496, P489, DOI 10.1007/BFb0056234; Preusser T, 2002, SIAM J APPL MATH, V62, P1772; RAO AR, 1991, CVGIP-GRAPH MODEL IM, V53, P157, DOI 10.1016/1049-9652(91)90059-S; SAPIRO G, 1995, IEEE T PATTERN ANAL, V17, P67, DOI 10.1109/34.368150; Sapiro G, 1996, IEEE T IMAGE PROCESS, V5, P1582, DOI 10.1109/83.541429; Sapiro G., 2001, GEOMETRIC PARTIAL DI; SAPIRO G, 1996, P IEEE INT C IM PROC, V1, P817; Schwarz H.-R., 1989, NUMERICAL ANAL COMPR; Sethian J. A., 1999, LEVEL SET METHODS FA; Tschumperle D, 2003, PROC CVPR IEEE, P651; Tschumperle D, 2002, INT J COMPUT VISION, V50, P237, DOI 10.1023/A:1020870207168; VEMURI B, 2001, P 1 IEEE WORKSH VAR, P73; WANG Z, 2004, P 2004 IEEE COMP SOC; Weickert J, 1999, IMAGE VISION COMPUT, V17, P201, DOI 10.1016/S0262-8856(98)00102-4; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; WEICKERT J, 2002, CONT MATH, V313, P251; Welk M, 2003, LECT NOTES COMPUT SC, V2781, P17; Westin CF, 1999, LECT NOTES COMPUT SC, V1679, P441; Whitaker RT, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P142, DOI 10.1109/ICIP.2001.958071; Zhukov L, 2003, J ELECTRON IMAGING, V12, P125, DOI 10.1117/1.1527628	61	31	33	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	1					93	107		10.1007/s11263-006-6854-8	http://dx.doi.org/10.1007/s11263-006-6854-8			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	063QH		Green Submitted			2022-12-18	WOS:000239034100007
J	Paris, S; Sillion, FX; Quan, L				Paris, S; Sillion, FX; Quan, L			A surface reconstruction method using global graph cut optimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	6th Asian Conference on Computer Vision	JAN 28-30, 2004	Cheju Isl, SOUTH KOREA			graph flow; graph cut; 3D reconstruction from calibrated cameras; discontinuities; self-occlusions; occlusions; global minimum		Surface reconstruction from multiple calibrated images has been mainly approached using local methods, either as a continuous optimization problem driven by level sets, or by discrete volumetric methods such as space carving. We propose a direct surface reconstruction approach which starts from a continuous geometric functional that is minimized up to a discretization by a global graph-cut algorithm operating on a 3D embedded graph. The method is related to the stereo disparity computation based on graph-cut formulation, but fundamentally different in two aspects. First, existing stereo disparity methods are only interested in obtaining layers of constant disparity, while we focus on high resolution surface geometry. Second, most of the existing graph-cut algorithms only reach approximate solutions, while we guarantee a global minimum. The whole procedure is consistently incorporated into a voxel representation that handles both occlusions and discontinuities. We demonstrate our algorithm on real sequences, yielding remarkably detailed Surface geometry up to 1/10th of a pixel.	MIT, CSAIL, Cambridge, MA 02139 USA; ARTIS, GRAVIR, IMAG, INRIA, F-38334 Saint Ismier, France; Hong Kong Univ Sci & Technol, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China	Massachusetts Institute of Technology (MIT); Inria; Hong Kong University of Science & Technology	Paris, S (corresponding author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	sparis@csail.mit.edu; francois.sillion@imag.fr; quan@cs.ust.hk						Ahuja R. K., 1993, NETWORK FLOWS THEORY; [Anonymous], EUR C COMP VIS; Aubert G., 2002, APPL MATH SCI; BLACK MJ, 1998, IEEE T IMAGE PROCESS; BLAKE A, 1987, ARTIFICIAL INTELLIGE; Boykov Y., 2004, IEEE T PATTERN ANAL; Boykov Y., 2003, INT C COMP VIS; Boykov Y., 2001, IEEE T PATTERN ANAL; BUEHLER C, 2002, EUR C COMP VIS ECCV; Caselles Vicent, 1997, INT J COMPUTER VISIO; CATTE F, 1992, SIAM J NUMERICAL ANA; Cherkassky BV, 1997, ALGORITHMICA, V19, P390, DOI 10.1007/PL00009180; FAUGERAS O, 1998, T IMAGE PROCESSING; Ford L. R. J., 1962, FLOWS NETWORKS; ISHIKAWA H, 1998, EUR C COMP VIS ECCV; Ishikawa H., 2003, IEEE T PATTERN ANAL; ISHIKAWA H, 2000, THESIS NEW YORK U; Isidoro J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1335; Kim J., 2003, INT C COMP VIS; Kolmogorov V., 2001, INT C COMP VIS; Kutulakos K. N., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P307, DOI 10.1109/ICCV.1999.791235; KUTULAKOS KN, 2000, EUR C COMP VIS ECCV; LHUILLIER M, 2002, EUR C COMP VIS ECCV; LHUILLIER M, 2003, P INT C COMP VIS IEE; Museth K, 2002, ACM T GRAPHIC, V21, P330, DOI 10.1145/566570.566585; Osher Stanley, 1988, J COMPUTATIONAL PHYS; Paris S, 2003, GRAPH MODELS, V65, P222, DOI 10.1016/S1524-0703(03)00041-9; Roy S, 1999, INT J COMPUT VISION, V34, P147, DOI 10.1023/A:1008192004934; Roy Sebastien, 1998, IEEE INT C COMP VIS; SAITO H, 1999, CVPR, P49; Seitz S. M., 1999, IJCV; SETHIAN JA, 1999, LEVEL SETS METHODS F; SLABAUGH G, 2000, 3D STRUCT IM SMILE 2; SLABAUGH G, 2001, GRAPHICS 01; SZELISKI R, 1998, INT C COMP VIS ICCV; Terzopoulos Demetri, 1988, ARTIFICIAL INTELLIGE; ULVKLO M, 1998, SPIE VISUAL INFORM P; VEKSLER O, 1999, THESIS CORNELL U	38	31	41	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2006	66	2					141	161		10.1007/s11263-005-3953-x	http://dx.doi.org/10.1007/s11263-005-3953-x			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	022CY		Green Submitted			2022-12-18	WOS:000236033500004
J	Rosenhahn, B; Perwass, C; Sommer, G				Rosenhahn, B; Perwass, C; Sommer, G			Pose estimation of 3D free-form contours	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						2D-3D pose estimation; free-form contours; algebraic curves; conformal geometry; geometric algebra	FOURIER DESCRIPTORS; RECOGNITION; KINEMATICS	In this article we discuss the 2D-3D pose estimation problem of 3D free-form contours. In our scenario we observe objects of any 3D shape in an image of a calibrated camera. Pose estimation means to estimate the relative position and orientation (containing a rotation R and translation T) of the 3D object to the reference camera system. The fusion of modeling free-form contours within the pose estimation problem is achieved by using the conformal geometric algebra. The conformal geometric algebra is a geometric algebra which models entities as stereographically projected entities in a homogeneous model. This leads to a linear description of kinematics on the one hand and projective geometry on the other hand. To model free-form contours in the conformal framework we use twists to model cycloidal curves as twist-depending functions and interpret n-times nested twist generated curves as functions generated by 3D Fourier descriptors. This means, we use the twist concept to apply a spectral domain representation of 3D contours within the pose estimation problem. We will show that twist representations of objects can be numerically efficient and easily be applied to the pose estimation problem. The pose problem itself is formalized as implicit problem and we gain constraint equations, which have to be fulfilled with respect to the unknown rigid body motion. Several experiments visualize the robustness and real-time performance of our algorithms.	Univ Auckland CITR, Dept Comp Sci, Auckland, New Zealand; Univ Kiel, Inst Informat & Prakt Math, D-24105 Kiel, Germany	University of Auckland; University of Kiel	Rosenhahn, B (corresponding author), Univ Auckland CITR, Dept Comp Sci, Tamaki Campus,Private Bag 92019, Auckland, New Zealand.	bros028@cs.auckland.ac.nz; chp@ks.informatik.uni-kiel.de; gs@ks.infonnatik.uni-kiel.de	Rohlf, F J/A-8710-2008					Arbter K., 1991, Informationstechnik - IT, V33, P19; Arbter K., 1989, From Pixels to Features. Proceedings of a Workshop, P153; ARBTER K, 1990, IEEE T PATTERN ANAL, V12, P640, DOI 10.1109/34.56206; ARBTER K, 1990, THESIS TU HAMBURG HA; Bayro-Corrochano E, 2000, J MATH IMAGING VIS, V13, P79, DOI 10.1023/A:1026567812984; Besl P J., 1990, MACHINE VISION 3 DIM, P25; Blaschke W., 1960, MATH MONOGRAPHIEN, V4, P4; Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581; Campbell RJ, 2001, COMPUT VIS IMAGE UND, V81, P166, DOI 10.1006/cviu.2000.0889; Chiuso A., 1998, Confluence of Vision and Control, P90; *CLU LIB, 2001, C LIB CLIFF ALG; CZOPF A, 1999, PERIOD POLYTECH, V43, P19; DRUMMOND T, 2000, 6 EUR C COMP VIS ECC, P20; FAUGERAS O, 1995, J OPTICAL SOC AM, V12; Gallier J, 2001, GEOMETRIC METHODS AP; GRANLUND GH, 1972, IEEE T COMPUT, VC 21, P195, DOI 10.1109/TC.1972.5008926; Grimson W. E. L., 1990, OBJECT RECOGNITION C; HESTENES D, 1991, ACTA APPL MATH, V23, P25, DOI 10.1007/BF00046919; HESTENES D, 1994, NEURAL NETWORKS, V7, P65, DOI 10.1016/0893-6080(94)90056-6; Hestenes D, 2012, CLIFFORD ALGEBRA GEO; Kaminski JY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P181, DOI 10.1109/ICCV.2001.937622; KAUPPINEN H, 1995, IEEE T PATTERN ANAL, V17, P201, DOI 10.1109/34.368168; KLINGSPOHR H, 1997, LNCS, V1296, P718; KRIEGMAN DJ, 1992, LECT NOTES COMPUT SC, V588, P829; LEE X, VISUAL DICT SPECIAL; LIN CS, 1987, PATTERN RECOGN, V20, P535, DOI 10.1016/0031-3203(87)90080-X; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; LOWE DG, 1980, P ARPA IM UND WORKSH, P121; McCarthy J.M., 1990, INTRO THEORETICAL KI; Murray R. M., 1994, MATH INTRO ROBOTIC M; Needham T., 1997, VISUAL COMPLEX ANAL; OCONNOR JJ, FAMOUS CURVES INDEX; PERWASS C, 1999, CUEDFINFENGTR347; PERWASS C, 2003, 0310 C ALBR U KIEL I; PHONG TQ, 1995, INT J COMPUT VISION, V15, P225, DOI 10.1007/BF01451742; REISS TH, 1993, LNCS, V676; ROONEY J, 1978, ENVIRON PLANN B, V5, P45, DOI 10.1068/b050045; Rosenhahn B, 2002, APPLICATIONS OF GEOMETRIC ALGEBRA IN COMPUTER SCIENCE AND ENGINEERING, P373; Rosenhahn B, 2000, LECT NOTES COMPUT SC, V1888, P284; ROSENHAHN B, 2003, 0308 C ALBR U KIEL I; ROSENHAHN B, 2002, 0206 U KIEL; RUSINKIEWICZ S, 2001, 3 INT C 3D DIG IM MO; SELIG JM, 2000, GEOMETRIC FDN ROBOTI; Sommer G., 2001, GEOMETRIC COMPUTING; STARK K, 1996, 9610 TUDFFI; TELLO R, 1995, IEEE T SYST MAN CYB, V25, P861, DOI 10.1109/21.376498; Ude A, 1999, ROBOT AUTON SYST, V28, P163, DOI 10.1016/S0921-8890(99)00014-7; WALKER MW, 1991, CVGIP-IMAG UNDERSTAN, V54, P358, DOI 10.1016/1049-9660(91)90036-O; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949; ZANG Z, 1999, IJCV INT J COMPUTER, V13, P119; Zerroug M, 1996, IMAGE UNDERSTANDING WORKSHOP, 1996 PROCEEDINGS, VOLS I AND II, P831; [No title captured]; [No title captured]	53	31	33	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY-JUN	2005	62	3					267	289		10.1007/s11263-005-4883-3	http://dx.doi.org/10.1007/s11263-005-4883-3			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	906ME		Green Submitted			2022-12-18	WOS:000227645500004
J	Anandan, P; Irani, M				Anandan, P; Irani, M			Factorization with uncertainty	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						factorization; structure from motion; directional uncertainty	MOTION; ALGORITHM; SHAPE	Factorization using Singular Value Decomposition (SVD) is often used for recovering 3D shape and motion from feature correspondences across multiple views. SVD is powerful at finding the global solution to the associated least-square-error minimization problem. However, this is the correct error to minimize only when the x and y positional errors in the features are uncorrelated and identically distributed. But this is rarely the case in real data. Uncertainty in feature position depends on the underlying spatial intensity structure in the image, which has strong directionality to it. Hence, the proper measure to minimize is covariance-weighted squared-error (or the Mahalanobis distance). In this paper, we describe a new approach to covariance-weighted factorization, which can factor noisy feature correspondences with high degree of directional uncertainty into structure and motion. Our approach is based on transforming the raw-data into a covariance-weighted data space, where the components of noise in the different directions are uncorrelated and identically distributed. Applying SVD to the transformed data now minimizes a meaningful objective function in this new data space. This is followed by a linear but suboptimal second step to recover the shape and motion in the original data space. We empirically show that our algorithm gives very good results for varying degrees of directional uncertainty. In particular, we show that unlike other SVD-based factorization algorithms, our method does not degrade with increase in directionality of uncertainty, even in the extreme when only normal-flow data is available. It thus provides a unified approach for treating corner-like points together with points along linear structures in the image.	Microsoft Corp, Redmond, WA 98052 USA; Weizmann Inst Sci, Dept Appl Math & Comp Sci, IL-76100 Rehovot, Israel	Microsoft; Weizmann Institute of Science	Anandan, P (corresponding author), Microsoft Corp, 1 Microsoft Way, Redmond, WA 98052 USA.							AGUIAR PMQ, 1999, IEEE COMPUTER VISI A, V9, P178; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; Anandan P, 2000, LECT NOTES COMPUT SC, V1842, P907; Ben-Ezra M, 2000, COMPUT VIS IMAGE UND, V78, P32, DOI 10.1006/cviu.1999.0826; CRIMINISI A, 1998, EUR C COMP VIS FREIB; Hanna K. J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P357, DOI 10.1109/ICCV.1993.378192; Irani M, 1997, IEEE T PATTERN ANAL, V19, P268, DOI 10.1109/34.584105; Irani M, 2002, INT J COMPUT VISION, V48, P173, DOI 10.1023/A:1016372015744; Irani M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P626, DOI 10.1109/ICCV.1999.791283; IRANI M, 1996, EUR C COMP VIS, P17; IRANI M, 1998, EUR C COMP VIS FREIB; IRANI M, 2000, ECCV, P539; IRANI M, 1999, VISION ALGORITHMS TH; Kanatani K., 1996, STAT OPTIMIZATION GE; KUMAR R, 1994, INT C PATT RECOG, P685, DOI 10.1109/ICPR.1994.576402; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; Matei B, 2000, PROC CVPR IEEE, P18, DOI 10.1109/CVPR.2000.854727; MORRIS D, 1999, VIS ALG THEOR PRACT, P33; Morris DD, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P696, DOI 10.1109/ICCV.1998.710793; Oliensis J, 1999, INT J COMPUT VISION, V34, P163, DOI 10.1023/A:1008139920864; Oliensis J, 2001, IEEE T PATTERN ANAL, V23, P546, DOI 10.1109/34.927457; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; Quan L, 1996, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.1996.517164; SAWHNEY H, 1994, IEEE C COMP VIS PATT; SHAPIRO LS, 1995, AFFINE ANAL IMAGE SE; SHASHUA A, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P483, DOI 10.1109/CVPR.1994.323870; Stein GP, 2000, IEEE T PATTERN ANAL, V22, P992, DOI 10.1109/34.877522; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TRIGGS W, 2000, EUR C COMP VIS DUBL, P522; Van Huffel S., 1991, TOTAL LEAST SQUARES	32	31	35	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP-OCT	2002	49	2-3					101	116		10.1023/A:1020137420717	http://dx.doi.org/10.1023/A:1020137420717			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	590TA					2022-12-18	WOS:000177837100002
J	Cohen, I; Herlin, I				Cohen, I; Herlin, I			Non uniform multiresolution method for optical flow and phase portrait models: Environmental applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						non uniform multiresolution; optical flow; non quadratic regularization; finite element method; adaptive mesh; phase portrait; flow pattern classification; ocean and atmospheric circulation	FIELDS	In this paper we define a complete framework for processing large image sequences for a global monitoring of short range oceanographic and atmospheric processes. This framework is based on the use of a non quadratic regularization technique for optical flow computation that preserves flow discontinuities. We also show that using an appropriate tessellation of the image according to an estimate of the motion field can improve optical flow accuracy and yields more reliable flows. This method defines a non uniform multiresolution approach for coarse to fine grid generation. It allows to locally increase the resolution of the grid according to the studied problem. Each added node refines the grid in a region of interest and increases the numerical accuracy of the solution in this region. We make use of such a method for solving the optical flow equation with a non quadratic regularization scheme allowing the computation of optical flow field while preserving its discontinuities. The second part of the paper deals with the interpretation of the obtained displacement field. For this purpose a phase portrait model used along with a new formulation of the approximation of an oriented flow field allowing to consider arbitrary polynomial phase portrait models for characterizing salient flow features. This new framework is used for processing oceanographic and atmospheric image sequences and presents an alternative to complex physical modeling techniques.	Univ So Calif, Inst Robot & Intelligent Syst, Los Angeles, CA 90089 USA; Inst Natl Rech Informat & Automat, Projet AIR, F-78153 Le Chesnay, France	University of Southern California	Cohen, I (corresponding author), Univ So Calif, Inst Robot & Intelligent Syst, Los Angeles, CA 90089 USA.	icohen@iris.usc.edu; Isabelle.Herlin@inria.fr						BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BERROIR JP, 1995, EOS SPIE SATELLITE R, V2; Black M. J., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P138; Black M. J., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P296, DOI 10.1109/CVPR.1991.139705; Ciarlet P.G., 1987, HDB NUMERICAL ANAL; Cohen I, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P396, DOI 10.1109/ICCV.1998.710749; COHEN I, 1998, IEEE P COMP VIS PATT; COHEN I, 1996, P 4 EUR C COMP VIS 1; FORD RM, 1994, CVGIP-GRAPH MODEL IM, V56, P75, DOI 10.1006/cgip.1994.1007; FORD RM, 1993, IEEE P COMP VIS PATT, P644; Glowinski R., 2015, VARIATIONAL METHODS; Golub G. H., 1996, MATRIX COMPUTATIONS; HERLIN I, 1996, IN PRESS J VISUALIZA; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; IRANI M, 1992, LECT NOTES COMPUT SC, V588, P282; KIMIA BB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P113, DOI 10.1109/CVPR.1994.323817; LEFSCHETZ S, 1976, DIFFERENTIAL EQUATIO; LIONS PL, 1990, 9046 CEREMADE U PAR; Memin E, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P933, DOI 10.1109/ICCV.1998.710828; MULLER JR, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P953, DOI 10.1109/CVPR.1994.323932; NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833; Nemytskii V. V., 1989, QUALITATIVE THEORY D; NIESSEN WJ, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P92, DOI 10.1109/CVPR.1994.323815; OTTE M, 1994, P 3 EUR C COMP VIS S, P51; RAO AR, 1992, IEEE T PATTERN ANAL, V14, P693, DOI 10.1109/34.142908; ROSEN JB, 1960, J SOC IND APPL MATH, V8, P181, DOI 10.1137/0108011; RUDIN LI, 1992, ECOLES CEA EDF INRIA, P149; Samet H, 1989, DESIGN ANAL SPATIAL; SCHNORR C, 1992, INT J COMPUT VISION, V8, P153, DOI 10.1007/BF00127172; SHU CF, 1992, IEEE P COMP VIS PATT, P673; SZELISKI R, 1995, MOTION ESTIMATION QU; VASILESCU M, 1992, IEEE P COMP VIS PATT, P829; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; YAHIA H, 1995, 2701 INRIA; ZHONG JL, 1994, IEEE P COMP VIS PATT, P310	35	31	31	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1999	33	1					29	49		10.1023/A:1008161130332	http://dx.doi.org/10.1023/A:1008161130332			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	229HL					2022-12-18	WOS:000082187800002
J	WENG, J				WENG, J			IMAGE MATCHING USING THE WINDOWED FOURIER PHASE	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CATS VISUAL-CORTEX; BINOCULAR ORGANIZATION; APPARENT MOTION; COMPLEX CELLS; STEREO; COMPUTATION; INFORMATION; VISION; MODEL	A theoretical framework is presented in which windowed Fourier phase (WFP) is introduced as the primary matching primitive. Zero-crossings and peaks correspond to special values of the phase. The WFP is quasi-linear and dense; and its spatial period and slope are controlled by the scale. This framework has the following important characteristics: 1) matching primitives are available almost everywhere to convey dense disparity information in every channel, either coarse or fine; 2) the false-target problem is significantly mitigated; 3) the matching is easier, uniform, and can be performed by a network suitable for parallel computer architecture; 4) the matching is fast since very few iterations are needed. In fact, the WFP is so informative that the original signal can be uniquely determined up to a multiplicative constant by the WFP in any channel. The use of phase as matching primitive is also supported by some existing psychophysical and neurophysiological studies. An implementation of the proposed theory has shown good results from synthesized and natural images.			WENG, J (corresponding author), MICHIGAN STATE UNIV, DEPT COMP SCI, E LANSING, MI 48824 USA.							AYACHE N, 1987, INT J COMPUT VISION, V1, P107, DOI 10.1007/BF00123161; BAKER HH, 1981, 7TH P INT JOINT C AR, P631; BRACEWELL RN, 1986, FOURIER TRANSFORMATI; BRADDICK O, 1974, VISION RES, V14, P519, DOI 10.1016/0042-6989(74)90041-8; BRADDICK OJ, 1980, PHILOS T ROY SOC B, V290, P137, DOI 10.1098/rstb.1980.0087; BURT PJ, 1989, IEEE MOTION WORKSHOP, P2; CURTIS SR, 1985, IEEE T ACOUST SPEECH, V33, P643, DOI 10.1109/TASSP.1985.1164589; DRESCHLER L, 1982, COMPUT VISION GRAPH, V20, P199, DOI 10.1016/0146-664X(82)90081-8; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; FOSTER KH, 1983, J PHYSL LOND, V345; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; GLAZER F, 1983, 1ST P IEEE C COMP VI, P432; GRIMSON WEL, 1981, IMAGES SURFACES; HEEGER DJ, 1987, INT J COMPUT VISION, V2, P279; HILDRETH E, 1983, MEASUREMENT VISUAL M; HOFF W, 1989, IEEE T PATTERN ANAL, V11, P121, DOI 10.1109/34.16709; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HUANG TS, 1975, IMAGE SEQUENCE PROCE; HUANG TT, 1988, IEEE T ACOUST SPEECH, V36, P1292, DOI 10.1109/29.1657; HUMMEL R, 1989, IEEE T ACOUST SPEECH, V37, P2111, DOI 10.1109/29.45555; JENKIN MRM, 1991, CVGIP-IMAG UNDERSTAN, V53, P14, DOI 10.1016/1049-9660(91)90002-7; JEPSON AD, 1989, JUN P IEEE C COMP VI, P398; JULESZ B, 1975, PERCEPTION, V4, P125, DOI 10.1068/p040125; JULESZ B, 1960, AT&T TECH J, V39, P1125, DOI 10.1002/j.1538-7305.1960.tb03954.x; JULESZ B, 1971, F CYCLOPEAN PERCEPTI; KASS M, 1983, P DARPA IMAGE UNDERS; LANGLEY K, 1990, 1ST P EUR C COMP VIS, P315; LIM HL, 1987, P DARPA IMAGE UNDERS; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; MAYHEW JEW, 1981, ARTIF INTELL, V17, P349, DOI 10.1016/0004-3702(81)90029-1; MORAVEC HP, 1980, 340 STANF ART INT LA; NIELSON KPK, 1983, MIT740 ART INT MEM; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; OHZAWA I, 1986, J NEUROPHYSIOL, V56, P243, DOI 10.1152/jn.1986.56.1.243; OHZAWA I, 1986, J NEUROPHYSIOL, V56, P221, DOI 10.1152/jn.1986.56.1.221; OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022; POGGIO GF, 1984, ANNU REV NEUROSCI, V7, P379, DOI 10.1146/annurev.ne.07.030184.002115; POGGIO T, 1985, NATURE, V317, P638; POLLEN DA, 1982, VISION RES, V22, P101, DOI 10.1016/0042-6989(82)90172-9; SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114; SPITZER H, 1985, J NEUROPHYSIOL, V53, P1244, DOI 10.1152/jn.1985.53.5.1244; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; Waxman A. M., 1984, Proceedings of the Workshop on Computer Vision: Representation and Control, P49; WENG J, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P200; WENG J, 1992, MAR P INT C AC SPEEC, V4, P145; WENG J, 1988, DEC P INT C COMP VIS, P64; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P806, DOI 10.1109/34.149592; WENG JY, 1993, IEEE T SIGNAL PROCES, V41, P657, DOI 10.1109/78.193207; WILSON HR, 1979, VISION RES, V19, P19, DOI 10.1016/0042-6989(79)90117-2	50	31	35	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1993	11	3					211	236		10.1007/BF01469343	http://dx.doi.org/10.1007/BF01469343			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MD681					2022-12-18	WOS:A1993MD68100001
J	TSOTSOS, JK				TSOTSOS, JK			A COMPLEXITY LEVEL ANALYSIS OF IMMEDIATE VISION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									UNIV TORONTO,DEPT COMP SCI,TORONTO MS5 1A4,ONTARIO,CANADA	University of Toronto			Tsotsos, John K/G-3436-2011; Tsotsos, John/N-1131-2019	Tsotsos, John/0000-0002-8621-9147				BALLARD DH, 1986, BEHAV BRAIN SCI, V9, P67, DOI 10.1017/S0140525X00021555; BALLARD DH, 1983, NATURE, V306, P21, DOI 10.1038/306021a0; Barrow H., 1978, COMPUT VIS SYST, V2, P2; Corbeil J. C., 1986, STODDART VISUAL DICT; COWEY A, 1979, Q J EXP PSYCHOL, V31, P1, DOI 10.1080/14640747908400703; DANIEL PM, 1961, J PHYSIOL-LONDON, V159, P203, DOI 10.1113/jphysiol.1961.sp006803; FELDMAN JA, 1985, COGNITIVE SCI, V9, P1; FELDMAN JA, 1982, COGNITIVE SCI, V6, P205, DOI 10.1207/s15516709cog0603_1; HINTON GE, 1981, 7TH P INT JOINT C AR, P1088; HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085; KIROUSIS L, 1985, 26TH ANN S F COMP SC; MACKWORTH AK, 1985, ARTIF INTELL, V25, P65, DOI 10.1016/0004-3702(85)90041-4; Marr D., 1982, VISION; MORAN J, 1985, SCIENCE, V229, P782, DOI 10.1126/science.4023713; NAKAYAMA K, 1986, NATURE, V320, P264, DOI 10.1038/320264a0; Neisser U., 1967, COGNITIVE PSYCHOL; POGGIO T, 1982, MIT AI683 MEM; RICHARDS W, 1982, MIT AI660 MEM; Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318; RUMELHART DE, 1986, PARALLEL DISTRIBUTED, P110; STENSAAS SS, 1974, J NEUROSURG, V40, P747, DOI 10.3171/jns.1974.40.6.0747; TREISMAN A, 1985, COMPUT VISION GRAPH, V31, P156, DOI 10.1016/S0734-189X(85)80004-9; TSOTSOS J, 1980, CSRITR114 U TOR DEP; Tsotsos J. K., 1987, ENCY ARTIFICIAL INTE, P389; TSOTSOS JK, 1986, BEHAV BRAIN SCI, V9, P106, DOI 10.1017/S0140525X00021737; TSOTSOS JK, 1987, VISION BRAIN COOPERA, P361; TSOTSOS JK, 1985, COMPUT INTELL, V1, P16; UHR L, 1972, IEEE T COMPUT, VC 21, P758, DOI 10.1109/T-C.1972.223579; ULLMAN S, 1983, MIT AI723 MEM; VANESSEN DC, 1978, J PHYSIOL-LONDON, V277, P193; VANESSEN DC, 1983, TRENDS NEUROSCI, P370; ZUCKER SW, 1985, BEHAV BRAIN SCI, V8, P301, DOI 10.1017/S0140525X00020823	32	31	31	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		1987	1	4					303	320						18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	M2053					2022-12-18	WOS:A1987M205300002
J	Wang, N; Zhou, WG; Song, YB; Ma, C; Liu, W; Li, HQ				Wang, Ning; Zhou, Wengang; Song, Yibing; Ma, Chao; Liu, Wei; Li, Houqiang			Unsupervised Deep Representation Learning for Real-Time Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual tracking; Unsupervised learning; Correlation filter; Siamese network	CORRELATION FILTERS; OBJECT TRACKING	The advancement of visual tracking has continuously been brought by deep learning models. Typically, supervised learning is employed to train these models with expensive labeled data. In order to reduce the workload of manual annotation and learn to track arbitrary objects, we propose an unsupervised learning method for visual tracking. The motivation of our unsupervised learning is that a robust tracker should be effective in bidirectional tracking. Specifically, the tracker is able to forward localize a target object in successive frames and backtrace to its initial position in the first frame. Based on such a motivation, in the training process, we measure the consistency between forward and backward trajectories to learn a robust tracker from scratch merely using unlabeled videos. We build our framework on a Siamese correlation filter network, and propose a multi-frame validation scheme and a cost-sensitive loss to facilitate unsupervised learning. Without bells and whistles, the proposed unsupervised tracker achieves the baseline accuracy of classic fully supervised trackers while achieving a real-time speed. Furthermore, our unsupervised framework exhibits a potential in leveraging more unlabeled or weakly labeled data to further improve the tracking accuracy.	[Wang, Ning; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab GIPAS, Hefei, Peoples R China; [Zhou, Wengang; Li, Houqiang] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China; [Song, Yibing; Liu, Wei] Tencent AI Lab, Shenzhen, Peoples R China; [Ma, Chao] Shanghai Jiao Tong Univ, AI Inst, MOE Key Lab Artificial Intelligence, Shanghai, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Tencent; Shanghai Jiao Tong University	Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, CAS Key Lab GIPAS, Hefei, Peoples R China.; Zhou, WG; Li, HQ (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China.	wn6149@mail.ustc.edu.cn; zhwg@ustc.edu.cn; yibingsong.cv@gmail.com; chaoma@sjtu.edu.cn; wl2223@columbia.edu; lihq@ustc.edu.cn		Liu, Wei/0000-0002-3865-8145	NSFC [60906119, 61836011, 61822208, 61632019]; Youth Innovation Promotion Association CAS [2018497]; Shanghai Pujiang Program	NSFC(National Natural Science Foundation of China (NSFC)); Youth Innovation Promotion Association CAS; Shanghai Pujiang Program(Shanghai Pujiang Program)	This work was supported in part to Dr. Houqiang Li by NSFC under contract No. 61836011, and in part to Dr. Wengang Zhou by NSFC under contract No. 61822208 & 61632019 and Youth Innovation Promotion Association CAS (No. 2018497). Dr. Chao Ma was supported by NSFC under contract No. 60906119 and Shanghai Pujiang Program.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen BY, 2018, LECT NOTES COMPUT SC, V11211, P328, DOI 10.1007/978-3-030-01234-2_20; Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057; Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513; Choi J, 2016, PROC CVPR IEEE, P4321, DOI 10.1109/CVPR.2016.468; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159; Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29; Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490; Danelljan Martin, 2014, BRIT MACH VIS C NOTT; Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28; Dong XW, 2018, IEEE CONF COMPUT; Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552; Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI 10.1109/ICCV.2017.129; He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Huang C, 2017, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2017.21; Huang DF, 2017, INT J COMPUT VISION, V122, P524, DOI 10.1007/s11263-016-0974-6; Jung I, 2018, LECT NOTES COMPUT SC, V11208, P89, DOI 10.1007/978-3-030-01225-0_6; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kristan M., 2017, P IEEE INT C COMP VI; Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1; Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982; Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le Q.V., 2011, ARXIV11126209; Lee D.Y, 2015, P IEEE C COMP VIS PA; Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79; Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935; Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515; Li F, 2017, IEEE INT CONF COMP V, P2001, DOI 10.1109/ICCVW.2017.234; Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905; Liu S, 2016, PROC CVPR IEEE, P4312, DOI 10.1109/CVPR.2016.467; Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22; Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3; Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515; Ma C, 2018, INT J COMPUT VISION, V126, P771, DOI 10.1007/s11263-018-1076-4; Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352; Meister S, 2018, AAAI CONF ARTIF INTE, P7251; Muller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19; Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152; NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937; Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279; Sui Y, 2019, INT J COMPUT VISION, V127, P1084, DOI 10.1007/s11263-019-01156-6; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Valmadre J, 2018, LECT NOTES COMPUT SC, V11207, P692, DOI 10.1007/978-3-030-01219-9_41; Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Wang N, 2013, ADV NEURAL INFORM PR, DOI DOI 10.5555/2999611.2999702; Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140; Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509; Wang Q., 2017, ARXIV170404057; Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Wang Xinshao, 2019, P IEEE C COMPUTER VI; WEIJER J, 2009, IEEE T IMAGE PROCESS, V18, P1512; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Zhang MD, 2018, LECT NOTES COMPUT SC, V11207, P484, DOI 10.1007/978-3-030-01219-9_29; Zhang Yulun, 2018, P EUROPEAN C COMPUTE, P286; Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459; Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710	78	30	31	7	38	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					400	418		10.1007/s11263-020-01357-4	http://dx.doi.org/10.1007/s11263-020-01357-4		SEP 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000571683700001
J	Stutz, D; Geiger, A				Stutz, David; Geiger, Andreas			Learning 3D Shape Completion Under Weak Supervision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D shape completion; 3D reconstruction; Weakly-supervised learning; Amortized inference; Benchmark		We address the problem of 3D shape completion from sparse and noisy point clouds, a fundamental problem in computer vision and robotics. Recent approaches are either data-driven or learning-based: Data-driven approaches rely on a shape model whose parameters are optimized to fit the observations; Learning-based approaches, in contrast, avoid the expensive optimization step by learning to directly predict complete shapes from incomplete observations in a fully-supervised setting. However, full supervision is often not available in practice. In this work, we propose a weakly-supervised learning-based approach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, i.e., learn, maximum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. On synthetic benchmarks based on ShapeNet (Chang et al. Shapenet: an information-rich 3d model repository, 2015. ) and ModelNet (Wu et al., in: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR), 2015) as well as on real robotics data from KITTI (Geiger et al., in: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR), 2012) and Kinect (Yang et al., 3d object dense reconstruction from a single depth view, 2018. ), we demonstrate that the proposed amortized maximum likelihood approach is able to compete with the fully supervised baseline of Dai et al. (in: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR), 2017) and outperforms the data-driven approach of Engelmann et al. (in: Proceedings of the German conference on pattern recognition (GCPR), 2016), while requiring less supervision and being significantly faster.	[Stutz, David] Max Planck Inst Informat, Campus E1 4, D-66123 Saarbrucken, Germany; [Geiger, Andreas] Max Planck Inst Intelligent Syst, Max Planck Ring 4, D-72076 Tubingen, Germany; [Geiger, Andreas] Univ Tubingen, Max Planck Ring 4, D-72076 Tubingen, Germany	Max Planck Society; Max Planck Society; Eberhard Karls University of Tubingen	Stutz, D (corresponding author), Max Planck Inst Informat, Campus E1 4, D-66123 Saarbrucken, Germany.	david.stutz@mpi-inf.mpg.de		Stutz, David/0000-0002-6286-1805				Abramowitz M., 1974, HDB MATH FUNCTIONS F; Agarwal S., 2012, OTHERS CERES SOLVER; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054; Bao SY, 2013, PROC CVPR IEEE, P1264, DOI 10.1109/CVPR.2013.167; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Blei DM, 2016, ARXIV160100670; Brock Andrew, 2016, ARXIV160804236; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen X., 2016, ARXIV160807711; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Cignoni p, 2008, P 6 EUR IT CHAPT C S; Collobert R., 2011, NIPS; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dame A, 2013, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2013.170; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eigen David, 2014, NEURIPS; Engelmann F, 2017, IEEE WINT CONF APPL, P400, DOI 10.1109/WACV.2017.51; Engelmann F, 2016, LECT NOTES COMPUT SC, V9796, P219, DOI 10.1007/978-3-319-45886-1_18; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586; Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052; Geiger A., 2012, P IEEE COMP SOC C CO; Gershman SJ, 2014, P 36 ANN C COGN SCI; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Glorot X., 2010, C ART INT STAT AISTA; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guney F, 2015, PROC CVPR IEEE, P4165, DOI 10.1109/CVPR.2015.7299044; Gwak J., 2017, ARXIV170510904, P263; Haene C., 2014, P IEEE C COMP VIS PA; Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19; Im DJ, 2017, AAAI CONF ARTIF INTE, P2059; Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59; Jones E., 2001, SCIPY OPEN SOURCE SC; Kar A., 2015, CVPR; Kato H., 2017, ARXIV171107566; Kingma D.P, P 3 INT C LEARNING R; Kroemer O., 2012, IEEE RAS INT C HUM R; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Law AJ, 2011, COMPUT VIS IMAGE UND, V115, P603, DOI 10.1016/j.cviu.2010.11.019; Leotta M. J., 2009, P IEEE C COMP VIS PA; Li YY, 2015, COMPUT GRAPH FORUM, V34, P435, DOI 10.1111/cgf.12573; Lin Chen-Hsuan, 2017, ARXIV170607036; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu S, 2017, ARXIV170505994; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Ma L., 2014, P EUR C COMP VIS ECC; Malik, 2015, COMPUTER VISION PATT; Menze Moritz, 2015, CVPR; Nan LL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366156; Nash C, 2017, COMPUT GRAPH FORUM, V36, P1, DOI 10.1111/cgf.13240; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Nguyen D. T., 2016, P IEEE C COMP VIS PA; Oswald M. R., 2013, REV GEOMETRY RECOVER, P343; Pauly M., 2005, S GEOM PROC, P23; Pauly M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360642; Pepik Bojan, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301358; Pizlo Z., 2010, 3D SHAPE ITS UNIQUE; Pizlo Z., 2007, P INT C COMP AN IM P; Prisacariu V. A., 2013, AS C COMP VIS, P593; Prisacariu VA, 2011, PROC CVPR IEEE; Qi Charles R, 2017, ARXIV170602413; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2016, ADV NEUR IN, V29; Riegler G, 2017, INT CONF 3D VISION, P57, DOI 10.1109/3DV.2017.00017; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Ritchie D., 2016, ARXIV161005735; Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863; Ronneberger O, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sandhu R., 2009, P IEEE C COMP VIS PA; Sandhu R, 2011, IEEE T PATTERN ANAL, V33, P1098, DOI 10.1109/TPAMI.2010.162; Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20; Smith Edward J., 2017, ABS170709557 CORR; Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41; Steinbrucker F, 2013, IEEE I CONF COMP VIS, P3264, DOI 10.1109/ICCV.2013.405; Stutz D, 2018, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2018.00209; Sung M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818094; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Thrun S, 2005, IEEE I CONF COMP VIS, P1824; Tulsiani S., 2018, ARXIV180103910; Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Varley J., 2017, P IEEE INT C INT ROB; Wang Shenlong, 2016, ARXIV161200423; Wang Weiyue, 2017, P IEEE INT C COMP VI, P2298; Wen H., 2018, ARXIV PREPRINT ARXIV; Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Wu Jiajun, 2016, ADV NEURAL INFORM PR; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xie J, 2016, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2016.401; Yang B, 2017, IEEE INT CONF COMP V, P679, DOI 10.1109/ICCVW.2017.86; Zheng QA, 2010, ACTA OCEANOL SIN, V29, P1, DOI 10.1007/s13131-010-0044-9; Zheng S., 2015, P GERM C PATT REC GC; Zia MZ, 2013, IEEE T PATTERN ANAL, V35, P2608, DOI 10.1109/TPAMI.2013.87; Zia MZ, 2014, PROC CVPR IEEE, P3678, DOI 10.1109/CVPR.2014.470	101	30	30	8	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1162	1181		10.1007/s11263-018-1126-y	http://dx.doi.org/10.1007/s11263-018-1126-y			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Green Submitted, Green Published, hybrid			2022-12-18	WOS:000531431500007
J	Perrone, D; Favaro, P				Perrone, Daniele; Favaro, Paolo			A Logarithmic Image Prior for Blind Deconvolution	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Blind deconvolution; Majorization-minimization; Primal-dual; Image prior; Total variation; Logarithmic prior	CAMERA	Blind Deconvolution consists in the estimation of a sharp image and a blur kernel from an observed blurry image. Because the blur model admits several solutions it is necessary to devise an image prior that favors the true blur kernel and sharp image. Many successful image priors enforce the sparsity of the sharp image gradients. Ideally the "norm" is the best choice for promoting sparsity, but because it is computationally intractable, some methods have used a logarithmic approximation. In this work we also study a logarithmic image prior. We show empirically how well the prior suits the blind deconvolution problem. Our analysis confirms experimentally the hypothesis that a prior should not necessarily model natural image statistics to correctly estimate the blur kernel. Furthermore, we show that a simple Maximum a Posteriori formulation is enough to achieve state of the art results. To minimize such formulation we devise two iterative minimization algorithms that cope with the non-convexity of the logarithmic prior: one obtained via the primal-dual approach and one via majorization-minimization.	[Perrone, Daniele; Favaro, Paolo] Univ Bern, Dept Comp Sci & Appl Math, Neubruckstr 10, CH-3012 Bern, Switzerland	University of Bern	Perrone, D (corresponding author), Univ Bern, Dept Comp Sci & Appl Math, Neubruckstr 10, CH-3012 Bern, Switzerland.	daniele85@gmail.com; favaro@iam.unibe.ch						Babacan S. D., 2012, ECCV; Burger M, 2014, INVERSE PROBL, V30, DOI 10.1088/0266-5611/30/11/114004; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187; Chaudhuri S., 2014, BLIND IMAGE DECONVOL; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; Kenig T, 2010, IEEE T PATTERN ANAL, V32, P2191, DOI 10.1109/TPAMI.2010.45; Keuper M, 2013, PROC CVPR IEEE, P2179, DOI 10.1109/CVPR.2013.283; Krishnan D., 2013, ARXIV13114029 CORR; Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521; Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148; Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815; Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51; Mollenhoff T, 2015, LECT NOTES COMPUT SC, V8932, P126, DOI 10.1007/978-3-319-14612-6_10; Mollenhoff T., 2014, ARXIV14071723 CORR; Ochs P, 2014, SIAM J IMAGING SCI, V7, P1388, DOI 10.1137/130942954; Perrone D., 2014, P 10 INT C EN MIN ME, P112; Perrone D., 2014, IEEE C COMP VIS PATT; Rockafellar R. T., 1970, CONVEX ANAL; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444; Strekalovskiy E, 2014, LECT NOTES COMPUT SC, V8690, P127, DOI 10.1007/978-3-319-10605-2_9; Sun LH, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL WORKSHOP ON COMPUTER SCIENCE IN SPORTS, P1; Wipf David, 2013, Energy Minimization Methods in Computer Vision and Pattern Recognition. 9th International Conference, EMMCVPR 2013. Proceedings. LNCS 8081, P40, DOI 10.1007/978-3-642-40395-8_4; Wipf D, 2014, J MACH LEARN RES, V15, P3595; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	35	30	31	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2016	117	2					159	172		10.1007/s11263-015-0857-2	http://dx.doi.org/10.1007/s11263-015-0857-2			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DH6UB		Green Published			2022-12-18	WOS:000372926500004
J	Papadhimitri, T; Favaro, P				Papadhimitri, Thoma; Favaro, Paolo			A Closed-Form, Consistent and Robust Solution to Uncalibrated Photometric Stereo Via Local Diffuse Reflectance Maxima	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Photometric stereo; Uncalibrated; Diffuse; Maxima; GBR; Ambiguity	REFLECTIONS; SHAPE	Images of an object under different illumination are known to provide strong cues about the object surface. A mathematical formalization of how to recover the normal map of such a surface leads to the so-called uncalibrated photometric stereo problem. In the simplest instance, this problem can be reduced to the task of identifying only three parameters: the so-called generalized bas-relief (GBR) ambiguity. The challenge is to find additional general assumptions about the object, that identify these parameters uniquely. Current approaches are not consistent, i.e., they provide different solutions when run multiple times on the same data. To address this limitation, we propose exploiting local diffuse reflectance (LDR) maxima, i.e., points in the scene where the normal vector is parallel to the illumination direction (see Fig. 1). We demonstrate several noteworthy properties of these maxima: a closed-form solution, computational efficiency and GBR consistency. An LDR maximum yields a simple closed-form solution corresponding to a semi-circle in the GBR parameters space (see Fig. 2); because as few as two diffuse maxima in different images identify a unique solution, the identification of the GBR parameters can be achieved very efficiently; finally, the algorithm is consistent as it always returns the same solution given the same data. Our algorithm is also remarkably robust: It can obtain an accurate estimate of the GBR parameters even with extremely high levels of outliers in the detected maxima (up to 80 % of the observations). The method is validated on real data and achieves state-of-the-art results.	[Papadhimitri, Thoma; Favaro, Paolo] Univ Bern, Bern, Switzerland	University of Bern	Papadhimitri, T (corresponding author), Univ Bern, Bern, Switzerland.	thoma.papadhimitri@iam.unibe.ch; paolo.favaro@iam.unibe.ch						Agrawal A, 2006, LECT NOTES COMPUT SC, V3951, P578; Alldrin NG, 2007, IEEE I CONF COMP VIS, P417; [Anonymous], 2007, CVPR; Basri R, 2001, PROC CVPR IEEE, P374; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Chandraker M, 2011, PROC CVPR IEEE; Chandraker MK, 2005, PROC CVPR IEEE, P788; Drbohlav O, 2005, IEEE I CONF COMP VIS, P1850; Drbohlav O, 2002, LECT NOTES COMPUT SC, V2351, P46; Favaro P, 2012, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2012.6247754; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Georghiades AS, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P816; HAYAKAWA H, 1994, J OPT SOC AM A, V11, P3079, DOI 10.1364/JOSAA.11.003079; Hertzmann A, 2005, IEEE T PATTERN ANAL, V27, P1254, DOI 10.1109/TPAMI.2005.158; Koppal S. J., 2006, PROC IEEE C COMPUT V, P1323; Lagger P, 2008, COMPUT VIS IMAGE UND, V111, P207, DOI 10.1016/j.cviu.2007.11.002; Lin Z, 2009, UILUENG092215 UIUC; Okabe T, 2009, IEEE I CONF COMP VIS, P1693, DOI 10.1109/ICCV.2009.5459381; Oren M, 1997, INT J COMPUT VISION, V24, P105, DOI 10.1023/A:1007954719939; Sato I, 2007, IEEE I CONF COMP VIS, P1493; Shi BX, 2010, PROC CVPR IEEE, P1118, DOI 10.1109/CVPR.2010.5540091; Sunkavalli K, 2010, LECT NOTES COMPUT SC, V6312, P251, DOI 10.1007/978-3-642-15552-9_19; Tan P., 2007, COMP VIS PATT REC C, V1, P17; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wright J., 2009, P ADV NEUR INF PROC, V58, P289; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Wu TP, 2005, PROC CVPR IEEE, P140; Yuille A, 1997, PROC CVPR IEEE, P158, DOI 10.1109/CVPR.1997.609314; Zickler TE, 2002, INT J COMPUT VISION, V49, P215, DOI 10.1023/A:1020149707513	29	30	30	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2014	107	2			SI		139	154		10.1007/s11263-013-0665-5	http://dx.doi.org/10.1007/s11263-013-0665-5			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AD3QP		Green Published, Green Submitted			2022-12-18	WOS:000333161200004
J	Woodford, OJ; Pham, MT; Maki, A; Perbet, F; Stenger, B				Woodford, Oliver J.; Minh-Tri Pham; Maki, Atsuto; Perbet, Frank; Stenger, Bjoern			Demisting the Hough Transform for 3D Shape Recognition and Registration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hough transform; Object recognition; 3d shape; Registration	SHIFT	In applying the Hough transform to the problem of 3D shape recognition and registration, we develop two new and powerful improvements to this popular inference method. The first, intrinsic Hough, solves the problem of exponential memory requirements of the standard Hough transform by exploiting the sparsity of the Hough space. The second, minimum-entropy Hough, explains away incorrect votes, substantially reducing the number of modes in the posterior distribution of class and pose, and improving precision. Our experiments demonstrate that these contributions make the Hough transform not only tractable but also highly accurate for our example application. Both contributions can be applied to other tasks that already use the standard Hough transform.	[Woodford, Oliver J.; Minh-Tri Pham; Maki, Atsuto; Perbet, Frank; Stenger, Bjoern] Toshiba Res Europe Ltd, Cambridge CB4 0GZ, England	Toshiba Corporation	Woodford, OJ (corresponding author), Toshiba Res Europe Ltd, 208 Cambridge Sci Pk,Milton Rd, Cambridge CB4 0GZ, England.	oliver.woodford@crl.toshiba.co.uk						Allan M, 2009, COMPUT VIS IMAGE UND, V113, P824, DOI 10.1016/j.cviu.2009.02.002; [Anonymous], 2011, TOSH CAD MOD POINT C; Ashbrook A. P., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P674, DOI 10.1007/BFb0054772; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; BARINOVA O, 2010, P IEEE C COMP VIS PA; BENTZVI D, 1990, PATTERN RECOGN LETT, V11, P167, DOI 10.1016/0167-8655(90)90002-J; BESAG J, 1986, J R STAT SOC B, V48, P259; BIRCHFIELD S, 1999, P IEEE INT C COMP VI; Bober M., 1993, P BRIT MACH VIS C; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; Delong A., 2012, P EUR C COMP VIS; Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z; Drost Bertram, 2010, 2010 IEEE COMP SOC C, DOI DOI 10.1109/CVPR.2010.5540108; DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242; FALK H, 1970, AM J PHYS, V38, P858, DOI 10.1119/1.1976484; Gall J, 2009, PROC CVPR IEEE, P1022, DOI 10.1109/CVPRW.2009.5206740; Gerig G., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P112; Hough P.V., 1962, US Patent, Patent No. [US3069654A, 3069654, 3,069,654]; ILLINGWORTH J, 1987, IEEE T PATTERN ANAL, V9, P690, DOI 10.1109/TPAMI.1987.4767964; Isack H, 2012, INT J COMPUT VISION, V97, P123, DOI 10.1007/s11263-011-0474-7; Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43; Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995; Leibe B., 2004, STAT LEARNING COMPUT; Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3; LI HW, 1986, COMPUT VISION GRAPH, V36, P139, DOI 10.1016/0734-189X(86)90073-3; MacKay D. J. C., 2009, INFORM THEORY INFERE; Maji S., 2009, P IEEE C COMP VIS PA; Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213; Minka T., 2003, SUMMATION HACK OUTLI; Okada R, 2009, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2009.5459441; Pham M., 2011, P IEEE INT C COMP VI; Rosten E., 2009, MACHINE VISION APPL; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X; SHEIKH Y, 2007, P IEEE INT C COMP VI; STEPHENS RS, 1991, IMAGE VISION COMPUT, V9, P66, DOI 10.1016/0262-8856(91)90051-P; Toldo R., 2008, P EUR C COMP VIS; Tombari F., 2010, 4 PAC RIM S IM VID T, P349, DOI DOI 10.1109/PSIVT.2010.65; Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52; Vincent E, 2001, ISPA 2001: PROCEEDINGS OF THE 2ND INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P182, DOI 10.1109/ISPA.2001.938625; Vogiatzis G, 2011, IMAGE VISION COMPUT, V29, P434, DOI 10.1016/j.imavis.2011.01.006; Woodford O. J., 2012, P EUR C COMP VIS; XU L, 1990, PATTERN RECOGN LETT, V11, P331, DOI 10.1016/0167-8655(90)90042-Z; Zhang W, 2007, LECT NOTES COMPUT SC, V4358, P60; Zhou HY, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.28; ZULIANI M, 2005, P IEEE INT C IM PROC	45	30	33	5	33	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2014	106	3			SI		332	341		10.1007/s11263-013-0623-2	http://dx.doi.org/10.1007/s11263-013-0623-2			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AA3DC					2022-12-18	WOS:000330972100008
J	Coutinho, FL; Morimoto, CH				Coutinho, Flavio L.; Morimoto, Carlos H.			Improving Head Movement Tolerance of Cross-Ratio Based Eye Trackers	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Eye tracking; Gaze tracking; Remote eye gaze tracking; Head movement tolerance; Free-head motion; Cross-ratio; Homography	GAZE ESTIMATION; MOTION	When first introduced, the cross-ratio (CR) based remote eye tracking method offered many attractive features for natural human gaze-based interaction, such as simple camera setup, no user calibration, and invariance to head motion. However, due to many simplification assumptions, current CR-based methods are still sensitive to head movements. In this paper, we revisit the CR-based method and introduce two new extensions to improve the robustness of the method to head motion. The first method dynamically compensates for scale changes in the corneal reflection pattern, and the second method estimates true coplanar eye features so that the cross-ratio can be applied. We present real-time implementations of both systems, and compare the performance of these new methods using simulations and user experiments. Our results show a significant improvement in robustness to head motion and, for the user experiments in particular, an average reduction of up to 40 % in gaze estimation error was observed.	[Coutinho, Flavio L.; Morimoto, Carlos H.] Univ Sao Paulo, Dept Comp Sci, Sao Paulo, Brazil	Universidade de Sao Paulo	Coutinho, FL (corresponding author), Univ Sao Paulo, Dept Comp Sci, Sao Paulo, Brazil.	flc@ime.usp.br; hitoshi@ime.usp.br	Morimoto, Carlos H/H-2420-2012	Morimoto, Carlos H/0000-0003-4679-2827	Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES); Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP); IBM	Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)(Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)); IBM(International Business Machines (IBM))	The authors would like to thank Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES), Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP) and IBM for their financial support.	Cerrolaza JJ, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P259, DOI 10.1145/1344471.1344530; Chen JX, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P189, DOI 10.1145/1344471.1344518; Coutinho FL, 2006, SIBGRAPI, P171; Duchowski A.T., 2003, EYE TRACKING METHODO, DOI [10.1007/978-1-4471-3750-4, DOI 10.1007/978-1-4471-3750-4]; Ebisawa Y., 1995, Visualization and Intelligent Design in Engineering and Architecture II, P79; Guestrin E. D., 2010, P ETRA 2010 MAR, P199; Guestrin ED, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P237, DOI 10.1145/1344471.1344526; Guestrin ED, 2006, IEEE T BIO-MED ENG, V53, P1124, DOI 10.1109/TBME.2005.863952; Guestrin ED, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P267, DOI 10.1145/1344471.1344531; Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30; Hansen Dan Witzner, 2010, P 2010 S EYE TRACK R, P13; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hennessey Craig, 2006, P 2006 S EYE TRACK R, DOI DOI 10.1145/1117309.1117349; Kang J.J., 2007, 30 CAN MED BIOL ENG; Kaufman A., 1993, P RES FRONT VIRT REA, P78; Model D., 2010, P 2010 S EYE TRACK R, P29; Morimoto C. H., 1999, P IEEE ICCV 99 FRAM; Morimoto CH, 2005, COMPUT VIS IMAGE UND, V98, P4, DOI 10.1016/j.cviu.2004.07.010; Nagamatsu T., 2010, P 2010 S EYE TRACK R, P251; Nagamatsu T, 2010, P 2010 S EYE TRACK R, P255, DOI DOI 10.1145/1743666.1743726; Nagamatsu T, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P95, DOI 10.1145/1344471.1344496; ROBINSON DA, 1963, IEEE T BIO-MED ENG, VBM10, P137, DOI 10.1109/TBMEL.1963.4322822; Shih S., 2003, IEEE T SYST MAN CYB, P1; Trucco AV, 1998, INTRO TECHNIQUES 3 D; Villanueva A., 2008, COMMUNICATION GAZE I; Wang JG, 2002, IEEE T SYST MAN CY B, V32, P332, DOI 10.1109/TSMCB.2002.999809; Yoo D.H., 2002, P 5 IEEE INT C AUT F, P94; Yoo DH, 2005, COMPUT VIS IMAGE UND, V98, P25, DOI 10.1016/j.cviu.2004.07.011	28	30	33	1	30	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2013	101	3			SI		459	481		10.1007/s11263-012-0541-8	http://dx.doi.org/10.1007/s11263-012-0541-8			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	086WF					2022-12-18	WOS:000314719000005
J	Hartley, R; Kahl, F				Hartley, Richard; Kahl, Fredrik			Critical configurations for projective reconstruction from multiple views	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						projective geometry; structure from motion; degeneracy; critical sets; multiple view geometry; geometry 3D reconstruction		This paper investigates a classical problem in computer vision: Given corresponding points in multiple images, when is there a unique projective reconstruction of the 3D geometry of the scene points and the camera positions? A set of points and cameras is said to be critical when there is more than one way of realizing the resulting image points. For two views, it has been known for almost a century that the critical configurations consist of points and camera lying on a ruled quadric surface. We give a classification of all possible critical configurations for any number of points in three images, and show that in most cases, the ambiguity extends to any number of cameras. The underlying framework for deriving the critical sets is projective geometry. Using a generalization of Pascal's Theorem, we prove that any number of cameras and scene points on an elliptic quartic form a critical set. Another important class of critical configurations consists of cameras and points on rational quartics. The theoretical results are accompanied by many examples and illustrations.	Australian Natl Univ, Canberra, ACT, Australia; Lund Univ, Ctr Math Sci, Lund, Sweden	Australian National University; Lund University	Hartley, R (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.			Hartley, Richard/0000-0002-5005-0191				Astrom K, 2003, J MATH IMAGING VIS, V18, P191, DOI 10.1023/A:1022120702016; BUCHANAN T, 1988, COMPUT VISION GRAPH, V42, P130, DOI 10.1016/0734-189X(88)90146-6; CARLSSON S, 1995, IEEE WORKSH REPR VIS, P85; Evelyn CJA, 1974, 7 CIRCLES THEOREM OT; Hartley R., 1998, 3D Structure from Multiple Images of Large-Scale Environments. European Workshop, SMILE'98. Proceedings, P14; HARTLEY R, 2000, EUR C COMP VIS, V1, P922; HARTLEY R, 2003, C COMP VIS PATT REC, V1, P511; Hartley R., 2003, MULTIPLE VIEW GEOMET; HARTLEY RI, 1994, IEEE T PATTERN ANAL, V16, P1036, DOI 10.1109/34.329005; KAHL F, 2001, C COMP VIS PATT REC, V2, P158; KAHL F, 2002, EUR C COMP VIS COP D, V2, P447; KAHL F, 2001, THESIS LUND I TECHNO; Krames J., 1940, MONATSHEFTE MATH PHY, V49, P327, DOI DOI 10.1007/BF01707311; MAYBANK S, 1993, THEORY RECONSTRUCTIO; MAYBANK SJ, 1995, APPL ALGEBR ENG COMM, V6, P89, DOI 10.1007/BF01225646; Maybank SJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P703, DOI 10.1109/ICCV.1998.710794; Navab N., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P713, DOI 10.1109/ICCV.1993.378143; Semple J., 1979, ALGEBRAIC PROJECTIVE	18	30	30	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2007	71	1					5	47		10.1007/s11263-005-4796-1	http://dx.doi.org/10.1007/s11263-005-4796-1			43	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	094GT					2022-12-18	WOS:000241228600001
J	Arbelaez, PA; Cohen, LD				Arbelaez, Pablo A.; Cohen, Laurent D.			A metric approach to vector-valued image segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	2nd IEEE Workshop on Variational, Geometric and Level Set Methods held in Conjunction with the IEEE International Conference on Computer Vision	OCT, 2003	Nice, FRANCE	IEEE, French Natl Inst Res Comp Sci & Control, Siemens Corp Res, Imaging & Visualizat Dept		image segmentation; distance transforms; path variation; ultrametrics; vector-valued image; color; boundary detection		We address the issue of low-level segmentation of vector-valued images. focusing on the case of color natural images. The proposed approach relies on the formulation of the problem in the metric framework, as a Voronoi tessellation of the image domain. In this context, a segmentation is determined by a distance transform and a set of sites. Our method consists in dividing the segmentation task in two successive sub-tasks: pre-segmentation and hierarchical representation. We design specific distances for both sub-problems by considering low-level image attributes and, particularly, color and lightness information. Then, the interpretation of the metric formalism in terms of boundaries allows the definition of a soft contour map that has the property of producing a set of closed curves for any threshold. Finally, we evaluate the quality of our results with respect to ground-truth segmentation data.	Univ Paris 09, CEREMADE, CNRS, UMR 7534, F-75775 Paris 16, France	Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Mathematical Sciences (INSMI); UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine; Universite Paris Cite	Arbelaez, PA (corresponding author), Univ Paris 09, CEREMADE, CNRS, UMR 7534, Pl Marechal Lattre Tassigny, F-75775 Paris 16, France.	arbelaez@ceremade.dauphine.fr; cohen@ceremade.dauphine.fr		Arbelaez, Pablo/0000-0001-5244-2407				AHUJA N, 1985, COMPUT VISION GRAPH, V29, P286, DOI 10.1016/0734-189X(85)90126-4; Arbelaez P.A., 2003, P 2 IEEE WORKSH VAR, P49; ARBELAEZ PA, 2003, P EMMCVPR 03 LISB PR, P246; Aurenhammer F, 2000, HANDBOOK OF COMPUTATIONAL GEOMETRY, P201, DOI 10.1016/B978-044482537-7/50006-1; Benzecri J.P., 1984, ANAL DONNEES, VI; Dirichlet G. L., 1850, J REINE ANGEW MATH, V40, P209, DOI DOI 10.1515/CRLL.1850.40.209; Forsyth David A, 2012, COMPUTER VISION MODE; Garrido L, 1998, SIGNAL PROCESS, V66, P157, DOI 10.1016/S0165-1684(98)00004-8; Kelley J. L., 1975, GEN TOPOLOGY; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Mayya N, 1996, J MATH IMAGING VIS, V6, P355, DOI 10.1007/BF00123352; Najman L, 1996, IEEE T PATTERN ANAL, V18, P1163, DOI 10.1109/34.546254; OKABE A, 2002, SPATIAL TESSELLATION; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; TUCERYAN M, 1990, IEEE T PATTERN ANAL, V12, P211, DOI 10.1109/34.44407; VANRIJSBERGEN V, 1979, INFORM RETRIEVAL; Voronoi G, 1908, J REINE ANGEW MATH, V133, P97, DOI 10.1515/crll.1908.133.97; Wyszecki G., 2000, COLOR SCI CONCEPTS M, V2nd	20	30	30	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	1					119	126		10.1007/s11263-006-6857-5	http://dx.doi.org/10.1007/s11263-006-6857-5			8	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	063QH		Green Submitted			2022-12-18	WOS:000239034100009
J	Maki, A; Watanabe, M; Wiles, C				Maki, A; Watanabe, M; Wiles, C			Geotensity: Combining motion and lighting for 3D surface reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						geotensity; linear image subspaces; structure-from-motion; surface reconstruction	PHOTOMETRIC STEREO; SHAPE	This paper is about automatically reconstructing the full 3D surface of an object observed in motion by a single static camera. Based on the two paradigms, structure from motion and linear intensity subspaces, we introduce the geotensity constraint that governs the relationship between four or more images of a moving object. We show that it is possible in theory to solve for 3D Lambertian surface structure for the case of a single point light source and propose that a solution exists for an arbitrary number point light sources. The surface may or may not be textured. We then give an example of automatic surface reconstruction of a face under a point light source using arbitrary unknown object motion and a single fixed camera.	Toshiba Co Ltd, Ctr Corp Res & Dev, Saiwai Ku, Kawasaki, Kanagawa 2128582, Japan	Toshiba Corporation	Maki, A (corresponding author), Toshiba Co Ltd, Ctr Corp Res & Dev, Saiwai Ku, 1 Komukai Toshiba Cho, Kawasaki, Kanagawa 2128582, Japan.	atsuto.maki@toshiba.co.jp						BEARDSLEY PA, 1996, 4 ECCV CAMBR UK, P683; BELHUMEUR P, 1996, CVPR, P270; DEVERNAY F, 1994, CVPR, P208; EPSTEIN R, 1995, IEEE WORKSH PHYS BAS, P108; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; HORN BKP, 1977, ARTIF INTELL, V8, P201, DOI 10.1016/0004-3702(77)90020-0; IKEUCHI K, 1981, IEEE T PATTERN ANAL, V3, P661, DOI 10.1109/TPAMI.1981.4767167; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; Maki A, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1053, DOI 10.1109/ICCV.1998.710847; MAKI A, 2000, 6 ECCV, P725; Mundy J., 1992, GEOMETRIC INVARIANCE; NAYAR SK, 1990, IEEE T ROBOTIC AUTOM, V6, P418, DOI 10.1109/70.59367; ONN R, 1990, INT J COMPUT VISION, V5, P105, DOI 10.1007/BF00056773; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P879, DOI 10.1109/34.93807; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P170, DOI 10.1109/TPAMI.1984.4767501; Seitz SM, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P17, DOI 10.1109/ICCV.1998.710696; Shashua A, 1992, THESIS MIT; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Weinshall D., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P675, DOI 10.1109/ICCV.1993.378147; Wiles CS, 2001, IEEE T PATTERN ANAL, V23, P1391, DOI 10.1109/34.977563; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479	22	30	31	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2002	48	2					75	90		10.1023/A:1016057422703	http://dx.doi.org/10.1023/A:1016057422703			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	567AZ					2022-12-18	WOS:000176461600001
J	Geusebroek, JM; Smeulders, AWM; Geerts, H				Geusebroek, JM; Smeulders, AWM; Geerts, H			A minimum cost approach for segmenting networks of lines	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						networks; graph morphology; line detection; dashed line detection; minimum cost path; watershed segmentation; differential geometry; mathematical morphology; Gaussian scale-space	IMAGES	The extraction and interpretation of networks of lines from images yields important organizational information of the network under consideration. In this paper, a one-parameter algorithm for the extraction of line networks from images is presented. The parameter indicates the extracted saliency level from a hierarchical graph. Input for the algorithm is the domain specific knowledge of interconnection points. Graph morphological tools are used to extract the minimum cost graph which best segments the network. We give an extensive error analysis for the general case of line extraction. Our method is shown to be robust against gaps in lines, and against spurious vertices at lines, which we consider as the most prominent source of error in line detection. The method indicates detection confidence, thereby supporting error proof interpretation of the network functionality. The method is demonstrated to be applicable on a broad variety of line networks, including dashed lines. Hence, the proposed method yields a major step towards general line tracking algorithms.	Univ Amsterdam, Fac Sci, Dept Comp Sci, NL-1098 SJ Amsterdam, Netherlands; Janssen Res Fdn, CNS Dis Res, Neurobiol, B-2340 Beerse, Belgium	University of Amsterdam; Johnson & Johnson	Geusebroek, JM (corresponding author), Univ Amsterdam, Fac Sci, Dept Comp Sci, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.	geusebroek@science.uva.nl						Ausma J, 1997, CIRCULATION, V96, P3157, DOI 10.1161/01.CIR.96.9.3157; BARZOHAR M, 1993, IEEE COMPUTER VISION, P459; Bellman RE, 1957, DYNAMIC PROGRAMMING; BELTRAMI CA, 1994, CIRCULATION, V89, P151, DOI 10.1161/01.CIR.89.1.151; Beucher S., 1993, MATH MORPHOLOGY IMAG, P433, DOI DOI 10.1201/9781482277234-12; BUCKLEY M, 2000, MATH MORPHOLOGY ITS; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; FLORACK LMJ, 1992, IMAGE VISION COMPUT, V10, P376, DOI 10.1016/0262-8856(92)90024-W; ILLINGWORTH J, 1988, COMPUT VISION GRAPH, V44, P87, DOI 10.1016/S0734-189X(88)80033-1; KONG B, 1996, GRAPHICS RECOGNITION, P270; Lindeberg T., 1994, SCALE SPACE THEORY C; LORENZ C, 1998, SCALE SPACE THEORIES, P152; MEYER F, 1994, SIGNAL PROCESS, V38, P113, DOI 10.1016/0165-1684(94)90060-4; Romeny B.M., 1994, GEOMETRY DRIVEN DIFF; Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591; Sha'ashua A., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P321, DOI 10.1109/CCV.1988.590008; Steger C, 1998, IEEE T PATTERN ANAL, V20, P113, DOI 10.1109/34.659930; VINCENT L, 1989, SIGNAL PROCESS, V16, P365, DOI 10.1016/0165-1684(89)90031-5; Vincent L, 1998, COMP IMAG VIS, V12, P331; VLIEGEN HW, 1987, CARDIOVASC RES, V21, P352, DOI 10.1093/cvr/21.5.352	21	30	30	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2001	43	2					99	111		10.1023/A:1011118718821	http://dx.doi.org/10.1023/A:1011118718821			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	451EH					2022-12-18	WOS:000169787800002
J	Carlsson, S; Weinshall, D				Carlsson, S; Weinshall, D			Dual computation of projective shape and camera positions from multiple images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						projective shape; reconstruction; positioning; epipolar geometry; duality; multiple views	MOTION	Given multiple image data from a set of points in 3D, there are two fundamental questions that can addressed: What is the structure of the set of points in 3D? What are the positions of the cameras relative to the points? In this paper we show that, for projective views and with structure and position defined projectively, these problems are dual because they can be solved using constraint equations where space points and camera positions occur in a reciprocal way. More specifically, by using canonical projective reference frames for all points in space and images, the imaging of point sets in space by multiple cameras can be captured by constraint relations involving three different kinds of parameters only, coordinates of: (1) space points, (2) camera positions (3) image points. The duality implies that the problem of computing camera positions from p points in q views can be solved with;the same algorithm as the problem of directly reconstructing q + 4 points in p - 4 views. This unifies different apl,roaches to projective reconstruction: methods based on external calibration and direct methods exploiting constraints that exist between shape and image invariants.	Royal Inst Technol, Dept Numer Anal & Comp Sci, S-10044 Stockholm, Sweden; Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel	Royal Institute of Technology; Hebrew University of Jerusalem	Carlsson, S (corresponding author), Royal Inst Technol, Dept Numer Anal & Comp Sci, S-10044 Stockholm, Sweden.	stefanc@nada.kth.se; daphna@cs.huji.ac.il						CARLSSON S, 1995, IEEE WORKSH REPR VIS; CARLSSON S, 1994, LECT NOTES COMPUTER, V825, P145; CARLSSON S, 1995, KTHNAP9522SE ISRN RO; CHEN HH, 1990, IEEE T PATTERN ANAL, V12, P1002, DOI 10.1109/34.58872; CSURKA G, 1994, COMPUTING 3 DIMENSIO; DEMEY S, 1992, P BMVC 92; FAUGERAS O, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P951, DOI 10.1109/ICCV.1995.466832; FAUGERAS OD, 1992, P 2 EUR C COMP VIS S, P563; FAUGERAS OD, 1992, P EUR C COMP VIS, P321; GROS P, 1994, LNCS, V825, P107; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; HARTLEY R, 1994, LINES POINTS 3 VIEWS; HEYDEN A, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1058, DOI 10.1109/ICCV.1995.466817; HEYDEN A, 1995, RECONSTRUCTION MULTI; IRANI M, 1996, P 4 EUR C COMP VIS C, V1, P17; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LUONG QT, 1994, P 3 ECCV; MAYBANK SJ, 1990, PHIL T R SOC LONDO A, V322, P1; MOHR R, 1995, J ROBOTICS RES, V6, P619; MOHR R, 1992, HDB PATTERN RECOGNIT; QUAN L, 1994, P EUR C COMP VIS, V2, P459; ROTHWELL CA, 1993, P IEEE INT C COMPUTE, P573; SHASHUA A, 1995, IEEE T PATTERN ANAL, V17, P779, DOI 10.1109/34.400567; Shashua A., 1995, Proceedings. Fifth International Conference on Computer Vision (Cat. No.95CB35744), P920, DOI 10.1109/ICCV.1995.466837; SHASHUA A, 1996, P EUR C COMP VIS, P196; Shashua A., 1994, P 3 EUR C COMP VIS, P479; SHASHUA A, 1993, P INT C COMP VIS BER, P583; SPARR G, 1992, IMAGE VISION COMPUT, V10, P683, DOI 10.1016/0262-8856(92)90013-S; SPARR G, 1991, P 1 ESPRIT DARPA WOR; SPARR G, 1994, P 4 EUR C COMP VIS C, P471; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TRIGGS B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P338, DOI 10.1109/ICCV.1995.466920; WEINSHALL D, 1995, IEEE T PATTERN ANAL, V17, P512, DOI 10.1109/34.391392; WEINSHALL D, 1993, INT J COMPUT VISION, V10, P27, DOI 10.1007/BF01440845; WEINSHALL D, 1995, P IEEE WORKSH REPR V; ZISSERMAN A, 1994, LNCS, V825, P69	38	30	31	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	1998	27	3					227	241		10.1023/A:1007961913417	http://dx.doi.org/10.1023/A:1007961913417			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZL840					2022-12-18	WOS:000073477400002
J	Chan, L; Hosseini, MS; Plataniotis, KN				Chan, Lyndon; Hosseini, Mahdi S.; Plataniotis, Konstantinos N.			A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Weakly supervised semantic segmentation; Self-supervised Learning; Natural imaging; Digital pathology; Satellite imaging; Deep learning; Convolutional neural network	OBJECT; RECOGNITION; FEATURES; DATASET	Recently proposed methods for weakly-supervised semantic segmentation have achieved impressive performance in predicting pixel classes despite being trained with only image labels which lack positional information. Because image annotations are cheaper and quicker to generate, weak supervision is more practical than full supervision for training segmentation algorithms. These methods have been predominantly developed to solve the background separation and partial segmentation problems presented by natural scene images and it is unclear whether they can be simply transferred to other domains with different characteristics, such as histopathology and satellite images, and still perform well. This paper evaluates state-of-the-art weakly-supervised semantic segmentation methods on natural scene, histopathology, and satellite image datasets and analyzes how to determine which method is most suitable for a given dataset. Our experiments indicate that histopathology and satellite images present a different set of problems for weakly-supervised semantic segmentation than natural scene images, such as ambiguous boundaries and class co-occurrence. Methods perform well for datasets they were developed on, but tend to perform poorly on other datasets. We present some practical techniques for these methods on unseen datasets and argue that more work is needed for a generalizable approach to weakly-supervised semantic segmentation. Our full code implementation is available on GitHub:.	[Chan, Lyndon; Hosseini, Mahdi S.; Plataniotis, Konstantinos N.] Univ Toronto, Edward S Rogers Sr Dept Elect & Comp Engn, Toronto, ON, Canada	University of Toronto	Chan, L; Hosseini, MS (corresponding author), Univ Toronto, Edward S Rogers Sr Dept Elect & Comp Engn, Toronto, ON, Canada.	lyndon.chan@mail.utoronto.ca; mahdi.hosseini@mail.utoronto.ca	; Plataniotis, Konstantinos/E-8471-2014	Chan, Lyndon/0000-0002-1185-7961; Plataniotis, Konstantinos/0000-0003-3647-5473; Hosseini, Mahdi/0000-0002-9147-0731				Ahn J., 2018, ARXIV180310464 CORR; Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231; Aresta G., 2018, CORR; Audebert N, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9040368; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Beck AH, 2011, SCI TRANSL MED, V3, DOI 10.1126/scitranslmed.3002564; Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5; Carbonneau MA, 2018, PATTERN RECOGN, V77, P329, DOI 10.1016/j.patcog.2017.10.009; Chan L., 2019, INT C COMP VIS ICCV; Chen H, 2016, PROC CVPR IEEE, P2487, DOI 10.1109/CVPR.2016.273; Chen L.-C., 2014, ARXIV PREPRINT ARXIV; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen Liang-Chieh, 2018, P EUROPEAN C COMPUTE, P801; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; Demir I, 2018, IEEE COMPUT SOC CONF, P172, DOI 10.1109/CVPRW.2018.00031; Durand Thibaut, 2017, CVPR, DOI DOI 10.1109/CVPR.2017.631; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gao JF, 2018, INT J APPL EARTH OBS, V67, P43, DOI 10.1016/j.jag.2017.12.012; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HELBER P, 2019, IEEE J-STARS, V12, P2217, DOI [10.1109/IGARSS.2018.8519248, DOI 10.1109/JSTARS.2019.2918242]; Hosseini MS, 2019, PROC CVPR IEEE, P11739, DOI 10.1109/CVPR.2019.01202; Hou L, 2016, PROC CVPR IEEE, P2424, DOI 10.1109/CVPR.2016.266; Huang XY, 2020, IEEE T PATTERN ANAL, V42, P2702, DOI 10.1109/TPAMI.2019.2926463; Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733; Jia ZP, 2017, IEEE T MED IMAGING, V36, P2376, DOI 10.1109/TMI.2017.2724070; Kainz P., 2015, CORR; Kather JN, 2019, PLOS MED, V16, DOI 10.1371/journal.pmed.1002730; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Kervadec H, 2019, MED IMAGE ANAL, V54, P88, DOI 10.1016/j.media.2019.02.009; Kolesnikov A., 2016, ARXIV160505538; Kolesnikov A., 2016, CORR; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Kothari S, 2013, BMC MED IMAGING, V13, DOI 10.1186/1471-2342-13-9; Kumar N, 2017, IEEE T MED IMAGING, V36, P1550, DOI 10.1109/TMI.2017.2677499; Kuo TS, 2018, IEEE COMPUT SOC CONF, P247, DOI 10.1109/CVPRW.2018.00046; Kwak S, 2017, AAAI CONF ARTIF INTE, P4111; Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541; Lenz M, 2016, BIOINFORMATICS, V32, P396, DOI 10.1093/bioinformatics/btw431; Li WQ, 2016, I S BIOMED IMAGING, P1405, DOI 10.1109/ISBI.2016.7493530; Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344; Lin H., 2017, CORR; Lin H, 2018, IEEE WINT CONF APPL, P539, DOI 10.1109/WACV.2018.00065; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Malon C., 2013, J PATHOLOGY INFORN; Mnih V, 2010, LECT NOTES COMPUT SC, V6316, P210, DOI 10.1007/978-3-642-15567-3_16; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Nivaggioli A., 2019, ARXIV190403983; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Pathak D., 2014, 14127144 ARXIV; Pathak D, 2015, IEEE I CONF COMP VIS, P1796, DOI 10.1109/ICCV.2015.209; Prince S.J.D., 2012, COMPUTER VISION MODE, P2; Prince SJ, 2012, COMPUTER VISION MODE, P201; Rahnemoonfar M, 2018, INT GEOSCI REMOTE SE, P1788; Riordan DP, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0128975; Robinson C, 2019, PROC CVPR IEEE, P12718, DOI 10.1109/CVPR.2019.01301; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roux L., 2013, J PATHOLOGY INFORN; Saleh F., 2016, ARXIVABS160900446; Seferbekov S, 2018, IEEE COMPUT SOC CONF, P272, DOI 10.1109/CVPRW.2018.00051; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; SHAPIRO T, 2000, COMPUTER VISION, P305; Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14; Shkolyar A, 2015, IEEE ENG MED BIO, P743, DOI 10.1109/EMBC.2015.7318469; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Sirinukunwattana K, 2017, MED IMAGE ANAL, V35, P489, DOI 10.1016/j.media.2016.08.008; Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58; Tian C., 2019, ARXIV191108169; Tian C, 2018, IEEE COMPUT SOC CONF, P262, DOI 10.1109/CVPRW.2018.00049; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Turkki Riku, 2016, J Pathol Inform, V7, P38, DOI 10.4103/2153-3539.189703; Veta M, 2015, MED IMAGE ANAL, V20, P237, DOI 10.1016/j.media.2014.11.010; Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686; Wang X., 2018, IEEE T CYBERNETICS; Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759; Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644; Xie JY, 2019, FRONT GENET, V10, DOI 10.3389/fgene.2019.00080; Xu J, 2015, PROC CVPR IEEE, P3781, DOI 10.1109/CVPR.2015.7299002; Xu J, 2016, NEUROCOMPUTING, V191, P214, DOI 10.1016/j.neucom.2016.01.034; Xu Y, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1685-x; Xu Y, 2014, MED IMAGE ANAL, V18, P591, DOI 10.1016/j.media.2014.01.010; Yang Y, 2010, P 18 SIGSPATIAL INT, P270, DOI [10.1145/1869790.1869829, DOI 10.1145/1869790.1869829]; Yao XW, 2016, IEEE T GEOSCI REMOTE, V54, P3660, DOI 10.1109/TGRS.2016.2523563; Ye LW, 2018, IEEE WINT CONF APPL, P1461, DOI 10.1109/WACV.2018.00164; Yu Fisher, 2018, ARXIV180504687; Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11; Zhang C, 2019, ISPRS INT J GEO-INF, V8, DOI 10.3390/ijgi8040189; Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684; Zhang XF, 2015, PROC CVPR IEEE, P5361, DOI 10.1109/CVPR.2015.7299174; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472	104	29	29	13	44	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					361	384		10.1007/s11263-020-01373-4	http://dx.doi.org/10.1007/s11263-020-01373-4		SEP 2020	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000568976600001
J	Wang, X; Liu, SF; Ma, HM; Yang, MH				Wang, Xiang; Liu, Sifei; Ma, Huimin; Yang, Ming-Hsuan			Weakly-Supervised Semantic Segmentation by Iterative Affinity Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Weakly-supervised learning; Semantic segmentation; Affinity		Weakly-supervised semantic segmentation is a challenging task as no pixel-wise label information is provided for training. Recent methods have exploited classification networks to localize objects by selecting regions with strong response. While such response map provides sparse information, however, there exist strong pairwise relations between pixels in natural images, which can be utilized to propagate the sparse map to a much denser one. In this paper, we propose an iterative algorithm to learn such pairwise relations, which consists of two branches, a unary segmentation network which learns the label probabilities for each pixel, and a pairwise affinity network which learns affinity matrix and refines the probability map generated from the unary network. The refined results by the pairwise network are then used as supervision to train the unary network, and the procedures are conducted iteratively to obtain better segmentation progressively. To learn reliable pixel affinity without accurate annotation, we also propose to mine confident regions. We show that iteratively training this framework is equivalent to optimizing an energy function with convergence to a local minimum. Experimental results on the PASCAL VOC 2012 and COCO datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.	[Wang, Xiang] Tencent Res, Beijing, Peoples R China; [Wang, Xiang] Tsinghua Univ, Beijing, Peoples R China; [Liu, Sifei] Nvidia, Santa Clara, CA USA; [Ma, Huimin] Univ Sci & Technol Beijing, Beijing, Peoples R China; [Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA USA	Tencent; Tsinghua University; Nvidia Corporation; University of Science & Technology Beijing; University of California System; University of California Merced	Ma, HM (corresponding author), Univ Sci & Technol Beijing, Beijing, Peoples R China.	andyxwang@tencent.com; sifeil@nvidia.com; mhmpub@ustb.edu.cn; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019; Liu, Sifei/AGE-1968-2022	Yang, Ming-Hsuan/0000-0003-4848-2304; Liu, Sifei/0000-0002-6011-3686	National Key Basic Research Program of China [2016YFB0100900]; Beijing Science and Technology Planning Project [Z191100007419001]; National Natural Science Foundation of China [61773231]; National Science Foundation [1149783]	National Key Basic Research Program of China(National Basic Research Program of China); Beijing Science and Technology Planning Project; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Science Foundation(National Science Foundation (NSF))	This work is supported by National Key Basic Research Program of China (No. 2016YFB0100900), Beijing Science and Technology Planning Project (No. Z191100007419001), National Natural Science Foundation of China (No. 61773231), and National Science Foundation (CAREER No. 1149783).	Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Bertasius G, 2017, PROC CVPR IEEE, P6137, DOI 10.1109/CVPR.2017.650; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan RC, 2018, LECT NOTES COMPUT SC, V11213, P371, DOI 10.1007/978-3-030-01240-3_23; Fan RC, 2019, PROC CVPR IEEE, P6096, DOI 10.1109/CVPR.2019.00626; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; HAGEN L, 1992, IEEE T COMPUT AID D, V11, P1074, DOI 10.1109/43.159993; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181; Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu SF, 2017, ADV NEUR IN, V30; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Maire M, 2016, PROC CVPR IEEE, P174, DOI 10.1109/CVPR.2016.26; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Pathak D., 2014, 14127144 ARXIV; Pathak D, 2015, IEEE I CONF COMP VIS, P1796, DOI 10.1109/ICCV.2015.209; Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780; Qi XJ, 2016, LECT NOTES COMPUT SC, V9912, P90, DOI 10.1007/978-3-319-46484-8_6; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Roy A, 2017, PROC CVPR IEEE, P7282, DOI 10.1109/CVPR.2017.770; Saleh F, 2016, LECT NOTES COMPUT SC, V9912, P413, DOI 10.1007/978-3-319-46484-8_25; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14; Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147; WEI YC, 1989, 1989 IEEE INTERNATIONAL CONFERENCE ON COMPUTER-AIDED DESIGN, P298; Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759; Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0	43	29	30	6	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1736	1749		10.1007/s11263-020-01293-3	http://dx.doi.org/10.1007/s11263-020-01293-3		JAN 2020	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN		Green Submitted			2022-12-18	WOS:000510276000001
J	Shi, XC; Ling, HB; Pang, Y; Hu, WM; Chu, P; Xing, JL				Shi, Xinchu; Ling, Haibin; Pang, Yu; Hu, Weiming; Chu, Peng; Xing, Junliang			Rank-1 Tensor Approximation for High-Order Association in Multi-target Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-target tracking; Multi-dimensional assignment; Rank-1 tensor approximation; Data association	ALGORITHM	High-order motion information is important in multi-target tracking (MTT) especially when dealing with large inter-target ambiguities. Such high-order information can be naturally modeled as a multi-dimensional assignment (MDA) problem, whose global solution is however intractable in general. In this paper, we propose a novel framework to the problem by reshaping MTT as a rank-1 tensor approximation problem (R1TA). We first show that MDA and R1TA share the same objective function and similar constraints. This discovery opens a door to use high-order tensor analysis for MTT and suggests the exploration of R1TA. In particular, we develop a tensor power iteration algorithm to effectively capture high-order motion information as well as appearance variation. The proposed algorithm is evaluated on a diverse set of datasets including aerial video sequences containing ariel borne dense highway scenes, top-view pedestrian trajectories, multiple similar objects, normal view pedestrians and vehicles. The effectiveness of the proposed algorithm is clearly demonstrated in these experiments.	[Shi, Xinchu; Xing, Junliang] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Hu, Weiming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing, Peoples R China; [Hu, Weiming] Univ Chinese Acad Sci, Beijing, Peoples R China; [Ling, Haibin; Pang, Yu; Chu, Peng] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple University	Hu, WM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing, Peoples R China.; Hu, WM (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.	xcshi@nlpr.ia.ac.cn; hbling@temple.edu; ypang@temple.edu; wmhu@nlpr.ia.ac.cn; peng.chu@temple.edu; jlxing@nlpr.ia.ac.cn	Xing, Junliang/HGE-9630-2022	Xing, Junliang/0000-0001-6801-0510	Beijing Natural Science Foundation [L172051]; Natural Science Foundation of China [61502492, 61751212, 61721004]; NSFC-general technology collaborative Fund for basic research [U1636218]; Key Research Program of Frontier Sciences, CAS [QYZDJ-SSW-JSC040]; CAS External cooperation key project; US NSF [1814745, 1407156, 1350521]	Beijing Natural Science Foundation(Beijing Natural Science Foundation); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); NSFC-general technology collaborative Fund for basic research; Key Research Program of Frontier Sciences, CAS; CAS External cooperation key project; US NSF(National Science Foundation (NSF))	We would like to express our sincere appreciation to Professor Steve Maybank for his valuable suggestion and careful revision on the wordings and grammar in the paper. This work is supported by Beijing Natural Science Foundation (Grant No. L172051), the Natural Science Foundation of China (Grant Nos. 61502492, 61751212, 61721004), the NSFC-general technology collaborative Fund for basic research (Grant No. U1636218), the Key Research Program of Frontier Sciences, CAS, Grant No. QYZDJ-SSW-JSC040, and the CAS External cooperation key project. H. Ling was supported in part by US NSF (Grant Nos. 1814745, 1407156, and 1350521).	Andriluka M., 2008, P IEEE C COMP VIS PA; [Anonymous], [No title captured]; [Anonymous], 2006, CLIF DATASET; [Anonymous], 2017, P AAAI; [Anonymous], [No title captured]; Bae S.-H., 2014, P IEEE C COMP VIS PA; Ban Y., 2016, P EUR C COMP VIS; Bar-Shalom Y., 1988, TRACKING DATA ASS; Benfold B., 2011, P IEEE C COMP VIS PA; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Black J., 2002, WORKSH MOT VIS COMP; Blackman S. S., 1999, DESIGN ANAL MODERN T; Breitenstein MD, 2011, IEEE T PATTERN ANAL, V33, P1820, DOI 10.1109/TPAMI.2010.232; Butt A., 2012, P AS C COMP VIS; Butt A. A., 2013, P IEEE C COMP VIS PA; Chari Visesh, 2015, P IEEE C COMP VIS PA; Chen Z, 2012, IEEE IC COMP COM NET; Collins R. T, 2012, P IEEE C COMP VIS PA; COX IJ, 1993, INT J COMPUT VISION, V10, P53, DOI 10.1007/BF01440847; Dalal N., 2005, HISTOGRAMS ORIENTED; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995; Deb S, 1997, IEEE T AERO ELEC SYS, V33, P523, DOI 10.1109/7.575891; Dicle C., 2013, P IEEE C COMP VIS PA; Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fortmann T. E., 1980, Proceedings of the 19th IEEE Conference on Decision & Control Including the Symposium on Adaptive Processes, P807; Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176; Geiger A., 2012, P IEEE C COMP VIS PA; Huang C., 2008, P EUR C COMP VIS; Kumar K. C. A., 2013, P IEEE C COMP VIS; Le N., 2016, P EUR C COMP VIS; Leal-Taixe L., 2016, P IEEE C COMP VIS PA; Lenz P., 2015, P IEEE INT C COMP VI; Li Y., 2009, P IEEE C COMP VIS PA; Milan A., 2015, ARXIV150401942CS; Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103; Okuma K., 2004, P EUR C COMP VIS; Pirsiavash H., 2011, P IEEE C COMP VIS PA; Poore A. B., 1994, Computational Optimization and Applications, V3, P27, DOI 10.1007/BF01299390; Possegger Horst, 2014, P IEEE C COMP VIS PA; Regalia P.A., 2000, P IEEE INT C AC SPEE; REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177; Ricardo S., 2016, COMPUTER VISION; Schulter S., 2017, P IEEE C COMP VIS PA; Shafique K, 2005, IEEE T PATTERN ANAL, V27, P51, DOI 10.1109/TPAMI.2005.1; Shafique K., 2008, P IEEE C COMP VIS PA; Shi X., 2013, P IEEE C COMP VIS PA; SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591; Son J., 2017, P IEEE C COMP VIS PA; Tang S., 2016, COMPUTER VISION; Tang S., 2015, P IEEE C COMP VIS PA; Wang B., 2016, P IEEE C COMP VIS PA; Wang S., 2016, INT J COMPUTER VISIO; Wen L., 2014, P IEEE C COMP VIS PA; Xing J., 2014, ARXIV PREPRINT ARXIV; Yang B., 2012, P IEEE C COMP VIS PA; Yu Q, 2009, IEEE T PATTERN ANAL, V31, P2196, DOI 10.1109/TPAMI.2008.253; Zamir A., 2012, P EUR C COMP VIS; Zhang L., 2008, P IEEE C COMP VIS PA	62	29	30	1	90	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					1063	1083		10.1007/s11263-018-01147-z	http://dx.doi.org/10.1007/s11263-018-01147-z			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM					2022-12-18	WOS:000474559000006
J	Li, S; Deng, WH				Li, Shan; Deng, Weihong			Blended Emotion in-the-Wild: Multi-label Facial Expression Recognition Using Crowdsourced Annotations and Deep Locality Feature Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Facial expression recognition; Deep feature learning; Multi-label classification; Crowdsourced database in-the-wild		Comprehending different categories of facial expressions plays a great role in the design of computational model analyzing human perceived and affective state. Authoritative studies have revealed that facial expressions in human daily life are in multiple or co-occurring mental states. However, due to the lack of valid datasets, most previous studies are still restricted to basic emotions with single label. In this paper, we present a novel multi-label facial expression database, RAF-ML, along with a new deep learning algorithm, to address this problem. Specifically, a crowdsourcing annotation of 1.2 million labels from 315 participants was implemented to identify the multi-label expressions collected from social network, then EM algorithm was designed to filter out unreliable labels. For all we know, RAF-ML is the first database in the wild that provides with crowdsourced cognition for multi-label expressions. Focusing on the ambiguity and continuity of blended expressions, we propose a new deep manifold learning network, called Deep Bi-Manifold CNN, to learn the discriminative feature for multi-label expressions by jointly preserving the local affinity of deep features and the manifold structures of emotion labels. Furthermore, a deep domain adaption method is leveraged to extend the deep manifold features learned from RAF-ML to other expression databases under various imaging conditions and cultures. Extensive experiments on the RAF-ML and other diverse databases (JAFFE, CK+, SFEW and MMI) show that the deep manifold feature is not only superior in multi-label expression recognition in the wild, but also captures the elemental and generic components that are effective for a wide range of expression recognition tasks.	[Li, Shan; Deng, Weihong] Beijing Univ Posts & Telecommun, Beijing, Peoples R China	Beijing University of Posts & Telecommunications	Deng, WH (corresponding author), Beijing Univ Posts & Telecommun, Beijing, Peoples R China.	ls1995@bupt.edu.cn; whdeng@bupt.edu.cn	Deng, Wei/GWC-9207-2022	Deng, Weihong/0000-0001-5952-6996	National Natural Science Foundation of China [61573068, 61471048]; Beijing Nova Program [Z161100004916088]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission)	The funding was provided by National Natural Science Foundation of China (Grant Nos 61573068, 61471048), Beijing Nova Program (Grant No Z161100004916088).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anitha CM., 2010, INT J ENG SCI TECHNO, V2, P5158; Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600; Chai R, 2014, 2014 14TH INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES (ISCIT), P30, DOI 10.1109/ISCIT.2014.7011864; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chang Y., 2004, COMP VIS PATT REC 20, V2, pII; Chen JX, 2013, PATTERN RECOGN LETT, V34, P1964, DOI 10.1016/j.patrec.2013.02.002; Chu Wen-Sheng, 2013, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2013, P3515; Cour T, 2011, J MACH LEARN RES, V12, P1501; Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197; Csurka G, 2017, ARXIV170205374 CORR; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dhall A, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P423, DOI 10.1145/2818346.2829994; Dhall A, 2015, IEEE T AFFECT COMPUT, V6, P13, DOI 10.1109/TAFFC.2015.2397456; Ding XY, 2013, IEEE I CONF COMP VIS, P2400, DOI 10.1109/ICCV.2013.298; Donahue J, 2014, PR MACH LEARN RES, V32; Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111; Ekman P., 2003, J PERS, P212; Ekman P., 1984, EXPRESSION NATURE EM, V3, P319; Ekman P., 2013, EMOTION HUMAN FACE G, V11; Ekman Paul, 1997, WHAT FACE REVEALS BA, P2; ELEFTHERIADIS S, 2004, TIP, V24, P189, DOI DOI 10.1109/TIP.2014.2375634; Eleftheriadis S, 2015, IEEE I CONF COMP VIS, P3792, DOI 10.1109/ICCV.2015.432; Furnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8; GAO BB, 2017, TIP, V26, P2825, DOI DOI 10.1109/TIP.2017.2689998; Gretton A, 2012, ADV NEURAL INF PROCE; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; Haibin Yan, 2011, 2011 IEEE International Conference on Robotics and Automation, P5985; Hassin RR, 2013, EMOT REV, V5, P60, DOI 10.1177/1754073912451331; He XF, 2004, ADV NEUR IN, V16, P153; Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593; Hou P, 2016, AAAI CONF ARTIF INTE, P1680; Huang S.J., 2012, P AAAI C ART INT; Inc. M, 2013, FAC RES TOOLK; Izard C. E, 1972, ANXIETY CURRENT TREN, V1, P55; Izard CE., 1977, HUMAN EMOTIONS; Jack RE, 2012, P NATL ACAD SCI USA, V109, P7241, DOI 10.1073/pnas.1200155109; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341; Kim BK, 2016, J MULTIMODAL USER IN, V10, P173, DOI 10.1007/s12193-015-0209-0; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LI S, 1970, TIP, V28, P356, DOI DOI 10.1109/TIP.2018.2868382; Li S., 2018, ARXIV180408348 CORR; Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277; LIU M, 1932, TIP, V25, P5920, DOI DOI 10.1109/TIP.2016.2615424; Liu M., 2013, 10 IEEE INT C WORKSH, P1, DOI [DOI 10.1109/FG.2013.6553734, DOI 10.1128/GEN0MEA.00300-13]; Liu MY, 2015, LECT NOTES COMPUT SC, V9006, P143, DOI 10.1007/978-3-319-16817-3_10; Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226; Long MS, 2015, PR MACH LEARN RES, V37, P97; Lucey P., 2010, P IEEE COMP SOC C CO, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949; Miao YQ, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 2, P326, DOI 10.1109/ICMLA.2012.178; Mollahosseini A., 2016, PROC IEEE WINTER C A, P1, DOI [10.1109/WACV.2016.7477450, DOI 10.1109/WACV.2016.7477450]; NUMMENMAA T, 1988, SCAND J PSYCHOL, V29, P33, DOI 10.1111/j.1467-9450.1988.tb00773.x; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Plutchik R, 1991, EMOTIONS; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Russell JA, 1999, J PERS SOC PSYCHOL, V76, P805, DOI 10.1037/0022-3514.76.5.805; SARIYANIDI E, 1978, TIP, V26, P1965, DOI DOI 10.1109/TIP.2017.2662237; Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Tomkins, 1963, AFFECT IMAGERY CONSC, V2; Tsoumakas G., 2007, INT J DATA WAREHOUSI, V3, P1; Tsoumakas G, 2007, LECT NOTES ARTIF INT, V4701, P406; Valstar M, 2010, PROCEEDING 2010 IEEE, P65; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wang SF, 2014, IMAGE VISION COMPUT, V32, P682, DOI 10.1016/j.imavis.2014.04.013; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Whitehill J, 2009, ADV NEURAL INFORM PR, P2035; Xing C, 2016, PROC CVPR IEEE, P4489, DOI 10.1109/CVPR.2016.486; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435; Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421; Zeng JB, 2015, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2015.413; Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019; Zhang ML, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4048; Zhang ML, 2015, IEEE T PATTERN ANAL, V37, P107, DOI 10.1109/TPAMI.2014.2339815; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39; Zhang ZP, 2018, INT J COMPUT VISION, V126, P550, DOI 10.1007/s11263-017-1055-1; Zhao KL, 2015, NEUROCOMPUTING, V157, P280, DOI 10.1016/j.neucom.2015.01.005; Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974; Zhou Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1247, DOI 10.1145/2733373.2806328; Zhu RH, 2016, INT CONF BIOMETR; Zong Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P872, DOI 10.1145/3123266.3123367	89	29	33	6	42	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		884	906		10.1007/s11263-018-1131-1	http://dx.doi.org/10.1007/s11263-018-1131-1			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	HZ0JD					2022-12-18	WOS:000468525900019
J	Supancic, JS; Rogez, G; Yang, Y; Shotton, J; Ramanan, D				Supancic, James Steven, III; Rogez, Gregory; Yang, Yi; Shotton, Jamie; Ramanan, Deva			Depth-Based Hand Pose Estimation: Methods, Data, and Challenges	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hand pose; RGB-D sensor; Datasets; Benchmarking	REAL-TIME; RECOGNITION; MODELS	Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and have released software and evaluation code. We summarize important conclusions here: (1) Coarse pose estimation appears viable for scenes with isolated hands. However, high precision pose estimation [required for immersive virtual reality and cluttered scenes (where hands may be interacting with nearby objects and surfaces) remain a challenge. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.	[Supancic, James Steven, III] Univ Calif Irvine, Irvine, CA 92697 USA; [Rogez, Gregory] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France; [Rogez, Gregory] Univ Grenoble Alpes, Inst Engn, Grenoble, France; [Yang, Yi] Baidu Inst Deep Learning, Sunnyvale, CA USA; [Shotton, Jamie] Microsoft Res, Cambridge, England; [Ramanan, Deva] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University of California System; University of California Irvine; Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria; Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Baidu; Microsoft; Carnegie Mellon University	Supancic, JS (corresponding author), Univ Calif Irvine, Irvine, CA 92697 USA.	jsupanci@uci.edu	yang, yang/HGT-7999-2022; yang, yang/GWB-9426-2022; Yang, Yi/B-9273-2017; yang, yang/GVT-5210-2022	Yang, Yi/0000-0002-0512-880X; 	National Science Foundation [0954083]; Office of Naval Research-MURI Grant [N00014-10-1-0933]; Intel Science and Technology Center-Visual Computing; European Commission FP7 Marie Curie IOF grant "Egovision4Health" [PIOF-GA-2012-328288]	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research-MURI Grant; Intel Science and Technology Center-Visual Computing; European Commission FP7 Marie Curie IOF grant "Egovision4Health"	National Science Foundation Grant 0954083, Office of Naval Research-MURI Grant N00014-10-1-0933, and the Intel Science and Technology Center-Visual Computing supported JS&DR. The European Commission FP7 Marie Curie IOF grant "Egovision4Health" (PIOF-GA-2012-328288) supported GR.	Ballan L, 2012, LECT NOTES COMPUT SC, V7577, P640, DOI 10.1007/978-3-642-33783-3_46; Bray M., 2004, 1 EUR C VIS MED PROD; Bullock I. M., 2013, IEEE T GRASP FREQUEN; Camplani M., 2012, P SPIE; Castellini C., 2011, IEEE T AUTONOMOUS ME; Choi C, 2015, IEEE I CONF COMP VIS, P2336, DOI 10.1109/ICCV.2015.269; Cooper H, 2012, J MACH LEARN RES, V13, P2205; Delamarre Q, 2001, COMPUT VIS IMAGE UND, V81, P328, DOI 10.1006/cviu.2000.0892; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Fathi A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3281, DOI 10.1109/CVPR.2011.5995444; Feix T., 2013, IEEE T ROBOTICS; Girard M, 1985, COMPUT GRAPH, P263, DOI DOI 10.1145/325334.325244; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Intel, 2013, PERC COMP SDK; Janoch A., 2013, CONSUMER DEPTH CAMER, P141, DOI DOI 10.1007/978-1-4471-4640-7_8; Keskin C, 2012, LECT NOTES COMPUT SC, V7577, P852, DOI 10.1007/978-3-642-33783-3_61; Khamis S, 2015, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR.2015.7298869; Li C., 2013, COMPUTER VISION PATT; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Li PY, 2015, IEEE I CONF COMP VIS, P819, DOI 10.1109/ICCV.2015.100; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Melax S., 2013, P ACM SIGGRAPH S INT; Mo Z., 2006, 2006 IEEE COMP SOC C, P1499, DOI DOI 10.1109/CVPR.2006.237; Moore A. W., 2001, MINING THE SKY; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; Oberweger M, 2016, PROC CVPR IEEE, P4957, DOI 10.1109/CVPR.2016.536; Oberweger M, 2015, IEEE I CONF COMP VIS, P3316, DOI 10.1109/ICCV.2015.379; Oberweger Markus, 2015, ARXIV150206807; Ohn-Bar E, 2014, IEEE T INTELL TRANSP, V15, P2368, DOI 10.1109/TITS.2014.2337331; Oikonomidis I, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.101; Pang Y., 2013, INT C COMP VIS ICCV; Pedro F. F., 2010, PAMI, V32, P1627, DOI [10.1109/TPAMI.2009.167, DOI 10.1109/TPAMI.2009.167]; Pieropan A., 2014, INT C INT ROB SYST I; Premaratne P., 2010, HUMAN COMPUTER INTER; PrimeSense, 2013, NITE2 MIDDL VERS 2 2; Qian C., 2014, COMPUTER VISION PATT; Ren Z., 2011, P 19 ACM INT C MULTI, P759, DOI DOI 10.1145/2072298.2072443; Rogez G., 2014, CDC4CV WORKSH EUR C; Rogez G., 2015, COMPUTER VISION PATT; Rogez G, 2015, IEEE I CONF COMP VIS, P3889, DOI 10.1109/ICCV.2015.443; Romero J., 2009, INT C HUM ROB; Russakovsky O., 2013, INT C COMP VIS ICCV; Sari M., 2011, LIBHAND LIB HAND ART; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Sharp T., 2015, ACM C COMP HUM INT; Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381; Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41; Sridhar S., 2015, COMPUTER VISION PATT; Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305; Stenger B., 2006, IEEE T PATTERN ANAL; Stokoe WC, 2005, J DEAF STUD DEAF EDU, V10, P3, DOI 10.1093/deafed/eni001; Sun X, 2015, PROC CVPR IEEE, P824, DOI 10.1109/CVPR.2015.7298683; Tang D., 2014, COMPUTER VISION PATT; Tang DH, 2015, IEEE I CONF COMP VIS, P3325, DOI 10.1109/ICCV.2015.380; Tang DH, 2013, IEEE I CONF COMP VIS, P3224, DOI 10.1109/ICCV.2013.400; Taylor J., 2014, COMPUTER VISION PATT; Taylor J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925965; Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tzionas D., 2014, LECT NOTES COMPUTER; Vezhnevets V., 2003, P GRAPH MOSC RUSS; Wan CD, 2016, LECT NOTES COMPUT SC, V9907, P554, DOI 10.1007/978-3-319-46487-9_34; Wetzler A., 2015, BRIT MACH VIS C BMVC; Xu C., 2013, INT C COMP VIS ICCV; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Ye Q, 2016, LECT NOTES COMPUT SC, V9912, P346, DOI 10.1007/978-3-319-46484-8_21; Zhu X, 2012, BRIT MACH VIS C BMVC, V3, P5	72	29	30	3	30	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2018	126	11					1180	1198		10.1007/s11263-018-1081-7	http://dx.doi.org/10.1007/s11263-018-1081-7			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GT3IE		Green Submitted			2022-12-18	WOS:000444394200002
J	Li, JW; Ma, AJ; Yuen, PC				Li, Jiawei; Ma, Andy J.; Yuen, Pong C.			Semi-supervised Region Metric Learning for Person Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Semi-supervised learning; Imbalanced unlabeled data		In large-scale camera networks, label information for person re-identification is usually not available under a large amount of cameras due to expensive human labor efforts. Semi-supervised learning could be employed to train a discriminative classifier by using unlabeled data and unmatched image pairs (negatives) generated from non-overlapping camera views, but existing methods suffer from the problem of imbalanced unlabeled data. In this context, this paper proposes a novel semi-supervised region metric learning method to improve person re-identification performance under imbalanced unlabeled data. Firstly, instead of seeking for matched image pairs (positives) from the unlabeled data, we propose to estimate positive neighbors by label propagation with cross person score distribution alignment. Secondly, multiple positive regions are generated using sets of positive neighbors to learn a discriminative region-to-point metric. Experimental results demonstrate that the superiority of the proposed method over existing unsupervised, semi-supervised and person re-identification methods.	[Li, Jiawei; Ma, Andy J.; Yuen, Pong C.] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China; [Ma, Andy J.] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China	Hong Kong Baptist University; Sun Yat Sen University	Yuen, PC (corresponding author), Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.	jwli@comp.hkbu.edu.hk; majh8@mail.sysu.edu.cn; pcyuen@comp.hkbu.edu.hk	Jinhua, Andy/Y-9408-2019; Li, Jiawei/GXM-4151-2022		Hong Kong RGC General Research Fund [HKBU 212313, HKBU 12202514]; SYSU Research Fund [67000-18821116]	Hong Kong RGC General Research Fund; SYSU Research Fund	This work is partially supported by Hong Kong RGC General Research Fund HKBU 212313, HKBU 12202514 and SYSU Research Fund 67000-18821116. The authors would like to thank the editor and reviewers for their helpful comments which improve the quality of this paper.	Ahmed E., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299016; Bootkrajang Jakramate, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P143, DOI 10.1007/978-3-642-33460-3_15; Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Chen Y.-T., 2017, IEEE T POWER ELECTR, V99, P1; Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Figueira D, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P111, DOI 10.1109/AVSS.2013.6636625; Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894; Gheissari N., 2006, P IEEE C COMP VIS PA, V2, P1528, DOI DOI 10.1109/CVPR.2006.223; Gray D., 2007, 10 IEEE INT WORKSH P; Hirzer M, 2012, LECT NOTES COMPUT SC, V7577, P780, DOI 10.1007/978-3-642-33783-3_56; Jing XY, 2015, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2015.7298669; Kodirov E, 2016, LECT NOTES COMPUT SC, V9905, P178, DOI 10.1007/978-3-319-46448-0_11; Kostinger Martin, 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247939; Kumar KCA, 2013, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2013.250; Kviatkovsky I, 2013, IEEE T PATTERN ANAL, V35, P1622, DOI 10.1109/TPAMI.2012.246; Lan XY, 2018, IEEE T IMAGE PROCESS, V27, P2022, DOI 10.1109/TIP.2017.2777183; Lan XY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2481325; Lan XY, 2014, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2014.156; Lee Wee Sun, 2003, ICML, P448, DOI DOI 10.1016/J.TCS.2005.09.007; Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782; Li FQ, 2014, NEW REV HYPERMEDIA M, V20, P5, DOI 10.1080/13614568.2013.846416; Li S., 2011, IJCAI; Li W., 2013, LNCS, V7724, P31, DOI [10.1007/978-3-642-37331-2, DOI 10.1007/978-3-642-37331-2]; Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461; Li Xiaoli, 2003, IJCAI 03 P 18 INT JO, P587; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin J, 2017, PROC CVPR IEEE, P3396, DOI 10.1109/CVPR.2017.362; Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055; Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Liu X, 2014, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2014.454; Liu Z., 2017, THE IEEE INTERNATION; Ma AJ, 2015, IEEE T IMAGE PROCESS, V24, P1599, DOI 10.1109/TIP.2015.2395715; Ma AJ, 2015, LECT NOTES COMPUT SC, V9007, P397, DOI 10.1007/978-3-319-16814-2_26; Ma B., 2012, COMPUTER VISION ECCV; Mahmood A, 2014, PROC CVPR IEEE, P121, DOI 10.1109/CVPR.2014.23; Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152; Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146; Prosser B. J., 2010, PROC BRIT MACH VIS C, P6, DOI DOI 10.5244/C.24.21; Roth PM, 2014, ADV COMPUT VIS PATT, P247, DOI 10.1007/978-1-4471-6296-4_12; Xie J., 2011, ACT LEARN EXP DES WO, P85; Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35; Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550; Ye M, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1239, DOI 10.1145/2733373.2806326; Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058; Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138; Zheng Wei-Shi, 2009, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.23.23; Zhu PF, 2013, IEEE I CONF COMP VIS, P2664, DOI 10.1109/ICCV.2013.331	55	29	30	2	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2018	126	8					855	874		10.1007/s11263-018-1075-5	http://dx.doi.org/10.1007/s11263-018-1075-5			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GL5ZB					2022-12-18	WOS:000437253500004
J	Chen, D; Mirebeau, JM; Cohen, LD				Chen, Da; Mirebeau, Jean-Marie; Cohen, Laurent D.			Global Minimum for a Finsler Elastica Minimal Path Approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Minimal path; Geodesic; Eikonal equation; Curvature penalty; Anisotropic fast marching method; Image segmentation; Tubular structure extraction	HAMILTON-JACOBI EQUATIONS; ACTIVE CONTOUR MODELS; EULERS ELASTICA; SEGMENTATION; EXTRACTION; ALGORITHMS; GEODESICS; CURVES; SHAPE	In this paper, we propose a novel curvature penalized minimal path model via an orientation-lifted Finsler metric and the Euler elastica curve. The original minimal path model computes the globally minimal geodesic by solving an Eikonal partial differential equation (PDE). Essentially, this first-order model is unable to penalize curvature which is related to the path rigidity property in the classical active contour models. To solve this problem, we present an Eikonal PDE-based Finsler elastica minimal path approach to address the curvature-penalized geodesic energy minimization problem. We were successful at adding the curvature penalization to the classical geodesic energy (Caselles et al. in Int J Comput Vis 22(1):61-79, 1997; Cohen and Kimmel in Int J Comput Vis 24(1):57-78, 1997). The basic idea of this work is to interpret the Euler elastica bending energy via a novel Finsler elastica metric that embeds a curvature penalty. This metric is non-Riemannian, anisotropic and asymmetric, and is defined over an orientation-lifted space by adding to the image domain the orientation as an extra space dimension. Based on this orientation lifting, the proposed minimal path model can benefit from both the curvature and orientation of the paths. Thanks to the fast marching method, the global minimum of the curvature-penalized geodesic energy can be computed efficiently. We introduce two anisotropic image data-driven speed functions that are computed by steerable filters. Based on these orientation-dependent speed functions, we can apply the proposed Finsler elastica minimal path model to the applications of closed contour detection, perceptual grouping and tubular structure extraction. Numerical experiments on both synthetic and real images show that these applications of the proposed model indeed obtain promising results.	[Chen, Da; Cohen, Laurent D.] PSL Res Univ, Univ Paris Dauphine, CNRS, UMR 7534,CEREMADE, F-75016 Paris, France; [Mirebeau, Jean-Marie] Univ Paris Saclay, Univ Paris Sud, CNRS, Lab Math Orsay, F-91405 Orsay, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay	Chen, D (corresponding author), PSL Res Univ, Univ Paris Dauphine, CNRS, UMR 7534,CEREMADE, F-75016 Paris, France.	chenda@ceremade.dauphine.fr; jean-marie.mirebeau@math.u-psud.fr; cohen@ceremade.dauphine.fr			ANR Grant NS-LBR [ANR-13-JS01-0003-01]	ANR Grant NS-LBR(French National Research Agency (ANR))	The authors would like to thank all the anonymous reviewers for their detailed remarks that helped us improve the presentation of this paper. This work was partially supported by ANR Grant NS-LBR, ANR-13-JS01-0003-01.	Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130; Appia V, 2011, IEEE I CONF COMP VIS, P1975, DOI 10.1109/ICCV.2011.6126468; Appleton B, 2005, J MATH IMAGING VIS, V23, P67, DOI 10.1007/s10851-005-4968-1; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bekkers EJ, 2015, SIAM J IMAGING SCI, V8, P2740, DOI 10.1137/15M1018460; Benmansour F, 2011, INT J COMPUT VISION, V92, P192, DOI 10.1007/s11263-010-0331-0; Benmansour F, 2009, J MATH IMAGING VIS, V33, P209, DOI 10.1007/s10851-008-0131-0; Bornemann F, 2006, COMPUT VIS SCI, V9, P57, DOI 10.1007/s00791-006-0016-y; Bougleux S, 2008, LECT NOTES COMPUT SC, V5303, P129, DOI 10.1007/978-3-540-88688-4_10; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; Chan TF, 2003, SIAM J APPL MATH, V63, P564; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; Cohen LD, 2001, J MATH IMAGING VIS, V14, P225, DOI 10.1023/A:1011281928379; Deschamps T, 2001, MED IMAGE ANAL, V5, P281, DOI 10.1016/S1361-8415(01)00046-9; Dijkstra EW, 1959, NUMER MATH, V1, P269, DOI 10.1007/BF01386390; DIZENZO S, 1986, COMPUT VISION GRAPH, V33, P116, DOI 10.1016/0734-189X(86)90223-9; El-Zehiry NY, 2010, PROC CVPR IEEE, P3257, DOI 10.1109/CVPR.2010.5540057; Jacob M, 2004, IEEE T PATTERN ANAL, V26, P1007, DOI 10.1109/TPAMI.2004.44; Jbabdi S., 2008, INT J BIOMED IMAGING, V2008, P2; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kaul V, 2012, IEEE T PATTERN ANAL, V34, P1952, DOI 10.1109/TPAMI.2011.267; Kimmel R, 2001, J MATH IMAGING VIS, V14, P237, DOI 10.1023/A:1011234012449; Law MWK, 2008, LECT NOTES COMPUT SC, V5305, P368, DOI 10.1007/978-3-540-88693-8_27; Li H, 2007, IEEE T MED IMAGING, V26, P1213, DOI 10.1109/TMI.2007.903696; LIONS PL, 1982, GENERALIZED SOLUTION, V69; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; Malladi R., 1994, LECT NOTES COMPUTER, V800, P1; Mille J, 2015, INT J COMPUT VISION, V112, P1, DOI 10.1007/s11263-014-0751-3; Mirebeau JM, 2014, SIAM J NUMER ANAL, V52, P1573, DOI 10.1137/120861667; Mirebeau JM, 2014, NUMER MATH, V126, P515, DOI 10.1007/s00211-013-0571-3; Mumford D., 1994, ALGEBRAIC GEOMETRY I, V5681, P491, DOI DOI 10.1007/978-1-4612-2628-4_31; Nitzberg M., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P138, DOI 10.1109/ICCV.1990.139511; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Pechaud M, 2009, PROC CVPR IEEE, P336, DOI 10.1109/CVPRW.2009.5206782; Petitot J, 2003, J PHYSIOL-PARIS, V97, P265, DOI 10.1016/j.jphysparis.2003.10.010; Peyre G, 2009, FOUND TRENDS COMPUT, V5, DOI 10.1561/0600000029; Rouchdy Y, 2013, COMPUT VIS IMAGE UND, V117, P1453, DOI 10.1016/j.cviu.2013.06.001; Sanguinetti G, 2015, LECT NOTES COMPUT SC, V9423, P366, DOI 10.1007/978-3-319-25751-8_44; Schoenemann T, 2012, INT J COMPUT VISION, V99, P53, DOI 10.1007/s11263-012-0518-7; Schoenemann T, 2011, IEEE T IMAGE PROCESS, V20, P2565, DOI 10.1109/TIP.2011.2118225; Sethian JA, 1999, SIAM REV, V41, P199, DOI 10.1137/S0036144598347059; Sethian JA, 2003, SIAM J NUMER ANAL, V41, P325, DOI 10.1137/S0036142901392742; Tai XC, 2011, SIAM J IMAGING SCI, V4, P313, DOI 10.1137/100803730; TSITSIKLIS JN, 1995, IEEE T AUTOMAT CONTR, V40, P1528, DOI 10.1109/9.412624; Ulen J, 2015, IEEE T PATTERN ANAL, V37, P2588, DOI 10.1109/TPAMI.2015.2409869; Yezzi A, 1997, IEEE T MED IMAGING, V16, P199, DOI 10.1109/42.563665; Zhu W, 2013, J SCI COMPUT, V57, P414, DOI 10.1007/s10915-013-9710-3; [No title captured]	51	29	29	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2017	122	3			SI		458	483		10.1007/s11263-016-0975-5	http://dx.doi.org/10.1007/s11263-016-0975-5			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ES7OH		Green Submitted			2022-12-18	WOS:000399739200005
J	Ma, ZG; Yang, Y; Nie, FP; Sebe, N; Yan, SC; Hauptmann, AG				Ma, Zhigang; Yang, Yi; Nie, Feiping; Sebe, Nicu; Yan, Shuicheng; Hauptmann, Alexander G.			Harnessing Lab Knowledge for Real-World Action Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; Lab to real-world; Transfer learning; General Schatten-p norm		Much research on human action recognition has been oriented toward the performance gain on lab-collected datasets. Yet real-world videos are more diverse, with more complicated actions and often only a few of them are precisely labeled. Thus, recognizing actions from these videos is a tough mission. The paucity of labeled real-world videos motivates us to "borrow" strength from other resources. Specifically, considering that many lab datasets are available, we propose to harness lab datasets to facilitate the action recognition in real-world videos given that the lab and real-world datasets are related. As their action categories are usually inconsistent, we design a multi-task learning framework to jointly optimize the classifiers for both sides. The general Schatten -norm is exerted on the two classifiers to explore the shared knowledge between them. In this way, our framework is able to mine the shared knowledge between two datasets even if the two have different action categories, which is a major virtue of our method. The shared knowledge is further used to improve the action recognition in the real-world videos. Extensive experiments are performed on real-world datasets with promising results.	[Ma, Zhigang; Hauptmann, Alexander G.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Yang, Yi] Univ Queensland, ITEE, Brisbane, Qld, Australia; [Nie, Feiping] Univ Texas Arlington, Arlington, TX 76019 USA; [Sebe, Nicu] Univ Trento, Trento, Italy; [Yan, Shuicheng] Natl Univ Singapore, Singapore 117548, Singapore	Carnegie Mellon University; University of Queensland; University of Texas System; University of Texas Arlington; University of Trento; National University of Singapore	Yang, Y (corresponding author), Univ Queensland, ITEE, Brisbane, Qld, Australia.	yee.i.yang@gmail.com	Nie, Feiping/B-3039-2012; Yan, Shuicheng/HCI-1431-2022; Ma, Zhigang/H-3543-2015; yang, yang/GWB-9426-2022; yang, yang/GVT-5210-2022; yang, yang/HGT-7999-2022; Yang, Yi/B-9273-2017	Yang, Yi/0000-0002-0512-880X; Sebe, Niculae/0000-0002-6597-7248	US Department of Defense; U.S. Army Research Office [W911NF-13-1-0277]; National Science Foundation [IIS-1251187]; xLiMe EC project; ARC [DE130101311]; Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative; Div Of Information & Intelligent Systems [1251187] Funding Source: National Science Foundation	US Department of Defense(United States Department of Defense); U.S. Army Research Office; National Science Foundation(National Science Foundation (NSF)); xLiMe EC project; ARC(Australian Research Council); Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative(National Research Foundation, Singapore); Div Of Information & Intelligent Systems(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This paper was partially supported by the US Department of Defense, the U.S. Army Research Office (W911NF-13-1-0277) and by the National Science Foundation under Grant No. IIS-1251187, the xLiMe EC project, the ARC Project DE130101311 and the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.	[Anonymous], 2007, P 15 ACM INT C MULTI; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Argyriou A, 2010, J MACH LEARN RES, V11, P935; Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504; Cao LL, 2010, PROC CVPR IEEE, P1998, DOI 10.1109/CVPR.2010.5539875; Chen C, 2011, IEEE T VIS COMPUT GR, V17, P1676, DOI 10.1109/TVCG.2010.272; Chen M. Y., 2009, CMUCS09161 CARN MELL; Deselaers T, 2012, INT J COMPUT VISION, V100, P275, DOI 10.1007/s11263-012-0538-3; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114; Farhadi A, 2008, LECT NOTES COMPUT SC, V5302, P154, DOI 10.1007/978-3-540-88682-2_13; Hsu C. W., 2003, PRACTICAL GUIDE SUPP; Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924; Jie L, 2011, IEEE I CONF COMP VIS, P1863, DOI 10.1109/ICCV.2011.6126454; Klaser A., 2008, BRIT MACH VIS C; Kovashka A, 2010, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2010.5539881; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378; Laptev I., 2008, IEEE C COMP VIS PATT; Liu JG, 2009, PROC CVPR IEEE, P1996; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma Z, 2012, P 20 ACM INT C MULT, P469, DOI [10.1145/2393347.2393414, DOI 10.1145/2393347.2393414]; Nie F., 2012, P AAAI C ART INT; Obozinski G, 2010, STAT COMPUT, V20, P231, DOI 10.1007/s11222-008-9111-x; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Qadir O, 2011, IEEE C EVOL COMPUTAT, P208; Qi GJ, 2011, PROC CVPR IEEE, P897, DOI 10.1109/CVPR.2011.5995312; Saberian M. J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2929, DOI 10.1109/CVPR.2011.5995605; Salakhutdinov R, 2011, PROC CVPR IEEE, P1481, DOI 10.1109/CVPR.2011.5995720; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shi QF, 2011, INT J COMPUT VISION, V93, P22, DOI 10.1007/s11263-010-0384-0; Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56; Wang Heng, 2009, BMVC; Wang LA, 2011, INT J COMPUT VISION, V93, P162, DOI 10.1007/s11263-010-0393-z; Wang S, 2012, PROC CVPR IEEE, P1370, DOI 10.1109/CVPR.2012.6247823; Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48; Wu XX, 2011, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2011.5995624; Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023; You D, 2010, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR.2010.5539952; Yu XD, 2010, LECT NOTES COMPUT SC, V6315, P127	42	29	29	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2014	109	1-2			SI		60	73		10.1007/s11263-014-0717-5	http://dx.doi.org/10.1007/s11263-014-0717-5			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AI7QY					2022-12-18	WOS:000337091700005
J	Delbracio, M; Muse, P; Almansa, A; Morel, JM				Delbracio, Mauricio; Muse, Pablo; Almansa, Andres; Morel, Jean-Michel			The Non-parametric Sub-pixel Local Point Spread Function Estimation Is a Well Posed Problem	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image blur; Subpixel convolution kernel estimation; Aliasing; Inverse problems; Camera quality assessment; Point spread function; Modulated transfer function	MODULATION TRANSFER-FUNCTION; PSF ESTIMATION; SUPERRESOLUTION; NOISE	Most medium to high quality digital cameras (dslrs) acquire images at a spatial rate which is several times below the ideal Nyquist rate. For this reason only aliased versions of the cameral point-spread function (psf) can be directly observed. Yet, it can be recovered, at a sub-pixel resolution, by a numerical method. Since the acquisition system is only locally stationary, this psf estimation must be local. This paper presents a theoretical study proving that the sub-pixel psf estimation problem is well-posed even with a single well chosen observation. Indeed, theoretical bounds show that a near-optimal accuracy can be achieved with a calibration pattern mimicking a Bernoulli(0.5) random noise. The physical realization of this psf estimation method is demonstrated in many comparative experiments. We use an algorithm to accurately estimate the pattern position and its illumination conditions. Once this accurate registration is obtained, the local psf can be directly computed by inverting a well conditioned linear system. The psf estimates reach stringent accuracy levels with a relative error of the order of 2% to 5%. To the best of our knowledge, such a regularization-free and model-free sub-pixel psf estimation scheme is the first of its kind.	[Delbracio, Mauricio; Morel, Jean-Michel] ENS, CMLA, F-94235 Cachan, France; [Delbracio, Mauricio; Muse, Pablo] Univ Republica, IIE, Fac Ingn, Montevideo 11300, Uruguay; [Almansa, Andres] Telecom ParisTech, LTCI, F-75634 Paris 13, France; [Almansa, Andres] CNRS, F-75634 Paris 13, France	UDICE-French Research Universities; Universite Paris Saclay; Universidad de la Republica, Uruguay; IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite	Delbracio, M (corresponding author), ENS, CMLA, 61 Ave President Wilson, F-94235 Cachan, France.	mdelbra@fing.edu.uy; pmuse@fing.edu.uy; andres.almansa@telecom-paristech.fr; morel@cmla.ens-cachan.fr	Almansa, Andrés/A-4152-2008	Morel, Jean-Michel/0000-0002-6108-897X	Uruguayan Agency for Research and Innovation (ANII) [PR-POS-2008-003]; ECOS-Sud Project [U06E01]; FUI FEDER (CEDCA); STIC AmSud project MMVPSCV; MISS-CNES project; ONR [N00014-97-1-0839]; Callisto [ANR-09-CORD-003]; European Research Council	Uruguayan Agency for Research and Innovation (ANII); ECOS-Sud Project; FUI FEDER (CEDCA); STIC AmSud project MMVPSCV; MISS-CNES project; ONR(Office of Naval Research); Callisto; European Research Council(European Research Council (ERC)European Commission)	The authors would like to thank Rafael Grompone von Gioi and Said Ladjal for fruitful comments and discussions. This work was partially funded by: the Uruguayan Agency for Research and Innovation (ANII) under grant PR-POS-2008-003, ECOS-Sud Project number U06E01, FUI FEDER (CEDCA), STIC AmSud project MMVPSCV, MISS-CNES project, ONR grant N00014-97-1-0839, Callisto (ANR-09-CORD-003), and the European Research Council advanced grant "Twelve labours".	[Anonymous], 2000, 122332000 ISO; Backman S, 2003, P SOC PHOTO-OPT INS, V4876, P1100, DOI 10.1117/12.463918; Backman SM, 2004, OPT EXPRESS, V12, P2610, DOI 10.1364/OPEX.12.002610; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Bouguet J. Y., 2008, CAMERA CALIBRATION T; Brauers J., 2010, P SPIE INT SOC OPTIC, V7537; Capel D. P., 2004, IMAGE MOSAICING SUPE; CHALMOND B, 1991, CVGIP-GRAPH MODEL IM, V53, P364, DOI 10.1016/1049-9652(91)90039-M; Claxton CD, 2008, J OPT SOC AM A, V25, P159, DOI 10.1364/JOSAA.25.000159; DANIELS A, 1995, OPT ENG, V34, P860, DOI 10.1117/12.190433; Delbracio M., 2011, IM PROC LIN IPOL WOR; Goodman J., 1996, OPT ENG, V2nd, DOI DOI 10.1117/1.601121; Grant M., 2009, 711 U CAMBR; Harris C. G., 1988, P 4 ALV VIS C, V15, P10, DOI [10.5244/C.2.23, DOI 10.5244/C.2.23]; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; Joshi N., 2008, CVPR, P1; Joshi N., 2008, THESIS U CALIFORNIA; Ladjal S, 2005, THESIS ECOLE NORMALE; Levy E, 1999, APPL OPTICS, V38, P679, DOI 10.1364/AO.38.000679; LLC I, 2010, LLC I IM 3 6; Lucchese L, 2002, APCCAS 2002: ASIA-PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, VOL 2, PROCEEDINGS, P191, DOI 10.1109/APCCAS.2002.1115151; LUXEN M., 2002, P PHOT COMP VIS PCV, VXXXIV-3A, P205; Marion A, 1997, ACQUISITION VISUALIS; Ouyang C, 2005, P ANN INT IEEE EMBS, P6383, DOI 10.1109/IEMBS.2005.1615958; PORTUGAL LF, 1994, MATH COMPUT, V63, P625, DOI 10.2307/2153286; REICHENBACH SE, 1991, OPT ENG, V30, P170, DOI 10.1117/12.55783; Rooms F, 2004, PROC SPIE, V5607, P26, DOI 10.1117/12.571539; Smith EHB, 2006, P SOC PHOTO-OPT INS, V6059, pE590; Sprengel R, 1997, P IEEE EMBS, V18, P1190, DOI 10.1109/IEMBS.1996.652767; Sroubek F, 2007, IEEE T IMAGE PROCESS, V16, P2322, DOI 10.1109/TIP.2007.903256; Tian H, 2001, IEEE J SOLID-ST CIRC, V36, P92, DOI 10.1109/4.896233; Williams C. S., 2002, SPIE PRESS MONOGRAPH, VPM112; Yadid-Pecht O, 2000, OPT ENG, V39, P859, DOI 10.1117/1.602462; Zandhuis JA, 1997, IEE P-VIS IMAGE SIGN, V144, P285, DOI 10.1049/ip-vis:19971307; Zhang W, 2008, IEEE IMAGE PROC, P329, DOI 10.1109/ICIP.2008.4711758; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zhao T., 2006, P SPIE INT SOC OPTIC, V6034	37	29	32	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2012	96	2					175	194		10.1007/s11263-011-0460-0	http://dx.doi.org/10.1007/s11263-011-0460-0			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	876AW		Green Submitted			2022-12-18	WOS:000299080200003
J	Guillaumin, M; Mensink, T; Verbeek, J; Schmid, C				Guillaumin, Matthieu; Mensink, Thomas; Verbeek, Jakob; Schmid, Cordelia			Face Recognition from Caption-Based Supervision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face recognition; Metric learning; Weakly supervised learning; Face retrieval; Constrained clustering		In this paper, we present methods for face recognition using a collection of images with captions. We consider two tasks: retrieving all faces of a particular person in a data set, and establishing the correct association between the names in the captions and the faces in the images. This is challenging because of the very large appearance variation in the images, as well as the potential mismatch between images and their captions. For both tasks, we compare generative and discriminative probabilistic models, as well as methods that maximize subgraph densities in similarity graphs. We extend them by considering different metric learning techniques to obtain appropriate face representations that reduce intra person variability and increase inter person separation. For the retrieval task, we also study the benefit of query expansion. To evaluate performance, we use a new fully labeled data set of 31147 faces which extends the recent Labeled Faces in the Wild data set. We present extensive experimental results which show that metric learning significantly improves the performance of all approaches on both tasks.	[Guillaumin, Matthieu; Mensink, Thomas; Verbeek, Jakob; Schmid, Cordelia] INRIA Rhone Alpes, F-38330 Montbonnot St Martin, France		Guillaumin, M (corresponding author), INRIA Rhone Alpes, 655 Ave Europe, F-38330 Montbonnot St Martin, France.	matthieu.guillaumin@inria.fr; thomas.mensink@inria.fr; jakob.verbeek@inria.fr; cordelia.schmid@inria.fr		Mensink, Thomas/0000-0002-5730-713X	European Union Information Society Technologies unit E5	European Union Information Society Technologies unit E5	This research is partially funded by the Cognitive-Level Annotation using Latent Statistical Structure (CLASS) project of the European Union Information Society Technologies unit E5 (Cognition). We would also like to thank Tamara Berg, Mark Everingham, and Gary Huang for their help by providing data and code. We also thank Benoit Mordelet, Nicolas Breitner and Lucie Daubigney for their participation in the annotation effort.	Anguelov D., 2007, CVPR; Bar-Hillel AB, 2005, J MACH LEARN RES, V6, P937; Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214; Bekkerman Ron, 2007, CVPR; Berg T., 2004, CVPR; BERTSEKAS DP, 1976, IEEE T AUTOMAT CONTR, V21, P174, DOI 10.1109/TAC.1976.1101194; Bressan M., 2008, P INT C COMP VIS THE; Buckley C., 1995, Text REtrieval Conference (TREC-3) (NIST SP 500-225), P69; Charikar M., 2000, APPROXIMATION ALGORI, P84, DOI [10.1007/3-540-44436-X, DOI 10.1007/3-540-44436-X, 10.1007/3-540-44436-X_10, DOI 10.1007/3-540-44436-X_10]; Chopra S., 2005, CVPR; Chum O., 2007, ICCV; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deschacht K., 2006, P WORKSH ONT LEARN P; Dhillon I. S., 2007, ICML; Everingham M., 2006, BMVC; Ferencz A, 2008, INT J COMPUT VISION, V77, P3, DOI 10.1007/s11263-007-0093-5; Fergus R, 2005, IEEE I CONF COMP VIS, P1816; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Globerson A., 2006, NIPS; Grangier D, 2006, LECT NOTES COMPUT SC, V4212, P162; Guillaumin M., 2009, ICCV; Guillaumin M., 2010, ECCV; Guillaumin M., 2008, CVPR; Holub A., 2008, IEEE C FACE GEST REC; Huang G. B., 2007, ICCV; Huang G.B., 2008, WORKSHOP FACESREAL L; Jain V., 2006, BMVC; JAIN V, 2007, ICCV; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Kumar N., 2009, ICCV; Laptev I., 2008, CVPR; Lazebnik S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P649; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li L.-J., 2007, CVPR; Marcel S., 2007, IDIAPCOM0707; Mensink T., 2008, ECCV; Naaman M., 2005, P JOINT C DIG LIB; Nowak E., 2007, CVPR; Ozkan D., 2009, PATTERN RECOGNITION; Ozkan D., 2006, 2006 IEEE COMP SOC C, P1477; Pham P., 2008, P ECCV WORKSH FAC RE; Pinto N., 2009, CVPR; RAMANAN D, 2009, ICCV; Satoh S, 1999, IEEE MULTIMEDIA, V6, P22, DOI 10.1109/93.752960; Sivic J., 2009, CVPR; SRIHARI RK, 1991, PROCEEDINGS : NINTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P80; Stone Z., 2008, CVPR WORKSH; Taigman Y., 2009, BRIT MACH VIS C; Tian Y., 2007, CVPR; Torresani L., 2010, CVPR; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; VERBEEK J, 2007, CVPR; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wagstaff K., 2001, ICML, V1, P577, DOI DOI 10.1109/TPAMI.2002.1017616; Weinberger KQ, 2006, NIPS; Wolf L., 2008, WORKSH FAC REAL LIF; Xing E., 2004, NIPS; Zhang L, 2004, P 12 ANN ACM INT C M, P716, DOI DOI 10.1145/1027527.1027689	60	29	35	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2012	96	1					64	82		10.1007/s11263-011-0447-x	http://dx.doi.org/10.1007/s11263-011-0447-x			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	872LF		Green Submitted			2022-12-18	WOS:000298810300004
J	Ziyan, U; Sabuncu, MR; Grimson, WEL; Westin, CF				Ziyan, Ulas; Sabuncu, Mert R.; Grimson, W. Eric L.; Westin, Carl-Fredrik			Consistency Clustering: A Robust Algorithm for Group-wise Registration, Segmentation and Automatic Atlas Construction in Diffusion MRI	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						DTI; Anatomical atlas; Clustering; Segmentation; Tractography; Diffusion imaging; White matter atlas	BRAIN WHITE-MATTER; DTI SEGMENTATION; PATHWAYS; NUCLEI	We propose an integrated registration and clustering algorithm, called "consistency clustering", that automatically constructs a probabilistic white-matter atlas from a set of multi-subject diffusion weighted MR images. We formulate the atlas creation as a maximum likelihood problem which the proposed method solves using a generalized Expectation Maximization (EM) framework. Additionally, the algorithm employs an outlier rejection and denoising strategy to produce sharp probabilistic maps of certain bundles of interest. We test this algorithm on synthetic and real data, and evaluate its stability against initialization. We demonstrate labeling a novel subject using the resulting spatial atlas and evaluate the accuracy of this labeling. Consistency clustering is a viable tool for completely automatic white-matter atlas construction for sub-populations and the resulting atlas is potentially useful for making diffusion measurements in a common coordinate system to identify pathology related changes or developmental trends.	[Ziyan, Ulas; Sabuncu, Mert R.; Grimson, W. Eric L.; Westin, Carl-Fredrik] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA; [Westin, Carl-Fredrik] Harvard Univ, Brigham & Womens Hosp, Lab Math Imaging, Sch Med, Boston, MA 02115 USA	Massachusetts Institute of Technology (MIT); Harvard University; Brigham & Women's Hospital; Harvard Medical School	Westin, CF (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	ulas@csail.mit.edu; msabuncu@csail.mit.edu; welg@csail.mit.edu; westin@bwh.harvard.edu	Sabuncu, Mert Rory/ABE-2284-2021	Sabuncu, Mert/0000-0002-7068-719X	NIH [U54-EB005149, P41-RR13218, R01-MH074794, 1-R01-NS051826-01, U41 RR019703]; Athinoula A. Martinos Foundation; NATIONAL CENTER FOR RESEARCH RESOURCES [U41RR019703, P41RR015241, P41RR013218] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF BIOMEDICAL IMAGING AND BIOENGINEERING [U54EB005149] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH074794] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE [R01NS051826] Funding Source: NIH RePORTER; NATIONAL INSTITUTE ON AGING [R01AG020012] Funding Source: NIH RePORTER	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Athinoula A. Martinos Foundation; NATIONAL CENTER FOR RESEARCH RESOURCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR)); NATIONAL INSTITUTE OF BIOMEDICAL IMAGING AND BIOENGINEERING(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Biomedical Imaging & Bioengineering (NIBIB)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS)); NATIONAL INSTITUTE ON AGING(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute on Aging (NIA))	This work was supported by NIH NIBIB NAMIC U54-EB005149, NIH NCRR NAC P41-RR13218, R01-MH074794, NIH 1-R01-NS051826-01, NIH U41 RR019703 and the Athinoula A. Martinos Foundation. We are grateful to Susumu Mori at JHU for the diffusion MRI data (R01-AG20012/P41-RR15241).	ARSIGNY V, 2006, RR5865 INRIA SOPH AN; BASSER PJ, 1994, BIOPHYS J, V66, P259, DOI 10.1016/S0006-3495(94)80775-1; Brun A, 2004, LECT NOTES COMPUT SC, V3216, P368; BRUN A, 2003, INT C COMP AID SYST, P564; Callaghan P.T, 1993, PRINCIPLES NUCL MAGN; Ciccarelli O, 2003, NEUROIMAGE, V19, P1545, DOI 10.1016/S1053-8119(03)00190-3; Corouge I, 2004, 2004 2ND IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: MACRO TO NANO, VOLS 1 and 2, P344; Ding ZH, 2003, MAGNET RESON MED, V49, P716, DOI 10.1002/mrm.10415; DUAN Y, 2007, J BIOMEDICAL IMAGING, V2; FEDDERN C, 2003, P 2 IEEE WORKSH GEOM, P65; GERIG G, 2004, IEEE ENG MED BIOL SO, P426; Heiervang E, 2006, NEUROIMAGE, V33, P867, DOI 10.1016/j.neuroimage.2006.07.037; Jonasson L, 2005, MED IMAGE ANAL, V9, P223, DOI 10.1016/j.media.2004.07.004; Jonasson L, 2007, SIGNAL PROCESS, V87, P309, DOI 10.1016/j.sigpro.2005.12.017; Kubicki M, 2005, NEUROIMAGE, V26, P1109, DOI 10.1016/j.neuroimage.2005.03.026; Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470; LEBIHAN D, 1993, NEUROREPORT, V4, P887; Lenglet C, 2006, IEEE T MED IMAGING, V25, P685, DOI 10.1109/TMI.2006.873299; Maddah M, 2005, LECT NOTES COMPUT SC, V3749, P188; MADDAH M, 2007, INFORM PROCESSING ME; Moberts B, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P65; O'Donnell L, 2005, LECT NOTES COMPUT SC, V3749, P140; O'Donnell LJ, 2006, AM J NEURORADIOL, V27, P1032; O'Donnell L, 2006, LECT NOTES COMPUT SC, V4191, P243; O'Donnell LJ, 2007, IEEE T MED IMAGING, V26, P1562, DOI 10.1109/TMI.2007.906785; Pfefferbaum A, 2000, MAGNET RESON MED, V44, P259, DOI 10.1002/1522-2594(200008)44:2<259::AID-MRM13>3.0.CO;2-6; Pierpaoli C, 1996, MAGNET RESON MED, V36, P893, DOI 10.1002/mrm.1910360612; ROUSSON M, 2004, MATH METHODS BIOMEDI; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Tuch DS, 2002, DIFFUSION MRI COMPLE; WANG Z, 2004, EUR C COMP VIS, P304; Wang ZZ, 2005, IEEE T MED IMAGING, V24, P1267, DOI 10.1109/TMI.2005.854516; WEDEN V, 2000, ISMRM DENV, P82; Westin CF, 2002, MED IMAGE ANAL, V6, P93, DOI 10.1016/S1361-8415(02)00053-1; Wiegell MR, 2003, NEUROIMAGE, V19, P391, DOI 10.1016/S1053-8119(03)00044-2; Xia Y, 2005, LECT NOTES COMPUT SC, V3749, P205; Xu DR, 2002, NEUROIMAGE, V17, P1131, DOI 10.1006/nimg.2002.1285; ZHANG S, 2005, INT SOC MAGNETIC RES; Zhukov L, 2003, J ELECTRON IMAGING, V12, P125, DOI 10.1117/1.1527628; Ziyan U, 2007, LECT NOTES COMPUT SC, V4791, P351; Ziyan U, 2006, LECT NOTES COMPUT SC, V4191, P807; Ziyan U, 2008, LECT NOTES COMPUT SC, V5241, P279, DOI 10.1007/978-3-540-85988-8_34; ZOLLEI L, 2005, INT C COMP VIS BIOM	44	29	29	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2009	85	3					279	290		10.1007/s11263-009-0217-1	http://dx.doi.org/10.1007/s11263-009-0217-1			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	502CO	20442792	Green Accepted			2022-12-18	WOS:000270432200006
J	Schindler, K; Suter, D; Wang, H				Schindler, Konrad; Suter, David; Wang, Hanzi			A model-selection framework for multibody structure-and-motion of image sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						multibody structure-and-motion; 3D motion segmentation; model selection	GEOMETRY ESTIMATION; SEGMENTATION; INFORMATION	Given an image sequence of a scene consisting of multiple rigidly moving objects, multi-body structure-and-motion (MSaM) is the task to segment the image feature tracks into the different rigid objects and compute the multiple-view geometry of each object. We present a framework for multibody structure-and-motion based on model selection. In a recover-and-select procedure, a redundant set of hypothetical scene motions is generated. Each subset of this pool of motion candidates is regarded as a possible explanation of the image feature tracks, and the most likely explanation is selected with model selection. The framework is generic and can be used with any parametric camera model, or with a combination of different models. It can deal with sets of correspondences, which change over time, and it is robust to realistic amounts of outliers. The framework is demonstrated for different camera and scene models.	[Schindler, Konrad] ETH, Comp Vis Lab, CH-8092 Zurich, Switzerland; [Suter, David] Monash Univ, Clayton, Vic 3800, Australia; [Wang, Hanzi] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA	Swiss Federal Institutes of Technology Domain; ETH Zurich; Monash University; Johns Hopkins University	Schindler, K (corresponding author), ETH, Comp Vis Lab, Sternwarstr 7, CH-8092 Zurich, Switzerland.	konrads@vision.ee.ethz.ch	Wang, Hanzi/F-8796-2012	Pauldurai, Jona/0000-0002-7217-0872; Suter, David/0000-0001-6306-3023				Akaike H., 1973, 2 INT S INFORM THEOR, P267, DOI DOI 10.1007/978-1-4612-1694-0_15; BOZDOGAN H, 1987, PSYCHOMETRIKA, V52, P345, DOI 10.1007/BF02294361; Chum O, 2005, PROC CVPR IEEE, P772; COSTEIRA J, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1071, DOI 10.1109/ICCV.1995.466815; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Fitzgerald H. E., 2000, ALCOSCOPE INT REV AL, V3, P6; Frahm J.-M., 2006, P C COMP VIS PATT RE, P453; Han M, 2000, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2000.854908; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Irani M, 1998, IEEE T PATTERN ANAL, V20, P577, DOI 10.1109/34.683770; Kanatani K, 2004, IEEE T PATTERN ANAL, V26, P1307, DOI 10.1109/TPAMI.2004.93; Kverh B, 2004, PATTERN ANAL APPL, V7, P51, DOI 10.1007/s10044-004-0206-5; Langs G, 2005, PROC CVPR IEEE, P310; LEONARDIS A, 1995, INT J COMPUT VISION, V14, P253, DOI 10.1007/BF01679685; LEONTARITIS IJ, 1987, INT J CONTROL, V45, P311, DOI 10.1080/00207178708933730; LI T, 2007, P 2007 ACM IEEE C SU, P1; Ma Y., 2003, INVITATION 3 D VISIO; Matsunaga C., 2000, P 6 EUR C COMP VIS D, V2, P595; MAYBANK SJ, 1999, P 10 BRIT MACH VIS C; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Ripley BD., 1996; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; RISSANEN J, 1984, IEEE T INFORM THEORY, V30, P629, DOI 10.1109/TIT.1984.1056936; Schindler K, 2005, PROC CVPR IEEE, P676; Schindler K, 2006, LECT NOTES COMPUT SC, V3951, P606; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; STRIETZEL M, 1995, TRANSPUT OCCAM ENG S, P90; Tomasi C, 1991, CMUCS91132; Tong WS, 2004, IEEE T PATTERN ANAL, V26, P1167, DOI 10.1109/TPAMI.2004.72; Torr PHS, 2000, DATA SEGMENTATION AND MODEL SELECTION FOR COMPUTER VISION, P143; Torr PHS, 1998, PHILOS T R SOC A, V356, P1321, DOI 10.1098/rsta.1998.0224; Vidal R, 2004, PROC CVPR IEEE, P310; Vidal R, 2004, LECT NOTES COMPUT SC, V3021, P1; VIDAL R, 2002, P ECCV WORKSH VIS MO; WALLACE CS, 1968, COMPUT J, V11, P185, DOI 10.1093/comjnl/11.2.185; WALLACE CS, 1987, J ROY STAT SOC B MET, V49, P240; Wang HZ, 2004, LECT NOTES COMPUT SC, V3023, P107; Wills J, 2003, PROC CVPR IEEE, P37; Wolf L, 2001, PROC CVPR IEEE, P263	42	29	29	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2008	79	2					159	177		10.1007/s11263-007-0111-7	http://dx.doi.org/10.1007/s11263-007-0111-7			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	302ZV		Green Published, Green Submitted			2022-12-18	WOS:000256009700004
J	Rav-Acha, A; Engel, G; Peleg, S				Rav-Acha, Alex; Engel, Giora; Peleg, Shmuel			Minimal aspect distortion (MAD) mosaicing of long scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						video mosaicing; ego motion; stereo; panorama; X-Slits; multi-perspective		Long scenes can be imaged by mosaicing multiple images from cameras scanning the scene. We address the case of a video camera scanning a scene while moving in a long path, e.g. scanning a city street from a driving car, or scanning a terrain from a low flying aircraft. A robust approach to this task is presented, which is applied successfully to sequences having thousands of frames even when using a hand-held camera. Examples are given on a few challenging sequences. The proposed system consists of two components: (i) Motion and depth computation. (ii) Mosaic rendering. In the first part a "direct" method is presented for computing motion and dense depth. Robustness of motion computation has been increased by limiting the motion model for the scanning camera. An iterative graph-cuts approach, with planar labels and a flexible similarity measure, allows the computation of a dense depth for the entire sequence. In the second part a new minimal aspect distortion (MAD) mosaicing uses depth to minimize the geometrical distortions of long panoramic images. In addition to MAD mosaicing, interactive visualization using X-Slits is also demonstrated.	[Rav-Acha, Alex; Engel, Giora; Peleg, Shmuel] Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Rav-Acha, A (corresponding author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.		Peleg, Shmuel/B-7454-2011	Peleg, Shmuel/0000-0002-4468-2619				Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718; AGARWALA A, 2006, SIGGRAPH 2006 JUL, P853; Agarwala A, 2006, ACM T GRAPHIC, V25, P853, DOI 10.1145/1141911.1141966; BERGEN JR, 1992, EUR C COMP VIS, P237; BIRCHFIELD, 1999, ICCV, V1, P489; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Deng Y, 2005, IEEE I CONF COMP VIS, P1316; FELDMAN D, 2004, P IEEE COMP SOC C CO, V11, P163; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; GORTLER SJ, 1996, SIGGRAPH, V30, P43; HANNA KJ, 1991, MOT WORKSH PRINC NJ, P156; Hartley R., 2004, ROBOTICA; Hong L, 2004, PROC CVPR IEEE, P74; Irani M, 2002, IEEE T PATTERN ANAL, V24, P1528, DOI 10.1109/TPAMI.2002.1046174; IRANI M, 1992, LECT NOTES COMPUT SC, V588, P282; KAWASAKI H, 2001, ITSWC 01; Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668; KOLMOGOROV V, 2002, ECCV, P65; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Montoliu R, 2003, LECT NOTES COMPUT SC, V2905, P62; ONO S, 2003, ITSWC 03; Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a; Rav-Acha A, 2006, IEEE IMAGE PROC, P1097, DOI 10.1109/ICIP.2006.312747; RAVACHA A, 2004, 2 IEEE WORKSH IM VID; RAVACHA A, 2004, 2 IEEE INT S 3D DAT; Roman A, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P537, DOI 10.1109/VISUAL.2004.50; ROMAN A, 2006, P EUR S REND, P161; SHI M, 2005, CVPR 05, V1, P1047; WEXLER Y, 2005, INT C COMP VIS ICCV, V1, P858; Xiao JJ, 2005, IEEE T PATTERN ANAL, V27, P1644, DOI 10.1109/TPAMI.2005.202; YANG Q, 2006, BRIT MACH VIS C, P989; Zheng JY, 2000, IEEE MULTIMEDIA, V7, P31, DOI 10.1109/93.848423; Zhu ZG, 2004, IEEE T PATTERN ANAL, V26, P226, DOI 10.1109/TPAMI.2004.1262190; Zomet A, 2003, IEEE T PATTERN ANAL, V25, P741, DOI 10.1109/TPAMI.2003.1201823	37	29	38	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2008	78	2-3					187	206		10.1007/s11263-007-0101-9	http://dx.doi.org/10.1007/s11263-007-0101-9			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	275RK					2022-12-18	WOS:000254089100005
J	Jin, HL; Cremers, D; Wang, DJ; Prados, E; Yezzi, A; Soatto, S				Jin, Hailin; Cremers, Daniel; Wang, Dejun; Prados, Emmanuel; Yezzi, Anthony; Soatto, Stefano			3-D reconstruction of shaded objects from multiple images under unknown illumination	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						stereoscopic segmentation; shape from shading; multi view stereo; variational 3D reconstruction; level set methods; lighting and appearance reconstruction	SHAPE; SEGMENTATION; DIRECTION; ALBEDO; MODELS; LIGHT	We propose a variational algorithm to jointly estimate the shape, albedo, and light configuration of a Lambertian scene from a collection of images taken from different vantage points. Our work can be thought of as extending classical multi-view stereo to cases where point correspondence cannot be established, or extending classical shape from shading to the case of multiple views with unknown light sources. We show that a first naive formalization of this problem yields algorithms that are numerically unstable, no matter how close the initialization is to the true geometry. We then propose a computational scheme to overcome this problem, resulting in provably stable algorithms that converge to (local) minima of the cost functional. We develop a new model that explicitly enforces positivity in the light sources with the assumption that the object is Lambertian and its albedo is piecewise constant and show that the new model significantly improves the accuracy and robustness relative to existing approaches.	[Jin, Hailin] Adobe Syst Inc, Off Technol, San Jose, CA 95110 USA; [Cremers, Daniel] Dept Comp Sci, D-53117 Bonn, Germany; [Wang, Dejun; Soatto, Stefano] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA; [Prados, Emmanuel] INRIA Rhone Alpes, F-38330 Montbonnot St Martin, France; [Yezzi, Anthony] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA	Adobe Systems Inc.; University of California System; University of California Los Angeles; University System of Georgia; Georgia Institute of Technology	Jin, HL (corresponding author), Adobe Syst Inc, Off Technol, 345 Pk Ave, San Jose, CA 95110 USA.	hljin@adobe.com; dcremers@cs.uni-bonn.de; emmanuel.prados@inrialpes.fr; ayezzi@ece.gatech.edu; soatto@cs.ucla.edu	Yezzi, Anthony/AAB-4235-2020					ABRAHAM R, 1993, APPL MATH SCI, V75; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Brox T, 2006, IEEE T IMAGE PROCESS, V15, P3213, DOI 10.1109/TIP.2006.877481; ChefD'Hotel C, 2004, J MATH IMAGING VIS, V20, P147, DOI 10.1023/B:JMIV.0000011324.14508.fb; CHEN H, 2000, P IEEE C COMP VIS PA; DUROU JD, 2004, 20042R IIRIT; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; Horn B., 1986, ROBOT VISION, P1; Horn B.K.P., 1989, SHAPE SHADING; JIN H, 2003, THESIS WASHINGTON U; JIN H, 2007, IN PRESS J MATH IMAG; JIN H, 2003, P IEEE C COMP VIS PA, V1, P171; Jin HL, 2004, PROC CVPR IEEE, P36; Jin HL, 2004, LECT NOTES COMPUT SC, V3022, P114; Jin HL, 2003, J SCI COMPUT, V19, P267, DOI 10.1023/A:1025308109816; KERIVEN R, 2006, INT J COMPUTER VISIO; KLETTE R, 1998, CITRTR20; KOENDERINK JJ, 1980, OPT ACTA, V27, P981, DOI 10.1080/713820338; Kolev K, 2006, LECT NOTES COMPUT SC, V4174, P688; Kuhn H., 1951, P 2 BERK S MATH STAT, P481, DOI DOI 10.1007/BF01582292; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; LANGER MS, 1994, J OPT SOC AM A, V11, P467, DOI 10.1364/JOSAA.11.000467; Ma Y., 2003, INVITATION 3D VISION; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; Oliensis J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P692, DOI 10.1109/ICCV.1993.378145; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; PRADOS E, 2004, THESIS U NICESOPHIA; ROBLESKELLY A, 2004, 3DPPVT, P494; ROBLESKELLY A, 2005, PATTERN RECOGN, V38, P1574; Samaras D, 2003, IEEE T PATTERN ANAL, V25, P247, DOI 10.1109/TPAMI.2003.1177155; Samaras D, 2000, PROC CVPR IEEE, P480, DOI 10.1109/CVPR.2000.855858; Seitz S., 2006, P INT C COMP VIS PAT, P519, DOI DOI 10.1109/CVPR.2006.19; Stewart AJ, 1997, IEEE T PATTERN ANAL, V19, P1020, DOI 10.1109/34.615450; Urakawa H., 1993, CALCULUS VARIATIONS; VEDALDI A, 2006, TR050012; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Yezzi AJ, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P59, DOI 10.1109/ICCV.2001.937499; Yuille A, 2003, J OPT SOC AM A, V20, P24, DOI 10.1364/JOSAA.20.000024; Yuille AL, 1999, INT J COMPUT VISION, V35, P203, DOI 10.1023/A:1008180726317; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658	41	29	30	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2008	76	3					245	256		10.1007/s11263-007-0055-y	http://dx.doi.org/10.1007/s11263-007-0055-y			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	255VI		Green Submitted			2022-12-18	WOS:000252685500003
J	Savarese, S; Andreetto, M; Rushmeier, H; Bernardini, F; Perona, P				Savarese, Silvio; Andreetto, Marco; Rushmeier, Holly; Bernardini, Fausto; Perona, Pietro			3D reconstruction by shadow carving: Theory and practical evaluation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shape recovery; shape from shadows; 3D reconstruction; computer vision; shape from silhouettes; shape from contours	CONSTRUCTION	Cast shadows are an informative cue to the shape of objects. They are particularly valuable for discovering object's concavities which are not available from other cues such as occluding boundaries. We propose a new method for recovering shape from shadows which we call shadow carving. Given a conservative estimate of the volume occupied by an object, it is possible to identify and carve away regions of this volume that are inconsistent with the observed pattern of shadows. We prove a theorem that guarantees that when these regions are carved away from the shape, the shape still remains conservative. Shadow carving overcomes limitations of previous studies on shape from shadows because it is robust with respect to errors in shadows detection and it allows the reconstruction of objects in the round, rather than just bas-reliefs. We propose a reconstruction system to recover shape from silhouettes and shadow carving. The silhouettes are used to reconstruct the initial conservative estimate of the object's shape and shadow carving is used to carve out the concavities. We have simulated our reconstruction system with a commercial rendering package to explore the design parameters and assess the accuracy of the reconstruction. We have also implemented our reconstruction scheme in a table-top system and present the results of scanning of several objects.	CALTECH, Pasadena, CA 91125 USA; IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	California Institute of Technology; International Business Machines (IBM)	Savarese, S (corresponding author), CALTECH, Mail Stop 136-93, Pasadena, CA 91125 USA.	savarese@vision.caltech.edu; marco@vision.caltech.edu; holly@acm.org; fausto@watson.ibm.com; perona@vision.caltech.edu		Rushmeier, Holly/0000-0001-5241-0886				Andreetto M, 2004, IEEE T IMAGE PROCESS, V13, P352, DOI 10.1109/TIP.2003.821351; Bhotika R, 2002, LECT NOTES COMPUT SC, V2352, P112; Bouguet JY, 1999, INT J COMPUT VISION, V35, P129, DOI 10.1023/A:1008124523456; Boyer E, 1997, INT J COMPUT VISION, V22, P219, DOI 10.1023/A:1007978616082; Broadhurst A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P388, DOI 10.1109/ICCV.2001.937544; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; Daum M, 1998, PROC CVPR IEEE, P461, DOI 10.1109/CVPR.1998.698646; De Bonet J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P418, DOI 10.1109/ICCV.1999.791251; Eisert P, 1999, INT CONF ACOUST SPEE, P3509, DOI 10.1109/ICASSP.1999.757599; Farouk M, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P310, DOI 10.1109/IM.2003.1240264; FITZGIBBON A, 1998, P SMILE WORKSH STRUC, V1520, P154; GIBLIN P, 1986, P INT C COMP VIS, P136; HAMBRICK LN, 1987, IEEE T PATTERN ANAL, V9, P597, DOI 10.1109/TPAMI.1987.4767954; Hatzitheodorou M., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P486, DOI 10.1109/CVPR.1988.196279; Horn B.K.P., 1989, SHAPE SHADING; Kampel M, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P754, DOI 10.1109/TDPVT.2002.1024154; KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321; Kriegman DJ, 2001, J OPT SOC AM A, V18, P1804, DOI 10.1364/JOSAA.18.001804; Kutulakos K. N., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P307, DOI 10.1109/ICCV.1999.791235; Langer M.S., 1995, P IEEE RSJ IROS, P390; LAURENTINI A, 1995, IEEE T PATTERN ANAL, V17, P188, DOI 10.1109/34.368170; Lazebnik S, 2001, PROC CVPR IEEE, P156; Leibe B, 2000, IEEE COMPUT GRAPH, V20, P54, DOI 10.1109/38.888008; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; MARTIN WN, 1983, IEEE T PATTERN ANAL, V5, P150, DOI 10.1109/TPAMI.1983.4767367; RAVIV D, 1989, IEEE T ROBOTIC AUTOM, V5, P701, DOI 10.1109/70.88087; Reed MK, 1999, IMAGE VISION COMPUT, V17, P99, DOI 10.1016/S0262-8856(98)00114-0; Rocchini C, 2001, COMPUT GRAPH FORUM, V20, pC299; Savarese S, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P12, DOI 10.1109/TDPVT.2002.1024034; Savarese S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P190, DOI 10.1109/ICCV.2001.937517; Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462; SHAFER SA, 1983, COMPUT VISION GRAPH, V22, P145, DOI 10.1016/0734-189X(83)90099-3; Slabaugh GG, 2004, INT J COMPUT VISION, V57, P179, DOI 10.1023/B:VISI.0000013093.45070.3b; Sullivan S, 1998, IEEE T PATTERN ANAL, V20, P1091, DOI 10.1109/34.722621; SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029; TAUBIN G, 1996, RC20404 IBM RES; TAUBIN G, 1995, SIGGRAPH 95 C P, P351, DOI 10.1145/218380.218473; Treuille A, 2004, LECT NOTES COMPUT SC, V3022, P457; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; WONG KYK, 2001, THESIS U CAMBRIDGE; YANG DKM, 1996, THESIS COLUMBIA U; Yu YH, 2005, INT J COMPUT VISION, V62, P35, DOI 10.1007/s11263-005-4634-5; ZHENG JY, 1994, IEEE T PATTERN ANAL, V16, P163, DOI 10.1109/34.273734	45	29	35	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2007	71	3					305	336		10.1007/s11263-006-8323-9	http://dx.doi.org/10.1007/s11263-006-8323-9			32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	104CB					2022-12-18	WOS:000241932400003
J	Haworth, CD; De Saint-Pern, Y; Clark, D; Trucco, E; Petillot, YR				Haworth, C. D.; De Saint-Pern, Y.; Clark, D.; Trucco, E.; Petillot, Y. R.			Detection and tracking of multiple metallic objects in millimetre-wave images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	3rd Workshop on Object Tracking and Classification Beyond the Visible Spectrum	JUN 22, 2006	New York, NY	IEEE		millimetre-wave; detection; tracking; metallic objects	WEAPONS	In this paper we present a system for the automatic detection and tracking of metallic objects concealed on moving people in sequences of millimetre-wave (MMW) images. The millimetre-wave sensor employed has been demonstrated for use in covert detection because of its ability to see through clothing, plastics and fabrics. The system employs two distinct stages: detection and tracking. In this paper a single detector, for metallic objects, is presented which utilises a statistical model also developed in this paper. The second stage tracks the target locations of the objects using a Probability Hypothesis Density filter. The advantage of this filter is that it has the ability to track a variable number of targets, estimating both the number of targets and their locations. This avoids the need for data association techniques as the identities of the individual targets are not required. Results are presented for both simulations and real millimetre-wave image test sequences demonstrating the benefits of our system for the automatic detection and tracking of metallic objects.	Heriot Watt Univ, Edinburgh EH14 4AS, Midlothian, Scotland	Heriot Watt University	Haworth, CD (corresponding author), Heriot Watt Univ, Edinburgh EH14 4AS, Midlothian, Scotland.		/AAB-9394-2020; Clark, Daniel E/K-7715-2012	/0000-0002-0218-7994; Trucco, Emanuele/0000-0002-5055-0794; Petillot, Yvan/0000-0002-1596-289X	Engineering and Physical Sciences Research Council [GR/S68088/01] Funding Source: researchfish	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))		Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374; Bar-Shalom Y., 1998, TRACKING DATA ASS; Choi KN, 2002, PATTERN RECOGN, V35, P2073, DOI 10.1016/S0031-3203(01)00173-X; Clark DE, 2005, IEE P-RADAR SON NAV, V152, P327, DOI 10.1049/ip-rsn:20045068; CLARK DE, 2005, OC EUR C BREST IEEE; CLARK DE, 2005, IN PRESS ISSNIP 2005; CLARK DE, IN PRESS IEEE T SIGN; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Coward P, 2003, P SOC PHOTO-OPT INS, V5077, P54, DOI 10.1117/12.487031; HALL DL, 2001, HDB MULTISENSOR DATA, pCH7; Haworth CD, 2004, PROC SPIE, V5619, P117, DOI 10.1117/12.580475; Hue C, 2002, IEEE T AERO ELEC SYS, V38, P791, DOI 10.1109/TAES.2002.1039400; Johansen A., 2005, CUEDFINFENGTR517 U C; KARLSSON R, 2001, 2001174 IEE; Keller PE, 2000, IEEE AERO EL SYS MAG, V15, P17, DOI 10.1109/62.825667; LIN L, 2004, THESIS U CONNECTICUT; Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119; Murphy KS, 2002, 36TH ANNUAL 2002 INTERNATIONAL CARNAHAN CONFERENCE ON SECURITY TECHNOLOGY, PROCEEDINGS, P162, DOI 10.1109/CCST.2002.1049244; Panta K, 2004, P SOC PHOTO-OPT INS, V5429, P284, DOI 10.1117/12.543357; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Schulz D, 2003, INT J ROBOT RES, V22, P99, DOI 10.1177/0278364903022002002; Sidenbladh H, 2003, FUSION 2003: PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE OF INFORMATION FUSION, VOLS 1 AND 2, P800; Sinclair GN, 2002, 36TH ANNUAL 2002 INTERNATIONAL CARNAHAN CONFERENCE ON SECURITY TECHNOLOGY, PROCEEDINGS, P167, DOI 10.1109/CCST.2002.1049245; Slamani MA, 2002, P SOC PHOTO-OPT INS, V4719, P296, DOI 10.1117/12.477470; Slamani MA, 2001, P SOC PHOTO-OPT INS, V4387, P176, DOI 10.1117/12.421137; SLAMANI MA, 1999, P INT C IM PROC KOB, V3, P518; SNCLAIR GN, 2001, P 35 INT CARN C SEC, P172; Tobias M, 2004, SE SYM SYS THRY, P205; Tommasini T, 1998, PROC CVPR IEEE, P178, DOI 10.1109/CVPR.1998.698606; TRUCCO E, 2005, IN PRESS IEEE J OCEA; Varshney P. K., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P532, DOI 10.1109/ICIP.1999.817171; Vo BN, 2003, FUSION 2003: PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE OF INFORMATION FUSION, VOLS 1 AND 2, P792; Xue ZY, 2003, FUSION 2003: PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE OF INFORMATION FUSION, VOLS 1 AND 2, P622; Zajic T, 2003, PROC SPIE, V5096, P291, DOI 10.1117/12.488533	35	29	31	2	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2007	71	2					183	196		10.1007/s11263-006-6275-8	http://dx.doi.org/10.1007/s11263-006-6275-8			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	098DN					2022-12-18	WOS:000241501300005
J	Wang, T; Rui, Y; Sun, JG				Wang, T; Rui, Y; Sun, JG			Constraint based region matching for image retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						content-based image retrieval; region matching; probabilistic weight estimation; similarity model		Objects and their spatial relationships are important features for human visual perception. Inmost existme, content-based image retrieval systems, however, only global features extracted from the whole image are used. While they are easy to implement, they have limited power to model semantic-level objects and spatial relationship. To overcome this difficulty, this paper proposes a constraint-based region matching approach to image retrieval. Unlike existing region-based approaches where either individual regions are used or only first-order constraints are modeled, the proposed approach formulates the problem in a probabilistic framework and simultaneously models both first-order region properties and second-order spatial relationships for all the regions in the image. Specifically, in this paper we present a complete system that includes image segmentation, local feature extraction, first- and second-order constraints, and probabilistic region weight estimation. Extensive experiments have been carried out on a large heterogeneous image collection with 17,000 images. The proposed approach achieves significantly better performance than the state-of-the-art approaches.	Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China; Microsoft Corp, Res, Redmond, WA 98052 USA	Tsinghua University; Microsoft	Wang, T (corresponding author), Intel China Res Ctr, Stat Comp Grp, 06-01 Beijing Kerry Ctr,1 GuangHua Rd,Chaoyang Di, Beijing 100020, Peoples R China.	tao.wang@intel.com; yongrui@microsoft.com; sunjg@mail.tsinghua.edu.cn						Carson C, 1997, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P42, DOI 10.1109/IVL.1997.629719; Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P524, DOI 10.1109/TIP.2000.826790; Deng YN, 1999, INT CONF ACOUST SPEE, P3017, DOI 10.1109/ICASSP.1999.757476; Felzenszwalb PF, 1998, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.1998.698594; GOUET V, 2001, P IEEE WORKSH CONT B; JAIN AK, 1989, FUNDAMENTALS DIGITAL, P412; Jia Li, 2000, Proceedings ACM Multimedia 2000, P147; Kam AH, 2000, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P91, DOI 10.1109/IVL.2000.853846; KAZUYOSHI H, 1995, P 11 C ART INT APPL, P38; Ko B, 2000, INT C PATT RECOG, P283, DOI 10.1109/ICPR.2000.902914; LEU JG, 1991, PATTERN RECOGN, V24, P949, DOI 10.1016/0031-3203(91)90092-J; Ma WY, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P568, DOI 10.1109/ICIP.1997.647976; MOGHADDAM B, 2000, P IEEE ICIP, V2, P542; NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173; PAVLIDIS T, 1990, IEEE T PATTERN ANAL, V12, P225, DOI 10.1109/34.49050; Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143; PFEFFERKORN CE, 1975, COMMUN ACM, V18, P286, DOI 10.1145/360762.360817; Rodden K., 2001, P SIGCHI C HUM FACT, P190; Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413; Rui Y, 2000, PROC CVPR IEEE, P236, DOI 10.1109/CVPR.2000.855825; RUI Y, 1997, P IEEE INT C IM PROC; SALTON G, 1982, INTRO MODERN INFORMA; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; Smith JR, 1997, 1997 IEEE FIRST WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P301, DOI 10.1109/MMSP.1997.602652; SMITH JR, 1996, ACM MULTIMEDIA 1996; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487; Tian Q, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P746, DOI 10.1109/ICIP.2000.899562; Tian Q, 2001, J ELECTRON IMAGING, V10, P835, DOI 10.1117/1.1406945; Wang T, 2001, PROC CVPR IEEE, P1140; WANG T, 2002, P IEEE COMP VIS PATT; WANG W, 2002, P 15 INT C VIS INT C; XU Y, 2000, P IEEE ICASSP IST TU, V6, P2019	32	29	32	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN-FEB	2004	56	1-2			SI		37	45		10.1023/B:VISI.0000004831.53436.88	http://dx.doi.org/10.1023/B:VISI.0000004831.53436.88			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	745MJ					2022-12-18	WOS:000186692200004
J	Nielsen, AA				Nielsen, AA			Spectral mixture analysis: Linear and semi-parametric full and iterated partial unmixing in multi- and hyperspectral image data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						least squares regression; spectral angle mapping (SAM); orthogonal subspace projection (OSP); matched filtering; iterated constrained energy minimization (CEM); generalized eigenvalue problem; target constrained interference minimized filter (TCIMF); non-linear semi-parametric unmixing (SPU)	CLASSIFICATION; HIPS; UNIX	As a supplement or an alternative to classification of hyperspectral image data linear and semi-parametric mixture models are considered in order to obtain estimates of abundance of each class or end-member in pixels with mixed membership. Full unmixing based on both ordinary least squares (OLS) and non-negative least squares (NNLS), and the partial unmixing methods orthogonal subspace projection (OSP), constrained energy minimization (CEM) and an eigenvalue formulation alternative are dealt with. The solution to the eigenvalue formulation alternative proves to be identical to the CEM solution. The matrix inversion involved in CEM can be avoided by working on (a subset of) orthogonally transformed data such as signal maximum autocorrelation factors, MAFs, or signal minimum noise fractions, MNFs. This will also cause the partial unmixing result to be independent of the noise isolated in the MAF/MNFs not included in the analysis. CEM and the eigenvalue formulation alternative enable us to perform partial unmixing when we know one desired end-member spectrum only and not the full set of end-member spectra. This is an advantage over full unmixing and OSP. The eigenvalue formulation of CEM inspires us to suggest an iterated CEM scheme. Also the target constrained interference minimized filter (TCIMF) is described. Spectral angle mapping (SAM) is briefly described. Finally, semi-parametric unmixing (SPU) based on a combined linear and additive model with a non-linear, smooth function to represent end-member spectra unaccounted for is introduced. An example with two generated bands shows that both full unmixing, the CEM, the iterated CEM and TCIMF methods perform well. A case study with a 30 bands subset of AVIRIS data shows the utility of full unmixing, SAM, CEM and iterated CEM to more realistic data. Iterated CEM seems to suppress noise better than CEM. A study with AVIRIS spectra generated from real spectra shows (1) that ordinary least squares in this case with one unknown spectrum performs better than non-negative least squares, and (2) that although not fully satisfactory the semi-parametric model gives better estimates of end-member abundances than the linear model.	Tech Univ Denmark, IMM, DK-2800 Lyngby, Denmark	Technical University of Denmark	Nielsen, AA (corresponding author), Tech Univ Denmark, IMM, Bldg 321,Richard Petersens Plads, DK-2800 Lyngby, Denmark.	aa@imm.dtu.dk	Nielsen, Allan Aasbjerg/R-5351-2019; Nielsen, Allan A./L-2154-2016	Nielsen, Allan Aasbjerg/0000-0002-4837-9449; Nielsen, Allan A./0000-0002-4837-9449				AHSTIE TJ, 1990, GEN ADDITIVE MODELS; Anderson E, 1995, LAPACK USERS GUIDE; Anderson T. W, 1984, INTRO MULTIVARIATE S; [Anonymous], 1984, SWINSF6 STANF U; Bernard AC, 1997, INT J REMOTE SENS, V18, P1851, DOI 10.1080/014311697218160; Chambers JM., 1992, STAT MODELS S; Conradsen K, 1991, P 24 INT S REM SENS, P403; CONRADSEN K, 1985, P S APPL STAT LYNGB, P47; CRIPPEN RE, 1999, P 13 INT C APPL GEOL, V1, P150; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; DIXON WJ, 1985, BMDP STAT SOFTWARE, P734; ERSBOLL BK, 1989, THESIS TU DENMARK LY, P297; Flesche H, 2000, MATH GEOL, V32, P337, DOI 10.1023/A:1007538028119; GILL PE, 1986, 861 STANF U DEP OP R; GREEN AA, 1988, IEEE T GEOSCI REMOTE, V26, P65, DOI 10.1109/36.3001; HARSANYI JC, 1994, IEEE T GEOSCI REMOTE, V32, P779, DOI 10.1109/36.298007; Jacobsen A, 1998, 1ST EARSEL WORKSHOP ON IMAGING SPECTROSCOPY, P309; JACOBSON AS, 1994, COMMUN ACM, V37, P43; KENT JT, 1988, IEEE T PATTERN ANAL, V10, P659, DOI 10.1109/34.6774; KRUSE FA, 1993, REMOTE SENS ENVIRON, V44, P145, DOI 10.1016/0034-4257(93)90013-N; LANDY MS, 1984, COMPUT VISION GRAPH, V25, P331, DOI 10.1016/0734-189X(84)90199-3; LANDY MS, 1984, BEHAV RES METH INS C, V16, P199, DOI 10.3758/BF03202390; LANDY MS, 1993, P SPIE 1964 APPL ART, P352; Larsen R, 2000, PATTERN RECOGN LETT, V21, P1175, DOI 10.1016/S0167-8655(00)00079-9; LARSEN R, 1999, P SCAND IM AN C SCIA, V2, P785; LARSEN R, 1997, P 17 EARSEL S FUT TR, P157; LEE JB, 1990, IEEE T GEOSCI REMOTE, V28, P295, DOI 10.1109/36.54356; MARSH SE, 1980, PHOTOGRAMM ENG REM S, V46, P1079; Maselli F, 1998, IEEE T GEOSCI REMOTE, V36, P1809, DOI 10.1109/36.718648; Miller JWV, 1992, IEEE T IMAGE PROCESS, V1, P148, DOI 10.1109/83.136592; Nielsen A. A., 1997, P 3 ANN C INT ASS MA, P955; Nielsen A. A., 1994, THESIS TU DENMARK LY; Nielsen AA, 1998, REMOTE SENS ENVIRON, V64, P1, DOI 10.1016/S0034-4257(97)00162-4; Nielsen AA, 1998, 1ST EARSEL WORKSHOP ON IMAGING SPECTROSCOPY, P165; NIELSEN AA, 1996, GEOSTATISTICS WOLLON, P1173; NIELSEN AA, 1999, MACHINE VISION ADV I; NIELSEN AA, 2000, P 6 INT GEOST C GEOS; NIELSEN AA, 1998, P 4 ANN C INT ASS MA, P473; NIELSEN AA, 1999, P SCAND IM AN C SCIA, V2, P898; NIELSEN AA, 1994, P 1 INT AIRB REM SEN, V2, P557; NIELSEN AA, 1999, P 4 INT AIRB REM SES, V2, P535; PENDOCK N, 1993, P 4 S AFR WORKSH PAT, P2; REN H, 2000, P IEEE 2000 IGARSS H; Resmini RG, 1997, INT J REMOTE SENS, V18, P1553, DOI 10.1080/014311697218278; SADEGH P, 1999, 199917 TU DENM DEP M; SETTLE JJ, 1993, INT J REMOTE SENS, V14, P1159, DOI 10.1080/01431169308904402; Settle JJ, 1996, IEEE T GEOSCI REMOTE, V34, P1045, DOI 10.1109/36.508422; STAN SS, 1997, THESIS U WITWATERSRA; STROBL P, 1996, P 2 INT AIRB REM SEN, V2, P325; Tu TM, 1998, IEEE T GEOSCI REMOTE, V36, P171, DOI 10.1109/36.655327; Van der Meer F, 1999, INT J REMOTE SENS, V20, P3431, DOI 10.1080/014311699211462; VANE G, 1988, REMOTE SENS ENVIRON, V24, P1, DOI 10.1016/0034-4257(88)90003-X; VANE G, 1993, REMOTE SENS ENVIRON, V44, P127, DOI 10.1016/0034-4257(93)90012-M; VENABLES W, 1999, MODERN APPL STAT SPL; [No title captured]	55	29	35	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		2001	42	1-2					17	37		10.1023/A:1011181216297	http://dx.doi.org/10.1023/A:1011181216297			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	437VX		Green Submitted			2022-12-18	WOS:000169015200003
J	Benayoun, S; Ayache, N				Benayoun, S; Ayache, N			Dense non-rigid motion estimation in sequences of medical images using differential constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						medical imaging; adaptive meshes; non-rigid motion; differential geometry; finite element method	MODELS	We describe a new method for computing the displacement vector field in time sequences of 2D or 3D images (4D data). The method is energy-minimizing on the space of correspondence functions; the energy is split into two terms, with one term matching differential singularities in the images, and the other constraining the regularity of the field. In order to reduce the computational time of the motion estimation, we use an adaptive image mesh, the resolution of which depends on the value of the gradient intensity. We solve numerically the minimization problem with the finite element method which gives a continuous approximation of the solution. We present experimental results on synthetic data and on medical images and we show how to use these results for analyzing cardiac deformations.	INRIA, F-06902 Sophia Antipolis, France	Inria	Benayoun, S (corresponding author), Alcatel Alsthom Rech, Route Nozay, F-91460 Marcoussis, France.							AMINI AA, 1992, IMAGE VISION COMPUT, V10, P418, DOI 10.1016/0262-8856(92)90027-Z; AYACHE N, 1995, IMAGE VISION COMPUT, V13, P295, DOI 10.1016/0262-8856(95)99717-F; AZHARI H, 1995, AM J PHYSIOL-HEART C, V268, pH1918, DOI 10.1152/ajpheart.1995.268.5.H1918; BAJCSY R, 1989, COMPUT VISION GRAPH, V46, P1, DOI 10.1016/S0734-189X(89)80014-3; BARDINET E, 1995, P 1 INT C COMP VIS V; BENAYOUN S, 1994, THESIS U PARIS DAUPH; BENAYOUN S, 1996, EUR C COMP VIS CAMBR; BENAYOUN S, 1995, P 1 INT C COMP VIS V; BENAYOUN S, 1994, INT C PATT REC JER I; BRAUNWALD E, 1992, HEART DIS TXB CARDIO, V1; Ciarlet P.G., 1987, HDB NUMERICAL ANAL; COHEN, 1992, EUR C COMP VIS SANT, P458; COHEN LD, 1993, IEEE T PATTERN ANAL, V15; GOURDON A, 1995, P 1 INT C COMP VIS V; GUTMAN MA, 1994, IEEE T MED IMAGING, V13; KASS, 1987, IEEE INT C COMP VIS, P259; MALANDAIN G, 1992, THESIS ECOLE CENT PA; MCINERNEY T, 1995, COMPUT MED IMAG GRAP, V19, P69, DOI 10.1016/0895-6111(94)00040-9; MONGA O, 1995, COMPUTER VISION IMAG, V61; MONGA O, 1992, IEEE C COMP VIS PATT; MONGA O, 1991, IMAGE VISION COMPUTI, V9; NASTAR C, 1994, P IEEE WORKSH NONR A; NASTAR C, 1994, EUR C COMP VIS STOCK; PARK J, 1995, INT C COMP VIS, P700; Raviart P.A., 1992, INTRO ANAL NUMERIQUE; SZELISKI R, 1994, IEEE WORKSH BIOM IM; TANG LA, 1994, P WORKSH JOURN INRIA, P22; TERZOPOULOS D, 1991, IEEE T PATTERN ANAL, V13, P703, DOI 10.1109/34.85659; TERZOPOULOS D, 1988, ARTIF INTELL, V36, P91, DOI 10.1016/0004-3702(88)90080-X; TERZOPOULOS D, 1991, IEEE INT C COMP VIS, P70; THIRION J, 1992, 1672 I NAT RECH INF; VASILESCU M, 1992, IEEE P COMP VIS PATT, P829	32	29	29	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1998	26	1					25	40		10.1023/A:1007932523504	http://dx.doi.org/10.1023/A:1007932523504			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZC476		Green Submitted			2022-12-18	WOS:000072583100002
J	FERMULLER, C				FERMULLER, C			PASSIVE NAVIGATION AS A PATTERN-RECOGNITION PROBLEM	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							MOTION; FIELD; IMAGE; FLOW	The most basic visual capabilities found in living organisms are based on motion. Machine vision, of course, does not have to copy animal vision, but the existence of reliably functioning vision modules in nature gives us some reason to believe that it is possible for an artificial system to work in the same or a similar way. In this article it is argued that many navigational capabilities can be formulated as pattern recognition problems. An appropriate retinotopic representation of the image would make it possible to extract the information necessary to solve motion-related tasks through the recognition of a set of locations on the retina. This argument is illustrated by introducing a representation of image motion by which an observer's egomotion could be derived from information globally encoded in the image-motion field. In the past, the problem of determining a system's own motion from dynamic imagery has been considered as one of the classical visual reconstruction problems, wherein local constraints have been employed to compute from exact 2-D image measurements (correspondence, optical flow) the relative 3-D motion and structure of the scene in view. The approach introduced here is based on new global constraints defined on local normal-flow measurements-the spatio-temporal derivatives of the image-intensity function. Classifications are based on orientations of normal-flow vectors, which allows selection of vectors that form global patterns in the image plane. The position of these patterns is related to the 3-D motion of the observer, and their localization provides the axis of rotation and the direction of translation. The constraints introduced are utilized in algorithmic procedures formulated as search techniques. These procedures are very stable, since they are not affected by small perturbations in the image measurements. As a matter of fact, the solution to the two directions of translation and rotation is not affected, as long as the measurement of the sign of the normal flow is correct.			FERMULLER, C (corresponding author), UNIV MARYLAND,CTR AUTOMAT RES,COMP VIS LAB,COLLEGE PK,MD 20742, USA.							ANANDAN P, 1985, 3RD P WORKSH COMP VI, P186; BRUSS AR, 1983, COMPUT VISION GRAPH, V21, P3, DOI 10.1016/S0734-189X(83)80026-7; DUFFY CJ, 1991, J NEUROPHYSIOL, V65, P1329, DOI 10.1152/jn.1991.65.6.1329; Faugeras O., 1992, 3 DIMENSIONAL COMPUT; FERMULLER C, 1993, INT J COMPUT VISION, V11, P165, DOI 10.1007/BF01469227; FERMULLER C, 1992, QUALITATIVE EGOMOTIO; FERMULLER C, 1993, ADV COMPUTER VISION; HILDRETH E, 1983, MEASUREMENT VISUAL M; HO YC, 1965, IEEE TRANS ELECTRON, VEC14, P683, DOI 10.1109/PGEC.1965.264207; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; KOENDERINK JJ, 1976, J OPT SOC AM, V66, P717, DOI 10.1364/JOSA.66.000717; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; NELSON RC, 1991, INT J COMPUT VISION, V7, P33, DOI 10.1007/BF00130488; SPETSAKIS ME, 1988, DEC P INT C COMP VIS, P449; TANAKA K, 1989, J NEUROPHYSIOL, V62, P626, DOI 10.1152/jn.1989.62.3.626; TISTARELLI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P108, DOI 10.1016/1049-9660(92)90089-L; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; ULLMAN S, 1979, PROC R SOC SER B-BIO, V203, P405, DOI 10.1098/rspb.1979.0006; ZEKI S, 1992, SCI AM, V267, P69, DOI 10.1038/scientificamerican0992-68	21	29	31	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1995	14	2					147	158		10.1007/BF01418980	http://dx.doi.org/10.1007/BF01418980			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT789					2022-12-18	WOS:A1995QT78900004
J	GREMBAN, KD; IKEUCHI, K				GREMBAN, KD; IKEUCHI, K			PLANNING MULTIPLE OBSERVATIONS FOR OBJECT RECOGNITION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							AUTOMATIC-GENERATION; PROGRAMS; VISION; MODEL; SHAPE; VIEWS	Most computer vision systems perform object recognition on the basis of the features extracted from a single image of the object. The problem with this approach is that it implicitly assumes that the available features are sufficient to determine the identity and pose of the object uniquely. If this assumption is not met, then the feature set is insufficient, and ambiguity results. Consequently, much research in computer vision has gone toward finding sets of features that are sufficient for specific tasks, with the result that each system has its own associated set of features. A single, general feature set would be desirable. However, research in automatic generation of object recognition programs has demonstrated that predetermined, fixed feature sets are often incapable of providing enough information to unambiguously determine either object identity or pose. One approach to overcoming the inadequacy of any feature set is to utilize multiple sensor observations obtained from different viewpoints, and combine them with knowledge of the 3-D structure of the object to perform unambiguous object recognition. This article presents initial results toward performing object recognition by using multiple observations to resolve ambiguities. Starting from the premise that sensor motions should be planned in advance, the difficulties involved in planning with ambiguous information are discussed. A representation for planning that combines geometric information with viewpoint uncertainty is presented. A sensor planner utilizing the representation was implemented, and the results of pose-determination experiments performed with the planner are discussed.			GREMBAN, KD (corresponding author), CARNEGIE MELLON UNIV,DEPT COMP SCI,5000 FORBES AVE,PITTSBURGH,PA 15213, USA.			Gremban, Keith/0000-0003-2077-2852				ALLACK AS, UNPUB LINEAR TIME AL; ARMAN F, 1991, JUN IEEE WORKSH DIR, P124; BLAAKUMAR P, 1991, FRAME BASED GEOMETRI; Bolles RC, 1987, 3 DIMENSIONAL MACHIN, P399; BROST RC, 1988, INT J ROBOT RES, V7, P3, DOI 10.1177/027836498800700101; Camps O. I., 1991, P IEEE WORKSHOP DIRE, P11; COWAN GK, 1988, IEEE T PATTERN ANAL, V10, P407; FLYNN PJ, 1991, IEEE T PATTERN ANAL, V13, P1066, DOI 10.1109/34.99239; FUJIWARA Y, 1991, CMURITR9116 CARN MEL; GREMBAN KD, 1993, 3 DIMENSIONAL OBJECT, P229; HANSEN C, 1989, IEEE T PATTERN ANAL, V11, P1181, DOI 10.1109/34.42856; HONG KS, 1990, 10TH P INT C PATT RE, P65; HUTCHINSON SA, 1989, IEEE T ROBOTIC AUTOM, V5, P765, DOI 10.1109/70.88098; IKEUCHI K, 1988, P IEEE, V76, P1016, DOI 10.1109/5.5972; IKEUCHI K, 1987, INT J COMPUT VISION, V1, P145, DOI 10.1007/BF00123163; IKEUCHI K, 1991, CVGIP-IMAG UNDERSTAN, V53, P154, DOI 10.1016/1049-9660(91)90024-J; KOENDERINK JJ, 1979, BIOL CYBERN, V32, P211, DOI 10.1007/BF00337644; LIU CH, 1990, COMPUT VISION GRAPH, V50, P177, DOI 10.1016/0734-189X(90)90040-3; MAVER J, 1990, P DARPA IMAGE UNDERS, P482; SAFRANEK RJ, 1990, IEEE T ROBOT AUTOM, V6, P407; SATO K, 1992, CVGIP-IMAG UNDERSTAN, V55, P155, DOI 10.1016/1049-9660(92)90014-T; Schafer G., 1976, MATH THEORY EVIDENCE; SEIBERT M, 1992, IEEE T PATTERN ANAL, V14, P107, DOI 10.1109/34.121784; Tan M., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P858, DOI 10.1109/ROBOT.1990.126097; YI S, 1990, 10TH P INT C PATT RE, P55; [No title captured]; [No title captured]	27	29	29	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1994	12	2-3					137	172		10.1007/BF01421201	http://dx.doi.org/10.1007/BF01421201			36	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NL558		Green Submitted			2022-12-18	WOS:A1994NL55800002
J	BROOKS, MJ; CHOJNACKI, W; KOZERA, R				BROOKS, MJ; CHOJNACKI, W; KOZERA, R			IMPOSSIBLE AND AMBIGUOUS SHADING PATTERNS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								A smooth object depicted in a monochrome image will often exhibit brightness variation, or shading. A problem much studied in computer vision has been that of how object shape may be recovered from image shading. When the imaging conditions are such that an overhead point-source illuminates a smooth Lambertian surface, the problem may be formulated as that of finding a solution to an eikonal equation. This article will focus on the existence and uniqueness of such solutions, reporting recent results obtained. With regard to existence, shading patterns are exhibited for which there is no corresponding object shape. Specifically, a necessary and sufficient condition is presented for a circularly symmetric eikonal equation to admit exclusively unbounded solutions; additionally, a sufficient condition is given for an eikonal equation to have no solution whatsoever. In connection with uniqueness, we consider eikonal equations, defined over a disc, such that the Euclidean norm of the gradient of any solution is circularly symmetric, vanishes exactly at the disc center, and diverges to infinity as the circumference of the disc is approached. Contrary to earlier influential work, a class of such eikonal equations is shown to possess simultaneously circularly symmetric and noncircularly symmetric bounded smooth solutions.	UNIV ADELAIDE,DEPT COMP SCI,ADELAIDE,SA 5001,AUSTRALIA; UNIV WARSAW,INST APPL MATH & MECH,PL-02097 WARSAW,POLAND; FLINDERS UNIV,SCH INFORMAT SCI & TECHNOL,DISCIPLINE COMP SCI,ADELAIDE,SA 5001,AUSTRALIA; UNIV WESTERN AUSTRALIA,DEPT COMP SCI,NEDLANDS,WA 6009,AUSTRALIA	University of Adelaide; University of Warsaw; Flinders University South Australia; University of Western Australia			Brooks, Michael/G-5614-2012; Chojnacki, Wojciech/AAE-9875-2020	Brooks, Michael/0000-0001-9612-5884; Chojnacki, Wojciech/0000-0001-7782-1956; Kozera, Ryszard/0000-0002-2907-8632				Brooks M., 1983, AUG P NAT C ART INT, P36; BROOKS MJ, 1992, Q APPL MATH, V50, P27, DOI 10.1090/qam/1146621; BROOKS MJ, 1992, J MATH ANAL APPL, V165, P192, DOI 10.1016/0022-247X(92)90075-O; BRUSS AR, 1982, J MATH PHYS, V23, P890, DOI 10.1063/1.525441; DEIFT P, 1981, J MATH ANAL APPL, V34, P235; Horn B.K.P., 1989, SHAPE SHADING; Horn Berthold K. P., 1975, PSYCHOL COMPUTER VIS, P115; HORN BKP, 1989, COMMUNICATION; HRON BKP, 1987, COMMUNICATION	9	29	30	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1992	7	2					119	126		10.1007/BF00128131	http://dx.doi.org/10.1007/BF00128131			8	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HK110					2022-12-18	WOS:A1992HK11000002
J	Zhang, H; Ma, JY				Zhang, Hao; Ma, Jiayi			SDNet: A Versatile Squeeze-and-Decomposition Network for Real-Time Image Fusion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image fusion; Real time; Adaptive; Proportion; Squeeze decomposition	GRADIENT; MODEL; GAN	In this paper, a squeeze-and-decomposition network (SDNet) is proposed to realize multi-modal and digital photography image fusion in real time. Firstly, we generally transform multiple fusion problems into the extraction and reconstruction of gradient and intensity information, and design a universal form of loss function accordingly, which is composed of intensity term and gradient term. For the gradient term, we introduce an adaptive decision block to decide the optimization target of the gradient distribution according to the texture richness at the pixel scale, so as to guide the fused image to contain richer texture details. For the intensity term, we adjust the weight of each intensity loss term to change the proportion of intensity information from different images, so that it can be adapted to multiple image fusion tasks. Secondly, we introduce the idea of squeeze and decomposition into image fusion. Specifically, we consider not only the squeeze process from source images to the fused result, but also the decomposition process from the fused result to source images. Because the quality of decomposed images directly depends on the fused result, it can force the fused result to contain more scene details. Experimental results demonstrate the superiority of our method over the state-of-the-arts in terms of subjective visual effect and quantitative metrics in a variety of fusion tasks. Moreover, our method is much faster than the state-of-the-arts, which can deal with real-time fusion tasks.	[Zhang, Hao; Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China	Wuhan University	Ma, JY (corresponding author), Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.	zhpersonalbox@gmail.com; jyma2010@gmail.com		Ma, Jiayi/0000-0003-3264-3265; Zhang, Hao/0000-0001-5467-3428				Ballester C, 2006, INT J COMPUT VISION, V69, P43, DOI 10.1007/s11263-006-6852-x; Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218; Fu XY, 2019, PROC CVPR IEEE, P10257, DOI 10.1109/CVPR.2019.01051; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goshtasby AA, 2005, IMAGE VISION COMPUT, V23, P611, DOI 10.1016/j.imavis.2005.02.004; Ha Q, 2017, IEEE INT C INT ROBOT, P5108; Haghighat M, 2014, I C APPL INF COMM TE, P424; Hayat N, 2019, J VIS COMMUN IMAGE R, V62, P295, DOI 10.1016/j.jvcir.2019.06.002; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kong SG, 2007, INT J COMPUT VISION, V71, P215, DOI 10.1007/s11263-006-6655-0; Lai SH, 1998, NEURAL NETWORKS FOR SIGNAL PROCESSING VIII, P438, DOI 10.1109/NNSP.1998.710674; Lee SH, 2018, IEEE IMAGE PROC, P1737, DOI 10.1109/ICIP.2018.8451153; Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984; Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342; Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222; Li ST, 2012, IEEE T BIO-MED ENG, V59, P3450, DOI 10.1109/TBME.2012.2217493; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Y, 2015, IET IMAGE PROCESS, V9, P347, DOI 10.1049/iet-ipr.2014.0311; Liu Y, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1070; Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001; Liu Y, 2015, INFORM FUSION, V23, P139, DOI 10.1016/j.inffus.2014.05.004; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma BY, 2021, NEURAL COMPUT APPL, V33, P5793, DOI 10.1007/s00521-020-05358-9; Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2; Ma JY, 2020, INFORM FUSION, V62, P110, DOI 10.1016/j.inffus.2020.04.006; Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573; Ma JY, 2020, INFORM FUSION, V54, P85, DOI 10.1016/j.inffus.2019.07.005; Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004; Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001; Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921; Naidu VPS, 2008, DEFENCE SCI J, V58, P338; Nejati M, 2015, INFORM FUSION, V25, P72, DOI 10.1016/j.inffus.2014.10.004; Paul S, 2016, J CIRCUIT SYST COMP, V25, DOI 10.1142/S0218126616501231; Piella G., 2003, Information Fusion, V4, P259, DOI 10.1016/S1566-2535(03)00046-0; Piella G, 2009, INT J COMPUT VISION, V83, P1, DOI 10.1007/s11263-009-0206-4; Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505; Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910; Shen JB, 2014, IEEE T CYBERNETICS, V44, P1579, DOI 10.1109/TCYB.2013.2290435; Shen XY, 2015, IEEE T PATTERN ANAL, V37, P2518, DOI 10.1109/TPAMI.2015.2417569; Shreyamsha Kumar BK, 2013, SIGNAL IMAGE VIDEO P, V7, P1125, DOI 10.1007/s11760-012-0361-x; Vedaldi Andrea, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249; Xing L, 2018, SIGNAL PROCESS, V145, P233, DOI 10.1016/j.sigpro.2017.12.013; Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548; Xu H, 2020, IEEE T IMAGE PROCESS, V29, P7203, DOI 10.1109/TIP.2020.2999855; Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797; Zhao F, 2019, IEEE ACCESS, V7, P44002, DOI 10.1109/ACCESS.2019.2908378; Zhou F, 2019, IEEE J-STARS, V12, P1549, DOI 10.1109/JSTARS.2019.2910990; Zhu ZQ, 2019, IEEE ACCESS, V7, P20811, DOI 10.1109/ACCESS.2019.2898111	48	28	28	17	29	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2761	2785		10.1007/s11263-021-01501-8	http://dx.doi.org/10.1007/s11263-021-01501-8		JUL 2021	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO					2022-12-18	WOS:000679638500002
J	Ding, KY; Ma, KD; Wang, SQ; Simoncelli, EP				Ding, Keyan; Ma, Kede; Wang, Shiqi; Simoncelli, Eero P.			Comparison of Full-Reference Image Quality Models for Optimization of Image Processing Systems	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image quality assessment; Perceptual optimization; Performance evaluation		The performance of objective image quality assessment (IQA) models has been evaluated primarily by comparing model predictions to human quality judgments. Perceptual datasets gathered for this purpose have provided useful benchmarks for improving IQA methods, but their heavy use creates a risk of overfitting. Here, we perform a large-scale comparison of IQA models in terms of their use as objectives for the optimization of image processing algorithms. Specifically, we use eleven full-reference IQA models to train deep neural networks for four low-level vision tasks: denoising, deblurring, super-resolution, and compression. Subjective testing on the optimized images allows us to rank the competing models in terms of their perceptual performance, elucidate their relative advantages and disadvantages in these tasks, and propose a set of desirable properties for incorporation into future IQA models.	[Ding, Keyan; Ma, Kede; Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China; [Simoncelli, Eero P.] NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10003 USA; [Simoncelli, Eero P.] NYU, Courant Inst Math Sci, New York, NY USA	City University of Hong Kong; Howard Hughes Medical Institute; New York University; New York University	Ma, KD (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.	keyan.ding@my.cityu.edu.hk; kede.ma@cityu.edu.hk; shiqwang@cityu.edu.hk; eero.simoncelli@nyu.edu	Ding, Keyan/ABF-5635-2021	Ding, Keyan/0000-0003-2900-7313; Ma, Kede/0000-0001-8608-1128; Simoncelli, Eero/0000-0002-1206-527X	National Natural Science Foundation of China [62071407, 62022002]; CityU SRG-Fd; APRC [7005560, 9610487]; Hong Kong RGC Early Career Scheme [9048122]; Howard Hughes Medical Institute	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CityU SRG-Fd; APRC; Hong Kong RGC Early Career Scheme; Howard Hughes Medical Institute(Howard Hughes Medical Institute)	The authors would like to thank all subjects who participated in our subjective study during this period of the coronavirus pandemic. This work was supported in part by the National Natural Science Foundation of China (62071407 to KDM and 62022002 to SQW), the CityU SRG-Fd and APRC Grants (7005560 and 9610487 to KDM), the Hong Kong RGC Early Career Scheme (9048122 to SQW), and the Howard Hughes Medical Institute (investigatorship to EPS).	Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ball Johannes, 2018, INT C LEARN REPR ICL; Balle J, 2016, PICT COD SYMP, DOI 10.1109/pcs.2016.7906310; Balle Johannes, 2017, 5 INT C LEARN REPR I; Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652; Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Chang HW, 2013, IEEE T IMAGE PROCESS, V22, P4007, DOI 10.1109/TIP.2013.2266579; Channappayya SS, 2008, INT CONF ACOUST SPEE, P765, DOI 10.1109/ICASSP.2008.4517722; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952; Ding, 2020, ARXIV20040772 CORR; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.1080/01621459.1995.10476626; Egiazarian K, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6752; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Girod Bernd, 1993, P207; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grenander Ulf, 1970, ADV COMPUT, V10, P175, DOI DOI 10.1016/S0065-2458(08)60436-2; Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156; ITU-R, 2002, METHODOLOGY SUBJECTI; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jordan C., 1881, CR HEBD ACAD SCI, V2, P228; Karam, 2015, APPL DIGITAL IMAGE P, V9599, P356; Kingma D.P, P 3 INT C LEARNING R; Kohler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3; Kovesi P, 1999, VIDERE J COMPUT VIS, V1, P1; Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188; Laparra V, 2016, ELECT IMAGING, V2016, P1, DOI DOI 10.2352/ISSN.2470-1173.2016.16.HVEI-103; Laparra V, 2017, J OPT SOC AM A, V34, P1511, DOI 10.1364/JOSAA.34.001511; Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105; Ledig C., 2017, PROC CVPR IEEE, P4681, DOI [10.1109/CVPR.2017.19, DOI 10.1109/CVPR.2017.19]; Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935; Liu TJ, 2013, IEEE T IMAGE PROCESS, V22, P1793, DOI 10.1109/TIP.2012.2236343; Liu YM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508391; Loy Y., 2018, EUR C COMP VIS WORKS; Lubin Jeffrey, 1993, P163; LUCY LB, 1974, ASTRON J, V79, P745, DOI 10.1086/111605; Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009; Ma KD, 2019, IEEE IMAGE PROC, P2344, DOI 10.1109/ICIP.2019.8803390; Ma Kede, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P851, DOI 10.1109/TPAMI.2018.2889948; Ma KD, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6732; Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888; Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2456638, 10.1109/TIP.2015.2436340]; Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462; Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097; Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180; Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009; Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640; Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194; Rajashekar U, 2009, IEEE IMAGE PROC, P2213, DOI 10.1109/ICIP.2009.5413889; Raphan M, 2008, IEEE T IMAGE PROCESS, V17, P1342, DOI 10.1109/TIP.2008.925392; RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055; Safranek R. J., 1989, ICASSP-89: 1989 International Conference on Acoustics, Speech and Signal Processing (IEEE Cat. No.89CH2673-2), P1945, DOI 10.1109/ICASSP.1989.266837; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X; Sheikh H. R., 2006, IMAGE VIDEO QUALITY; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378; Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Simoncelli EP, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL I, P379; Snell J, 2017, IEEE IMAGE PROC, P4277; Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659; Szegedy C, 2013, P ICLR, P1, DOI DOI 10.1021/CT2009208; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; TEO PC, 1994, P SOC PHOTO-OPT INS, V2179, P127, DOI 10.1117/12.172664; Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307; Timofte R., 2017, P IEEE C COMP VIS PA, P114; Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Vukadinovic V, 2009, IEEE T MULTIMEDIA, V11, P1105, DOI 10.1109/TMM.2009.2026096; Wang SQ, 2015, IEEE SIGNAL PROC LET, V22, P2387, DOI 10.1109/LSP.2015.2487369; Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269; Wang TH, 2016, SIGNAL PROCESS-IMAGE, V45, P1, DOI 10.1016/j.image.2016.04.005; Wang Z, 2005, INT CONF ACOUST SPEE, P573; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZJ, 2011, IEEE SIGNAL PROC MAG, V28, P2, DOI 10.1109/MSP.2011.940297; Wang Z, 2008, J VISION, V8, DOI 10.1167/8.12.8; Watson A.B, 1993, P SOC INF DISPL, DOI DOI 10.1109/DCC.1993.253132; Watson AB, 1997, IEEE T IMAGE PROCESS, V6, P1164, DOI 10.1109/83.605413; Wiener N., 1950, EXTRAPOLATION INTERP; Xue WF, 2013, IEEE I CONF COMP VIS, P705, DOI 10.1109/ICCV.2013.93; Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423; Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Ye P, 2014, PROC CVPR IEEE, P4241, DOI 10.1109/CVPR.2014.540; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028; Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang W., 2019, ARXIV190700516; Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319; Zhang XD, 2013, IEEE SIGNAL PROC LET, V20, P319, DOI 10.1109/LSP.2013.2244081; Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865; Zhou F, 2019, IEEE T IMAGE PROCESS, V28, P3528, DOI 10.1109/TIP.2019.2898638	106	28	31	3	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1258	1281		10.1007/s11263-020-01419-7	http://dx.doi.org/10.1007/s11263-020-01419-7		JAN 2021	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK	33495671	Bronze, Green Published, Green Submitted			2022-12-18	WOS:000609373400001
J	Song, YB; Zhang, JW; Gong, LJ; He, SF; Bao, LC; Pan, JS; Yang, QX; Yang, MH				Song, Yibing; Zhang, Jiawei; Gong, Lijun; He, Shengfeng; Bao, Linchao; Pan, Jinshan; Yang, Qingxiong; Yang, Ming-Hsuan			Joint Face Hallucination and Deblurring via Structure Generation and Detail Enhancement	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face hallucination; Face deblurring; Convolutional Neural Network	IMAGES	We address the problem of restoring a high-resolution face image from a blurry low-resolution input. This problem is difficult as super-resolution and deblurring need to be tackled simultaneously. Moreover, existing algorithms cannot handle face images well as low-resolution face images do not have much texture which is especially critical for deblurring. In this paper, we propose an effective algorithm by utilizing the domain-specific knowledge of human faces to recover high-quality faces. We first propose a facial component guided deep Convolutional Neural Network (CNN) to restore a coarse face image, which is denoted as the base image where the facial component is automatically generated from the input face image. However, the CNN based method cannot handle image details well. We further develop a novel exemplar-based detail enhancement algorithm via facial component matching. Extensive experiments show that the proposed method outperforms the state-of-the-art algorithms both quantitatively and qualitatively.	[Song, Yibing; Bao, Linchao] Tencent AI Lab, Shenzhen, Peoples R China; [Zhang, Jiawei] Sensetime Res, Shenzhen, Peoples R China; [Gong, Lijun] Tencent, Shenzhen, Peoples R China; [He, Shengfeng] South China Univ Technol, Guangzhou, Guangdong, Peoples R China; [Pan, Jinshan] Nanjing Univ Sci & Technol, Nanjing, Jiangsu, Peoples R China; [Yang, Qingxiong] Univ Sci & Technol China, Hefei, Anhui, Peoples R China; [Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA USA	Tencent; Tencent; South China University of Technology; Nanjing University of Science & Technology; Chinese Academy of Sciences; University of Science & Technology of China, CAS; University of California System; University of California Merced	Pan, JS (corresponding author), Nanjing Univ Sci & Technol, Nanjing, Jiangsu, Peoples R China.	dynamicstevenson@gmail.com; zhjw1988@gmail.com; gljvivi@gmail.com; shengfenghe7@gmail.com; linchaobao@gmail.com; sdluran@gmail.com; liiton.research@gmail.com; mhyang@ucmerced.edu	He, Shengfeng/E-5682-2016; Pan, Jinshan/AAO-2258-2021; Zhang, Jiawei/AAF-2390-2019; Yang, Ming-Hsuan/AAE-7350-2019; Bao, Linchao/AAG-9148-2020; Yang, Ming-Hsuan/T-9533-2019	He, Shengfeng/0000-0002-3802-4644; Bao, Linchao/0000-0001-9543-3754; Yang, Ming-Hsuan/0000-0003-4848-2304	NSF CAREER [1149783]; NSF of China [61872421, 61572099]; NSF of Jiangsu Province [BK20180471]; National Science and Technology Major Project [2018ZX04041001-007]	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF of China(National Natural Science Foundation of China (NSFC)); NSF of Jiangsu Province; National Science and Technology Major Project	This work has been supported in part by the NSF CAREER(No. 1149783), NSF of China (No. 61872421 and 61572099), NSF of Jiangsu Province (No. BK20180471), and National Science and Technology Major Project (2018ZX04041001-007).	Agustsson E, 2017, IEEE I CONF COMP VIS, P1652, DOI 10.1109/ICCV.2017.182; Cao Q., 2017, IEEE C COMP VIS PATT; Chen Y., 2017, ARXIV171110703; Delac K, 2005, INT J IMAG SYST TECH, V15, P252, DOI 10.1002/ima.20059; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Dong XW, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901327; Dong XW, 2018, IEEE CONF COMPUT; Eisemann E., 2004, ACM T GRAPHICS SIGGR; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Gu SH, 2015, IEEE I CONF COMP VIS, P1823, DOI 10.1109/ICCV.2015.212; Gunturk BK, 2003, IEEE T IMAGE PROCESS, V12, P597, DOI 10.1109/TIP.2003.811513; HaCohen Y, 2013, IEEE I CONF COMP VIS, P2384, DOI 10.1109/ICCV.2013.296; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2010, LECT NOTES COMPUT SC, V6311, P1; HE KM, 2013, TPAMI, V35, P1397, DOI DOI 10.1109/TPAMI.2012.213; Huang ZY, 2017, IEEE INT SYMP ELEC; Jia K, 2005, IEEE I CONF COMP VIS, P1683; Jia K., 2006, IEEE C COMP VIS PATT; Jia K, 2008, IEEE T IMAGE PROCESS, V17, P873, DOI 10.1109/TIP.2008.922421; Jin Y., 2015, IEEE C COMP VIS PATT; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jourabloo A., 2017, IEEE INT C COMP VIS; Kim J, 2016, IEEE CONF COMPUT; Kingma D.P, P 3 INT C LEARNING R; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618; Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu S., 2014, IEEE INT C IM PROC; Liu Wei, 2005, IEEE C COMP VIS PATT; Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019; Mu N, 2017, IEEE INT C ELECTR TA; Pan JS, 2018, IEEE T PATTERN ANAL, V40, P2315, DOI 10.1109/TPAMI.2017.2753804; Pan JS, 2014, LECT NOTES COMPUT SC, V8695, P47, DOI 10.1007/978-3-319-10584-0_4; Pan Junting, 2017, ABS170101081 CORR; Park H, 2017, IEEE I CONF COMP VIS, P4623, DOI 10.1109/ICCV.2017.494; Petschnigg G., 2004, ACM T GRAPHICS SIGGR; Sajjadi Mehdi S. M., 2017, ICCV, DOI DOI 10.1109/CVPR.2019.00817; Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Song YB, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4530; Song YB, 2017, COMPUT VIS IMAGE UND, V162, P135, DOI 10.1016/j.cviu.2017.08.009; Song Yibing, 2014, EUR C COMP VIS; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Tappen Marshall, 2012, EUR C COMP VIS; Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149; Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9; Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Yang CY, 2018, INT J COMPUT VISION, V126, P597, DOI 10.1007/s11263-017-1044-4; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Yu F, 2017, IEEE INT SYMP ELEC; Yu X, 2017, AAAI CONF ARTIF INTE, P4327; Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20; Zhang W, 2018, IEEE CONF COMPUT; Zhou EJ, 2015, AAAI CONF ARTIF INTE, P3871; Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	64	28	28	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		785	800		10.1007/s11263-019-01148-6	http://dx.doi.org/10.1007/s11263-019-01148-6			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	HZ0JD		Green Submitted			2022-12-18	WOS:000468525900014
J	Goyal, Y; Khot, T; Agrawal, A; Summers-Stay, D; Batra, D; Parikh, D				Goyal, Yash; Khot, Tejas; Agrawal, Aishwarya; Summers-Stay, Douglas; Batra, Dhruv; Parikh, Devi			Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual question answering; VQA; VQA challenge		The problem of visual question answering (VQA) is of significant importance both as a challenging research question and for the rich set of applications it enables. In this context, however, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in VQA models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of VQA and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., in: ICCV, 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the VQA Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. We also present interesting insights from analysis of the participant entries in VQA Challenge 2017, organized by us on the proposed VQA v2.0 dataset. The results of the challenge were announced in the 2nd VQA Challenge Workshop at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.	[Goyal, Yash; Agrawal, Aishwarya; Batra, Dhruv; Parikh, Devi] Georgia Tech, Atlanta, GA 30332 USA; [Khot, Tejas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Summers-Stay, Douglas] Army Res Lab, Adelphi, MD USA; [Batra, Dhruv; Parikh, Devi] Facebook AI Res, Menlo Pk, CA USA	University System of Georgia; Georgia Institute of Technology; Carnegie Mellon University; United States Department of Defense; United States Army; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL); Facebook Inc	Goyal, Y (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.	ygoyal@gatech.edu; tkhot@andrew.cmu.edu; aishwarya@gatech.edu; douglas.a.summers-stay.civ@mail.mil; dbatra@gatech.edu; parikh@gatech.edu			NSF CAREER awards; ONR [N00014-14-1-0679]; Sloan Fellowship; ARO YIP awards; Allen Distinguished Investigator award from the Paul G. Allen Family Foundation; ICTAS Junior Faculty awards; Google Faculty Research Awards; Amazon Academic Research Awards; AWS in Education Research Grant; NVIDIA GPU; ONR YIP award	NSF CAREER awards(National Science Foundation (NSF)); ONR(Office of Naval Research); Sloan Fellowship(Alfred P. Sloan Foundation); ARO YIP awards; Allen Distinguished Investigator award from the Paul G. Allen Family Foundation; ICTAS Junior Faculty awards; Google Faculty Research Awards(Google Incorporated); Amazon Academic Research Awards; AWS in Education Research Grant; NVIDIA GPU; ONR YIP award	We thank Anitha Kannan for helpful discussions. This work was funded in part by NSF CAREER awards to DP and DB, an ONR YIP award to DP, ONR Grant N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, an Allen Distinguished Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and DP, Google Faculty Research Awards to DP and DB, Amazon Academic Research Awards to DP and DB, AWS in Education Research Grant to DB, and NVIDIA GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.	Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522; Agrawal Aishwarya, 2016, ARXIV160607356; Agrawal Aishwarya, 2017, ARXIV170408243; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Berg T., 2013, ICCV; Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856; Devlin J., 2015, ARXIV150504467; Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Fukui Akira, 2016, ARXIV160601847; Gao H., 2015, NIPS; Goyal Y., 2016, ICML; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hodosh M., 2016, WORKSH VIS LANG ANN; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Ilievski I., 2016, ARXIV160401485; Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kafle K, 2017, IEEE I CONF COMP VIS, P1983, DOI 10.1109/ICCV.2017.217; Kafle K, 2016, PROC CVPR IEEE, P4976, DOI 10.1109/CVPR.2016.538; Kafle Kushal, 2016, ARXIV161001465; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kim Jin-Hwa, 2016, ADV NEURAL INFORM PR, P1; Kiros R., 2015, TACL; Koh PW, 2017, PR MACH LEARN RES, V70; Krishna Ranjay, 2016, ARXIV160207332; Lim W., 2017, P INT C LEARN REPR; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lu J., 2015, DEEPER LSTM NORMALIZ, V6, P1; Lu JS, 2016, ADV NEUR IN, V29; Malinowski M., 2014, ADV NEURAL INFORM PR, V27, P1682; Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9; Mao J., 2014, ARXIV14101090; Noh Hyeonwoo, 2016, ARXIV160603647; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Ray A., 2016, EMNLP; Ren M., 2015, ADV NEURAL INFORM PR, V28, P2953; Ribeiro Marco Tulio, 2016, P KDD, P97, DOI [10.18653/v1/n16-3020, DOI 10.1145/2939672.2939778]; Saito K., 2016, ARXIV160606108; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499; Shin A., 2016, ARXIV160906657; Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501; Teney D, 2018, PROC CVPR IEEE, P4223, DOI 10.1109/CVPR.2018.00444; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang Peng, 2015, ARXIV151102570; Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500; Xiong CM, 2016, PR MACH LEARN RES, V48; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yu LC, 2015, IEEE I CONF COMP VIS, P2461, DOI 10.1109/ICCV.2015.283; Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202; Zhang P, 2016, PROC CVPR IEEE, P5014, DOI 10.1109/CVPR.2016.542; Zhou B., 2015, CVPR; Zhou Bolei, 2015, ARXIV151202167; Zhu YX, 2016, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2016.415	62	28	30	9	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2019	127	4					398	414		10.1007/s11263-018-1116-0	http://dx.doi.org/10.1007/s11263-018-1116-0			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HN4DR		Green Submitted			2022-12-18	WOS:000460135000005
J	Durrleman, S; Allassonniere, S; Joshi, S				Durrleman, Stanley; Allassonniere, Stephanie; Joshi, Sarang			Sparse Adaptive Parameterization of Variability in Image Ensembles	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Atlas construction; Image variability; Diffeomorphisms; Sparsity; Control points; FISTA	FRAMEWORK; DIFFEOMORPHISMS; ALGORITHM; FLOWS	This paper introduces a new parameterization of diffeomorphic deformations for the characterization of the variability in image ensembles. Dense diffeomorphic deformations are built by interpolating the motion of a finite set of control points that forms a Hamiltonian flow of self-interacting particles. The proposed approach estimates a template image representative of a given image set, an optimal set of control points that focuses on the most variable parts of the image, and template-to-image registrations that quantify the variability within the image set. The method automatically selects the most relevant control points for the characterization of the image variability and estimates their optimal positions in the template domain. The optimization in position is done during the estimation of the deformations without adding any computational cost at each step of the gradient descent. The selection of the control points is done by adding a L (1) prior to the objective function, which is optimized using the FISTA algorithm.	[Durrleman, Stanley; Joshi, Sarang] Sci Comp & Imaging SCI Inst, Salt Lake City, UT 84112 USA; [Allassonniere, Stephanie] Ecole Polytech, Ctr Mathemat Appl CMAP, UMR CNRS 7641, F-91128 Palaiseau, France	Institut Polytechnique de Paris	Durrleman, S (corresponding author), Sci Comp & Imaging SCI Inst, 72 S Cent Dr, Salt Lake City, UT 84112 USA.	stanley@sci.utah.edu			ANR grant IRMGroup; NIH grant NIBIB [5R01EB007688]; NIH grant NCRR [2P41 RR0112553-12]	ANR grant IRMGroup(French National Research Agency (ANR)); NIH grant NIBIB; NIH grant NCRR	We would like to thank Timothy O'Keefe and Paul Sanders for their kind proofreading of the manuscript. This work has been supported by ANR grant IRMGroup and NIH grants NIBIB (5R01EB007688) and NCRR (2P41 RR0112553-12).	Allassonniere S, 2007, J R STAT SOC B, V69, P3; Allassonniere S, 2010, BERNOULLI, V16, P641, DOI 10.3150/09-BEJ229; Arsigny V, 2006, LECT NOTES COMPUT SC, V4190, P924; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Dupuis P, 1998, Q APPL MATH, V56, P587, DOI 10.1090/qam/1632326; Durrleman S, 2011, LECT NOTES COMPUT SC, V6801, P123, DOI 10.1007/978-3-642-22092-0_11; Durrleman S, 2011, NEUROIMAGE, V55, P1073, DOI 10.1016/j.neuroimage.2010.11.056; Durrleman S, 2009, MED IMAGE ANAL, V13, P793, DOI 10.1016/j.media.2009.07.007; Durrleman Stanley, 2010, THESIS U NICE SOPHIA; Friedman J., 2009, ELEMENTS STAT LEARNI, DOI 10.1007/978-0-387-84858-7; Glasbey CA, 2001, J R STAT SOC B, V63, P465, DOI 10.1111/1467-9868.00295; Glaunes J, 2008, INT J COMPUT VISION, V80, P317, DOI 10.1007/s11263-008-0141-9; Grenander U, 1998, Q APPL MATH, V56, P617, DOI 10.1090/qam/1668732; Grenander U, 1994, GEN PATTERN THEORY M; Grenander U, 2007, IEEE T MED IMAGING, V26, P648, DOI 10.1109/TMI.2006.891500; Hansen M.S., 2008, 2008 IEEE C COMP VIS, P1; Joshi SC, 2000, IEEE T IMAGE PROCESS, V9, P1357, DOI 10.1109/83.855431; Lorenzen P, 2005, LECT NOTES COMPUT SC, V3750, P411, DOI 10.1007/11566489_51; Marsland S, 2007, LECT NOTES COMPUT SC, V4584, P396; Meyer Y., 2001, U LECT SER, V22, px+122, DOI 10.1090/ulect/022; Miller MI, 2006, J MATH IMAGING VIS, V24, P209, DOI 10.1007/s10851-005-3624-0; Miller MI, 2002, ANNU REV BIOMED ENG, V4, P375, DOI 10.1146/annurev.bioeng.4.092101.125733; Miller MI, 2001, INT J COMPUT VISION, V41, P61, DOI 10.1023/A:1011161132514; Nesterov Y., 1983, SOVIET MATH DOKLADY, V27; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Risser L, 2011, IEEE T MED IMAGING, V30, P1746, DOI 10.1109/TMI.2011.2146787; Rueckert D, 2006, LECT NOTES COMPUT SC, V4191, P702; Singh N, 2010, LECT NOTES COMPUT SC, V6363, P529; Sommer S., 2012, ARXIV11123166V1; Sommer S., 2012, IEEE WORKSH MATH MET; Trouve A, 2005, FOUND COMPUT MATH, V5, P173, DOI 10.1007/s10208-004-0128-z; Trouve A, 1998, INT J COMPUT VISION, V28, P213, DOI 10.1023/A:1008001603737; Vaillant M, 2005, LECT NOTES COMPUT SC, V3565, P381; Vaillant M, 2004, NEUROIMAGE, V23, pS161, DOI 10.1016/j.neuroimage.2004.07.023; Vercauteren T, 2009, NEUROIMAGE, V45, pS61, DOI 10.1016/j.neuroimage.2008.10.040; Wang L, 2007, IEEE T MED IMAGING, V26, P462, DOI 10.1109/TMI.2006.887380; Yu GS, 2010, IEEE IMAGE PROC, P1641, DOI 10.1109/ICIP.2010.5653853; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	41	28	28	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	1					161	183		10.1007/s11263-012-0556-1	http://dx.doi.org/10.1007/s11263-012-0556-1			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	080XV		Green Submitted			2022-12-18	WOS:000314278500007
J	Foytik, J; Asari, VK				Foytik, Jacob; Asari, Vijayan K.			A Two-Layer Framework for Piecewise Linear Manifold-Based Head Pose Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Head pose estimation; Piecewise linear manifold; Coarse to fine; Phase congruency; Gabor filter	ILLUMINATION; PHASE; SPACE	Fine-grain head pose estimation from imagery is an essential operation for many human-centered systems, including pose independent face recognition and human-computer interaction (HCI) systems. It is only recently that estimation systems have evolved past coarse level classification of pose and concentrated on fine-grain estimation. In particular, the state of the art of such systems consists of nonlinear manifold embedding techniques that capture the intrinsic relationship of a pose varying face dataset. The success of these solutions can be attributed to the acknowledgment that image variation corresponding to pose change is nonlinear in nature. Yet, the algorithms are limited by the complexity of embedding functions that describe the relationship. We present a pose estimation framework that seeks to describe the global nonlinear relationship in terms of localized linear functions. A two layer system (coarse/fine) is formulated on the assumptions that coarse pose estimation can be performed adequately using supervised linear methods, and fine pose estimation can be achieved using linear regressive functions if the scope of the pose manifold is limited. A pose estimation system is implemented utilizing simple linear subspace methods and oriented Gabor and phase congruency features. The framework is tested using widely accepted pose-varying face databases (FacePix(30) and Pointing'04) and shown to perform fine head pose estimation with competitive accuracy when compared with state of the art nonlinear manifold methods.	[Foytik, Jacob; Asari, Vijayan K.] Univ Dayton, Dayton, OH 45469 USA	University of Dayton	Foytik, J (corresponding author), Univ Dayton, 300 Coll Pk, Dayton, OH 45469 USA.	jfoytik1@notes.udayton.edu; vijayan.asari@notes.udayton.edu						Balasubramanian V. N., 2007, CEAS, DOI [10.1109/CVPR.2007.383280., DOI 10.1109/CVPR.2007.383280]; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BenAbdelkader C, 2010, LECT NOTES COMPUT SC, V6316, P518, DOI 10.1007/978-3-642-15567-3_38; Foytik J, 2010, LECT NOTES COMPUT SC, V6455, P49; Fu Y, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P3; Gourier N., 2004, P ICPR INT WORKSH VI; Gourier N., 2006, CLEAR 2006 S; Haj M.A., 2012, 25 IEEE COMP VIS PAT; Kovesi P, 1999, VIDERE J COMPUT VIS, V1, P1; Little G, 2005, INT CONF ACOUST SPEE, P89; Ma BP, 2006, INT C PATT RECOG, P512; Ma BP, 2008, IEEE T SYST MAN CY B, V38, P1501, DOI 10.1109/TSMCB.2008.928231; Melzer T, 2003, PATTERN RECOGN, V36, P1961, DOI 10.1016/S0031-3203(03)00058-X; Murphy-Chutorian E., 2007, INT TRANSP SYST C 20, P709, DOI DOI 10.1109/ITSC.2007.4357803; Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106; OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022; Sherrah J, 2001, IMAGE VISION COMPUT, V19, P807, DOI 10.1016/S0262-8856(00)00096-2; Stiefelhagen R., 2004, P ICPR WORKSH VIS OB; Tu JL, 2007, LECT NOTES COMPUT SC, V4122, P281; Voit M., 2006, LNCS, P299; Wang XW, 2008, LECT NOTES COMPUT SC, V5303, P624, DOI 10.1007/978-3-540-88688-4_46; Wu JW, 2005, LECT NOTES COMPUT SC, V3723, P321; Zhu Li, 2007, 2007 International Conference on Multimedia & Expo, P1810	23	28	30	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	2					270	287		10.1007/s11263-012-0567-y	http://dx.doi.org/10.1007/s11263-012-0567-y			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	081BW					2022-12-18	WOS:000314291600003
J	Ying, XH; Zha, HB				Ying, Xianghua; Zha, Hongbin			Identical projective geometric properties of central catadioptric line images and sphere images with applications to calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						catadioptric camera; omnidirectional vision; camera calibration; line image; sphere image; double-contact theorem	CAMERA CALIBRATION	Central catadioptric cameras are imaging devices that use mirrors to enhance the field of view while preserving a single effective viewpoint. Lines and spheres in space are all projected into conics in the central catadioptric image planes, and such conics are called line images and sphere images, respectively. We discovered that there exists an imaginary conic in the central catadioptric image planes, defined as the modified image of the absolute conic (MIAC), and by utilizing the MIAC, the novel identical projective geometric properties of line images and sphere images may be exploited: Each line image or each sphere image is double-contact with the MIAC, which is an analogy of the discovery in pinhole camera that the image of the absolute conic (IAC) is double-contact with sphere images. Note that the IAC also exists in the central catadioptric image plane, but it does not have the double-contact properties with line images or sphere images. This is the main reason to propose the MIAC. From these geometric properties with the MIAC, two linear calibration methods for central catadioptric cameras using sphere images as well as using line images are proposed in the same framework. Note that there are many linear approaches to central catadioptric camera calibration using line images. It seems that to use the properties that line images are tangent to the MIAC only leads to an alternative geometric construction for calibration. However, for sphere images, there are only some nonlinear calibration methods in literature. Therefore, to propose linear methods for sphere images may be the main contribution of this paper. Our new algorithms have been tested in extensive experiments with respect to noise sensitivity.	[Ying, Xianghua; Zha, Hongbin] Peking Univ, Natl Lab Machine Percept, Beijing 100871, Peoples R China	Peking University	Ying, XH (corresponding author), Peking Univ, Natl Lab Machine Percept, Beijing 100871, Peoples R China.	xhying@cis.pku.edu.cn; zha@cis.pku.edu.cn	cai, bo/G-1491-2010					Agrawal M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P782; Aliaga DG, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P127, DOI 10.1109/ICCV.2001.937508; Baker S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P35, DOI 10.1109/ICCV.1998.710698; Barreto JP, 2006, COMPUT VIS IMAGE UND, V101, P151, DOI 10.1016/j.cviu.2005.07.002; Barreto JP, 2005, IEEE T PATTERN ANAL, V27, P1327, DOI 10.1109/TPAMI.2005.163; BARRETO JP, 2001, P IEEE INT C COMP VI; BOGNER S, 1995, P IEEE C SYST MAN CY, P3099; Evelyn CJA, 1974, 7 CIRCLES THEOREM OT; Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658; Geyer C, 2002, IEEE T PATTERN ANAL, V24, P687, DOI 10.1109/34.1000241; Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135; Geyer C., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P398, DOI 10.1109/ICCV.1999.791248; Gluckman J, 2001, INT J COMPUT VISION, V44, P65, DOI 10.1023/A:1011172403203; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hecht E, 1997, OPTICS; HONG JW, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P620, DOI 10.1109/ROBOT.1991.131651; Kang SB, 2000, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2000.855820; NALWA V, 1996, TRUE OMNIDIRECTIONAL; NAYAR SK, 1988, P SPIE OPTICS ILLUMI, V2; Nene SA, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1087, DOI 10.1109/ICCV.1998.710852; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; SVOBODA T, 1998, P 5 EUR C COMP VIS, P218; Swaminathan R, 2006, INT J COMPUT VISION, V66, P211, DOI 10.1007/s11263-005-3220-1; Teramoto H., 2002, Proceedings of the Fifth Asian Conference on Computer Vision, P499; Yagi Y, 2004, INT J COMPUT VISION, V58, P173, DOI 10.1023/B:VISI.0000019684.35147.fc; Yagi Y., 1990, Proceedings. IROS '90. IEEE International Workshop on Intelligent Robots and Systems '90. Towards a New Frontier of Applications (Cat. No.90TH0332-7), P181, DOI 10.1109/IROS.1990.262385; Ying XH, 2004, IEEE T PATTERN ANAL, V26, P1260, DOI 10.1109/TPAMI.2004.79; Ying XH, 2005, IEEE I CONF COMP VIS, P596; Ying XH, 2006, IEEE T PATTERN ANAL, V28, P2031, DOI 10.1109/TPAMI.2006.245; Zhang H, 2007, IEEE T PATTERN ANAL, V29, P499, DOI 10.1109/TPAMI.2007.45	30	28	33	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2008	78	1					89	105		10.1007/s11263-007-0082-8	http://dx.doi.org/10.1007/s11263-007-0082-8			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	270YE					2022-12-18	WOS:000253755300006
J	Modersitzki, J				Modersitzki, Jan			FLIRT with rigidity - Image registration with a local non-rigidity penalty	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Conference on Computer Vision and Pattern Recognition	JUN 20-25, 2005	San Diego, CA	IEEE Comp Soc		image processing; image registration; warping; fusion; rigidity constraints; variational techniques; constrained optimization	FREE-FORM DEFORMATIONS; INFORMATION	Registration is a technique nowadays commonly used in medical imaging. A drawback of most of the current registration schemes is that all tissue is being considered as non-rigid (Staring et al., Proceedings of the SPIE 2006, vol. 6144, pp. 1-10, 2006). Therefore, rigid objects in an image, such as bony structures or surgical instruments, may be transformed non-rigidly. In this paper, we integrate the concept of local rigidity to the FLexible Image Registration Toolbox (FLIRT) (Haber and Modersitzki, in SIAM J. Sci. Comput. 27(5): 1594-1607, 2006; Modersitzki, Numerical Methods for Image Registration, 2004). The idea is to add a penalty for local non-rigidity to the cost function and thus to penalize non-rigid transformations of rigid objects. As our examples show, the new approach allows the maintenance of local rigidity in the desired fashion. For example, the new scheme can keep bony structures rigid during registration. We show, how the concept of local rigidity can be integrated in the FLIRT approach and present the variational backbone, a proper discretization, and a multilevel optimization scheme. We compare the FLIRT approach to the B-spline approach. As expected from the more general setting of the FLIRT approach, our examples demonstrate that the FLIRT results are superior: much smoother, smaller deformations, visually much more pleasing.	Med Univ Lubeck, Inst Math, D-23560 Lubeck, Germany	University of Lubeck	Modersitzki, J (corresponding author), Med Univ Lubeck, Inst Math, Wallstr 40, D-23560 Lubeck, Germany.	modersitzki@math.uni-luebeck.de						AMIT Y, 1994, SIAM J SCI COMPUT, V15, P207, DOI 10.1137/0915014; BREWER JW, 1978, IEEE T CIRCUITS SYST, V25, P772, DOI 10.1109/TCS.1978.1084534; COLLIGNON A, 1995, COMP IMAG VIS, V3, P263; Droske M, 2003, SIAM J APPL MATH, V64, P668, DOI 10.1137/S0036139902419528; Fischer B, 2003, LECT NOTES COMPUT SC, V2717, P261; Fischer B, 2003, PROC SPIE, V5032, P1037, DOI 10.1117/12.480118; Fischer B., 2003, PAMM, V3, P32; Fitzpatrick J. M., 2000, HDB MED IMAGING, V2, P447; Gill P. E., 1981, PRACTICAL OPTIMIZATI; Glasbey CA, 1998, J APPL STAT, V25, P155, DOI 10.1080/02664769823151; Gurtin ME, 1982, INTRO CONTINUUM MECH; Haber E, 2006, SIAM J SCI COMPUT, V27, P1594, DOI 10.1137/040608106; Haber E, 2004, INVERSE PROBL, V20, P1621, DOI 10.1088/0266-5611/20/5/018; Haber E., 2005, BILDVERARBEITUNG MED, P350, DOI DOI 10.1007/3-540-26431-0; Haber E, 2007, INT J COMPUT VISION, V71, P361, DOI 10.1007/s11263-006-8984-4; Hajnal JV, 2001, MED IMAGE REGISTRATI; Johnson C, 2002, IEEE TECHNOL SOC MAG, V21, P5; Kabus S, 2006, LECT NOTES COMPUT SC, V4057, P92; Keeling SL, 2005, J MATH IMAGING VIS, V23, P47, DOI 10.1007/s10851-005-4967-2; Little JA, 1997, COMPUT VIS IMAGE UND, V66, P223, DOI 10.1006/cviu.1997.0608; Loeckx D, 2004, LECT NOTES COMPUT SC, V3216, P639; Maintz J B, 1998, Med Image Anal, V2, P1, DOI 10.1016/S1361-8415(01)80026-8; *MATH WORKS, 1992, MATL US GUID; MODERSITZKI J, 2004, INFORMATIK AKTUELL B; Peckar W, 1999, J MATH IMAGING VIS, V10, P143, DOI 10.1023/A:1008375006703; Pluim JPW, 2000, IEEE T MED IMAGING, V19, P809, DOI 10.1109/42.876307; Rohlfing T, 2003, IEEE T MED IMAGING, V22, P730, DOI 10.1109/TMI.2003.814791; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; STARING M, 2006, P SPIE 2006 MED IM 2, V6144, P1; TROTTENBERG U., 2000, MULTIGRID; Wells W M 3rd, 1996, Med Image Anal, V1, P35; Yoo T.S, 2004, INSIGHT IMAGES; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	33	28	28	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2008	76	2					153	163		10.1007/s11263-007-0079-3	http://dx.doi.org/10.1007/s11263-007-0079-3			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	255VH					2022-12-18	WOS:000252685400006
J	Shic, F; Scassellati, B				Shic, Frederick; Scassellati, Brian			A behavioral analysis of computational models of visual attention	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						computational attention; robot attention; visual attention model; behavioral analysis; eye-tracking; human validation; saliency map; dimensionality reduction; gaze metric; classification strategy	SALIENCE; PULVINAR; SEARCH; SHIFTS	Robots often incorporate computational models of visual attention to streamline processing. Even though the number of visual attention systems employed on robots has increased dramatically in recent years, the evaluation of these systems has remained primarily qualitative and subjective. We introduce quantitative methods for evaluating computational models of visual attention by direct comparison with gaze trajectories acquired from humans. In particular, we focus on the need for metrics based not on distances within the image plane, but that instead operate at the level of underlying features. We present a framework, based on dimensionality-reduction over the features of human gaze trajectories, that can simultaneously be used for both optimizing a particular computational model of visual attention and for evaluating its performance in terms of similarity to human behavior. We use this framework to evaluate the Itti et al. (1998) model of visual attention, a computational model that serves as the basis for many robotic visual attention systems.	Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA	Yale University	Shic, F (corresponding author), Yale Univ, Dept Comp Sci, POB 2158, New Haven, CT 06520 USA.		Shic, Frederick/AAE-9828-2020	Shic, Frederick/0000-0002-9040-1259				[Anonymous], 2004, ELECT LETT COMPUT VI, DOI DOI 10.5565/REV/ELCVIA.66; BALKENIUS C, 2004, P LAVS 04 ST CATH CO; Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141; Breazeal C, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1146; Burgard W, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P11; Draper BA, 2005, COMPUT VIS IMAGE UND, V100, P152, DOI 10.1016/j.cviu.2004.08.006; Duda RO, 1973, PATTERN RECOGNITION; Fong T, 2003, ROBOT AUTON SYST, V42, P143, DOI 10.1016/S0921-8890(02)00372-X; Fujita M, 2001, INT J ROBOT RES, V20, P781, DOI 10.1177/02783640122068092; Gockley R, 2005, 2005 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2199; Gottlieb JP, 1998, NATURE, V391, P481, DOI 10.1038/35135; HEEGAARD NHH, 1988, HDB IMMUNOBLOTTING P, V1, P1; Imai M, 2002, IEEE ROMAN 2002, PROCEEDINGS, P411, DOI 10.1109/ROMAN.2002.1045657; Itti L, 2005, VIS COGN, V12, P1093, DOI 10.1080/13506280444000661; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Itti L., 2006, ADV NEURAL INFORM PR, P1; Itti L., 2005, NEUROBIOLOGY ATTENTI; Itti L., 2003, P SPIE 48 ANN INT S; ITTI L, 2006, IN PRESS VISUAL COGN; Jain R., 1995, MACHINE VISION; Klin A, 2002, ARCH GEN PSYCHIAT, V59, P809, DOI 10.1001/archpsyc.59.9.809; KOCH C, 1985, HUM NEUROBIOL, V4, P219; KOCH C, 1984, 787 MIT ART INT LAB; Kustov AA, 1996, NATURE, V384, P74, DOI 10.1038/384074a0; Lee DK, 1999, NAT NEUROSCI, V2, P375, DOI 10.1038/7286; Li ZP, 2002, TRENDS COGN SCI, V6, P9, DOI 10.1016/S1364-6613(00)01817-9; Mazer JA, 2003, NEURON, V40, P1241, DOI 10.1016/S0896-6273(03)00764-5; Nagai Y, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P932, DOI 10.1109/IRDS.2002.1041510; Niebur E, 1996, ADV NEUR IN, V8, P802; NIEBUR E, 1995, P 2 JOINT S NEUR COM, V5, P26; Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4; PETERSEN SE, 1987, NEUROPSYCHOLOGIA, V25, P97, DOI 10.1016/0028-3932(87)90046-7; ROBINSON DL, 1992, TRENDS NEUROSCI, V15, P127, DOI 10.1016/0166-2236(92)90354-B; Salvucci D. D., 2000, P 2000 S EYE TRACKIN, P71, DOI [DOI 10.1145/355017.355028, 10.1145/355017.355028]; SCASSELLATI B, 1999, LECT NOTES COMPUTER, V1562, P176; Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017; Tessier-Lavigne M, 1991, PRINCIPLES NEURAL SC, P401; Torralba A, 2003, J OPT SOC AM A, V20, P1407, DOI 10.1364/JOSAA.20.001407; Treue S, 2003, CURR OPIN NEUROBIOL, V13, P428, DOI 10.1016/S0959-4388(03)00105-3; TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9; Tsotsos JK, 2005, COMPUT VIS IMAGE UND, V100, P3, DOI 10.1016/j.cviu.2004.10.011; TSOTSOS JK, 1987, INT J COMPUT VISION, V1, P303, DOI 10.1007/BF00133569; Turano KA, 2003, VISION RES, V43, P333, DOI 10.1016/S0042-6989(02)00498-4; WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774; WOLFE JM, 1996, BASIC CLIN APPL VISI; YEE C, 2002, MOTION DETECTION BOT; [No title captured]; 2006, ILAB NEUROMOPRHIC VI	49	28	31	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2007	73	2					159	177		10.1007/s11263-006-9784-6	http://dx.doi.org/10.1007/s11263-006-9784-6			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	146OI		Green Submitted			2022-12-18	WOS:000244943500003
J	Prados, E; Faugeras, O				Prados, E; Faugeras, O			A generic and provably convergent shape-from-shading method for orthographic and pinhole cameras	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shape from shading; Lambertian reflectance; pinhole camera; orthographic and perspective projection; black shadows; discontinuous images; viscosity solutions	SINGLE PERSPECTIVE IMAGE; VISCOSITY SOLUTIONS; PROJECTION; EQUATIONS	We describe a mathematical and algorithmic study of the Lambertian "Shape-From-Shading" problem for orthographic and pinhole cameras. Our approach is based upon the notion of viscosity solutions of Hamilton-Jacobi equations. This approach provides a mathematical framework in which we can show that the problem is well-posed (we prove the existence of a solution and we characterize all the solutions). Our contribution is threefold. First, we model the camera both as orthographic and as perspective (pinhole), whereas most authors assume an orthographic projection (see Horn and Brooks (1989) for a survey of the SFS problem up to 1989 and Zhang et al. (1999), Kozera (1998), Durou et al. (2004) for more recent ones); thus we extend the applicability of shape from shading methods to more realistic acquisition models. In particular it extends the work of Prados et al. (2002a) and Rouy and Tourin (1992). We provide some novel mathematical formulations of this problem yielding new partial differential equations. Results about the existence and uniqueness of their solutions are also obtained. Second, by introducing a "generic" Hamiltonian, we define a general framework allowing to deal with both models (orthographic and perspective), thereby simplifying the formalization of the problem. Thanks to this unification, each algorithm we propose can compute numerical solutions corresponding to all the modeling. Third, our work allows us to come up with two new generic algorithms for computing numerical approximations of the "continuous solution of the "Shape-From-Shading" problem as well as a proof of their convergence toward that solution. Moreover, our two generic algorithms are able to deal with discontinuous images as well as images containing black shadows.	INRIA, Odyssee Lab, F-06902 Sophia Antipolis, France	Inria	Prados, E (corresponding author), INRIA, Odyssee Lab, 2004 ROute Lucioles BP 93, F-06902 Sophia Antipolis, France.							[Anonymous], 1989, ANN SCUOLA NORM-SCI; Bardi M., 1997, OPTIMAL CONTROL VISC, V12; Barles G., 1991, Asymptotic Analysis, V4, P271; Barles G., 1994, MATH APPL; BROOKS MJ, 1992, Q APPL MATH, V50, P27, DOI 10.1090/qam/1146621; BRUCKSTEIN AM, 1988, COMPUT VISION GRAPH, V44, P139, DOI 10.1016/S0734-189X(88)80002-1; BRUSS AR, 1982, J MATH PHYS, V23, P890, DOI 10.1063/1.525441; Camilli F, 1999, INDIANA U MATH J, V48, P1111; Camilli F, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL I, P49, DOI 10.1109/ICIP.1996.559430; CAMILLI F, 2001, SERIES ADV MATH APPL, V59, P47; CMAILLI F, 2003, ADV DIFFERENTIAL EQU, V8, P733; COURTEILLE F, 2004, P RFIA 04; CRANDALL MG, 1992, B AM MATH SOC, V27, P1, DOI 10.1090/S0273-0979-1992-00266-5; CRANDALL MG, 1983, T AM MATH SOC, V277, P1, DOI 10.2307/1999343; CRANDALL MG, 1982, NONLINEAR PROBLEMS P, V61, P117; DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4; DUPUIS P, 1991, SPIE, V1570, P116; Dupuis P, 1994, ANN APPL PROBAB, V4, P287, DOI 10.1214/aoap/1177005063; Durou JD, 2000, J MATH IMAGING VIS, V12, P99, DOI 10.1023/A:1008361021281; Durou JD, 1996, INT J COMPUT VISION, V17, P273, DOI 10.1007/BF00128234; DUROU JD, 2004, 20042R IRIT; FALCONE M, 1997, LNCS, V1310, P596; FALCONE M, 2001, P ENUMATH 2001; FALCONE M, 2001, SERIES ADV MATH APPL, V59; Hasegawa JK, 1996, COMPUT GRAPH, V20, P351, DOI 10.1016/0097-8493(96)00004-0; Horn B.K.P., 1975, PSYCHOL COMPUTER VIS; Horn B.K.P., 1989, SHAPE SHADING; HORN BKP, 1993, IEEE T PATTERN ANAL, V15, P166, DOI 10.1109/34.192489; ISHII H, 1995, COMMUN PART DIFF EQ, V20, P2187, DOI 10.1080/03605309508821168; ISHII H, 1987, P AM MATH SOC, V100, P247, DOI 10.2307/2045953; ISHII H., 1985, B FAC SCI ENG CHUO U, V28, P33; Kain J, 2001, INT J COMPUT VISION, V44, P163, DOI 10.1023/A:1012235914303; KERAUTRET B, 2004, P INT C COMP VIS GRA; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P47, DOI 10.1006/cviu.1995.1040; Kimmel R, 2001, J MATH IMAGING VIS, V14, P237, DOI 10.1023/A:1011234012449; KIMMEL R, 1995, CVGIP-IMAG UNDERSTAN, P360; KLETTE R, 1998, CITRTR20 U AUCKL; Kozera R, 1997, J MATH IMAGING VIS, V7, P123, DOI 10.1023/A:1008249420974; Kozera R., 1998, MACHINE GRAPHICS VIS, V7, P291; LEE KM, 1994, CVGIP-IMAG UNDERSTAN, V59, P202, DOI 10.1006/ciun.1994.1013; LIONS PL, 1993, NUMER MATH, V64, P323, DOI 10.1007/BF01388692; LIONS PL, 1982, RES NOTES MATH, V69; Okatani T., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P830, DOI 10.1109/ICPR.1996.546140; Okatani T, 1997, COMPUT VIS IMAGE UND, V66, P119, DOI 10.1006/cviu.1997.0613; OLIENSIS J, 1991, CVGIP-IMAG UNDERSTAN, V54, P163, DOI 10.1016/1049-9660(91)90061-S; OLIENSIS J, 1993, P INT C COMP VIS, P692; Ostrov DN, 2000, NONLINEAR ANAL-THEOR, V42, P709, DOI 10.1016/S0362-546X(99)00164-9; PENNA MA, 1989, COMPUT VISION GRAPH, V46, P346, DOI 10.1016/0734-189X(89)90037-6; PENNA MA, 1989, IEEE T PATTERN ANAL, V11, P545, DOI 10.1109/34.24790; Prados E, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P826; Prados E, 2002, LECT NOTES COMPUT SC, V2351, P790; PRADOS E, 2001, THESIS U NICE SOPHIA; PRADOS E, 2002, 4638 INRIA; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; SAMARAS D, 1999, P INT C COMP VIS, V2, P868; Sethian J., 1996, LEVEL SET METHODS; Shiu Yin Yuen, 2002, Proceedings of Second IASTED International Conference Visualization, Imaging, and Image Processing, P584; Soravia P, 2000, IEEE DECIS CONTR P, P79, DOI 10.1109/CDC.2000.912736; Tankus A, 2005, INT J COMPUT VISION, V63, P21, DOI 10.1007/s11263-005-4945-6; Tankus A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P862; TANKUS A, 2004, IEEE C COMP VIS PATT; Weiss I., 1997, P DARPA IMAGE UNDERS, V2, P1393; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; [No title captured]	64	28	32	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2005	65	1-2					97	125		10.1007/s11263-005-3844-1	http://dx.doi.org/10.1007/s11263-005-3844-1			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	994NU		Green Submitted			2022-12-18	WOS:000234039300005
J	Ikeuchi, K; Sakauchi, M; Kawasaki, H; Sato, I				Ikeuchi, K; Sakauchi, M; Kawasaki, H; Sato, I			Constructing virtual cities by using panoramic images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						omni directional camera; VR; spatio-temporal analysis; ITS	REAL	Simultaneously acquired omni-directional images contain rays of 360 degree viewing directions. To take advantage of this unique characteristic, we have been developing several methods for constructing virtual cities. In this paper, we first describe a system to generate the appearance of a virtual city; the system, which is based on image-based rendering (IBR) techniques, utilizes the characteristics of omni-directional images to reduce the number of samplings required to construct such IBR images. We then describe a method to add geometric information to the IBR images; this method is based on the analysis of a sequence of omni-directional images. Then, we describe a method to seamlessly superimpose a new building model onto a previously created virtual city image; the method enables us to estimate illumination distributions by using an omni-directional camera. Finally, to demonstrate the methods' effectiveness, we describe how we implemented and applied them to urban scenes.	Univ Tokyo, Inst Ind Sci, Meguro Ku, Tokyo 1538505, Japan	University of Tokyo	Ikeuchi, K (corresponding author), Univ Tokyo, Inst Ind Sci, Meguro Ku, 6-4-1 Komaba, Tokyo 1538505, Japan.							ANTONE M, 2001, CVPR; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932; Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395; Fruh C, 2001, PROC CVPR IEEE, P31; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; HIROSE M, 1999, P INT S MIX REAL, P183; KAMIJO S, 1999, 6 WORLD C ITS TRONT; KAWASAKI H, 2001, 8 WORLD C INT TRANS; KAWASAKI H, 2000, AS C COMP VIS JAN, V2, P1149; KAWASAKI H, 2001, CVPR KAUAI HAW US, V2, P64; KURAZUME R, 2001, WORKSH DAT FUS IEEE; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; LI S, 2000, AS C COMP VIS, V1, P11; LIPPMAN A, 1990, P ACM SIGGRAPH 80, P32; NAEMURA T, 1997, VIRTUAL REALITY MULT, P59; NAYAR SK, 1997, P COMP VIS PATT REC; ONOUE Y, 1998, COMPUTER VISION IMAG, V71, P154; Sato I, 1999, IEEE T VIS COMPUT GR, V5, P1, DOI 10.1109/2945.764865; SATO Y, 1997, P SIGGRAPH 97, P379; Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573; STAMOS I, 2000, CVPR; Takahashi T, 2000, INT C PATT RECOG, P468, DOI 10.1109/ICPR.2000.902959; TAKAHASHI T, 2000, 7 WORLD C INT TRANSP; TAKAHASHI T, 2000, CVPR, V2, P296; Tamura H, 2001, IEEE COMPUT GRAPH, V21, P64, DOI 10.1109/38.963462; TELLER S, 2001, CVPR; Wood Daniel N, 2000, ACM SIGGRAPH; Yagi Y., 1990, Proceedings. IROS '90. IEEE International Workshop on Intelligent Robots and Systems '90. Towards a New Frontier of Applications (Cat. No.90TH0332-7), P181, DOI 10.1109/IROS.1990.262385; YAMAMOTO M, 1986, IEICE T INF SYST, V69, P1631; Yamazawa K., 1993, Proceedings of the 8th Scandinavian Conference on Image Analysis, P1381; ZHAO H, MVA, P232; ZHENG JY, 1990, INT C PATT REC, P161	33	28	32	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	2004	58	3					237	247		10.1023/B:VISI.0000019686.74089.5d	http://dx.doi.org/10.1023/B:VISI.0000019686.74089.5d			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	836GD					2022-12-18	WOS:000222542900005
J	Tonko, M; Nagel, HH				Tonko, M; Nagel, HH			Model-based stereo-tracking of non-polyhedral objects for automatic disassembly experiments	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						machine vision; visual servoing; image sequence evaluation; stereo-vision; model-based tracking; non-polyhedral objects; Kalman-filtering; disassembly	HAND-EYE COORDINATION; SYSTEM; VISION	Automatic disassembly tasks in the engine compartment of a used car constitute a challenge for control of a disassembly robot by machine vision. Experience in exploratory experiments under such conditions forced us to abandon data-driven aggregation of edge elements into straight-line data segments in favor of a direct association of individual edge elements with model segments obtained from scene domain models of tools and workpieces. In addition, we had to switch from a conventional single camera hand-eye configuration to a movable stereoconfiguration mounted on a separate 'observer' robot. A eneralisation of our model-based tracking includes the parameters, which characterize the relative pose of one camera with respect to the other one of the stereo-camera set-up, into the set of parameters to be re-estimated for each new stereo image pair. This results in a continuous re-calibration during a relative movement between stereo-camera set-up and tracked objects. Our approach had to be extended further in order to cope with non-polyhedral objects. The methodological improvements of machine vision in the course of this research are treated in detail. We discuss, moreover, the systematic trading-off of computational resources for increased robustness which is vital for visual control of automatic disassembly robots.	Univ Karlsruhe TH, Inst Algorithmen & Kognit Syst, D-76128 Karlsruhe, Germany; Fraunhofer Inst Informat & Datenverarbeitung IITB, D-76131 Karlsruhe, Germany	Helmholtz Association; Karlsruhe Institute of Technology; Fraunhofer Gesellschaft	Tonko, M (corresponding author), Univ Karlsruhe TH, Inst Algorithmen & Kognit Syst, Postfach 6980, D-76128 Karlsruhe, Germany.	nagel@ira.uka.de						ALLEN PK, 1993, IEEE T ROBOTIC AUTOM, V9, P152, DOI 10.1109/70.238279; Bar-Shalom Y., 1988, TRACKING DATA ASS; Bell GS, 1996, IEEE INT CONF ROBOT, P1650, DOI 10.1109/ROBOT.1996.506949; Bradshaw KJ, 1997, IEEE T PATTERN ANAL, V19, P219, DOI 10.1109/34.584099; Cipolla R, 1997, ROBOT AUTON SYST, V19, P337, DOI 10.1016/S0921-8890(96)00060-7; Gelb A., 1974, APPL OPTIMAL ESTIMAT; Gengenbach V., 1995, Proceedings of the Intelligent Vehicles '95. Symposium (Cat. No.95TH8132), P512, DOI 10.1109/IVS.1995.528334; Gengenbach V, 1996, IEEE INT CONF ROBOT, P1320, DOI 10.1109/ROBOT.1996.506889; GENGENBACH V, 1994, THESIS U KARLSRUHE; Grosso E, 1996, IEEE T ROBOTIC AUTOM, V12, P732, DOI 10.1109/70.538977; Hager GD, 1997, IEEE T ROBOTIC AUTOM, V13, P582, DOI 10.1109/70.611326; Hager GD, 1996, IEEE T ROBOTIC AUTOM, V12, P649; Hashimoto K., 1993, VISUAL SERVOING REAL; Heimes F, 1998, MATH COMPUT MODEL, V27, P189, DOI 10.1016/S0895-7177(98)00059-4; HOLLINGHURST N, 1994, IMAGE VISION COMPUT, V12, P187, DOI 10.1016/0262-8856(94)90071-X; HORAUD R, 1997, LECT NOTES CONTROL I, V223, P71; Hutchinson S, 1996, IEEE T ROBOTIC AUTOM, V12, P651, DOI 10.1109/70.538972; Kanade T., 1978, Proceedings of the 4th International Joint Conference on Pattern Recognition, P95; LEE JW, 1995, P IEEE RSJ INT C INT, P342; Nagel H.-H., 1996, Informatik Forschung und Entwicklung, V11, P3, DOI 10.1007/s004500050034; NAGEL HH, 1979, INFORMATIK FACHBERIC, V20, P3; NAGEL HH, 1987, FUNDAMENTALS COMPUTE, P113; NAGEL HH, 1989, INFORMATIONSTECHNIK, P25; NELSON BJ, 1995, IEEE INT CONF ROBOT, P184, DOI 10.1109/ROBOT.1995.525283; Paul R. P., 1981, ROBOT MANIPULATORS M; RUF A, 1997, P IEEE RSJ INT C INT, V2, P893; Tonko M, 1997, IEEE INT CONF ROBOT, P3166, DOI 10.1109/ROBOT.1997.606770; TONKO M, 1995, LECT NOTES CONTROL I, V223, P212; TONKO M, 1997, THESIS U KARLSRUHE; Wilson WJ, 1996, IEEE T ROBOTIC AUTOM, V12, P684, DOI 10.1109/70.538974; Xie M, 1997, MACH VISION APPL, V10, P136, DOI 10.1007/s001380050066; Yi JW, 1997, IMAGE VISION COMPUT, V15, P181, DOI 10.1016/S0262-8856(96)01118-3	32	28	30	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	37	1					99	118		10.1023/A:1008133614366	http://dx.doi.org/10.1023/A:1008133614366			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	341FC					2022-12-18	WOS:000088579500007
J	HARRIS, JG; KOCH, C; STAATS, E; LUO, J				HARRIS, JG; KOCH, C; STAATS, E; LUO, J			ANALOG HARDWARE FOR DETECTING DISCONTINUITIES IN EARLY VISION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									CALTECH,COMPUTAT & NEURAL SYST PROGRAM,PASADENA,CA 91125	California Institute of Technology				Koch, Christof/0000-0001-6482-8067				BLAKE A, 1989, IEEE T PATTERN ANAL, V11, P2, DOI 10.1109/34.23109; CHOU PB, 1988, FEB P IM UND WORKSH, P214; GAMBLE EB, 1987, MIT970 ART INT LAB M; GEIGER D, 1989, AI1114 MIT MEM; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GRIMSON WEL, 1981, IMAGES SURFACES; HARRIS JG, 1989, APR SNOWB NEUR NETW; HARRIS JG, 1988, AUG AAAI NEUR ARCH C; HARRIS JG, 1989, NEURAL INFORMATION P; HARRIS JG, 1989, ANALOG VLSI IMPLEMEN, P27; Hildreth E., 1984, MEASUREMENT VISUAL M; HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141; Horn B., 1986, ROBOT VISION, P1; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1988, MIT1071 ART INT LAB; HUANG K, 1963, STATISTICAL MECHANIC; HUTCHINSON J, 1988, IEEE COMPUT, V21, P52; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; KOCH C, 1986, P NATL ACAD SCI USA, V83, P4263, DOI 10.1073/pnas.83.12.4263; KOCH C, 1989, NOV NEUR INF PROC SY; KOCH C, 1990, IN PRESS NEURAL NETW; Koch C, 1989, NEURAL COMPUT, V1, P184, DOI 10.1162/neco.1989.1.2.184; LIU SC, 1989, COMPUT VISION PATTER; LUO J, 1988, NOV NEUR INF PROC SY; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; MARROQUIN J, 1987, J AM STAT ASSOC, V82, P76, DOI 10.2307/2289127; MARROQUIN JL, 1985, AI860 MIT LAB MEM; MAXWELL JC, 1954, TREATISE ELECTRICITY, V1, P407; Mead, 1989, ANALOG VLSI NEURAL S; Mead C., 1985, 1985 Chapel Hill Conference on Very Large Scale Integration, P463; MILLAR W, 1951, PHILOS MAG, V42, P1150; NAGEL HH, 1987, ARTIF INTELL, V33, P299, DOI 10.1016/0004-3702(87)90041-5; Ortega J. M., 1970, ITERATIVE SOLUTION N, V30; Perona P., 1988, 1988 IEEE International Symposium on Circuits and Systems. Proceedings (Cat. No.88CH2458-8), P2565, DOI 10.1109/ISCAS.1988.15465; POGGIO T, 1988, SCIENCE, V242, P436, DOI 10.1126/science.3175666; POGGIO T, 1985, PROC R SOC SER B-BIO, V226, P303, DOI 10.1098/rspb.1985.0097; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; POGGIO T, 1986, MIT833 ART INT LAB M; Sivilotti M. A., 1987, 1987 STANF C VER LAR, P295; TANNER J, 1986, THESIS CALTECH; TERZOPOULOS D, 1983, COMPUT VISION GRAPH, V24, P52, DOI 10.1016/0734-189X(83)90020-8; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P413, DOI 10.1109/TPAMI.1986.4767807; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; VERRI A, 1989, J OPT SOC AM A, V6, P469; Wyatt JL, 1989, NEURAL COMPUT, V1, P58, DOI 10.1162/neco.1989.1.1.58; YUILLE AL, 1988, NATURE, V333, P71, DOI 10.1038/333071a0; ZHOU YT, 1988, JUL P IEEE INT C NEU, V2, P71	49	28	29	0	3	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1990	4	3					211	223		10.1007/BF00054996	http://dx.doi.org/10.1007/BF00054996			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DJ379		Green Accepted			2022-12-18	WOS:A1990DJ37900003
J	Xie, HZ; Yao, HX; Zhang, SP; Zhou, SC; Sun, WX				Xie, Haozhe; Yao, Hongxun; Zhang, Shengping; Zhou, Shangchen; Sun, Wenxiu			Pix2Vox++: Multi-scale Context-aware 3D Object Reconstruction from Single and Multiple Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D object reconstruction; Multi-scale; Context-aware; Convolutional neural network		Recovering the 3D shape of an object from single or multiple images with deep neural networks has been attracting increasing attention in the past few years. Mainstream works (e.g. 3D-R2N2) use recurrent neural networks (RNNs) to sequentially fuse feature maps of input images. However, RNN-based approaches are unable to produce consistent reconstruction results when given the same input images with different orders. Moreover, RNNs may forget important features from early input images due to long-term memory loss. To address these issues, we propose a novel framework for single-view and multi-view 3D object reconstruction, named Pix2Vox++. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. A multi-scale context-aware fusion module is then introduced to adaptively select high-quality reconstructions for different parts from all coarse 3D volumes to obtain a fused 3D volume. To further correct the wrongly recovered parts in the fused 3D volume, a refiner is adopted to generate the final output. Experimental results on the ShapeNet, Pix3D, and Things3D benchmarks show that Pix2Vox++ performs favorably against state-of-the-art methods in terms of both accuracy and efficiency.	[Xie, Haozhe; Yao, Hongxun] Harbin Inst Technol, State Key Lab Robot & Syst, Harbin, Peoples R China; [Xie, Haozhe; Yao, Hongxun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Peoples R China; [Zhang, Shengping] Harbin Inst Technol, Sch Comp Sci & Technol, Weihai, Peoples R China; [Xie, Haozhe] SenseTime Res, Shenzhen, Peoples R China; [Sun, Wenxiu] SenseTime Res, Hong Kong, Peoples R China; [Zhou, Shangchen] Nanyang Technol Univ, Singapore, Singapore; [Zhang, Shengping] Peng Cheng Lab, Shenzhen, Peoples R China	Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Peng Cheng Laboratory	Yao, HX (corresponding author), Harbin Inst Technol, State Key Lab Robot & Syst, Harbin, Peoples R China.; Yao, HX (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Peoples R China.	hzxie@hit.edu.cn; h.yao@hit.edu.cn; s.zhang@hit.edu.cn; shangchenzhou@gmail.com; sunwenxiu@sensetime.com	Xie, Haozhe/B-7349-2017	Xie, Haozhe/0000-0001-9596-5179	National Natural Science Foundation of China [61772158, 61702136, 61872112]; National Key Research and Development Program of China [2018YFC0806802, 2018YFC0832105]; Self-Planned Task from the State Key Laboratory of Robotics and System (HIT) [SKLRS202002D]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China; Self-Planned Task from the State Key Laboratory of Robotics and System (HIT)	This work was supported in part by the National Natural Science Foundation of China (Nos. 61772158, 61702136 and 61872112), in part by National Key Research and Development Program of China (Nos. 2018YFC0806802 and 2018YFC0832105), and in part by Self-Planned Task (No. SKLRS202002D) from the State Key Laboratory of Robotics and System (HIT). We would like to thank anonymous reviewers for their valuable feedback during this research.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Dibra E, 2017, PROC CVPR IEEE, P5504, DOI 10.1109/CVPR.2017.584; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Han XF, 2021, IEEE T PATTERN ANAL, V43, P1578, DOI 10.1109/TPAMI.2019.2954885; Harltey A., 2006, MULTIPLE VIEW GEOMET, V2nd; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298; Hwang Kyuyeon, 2015, ICASSP; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Kar A., 2017, P NIPS, P365; Kato H, 2019, PROC CVPR IEEE, P9770, DOI 10.1109/CVPR.2019.01001; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071; Lin CH, 2018, AAAI CONF ARTIF INTE, P7114; Lin Chen-Hsuan, 2019, CVPR; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Mendoza-Silva GM, 2018, DATA, V3, DOI 10.3390/data3010003; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527; Mo Kaichun, 2019, CVPR; Ozyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Paschalidou D., 2018, CVPR; Paschalidou D., 2020, CVPR; Richter S. R., 2015, CVPR; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shin D., 2018, CVPR; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Sun Xingyuan, 2018, P IEEE C COMP VIS PA; Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Vinyals O., 2016, P ICLR; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113; WITKIN AP, 1981, ARTIF INTELL, V17, P17, DOI 10.1016/0004-3702(81)90019-9; Wu J., 2018, ARXIV171201026V4; Wu JJ, 2017, ADV NEUR IN, V30; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991; Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278; Xu Qiangeng, 2019, ARXIV190510711; Yang B, 2020, INT J COMPUT VISION, V128, P53, DOI 10.1007/s11263-019-01217-w; Yang B, 2019, IEEE T PATTERN ANAL, V41, P2820, DOI 10.1109/TPAMI.2018.2868195; Zhang Y, 2019, IEEE ACCESS, V7, P57539, DOI 10.1109/ACCESS.2019.2914150; Zhu CY, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275008	57	27	30	6	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2919	2935		10.1007/s11263-020-01347-6	http://dx.doi.org/10.1007/s11263-020-01347-6		JUL 2020	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ		Green Submitted			2022-12-18	WOS:000548483600001
J	Dai, DX; Sakaridis, C; Hecker, S; Van Gool, L				Dai, Dengxin; Sakaridis, Christos; Hecker, Simon; Van Gool, Luc			Curriculum Model Adaptation with Synthetic and Real Data for Semantic Foggy Scene Understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic foggy scene understanding; Fog simulation; Learning with synthetic and real data; Curriculum model adaptation; Network distillation; Adverse weather conditions	CONTRAST RESTORATION; VISION	This work addresses the problem of semantic scene understanding under fog. Although marked progress has been made in semantic scene understanding, it is mainly concentrated on clear-weather scenes. Extending semantic segmentation methods to adverse weather conditions such as fog is crucial for outdoor applications. In this paper, we propose a novel method, named Curriculum Model Adaptation (CMAda), which gradually adapts a semantic segmentation model from light synthetic fog to dense real fog in multiple steps, using both labeled synthetic foggy data and unlabeled real foggy data. The method is based on the fact that the results of semantic segmentation in moderately adverse conditions (light fog) can be bootstrapped to solve the same problem in highly adverse conditions (dense fog). CMAda is extensible to other adverse conditions and provides a new paradigm for learning with synthetic data and unlabeled real data. In addition, we present four other main stand-alone contributions: (1) a novel method to add synthetic fog to real, clear-weather scenes using semantic input; (2) a new fog density estimator; (3) a novel fog densification method for real foggy scenes without known depth; and (4) the Foggy Zurich dataset comprising 3808 real foggy images, with pixel-level semantic annotations for 40 images with dense fog. Our experiments show that (1) our fog simulation and fog density estimator outperform their state-of-the-art counterparts with respect to the task of semantic foggy scene understanding (SFSU); (2) CMAda improves the performance of state-of-the-art models for SFSU significantly, benefiting both from our synthetic and real foggy data. The foggy datasets and code are publicly available.	[Dai, Dengxin; Sakaridis, Christos; Hecker, Simon; Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland; [Van Gool, Luc] Katholieke Univ Leuven, Leuven, Belgium	Swiss Federal Institutes of Technology Domain; ETH Zurich; KU Leuven	Dai, DX (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	dai@vision.ee.ethz.ch						Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Alvarez J. M., 2012, EUR C COMP VIS; [Anonymous], 2005, FED MET HDB; Bar Hillel A, 2014, MACH VISION APPL, V25, P727, DOI 10.1007/s00138-011-0404-2; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185; Bhalla S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-52134-4; Bronte S, 2009, 2009 12TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC 2009), P30; Brostow Gabriel J., 2008, ECCV, P44, DOI [10.1007/978-3-540-88682-2_5, DOI 10.1007/978-3-540-88682-2_5]; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352; Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai D., 2013, INT C COMP VIS ICCV; Dai DX, 2015, PROC CVPR IEEE, P3527, DOI 10.1109/CVPR.2015.7298975; Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362; Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671; Gallen R., 2011, IEEE INT VEH S GOLD; Gallen R, 2015, IEEE T INTELL TRANSP, V16, P310, DOI 10.1109/TITS.2014.2331177; Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6; Geiger A., 2012, P IEEE COMP SOC C CO; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Hautiere N, 2006, MACH VISION APPL, V17, P8, DOI [10.1007/s00138-005-0011-1, 10.1007/s00138-006-0011-9]; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hecker S, 2018, LECT NOTES COMPUT SC, V11211, P449, DOI 10.1007/978-3-030-01234-2_27; Hinton G., 2015, ARXIV150302531; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoffman Judy, 2014, NIPS; Jensen MB, 2016, IEEE T INTELL TRANSP, V17, P1800, DOI 10.1109/TITS.2015.2509509; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Koschmieder H., 1924, BEITR PHYS ATMOS, V12, P33, DOI DOI 10.1007/978-3-663-04661-5_2; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Levinkov E, 2013, IEEE I CONF COMP VIS, P1321, DOI 10.1109/ICCV.2013.167; Li Y., 2016, ARXIV160706235 CORR; LI YY, 2018, IEEE ICC; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Ling Z, 2016, IEEE INFOCOM SER; Miclea R.C., 2015, INT C CONTR SYST COM; Misra Ishan, 2015, IEEE C COMP VIS PATT; Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821; Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723; Negru M, 2015, IEEE T INTELL TRANSP, V16, P2257, DOI 10.1109/TITS.2015.2405013; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1; Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8; Pavli M., 2013, IEEE INT VEH S GOLD; Pavli M., 2012, IEEE INT VEH S GOLD; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sakaridis C, 2018, LECT NOTES COMPUT SC, V11217, P707, DOI 10.1007/978-3-030-01261-8_42; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Shen XY, 2015, IEEE I CONF COMP VIS, P3406, DOI 10.1109/ICCV.2015.389; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Spinneker R., 2014, INT IEEE C INT TRANP; Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643; Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190; Tarel JP, 2012, IEEE INTEL TRANSP SY, V4, P6, DOI 10.1109/MITS.2012.2189969; Tarel JP, 2010, IEEE INT VEH SYM, P478, DOI 10.1109/IVS.2010.5548128; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Wang YK, 2014, IEEE T IMAGE PROCESS, V23, P4826, DOI 10.1109/TIP.2014.2358076; Wulfmeier M, 2018, IEEE INT CONF ROBOT, P4489; Xu Y, 2016, IEEE ACCESS, V4, P165, DOI 10.1109/ACCESS.2015.2511558; Yu F, 2016, INT NANOELECTR CONF; Zhang H, 2017, ARXIV COMPUTER VISIO; Zhang Yonghui, 2017, ICCV	75	27	27	6	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1182	1204		10.1007/s11263-019-01182-4	http://dx.doi.org/10.1007/s11263-019-01182-4			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Green Submitted, Green Accepted			2022-12-18	WOS:000531431500008
J	Jamaludin, A; Chung, JS; Zisserman, A				Jamaludin, Amir; Chung, Joon Son; Zisserman, Andrew			You Said That?: Synthesising Talking Faces from Audio	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision; Machine learning; Visual speech synthesis; Video synthesis		We describe a method for generating a video of a talking face. The method takes still images of the target face and an audio speech segment as inputs, and generates a video of the target face lip synched with the audio. The method runs in real time and is applicable to faces and audio not seen at training time. To achieve this we develop an encoder-decoder convolutional neural network (CNN) model that uses a joint embedding of the face and audio to generate synthesised talking face video frames. The model is trained on unlabelled videos using cross-modal self-supervision. We also propose methods to re-dub videos by visually blending the generated face into the source video frame using a multi-stream CNN model.	[Jamaludin, Amir; Chung, Joon Son; Zisserman, Andrew] Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England	University of Oxford	Chung, JS (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	joon@robots.ox.ac.uk	Chung, Joon Son/AAV-5933-2021		EPSRC Programme Grant [Seebibyte EP/M013774/1]; RCUK CDT in Healthcare Innovation [EP/G036861/1]; EPSRC [EP/M013774/1] Funding Source: UKRI	EPSRC Programme Grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); RCUK CDT in Healthcare Innovation; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Funding for this research is provided by the EPSRC Programme Grant Seebibyte EP/M013774/1. Amir Jamaludin is funded by the RCUK CDT in Healthcare Innovation EP/G036861/1. We would like to thank Aravindh Mahendran for helpful discussions and the reviewers for their suggestions.	Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052; Arandjelovi R., 2017, P INT C COMP VIS; Cappelletta L., 2012, ICPRAM; Charles J, 2016, LECT NOTES COMPUT SC, V9915, P879, DOI 10.1007/978-3-319-49409-8_71; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Chung J. S., 2017, P BRIT MACH VIS C; Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367; Chung Joon Son, 2016, P AS C COMP VIS, P251, DOI [DOI 10.1007/978-3-319-54427-4_19, 10.1007/978-3-319-54427-4{_}19, DOI 10.1007/978-3-319-54427-4{_}19]; Chung S.-W., 2019, IEEE INT C AC SPEECH; Doersch C., 2015, P IEEE C COMP VIS PA; Ezzat T, 2000, INT J COMPUT VISION, V38, P45, DOI 10.1023/A:1008166717597; Fan B., 2015, IEEE INT C AC SPEECH; Fernando B, 2017, IEEE COMPUT SOC CONF, P1604, DOI 10.1109/CVPRW.2017.205; Garrido P, 2015, COMPUT GRAPH FORUM, V34, P193, DOI 10.1111/cgf.12552; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Isola P., 2016, WORKSH INT C LEARN R; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Kim J, 2016, IEEE CONF COMPUT; King DE, 2009, J MACH LEARN RES, V10, P1755; Lee WS, 2018, JCO CLIN CANCER INFO, V2, DOI 10.1200/CCI.17.00109; Lienhart R., INT J IMAGE GRAPH, V1, P469, DOI DOI 10.1142/S021946780100027X; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Misra I., 2016, P EUR C COMP VIS; Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879; Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264; Parkhi O. M, 2015, THESIS; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269; Ptrucean V., 2016, ADV NEURAL INFORM PR; Reed S, 2016, PR MACH LEARN RES, V48; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640; Taylor S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073699; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang X., 2015, P INT C COMP VIS; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40	50	27	28	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1767	1779		10.1007/s11263-019-01150-y	http://dx.doi.org/10.1007/s11263-019-01150-y			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY		Green Published, hybrid			2022-12-18	WOS:000492425300012
J	Bozic-Stulic, D; Marusic, Z; Gotovac, S				Bozic-Stulic, Dunja; Marusic, Zeljko; Gotovac, Sven			Deep Learning Approach in Aerial Imagery for Supporting Land Search and Rescue Missions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Convolutional neural networks; RCNN; Salient object detection; Unmanned aerial vehicles (UAV); Search and rescue; SAR image database		In this paper, we propose a novel approach to person detection in UAV aerial images for search and rescue tasks in Mediterranean and Sub-Mediterranean landscapes. Person detection in very high spatial resolution images involves target objects that are relatively small and often camouflaged within the environment; thus, such detection is a challenging and demanding task. The proposed method starts by reducing the search space through a visual attention algorithm that detects the salient or most prominent segments in the image. To reduce the number of non-relevant salient regions, we selected those regions most likely to contain a person using pre-trained and fine-tuned convolutional neural networks (CNNs) for detection. We established a special database called HERIDAL to train and test our model. This database was compiled for training purposes, and it contains over 68,750 image patches of wilderness acquired from an aerial perspective as well as approximately 500 labelled full-size real-world images intended for testing purposes. The proposed method achieved a detection rate of 88.9% and a precision of 34.8%, which demonstrates better effectiveness than the system currently used by Croatian Mountain search and rescue (SAR) teams (IPSAR), which is based on mean-shift segmentation. We also used the HERIDAL database to train and test a state-of-the-art region proposal network, Faster R-CNN (Ren et al. in Faster R-CNN: towards real-time object detection with region proposal networks, 2015. CoRR arXiv:1506.01497), which achieved comparable but slightly worse results than those of our proposed method.	[Bozic-Stulic, Dunja; Gotovac, Sven] Univ Split, Fac Elect Engn Mech Engn & Naval Architecture, Rudera Boskovica 32, Split, Croatia; [Marusic, Zeljko] Univ Mostar, Fac Sci & Educ, Trg Hrvatskih Velikana 1, Mostar, Bosnia & Herceg	University of Split; University of Mostar	Bozic-Stulic, D (corresponding author), Univ Split, Fac Elect Engn Mech Engn & Naval Architecture, Rudera Boskovica 32, Split, Croatia.	dgotovac@fesb.hr; zeljko.marusic@fpmoz.sum.ba; gotovac@fesb.hr	Božić-Štulić, Dunja/E-2743-2017		Federal Ministry of Education and Science, Bosnia and Herzegovina [NG 05-39-2945-3/16]	Federal Ministry of Education and Science, Bosnia and Herzegovina	This research was carried out in part within the framework of a IPSAR project, University of Split, Croatia. It is also partly supported by Federal Ministry of Education and Science, Bosnia and Herzegovina by Grant (NG 05-39-2945-3/16) to Faculty of Science and Education, University of Mostar. We thank NVIDIA Corporation for GPUs donation through Nvidia GPU Edcuation Center program at University of Mostar.	Angelova A., 2015, P BMVC 2015; Borji A., 2014, ARXIV14115878; Chen CY, 2017, LECT NOTES COMPUT SC, V10115, P214, DOI 10.1007/978-3-319-54193-8_14; Daubechies I., 1992, 10 LECT WAVELETS, DOI [10.1137/1.9781611970104.ch1, DOI 10.1137/1.9781611970104.CH1]; Eggert C, 2017, IEEE INT CON MULTI, P421, DOI 10.1109/ICME.2017.8019550; Enzweiler M, 2009, IEEE T PATTERN ANAL, V31, P2179, DOI 10.1109/TPAMI.2008.260; GASZCZAK A, 2011, REAL TIME PEOPLE VEH, V7878; Girshick R., 2013, ARXIV; Girshick R.B, 2015, ARXIV150408083 CORR; Gotovac S, 2016, INT CONF SOFTW, P378; He K., 2015, CORR; Hosang J., 2015, IEEE C COMP VIS PATT; Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034; Koch C., 1987, MATTERINTELLIGENCE, P115, DOI [10.1007/978-94-009-3833-5, DOI 10.1007/978-94-009-3833-5, DOI 10.1007/978-94-009-3833-5_5]; Koester R. J., 2008, LOST PERSON BEHAV SE; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Leroy J., 2014, SUPERRARE OBJECT ORI; Li J., 2016, CORR; Music J, 2016, MATH PROBL ENG, V2016, DOI 10.1155/2016/6827414; Ren S., 2015, CORR; Rudol P., 2008, 2008 IEEE AEROSPACE, P1, DOI [10.1109/AERO.2008.4526559, DOI 10.1109/AERO.2008.4526559]; Russakovsky O., 2014, CORR; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sokalski J., 2010, PROC 25 INT UNMANNED, P1; Syrotuck W.G., 2000, ANAL LOST PERSON BEH; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tian YL, 2015, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2015.221; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Turic H, 2010, INF TECHNOL CONTROL, V39, P138; Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734; Yuan P., 2017, FASTER R CNN REGION; Zendel O, 2017, INT J COMPUT VISION, V125, P95, DOI 10.1007/s11263-017-1020-z; Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28	34	27	27	6	71	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2019	127	9					1256	1278		10.1007/s11263-019-01177-1	http://dx.doi.org/10.1007/s11263-019-01177-1			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IL9YV					2022-12-18	WOS:000477642300005
J	Zhu, XY; Liu, H; Lei, Z; Shi, HL; Yang, F; Yi, D; Qi, GJ; Li, SZ				Zhu, Xiangyu; Liu, Hao; Lei, Zhen; Shi, Hailin; Yang, Fan; Yi, Dong; Qi, Guojun; Li, Stan Z.			Large-Scale Bisample Learning on ID Versus Spot Face Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face recognition; ID versus spot; Large-scale bisample learning; Dominant prototype softmax		In real-world face recognition applications, there is a tremendous amount of data with two images for each person. One is an ID photo for face enrollment, and the other is a probe photo captured on spot. Most existing methods are designed for training data with limited breadth (a relatively small number of classes) and sufficient depth (many samples for each class). They would meet great challenges on ID versus Spot (IvS) data, including the under-represented intra-class variations and an excessive demand on computing devices. In this paper, we propose a deep learning based large-scale bisample learning (LBL) method for IvS face recognition. To tackle the bisample problem with only two samples for each class, a classification-verification-classification training strategy is proposed to progressively enhance the IvS performance. Besides, a dominant prototype softmax is incorporated to make the deep learning scalable on large-scale classes. We conduct LBL on a IvS face dataset with more than two million identities. Experimental results show the proposed method achieves superior performance to previous ones, validating the effectiveness of LBL on IvS face recognition.	[Zhu, Xiangyu; Liu, Hao; Lei, Zhen; Shi, Hailin; Li, Stan Z.] Chinese Acad Sci, Ctr Biometr & Secur Res, Beijing, Peoples R China; [Zhu, Xiangyu; Liu, Hao; Lei, Zhen; Shi, Hailin; Li, Stan Z.] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China; [Zhu, Xiangyu; Liu, Hao; Lei, Zhen; Li, Stan Z.] Univ Chinese Acad Sci, Beijing, Peoples R China; [Yang, Fan] Beihang Univ, Coll Software, Beijing, Peoples R China; [Yi, Dong] DAMO Acad, Alibaba Grp, Hangzhou, Zhejiang, Peoples R China; [Qi, Guojun] HUAWEI Cloud, Boston, MA USA	Chinese Academy of Sciences; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Beihang University; Alibaba Group; Huawei Technologies	Lei, Z (corresponding author), Chinese Acad Sci, Ctr Biometr & Secur Res, Beijing, Peoples R China.; Lei, Z (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.; Lei, Z (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.	xiangyu.zhu@nlpr.ia.ac.cn; hao.liu2016@nlpr.ia.ac.cn; zlei@nlpr.ia.ac.cn; hailin.shi@nlpr.ia.ac.cn; fanyang@buaa.edu.cn; yidong.yd@alibaba-inc.com; guojunq@gmail.com; szli@nlpr.ia.ac.cn	Qi, Guo-Jun/AAH-8294-2019	Qi, Guo-Jun/0000-0003-3508-1851	Chinese National Natural Science Foundation [61876178, 61806196]; National Key Research and Development Plan [2016YFC080-1002]; AuthenMetric RD Funds	Chinese National Natural Science Foundation(National Natural Science Foundation of China (NSFC)); National Key Research and Development Plan; AuthenMetric RD Funds	This work was supported by the Chinese National Natural Science Foundation Projects #61876178, #61806196, the National Key Research and Development Plan (Grant No. 2016YFC080-1002), and AuthenMetric R&D Funds. Zhen Lei is the corresponding author.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Babbar R, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P721, DOI 10.1145/3018661.3018741; Balntas V., 2016, BRIT MACH VIS C; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Bertinetto Luca, 2016, NIPS; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Cao Q., 2017, VGGFACE2 DATASET REC; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Choe J, 2017, IEEE INT CONF COMP V, P1940, DOI 10.1109/ICCVW.2017.229; Choromanska A., 2013, NIPS WORKSH EX UNPUB; Feng Z.-H., 2017, ARXIV171106753; Guo Y., 2017, ONE SHOT FACE RECOGN; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Hariharan B, 2016, ARXIV160602819; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hsu D., 2009, P 22 INT C NEURAL IN, V22, P772; Huang G.B., 2008, WORKSH FAC REAL LIF; Huang M, 2016, 2016 3RD INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P1262, DOI 10.1109/ICISCE.2016.270; Koch G., 2015, ICML DEEP LEARNING W; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar V. B, 2017, ARXIV170401285; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Liu Weiyang, 2017, NIPS; Mnih A., 2013, ADV NEURAL INFORM PR, V26, P2265; Mnih Andriy, 2012, ARXIV12066426, P419; Nech A., 2017, ARXIV170500393; Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100; Parkhi Omkar M., 2015, BRIT MACH VIS C; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Ranjan R., 2017, ARXIV PREPRINT ARXIV; Santoro A, 2016, PR MACH LEARN RES, V48; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Smirnov E, 2017, IEEE INT CONF COMP V, P1916, DOI 10.1109/ICCVW.2017.226; Sohn Kihyuk, 2016, NEURIPS, DOI DOI 10.5555/3157096.3157304; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tagami Y, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P455, DOI 10.1145/3097983.3097987; Taigman Y., 2014, CORR; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Vaswani A., 2013, EMNLP, P1387; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang C, 2017, IEEE INT CONF COMP V, P1907, DOI 10.1109/ICCVW.2017.225; Wang Feng, 2017, ARXIV170406369; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang YX, 2016, LEARNING LEARN MODEL; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Weston J., 2014, ARXIV14103916; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; Xu C, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1275, DOI 10.1145/2939672.2939798; Xu ZW, 2017, PROC CVPR IEEE, P5358, DOI 10.1109/CVPR.2017.569; Yang JM, 2014, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2014.415; Yi D., 2014, COMPUTER VISION AND; Yin Q., 2015, NAIVE DEEP FACE RECO; Zhang SF, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P1, DOI 10.1109/BTAS.2017.8272675; Zhao Y., 2018, EUR C COMP VIS	63	27	30	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		684	700		10.1007/s11263-019-01162-8	http://dx.doi.org/10.1007/s11263-019-01162-8			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD		Green Submitted			2022-12-18	WOS:000468525900009
J	Steger, C				Steger, Carsten			A Comprehensive and Versatile Camera Model for Cameras with Tilt Lenses	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Camera models; Tilt lenses; Scheimpflug optics; Camera model degeneracies; Camera calibration; Bias removal; Stereo rectification	PARTICLE IMAGE VELOCIMETRY; COMPUTER VISION SYSTEM; CCD CAMERA; CALIBRATION; PARAMETERIZATION; ACCURACY	We propose camera models for cameras that are equipped with lenses that can be tilted in an arbitrary direction (often called Scheimpflug optics). The proposed models are comprehensive: they can handle all tilt lens types that are in common use for machine vision and consumer cameras and correctly describe the imaging geometry of lenses for which the ray angles in object and image space differ, which is true for many lenses. Furthermore, they are versatile since they can also be used to describe the rectification geometry of a stereo image pair in which one camera is perspective and the other camera is telecentric. We also examine the degeneracies of the models and propose methods to handle the degeneracies. Furthermore, we examine the relation of the proposed camera models to different classes of projective camera matrices and show that all classes of projective cameras can be interpreted as cameras with tilt lenses in a natural manner. In addition, we propose an algorithm that can calibrate an arbitrary combination of perspective and telecentric cameras (no matter whether they are tilted or untilted). The calibration algorithm uses a planar calibration object with circular control points. It is well known that circular control points may lead to biased calibration results. We propose two efficient algorithms to remove the bias and thus obtain accurate calibration results. Finally, we perform an extensive evaluation of the proposed camera models and calibration algorithms that establishes the validity and accuracy of the proposed models.	[Steger, Carsten] MVTec Software GmbH, Arnulfstr 205, D-80634 Munich, Germany		Steger, C (corresponding author), MVTec Software GmbH, Arnulfstr 205, D-80634 Munich, Germany.	steger@mvtec.com		Steger, Carsten/0000-0003-3426-1703				Aggarwal M, 2002, INT J COMPUT VISION, V48, P195, DOI 10.1023/A:1016324132583; Ahn SJ, 1999, PHOTOGRAMM REC, V16, P485, DOI 10.1111/0031-868X.00138; Albers O, 2015, OPT EXPRESS, V23, P29592, DOI 10.1364/OE.23.029592; [Anonymous], 2003, 1335200312 DIN; Astarita T, 2012, 16 INT S APPL LAS TE, P9; Bauchau OA, 2003, NONLINEAR DYNAM, V32, P71, DOI 10.1023/A:1024265401576; Beyerer J., 2016, MACHINE VISION AUTOM; Blahusch G., 1999, QCAV99. 5th International Conference on Quality Control by Artificial Vision. Proceedings, P31; BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855; Brown Duane C, 1966, PHOTOGRAMMETRIC ENG, P2, DOI DOI 10.1234/12345678; Claus D, 2005, PROC CVPR IEEE, P213; Cornic P., 2015, 11 INT S PART IM VEL; Datta Ankur, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1201, DOI 10.1109/ICCVW.2009.5457474; Dold J., 1997, THESIS U BUND MUNCHE; Dold J., 1996, INT ARCH PHOTOGRAMME, V31, P119; Douxchamps D, 2009, IEEE T PATTERN ANAL, V31, P376, DOI 10.1109/TPAMI.2008.214; Evens L., 2008, DEPTH FIELD TILTED L; Evens L., 2008, VIEW CAMERA GEOMETRY; Fasogbon P., 2015, P SPIE, V9534; Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658; Fitzgibbon AW, 2001, PROC CVPR IEEE, P125; Fournel T, 2004, PARTICLE IMAGE VELOCIMETRY: RECENT IMPROVEMENTS, P391; Fournel T, 2003, MEAS SCI TECHNOL, V14, P494, DOI 10.1088/0957-0233/14/4/313; Fournel T, 2006, 12 INT S FLOW VIS; Gennery D. B., 1987, Proceedings of the 1987 IEEE International Conference on Robotics and Automation (Cat. No.87CH2413-3), P311; Gennery DB, 2001, SPRINGER SERIES INFO, V34, P123; Gennery DB, 2006, INT J COMPUT VISION, V68, P239, DOI 10.1007/s11263-006-5168-1; GERDES R, 1993, TECH MESS, V60, P255; GERDES R, 1993, TECH MESS, V60, P283; Gluckman J, 2001, PROC CVPR IEEE, P111; HAIG C, 2006, INT ARCH PHOTOGRAM 5, V36; Hamrouni S, 2012, 15 INT S FLOW VIS; Hanning T., 2011, HIGH PRECISION CAMER; Hartley R., 2003, MULTIPLE VIEW GEOMET; Heikkila J, 2000, IEEE T PATTERN ANAL, V22, P1066, DOI 10.1109/34.879788; Heikkila J, 1997, THESIS; Konrath R, 2002, EXP FLUIDS, V33, P703, DOI 10.1007/s00348-002-0531-7; Konrath R., 2000, 10 INT S APPL LAS TE; Kumar A, 2015, IEEE I CONF COMP VIS, P2345, DOI 10.1109/ICCV.2015.270; Kumar A, 2014, INT C PATT RECOG, P184, DOI 10.1109/ICPR.2014.41; Kumar A, 2014, PROC CVPR IEEE, P3970, DOI 10.1109/CVPR.2014.507; Lanser S, 1997, THESIS; Lanser S, 1995, MUSTERERKENNUNG, P481, DOI 10.1007/978-3-642-79980-8\_57; Legarda A., 2013, 11 INT WORKSH EL CON; Legarda A., 2011, 10 INT WORKSH EL CON; Lenhardt K., 2006, HDB MACHINE VISION; LENZ R, 1990, ISPRS J PHOTOGRAMM, V45, P90, DOI 10.1016/0924-2716(90)90095-S; Lenz R., 1987, INFORM FACHBERICHTE, V149, P212; Lenz R., 1988, VIEDEOMETRIE CCD SEN; Li JF, 2007, OPT LASER ENG, V45, P1077, DOI 10.1016/j.optlaseng.2007.05.006; Louhichi H, 2007, MEAS SCI TECHNOL, V18, P2616, DOI 10.1088/0957-0233/18/8/037; Louhichi H, 2006, LECT NOTES COMPUT SC, V4292, P891; Luhmann T., 2014, CLOSE RANGE PHOTOGRA, V2nd; Mallon J, 2007, PATTERN RECOGN LETT, V28, P921, DOI 10.1016/j.patrec.2006.12.008; Merklinger H.M., 2010, FOCUSING VIEW CAMERA; Mikhail E.M., 2001, INTRO MODERN PHOTOGR, V19; Vo M, 2011, OPT ENG, V50, DOI 10.1117/1.3647521; Morawiec A, 1996, PHILOS MAG A, V73, P1113, DOI 10.1080/01418619608243708; Nocedal J, 2006, SPRINGER SER OPER RE, P135; Otepka J, 2004, THESIS; Otepka J. O., 2004, INT ARCH PHOTOGRAMME, V35, P873; Peng JZ, 2015, APPL OPTICS, V54, P10055, DOI 10.1364/AO.54.010055; Prasad AK, 2000, EXP FLUIDS, V29, P103, DOI 10.1007/s003480000143; Scheimpflug T., 1902, Austrian Patent, Patent No. 20299; Scheimpflug T., 1902, Austrian Patent, Patent No. 22923; Scheimpflug T., 1903, Austrian Patent, Patent No. 22924; Scheimpflug T., 1903, United States Patent, Patent No. 752596; Scheimpflug T., 1903, United States Patent, Patent No. 751347; Scheimpflug T., 1902, Austrian Patent, Patent No. 20937; Singer W, 2005, HANDBOOK OF OPTICAL SYSTEMS, VOL 2: PHYSICAL IMAGE FORMATION, P1, DOI 10.1002/3527606688; Spong MW, 2006, ROBOT MODELING CONTR; Steger C., 2000, INT ARCH PHOTOGRA B3, V33, P141; Steger C., 1998, UNBIASED EXTRACTION; Steger C., 2008, MACHINE VISION ALGOR; Sturm P., 1999, P IEEE C COMP VIS PA, P432, DOI DOI 10.1109/CVPR.1999.786974; Sturm P, 2010, FOUND TRENDS COMPUT, V6, P1, DOI 10.1561/0600000023; Walker S, 2002, MEAS SCI TECHNOL, V13, P1, DOI 10.1088/0957-0233/13/1/301; Wang J, 2008, PATTERN RECOGN, V41, P607, DOI 10.1016/j.patcog.2007.06.012; Wheeler R., 2003, NOTES VIEW CAMERA GE; Willert C, 1997, MEAS SCI TECHNOL, V8, P1465, DOI 10.1088/0957-0233/8/12/010	81	27	29	1	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2017	123	2					121	159		10.1007/s11263-016-0964-8	http://dx.doi.org/10.1007/s11263-016-0964-8			39	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EU2OM		hybrid			2022-12-18	WOS:000400868800001
J	Xu, CL; Corso, JJ				Xu, Chenliang; Corso, Jason J.			LIBSVX: A Supervoxel Library and Benchmark for Early Video Processing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Supervoxels; Segmentation and grouping; Video segmentation; Spatiotemporal processing	SEGMENTATION	Supervoxel segmentation has strong potential to be incorporated into early video analysis as superpixel segmentation has in image analysis. However, there are many plausible supervoxel methods and little understanding as to when and where each is most appropriate. Indeed, we are not aware of a single comparative study on supervoxel segmentation. To that end, we study seven supervoxel algorithms, including both off-line and streaming methods, in the context of what we consider to be a good supervoxel: namely, spatiotemporal uniformity, object/region boundary detection, region compression and parsimony. For the evaluation we propose a comprehensive suite of seven quality metrics to measure these desirable supervoxel characteristics. In addition, we evaluate the methods in a supervoxel classification task as a proxy for subsequent high-level uses of the supervoxels in video analysis. We use six existing benchmark video datasets with a variety of content-types and dense human annotations. Our findings have led us to conclusive evidence that the hierarchical graph-based (GBH), segmentation by weighted aggregation (SWA) and temporal superpixels (TSP) methods are the top-performers among the seven methods. They all perform well in terms of segmentation accuracy, but vary in regard to the other desiderata: GBH captures object boundaries best; SWA has the best potential for region compression; and TSP achieves the best undersegmentation error.	[Xu, Chenliang; Corso, Jason J.] Univ Michigan, Elect Engn & Comp Sci, 1301 Beal Ave, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Xu, CL (corresponding author), Univ Michigan, Elect Engn & Comp Sci, 1301 Beal Ave, Ann Arbor, MI 48109 USA.	cliangxu@umich.edu; jjcorso@eecs.umich.edu		Corso, Jason/0000-0001-6454-9594	National Science Foundation CAREER grant [IIS-0845282]; Army Research Office [W911NF-11-1-0090]; DARPA Mind's Eye program [W911NF-10-2-0062]	National Science Foundation CAREER grant(National Science Foundation (NSF)); Army Research Office; DARPA Mind's Eye program	This work was partially supported by the National Science Foundation CAREER grant (IIS-0845282), the Army Research Office (W911NF-11-1-0090) and the DARPA Mind's Eye program (W911NF-10-2-0062). We are grateful to the authors of the code and datasets that we have relied upon in this study, and we are grateful to the reviewers' comments, which have greatly improved this paper.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Baker Simon, 2007, 2007 11th IEEE International Conference on Computer Vision, P1; Brendel W, 2009, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2009.5459242; Brostow Gabriel J., 2008, ECCV, P44, DOI [10.1007/978-3-540-88682-2_5, DOI 10.1007/978-3-540-88682-2_5]; Budvytis I, 2011, PROC CVPR IEEE; Chang J, 2013, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2013.267; Chen A. Y. C., 2010, Proceedings of 2010 Western New York Image Processing Workshop (WNYIPW), P14, DOI 10.1109/WNYIPW.2010.5649773; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Corso JJ, 2008, IEEE T MED IMAGING, V27, P629, DOI 10.1109/TMI.2007.912817; de Souza K. J. F., 2014, PATTERN RECOGNITION; DEMENTHON D, 2002, STAT METH VID PROC W; Drucker F., 2009, IEEE WORKSH MOT VID; Erdem CE, 2004, IEEE T IMAGE PROCESS, V13, P937, DOI 10.1109/TIP.2004.828427; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; Fowlkes C., 2001, IEEE C COMP VIS PATT; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Galasso F., 2012, P AS C COMP VIS; Galasso F, 2013, IEEE I CONF COMP VIS, P3527, DOI 10.1109/ICCV.2013.438; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; Greenspan H, 2004, IEEE T PATTERN ANAL, V26, P384, DOI 10.1109/TPAMI.2004.1262334; Grundmann M, 2010, PROC CVPR IEEE, P569, DOI 10.1109/CVPR.2010.5540165; Hanbury A, 2008, LECT NOTES COMPUT SC, V5197, P178, DOI 10.1007/978-3-540-85920-8_22; He X., 2006, EUR C COMP VIS; He Y, 2013, IEEE INT CON MULTI; Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232; Khan S., 2001, IEEE C COMP VIS PATT; Klaser Alexander, 2008, BMVC; Kum SW, 2011, IEEE IC COMP COM NET; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Lee J, 2014, JMLR WORKSH CONF PRO, V33, P558; Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96; Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273; Liu C, 2008, PROC CVPR IEEE, P3911; Liu SY, 2008, PROC CVPR IEEE, P657, DOI 10.1109/ISPA.2008.47; Megret R., 2002, TECHNICAL REPORT; Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323; Moore AP, 2008, PROC CVPR IEEE, P998; Mori G., 2004, IEEE C COMP VIS PATT; Palou G., 2013, IEEE C COMP VIS PATT; Paris S., 2008, EUR C COMP VIS; Paris S, 2007, PROC CVPR IEEE, P1978; Patel NV, 1997, PATTERN RECOGN, V30, P583, DOI 10.1016/S0031-3203(96)00114-8; Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10; Reso M, 2013, IEEE I CONF COMP VIS, P385, DOI 10.1109/ICCV.2013.55; Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215; Sharon E., 2000, IEEE C COMP VIS PATT; Sharon E, 2006, NATURE, V442, P810, DOI 10.1038/nature04977; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Sundberg P., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2233, DOI 10.1109/CVPR.2011.5995364; Tighe J., 2010, INT J COMPUTER VISIO; Tripathi S., 2014, IEEE WINT C APPL COM; Tsai David, 2010, BMVC, DOI [10.5244/C.24.56, DOI 10.5244/C.24.56]; Van den Bergh M, 2013, IEEE I CONF COMP VIS, P377, DOI 10.1109/ICCV.2013.54; Vazquez-Reina A., 2010, EUR C COMP VIS; Veksler O., 2010, EUR C COMP VIS, DOI DOI 10.1007/978-3-642-15555-0_16; VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; WANG J, 2004, EUR C COMP VIS; Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45; Xu CL, 2012, PROC CVPR IEEE, P1202, DOI 10.1109/CVPR.2012.6247802; Zeng G., 2011, IEEE INT C COMP VIS	64	27	30	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	3			SI		272	290		10.1007/s11263-016-0906-5	http://dx.doi.org/10.1007/s11263-016-0906-5			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FE		Green Submitted			2022-12-18	WOS:000380270000005
J	Weickert, J; Grewenig, S; Schroers, C; Bruhn, A				Weickert, Joachim; Grewenig, Sven; Schroers, Christopher; Bruhn, Andres			Cyclic Schemes for PDE-Based Image Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Explicit scheme; Jacobi method; Partial differential equations (PDEs); PDE-based image analysis; Fast explicit diffusion; Super time stepping; First order optimisation methods	RICHARDSON ITERATION; DIFFUSION; EFFICIENT	We investigate a class of efficient numerical algorithms for many partial differential equations (PDEs) in image analysis. They are applicable to parabolic or elliptic PDEs that have bounded coefficients and lead to space discretisations with symmetric matrices. Our schemes are easy to implement and well-suited for parallel implementations on GPUs, since they are based on the explicit diffusion scheme in the parabolic case, and the Jacobi method in the elliptic case. By supplementing these methods with cyclically varying time step sizes or relaxation parameters, we achieve efficiency gains of several orders of magnitude. We call the resulting algorithms Fast Explicit Diffusion (FED) and Fast Jacobi (FJ) methods. To achieve a good compromise between efficiency and accuracy, we show that one should use parameter cycles that result from factorisations of box filters. For these cycles we establish stability results in the Euclidean norm. Our schemes perform favourably in a number of applications, including isotropic nonlinear diffusion filters with widely varying diffusivities as well as anisotropic diffusion methods for image filtering, inpainting, and regularisation in computer vision. Moreover, they are equally suited for higher dimensional problems as well as higher order PDEs, and they can also be interpreted as efficient first order methods for smooth optimisation problems.	[Weickert, Joachim; Grewenig, Sven; Schroers, Christopher] Univ Saarland, Dept Math & Comp Sci, Math Image Anal Grp, Campus E1-7, D-66041 Saarbrucken, Germany; [Bruhn, Andres] Univ Stuttgart, Inst Visualizat & Interact Syst, Comp Vis & Intelligent Syst Grp, Univ Str 38, D-70569 Stuttgart, Germany	Saarland University; University of Stuttgart	Weickert, J (corresponding author), Univ Saarland, Dept Math & Comp Sci, Math Image Anal Grp, Campus E1-7, D-66041 Saarbrucken, Germany.	weickert@mia.uni-saarland.de			German Research Foundation (DFG) [We 2602/7-1, We 2602/9-1]	German Research Foundation (DFG)(German Research Foundation (DFG))	We gratefully acknowledge funding by the German Research Foundation (DFG) through the Project We 2602/7-1 as well as the Gottfried Wilhelm Leibniz Prize We 2602/9-1.	Abramowitz M., 1972, HDB MATH FUNCTIONS F, P771; Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13; Alexiades V, 1996, COMMUN NUMER METH EN, V12, P31, DOI 10.1002/(SICI)1099-0887(199601)12:1<31::AID-CNM950>3.0.CO;2-5; ALEXIADES V, 1995, P DYN SYST APPL ATL, V2, P39; Anderssen R.S., 1972, STANCS72304 STANF U; Bansch E., 1997, Computing and Visualization in Science, V1, P53, DOI 10.1007/s007910050005; Ben-Ari Rami, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P522, DOI 10.1109/ICCVW.2011.6130287; Bornemann FA, 1996, NUMER MATH, V75, P135, DOI 10.1007/s002110050234; Boyd S, 2004, CONVEX OPTIMIZATION; Brakhage H., 1960, NUMER MATH, V2, P183; Bruhn A, 2006, INT J COMPUT VISION, V70, P257, DOI 10.1007/s11263-006-6616-7; Calvetti D, 2003, NUMER ALGORITHMS, V33, P153, DOI 10.1023/A:1025555803588; Calvetti D, 1996, J COMPUT APPL MATH, V71, P267, DOI 10.1016/0377-0427(96)87162-7; CATTE F, 1992, SIAM J NUMER ANAL, V32, P1895; Chambolle A, 2004, J MATH IMAGING VIS, V20, P89; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; CHARBONNIER P, 1994, IEEE IMAGE PROC, P168; CRANK J, 1947, P CAMB PHILOS SOC, V43, P50, DOI 10.1007/BF02127704; Drori Y, 2014, MATH PROGRAM, V145, P451, DOI 10.1007/s10107-013-0653-0; Frankel S.P., 1950, MATH TABLES OTHER AI, V4, P65, DOI DOI 10.1090/S0025-5718-1950-0046149-3; Galic I, 2008, J MATH IMAGING VIS, V31, P255, DOI 10.1007/s10851-008-0087-0; GENTZSCH W, 1978, Z ANGEW MATH MECH, V58, pT415; Gentzsch W., 1979, P 3 GAMM C NUM METH; Gordeziani D.G., 1974, COMP MATH MATH PHYS+, V14, P249, DOI DOI 10.1016/0041-5553(74)90157-8; Grewenig S, 2010, LECT NOTES COMPUT SC, V6376, P533; Gurski KF, 2010, AIP CONF PROC, V1281, P761; Gwosdek P., 2012, LNCS, V6554, P372, DOI DOI 10.1007/978-3-642-35740-4_29; Hackbusch W., 1985, MULTIGRID METHODS AP; HELLWIG G., 1977, PARTIAL DIFFERENTIAL, V2nd; Hoffmann S., 2013, SCALE SPACE VARIATIO, V7893, P319, DOI [10.1007/978-3-642-38267-3_, DOI 10.1007/978-3-642-38267-3]; Jawerth B, 1999, J MATH IMAGING VIS, V11, P231, DOI 10.1023/A:1008304519705; Kriva Z, 2002, J VIS COMMUN IMAGE R, V13, P22, DOI 10.1006/jvci.2001.0502; LAASONEN P, 1949, ACTA MATH-DJURSHOLM, V81, P309, DOI 10.1007/BF02395025; LEBEDEV VI, 1971, USSR COMPUT MATHS MA, V11, P155, DOI 10.1016/0041-5553(71)90169-8; LU T, 1991, APPL MATH LETT, V4, P25, DOI 10.1016/0893-9659(91)90161-N; Luxenburger A, 2012, LECT NOTES COMPUT SC, V6667, P544, DOI 10.1007/978-3-642-24785-9_46; Mang A, 2012, P VIS MOD VIS WORKSH, P143, DOI DOI 10.2312/PE/VMV/VMV12/143-150; MEIJERINK JA, 1977, MATH COMPUT, V31, P148, DOI 10.2307/2005786; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Nocedal J, 2006, SPRINGER SER OPER RE, P135; Ochs P, 2015, J MATH IMAGING VIS, V53, P171, DOI 10.1007/s10851-015-0565-0; OPFER G, 1984, LINEAR ALGEBRA APPL, V58, P343, DOI 10.1016/0024-3795(84)90219-2; PEACEMAN DW, 1955, J SOC IND APPL MATH, V3, P28, DOI 10.1137/0103003; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Peter P, 2013, LECT NOTES COMPUT SC, V8142, P231, DOI 10.1007/978-3-642-40602-7_24; Pock T, 2011, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2011.6126441; Raket L. L., 2014, COMPUTATIONAL STAT D, V72, P1723; REICHEL L, 1990, BIT, V30, P332, DOI 10.1007/BF02017352; Richardson L. F., 1910, PHILOS T R SOC A, V210, P307, DOI DOI 10.1098/RSTA.1911.0009; Rosman G, 2009, SIAM J IMAGING SCI, V2, P858, DOI 10.1137/080728391; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Saad Y., 2003, ITERATIVE METHODS SP, Vsecond, DOI DOI 10.1137/1.9780898718003; Saul'yev V.K., 1964, INTEGRATION EQUATION; Schmidt-Richberg Alexander, 2012, Biomedical Image Registration. Proceedings 5th International Workshop, WBIR 2012, P220, DOI 10.1007/978-3-642-31340-0_23; Schroers Christopher, 2012, Pattern Recognition. Proceedings Joint 34th DAGM and 36th OAGM Symposium, P73, DOI 10.1007/978-3-642-32717-9_8; Setzer S, 2013, COMPUT OPTIM APPL, V54, P417, DOI 10.1007/s10589-012-9525-4; Spira A, 2007, IEEE T IMAGE PROCESS, V16, P1628, DOI 10.1109/TIP.2007.894253; Varga RS., 1999, MATRIX ITERATIVE ANA; Weickert J, 1998, IEEE T IMAGE PROCESS, V7, P398, DOI 10.1109/83.661190; Weickert J, 1999, INT J COMPUT VISION, V31, P111, DOI 10.1023/A:1008009714131; Weickert J., 2001, ACTA MATH U COMENIAN, V70, P33; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; Weickert J., 2013, LECT NOTES COMPUTER, V8081, P29; Welk M, 2008, APPL COMPUT HARMON A, V24, P195, DOI 10.1016/j.acha.2007.05.004; WELLS WM, 1986, IEEE T PATTERN ANAL, V8, P234, DOI 10.1109/TPAMI.1986.4767776; YOUNG D, 1954, J MATH PHYS CAMB, V32, P243; Young D. M., 1950, THESIS; Yuan'Chzhao-Din, 1958, THESIS	70	27	31	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2016	118	3					275	299		10.1007/s11263-015-0874-1	http://dx.doi.org/10.1007/s11263-015-0874-1			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DP9AM		Green Submitted			2022-12-18	WOS:000378789100001
J	Long, CJ; Hua, G; Kapoor, A				Long, Chengjiang; Hua, Gang; Kapoor, Ashish			A Joint Gaussian Process Model for Active Visual Recognition with Expertise Estimation in Crowdsourcing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Active learning; Crowdsourcing; Gaussian process classifiers	CLASSIFICATION; ANNOTATORS; MACHINE	We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noise and the expertise level of each individual labeler with two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy for active selection of data samples to be labeled, and active selection of high quality labelers based on their estimated expertise to label the data. We apply the proposed model for four visual recognition tasks, i.e., object category recognition, multi-modal activity recognition, gender recognition, and fine-grained classification, on four datasets with real crowd-sourced labels from the Amazon Mechanical Turk. The experiments clearly demonstrate the efficacy of the proposed model. In addition, we extend the proposed model with the Predictive Active Set Selection Method to speed up the active learning system, whose efficacy is verified by conducting experiments on the first three datasets. The results show our extended model can not only preserve a higher accuracy, but also achieve a higher efficiency.	[Long, Chengjiang; Hua, Gang] Stevens Inst Technol, Hoboken, NJ 07030 USA; [Kapoor, Ashish] Microsoft Res, Redmond, WA 98052 USA	Stevens Institute of Technology; Microsoft	Hua, G (corresponding author), Stevens Inst Technol, Hoboken, NJ 07030 USA.	clong@stevens.edu; ganghua@gmail.com; akapoor@microsoft.com			National Institute Of Nursing Research of the National Institutes of Health [R01NR015371]; US National Science Foundation [IIS 1350763]; China National Natural Science Foundation [61228303]; Stevens Institute of Technology; Google Research Faculty Award; Microsoft Research; NEC Labs America; NATIONAL INSTITUTE OF NURSING RESEARCH [R01NR015371] Funding Source: NIH RePORTER	National Institute Of Nursing Research of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Nursing Research (NINR)); US National Science Foundation(National Science Foundation (NSF)); China National Natural Science Foundation(National Natural Science Foundation of China (NSFC)); Stevens Institute of Technology; Google Research Faculty Award(Google Incorporated); Microsoft Research(Microsoft); NEC Labs America; NATIONAL INSTITUTE OF NURSING RESEARCH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Nursing Research (NINR))	Research reported in this publication was partly supported by the National Institute Of Nursing Research of the National Institutes of Health under Award Number R01NR015371. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. This work is also partly supported by US National Science Foundation Grant IIS 1350763, China National Natural Science Foundation Grant 61228303, GH's start-up funds form Stevens Institute of Technology, a Google Research Faculty Award, a gift grant from Microsoft Research, and a gift grant from NEC Labs America.	Ambati V., 2010, P 7 INT C LANG RES E; [Anonymous], 2011, P 24 INT C NEUR INF; Branson S., 2011, P IEEE INT C COMP VI; Branson S., 2010, P EUR C COMP VIS HER; Burl M., 1998, P IEEE C COMP VIS PA, P23; Burl M.C., P INT WORKSH AUT FAC, P154; Chen S, 2010, AAAI CONF ARTIF INTE, P419; Dekel O., 2009, P IEEE INT C MACH LE; Dekel O., 2009, COLT; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donmez P., 2009, SPECIAL INTEREST GRO; Donmez P., 2010, P SIAM C DAT MIN SDM; Ebert S., 2012, P IEEE INT C COMP VI; FERGUS R, 2005, P 10 INT C COMP VIS; Gibbs MN, 2000, IEEE T NEURAL NETWOR, V11, P1458, DOI 10.1109/72.883477; Groot P, 2011, LECT NOTES COMPUT SC, V6792, P159, DOI 10.1007/978-3-642-21738-8_21; Henao R., 2010, P MACH LEARN SIGN PR; Henao R, 2012, NEUROCOMPUTING, V80, P10, DOI 10.1016/j.neucom.2011.09.017; Hua G, 2013, IEEE I CONF COMP VIS, P1209, DOI 10.1109/ICCV.2013.153; Kapoor A., 2007, P IEEE INT C COMP VI; Kapoor A., 2009, P IEEE INT C COMP VI; Kim HC, 2006, IEEE T PATTERN ANAL, V28, P1948, DOI 10.1109/TPAMI.2006.238; Kim HC, 2008, LECT NOTES COMPUT SC, V5342, P896; Krizhevsky A., 2012, NEURAL INFORM PROCES, V1, P31; Lawrence N., 2003, P 16 ANN C NEURAL IN, P609; Lin Y., 2011, P IEEE INT C COMP VI; Liu David Q., 2008, P INT C MOB TECHN AP, P1; Long C., 2013, P IEEE INT C COMP VI; Loy C., 2012, P IEEE INT C COMP VI; Minka T. P., 2001, THESIS MIT CAMBRIDGE; Naish-Guzman A., 2007, ADV NEURAL INFORM PR, V20, P1057; Neal R.M., 1997, CRGTR972 U TOR; Opper M., 1999, NEURAL COMPUT, V12, P2000; Parikh D, 2011, P IEEE INT C COMP VI; Parikh D., 2011, P IEEE C COMP VIS PA; Parikh D, 2012, IEEE T PATTERN ANAL, V34, P1978, DOI 10.1109/TPAMI.2011.276; Patterson G., 2013, P NEUR INF PROC SYST; Quinonero-~Candela J., 2005, J MACHINE LEARNING R, V6; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Raykar V. C., 2009, P IEEE INT C MACH LE; Raykar VC, 2012, J MACH LEARN RES, V13, P491; Rodrigues F., 2013, MACH LEARN, V95, P165; Rodrigues F., 2014, P IEEE INT C MACH LE; Rodrigues F, 2013, PATTERN RECOGN LETT, V34, P1428, DOI 10.1016/j.patrec.2013.05.012; Roy Nicholas, 2001, P 18 INT C MACH LEAR, P441; Sanchez J., 2011, P IEEE INT C COMP VI; Seeger M, 2003, J MACH LEARN RES, V3, P233, DOI 10.1162/153244303765208386; Seeger M.W., 2003, INT WORKSHOP ARTIFIC, VR4, P254; Snelson E., 2006, P C UNC ART INT UAI; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Spriggs E. H., 2009, P IEEE C COMP VIS PA; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Tivive FHC, 2006, IEEE IJCNN, P5336; Vijayanarasimhan S, 2014, INT J COMPUT VISION, V108, P97, DOI 10.1007/s11263-014-0721-9; von Ahn L., 2006, P SIGCHI C HUMAN FAC, P55, DOI DOI 10.1145/1124772.1124782; Von Ahn Luis, 2004, P SIGCHI C HUM FACT, P319, DOI DOI 10.1145/985692.985733; Wah C., 2011, P IEEE INT C COMP VI; Welinder P., 2010, NEURAL INFORM PROCES; Welinder P., 2010, P IEEE C COMP VIS PA; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Wu O., 2011, P 22 INT JOINT C ART, P1571; Yan F., 2010, P IEEE INT C MACH LE, P1183; Yan Y., 2012, P 15 INT C ART INT S; Yan Y., 2011, ICML, V11, P1161; Yao A., 2012, P IEEE C COMP VIS PA; Yao B., 2011, P IEEE C COMP VIS PA; Zhang ZH, 2011, J MACH LEARN RES, V12, P111; Zhao L., 2011, P IEEE 3 INT C 2011; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236; Zhu X., 2003, ICML 2003 WORKSHOP C; ZITNICK C. L., 2010, P IEEE C COMP VIS PA; Zitnick CL, 2012, PROC CVPR IEEE, P622, DOI 10.1109/CVPR.2012.6247729	73	27	27	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2016	116	2					136	160		10.1007/s11263-015-0834-9	http://dx.doi.org/10.1007/s11263-015-0834-9			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7TJ	26924892	Green Accepted			2022-12-18	WOS:000369423000002
J	Chang, H; Zhou, Y; Borowsky, A; Barner, K; Spellman, P; Parvin, B				Chang, Hang; Zhou, Yin; Borowsky, Alexander; Barner, Kenneth; Spellman, Paul; Parvin, Bahram			Stacked Predictive Sparse Decomposition for Classification of Histology Sections	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Tissue histology; Classification; Sparse coding; Unsupervised feature learning		Image-based classification of histology sections, in terms of distinct components (e.g., tumor, stroma, normal), provides a series of indices for histology composition (e.g., the percentage of each distinct components in histology sections), and enables the study of nuclear properties within each component. Furthermore, the study of these indices, constructed from each whole slide image in a large cohort, has the potential to provide predictive models of clinical outcome. For example, correlations can be established between the constructed indices and the patients' survival information at cohort level, which is a fundamental step towards personalized medicine. However, performance of the existing techniques is hindered as a result of large technical variations (e.g., variations of color/textures in tissue images due to non-standard experimental protocols) and biological heterogeneities (e.g., cell type, cell state) that are always present in a large cohort. We propose a system that automatically learns a series of dictionary elements for representing the underlying spatial distribution using stacked predictive sparse decomposition. The learned representation is then fed into the spatial pyramid matching framework with a linear support vector machine classifier. The system has been evaluated for classification of distinct histological components for two cohorts of tumor types. Throughput has been increased by using of graphical processing unit (GPU), and evaluation indicates a superior performance results, compared with previous research.	[Chang, Hang; Zhou, Yin; Parvin, Bahram] Univ Calif Berkeley, Lawrence Berkeley Natl Lab, Div Life Sci, Berkeley, CA 94720 USA; [Chang, Hang; Parvin, Bahram] Univ Calif Riverside, Dept Elect & Comp Engn, Riverside, CA 92521 USA; [Parvin, Bahram] Univ Nevada, Dept Biomed Engn, Reno, NV 89557 USA; [Spellman, Paul] Oregon Hlth & Sci Univ, Ctr Spatial Syst Biomed, Portland, OR USA; [Borowsky, Alexander] Univ Calif Davis, Ctr Comparat Med, Davis, CA USA; [Barner, Kenneth] Univ Delaware, ECE Dept, Newark, DE USA	United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; University of California System; University of California Berkeley; University of California System; University of California Riverside; Nevada System of Higher Education (NSHE); University of Nevada Reno; Oregon Health & Science University; University of California System; University of California Davis; University of Delaware	Chang, H (corresponding author), Univ Calif Berkeley, Lawrence Berkeley Natl Lab, Div Life Sci, Berkeley, CA 94720 USA.	hchang@lbl.gov; yinzhou@lbl.gov; adborowsky@ucdavis.edu; barner@udel.edu; spellmap@ohsu.edu; b_parvin@lbl.gov			National Institute of Health (NIH) [U24 CA1437991]; NIH at Lawrence Berkeley National Laboratory [R01 CA140663, DE-AC02-05CH11231]; NATIONAL CANCER INSTITUTE [U24CA143799, R01CA140663] Funding Source: NIH RePORTER	National Institute of Health (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NIH at Lawrence Berkeley National Laboratory; NATIONAL CANCER INSTITUTE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Cancer Institute (NCI))	This work was supported by National Institute of Health (NIH) U24 CA1437991 and NIH R01 CA140663 carried out at Lawrence Berkeley National Laboratory under Contract No. DE-AC02-05CH11231.	Acar E., 2012, PLOS ONE, V7; Cruz-Roa AA, 2013, LECT NOTES COMPUT SC, V8150, P403, DOI 10.1007/978-3-642-40763-5_50; Axelrod DE, 2008, CANCER INFORM, V6, P99; Basavanhally A, 2009, I S BIOMED IMAGING, P851, DOI 10.1109/ISBI.2009.5193186; Bhagavatula R, 2010, I S BIOMED IMAGING, P1041, DOI 10.1109/ISBI.2010.5490168; Bilgin CC, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0032906; Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598; Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716; Chang H., 2013, P C COMP VIS PATT RE; Chang H., 2013, MED IMAGE COMPUTING; Chang H, 2013, IEEE T MED IMAGING, V32, P670, DOI 10.1109/TMI.2012.2231420; Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51; Datar M, 2008, I S BIOMED IMAGING, P292, DOI 10.1109/ISBI.2008.4540990; Demir C., 2009, TECHNICAL REPORT; Doyle S, 2011, I S BIOMED IMAGING, P715, DOI 10.1109/ISBI.2011.5872506; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Everingham M., 2012, PASCAL VISUAL OBJECT; Fatakdawala H, 2010, IEEE T BIO-MED ENG, V57, P1676, DOI 10.1109/TBME.2010.2041232; Gurcan Metin N, 2009, IEEE Rev Biomed Eng, V2, P147, DOI 10.1109/RBME.2009.2034865; Han J, 2011, I S BIOMED IMAGING, P711; Huang CH, 2011, COMPUT MED IMAG GRAP, V35, P579, DOI 10.1016/j.compmedimag.2010.11.009; Huang Fu Jie, 2006, 2006 IEEE COMPUTER S, V1, P284, DOI 10.1109/CVPR.2006.164; Jarrett K, 2009, P INT C COMP VIS ICC; Kavukcuoglu K., 2008, CBLLTR20081201 NYU C; Kong J, 2010, INT CONF ACOUST SPEE, P457, DOI 10.1109/ICASSP.2010.5495724; Kothari S, 2012, ACM C BIOINF COMP BI; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Le Q. V., 2012, P 9 IEEE INT S BIOM, P302, DOI [10.1109/ISBI.2012.6235544, DOI 10.1109/ISBI.2012.6235544]; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee H, 2016, ADV NEURAL INFORM PR, V19; Lee H., 2007, ADV NEURAL INFORM PR, V20; Mairal J, 2010, J MACH LEARN RES, V11, P19; Moosmann F, 2008, IEEE T PATTERN ANAL, V30, P1632, DOI 10.1109/TPAMI.2007.70822; Nayak N, 2013, I S BIOMED IMAGING, P410; Poultney C., 2006, ADV NEURAL INFORM PR; Ranzato M., 2008, PROC NEURAL INF PROC, P1185; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wu RB, 2013, PROC CVPR IEEE, P867, DOI 10.1109/CVPR.2013.117; Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757; Young R. A., 2001, CORTICAL MODEL SPATI, V2001, P3; Yu Kai, 2009, ADV NEURAL INFORM PR, P2223; Zhou Y, 2014, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2014.394	43	27	29	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2015	113	1			SI		3	18		10.1007/s11263-014-0790-9	http://dx.doi.org/10.1007/s11263-014-0790-9			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CH6GX	27721567	Green Accepted			2022-12-18	WOS:000354135700002
J	Makadia, A; Daniilidis, K				Makadia, Ameesh; Daniilidis, Kostas			Spherical Correlation of Visual Representations for 3D Model Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D shape retrieval; Visual similarity; Spherical Fourier transform	OBJECT RECOGNITION; IMAGES	In recent years we have seen a tremendous growth in the amount of freely available 3D content, in part due to breakthroughs for 3D model design and acquisition. For example, advances in range sensor technology and design software have dramatically reduced the manual labor required to construct 3D models. As collections of 3D content continue to grow rapidly, the ability to perform fast and accurate retrieval from a database of models has become a necessity. At the core of this retrieval task is the fundamental challenge of defining and evaluating similarity between 3D shapes. Some effective methods dealing with this challenge consider similarity measures based on the visual appearance of models. While collections of rendered images are discriminative for retrieval tasks, such representations come with a few inherent limitations such as restrictions in the image viewpoint sampling and high computational costs. In this paper we present a novel algorithm for model similarity that addresses these issues. Our proposed method exploits techniques from spherical signal processing to efficiently evaluate a visual similarity measure between models. Extensive evaluations on multiple datasets are provided.	[Makadia, Ameesh] Google Res New York, New York, NY 10011 USA; [Daniilidis, Kostas] Univ Penn, Dept Comp Sci, Philadelphia, PA 19104 USA	Google Incorporated; University of Pennsylvania	Makadia, A (corresponding author), Google Res New York, New York, NY 10011 USA.	makadia@grasp.cis.upenn.edu; kostas@cis.upenn.edu		Daniilidis, Kostas/0000-0003-0498-0758				Ankerst M, 1999, Proc Int Conf Intell Syst Mol Biol, P34; Arfken G., 1966, MATH METHODS PHYS; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Burel G, 1995, GRAPH MODEL IM PROC, V57, P400, DOI 10.1006/gmip.1995.1034; Chen D.-Y., 2003, EUROGRAPHICS; DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008; Frome A., 2004, P EUR C COMP VIS ECC; Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279; HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073; Johnson A., 1997, THESIS CARNEGIE MELL; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; KANG SB, 1991, P 1991 IEEE COMP SOC; Kazhdan M., 2003, S GEOMETRY PROCESSIN; Kazhdan M, 2007, IEEE T PATTERN ANAL, V29, P1221, DOI 10.1109/TPAMI.2007.1032; KORTGEN M, 2003, 7 CENTR EUR SEM COMP; KOSTELEC PJ, 2003, WORKING PAPER SERIES; Kovacs JA, 2002, ACTA CRYSTALLOGR D, V58, P1282, DOI 10.1107/S0907444902009794; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Makadia A, 2006, IEEE T PATTERN ANAL, V28, P1170, DOI 10.1109/TPAMI.2006.150; MAKADIA A, 2007, 3DTV KOS; MAKADIA A, 2003, IEEE C COMP VIS PATT; MAKADIA A, 2004, ICPR 04, V3, P590; Novotni M., 2003, P 8 ACM S SOL MOD AP, P216; Ohbuchi R, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P97, DOI 10.1109/TPCG.2003.1206936; Ohbuchi R., 2008, IEEE INT C SHAP MOD; Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386; Saupe D., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P392; SHILANE P, 2004, SHAPE MODELING INT; Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0; Thurston W.P., 1997, PRINCETON MATH SERIE, V35; TYPKE R, 2006, P INT C MULT EXP; Veltkamp RC, 2006, SHREC2006 3D SHAPE R; Vranic DV, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P757; ZHANG DS, 2002, P 5 AS C COMP VIS AC, P652	34	27	28	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2010	89	2-3			SI		193	210		10.1007/s11263-009-0280-7	http://dx.doi.org/10.1007/s11263-009-0280-7			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	594OS					2022-12-18	WOS:000277547600005
J	Bartoli, A; Perriollat, M; Chambon, S				Bartoli, Adrien; Perriollat, Mathieu; Chambon, Sylvie			Generalized Thin-Plate Spline Warps	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Thin-plate spline; Deformable surface; Image warp; Perspective projection; Rigidity; Fundamental matrix		The Thin-Plate Spline warp has been shown to be a very effective parameterized model of the optic flow field between images of various types of deformable surfaces, such as a paper sheet being bent. Recent work has also used such warps for images of a smooth and rigid surface. Standard Thin-Plate Spline warps are however not rigid, in the sense that they do not comply with the epipolar geometry. They are also intrinsically affine, in the sense of the affine camera model, since they are not able to simply model the effect of perspective projection. We propose three types of warps based on the Thin-Plate Spline. The first one is a rigid flexible warp. It describes the optic flow field induced by a smooth and rigid surface, and satisfies the affine epipolar geometry constraint. The second and third proposed warps extend the standard Thin-Plate Spline warp and the proposed rigid flexible warp to the perspective camera model. The properties of these warps are studied in details and a hierarchy is defined. Experimental results on simulated and real data are reported.	[Bartoli, Adrien] Univ Auvergne, Clermont Ferrand, France; [Perriollat, Mathieu] VI Technol, Grenoble, France; [Chambon, Sylvie] LCPC, Nantes, France	Universite Clermont Auvergne (UCA); Universite Gustave-Eiffel; Laboratoire Central des Ponts et Chaussees (LCPC)	Bartoli, A (corresponding author), Univ Auvergne, Clermont Ferrand, France.	Adrien.Bartoli@gmail.com						Anand Abhishek, 2012, BRIT MACH VIS C; Bartoli A, 2008, J MATH IMAGING VIS, V31, P133, DOI 10.1007/s10851-007-0062-1; Bartoli A, 2009, INT J COMPUT VISION, V85, P133, DOI 10.1007/s11263-009-0253-x; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; BREGLER C, 2000, INT C COMP VIS PATT; Chojnacki W, 2003, IEEE T PATTERN ANAL, V25, P1172, DOI 10.1109/TPAMI.2003.1227992; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; COOTES TF, 2004, EUR C COMP VIS; DONATO G, 2002, EUR C COMP VIS; DUCHON J, 1976, REV FR AUTOMAT INFOR, V10, P5; GAYBELLILE V, 2009, IEEE T PATT IN PRESS; Hartley R., 2003, MULTIPLE VIEW GEOMET; IRANI M, 1999, P INT WORKSH VIS ALG; Lee SY, 1996, J VISUAL COMP ANIMAT, V7, P3, DOI 10.1002/(SICI)1099-1778(199601)7:1<3::AID-VIS131>3.0.CO;2-U; LIM J, 2005, INT C COMP VIS PATT; Luong QT, 1996, COMPUT VIS IMAGE UND, V64, P193, DOI 10.1006/cviu.1996.0055; Malgouyres R, 2009, BRIT MACH VIS C; MASSON L, 2005, BRIT MACH VIS C; MORENO A, 2006, WORKSH IM REG DEF EN; PERRIOLLAT M, 2006, WORKSH IM REG DEF EN; Pilet J, 2008, INT J COMPUT VISION, V76, P109, DOI 10.1007/s11263-006-0017-9; PRASAD M, 2006, INT C COMP VIS PATT; Salzmann M, 2007, IEEE T PATTERN ANAL, V29, P1481, DOI 10.1109/TPAMI.2007.1080; Schaefer Scott, 2006, SIGGRAPH; Sederberg T., 1986, SIGGRAPH; TERZOPOULOS D, 1983, COMPUT VISION GRAPH, V24, P52, DOI 10.1016/0734-189X(83)90020-8; WILLS J, 2004, EUR C COMP VIS; YANG AY, 2005, INT C COMP VIS PATT	29	27	28	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	88	1					85	110		10.1007/s11263-009-0303-4	http://dx.doi.org/10.1007/s11263-009-0303-4			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	571LS		Green Submitted			2022-12-18	WOS:000275753900005
J	Hoogendoorn, C; Sukno, FM; Ordas, S; Frangi, A				Hoogendoorn, Corne; Sukno, Federico M.; Ordas, Sebastian; Frangi, Alejandro F.			Bilinear Models for Spatio-Temporal Point Distribution Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Statistical shape modeling; Cardiac modeling; Cardiac dynamics; Bilinear models; Spatiotemporal decomposition	ACTIVE SHAPE MODELS; CARDIAC MOTION; LEFT-VENTRICLE; SEGMENTATION; RECOGNITION; MR	In this work we describe the usage of bilinear statistical models as a means of factoring the shape variability into two components attributed to inter-subject variation and to the intrinsic dynamics of the human heart. We show that it is feasible to reconstruct the shape of the heart at discrete points in the cardiac cycle. Provided we are given a small number of shape instances representing the same heart at different points in the same cycle, we can use the bilinear model to establish this. Using a temporal and a spatial alignment step in the preprocessing of the shapes, around half of the reconstruction errors were on the order of the axial image resolution of 2 mm, and over 90% was within 3.5 mm. From this, we conclude that the dynamics were indeed separated from the inter-subject variability in our dataset.	[Hoogendoorn, Corne; Sukno, Federico M.; Ordas, Sebastian; Frangi, Alejandro F.] Univ Pompeu Fabra, Ctr Computat Imaging & Simulat Technol Biomed, Dept Informat & Commun Technol, Barcelona 08018, Spain; [Hoogendoorn, Corne; Sukno, Federico M.; Frangi, Alejandro F.] Networking Biomed Res Ctr Bioengn Biomat & Nanome, CIBER BBN, Barcelona, Spain	Pompeu Fabra University; CIBER - Centro de Investigacion Biomedica en Red; CIBERBBN	Frangi, A (corresponding author), Univ Pompeu Fabra, Ctr Computat Imaging & Simulat Technol Biomed, Dept Informat & Commun Technol, Carrer Roc Boronat 138, Barcelona 08018, Spain.	corne.hoogendoorn@upf.edu; federico.sukno@upf.edu; alejandro.frangi@upf.edu	Frangi, Alejandro F/C-6500-2008; Sukno, Federico/AAM-4440-2021	Frangi, Alejandro F/0000-0002-2675-528X; Sukno, Federico/0000-0002-2029-1576; Hoogendoorn, Corne/0000-0002-4914-9936	Spanish Ministry of Education and Science; Spanish CDTI-MITYC; MEC [TEC2006-03617/TCM]; ISCIII [FIS2004/40676]	Spanish Ministry of Education and Science(Spanish Government); Spanish CDTI-MITYC; MEC(European Commission); ISCIII(Instituto de Salud Carlos III)	The work of A. F. F. was supported by the Spanish Ministry of Education and Science under a Ramon y Cajal Research Fellowship. This work was partially developed within the framework of the CENIT-CDTEAM Project funded by the Spanish CDTI-MITYC, and also partially supported by grants MEC TEC2006-03617/TCM and ISCIII FIS2004/40676.	Abboud B, 2005, IEE P-VIS IMAGE SIGN, V152, P327, DOI 10.1049/ip-vis:20045060; Bistoquet A, 2007, IEEE T MED IMAGING, V26, P1136, DOI 10.1109/TMI.2007.903693; BLACKALL JM, 2001, LECT NOTES COMPUTER, V2208, P1338; Bosch JG, 2002, IEEE T MED IMAGING, V21, P1374, DOI 10.1109/TMI.2002.806427; Chandrashekara R, 2003, LECT NOTES COMPUT SC, V2732, P599; Chuang E, 2005, ACM T GRAPHIC, V24, P331, DOI 10.1145/1061347.1061355; COOTES TF, 1992, IMAGE VISION COMPUT, V10, P289, DOI 10.1016/0262-8856(92)90044-4; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cuzzolin F., 2006, P 2006 IEEE COMP SOC, V2, P1701; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Dryden I.L., 1998, STAT SHAPE ANAL, DOI [DOI 10.5555/1046920.1088707, 10.1002/9781119072492]; Duncan JS, 2000, IEEE T PATTERN ANAL, V22, P85, DOI 10.1109/34.824822; FRANGI AF, 2005, ADV IMAGE PROCESSING, P267; GONZALEZMORA J, 2007, P INT WORKSH NONR RE; GOODALL C, 1991, J ROY STAT SOC B MET, V53, P285, DOI 10.1111/j.2517-6161.1991.tb01825.x; Grimes DB, 2005, NEURAL COMPUT, V17, P47, DOI 10.1162/0899766052530893; Hamarneh G, 2004, IMAGE VISION COMPUT, V22, P461, DOI 10.1016/j.imavis.2003.11.009; HOOGENDOORN C, 2007, P 8 INT WORKSH MATH; Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; LE HL, 1993, ANN STAT, V21, P1225, DOI 10.1214/aos/1176349259; Lee CS, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P147; Lekadir K, 2007, LECT NOTES COMPUT SC, V4792, P834; Leung KYE, 2007, LECT NOTES COMPUT SC, V4791, P52; Liu HF, 2007, IEEE T IMAGE PROCESS, V16, P901, DOI 10.1109/TIP.2007.891773; Lynch M, 2008, IEEE T MED IMAGING, V27, P195, DOI 10.1109/TMI.2007.904681; Magnus J. R., 1988, WILEY SERIES PROBABI; MARDIA KV, 1989, ADV APPL PROBAB, V21, P742, DOI 10.2307/1427764; MARIMONT DH, 1992, J OPT SOC AM A, V9, P1905, DOI 10.1364/JOSAA.9.001905; McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7; Mitchell SC, 2002, IEEE T MED IMAGING, V21, P1167, DOI 10.1109/TMI.2002.804425; Montagnat J, 2005, MED IMAGE ANAL, V9, P87, DOI 10.1016/j.media.2004.06.025; OHNESORGE BM, 2002, MULTISLICE CT CARDIA; ORDAS S, 2007, P SPIE MED IMAGING, V6511; Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976; Perperidis D, 2005, MED IMAGE ANAL, V9, P441, DOI 10.1016/j.media.2005.05.004; PERPERIDIS D, 2005, THESIS IMPERIAL COLL; Shin D, 2008, PATTERN RECOGN LETT, V29, P49, DOI 10.1016/j.patrec.2007.08.013; Styner MA, 2003, LECT NOTES COMPUT SC, V2732, P63; Suri JS, 2000, PATTERN ANAL APPL, V3, P209, DOI 10.1007/s100440070008; Syeda-Mahmood T, 2007, LECT NOTES COMPUT SC, V4791, P261; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; TENENBAUM JB, 1996, ADV NEURAL INFORM PR, P662; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Vasilescu MAO, 2003, PROC CVPR IEEE, P93	46	27	27	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2009	85	3					237	252		10.1007/s11263-009-0212-6	http://dx.doi.org/10.1007/s11263-009-0212-6			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	502CO		Green Accepted			2022-12-18	WOS:000270432200004
J	Makadia, A; Geyer, C; Daniilidis, K				Makadia, Ameesh; Geyer, Christopher; Daniilidis, Kostas			Correspondence-free structure from motion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion estimation; structure from motion; registration; harmonic analysis; correspondence-free motion		We present a novel approach for the estimation of 3D-motion directly from two images using the Radon transform. The feasibility of any camera motion is computed by integrating over all feature pairs that satisfy the epipolar constraint. This integration is equivalent to taking the inner product of a similarity function on feature pairs with a Dirac function embedding the epipolar constraint. The maxima in this five dimensional motion space will correspond to compatible rigid motions. The main novelty is in the realization that the Radon transform is a filtering operator: If we assume that the similarity and Dirac functions are defined on spheres and the epipolar constraint is a group action of rotations on spheres, then the Radon transform is a correlation integral. We propose a new algorithm to compute this integral from the spherical Fourier transform of the similarity and Dirac functions. Generating the similarity function now becomes a preprocessing step which reduces the complexity of the Radon computation by a factor equal to the number of feature pairs processed. The strength of the algorithm is in avoiding a commitment to correspondences, thus being robust to erroneous feature detection, outliers, and multiple motions.	Univ Penn, Philadelphia, PA 19104 USA; Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University of Pennsylvania; Carnegie Mellon University	Makadia, A (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	makadia@cis.upenn.edu; cgeyer@cs.cmu.edu; kostas@cis.upenn.edu		Daniilidis, Kostas/0000-0003-0498-0758				Antone M, 2002, INT J COMPUT VISION, V49, P143, DOI 10.1023/A:1020141505696; Arfken G., 1966, MATH METHODS PHYS; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; CHIRIKJIAN G, 2000, ENG APPL NONCOMMUNIC; DANIILIDIS K, 1996, VISUAL NAVIGATION, P61; DEANS SR, 1981, IEEE T PATTERN ANAL, V3, P185, DOI 10.1109/TPAMI.1981.4767076; DELLAERT F, 2000, STRUCTURE MOTION COR; DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008; FERMULLER C, 1995, SCIENCE, V270, P1973, DOI 10.1126/science.270.5244.1973; Gallier J., 2005, NOTES GROUP ACTIONS; Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135; GEYER C, 2004, WORKSH OMNIDIRECTION; Healy DM, 2003, J FOURIER ANAL APPL, V9, P341, DOI 10.1007/s00041-003-0018-9; Helgason S, 2000, GROUPS GEOMETRIC ANA; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; JIN H, 2003, VISUAL COMPUT, V19, P1; KOSTELEC PJ, 2003, WORKING PAPER SERIES; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MAHAJAN D, 2006, EUR C COMP VIS, V4, P41; MAKADIA A, 2004, P INT C PATTERN RECO; MAKADIA A, 2005, IEEE C COMPUTER VISI; MARIOTTINI GL, 2005, IEEE ROBOTICS AUTOMA, V3; MASLEN D, 1995, P DIMACS WORKSH GROU; Nayar SK, 1997, PROC CVPR IEEE, P482, DOI 10.1109/CVPR.1997.609369; NEGAHDARIPOUR S, 1987, IEEE T PATTERN ANAL, V9, P168, DOI 10.1109/TPAMI.1987.4767884; Oliensis J, 2000, COMPUT VIS IMAGE UND, V80, P172, DOI 10.1006/cviu.2000.0869; ROY S, 1996, P INT C PATT REC VIE; Schroder P., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P161, DOI 10.1145/218380.218439; SHI J, 1998, P INT C COMP VIS; Sugiura M., 1990, UNITARY REPRESENTATI; SZELISKI R, 1995, IEEE WORKSH REPR VIS, P26; VIDAL R, 2004, EUR C COMP VIS, P1; WEXLER Y, 2003, IEEE C COMP VIS PATT	33	27	28	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2007	75	3					311	327		10.1007/s11263-007-0035-2	http://dx.doi.org/10.1007/s11263-007-0035-2			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	211QS		Green Submitted			2022-12-18	WOS:000249539000001
J	Tan, S; Jiao, LC				Tan, Shan; Jiao, Licheng			Multivariate statistical models for image denoising in the wavelet domain	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						natural image statistics; multivariate model; elliptically contoured distribution family; image denoising	INVARIANT RANDOM-PROCESSES; RIDGELET BI-FRAME; NATURAL IMAGES; SCALE MIXTURES; CONTOURLET TRANSFORM; BIVARIATE SHRINKAGE; REPRESENTATION; DISTRIBUTIONS; COEFFICIENTS; INTERSCALE	We model wavelet coefficients of natural images in a neighborhood using the multivariate Elliptically Contoured Distribution Family (ECDF) and discuss its application to the image denoising problem. A desirable property of the ECDF is that a multivariate Elliptically Contoured Distribution (ECD) can be deduced directly from its lower dimension marginal distribution. Using the property, we extend a bivariate model that has been used to successfully model the 2-D joint probability distribution of a two dimension random vector-a wavelet coefficient and its parent to multivariate cases. Though our method only provides a simple and rough characterization of the full probability distribution of wavelet coefficients in a neighborhood, we find that the resulting denoising algorithm based on the extended multivariate models is computably tractable and produces state-of-the-art restoration results. In addition, we discuss the equivalence relation between our denoising algorithm and several other state-of-the-art denoising algorithms. Our work provides a unified mathematic interpretation of a type of statistical denoising algorithms. We also analyze the limitations and advantages of algorithms of this type.	Xidian Univ, Inst Intelligent Informat Proc, Xian, Peoples R China	Xidian University	Tan, S (corresponding author), Xidian Univ, Inst Intelligent Informat Proc, Xian, Peoples R China.			Tan, Shan/0000-0001-9350-5128				ANDERSON TW, 1987, SANKHYA SER A, V49, P305; ANDERSON TW, 1992, DEV STAT RECENT CONT, P41; ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99; AWATE SP, 2005, P IEEE INT C COMP VI; BREHM H, 1987, SIGNAL PROCESS, V12, P119, DOI 10.1016/0165-1684(87)90001-6; Candes E. J., 1999, CURVE SURFACE FITTIN; Candes EJ, 1999, APPL COMPUT HARMON A, V6, P197, DOI 10.1006/acha.1998.0248; CHANG S, 1998, P 5 IEEE INT C IM PR; Crouse MS, 1998, IEEE T SIGNAL PROCES, V46, P886, DOI 10.1109/78.668544; Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376; Do MN, 2003, IEEE T IMAGE PROCESS, V12, P16, DOI 10.1109/TIP.2002.806252; Donoho DL, 2000, SIAM J MATH ANAL, V31, P1062, DOI 10.1137/S0036141098344403; Fang KT, 1990, GENERALIZED MULTIVAR; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; Figueiredo MAT, 2001, IEEE T IMAGE PROCESS, V10, P1322, DOI 10.1109/83.941856; GEHLER PV, 2005, ADV NEURAL PROCESSIN; HUANG J, 2000, THESIS BROWN  DIVISI; Lee AB, 2003, INT J COMPUT VISION, V54, P83, DOI 10.1023/A:1023705401078; Liu J, 2001, IEEE T IMAGE PROCESS, V10, P1647, DOI 10.1109/83.967393; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Mihcak MK, 1999, IEEE SIGNAL PROC LET, V6, P300, DOI 10.1109/97.803428; Moulin P, 1999, IEEE T INFORM THEORY, V45, P909, DOI 10.1109/18.761332; MUMFORD D, 2005, NEW DIRECTIONS STAT; Pizurica A, 2002, IEEE T IMAGE PROCESS, V11, P545, DOI 10.1109/TIP.2002.1006401; Po DDY, 2006, IEEE T IMAGE PROCESS, V15, P1610, DOI 10.1109/TIP.2006.873450; Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640; RANGASWAMY M, 1993, IEEE T AERO ELEC SYS, V29, P111, DOI 10.1109/7.249117; Schoenberg IJ, 1938, ANN MATH, V39, P811, DOI 10.2307/1968466; Sendur L, 2002, IEEE SIGNAL PROC LET, V9, P438, DOI 10.1109/LSP.2002.806054; Sendur L, 2002, IEEE T SIGNAL PROCES, V50, P2744, DOI 10.1109/TSP.2002.804091; SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085; Simoncelli Eero P., 1996, P IEEE INT C IM PROC, V1, P379; SIMONCELLI EP, 1997, P 31 AS C SIGN SYST; Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444; Starck JL, 2001, P SOC PHOTO-OPT INS, V4478, P9, DOI 10.1117/12.449693; Starck JL, 2002, IEEE T IMAGE PROCESS, V11, P670, DOI 10.1117/12.408568; Tan S, 2006, APPL COMPUT HARMON A, V20, P391, DOI 10.1016/j.acha.2005.10.004; Tan S, 2006, J OPT SOC AM A, V23, P2449, DOI 10.1364/JOSAA.23.002449; Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302; Voloshynovskiy S, 2005, SIGNAL PROCESS, V85, P1950, DOI 10.1016/j.sigpro.2005.04.007; Wainwright MJ, 2000, ADV NEUR IN, V12, P855; WEGMANN B, 1990, P SOC PHOTO-OPT INS, V1360, P909, DOI 10.1117/12.24279; YAO K, 1973, IEEE T INFORM THEORY, V19, P600, DOI 10.1109/TIT.1973.1055076	43	27	27	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2007	75	2					209	230		10.1007/s11263-006-0019-7	http://dx.doi.org/10.1007/s11263-006-0019-7			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	207LU					2022-12-18	WOS:000249253400001
J	Hung, YS; Tang, WK				Hung, YS; Tang, WK			Projective reconstruction from multiple views with minimization of 2D reprojection error	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						multiple views; projective reconstruction; structure and motion; sub-space method; factorization method; projective bundle adjustment	AFFINE	The problem of projective reconstruction by minimization of the 2D reprojection error in multiple images is considered. Although bundle adjustment techniques can be used to minimize the 2D reprojection error, these methods being based on nonlinear optimization alcyorithms require a good starting point. Quasi-linear algorithms with better global convergence properties can be used to generate an initial solution before submitting it to bundle adjustment for refinement. In this paper, we propose a factorization-based method to integrate the initial search as well as the bundle adjustment into a single algorithm consisting of a sequence of weighted least-squares problems, in which a control parameter is initially set to a relaxed state to allow the search of a good initial solution, and subsequently tightened up to force the final solution to approach a minimum point of the 2D reprojection error. The proposed algorithm is guaranteed to converge. Our method readily handles images with missing points.	Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China	University of Hong Kong	Hung, YS (corresponding author), Univ Hong Kong, Dept Elect & Elect Engn, Pokfulam Rd, Hong Kong, Hong Kong, Peoples R China.	yshung@eee.hku.hk; wktang@eee.hku.hk	Tang, Arvin/C-1767-2009; Hung, Yeung Sam/C-1852-2009					BARTOLI A, 2001, 4236 INRIA; Beardsley PA, 1997, INT J COMPUT VISION, V23, P235, DOI 10.1023/A:1007923216416; Chen G., 1999, INT C COMP VIS PATT, V2, P55; CHEN G, 2002, PRACTICAL ALGORITHMS, V20, P103; FAUGERAS O, 1995, J OPT SOC AM A, V12, P465, DOI 10.1364/JOSAA.12.000465; Frahm J.-M., 1996, EUR C COMP VIS CAMBR, P2; Han M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P163, DOI 10.1109/ICCV.2001.937513; HAN M, 2000, CMURITR0009 ROB I; HARTLEY R, 1993, LECT NOTES COMPUTER, V825, P237; Heyden A, 1999, IMAGE VISION COMPUT, V17, P981, DOI 10.1016/S0262-8856(99)00002-5; Jacobs DW, 2001, COMPUT VIS IMAGE UND, V82, P57, DOI 10.1006/cviu.2001.0906; Mahamud S, 2001, PROC CVPR IEEE, P1018; Mahamud S, 2000, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2000.854872; MORRIS DD, 1999, VISION ALGORITHMS TH; Oliensis J., 1996, IEEE INT COMPUTER VI, P745; Pollefeys M, 1999, IEEE T PATTERN ANAL, V21, P707, DOI 10.1109/34.784285; POWELL MJD, 1970, NUMERICAL METHODS NO; SHUM HY, 1995, IEEE T PATTERN ANAL, V17, P854, DOI 10.1109/34.406651; SHUM HY, 1999, INT C COMP VIS PATT; SPARR G, 1996, INT C PATT REC; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Triggs B, 1996, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.1996.517170; TRIGGS B, 1998, UNPUB SOME NOTES FAC; TRIGGS B, 2000, LECT NOTES COMPUTER, V1883, P298, DOI DOI 10.1007/3-540-44480-7	24	27	29	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2006	66	3					305	317		10.1007/s11263-005-3675-0	http://dx.doi.org/10.1007/s11263-005-3675-0			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	028GP					2022-12-18	WOS:000236475400005
J	Brodsky, T; Fermuller, C; Aloimonos, Y				Brodsky, T; Fermuller, C; Aloimonos, Y			Structure from motion: Beyond the epipolar constraint	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D motion estimation; scene reconstruction; smoothing and discontinuity detection; depth variability constraint	VISUAL-MOTION; FIELDS; SEGMENTATION; PERCEPTION; STEREO	The classic approach to structure from motion entails a clear separation between motion estimation and structure estimation and between two-dimensional (2D) and three-dimensional (3D) information. For the recovery of the rigid transformation between different views only 2D image measurements are used. To have available enough information, most existing techniques are based on the intermediate computation of optical flow which, however, poses a problem at the locations of depth discontinuities. If we knew where depth discontinuities were, we could (using a multitude of approaches based on smoothness constraints) accurately estimate flow values for image patches corresponding to smooth scene patches; but to know the discontinuities requires solving the structure from motion problem first. This paper introduces a novel approach to structure from motion which addresses the processes of smoothing, 3D motion and structure estimation in a synergistic manner. It provides an algorithm for estimating the transformation between two views obtained by either a calibrated or uncalibrated camera. The results of the estimation are then utilized to perform a reconstruction of the scene from a short sequence of images. The technique is based on constraints on image derivatives which involve the 3D motion and shape of the scene, leading to a geometric and statistical estimation problem. The interaction between 3D motion and shape allows us to estimate the 3D motion while at the same time segmenting the scene. If we use a wrong 3D motion estimate to compute depth, we obtain a distorted version of the depth function. The distortion, however, is such that the worse the motion estimate, the more likely we are to obtain depth estimates that vary locally more than the correct ones. Since local variability of depth is due either to the existence of a discontinuity or to a wrong 3D motion estimate, being able to differentiate between these two cases provides the correct motion, which yields the "least varying" estimated depth as well as the image locations of scene discontinuities. We analyze the new constraints, show their relationship to the minimization of the epipolar constraint, and present experimental results using real image sequences that indicate the robustness of the method.	Univ Maryland, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Brodsky, T (corresponding author), Philips Res, 345 Scarborough Rd, Briarcliff Manor, NY 10510 USA.	tbr@philabs.research.philips.com; fer@cfar.umd.edu; yiannis@cfar.umd.edu	Aloimonos, Yiannis/AAI-2969-2020	Aloimonos, Yiannis/0000-0002-8152-4281				BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BEARDSLEY P, 1996, P EUR C COMP VIS CAM, V2, P683; BERGEN JR, 1992, P EUR C COMP VIS, P237; Brodsky T, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P83, DOI 10.1109/ICCV.1998.710704; Brodsky T., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P146, DOI 10.1109/CVPR.1999.784622; Brodsky T, 1998, INT J COMPUT VISION, V26, P5, DOI 10.1023/A:1007928406666; BRODSKY T, 1998, P DARPA IM UND WORKS, P1003; BRODSKY T, 1998, P EUR C COMP VIS, P342; Cheong L, 1998, COMPUT VIS IMAGE UND, V71, P356, DOI 10.1006/cviu.1997.0649; Cormen T.H., 1989, INTRO ALGORITHMS; Cox IJ, 1996, COMPUT VIS IMAGE UND, V63, P542, DOI 10.1006/cviu.1996.0040; DANIILIDIS K, 1997, VISUAL NAVIGATION BI, pCH4; Faugeras O., 1992, 3 DIMENSIONAL COMPUT; FAUGERAS O, 1998, P ECCV 98, V1, P379; Fermuller C, 2000, VISION RES, V40, P77, DOI 10.1016/S0042-6989(99)00162-5; FERMULLER C, 1995, SCIENCE, V270, P1973, DOI 10.1126/science.270.5244.1973; FERMULLER C, 1993, ACTIVE PERCEPTION, pCH3; FITZGIBBON AW, 1998, P 5 EUR C COMP VIS, V1, P311; FOLEY JM, 1980, PSYCHOL REV, V87, P411, DOI 10.1037/0033-295X.87.5.411; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; HARTLEY RI, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P908; HEITZ F, 1993, IEEE T PATTERN ANAL, V15, P1217, DOI 10.1109/34.250841; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; HORN BKP, 1987, INT J COMPUT VISION, V1, P259, DOI 10.1007/BF00127824; KOCH R, 1998, P 5 EUR C COMP VIS, V1, P55; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; LUCAS BD, 1984, THESIS CARNEGIEMELLO; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; MARROQUIN J, 1985, THESIS I TECHNOLOGY; MAYBANK SJ, 1986, IMAGE VISION COMPUT, V4, P38, DOI 10.1016/0262-8856(86)90006-5; MAYBANK SJ, 1987, THESIS U LONDON; MENDELSOHN J, 1997, P INT C COMP AN IM P, P255; Mumford D., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P22; MURRAY DW, 1987, IEEE T PATTERN ANAL, V9, P220, DOI 10.1109/TPAMI.1987.4767896; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; Pollefeys M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P90, DOI 10.1109/ICCV.1998.710705; ROBERT L, 1996, P EUR C COMP VIS, V1, P439; SCHUNCK BG, 1989, IEEE T PATTERN ANAL, V11, P1010, DOI 10.1109/34.42834; Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462; Spoerri A., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P209; Szeliski R, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P517, DOI 10.1109/ICCV.1998.710766; THOMPSON WB, 1985, IEEE T PATTERN ANAL, V7, P374, DOI 10.1109/TPAMI.1985.4767677; TITTLE JS, 1995, J EXP PSYCHOL HUMAN, V21, P663, DOI 10.1037/0096-1523.21.3.663	44	27	27	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	37	3					231	258		10.1023/A:1008132107950	http://dx.doi.org/10.1023/A:1008132107950			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	342GF					2022-12-18	WOS:000088636300002
J	Morris, RJ; Hogg, DC				Morris, RJ; Hogg, DC			Statistical models of object interaction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	1st IEEE Workshop on Visual Surveillance	JAN 02, 1998	BOMBAY, INDIA	IEEE Comp Soc		behaviour modelling; object interactions; landmark data		We present a method for assessing the likelihood of a trajectory of an object through a scene consisting of a number of other objects. The closest points on the trajectory to the other objects are chosen as landmark points and at each landmark we calculate the probability of the interaction based on the speed and distance. Sequences of such probabilities are then sorted in increasing order. Finally a weighted sum of the first few elements in this weighted list is used to classify trajectories in a supervised learning framework.	Univ Leeds, Dept Stat, Leeds LS2 9JT, W Yorkshire, England; Univ Leeds, Sch Comp Studies, Leeds LS2 9JT, W Yorkshire, England	University of Leeds; University of Leeds	Morris, RJ (corresponding author), Univ Leeds, Dept Stat, Leeds LS2 9JT, W Yorkshire, England.		Rohlf, F J/A-8710-2008	Hogg, David/0000-0002-6125-9564				BAUMBERG A, 1994, P EUR C COMP VIS, V1, P299; BAUMBERG A, 1996, IMAGE VISION COMPUTI; Baumberg A. M., 1994, Proceedings of the 1994 IEEE Workshop on Motion of Non-Rigid and Articulated Objects (Cat. No.94TH0671-8), P194, DOI 10.1109/MNRAO.1994.346236; Bobick A. F., 1995, Proceedings. Fifth International Conference on Computer Vision (Cat. No.95CB35744), P382, DOI 10.1109/ICCV.1995.466914; Bookstein F L, 1997, Med Image Anal, V1, P225; Bookstein F. L., 1991, MORPHOMETRIC TOOLS L; Bregler C, 1997, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.1997.609382; BULPITT AJ, 1993, P WORLD C NEUR NETW, V3, P708; BUXTON H, 1995, ARTIF INTELL, V78, P431, DOI 10.1016/0004-3702(95)00041-0; Gumbel E., 1958, STAT EXTREMES; Johnson N, 1996, IMAGE VISION COMPUT, V14, P609, DOI 10.1016/0262-8856(96)01101-8; MONHAUPT M, 1990, P ECCV, P598; NAGEL HH, 1988, IMAGE VISION COMPUT, V6, P59, DOI 10.1016/0262-8856(88)90001-7; Remagnino P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P857, DOI 10.1109/ICCV.1998.710817; Remagnino P., 1997, P 8 BRIT MACH VIS C, P380; STARNER T, 1995, P INT WORKSH AUT FAC; TAN TN, 1994, IMAGE VISION COMPUT, V12, P164, DOI 10.1016/0262-8856(94)90068-X	17	27	27	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	37	2					209	215		10.1023/A:1008159822101	http://dx.doi.org/10.1023/A:1008159822101			7	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	341FD					2022-12-18	WOS:000088579600007
J	Alvarez, L; Morales, F				Alvarez, L; Morales, F			Affine morphological multiscale analysis of corners and multiple junctions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						scale space; mathematical morphology; invariant theory; corner detector; multiple junctions	SCALE-SPACE; CURVE EVOLUTION; IMAGE-ANALYSIS; PLANE-CURVES; DIFFUSION; EQUATIONS; AXIOMS	In this paper we study the application of the Affine Morphological Scale Space (AMSS) to the analysis of singularities (corners or multiple junctions) of the shapes present in a 2-D image. We introduce a new family of travelling wave solutions of AMSS which determines the evolution of the initial shapes given by conics. We characterize the evolution of corners accross the scales according to their angle, We develop a numerical algorithm to compute AMSS accross the scales and we present some experimental results about corners and multiple junction detection.			Alvarez, L (corresponding author), UNIV LAS PALMAS GRAN CANARIA,DEPT INFORMAT & SISTEMAS,CAMPUS TAFIRA,LAS PALMAS 35017,SPAIN.		Alvarez, Luis/A-9190-2009	Alvarez, Luis/0000-0002-6953-9587				ALVAREZ L, 1993, ARCH RATION MECH AN, V123, P199, DOI 10.1007/BF00375127; ALVAREZ L, 1992, SIAM J NUMER ANAL, V29, P845, DOI 10.1137/0729052; ALVAREZ L, 1992, CR ACAD SCI I-MATH, V315, P265; ALVAREZ L, 1994, SIAM J NUMER ANAL, V31, P590, DOI 10.1137/0731032; ASADA H, 1986, IEEE T PATTERN ANAL, V8; BEAUDET PR, 1978, 4TH P INT C PATT REC, P579; Caselles V, 1996, SIAM J APPL MATH, V56, P1199, DOI 10.1137/S0036139994269352; CATTE F, 1993, 9310 CEREMADE U PAR; COHIGNAC T, 1993, IN PRESS NUMERICAL A; CRANDALL MG, 1992, B AM MATH SOC, V27, P1, DOI 10.1090/S0273-0979-1992-00266-5; DERICHE R, 1993, INT J COMPUT VISION, V10, P101, DOI 10.1007/BF01420733; DERICHE R, 1994, COMMUNICATION; DERICHE R, 1993, P IEEE C COMP VIS PA, P14; Dreschler L. S., 1982, Proceedings of the 6th International Conference on Pattern Recognition, P542; FAUGERAS O, 1993, CR ACAD SCI I-MATH, V317, P565; GAGE M, 1986, J DIFFER GEOM, V23, P69; GRAYSON MA, 1987, J DIFFER GEOM, V26, P285; Hummel R. A., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P204; JULESZ B, 1981, NATURE, V290; Kitchen L, 1982, PATTERN RECOGN LETT, V1, P95, DOI 10.1016/0167-8655(82)90020-4; LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346; LOPEZ C, 1992, P TRENT C SURF TENS; MACKWORTH A, 1986, IEEE T PATTERN ANAL, V8; MACKWORTH A, 1992, IEEE PAMI, V14, P789; MARAGOS P, 1987, OPTICAL ENG, V26; Marr D., 1982, VISION; MATHERON G., 1975, RANDOM SETS INTEGRAL; MERRIMAN B, 1994, J COMPUT PHYS, V112, P334, DOI 10.1006/jcph.1994.1105; Merriman B., 1992, 9218 CAM U CAL DEP M; Morel J.M., 1994, ACTA NUMER, V3, P1, DOI 10.1017/S0962492900002415; MOREL JM, 1994, VARIATIONAL METHODS; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; PERONA P, 1987, P IEEE COMP SOC WORK; Rohr K., 1994, Journal of Mathematical Imaging and Vision, V4, P139, DOI 10.1007/BF01249893; SAPIRO G, 1993, INDIANA U MATH J, V42, P985, DOI 10.1512/iumj.1993.42.42046; SAPIRO G, 1993, INT J COMPUT VISION, V11, P25, DOI 10.1007/BF01420591; SAPIRO G, 1994, J FUNCT ANAL, V119, P79, DOI 10.1006/jfan.1994.1004; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; YUILLE A, 1986, IEEE T PATTERN ANAL, V8	41	27	27	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1997	25	2					95	107		10.1023/A:1007959616598	http://dx.doi.org/10.1023/A:1007959616598			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YG836					2022-12-18	WOS:A1997YG83600001
J	Jacobs, DW				Jacobs, DW			Matching 3-D models to 2-D images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OBJECT RECOGNITION; ASPECT GRAPHS; MOTION	We consider the problem of analytically characterizing the set of all 2-D images that a group of 3-D features may produce, and demonstrate that this is a useful thing to do. Our results apply for simple point features and point features with associated orientation vectors when we model projection as a 3-D to 2-D affine transformation. We show how to represent the set of images that a group of 3-D points produces with two lines (1-D subspaces), one in each of two orthogonal, high-dimensional spaces, where a single image group corresponds to one point in each space. The images of groups of oriented point features can be represented by a 2-D hyperbolic surface in a single high-dimensional space. The problem of matching an image to models is essentially reduced to the problem of matching a point to simple geometric structures. Moreover, we show that these are the simplest and lowest dimensional representations possible for these cases. We demonstrate the value of this way of approaching matching by applying our results to a variety of vision problems. In particular, we use this result to build a space-efficient indexing system that performs 3-D to 2-D matching by table lookup. This system is analytically built and accessed, accounts for the effects of sensing error, and is tested on real images. We also derive new results concerning the existence of invariants and non-accidental properties in this domain. Finally, we show that oriented points present unexpected difficulties: indexing requires fundamentally more space with oriented than with simple points, we must use more images in a motion sequence to determine the affine structure of oriented points, and the linear combinations result does not hold for oriented points.			Jacobs, DW (corresponding author), NEC CORP LTD,4 INDEPENDENCE WAY,PRINCETON,NJ 08540, USA.							[Anonymous], 1985, PERCEPTUAL ORG VISUA; BASRI R, 1988, COMPUTER GRAPHICS VI, V57, P331; BERGEVIN R, 1993, IEEE T PATTERN ANAL, V15, P19, DOI 10.1109/34.184772; BIEDERMAN I, 1985, COMPUT VISION GRAPH, V32, P29, DOI 10.1016/0734-189X(85)90002-7; Bowyer K. W., 1990, International Journal of Imaging Systems and Technology, V2, P315, DOI 10.1002/ima.1850020407; BREUEL T, 1990, COMMUNICATION; BREUEL T, 1990, 1108 MIT AI; BREUEL TM, 1993, 9308 IDIAP; Brian Burns J., 1992, GEOMETRIC INVARIANCE; BURNS J, 1992, IEEE C COMP VIS PATT, P328; BURNS JB, 1990, P DARPA IMAGE UNDERS, P650; CLEMENS D, 1991, TR1307 MIT AI; CLEMENS DT, 1991, IEEE T PATTERN ANAL, V13, P1007, DOI 10.1109/34.99235; COSTA M, 1990, 6TH P ISR C AI, P35; Cutting J.E., 1986, PERCEPTION EYE MOTIO, VVolume 1; CYGANSKI D, 1985, IEEE T PATTERN ANAL, V7, P662, DOI 10.1109/TPAMI.1985.4767722; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; GIGUS Z, 1991, IEEE T PATTERN ANAL, V13, P542, DOI 10.1109/34.87341; GRIM WEL, 1990, OBJECT RECOGNITION C; GRIMSON WEL, 1994, INT J COMPUT VISION, V13, P7, DOI 10.1007/BF01420793; HUTTENLOCHER DP, 1992, INT J COMPUT VISION, V8, P7, DOI 10.1007/BF00126398; JACOBS D, 1993, TR1416 MIT AI; JACOBS D, 1992, IEEE C COMP VIS PATT, P439; JACOBS D, IN PRESS IEEE T PATT; JACOBS D, 1993, IEEE C COMP VIS PATT, P226; JACOBS D, 1991, IEEE C COMP VIS PATT, P269; Jacobs D. W., 1994, Applications of Invariance in Computer Vision. Second Joint European - US Workshop Proceedings, P415; JACOBS DW, 1989, 1177 MIT AI; KALVIN A, 1986, INT J ROBOT RES, V5, P38, DOI 10.1177/027836498600500403; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; Korn G. A., 1968, MATH HDB SCI ENG; KRIEGMAN DJ, 1990, INT J COMPUT VISION, V5, P119, DOI 10.1007/BF00054918; LAMDAN Y, 1990, IEEE T ROBOTIC AUTOM, V6, P578, DOI 10.1109/70.62047; Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995; LAMDAN Y, 1991, IEEE C COMP VIS PATT, P22; MOSES Y, 1992, 2 EUR C COMP VIS, P820; Poggio T., 1990, 900503 IRST; RIGOUTSOS I, 1991, 8 ISR C ART INT COMP; Rothwell C. A., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P109, DOI 10.1109/CVPR.1992.223219; SCHWARTZ JT, 1987, INT J ROBOT RES, V6, P29, DOI 10.1177/027836498700600203; SHASHUA A, 1991, 1327 MIT AI; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; SYEDAMAMHOOD T, 1992, 2ND P EUR C COMP VIS, P115; THOMPSON DW, 1987, IEEE J ROBOTIC AUTOM, P208; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; Van Gool L., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P454, DOI 10.1109/CVPR.1991.139735; WALLACE AM, 1987, IMAGE VISION COMPUT, V5, P114, DOI 10.1016/0262-8856(87)90037-0; WALLACK A, 1994, IEEE C COMP VIS PATT, P259; Wayner P. C., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P473, DOI 10.1109/CVPR.1991.139738; WEINSHALL D, 1993, INT J COMPUT VISION, V10, P27, DOI 10.1007/BF01440845; WEISS I, 1988, DARPA IM UND WORKSH, P1125; [No title captured]	54	27	27	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1997	21	1-2					123	153		10.1023/A:1007927623619	http://dx.doi.org/10.1023/A:1007927623619			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WM797					2022-12-18	WOS:A1997WM79700006
J	Basri, R				Basri, R			Recognition by prototypes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OBJECT RECOGNITION; MODELS; IMAGE; REPRESENTATION; ALIGNMENT; SHAPE	A scheme for recognizing 3D objects from single 2D images under orthographic projection is introduced. The scheme proceeds in two stages. In the first stage, the categorization stage, the image is compared to prototype objects. For each prototype, the view that most resembles the image is recovered, and, if the view is found to be similar to the image, the class identity of the object is determined. In the second stage, the identification stage, the observed object is compared to the individual models of its class, where classes are expected to contain objects with relatively similar shapes. For each model, a view that matches the image is sought. If such a view is found, the object's specific identity is determined. The advantage of categorizing the object before it is identified is twofold. First, the image is compared to a smaller number of models, since only models that belong to the object's class need to be considered. Second, the cost of comparing the image to each model in a class is very low, because correspondence is computed once for the whole class. More specifically, the correspondence and object pose computed in the categorization stage to align the prototype with the image are reused in the identification stage to align the individual models with the image. As a result, identification is reduced to a series of simple template comparisons. The paper concludes with an algorithm for constructing optimal prototypes for classes of objects.			Basri, R (corresponding author), WEIZMANN INST SCI,DEPT APPL MATH,IL-76100 REHOVOT,ISRAEL.							BAJCSY R, 1987, 1ST P INT C COMP VIS, P231; BASRI R, 1993, CVGIP-IMAG UNDERSTAN, V57, P331, DOI 10.1006/ciun.1993.1022; BASRI R, 1995, IN PRESS INT J COMPU; BASRI R, 1993, HDB PATTERN RECOGNIT, V5, P863; BIEDERMAN I, 1985, COMPUT VISION GRAPH, V32, P29, DOI 10.1016/0734-189X(85)90002-7; BINFORD TO, 1971, IEEE C SYSTEMS CONTR; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; Chien C. H., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P481; CONNELL JH, 1987, ARTIF INTELL, V31, P159, DOI 10.1016/0004-3702(87)90018-X; DAVIS LS, 1979, IEEE T PATTERN ANAL, V1, P60, DOI 10.1109/TPAMI.1979.4766876; Duda R.O., 1973, J ROYAL STAT SOC SER; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; JACOBS DW, 1992, P IM UND WORKSH, P717; KOENDERINK JJ, 1982, PERCEPTION, V11, P129, DOI 10.1068/p110129; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; LAMDAU Y, 1987, RECOGNITION 3 D OBJE; LOWE DG, 1985, 202 COUR I MATH SCI; MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020; Mundy J., 1992, GEOMETRIC INVARIANCE; PENTLAND A, 1987, 1ST INT C COMP VIS, P612; Poggio T., 1990, 900503 IRST; POGGIO T, 1992, 1347 MIT AI; ROSCH E, 1976, COGNITIVE PSYCHOL, V8, P382, DOI 10.1016/0010-0285(76)90013-X; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519; Shapira Y., 1991, P IJCAI, P1257; STARK L, 1991, IEEE T PATTERN ANAL, V13, P992; THOMPSON DW, 1987, IEEE J ROBOTIC AUTOM, P208; TOMASI C, 1992, INT J COMPUTER VISIO, V9; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; ULLMAN S, 1989, COGNITION, V32, P193, DOI 10.1016/0010-0277(89)90036-X; VAINA LM, 1990, BIOL CYBERN, V62, P225, DOI 10.1007/BF00198097; WEINSHALL D, 1993, INT J COMPUT VISION, V10, P27, DOI 10.1007/BF01440845; WEISS I, 1988, P IMAGE UNDERSTANDIN, P1125; WINSTON PH, 1984, 679 MIT AI; [No title captured]; [No title captured]	41	27	27	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1996	19	2					147	167		10.1007/BF00055802	http://dx.doi.org/10.1007/BF00055802			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VE939					2022-12-18	WOS:A1996VE93900003
J	CHRISTENSEN, PH; SHAPIRO, LG				CHRISTENSEN, PH; SHAPIRO, LG			3-DIMENSIONAL SHAPE FROM COLOR PHOTOMETRIC STEREO	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							SPECULAR SURFACES; UNIQUENESS	Computer vision systems can be used to determine the shapes of real three-dimensional objects for purposes of object recognition and pose estimation or for CAD applications. One method that has been developed is photometric stereo. This method uses several images taken from the same viewpoint, but with different lightings, to determine the three-dimensional shape of an object. Most previous work in photometric stereo has been with gray-tone images; color images have only been used for dielectric materials. In this paper we describe a procedure for color photometric stereo, which recovers the shape of a colored object from two or more color images of the object under white illumination. This method can handle different types of materials, such as composites and metals, and can employ various reflection models such as the Lambertian, dichromatic, and Torrance-Sparrow models. For composite materials, colored metals, and dielectrics, there are two advantages of utilizing color information: at each pixel, there are more constraints on the orientation, and the result is less sensitive to noise. Consequently, the shape can be found more accurately. The method has been tested on both artificial and real images of objects of various materials, and on real images of a multi-colored object.			CHRISTENSEN, PH (corresponding author), UNIV WASHINGTON,DEPT COMP SCI & ENGN,SEATTLE,WA 98195, USA.							Beckmann Petr, 1987, SCATTERING ELECTROMA, P4; Berthold KP Horn, 1970, SHAPE SHADING METHOD, P1; BUITUONG P, 1975, COMMUN ACM, V18, P311; CHRISTENSEN PH, 1993, JUN P C COMP VIS PAT, P767; COLEMAN EN, 1982, COMPUT VISION GRAPH, V18, P309, DOI 10.1016/0146-664X(82)90001-6; DREW MS, 1992, CSSLCCR TR9207 S FRA; Horn B.K.P., 1989, SHAPE SHADING; HORN BKP, 1990, INT J COMPUT VISION, V5, P37, DOI 10.1007/BF00056771; IKEUCHI K, 1981, IEEE T PATTERN ANAL, V3, P661, DOI 10.1109/TPAMI.1981.4767167; KLINKER GJ, 1990, INT J COMPUT VISION, V4, P7, DOI 10.1007/BF00137441; KOZERA R, 1991, APPL MATH COMPUT, V44, P1, DOI 10.1016/0096-3003(91)90001-4; Lambert J.H., 1760, PHOTOMETRIA SIVE MEN; NAYAR SK, 1990, IEEE T ROBOTIC AUTOM, V6, P418, DOI 10.1109/70.59367; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; NAYAR SK, 1992, JAN DARPA IM UND WOR; NOVAK CL, 1992, JUN P C COMP VIS PAT, P599; OLIENSIS J, 1991, INT J COMPUT VISION, V6, P75, DOI 10.1007/BF00128151; ONN R, 1990, INT J COMPUT VISION, V5, P105, DOI 10.1007/BF00056773; Park J.-S., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P331, DOI 10.1109/ICPR.1990.118125; Rogers D.F., 1990, MATH ELEMENTS COMPUT, Vsecond; SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409; SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; WOLFF LB, 1987, JUN P INT C COMP VIS, P708; Woodham R.J., 1979, IMAGE UNDERSTANDING, V155, P136	26	27	32	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1994	13	2					213	227		10.1007/BF01427152	http://dx.doi.org/10.1007/BF01427152			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PM568					2022-12-18	WOS:A1994PM56800004
J	KROTKOV, E; BAJCSY, R				KROTKOV, E; BAJCSY, R			ACTIVE VISION FOR RELIABLE RANGING - COOPERATING FOCUS, STEREO, AND VERGENCE	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							ACCOMMODATION; UNCERTAINTY; SYSTEMS; DEPTH	This article addresses the problem of measuring reliabily the absolute three-dimensional position of objects in an unknown and cluttered scene. It circumvents the limitations of a single sensor or single algorithm by using several range recovery techniques together, so that they cooperate in visual behaviors similar to those exhibited by the human visual system. Implemented visual behaviors include (i) aperture adjustment to vary depth of field and contrast, (ii) focus ranging followed by fixation, (iii) stereo ranging followed by focus ranging, and (iv) focus ranging followed by disparity prediction followed by focus ranging. The main contribution is a demonstration that two particular visual ranging processes-focusing and stereo-can cooperate to improve measurement reliability. The results of 75 experiments processing close to 3000 different object points lying at distances between 1 and 3 meters demonstrate that the computed range values are highly reliable.	UNIV PENN,GRASP LAB,PHILADELPHIA,PA 19104	University of Pennsylvania	KROTKOV, E (corresponding author), CARNEGIE MELLON UNIV,INST ROBOT,5000 FORBES AVE,PITTSBURGH,PA 15213, USA.			Krotkov, Eric/0000-0001-9726-3920				Abbott A. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P532, DOI 10.1109/CCV.1988.590034; ALOIMONOS J, 1988, P IEEE, V76, P899, DOI 10.1109/5.5964; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; Ballard D. H., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P524, DOI 10.1109/CCV.1988.590033; BALLARD DH, 1989, P INT JOINT C ART IN, P1635; Bond A.H., 1988, READINGS DISTRIBUTED; CARDILLO J, 1991, IEEE T PATTERN ANAL, V13, P809, DOI 10.1109/34.85671; Clark J. J., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P514, DOI 10.1109/CCV.1988.590032; Coombs D. J., 1990, Proceedings. 5th IEEE International Symposium on Intelligent Control 1990 (Cat. No.90TH0333-5), P239, DOI 10.1109/ISIC.1990.128464; DAS S, 1990, DEC P IEEE INTERN C, P485; DAVIS R, 1983, ARTIF INTELL, V20, P63, DOI 10.1016/0004-3702(83)90015-2; Durfee E. H., 1989, IEEE Transactions on Knowledge and Data Engineering, V1, P63, DOI 10.1109/69.43404; ENS JE, 1990, THESIS U BRIT COLUMB; FINCHAM EF, 1957, J PHYSIOL-LONDON, V137, P488, DOI 10.1113/jphysiol.1957.sp005829; GEIGER D, 1987, 1ST P INT C COMP VIS, P306; GOGEL WC, 1961, J PSYCHOL, V52, P287, DOI 10.1080/00223980.1961.9916529; GRAHAM C, 1965, VISUAL SPACE PERCEPT; GRIMSON WEL, 1981, IMAGES SURFACES; GROSSMANN P, 1987, PATTERN RECOGN LETT, V5, P63, DOI 10.1016/0167-8655(87)90026-2; HORN B, 1968, AI160 MEMO; JARVIS RA, 1976, MICROSCOPE, V24, P163; KERSTEN D, 1983, J OPT SOC AM, V73, P332, DOI 10.1364/JOSA.73.000332; KRISHNAN V, 1977, IEEE T BIOMED ENG, P24; KROTKOV E, 1988, IEEE T ROBOTIC AUTOM, V4, P108, DOI 10.1109/56.782; KROTKOV E, 1987, INT J COMPUT VISION, V1, P223, DOI 10.1007/BF00127822; KROTKOV E, 1990, IEEE T PATTERN ANAL, V12, P1200, DOI 10.1109/34.62610; Krotkov E., 1988, Proceedings of the 1988 IEEE International Conference on Robotics and Automation (Cat. No.88CH2555-1), P548, DOI 10.1109/ROBOT.1988.12109; Krotkov EP, 1989, ACTIVE COMPUTER VISI; LESSER VR, 1981, IEEE T SYST MAN CYB, V11, P81, DOI 10.1109/TSMC.1981.4308581; LESSER VR, 1983, AI MAG, V4, P15; MILES FA, 1987, J NEUROSCI, V7, P2576; NAYAR S, 1990, MAY P IEEE INT C ROB, P218; OLSON TJ, 1991, INT J COMPUT VISION, V7, P67, DOI 10.1007/BF00130490; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; SCHOR CM, 1979, VISION RES, V19, P1359, DOI 10.1016/0042-6989(79)90208-6; SHMUEL A, 1990, 10TH P INT C PATT RE, P48; SMITH RG, 1981, IEEE T SYST MAN CYB, V11, P61, DOI 10.1109/TSMC.1981.4308579; SPERLING G, 1970, AM J PSYCHOL, V83, P461, DOI 10.2307/1420686; Subbarao M., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P149, DOI 10.1109/CCV.1988.589986; SWAIN J, 1991, CS9127 U CHIC TECH R; Tenenbaum, 1970, ACCOMMODATION COMPUT; WESTHEIMER G, 1976, EYE MOVEMENT PSYCHOL, P55; WHAITE P, 1991, IEEE T PATTERN ANAL, V13, P1038, DOI 10.1109/34.99237; ZHANG CQ, 1992, ARTIF INTELL, V56, P21, DOI 10.1016/0004-3702(92)90064-5	44	27	42	1	9	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1993	11	2					187	203		10.1007/BF01469228	http://dx.doi.org/10.1007/BF01469228			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MB329					2022-12-18	WOS:A1993MB32900006
J	Park, J; Woo, S; Lee, JY; Kweon, IS				Park, Jongchan; Woo, Sanghyun; Lee, Joon-Young; Kweon, In So			A Simple and Light-Weight Attention Module for Convolutional Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Attention mechanism; Deep learning; Convolutional Neural Networks; Image Recognition; Self-attention		Many aspects of deep neural networks, such as depth, width, or cardinality, have been studied to strengthen the representational power. In this work, we study the effect of attention in convolutional neural networks and present our idea in a simple self-contained module, called Bottleneck Attention Module (BAM). Given an intermediate feature map, BAM efficiently produces the attention map along two factorized axes, channel and spatial, with negligible overheads. BAM is placed at bottlenecks of various models where the downsampling of feature maps occurs, and is jointly trained in an end-to-end manner. Ablation studies and extensive experiments are conducted in CIFAR-100/ImageNet classification, VOC2007/MS-COCO detection, super resolution and scene parsing with various architectures including mobile-oriented networks. BAM shows consistent improvements over all experiments, demonstrating the wide applicability of BAM. The code and models are available at .	[Park, Jongchan] Lunit, 175 Yeoksam Ro, Seoul, South Korea; [Woo, Sanghyun; Kweon, In So] Korea Adv Inst Sci & Technol, 291 Daehak Ro, Daejeon, South Korea; [Lee, Joon-Young] Adobe Res, 345 Pk Ave, San Jose, CA 95110 USA	Korea Advanced Institute of Science & Technology (KAIST); Adobe Systems Inc.	Park, J (corresponding author), Lunit, 175 Yeoksam Ro, Seoul, South Korea.	jcpark@lunit.io; shwoo93@kaist.ac.kr; jolee@adobe.com; iskweon77@kaist.ac.kr		Park, Jongchan/0000-0001-9808-6823				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; CHEN LC, 2016, ARXIV 1606 00915 CS, V1606, P915, DOI DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Chollet F., 2017, P COMP VIS PATT REC; Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; De Brabandere B, 2016, ADV NEUR IN, V29; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; HIRSCH J, 1989, VISION RES, V29, P1095, DOI 10.1016/0042-6989(89)90058-8; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu J., 2018, P COMP VIS PATT REC; Hu J., 2018, ADV NEURAL INFORM PR, P9422; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308; Iandola F.N., 2016, ARXIV; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jaderberg M., 2015, P NEUR INF PROC SYST; Jin XX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901353; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle Hugo, 2010, ADV NEURAL INFORM PR, V23; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mao HZ, 2017, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2017.241; Marr D, 1982, COMPUTATIONAL INVEST, V1; Mnih V., 2014, P NEUR INF PROC SYST; Morcos Ari S, 2018, ARXIV180306959; Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Woo S., 2018, P WINT C APPL COMP V; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xiao Tete, 2018, P EUR C COMP VIS ECC; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yu F., 2016, P ICLR 2016; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zeiler Matthew D, 2012, ARXIV12125701; Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128; Zhou B., 2018, SEMANTIC SEGMENTATIO; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zhu YS, 2017, IEEE I CONF COMP VIS, P4146, DOI 10.1109/ICCV.2017.444	62	26	29	12	38	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		783	798		10.1007/s11263-019-01283-0	http://dx.doi.org/10.1007/s11263-019-01283-0		JAN 2020	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN					2022-12-18	WOS:000515719200002
J	Chang, FJ; Tran, AT; Hassner, T; Masi, I; Nevatia, R; Medioni, G				Chang, Feng-Ju; Anh Tuan Tran; Hassner, Tal; Masi, Iacopo; Nevatia, Ram; Medioni, Gerard			Deep, Landmark-Free FAME: Face Alignment, Modeling, and Expression Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D face modeling; Face alignment; Facial expression estimation; Facial landmark detection		We present a novel method for modeling 3D face shape, viewpoint, and expression from a single, unconstrained photo. Our method uses three deep convolutional neural networks to estimate each of these components separately. Importantly, unlike others, our method does not use facial landmark detection at test time; instead, it estimates these properties directly from image intensities. In fact, rather than using detectors, we show how accurate landmarks can be obtained as a by-product of our modeling process. We rigorously test our proposed method. To this end, we raise a number of concerns with existing practices used in evaluating face landmark detection methods. In response to these concerns, we propose novel paradigms for testing the effectiveness of rigid and non-rigid face alignment methods without relying on landmark detection benchmarks. We evaluate rigid face alignment by measuring its effects on face recognition accuracy on the challenging IJB-A and IJB-B benchmarks. Non-rigid, expression estimation is tested on the CK+ and EmotiW'17 benchmarks for emotion classification. We do, however, report the accuracy of our approach as a landmark detector for 3D landmarks on AFLW2000-3D and 2D landmarks on 300W and AFLW-PIFA. A surprising conclusion of these results is that better landmark detection accuracy does not necessarily translate to better face processing. Parts of this paper were previously published by Tran et al. (2017) and Chang et al. (2017, 2018).	[Masi, Iacopo] USC, ISI, Marina Del Rey, CA USA; [Chang, Feng-Ju; Anh Tuan Tran; Nevatia, Ram; Medioni, Gerard] USC, Inst Robot & Intelligent Syst, Los Angeles, CA USA; [Hassner, Tal] Open Univ Israel, Raanana, Israel	University of Southern California; University of Southern California; Open University Israel	Hassner, T (corresponding author), Open Univ Israel, Raanana, Israel.	fengjuch@usc.edu; anhttran@usc.edu; talhassner@gmail.com; iacopo@isi.edu; nevatia@usc.edu; medioni@usc.edu	Tran, Anh Tuan/GZB-0391-2022	Chang, Feng-Ju/0000-0003-2405-3118	Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) [2014-14071600011]	Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA)	This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA 2014-14071600011. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints forGovernmental purpose notwithstanding any copyright annotation thereon.	Artizzu X. P., 2013, P INT C COMP VIS; Asthana A., 2014, P C COMP VIS PATT RE; Baltruaitis T., 2016, WINT C APPLL COMP VI; Baltrusaitis T., 2013, P C COMP VIS PATT RE; Bansal A., 2016, P C COMP VIS PATT RE; Bas A., 2016, ACCV WORKSH; Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23; Bhagavatula C., 2017, P INT C COMP VIS; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Blanz V, 2004, COMPUT GRAPH FORUM, V23, P669, DOI 10.1111/j.1467-8659.2004.00799.x; Blanz V., 1999, P ACM SIGGRAPH C COM; Blanz V., 2002, INT C AUT FAC GEST R; Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580; Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Chang FJ, 2017, IEEE INT CONF COMP V, P1599, DOI 10.1109/ICCVW.2017.188; Chang FJ, 2018, IEEE INT CONF AUTOMA, P122, DOI 10.1109/FG.2018.00027; Chu B, 2014, PROC CVPR IEEE, P1907, DOI 10.1109/CVPR.2014.245; Crosswhite N, 2017, IEEE INT CONF AUTOMA, P1, DOI 10.1109/FG.2017.11; Dantone M., 2012, P C COMP VIS PATT RE; Dhall A., 2017, ACM ICMI; Dhall A., 2015, ACM ICMI; Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26; Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047; Dong XY, 2019, IEEE T PATTERN ANAL, V41, P1641, DOI 10.1109/TPAMI.2018.2844853; Dou P., 2017, PROCEEDINGS OF CONFE; Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646; Everingham M., 2006, BMVC, DOI DOI 10.5244/C.20.92; Fabian Benitez-Quiroz C., 2016, P C COMP VIS PATT RE; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hassner T., 2016, P C COMP VIS PATT RE; Hassner T., 2015, P IEEE C COMP VIS PA; Hassner T, 2013, P INT C COMP VIS; Hassner T., 2006, 2006 C COMP VIS PATT, P15, DOI DOI 10.1109/CVPRW.2006.76; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang G. B., 2007, P INT C COMP VIS; Huber P., 2016, VISAPP; Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117; Jeni L. A., 2015, INT C AUT FAC GEST R; Jourabloo A., 2015, P C COMP VIS PATT RE; Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63; King DE, 2009, J MACH LEARN RES, V10, P1755; Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803; Kostinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Kosti R., 2017, P C COMP VIS PATT RE; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar A., 2018, P C COMP VIS PATT RE; Kumar A., 2017, AUTOMATIC FACE GESTU; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Levi G., 2015, ACM ICMI; Li C., 2014, EUR C COMP VIS; Liu Y., 2017, P C COMP VIS PATT RE; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lucey P., 2010, P C COMP VIS PATT RE; Masi I., 2016, P C COMP VIS PATT RE; Masi I, 2019, IEEE T PATTERN ANAL, V41, P379, DOI 10.1109/TPAMI.2018.2792452; Masi I, 2018, SIBGRAPI, P471, DOI 10.1109/SIBGRAPI.2018.00067; Masi I, 2017, IEEE INT CONF AUTOMA, P604, DOI 10.1109/FG.2017.76; Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35; Masi I, 2014, INT C PATT RECOG, P4477, DOI 10.1109/ICPR.2014.766; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Poirson P., 2016, 3DV; Ranjan R., 2017, ARXIV PREPRINT ARXIV; Ren S., 2014, P C COMP VIS PATT RE; Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589; Richardson E, 2016, INT CONF 3D VISION, P460, DOI 10.1109/3DV.2016.56; Romdhani S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P59; Romdhani S., 2005, P C COMP VIS PATT RE; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175; Sengupta S., 2018, P C COMP VIS PATT RE; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Su H., 2015, P INT C COMP VIS; Surace L., 2017, ACM ICMI; Tang H., 2008, INT C MULT EXP; Tewari A., 2018, P C COMP VIS PATT RE; Tran A. T., 2018, P C COMP VIS PATT RE; Tran AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163; Vetter T., 1998, EUR C COMP VIS; Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87; Wolf L., 2011, P C COMP VIS PATT RE; Wu Y, 2018, IEEE T PATTERN ANAL, V40, P3067, DOI 10.1109/TPAMI.2017.2787130; Xiang Y., 2014, WINT C APPL COMP VIS; Xiang Y, 2016, LECT NOTES COMPUT SC, V9912, P160, DOI 10.1007/978-3-319-46484-8_10; Xie LX, 2016, PROC CVPR IEEE, P4753, DOI 10.1109/CVPR.2016.514; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; YANG F, 2011, ACM T GRAPHIC, V30, P1; Yang Z., 2016, ICPR; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yu X, 2013, IEEE I CONF COMP VIS, P1944, DOI 10.1109/ICCV.2013.244; Zadeh Amir, 2016, ARXIV161108657; Zafeiriou S., 2016, PROC IEEE C COMPUT V, P36, DOI [10.1109/CVPRW.2010.5543148, DOI 10.1109/CVPRW.2010.5543148]; Zafeiriou S., 2017, P INT C COMP VIS WOR; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhang K, 2016, IEEE C COMPUTER VISI, P34; Zhu S., 2016, P C COMP VIS PATT RE; Zhu S., 2015, P C COMP VIS PATT RE; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23; Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679	103	26	28	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		930	956		10.1007/s11263-019-01151-x	http://dx.doi.org/10.1007/s11263-019-01151-x			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD					2022-12-18	WOS:000468525900021
J	Rao, YM; Lu, J; Zhou, J				Rao, Yongming; Lu, Jiwen; Zhou, Jie			Learning Discriminative Aggregation Network for Video-Based Face Recognition and Person Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face recognition; Person re-identification; Metric learning; Adversarial learning; Video-based recognition		In this paper, we propose a discriminative aggregation network method for video-based face recognition and person re-identification, which aims to integrate information from video frames for feature representation effectively and efficiently. Unlike existing video aggregation methods, our method aggregates raw video frames directly instead of the features obtained by complex processing. By combining the idea of metric learning and adversarial learning, we learn an aggregation network to generate more discriminative images compared to the raw input frames. Our framework reduces the number of image frames per video to be processed and significantly speeds up the recognition procedure. Furthermore, low-quality frames containing misleading information can be well filtered and denoised during the aggregation procedure, which makes our method more robust and discriminative. Experimental results on several widely used datasets show that our method can generate discriminative images from video clips and improve the overall recognition performance in both the speed and the accuracy for video-based face recognition and person re-identification.	[Rao, Yongming; Lu, Jiwen; Zhou, Jie] Tsinghua Univ, Dept Automat, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China	Tsinghua University	Lu, J (corresponding author), Tsinghua Univ, Dept Automat, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.	rym18@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn; jzhou@tsinghua.edu.cn	Rao, Yongming/U-8310-2019; Lu, Jiwen/C-5291-2009	Rao, Yongming/0000-0003-3952-8753; Lu, Jiwen/0000-0002-6121-5529	National Key Research and Development Program of China [2017YFA0700802]; National Natural Science Foundation of China [61822603, 61672306, U1713214, 61572271]; Shenzhen Fundamental Research Fund (Subject Arrangement) [JCYJ20170412170602564]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Fundamental Research Fund (Subject Arrangement)	This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under Grant 61822603, Grant 61672306, Grant U1713214, Grant 61572271, and in part by the Shenzhen Fundamental Research Fund (Subject Arrangement) under Grant JCYJ20170412170602564.	Baltieri D., 2011, P 2011 JOINT ACMWORK, P59, DOI DOI 10.1145/2072572.2072590; Beveridge J. R., 2013, P 6 INT C BIOMETRICS, P1; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965; Chen J, 2015, IEEE ICC, P1801, DOI 10.1109/ICC.2015.7248586; Chen J, 2016, MOB INF SYST, V2016, DOI 10.1155/2016/5894752; Chen Y.-C., 2012, DICT BASED FACE RECO, P766, DOI 10.1007/978-3-642-33783-3_55; Ding CX, 2018, IEEE T PATTERN ANAL, V40, P1002, DOI 10.1109/TPAMI.2017.2700390; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; FELZENSZWALB PF, 2010, PROC CVPR IEEE, P2241, DOI DOI 10.1109/CVPR.2010.5539906; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gray D., 2007, IEEE INT WORKSH PERF, V3, P1; Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197; Hassner T., 2016, P IEEE C COMP VIS PA, P59; Hayat M, 2015, IEEE T PATTERN ANAL, V37, P713, DOI 10.1109/TPAMI.2014.2353635; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hermans Alexander, 2017, ARXIV170307737; Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Hu Y, 2011, PROC CVPR IEEE, P121, DOI 10.1109/CVPR.2011.5995500; Huang Gary B., 2007, 0749 U MASS, P7; Huang Z., 2016, ARXIV160804233; Huang Z., 2016, ARXIV161105742; Huang ZW, 2015, PR MACH LEARN RES, V37, P720; Huang ZW, 2014, PROC CVPR IEEE, P1677, DOI 10.1109/CVPR.2014.217; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jia Y., 2014, P 22 ACM INT C MULT, P675; Jiang HQ, 2014, 2014 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P252, DOI [10.1109/ICDH.2014.55, 10.1109/APCCAS.2014.7032767]; Kawanishi Y., 2014, SHINPUHKAN 2014 MULT; Kim M, 2008, PROC CVPR IEEE, P1787; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; KLARE BF, 2015, PROC CVPR IEEE, P1931, DOI DOI 10.1109/CVPR.2015.7298803; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li H., 2014, AS C COMP VIS ACCV; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461; Lin J, 2017, PROC CVPR IEEE, P3396, DOI 10.1109/CVPR.2017.362; Liu YZ, 2017, DATA, V2, DOI 10.3390/data2010008; Lu JW, 2016, IEEE T CIRC SYST VID, V26, P529, DOI 10.1109/TCSVT.2015.2412831; Lu JW, 2015, PROC CVPR IEEE, P1137, DOI 10.1109/CVPR.2015.7298717; Lu JW, 2013, IEEE I CONF COMP VIS, P329, DOI 10.1109/ICCV.2013.48; Parkhi Omkar M., 2015, BRIT MACH VIS C; Paszke Adam, 2017, PYTORCH TENSORS DYNA, P6; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rao YM, 2017, IEEE I CONF COMP VIS, P3801, DOI 10.1109/ICCV.2017.408; Reed W, 2016, RES POLITICS, V3, DOI 10.1177/2053168016666848; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Sohn Kihyuk, 2017, P IEEE INT C COMP VI, P3210; Sun G., 2018, P IEEE CVF C COMP VI, P7132, DOI DOI 10.1109/CVPR.2018.00745; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tesfaye Y. T., 2017, ARXIV170606196; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang J, 2014, IEEE T CYBERNETICS, V44, P2368, DOI 10.1109/TCYB.2014.2307067; Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965; Wang RP, 2009, PROC CVPR IEEE, P429, DOI 10.1109/CVPRW.2009.5206850; Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87; Wolf L., 2011, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2011.5995566; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Yang J., 2016, ARXIV160305474; Yang M., 2016, IVC; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang W., 2017, ARXIV170206294; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng Wei-Shi, 2009, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.23.23; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717	82	26	29	2	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		701	718		10.1007/s11263-018-1135-x	http://dx.doi.org/10.1007/s11263-018-1135-x			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD					2022-12-18	WOS:000468525900010
J	Fermuller, C; Wang, F; Yang, YZ; Zampogiannis, K; Zhang, Y; Barranco, F; Pfeiffer, M				Fermueller, Cornelia; Wang, Fang; Yang, Yezhou; Zampogiannis, Konstantinos; Zhang, Yi; Barranco, Francisco; Pfeiffer, Michael			Prediction of Manipulation Actions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Online action recognition; Hand motions; Forces on the hand; Action prediction	REAL-TIME; RECOGNITION; OBJECT; GRASP; IMITATION; TASK	By looking at a person's hands, one can often tell what the person is going to do next, how his/her hands are moving and where they will be, because an actor's intentions shape his/her movement kinematics during action execution. Similarly, active systems with real-time constraints must not simply rely on passive video-segment classification, but they have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded videos of subjects performing different manipulation actions on the same object, such as "squeezing", "flipping", "washing", "wiping" and "scratching" with a sponge. In psychophysical experiments, we evaluated human observers' skills in predicting actions from video sequences of different length, depicting the hand movement in the preparation and execution of actions before and after contact with the object. We then developed a recurrent neural network based method for action prediction using as input image patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams. Evaluations on two new datasets show that our system closely matches human performance in the recognition task, and demonstrate the ability of our algorithms to predict in real time what and how a dexterous action is performed.	[Fermueller, Cornelia; Zampogiannis, Konstantinos; Zhang, Yi] Univ Maryland, College Pk, MD 20742 USA; [Wang, Fang] Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT, Australia; [Barranco, Francisco] Univ Granada, Granada, Spain; [Pfeiffer, Michael] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland; [Pfeiffer, Michael] Bosch Ctr Artificial Intelligence Res, D-71272 Renningen, Germany; [Yang, Yezhou] Arizona State Univ, Tempe, AZ 85281 USA	University System of Maryland; University of Maryland College Park; Australian National University; University of Granada; University of Zurich; Arizona State University; Arizona State University-Tempe	Fermuller, C (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	fer@cfar.umd.edu	Barranco, Francisco/B-7314-2012	Barranco, Francisco/0000-0002-3721-0170	National Science Foundation [SMA 1540917, CNS 1544797]; Samsung under the GRO Program [20477, 355022]; DARPA through U.S. Army Grant [W911NF-14-1-0384]	National Science Foundation(National Science Foundation (NSF)); Samsung under the GRO Program(Samsung); DARPA through U.S. Army Grant	This work was funded by the support of the National Science Foundation under Grant SMA 1540917 and Grant CNS 1544797, by Samsung under the GRO Program (Nos. 20477, 355022), and by DARPA through U.S. Army Grant W911NF-14-1-0384.	Aloimonos Y., 2015, IMAGE VISION COMPUT, V35, P2891; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ansuini C, 2008, EXP BRAIN RES, V185, P111, DOI 10.1007/s00221-007-1136-4; Ansuini C, 2015, NEUROSCIENTIST, V21, P126, DOI 10.1177/1073858414533827; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Aviles AI, 2014, INT CONF IMAG PROC, P111; BRADSKI GR, 1998, INTEL TECHNOLOGY J Q; Bullock IM, 2015, INT J ROBOT RES, V34, P251, DOI 10.1177/0278364914555720; Cai MJ, 2015, IEEE INT CONF ROBOT, P1360, DOI 10.1109/ICRA.2015.7139367; Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761; Craje C, 2011, EXP BRAIN RES, V212, P119, DOI 10.1007/s00221-011-2704-1; CUTKOSKY MR, 1989, IEEE T ROBOTIC AUTOM, V5, P269, DOI 10.1109/70.34763; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Doyle JC, 2011, P NATL ACAD SCI USA, V108, P15624, DOI 10.1073/pnas.1103557108; Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012; Fanello SR, 2013, J MACH LEARN RES, V14, P2617; Fathi A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3281, DOI 10.1109/CVPR.2011.5995444; Feix T., 2009, ROB SCI SYST C WORKS; Fermuller C., 2016, PREDICTION MANIPULAT; Fouhey DF, 2014, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2014.260; Fragkiadaki K., 2015, ARXIV150800271; Gallese V, 1998, TRENDS COGN SCI, V2, P493, DOI 10.1016/S1364-6613(98)01262-5; Gams A., 2010, IEEE INT C ROB RES I, P3192; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Greminger MA, 2004, IEEE T PATTERN ANAL, V26, P290, DOI 10.1109/TPAMI.2004.1262305; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96; Ijina E., 2014, INT C MACH LEARN APP; JEANNEROD M, 1984, J MOTOR BEHAV, V16, P235; Joo J, 2014, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2014.35; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Keskin Cem, 2013, CONSUMER DEPTH CAMER, p119?137; Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15; Kober J., 2000, IEEE C COMP VIS PATT, V2, P142; Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335; Kormushev P, 2011, ADV ROBOTICS, V25, P581, DOI 10.1163/016918611X558261; Lea C, 2016, LECT NOTES COMPUT SC, V9907, P36, DOI 10.1007/978-3-319-46487-9_3; Lenz I, 2015, INT J ROBOT RES, V34, P705, DOI 10.1177/0278364914549607; Li Y, 2010, PROC CVPR IEEE, P2630, DOI 10.1109/CVPR.2010.5539977; Liu J., 2014, 14 IEEE RAS INT C HU; Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359; Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214; Mandery C, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P329, DOI 10.1109/ICAR.2015.7251476; Melax S., 2013, GRAPHICS INTERFACE 2, P63, DOI DOI 10.1145/2448196.2448232; Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Molchanov Pavlo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301342; Ng J. Y. - H., 2015, CVPR 2015; Ohn-Bar E, 2014, IEEE T INTELL TRANSP, V15, P2368, DOI 10.1109/TITS.2014.2337331; Oikonomidis I, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.101; Panteleris P., 2015, P BRIT MACH VIS, P123; Pham T. H., 2015, IEEE C COMP VIS PATT; Pieropan A, 2013, IEEE INT CONF ROBOT, P1282, DOI 10.1109/ICRA.2013.6630736; Pirsiavash Hamed, 2014, TECHNICAL REPORT; Rizzolatti G, 2001, NAT REV NEUROSCI, V2, P661, DOI 10.1038/35090060; Rogez G., 2015, IEEE INT C COMP VIS; Rogez Gregory, 2014, WORKSH EUR C COMP VI, P356; Romero J, 2013, IEEE T ROBOT, V29, P1342, DOI 10.1109/TRO.2013.2272249; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Ryoo MS, 2015, ACMIEEE INT CONF HUM, P295, DOI 10.1145/2696454.2696462; Ryoo MS, 2013, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2013.352; Ryoo M. S., 2011, ICCV; Saxena A, 2008, INT J ROBOT RES, V27, P157, DOI 10.1177/0278364907087172; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shi QF, 2011, INT J COMPUT VISION, V93, P22, DOI 10.1007/s11263-010-0384-0; Shimoga KB, 1996, INT J ROBOT RES, V15, P230, DOI 10.1177/027836499601500302; Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381; Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811; Stein S, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P729, DOI 10.1145/2493432.2493482; Supancic JS, 2015, IEEE I CONF COMP VIS, P1868, DOI 10.1109/ICCV.2015.217; Takano W, 2015, ADV ROBOTICS, V29, P771, DOI 10.1080/01691864.2014.996604; Tiest W.M.B., 2014, MULTISENSORY SOFTNES, P3, DOI [10.1007/978-1-4471-6533-0_1, DOI 10.1007/978-1-4471-6533-0_1]; Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594; Venugopalan Subhashini, 2014, ARXIV14124729; Visin F., 2015, ARXIV150500393; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Walker J, 2014, PROC CVPR IEEE, P3302, DOI 10.1109/CVPR.2014.416; Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198; Wang S. B., 2006, PROC IEEE COMPUT SOC, P1521, DOI DOI 10.1109/CVPR.2006.132; Xie D, 2013, IEEE I CONF COMP VIS, P2224, DOI 10.1109/ICCV.2013.277; Yang Yezhou, 2015, IEEE C COMP VIS PATT; Zhu YX, 2015, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2015.7298903	82	26	26	1	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		358	374		10.1007/s11263-017-0992-z	http://dx.doi.org/10.1007/s11263-017-0992-z			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA		Green Submitted			2022-12-18	WOS:000425619100012
J	Ryoo, MS; Matthies, L				Ryoo, M. S.; Matthies, Larry			First-Person Activity Recognition: Feature, Temporal Structure, and Prediction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CLASSIFICATION	This paper discusses the problem of recognizing interaction-level human activities from a first-person viewpoint. The goal is to enable an observer (e.g., a robot or a wearable camera) to understand 'what activity others are performing to it' from continuous video inputs. These include friendly interactions such as 'a person hugging the observer' as well as hostile interactions like 'punching the observer' or 'throwing objects at the observer', whose videos involve a large amount of camera ego-motion caused by physical interactions. The paper investigates multi-channel kernels to integrate global and local motion information, and presents a new activity learning/recognition methodology that explicitly considers temporal structures displayed in first-person activity videos. Furthermore, we present a novel algorithm for early recognition (i.e., prediction) of activities from first-person videos, which allows us to infer ongoing activities at their early stage. In our experiments, we not only show classification results with segmented videos, but also confirm that our new approach is able to detect activities from continuous videos and perform early recognition reliably.	[Ryoo, M. S.; Matthies, Larry] CALTECH, Jet Prop Lab, 4800 Oak Grove Dr, Pasadena, CA 91125 USA	California Institute of Technology; National Aeronautics & Space Administration (NASA); NASA Jet Propulsion Laboratory (JPL)	Ryoo, MS (corresponding author), CALTECH, Jet Prop Lab, 4800 Oak Grove Dr, Pasadena, CA 91125 USA.	mryoo@jpl.nasa.gov			National Aeronautics and Space Administration; Army Research Laboratory	National Aeronautics and Space Administration(National Aeronautics & Space Administration (NASA)); Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL))	The research described in this paper was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration. This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-2-0016.	Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653; Choi J., 2008, ACM MIR; Dollar P, 2005, IEEE WORKSH VS PETS; Fathi A., 2011, ICCV; Fathi Alircza, 2012, CVPR; Hoai M., 2012, CVPR; Iwashita Y., 2014, ICPR; Kitani K. M., 2011, CVPR; Kitani K. M, 2012, ECCV; Koppula H. S., 2013, ROBOTICS SCI SYSTEMS; Lan T., 2012, CVPR; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laptev I., 2008, CVPR; Lazebnik S., 2006, P 2006 IEEE COMP VIS, P2169; Lee S., 2014, CVPRW; Lee Y. J., 2012, CVPR; Li Y., 2013, ICCV; Niebles J., 2010, ECCV; Pirsiavash H., 2012, CVPR; Ryoo M., 2009, ICCV; Ryoo MS, 2011, INT J COMPUT VISION, V93, P183, DOI 10.1007/s11263-010-0355-5; Ryoo M. S., 2013, CVPR; Ryoo M. S., 2011, ICCV; Sadanand S., 2012, P 2012 IEEE C COMP V; Schuldt C, 2004, ICPR; Shawe-Taylor N., 2002, NIPS; Shotton J., 2011, CVPR; Si Z., 2011, ICCV; Spriggs E. H., 2009, IEEE WORKSH EG VIS C; Wu TF, 2004, J MACH LEARN RES, V5, P975; Xia Lu, 2012, CVPRW; Xie D., 2013, ICCV; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4	33	26	26	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	3			SI		307	328		10.1007/s11263-015-0847-4	http://dx.doi.org/10.1007/s11263-015-0847-4			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FE					2022-12-18	WOS:000380270000007
J	Choi, W; Chao, YW; Pantofaru, C; Savarese, S				Choi, Wongun; Chao, Yu-Wei; Pantofaru, Caroline; Savarese, Silvio			Indoor Scene Understanding with Geometric and Semantic Contexts	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene understanding; Scene parsing; Object recognition; 3D layout		Truly understanding a scene involves integrating information at multiple levels as well as studying the interactions between scene elements. Individual object detectors, layout estimators and scene classifiers are powerful but ultimately confounded by complicated real-world scenes with high variability, different viewpoints and occlusions. We propose a method that can automatically learn the interactions among scene elements and apply them to the holistic understanding of indoor scenes from a single image. This interpretation is performed within a hierarchical interaction model which describes an image by a parse graph, thereby fusing together object detection, layout estimation and scene classification. At the root of the parse graph is the scene type and layout while the leaves are the individual detections of objects. In between is the core of the system, our 3D Geometric Phrases (3DGP). We conduct extensive experimental evaluations on single image 3D scene understanding using both 2D and 3D metrics. The results demonstrate that our model with 3DGPs can provide robust estimation of scene type, 3D space, and 3D objects by leveraging the contextual relationships among the visual elements.	[Choi, Wongun] NEC Labs Amer, Cupertino, CA 95014 USA; [Chao, Yu-Wei] Univ Michigan, Ann Arbor, MI 48109 USA; [Pantofaru, Caroline] Google Inc, Mountain View, CA USA; [Savarese, Silvio] Stanford Univ, Stanford, CA 94305 USA	NEC Corporation; University of Michigan System; University of Michigan; Google Incorporated; Stanford University	Choi, W (corresponding author), NEC Labs Amer, Cupertino, CA 95014 USA.	wongun@nec-labs.com; ywchao@umich.edu; cpantofaru@google.com; ssilvio@stanford.edu						Bao S., 2010, P C COMP VIS PATT RE; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chao Y.W., 2013, P INT C IM AN PROC; Choi Wongun, 2013, CVPR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Desai C, 2011, INT J COMPUT VISION, V95, P1, DOI 10.1007/s11263-011-0439-x; Everingham Mark, 2010, IJCV; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Fouhey DF, 2012, LECT NOTES COMPUT SC, V7576, P732, DOI 10.1007/978-3-642-33715-4_53; Geiger A., 2011, ADV NEURAL INFORM PR, V24; Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hedau V, 2010, ECCV; Hedau V., 2009, INT C COMP VIS; Hedau V, 2012, PROC CVPR IEEE, P2807; Hoiem D., 2008, IJCV; Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y; jia Li L., 2010, NIPS, DOI [10.1184/R1/6475985.v1, DOI 10.1184/R1/6475985.V1]; Lagarias J. C., 1998, SIAM J OPTIMIZ, V9, P148; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872; Lee David Changsoo, 2010, NIPS; Leibe B., 2004, STAT LEARNING COMPUT; Li C., 2012, CVPR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383; Pero L. D., 2012, CVPR; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Rother C, 2002, IMAGE VISION COMPUT, V20, P647, DOI 10.1016/S0262-8856(02)00054-9; Sadeghi A., 2011, CVPR; Satkin S, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.128; Schwing AG, 2012, LECT NOTES COMPUT SC, V7577, P299, DOI 10.1007/978-3-642-33783-3_22; Wang H., 2010, ECCV; Wang Y., 2011, PAMI; Xiang Y., 2012, CVPR; Zhao Y., 2011, ADV NEURAL INFORM PR, P73	36	26	26	3	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2015	112	2			SI		204	220		10.1007/s11263-014-0779-4	http://dx.doi.org/10.1007/s11263-014-0779-4			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CE0TD					2022-12-18	WOS:000351518500006
J	Perea, JA; Carlsson, G				Perea, Jose A.; Carlsson, Gunnar			A Klein-Bottle-Based Dictionary for Texture Representation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Texture representation; Texture classification; Klein bottle; Fourier coefficients; Patch distribution; Density estimation	RECEPTIVE-FIELDS; CLASSIFICATION; DISTANCE	A natural object of study in texture representation and material classification is the probability density function, in pixel-value space, underlying the set of small patches from the given image. Inspired by the fact that small high-contrast patches from natural images in gray-scale accumulate with high density around a surface with the topology of a Klein bottle (Carlsson et al. International Journal of Computer Vision 76(1):1-12, 2008), we present in this paper a novel framework for the estimation and representation of distributions around , of patches from texture images. More specifically, we show that most patches from a given image can be projected onto yielding a finite sample , whose underlying probability density function can be represented in terms of Fourier-like coefficients, which in turn, can be estimated from . We show that image rotation acts as a linear transformation at the level of the estimated coefficients, and use this to define a multi-scale rotation-invariant descriptor. We test it by classifying the materials in three popular data sets: The CUReT, UIUCTex and KTH-TIPS texture databases.	[Perea, Jose A.] Duke Univ, Dept Math, Durham, NC 27708 USA; [Carlsson, Gunnar] Stanford Univ, Dept Math, Stanford, CA 94305 USA	Duke University; Stanford University	Perea, JA (corresponding author), Duke Univ, Dept Math, Durham, NC 27708 USA.	joperea@math.duke.edu; gunnar@math.stanford.edu			National Science Foundation (NSF) [DMS 0905823, DMS 096422]; Air Force Office of Scientific Research [FA9550-09-1-0643, FA9550-09-1-0531]; National Institutes of Health [I-U54-ca149145-01]	National Science Foundation (NSF)(National Science Foundation (NSF)); Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	Jose Perea was partially supported by the National Science Foundation (NSF) through grant DMS 0905823. Gunnar Carlsson was supported by the NSF through grants DMS 0905823 and DMS 096422, by the Air Force Office of Scientific Research through grants FA9550-09-1-0643 and FA9550-09-1-0531, and by the National Institutes of Health through grant I-U54-ca149145-01.	Aherne FJ, 1998, KYBERNETIKA, V34, P363; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217; Broadhurst R.E., 2005, P INT WORKSH TEXT AN, V2, P25; Brodatz P., 1966, TEXTURES PHOTOGRAPHI, V66; Carlsson G, 2008, INT J COMPUT VISION, V76, P1, DOI 10.1007/s11263-007-0056-x; Carlsson G, 2009, B AM MATH SOC, V46, P255, DOI 10.1090/S0273-0979-09-01249-X; Crosier M, 2010, INT J COMPUT VISION, V88, P447, DOI 10.1007/s11263-009-0315-0; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; de Silva V, 2011, DISCRETE COMPUT GEOM, V45, P737, DOI 10.1007/s00454-011-9344-x; de Wit TD, 1998, PHYS REV E, V58, P5115, DOI 10.1103/PhysRevE.58.5115; EDELMAN A, 1995, MATH COMPUT, V64, P763, DOI 10.1090/S0025-5718-1995-1262279-2; Griffin LD, 2005, NETWORK-COMP NEURAL, V16, P301, DOI 10.1080/09548980500289874; Griffin LD, 2007, IEEE T PATTERN ANAL, V29, P1355, DOI 10.1109/TPAMI.2007.1066; Harris C. G., 1988, P 4 ALV VIS C, V15, P10, DOI [10.5244/C.2.23, DOI 10.5244/C.2.23]; Hatcher A., 2005, ALGEBRAIC TOPOLOGY; Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253; HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455; HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308; Jurie F, 2005, IEEE I CONF COMP VIS, P604; Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151; Lee AB, 2003, INT J COMPUT VISION, V54, P83, DOI 10.1023/A:1023705401078; Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638; Moler C. B., 1991, MATHWORKS NEWSLETTER, V5, P8; Pedersen KS, 2002, LECT NOTES COMPUT SC, V2350, P328; Reed Michael, 1972, METHODS MODERN MATH, VI; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Silverman B. W., 1986, DENSITY ESTIMATION S, V26; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303; Varma M, 2004, IMAGE VISION COMPUT, V22, P1175, DOI 10.1016/j.imavis.2004.03.012; Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4; Varma M., 2007, P IEEE INT C COMP VI, V1, P1, DOI DOI 10.1109/ICCV.2007.4408875; Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182; WATSON GS, 1969, ANN MATH STAT, V40, P1496, DOI 10.1214/aoms/1177697523; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4; [No title captured]	38	26	26	1	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2014	107	1					75	97		10.1007/s11263-013-0676-2	http://dx.doi.org/10.1007/s11263-013-0676-2			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AB2RT					2022-12-18	WOS:000331640500005
J	Kadri-Harouna, S; Derian, P; Heas, P; Memin, E				Kadri-Harouna, S.; Derian, P.; Heas, P.; Memin, E.			Divergence-Free Wavelets and High Order Regularization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Divergence-free wavelets; High order derivatives regularization; Optic-flow estimation	FLUID-FLOW	Expanding on a wavelet basis the solution of an inverse problem provides several advantages. First of all, wavelet bases yield a natural and efficient multiresolution analysis which allows defining clear optimization strategies on nested subspaces of the solution space. Besides, the continuous representation of the solution with wavelets enables analytical calculation of regularization integrals over the spatial domain. By choosing differentiable wavelets, accurate high-order derivative regularizers can be efficiently designed via the basis's mass and stiffness matrices. More importantly, differential constraints on vector solutions, such as the divergence-free constraint in physics, can be nicely handled with biorthogonal wavelet bases. This paper illustrates these advantages in the particular case of fluid flow motion estimation. Numerical results on synthetic and real images of incompressible turbulence show that divergence-free wavelets and high-order regularizers are particularly relevant in this context.	[Kadri-Harouna, S.; Derian, P.; Heas, P.; Memin, E.] INRIA Rennes Bretagne Atlantique, F-35042 Rennes, France		Kadri-Harouna, S (corresponding author), INRIA Rennes Bretagne Atlantique, Campus Univ Beaulieu, F-35042 Rennes, France.	Souleymane.Kadri_harouna@inria.fr; Pierre.Derian@inria.fr; Patrick.Heas@inria.fr; Etienne.Memin@inria.fr		Heas, Patrick/0000-0001-6860-7149				BERGEN JR, 1992, IEEE T PATTERN ANAL, V14, P886, DOI 10.1109/34.161348; BEYLKIN G, 1992, SIAM J NUMER ANAL, V29, P1716, DOI 10.1137/0729097; COHEN A, 1992, COMMUN PUR APPL MATH, V45, P485, DOI 10.1002/cpa.3160450502; Derian P, 2011, LNCS; Deriaz E, 2006, J TURBUL, V7, P1, DOI 10.1080/14685240500260547; Girault V., 1986, SPRINGER SERIES COMP, V5; Heas P, 2012, TELLUS A, V64, DOI 10.3402/tellusa.v64i0.10962; Heitz D, 2007, TECHNICAL REPORT; Heitz D, 2010, EXP FLUIDS, V48, P369, DOI 10.1007/s00348-009-0778-3; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Jullien MC, 2000, PHYS REV LETT, V85, P3636, DOI 10.1103/PhysRevLett.85.3636; Kahane J-P, 1995, FOURIER SERIES WAVEL; Kohlberger T, 2003, LECT NOTES COMPUT SC, V2695, P432; Lemarie-Rieusset P., 1992, REV MAT IBEROAM, V8, P221; Liu TS, 2008, J FLUID MECH, V614, P253, DOI 10.1017/S0022112008003273; Mallat S., 2008, WAVELET TOUR SIGNAL; Nocedal J., 1999, SPRINGER SERIES OPER; PAPADAKIS N, 2007, IEEE INT C COMP VIS; Ruhnau P, 2007, EXP FLUIDS, V42, P61, DOI 10.1007/s00348-006-0220-z; Sass-Hansen M, 2009, VISSAPP, V1, P79; SIMARD PY, 1988, IEEE T PATTERN ANAL, V10, P248, DOI 10.1109/34.3886; Steinbruecker F., 2009, IEEE INT C COMP VIS, P1905; SUTER D, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P939, DOI 10.1109/CVPR.1994.323929; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P413, DOI 10.1109/TPAMI.1986.4767807; Wu YT, 2000, INT J COMPUT VISION, V38, P129, DOI 10.1023/A:1008101718719; Yuan J, 2007, J MATH IMAGING VIS, V28, P67, DOI 10.1007/s10851-007-0014-9	26	26	28	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2013	103	1					80	99		10.1007/s11263-012-0595-7	http://dx.doi.org/10.1007/s11263-012-0595-7			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	137CV		Green Submitted			2022-12-18	WOS:000318413500004
J	Friedman, S; Stamos, I				Friedman, Sam; Stamos, Ioannis			Online Detection of Repeated Structures in Point Clouds of Urban Scenes for Compression and Registration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Urban range scans; 3D scan registration; 3D scan compression; Regularity detection	RANGE REGISTRATION; SEGMENTATION; GEOMETRY; MODEL	Laser range scans of urban areas have a distinctive geometry dominated by facade and ground planes and repetitive regular fenestration. Detection of these ubiquitous features provides profound insights into the scene. We present a novel method for detecting major planes and repetitive architectural features. Armed with this knowledge we illustrate its application in compression and registration of range scans. What is more our algorithm operates online, processing the scan as it is retrieved by the scanner. This realtime approach opens up new possibilities in range data segmentation, compression and registration.	[Friedman, Sam; Stamos, Ioannis] CUNY Hunter Coll, New York, NY 10021 USA; [Friedman, Sam; Stamos, Ioannis] CUNY, Grad Ctr, New York, NY USA	City University of New York (CUNY) System; Hunter College (CUNY); City University of New York (CUNY) System	Stamos, I (corresponding author), CUNY Hunter Coll, New York, NY 10021 USA.	umpteee@yahoo.com; istamos@hunter.cuny.edu			NSF [IIS-0915971, CCF-0916452, MRI CNS-0821384]	NSF(National Science Foundation (NSF))	This work has been supported in part by the following NSF grants: IIS-0915971, CCF-0916452 and MRI CNS-0821384. We would like to thank Tom Flynn for his labeling and registration software. We are also thankful to the anonymous reviewers for their helpful comments.	Aiger D., 2008, SIGGRAPH, V27; Allen PK, 2003, IEEE COMPUT GRAPH, V23, P32, DOI 10.1109/MCG.2003.1242380; Bellon ORP, 2002, IEEE SIGNAL PROC LET, V9, P43, DOI 10.1109/97.991134; Berner A., 2008, S VOL POINT BAS GRAP, P1; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; BESL PJ, 1988, IEEE T PATTERN ANAL, V10, P167, DOI 10.1109/34.3881; Bokeloh M., 2009, COMPUT GRAPH FORUM, V28, P607; Cazals F, 2005, COMPUT AIDED GEOM D, V22, P121, DOI 10.1016/j.cagd.2004.09.004; Chao C., 2007, 6 INT C 3 D DIG IM M; Friedman S., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P220, DOI 10.1109/3DIMPVT.2011.35; Huber DF, 2003, IMAGE VISION COMPUT, V21, P637, DOI 10.1016/S0262-8856(03)00060-X; Huynh DQ, 2009, J MATH IMAGING VIS, V35, P155, DOI 10.1007/s10851-009-0161-2; Lee SC, 2004, PROC CVPR IEEE, P113; Li YY, 2011, IEEE I CONF COMP VIS, P882, DOI 10.1109/ICCV.2011.6126329; Marshall D, 2001, IEEE T PATTERN ANAL, V23, P304, DOI 10.1109/34.910883; Martinet A, 2006, ACM T GRAPHIC, V25, P439, DOI 10.1145/1138450.1138462; Mayer H, 2007, ISPRS J PHOTOGRAMM, V61, P371, DOI 10.1016/j.isprsjprs.2006.10.007; Mitra Niloy J., 2012, EUROGRAPHICS STATE A, DOI [DOI 10.1111/CGF.12010, 10.1111/cgf.12010]; Muller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]; Muller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931; Nan LL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778830; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Park M, 2009, IEEE T PATTERN ANAL, V31, P1804, DOI 10.1109/TPAMI.2009.73; Pauly M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360642; Pele O, 2010, LECT NOTES COMPUT SC, V6312, P749, DOI 10.1007/978-3-642-15552-9_54; Pulli K., 1993, Proceedings of the 8th Scandinavian Conference on Image Analysis, P893; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Shen CH, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024218; Stamos I, 2003, PROC CVPR IEEE, P555; Stamos I, 2002, COMPUT VIS IMAGE UND, V88, P94, DOI 10.1006/cviu.2002.0963; Stamos I., 2006, INT S 3D DAT PROC VI; Stamos I, 2008, INT J COMPUT VISION, V78, P237, DOI 10.1007/s11263-007-0089-1; Stiny G., 1982, ENVIRON PLANN B, V9, P313; Teboul O., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2273, DOI 10.1109/CVPR.2011.5995319; Teboul O, 2010, PROC CVPR IEEE, P3105, DOI 10.1109/CVPR.2010.5540068; Triebel R, 2006, IEEE INT CONF ROBOT, P2603, DOI 10.1109/ROBOT.2006.1642094; WANI MA, 1994, IEEE T PATTERN ANAL, V16, P314, DOI 10.1109/34.276131; Wu CC, 2010, LECT NOTES COMPUT SC, V6312, P142; Yu YZ, 2001, IEEE T VIS COMPUT GR, V7, P351, DOI 10.1109/2945.965349; Zhao HJ, 2003, MACH VISION APPL, V14, P35, DOI 10.1007/s00138-002-0099-5	40	26	27	1	39	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					112	128		10.1007/s11263-012-0575-y	http://dx.doi.org/10.1007/s11263-012-0575-y			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO					2022-12-18	WOS:000315501800008
J	Wang, L; Gong, ML; Zhang, CX; Yang, RG; Zhang, C; Yang, YH				Wang, Liang; Gong, Minglun; Zhang, Chenxi; Yang, Ruigang; Zhang, Cha; Yang, Yee-Hong			Automatic Real-Time Video Matting Using Time-of-Flight Camera and Multichannel Poisson Equations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Bilayer segmentation; Video matting; Time-of-flight camera	IMAGE	This paper presents an automatic real-time video matting system. The proposed system consists of two novel components. In order to automatically generate trimaps for live videos, we advocate a Time-of-Flight (TOF) camera-based approach to video bilayer segmentation. Our algorithm combines color and depth cues in a probabilistic fusion framework. The scene depth information returned by the TOF camera is less sensitive to environment changes, which makes our method robust to illumination variation, dynamic background and camera motion. For the second step, we perform alpha matting based on the segmentation result. Our matting algorithm uses a set of novel Poisson equations that are derived for handling multichannel color vectors, as well as the depth information captured. Real-time processing speed is achieved through optimizing the algorithm for parallel processing on graphics hardware. We demonstrate the effectiveness of our matting system on an extensive set of experimental results.	[Wang, Liang; Yang, Ruigang] Univ Kentucky, Dept Comp Sci, Lexington, KY 40506 USA; [Gong, Minglun] Mem Univ Newfoundland, Dept Comp Sci, St John, NF A1B 3X5, Canada; [Zhang, Cha] Microsoft Res, Redmond, WA 98052 USA; [Yang, Yee-Hong] Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada	University of Kentucky; Memorial University Newfoundland; Microsoft; University of Alberta	Wang, L (corresponding author), Univ Kentucky, Dept Comp Sci, 329 Rose St, Lexington, KY 40506 USA.	lwangd@cs.uky.edu; gong@cs.mun.ca; chenxi.zhang@uky.edu; ryang@cs.uky.edu; chazhang@microsoft.com; yang@cs.ualberta.ca		Gong, Minglun/0000-0001-5820-5381; Yang, Ruigang/0000-0001-5296-6307	University of Kentucky Research Foundation; US National Science Foundation [IIS-0448185, CPA-0811647, MRI-0923131]	University of Kentucky Research Foundation; US National Science Foundation(National Science Foundation (NSF))	The authors would like to thank Mr. Mao Ye, Dr. Matt Steel and Dr. Melody Carswell for their help in data capture. This work is supported in part by University of Kentucky Research Foundation and US National Science Foundation award IIS-0448185, CPA-0811647, and MRI-0923131. Finally, the authors would like to thank anonymous reviewers for their constructive comments and suggestions.	Bai X., 2007, P ICCV, P1; BLAKE A, 2004, P ECCV; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; BOYKOV Y, 2001, P ICCV; Chuang YY, 2001, PROC CVPR IEEE, P264; Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572; Crabb R., 2008, P IEEE WORKSH TIM FL; Criminisi A., 2006, P CVPR; Davis J., 2003, P IEEE WORKSH PROJ C; Gastal E. S. L., 2010, P EUR; Gong M., 2009, P CAN C COMP ROB VIS; Gong M., 2010, P GRAPH INT; Gordon G., 1999, P CVPR; Grady L, 2005, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P423; Harville M., 2001, P IEEE WORKSH DET RE; Joshi N, 2006, ACM T GRAPHIC, V25, P779, DOI 10.1145/1141911.1141955; KOLMOGOROV V, 2005, P CVPR; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234; McGuire M, 2005, ACM T GRAPHIC, V24, P567, DOI 10.1145/1073204.1073231; MCGUIRE M, 2006, P EUR S REND; Mishima Y., 1993, U.S. Patent, Patent No. [5,355,174, 5355174]; Pham V.-Q., 2009, P ACCV; Porter T., 1984, P SIGGRAPH, P673; Rhemann C., 2008, P CVPR; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721; Sun J., 2007, P CVPR; Sun J, 2006, LECT NOTES COMPUT SC, V3952, P628; Wang J, 2005, IEEE I CONF COMP VIS, P936; Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233; WANG J, 2007, P SIGGRAPH; Wang J., 2007, P CVPR; Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019; Wang Liang, 2010, P 3DPVT; Wang O., 2007, P PAC GRAPH; Wu Q., 2008, P CAN C COMP ROB VIS; YANG Q, 2007, P CVPR; Yin P., 2007, P CVPR; Yu T., 2007, P IEEE WORKSH MOT VI; Zhang G., 2008, P CVPR; Zhang GF, 2011, IEEE T PATTERN ANAL, V33, P603, DOI 10.1109/TPAMI.2010.115; Zhu J., 2008, P CVPR; Zhu J., 2009, P CVPR	44	26	35	0	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2012	97	1					104	121		10.1007/s11263-011-0471-x	http://dx.doi.org/10.1007/s11263-011-0471-x			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	897SD					2022-12-18	WOS:000300675300008
J	Wirth, B; Bar, L; Rumpf, M; Sapiro, G				Wirth, Benedikt; Bar, Leah; Rumpf, Martin; Sapiro, Guillermo			A Continuum Mechanical Approach to Geodesics in Shape Space	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Riemannian shape space; Geodesics; Variational time discretization; Viscous fluids; Rigid body motion invariance	METRICS; SEGMENTATION; FRAMEWORK; FLOWS	In this paper concepts from continuum mechanics are used to define geodesic paths in the space of shapes, where shapes are implicitly described as boundary contours of objects. The proposed shape metric is derived from a continuum mechanical notion of viscous dissipation. A geodesic path is defined as the family of shapes such that the total amount of viscous dissipation caused by an optimal material transport along the path is minimized. The approach can easily be generalized to shapes given as segment contours of multi-labeled images and to geodesic paths between partially occluded objects. The proposed computational framework for finding such a minimizer is based on the time discretization of a geodesic path as a sequence of pairwise matching problems, which is strictly invariant with respect to rigid body motions and ensures a 1-1 correspondence along the induced flow in shape space. When decreasing the time step size, the proposed model leads to the minimization of the actual geodesic length, where the Hessian of the pairwise matching energy reflects the chosen Riemannian metric on the underlying shape space. If the constraint of pairwise shape correspondence is replaced by the volume of the shape mismatch as a penalty functional, one obtains for decreasing time step size an optical flow term controlling the transport of the shape by the underlying motion field. The method is implemented via a level set representation of shapes, and a finite element approximation is employed as spatial discretization both for the pairwise matching deformations and for the level set representations. The numerical relaxation of the energy is performed via an efficient multi-scale procedure in space and time. Various examples for 2D and 3D shapes underline the effectiveness and robustness of the proposed approach.	[Wirth, Benedikt; Rumpf, Martin] Univ Bonn, Inst Numer Simulat, D-5300 Bonn, Germany; [Bar, Leah; Sapiro, Guillermo] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN USA	University of Bonn; University of Minnesota System; University of Minnesota Twin Cities	Wirth, B (corresponding author), Univ Bonn, Inst Numer Simulat, D-5300 Bonn, Germany.	Benedikt.Wirth@ins.uni-bonn.de			NSF; ARO; ONR; NGA; DARPA; DFG [SPP 611]; German Science Foundation	NSF(National Science Foundation (NSF)); ARO; ONR(Office of Naval Research); NGA; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DFG(German Research Foundation (DFG)); German Science Foundation(German Research Foundation (DFG))	This work has been partially supported by NSF, ARO, ONR, NGA, DARPA and DFG SPP 611. Benedikt Wirth was supported by BIGS Mathematics and the Hausdorff Center for Mathematics at Bonn University funded by the German Science Foundation. Furthermore, the authors thank Sergio Conti for valuable hints on the physical model.	Adams R. A., 2003, SOBOLEV SPACES, VSecond; BALL JM, 1981, P ROY SOC EDINB A, V88, P315, DOI 10.1017/S030821050002014X; BALL JM, 1977, ARCH RATION MECH AN, V63, P337, DOI 10.1007/BF00279992; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; BLACK MJ, 1993, ICCV, P231; Bornemann FA, 1996, NUMER MATH, V75, P135, DOI 10.1007/s002110050234; Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Charpiat G, 2007, INT J COMPUT VISION, V73, P325, DOI 10.1007/s11263-006-9966-2; Charpiat G, 2005, FOUND COMPUT MATH, V5, P1, DOI 10.1007/s10208-003-0094-x; CHIPOT M, 1986, P ROY SOC EDINB A, V102, P291, DOI 10.1017/S0308210500026378; CHORIN AJ, 1990, TEXTS APPL MATH, V4; Ciarlet PG, 1988, 3 DIMENSIONAL ELASTI; DELFOUR MC, 2001, ADV CONTROL, V4; do Carmo M. P., 1992, RIEMANNIAN GEOMETRY; Droske M, 2007, IEEE T PATTERN ANAL, V29, P2181, DOI 10.1109/TPAMI.2007.1120; Duci A, 2006, IMAGE VISION COMPUT, V24, P271, DOI 10.1016/j.imavis.2005.07.021; Dupuis P, 1998, Q APPL MATH, V56, P587, DOI 10.1090/qam/1632326; ECKSTEIN I, 2007, EUR S GEOM PROC; FLETCHER P, 2006, MICCAI 2006 MED IM C; FUCHS M, 2009, J MATH IMAG IN PRESS; Glaunes J, 2008, INT J COMPUT VISION, V80, P317, DOI 10.1007/s11263-008-0141-9; Gromov M., 1999, PROGR MATH, V152; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kilian M, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276457, 10.1145/1239451.1239515]; Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333; Liu XW, 2010, INT J COMPUT VISION, V89, P69, DOI 10.1007/s11263-010-0323-0; LUCKHAUS S, 1995, CALC VAR PARTIAL DIF, V3, P253, DOI 10.1007/s005260050015; Manay S, 2006, IEEE T PATTERN ANAL, V28, P1602, DOI 10.1109/TPAMI.2006.208; Memoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Miller MI, 2002, ANNU REV BIOMED ENG, V4, P375, DOI 10.1146/annurev.bioeng.4.092101.125733; Miller MI, 2001, INT J COMPUT VISION, V41, P61, DOI 10.1023/A:1011161132514; NECAS J, 1991, Q APPL MATH, V49, P247; Schmidt FR, 2006, LECT NOTES COMPUT SC, V4174, P142; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; Sundaramoorthi G, 2007, INT J COMPUT VISION, V73, P345, DOI 10.1007/s11263-006-0635-2; Trouve A, 2005, FOUND COMPUT MATH, V5, P173, DOI 10.1007/s10208-004-0128-z; Truesdell C., 2004, NONLINEAR FIELD THEO; Vaillant M, 2005, LECT NOTES COMPUT SC, V3565, P381; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Yezzi A, 2005, IEEE I CONF COMP VIS, P913; Yezzi A, 2001, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P44, DOI 10.1109/MMBIA.2001.991698; Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685; Younes L, 2008, REND LINCEI-MAT APPL, V19, P25; Younes L, 2008, J MATH IMAGING VIS, V32, P41, DOI 10.1007/s10851-008-0074-5; Zhao HK, 1996, J COMPUT PHYS, V127, P179, DOI 10.1006/jcph.1996.0167; Zhu L, 2007, IEEE T IMAGE PROCESS, V16, P1481, DOI 10.1109/TIP.2007.896637; ZOLESIO JP, 2004, IFIP C SYST MOD OPT, V21, P185	50	26	26	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2011	93	3					293	318		10.1007/s11263-010-0416-9	http://dx.doi.org/10.1007/s11263-010-0416-9			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	746ND		Green Submitted			2022-12-18	WOS:000289253100002
J	Venkat, I; De Wilde, P				Venkat, Ibrahim; De Wilde, Philippe			Robust Gait Recognition by Learning and Exploiting Sub-gait Characteristics	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Gait recognition; Biometrics; Human motion analysis; Bayesian Network; Machine learning	OBJECT DETECTION; MODEL; REPRESENTATION; PERFORMANCE; HUMANS	Gait recognition algorithms often perform poorly because of low resolution video sequences, subjective human motion and challenging outdoor scenarios. Despite these challenges, gait recognition research is gaining momentum due to increasing demand and more possibilities for deployment by the surveillance industry. Therefore every research contribution which significantly improves this new biometric is a milestone. We propose a probabilistic sub-gait interpretation model to recognize gaits. A sub-gait is defined by us as part of the silhouette of a moving body. Binary silhouettes of gait video sequences form the basic input of our approach. A novel modular training scheme has been introduced in this research to efficiently learn subtle sub-gait characteristics from the gait domain. For a given gait sequence, we get useful information from the sub-gaits by identifying and exploiting intrinsic relationships using Bayesian networks. Finally, by incorporating efficient inference strategies, robust decisions are made for recognizing gaits. Our results show that the proposed model tackles well the uncertainties imposed by typical covariate factors and shows significant recognition performance.	[Venkat, Ibrahim; De Wilde, Philippe] Heriot Watt Univ, Dept Comp Sci, Sch Math & Comp Sci, Edinburgh EH14 4AS, Midlothian, Scotland	Heriot Watt University	Venkat, I (corresponding author), Heriot Watt Univ, Dept Comp Sci, Sch Math & Comp Sci, Edinburgh EH14 4AS, Midlothian, Scotland.	vk52@hw.ac.uk	De Wilde, Philippe/C-2256-2009	De Wilde, Philippe/0000-0002-4332-1715	School of Mathematical and Computer Sciences, Heriot-Watt University; Kolej University TATI; Universiti Sains Malaysia	School of Mathematical and Computer Sciences, Heriot-Watt University; Kolej University TATI; Universiti Sains Malaysia(Universiti Sains Malaysia)	The authors would like to thank Prof. Sarkar and his team, University of South Florida for providing the gait challenge data sets. Portions of the research in this paper use the CASIA Gait Database collected by Institute of Automation, Chinese Academy of Sciences. We also thank Dr. Haiping, University of Toronto for providing gait resources and valuable advice. We thank Dr. Murphy and his team, University of British Columbia, for providing the Bayes Net Toolbox. This research is supported by James Watt Scholarship awarded by the School of Mathematical and Computer Sciences, Heriot-Watt University. We also thank Kolej University TATI and Universiti Sains Malaysia for their support. We thank the editor and anonymous reviewers for providing useful suggestions.	ABENI P, 2006, P CAN C COMP ROB VIS; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; [Anonymous], 2006, HUMAN IDENTIFICATION; Bauckhage C, 2009, IMAGE VISION COMPUT, V27, P108, DOI 10.1016/j.imavis.2006.10.004; BERGTHOLDT M, 2009, INT J COMPUTER VISIO; Boulgouris NV, 2007, IEEE IMAGE PROC, P353; Boulgouris NV, 2006, PATTERN RECOGN, V39, P969, DOI 10.1016/j.patcog.2005.10.013; *CASIA, 2006, GAIT DAT; Collins RT, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P366, DOI 10.1109/AFGR.2002.1004181; Dahyot R, 2004, PATTERN ANAL APPL, V7, P317, DOI 10.1007/s10044-004-0230-5; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Gonzalez R.C., 2006, DIGITAL IMAGE PROCES; HALLINAN P, 1999, 2 2 DIMENSIONAL PATT; Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38; Heisele B, 2007, INT J COMPUT VISION, V74, P167, DOI 10.1007/s11263-006-0006-z; INTILLE S, 2001, COMPUTER VISION IMAG, V81, P441; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Kale A, 2004, IEEE T IMAGE PROCESS, V13, P1163, DOI 10.1109/TIP.2004.832865; KEMP C, 2006, P 20 ANN C NEUR INF; Kim J, 2005, IEEE T PATTERN ANAL, V27, P1977, DOI 10.1109/TPAMI.2005.242; Krynski TR, 2007, J EXP PSYCHOL GEN, V136, P430, DOI 10.1037/0096-3445.136.3.430; Lee L, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P155, DOI 10.1109/AFGR.2002.1004148; Li XL, 2008, IEEE T SYST MAN CY C, V38, P145, DOI 10.1109/TSMCC.2007.913886; Liu ZY, 2006, IEEE T PATTERN ANAL, V28, P863, DOI 10.1109/TPAMI.2006.122; Liu ZY, 2005, IEEE T SYST MAN CY B, V35, P170, DOI 10.1109/TSMCB.2004.842251; Lu HP, 2008, IEEE T NEURAL NETWOR, V19, P18, DOI 10.1109/TNN.2007.901277; Martinez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382; Mohan A, 2001, IEEE T PATTERN ANAL, V23, P349, DOI 10.1109/34.917571; Moon H, 2001, PERCEPTION, V30, P303, DOI 10.1068/p2896; Murphy K., 2007, ISBA B; Murray M P, 1967, Am J Phys Med, V46, P290; MURRAY MP, 1964, J BONE JOINT SURG AM, V46, P335, DOI 10.2106/00004623-196446020-00009; Myung IJ, 2003, J MATH PSYCHOL, V47, P90, DOI 10.1016/S0022-2496(02)00028-7; Neapolitan R. E., 2003, LEARNING BAYESIAN NE; Nielsen T.D., 2007, COMPSTAT, V2nd, DOI [DOI 10.1007/978-0-387-68282-2, DOI 10.1007/978-3-642-46890-2_1]; Nixon MS, 2006, P IEEE, V94, P2013, DOI 10.1109/JPROC.2006.886018; Pearl J, 1997, PROBABILISTIC REASON; Russel SJ, 2020, ARTIF INTELL, V4th; Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Veeraraghavan A, 2004, PROC CVPR IEEE, P730; Veeraraghavan A, 2009, IEEE T IMAGE PROCESS, V18, P1326, DOI 10.1109/TIP.2009.2017143; Xu D, 2006, IEEE T CIRC SYST VID, V16, P896, DOI 10.1109/TCSVT.2006.877418; Xu ZJ, 2008, IEEE T PATTERN ANAL, V30, P955, DOI 10.1109/TPAMI.2008.50; Yan SC, 2005, PROC CVPR IEEE, P526; Yu Shiqi, 2006, 18 INT C PATT REC IC; Zhou ZH, 2006, IEEE T PATTERN ANAL, V28, P1738, DOI 10.1109/TPAMI.2006.214	49	26	27	0	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2011	91	1					7	23		10.1007/s11263-010-0362-6	http://dx.doi.org/10.1007/s11263-010-0362-6			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	705HB					2022-12-18	WOS:000286118400001
J	Pears, N; Heseltine, T; Romero, M				Pears, Nick; Heseltine, Tom; Romero, Marcelo			From 3D Point Clouds to Pose-Normalised Depth Maps	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D feature extraction; Invariance; 3D landmark localisation; 3D pose normalisation	3-DIMENSIONAL FACE RECOGNITION; REPRESENTATION; INTERPOLATION; EIGENFACES	We consider the problem of generating either pairwise-aligned or pose-normalised depth maps from noisy 3D point clouds in a relatively unrestricted poses. Our system is deployed in a 3D face alignment application and consists of the following four stages: (i) data filtering, (ii) nose tip identification and sub-vertex localisation, (iii) computation of the (relative) face orientation, (iv) generation of either a pose aligned or a pose normalised depth map. We generate an implicit radial basis function (RBF) model of the facial surface and this is employed within all four stages of the process. For example, in stage (ii), construction of novel invariant features is based on sampling this RBF over a set of concentric spheres to give a spherically-sampled RBF (SSR) shape histogram. In stage (iii), a second novel descriptor, called an isoradius contour curvature signal, is defined, which allows rotational alignment to be determined using a simple process of 1D correlation. We test our system on both the University of York (UoY) 3D face dataset and the Face Recognition Grand Challenge (FRGC) 3D data. For the more challenging UoY data, our SSR descriptors significantly outperform three variants of spin images, successfully identifying nose vertices at a rate of 99.6%. Nose localisation performance on the higher quality FRGC data, which has only small pose variations, is 99.9%. Our best system successfully normalises the pose of 3D faces at rates of 99.1% (UoY data) and 99.6% (FRGC data).	[Pears, Nick; Romero, Marcelo] Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England; [Heseltine, Tom] Aurora Comp Serv Ltd, Northampton, England	University of York - UK	Pears, N (corresponding author), Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England.	nep@cs.york.ac.uk; t.heseltine@auroracs.co.uk; mromero@cs.york.ac.uk						Ankerst M, 1999, LECT NOTES COMPUT SC, V1651, P207; ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965; Assfalg J, 2004, INT C PATT RECOG, P906, DOI 10.1109/ICPR.2004.1334675; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BESL PJ, 1985, COMPUT SURV, V17, P75, DOI 10.1145/4078.4081; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Bowyer KW, 2006, COMPUT VIS IMAGE UND, V101, P1, DOI 10.1016/j.cviu.2005.05.005; Bronstein AM, 2007, IEEE T IMAGE PROCESS, V16, P188, DOI 10.1109/TIP.2006.884940; Bronstein AM, 2005, INT J COMPUT VISION, V64, P5, DOI 10.1007/s11263-005-1085-y; Carr JC, 1997, IEEE T MED IMAGING, V16, P96, DOI 10.1109/42.552059; Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266; Chang KI, 2005, IEEE T PATTERN ANAL, V27, P619, DOI 10.1109/TPAMI.2005.70; Chang KI, 2006, IEEE T PATTERN ANAL, V28, P1695, DOI 10.1109/TPAMI.2006.210; CHEN DY, 2003, EUROGRAPHICS, V22; Chen JC, 2005, EURASIP J APPL SIG P, V2005, P1, DOI 10.1155/ASP.2005.1; Chetverikov D, 2005, IMAGE VISION COMPUT, V23, P299, DOI 10.1016/j.imavis.2004.05.007; Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186; CHUA FHC, 2001, 4 IEEE INT C AUT FAC, P233; COLBRY D, 2005, IEEE COMP SOC C COMP, P118; CONDE C, 2005, IAPR C MACH VIS APPL, P418; DINH HQ, 2006, P CVPR, P863; Dorai C, 1997, IEEE T PATTERN ANAL, V19, P1115, DOI 10.1109/34.625113; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FRANKE R, 1982, MATH COMPUT, V38, P181, DOI 10.2307/2007474; Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279; Gordon G. G., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P808, DOI 10.1109/CVPR.1992.223253; GREENGARD L, 1987, J COMPUT PHYS, V73, P325, DOI 10.1016/0021-9991(87)90140-9; HARALICK RM, 1989, IEEE T SYST MAN CYB, V19, P1426, DOI 10.1109/21.44063; Heseltine T, 2004, LECT NOTES COMPUT SC, V3212, P684; Heseltine T., 2004, P IEEE INT C IM PROC, P1; Heseltine T, 2008, IMAGE VISION COMPUT, V26, P382, DOI 10.1016/j.imavis.2006.12.008; HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073; Hou QB, 2005, COMPUTER GRAPHICS, IMAGING AND VISION: NEW TRENDS, P79; HU XL, 2008, IEEE INT C NEUR NETW, P412; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; KAKADIARIS I, 2006, BRIT MACH VIS C BMVC; Kazhdan M., 2003, Symposium on Geometry Processing, P156; LORENSEN WE, 1987, COMPUTER GRAPHICS, V21, P21; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu XG, 2006, IEEE T PATTERN ANAL, V28, P31, DOI 10.1109/TPAMI.2006.15; Mian AS, 2008, INT J COMPUT VISION, V79, P1, DOI 10.1007/s11263-007-0085-5; Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105; Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026; PEARS NE, 2008, 8 IEEE INT C AUT FAC; Pears N, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P176; Phillips PJ, 2005, PROC CVPR IEEE, P947; Rohling R, 1999, LECT NOTES COMPUT SC, V1613, P478; Saupe D., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P392; SAVCHENKO VV, 1985, COMPUT GRAPH FORUM, V14, P181; Se S, 2002, INT J ROBOT RES, V21, P735, DOI 10.1177/027836402761412467; Segundo MP, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P431, DOI 10.1109/ICIAP.2007.4362816; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; Theoharis T, 2008, PATTERN RECOGN, V41, P796, DOI 10.1016/j.patcog.2007.06.024; Theoharis T, 2008, STUD COMPUT INTELL, V159, P55; Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Wang YJ, 2002, PATTERN RECOGN LETT, V23, P1191, DOI 10.1016/S0167-8655(02)00066-1; Whitmarsh T., 2006, P 1 INT WORKSH SHAP, P71; Xu CH, 2006, PATTERN RECOGN LETT, V27, P1487, DOI 10.1016/j.patrec.2006.02.015	62	26	27	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2010	89	2-3			SI		152	176		10.1007/s11263-009-0297-y	http://dx.doi.org/10.1007/s11263-009-0297-y			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	594OS		Green Accepted			2022-12-18	WOS:000277547600003
J	Toderici, G; O'Malley, SM; Passalis, G; Theoharis, T; Kakadiaris, IA				Toderici, George; O'Malley, Sean M.; Passalis, George; Theoharis, Theoharis; Kakadiaris, Ioannis A.			Ethnicity- and Gender-based Subject Retrieval Using 3-D Face-Recognition Techniques	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Ethnicity; Face; Gender; Identification; Race; Recognition; Retrieval	CLASSIFICATION	While the retrieval of datasets from human subjects based on demographic characteristics such as gender or race is an ability with wide-ranging application, it remains poorly-studied. In contrast, a large body of work exists in the field of biometrics which has a different goal: the recognition of human subjects. Due to this disparity of interest, existing methods for retrieval based on demographic attributes tend to lag behind the more well-studied algorithms designed purely for face matching. The question this raises is whether a face recognition system could be leveraged to solve these other problems and, if so, how effective it could be. In the current work, we explore the limits of such a system for gender and ethnicity identification given (1) a ground truth of demographically-labeled, textureless 3-D models of human faces and (2) a state-of-the-art face-recognition algorithm. Once trained, our system is capable of classifying the gender and ethnicity of any such model of interest. Experiments are conducted on 4007 facial meshes from the benchmark Face Recognition Grand Challenge v2 dataset.	[Toderici, George; O'Malley, Sean M.; Passalis, George; Theoharis, Theoharis; Kakadiaris, Ioannis A.] Univ Houston, Dept Comp Sci, Computat Biomed Lab, Houston, TX 77204 USA; [Passalis, George; Theoharis, Theoharis] Univ Athens, Dept Informat & Telecommun, Comp Graph Lab, Athens 15784, Greece	University of Houston System; University of Houston; National & Kapodistrian University of Athens	Toderici, G (corresponding author), Univ Houston, Dept Comp Sci, Computat Biomed Lab, 4800 Calhoun, Houston, TX 77204 USA.	gtoderici@uh.edu; somalley@uh.edu; passalis@di.uoa.gr; theotheo@di.uoa.gr; ioannisk@uh.edu	Theoharis, Theoharis/AAN-2555-2020	Kakadiaris, Ioannis/0000-0002-0591-1079				Aharon M, 2006, INT J COMPUT VISION, V67, P297, DOI 10.1007/s11263-006-5166-3; Baluja S, 2007, INT J COMPUT VISION, V71, P111, DOI 10.1007/s11263-006-8910-9; Bronstein AM, 2007, IEEE T IMAGE PROCESS, V16, P188, DOI 10.1109/TIP.2006.884940; Bronstein AM, 2006, SIAM J SCI COMPUT, V28, P1812, DOI 10.1137/050639296; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902; Gutta S, 2000, IEEE T NEURAL NETWOR, V11, P948, DOI 10.1109/72.857774; Hardle W, 2003, APPL MULTIVARIATE ST; HOSOI S, 2004, IEEE INT C AUT FAC G; Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017; Kruskal Joseph B, 1978, MULTIDIMENSIONAL SCA, P7; Lian HC, 2005, LECT NOTES COMPUT SC, V3611, P438; LU X, 2006, INT C BIOM HONG KONG; LU X, 2004, SPIE BIOM TECHN HUM, P114; Makinen E, 2008, IEEE T PATTERN ANAL, V30, P541, DOI 10.1109/TPAMI.2007.70800; Moghaddam B, 2002, IEEE T PATTERN ANAL, V24, P707, DOI 10.1109/34.1000244; OToole AJ, 1997, PERCEPTION, V26, P75, DOI 10.1068/p260075; OTOOLE AJ, 1995, ROLE SHAPE TEXTURE I; Phillips PJ, 2005, PROC CVPR IEEE, P947; Potter T, 2007, PSYCHON B REV, V14, P368, DOI 10.3758/BF03194079; Seber G. A. F., 1984, MULTIVARIATE OBSERVA; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725; Tsogo L, 2000, MULTIVAR BEHAV RES, V35, P307, DOI 10.1207/S15327906MBR3503_02; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; WU J, 2008, INT C IM REC; WU J, 2007, BRIT MACH VIS C; Yang Z., 2007, P INT C ADV BIOM ICB, P464; Young F.W., 1987, MULTIDIMENSIONAL SCA	29	26	26	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2010	89	2-3			SI		382	391		10.1007/s11263-009-0300-7	http://dx.doi.org/10.1007/s11263-009-0300-7			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	594OS					2022-12-18	WOS:000277547600015
J	Silveira, G; Malis, E				Silveira, Geraldo; Malis, Ezio			Unified Direct Visual Tracking of Rigid and Deformable Surfaces Under Generic Illumination Changes in Grayscale and Color Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual tracking; Image registration; Nonlinear optimization; Direct methods; Nonrigid objects; Lighting changes; Robust techniques; Robotics	COMPONENTS	The fundamental task of visual tracking is considered in this work as an incremental direct image registration problem. Direct methods refer to those that exploit the pixel intensities without resorting to image features. We propose new transformation models and optimization methods for directly and robustly registering images (including color ones) of rigid and deformable objects, all in a unified manner. We also show that widely adopted models are in fact particular cases of the proposed ones. Indeed, the proposed general models combine various classes of image warps and ensure robustness to generic lighting changes. Finally, the proposed optimization method together with the exploitation of all possible image information allow the algorithm to achieve high levels of accuracy. Extensive experiments are reported to demonstrate that visual tracking can indeed be highly accurate and robust despite deforming objects and severe illumination changes.	[Silveira, Geraldo] CTI Renato Archer, BR-13069901 Campinas, SP, Brazil; [Silveira, Geraldo; Malis, Ezio] INRIA Sophia Antipolis, F-06902 Sophia Antipolis, France		Silveira, G (corresponding author), CTI Renato Archer, Rod Pedro I,Km 143,6 Amarais, BR-13069901 Campinas, SP, Brazil.	Geraldo.Silveira@cti.gov.br; Ezio.Malis@sophia.inria.fr	Estrela, Vania Vieira/I-7599-2012	Estrela, Vania Vieira/0000-0002-4465-7691	Brazilian CAPES Foundation [1886/03-7]; FAPESP-INRIA [04/13467-5]	Brazilian CAPES Foundation(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); FAPESP-INRIA	This work is also partially supported by the Brazilian CAPES Foundation under grant no. 1886/03-7, and by the international agreement FAPESP-INRIA under grant no. 04/13467-5.	Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Baker S, 2001, PROC CVPR IEEE, P1090; Bartoli A, 2008, IEEE T PATTERN ANAL, V30, P2098, DOI 10.1109/TPAMI.2008.22; Bartolomucci A, 2004, PSYCHONEUROENDOCRINO, V29, P899, DOI 10.1016/j.psyneuen.2003.08.003; Benhimane S, 2007, INT J ROBOT RES, V26, P661, DOI 10.1177/0278364907080252; Black MJ, 2000, COMPUT VIS IMAGE UND, V78, P8, DOI 10.1006/cviu.1999.0825; BLINN JF, 1977, SIGGRAPH 77, P192, DOI DOI 10.1145/563858.563893; BROWN LG, 1992, COMPUT SURV, V24, P325, DOI 10.1145/146370.146374; Carruthers JJ, 1997, CELL POLYM, V16, P1; COMANICIU D, 2000, IEEE COMP VIS PATT R; Cook R., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293; FINLAYSON GD, 1994, J OPT SOC AM A, V11, P3011, DOI 10.1364/JOSAA.11.003011; GOUIFFES M, 2006, P EUR C COL GRAPH IM, P18; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Haussecker HW, 2001, IEEE T PATTERN ANAL, V23, P661, DOI 10.1109/34.927465; HORST R., 1995, HDB GLOBAL OPTIMIZAT; Huber P., 1981, ROBUST STAT; Irani M., 1999, P WORKSH VIS ALG THE; Jin HL, 2003, VISUAL COMPUT, V19, P377, DOI 10.1007/s00371-003-0202-6; Jin HL, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P684, DOI 10.1109/ICCV.2001.937588; Jurie F., 2002, P BRIT MACHINE VISIO, V2002, P123, DOI [10.5244/C.16.10, DOI 10.5244/C.16.10]; KLINKER GJ, 1990, INT J COMPUT VISION, V4, P7, DOI 10.1007/BF00137441; La Cascia M, 2000, IEEE T PATTERN ANAL, V22, P322, DOI 10.1109/34.845375; LAI SH, 1999, IEEE COMPUTER VISION; Lucas B.D., 1981, ITERATIVE IMAGE REGI, P674; Maintz J B, 1998, Med Image Anal, V2, P1, DOI 10.1016/S1361-8415(01)80026-8; MALIS E, 2004, P IEEE INT C ROB AUT; MALIS E, 2007, P IEEE RSJ INT C INT; MEGRET R, 2008, P EUR C COMP VIS; Montesinos P, 2000, IMAGE VISION COMPUT, V18, P659, DOI 10.1016/S0262-8856(99)00070-0; NASTAR C, 1996, P EUR C COMP VIS; Negahdaripour S, 1998, IEEE T PATTERN ANAL, V20, P961, DOI 10.1109/34.713362; SHUM HY, 2000, INT J COMPUT VISION, V16, P63; SILVEIRA G, 2007, P IEEE RSJ INT C INT; Silveira G., 2007, IEEE COMPUTER VISION; Silveira G, 2008, IEEE T ROBOT, V24, P969, DOI 10.1109/TRO.2008.2004829; Szeliski R, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P273, DOI 10.1007/0-387-28831-7_17; Tan RT, 2005, IEEE T PATTERN ANAL, V27, P178, DOI 10.1109/TPAMI.2005.36; VARADARAJAN VS, 1974, LIE GROUPS LIE ALGEB; WARNER FW, 1987, FDN DIFFERENTIAL MAN	42	26	31	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2010	89	1					84	105		10.1007/s11263-010-0324-z	http://dx.doi.org/10.1007/s11263-010-0324-z			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	584RF					2022-12-18	WOS:000276769500006
J	Ommer, B; Mader, T; Buhmann, JM				Ommer, Bjorn; Mader, Theodor; Buhmann, Joachim M.			Seeing the Objects Behind the Dots: Recognition in Videos from a Moving Camera	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object recognition; Segmentation; Tracking; Video analysis; Compositionality; Visual learning	SEGMENTATION	Category-level object recognition, segmentation, and tracking in videos becomes highly challenging when applied to sequences from a hand-held camera that features extensive motion and zooming. An additional challenge is then to develop a fully automatic video analysis system that works without manual initialization of a tracker or other human intervention, both during training and during recognition, despite background clutter and other distracting objects. Moreover, our working hypothesis states that category-level recognition is possible based only on an erratic, flickering pattern of interest point locations without extracting additional features. Compositions of these points are then tracked individually by estimating a parametric motion model. Groups of compositions segment a video frame into the various objects that are present and into background clutter. Objects can then be recognized and tracked based on the motion of their compositions and on the shape they form. Finally, the combination of this flow-based representation with an appearance-based one is investigated. Besides evaluating the approach on a challenging video categorization database with significant camera motion and clutter, we also demonstrate that it generalizes to action recognition in a natural way.	[Ommer, Bjorn] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA; [Mader, Theodor; Buhmann, Joachim M.] ETH, Dept Comp Sci, Zurich, Switzerland	University of California System; University of California Berkeley; Swiss Federal Institutes of Technology Domain; ETH Zurich	Ommer, B (corresponding author), Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.	ommer@eecs.berkeley.edu; jbuhmann@inf.ethz.ch	Buhmann, Joachim/AAU-4760-2020					Avidan S, 2005, PROC CVPR IEEE, P494; Blank M, 2005, IEEE I CONF COMP VIS, P1395; Brostow G.J., 2006, P IEEE INT C COMP VI, P594, DOI DOI 10.1109/CVPR.2006.320; Brostow Gabriel J., 2008, ECCV, P44, DOI [10.1007/978-3-540-88682-2_5, DOI 10.1007/978-3-540-88682-2_5]; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; DOLLAR P, 2005, IEEE INT WORKSH VIS, P65; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Fergus R, 2003, PROC CVPR IEEE, P264; Goldberger J, 2006, IEEE T PATTERN ANAL, V28, P463, DOI 10.1109/TPAMI.2006.47; GRABNER M, 2007, P IEEE C COMP VIS PA; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; IRANI M, 1994, INT J COMPUT VISION, V12, P5, DOI 10.1007/BF01420982; JHUANG H, 2007, P IEEE INT C COMP VI; Jin Y., 2006, CVPR, V2, P2145; Kumar MP, 2008, INT J COMPUT VISION, V76, P301, DOI 10.1007/s11263-007-0064-x; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; LEIBE B, 2004, P EUR C COMP VIS WOR; Leibe B., 2007, P IEEE C COMP VIS PA; Lepetit V, 2005, PROC CVPR IEEE, P775; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucas Bruce D, 1981, P 7 INT JOINT C ART, DOI DOI 10.1042/CS0730285; Magee DR, 2002, IMAGE VISION COMPUT, V20, P581, DOI 10.1016/S0262-8856(02)00047-1; MAHINDROO A, 2002, P IND C COMP VIS GRA, P105; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; McLachlan, 1997, EM ALGORITHM EXTENSI; Niebles J.C., 2007, P IEEE C COMP VIS PA; OMMER B, 2006, P EUR C COMP VIS, P316; OMMER B, 2007, P IEEE C COMP VIS PA; Ommer B, 2007, LECT NOTES COMPUT SC, V4679, P318; PERERA AGA, 2006, P IEEE C COMP VIS PA; PONTIL M, 1998, P EUR C COMP VIS, P469; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Seemann E, 2006, LECT NOTES COMPUT SC, V4174, P242; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Sivic J, 2006, INT J COMPUT VISION, V67, P189, DOI 10.1007/s11263-005-4264-y; Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637; Vidal R, 2005, PROC CVPR IEEE, P516; Vidal R, 2003, PROC CVPR IEEE, P621; Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734; WALLRAVEN C, 2001, P IEEE C COMP VIS PA; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94; Zhang H, 2006, 2006 IEEE COMP SOC C, P2126, DOI [10.1109/CVPR.2006.301, DOI 10.1109/CVPR.2006.301]	47	26	26	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2009	83	1					57	71		10.1007/s11263-009-0211-7	http://dx.doi.org/10.1007/s11263-009-0211-7			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	413JY					2022-12-18	WOS:000263790600004
J	Civera, J; Davison, AJ; Magallon, JA; Montiel, JMM				Civera, Javier; Davison, Andrew J.; Magallon, Juan A.; Montiel, J. M. M.			Drift-Free Real-Time Sequential Mosaicing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video mosaicing; Real-time mosaicing; SLAM		We present a sequential mosaicing algorithm for a calibrated rotating camera which can for the first time build drift-free, consistent spherical mosaics in real-time, automatically and seamlessly even when previously viewed parts of the scene are re-visited. Our mosaic is composed of elastic triangular tiles attached to a backbone map of feature directions over the unit sphere built using a sequential EKF SLAM (Extend Kalman Filter Simultaneous Localization And Mapping) approach. This method represents a significant advance on previous mosaicing techniques which either require off-line optimization or which work in real-time but use local alignment of nearby images and ultimately drift. We demonstrate the system's real-time performance with real-time mosaicing results from sequences with 360 degrees pan. The system shows good global mosaicing ability despite the challenging conditions: hand-held simple low-resolution webcam, varying natural outdoor illumination, and people moving in the scene.	[Civera, Javier; Magallon, Juan A.; Montiel, J. M. M.] Univ Zaragoza, I3A, Zaragoza, Spain; [Davison, Andrew J.] Univ London Imperial Coll Sci Technol & Med, London, England	University of Zaragoza; Imperial College London	Montiel, JMM (corresponding author), Univ Zaragoza, I3A, Zaragoza, Spain.	jcivera@unizar.es; ajd@doc.ic.ac.uk; magallon@unizar.es; josemari@unizar.es	Magallón, Juan/L-3098-2014; Civera, Javier/I-3651-2015; Montiel, Jose María Martínez/A-1197-2012	Magallón, Juan/0000-0002-3355-0055; Civera, Javier/0000-0003-1368-1151; Montiel, Jose María Martínez/0000-0002-3627-7306	Spanish CICYT [DPI 2006-13578]; EPSRC [GR/T24685]; EPSRC Advanced Research Fellowship; Royal Society International Joint Project	Spanish CICYT(Consejo Interinstitucional de Ciencia y Tecnologia (CICYT)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC Advanced Research Fellowship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Royal Society International Joint Project(Royal Society of London)	Research supported by Spanish CICYT DPI 2006-13578, EPSRC GR/T24685, an EPSRC Advanced Research Fellowship to AJD, and a Royal Society International Joint Project between the University of Oxford, University of Zaragoza and Imperial College London.	Agapito L, 2001, INT J COMPUT VISION, V45, P107, DOI 10.1023/A:1012471930694; BARSHALOM Y, 1988, MATH SCI ENG, V179; Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218; Capel D, 1998, PROC CVPR IEEE, P885, DOI 10.1109/CVPR.1998.698709; CASTELLANOS JA, 1994, LECT NOTES CONTROL I, V250, P287; CASTELLANOS JA, 1999, MOBILE ROBOT LOCALIZ; CIVERA J, 2008, IEEE T ROBO IN PRESS; Davison A. J., 2003, ICCV, p[1, 4]; DAVISON AJ, 1998, ICCV, P809; EUSTICE RM, 2005, RSS; Feder HJS, 1999, INT J ROBOT RES, V18, P650, DOI 10.1177/02783649922066484; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; JUNG I, 2003, ICCV; Kim DW, 2006, J ELECTRON IMAGING, V15, DOI 10.1117/1.2194488; Kim JH, 2003, IEEE INT CONF ROBOT, P406; MARKS RL, 1995, IEEE J OCEANIC ENG, V20, P229, DOI 10.1109/48.393078; Mikhail E.M., 2001, INTRO MODERN PHOTOGR, V19; Montiel JMM, 2006, IEEE INT CONF ROBOT, P1917, DOI 10.1109/ROBOT.2006.1641986; Morimoto C, 1997, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.1997.609396; ORTIN D, 2003, ICRA; Renka RJ, 1997, ACM T MATH SOFTWARE, V23, P435, DOI 10.1145/275323.275330; SAWHNEY H, 1998, ECCV; SMITH RC, 1986, INT J ROBOT RES, V5, P56, DOI 10.1177/027836498600500404; STEEDLY D, 2005, ICCV; SZELISKI R, 1997, P SIGGRAPH, P251; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Zhu ZG, 2006, IMAGE VISION COMPUT, V24, P13, DOI 10.1016/j.imavis.2005.09.006	27	26	29	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2009	81	2					128	137		10.1007/s11263-008-0129-5	http://dx.doi.org/10.1007/s11263-008-0129-5			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	393VI					2022-12-18	WOS:000262401600002
J	Shamir, L				Shamir, Lior			Evaluation of face datasets as tools for assessing the performance of face recognition methods	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						face recognition; biometrics; FERET	QUANTITATIVE-ANALYSIS; INFORMATICS	Face datasets are considered a primary tool for evaluating the efficacy of face recognition methods. Here we show that in many of the commonly used face datasets, face images can be recognized accurately at a rate significantly higher than random even when no face, hair or clothes features appear in the image. The experiments were done by cutting a small background area from each face image, so that each face dataset provided a new image dataset which included only seemingly blank images. Then, an image classification method was used in order to check the classification accuracy. Experimental results show that the classification accuracy ranged between 13.5% (color FERET) to 99% (YaleB). These results indicate that the performance of face recognition methods measured using face image datasets may be biased. Compilable source code used for this experiment is freely available for download via the Internet.	NIA, NIH, Genet Lab, Baltimore, MD 21224 USA	National Institutes of Health (NIH) - USA; NIH National Institute on Aging (NIA)	Shamir, L (corresponding author), NIA, NIH, Genet Lab, 333 Cassell Dr, Baltimore, MD 21224 USA.	shamirl@mail.nih.gov			Intramural NIH HHS [NIH0012688432] Funding Source: Medline	Intramural NIH HHS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)		Bishop C.M, 2006, PATTERN RECOGN; Chen LF, 2001, PATTERN RECOGN, V34, P1393, DOI 10.1016/S0031-3203(00)00078-9; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Goldberg IG, 2005, GENOME BIOL, V6, DOI 10.1186/gb-2005-6-5-r47; GRADSHTEIN I, 1994, TABLE INTEGRALS SERI, P1054; GRAY SB, 1971, IEEE T COMPUT, VC 20, P551, DOI 10.1109/T-C.1971.223289; Grigorescu SE, 2002, IEEE T IMAGE PROCESS, V11, P1160, DOI 10.1109/TIP.2002.804262; GROS R, 2004, HDB FACE RECOGNITION; Gurevich I. B., 2006, Pattern Recognition and Image Analysis, V16, P265, DOI 10.1134/S1054661806030023; HADJIDEMENTRIOU E, 2001, IEEE C COMP VIS PATT, V1, P702; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; Hond D., 1997, BMVC, P0; Kong SG, 2005, COMPUT VIS IMAGE UND, V97, P103, DOI 10.1016/j.cviu.2004.04.001; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; LIM JS, 1990, SIGNALS SYSTEMS FOUR, P42; Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949; Murphy RF, 2001, 2ND ANNUAL IEEE INTERNATIONAL SYMPOSIUM ON BIOINFORMATICS AND BIOENGINEERING, PROCEEDINGS, P119, DOI 10.1109/BIBE.2001.974420; Orlov Nikita, 2007, Vision Systems - Segmentation and Pattern Recognition, P221; ORLOV N, 2008, PATTERN RECOGNITION; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790; Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X; Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027; Prewitt, 1970, PICTURE PROCESSING P, V10, P15, DOI DOI 10.4236/AD.2014.22003; Rodenacker K, 2003, ANAL CELL PATHOL, V25, P1; Samaria F., 1994, P 2 IEEE WORKSH APPL; SPACEK L, 2002, U ESSEX FACE DATABAS; Swedlow JR, 2003, SCIENCE, V300, P100, DOI 10.1126/science.1082602; TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999; TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920; ZHAO W, 2005, ACM COMPUT SURV, V35, P399	32	26	28	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2008	79	3					225	230		10.1007/s11263-008-0143-7	http://dx.doi.org/10.1007/s11263-008-0143-7			6	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	310KW	18776952	Green Accepted			2022-12-18	WOS:000256529500001
J	Chen, Y; Huang, F; Tagare, HD; Rao, M				Chen, Yunmei; Huang, Feng; Tagare, Hemant D.; Rao, Murali			A coupled minimization problem for medical image segmentation with priors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						segmentation; registration; shape prior; intensity prior; mutual information of image geometry; variational method	STATISTICAL-MODELS	We present a coupled minimization problem for image segmentation using prior shape and intensity profile. One part of the model minimizes a shape related energy and the energy of geometric active contour with a parameter that balances the influence from these two. The minimizer corresponding to a fixed parameter in this minimization gives a segmentation and an alignment between the segmentation and prior shape. The second part of this model optimizes the selection of the parameter by maximizing the mutual information of image geometry between the prior and the aligned novel image over all the alignments corresponding to different parameters in the first part. By this coupling the segmentation arrives at higher image gradient, forms a shape similar to the prior, and captures the prior intensity profile. We also propose using mutual information of image geometry to generate intensity model from a set of training images. Experimental results on cardiac ultrasound images are presented. These results indicate that the proposed model provides close agreement with expert traced borders, and the parameter determined in this model for one image can be used for images with similar properties.	Univ Florida, Dept Math, Gainesville, FL 32611 USA; Invivo Corp, Gainesville, FL 32608 USA; Yale Univ, Dept Elect Engn, Dept Diagnost Radiol, New Haven, CT 06520 USA	State University System of Florida; University of Florida; Yale University	Chen, Y (corresponding author), Univ Florida, Dept Math, Gainesville, FL 32611 USA.	yun@math.ufl.edu; fhuang@invivocorp.com; hemant.tagare@yale.edu; rao@math.ufl.edu						Ballester MAG, 2004, MED IMAGE ANAL, V8, P361, DOI 10.1016/j.media.2004.06.012; Ballester MAG, 2003, LECT NOTES COMPUT SC, V2879, P150; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chen YC, 2001, PROCEEDINGS OF THE 2001 12TH IEEE INTERNATIONAL SYMPOSIUM ON APPLICATIONS OF FERROELECTRICS, VOLS I AND II, P227, DOI 10.1109/ISAF.2000.941546; Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985; COLLIGNON A, 1995, COMP IMAG VIS, V3, P263; Cootes TF, 1999, IMAGE VISION COMPUT, V17, P567, DOI 10.1016/S0262-8856(98)00175-9; Cootes TF, 1999, LECT NOTES COMPUT SC, V1613, P322; Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915; Hermosillo G, 2002, INT J COMPUT VISION, V50, P329, DOI 10.1023/A:1020830525823; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; LEVENTON M, 2000, MATH METHODS BIOMEDI; Leventon ME, 2000, PROC CVPR IEEE, P316, DOI 10.1109/CVPR.2000.855835; MA T, 1997, THESIS YALE U; McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; PARAGIOS N, 2002, ECCV, P775; ROUSSON M, 2002, P EUR C COMP VIS, P78; SOATTO S, 2002, P 7 EUR C COMP VIS C; SUSSMAN M, 1994, J COMPUT PHYS, V114, P146, DOI 10.1006/jcph.1994.1155; SZEKELY G, MED IMAGE ANAL, V1, P199; Tagare HD, 1997, IEEE T MED IMAGING, V16, P108, DOI 10.1109/42.552060; TAO Z, 2002, INT S BIOM IM WASH D; Thevenaz P, 2000, IEEE T IMAGE PROCESS, V9, P2083, DOI 10.1109/83.887976; Tsai A, 2001, IEEE T IMAGE PROCESS, V10, P1169, DOI 10.1109/83.935033; Viola P, 1997, INT J COMPUT VISION, V24, P137, DOI 10.1023/A:1007958904918; Wang YM, 1998, PROC CVPR IEEE, P338, DOI 10.1109/CVPR.1998.698628; Wells W M 3rd, 1996, Med Image Anal, V1, P35; YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169; Zhao HK, 1996, J COMPUT PHYS, V127, P179, DOI 10.1006/jcph.1996.0167	33	26	26	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2007	71	3					259	272		10.1007/s11263-006-8524-2	http://dx.doi.org/10.1007/s11263-006-8524-2			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	104CB					2022-12-18	WOS:000241932400001
J	Laporte, C; Arbel, T				Laporte, Catherine; Arbel, Tal			Efficient discriminant viewpoint selection for active Bayesian recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						active vision; object recognition; pose estimation; Bayesian inference; efficient viewpoint selection	OBJECT RECOGNITION	This paper presents a novel viewpoint selection criterion for active object recognition and pose estimation whose key advantage resides in its low computational cost with respect to current popular approaches in the literature. The proposed observation selection criterion associates high utility with observations that predictably facilitate distinction between pairs of competing hypotheses by a Bayesian classifier. Rigorous experimentation of the proposed approach was conducted on two case studies, involving synthetic and real data, respectively. The results show the proposed algorithm to perform better than a random navigation strategy in terms of the amount of data required for recognition while being much faster than a strategy based on mutual information, without compromising accuracy.	McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2T5, Canada	McGill University	Laporte, C (corresponding author), McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2T5, Canada.	cathy@cim.mcgill.ca; arbel@cim.mcgill.ca						ALOIMONOS J, 1987, INT J COMPUT VISION, V1, P333; Arbel T, 2002, IMAGE VISION COMPUT, V20, P639, DOI 10.1016/S0262-8856(02)00053-7; Arbel T, 2001, INT J COMPUT VISION, V43, P205, DOI 10.1023/A:1011187530616; Arbel T, 2001, IMAGE VISION COMPUT, V19, P779, DOI 10.1016/S0262-8856(00)00103-7; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; Borotschnig H, 2000, IMAGE VISION COMPUT, V18, P715, DOI 10.1016/S0262-8856(99)00075-X; Callari FG, 2001, INT J COMPUT VISION, V43, P189, DOI 10.1023/A:1011135513777; Chen JH, 2004, PATTERN RECOGN, V37, P1913, DOI 10.1016/j.patcog.2003.12.003; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; DARRELL T, 1995, NEURAL INFORMATION P, V8, P858; Denzler J, 2002, IEEE T PATTERN ANAL, V24, P145, DOI 10.1109/34.982896; Dickinson SJ, 1997, COMPUT VIS IMAGE UND, V67, P239, DOI 10.1006/cviu.1997.0532; ERTIN E, 2002, P SPIE C APPL SCI CO, V4; Geman D, 2001, IEEE T INFORM THEORY, V47, P1075, DOI 10.1109/18.915664; Glover FLM., 1998, HDB COMBINATORIAL OP; Goldberger J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P487; GREMBAN KD, 1994, INT J COMPUT VISION, V12, P137, DOI 10.1007/BF01421201; Herbin S, 1996, PROC CVPR IEEE, P35, DOI 10.1109/CVPR.1996.517050; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Kovarsky AP, 1998, MRS INTERNET J N S R, V3, part. no.; Laporte C, 2004, INT C PATT RECOG, P91, DOI 10.1109/ICPR.2004.1334476; LAPORTE C, 2004, THESIS MCGLL U MONTR; MACK JA, 1992, WOLVES YELLOWSTONE R, V4, P4; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; Paletta L, 2000, ROBOT AUTON SYST, V31, P71, DOI 10.1016/S0921-8890(99)00079-2; Paletta L, 2000, INT C PATT RECOG, P695, DOI 10.1109/ICPR.2000.905482; Russel SJ, 2020, ARTIF INTELL, V4th; Schiele B, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P249, DOI 10.1109/ICCV.1998.710726; SEIBERT M, 1992, IEEE T PATTERN ANAL, V14, P107, DOI 10.1109/34.121784; Sipe MA, 2002, IEEE T PATTERN ANAL, V24, P1634, DOI 10.1109/TPAMI.2002.1114854; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Vasconcelos N, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P400, DOI 10.1109/ICCV.2001.937653; Zhou XS, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1502	34	26	26	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2006	68	3					267	287		10.1007/s11263-005-4436-9	http://dx.doi.org/10.1007/s11263-005-4436-9			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	064NU					2022-12-18	WOS:000239097200003
J	Schmid, C				Schmid, C			Weakly supervised learning of visual models and its application to content-based retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						visual model; two-layer image description; weakly supervised learning	SEGMENTATION; PERFORMANCE	This paper presents a method for weakly supervised learning of visual models. The visual model is based on a two-layer image description: a set of "generic" descriptors and their distribution over neighbourhoods. "Generic" descriptors represent sets of similar rotational invariant feature vectors. Statistical spatial constraints describe the neighborhood structure and make our description more discriminant. The joint probability of the frequencies of descriptors over a neighbourhood is multi-modal and is represented by a set of "neighbourhood-frequency" clusters. Our image description is rotationally invariant, robust to model deformations and characterizes efficiently "appearance-based" visual structure. The selection of distinctive clusters determines model features (common to the positive and rare in the negative examples). Visual models are retrieved and localized using a probabilistic score. Experimental results for "textured" animals and faces show a very good performance for retrieval as well as localization.	INRIA Rhone Alpes, F-38330 Montbonnot St Martin, France		Schmid, C (corresponding author), INRIA Rhone Alpes, 655 Av Europe, F-38330 Montbonnot St Martin, France.	Cordelia.Schmid@inrialpes.fr						Amit Y, 1999, NEURAL COMPUT, V11, P1691, DOI 10.1162/089976699300016197; Belongie S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P675, DOI 10.1109/ICCV.1998.710790; Bishop, 1995, NEURAL NETWORKS PATT; Cozzi A, 1997, MACH VISION APPL, V9, P334, DOI 10.1007/s001380050052; Duda R.O., 1973, J ROYAL STAT SOC SER; Forsyth DA, 1997, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.1997.609399; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S; KOENDERINK JJ, 1987, BIOL CYBERN, V55, P367, DOI 10.1007/BF00318371; Konishi S, 2000, PROC CVPR IEEE, P125, DOI 10.1109/CVPR.2000.855809; Lai C, 2002, LECT NOTES COMPUT SC, V2364, P212; LAZEBNIK S, 2003, P C COMP VIS PATT RE, V2, P313; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Malik J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P918, DOI 10.1109/ICCV.1999.790346; Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561; NIBLACK W, 1993, P SPIE C GEOM METH C, V2; Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689; Paragios N., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P926, DOI 10.1109/ICCV.1999.790347; Ratan A. L., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P423, DOI 10.1109/CVPR.1999.786973; Rikert T. D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1046, DOI 10.1109/ICCV.1999.790386; Rubner Y., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1018, DOI 10.1109/ICCV.1999.790380; Schmolck H, 2000, PSYCHOL SCI, V11, P39, DOI 10.1111/1467-9280.00212; Schneidereit T., 2000, INT J MED TOXICOL, V3, P1; Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Varma M, 2002, LECT NOTES COMPUT SC, V2352, P255; Vogelhuber V, 2000, INT C PATT RECOG, P1084, DOI 10.1109/ICPR.2000.905660; Weber M, 2000, LECT NOTES COMPUT SC, V1842, P18	28	26	30	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN-FEB	2004	56	1-2			SI		7	16		10.1023/B:VISI.0000004829.38247.b0	http://dx.doi.org/10.1023/B:VISI.0000004829.38247.b0			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	745MJ		Green Submitted			2022-12-18	WOS:000186692200002
J	Guichard, F; Morel, JM				Guichard, F; Morel, JM			A note on two classical enhancement filters and their associated PDE's	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	3rd International Conference on Scale-Space and Morphology held in conjunction with the 8th International Conference o n Computer Vision	JUL 07-08, 2001	VANCOUVER, CANADA	IEEE Tech Comm Pattern Analy & Machine Intelligence		shock filters; PDE; enhancement filters; deblurring filters	MORPHOLOGICAL SCALE-SPACE; MATHEMATICAL MORPHOLOGY; DIFFERENTIAL-EQUATIONS; SLOPE TRANSFORM; SIGNAL; IMAGE; DIFFUSION; LATTICE	We establish in 2D, the P.D.E. associated with a classical image enhancement filter, the Kramer operator and compare it with another classical shock filter, the Osher-Rudin filter. We show that each one corresponds to a non-flat mathematical morphology operator conditioned by a the sign of an edge detector. In the case of the Kramer operator, the equation is conditioned by the Canny edge detector while in the case of the original Rudin-Osher filter, the equation is conditioned by the sign of the Laplacian.	Poseidon Technol, F-92100 Boulogne, France; ENS, CMLA, F-91235 Cachan, France	UDICE-French Research Universities; Universite Paris Saclay	Guichard, F (corresponding author), Poseidon Technol, 3 Rue Natl, F-92100 Boulogne, France.	fguichard@poseidon.fr; morel@cmla.ens-cachan.fr		Morel, Jean-Michel/0000-0002-6108-897X				Alvarez L, 1997, SIAM J APPL MATH, V57, P153; ALVAREZ L, 1994, SIAM J NUMER ANAL, V31, P590, DOI 10.1137/0731032; Alvarez L., 1993, ARCH RATION MECH AN, V123, P200; Cao F, 1998, J MATH PURE APPL, V77, P909, DOI 10.1016/S0021-7824(01)80003-9; DORST L, 1994, SIGNAL PROCESS, V38, P79, DOI 10.1016/0165-1684(94)90058-2; ENGQUIST B, 1989, MATH COMPUT, V52, P509, DOI 10.1090/S0025-5718-1989-0955750-9; GOLDMARK, 1951, P IRE, P1314; GUICHARD F, 2000, IMAGE ITERATIVE SMOO; Heijmans H., 1994, MORPHOLOGICAL IMAGE; HEIJMANS HJAM, 1991, IEEE T PATTERN ANAL, V13, P568, DOI 10.1109/34.87343; Heijmans HJAM, 1997, SIGNAL PROCESS, V59, P17, DOI 10.1016/S0165-1684(97)00036-4; HEIJMANS HJAM, 2001, UNPUB 3 INT C SCAL S; HUMMEL RA, 1987, COMPUT VISION GRAPH, V38, P66, DOI 10.1016/S0734-189X(87)80153-6; HUMMEL RA, 1983, CVPR83, P413; JACKWAY PT, 1995, J VIS COMMUN IMAGE R, V6, P189, DOI 10.1006/jvci.1995.1017; KRAMER HP, 1975, PATTERN RECOGN, P7; LINDENBAUM M, 1994, PATTERN RECOGN, V27, P1, DOI 10.1016/0031-3203(94)90013-2; MARAGOS P, 1987, IEEE T ACOUST SPEECH, V35, P1170, DOI 10.1109/TASSP.1987.1165254; MARAGOS P, 1995, IEEE T SIGNAL PROCES, V43, P864, DOI 10.1109/78.376839; MARAGOS P, 1989, IEEE T PATTERN ANAL, V11, P586, DOI 10.1109/34.24793; MATHERON G., 1975, RANDOM SETS INTEGRAL; MEYER F, 1989, SIGNAL PROCESS, V16, P303, DOI 10.1016/0165-1684(89)90028-5; PRICE CB, 1990, IEE PROC-I, V137, P136, DOI 10.1049/ip-i-2.1990.0020; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; RUDIN LI, 1987, THESIS CALTECH; SCHAVEMAKER JGM, 1999, UNPUB PATTERN RECOGN; SCHREIBER WF, 1970, PATTERN RECOGNITION; SCOLLAR I, 1970, COMPUTER VISION GRAP, V2, P117; SERRA J, 1986, COMPUT VISION GRAPH, V35, P283, DOI 10.1016/0734-189X(86)90002-2; Serra J, 1982, IMAGE ANAL MATH MORP; STANLEY O, 1990, SIAM J NUMER ANAL, V27, P919; STERNBERG SR, 1986, COMPUT VISION GRAPH, V35, P333, DOI 10.1016/0734-189X(86)90004-6; VANDENBOOMGAARD R, 1994, IEEE T PATTERN ANAL, V16, P1101, DOI 10.1109/34.334389; YOU YL, 1997, ICIP97, V3, P388	34	26	27	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAY-JUN	2003	52	2-3					153	160		10.1023/A:1022904124348	http://dx.doi.org/10.1023/A:1022904124348			8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	659EL					2022-12-18	WOS:000181764500006
J	Gong, MG; Yang, YH				Gong, MG; Yang, YH			Genetic-based stereo algorithm and disparity map evaluation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						disparity map evaluation; multi-resolution image; stereovision; genetic algorithm; Markov random fields	COLOR IMAGE SEGMENTATION	In this paper, a new genetic-based stereo algorithm is presented. Our motivation is to improve the accuracy of the disparity map by removing the mismatches caused by both occlusions and false targets. In our approach, the stereo matching problem is considered as an optimization problem. The algorithm first takes advantage of multi-view stereo images to detect occlusions, and therefore, removes mismatches caused by visibility problems. By optimizing the compatibility between corresponding points and the continuity of the disparity map using a genetic algorithm, mismatches caused by false targets are removed. The quadtree structure is used to implement the multi-resolution framework. Since nodes at different level of the quadtree cover different number of pixels, selecting nodes at different levels gives a similar effect as adjusting the window size at different locations of the image. The experimental results show that our approach can generate more accurate disparity maps than two existing approaches. In addition, we introduce a new disparity map evaluation technique, which is developed based on a similar technique employed in the image segmentation area. Comparing with two existing evaluation approaches, the new technique can evaluate the disparity maps generated without additional knowledge of the scene, such as the correct depth information or novel views.	Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada	University of Alberta	Gong, MG (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.			Gong, Minglun/0000-0001-5820-5381				Borsotti M, 1998, PATTERN RECOGN LETT, V19, P741, DOI 10.1016/S0167-8655(98)00052-X; CHEN Q, 1999, COMPUTER VISION PATT, V1, P29; Goldberg DE, 1989, GENETIC ALGORITHMS S; Gong M., 2001, VISION INTERFACE, P71; GRIMSON WEL, 1985, IEEE T PATTERN ANAL, V7, P17, DOI 10.1109/TPAMI.1985.4767615; GRIMSON WEL, 1981, PHILOS T ROY SOC B, V292, P217, DOI 10.1098/rstb.1981.0031; Han KP, 2001, PATTERN RECOGN, V34, P1729, DOI 10.1016/S0031-3203(00)00114-X; HOFF W, 1989, IEEE T PATTERN ANAL, V11, P121, DOI 10.1109/34.16709; HOLLAND J, 1975, ADAPTATION NATURAL A; INTILLE SS, 1994, EUR C COMP VIS STOCK, P179; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; LIU JQ, 1994, IEEE T PATTERN ANAL, V16, P689, DOI 10.1109/34.297949; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; SAITO H, 1995, PATTERN RECOGN LETT, V16, P815, DOI 10.1016/0167-8655(95)00048-L; Satoh K., 1995, ACCV '95. Second Asian Conference on Computer Vision. Proceedings, P331; SATOH K, 1994, IEICE T INF SYST, VE77D, P949; SATOH K, 1996, INT C PATT REC INT A, V1, P280; SRINIVAS M, 1994, COMPUTER, V27, P17, DOI 10.1109/2.294849; SZELISKI R, 1999, INT WORKSH VIS ALG K, P1; Yagi, 1973, COMPUT VISION GRAPH, V2, P131; YANG Y, 1993, CVPR, P274; Zitnick CL, 2000, IEEE T PATTERN ANAL, V22, P675, DOI 10.1109/34.865184; ZITNICK LC, 2000, SOFTWARE ZK STEREO	23	26	31	0	7	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					63	77		10.1023/A:1014529404956	http://dx.doi.org/10.1023/A:1014529404956			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN					2022-12-18	WOS:000174354700005
J	Van Ginneken, B; Koenderink, JJ; Dana, KJ				Van Ginneken, B; Koenderink, JJ; Dana, KJ			Texture histograms as a function of irradiation and viewing direction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						texture; reflection models; BRDF; physics-based vision	SURFACES; REFLECTANCE; DIFFUSE; SHAPE; MODEL	The textural appearance of materials encountered in our daily environment depends on two directions, the irradiation and viewing direction. We investigate the bidirectional grey level histograms of a large set of materials, obtained from a texture database. We distinguish important categories, relate the various effects to physical mechanisms, and list material attributes that influence the bidirectional histograms. We use a model for rough surfaces with locally diffuse and/or specular reflection properties, a class of materials that commonly occurs, to generate bidirectional histograms and obtain close agreement with experimental data. We discuss several applications of bidirectional texture functions and histograms. In particular, we present a new approach to texture mapping based on bidirectional histograms. For 3D texture, this technique is superior to standard 2D texture mapping at hardly any extra computational cost or memory requirements.	Univ Utrecht, Helmholtz Inst, Buys Ballot Lab, NL-3584 CC Utrecht, Netherlands; Columbia Univ, Dept Comp Sci, New York, NY 10027 USA	Utrecht University; Columbia University	Van Ginneken, B (corresponding author), Univ Utrecht, Helmholtz Inst, Buys Ballot Lab, Princeton Plein 5, NL-3584 CC Utrecht, Netherlands.	bram@isi.uu.nl; j.j.koenderink@phys.uu.nl; dana@cs.columbia.edu	van Ginneken, Bram/A-3728-2012	van Ginneken, Bram/0000-0003-2028-8972				ALOIMONOS J, 1988, BIOL CYBERN, V58, P345, DOI 10.1007/BF00363944; Beckmann Petr, 1987, SCATTERING ELECTROMA, P4; BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384; Chandrasekhar S., 1960, RAD TRANSFER; Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819; Dana K. J., 1996, CUCS04896; Dana KJ, 1997, PROC CVPR IEEE, P151, DOI 10.1109/CVPR.1997.609313; DORSEY J, 1996, P SIGGRAPH 96, P387; Garding J, 1996, INT J COMPUT VISION, V17, P163, DOI 10.1007/BF00058750; GERSTL SAW, 1992, IEEE T GEOSCI REMOTE, P271; Hanrahan P., 1993, P 20 ANN C COMPUTER, P165; HE XD, 1991, COMP GRAPH, V25, P175, DOI 10.1145/127719.122738; Horn B.K.P., 1989, SHAPE SHADING; Hulst, 1981, LIGHT SCATTERING SMA; Kajiya J. T., 1989, Computer Graphics, V23, P271, DOI 10.1145/74334.74361; Koenderink JJ, 1996, J OPT SOC AM A, V13, P452, DOI 10.1364/JOSAA.13.000452; Koenderink JJ., 1996, ECCV 96 P 4 EUR C CO, P28; Kortum G., 1969, SPECTROSCOPY-US, P5; Kubelka P., 1931, Z TECH PHYS, V12, P593, DOI DOI 10.4236/MSCE.2014.28004; LEADER JC, 1979, J OPT SOC AM, V69, P610, DOI 10.1364/JOSA.69.000610; Middleton D., 1960, INTRO STAT COMMUNICA; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; Nicodemus FE, 1977, NBS MONOGRAPH, V160; OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; POULIN P, 1990, ACM COMPUTER GRAPHIC, P273; RICHARDS WA, 1982, APPL OPTICS, V21, P2569, DOI 10.1364/AO.21.002569; SMITH BG, 1967, IEEE T ANTENN PROPAG, VAP15, P668, DOI 10.1109/TAP.1967.1138991; TAGARE HD, 1991, IEEE T PATTERN ANAL, V13, P133, DOI 10.1109/34.67643; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; van Ginneken B, 1998, APPL OPTICS, V37, P130, DOI 10.1364/AO.37.000130; WESTIN SH, 1992, COMP GRAPH, V26, P255, DOI 10.1145/142920.134075; WOLFF LB, 1994, J OPT SOC AM A, V11, P2956, DOI 10.1364/JOSAA.11.002956	33	26	26	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1999	31	2-3					169	184		10.1023/A:1008018015948	http://dx.doi.org/10.1023/A:1008018015948			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	209NA					2022-12-18	WOS:000081053100006
J	Uras, C; Verri, A				Uras, C; Verri, A			Computing size functions from edge maps	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OBJECT RECOGNITION; DISTANCE; IMAGES	Size functions are integer valued functions of two real variables which have been recently proposed for the representation and recognition of shape. A main limitation of the theory of size functions appeared to be the fragility of the produced representation with respect to edge fragmentation. In this paper it is shown that size functions can actually be defined without making assumptions on the topological structure of the viewed shape. Consequently, size functions can be profitably used even in the presence of fragmented edge maps. In order to demonstrate the potential of size functions for computer vision, a system for shape recognition is described and tested on two different domains. The very good performances of the system indicate that size functions are extremely effective for the analysis of shapes for which geometric models might be difficult to obtain.			Uras, C (corresponding author), UNIV GENOA,DIPARTIMENTO FIS,VIA DODECANESO 33,I-16146 GENOA,ITALY.							AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; Frosini, 1992, J COMBIN INFORM SYST, V17, P232; FROSINI P, 1990, B AUST MATH SOC, V42, P407, DOI 10.1017/S0004972700028574; FROSINI P, 1991, P SPIE INT ROB COMP, V1607, P122; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; HUTTENLOCHER DP, 1989, 104 MIT AI LAB; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920; ULLMAN S, 1987, 931 MIT AI LAB; URAS C, 1994, NATO ASI SERIES F, V126, P81; URAS C, 1994, P 12 IAPR INT C PATT, V2, P334; VERRI A, 1993, BIOL CYBERN, V70, P99, DOI 10.1007/BF00200823; VERRI A, 1994, LECT NOTES COMPUTER, V825, P215; VERRI A, 1996, UNPUB IMAGE VISION C, V14, P189	17	26	26	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1997	23	2					169	183		10.1023/A:1007910913691	http://dx.doi.org/10.1023/A:1007910913691			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XK384					2022-12-18	WOS:A1997XK38400004
J	Bruce, JW; Giblin, PJ; Tari, F				Bruce, JW; Giblin, PJ; Tari, F			Ridges, crests and sub-parabolic lines of evolving surfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							REPRESENTATION; SINGULARITIES; R3	The ridge lines on a surface can be defined either via contact of the surface with spheres, or via extrema of principal curvatures along lines of curvature. Certain subsets of ridge lines called crest lines have been singled out by some authors for medical imaging applications. There is a related concept of sub-parabolic line on a surface, also defined via extrema of principal curvatures. In this paper we study in detail the structure of the ridge lines, crest lines and sub-parabolic lines on a generic surface, and on a surface which is evolving in a generic (one-parameter) family. The mathematical details of this study are in Bruce et al. (1944c).			Bruce, JW (corresponding author), UNIV LIVERPOOL, DEPT PURE MATH, LIVERPOOL L69 3BX, MERSEYSIDE, ENGLAND.		Tari, Farid/F-2550-2011	Tari, Farid/0000-0002-9398-7275				Arnold VI, 1986, BASIC QUEUEING THEOR, V2nd; Bruce J.W., 1995, REAL COMPLEX SINGULA, VVolume 333, P148; Bruce J.W., 1992, CURVES SINGULARITIES; Bruce JW, 1996, INT J COMPUT VISION, V17, P291, DOI 10.1007/BF00128235; BRUCE JW, 1989, P ROY SOC EDINB A, V111, P147, DOI 10.1017/S0308210500025087; BRUCE JW, 1984, MATH SCAND, V54, P262, DOI 10.7146/math.scand.a-12058; BRUCE JW, 1994, FAMILIES SURFACES HE; BRUCE JW, 1994, FAMILIES SURFACES FO; BRUCE JW, 1994, IN PRESS P ROYAL SOC; BRUCE JW, 1991, LECT NOTES MATH, V1462, P63; COLCHESTER ACF, 1990, NATO ADV SCI I F-COM, V60, P45; CROWLEY JL, 1984, IEEE T PATTERN ANAL, V6, P156, DOI 10.1109/TPAMI.1984.4767500; Eisenhart LP, 1909, DIFFERENTIAL GEOMETR; GORDON AP, 1991, P SPIE C GEOM METH C; GUEZIEC A, 1995, IEEE COMPUT SCI ENG, V2, P69, DOI 10.1109/99.372946; Koenderink J., 1990, SOLID SHAPE; Morris R.J., 1991, THESIS U LIVERPOOL; MORRIS RJ, 1994, IN PRESS MATH SURFAC, V6; ONeill B., 1966, ELEMENTARY DIFFERENT; PORTEOUS IR, 1983, P SYMP PURE MATH, V40, P379; PORTEOUS IR, 1994, GEOMETRIC DIFFERENTI; SANDER PT, 1992, IEEE T PATTERN ANAL, V14, P309, DOI 10.1109/34.120326; SOTTOMAYER J, 1982, ASTERISQUE, V98, P195; THIRION JP, 1995, COMPUT VIS IMAGE UND, V61, P190, DOI 10.1006/cviu.1995.1015; THIRION JP, 1996, IN PRESS INT J COMPU; WILKINSON TC, 1991, THESIS U NEWCASTLE T	26	26	26	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1996	18	3					195	210		10.1007/BF00123141	http://dx.doi.org/10.1007/BF00123141			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VB108					2022-12-18	WOS:A1996VB10800001
J	Wojna, Z; Ferrari, V; Guadarrama, S; Silberman, N; Chen, LC; Fathi, A; Uijlings, J				Wojna, Zbigniew; Ferrari, Vittorio; Guadarrama, Sergio; Silberman, Nathan; Chen, Liang-Chieh; Fathi, Alireza; Uijlings, Jasper			The Devil is in the Decoder: Classification, Regression and GANs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Machine vision; Computer vision; Neural network architectures; Decoders; 2D imagery; Per-pixel prediction; Semantic segmentation; Depth prediction; GANs		Many machine vision applications, such as semantic segmentation and depth prediction, require predictions for every pixel of the input image. Models for such problems usually consist of encoders which decrease spatial resolution while learning a high-dimensional representation, followed by decoders who recover the original input resolution and result in low-dimensional predictions. While encoders have been studied rigorously, relatively few studies address the decoder side. This paper presents an extensive comparison of a variety of decoders for a variety of pixel-wise tasks ranging from classification, regression to synthesis. Our contributions are: (1) decoders matter: we observe significant variance in results between different types of decoders on various problems. (2) We introduce new residual-like connections for decoders. (3) We introduce a novel decoder: bilinear additive upsampling. (4) We explore prediction artifacts.	[Wojna, Zbigniew] UCL, London, England; [Ferrari, Vittorio; Guadarrama, Sergio; Silberman, Nathan; Chen, Liang-Chieh; Fathi, Alireza; Uijlings, Jasper] Google Inc, Mountain View, CA USA	University of London; University College London; Google Incorporated	Wojna, Z (corresponding author), UCL, London, England.	zbigniewwojna@gmail.com; vittoferrari@google.com; sguada@google.com; nsilberman@google.com; lcchen@google.com; alirezafathi@google.com; jrru@google.com		Wojna, Zbigniew/0000-0002-9629-5688				Alvarez J. M., 2016, ARXIV160605426 CORR; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Bishop, 1995, NEURAL NETWORKS PATT; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Everingham M., 2012, PASCAL VISUAL OBJECT; Francesco Visin, 2018, Arxiv, DOI arXiv:1603.07285; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heusel M., 2017, GANS TRAINED 2 TIME, P6629; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22; IIZUKA S, 2016, ACM T GRAPHIC, V35, DOI DOI 10.1145/2897824.2925974; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Kendall A, 2015, P BRIT MACH VIS C 20; Khoreva A, 2016, PROC CVPR IEEE, P183, DOI 10.1109/CVPR.2016.27; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Nair V, 2010, P 27 INT C MACHINE L, P807; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Odena A, 2016, DISTILL, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003]; Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Ripley BD., 1996; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Silberman Nathan, 2012, EUR C COMP VIS, DOI 10.1007/978-3-642-33715-4_54; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tighe Joseph, 2010, ECCV, DOI [10.1007/978-3-642-15555-0_26, DOI 10.1007/978-3-642-15555-0_26]; Uijlings JRR, 2015, PROC CVPR IEEE, P4712, DOI 10.1109/CVPR.2015.7299103; Xu Jia, 2017, ARXIV171206463; Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20; Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40	60	25	26	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1694	1706		10.1007/s11263-019-01170-8	http://dx.doi.org/10.1007/s11263-019-01170-8			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY		Green Submitted			2022-12-18	WOS:000492425300007
J	Liu, HM; Wang, RP; Shan, SG; Chen, XL				Liu, Haomiao; Wang, Ruiping; Shan, Shiguang; Chen, Xilin			Deep Supervised Hashing for Fast Image Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image retrieval; Hashing; Convolutional network; Contrastive loss; Triplet ranking loss	REPRESENTATION; QUANTIZATION	In this paper, we present a new hashing method to learn compact binary codes for highly efficient image retrieval on large-scale datasets. While the complex image appearance variations still pose a great challenge to reliable retrieval, in light of the recent progress of Convolutional Neural Networks (CNNs) in learning robust image representation on various vision tasks, this paper proposes a novel Deep Supervised Hashing method to learn compact similarity-preserving binary code for the huge body of image data. Specifically, we devise a CNN architecture that takes pairs/triplets of images as training inputs and encourages the output of each image to approximate discrete values (e.g. +1). To this end, the loss functions are elaborately designed to maximize the discriminability of the output space by encoding the supervised information from the input image pairs/triplets, and simultaneously imposing regularization on the real-valued outputs to approximate the desired discrete values. For image retrieval, new-coming query images can be easily encoded by forward propagating through the network and then quantizing the network outputs to binary codes representation. Extensive experiments on three large scale datasets CIFAR-10, NUS-WIDE, and SVHN show the promising performance of our method compared with the state-of-the-arts.	[Liu, Haomiao; Wang, Ruiping; Shan, Shiguang; Chen, Xilin] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Liu, Haomiao; Wang, Ruiping; Shan, Shiguang; Chen, Xilin] Univ Chinese Acad Sci, Beijing 100049, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Wang, RP (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Wang, RP (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.	haomiao.liu@vipl.ict.ac.cn; wangruiping@ict.ac.cn; sgshan@ict.ac.cn; xlchen@ict.ac.cn	Chen, Xilin/I-4153-2014	Chen, Xilin/0000-0003-3024-4404; Shan, Shiguang/0000-0002-8348-392X	973 Program [2015CB351802]; Natural Science Foundation of China [61390511, 61772500]; Frontier Science Key Research Project CAS [QYZDJ-SSW-JSC009]; Youth Innovation Promotion Association CAS [2015085]	973 Program(National Basic Research Program of China); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Frontier Science Key Research Project CAS; Youth Innovation Promotion Association CAS	This work is partially supported by 973 Program under Contract No. 2015CB351802, Natural Science Foundation of China under Contracts Nos. 61390511, 61772500, Frontier Science Key Research Project CAS No. QYZDJ-SSW-JSC009, and Youth Innovation Promotion Association CAS No. 2015085.	Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Glorot X., 2010, PROC MACH LEARN RES, P249; Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hermans Alexander, 2017, ARXIV170307737; Huang R, 2017, ALIMENT PHARM THER, V46, P769, DOI 10.1111/apt.14266; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348; Kang WC, 2016, AAAI CONF ARTIF INTE, P1230; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; Lin K, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301269; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu L, 2017, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR.2017.247; Liu W., 2014, ADV NEURAL INFORM PR, V4, P3419; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mohammad Norouzi, 2012, ADV NEURAL INFORM PR, P1061; Nair V, 2010, P 27 INT C MACHINE L, P807; Nan DL, 2016, C IND ELECT APPL, P1175, DOI 10.1109/ICIEA.2016.7603762; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Rastegari M, 2012, LECT NOTES COMPUT SC, V7577, P876, DOI 10.1007/978-3-642-33783-3_63; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29; Soudry D., 2014, PROC 27 INT C NEURAL, P963, DOI DOI 10.5555/2968826.2968934; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szegedy C, 2013, ADV NEURAL INFORM PR, P2553; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48; Wang X., 2016, P AS C COMP VIS, P70; Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824; Xia R, 2014, 2014 INTERNATIONAL CONFERENCE ON POWER SYSTEM TECHNOLOGY (POWERCON), P1521, DOI 10.1109/POWERCON.2014.6993796; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang ZM, 2016, PROC CVPR IEEE, P1487, DOI 10.1109/CVPR.2016.165; Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763	48	25	28	1	26	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2019	127	9					1217	1234		10.1007/s11263-019-01174-4	http://dx.doi.org/10.1007/s11263-019-01174-4			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IL9YV					2022-12-18	WOS:000477642300003
J	Huang, HB; He, R; Sun, ZN; Tan, TN				Huang, Huaibo; He, Ran; Sun, Zhenan; Tan, Tieniu			Wavelet Domain Generative Adversarial Network for Multi-scale Face Hallucination	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face hallucination; Super-resolution; Wavelet transform; Generative adversarial network; Face recognition	SUPERRESOLUTION; IMAGE	Most modern face hallucination methods resort to convolutional neural networks (CNN) to infer high-resolution (HR) face images. However, when dealing with very low-resolution (LR) images, these CNN based methods tend to produce over-smoothed outputs. To address this challenge, this paper proposes a wavelet-domain generative adversarial method that can ultra-resolve a very low-resolution (like 16x16 or even 8x8) face image to its larger version of multiple upscaling factors (2x to 16x) in a unified framework. Different from the most existing studies that hallucinate faces in image pixel domain, our method firstly learns to predict the wavelet information of HR face images from its corresponding LR inputs before image-level super-resolution. To capture both global topology information and local texture details of human faces, a flexible and extensible generative adversarial network is designed with three types of losses: (1) wavelet reconstruction loss aims to push wavelets closer with the ground-truth; (2) wavelet adversarial loss aims to generate realistic wavelets; (3) identity preserving loss aims to help identity information recovery. Extensive experiments demonstrate that the presented approach not only achieves more appealing results both quantitatively and qualitatively than state-of-the-art face hallucination methods, but also can significantly improve identification accuracy for low-resolution face images captured in the wild.	[Huang, Huaibo; He, Ran; Sun, Zhenan; Tan, Tieniu] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China; [Huang, Huaibo; He, Ran; Sun, Zhenan; Tan, Tieniu] CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; [Huang, Huaibo; He, Ran; Sun, Zhenan; Tan, Tieniu] CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Huang, Huaibo; He, Ran; Sun, Zhenan; Tan, Tieniu] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences	He, R (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; He, R (corresponding author), CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.; He, R (corresponding author), CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China.; He, R (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.	huaibo.huang@cripac.ia.ac.cn; rhe@nlpr.ia.ac.cn; znsun@nlpr.ia.ac.cn; tnt@nlpr.ia.ac.cn		Huang, Huaibo/0000-0001-5866-2283	State Key Development Program [2016YFB1001001]; National Natural Science Foundation of China [61622310, 61427811]; Beijing Natural Science Foundation [JQ18017]	State Key Development Program; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work is partially funded by the State Key Development Program (Grant No. 2016YFB1001001), National Natural Science Foundation of China (Grant No. 61622310, 61427811), and Beijing Natural Science Foundation (Grants No. JQ18017).	Anbarjafari G, 2010, ETRI J, V32, P390, DOI 10.4218/etrij.10.0109.0303; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bruna J., 2016, INT C LEARN REPR; Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019; Bulat A, 2018, LECT NOTES COMPUT SC, V11210, P187, DOI 10.1007/978-3-030-01231-1_12; Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043; Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264; COIFMAN RR, 1992, IEEE T INFORM THEORY, V38, P713, DOI 10.1109/18.119732; Dahl R, 2017, IEEE I CONF COMP VIS, P5449, DOI 10.1109/ICCV.2017.581; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; FARRUGIA RA, 1977, TIP, V26, P4562, DOI DOI 10.1109/TIP.2017.2717181; Gao X, 2016, IEEE IMAGE PROC, P1439, DOI 10.1109/ICIP.2016.7532596; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hayat M, 2017, INT J COMPUT VISION, V123, P479, DOI 10.1007/s11263-017-1000-3; Huang Gary B., 2007, 0749 U MASS, P7; Huang H., 2018, P ADV NEURAL INFORM, P52; Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187; Huang Jia-Bin, 2015, CVPR, DOI DOI 10.1109/CVPR.2015.7299156; JI H, 2009, TPAMI, V31, P649, DOI DOI 10.1109/TPAMI.2008.103; Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jung CK, 2011, IEEE SIGNAL PROC LET, V18, P367, DOI 10.1109/LSP.2011.2140370; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Lai Wei-Sheng, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li B, 2009, IEEE SIGNAL PROC LET, V16, P957, DOI 10.1109/LSP.2009.2027657; Lin ZC, 2008, INT J COMPUT VISION, V80, P406, DOI 10.1007/s11263-008-0148-2; Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019; Mallat S, 1996, P IEEE, V84, P604, DOI 10.1109/5.488702; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Mallat S, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0203; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Naik S., 2013, INT J MULTIMEDIA ITS, V5, P23, DOI DOI 10.5121/ijma.2013.5402; Nguyen N, 2000, CIRC SYST SIGNAL PR, V19, P321, DOI 10.1007/BF01200891; Odena A, 2017, PR MACH LEARN RES, V70; PARK JS, 1971, TIP, V17, P1806, DOI DOI 10.1109/TIP.2008.2001394; Parkhi Omkar M., 2015, BRIT MACH VIS C; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481; Shamir L, 2008, INT J COMPUT VISION, V79, P225, DOI 10.1007/s11263-008-0143-7; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Singh A, 2014, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2014.364; Sohn Kihyuk, 2017, P IEEE INT C COMP VI, P3210; Sonderby C.K., 2017, AMORTISED MAP INFERE; Sun JG, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MANAGEMENT OF INNOVATION AND TECHNOLOGY, VOLS 1-3, P18, DOI 10.1109/ICMIT.2008.4654330; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514; van den Oord Aaron, 2016, ARXIV160605328; Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9; Wang P, 2018, P ASME INT C OCEAN; Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171; Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36; Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75; Yang CY, 2013, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR.2013.146; Yang CY, 2017, INT J COMPUTER VISIO; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yang J, 2008, IEEE IMAGE PROC, P1264, DOI 10.1109/ICIP.2008.4711992; Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14; Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101; Yu X, 2017, AAAI CONF ARTIF INTE, P4327; Yu X, 2017, PROC CVPR IEEE, P5367, DOI 10.1109/CVPR.2017.570; Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhao SB, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P953; Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37	71	25	26	0	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		763	784		10.1007/s11263-019-01154-8	http://dx.doi.org/10.1007/s11263-019-01154-8			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	HZ0JD					2022-12-18	WOS:000468525900013
J	Gilbert, A; Trumble, M; Malleson, C; Hilton, A; Collomosse, J				Gilbert, Andrew; Trumble, Matthew; Malleson, Charles; Hilton, Adrian; Collomosse, John			Fusing Visual and Inertial Sensors with Semantics for 3D Human Pose Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D pose estimation; Sensor fusion; Deep neural networks; Multi viewpoint video; Inertial measurement units		We propose an approach to accurately estimate 3D human pose by fusing multi-viewpoint video (MVV) with inertial measurement unit (IMU) sensor data, without optical markers, a complex hardware setup or a full body model. Uniquely we use a multi-channel 3D convolutional neural network to learn a pose embedding from visual occupancy and semantic 2D pose estimates from the MVV in a discretised volumetric probabilistic visual hull. The learnt pose stream is concurrently processed with a forward kinematic solve of the IMU data and a temporal model (LSTM) exploits the rich spatial and temporal long range dependencies among the solved joints, the two streams are then fused in a final fully connected layer. The two complementary data sources allow for ambiguities to be resolved within each sensor modality, yielding improved accuracy over prior methods. Extensive evaluation is performed with state of the art performance reported on the popular Human 3.6M dataset(Ionescu et al. in Intell IEEE Trans Pattern Anal Mach 36(7):1325-1339, 2014), the newly released TotalCapture dataset and a challenging set of outdoor videos TotalCaptureOutdoor. We release the new hybrid MVV dataset (TotalCapture) comprising of multi-viewpoint video, IMU and accurate 3D skeletal joint ground truth derived from a commercial motion capture system. The dataset is available online at http://cvssp.org/data/totalcapture/.	[Gilbert, Andrew; Trumble, Matthew; Malleson, Charles; Hilton, Adrian; Collomosse, John] Univ Surrey, CVSSP, Guildford GU2 7XH, Surrey, England	University of Surrey	Gilbert, A (corresponding author), Univ Surrey, CVSSP, Guildford GU2 7XH, Surrey, England.	a.gilbert@surrey.ac.uk	Hilton, Adrian/N-3736-2014	Hilton, Adrian/0000-0003-4223-238X	EPSRC doctoral bursary; InnovateUK via the Total Capture Project [102685]; Visual Media project (EU H2020 Grant) [687800]; EPSRC [EP/P022529/1, EP/M028321/1] Funding Source: UKRI	EPSRC doctoral bursary(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); InnovateUK via the Total Capture Project; Visual Media project (EU H2020 Grant); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The work was supported by an EPSRC doctoral bursary and InnovateUK via the Total Capture Project, Grant Agreement 102685. The work was supported in part by the Visual Media project (EU H2020 Grant 687800) and through the donation of GPU hardware by Nvidia.	Agarwal A., 2004, P CVPR; Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Andrews S., 2016, CVMP; Andriluka M., 2009, P COMP VIS PATT REC; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; [Anonymous], 2016, ARXIV161105708; BAAK A., 2010, EUR C COMP VIS, P139; Cao Z., 2016, ECCV 16; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chung J., 2014, ARXIV14123555; Dauphin Y. N., 2015, ARXIV150204390, V28, P1504; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Elhayek A, 2015, PROC CVPR IEEE, P3810, DOI 10.1109/CVPR.2015.7299005; Grauman K., 2003, P CVPR; Graves A, 2013, ARXIV13080850; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Helten T, 2013, IEEE I CONF COMP VIS, P1105, DOI 10.1109/ICCV.2013.141; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang P, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2699643; Huang Yinghao, 2017, 3DV; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jiang H, 2009, INT C COMP VIS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lan XY, 2005, IEEE I CONF COMP VIS, P470; Lassner Christoph, 2017, CVPR; Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu H, 2011, S INTERACT D, P133, DOI [10.1145/1944745.1944768, DOI 10.1145/1944745.1944768]; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Malleson C, 2017, INT CONF 3D VISION, P449, DOI 10.1109/3DV.2017.00058; Marcard T. V., 2016, TECHNICAL REPORT; Martinez Julieta, 2017, ICCV; Mude L., 2017, CVPR; Park D., 2015, P CHA LEARN WORKSH L; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Pons-Moll G, 2011, IEEE I CONF COMP VIS, P1243, DOI 10.1109/ICCV.2011.6126375; Pons-Moll G, 2010, PROC CVPR IEEE, P663, DOI 10.1109/CVPR.2010.5540153; Ren RD, 2012, IEEE T MULTIMEDIA, V14, P1652, DOI 10.1109/TMM.2012.2199971; Ren XF, 2005, IEEE I CONF COMP VIS, P824; Rhodin H, 2016, LECT NOTES COMPUT SC, V9909, P509, DOI 10.1007/978-3-319-46454-1_31; Roetenberg D., 2009, XSENS MVN FULL 6D HU; Rogez G, 2016, ADV NEUR IN, V29; Sak H, 2014, INTERSPEECH, P338; Sanzari M, 2016, LECT NOTES COMPUT SC, V9912, P566, DOI 10.1007/978-3-319-46484-8_34; Schwarz LA, 2009, LECT NOTES COMPUT SC, V5903, P159, DOI 10.1007/978-3-642-10470-1_14; Slyper R., 2008, P 2008 ACM SIGGRAPHE, P193, DOI DOI 10.2312/SCA/SCA08/193-199; Srinivasan P., 2007, CVPR, P1; Tan J. K. V., 2017, BMVC; Tekin Bugra, 2016, BRIT MACH VIS C BMVC, P2; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Trumble M., 2018, EUR C COMP VIS ECCV; Trumble M, 2016, P 13 EUR C VIS MED P, DOI 10.1145/2998559.2998565; Trumble M., 2017, 2017 BRIT MACH VIS C; von Marcard T, 2016, IEEE T PATTERN ANAL, V38, P1533, DOI 10.1109/TPAMI.2016.2522398; von Marcard Timo, 2017, COMPUTER GRAPHICS FO; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Yub H. J., 2016, P EUR C COMP VIS ECC; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537	62	25	25	1	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2019	127	4					381	397		10.1007/s11263-018-1118-y	http://dx.doi.org/10.1007/s11263-018-1118-y			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HN4DR		Green Published, hybrid			2022-12-18	WOS:000460135000004
J	Egger, B; Schonborn, S; Schneider, A; Kortylewski, A; Morel-Forster, A; Blumer, C; Vetter, T				Egger, Bernhard; Schonborn, Sandro; Schneider, Andreas; Kortylewski, Adam; Morel-Forster, Andreas; Blumer, Clemens; Vetter, Thomas			Occlusion-Aware 3D Morphable Models and an Illumination Prior for Face Image Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face image analysis; Markov chain Monte Carlo; Morphable model; Generative models; Occlusion-aware model fitting; Inverse rendering; Robust illumination estimation; Illumination prior		Faces in natural images are often occluded by a variety of objects. We propose a fully automated, probabilistic and occlusion-aware 3D morphable face model adaptation framework following an analysis-by-synthesis setup. The key idea is to segment the image into regions explained by separate models. Our framework includes a 3D morphable face model, a prototype-based beard model and a simple model for occlusions and background regions. The segmentation and all the model parameters have to be inferred from the single target image. Face model adaptation and segmentation are solved jointly using an expectation-maximization-like procedure. During the E-step, we update the segmentation and in the M-step the face model parameters are updated. For face model adaptation we apply a stochastic sampling strategy based on the Metropolis-Hastings algorithm. For segmentation, we apply loopy belief propagation for inference in a Markov random field. Illumination estimation is critical for occlusion handling. Our combined segmentation and model adaptation needs a proper initialization of the illumination parameters. We propose a RANSAC-based robust illumination estimation technique. By applying this method to a large face image database we obtain a first empirical distribution of real-world illumination conditions. The obtained empirical distribution is made publicly available and can be used as prior in probabilistic frameworks, for regularization or to synthesize data for deep learning methods.	[Egger, Bernhard; Schonborn, Sandro; Schneider, Andreas; Kortylewski, Adam; Morel-Forster, Andreas; Blumer, Clemens; Vetter, Thomas] Univ Basel, Dept Math & Comp Sci, Basel, Switzerland	University of Basel	Egger, B (corresponding author), Univ Basel, Dept Math & Comp Sci, Basel, Switzerland.	bernhard.egger@unibas.ch	Egger, Bernhard/AAE-5389-2019	Egger, Bernhard/0000-0002-4736-2397; Schneider, Andreas/0000-0002-5972-7066; Schonborn, Sandro/0000-0003-4316-8519; Kortylewski, Adam/0000-0002-9146-4403	Schweizerischer Nationalfonds zur Forderung der Wissenschaftlichen Forschung [SNF153297]	Schweizerischer Nationalfonds zur Forderung der Wissenschaftlichen Forschung(Austrian Science Fund (FWF))	Funding was provided by Schweizerischer Nationalfonds zur Forderung der Wissenschaftlichen Forschung (Grant No. SNF153297).	Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206; [Anonymous], 2016, CVPR; Arthur D, 2007, SOC IND APPL MATH, P1027, DOI DOI 10.1145/1283383.1283494; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Dalca AV, 2014, LECT NOTES COMPUT SC, V8674, P773, DOI 10.1007/978-3-319-10470-6_96; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; DeSmet M., 2006, CVPR, V2, P1423; Egger B, 2017, THESIS; Egger B., 2016, BRIT MACH VIS C BMVC; Egger B., 2017, STAT SHAPE DEFORMATI, P115; Egger B, 2014, LECT NOTES COMPUT SC, V8753, P317, DOI 10.1007/978-3-319-11752-2_25; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gerig T., 2017, ARXIV170908398; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Huang G.B., 2008, WORKSHOP FACESREAL L; Huang R, 2004, PROC CVPR IEEE, P739; Huber P, 2015, IEEE IMAGE PROC, P1195, DOI 10.1109/ICIP.2015.7350989; Koestinger M., 2011, ICCV WORKSH, DOI [10.1109/ICCVW.2011.6130513, DOI 10.1109/ICCVW.2011.6130513]; Kortylewski A., 2017, THESIS; Kortylewski A., 2017, ARXIV171201619; Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068; Le THN, 2015, INT CONF BIOMETR, P507, DOI 10.1109/ICB.2015.7139066; Luthi M., 2012, INSIGHT J, V1, P1; Maninchedda F, 2016, LECT NOTES COMPUT SC, V9910, P667, DOI 10.1007/978-3-319-46466-4_40; Marschner SR, 1997, FIFTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS, AND APPLICATIONS, P262; Martinez A., 1998, 24 CVC, P24; Morel-Forster A., 2017, THESIS; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106; Nguyen MH, 2008, COMPUT GRAPH FORUM, V27, P627, DOI 10.1111/j.1467-8659.2008.01160.x; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Pierrard J.-S., 2007, P IEEE C COMP VIS PA, P1, DOI [DOI 10.1109/CVPR.2007.383264, DOI 10.1145/1362622.1362660]; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Richardson E., 2016, ARXIV160904387; Romdhani S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P59; Saito S, 2016, LECT NOTES COMPUT SC, V9912, P244, DOI 10.1007/978-3-319-46484-8_15; Schneider Andreas, 2017, P IEEE C COMP VIS PA, P3865; Schonborn S, 2013, LECT NOTES COMPUT SC, V8142, P101, DOI 10.1007/978-3-642-40602-7_11; Schonborn S, 2017, INT J COMPUT VISION, V123, P160, DOI 10.1007/s11263-016-0967-5; Shahlaei Davoud, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163128; Tewari A., 2017, ARXIV170310580; Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x; Uricar Michal, 2015, P 2015 11 IEEE INT C, V2, P1; Yildirim I, 2017, 39 ANN C COGN SCI SO; Zhu X., 2015, P 11 IEEE INT C AUT; Zhu X., 2016, CVPR; Zivanov J, 2013, INT CONF BIOMETR	51	25	25	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2018	126	12			SI		1269	1287		10.1007/s11263-018-1064-8	http://dx.doi.org/10.1007/s11263-018-1064-8			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GZ3KT		Green Accepted			2022-12-18	WOS:000449286200002
J	Wang, LM; Wang, Z; Qiao, Y; Van Gool, L				Wang, Limin; Wang, Zhe; Qiao, Yu; Van Gool, Luc			Transferring Deep Object and Scene Representations for Event Recognition in Still Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Event recognition; Deep learning; Transfer learning; Multitask learning	ADAPTATION	This paper addresses the problem of image-based event recognition by transferring deep representations learned from object and scene datasets. First we empirically investigate the correlation of the concepts of object, scene, and event, thus motivating our representation transfer methods. Based on this empirical study, we propose an iterative selection method to identify a subset of object and scene classes deemed most relevant for representation transfer. Afterwards, we develop three transfer techniques: (1) initialization-based transfer, (2) knowledge-based transfer, and (3) data-based transfer. These newly designed transfer techniques exploit multitask learning frameworks to incorporate extra knowledge from other networks or additional datasets into the fine-tuning procedure of event CNNs. These multitask learning frameworks turn out to be effective in reducing the effect of over-fitting and improving the generalization ability of the learned CNNs. We perform experiments on four event recognition benchmarks: the ChaLearn LAP Cultural Event Recognition dataset, the Web Image Dataset for Event Recognition, the UIUC Sports Event dataset, and the Photo Event Collection dataset. The experimental results show that our proposed algorithm successfully transfers object and scene representations towards the event dataset and achieves the current state-of-the-art performance on all considered datasets.	[Wang, Limin; Van Gool, Luc] ETH, Comp Vis Lab, Zurich, Switzerland; [Wang, Zhe] Univ Calif Irvine, Dept Comp Sci, Irvine, CA USA; [Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China	Swiss Federal Institutes of Technology Domain; ETH Zurich; University of California System; University of California Irvine; Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS	Wang, LM (corresponding author), ETH, Comp Vis Lab, Zurich, Switzerland.	07wanglimin@gmail.com; buptwangzhe2012@gmail.com; yu.qiao@siat.ac.cn; vangool@vision.ee.ethz.ch	Qiao, Yu/ABD-5787-2021; Wang, Limin/AAE-3419-2019	Wang, Limin/0000-0002-3674-7718; Wang, Zhe/0000-0002-1385-9012	ERC Advanced Grant VarCity; Toyota Research Project TRACE-Zurich; National Key Research and Development Program of China [2016YFC1400704]; National Natural Science Foundation of China [U1613211, 61633021]; External Cooperation Program of BIC Chinese Academy of Sciences [172644KYSB20150019, 172644KYSB20160033]	ERC Advanced Grant VarCity; Toyota Research Project TRACE-Zurich; National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); External Cooperation Program of BIC Chinese Academy of Sciences	This work is partially supported by the ERC Advanced Grant VarCity, the Toyota Research Project TRACE-Zurich, the National Key Research and Development Program of China (2016YFC1400704), the National Natural Science Foundation of China (U1613211, 61633021), and the External Cooperation Program of BIC Chinese Academy of Sciences (172644KYSB20150019, 172644KYSB20160033).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Azizpour Hossein, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301270; Baro X, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301329; Bhattacharya S, 2014, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2014.287; Bossard L, 2013, IEEE I CONF COMP VIS, P1193, DOI 10.1109/ICCV.2013.151; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Cheng D, 2015, IEEE INT C SEMANT CO, P32, DOI 10.1109/ICOSC.2015.7050775; COOPER M, 2003, ACM MULTIMEDIA, P364; Das A., 2012, P NEUR INF PROC SYST, P1592; Delaitre Vincent, 2011, NIPS; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Desai C, 2012, LECT NOTES COMPUT SC, V7575, P158, DOI 10.1007/978-3-642-33765-9_12; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114; Ebadollahi S, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P881, DOI 10.1109/ICME.2006.262691; Escalera S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P243, DOI 10.1109/ICCVW.2015.40; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872; Gao B., 2015, ABS150405277 CORR; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gong BQ, 2014, INT J COMPUT VISION, V109, P3, DOI 10.1007/s11263-014-0718-4; Habibian A, 2014, COMPUT VIS IMAGE UND, V124, P110, DOI 10.1016/j.cviu.2014.02.003; Hauptmann AG, 2008, P IEEE, V96, P602, DOI 10.1109/JPROC.2008.916355; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31; Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599; Jia Y., 2014, P 22 ACM INT C MULT, P675; Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li LL, 2007, NANOTECHNOLOGY, V18, DOI 10.1088/0957-4484/18/40/405102; Limin Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P30, DOI 10.1109/CVPRW.2015.7301333; Liu Mengyi, 2015, P IEEE INT C COMP VI, P32, DOI 10.1016/j.ijsolstr.2015.02.031; Ma ZG, 2014, IEEE T PATTERN ANAL, V36, P1789, DOI 10.1109/TPAMI.2014.2306419; Marszaek M., 2009, CVPR, P2929, DOI DOI 10.1109/CVPR.2009.5206557; Mazloom M, 2014, IEEE T MULTIMEDIA, V16, P2214, DOI 10.1109/TMM.2014.2359771; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Park S., 2015, IEEE C COMP VIS PATT, P45; Ramanathan V, 2015, IEEE I CONF COMP VIS, P4471, DOI 10.1109/ICCV.2015.508; Razavian Ali Sharif, 2014, P IEEE C COMP VIS PA, P806, DOI DOI 10.1109/CVPRW.2014.131; Rothe R., 2015, IEEE INT C COMP VIS, P53; Salvador A, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301334; Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Vu TH, 2014, LECT NOTES COMPUT SC, V8693, P421, DOI 10.1007/978-3-319-10602-1_28; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang LD, 2015, INTERNATIONAL CONFERENCE ON ENERGY, ENVIRONMENT AND CHEMICAL ENGINEERING (ICEECE 2015), P45; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2017, IEEE T IMAGE PROCESS, V26, P2055, DOI 10.1109/TIP.2017.2675339; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang LM, 2016, INT J COMPUT VISION, V119, P254, DOI 10.1007/s11263-015-0859-0; Wang Limin, 2015, ABS150702159 CORR; Xiong YJ, 2015, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2015.7298768; Yan Y, 2015, AAAI CONF ARTIF INTE, P3841; Yang Y., 2012, P ACM INT C MULT, P1045; Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386; Yao Bangpeng, 2010, CVPR, DOI DOI 10.1109/CVPR.2010.5540234; Yao J, 2012, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2012.6247739; Zheng J., 2014, ADV NEURAL INFORM PR, P1341; Zhou  B., 2015, ABS151204150 CORR; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	71	25	25	1	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		390	409		10.1007/s11263-017-1043-5	http://dx.doi.org/10.1007/s11263-017-1043-5			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA					2022-12-18	WOS:000425619100014
J	Liu, L; Yu, MY; Shao, L				Liu, Li; Yu, Mengyang; Shao, Ling			Latent Structure Preserving Hashing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hashing; Nonnegative matrix factorization; Latent structure; Dimensionality reduction; Multi-layer extension	NONNEGATIVE MATRIX FACTORIZATION; IMAGE; QUANTIZATION; FEATURES; OBJECT; SCENE	Aiming at efficient similarity search, hash functions are designed to embed high-dimensional feature descriptors to low-dimensional binary codes such that similar descriptors will lead to binary codes with a short distance in the Hamming space. It is critical to effectively maintain the intrinsic structure and preserve the original information of data in a hashing algorithm. In this paper, we propose a novel hashing algorithm called Latent Structure Preserving Hashing (LSPH), with the target of finding a well-structured low-dimensional data representation from the original high-dimensional data through a novel objective function based on Nonnegative Matrix Factorization (NMF) with their corresponding Kullback-Leibler divergence of data distribution as the regularization term. Via exploiting the joint probabilistic distribution of data, LSPH can automatically learn the latent information and successfully preserve the structure of high-dimensional data. To further achieve robust performance with complex and nonlinear data, in this paper, we also contribute a more generalized multi-layer LSPH (ML-LSPH) framework, in which hierarchical representations can be effectively learned by a multiplicative up-propagation algorithm. Once obtaining the latent representations, the hash functions can be easily acquired through multi-variable logistic regression. Experimental results on three large-scale retrieval datasets, i.e., SIFT 1M, GIST 1M and 500 K TinyImage, show that ML-LSPH can achieve better performance than the single-layer LSPH and both of them outperform existing hashing techniques on large-scale data.	[Liu, Li; Yu, Mengyang; Shao, Ling] Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England	Northumbria University	Shao, L (corresponding author), Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.	li2.liu@northumbria.ac.uk; m.y.yu@ieee.org; ling.shao@ieee.org	Shao, Ling/D-3535-2011	Shao, Ling/0000-0002-8264-6117				Ahn J. H., 2004, INT C MACH LEARN; [Anonymous], 2005, THESIS MIT CAMBRIDGE; Baluja S, 2008, DATA MIN KNOWL DISC, V17, P402, DOI 10.1007/s10618-008-0096-z; Bian W, 2010, IEEE T IMAGE PROCESS, V19, P545, DOI 10.1109/TIP.2009.2035223; Cai D., 2007, P IEEE INT C COMP VI, P1; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Cai Z., 2015, BRIT MACH VIS C; Cao L., 2012, P 20 ACM INT C MULTI, P299; Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gu Q., 2009, BRIT MACH VIS C; Guan NY, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0083291; Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hosmer DW, 1989, APPL LOGISTIC REGRES; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; Jayaraman D, 2014, ADV NEUR IN, V27; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lee D. D., 2000, ADV NEURAL INFORM PR; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Li P, 2014, EXPERT SYST APPL, V41, P1283, DOI 10.1016/j.eswa.2013.08.026; Li S., 2001, IEEE C COMP VIS PATT; Lin Y, 2013, PROC CVPR IEEE, P446, DOI 10.1109/CVPR.2013.64; Liu L., 2015, INT C COMP VIS; Liu L., 2015, IEEE T NEURAL NETWOR; Liu L., 2015, IEEE T CYBERNETICS; Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Liu Y, 2012, IEEE T IMAGE PROCESS, V21, P4480, DOI 10.1109/TIP.2012.2207394; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Qin J., 2015, BRIT MACH VIS C; Raginsky M., 2009, ADV NEURAL INFORM PR, P1509, DOI [10.5555/2984093.2984263, DOI 10.5555/2984093.2984263]; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Salakhutdinov Ruslan, 2007, J MACHINE LEARNING R, P412, DOI DOI 10.1109/ICCV.2017.74; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Song JK, 2014, IEEE T CYBERNETICS, V44, P1225, DOI 10.1109/TCYB.2013.2289351; Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tao R, 2015, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2015.7298613; Torralba A, 2008, PROC CVPR IEEE, P2269; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890; Wang D, 2015, NEUROCOMPUTING, V167, P230, DOI 10.1016/j.neucom.2015.04.072; Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48; Wang Q, 2014, COMPUT VIS IMAGE UND, V124, P22, DOI 10.1016/j.cviu.2014.03.002; Weiss Y., 2009, NIPS; Xie B, 2011, IEEE T SYST MAN CY B, V41, P1088, DOI 10.1109/TSMCB.2011.2106208; Yu F. X., 2013, IEEE C COMP VIS PATT; Yu FX, 2014, ARXIV14053162; Yu J, 2015, IEEE T CYBERNETICS, V45, P767, DOI 10.1109/TCYB.2014.2336697; Yuan ZJ, 2005, LECT NOTES COMPUT SC, V3540, P333; Zhang D., 2006, PAC RIM INT C ART IN; Zhang D., 2010, C SPEC INT GROUP INF; Zhang X., 2015, INT C COMP VIS; Zheng W., 2011, ARTIFICIAL INTELLIGE	63	25	28	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2017	122	3			SI		439	457		10.1007/s11263-016-0931-4	http://dx.doi.org/10.1007/s11263-016-0931-4			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ES7OH		hybrid, Green Accepted			2022-12-18	WOS:000399739200004
J	Wen, LY; Lei, Z; Chang, MC; Qi, HG; Lyu, SW				Wen, Longyin; Lei, Zhen; Chang, Ming-Ching; Qi, Honggang; Lyu, Siwei			Multi-Camera Multi-Target Tracking with Space-Time-View Hyper-graph	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-camera multi-target tracking; Single-camera multi-target tracking; Space-time-view hyper-graph; Dense sub-hypergraph search	ALGORITHM	Incorporating multiple cameras is an effective solution to improve the performance and robustness of multi-target tracking to occlusion and appearance ambiguities. In this paper, we propose a new multi-camera multi-target tracking method based on a space-time-view hyper-graph that encodes higher-order constraints (i.e., beyond pairwise relations) on 3D geometry, appearance, motion continuity, and trajectory smoothness among 2D tracklets within and across different camera views. We solve tracking in each single view and reconstruction of tracked trajectories in 3D environment simultaneously by formulating the problem as an efficient search of dense sub-hypergraphs on the space-time-view hyper-graph using a sampling based approach. Experimental results on the PETS 2009 dataset and MOTChallenge 2015 3D benchmark demonstrate that our method performs favorably against the state-of-the-art methods in both single-camera and multi-camera multi-target tracking, while achieving close to real-time running efficiency. We also provide experimental analysis of the influence of various aspects of our method to the final tracking performance.	[Wen, Longyin; Lyu, Siwei] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA; [Lei, Zhen] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Chang, Ming-Ching] SUNY Albany, Dept Comp Engn, Albany, NY 12222 USA; [Qi, Honggang] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing, Peoples R China	State University of New York (SUNY) System; State University of New York (SUNY) Albany; Chinese Academy of Sciences; Institute of Automation, CAS; State University of New York (SUNY) System; State University of New York (SUNY) Albany; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Qi, HG (corresponding author), Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing, Peoples R China.	lwen@albany.edu; zlei@nlpr.ia.ac.cn; mchang2@albany.edu; hgqi@jdl.ac.cn; slyu@albany.edu			US National Science Foundation Research Grant [CCF-1319800]; National Key Research and Development Plan [2016 YFC0801002]; Chinese National Natural Science Foundation [61375037, 61473291]; National Nature Science Foundation of China [61472388]	US National Science Foundation Research Grant; National Key Research and Development Plan; Chinese National Natural Science Foundation(National Natural Science Foundation of China (NSFC)); National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC))	We would like to thank Dawei Du for a number of suggestions that considerably improved the quality of this paper. Longyin Wen and Siwei Lyu were supported by US National Science Foundation Research Grant (CCF-1319800). Zhen Lei was supported by the National Key Research and Development Plan (Grant No. 2016 YFC0801002), the Chinese National Natural Science Foundation Projects #61375037, #61473291. Honggang Qi was supported by National Nature Science Foundation of China #61472388.	[Anonymous], 2015, CORR; Attanasi A, 2015, IEEE T PATTERN ANAL, V37, P2451, DOI 10.1109/TPAMI.2015.2414427; Berclaz J., 2009, 2009 12 IEEE INT WOR, P1, DOI [10.1109/PETSWINTER.2009.5399488, DOI 10.1109/PETSWINTER.2009.5399488, DOI 10.1109/PETS-WINTER.2009.5399488]; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Breitenstein MD, 2011, IEEE T PATTERN ANAL, V33, P1820, DOI 10.1109/TPAMI.2010.232; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Felzenszwalb P, 2008, 2008 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2008.4587597; Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174; Hofmann M, 2013, PROC CVPR IEEE, P3650, DOI 10.1109/CVPR.2013.468; Hong L, 2000, SIGNAL PROCESS, V80, P1561, DOI 10.1016/S0165-1684(00)00056-6; Huang C, 2013, IEEE T PATTERN ANAL, V35, P898, DOI 10.1109/TPAMI.2012.159; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Izadinia H., 2012, EUR C COMP VIS, P100; Khan Z, 2005, IEEE T PATTERN ANAL, V27, P1805, DOI 10.1109/TPAMI.2005.223; Kim JH, 2013, IEEE I CONF COMP VIS, P1896, DOI 10.1109/ICCV.2013.238; Klinger T, 2015, ISPRS ANN PHOTO REM, VII-3, P435, DOI 10.5194/isprsannals-II-3-W5-435-2015; Kostrikov Ilya, 2014, 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1534, DOI 10.1109/CVPR.2014.199; Kuhn H., 1951, P 2 BERK S MATH STAT, P481, DOI DOI 10.1007/BF01582292; Kuo CH, 2011, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2011.5995384; Leal-Taixe Laura, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P120, DOI 10.1109/ICCVW.2011.6130233; Leal-Taixe L, 2012, PROC CVPR IEEE, P1987, DOI 10.1109/CVPR.2012.6247901; Leven WF, 2009, IEEE T AUTOMAT CONTR, V54, P370, DOI 10.1109/TAC.2008.2008327; Liu HR, 2012, PROC CVPR IEEE, P574, DOI 10.1109/CVPR.2012.6247723; Liu HR, 2012, INT J COMPUT VISION, V98, P65, DOI 10.1007/s11263-011-0496-1; Liu Y, 2012, LECT NOTES COMPUT SC, V7575, P730, DOI 10.1007/978-3-642-33765-9_52; Marcenaro L, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P341, DOI 10.1109/ICIP.2002.1038975; Milan A., 2011, CONTINUOUS ENERGY MI; Milan A., 2015, MULTIPLE OBJECT TRAC; Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103; Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404; Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260; REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177; Shi XC, 2014, PROC CVPR IEEE, P3518, DOI 10.1109/CVPR.2014.450; Shu G, 2012, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2012.6247879; Smith K, 2005, PROC CVPR IEEE, P962; Stiefelhagen R., 2006, INT EV WORKSH CLASS, P1; Wen L., 2014, P IEEE C COMP VIS PA, P3457; Wu Z., 2009, P WORKSH MOT VID COM, P1, DOI [10.1109/WMVC.2009.5399245, DOI 10.1109/WMVC.2009.5399245]; Wu Z, 2011, PROC CVPR IEEE, P1185, DOI 10.1109/CVPR.2011.5995515; Yang B, 2012, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR.2012.6247892; Yang M, 2014, PROC ASME IMECE2014, P1; Yu Q, 2009, IEEE T PATTERN ANAL, V31, P2196, DOI 10.1109/TPAMI.2008.253; Zhang L, 2008, INT C WAVEL ANAL PAT, P11, DOI 10.1109/ICWAPR.2008.4635742; Zhou D., 2006, ADV NEURAL INF PROCE, V19, P1601	53	25	26	7	39	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		313	333		10.1007/s11263-016-0943-0	http://dx.doi.org/10.1007/s11263-016-0943-0			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS					2022-12-18	WOS:000398162200008
J	Gadde, R; Marlet, R; Paragios, N				Gadde, Raghudeep; Marlet, Renaud; Paragios, Nikos			Learning Grammars for Architecture-Specific Facade Parsing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Grammar learning; Facade parsing; Subtree isomorphism; Clustering	COMPUTER VISION; ALGORITHMS; INFERENCE	Parsing facade images requires optimal handcrafted grammar for a given class of buildings. Such a handcrafted grammar is often designed manually by experts. In this paper, we present a novel framework to learn a compact grammar from a set of ground-truth images. To this end, parse trees of ground-truth annotated images are obtained running existing inference algorithms with a simple, very general grammar. From these parse trees, repeated subtrees are sought and merged together to share derivations and produce a grammar with fewer rules. Furthermore, unsupervised clustering is performed on these rules, so that, rules corresponding to the same complex pattern are grouped together leading to a rich compact grammar. Experimental validation and comparison with the state-of-the-art grammar-based methods on four different datasets show that the learned grammar helps in much faster convergence while producing equal or more accurate parsing results compared to handcrafted grammars as well as grammars learned by other methods. Besides, we release a new dataset of facade images following the Art-deco style and demonstrate the general applicability and extreme potential of the proposed framework.	[Gadde, Raghudeep; Marlet, Renaud] Univ Paris Est, ENPC, LIGM UMR CNRS 8049, F-77455 Marne La Vallee, France; [Paragios, Nikos] Univ Paris Saclay, Inria, Cent Supelec, Ctr Visual Comp, F-92295 Chatenay Malabry, France	Universite Gustave-Eiffel; ESIEE Paris; Centre National de la Recherche Scientifique (CNRS); Ecole des Ponts ParisTech; Inria; UDICE-French Research Universities; Universite Paris Saclay	Gadde, R (corresponding author), Univ Paris Est, ENPC, LIGM UMR CNRS 8049, F-77455 Marne La Vallee, France.	raghudeep.gadde@enpc.fr; renaud.marlet@enpc.fr; nikos.paragios@ecp.fr			ANR [ANR-13-CORD-0003]; European Research Council [ERC-STG-259112]	ANR(French National Research Agency (ANR)); European Research Council(European Research Council (ERC)European Commission)	We thank Prof. Nikos Komodakis for providing the code for LP-based clustering. This work was partly carried out in IMAGINE, a joint research project between Ecole des Ponts ParisTech (ENPC) and the Scientific and Technical Centre for Building (CSTB). It was partly supported by ANR project Semapolis ANR-13-CORD-0003 and the European Research Council Starting Grant ERC-STG-259112.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Alegria FA, 2004, ACTA HORTIC, P25; Benz F, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P487; Berg AC, 2007, IEEE I CONF COMP VIS, P2057; Bod R, 2003, EACL 2003: 10TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P19; Bod R, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P865; Carrasco RC, 2001, MACH LEARN, V44, P185, DOI 10.1023/A:1010836331703; Charikar M, 2005, IEEE T INFORM THEORY, V51, P2554, DOI 10.1109/TIT.2005.850116; Charikar Moses, 2002, P 34 ANN ACM S THEOR, P792, DOI DOI 10.1007/S10588-021-09347-8; Chi Y, 2005, FUND INFORM, V66, P161; Clark A, 2010, LECT NOTES ARTIF INT, V6339, P24, DOI 10.1007/978-3-642-15488-1_4; Cohen A., 2014, 2014 IEEE C COMP VIS; Cohen SB, 2014, J MACH LEARN RES, V15, P2399; Cohen Shay B., 2013, P NAACL HLT, P148; Cohn T, 2010, J MACH LEARN RES, V11, P3053; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; D'Ulizia A, 2011, ARTIF INTELL REV, V36, P1, DOI 10.1007/s10462-010-9199-1; Dai DX, 2012, LECT NOTES COMPUT SC, V7572, P710, DOI 10.1007/978-3-642-33718-5_51; DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909; de la Higuera C, 2005, PATTERN RECOGN, V38, P1332, DOI 10.1016/j.patcog.2005.01.003; De la Higuera Colin, 2010, GRAMMATICAL INFERENC; Dunn J. C., 1974, Journal of Cybernetics, V4, P95, DOI 10.1080/01969727408546059; Flajolet P., 1990, P 17 INT C AUT LANG, P220; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Gould S, 2012, J MACH LEARN RES, V13, P3533; Grunwald P., 1996, Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing, P203; Jampani V, 2015, IEEE WINT CONF APPL, P1038, DOI 10.1109/WACV.2015.143; Johnson Mark, 2007, P HUMAN LANGUAGE TEC, P139; Kass M., 1987, International Journal of Computer Vision, V1, P321, DOI 10.1007/BF00133570; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Komodakis N., 2009, ADV NEURAL INFORM PR, V21, P865; Korc F., 2009, TRIGGP200901 U BONN; Koutsourakis P, 2009, IEEE I CONF COMP VIS, P1795, DOI 10.1109/ICCV.2009.5459400; Kozinski M., 2015, 2015 IEEE C COMP VIS; Kozinski M., 2014, WINT C APPL COMP VIS; Kozinski M., 2014, 12 AS C COMP VIS ACC; Lehman E, 2002, SIAM PROC S, P205; MAKINEN E, 1989, INFORM PROCESS LETT, V32, P271, DOI 10.1016/0020-0190(89)90056-2; Manning CD, 2011, LECT NOTES COMPUT SC, V6608, P171, DOI 10.1007/978-3-642-19400-9_14; Martinovic A., 2013, KULESATPSI1301; Martinovic A, 2013, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2013.33; Martinovic A, 2012, LECT NOTES COMPUT SC, V7578, P416, DOI 10.1007/978-3-642-33786-4_31; Matsuzaki Takuya, 2005, P 43 ANN M ASS COMP, P75; Miller P., 1999, STRONG GENERATIVE CA; Muller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931; NevillManning CG, 1997, J ARTIF INTELL RES, V7, P67, DOI 10.1613/jair.374; Nivre J., 2007, Natural Language Engineering, V13, P95, DOI 10.1017/S1351324906004505; Ok D., 2012, 2 JOINT 3DIM 3DPVT C; Osher S., 2003, GEOMETRIC LEVEL SET; Parisot S, 2012, PROC CVPR IEEE, P988, DOI 10.1109/CVPR.2012.6247775; Parisot S, 2011, LECT NOTES COMPUT SC, V6892, P508, DOI 10.1007/978-3-642-23629-7_62; Petrov S., 2007, HUMAN LANGUAGE TECHN, P404; Riemenschneider H, 2012, PROC CVPR IEEE, P1640, DOI 10.1109/CVPR.2012.6247857; Ripperda N, 2006, LECT NOTES COMPUT SC, V4174, P750; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Sakakibara Y, 1999, MACHINE LEARNING, PROCEEDINGS, P354; Si ZZ, 2013, IEEE T PATTERN ANAL, V35, P2189, DOI 10.1109/TPAMI.2013.35; Simon L, 2012, PROC CVPR IEEE, P518, DOI 10.1109/CVPR.2012.6247716; Simon L, 2011, INT J COMPUT VISION, V93, P253, DOI 10.1007/s11263-010-0370-6; Sutton R.S., 1998, INTRO REINFORCEMENT, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; Teboul O., 2011, THESIS ECOLE CENTRAL; TEBOUL O, 2010, PROC CVPR IEEE, P3105, DOI DOI 10.1109/CVPR.2010.5540068; Teboul O, 2013, IEEE T PATTERN ANAL, V35, P1744, DOI 10.1109/TPAMI.2012.252; Tomita M., 1991, SPRINGER INT SERIES, V126, P277; Tu K., 2013, ADV NEURAL INFORM PR, P1322; Tylecek R., 2012, CTUCMP201224; VALIENTE G, 2002, ALGORITHMS TREES GRA; Wang CH, 2013, COMPUT VIS IMAGE UND, V117, P1610, DOI 10.1016/j.cviu.2013.07.004; Weissenberg J, 2013, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2013.31; Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324; Zaki Mohammed J., 2002, P 8 ACM SIGKDD INT C, P71, DOI DOI 10.1145/775047.775058	71	25	25	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2016	117	3					290	316		10.1007/s11263-016-0887-4	http://dx.doi.org/10.1007/s11263-016-0887-4			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DL1TF		Green Submitted			2022-12-18	WOS:000375414600005
J	Park, HS; Shiratori, T; Matthews, I; Sheikh, Y				Park, Hyun Soo; Shiratori, Takaaki; Matthews, Iain; Sheikh, Yaser			3D Trajectory Reconstruction under Perspective Projection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dynamic 3D reconstruction; Trajectory triangulation; Trajectory space; Reconstructability	SHAPE; MOTION	We present an algorithm to reconstruct the 3D trajectory of a moving point from its correspondence in a collection of temporally non-coincidental 2D perspective images, given the time of capture that produced each image and the relative camera poses at each time instant. Triangulation-based solutions do not apply, as multiple views of the point may not exist at each time instant. We represent a 3D trajectory using a linear combination of compact trajectory basis vectors, such as the discrete cosine transform basis, that have been shown to approximate object independence. We note that such basis vectors are also coordinate independent, which allows us to directly use camera poses estimated from stationary areas in the scene (in contrast to nonrigid structure from motion techniques where cameras are simultaneously estimated). This reduces the reconstruction optimization to a linear least squares problem, allowing us to robustly handle missing data that often occur due to motion blur, texture deformation, and self occlusion. We present an algorithm to determine the number of trajectory basis vectors, individually for each trajectory via a cross validation scheme and refine the solution by minimizing the geometric error. The relationship between point and camera motion can cause degeneracies to occur. We geometrically analyze the problem by studying the relationship of the camera motion, point motion, and trajectory basis vectors. We define the reconstructability of a 3D trajectory under projection, and show that the estimate approaches the ground truth when reconstructability approaches infinity. This analysis enables us to precisely characterize cases when accurate reconstruction is achievable. We present qualitative results for the reconstruction of several real-world scenes from a series of 2D projections where high reconstructability can be guaranteed, and report quantitative results on motion capture sequences.	[Park, Hyun Soo; Sheikh, Yaser] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Shiratori, Takaaki] Microsoft Res Asia, Beijing, Peoples R China; [Matthews, Iain] Disney Res Pittsburgh, Pittsburgh, PA USA	Carnegie Mellon University; Microsoft; Microsoft Research Asia	Park, HS (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	hyunsoop@cs.cmu.edu; takaakis@microsoft.com; iainm@disneyresearch.com; yaser@cs.cmu.edu			NSF [IIS-0916272]	NSF(National Science Foundation (NSF))	This work was supported by NSF Grant IIS-0916272.	Akhter I., 2008, ADV NEURAL INFORM PR; Akhter I., 2009, P IEEE C COMP VIS PA; Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201; Avidan S, 2000, IEEE T PATTERN ANAL, V22, P348, DOI 10.1109/34.845377; BARTOLI A, 2008, P IEEE C COMP VIS PA; Blanz V., 1999, ACM T GRAPHICS SIGGR; Brand M., 2005, P IEEE C COMP VIS PA; Brand M., 2001, P IEEE C COMP VIS PA; Bregler C., 1999, P IEEE C COMP VIS PA; Dai Y., 2012, P IEEE C COMP VIS PA; DELBUE A, 2008, P IEEE C COMP VIS PA; DELBUE A, 2006, P IEEE C COMP VIS PA; Fayad J., 2010, P EUR C COMP VIS; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gotardo PFU, 2011, IEEE T PATTERN ANAL, V33, P2051, DOI 10.1109/TPAMI.2011.50; HAMIDI M, 1976, IEEE T ACOUST SPEECH, V24, P428, DOI 10.1109/TASSP.1976.1162839; HARTLEY R, 2008, P EUR C COMP VIS; Hartley R., 2004, ROBOTICA; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Kaminski JY, 2004, J MATH IMAGING VIS, V21, P27, DOI 10.1023/B:JMIV.0000026555.79056.b8; Llado X, 2010, IMAGE VISION COMPUT, V28, P1339, DOI 10.1016/j.imavis.2010.01.014; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Lourakis MIA, 2009, ACM T MATH SOFTWARE, V36, DOI 10.1145/1486525.1486527; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma Y., 2003, INVITATION 3 D VISIO; MORENONOGUER F, 2007, P INT C COMP VIS; OLSEN S, 2007, P BRIT MACH VIS C; Ostlund J., 2012, P EUR C COMP VIS; Ozden K. E., 2004, COMPUTER VISION IMAG, V93, P1453; PALADINI M, 2009, P IEEE C COMP VIS PA; Park H. S., 2010, P EUR C COMP VIS; Salzmann M, 2007, IEEE T PATTERN ANAL, V29, P1481, DOI 10.1109/TPAMI.2007.1080; SHASHUA A, 2000, P EUR C COMP VIS; Sidenbladh H., 2000, P EUR C COMP VIS; Snavely N., 2006, ACM T GRAPHICS SIGGR; Taylor J., 2010, P IEEE C COMP VIS PA; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TORRESANI L, 2001, P IEEE C COMP VIS PA; Torresani L., 2002, P EUR C COMP VIS; TORRESANI L, 2003, ADV NEURAL INFORM PR; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Valmadre J., 2012, P IEEE C COMP VIS PA; Vidal R., 2006, P EUR C COMP VIS; VIDAL R, 2004, P IEEE C COMP VIS PA; WEXLER Y, 2000, P IEEE C COMP VIS PA; Wolf L, 2002, INT J COMPUT VISION, V48, P53, DOI 10.1023/A:1014855311993; XIAO J, 2004, P IEEE C COMP VIS PA; Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9; Yan J., 2005, P IEEE C COMP VIS PA; Zhu S., 2010, P IEEE C COMP VIS PA	51	25	27	0	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2015	115	2					115	135		10.1007/s11263-015-0804-2	http://dx.doi.org/10.1007/s11263-015-0804-2			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CS7SL					2022-12-18	WOS:000362285700003
J	Xiang, SM; Meng, GF; Wang, Y; Pan, CH; Zhang, CS				Xiang, Shiming; Meng, Gaofeng; Wang, Ying; Pan, Chunhong; Zhang, Changshui			Image Deblurring with Coupled Dictionary Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image deblurring; Coupled dictionary learning; Sparse representation; Multiple kernel deblurring	SPARSE REPRESENTATION; K-SVD; ALGORITHM; SUPERRESOLUTION; DECONVOLUTION; SHRINKAGE; CAMERA	Image deblurring is a challenging problem in vision computing. Traditionally, this task is addressed as an inverse problem that is enclosed into the image itself. This paper presents a learning-based framework where the knowledge hidden in huge amounts of available data is explored and exploited for image deblurring. To this end, our algorithm is developed under the conceptual framework of coupled dictionary learning. Specifically, given pairs of blurred image patches and their corresponding clear ones, a learning model is constructed to learn a pair of dictionaries. Among them, one dictionary is responsible for the representation of clear images, while the other is responsible for that of the blurred images. Theoretically, the learning model is analyzed with coupled sparse representations for training samples. As the atoms of these dictionaries are coupled together one-by-one, the reconstruction information can be transmitted between the clear and blurry images. In application phase, the blurry dictionary is employed to reconstruct linearly the blurry image to be restored. Then, the reconstruction coefficients are kept unchanged along with the clear dictionary to restore the final results. The main advantage of our approach lies in that it works in the case of unknown blur kernels. Comparative experiments indicate the validity of our approach.	[Xiang, Shiming; Meng, Gaofeng; Wang, Ying; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Zhang, Changshui] Tsinghua Univ, Dept Automat, TNList, Beijing 100084, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Tsinghua University	Xiang, SM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.	smxiang@nlpr.ia.ac.cn; gfmeng@nlpr.ia.ac.cn; ywang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn; zcs@mail.tsinghua.edu.cn			National Natural Science Foundation of China [61272331, 91338202, 61370039, 91120301]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China (Grant Nos. 61272331, 91338202, 61370039, and 91120301).	Afonso MV, 2010, IEEE T IMAGE PROCESS, V19, P2345, DOI 10.1109/TIP.2010.2047910; Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; [Anonymous], 2011, P 28 INT C MACH LEAR; Bar L, 2007, LECT NOTES COMPUT SC, V4485, P533; Batra D, 2011, INT J COMPUT VISION, V93, P273, DOI 10.1007/s11263-010-0415-x; Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080; Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319; Bronstein MM, 2005, IEEE T IMAGE PROCESS, V14, P726, DOI 10.1109/TIP.2005.847322; Chantas G, 2010, IEEE T IMAGE PROCESS, V19, P351, DOI 10.1109/TIP.2009.2033398; Couzinie-Devy F., 2011, CORR; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Danielyan A, 2012, IEEE T IMAGE PROCESS, V21, P1715, DOI 10.1109/TIP.2011.2176954; Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847; Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Elad M, 2010, P IEEE, V98, P972, DOI 10.1109/JPROC.2009.2037655; Engan K, 2000, SIGNAL PROCESS, V80, P2121, DOI 10.1016/S0165-1684(00)00072-4; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Goldluecke B, 2010, PROC CVPR IEEE, P327, DOI 10.1109/CVPR.2010.5540194; Gorodnitsky IF, 1997, IEEE T SIGNAL PROCES, V45, P600, DOI 10.1109/78.558475; He L, 2013, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2013.51; Hu Z, 2010, IEEE IMAGE PROC, P1169, DOI 10.1109/ICIP.2010.5651892; Jenatton R, 2011, J MACH LEARN RES, V12, P2297; Jia K, 2013, IEEE T PATTERN ANAL, V35, P367, DOI 10.1109/TPAMI.2012.95; Joshi N., 2009, INT C COMP VIS PATT, P1; Joshi N, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731050; Kamilov U, 2012, IEEE SIGNAL PROC LET, V19, P187, DOI 10.1109/LSP.2012.2185929; Kenig T, 2010, IEEE T PATTERN ANAL, V32, P2191, DOI 10.1109/TPAMI.2010.45; Levin A., 2009, INT C COMP VIS PATT, P1; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Li DL, 2007, IEEE T NEURAL NETWOR, V18, P931, DOI 10.1109/TNN.2007.891622; Lou YF, 2011, J MATH IMAGING VIS, V39, P1, DOI 10.1007/s10851-010-0220-8; LUCY LB, 1974, ASTRON J, V79, P745, DOI 10.1086/111605; Ma LY, 2013, IEEE T MED IMAGING, V32, P1277, DOI 10.1109/TMI.2013.2255883; Ma S., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587391; Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Mairal J, 2011, J MACH LEARN RES, V12, P2681; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; Mairal J, 2010, J MACH LEARN RES, V11, P19; Peleg T, 2012, IEEE T SIGNAL PROCES, V60, P2286, DOI 10.1109/TSP.2012.2188520; Portilla J, 2009, IEEE IMAGE PROC, P3909, DOI 10.1109/ICIP.2009.5413975; RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055; Rodriguez P, 2009, IEEE T IMAGE PROCESS, V18, P322, DOI 10.1109/TIP.2008.2008420; Rubinstein R, 2013, IEEE T SIGNAL PROCES, V61, DOI 10.1109/TSP.2012.2226445; Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; Su M, 2002, PATTERN RECOGN, V35, P2881, DOI 10.1016/S0031-3203(01)00239-4; Takeda H, 2008, IEEE T IMAGE PROCESS, V17, P550, DOI 10.1109/TIP.2007.918028; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930; Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435; Wen YW, 2012, IEEE T IMAGE PROCESS, V21, P106, DOI 10.1109/TIP.2011.2159983; Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7; Xiang SM, 2010, IEEE T PATTERN ANAL, V32, P2039, DOI 10.1109/TPAMI.2010.35; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang HC, 2013, PROC CVPR IEEE, P1051, DOI 10.1109/CVPR.2013.140; [张华 ZHANG Hua], 2011, [高分子通报, Polymer Bulletin], P1	65	25	28	0	38	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2015	114	2-3			SI		248	271		10.1007/s11263-014-0755-z	http://dx.doi.org/10.1007/s11263-014-0755-z			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CP7MJ					2022-12-18	WOS:000360071900009
J	Rajagopal, AK; Subramanian, R; Ricci, E; Vieriu, RL; Lanz, O; Kalpathi, RR; Sebe, N				Rajagopal, Anoop Kolar; Subramanian, Ramanathan; Ricci, Elisa; Vieriu, Radu L.; Lanz, Oswald; Kalpathi, Ramakrishnan R.; Sebe, Nicu			Exploring Transfer Learning Approaches for Head Pose Classification from Multi-view Surveillance Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Transfer learning; Multi-view head pose classification; Varying acquisition conditions; Moving persons	LOW-RESOLUTION; TRACKING	Head pose classification from surveillance images acquired with distant, large field-of-view cameras is difficult as faces are captured at low-resolution and have a blurred appearance. Domain adaptation approaches are useful for transferring knowledge from the training (source) to the test (target) data when they have different attributes, minimizing target data labeling efforts in the process. This paper examines the use of transfer learning for efficient multi-view head pose classification with minimal target training data under three challenging situations: (i) where the range of head poses in the source and target images is different, (ii) where source images capture a stationary person while target images capture a moving person whose facial appearance varies under motion due to changing perspective, scale and (iii) a combination of (i) and (ii). On the whole, the presented methods represent novel transfer learning solutions employed in the context of multi-view head pose classification. We demonstrate that the proposed solutions considerably outperform the state-of-the-art through extensive experimental validation. Finally, the DPOSE dataset compiled for benchmarking head pose classification performance with moving persons, and to aid behavioral understanding applications is presented in this work.	[Rajagopal, Anoop Kolar; Kalpathi, Ramakrishnan R.] Indian Inst Sci, Dept Elect Engn, Bangalore 560012, Karnataka, India; [Subramanian, Ramanathan] Univ Illinois, ADSC, Singapore, Singapore; [Ricci, Elisa; Lanz, Oswald] Fdn Bruno Kessler, Trento, Italy; [Ricci, Elisa] Univ Perugia, Dept Elect & Informat Engn, I-06100 Perugia, Italy; [Vieriu, Radu L.; Sebe, Nicu] Dept Comp Sci & Informat Engn DISI, Trento, Italy	Indian Institute of Science (IISC) - Bangalore; Fondazione Bruno Kessler; University of Perugia	Subramanian, R (corresponding author), Univ Illinois, ADSC, Singapore, Singapore.	anoopkr@ee.iisc.ernet.in; Subramanian.R@adsc.com.sg; eliricci@fbk.eu; vieriu@disi.unitn.it; lanz@fbk.eu; krr@ee.iisc.ernet.in; sebe@disi.unitn.it	Lanz, Oswald/AAW-7865-2021	Ricci, Elisa/0000-0002-0228-1147; Sebe, Niculae/0000-0002-6597-7248; Subramanian, Ramanathan/0000-0001-9441-7074; Lanz, Oswald/0000-0003-4793-4276	Singapore's Agency for Science, Technology and Research (A*STAR) under the Human Sixth Sense Programme (HSSP) grant; EIT ICT Labs SSP 12205 Activity TIK-The Interaction Toolkit [T1320A-T1321A]; FP7 EU project DALI	Singapore's Agency for Science, Technology and Research (A*STAR) under the Human Sixth Sense Programme (HSSP) grant; EIT ICT Labs SSP 12205 Activity TIK-The Interaction Toolkit; FP7 EU project DALI	The authors gratefully acknowledge partial support from Singapore's Agency for Science, Technology and Research (A*STAR) under the Human Sixth Sense Programme (HSSP) grant, EIT ICT Labs SSP 12205 Activity TIK-The Interaction Toolkit, tasks T1320A-T1321A and the FP7 EU project DALI.	[Anonymous], 2007, P 15 ACM INT C MULTI; Benfold B, 2011, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2011.6126516; Chen C, 2012, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2012.6247845; Dai W., 2007, PROC INT C MACH LEAR, P193, DOI [10.1145/1273496.1273521, DOI 10.1145/1273496.1273521]; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Daume H, 2007, P 45 ANN M ASS COMP, V45, P256; Doshi A, 2012, J VISION, V12, DOI [10.1167/12.2.9, 10.1167/12.6.9]; Duan L., 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114; Farhadi A, 2008, LECT NOTES COMPUT SC, V5302, P154, DOI 10.1007/978-3-540-88682-2_13; Ferencz A, 2008, INT J COMPUT VISION, V77, P3, DOI 10.1007/s11263-007-0093-5; HOSDB, 2006, IEEE CRIM SEC; Katzenmeier M., 2004, P 6 INT C MULT INT, P144, DOI DOI 10.1145/1027933.1027959; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Lanz O, 2008, LECT NOTES COMPUT SC, V4625, P287; Lanz O, 2006, IEEE T PATTERN ANAL, V28, P1436, DOI 10.1109/TPAMI.2006.177; Lepri B, 2012, IEEE T AFFECT COMPUT, V3, P443, DOI 10.1109/T-AFFC.2012.17; Lim J.J., 2011, ADV NEURAL INFORM PR, P118; Munoz-Salinas R, 2012, MACH VISION APPL, V23, P479, DOI 10.1007/s00138-012-0410-z; Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106; Orozco J., 2009, BRIT MACH VIS C, P1; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pardoe D., 2010, ICML, P863; Rajagopal A., 2012, ACCV, P652; Ricci E, 2009, IEEE IMAGE PROC, P2593, DOI 10.1109/ICIP.2009.5413994; Smith K, 2008, IEEE T PATTERN ANAL, V30, P1212, DOI 10.1109/TPAMI.2007.70773; Stiefelhagen Rainer, 2007, REVISED SELECTED PAP, V4625; Subramanian R., 2010, INT C MULT, P659, DOI [10.1145/1873951.1874045, DOI 10.1145/1873951.1874045]; Subramanian R., 2013, ACM INT C MULT INT; Tosato D, 2010, LECT NOTES COMPUT SC, V6312, P378, DOI 10.1007/978-3-642-15552-9_28; Voit M, 2009, LECT NOTES COMPUT SC, V5815, P415, DOI 10.1007/978-3-642-04667-4_42; Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207; Williams, 2007, NEURAL INFORM PROCES, P153, DOI DOI 10.5555/2981562.2981582; Yan Y, 2013, INT C COMP VIS; Yan Y, 2012, INT C PATT RECOG, P1168; Yang W., 2009, INT WORKSH MACH LEAR; Yang WL, 2010, LECT NOTES COMPUT SC, V5995, P417; Zabulis X., 2009, P BRIT MACH VIS ASS, P1; Zhai, 2007, ANN M ASS COMP LING, P264, DOI [DOI 10.1145/1273496.1273558, DOI 10.1039/B610011B]; Zhang Y, 2010, PROCEEDINGS OF THE ASME 29TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2010, VOL 6, P733; Zheng J., 2012, P BRIT MACH VIS C, P1	41	25	29	0	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2014	109	1-2			SI		146	167		10.1007/s11263-013-0692-2	http://dx.doi.org/10.1007/s11263-013-0692-2			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AI7QY					2022-12-18	WOS:000337091700010
J	Yu, TH; Woodford, OJ; Cipolla, R				Yu, Tsz-Ho; Woodford, Oliver J.; Cipolla, Roberto			A Performance Evaluation of Volumetric 3D Interest Point Detectors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D interest points; Volumetric interest points; Feature detection; Performance evaluation	OBJECT RECOGNITION; SCALE; DISTANCE	This paper presents the first performance evaluation of interest points on scalar volumetric data. Such data encodes 3D shape, a fundamental property of objects. The use of another such property, texture (i.e. 2D surface colouration), or appearance, for object detection, recognition and registration has been well studied; 3D shape less so. However, the increasing prevalence of 3D shape acquisition techniques and the diminishing returns to be had from appearance alone have seen a surge in 3D shape-based methods. In this work, we investigate the performance of several state of the art interest points detectors in volumetric data, in terms of repeatability, number and nature of interest points. Such methods form the first step in many shape-based applications. Our detailed comparison, with both quantitative and qualitative measures on synthetic and real 3D data, both point-based and volumetric, aids readers in selecting a method suitable for their application.	[Yu, Tsz-Ho; Cipolla, Roberto] Univ Cambridge, Cambridge, England; [Woodford, Oliver J.] Toshiba Res Europe Ltd, Cambridge, England	University of Cambridge; Toshiba Corporation	Yu, TH (corresponding author), Univ Cambridge, Cambridge, England.	thy23@cam.ac.uk; oliver.woodford@crl.toshiba.co.uk; cipolla@eng.cam.ac.uk	Arandjelović, Ognjen/V-5255-2019	Arandjelović, Ognjen/0000-0002-9314-194X; Cipolla, Roberto/0000-0002-8999-2151				Aanaes H., 2010, P 5 INT S 3D DAT PRO; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bhatia A, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P61, DOI 10.1109/ICIAP.2007.4362758; Bowyer K, 1999, IEEE T PATTERN ANAL, V21, P1; BRAND P, 1994, P SOC PHOTO-OPT INS, V2350, P218, DOI 10.1117/12.189134; Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1; Brown M, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P56, DOI 10.1109/3DIM.2005.81; Cocosco C. A., 1997, NEUROIMAGE, V5, P425; COELHO C, 1992, ARTIF INT, P87; Cornelis N, 2008, PROC CVPR IEEE, P1013; Criminisi A, 2011, LECT NOTES COMPUT SC, V6533, P106, DOI 10.1007/978-3-642-18421-5_11; Dalvi R., 2010, SPIE MED IMAGING; Dohi K., 2011, 2011 International Conference on Field Programmable Logic and Applications, P478, DOI 10.1109/FPL.2011.94; Donner R, 2011, LECT NOTES COMPUT SC, V6533, P86, DOI 10.1007/978-3-642-18421-5_9; Donoser M, 2006, INT C PATT RECOG, P63; Dutagaci H., 2011, P EUR WORKSH 3D OBJ, P57; Fisher R. B., 1987, P 3 ALV VIS C, P79; Flitton G., 2010, P BRIT MACH VIS C, P11; Glomb P, 2009, ADV INTEL SOFT COMPU, V57, P103; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Heath MD, 1997, IEEE T PATTERN ANAL, V19, P1338, DOI 10.1109/34.643893; Janoch A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1168, DOI 10.1109/ICCVW.2011.6130382; Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43; Koelstra S, 2009, 2009 10TH INTERNATIONAL WORKSHOP ON IMAGE ANALYSIS FOR MULTIMEDIA INTERACTIVE SERVICES, P242, DOI 10.1109/WIAMIS.2009.5031478; Kristensen F, 2007, IEEE INT SYMP CIRC S, P165, DOI 10.1109/ISCAS.2007.378247; Lai K, 2010, INT J ROBOT RES, V29, P1019, DOI 10.1177/0278364910369190; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laptev I, 2003, INT J COMPUT VISION, V52, P97, DOI 10.1023/A:1022947906601; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9; Pham MT, 2011, IEEE I CONF COMP VIS, P145, DOI 10.1109/ICCV.2011.6126236; Ni D, 2008, LECT NOTES COMPUT SC, V5242, P52; Papazov C, 2011, LECT NOTES COMPUT SC, V6492, P135, DOI 10.1007/978-3-642-19315-6_11; RAJAN PK, 1989, SOUTHEAST SYMP SYSTE, P29, DOI 10.1109/SSST.1989.72429; Riemenschneider H., 2009, P BRIT MACH VIS C; Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275; Ruiz-Alzola J, 2001, SIGNAL PROCESS, V81, P2243, DOI 10.1016/S0165-1684(01)00100-1; Salti S., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P236, DOI 10.1109/3DIMPVT.2011.37; Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; Shin M. C., 1999, P IEEE C COMP VIS PA, V1, P1360; Sinha S. N., 2006, WORKSH EDG COMP US N, V278; Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y; Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710; Teixeira L., 2009, ACCELERATED CORNER D; Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017; Unnikrishnan Ranjith, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563030; Viksten F, 2008, INT C PATT RECOG, P3205; Vogiatzis G, 2011, IMAGE VISION COMPUT, V29, P434, DOI 10.1016/j.imavis.2011.01.006; Wessel R., 2006, VISION MODELING VISU, P365; WILLEMS G, 2009, P BRIT MACH VIS C; Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48; Willis A, 2009, IEEE I CONF COMP VIS, P2296, DOI 10.1109/ICCV.2009.5459443; Yu TH, 2010, P BRIT MACH VIS C; Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748	60	25	25	0	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					180	197		10.1007/s11263-012-0563-2	http://dx.doi.org/10.1007/s11263-012-0563-2			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO		Green Submitted			2022-12-18	WOS:000315501800011
J	Agarwal, A; Triggs, B				Agarwal, Ankur; Triggs, Bill			Multilevel image coding with hyperfeatures	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image coding; hierarchical representations; visual recognition; image classification	RECOGNITION; MODELS	Histograms of local appearance descriptors are a popular representation for visual recognition. They are highly discriminant with good resistance to local occlusions and to geometric and photometric variations, but they are not able to exploit spatial co- occurrence statistics over scales larger than the local input patches. We present a multilevel visual representation that remedies this. The starting point is the notion that to detect object parts in images, in practice it often suffices to detect co- occurrences of more local object fragments. This can be formalized by coding image patches against a codebook of known fragments or a more general statistical model and locally histogramming the resulting labels to capture their co- occurrence statistics. Local patch descriptors are converted into somewhat less local histograms over label occurrences. The histograms are themselves local descriptor vectors so the process can be iterated to code ever larger assemblies of object parts and increasingly abstract or 'semantic' image properties. We call these higher-level descriptors "hyperfeatures". We formulate the hyperfeature model and study its performance under several different image coding methods including k-means based Vector Quantization, Gaussian Mixtures, and combinations of these with Latent Dirichlet Allocation. We find that the resulting high-level features provide improved performance in several object image and texture image classification tasks.	[Agarwal, Ankur] Microsoft Res Ltd, Cambridge CB3 0FB, England; [Triggs, Bill] LJK INRIA, LJK CNRS, F-38330 Montbonnot St Martin, France	Microsoft	Agarwal, A (corresponding author), Microsoft Res Ltd, 7 JJ Thomson Ave, Cambridge CB3 0FB, England.	ankagar@microsoft.com; bill.triggs@imag.fr						Agarwal A, 2006, LECT NOTES COMPUT SC, V3951, P30; Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108; BERG A, 2001, ITN C COMP VIS PATT; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bouman CA., 1997, CLUSTER UNSUPERVISED; BUNTINE W, 2003, AI STAT; BUNTINE W, 2005, DISCRETE PRINCIPAL C; CANNY J, 2004, ACM C INF RETR SIGIR; Csurka G., 2004, EUR C COMP VIS; DORKO G, 2005, OBJECT CLASS RECOGNI; EPSHTEIN B, 2005, INT C COMP VIS; EVERINGHAM M, 2006, SPRINGER LECT NOTES; FEIFEI L, 2005, INT C COMP VIS PATT; FERENCZ A, 2004, NEURAL INFORM PROCES; FRITZ M, 2004, EUR C COMP VIS; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; HOFMANN T, 1999, P UNCERTAINITY ARTIF; Joachims T., 1999, ADV KERNEL METHODS S; Jurie F., 2005, INT C COMP VIS; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; KELLER M, 2004, PASCAL WORKSH LEARN; Lang GK, 1997, MACH VISION APPL, V10, P123, DOI 10.1007/s001380050065; LAZEBNIK, 2006, INT C COMP VIS PATT; LAZEBNIK S, 2004, BRIT MACH VIS C, V2, P779; LAZEBNIK S, 2003, INT C COMP VIS; LECUN Y, 2004, IEEE C COMP VIS PATT; Leung T., 1999, INT C COMP VIS; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; MORI G, 2003, INT C COMP VIS PATT; Mutch J, 2006, IEEE CVPR, V1, P11, DOI DOI 10.1109/CVPR.2006.200; Opelt A., 2004, EUR C COMP VIS; Puzicha J, 1999, PATTERN RECOGN LETT, V20, P899, DOI 10.1016/S0167-8655(99)00056-2; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Schaffalitzky F, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P636, DOI 10.1109/ICCV.2001.937686; Schiele B, 2000, INT J COMPUT VISION, V36, P31, DOI 10.1023/A:1008120406972; SCHIELE B, 1999, INT C COMP VIS; Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215; Schmid C, 2004, INT J COMPUT VISION, V56, P7, DOI 10.1023/B:VISI.0000004829.38247.b0; SERRE T, 2005, ITN C COMP VIS PATT; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; VARMA M, 2003, INT C COMP VIS PATT	45	25	28	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2008	78	1					15	27		10.1007/s11263-007-0072-x	http://dx.doi.org/10.1007/s11263-007-0072-x			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	270YE		Green Submitted			2022-12-18	WOS:000253755300002
J	Zhang, T; Tomasi, C				Zhang, T; Tomasi, C			On the consistency of instantaneous rigid motion estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						camera motion analysis; egomotion; stochastic estimation; consistency	PASSIVE NAVIGATION; OPTICAL-FLOW	Instantaneous camera motion estimation is an important research topic in computer vision. Although in theory more than five points uniquely determine the solution in an ideal situation, in practice one can usually obtain better estimates by using more image velocity measurements because of the noise present in the velocity measurements. However, the usefulness of using a large number of observations has never been analyzed in detail. In this paper, we formulate this problem in the statistical estimation framework. We show that under certain noise models, consistency of motion estimation can be established: that is, arbitrarily accurate estimates of motion parameters are possible with more and more observations. This claim does not simply follow from the general consistency result for maximum likelihood estimates. Some experiments will be provided to verify our theory. Our analysis and experiments also indicate that many previously proposed algorithms are inconsistent under even very simple noise models.	IBM Corp, Thomas J Watson Res Ctr, Dept Math Sci, Yorktown Hts, NY 10598 USA; Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	International Business Machines (IBM); Stanford University	Zhang, T (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Dept Math Sci, Yorktown Hts, NY 10598 USA.	tzhang@watson.ibm.com; tomasi@cs.stanford.edu						BEAUCHEMIN S, 1996, ACM COMPUT SURV, V27, P433; BRUSS AR, 1983, COMPUT VISION GRAPH, V21, P3, DOI 10.1016/S0734-189X(83)80026-7; Gibson JJ., 1966, SENSES CONSIDERED PE; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; HILDRETH EC, 1992, VISION RES, V32, P1177, DOI 10.1016/0042-6989(92)90020-J; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; HORN BKP, 1987, INT J COMPUT VISION, V1, P259, DOI 10.1007/BF00127824; Jepson A. D., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P124, DOI 10.1109/WVM.1991.212779; JEPSON AD, 1993, SPATIAL VISION IN HUMANS AND ROBOTS, P39; KANATANI K, 1993, INT J COMPUT VISION, V11, P267, DOI 10.1007/BF01469345; Kruppa E., 1913, SITZ BER AKAD WIS MN, V122, P1939; Lucas B. D., 1981, IJCAI; MACLEAN WJ, 1994, P 5 BRIT MACH VIS C, P13; MATTISON DR, 1989, REPROD TOXICOL, V3, P3, DOI 10.1016/0890-6238(89)90032-4; MAYBANK SJ, 1985, P ROY SOC LOND A MAT, V401, P317, DOI 10.1098/rspa.1985.0101; Morevec H.P., 1977, INT JOINT C ART INT, V2, P584; Nalwa V. S., 1993, GUIDED TOUR COMPUTER; NEGAHDARIPOUR S, 1987, IEEE T PATTERN ANAL, V9, P168, DOI 10.1109/TPAMI.1987.4767884; NEGAHDARIPOUR S, 1989, INT J COMPUT VISION, V3, P293, DOI 10.1007/BF00132601; PRAZDNY K, 1983, COMPUT VISION GRAPH, V22, P239, DOI 10.1016/0734-189X(83)90067-1; Rao C. R, 1973, LINEAR STAT INFERENC; RIEGER JH, 1985, J OPT SOC AM A, V2, P254; SHI J, 1994, P IEEE C COMP VIS PA, P593, DOI DOI 10.1109/CVPR.1994.323794; Trucco E., 1998, INTRO TECHNIQUES 3D; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074; ZHANG J, 1999, THESIS STANFORD U; ZHANG T, 1999, P IEEE COMP VIS PATT, V1, P164; ZHUANG XH, 1988, COMPUT VISION GRAPH, V42, P334, DOI 10.1016/S0734-189X(88)80043-4	29	25	25	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		2002	46	1					51	79		10.1023/A:1013248231976	http://dx.doi.org/10.1023/A:1013248231976			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	503NT					2022-12-18	WOS:000172805100003
J	Olson, CF				Olson, CF			A general method for geometric feature matching and model extraction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						divide-and-conquer; feature matching; generate-and-test; geometric primitive extraction; Hough transform; model extraction; motion segmentation; object recognition; randomized algorithm; robust regression	3-DIMENSIONAL OBJECT RECOGNITION; HOUGH TRANSFORM; SQUARES REGRESSION; CURVE DETECTION; ALGORITHM; LINES; IMAGE	Popular algorithms for feature matching and model extraction fall into two broad categories: generate-and-test and Hough transform variations. However, both methods suffer from problems in practical implementations. Generate-and-test methods are sensitive to noise in the data. They often fail when the generated model fit is poor due to error in the data used to generate the model position. Hough transform variations are less sensitive to noise, but implementations for complex problems suffer from large time and space requirements and from the detection of false positives. This paper describes a general method for solving problems where a model is extracted from, or fit to, data that draws benefits from both generate-and-test methods and those based on the Hough transform, yielding a method superior to both. An important component of the method is the subdivision of the problem into many subproblems. This allows efficient generate-and-test techniques to be used, including the use of randomization to limit the number of subproblems that must be examined. Each subproblem is solved using pose space analysis techniques similar to the Hough transform, which lowers the sensitivity of the method to noise. This strategy is easy to implement and results in practical algorithms that are efficient and robust. We describe case studies of the application of this method to object recognition, geometric primitive extraction, robust regression, and motion segmentation.	Univ Washington, Dept Comp & Software Syst, Bothell, WA 98011 USA	University of Washington; University of Washington Bothell	Olson, CF (corresponding author), Univ Washington, Dept Comp & Software Syst, 18115 Campus Way NE,Box 358534, Bothell, WA 98011 USA.							Alter TD, 1998, INT J COMPUT VISION, V27, P127, DOI 10.1023/A:1007989016491; [Anonymous], 1985, PERCEPTUAL ORG VISUA; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; BERGEN JR, 1991, J ALGORITHM, V12, P639, DOI 10.1016/0196-6774(91)90037-Y; Bolles R.C., 1981, P 7 INT JOINT C ART, V1981, P637; Breuel T. M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P445, DOI 10.1109/CVPR.1992.223152; Cass TA, 1997, INT J COMPUT VISION, V21, P37, DOI 10.1023/A:1007971405872; DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242; EDELSBRUNNER H, 1990, J AM STAT ASSOC, V85, P115, DOI 10.2307/2289532; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; GRIMSON WEL, 1990, IEEE T PATTERN ANAL, V12, P255, DOI 10.1109/34.49052; GRIMSON WEL, 1994, INT J COMPUT VISION, V13, P7, DOI 10.1007/BF01420793; Hough P.V., 1962, US Patent, Patent No. [US3069654A, 3069654, 3,069,654]; HUANG TS, 1994, P IEEE, V82, P252, DOI 10.1109/5.265351; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; ILLINGWORTH J, 1988, COMPUT VISION GRAPH, V44, P87, DOI 10.1016/S0734-189X(88)80033-1; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; KIRYATI N, 1991, PATTERN RECOGN, V24, P303, DOI 10.1016/0031-3203(91)90073-E; LEAVERS VF, 1992, CVGIP-IMAG UNDERSTAN, V56, P381, DOI 10.1016/1049-9660(92)90049-9; LEAVERS VF, 1993, CVGIP-IMAG UNDERSTAN, V58, P250, DOI 10.1006/ciun.1993.1041; LINNAINMAA S, 1988, IEEE T PATTERN ANAL, V10, P634, DOI 10.1109/34.6772; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; MURAKAMI K, 1986, P IAPR INT C PATT RE, P831; Olson CF, 1998, IMAGE VISION COMPUT, V16, P627, DOI 10.1016/S0262-8856(98)00083-3; Olson CF, 1999, COMPUT VIS IMAGE UND, V73, P329, DOI 10.1006/cviu.1998.0728; Olson CF, 1997, INT J COMPUT VISION, V23, P131, DOI 10.1023/A:1007906812782; Olson CF, 1997, INFORM PROCESS LETT, V63, P237, DOI 10.1016/S0020-0190(97)00132-4; OLSON CF, 2000, LNCS, V1883, P20; ROSENFELD A, 1969, PICTURE PROCESSING C; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; Shapiro S. D., 1978, Proceedings of the 4th International Joint Conference on Pattern Recognition, P710; SILBERBERG TM, 1984, PATTERN RECOGN, V17, P621, DOI 10.1016/0031-3203(84)90015-3; STOCKMAN G, 1987, COMPUT VISION GRAPH, V40, P361, DOI 10.1016/S0734-189X(87)80147-0; THOMPSON DW, 1987, P IEEE C ROB AUT, V1, P208; XU L, 1990, PATTERN RECOGN LETT, V11, P331, DOI 10.1016/0167-8655(90)90042-Z	37	25	26	0	7	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2001	45	1					39	54		10.1023/A:1012317923177	http://dx.doi.org/10.1023/A:1012317923177			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	487JN					2022-12-18	WOS:000171872100003
J	Berthilsson, R; Astrom, K; Heyden, A				Berthilsson, R; Astrom, K; Heyden, A			Reconstruction of general curves, using factorization and bundle adjustment	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						curves; structure from motion; 3D; affine shape; proximity measure; bundle adjustment; error analysis	MOTION; SHAPE	In this paper, we extend the notion of affine shape, introduced by Sparr, from finite point sets to curves. The extension makes it possible to reconstruct 3D-curves up to projective transformations, from a number of their 2D-projections. We also extend the bundle adjustment technique from point features to curves. The first step of the curve reconstruction algorithm is based on affine shape. It is independent of choice of coordinates, is robust, does not rely on any preselected parameters and works for an arbitrary number of images. In particular this means that, except for a small set of curves (e.g. a moving line), a solution is given to the aperture problem of finding point correspondences between curves. The second step takes advantage of any knowledge of measurement errors in the images. This is possible by extending the bundle adjustment technique to curves. Finally, experiments are performed on both synthetic and real data to show the performance and applicability of the algorithm.	Univ Lund, Ctr Math Sci, S-22100 Lund, Sweden	Lund University	Berthilsson, R (corresponding author), Univ Lund, Ctr Math Sci, Box 118, S-22100 Lund, Sweden.		Astrom, Kalle/AAT-9538-2020; Åström, Kalle/C-2836-2009	Astrom, Kalle/0000-0002-8689-7810; Åström, Kalle/0000-0002-8689-7810				Astrom K, 1999, INT J COMPUT VISION, V33, P51, DOI 10.1023/A:1008113231241; Astrom K, 1999, IEEE T PATTERN ANAL, V21, P114, DOI 10.1109/34.748821; Berthilsson R, 1997, PROC CVPR IEEE, P444, DOI 10.1109/CVPR.1997.609363; Berthilsson R, 1999, J MATH IMAGING VIS, V11, P119, DOI 10.1023/A:1008379110792; BERTHILSSON R, 1997, EXTENSION AFFINE SHA; BERTHILSSON R, 1997, SCAND C IM AN; Blake A., 1992, ACTIVE VISION; CARLSSON S, 1994, P 3 EUR C COMP VIS S, V1, P83; DUNFORD N, 1963, INTERSCIENCE, V2; FAUGERAS O, 1993, INT J COMPUT VISION, V10, P125, DOI 10.1007/BF01420734; HEYDEN A, 1999, IMAGE VISION COMPUTI; HEYDEN A, 1997, P 10 SCAND C IM AN, P963; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; PAPADOPOULO T, 1996, P 4 EUR C COMP VIS C, P696; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; PORRILL J, 1991, IMAGE VISION COMPUT, V9, P45, DOI 10.1016/0262-8856(91)90048-T; SCHULZMIRBACH H, 1994, 894 TU; SHASHUA A, 1994, CVPR, P483; SPARR G, 1996, P INT C PATT REC VIE; SPARR G, 1995, IN PRESS P SOPH LIE; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684	22	25	27	0	5	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		2001	41	3					171	182		10.1023/A:1011104020586	http://dx.doi.org/10.1023/A:1011104020586			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	428LE					2022-12-18	WOS:000168462500002
J	Shashua, A; Toelg, S				Shashua, A; Toelg, S			The quadric reference surface: Theory and applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OBSTACLE DETECTION; MOTION; STEREOPSIS; CONTOURS; CAPTURE	The conceptual component of this work is about ''reference surfaces'' which are the analogous to reference frames often used for shape representation purposes. The theoretical component of this work involves the question of whether one can find a unique (and simple) mapping that aligns two arbitrary perspective views of an opaque textured quadric surface in 3D, given (i) few corresponding points in the two views, or (ii) the outline conic of the surface in one view (only) and few corresponding points in the two views. The practical component of this work is concerned with applying the theoretical results as tools for the task of achieving full correspondence between views of arbitrary objects.	ZENTRUM NEUROINFORMAT GMBH, BOCHUM, GERMANY		Shashua, A (corresponding author), HEBREW UNIV JERUSALEM, INST COMP SCI, IL-91904 JERUSALEM, ISRAEL.							ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; BERGEN JR, 1992, P EUR C COMP VIS SAN; BERGEN JR, 1990, HIERARCHICAL MOTION; BEYMER D, 1993, 1431 MIT AI; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; Irani M., 1993, Computer Analysis of Images and Patterns. 5th International Conference, CAIP '93 Proceedings, P371; JACOBS DW, 1993, P 2 EUR WORKSH INV P; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; KUMAR R, 1994, P INT C PATT REC JER; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; LUONG QT, 1993, DETERMINING FUNDAMEN; MALLOT HA, 1991, BIOL CYBERN, V64, P177, DOI 10.1007/BF00201978; MARR D, 1981, PROC R SOC SER B-BIO, V211, P151, DOI 10.1098/rspb.1981.0001; MAYBANK SJ, 1990, PHILOS T ROY SOC A, V332, P1, DOI 10.1098/rsta.1990.0099; MITCHISON GJ, 1985, NATURE, V315, P402, DOI 10.1038/315402a0; PRAZDNY K, 1986, NATURE, V324, P393, DOI 10.1038/324393a0; RAMACHANDRAN V S, 1985, Spatial Vision, V1, P57, DOI 10.1163/156856885X00080; RAMACHANDRAN VS, 1986, PERCEPT PSYCHOPHYS, V39, P361, DOI 10.3758/BF03203005; RAMACHANDRAN VS, 1985, NATURE, V317, P527, DOI 10.1038/317527a0; Robert L., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P540, DOI 10.1109/ICCV.1993.378165; SAWHNEY HS, 1994, 3D GEOMETRY PLANAR P; Shashua A, 1996, IEEE T PATTERN ANAL, V18, P873, DOI 10.1109/34.537342; SHASHUA A, 1994, IEEE T PATTERN ANAL, V16, P778, DOI 10.1109/34.308472; Shashua A, 1997, INT J COMPUT VISION, V21, P99, DOI 10.1023/A:1007975506780; SHASHUA A, 1991, 1327 MIT AI; SHASHUA A, 1994, P EUR C COMP VIS STO; SHASHUA A, 1992, AITR1401 MIT ART INT; Shashua A., 1992, ADV NEUR IN, V4, P404; SHASHUA A, 1993, LNCS, V825; Storjohann K., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P761, DOI 10.1109/ROBOT.1990.126078; VANSANTEN JPH, 1985, J OPT SOC AM A, V2, P300, DOI 10.1364/JOSAA.2.000300; ZHENG Y, 1990, IMAGE VISION COMPUT, V8, P57, DOI 10.1016/0262-8856(90)90057-C	34	25	33	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1997	23	2					185	198		10.1023/A:1007962930529	http://dx.doi.org/10.1023/A:1007962930529			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XK384					2022-12-18	WOS:A1997XK38400005
J	Fermuller, C; Aloimonos, Y				Fermuller, C; Aloimonos, Y			On the geometry of visual correspondence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OPTICAL-FLOW; MOTION	Image displacement fields-optical flow fields, stereo disparity fields, normal flow fields-due to rigid motion possess a global geometric structure which is independent of the scene in view. Motion vectors of certain lengths and directions are constrained to lie on the imaging surface at particular loci whose location and form depends solely on the 3D motion parameters. If optical flow fields or stereo disparity fields are considered, then equal vectors are shown to lie on conic sections. Similarly, for normal motion fields, equal vectors lie within regions whose boundaries also constitute conics. By studying various properties of these curves and regions and their relationships, a characterization of the structure of rigid motion fields is given. The goal of this paper is to introduce a concept underlying the global structure of image displacement fields. This concept gives rise to various constraints that could form the basis of algorithms for the recovery of visual information from multiple views.	ROYAL INST TECHNOL,CTR AUTOMAT RES,COMP VIS LAB,S-10044 STOCKHOLM,SWEDEN	Royal Institute of Technology	Fermuller, C (corresponding author), UNIV MARYLAND,CTR AUTOMAT RES,COMP VIS LAB,COLLEGE PK,MD 20742, USA.		Aloimonos, Yiannis/AAI-2969-2020	Aloimonos, Yiannis/0000-0002-8152-4281				Aloimonos J., 1984, Proceedings of the Workshop on Computer Vision: Representation and Control, P72; ALOIMONOS J, 1990, P DARPA IMAGE UNDERS, P816; ANANDAN P, 1985, 3RD P WORKSH COMP VI, P186; BERGHOLM F, 1988, INT J COMPUT VISION, V3, P395; DANIILIDIS K, 1990, IMAGE VISION COMPUT, V8, P297, DOI 10.1016/0262-8856(90)80006-F; DANIILIDIS K, 1992, THESIS U KARLSRUHE G; FAUGERAS O, 1993, INT J COMPUT VISION, V10, P125, DOI 10.1007/BF01420734; Faugeras O., 1992, 3 DIMENSIONAL COMPUT; Faugeras O. D., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P25; FAUGERAS OD, 1990, INT J COMPUT VISION, V4, P225, DOI 10.1007/BF00054997; FERMULLER C, 1995, INT J COMPUT VISION, V15, P7, DOI 10.1007/BF01450848; FERMULLER C, 1993, INT J COMPUT VISION, V11, P165, DOI 10.1007/BF01469227; FERMULLER C, 1993, ACTIVE PERCEPTION AD; FERNMULLER C, 1996, VISUAL NAVIGATION BI; GARDING J, 1993, TRITANAP9334 CVAP RO; Helmholtz von Hermann, 1896, HDB PHYSIOLOGISCHEN; HILDRETH EC, 1984, ARTIF INTELL, V23, P309, DOI 10.1016/0004-3702(84)90018-3; Horn B. K. P., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P2; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; JAIN R, 1983, IEEE T PATTERN ANAL, V5, P58, DOI 10.1109/TPAMI.1983.4767345; KOENDERINK JJ, 1986, VISION RES, V26, P161, DOI 10.1016/0042-6989(86)90078-7; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; LIU YC, 1988, COMPUT VISION GRAPH, V43, P37, DOI 10.1016/0734-189X(88)90041-2; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; MAYBANK S, 1993, THEORY RECONSTRUCTIO; NAVAB N, 1993, P INT C COMP VIS, P713; NELSON RC, 1988, BIOL CYBERN, V58, P261, DOI 10.1007/BF00364131; PRAZDNY K, 1980, BIOL CYBERN, V36, P87, DOI 10.1007/BF00361077; Selby S., 1972, STANDARD MATH TABLES; Shulman D., 1989, Proceedings. Workshop on Visual Motion (IEEE Cat. No.89CH2716-9), P81, DOI 10.1109/WVM.1989.47097; SPETSAKIS ME, 1988, DEC P INT C COMP VIS, P449; TISTARELLI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P108, DOI 10.1016/1049-9660(92)90089-L; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781	37	25	26	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1997	21	3					223	247		10.1023/A:1007951901001	http://dx.doi.org/10.1023/A:1007951901001			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WN532					2022-12-18	WOS:A1997WN53200004
J	Krishnan, A; Ahuja, N				Krishnan, A; Ahuja, N			Range estimation from focus using a non-frontal imaging camera	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								This paper is concerned with active sensing of range information from focus. It describes a new type of camera whose sensor plane is not perpendicular to the optical axis as is standard. This special imaging geometry eliminates sensor plane movement usually necessary for focusing. Camera panning, required for panoramic viewing anyway, in addition enables focusing and range estimation. Thus panning integrates the two standard mechanical actions of focusing and panning, implying range estimation is done at the speed of panning. An implementation of the proposed Non-frontal Imaging Camera (NICAM) design is described. Experiments on range estimation are also presented.			Krishnan, A (corresponding author), UNIV ILLINOIS, BECKMAN INST, 405 N MATHEWS AVE, URBANA, IL 61801 USA.							Abbott A. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P532, DOI 10.1109/CCV.1988.590034; AHUJA N, 1993, IEEE T PATTERN ANAL, V15, P1007, DOI 10.1109/34.254059; Darrell T., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P504, DOI 10.1109/CVPR.1988.196282; Das S., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P485, DOI 10.1109/ICCV.1990.139578; Das S., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P194, DOI 10.1109/CVPR.1993.340989; DAS S, 1992, CV9252 U ILL BECKM I; Ens J., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P600, DOI 10.1109/CVPR.1991.139760; Goodman J. W., 2005, MCGRAW HILL PHYS QUA; JARVIS RA, 1976, MICROSCOPE, V24, P163; KLAUS, 1968, 160 MIT ART INT LAB; KRISHNAN A, 1993, PROCEEDINGS OF THE ELEVENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P830; KRISHNAN A, 1993, P ACCV93 AS C COMP V, P133; KRISHNAN A, 1993, P SPIE C INT ROB COM, V12, P62; KRISHNAN A, 1994, UIUCBIAIRCV9408 U IL; KRISHNAN A, 1993, P DARPA IM UND WORKS, P959; KROTKOV E, 1987, INT J COMPUT VISION, V1, P223, DOI 10.1007/BF00127822; Krotkov E., 1986, Eighth International Conference on Pattern Recognition. Proceedings (Cat. No.86CH2342-4), P1156; Krotkov EP, 1989, ACTIVE COMPUTER VISI; Ligthart G., 1982, Proceedings of the 6th International Conference on Pattern Recognition, P597; Nayar S. K., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P218, DOI 10.1109/ROBOT.1990.125976; Pentland A., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P256, DOI 10.1109/CVPR.1989.37858; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; SCHLAG JF, 1985, CMURITR8314 CARN MEL; SPERLING G, 1970, AM J PSYCHOL, V83, P461, DOI 10.2307/1420686; TENENBAUM JM, 1971, THESIS STANFORD U PA; WILCOX B, 1993, COMMUNICATION; Xiong Y., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P68, DOI 10.1109/CVPR.1993.340977	27	25	27	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1996	20	3					169	185						17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VW629					2022-12-18	WOS:A1996VW62900002
J	Brunnstrom, K; Eklundh, JO; Uhlin, T				Brunnstrom, K; Eklundh, JO; Uhlin, T			Active fixation for scene exploration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							SPACE PRIMAL SKETCH; SCALE; RECOGNITION; MODEL	It is well-known that active selection of fixation points in humans is highly context and task dependent. It is therefore likely that successful computational processes for fixation in active vision should be so too. We are considering active fixation in the context of recognition of man-made objects characterized by their shapes. In this situation the qualitative shape and type of observed junctions play an important role. The fixations are driven by a grouping strategy, which forms sets of connected junctions separated from the surrounding at depth discontinuities. We have furthermore developed a methodology for rapid active detection and classification of junctions by selection of fixation points. The approach is based on direct computations from image data and allows integration of stereo and accommodation cues with luminance information. This work form a part of an effort to perform active recognition of generic objects, in the spirit of Malik and Biederman, but on real imagery rather than on line-drawings.			Brunnstrom, K (corresponding author), ROYAL INST TECHNOL,DEPT NUMER ANAL & COMP SCI,CVAP,KTH,S-10044 STOCKHOLM,SWEDEN.		Brunnström, Kjell/T-9793-2019	Brunnström, Kjell/0000-0001-5060-9402				ALOIMONOS Y, 1987, P 1 INT C COMP VIS, P35; Anderson B. D. O., 1979, OPTIMAL FILTERING; [Anonymous], 1975, PSYCHOL COMPUTER VIS; BAJCSY R, 1992, CVGIP-IMAG UNDERSTAN, V56, P31, DOI 10.1016/1049-9660(92)90083-F; Bajcsy R., 1985, P IEEE WORKSHOP COMP, P55; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BENGTSSON A, 1991, IEEE T PATTERN ANAL, V13, P85, DOI 10.1109/34.67634; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BIEDERMAN I, 1985, HUMAN MACHINE VISION, V2, P13; BRUNNSTROM K, 1990, IMAGE VISION COMPUT, V8, P289, DOI 10.1016/0262-8856(90)80005-E; BRUNNSTROM K, 1992, LECT NOTES COMPUT SC, V588, P701; BRUNNSTROM K, 1993, KTHNAP93295E ISRN RO; Crowley J. L., 1995, VISION PROCESS BASIC; CULHANE SM, 1992, P EUR C COMP VIS, P551; DICKINSON SJ, 1992, IEEE T PATTERN ANAL, V14, P174, DOI 10.1109/34.121788; DICKINSON SJ, 1992, CVGIP-IMAG UNDERSTAN, V55, P130, DOI 10.1016/1049-9660(92)90013-S; DRESCHLER L, 1982, COMPUT VISION GRAPH, V20, P199, DOI 10.1016/0146-664X(82)90081-8; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FORSTNER WA, 1987, P INT WORKSH INT SOC; Kitchen L, 1982, PATTERN RECOGN LETT, V1, P95, DOI 10.1016/0167-8655(82)90020-4; KOENDERINK JJ, 1990, BIOL CYBERN, V63, P291, DOI 10.1007/BF00203452; LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346; Lindeberg T., 1994, SCALE SPACE THEORY C; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; MORAVEC HP, 1977, OBSTACLE AVOIDANCE N; PAHLAVAN K, 1992, P 2 EUR C COMP VIS S, P526; PAHLAVAN K, 1993, KTHNAP93165E ISRN RO; RIMEY RD, 1992, P 2 EUR C COMP VIS, P542; Tenenbaum J.M., 1970, THESIS STANFORD U; ULLMAN S, 1987, IMAGE UNDERSTANDING; WESTELIUS CJ, 1991, P 7 SCAND C IM AN AA, P667; Wilkes D., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P136, DOI 10.1109/CVPR.1992.223215; Wilkes D., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P701, DOI 10.1109/CVPR.1993.341021; WILKES D, 1993, P 1993 SPIE BOST, P225; ZHANG W, 1993, P 4 INT C COMP VIS B, P183	35	25	25	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1996	17	2					137	162		10.1007/BF00058749	http://dx.doi.org/10.1007/BF00058749			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TZ498		Green Submitted			2022-12-18	WOS:A1996TZ49800003
J	HUTTENLOCHER, DP; WAYNER, PC				HUTTENLOCHER, DP; WAYNER, PC			FINDING CONVEX EDGE GROUPINGS IN AN IMAGE	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							GRAPH	In an image, there are groups of intensity edges that are likely to have resulted from the same convex object in a scene. A new method for identifying such groups is described here. Groups of edges that form a convex polygonal chain, such as a convex polygon or a spiral, are extracted from a set of image edge fragments. A key property of the method is that its output is no more complex than the original image. The method uses a triangulation of the linear edge segments in an image to define a local neighborhood that is scale invariant. From this local neighborhood a local convexity graph is constructed; this encodes which neighboring image edges could be part of a convex group. A path in the graph corresponds to a convex polygonal chain in the image, with a cyclic path corresponding to a polygon. We have implemented the method and found that it is efficient in practice as well as in theory. Examples are presented to illustrate that the technique finds intuitively salient groups, including for images of cluttered scenes.	CORNELL UNIV, DEPT COMP SCI, ITHACA, NY 14853 USA	Cornell University								AHUJA N, 1989, COMPUT VISION GRAPH, V48, P304, DOI 10.1016/0734-189X(89)90146-1; [Anonymous], 1985, PERCEPTUAL ORG VISUA; [Anonymous], 1983, DATA STRUCTURES ALGO; ARKIN EM, 1991, IEEE T PATT ANAL MAC, V13; BAJCSY R, 1987, 1ST P INT C COMP VIS, P231; Ballard D.H., 1982, COMPUTER VISION; BRADY M, 1984, INT J ROBOT RES, V3, P36, DOI 10.1177/027836498400300302; BRASMEHLMAN E, 1988, 2ND P INT C COMP VIS, P54; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P200, DOI 10.1109/TPAMI.1986.4767773; CHEN YS, 1989, PATTERN RECOGN, V22, P619, DOI 10.1016/0031-3203(89)90030-7; CHEW LP, 1989, ALGORITHMICA, V4, P97, DOI 10.1007/BF01553881; CHEW LP, 1985, P 1 ANN ACM S COMP G, P235; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; GRIMSON WEL, 1988, MIT ARTIFICIAL INTEL, V1019; JACOBS DW, 1989, MIT ARTIFICIAL INTEL, V1117; LINGAS A, 1989, LECT NOTES COMPUT SC, V344, P253; Lowe D. G., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P558, DOI 10.1109/CCV.1988.590036; MOHAN R, 1989, IEEE T PATTERN ANAL, V11, P1121, DOI 10.1109/34.42852; Mohan R., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P333, DOI 10.1109/CVPR.1989.37869; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; SCHWARTZ JT, 1987, INT J ROBOT RES, V6, P29, DOI 10.1177/027836498700600203; Sha'ashua A., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P321, DOI 10.1109/CCV.1988.590008; THOMPSON D, 1987, P IEEE C ROB AUT, P280; TOUSSAINT GT, 1980, PATTERN RECOGN, V12, P261, DOI 10.1016/0031-3203(80)90066-7; URQUHART R, 1982, PATTERN RECOGN, V15, P173, DOI 10.1016/0031-3203(82)90069-3; WERTHEIMER M, 1958, READINGS PRECEPTION; Witkin A. P., 1983, HUMAN MACHINE VISION; ZAHN CT, 1971, IEEE T COMPUT, VC 20, P68, DOI 10.1109/T-C.1971.223083; Zucker S. W., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P568, DOI 10.1109/CCV.1988.590037	29	25	28	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	1992	8	1					7	27		10.1007/BF00126398	http://dx.doi.org/10.1007/BF00126398			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JF613					2022-12-18	WOS:A1992JF61300001
J	Pumarola, A; Agudo, A; Martinez, AM; Sanfeliu, A; Moreno-Noguer, F				Pumarola, Albert; Agudo, Antonio; Martinez, Aleix M.; Sanfeliu, Alberto; Moreno-Noguer, Francesc			GANimation: One-Shot Anatomically Consistent Facial Animation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	15th European Conference on Computer Vision (ECCV)	SEP 08-14, 2018	Munich, GERMANY			GAN; Face animation; Action-unit condition		Recent advances in generative adversarial networks (GANs) have shown impressive results for the task of facial expression synthesis. The most successful architecture is StarGAN (Choi et al. in CVPR, 2018), that conditions GANs' generation process with images of a specific domain, namely a set of images of people sharing the same expression. While effective, this approach can only generate a discrete number of expressions, determined by the content and granularity of the dataset. To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on action units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression. Our approach allows controlling the magnitude of activation of each AU and combining several of them. Additionally, we propose a weakly supervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit a novel self-learned attention mechanism that makes our network robust to changing backgrounds, lighting conditions and occlusions. Extensive evaluation shows that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild. The code of this work is publicly available at .	[Pumarola, Albert; Agudo, Antonio; Sanfeliu, Alberto; Moreno-Noguer, Francesc] CSIC UPC, Inst Robot & Informat Ind, Barcelona 08028, Spain; [Martinez, Aleix M.] Ohio State Univ, Columbus, OH 43210 USA	Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i Informatica Industrial (IRII); Universitat Politecnica de Catalunya; University System of Ohio; Ohio State University	Pumarola, A (corresponding author), CSIC UPC, Inst Robot & Informat Ind, Barcelona 08028, Spain.	apumarola@iri.upc.edu; aagudo@iri.upc.edu; martinez.158@osu.edu; sanfeliu@iri.upc.edu; fmoreno@iri.upc.edu	Agudo, Antonio/C-5147-2017	Agudo, Antonio/0000-0001-6845-4998				[Anonymous], 2017, IEEE C COMP VIS PATT; [Anonymous], 2016, ICML; [Anonymous], 2014, ICLR; [Anonymous], 2017, ICCV; Arjovsky M., 2017, ARXIV170107875; Baltruaitis T., 2015, FG; BenitezQuiroz F. C., 2016, CVPR; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Choi Y., 2018, CVPR; Dolhansky B., 2018, CVPR; Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111; Ekman P., 2002, FACIAL ACTION CODING; FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani Ishaan, 2017, NIPS; Isola P., 2017, CVPR; Johnson J, 2016, ECCV; Karras Tero, 2018, ICLR; Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283; Kim T, 2017, ICML; Kingma D.P., 2015, INT C LEARN REPR, P1; Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076; Larsen A., 2016, ICML; Ledig C., 2017, CVPR; Li C., 2016, ECCV; Li M., 2016, ARXIV161005586 CORR; Liu M. -Y., 2017, NIPS; Liu Z., 2015, ICCV; Mathieu Michael, 2016, P INT C LEARN REPR I; Mirza M., 2014, ARXIV PREPRINT ARXIV; Nagano K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275075; Nam S, 2019, PROC CVPR IEEE, P1409, DOI 10.1109/CVPR.2019.00150; Odena A., 2017, ICML; Pathak D., 2016, CVPR; Perarnau G., 2016, NIPS WORKSH; Pumarola A., 2018, ECCV; Pumarola A., 2018, CVPR; Radford A., 2016, ICLR; Salimans T, 2016, ADV NEURAL INFORM PR; SCHERER KR, 1982, SOC SCI INFORM, V21, P555, DOI 10.1177/053901882021004004; Shen W., 2017, CVPR; Shrivastava A., 2017, CVPR; Song Y., 2018, ARXIV180404786; Susskind J.M., 2008, AFFECTIVE COMPUTING; Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640; Thies J., 2016, CVPR; Tulyakov S., 2018, CVPR; Vondrick Carl, 2016, ADV NEURAL INFORM PR; Vougioukas Konstantinos, 2018, BMVC; Wang X., 2016, ECCV; Wang Z., 2015, ECCV; Yu H, 2012, COMPUT GRAPH-UK, V36, P152, DOI 10.1016/j.cag.2011.12.002; Zhang H., 2017, ICCV; Zhou Hang, 2019, AAAI; Zhu S., 2017, ICCV	55	24	26	2	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		698	713		10.1007/s11263-019-01210-3	http://dx.doi.org/10.1007/s11263-019-01210-3			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	KU1MV		Green Submitted			2022-12-18	WOS:000519475600010
J	Kollias, D; Cheng, SY; Ververas, E; Kotsia, I; Zafeiriou, S				Kollias, Dimitrios; Cheng, Shiyang; Ververas, Evangelos; Kotsia, Irene; Zafeiriou, Stefanos			Deep Neural Network Augmentation: Generating Faces for Affect Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dimensional; Categorical affect; Valence; Arousal; Basic emotions; Facial affect synthesis; 4DFAB; Blendshape models; 3DMM fitting; DNNs; StarGAN; GANimation; Data augmentation; Affect recognition; Facial expression transfer		This paper presents a novel approach for synthesizing facial affect; either in terms of the six basic expressions (i.e., anger, disgust, fear, joy, sadness and surprise), or in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the emotion activation). The proposed approach accepts the following inputs:(i) a neutral 2D image of a person; (ii) a basic facial expression or a pair of valence-arousal (VA) emotional state descriptors to be generated, or a path of affect in the 2D VA space to be generated as an image sequence. In order to synthesize affect in terms of VA, for this person, 600,000 frames from the 4DFAB database were annotated. The affect synthesis is implemented by fitting a 3D Morphable Model on the neutral image, then deforming the reconstructed face and adding the inputted affect, and blending the new face with the given affect into the original image. Qualitative experiments illustrate the generation of realistic images, when the neutral image is sampled from fifteen well known lab-controlled or in-the-wild databases, including Aff-Wild, AffectNet, RAF-DB; comparisons with generative adversarial networks (GANs) show the higher quality achieved by the proposed approach. Then, quantitative experiments are conducted, in which the synthesized images are used for data augmentation in training deep neural networks to perform affect recognition over all databases; greatly improved performances are achieved when compared with state-of-the-art methods, as well as with GAN-based data augmentation, in all cases.	[Kollias, Dimitrios; Ververas, Evangelos; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, London SW7 2AZ, England; [Cheng, Shiyang] Samsung AI Ctr, Cambridge, England; [Kotsia, Irene] Middlesex Univ London, Dept Comp Sci, London NW4 4BT, England	Imperial College London; Middlesex University	Kollias, D (corresponding author), Imperial Coll London, Dept Comp, London SW7 2AZ, England.	dimitrios.kollias15@imperial.ac.uk; shiyang.c@samsung.com; e.ververas16@imperial.ac.uk; I.Kotsia@mdx.ac.uk; s.zafeiriou@imperial.ac.uk	KOLLIAS, DIMITRIOS/AAX-3397-2020	KOLLIAS, DIMITRIOS/0000-0002-8188-3751				Abbasnejad I, 2017, IEEE INT CONF COMP V, P1609, DOI 10.1109/ICCVW.2017.189; Alabort-i-Medina J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P679, DOI 10.1145/2647868.2654890; Alabort-i-Medina J, 2017, INT J COMPUT VISION, V121, P26, DOI 10.1007/s11263-016-0916-3; Amberg B, 2007, IEEE I CONF COMP VIS, P1326; Antoniou Antreas, 2017, ARXIV171104340; Averbuch-Elor H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130818; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712; Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7; Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580; Booth J, 2014, IEEE IMAGE PROC, P4672, DOI 10.1109/ICIP.2014.7025947; Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Chang Wei-Yi, 2017, P IEEE C COMP VIS PA; Cheng S., 2018, P IEEE C COMP VIS PA; Chew SW, 2012, IEEE T SYST MAN CY B, V42, P1006, DOI 10.1109/TSMCB.2012.2194485; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Cosker D, 2011, IEEE I CONF COMP VIS, P2296, DOI 10.1109/ICCV.2011.6126510; Deng JK, 2018, IEEE INT CONF AUTOMA, P399, DOI 10.1109/FG.2018.00064; Dhall Abhinav, 2017, P 19 ACM INT C MULT, P524, DOI [DOI 10.1145/3136755.3143004, 10.1145/3136755.3143004]; Ding H., 2018, 32 AAAI C ART INT; Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646; Fried O, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925933; Garrido P, 2014, PROC CVPR IEEE, P4217, DOI 10.1109/CVPR.2014.537; Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874; Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Knyazev B., 2017, ARXIV171104598; Kollias D, 2019, INT J COMPUT VISION, V127, P907, DOI 10.1007/s11263-019-01158-4; Kollias D, 2017, IEEE COMPUT SOC CONF, P1972, DOI 10.1109/CVPRW.2017.247; Kossaifi J, 2017, IMAGE VISION COMPUT, V65, P23, DOI 10.1016/j.imavis.2017.02.001; Kuipers J.B., 1999, QUATERNIONS ROTATION; Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076; Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277; LIN LI, 1989, BIOMETRICS, V45, P255, DOI 10.2307/2532051; Liu XC, 2008, COMPUT ANIMAT VIRT W, V19, P235, DOI 10.1002/cav.248; Ma L, 2019, COMPUT GRAPH FORUM, V38, P470, DOI 10.1111/cgf.13586; Maimon OZ, 2005, DATA MINING KNOWLEDG; Mirza M., 2014, ARXIV; Mohammed U, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531363; Mollahosseini A., 2017, ARXIV170803985; Neumann T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508417; Parkhi Omkar M., 2015, BRIT MACH VIS C; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269; Pham H. X., 2018, ARXIV180307716; Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50; Qiao F., 2018, ARXIV180201822; Reed S, 2014, PR MACH LEARN RES, V32, P1431; Ringeval Fabien, 2013, AUT FAC GEST REC FG, P1, DOI [10.1109/FG.2013.6553805, DOI 10.1109/FG.2013.6553805]; Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41; RUSSELL JA, 1978, J PERS SOC PSYCHOL, V36, P1152, DOI 10.1037/0022-3514.36.10.1152; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Shang F., 2014, P 23 ACM INT C C INF, P1149, DOI DOI 10.1145/2661829.2662083; Sohn Kihyuk, 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628; Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Susskind Joshua M, 2008, AFFECTIVE COMPUTING, P421; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Thies J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201350; Thies Justus, 2016, CVPR, DOI DOI 10.1109/CVPR.2016.262; Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005; Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258; Vielzeuf V., P 19 ACM INT C MULT, P569, DOI DOI 10.1145/3136755.3143011; Wheeler M D, 1995, ITERATIVE ESTIMATION; Whissell Cynthia, 1989, MEASUREMENT EMOTIONS, P113; Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892; Wu WN, 2018, LECT NOTES COMPUT SC, V11205, P622, DOI 10.1007/978-3-030-01246-5_37; Zafeiriou S, 2017, IEEE COMPUT SOC CONF, P1980, DOI 10.1109/CVPRW.2017.248; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhou YQ, 2017, INT CONF AFFECT, P370; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu X., 2018, PACIFIC ASIA C KNOWL, P349	76	24	25	1	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1455	1484		10.1007/s11263-020-01304-3	http://dx.doi.org/10.1007/s11263-020-01304-3		FEB 2020	30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Green Submitted, Green Accepted, hybrid			2022-12-18	WOS:000516378000001
J	Akbarinia, A; Parraga, CA				Akbarinia, Arash; Alejandro Parraga, C.			Feedback and Surround Modulated Boundary Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Boundary detection; Surround modulation; Biologically-inspired vision	PRIMARY VISUAL-CORTEX; CONTOUR-DETECTION; SPATIAL SUMMATION; MACAQUE V1; MODEL; ARCHITECTURE; PERCEPTION	Edges are key components of any visual scene to the extent that we can recognise objects merely by their silhouettes. The human visual system captures edge information through neurons in the visual cortex that are sensitive to both intensity discontinuities and particular orientations. The classical approach assumes that these cells are only responsive to the stimulus present within their receptive fields, however, recent studies demonstrate that surrounding regions and inter-areal feedback connections influence their responses significantly. In this work we propose a biologically-inspired edge detection model in which orientation selective neurons are represented through the first derivative of a Gaussian function resembling double-opponent cells in the primary visual cortex (V1). In our model we account for four kinds of receptive field surround, i.e. full, far, iso- and orthogonal-orientation, whose contributions are contrast-dependant. The output signal from V1 is pooled in its perpendicular direction by larger V2 neurons employing a contrast-variant centre-surround kernel. We further introduce a feedback connection from higher-level visual areas to the lower ones. The results of our model on three benchmark datasets show a big improvement compared to the current non-learning and biologically-inspired state-of-the-art algorithms while being competitive to the learning-based methods.	[Akbarinia, Arash; Alejandro Parraga, C.] Univ Autonoma Barcelona, Ctr Visio Comp, Barcelona, Spain	Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	Akbarinia, A (corresponding author), Univ Autonoma Barcelona, Ctr Visio Comp, Barcelona, Spain.	arash.akbarinia@cvc.uab.es	Parraga, C. Alejandro/D-2329-2011	Parraga, C. Alejandro/0000-0002-3809-241X	Spanish Secretary of Research and Innovation [TIN2013-41751-P, TIN2013-49982-EXP]; CERCA Programme from the Generalitat de Catalunya	Spanish Secretary of Research and Innovation; CERCA Programme from the Generalitat de Catalunya	This work was funded by the Spanish Secretary of Research and Innovation (TIN2013-41751-P and TIN2013-49982-EXP) and the CERCA Programme from the Generalitat de Catalunya.	Akbarinia A., 2016, P BRIT MACH VIS C BM; Angelucci A, 2014, NEW VISUAL NEUROSCIENCES, P425; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bertasius G, 2015, IEEE I CONF COMP VIS, P504, DOI 10.1109/ICCV.2015.65; Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067; Briggs F, 2014, NEW VISUAL NEUROSCIENCES, P315; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Cour T, 2005, PROC CVPR IEEE, P1124; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Diaz-Pernas FJ, 2014, APPL SOFT COMPUT, V21, P250, DOI 10.1016/j.asoc.2014.03.040; Dollar P., 2006, P IEEE COMP SOC C CO, V2, P1964, DOI DOI 10.1109/CVPR.2006.298; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Field DJ, 2014, NEW VISUAL NEUROSCIENCES, P627; Freeman J, 2013, NAT NEUROSCI, V16, P974, DOI 10.1038/nn.3402; Fu H, 2016, PROC CVPR IEEE, P241, DOI 10.1109/CVPR.2016.33; Gao SB, 2013, IEEE I CONF COMP VIS, P929, DOI 10.1109/ICCV.2013.119; Georgeson MA, 1997, VISION RES, V37, P3255, DOI 10.1016/S0042-6989(97)00124-7; Grigorescu C, 2003, IEEE T IMAGE PROCESS, V12, P729, DOI 10.1109/TIP.2003.814250; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; Hess RF, 2014, NEW VISUAL NEUROSCIENCES, P595; Hupe JM, 1998, NATURE, V394, P784, DOI 10.1038/29537; Ichida JM, 2007, J NEUROPHYSIOL, V98, P2168, DOI 10.1152/jn.00298.2007; Kapadia MK, 1999, P NATL ACAD SCI USA, V96, P12073, DOI 10.1073/pnas.96.21.12073; Kivinen J. J., 2014, AISTATS, V1, P9; KOENDERINK JJ, 1982, PERCEPTION, V11, P129, DOI 10.1068/p110129; Landy MS, 2014, NEW VISUAL NEUROSCIENCES, P639; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Loffler G, 2008, VISION RES, V48, P2106, DOI 10.1016/j.visres.2008.03.006; MALACH R, 1993, P NATL ACAD SCI USA, V90, P10469, DOI 10.1073/pnas.90.22.10469; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Mely DA, 2016, VISION RES, V120, P93, DOI 10.1016/j.visres.2015.11.007; MORRONE MC, 1988, PROC R SOC SER B-BIO, V235, P221, DOI 10.1098/rspb.1988.0073; O'Herron P, 2011, J VISION, V11, DOI 10.1167/11.2.12; Olkkonen M, 2008, J VISION, V8, DOI 10.1167/8.8.8; Papari G, 2011, IMAGE VISION COMPUT, V29, P79, DOI 10.1016/j.imavis.2010.08.009; Parraga C. A., 2016, J VIS, V16, P214, DOI DOI 10.1167/16.12.214; Poirier FJAM, 2006, VISION RES, V46, P2443, DOI 10.1016/j.visres.2006.01.026; Prewitt J.M., 1970, PICTURE PROCESS PSYC, V10, P15; Shapley R, 2011, VISION RES, V51, P701, DOI 10.1016/j.visres.2011.02.012; Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024; Shushruth S, 2013, J NEUROSCI, V33, P106, DOI 10.1523/JNEUROSCI.2518-12.2013; Shushruth S, 2009, J NEUROPHYSIOL, V102, P2069, DOI 10.1152/jn.00512.2009; Spillmann L, 2015, J VISION, V15, DOI 10.1167/15.9.7; Spratling MW, 2013, IEEE T IMAGE PROCESS, V22, P1629, DOI 10.1109/TIP.2012.2235850; Stein A, 2007, IEEE I CONF COMP VIS, P110; Theriault C., 2015, BIOL INSPIRED COMPUT; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Tzvetanov T, 2002, VISION RES, V42, P2493, DOI 10.1016/S0042-6989(02)00198-0; Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808; Walther DB, 2011, P NATL ACAD SCI USA, V108, P9661, DOI 10.1073/pnas.1015666108; WATT RJ, 1985, VISION RES, V25, P1661, DOI 10.1016/0042-6989(85)90138-5; Wei H, 2013, NEUROCOMPUTING, V103, P247, DOI 10.1016/j.neucom.2012.09.027; Wilson HR, 2014, NEW VISUAL NEUROSCIENCES, P617; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yang KF, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425538; Yang KF, 2014, IEEE T IMAGE PROCESS, V23, P5020, DOI 10.1109/TIP.2014.2361210; Yang KF, 2013, PROC CVPR IEEE, P2810, DOI 10.1109/CVPR.2013.362; Zeki S., 1993, VISION BRAIN	62	24	24	2	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2018	126	12			SI		1367	1380		10.1007/s11263-017-1035-5	http://dx.doi.org/10.1007/s11263-017-1035-5			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GZ3KT					2022-12-18	WOS:000449286200007
J	Hahne, C; Aggoun, A; Velisavljevic, V; Fiebig, S; Pesch, M				Hahne, Christopher; Aggoun, Amar; Velisavljevic, Vladan; Fiebig, Susanne; Pesch, Matthias			Baseline and Triangulation Geometry in a Standard Plenoptic Camera	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Light field; Plenoptic; Camera; Microscope; Triangulation; Baseline; Distance; Estimation		In this paper, we demonstrate light field triangulation to determine depth distances and baselines in a plenoptic camera. Advances in micro lenses and image sensors have enabled plenoptic cameras to capture a scene from different viewpoints with sufficient spatial resolution. While object distances can be inferred from disparities in a stereo viewpoint pair using triangulation, this concept remains ambiguous when applied in the case of plenoptic cameras. We present a geometrical light field model allowing the triangulation to be applied to a plenoptic camera in order to predict object distances or specify baselines as desired. It is shown that distance estimates from our novel method match those of real objects placed in front of the camera. Additional benchmark tests with an optical design software further validate the model's accuracy with deviations of less than +/- 0.33% for several main lens types and focus settings. A variety of applications in the automotive and robotics field can benefit from this estimation model.	[Hahne, Christopher; Aggoun, Amar; Velisavljevic, Vladan] Univ Bedfordshire, Luton, Beds, England; [Fiebig, Susanne; Pesch, Matthias] ARRI Cine Tech GmbH & Co KG, Munich, Germany	University of Bedfordshire	Hahne, C (corresponding author), Univ Bedfordshire, Luton, Beds, England.	info@christopherhahne.de; a.aggoun@wlv.ac.uk; vladan.velisavljevic@beds.ac.uk; sfiebig@arri.com; mpesch@arri.com	Velisavljevic, Vladan/B-6471-2015; Hahne, Christopher/AAI-3861-2021	Velisavljevic, Vladan/0000-0001-9980-9368; Hahne, Christopher/0000-0003-2786-9905	EU as Project 3D VIVANT [EU-FP7 ICT-2010-248420]	EU as Project 3D VIVANT	This research was supported in part by the EU as Project 3D VIVANT under EU-FP7 ICT-2010-248420.	Abbeloos W., 2012, STEREO MATCHING; Abbeloos W., 2010, THESIS; ADELSON EH, 1992, IEEE T PATTERN ANAL, V14, P99, DOI 10.1109/34.121783; [Anonymous], 2006, DIGITAL LIGHT FIELD, P1; Bobick AF, 1999, INT J COMPUT VISION, V33, P181, DOI 10.1023/A:1008150329890; Bok Y., 2014, P EUR C COMP VIS ECC; Broxton M, 2013, OPT EXPRESS, V21, P25418, DOI 10.1364/OE.21.025418; Burger W., 2010, PRINCIPLES DIGITAL I; Caldwell JB, 2000, OPT PHOTONICS NEWS, V11, P49, DOI 10.1364/OPN.11.7.000049; Cho D, 2013, IEEE I CONF COMP VIS, P3280, DOI 10.1109/ICCV.2013.407; Dansereau D. G., 2014, THESIS; Dansereau DG, 2013, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2013.137; Ellison I., 2014, WHAT IS FOCAL LENGTH; Fiss J., 2014, IEEE INT C COMPUTATI, P1; Georgiev T., 2006, RENDER TECH, V2006, P21; Hahn C, 2014, 3DTV C TRUE VISION C, P1, DOI DOI 10.1109/3DTV.2014.6874734; Hahne C., 2016, THESIS; Hahne C, 2016, OPT EXPRESS, V24, P21521, DOI 10.1364/OE.24.021521; Hahne C, 2014, OPT EXPRESS, V22, P26659, DOI 10.1364/OE.22.026659; Heber Stefan, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8694, P751, DOI 10.1007/978-3-319-10599-4_48; Hirshfeld A. W., 2001, PARALLAX RACE MEASUR; Huang FC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766922; Iocchi L., 1998, STEREO VISION TRIANG; Jeon H., 2015, P INT C COMP VIS PAT; Levoy M., 1996, TECH REP; Levoy M, 2006, ACM T GRAPHIC, V25, P924, DOI 10.1145/1141911.1141976; Lumsdaine A., 2008, TECH REP; Marr D., 1976, TECH REP; Ng R., 2005, THESIS STANFORD U; Perwass C, 2012, PROC SPIE, V8291, DOI 10.1117/12.909882; Tao MW, 2017, IEEE T PATTERN ANAL, V39, P546, DOI 10.1109/TPAMI.2016.2554121; TRIOPTICS, 2015, MTF MEAS FURTH PAR; Wheatstone C, 1838, PHILOS T R SOC LOND, V128, P371; Yanagisawa M., 1990, OPTICAL SYSTEM HAVIN, Patent No. 4,908,639; YANG Y, 1993, IEEE COMP SOC C COMP, P274, DOI DOI 10.1109/CVPR.1993.340969; Zemax L. L. C., 2011, OPTICAL ILLUMINATION	36	24	25	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2018	126	1					21	35		10.1007/s11263-017-1036-4	http://dx.doi.org/10.1007/s11263-017-1036-4			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FS6MC		hybrid, Green Published, Green Submitted			2022-12-18	WOS:000419910500002
J	Xie, LX; Wang, JD; Lin, WY; Zhang, B; Tian, Q				Xie, Lingxi; Wang, Jingdong; Lin, Weiyao; Zhang, Bo; Tian, Qi			Towards Reversal-Invariant Image Representation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image classification; BoF; CNN; Reversal-invariant image representation	FEATURES; OBJECT	State-of-the-art image classification approaches are mainly based on robust image representation, such as the bag-of-features (BoF) model or the convolutional neural network (CNN) architecture. In real applications, the orientation (left/right) of an image or an object might vary from sample to sample, whereas some handcrafted descriptors (e.g., SIFT) and network operations (e.g., convolution) are not reversal-invariant, leading to the unsatisfied stability of image features extracted from these models. To deal with, a popular solution is to augment the dataset by adding a left-right reversed copy for each image. This strategy improves the recognition accuracy to some extent, but also brings the price of almost doubled time and memory consumptions on both the training and testing stages. In this paper, we present an alternative solution based on designing reversal-invariant representation of local patterns, so that we can obtain the identical representation for an image and its left-right reversed copy. For the BoF model, we design a reversal-invariant version of SIFT descriptor named Max-SIFT, a generalized RIDE algorithm which can be applied to a large family of local descriptors. For the CNN architecture, we present a simple idea of generating reversal-invariant deep features (RI-Deep), and, inspired by which, design reversal-invariant convolution (RI-Conv) layers to increase the CNN capacity without increasing the model complexity. Experiments reveal consistent accuracy gain on various image classification tasks, including scene understanding, fine-grained object recognition, and large-scale visual recognition.	[Xie, Lingxi] Johns Hopkins Univ, Malone Hall, Baltimore, MD 21218 USA; [Wang, Jingdong] Microsoft Res, Bldg 2,5 Danling St, Beijing 100080, Peoples R China; [Lin, Weiyao] Shanghai Jiao Tong Univ, SEIEE Bldg, Shanghai 200240, Peoples R China; [Zhang, Bo] Tsinghua Univ, FIT Bldg, Beijing 100084, Peoples R China; [Tian, Qi] Univ Texas San Antonio, One UTSA Circle, San Antonio, TX 78249 USA	Johns Hopkins University; Microsoft; Shanghai Jiao Tong University; Tsinghua University; University of Texas System; University of Texas at San Antonio (UTSA)	Wang, JD (corresponding author), Microsoft Res, Bldg 2,5 Danling St, Beijing 100080, Peoples R China.; Tian, Q (corresponding author), Univ Texas San Antonio, One UTSA Circle, San Antonio, TX 78249 USA.	198808xc@gmail.com; jingdw@microsoft.com; wylin@sjtu.edu.cn; dcszb@mail.tsinghua.edu.cn; qitian@cs.utsa.edu	Xie, Lingxi/ABF-6996-2020; Wang, Jingdong/E-9920-2017	Wang, Jingdong/0000-0002-4888-4445; Lin, Weiyao/0000-0001-8307-7107	973 Program of China [2013CB329403, 2012CB316301]; NSFC [61332007, 61273023, 61429201, 61471235]; Tsinghua ISRP [20121088071]; ARO [W911NF-15-1-0290, W911NF-12-1-0057]; Faculty Research Awards, NEC Lab of America	973 Program of China(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC)); Tsinghua ISRP; ARO; Faculty Research Awards, NEC Lab of America	This work was done when Lingxi Xie was an intern at Microsoft Research. This work is supported by the 973 Program of China 2013CB329403 and 2012CB316301, NSFC 61332007, 61273023, 61429201, 61471235, Tsinghua ISRP 20121088071, ARO Grants W911NF-15-1-0290 and W911NF-12-1-0057, and Faculty Research Awards, NEC Lab of America.	Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110; Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128; Bosch A, 2006, LECT NOTES COMPUT SC, V3954, P517; Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56; Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Clinchant S., 2007, IMAGEVAL WORKSH CVIR; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J., 2014, IEEE C INT C MACH LE; Feng J., 2011, IEEE C COMP VIS PATT; Gavves E, 2015, INT J COMPUT VISION, V111, P191, DOI 10.1007/s11263-014-0741-5; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Goodfellow I. J., 2013, ARXIV13024389; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; Griffin Gregory, 2007, CALTECH 256 OBJECT C; Guo X., 2010, INT C PATT REC; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Jia Y., 2014, P 22 ACM INT C MULT, P675; Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124; Kobayashi T., 2014, IEEE C COMP VIS PATT; Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lapin M., 2014, IEEE C COMP VIS PATT; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lee CY, 2015, INT CONF ADV ROBOT; Lee CY, 2016, JMLR WORKSH CONF PRO, V51, P464; Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542; Li L., 2015, INT C IM PROC; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775; Lin D, 2014, PROC CVPR IEEE, P3726, DOI 10.1109/CVPR.2014.476; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; Liu K, 2014, INT J COMPUT VISION, V106, P342, DOI 10.1007/s11263-013-0634-z; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma R., 2010, C IM VID RETR; Maji S., 2013, TECHNICAL REPORT, P6; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; Murray N, 2014, PROC CVPR IEEE, P2473, DOI 10.1109/CVPR.2014.317; Nagadomi, 2014, KAGGLE CIFAR10 NETWO; Newsam, 2010, P 18 SIGSPATIAL INT, P2, DOI DOI 10.1145/1869790.1869829; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092; Paulin M., 2014, IEEE C COMP VIS PATT; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Pu J, 2014, LECT NOTES COMPUT SC, V8691, P425, DOI 10.1007/978-3-319-10578-9_28; Qian Q., 2015, IEEE C COMP VIS PATT; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Razavian Ali Sharif, 2014, P IEEE C COMP VIS PA, P806, DOI DOI 10.1109/CVPRW.2014.131; Rublee E., 2011, IEEE C INT C COMP VI; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Skelly L., 2007, P SPIE, V6762; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Takacs G, 2013, IEEE T IMAGE PROCESS, V22, P2970, DOI 10.1109/TIP.2012.2230011; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Tuytelaars T, 2010, PROC CVPR IEEE, P2281, DOI 10.1109/CVPR.2010.5539911; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Vedaldi Andrea, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249; Wah C., 2011, TECH REP; WANG JJ, 2010, PROC CVPR IEEE, P3360, DOI DOI 10.1109/CVPR.2010.5540018; Wang Z., 2011, IEEE C INT C COMP VI; Wang ZL, 2015, INT J COMPUT VISION, V114, P322, DOI 10.1007/s11263-014-0739-z; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xie L., 2014, IEEE C COMP VIS PATT; Xie L., 2015, IEEE T CIRCUITS SYST; Xie L., 2013, INT C COMP VIS; Xie LX, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P3, DOI 10.1145/2671188.2749289; Xie LX, 2015, IEEE I CONF COMP VIS, P100, DOI 10.1109/ICCV.2015.20; Xie LX, 2014, IEEE T IMAGE PROCESS, V23, P1994, DOI 10.1109/TIP.2014.2310117; Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang B, 2015, INT C AC SPEECH SIGN; Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zhang S., 2009, ACM INT C MULT, DOI DOI 10.1145/1631272.1631285; Zhang X., 2014, ACM INTERNATIONAL CO; Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128; Zhao WL, 2013, IEEE T IMAGE PROCESS, V22, P980, DOI 10.1109/TIP.2012.2226043; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11	92	24	26	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2017	123	2					226	250		10.1007/s11263-016-0970-x	http://dx.doi.org/10.1007/s11263-016-0970-x			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EU2OM					2022-12-18	WOS:000400868800005
J	Huang, DF; Luo, L; Chen, ZY; Wen, M; Zhang, CY				Huang, Dafei; Luo, Lei; Chen, Zhaoyun; Wen, Mei; Zhang, Chunyuan			Applying Detection Proposals to Visual Tracking for Scale and Aspect Ratio Adaptability	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual tracking; Detection proposal; Scale and aspect ratio adaptability; Correlation filter	GRADIENTS	The newly proposed correlation filter based trackers can achieve appealing performance despite their great simplicity and superior speed. However, this kind of object trackers is not born with scale and aspect ratio adaptability, thus resulting in suboptimal tracking accuracy. To tackle this problem, this paper integrates the class-agnostic detection proposal method, which is widely adopted in object detection area, into a correlation filter tracker. In the tracker part, optimizations such as feature integration, robust model updating and proposal rejection are applied for efficient integration. As for proposal generation, through integrating and comparing four detection proposal generators along with two baseline methods, the quality of detection proposals is found to have considerable influence on tracking accuracy. Therefore, as the most promising proposal generator, EdgeBoxes is chosen and further enhanced with background suppression. Evaluations are mainly performed on a challenging 50-sequence dataset (OTB50) and its two subsets, 28 sequences with significant scale variation and 14 sequences with obvious aspect ratio change. Among the trackers equipped with different proposal generators, state-of-the-art trackers and existing correlation filter variants, our proposed tracker reports the highest accuracy while running efficiently at an average speed of 20.4 frames per second. Additionally, numerical performance analysis in per-sequence manner and experiment results on VOT2014 dataset are also presented to enable deeper insights into our approach.	[Huang, Dafei; Luo, Lei; Chen, Zhaoyun; Wen, Mei; Zhang, Chunyuan] Natl Univ Def Technol, Coll Comp, Changsha, Hunan, Peoples R China; [Huang, Dafei; Chen, Zhaoyun; Wen, Mei; Zhang, Chunyuan] Natl Univ Def Technol, Natl Key Lab Parallel & Distributed Proc, Changsha, Hunan, Peoples R China	National University of Defense Technology - China; National University of Defense Technology - China	Luo, L (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha, Hunan, Peoples R China.	huangdafei1012@163.com; l.luo@nudt.edu.cn; chenzhaoyun09@163.com; meiwen@nudt.edu.cn; cyzhang@nudt.edu.cn			National Natural Science Foundation of China [61272145, 61402504]; 863 Program of China [2012-AA012706]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); 863 Program of China(National High Technology Research and Development Program of China)	The authors gratefully acknowledge the support from National Natural Science Foundation of China under No. 61272145, 61402504, and 863 Program of China under No. 2012-AA012706.	Aiping Wang, 2013, Transactions on Edutainment IX, P225, DOI 10.1007/978-3-642-37042-7_16; Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Belagiannis V, 2012, LECT NOTES COMPUT SC, V7575, P842, DOI 10.1007/978-3-642-33765-9_60; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Cai Z., 2012, ACCV, P86; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143; Dollar P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231; Duffner S, 2013, IEEE I CONF COMP VIS, P2480, DOI 10.1109/ICCV.2013.308; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Godec M, 2011, IEEE I CONF COMP VIS, P81, DOI 10.1109/ICCV.2011.6126228; Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Henriques J. F., 2012, EUR C COMP VIS, P702, DOI DOI 10.1007/978-3-642-33765-9_50; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hosang J., 2015, TPAMI, DOI [10.1109/TPAMI.2015.2465908., DOI 10.1109/TPAMI.2015.2465908.]; Hosang J., 2014, BMVC; Hua Y, 2015, IEEE I CONF COMP VIS, P3092, DOI 10.1109/ICCV.2015.354; Huang Dafei, 2015, BMVC; Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880; Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231; Krahenbuhl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47; Kristan M., 2014, P INT C COMP VIS, P191; Kristan M, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P98, DOI 10.1109/ICCVW.2013.20; Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821; Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18; Liang PP, 2016, IEEE SIGNAL PROC LET, V23, P949, DOI 10.1109/LSP.2016.2556706; Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730; Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809; Wang AP, 2009, IEEE IMAGE PROC, P1449, DOI 10.1109/ICIP.2009.5414559; Wen LY, 2015, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2015.7298835; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9; Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882; Zhou T., 2015, BING OBJECTNESS PROP; Zhu G., 2016, P IEEE C COMP VIS PA, P26; Zhu G, 2016, PROC CVPR IEEE, P943, DOI 10.1109/CVPR.2016.108; Zhu GB, 2016, AAAI CONF ARTIF INTE, P3690; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	47	24	27	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2017	122	3			SI		524	541		10.1007/s11263-016-0974-6	http://dx.doi.org/10.1007/s11263-016-0974-6			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ES7OH					2022-12-18	WOS:000399739200008
J	Zhang, LL; Lu, HM; Hu, XP; Koch, R				Zhang, Lilian; Lu, Huimin; Hu, Xiaoping; Koch, Reinhard			Vanishing Point Estimation and Line Classification in a Manhattan World with a Unifying Camera Model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Vanishing points; Line classification; Manhattan world; Unifying camera model	IMAGES	The problem of estimating vanishing points for visual scenes under the Manhattan world assumption has been addressed for more than a decade. Surprisingly, the special characteristic of the Manhattan world that lines should be orthogonal or parallel to each other is seldom well utilized. In this paper, we present an algorithm that accurately and efficiently estimates vanishing points and classifies lines by thoroughly taking advantage of this simple fact in the Manhattan world for images grabbed by a camera with a single effective viewpoint (e.g. perspective camera or central catadioptric camera). The algorithm is also extended to estimate the focal length of the camera when it is uncalibrated. The key novelty is to estimate three orthogonal line directions in the camera frame simultaneously instead of estimating vanishing points in the image plane directly. The performance of the proposed algorithm is demonstrated on four publicly available databases. Compared to the state-of-the-art methods, the experiments show its superiority in terms of both accuracy and efficiency.	[Zhang, Lilian; Lu, Huimin; Hu, Xiaoping] Natl Univ Def Technol, Coll Mechatron & Automat, Dept Automat Control, Changsha, Hunan, Peoples R China; [Koch, Reinhard] Univ Kiel, Inst Comp Sci, Olshaussenstr 40, D-24098 Kiel, Germany	National University of Defense Technology - China; University of Kiel	Zhang, LL (corresponding author), Natl Univ Def Technol, Coll Mechatron & Automat, Dept Automat Control, Changsha, Hunan, Peoples R China.	lilian-zhang@hotmail.com; lhmnew@nudt.edu.cn; xphu@nudt.edu.cn; rk@mip.informatik.uni-kiel.de		Koch, Reinhard/0000-0003-4398-1569	China Scholarship Council [2009611008]; National Natural Science Foundation of China [61503403]	China Scholarship Council(China Scholarship Council); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by China Scholarship Council (No. 2009611008) and National Natural Science Foundation of China (No. 61503403). The authors thank the anonymous reviewers for their valuable comments.	Aguilera D.G., 2005, NEW METHOD VANISHING; Baker S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P35, DOI 10.1109/ICCV.1998.710698; Barreto JP, 2005, IEEE T PATTERN ANAL, V27, P1327, DOI 10.1109/TPAMI.2005.163; Bazin JC, 2010, COMPUT VIS IMAGE UND, V114, P254, DOI 10.1016/j.cviu.2009.04.006; Bazin JC, 2012, IEEE INT C INT ROBOT, P4282, DOI 10.1109/IROS.2012.6385802; Bazin JC, 2012, INT J ROBOT RES, V31, P63, DOI 10.1177/0278364911421954; Bazin Jean-Charles, 2012, CVPR; Ceriani S, 2009, AUTON ROBOT, V27, P353, DOI 10.1007/s10514-009-9156-5; CHEN HH, 1991, IEEE T PATTERN ANAL, V13, P530, DOI 10.1109/34.87340; Cipolla R., 1999, BMVC99. Proceedings of the 10th British Machine Vision Conference, P382; Coughlan J.M., 1999, P ICCV, V2, P941, DOI DOI 10.1109/ICCV.1999.790349; Coughlan JM, 2003, NEURAL COMPUT, V15, P1063, DOI 10.1162/089976603765202668; Denis P, 2008, LECT NOTES COMPUT SC, V5303, P197, DOI 10.1007/978-3-540-88688-4_15; Flint A, 2010, PROC CVPR IEEE, P467, DOI 10.1109/CVPR.2010.5540176; Forstner W, 2010, OPTIMAL VANISHING PO; Gallagher AC, 2002, PATTERN RECOGN, V35, P1527, DOI 10.1016/S0031-3203(01)00128-5; Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135; Guowei Wan, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P1214, DOI 10.1109/CISP.2011.6100448; Hartley R., 2004, ROBOTICA; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; Kessler Christoph, 2010, 2010 IEEE/ION Position, Location and Navigation Symposium - PLANS 2010, P310, DOI 10.1109/PLANS.2010.5507247; Koseck Jana, 2002, P EUR C COMP VIS, P476; Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872; Liebowitz D, 1998, PROC CVPR IEEE, P482, DOI 10.1109/CVPR.1998.698649; LIEBOWITZ D, 2001, THESIS U OXFORD; LUTTON E, 1994, IEEE T PATTERN ANAL, V16, P430, DOI 10.1109/34.277598; Mei C, 2007, THESIS INRIA SOPHIA; Mirzaei FM, 2011, IEEE I CONF COMP VIS, P2454, DOI 10.1109/ICCV.2011.6126530; Nieto M, 2011, PATTERN RECOGN LETT, V32, P1691, DOI 10.1016/j.patrec.2011.07.018; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Pronobis A, 2009, INT J ROBOT RES, V28, P588, DOI 10.1177/0278364909103912; Rother C., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P382; Schindler G, 2004, PROC CVPR IEEE, P203; Tardif JP, 2009, IEEE I CONF COMP VIS, P1250, DOI 10.1109/ICCV.2009.5459328; Tretyak E, 2012, INT J COMPUT VISION, V97, P305, DOI 10.1007/s11263-011-0488-1; Tuytelaars T, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P67, DOI 10.1109/ICCV.1998.710702; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; Wildenauer H, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P615, DOI 10.1109/ICIAP.2007.4362845; Wildenauer H, 2012, PROC CVPR IEEE, P2831, DOI 10.1109/CVPR.2012.6248008; Zhang L., 2012, ACCV 2, P38; Zhang L., 2011, IMVIP, P8	41	24	27	2	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2016	117	2					111	130		10.1007/s11263-015-0854-5	http://dx.doi.org/10.1007/s11263-015-0854-5			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DH6UB					2022-12-18	WOS:000372926500001
J	Kovashka, A; Grauman, K				Kovashka, Adriana; Grauman, Kristen			Discovering Attribute Shades of Meaning with the Crowd	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Attribute learning and perception; Vision and language; Attribute discovery		To learn semantic attributes, existing methods typically train one discriminative model for each word in a vocabulary of nameable properties. However, this "one model per word" assumption is problematic: while a word might have a precise linguistic definition, it need not have a precise visual definition. We propose to discover shades of attribute meaning. Given an attribute name, we use crowdsourced image labels to discover the latent factors underlying how different annotators perceive the named concept. We show that structure in those latent factors helps reveal shades, that is, interpretations for the attribute shared by some group of annotators. Using these shades, we train classifiers to capture the primary (often subtle) variants of the attribute. The resulting models are both semantic and visually precise. By catering to users' interpretations, they improve attribute prediction accuracy on novel images. Shades also enable more successful attribute-based image search, by providing robust personalized models for retrieving multi-attribute query results. They are widely applicable to tasks that involve describing visual content, such as zero-shot category learning and organization of photo collections.	[Kovashka, Adriana; Grauman, Kristen] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Kovashka, A (corresponding author), Univ Texas Austin, 2317 Speedway,Stop D9500, Austin, TX 78712 USA.	adriana@cs.utexas.edu; grauman@cs.utexas.edu			ONR ATL [N00014-11-1-0105]	ONR ATL	We thank the anonymous reviewers for their helpful feedback and suggestions. This research is supported in part by ONR ATL N00014-11-1-0105.	Barnard K, 2006, LECT NOTES COMPUT SC, V4170, P238; Barnard Kobus, 2006, INFORM THEORY APPL, V2; Berg T., 2010, P EUR C COMP VIS ECC; BERG TL, 2006, P IEEE C COMP VIS PA; BRANSON S., 2010, P EUR C COMP VIS ECC; Curran W., 2012, P ACM INT C INT US I; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Deng J., 2013, P IEEE C COMP VIS PA; Donahue J., 2011, P INT C COMP VIS ICC; Duan K., 2012, P IEEE C COMP VIS PA; Endres I., 2010, P WORKSH ADV COMP VI; Everett C, 2013, LINGUISTIC RELATIVIT; Farhadi A., 2009, P C COMP VIS PATT RE; Ferrari V., 2007, P ADV NEUR INF PROC; Gomes R., 2011, P ADV NEUR INF PROC; Gong B., 2013, P ADV NEUR INF PROC; Hall David, 2008, P EMP METH NAT LANG; Hoffman J., 2012, P EUR C COMP VIS ECC; Hofmann T., 1999, P UNC ART INT UAI; Kovashka A., 2011, P INT C COMP VIS ICC; Kovashka A., 2012, P INT C COMP VIS CVP; Kovashka A., 2013, P IEEE INT C COMP VI; Kumar N., 2011, P T PATT AN MACH INT; Lampert C. H., 2009, P IEEE C COMP VIS PA; Levinson SC, 1996, ANNU REV ANTHROPOL, V25, P353, DOI 10.1146/annurev.anthro.25.1.353; Loeff N., 2006, P COLING ACL MAIN C; Lucy John A., 1992, LANGUAGE DIVERSITY T; Mahajan D., 2011, P INT C COMP VIS ICC; Maji S., 2012, P EUR C COMP VIS WOR; Parikh D., 2011, P INT C COMP VIS ICC; Parikh D., 2011, P IEEE C COMP VIS PA; Parkash A., 2012, P EUR C COMP VIS ECC; Patterson Genevieve, 2012, P IEEE C COMP VIS PA; Rastegari M., 2013, P IEEE C COMP VIS PA; Rastegari M., 2012, P EUR C COMP VIS ECC; Rohrbach M., 2012, P EUR C COMP VIS ECC; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Saenko K., 2008, P NEUR INF PROC SYST; Salakhutdinov R., 2008, P INT C MACH LEARN I; Salakhutdinov R., 2007, ADV NEUR INF PROC SY; Scheirer Walter J, 2012, P IEEE C COMP VIS PA; Sharmanska V., 2012, P EUR C COMP VIS ECC; Siddiquie Behjat, 2011, P IEEE C COMP VIS PA; Tamuz O., 2011, P INT C MACH LEARN I; Vaquero D. A., 2009, P IEEE WINT C APPL C; Wang J., 2009, P BRIT MACH VIS C BM; Wang Y., 2010, P EUR C COMP VIS ECC; Welinder P., 2010, P C NEUR INF PROC SY; Xiong C, 2014, P AAAI C ART INT AAA; Xiong L., P SIAM INT C DAT MIN, P211; Yang J., 2007, P IEEE INT C DAT MIN; Yu F. X., 2013, P IEEE C COMP VIS PA	52	24	24	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2015	114	1					56	73		10.1007/s11263-014-0798-1	http://dx.doi.org/10.1007/s11263-014-0798-1			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CO1UF		Green Submitted			2022-12-18	WOS:000358940300004
J	Johns, E; Yang, GZ				Johns, Edward; Yang, Guang-Zhong			Generative Methods for Long-Term Place Recognition in Dynamic Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene recognition; Appearance-based localization; Topological localization; Image retrieval; Simultaneous localization and mapping		This paper proposes a new framework for visual place recognition that incrementally learns models of each place and offers adaptability to dynamic elements in the scene. Traditional Bag-Of-Words (BOW) image-retrieval approaches to place recognition typically treat images in a holistic manner and are not capable of dealing with sub-scene dynamics, such as structural changes to a building fa double dagger ade or seasonal effects on foliage. However, by treating local features as observations of real-world landmarks in a scene that is observed repeatedly over a period of time, such dynamics can be modelled at a local level, and the spatio-temporal properties of each landmark can be independently updated incrementally. The method proposed models each place as a set of such landmarks and their geometric relationships. A new BOW filtering stage and geometric verification scheme are introduced to compute a similarity score between a query image and each scene model. As further training images are acquired for each place, the landmark properties are updated over time and in the long term, the model can adapt to dynamic behaviour in the scene. Results on an outdoor dataset of images captured along a 7 km path, over a period of 5 months, show an improvement in recognition performance when compared to state-of-the-art image retrieval approaches to place recognition.	[Johns, Edward; Yang, Guang-Zhong] Univ London Imperial Coll Sci Technol & Med, London, England	Imperial College London	Johns, E (corresponding author), Univ London Imperial Coll Sci Technol & Med, London, England.	ej09@imperial.ac.uk; g.z.yang@imperial.ac.uk		Johns, Edward/0000-0002-8914-8786				Agarwal Sameer, 2009, P ICCV; Arandjelovic R., 2012, P CVPR; Arnaud E., 2006, P ICPR; Bowman K, 2007, FAR E J THEO STAT, V23, P133; Cao Y., 2010, P CVPR; Chatfield K., 2011, P BMVC; Chengxiang Zhai, 2001, SIGIR Forum, P334; Chum O., 2008, P BMVC; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601; Csurka Gabriella, 2004, P ECCV INT WORKSH ST; Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961; Hartley R., 2004, ROBOTICA; Jegou H., 2012, P ECCV; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Johns E., 2013, P ICRA; Johns E., 2011, P ICAR; Johns E., 2011, P BMVC; Johns E, 2011, IEEE I CONF COMP VIS, P874, DOI 10.1109/ICCV.2011.6126328; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Li YP, 2010, LECT NOTES COMPUT SC, V6312, P791; Lik F., 2006, P ICRA; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo J., 2007, P IROS; Marszalek M., 2006, P CVPR; Mikullk A., 2010, P ECCV; Newman P, 2009, P ROB SCI SYST; Ni K., 2009, IEEE T PAMI; Nister D., 2006, P CVPR, P1222; Orabona F., 2010, P CVPR; Philbin J., 2008, P CVPR; Pronobis A., 2007, P IROS; Raguram R, 2011, INT J COMPUT VISION, V95, P213, DOI 10.1007/s11263-011-0445-z; Schindler G., 2007, P CVPR; Se S., 2001, P ICRA; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Tolias G., 2011, P ICCV; Winn J, 2005, IEEE I CONF COMP VIS, P1800; Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528; Zheng Y.-T., 2009, P CVPR	40	24	24	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2014	106	3			SI		297	314		10.1007/s11263-013-0648-6	http://dx.doi.org/10.1007/s11263-013-0648-6			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AA3DC					2022-12-18	WOS:000330972100006
J	Shen, CH; Wang, P; Paisitkriangkrai, S; van den Hengel, A				Shen, Chunhua; Wang, Peng; Paisitkriangkrai, Sakrapee; van den Hengel, Anton			Training Effective Node Classifiers for Cascade Classification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						AdaBoost; Minimax probability machine; Cascade classifier; Object detection; Human detection	PEDESTRIAN DETECTION; FACE; ALGORITHMS; SPARSE	Cascade classifiers are widely used in real-time object detection. Different from conventional classifiers that are designed for a low overall classification error rate, a classifier in each node of the cascade is required to achieve an extremely high detection rate and moderate false positive rate. Although there are a few reported methods addressing this requirement in the context of object detection, there is no principled feature selection method that explicitly takes into account this asymmetric node learning objective. We provide such an algorithm here. We show that a special case of the biased minimax probability machine has the same formulation as the linear asymmetric classifier (LAC) of Wu et al. (linear asymmetric classifier for cascade detectors, 2005). We then design a new boosting algorithm that directly optimizes the cost function of LAC. The resulting totally-corrective boosting algorithm is implemented by the column generation technique in convex optimization. Experimental results on object detection verify the effectiveness of the proposed boosting algorithm as a node classifier in cascade object detection, and show performance better than that of the current state-of-the-art.	[Shen, Chunhua; Wang, Peng; Paisitkriangkrai, Sakrapee; van den Hengel, Anton] Univ Adelaide, Sch Comp Sci, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia	University of Adelaide	Shen, CH (corresponding author), Univ Adelaide, Sch Comp Sci, Australian Ctr Visual Technol, N Terrace, Adelaide, SA 5005, Australia.	chunhua.shen@adelaide.edu.au		van den Hengel, Anton/0000-0003-3027-8364	Australian Research Council [FT120100969]	Australian Research Council(Australian Research Council)	This work was in part supported by Australian Research Council Future Fellowship FT120100969	Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108; Aldavert D., 2010, P IEEE COMP SOC C CO; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310; Boyd S, 2004, CONVEX OPTIMIZATION; Brubaker SC, 2008, INT J COMPUT VISION, V77, P65, DOI 10.1007/s11263-007-0060-1; Collins M, 2008, J MACH LEARN RES, V9, P1775; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092; Dollar P., 2012, PIOTRS IMAGE VIDEO M; Dollar P, 2008, LECT NOTES COMPUT SC, V5303, P211, DOI 10.1007/978-3-540-88688-4_16; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Dundar M., 2007, P IEEE COMP SOC C CO; Enzweiler M., 2010, P IEEE COMP SOC C CO; Ess A., 2007, P INT C COMP VIS; Gehler P.V., 2009, P INT C COMP VIS; Huang KZ, 2004, J MACH LEARN RES, V5, P1253; Lanckriet G. R. G., 2003, Journal of Machine Learning Research, V3, P555, DOI 10.1162/153244303321897726; Lazebnik S., 2006, P IEEE COMP SOC C CO; Lefakis L., 2010, ADV NEURAL INFORM PR; Li SZ, 2004, IEEE T PATTERN ANAL, V26, P1112, DOI 10.1109/TPAMI.2004.68; Lin Z, 2009, PROC CVPR IEEE, P405, DOI 10.1109/CVPRW.2009.5206858; Liu C, 2003, PROC CVPR IEEE, P587; Maji S., 2008, P IEEE COMP SOC C CO; Masnadi-Shirazi H., 2007, P 24 INT C MACH LEAR, P609; Masnadi-Shirazi H, 2011, IEEE T PATTERN ANAL, V33, P294, DOI 10.1109/TPAMI.2010.71; MOSEK, 2010, MOSEK OPT TOOLB MATL; Mu Y., 2008, P IEEE COMP SOC C CO; Munder S, 2006, IEEE T PATTERN ANAL, V28, P1863, DOI 10.1109/TPAMI.2006.217; Paisitkriangkcrai S, 2008, IEEE T CIRC SYST VID, V18, P1140, DOI 10.1109/TCSVT.2008.928213; Paisitkriangkrai S., 2009, P IEEE COMP SOC C CO; Pham M.-T., 2007, P INT C COMP VIS; Pham M.-T., 2008, P IEEE COMP SOC C CO; Pham M.T., 2007, P IEEE COMP SOC C CO; Rao, 2006, P 12 ACM SIGKDD INT, P837, DOI DOI 10.1145/1150402.1150518; Ratsch G, 2002, IEEE T PATTERN ANAL, V24, P1184, DOI 10.1109/TPAMI.2002.1033211; Saberian M. J., 2010, ADV NEURAL INFORM PR; Saberian M. J., 2012, IEEE T PATTERN ANAL; Shen CH, 2010, LECT NOTES COMPUT SC, V6312, P608, DOI 10.1007/978-3-642-15552-9_44; Shen CH, 2011, IEEE T IMAGE PROCESS, V20, P22, DOI 10.1109/TIP.2010.2055880; Shen CH, 2010, IEEE T PATTERN ANAL, V32, P2216, DOI 10.1109/TPAMI.2010.47; Shen CH, 2010, IEEE T NEURAL NETWOR, V21, P659, DOI 10.1109/TNN.2010.2040484; Shen CH, 2008, IEEE IMAGE PROC, P2764; Sochman J., 2005, P IEEE COMP SOC C CO; Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055; Tu H.-H., 2010, P INT C MACH LEARN; Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Viola P, 2002, ADV NEUR IN, V14, P1311; Viola P., 2005, ADV NEURAL INFORM PR, P1417; Walk S., 2010, P IEEE COMP SOC C CO; Wang P, 2012, IEEE T NEUR NET LEAR, V23, P33, DOI 10.1109/TNNLS.2011.2178324; Wang W., 2010, P INT C IM PROC; Wang X, 2007, ADV INTEL SYS RES, DOI 10.2991/iske.2007.208; Wojek C., 2009, P IEEE COMP SOC C CO; Wu B., 2008, P IEEE COMP SOC C CO; Wu J., 2005, P INT C MACH LEARN, P988; Wu J., 2003, P ADV NEUR INF PROC; Wu JX, 2008, IEEE T PATTERN ANAL, V30, P369, DOI 10.1109/TPAMI.2007.1181; Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224; Xiao R, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P709; Xiao R., 2007, P INT C COMP VIS; Yu Y.-L., 2009, ADV NEURAL INF PROCE, V22, P2232; Zheng Y., 2010, P AS C COMP VIS; Zhu Qiang, 2006, CVPR, DOI DOI 10.1109/CVPR.2006.119	66	24	24	0	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2013	103	3					326	347		10.1007/s11263-013-0608-1	http://dx.doi.org/10.1007/s11263-013-0608-1			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	155WJ		Green Submitted			2022-12-18	WOS:000319778800003
J	Andreopoulos, A; Tsotsos, JK				Andreopoulos, Alexander; Tsotsos, John K.			A Computational Learning Theory of Active Object Recognition Under Uncertainty	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object recognition; Visual search; Active vision; Attention; Computational complexity of vision	VISUAL-ATTENTION; MODEL; COMPLEXITY; SALIENCY; SEARCH; TASK	We present some theoretical results related to the problem of actively searching a 3D scene to determine the positions of one or more pre-specified objects. We investigate the effects that input noise, occlusion, and the VC-dimensions of the related representation classes have in terms of localizing all objects present in the search region, under finite computational resources and a search cost constraint. We present a number of bounds relating the noise-rate of low level feature detection to the VC-dimension of an object representable by an architecture satisfying the given computational constraints. We prove that under certain conditions, the corresponding classes of object localization and recognition problems are efficiently learnable in the presence of noise and under a purposive learning strategy, as there exists a polynomial upper bound on the minimum number of examples necessary to correctly localize the targets under the given models of uncertainty. We also use these arguments to show that passive approaches to the same problem do not necessarily guarantee that the problem is efficiently learnable. Under this formulation, we prove the existence of a number of emergent relations between the object detection noise-rate, the scene representation length, the object class complexity, and the representation class complexity, which demonstrate that selective attention is not only necessary due to computational complexity constraints, but it is also necessary as a noise-suppression mechanism and as a mechanism for efficient object class learning. These results concretely demonstrate the advantages of active, purposive and attentive approaches for solving complex vision problems.	[Andreopoulos, Alexander] IBM Res Almaden, San Jose, CA 95120 USA; [Tsotsos, John K.] York Univ, Dept Comp Sci & Engn, Ctr Vis Res, Toronto, ON M3J 2R7, Canada	International Business Machines (IBM); York University - Canada	Andreopoulos, A (corresponding author), IBM Res Almaden, 650 Harry Rd, San Jose, CA 95120 USA.	aandreo@us.ibm.com; tsotsos@cse.yorku.ca	Tsotsos, John/N-1131-2019	Tsotsos, John/0000-0002-8621-9147				ALOIMONOS J, 1987, INT J COMPUT VISION, V1, P333; Andreopoulos A., 2008, P 5 CAN C COMP ROB V; Andreopoulos A., 2009, P INT C COMP VIS; Andreopoulos A, 2012, IEEE T PATTERN ANAL, V34, P110, DOI 10.1109/TPAMI.2011.91; Andreopoulos A, 2011, IEEE T ROBOT, V27, P47, DOI 10.1109/TRO.2010.2090058; Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; [Anonymous], 1994, INTRO COMPUTATIONAL; Bajcsy R., 1985, IEEE WORKSH COMP VIS; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; Barrow H. G., 1971, Machine Intelligence Volume 6, P377; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 1996, J COMPUT SYST SCI, V52, P434, DOI 10.1006/jcss.1996.0033; Baum EB, 1989, NEURAL COMPUT, V1, P151, DOI 10.1162/neco.1989.1.1.151; Ben-David S, 1998, MACH LEARN, V32, P207, DOI 10.1023/A:1007447530834; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Boshra M, 2000, IEEE T PATTERN ANAL, V22, P956, DOI 10.1109/34.877519; Brentano F., 1874, PSYCHOL EMPIRICAL ST, DOI DOI 10.4324/9781315747446; Broadbent Donald Eric, 2013, PERCEPTION COMMUNICA; Brooks R., 1979, P 6 INT JOINT C ART; Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5; Callari FG, 2001, INT J COMPUT VISION, V43, P189, DOI 10.1023/A:1011135513777; Dickinson SJ, 1997, COMPUT VIS IMAGE UND, V67, P239, DOI 10.1006/cviu.1997.0532; Dickinson SJ, 1999, IEEE T PATTERN ANAL, V21, P673, DOI 10.1109/34.784283; Ekvall S., 2006, P INT ROB SYST; Findlay J.M., 2003, ACTIVE VISION PSYCHO; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; GARVEY TD, 1976, 117 SRI INT; Gerstner W., 2002, SPIKING NEURON MODEL; Gibson J., 1979, ECOLOGICAL APPROACH; Giefing G., 1992, INT C PATT REC; GRIMSON WEL, 1991, IEEE T PATTERN ANAL, V13, P920, DOI 10.1109/34.93810; GROSSBERG S, 1973, STUD APPL MATH, V52, P213; Hinton G., 1978, THESIS U EDINBURGH; IKEUCHI K, 1988, P IEEE, V76, P1016, DOI 10.1109/5.5972; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Kearns M., 1993, P 25 ACM S THEOR COM; Laporte C, 2006, INT J COMPUT VISION, V68, P267, DOI 10.1007/s11263-005-4436-9; Lindenbaum M, 1997, IEEE T PATTERN ANAL, V19, P1251, DOI 10.1109/34.632984; Marr D., 1982, VISION COMPUTATIONAL; MAVER J, 1993, IEEE T PATTERN ANAL, V15, P417, DOI 10.1109/34.211463; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Meger D, 2008, ROBOT AUTON SYST, V56, P503, DOI 10.1016/j.robot.2008.03.008; Minsky M., 1969, PERCEPTRONS; Najemnik J, 2005, NATURE, V434, P387, DOI 10.1038/nature03390; Navalpakkam V, 2005, VISION RES, V45, P205, DOI 10.1016/j.visres.2004.07.042; NEVATIA R, 1977, ARTIF INTELL, V8, P77, DOI 10.1016/0004-3702(77)90006-6; RIMEY RD, 1994, INT J COMPUT VISION, V12, P173, DOI 10.1007/BF01421202; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Roy SD, 2000, IEEE T SYST MAN CY A, V30, P67, DOI 10.1109/3468.823482; Saidi F., 2007, P INT ROB SYST; Schiele B., 1998, P INT C COMP VIS; Seeger M., 2002, ADV NEURAL INFORM PR; Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0; Tsotsos J. K., 2011, COMPUTATIONAL PERSPE, DOI DOI 10.7551/MITPRESS/9780262015417.001.0001; TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9; Tsotsos JK, 2005, COMPUT VIS IMAGE UND, V100, P3, DOI 10.1016/j.cviu.2004.10.011; TSOTSOS JK, 1990, BEHAV BRAIN SCI, V13, P423, DOI 10.1017/S0140525X00079577; TSOTSOS JK, 1992, INT J COMPUT VISION, V7, P127, DOI 10.1007/BF00128132; Valiant L. G., 1985, P 9 INT JOINT C ART; VALIANT LG, 1984, PHILOS T R SOC A, V312, P441, DOI 10.1098/rsta.1984.0069; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; VERGHESE P, 1992, VISION RES, V32, P983, DOI 10.1016/0042-6989(92)90040-P; WIXSON LE, 1994, INT J COMPUT VISION, V12, P209, DOI 10.1007/BF01421203; Ye YM, 2001, COMPUT INTELL, V17, P605, DOI 10.1111/0824-7935.00166; Ye YM, 1999, COMPUT VIS IMAGE UND, V73, P145, DOI 10.1006/cviu.1998.0736	66	24	24	0	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	1					95	142		10.1007/s11263-012-0551-6	http://dx.doi.org/10.1007/s11263-012-0551-6			48	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	080XV					2022-12-18	WOS:000314278500005
J	Ghosh, A; Heidrich, W; Achutha, S; O'Toole, M				Ghosh, Abhijeet; Heidrich, Wolfgang; Achutha, Shruthi; O'Toole, Matthew			A Basis Illumination Approach to BRDF Measurement	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Reflectance; Computational illumination; Object scanning and acquisition; Optics; Compressive sensing	REFLECTANCE	Realistic descriptions of surface reflectance have long been a topic of interest in both computer vision and computer graphics research. In this paper, we describe a novel high speed approach for the acquisition of bidirectional reflectance distribution functions (BRDFs). We develop a new theory for directly measuring BRDFs in a basis representation by projecting incident light as a sequence of basis functions from a spherical zone of directions. We derive an orthonormal basis over spherical zones that is ideally suited for this task. BRDF values outside the zonal directions are extrapolated by re-projecting the zonal measurements into a spherical harmonics basis, or by fitting analytical reflection models to the data. For specular materials, we experiment with alternative basis acquisition approaches such as compressive sensing with a random subset of the higher order orthonormal zonal basis functions, as well as measuring the response to basis defined by an analytical model as a way of optically fitting the BRDF to such a representation. We verify this approach with a compact optical setup that requires no moving parts and only a small number of image measurements. Using this approach, a BRDF can be measured in just a few minutes.	[Ghosh, Abhijeet] USC Inst Creat Technol, Marina Del Rey, CA 90292 USA; [Ghosh, Abhijeet; Heidrich, Wolfgang; Achutha, Shruthi; O'Toole, Matthew] Univ British Columbia, Vancouver, BC V6T 1Z4, Canada	University of British Columbia	Ghosh, A (corresponding author), USC Inst Creat Technol, Marina Del Rey, CA 90292 USA.	ghosh@cs.ubc.ca; heidrich@cs.ubc.ca						Ashikhmin M., 2000, Journal of Graphics Tools, V5, P25, DOI 10.1080/10867651.2000.10487522; Ashikhmin M, 2000, COMP GRAPH, P65, DOI 10.1145/344779.344814; ASHIKHMIN M, 2006, DISTRIBUTION BASED B; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; CANDES E, 2005, P IS T SPIES 17 ANN; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Cook R., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293; Dana KJ, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P460, DOI 10.1109/ICCV.2001.937661; Debevec P., 1997, P ACM SIGGRAPH 1997, DOI [DOI 10.1145/258734.258884, 10.1145/258734.258884]; Gardner A, 2003, ACM T GRAPHIC, V22, P749, DOI 10.1145/882262.882342; Gautron P., 2004, P ESGR 04 PAP EUR C, P321; GHOSH A, 2007, P IEEE INT C COMP VI; Goesele M, 2003, ACM T GRAPHIC, V22, P621, DOI 10.1145/882262.882316; GORSKI KM, 1994, ASTROPHYS J, V430, pL85, DOI 10.1086/187444; Han JY, 2003, ACM T GRAPHIC, V22, P741, DOI 10.1145/882262.882341; HE XD, 1991, COMP GRAPH, V25, P175, DOI 10.1145/127719.122738; HE XD, 1992, COMP GRAPH, V26, P253, DOI 10.1145/142920.134073; KOENDERINK J, 1996, EUR C COMP VIS, V2, P28; Kuthirummal S, 2006, ACM T GRAPHIC, V25, P916, DOI 10.1145/1141911.1141975; LAFORTUNE EPF, 1997, P SIGGRAPH, P117; Lalonde P, 1997, IEEE T VIS COMPUT GR, V3, P329, DOI 10.1109/2945.646236; Lensch HPA, 2001, SPRING EUROGRAP, P103; Ma W., 2007, P EUR S REND; Malzbender T, 2001, COMP GRAPH, P519, DOI 10.1145/383259.383320; Marschner SR, 2000, APPL OPTICS, V39, P2592, DOI 10.1364/AO.39.002592; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; MUKAIHAWA Y, 2007, P C COMP VIS PATT RE, P1; Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280; NGAN A, 2005, P EUR S REND, P117; Nicodemus FE, 1977, NBS MONOGRAPH, V160; *NIST, 2003, NIST REF REFL STARR; Peers Pieter, 2005, RENDERING TECHNIQUES, P173; PHARR M, 2004, PHYS BASED REDERING; Press W.H., 1992, NUMERICAL RECIPES AR, V2nd; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Ramantoorthi R, 2002, ACM T GRAPHIC, V21, P517, DOI 10.1145/566570.566611; Sato I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P800; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; WESTIN SH, 1992, COMP GRAPH, V26, P255, DOI 10.1145/142920.134075; [No title captured]	41	24	28	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2010	90	2					183	197		10.1007/s11263-008-0151-7	http://dx.doi.org/10.1007/s11263-008-0151-7			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	640XQ					2022-12-18	WOS:000281087900004
J	Ferreira, A; Marini, S; Attene, M; Fonseca, MJ; Spagnuolo, M; Jorge, JA; Falcidieno, B				Ferreira, Alfredo; Marini, Simone; Attene, Marco; Fonseca, Manuel J.; Spagnuolo, Michela; Jorge, Joaquim A.; Falcidieno, Bianca			Thesaurus-based 3D Object Retrieval with Part-in-Whole Matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D shape retrieval; Part-in-whole matching; Thesaurus; Segmentation	SEARCH ENGINE	Research in content-based 3D retrieval has already started, and several approaches have been proposed which use in different manner a similarity assessment to match the shape of the query against the shape of the objects in the database. However, the success of these solutions are far from the success obtained by their textual counterparts. A major drawback of most existing 3D retrieval solutions is their inability to support partial queries, that is, a query which does not need to be formulated by specifying a whole query shape, but just a part of it, for example a detail of its overall shape, just like documents are retrieved by specifying words and not whole texts. Recently, researchers have focused their investigation on 3D retrieval which is solved by partial shape matching. However, at the extent of our knowledge, there is still no 3D search engine that provides an indexing of the 3D models based on all the interesting subparts of the models. In this paper we present a novel approach to 3D shape retrieval that uses a collection-aware shape decomposition combined with a shape thesaurus and inverted indexes to describe and retrieve 3D models using part-in-whole matching. The proposed method clusters similar segments obtained trough a multilevel decomposition of models, constructing from such partition the shape thesaurus. Then, to retrieve a model containing a sub-part similar to a given query, instead of looking on a large set of subparts or executing partial matching between the query and all models in the collection, we just perform a fast global matching between the query and the few entries in the thesaurus. With this technique we overcame the time complexity problems associated with partial queries in large collections.	[Ferreira, Alfredo; Fonseca, Manuel J.; Jorge, Joaquim A.] Univ Tecn Lisboa, Dept Comp Sci & Engn, INESC ID IST, Lisbon, Portugal; [Marini, Simone; Attene, Marco; Spagnuolo, Michela; Falcidieno, Bianca] CNR, Inst Matemat Applicata & Tecnol Informat, Genoa, Italy	INESC-ID; Universidade de Lisboa; Consiglio Nazionale delle Ricerche (CNR)	Ferreira, A (corresponding author), Univ Tecn Lisboa, Dept Comp Sci & Engn, INESC ID IST, Lisbon, Portugal.	alfredo.ferreira@inesc-id.pt; simone@ge.imati.cnr.it; attene@ge.imati.cnr.it; mjf@inesc-id.pt; michi@ge.imati.cnr.it; jaj@inesc-id.pt; bianca@ge.imati.cnr.it	Spagnuolo, Michela/F-5068-2013; Marini, Simone/C-3872-2012; Spagnuolo, Michela/ABA-1927-2021; Ferreira, Alfredo/L-6407-2015; Fonseca, Manuel J./D-5120-2011; Jorge, Joaquim/C-5596-2008; Marini, Simone/AAA-3513-2022	Spagnuolo, Michela/0000-0002-5682-6990; Marini, Simone/0000-0003-0665-7815; Spagnuolo, Michela/0000-0002-5682-6990; Ferreira, Alfredo/0000-0001-9278-142X; Fonseca, Manuel J./0000-0002-3559-828X; Jorge, Joaquim/0000-0001-5441-4637; Marini, Simone/0000-0003-0665-7815				Agathos A, 2007, COMPUT AIDED DES APP, V4, P827, DOI DOI 10.1080/16864360.2007.10738515; Anderberg MR, 1973, CLUSTER ANAL APPL; Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359; ANSARY TF, 2007, P ACM INT C IM VID R; Assfalg J, 2006, MULTIMED TOOLS APPL, V31, P29, DOI 10.1007/s11042-006-0034-2; Attene M, 2006, COMPUT GRAPH-UK, V30, P323, DOI 10.1016/j.cag.2006.02.007; Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x; Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Bespalov D., 2003, P 8 ACM S SOL MOD AP, P208, DOI DOI 10.1145/781606.781638; Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003; Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X; Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893; Campbell RJ, 2001, COMPUT VIS IMAGE UND, V81, P166, DOI 10.1006/cviu.2000.0889; Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669; Cormack G. V., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P282, DOI 10.1145/290941.291009; Cornea ND, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P366, DOI 10.1109/SMI.2005.1; Crouch C. J., 1988, 11th International Conference on Research and Development in Information Retrieval, P309, DOI 10.1145/62437.62467; Curran J.R., 2002, P ACL 02 WORKSHOP UN, V9, P59, DOI DOI 10.3115/1118627.1118635; Fonseca MJ, 2003, EIGHTH INTERNATIONAL CONFERENCE ON DATABASE SYSTEMS FOR ADVANCED APPLICATIONS, PROCEEDINGS, P267, DOI 10.1109/DASFAA.2003.1192391; Funkhouser T, 2005, COMMUN ACM, V48, P58, DOI 10.1145/1064830.1064859; Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775; Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279; Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507; Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007; Johnson AE, 1997, PROC CVPR IEEE, P684, DOI 10.1109/CVPR.1997.609400; JOYCE T, 1997, THESAURUS APPROACH I, P15; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; KAZHDAN M, 2004, THESIS PRINCETON U; Kazhdan M., 2003, P S GEOM PROCESS, P156; Kogan J, 2007, INTRODUCTION TO CLUSTERING LARGE AND HIGH-DIMENSIONAL DATA, P1, DOI 10.2277/ 0521617936; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Lou K, 2004, PROC INT CONF DATA, P754, DOI 10.1109/ICDE.2004.1320043; LV Q, 2006, SIGOPS OPERATING SYS, V40, P317; Milne D. N., 2007, P 16 ACM C C INF KNO, P445, DOI DOI 10.1145/1321440.1321504; PAQUET E, 1997, NEFERTITI QUERY CONT, P345; Ruge G., 1997, Foundations of computer science. Potential - theory - cognition, P499, DOI 10.1007/BFb0052119; Ruiz-Correa S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1126; Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981; Silverstein C., 1999, SIGIR Forum, V33, P6, DOI 10.1145/331403.331405; Suzuki MT, 2003, IEEE SYS MAN CYBERN, P3846; Suzuki MT, 2000, IEEE SYS MAN CYBERN, P2946, DOI 10.1109/ICSMC.2000.884448; SUZUKI MT, 2005, P 9 IASTED INT C SOF, P389; SUZUKI MT, 2005, ACM SIGGRAPH 2005 PO, P128; Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x; [No title captured]	46	24	26	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2010	89	2-3			SI		327	347		10.1007/s11263-009-0257-6	http://dx.doi.org/10.1007/s11263-009-0257-6			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	594OS					2022-12-18	WOS:000277547600012
J	Liu, Y; Wang, XL; Wang, HY; Zha, HB; Qin, H				Liu, Yi; Wang, Xu-Lei; Wang, Hua-Yan; Zha, Hongbin; Qin, Hong			Learning Robust Similarity Measures for 3D Partial Shape Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Partial similarity measure; 3D shape retrieval; Earth mover's distance; Adaboost	EARTH-MOVERS-DISTANCE; FEATURES; KERNEL; SETS	In this paper, we propose a novel approach to learning robust ground distance functions of the Earth Mover's distance to make it appropriate for quantifying the partial similarity between two feature-sets. First, we define the ground distance as a monotonic transformation of commonly used feature-to-feature base distance (or similarity) measures, so that in computing the Earth Mover's distance, the algorithm could better turn its focus on the feature pairs that are correctly matched, while being less affected by irrelevant ones. As a result, the proposed method is especially suited for 3D partial shape retrieval where occlusion and clutter are serious problems. We prove that when the transformation satisfies certain conditions, the metric property of the base distance is sufficient to guarantee the ground distance is a metric (and so is the Earth Mover's distance), which makes fast shape retrieval on large databases technically possible. Second, we propose a discriminative learning framework to optimize the transformation function based on the real Adaboost algorithm. The optimization is performed in the space of the piecewise constant approximations of the transformation without making any parametric assumption. Finally, extensive experiments on 3D partial shape retrieval convincingly demonstrate the effectiveness of the proposed techniques.	[Liu, Yi; Wang, Xu-Lei; Zha, Hongbin] Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing 100871, Peoples R China; [Liu, Yi] Chinese Acad Sci, Inst Genet & Dev Biol, Ctr Mol Syst Biol, Key Lab Mol Dev Biol, Beijing, Peoples R China; [Wang, Hua-Yan] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China; [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA	Peking University; Chinese Academy of Sciences; Institute of Genetics & Developmental Biology, CAS; Hong Kong University of Science & Technology; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Liu, Y (corresponding author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing 100871, Peoples R China.	liuyi@cis.pku.edu.cn; zha@cis.pku.edu.cn			NKBRP [2004CB318005]; NSFC [60803067]; NSF [IIS0949467, IIS0710819, IIS0830183]	NKBRP(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC)); NSF(National Science Foundation (NSF))	We sincerely thank for the thoughtful comments from the anonymous reviewers. Yi Liu, Xu-Lei Wang and Prof. Hongbin Zha are partially supported by NKBRP grant 2004CB318005 and NSFC grant 60803067. Professor Hong Qin (Stony Brook University)'s current research in this paper is partially supported by NSF grants: IIS0949467, IIS0710819, and IIS0830183.	Athitsos V, 2004, PROC CVPR IEEE, P268; Athitsos V., 2005, P ACM SIGMOD INT C M, P706; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Berretti S, 2000, IEEE T MULTIMEDIA, V2, P225, DOI 10.1109/6046.890058; Bespalov D., 2003, P 8 ACM S SOL MOD AP, P208, DOI DOI 10.1145/781606.781638; Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003; Bustos B, 2006, INT J DIGIT LIBRARIE, V6, P39, DOI 10.1007/s00799-005-0122-3; Cornea ND, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P366, DOI 10.1109/SMI.2005.1; DATAR KM, 2004, P ACM S COMP GEOM, P253; Davis J. V., 2007, P 24 INT C MACH LEAR; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279; FUNKHOUSER T, 2006, P 4 EUR S GEOM PROC, P131; Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; Grauman K, 2005, PROC CVPR IEEE, P627; Grauman K., 2007, NIPS, V19, P505, DOI DOI 10.7551/MITPRESS/7503.003.0068; Grauman K, 2007, J MACH LEARN RES, V8, P725; Hastie T, 2009, ELEMENTS STAT LEARNI; Indyk P., 2003, 3 INT WORKSH STAT CO; Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kim HK, 2000, SIGNAL PROCESS-IMAGE, V16, P87, DOI 10.1016/S0923-5965(00)00018-7; LING H, 2006, P IEEE C COMP VIS PA, V1, P246, DOI DOI 10.1109/CVPR.2006.99; Ling H, 2007, IEEE T PATTERN ANAL, V29, P840, DOI 10.1109/TPAMI.2007.1058; Liu Y., 2006, COMP VIS PATT REC 20, V2, P2025, DOI DOI 10.1109/CVPR.2006.278; LIU Y, 2008, P 23 AAAI C ART INT, P652; LIU Y, 2006, P INT C SHAP MOD APP, V16; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MARINI S, 2007, SHREC2007 3D SHAPE R; Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561; NISTER D, 2006, P IEEE C COMP VIS PA, V2, P2161, DOI DOI 10.1109/CVPR.2006.264; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; ROTE G, 1991, INFORM PROCESS LETT, V38, P123, DOI 10.1016/0020-0190(91)90233-8; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Shan Y, 2006, IEEE T PATTERN ANAL, V28, P568, DOI 10.1109/TPAMI.2006.83; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0; Tangelder JWH, 2003, INT J IMAGE GRAPH, V3, P209, DOI DOI 10.1142/S021946780300097X; Vranic DV, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P757; WANG XL, 2009, LEARNING ROBUST CROS; Weston J, 2003, ANN I STAT MATH, V55, P391, DOI 10.1023/A:1026338322729; Winn J, 2005, IEEE I CONF COMP VIS, P1800; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4	48	24	24	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2010	89	2-3			SI		408	431		10.1007/s11263-009-0298-x	http://dx.doi.org/10.1007/s11263-009-0298-x			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	594OS					2022-12-18	WOS:000277547600017
J	Ben Ayed, I; Li, S; Ross, I				Ben Ayed, Ismail; Li, Shuo; Ross, Ian			A Statistical Overlap Prior for Variational Image Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image segmentation; Distribution metrics; Variational methods; Active contours; Level sets; Segmentation priors	LEVEL SET METHODS; ACTIVE CONTOURS; REGION COMPETITION; TRACKING; SHAPE; MUMFORD; DRIVEN; MOTION; MODEL	This study investigates variational image segmentation with an original data term, referred to as statistical overlap prior, which measures the conformity of overlap between the nonparametric distributions of image data within the segmentation regions to a learned statistical description. This leads to image segmentation and distribution tracking algorithms that relax the assumption of minimal overlap and, as such, are more widely applicable than existing algorithms. We propose to minimize active curve functionals containing the proposed overlap prior, compute the corresponding Euler-Lagrange curve evolution equations, and give an interpretation of how the overlap prior controls such evolution. We model the overlap, measured via the Bhattacharyya coefficient, with a Gaussian prior whose parameters are estimated from a set of relevant training images. Quantitative and comparative performance evaluations of the proposed algorithms over several experiments demonstrate the positive effects of the overlap prior in regard to segmentation accuracy and convergence speed.	[Ben Ayed, Ismail; Li, Shuo] GE Healthcare, London, ON N6A 4A2, Canada; [Ross, Ian] Univ Hosp, London Hlth Sci Ctr, London, ON N6A 5A5, Canada	General Electric; London Health Sciences Centre; Western University (University of Western Ontario)	Ben Ayed, I (corresponding author), GE Healthcare, 268 Grosvenor,E5-137, London, ON N6A 4A2, Canada.	ismail.benayed@ge.com; shuo.li@ge.com	Li, Shuo/F-9736-2017; Li, Shuo/GXV-6545-2022; Li, Shuo/N-5364-2019	Li, Shuo/0000-0002-5184-3230; Li, Shuo/0000-0002-5184-3230	Natural Sciences and Engineering Research Council of Canada (NSERC)	Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC))	This study is supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC), under the Industrial Research Fellowship granted to Ismail Ben Ayed.	[Anonymous], 2008, CVPR; Aubert G, 2003, SIAM J APPL MATH, V63, P2128, DOI 10.1137/S0036139902408928; AUBERT G, 2006, MATH PROBLEMS AGE PR; AWATE SP, 2006, ECCV, V2, P494; Ben Ayed I, 2005, IEEE T PATTERN ANAL, V27, P793, DOI 10.1109/TPAMI.2005.106; Ben Ayed I, 2008, IEEE T IMAGE PROCESS, V17, P2301, DOI 10.1109/TIP.2008.2006425; Ben Ayed I, 2006, IEEE T IMAGE PROCESS, V15, P3431, DOI 10.1109/TIP.2006.881961; Ben Ayed I, 2006, IEEE T PATTERN ANAL, V28, P1493, DOI 10.1109/TPAMI.2006.191; BENAYED I, 2008, MICCAI; BENAYED I, 2008, CVPR; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; CHANG H, 2007, IEEE INT C IM PROC O; Cremers D, 2005, INT J COMPUT VISION, V62, P249, DOI 10.1007/s11263-005-4882-4; DEHEUVELS P, 1977, REV STATISTIQUE APPL, V25, P5; Freedman D, 2004, IEEE T IMAGE PROCESS, V13, P518, DOI 10.1109/TIP.2003.821445; Gao S, 2005, IEEE T IMAGE PROCESS, V14, P1537, DOI 10.1109/TIP.2005.852200; Georgiou T, 2007, LINEAR ALGEBRA APPL, V425, P663, DOI 10.1016/j.laa.2007.03.009; Holtzman-Gazit M, 2006, IEEE T IMAGE PROCESS, V15, P354, DOI [10.1109/TIP.2005.860624, 10.1109/tip.2005.860624]; Huang XL, 2008, IEEE T PATTERN ANAL, V30, P1444, DOI 10.1109/TPAMI.2007.70795; Idris F, 1997, J VIS COMMUN IMAGE R, V8, P146, DOI 10.1006/jvci.1997.0355; Jehan-Besson S, 2003, INT J COMPUT VISION, V53, P45, DOI 10.1023/A:1023031708305; Jones MC, 1996, J AM STAT ASSOC, V91, P401, DOI 10.2307/2291420; Kadir T, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1267; Kim JM, 2005, IEEE T IMAGE PROCESS, V14, P1486, DOI 10.1109/TIP.2005.854442; Lehmann Erich L, 2006, TESTING STAT HYPOTHE; Ling H., 2008, CVPR; MALCOLM J, 2007, CVPR; Mansouri AR, 2006, COMPUT VIS IMAGE UND, V101, P137, DOI 10.1016/j.cviu.2005.07.008; Martin P, 2004, IEEE T PATTERN ANAL, V26, P799, DOI 10.1109/TPAMI.2004.11; Michailovich O, 2007, IEEE T IMAGE PROCESS, V16, P2787, DOI 10.1109/TIP.2007.908073; Mitiche A, 2006, IEEE T PATTERN ANAL, V28, P1818, DOI 10.1109/TPAMI.2006.232; Morel J.-M., 1995, VARIATIONAL METHODS; Mory B, 2007, ICCV; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758; Paragios N, 2002, INT J COMPUT VISION, V46, P223, DOI 10.1023/A:1014080923068; Riklin-Raviv T, 2008, INT J COMPUT VISION, V79, P231, DOI 10.1007/s11263-007-0115-3; Rother C., 2004, SIGGRAPH; Rousson M, 2005, LECT NOTES COMPUT SC, V3750, P757, DOI 10.1007/11566489_93; Rousson M, 2008, INT J COMPUT VISION, V76, P231, DOI 10.1007/s11263-007-0054-z; Samson C, 2000, INT J COMPUT VISION, V40, P187, DOI 10.1023/A:1008183109594; SEGONNE F, 2008, INT J COMPUTER VISIO, V79; Sethian J. A., 1999, LEVEL SET METHODS FA; Tai XC, 2007, INT J COMPUT VISION, V73, P61, DOI 10.1007/s11263-006-9140-x; Vazquez C, 2006, IEEE T PATTERN ANAL, V28, P782, DOI 10.1109/TPAMI.2006.97; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Zhang T, 2005, IEEE T PATTERN ANAL, V27, P282, DOI 10.1109/TPAMI.2005.31; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343	49	24	24	1	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2009	85	1					115	132		10.1007/s11263-009-0249-6	http://dx.doi.org/10.1007/s11263-009-0249-6			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	470UY					2022-12-18	WOS:000268008000006
J	Harandi, MT; Nili Ahmadabadi, M; Araabi, BN				Harandi, Mehrtash T.; Nili Ahmadabadi, Majid; Araabi, Babak N.			Optimal Local Basis: A Reinforcement Learning Approach for Face Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face recognition; Feature selection; Reinforcement learning	DISCRIMINANT-ANALYSIS; PCA; SELECTION; SUBSPACE; FISHER; LDA	This paper presents a novel learning approach for Face Recognition by introducing Optimal Local Basis. Optimal local bases are a set of basis derived by reinforcement learning to represent the face space locally. The reinforcement signal is designed to be correlated to the recognition accuracy. The optimal local bases are derived then by finding the most discriminant features for different parts of the face space, which represents either different individuals or different expressions, orientations, poses, illuminations, and other variants of the same individual. Therefore, unlike most of the existing approaches that solve the recognition problem by using a single basis for all individuals, our proposed method benefits from local information by incorporating different bases for its decision. We also introduce a novel classification scheme that uses reinforcement signal to build a similarity measure in a non-metric space. Experiments on AR, PIE, ORL and YALE databases indicate that the proposed method facilitates robust face recognition under pose, illumination and expression variations. The performance of our method is compared with that of Eigenface, Fisherface, Subclass Discriminant Analysis, and Random Subspace LDA methods as well.	[Harandi, Mehrtash T.; Nili Ahmadabadi, Majid; Araabi, Babak N.] Univ Tehran, Sch Elect & Comp Engn, Control & Intelligent Proc Ctr Excellence, Tehran, Iran; [Harandi, Mehrtash T.; Nili Ahmadabadi, Majid; Araabi, Babak N.] Inst Studies Theoret Phys & Math, Sch Cognit Sci, Tehran, Iran	University of Tehran	Harandi, MT (corresponding author), Univ Tehran, Sch Elect & Comp Engn, Control & Intelligent Proc Ctr Excellence, Tehran, Iran.	mharandi@ece.ut.ac.ir; mnili@ut.ac.ir; araabi@ut.ac.ir	Nadjar Araabi, Babak/C-5069-2017; Harandi, Mehrtash/D-6586-2018	Nadjar Araabi, Babak/0000-0002-5283-263X; Harandi, Mehrtash/0000-0002-6937-6300				Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Bicego M., 2006, IEEE INT WORKSH BIOM; Cai D, 2006, IEEE T IMAGE PROCESS, V15, P3608, DOI 10.1109/TIP.2006.881945; CARBON CC, 2003, THESIS FREE U BERLIN; EDELMAN S, 1990, MECH PERCEPTUAL LEAR, P353; Ekenel HK, 2004, PATTERN RECOGN LETT, V25, P1377, DOI 10.1016/j.patrec.2004.05.013; FENG GC, 2002, ELECT IMAGING, V9, P226; Harandi MT, 2004, IEEE IMAGE PROC, P2709; Harandi MT, 2004, CONF CYBERN INTELL S, P1368; He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55; Kim KI, 2002, IEEE SIGNAL PROC LET, V9, P40, DOI 10.1109/97.991133; Kim TK, 2005, IEEE T PATTERN ANAL, V27, P318, DOI 10.1109/TPAMI.2005.58; Liu CJ, 2004, IEEE T PATTERN ANAL, V26, P572, DOI 10.1109/TPAMI.2004.1273927; Liu CJ, 2003, IEEE T NEURAL NETWOR, V14, P919, DOI 10.1109/TNN.2003.813829; Liu CJ, 2000, IEEE T PATTERN ANAL, V22, P570, DOI 10.1109/34.862196; Liu H, 1998, FEATURE EXTRACTION C; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P117, DOI 10.1109/TNN.2002.806629; MA JL, 2007, IEEE INT C AC SPEECH, P593; Martinez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974; O'Toole AJ, 2001, SCI PSYCH S, P1; *ORL, ORL DAT; PENTLAND A, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P84, DOI 10.1109/CVPR.1994.323814; ROSEIS ST, 2000, SCIENCE, V290, P2323; Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Wang XG, 2006, INT J COMPUT VISION, V70, P91, DOI 10.1007/s11263-006-8098-z; *YAL U FAC IM DAT, PUBL AV NONC US; Yamaguchi O, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P318, DOI 10.1109/AFGR.1998.670968; Yang J, 2005, IEEE T PATTERN ANAL, V27, P230, DOI 10.1109/TPAMI.2005.33; Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zheng WS, 2005, IEEE T SYST MAN CY B, V35, P1065, DOI 10.1109/TSMCB.2005.850175; Zhu ML, 2006, IEEE T PATTERN ANAL, V28, P1274, DOI 10.1109/TPAMI.2006.172	37	24	29	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2009	81	2					191	204		10.1007/s11263-008-0161-5	http://dx.doi.org/10.1007/s11263-008-0161-5			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	393VI					2022-12-18	WOS:000262401600007
J	Lin, ZC; He, JF; Tang, XO; Tang, CK				Lin, Zhouchen; He, Junfeng; Tang, Xiaoou; Tang, Chi-Keung			Limits of learning-based superresolution algorithms	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						superresolution; learning-based; limits; resolution	IMAGE-RECONSTRUCTION; HALLUCINATING FACES; RESOLUTION	Learning-based superresolution (SR) is a popular SR technique that uses application dependent priors to infer the missing details in low resolution images (LRIs). However, their performance still deteriorates quickly when the magnification factor is only moderately large. This leads us to an important problem: "Do limits of learning-based SR algorithms exist?" This paper is the first attempt to shed some light on this problem when the SR algorithms are designed for general natural images. We first define an expected risk for the SR algorithms that is based on the root mean squared error between the superresolved images and the ground truth images. Then utilizing the statistics of general natural images, we derive a closed form estimate of the lower bound of the expected risk. The lower bound only involves the covariance matrix and the mean vector of the high resolution images (HRIs) and hence can be computed by sampling real images. We also investigate the sufficient number of samples to guarantee an accurate estimate of the lower bound. By computing the curve of the lower bound w.r.t. the magnification factor, we could estimate the limits of learning-based SR algorithms, at which the lower bound of the expected risk exceeds a relatively large threshold. We perform experiments to validate our theory. And based on our observations we conjecture that the limits may be independent of the size of either the LRIs or the HRIs.	[Lin, Zhouchen; Tang, Xiaoou] Microsoft Res Asia, Beijing 100190, Peoples R China; [He, Junfeng; Tang, Chi-Keung] Hong Kong Univ Sci & Technol, Kowloon, Hong Kong, Peoples R China	Microsoft; Microsoft Research Asia; Hong Kong University of Science & Technology	Lin, ZC (corresponding author), Microsoft Res Asia, Sigma Bldg,Zhichun Rd 49, Beijing 100190, Peoples R China.	zhoulin@microsoft.com; hejf@cse.ust.hk; xitang@microsoft.com; cktang@cse.ust.hk	Tang, Xiaoou/G-6509-2012					Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210; BEGIN I, 2004, P INT C PATT REC AUG, V1, P549; BISHOP CM, 2003, P ART INT STAT SOC A; Borman S., 1998, SPATIAL RESOLUTION E; Candocia FM, 1999, IEEE T NEURAL NETWOR, V10, P372, DOI 10.1109/72.750566; Capel D, 2001, PROC CVPR IEEE, P627; Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043; CHANG TL, 2006, P EUR C COMP VIS, P281; Dedeoglu G, 2004, PROC CVPR IEEE, P151; Elad M, 1997, IEEE T IMAGE PROCESS, V6, P1646, DOI 10.1109/83.650118; FAN W, 2007, P COMP VIS PATT REC; Farsiu S, 2004, INT J IMAG SYST TECH, V14, P47, DOI 10.1002/ima.20007; Freeman W. T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1182, DOI 10.1109/ICCV.1999.790414; Gunturk BK, 2003, IEEE T IMAGE PROCESS, V12, P597, DOI 10.1109/TIP.2003.811513; Hardie RC, 1997, IEEE T IMAGE PROCESS, V6, P1621, DOI 10.1109/83.650116; Kim SP, 1993, IEEE T IMAGE PROCESS, V2, P534, DOI 10.1109/83.242363; KOMATSU T, 1993, IEE PROC-I, V140, P19, DOI 10.1049/ip-i-2.1993.0005; KURSUN O, 2003, ISTANBUL U J ELECT E, V3, P673; Li Y, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P723; LI Y, 2004, P 3 INT C IM GRAPH, P298; LIN Z, 2007, INT C COMP VIS; Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081; Liu C, 2001, PROC CVPR IEEE, P192; Liu W, 2005, PROC CVPR IEEE, P478; LIU W, 2005, IEEE INT C IM PROC I; LIU W, 2005, IEEE INT C MULT EXP; MIRAVET C, 2003, INT C NEUR INF PROC, P417; Nguyen N, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P351, DOI 10.1109/ICIP.2000.899387; Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207; Pickup LC, 2004, ADV NEUR IN, V16, P1587; Rhee S, 1999, OPT ENG, V38, P1348, DOI 10.1117/1.602177; Shah NR, 1999, IEEE T IMAGE PROCESS, V8, P879, DOI 10.1109/83.766865; SHIRYAEV AN, 1995, PROBABILITY; Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444; Sun J, 2003, PROC CVPR IEEE, P729; Tsai, 1984, ADV COMPUTER VISION, V1, P317; Vapnik V., 1998, STAT LEARNING THEORY; Wilson R, 2000, INT C PATT RECOG, P212, DOI 10.1109/ICPR.2000.905305; Zhang LM, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P2414	41	24	31	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2008	80	3					406	420		10.1007/s11263-008-0148-2	http://dx.doi.org/10.1007/s11263-008-0148-2			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	350RJ		Green Published			2022-12-18	WOS:000259370500007
J	Leichter, I; Lindenbaum, M; Rivlin, E				Leichter, Ido; Lindenbaum, Michael; Rivlin, Ehud			A general framework for combining visual trackers - The "black boxes" approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						visual tracking; tracker combination; Kalman filter; CONDENSATION	TRACKING; FUSION	Over the past few years researchers have been investigating the enhancement of visual tracking performance by devising trackers that simultaneously make use of several different features. In this paper we investigate the combination of synchronous visual trackers that use different features while treating the trackers as "black boxes". That is, instead of fusing the usage of the different types of data as has been performed in previous work, the combination here is allowed to use only the trackers' output estimates, which may be modified before their propagation to the next time step. We propose a probabilistic framework for combining multiple synchronous trackers, where each separate tracker outputs a probability density function of the tracked state, sequentially for each image. The trackers may output either an explicit probability density function, or a sample-set of it via CONDENSATION. Unlike previous tracker combinations, the proposed framework is fairly general and allows the combination of any set of trackers of this kind, even in different state-spaces of different dimensionality, under a few reasonable assumptions. The combination may consist of different trackers that track a common object, as well as trackers that track separate, albeit related objects, thus improving the tracking performance of each object. The benefits of merely using the final estimates of the separate trackers in the combination are twofold. Firstly, the framework for the combination is fairly general and may be easily used from the software aspects. Secondly, the combination may be performed in a distributed setting, where each separate tracker runs on a different site and uses different data, while avoiding the need to share the data. The suggested framework was successfully tested using various state-spaces and datasets, demonstrating that fusing the trackers' final distribution estimates may indeed be applicable.	Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Leichter, I (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.	idol@cs.technion.ac.il; mic@cs.technion.ac.il; ehudr@cs.technion.ac.il						Baker T, 1998, INT C PATT RECOG, P1339, DOI 10.1109/ICPR.1998.711949; Bar-Shalom Y., 1992, MULTITARGET MULTISEN; Bar-Shalom Y., 1988, TRACKING DATA ASS; Beal MJ, 2003, IEEE T PATTERN ANAL, V25, P828, DOI 10.1109/TPAMI.2003.1206512; BLAKE A, 1993, INT J COMPUT VISION, V11, P127, DOI 10.1007/BF01469225; Blatt D, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P929; Collins RT, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P346; Darrell T, 2000, INT J COMPUT VISION, V37, P175, DOI 10.1023/A:1008103604354; GIL S, 1996, P 4 EUR C COMP VIS, V2, P307; HUE C, 2001, P 2001 IEEE WORKSH M; Isard M, 2003, PROC CVPR IEEE, P613; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Isard M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P34, DOI 10.1109/ICCV.2001.937594; Jepson AD, 2001, PROC CVPR IEEE, P415; Leichter I, 2004, PROC CVPR IEEE, P445; LEICHTER I, 2004, P 2004 IEEE INT C IM, V2, P1045; MacCormick J, 2000, INT J COMPUT VISION, V39, P57, DOI 10.1023/A:1008122218374; McCane B, 2002, INT J COMPUT VISION, V49, P79, DOI 10.1023/A:1019833915960; OKUMA K, 2004, P EUR C COMP VIS, P28; Papoulis A., 1991, COMMUNICATIONS SIGNA, V3; Perez P, 2004, P IEEE, V92, P495, DOI 10.1109/JPROC.2003.823147; Perez P, 2002, LECT NOTES COMPUT SC, V2350, P661; Rasmussen C, 2001, IEEE T PATTERN ANAL, V23, P560, DOI 10.1109/34.927458; Rosenberg Y, 1997, PROC CVPR IEEE, P654, DOI 10.1109/CVPR.1997.609395; Scott D. W., 1992, MULTIVARIATE DENSITY, DOI 10.1002/9780470316849; Shearer K, 2001, PATTERN RECOGN, V34, P1257, DOI 10.1016/S0031-3203(00)00072-8; SIDENBLADH H, 2003, P 2003 IEEE WORKSH M; Siebel NT, 2002, LECT NOTES COMPUT SC, V2353, P373; Spengler M, 2003, MACH VISION APPL, V14, P50, DOI 10.1007/s00138-002-0095-9; Sudderth EB, 2003, PROC CVPR IEEE, P605; TOYAMA A, 1995, SPIE INT SOC OPTICAL, V2569, P38; Toyama K, 2002, INT J COMPUT VISION, V48, P9, DOI 10.1023/A:1014899027014; TRIESCH J, 2000, P 4 INT C AUT FAC GE; Vermaak J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1110; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wu Y, 2004, INT J COMPUT VISION, V58, P55, DOI 10.1023/B:VISI.0000016147.97880.cd; P IEEE INT WORKSH SE; P 2001 IEEE WORKSH M	38	24	25	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2006	67	3					343	363		10.1007/s11263-006-5568-2	http://dx.doi.org/10.1007/s11263-006-5568-2			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	052JA		Green Submitted			2022-12-18	WOS:000238228800006
J	Del Bue, A; Agapito, L				Del Bue, A; Agapito, L			Non-rigid stereo factorization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	6th Asian Conference on Computer Vision	JAN 28-30, 2004	Cheju Isl, SOUTH KOREA			non-rigid structure from motion; non-linear optimization; stereo; deformable model	SHAPE; TRACKING	In this paper we address the problem of recovering 3D non-rigid structure from a sequence of images taken with a stereo pair. We have extended existing non-rigid factorization algorithms to the stereo camera case and presented an algorithm to decompose the measurement matrix into the motion of the left and right cameras and the 3D shape, represented as a linear combination of basis-shapes. The added constraints in the stereo camera case are that both cameras are viewing the same structure and that the relative orientation between both cameras is fixed. Our focus in this paper is on the recovery of flexible 3D shape rather than on the correspondence problem. We propose a method to compute reliable 3D models of deformable structure from stereo images. Our experiments with real data show that improved reconstructions can be achieved using this method. The algorithm includes a non-linear optimization step that minimizes image reprojection error and imposes the correct structure to the motion matrix by choosing an appropriate parameterization. We show that 3D shape and motion estimates can be successfully disambiguated after bundle adjustment and demonstrate this on synthetic and real image sequences. While this optimization step is proposed for the stereo camera case, it can be readily applied to the case of non-rigid structure recovery using a monocular video sequence.	Queen Mary Univ London, Dept Comp Sci, London E1 4NS, England	University of London; Queen Mary University London	Del Bue, A (corresponding author), Queen Mary Univ London, Dept Comp Sci, London E1 4NS, England.	alessio@dcs.qmul.ac.uk; lourdes@dcs.qmul.ac.uk		Del Bue, Alessio/0000-0002-2262-4872				Aanaes H., 2002, WORKSH VIS MOD DYN S; Bar-Itzhack IY, 2000, J GUID CONTROL DYNAM, V23, P1085, DOI 10.2514/2.4654; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Brand M, 2001, PROC CVPR IEEE, P315; Brand M., 2001, P IEEE C COMP VIS PA; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; DELBUE A, 2004, AS C COMP VIS ACCV20, V1; DELBUE A, 2004, WORKSH ART NONR MOT; ESSA I, 1996, P COMP AN C GEN SWIT; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; IRANI M, 1999, P 7 INT C COMP VIS K; PARKE FI, 1996, COMPUTER FACIAL ANIM; PIGHIN F, 1998, P ACM SIGGRAPH C COM; Sugaya Y, 2004, IEICE T INF SYST, VE87D, P1031; Tan JK, 2001, COMPUT VIS IMAGE UND, V82, P101, DOI 10.1006/cviu.2001.0904; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TORRESANI L, 2001, P IEEE C COMP VIS PA; TRESADERN P, 2003, P BRIT MACH VIS C NO; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; XIAO J, 2004, 8 EUR C COMP VIS ECC	20	24	24	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2006	66	2					193	207		10.1007/s11263-005-3958-5	http://dx.doi.org/10.1007/s11263-005-3958-5			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	022CY		Green Submitted			2022-12-18	WOS:000236033500006
J	Yagi, Y; Yachida, M				Yagi, Y; Yachida, M			Real-time onmidirectional image sensors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						omnidirectional image; multiple image sensing system; sensor designing; optics; mobile robot navigation	OMNIDIRECTIONAL VISION; MAP; NAVIGATION; RESOLUTION	Conventional T.V. cameras are limited in their field of view. A real-time omnidirectional camera which can acquire an omnidirectional (360 degrees) field of view at video rate and which could be applied in a variety of fields, such as autonomous navigation, telepresence, virtual reality and remote monitoring, is presented. We have developed three different types of omnidirectional image sensors, and two different types of multiple-image sensing systems which consist of an omnidirectional image sensor and binocular vision. In this paper, we describe the outlines and fundamental optics of our developed sensors and show examples of applications for robot navigation.	Osaka Univ, Inst Sci & Ind Res, Suita, Osaka 565, Japan; Osaka Univ, Grad Sch Engn Sci, Suita, Osaka 565, Japan	Osaka University; Osaka University	Yagi, Y (corresponding author), Osaka Univ, Inst Sci & Ind Res, Suita, Osaka 565, Japan.	yagi@am.sanken.osaka-u.ac.jp						Aihara N, 1998, INT C PATT RECOG, P1799, DOI 10.1109/ICPR.1998.712078; AYRES WA, 1942, Patent No. 2304434; BANG SW, 1995, P IROS, P542; Barth M, 1996, IROS 96 - PROCEEDINGS OF THE 1996 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - ROBOTIC INTELLIGENCE INTERACTING WITH DYNAMIC WORLDS, VOLS 1-3, P626, DOI 10.1109/IROS.1996.570866; Brassart E, 2000, IEEE WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P145, DOI 10.1109/OMNVIS.2000.853822; Bruckstein AM, 2000, IEEE WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P79; CAO ZL, 1990, P SOC PHOTO-OPT INS, V1230, P788; CAO ZL, 1986, J ROBOTIC SYST, V3, P5, DOI 10.1002/rob.4620030103; Chahl JS, 1997, APPL OPTICS, V36, P8275, DOI 10.1364/AO.36.008275; Chang P, 2000, IEEE WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P127, DOI 10.1109/OMNVIS.2000.853819; DAVIS JE, 1997, Patent No. 5627675; Delahoche L, 1998, IEEE INT CONF ROBOT, P2560, DOI 10.1109/ROBOT.1998.680727; ETOH M, 1999, P 7 INT C COMP VIS, V1, P579; Gluckman J, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P999, DOI 10.1109/ICCV.1998.710838; GREGUSS P, 1985, OPT LASER TECHNOL, V17, P41, DOI 10.1016/0030-3992(85)90123-9; Hicks RA, 2000, J MATH IMAGING VIS, V13, P57, DOI 10.1023/A:1008381724192; HIURA R, 1995, P RSJ ROB S, P151; HOLENSTEIN A, 1991, P IEEE INT C ROB AUT, V1, P898; IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L; ISHIGURO H, 1992, IEEE T PATTERN ANAL, V14, P257, DOI 10.1109/34.121792; Ishiguro H, 1996, IROS 96 - PROCEEDINGS OF THE 1996 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - ROBOTIC INTELLIGENCE INTERACTING WITH DYNAMIC WORLDS, VOLS 1-3, P634, DOI 10.1109/IROS.1996.571018; KAWASAKI H, 2000, P AS C COMP VIS; KAWASAKI H, 2000, P IAPR INT C PATT RE, V1, P379; KONPARU T, 1998, P M IM REC UND, V2, P7; KONPARU T, 1997, P 15 ANN C RSJ, V3, P957; Li SG, 2000, AUTON ROBOT, V8, P117, DOI 10.1023/A:1008983511443; Matsunaga Y, 1997, ELEC SOC S, V97, P184; Matthews B, 1995, P SOC PHOTO-OPT INS, V2588, P438, DOI 10.1117/12.222697; Morita T., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P422, DOI 10.1109/CVPR.1989.37881; Nagahara H, 2001, IEEE IMAGE PROC, P654, DOI 10.1109/ICIP.2001.959130; Nagahara H, 2000, IEEE IND ELEC, P2559, DOI 10.1109/IECON.2000.972401; NAGAHARA H, 2001, PRMU2000152 IEICE, P39; Nayar SK, 2000, PROC CVPR IEEE, P388; NAYAR SK, 1999, P IEEE COMP SOC C CO, V2, P217; NELSON RC, 1988, BIOL CYBERN, V58, P261, DOI 10.1007/BF00364131; Pegard C, 1996, IEEE INT CONF ROBOT, P89, DOI 10.1109/ROBOT.1996.503578; PELEG S, 1987, PATTERN RECOGN LETT, V5, P223, DOI 10.1016/0167-8655(87)90067-5; Powell  I., 1995, U.S. patent, Patent No. [5,473,474, 5473474]; Rees D. W., 1970, US Patent, Patent No. 3505465; Roning J. J., 1987, Proceedings of the SPIE - The International Society for Optical Engineering, V728, P57, DOI 10.1117/12.937824; Rosendahl G., 1983, US patent, Patent No. 4395093; ROSSI B, 1962, OPTICS; SANTOSVICTOR J, 1995, INT J COMPUT VISION, V14, P159, DOI 10.1007/BF01418981; SARACHIK KB, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P984, DOI 10.1109/ROBOT.1989.100109; SIMAMURA J, 2000, P IEEE WORKSH OMN VI, P62; SVODOBA T, 1998, EUR C COMP VIS, P218; Takeya A, 1998, P SOC PHOTO-OPT INS, V3430, P50, DOI 10.1117/12.332486; Tsai, 1984, ADV COMPUTER VISION, V1, P317; TSUJI Y, 1997, P 15 ANN C RSJ, V3, P961; UTSUMI A, 1992, P IAPR MACH VIS APPL, P25; Wei SC, 1998, ADV ROBOTICS, V12, P433; Yachida M, 1998, COMPUTER VISION FOR VIRTUAL REALITY BASED HUMAN COMMUNICATIONS - 1998 IEEE AND ATR WORKSHOP PROCEEDINGS, P20, DOI 10.1109/CVVRHC.1998.660367; YAGI Y, 1994, IEEE T ROBOTIC AUTOM, V10, P11, DOI 10.1109/70.285581; Yagi Y, 1998, IEEE INT CONF ROBOT, P1250, DOI 10.1109/ROBOT.1998.677273; YAGI Y, 1995, IEEE T ROBOTIC AUTOM, V11, P634, DOI 10.1109/70.466602; Yagi Y., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P946, DOI 10.1109/ICPR.1996.546163; Yagi Y., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P3493, DOI 10.1109/ROBOT.2000.845273; Yagi Y, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P1487, DOI 10.1109/IROS.2000.893230; Yagi Y, 1996, IROS 96 - PROCEEDINGS OF THE 1996 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - ROBOTIC INTELLIGENCE INTERACTING WITH DYNAMIC WORLDS, VOLS 1-3, P618, DOI 10.1109/IROS.1996.570864; Yagi Y, 1999, IEICE T INF SYST, VE82D, P568; Yagi Y., 1990, Proceedings. IROS '90. IEEE International Workshop on Intelligent Robots and Systems '90. Towards a New Frontier of Applications (Cat. No.90TH0332-7), P181, DOI 10.1109/IROS.1990.262385; YAGI Y, 1994, IEEE INT CONF ROBOT, P1679, DOI 10.1109/ROBOT.1994.351350; YAGI Y, 1999, 1999 JSME C ROB MECH, P1; YAGI Y, 1994, P IEEE RSJ INT C INT, V2, P996; YAGI Y, 1999, P IEEE RSJ INT C INT, V1, P58; YAGI Y, 1996, P IEEE RSJ INT C INT, V2, P640; YAGI Y, 1998, P INT C QUAL CONTR A, P385; YAGI Y, 2000, P AS C COMP VIS, P23; YAGI Y, 1997, P INT C INT ROB SYST, V2, P1024; YAGI Y, 1998, Patent No. 1017251; YAMAZAWA K, 1993, IROS 93 : PROCEEDINGS OF THE 1993 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOL 1-3, P1029, DOI 10.1109/IROS.1993.583287; YAMAZAWA K, 1995, IEEE INT CONF ROBOT, P1062, DOI 10.1109/ROBOT.1995.525422; YAMAZAWA K, 2000, P INT C PATT REC, V3, P487; Zheng J. Y., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P1154, DOI 10.1109/ROBOT.1990.126152; Zheng J. Y., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P161, DOI 10.1109/ICPR.1990.118082; ZHENG JY, 1992, INT J COMPUT VISION, V9, P55	76	24	26	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	2004	58	3					173	207		10.1023/B:VISI.0000019684.35147.fc	http://dx.doi.org/10.1023/B:VISI.0000019684.35147.fc			35	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	836GD					2022-12-18	WOS:000222542900002
J	Enkelmann, W				Enkelmann, W			Video-based driver assistance-from basic functions to applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						real-time image sequence analysis; driver assistance; lane recognition; multi sensor fusion	VISION; SYSTEM; ROAD	Image sequences recorded with cameras mounted in a moving vehicle provide information about the vehicle's environment which has to be analysed in order to really support the driver in actual traffic situations. One type of information is the lane structure surrounding a vehicle. Therefore, we systematically developed and investigated driver assistance functions which make explicit use of the lane structure represented by lane borders and lane markings. With increasing computing power of standard PCs it was possible to realize more complex driver assistance with general purpose hardware. Investigations with a video-based lane departure warning system and a lane change assistant for highways will be discussed in detail. We integrated our lane keeping assistant in some experimental cars and performed systematic experiments in real traffic situations which enables the experience of video-based driver assistance on a high level-the action of a system. This allows us to assess whether a driver assistance system really understands the actual traffic situation which is the basis for reliable systems accepted by the user.	Fraunhofer Inst Informat & Data Proc, D-72131 Karlsruhe, Germany	Fraunhofer Gesellschaft	Enkelmann, W (corresponding author), DaimlerChrysler AG, Res & Technol, Alt Moabit 96 A, D-10559 Berlin, Germany.	wilfried.enkelmann@daimlerchrysler.com						BATAVIA PH, 1998, IEEE INT C INT VEH O, V1, P5; CARLSSON S, 1990, LECT NOTES COMPUT SC, V427, P297, DOI 10.1007/BFb0014876; CRISMAN JD, 1993, IEEE T ROBOTIC AUTOM, V9, P49, DOI 10.1109/70.210794; Dickmanns E. D., 1994, Proceedings of the Intelligent Vehicles '94 Symposium (Cat. No.94TH8011), P68, DOI 10.1109/IVS.1994.639472; DICKMANNS ED, 1992, IEEE T PATTERN ANAL, V14, P199, DOI 10.1109/34.121789; DICKMANNS ED, 1988, MACH VISION APPL, V1, P223; Enkelmann W., 1995, Proceedings of the Intelligent Vehicles '95. Symposium (Cat. No.95TH8132), P356, DOI 10.1109/IVS.1995.528307; ENKELMANN W, 1997, THESIS U KARLSRUHE T; Fairclough S. H., 1997, ERGONOMICS SAFETY IN, P363; GEGENBACH V, 1995, MITTEILUNGEN FRAUNHO, P53; Gengenbach V., 1995, Proceedings of the Intelligent Vehicles '95. Symposium (Cat. No.95TH8132), P512, DOI 10.1109/IVS.1995.528334; HAHN S, 1993, INT VEH 93 S JUL 14, P25; Heimes F, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P498, DOI 10.1109/IVS.2000.898392; HEIMES F, 2000, THESIS U KARLSRUHE T; JUBERTS M, 1993, INTELLIGENT AUTONOMOUS SYSTEMS : IAS-3, P135; KOVACS G, 1998, IEEE INT C INT VEH S, P46; Kreucher C., 1998, P IEEE INT C INT VEH P IEEE INT C INT VEH, V1, P17; Kruger W, 1999, MACH VISION APPL, V11, P203, DOI 10.1007/s001380050103; LeBlanc DJ, 1996, IEEE CONTR SYST MAG, V16, P61, DOI 10.1109/37.546271; LUONG QT, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P52, DOI 10.1109/ICCV.1995.466807; NAGEL HH, 1995, MATH COMPUT MODEL, V22, P185, DOI 10.1016/0895-7177(95)00133-M; NAGESHA K, 1991, RAPID COMMUN MASS SP, V5, P15, DOI 10.1002/rcm.1290050105; ONKEN R, 1994, IEEE INT C INT VEH P, P544; PAUL D, 1988, IEEE T PATTERN ANAL, V10, P399, DOI 10.1109/34.3904; RENNER G, 1997, 4 WORLD C INT TRANSP; RICHARDSON J, 1997, ERGONOMICS SAFETY IN, P329; Risack R, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P356, DOI 10.1109/IVS.2000.898369; RISACK R, 1998, INT VEH S STUTTG GER, V1, P35; SCHNEIDERMAN H, 1994, IEEE T ROBOTIC AUTOM, V10, P769, DOI 10.1109/70.338531; Serge B., 1994, INT VEH S 94 PAR FRA, P296; Siegmund G. P, 1996, SENSORS SAFETY SYSTE, P17; Taylor CJ, 1999, INT J ROBOT RES, V18, P442; TOLLE W, 1996, FORTSCHRITT BERIC 12, V299; Ulmer B., 1994, Proceedings of the Intelligent Vehicles '94 Symposium (Cat. No.94TH8011), P1, DOI 10.1109/IVS.1994.639460; Willersinn D, 1997, IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P717, DOI 10.1109/ITSC.1997.660562; ZHANG J, 1994, INT VEH S PAR FRANC, P260	36	24	27	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2001	45	3					201	221		10.1023/A:1013658100226	http://dx.doi.org/10.1023/A:1013658100226			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	487KH					2022-12-18	WOS:000171873900001
J	ANCONA, N; POGGIO, T				ANCONA, N; POGGIO, T			OPTICAL-FLOW FROM 1-D CORRELATION - APPLICATION TO A SIMPLE TIME-TO-CRASH DETECTOR	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							MOTION	In the first part of this article we show that a new technique exploiting 1-D correlation of 2-D or even 1-D patches between successive frames may be sufficient to compute a Satisfactory estimation of the optical flow field. The algorithm is well suited to VLSI implementations. The sparse measurements provided by the technique can be used to compute qualitative properties of the flow for a number of different visual tasks. In particular, the second part of the article shows how to combine our 1-D correlation technique with a scheme for detecting expansion or rotation (Poggio et al. 1991) in a simple algorithm which also suggests interesting biological implications. The algorithm provides a rough estimate of time-to-crash. It was tested on real-image sequences. We show its performance and compare the results to previous approaches.	MIT,ARTIFICIAL INTELLIGENCE LAB,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)	ANCONA, N (corresponding author), TECNOPOLIS CSATA NOVUS ORTUS,ROBOT & AUTOMAT LAB,I-70010 VALENZANO,ITALY.		Ancona, Nicola/AAD-4772-2022; Ancona, Nicola/E-9971-2013	Ancona, Nicola/0000-0003-0065-0321				CHRISTOF K, 1991, IEEE WORKSHOP VISUAL; CIPOLLA R, 1992, 2ND P EUR C COMP VIS, P187; LITTLE J, 1988, 2ND P INT C COMP VIS; POGGIO T, 1973, KYBERNETIK, V13, P223, DOI 10.1007/BF00274887; POGGIO T, 1991, AI1289 MIT ART INT L; POGGIO T, 1990, P IEEE, V78; POGGIO T, 1988, NEURON COMPUTATIONAL; REICHARDT W, 1987, J COMP PHYSIOL A, V161, P533, DOI 10.1007/BF00603660; Reichardt W., 1961, S PRINC SENS COMM, P303; VANSANTEN JPH, 1984, J OPT SOC AM A, V1, P451, DOI 10.1364/JOSAA.1.000451; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; VERRI A, 1990, P IEEE WORKSHOP ROBU	12	24	27	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1995	14	2					131	146		10.1007/BF01418979	http://dx.doi.org/10.1007/BF01418979			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT789		Green Submitted			2022-12-18	WOS:A1995QT78900003
J	VANDERWAL, GS; BURT, PJ				VANDERWAL, GS; BURT, PJ			A VLSI PYRAMID CHIP FOR MULTIRESOLUTION IMAGE-ANALYSIS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								Advanced techniques in image processing and computer vision increasingly require that image data be represented at multiple resolutions and at multiple sample rates. Application areas for such pyramid image representations include image compression, image enhancement, motion analysis, and object recognition. We have developed a VLSI chip, called PYR, to perform the standard filter and resampling operations required in pyramid and inverse pyramid transforms for these applications. The PYR chip processes image samples sequentially, in raster scan format, so is suited for pipeline architectures. The user can choose from a set of standard filters, through software control, to construct Gaussian, Laplacian, subband, and related pyramid structures. A unique feature of the design is that it includes timing signals that are passed with the image data. These signals coordinate successive processing steps in a pipeline system as image sizes and sample rates change. The chip also includes circuits for edge extension and image addition, and it can be run in "spread tap" mode to provide twice the standard sample density. The PYR chip is implemented in standard cell technology. At a clock rate of 15 MHz, a single chip can simultaneously construct a Gaussian and a Laplacian pyramid from a 512 by 480 image in 22.7 msec (44 frame/second).	DAVID SARNOFF RES CTR INC,PRINCETON,NJ 08543	Sarnoff Corporation								ADELSON EH, 1987, P SPIE C VISUAL COMM, V2; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; ANDERSON CH, 1984, RCA CORRESPONDENCE; ANDERSON CH, 1985, P SPIE C INTELLIGENT, V579; BERGEN JR, 1987, J OPT SOC AM A, V4, P35; BURT P, 1983, IEEE T COMMUN, V31; Burt P. J., 1988, FAMILY PYRAMID STRUC; BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247; BURT PJ, 1981, COMPUT VISION GRAPH, V16, P20, DOI 10.1016/0146-664X(81)90092-7; BURT PJ, 1987, P WORKSHOP COMPUTER; BURT PJ, 1988, P COMPUT VIS PATT RE; BURT PJ, 1992, PROC SOC INFORM DISP; BURT PJ, 1988, IEEE P, P76; BURT PJ, 1990, 10TH P ICPR, P305; Cantoni V., 1985, Integrated Technology for Parallel Image Processing, P121; CHEHIKIAN A, 1991, 7TH SCAND C IM AN DE; CROWLEY JL, 1984, IEEE T PATTERN ANAL, V6, P212, DOI 10.1109/TPAMI.1984.4767504; LEGALL D, 1988, P INT C ACOUST SPCH; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; MATTHIES L, 1991, STEREO VISION PLANET; MERIGOT A, 1986, PYRAMIDAL SYSTEMS CO; QUAM L, 1987, READINGS COMPUTER VI; ROSENFELD A, 1984, MULTIRESOLUTION IMAG; TANIMOTO SL, 1984, J PARALLEL DISTR COM, V1, P105, DOI 10.1016/0743-7315(84)90001-7; Toet A., 1990, Machine Vision and Applications, V3, P1, DOI 10.1007/BF01211447; VANDERWAL GS, 1985, SPIE P INTELLIGENT R, P300; Watson A. B., 1987, COMPUT GRAPH IMAGE P, P39	28	24	29	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1992	8	3					177	189		10.1007/BF00055150	http://dx.doi.org/10.1007/BF00055150			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JU903					2022-12-18	WOS:A1992JU90300002
J	CROWLEY, JL; STELMASZYK, P; SKORDAS, T; PUGET, P				CROWLEY, JL; STELMASZYK, P; SKORDAS, T; PUGET, P			MEASUREMENT AND INTEGRATION OF 3-D STRUCTURES BY TRACKING EDGE LINES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								This article describes techniques for dynamically modeling the 2-D appearance and 3-D geometry of a scene by integrating information from a moving camera. These techniques are illustrated by the design of a system that constructs a geometric description of a scene from the motion of a camera mounted on a robot arm. A framework for dynamic world modeling is described. The framework presents the fusion of perceptual information as a cyclic process composed of three phases: Predict, Match, and Update. A set of mathematical tools are presented for each of these phases. The use of these tools is illustrated by the design of a system for tracking edge lines in image coordinates and inferring the 3-D position from a known camera motion. The movement of edge-lines in a sequence of images is measured by tracking to maintain an image-plane description of movement. This description is composed of a list of edge-segments represented as a parametric primitive. Each parameter is composed of an estimated value, a temporal derivative, and a covariance matrix. Line-segment parameters are updated using a Kalman filter. The correspondence between observed and predicted segments is determined by a nearest-neighbor matching algorithm using distance between parameters normalized by covariance. It is observed that constraining the acceleration of edge-lines between frames permits the use of a very simple matching algorihtm, thus yielding a very short cycle time. Three-dimensional structure is computed using the correspondence provided by the 2-D segment tracking process. Fusion of 3-D data from different view points provides an accurate representation of the geometry of objects in the scene. An extended Kalman filter is applied to the inference of the 3-D position and orientation parameters of 2-D segments. This process demonstrates that 2-D tracking provides the information for an inexpensive technique for estimating 3-D shape from motion. Results from several image sequences taken from a camera mounted on a robot arm are presented to illustrate the reliability and precision of the technique.	INST NATL POLYTECH GRENOBLE,INST MATH APPL GRENOBLE,INFORMAT FONDAMENTALE & INTELLIGENCE,F-38031 GRENOBLE,FRANCE; ITMI,F-38240 MEYLAN,FRANCE	Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble			Puget, Pierre/C-6921-2008					[Anonymous], T ASME D; AYACHE N, 1988, THESIS U PARIS SUD C; BRAMMER K, 1969, KALMAN BUCY FILTERS; BROOKS RA, 1985, P C ROBOTICS AUTOM; BROWN C, 1989, MAY P DARPA IM UND W; Bucy R.S., 1968, FILTERING STOCHASTIC; BUCY RS, BBD600 J HOPK U APPL; CANNY J, 1986, IEEE T PAMI, V8; CHATILA R, 1985, MAR P C ROB AUT; CHEHIKIAN A, 1990, EUSIPCO BARCELON SEP; CHEHIKIAN A, 1989, WORKSHOP IND APPLICA; CROWLEY JL, 1988, 2ND INT C COMP VIS T; CROWLEY JL, 1989, MAY P C ROB AUT SCOT; CROWLEY JL, 1985, IEEE J ROBOTICS AUTO, V1; CROWLEY JL, 1987, AAAI WORKSHOP SPATIA; CROWLEY JL, 1984, MAR IEEE C COMP VIS; CROWLEY JL, 1990, P EUROPEAN C COMPUT; CROWLEY JL, 1986, APR P IEEE INT C ROB; CROWLEY JL, 1990, 1ST P EUR C COMP VIS; DERICHE R, 1987, INT J COPUT VIS, V1; DURRANTWHYTE HF, 1987, INT J ROBOTICS RES, V6; FAUGERAS OD, 1986, JUN P IEEE C COMP VI, P15; FAUGERAS OD, 1986, APR P INT C ROB AUT; GENNERY DB, 1982, 2ND P NATL C ART INT; HERMAN M, 1986, ARTIF INTELL, V30, P289, DOI 10.1016/0004-3702(86)90002-0; HILDRETH E, 1983, MEASUREMENT VISUAL M; HORAUD P, 1990, 1ST P EUR C COMP VIS; HUANG TS, 1983, IMAGE SEQUENCE PROCE; Jazwinski A.H., 1970, STOCHASTIC PROCESSES; Kalman R. E., 1961, T ASME D, V83; KOLMOGOROV AN, 1941, B ACAD SCI USSR MATH, V5; Marr D., 1980, P R SOC LONDON, V207, P187; MATTHIES L, 1987, CMUCS87185 CMU TECH; MELSA AP, 1971, ESTIMATION THEORY AP; PUGET P, 1990, IMAGE VIS COMPUT, V8; RAMPARANY F, 1989, THESIS INPG; SKORDAS T, 1989, 2ND P C INT AUT SYST; SMITH R, 1987, INT J ROBOTICS RES, V5; Tsai RY, 1986, EFFICIENT ACCURATE C	40	24	25	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	1992	8	1					29	52		10.1007/BF00126399	http://dx.doi.org/10.1007/BF00126399			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JF613					2022-12-18	WOS:A1992JF61300002
J	DAVID, C; ZUCKER, SW				DAVID, C; ZUCKER, SW			POTENTIALS, VALLEYS, AND DYNAMIC GLOBAL COVERINGS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CURVATURE	We present a new approach to effect the transition between local and global representations. It is based on the notion of a covering, or a collection of objects whose union is equivalent to the full one. The mathematics of computing global coverings are developed in the context of curve detection, where an intermediate representation (the tangent field) provides a reliable local description of curve structure. This local information is put together globally in the form of a potential distribution. The elements of the covering are then short curves, each of which evolves in parallel to seek the valleys of the potential distribution. The initial curve positions are also derived from the tangent field, and their evolution is governed by variational principles. When stationary configurations are achieved, the global dynamic covering is defined by the union of the local dynamic curves.	MCGILL UNIV,INTELLIGENT MACHINES RES CTR,MONTREAL H3A 2T5,QUEBEC,CANADA; MCGILL UNIV,DEPT ELECT ENGN,MONTREAL H3A 2T5,QUEBEC,CANADA	McGill University; McGill University								Ballard D.H., 1982, COMPUTER VISION; Benson A., 1977, ACM Transactions on Mathematical Software, V3, P96, DOI 10.1145/355719.355728; DAVID C, 1989, TRCIM891 MCGILL U TE; DOBBINS A, 1987, NATURE, V329, P438, DOI 10.1038/329438a0; Doob, 2000, CLASSICAL POTENTIAL; DRAPER BA, 1989, INT J COMPUT VISION, V2, P209, DOI 10.1007/BF00158165; HUMMEL RA, 1983, IEEE T PATTERN ANAL, V5, P267, DOI 10.1109/TPAMI.1983.4767390; Iverson L., 1987, Proceedings of the IEEE Computer Society Workshop on Computer Vision (Cat. No.87TH0210-5), P184; IVERSON LA, 1988, THESIS MCGILL U MONT; KAAS M, 1988, INT J COMPUT VISION, V1, P321; KAAS W, 1987, COMPUT VISION GRAPH, V37, P362; Kaplan W., 1952, ADV CALCULUS; Levine M., 1985, VISION MAN MACHINE; LOWE DG, 1988, P INT C COMPUT VISIO, V2, P558; MARTELLI A, 1976, COMMUN ACM, V19, P73, DOI 10.1145/359997.360004; MONTANARI U, 1971, COMMUN ACM, V14, P335, DOI 10.1145/362588.362594; PARENT P, 1989, IEEE T PATTERN ANAL, V11, P823, DOI 10.1109/34.31445; ROSEFELD A, 1982, DIGITAL IMAGE PROCES; SANDER PT, 1990, IEEE T PATTERN ANAL, V12, P833, DOI 10.1109/34.57680; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P129, DOI 10.1109/TPAMI.1986.4767767; TERZOPOULOS D, 1987, TECHNICAL DIGEST SER, V12, P160; Terzopoulos D., 1987, TECHNICAL DIGEST SER, V12, P164; TSOTSOS, 1987, ENCY ARTIFICIAL INTE, P389; Zucker SW, 1989, NEURAL COMPUT, V1, P68, DOI 10.1162/neco.1989.1.1.68; ZUCKER SW, 1987, ANNU REV COMPUT SCI, V2, P69, DOI 10.1146/annurev.cs.02.060187.000441; ZUCKER SW, 1985, COMPUT VISION GRAPH, V32, P74, DOI 10.1016/0734-189X(85)90003-9; ZUCKER SW, 1988, 2ND P INT C COMP VIS	28	24	24	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1990	5	3					219	238		10.1007/BF00126500	http://dx.doi.org/10.1007/BF00126500			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EZ751					2022-12-18	WOS:A1990EZ75100001
J	Li, SY; Ren, WQ; Wang, F; Araujo, IB; Tokuda, EK; Hirata, R; Cesar-Jr, RM; Wang, ZY; Cao, XC				Li, Siyuan; Ren, Wenqi; Wang, Feng; Araujo, Iago Breno; Tokuda, Eric K.; Junior, Roberto Hirata; Cesar-Jr., Roberto M.; Wang, Zhangyang; Cao, Xiaochun			A Comprehensive Benchmark Analysis of Single Image Deraining: Current Challenges and Future Perspectives	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image deraining; Image quality assessment; Deep convolution network; Benchmark analysis	QUALITY ASSESSMENT; RAIN; REMOVAL	The capability of image deraining is a highly desirable component of intelligent decision-making in autonomous driving and outdoor surveillance systems. Image deraining aims to restore the clean scene from the degraded image captured in a rainy day. Although numerous single image deraining algorithms have been recently proposed, these algorithms are mainly evaluated using certain type of synthetic images, assuming a specific rain model, plus a few real images. It remains unclear how these algorithms would perform on rainy images acquired "in the wild" and how we could gauge the progress in the field. This paper aims to bridge this gap. We present a comprehensive study and evaluation of existing single image deraining algorithms, using a new large-scale benchmark consisting of both synthetic and real-world rainy images of various rain types. This dataset highlights diverse rain models (rain streak, rain drop, rain and mist), as well as a rich variety of evaluation criteria (full- and no-reference objective, subjective, and task-specific). We further provide a comprehensive suite of criteria for deraining algorithm evaluation, including full- and no-reference metrics, subjective evaluation, and the novel task-driven evaluation. The proposed benchmark is accompanied with extensive experimental results that facilitate the assessment of the state-of-the-arts on a quantitative basis. Our evaluation and analysis indicate the gap between the achievable performance on synthetic rainy images and the practical demand on real-world images. We show that, despite many advances, image deraining is still a largely open problem. The paper is concluded by summarizing our general observations, identifying open research challenges and pointing out future directions. Our code and dataset is publicly available at .	[Li, Siyuan; Wang, Feng] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China; [Ren, Wenqi; Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China; [Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China; [Cao, Xiaochun] Peng Cheng Lab, Cyberspace Secur Res Ctr, Shenzhen 518055, Peoples R China; [Araujo, Iago Breno; Tokuda, Eric K.; Junior, Roberto Hirata; Cesar-Jr., Roberto M.] Univ Sao Paulo, Inst Math & Stat IME, Sao Paulo, Brazil; [Wang, Zhangyang] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	Tianjin University; Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Peng Cheng Laboratory; Universidade de Sao Paulo; University of Texas System; University of Texas Austin	Ren, WQ (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China.	siyuan@tju.edu.cn; rwq.renwenqi@gmail.com; ibacaraujo@gmail.com; tokudaek@usp.br; rhiratajr@usp.br; rmcesar@usp.br; atlaswang@utexas.edu; caoxiaochun@iie.ac.cn	Hirata, Roberto/E-4436-2011	Hirata, Roberto/0000-0003-3861-7260; Wang, Feng/0000-0002-0516-1618	National Key R&D Program of China [2019YFB1406500]; National Natural Science Foundation of China [61802403, U1605252, U1736219]; Beijing Education Committee Cooperation Beijing Natural Science Foundation [KZ201910005007]; Beijing Nova Program [Z201100006820074]; Beijing Natural Science Foundation [L182057]; Peng Cheng Laboratory Project of Guangdong Province [PCL2018KP004]; Elite Scientist Sponsorship Program by the Beijing Association for Science and Technology; CAPES; CNPq; Funding Agency FAPESP [15/22308-2]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Education Committee Cooperation Beijing Natural Science Foundation; Beijing Nova Program(Beijing Municipal Science & Technology Commission); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Peng Cheng Laboratory Project of Guangdong Province; Elite Scientist Sponsorship Program by the Beijing Association for Science and Technology; CAPES(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); CNPq(Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPQ)); Funding Agency FAPESP	This work is supported by the Supported by the National Key R&D Program of China under Grant 2019YFB1406500, National Natural Science Foundation of China (Nos. 61802403, U1605252, U1736219), Beijing Education Committee Cooperation Beijing Natural Science Foundation (No. KZ201910005007), Beijing Nova Program (No. Z201100006820074), Beijing Natural Science Foundation (No. L182057), Peng Cheng Laboratory Project of Guangdong Province PCL2018KP004, Elite Scientist Sponsorship Program by the Beijing Association for Science and Technology, CAPES, CNPq, and the Funding Agency FAPESP (No. 15/22308-2).	Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2; Bossu J, 2011, INT J COMPUT VISION, V93, P348, DOI 10.1007/s11263-011-0421-7; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Chen Wenlin, 2020, ARXIV201013723, P2; Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247; Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352; Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502; Dai D., 2016, P 2016 IEEE WINTER C, P1; Dai DX, 2020, INT J COMPUT VISION, V128, P1182, DOI 10.1007/s11263-019-01182-4; Ding XH, 2016, MULTIMED TOOLS APPL, V75, P2697, DOI 10.1007/s11042-015-2657-7; Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481; Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186; Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802; Garg K, 2005, IEEE I CONF COMP VIS, P1067; Garg K, 2004, PROC CVPR IEEE, P528; Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189; Hahner M, 2019, IEEE INT C INTELL TR, P3675, DOI 10.1109/ITSC.2019.8917518; Halder SS, 2019, IEEE I CONF COMP VIS, P10202, DOI 10.1109/ICCV.2019.01030; Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821; Jiang TX, 2017, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2017.301; Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462; Jin X, 2019, IEEE IMAGE PROC, P2761, DOI 10.1109/ICIP.2019.8803238; Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057; Kondermann D, 2016, IEEE COMPUT SOC CONF, P19, DOI 10.1109/CVPRW.2016.10; Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188; Lei Z., 2017, IEEE INT C COMP VIS; Li BY, 2018, AAAI CONF ARTIF INTE, P7016; Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951; Li H, 2017, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2017.560; Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173; Li SY, 2019, PROC CVPR IEEE, P3833, DOI 10.1109/CVPR.2019.00396; Li Y, 2017, IEEE T IMAGE PROCESS, V26, P3874, DOI 10.1109/TIP.2017.2708841; Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu D, 2020, IEEE T IMAGE PROCESS, V29, P3695, DOI 10.1109/TIP.2020.2964518; Liu D, 2019, IEEE T IMAGE PROCESS, V28, P4401, DOI 10.1109/TIP.2019.2908802; Liu D, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P842; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006; Liu YH, 2018, IEEE T NEUR NET LEAR, V29, P4983, DOI [10.1109/TSMC.2018.2867061, 10.1109/TNNLS.2017.2785278]; Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388; McCartney E. J., 1976, OPTICS ATMOSPHERE SC, P421; Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726; PEI Y, 2018, ARXIV181005716; Qian YW, 2018, IEEE INT CONF COMM; Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren WH, 2017, PROC CVPR IEEE, P2838, DOI 10.1109/CVPR.2017.303; Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8; Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343; Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178; Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10; Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Santhaseelan V, 2015, INT J COMPUT VISION, V112, P71, DOI 10.1007/s11263-014-0759-8; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Sheng H, 2020, IEEE INTERNET THINGS, V7, P9611, DOI 10.1109/JIOT.2020.2980549; Sun SH, 2014, IEEE IMAGE PROC, P4482, DOI 10.1109/ICIP.2014.7025909; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tokuda EK, 2020, ACM TRANS SPAT ALGOR, V6, DOI 10.1145/3397575; VidalMata RG, 2021, IEEE T PATTERN ANAL, V43, P4272, DOI 10.1109/TPAMI.2020.2996538; Wang T, 2019, IEEE IPCCC; Wang ZY, 2016, PROC CVPR IEEE, P4792, DOI 10.1109/CVPR.2016.518; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400; Xianhui Zheng, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P258, DOI 10.1007/978-3-642-42051-1_33; XU Q, 2020, C ART INT; Yan S, 2016, ABS160907769 CORR, V2; Yang WH, 2020, IEEE T IMAGE PROCESS, V29, P5737, DOI 10.1109/TIP.2020.2981922; Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183; Yasarla R, 2020, PROC CVPR IEEE, P2723, DOI 10.1109/CVPR42600.2020.00280; You S, 2016, IEEE T PATTERN ANAL, V38, P1721, DOI 10.1109/TPAMI.2015.2491937; Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407; Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079; Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300; Zhou X, 2019, PSYCHOL HEALTH, V34, P811, DOI 10.1080/08870446.2019.1574348; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	84	23	24	4	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1301	1322		10.1007/s11263-020-01416-w	http://dx.doi.org/10.1007/s11263-020-01416-w		JAN 2021	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000613054200002
J	Tavakolian, M; Hadid, A				Tavakolian, Mohammad; Hadid, Abdenour			A Spatiotemporal Convolutional Neural Network for Automatic Pain Intensity Estimation from Facial Dynamics	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Convolutional neural network; Facial dynamics; Pain intensity estimation; Cross-architecture knowledge transfer; Healthcare		Devising computational models for detecting abnormalities reflective of diseases from facial structures is a novel and emerging field of research in automatic face analysis. In this paper, we focus on automatic pain intensity estimation from faces. This has a paramount potential diagnosis values in healthcare applications. In this context, we present a novel 3D deep model for dynamic spatiotemporal representation of faces in videos. Using several convolutional layers with diverse temporal depths, our proposed model captures a wide range of spatiotemporal variations in the faces. Moreover, we introduce a cross-architecture knowledge transfer technique for training 3D convolutional neural networks using a pre-trained 2D architecture. This strategy is a practical approach for training 3D models, especially when the size of the database is relatively small. Our extensive experiments and analysis on two benchmarking and publicly available databases, namely the UNBC-McMaster shoulder pain and the BioVid, clearly show that our proposed method consistently outperforms many state-of-the-art methods in automatic pain intensity estimation.	[Tavakolian, Mohammad; Hadid, Abdenour] Univ Oulu, Ctr Machine Vis & Signal Anal CMVS, Oulu, Finland	University of Oulu	Tavakolian, M (corresponding author), Univ Oulu, Ctr Machine Vis & Signal Anal CMVS, Oulu, Finland.	mohammad.tavakolian@oulu.fi		Tavakolian, Mohammad/0000-0003-3004-8809	University of Oulu; Academy of Finland; Infotech Oulu; Nokia Foundation; Tauno Tonning Foundation; Oulu University Hospital	University of Oulu; Academy of Finland(Academy of Finland); Infotech Oulu; Nokia Foundation(Nokia Corporation); Tauno Tonning Foundation; Oulu University Hospital	Open access funding provided by University of Oulu including Oulu University Hospital. The financial support of the Academy of Finland, Infotech Oulu, Nokia Foundation, and Tauno Tonning Foundation is acknowledged.	ALHARBI S, 2017, IEEE IPCCC, DOI DOI 10.1109/TCYB.2017.2662199; Ashraf AB, 2009, IMAGE VISION COMPUT, V27, P1788, DOI 10.1016/j.imavis.2009.05.007; Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26; Brahnam S, 2006, ARTIF INTELL MED, V36, P211, DOI 10.1016/j.artmed.2004.12.003; Brahnam S, 2007, STUD COMPUT INTELL, V48, P225; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Ekman P., 1978, FACIAL ACTION CODING; FLOR H, 1995, BEHAV THER, V26, P255, DOI 10.1016/S0005-7894(05)80105-4; Florea C., 2014, EUR C COMP VIS, P778; Gholami B, 2010, IEEE T BIO-MED ENG, V57, P1457, DOI 10.1109/TBME.2009.2039214; Grunau R. V. E., 2011, HDB PAIN ASSESSMENT; Guo YM, 2012, PATTERN RECOGN, V45, P3834, DOI 10.1016/j.patcog.2012.04.003; Hammal Z, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P47, DOI 10.1145/2388676.2388688; Hammal Z, 2012, PATTERN RECOGN, V45, P1265, DOI 10.1016/j.patcog.2011.09.014; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Kaltwang S, 2012, LECT NOTES COMPUT SC, V7432, P368, DOI 10.1007/978-3-642-33191-6_36; Lesage FX, 2012, OCCUP MED-OXFORD, V62, P600, DOI 10.1093/occmed/kqs140; Littlewort GC, 2009, IMAGE VISION COMPUT, V27, P1797, DOI 10.1016/j.imavis.2008.12.010; Lucey P., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462; Lucey P, 2012, IMAGE VISION COMPUT, V30, P197, DOI 10.1016/j.imavis.2011.12.003; Lucey P, 2011, IEEE T SYST MAN CY B, V41, P664, DOI 10.1109/TSMCB.2010.2082525; Lynch M, 2001, J Intraven Nurs, V24, P85; Prkachin KM, 2008, PAIN, V139, P267, DOI 10.1016/j.pain.2008.04.010; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tavakolian M., 2018, EUR C COMP VIS ECCV; Thevenot J, 2017, IEEE J BIOMED HEALTH, P99, DOI [10.1109/JBHI.2017.2754861, DOI 10.1109/JBHI.2017.2754861]; Tran Du, 2017, ARXIV170805038; Walter Steffen, 2013, 2013 IEEE International Conference on Cybernetics (CYBCO), P128, DOI 10.1109/CYBConf.2013.6617456; Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668; WERNER P, 2013, BMVC, DOI DOI 10.5244/C.27.119; Werner P, 2017, IEEE T AFFECT COMPUT, V8, P286, DOI 10.1109/TAFFC.2016.2537327; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Yang R., 2016, 2016 6 INT C IMAGE P, P1; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang W, 2018, IEEE CONF COMPUT; Zhao R, 2016, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2016.377; Zhou J, 2016, IEEE COMPUT SOC CONF, P1535, DOI 10.1109/CVPRW.2016.191	44	23	23	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1413	1425		10.1007/s11263-019-01191-3	http://dx.doi.org/10.1007/s11263-019-01191-3			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL		Green Published, hybrid			2022-12-18	WOS:000485320300002
J	Xie, GS; Zhang, XY; Yan, SC; Liu, CL				Xie, Guo-Sen; Zhang, Xu-Yao; Yan, Shuicheng; Liu, Cheng-Lin			SDE: A Novel Selective, Discriminative and Equalizing Feature Representation for Visual Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Convolutional Neural Network; Feature learning; Pooling; Bag of Words; Bilevel optimization	SPARSE REPRESENTATION; SCENE RECOGNITION; IMAGE; DICTIONARY; CLASSIFICATION; LOCALIZATION	Bag of Words (BoW) model and Convolutional Neural Network (CNN) are two milestones in visual recognition. Both BoW and CNN require a feature pooling operation for constructing the frameworks. Particularly, the max-pooling has been validated as an efficient and effective pooling method compared with other methods such as average pooling and stochastic pooling. In this paper, we first evaluate different pooling methods, and then propose a new feature pooling method termed as selective, discriminative and equalizing pooling (SDE). The SDE representation is a feature learning mechanism by jointly optimizing the pooled representations with the target of learning more selective, discriminative and equalizing features. We use bilevel optimization to solve the joint optimization problem. Experiments on seven benchmark datasets (including both single-label and multi-label ones) well validate the effectiveness of our framework. Particularly, we achieve the state-of-the-art fused results (mAP) of 93.21 and 93.97% on the PASCAL VOC2007 andVOC2012 datasets, respectively.	[Xie, Guo-Sen] Henan Univ Sci & Technol, Informat Engn Coll, Luoyang 471023, Peoples R China; [Xie, Guo-Sen; Zhang, Xu-Yao; Liu, Cheng-Lin] Acad Sci, Inst Automat Chinese, Natl Lab Pattern Recognit, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China; [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore; [Liu, Cheng-Lin] Univ Chinese Acad Sci, Beijing, Peoples R China; [Liu, Cheng-Lin] CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing, Peoples R China	Henan University of Science & Technology; Chinese Academy of Sciences; Institute of Automation, CAS; National University of Singapore; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Liu, CL (corresponding author), Acad Sci, Inst Automat Chinese, Natl Lab Pattern Recognit, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.; Liu, CL (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Liu, CL (corresponding author), CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing, Peoples R China.	gsxiehm@gmail.com; xyz@nlpr.ia.ac.cn; eleyans@nus.edu.sg; liucl@nlpr.ia.ac.cn	Xie, Guo-Sen/AAL-6674-2020; Yan, Shuicheng/HCI-1431-2022	Xie, Guo-Sen/0000-0002-5487-9845; 	National Basic Research Program of China (973 Program) [2012CB316302]; Strategic Priority Research Program of the CAS [XDA06040102]; National Natural Science Foundation of China (NSFC) [61403380]; Henan International Cooperation Project [152102410036]	National Basic Research Program of China (973 Program)(National Basic Research Program of China); Strategic Priority Research Program of the CAS; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Henan International Cooperation Project	This work was supported by the National Basic Research Program of China (973 Program) Grant 2012CB316302, the Strategic Priority Research Program of the CAS (Grant XDA06040102), the National Natural Science Foundation of China (NSFC) (Grant 61403380), and the Henan International Cooperation Project (Grant 152102410036).	Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128; Bo L., 2013, CVPR; BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963; Bradley D. M., 2008, NEURAL INFORM PROCES; Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47; Chatfield K, 2014, P BRIT MACH VIS C 20, P1; Chen Q, 2015, IEEE T PATTERN ANAL, V37, P13, DOI 10.1109/TPAMI.2014.2343217; Chen Q, 2012, PROC CVPR IEEE, P3426, DOI 10.1109/CVPR.2012.6248083; Cimpoi M., 2015, CVPR; Cimpoi M, 2016, INT J COMPUT VISION, V118, P65, DOI 10.1007/s11263-015-0872-3; Colson B, 2007, ANN OPER RES, V153, P235, DOI 10.1007/s10479-007-0176-2; Csurka G., 2004, VISUAL CATEGORIZATIO; Doersch C., 2013, NIPS; Donahue J., 2013, 31 INT C MACH LEARN; Dong J, 2013, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2013.112; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fanello S., 2014, CVPR; Fernando B, 2012, LECT NOTES COMPUT SC, V7572, P214, DOI 10.1007/978-3-642-33718-5_16; Gao SH, 2013, IEEE T PATTERN ANAL, V35, P92, DOI 10.1109/TPAMI.2012.63; Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Gong Y., 2014, ECCV; He K., 2014, ARXIV14064729; Jegou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Jia Y., P ACM MULT, P675; Jiang YN, 2012, LECT NOTES COMPUT SC, V7573, P730, DOI 10.1007/978-3-642-33709-3_52; Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88; Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kwitt Roland, 2012, Computer Vision - ECCV 2012. Proceedings of the 12th European Conference on Computer Vision, P359, DOI 10.1007/978-3-642-33765-9_26; Lazebnik S., 2006, P IEEE COMP SOC C CO; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Li K., 2009, CVPR09; Li L.-J., 2010, NEURAL INFORM PROCES, P1378; Li QN, 2013, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2013.115; Lin D., 2014, CVPR; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu L., 2011, ICCV; Long J., 2014, ARXIV14114038; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Mairal J, 2010, J MACH LEARN RES, V11, P19; Murray N., 2014, CVPR; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; Nilsback M. E., 2006, CVPR; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383; Parizi SN, 2012, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2012.6248001; Perronnin F., 2007, CVPR; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Quattoni A., 2009, CVPR; Razavian Ali Sharif, 2014, CVPR; Reed Scott, 2015, 2015 IEEE C COMPUTE, P1, DOI [10.1109/CVPR.2015.7298594, DOI 10.1109/CVPR.2015.7298594]; Sadeghi F, 2012, LECT NOTES COMPUT SC, V7576, P228, DOI 10.1007/978-3-642-33715-4_17; Shabou A, 2012, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2012.6248107; Shao M, 2014, IEEE INT CON MULTI, DOI 10.1109/ICME.2014.6890269; Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093; Simonyan Karen, 2015, INT C LEARN REPR; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422; van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52; Vedaldi A., 2014, ARXIV14124564; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wang J., 2010, CVPR; Wang X., 2013, P INT C MACHINE LEAR, P846; Wei Y., 2014, ARXIV PREPRINT ARXIV; Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224; Xiang SM, 2012, IEEE T NEUR NET LEAR, V23, P1738, DOI 10.1109/TNNLS.2012.2212721; Xie G, 2015, 2015 IEEE MAGNETICS CONFERENCE (INTERMAG); Xie G. S., 2014, ACCV; Xie N., 2010, CVPR; Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789; Yan S, 2011, CVPR; Yang H., 2015, ARXIV150405843 CORR; Yang J., 2009, LINEAR SPATIAL PYRAM; Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127; Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8; Ye G., 2012, CVPR; Yoo Donggeun, 2015, CVPRW; Zeiler MD, 2014, ECCV; Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277; Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zhang Z, 2017, IEEE T IMAGE PROCESS, V26, P1466, DOI 10.1109/TIP.2017.2651396; Zhang Z, 2015, IEEE ACCESS, V3, P490, DOI 10.1109/ACCESS.2015.2430359; Zheng YB, 2012, LECT NOTES COMPUT SC, V7576, P172, DOI 10.1007/978-3-642-33715-4_13; Zhou B., 2014, NIPS; Zhou X., 2010, ECCV; Zhu J., 2010, P ADV NEUR INF PROC, P2586; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zuo Z, 2014, LECT NOTES COMPUT SC, V8689, P552, DOI 10.1007/978-3-319-10590-1_36	98	23	23	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	2					145	168		10.1007/s11263-017-1007-9	http://dx.doi.org/10.1007/s11263-017-1007-9			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FC3PK					2022-12-18	WOS:000406751100003
J	Li, Y; Liu, LQ; Shen, CH; van den Hengel, A				Li, Yao; Liu, Lingqiao; Shen, Chunhua; van den Hengel, Anton			Mining Mid-level Visual Patterns with Deep CNN Activations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Mid-level visual element discovery; Pattern mining; Convolutional neural networks	CLASSIFICATION; MODEL	The purpose of mid-level visual element discovery is to find clusters of image patches that are representative of, and which discriminate between, the contents of the relevant images. Here we propose a pattern-mining approach to the problem of identifying mid-level elements within images, motivated by the observation that such techniques have been very effective, and efficient, in achieving similar goals when applied to other data types. We show that Convolutional Neural Network (CNN) activations extracted from image patches typical possess two appealing properties that enable seamless integration with pattern mining techniques. The marriage between CNN activations and a pattern mining technique leads to fast and effective discovery of representative and discriminative patterns from a huge number of image patches, from which mid-level elements are retrieved. Given the patterns and retrieved mid-level visual elements, we propose two methods to generate image feature representations. The first encoding method uses the patterns as codewords in a dictionary in a manner similar to the Bag-of-Visual-Words model. We thus label this a Bag-of-Patterns representation. The second relies on mid-level visual elements to construct a Bag-of-Elements representation. We evaluate the two encoding methods on object and scene classification tasks, and demonstrate that our approach outperforms or matches the performance of the state-of-the-arts on these tasks.	[Li, Yao; Liu, Lingqiao; Shen, Chunhua; van den Hengel, Anton] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia	University of Adelaide	Shen, CH (corresponding author), Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia.	yao.li01@adelaide.edu.au; lingqiao.liu@adelaide.edu.au; chunhua.shen@adelaide.edu.au; anton.vandenhengel@adelaide.edu.au		liu, lingqiao/0000-0003-3584-795X	ARC Future Fellowship [FT120100969]	ARC Future Fellowship(Australian Research Council)	This work was in part supported by ARC Future Fellowship (FT120100969). Y. Li and L. Liu equally contributed to this work.	Agarwal A, 2008, INT J COMPUT VISION, V78, P15, DOI 10.1007/s11263-007-0072-x; Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22; Agrawal R., 1994, P 20 INT C VER LARG, P487; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591009; Azizpour H, 2016, IEEE T PATTERN ANAL, V38, P1790, DOI 10.1109/TPAMI.2015.2500224; Bansal A., 2015, ARXIV150407284; Borgelt C, 2012, WIRES DATA MIN KNOWL, V2, P437, DOI 10.1002/widm.1074; Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29; Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Bourdev L, 2010, LECT NOTES COMPUT SC, V6316, P168, DOI 10.1007/978-3-642-15567-3_13; BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Cheng H, 2008, PROC INT CONF DATA, P169; Choi MJ, 2012, IEEE T PATTERN ANAL, V34, P240, DOI 10.1109/TPAMI.2011.119; Cimpoi M., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299007; Cimpoi M, 2016, INT J COMPUT VISION, V118, P65, DOI 10.1007/s11263-015-0872-3; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Crowley Elliot J., 2014, P BRIT MACH VIS C; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Diba A., 2016, P IEEE C COMP VIS PA; Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532; Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597; Doersch Carl, 2013, NIPS; Donggeun Yoo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P71, DOI 10.1109/CVPRW.2015.7301274; Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522; Endres I, 2013, PROC CVPR IEEE, P939, DOI 10.1109/CVPR.2013.126; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fernando B, 2014, INT J COMPUT VISION, V108, P186, DOI 10.1007/s11263-014-0700-1; Fernando B, 2013, IEEE I CONF COMP VIS, P2544, DOI 10.1109/ICCV.2013.316; Fernando B, 2012, LECT NOTES COMPUT SC, V7572, P214, DOI 10.1007/978-3-642-33718-5_16; Fouhey DF, 2015, IEEE I CONF COMP VIS, P1053, DOI 10.1109/ICCV.2015.126; Fouhey DF, 2013, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2013.421; Gao Y., 2010, P IEEE C COMP VIS PA, P317; Gilbert A., 2014, P ASIANCONFERENCE CO, P290; Gilbert A, 2011, IEEE T PATTERN ANAL, V33, P883, DOI 10.1109/TPAMI.2010.144; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384; Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26; Grahne G, 2005, IEEE T KNOWL DATA EN, V17, P1347, DOI 10.1109/TKDE.2005.166; Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5; Jain A, 2013, PROC CVPR IEEE, P2571, DOI 10.1109/CVPR.2013.332; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Jia Y., 2014, P 22 ACM INT C MULT, P675; Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lee YJ, 2013, IEEE I CONF COMP VIS, P1857, DOI 10.1109/ICCV.2013.233; Li QN, 2013, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2013.115; Li Y, 2015, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2015.7298699; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu L., 2014, P ADV NEUR INF PROC, V27, P1143; Liu LQ, 2015, PROC CVPR IEEE, P4749, DOI 10.1109/CVPR.2015.7299107; Liu LQ, 2012, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2012.6248103; Malisiewicz T., 2009, ADV NEURAL INF PROCE, V22, P1222; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Matzen K, 2015, IEEE I CONF COMP VIS, P1931, DOI 10.1109/ICCV.2015.224; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Oramas J., 2016, ARXIV160400036; Owens A, 2013, IEEE I CONF COMP VIS, P33, DOI 10.1109/ICCV.2013.461; Parizi S. N., 2015, P INT C LEARN REPR; PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Quack T, 2007, IEEE I CONF COMP VIS, P612; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Rematas K, 2015, PROC CVPR IEEE, P4867, DOI 10.1109/CVPR.2015.7299120; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shih KJ, 2015, IEEE T PATTERN ANAL, V37, P1571, DOI 10.1109/TPAMI.2014.2366122; Shrivastava A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024188; Simonyan K., 2013, NEURAL INFORM PROCES, P163; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Snoek C. G. M., 2016, COMPUTER VISION IMAG; Song HO., 2014, ADV NEURAL INFORM PR, V2, P1637; Sun J, 2016, INT J COMPUT VISION, V2, P1; Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422; Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951; Uno T., 2003, FIMI, V90; Voravuthikunchai W, 2014, PROC CVPR IEEE, P224, DOI 10.1109/CVPR.2014.36; Vreeken J, 2011, DATA MIN KNOWL DISC, V23, P169, DOI 10.1007/s10618-010-0202-x; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198; Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345; Wang X., 2013, P INT C MACHINE LEAR, P846; Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131; Wei Y, 2014, ARXIV14065726 CORR; Yao Bangpeng, 2010, CVPR, DOI DOI 10.1109/CVPR.2010.5540234; Yuan J., 2007, P IEEE C COMP VIS PA; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	99	23	24	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2017	121	3					344	364		10.1007/s11263-016-0945-y	http://dx.doi.org/10.1007/s11263-016-0945-y			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK9UY		Green Submitted			2022-12-18	WOS:000394270600002
J	Xie, JW; Hu, WZ; Zhu, SC; Wu, YN				Xie, Jianwen; Hu, Wenze; Zhu, Song-Chun; Wu, Ying Nian			Learning Sparse FRAME Models for Natural Image Patterns	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generative models; Markov random fields; Shared sparse coding	ALGORITHM; REPRESENTATIONS; DICTIONARIES	It is well known that natural images admit sparse representations by redundant dictionaries of basis functions such as Gabor-like wavelets. However, it is still an open question as to what the next layer of representational units above the layer of wavelets should be. We address this fundamental question by proposing a sparse FRAME (Filters, Random field, And Maximum Entropy) model for representing natural image patterns. Our sparse FRAME model is an inhomogeneous generalization of the original FRAME model. It is a non-stationary Markov random field model that reproduces the observed statistical properties of filter responses at a subset of selected locations, scales and orientations. Each sparse FRAME model is intended to represent an object pattern and can be considered a deformable template. The sparse FRAME model can be written as a shared sparse coding model, which motivates us to propose a two-stage algorithm for learning the model. The first stage selects the subset of wavelets from the dictionary by a shared matching pursuit algorithm. The second stage then estimates the parameters of the model given the selected wavelets. Our experiments show that the sparse FRAME models are capable of representing a wide variety of object patterns in natural images and that the learned models are useful for object classification.	[Xie, Jianwen; Hu, Wenze; Zhu, Song-Chun; Wu, Ying Nian] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Wu, YN (corresponding author), Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.	ywu@stat.ucla.edu			NSF DMS [1310391]; NSF IIS [1423305]; ONR MURI [N00014-10-1-0933]; DARPA MSEE [FA8650-11-1-7149]	NSF DMS(National Science Foundation (NSF)); NSF IIS(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research); DARPA MSEE	The work is supported by NSF DMS 1310391, NSF IIS 1423305, ONR MURI N00014-10-1-0933, DARPA MSEE FA8650-11-1-7149. We thank the three reviewers for their insightful comments and valuable suggestions that have helped us improve the presentation and the content of this paper. We are grateful to one reviewer for sharing the insights on the analysis prior models. Thanks also go to an editor of the special issue for helpful suggestions. We thank Adrian Barbu for discussions.	ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Adler A, 2013, IEEE SIGNAL PROC LET, V20, P63, DOI 10.1109/LSP.2012.2229705; Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704; Chen J, 2005, INT CONF ACOUST SPEE, P257; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1; Fei-Fei L., 2004, P COMP VIS PATT REC; Fidler S., 2008, P IEEE C COMP VIS PA; FRIEDMAN JH, 1987, J AM STAT ASSOC, V82, P249, DOI 10.2307/2289161; Geman S, 2002, Q APPL MATH, V60, P707, DOI 10.1090/qam/1939008; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Griffin G., TECHNICAL REPORT; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hoffman J., 2013, P INT C LEARN REPR; Hong Y, 2014, Q APPL MATH, V72, P373; Jhou I., 2012, P IEEE C COMP VIS PA; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lee H, 2009, P 26 ANN INT C MACH, V26, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]; Liu C, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P281, DOI 10.1109/ICCV.2001.937530; Lounici Karim, 2009, P 22 C LEARN THEOR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Marszalek M., 2007, P IEEE C COMP VIS PA; Nam S, 2013, APPL COMPUT HARMON A, V34, P30, DOI 10.1016/j.acha.2012.03.006; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Obozinski G, 2011, ANN STAT, V39, P1, DOI 10.1214/09-AOS776; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465; Ranzato M., 2010, P IEEE C COMP VIS PA; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Shekhar S., 2013, P IEEE C COMP VIS PA; Si ZZ, 2012, IEEE T PATTERN ANAL, V34, P1354, DOI 10.1109/TPAMI.2011.227; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Teh YW, 2004, J MACH LEARN RES, V4, P1235; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030; Tuytelaars T, 2010, INT J COMPUT VISION, V88, P284, DOI 10.1007/s11263-009-0271-8; Vapnik V.N., 2000, NATURE STAT LEARNING, DOI DOI 10.1007/978-1-4757-3264-1_1; Welling M., 2003, P ADV NEUR INF PROC; Wu YN, 2010, INT J COMPUT VISION, V90, P198, DOI 10.1007/s11263-009-0287-0; Xie J., 2014, P IEEE C COMP VIS PA; Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286; Younes L., 1999, STOCHASTICS STOCHAST, V65, P177, DOI DOI 10.1080/17442509908834179; Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474; Zhu L, 2008, P EUR C COMP VIS ECC; Zhu SC, 1997, NEURAL COMPUT, V9, P1627, DOI 10.1162/neco.1997.9.8.1627; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	66	23	23	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2015	114	2-3			SI		91	112		10.1007/s11263-014-0757-x	http://dx.doi.org/10.1007/s11263-014-0757-x			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CP7MJ		Green Submitted			2022-12-18	WOS:000360071900002
J	Tran, QH; Chin, TJ; Chojnacki, W; Suter, D				Quoc Huy Tran; Chin, Tat-Jun; Chojnacki, Wojciech; Suter, David			Sampling Minimal Subsets with Large Spans for Robust Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Least squares; Total least squares; Minimal subsets; Robust fitting; Hypothesis sampling	LEAST-SQUARES; MODEL	When sampling minimal subsets for robust parameter estimation, it is commonly known that obtaining an all-inlier minimal subset is not sufficient; the points therein should also have a large spatial extent. This paper investigates a theoretical basis behind this principle, based on a little known result which expresses the least squares regression as a weighted linear combination of all possible minimal subset estimates. It turns out that the weight of a minimal subset estimate is directly related to the span of the associated points. We then derive an analogous result for total least squares which, unlike ordinary least squares, corrects for errors in both dependent and independent variables. We establish the relevance of our result to computer vision by relating total least squares to geometric estimation techniques. As practical contributions, we elaborate why naive distance-based sampling fails as a strategy to maximise the span of all-inlier minimal subsets produced. In addition we propose a novel method which, unlike previous methods, can consciously target all-inlier minimal subsets with large spans.	[Quoc Huy Tran; Chin, Tat-Jun; Chojnacki, Wojciech; Suter, David] Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia; [Quoc Huy Tran; Chin, Tat-Jun; Chojnacki, Wojciech; Suter, David] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia	University of Adelaide; University of Adelaide	Chin, TJ (corresponding author), Univ Adelaide, Australian Ctr Visual Technol, N Terrace, Adelaide, SA 5005, Australia.	huy@cs.adelaide.edu.au; tatjun@gmail.com; wojtek@cs.adelaide.edu.au; dsuter@cs.adelaide.edu.au	Chojnacki, Wojciech/AAE-9875-2020	Chojnacki, Wojciech/0000-0001-7782-1956; Suter, David/0000-0001-6306-3023				[Anonymous], 2004, EMERGING TOPICS COMP; Chin T.-J., 2010, EUR C COMP VIS ECCV; Chin TJ, 2012, IEEE T PATTERN ANAL, V34, P625, DOI 10.1109/TPAMI.2011.169; Chum O., 2003, DTSCH ARBEITSGEMEINS; Chum O., 2010, AS C COMP VIS ACCV; Chum O., 2004, AS C COMP VIS ACCV; Chum O., 2005, COMPUTER VISION PATT; de Groen P. P. N., 1996, NIEUW ARCH WISK, V14, P237; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Frahm J. M., 2006, RANSAC QUASIDEGENERA; GOLUB GH, 1980, SIAM J NUMER ANAL, V17, P883, DOI 10.1137/0717073; GOLUB GH, 1987, LINEAR ALGEBRA APPL, V88-9, P317, DOI 10.1016/0024-3795(87)90114-5; Goshen L., 2008, IEEE T PATTERN ANAL; Harker M., 2006, IND C COMP VIS GRAPH; Hartley R., 2004, ROBOTICA; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; HOERL AE, 1980, COMMUN STAT B-SIMUL, V9, P315, DOI 10.1080/03610918008812157; Jacobi C. G. J., 1841, J REINE ANGEW MATH, V9, P315; KAHL F, 2005, INT C COMP VIS ICCV; Kahl F, 2008, IEEE T PATTERN ANAL, V30, P1603, DOI 10.1109/TPAMI.2007.70824; Kanazawa Y., 2004, BRIT MACH VIS C BMVC; Kemp C., 2005, INT C COMP VIS ICCV; Kukush A, 2002, COMPUT STAT DATA AN, V41, P3, DOI 10.1016/S0167-9473(02)00068-3; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Muhlich M., 1998, EUR C COMP VIS ECCV; Myatt D. R., 2002, BRIT MACH VIS C BMVC; Olsson C., 2010, COMPUTER VISION PATT; Pham T. T., 2012, COMPUTER VISION PATT; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; Scherer-Negenborn N, 2010, INT J COMPUT VISION, V89, P120, DOI 10.1007/s11263-010-0329-7; Stigler S. M., 2000, HIST STAT MEASUREMEN; SUBRAHMANYAM M, 1972, SANKHYA SER B, V34, P355; Tordoff BJ, 2005, IEEE T PATTERN ANAL, V27, P1523, DOI 10.1109/TPAMI.2005.199; Van Huffel S., 1991, TOTAL LEAST SQUARES; VANHUFFEL S, 1989, NUMER MATH, V55, P431, DOI 10.1007/BF01396047; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Wong H. S., 2011, INT C COMP VIS ICCV; Zhang ZY, 1997, IMAGE VISION COMPUT, V15, P59, DOI 10.1016/S0262-8856(96)01112-2	40	23	23	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2014	106	1					93	112		10.1007/s11263-013-0643-y	http://dx.doi.org/10.1007/s11263-013-0643-y			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	288QM					2022-12-18	WOS:000329626800005
J	Levinshtein, A; Sminchisescu, C; Dickinson, S				Levinshtein, Alex; Sminchisescu, Cristian; Dickinson, Sven			Multiscale Symmetric Part Detection and Grouping	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Perceptual grouping; Symmetry; Part detection; Segmentation	SHAPE MODELS; REPRESENTATION; RECOGNITION; SEGMENTATION; BOUNDARIES; GRAPHS	Skeletonization algorithms typically decompose an object's silhouette into a set of symmetric parts, offering a powerful representation for shape categorization. However, having access to an object's silhouette assumes correct figure-ground segmentation, leading to a disconnect with the mainstream categorization community, which attempts to recognize objects from cluttered images. In this paper, we present a novel approach to recovering and grouping the symmetric parts of an object from a cluttered scene. We begin by using a multiresolution superpixel segmentation to generate medial point hypotheses, and use a learned affinity function to perceptually group nearby medial points likely to belong to the same medial branch. In the next stage, we learn higher granularity affinity functions to group the resulting medial branches likely to belong to the same object. The resulting framework yields a skeletal approximation that is free of many of the instabilities that occur with traditional skeletons. More importantly, it does not require a closed contour, enabling the application of skeleton-based categorization systems to more realistic imagery.	[Sminchisescu, Cristian] Univ Bonn, Bonn, Germany; [Sminchisescu, Cristian] Romanian Acad, Inst Math, Bucharest, Romania	University of Bonn; Institute of Mathematics of the Romanian Academy; Romanian Academy of Sciences; University of Bucharest		babalex@cs.toronto.edu; cristian.sminchisescu@ins.uni-bonn.de; sven@cs.toronto.edu			Army Research Laboratory [W911NF-10-2-0060]; European Commission under a Marie Curie Excellence Grant [MCEXT-025481]; CNCSIS-UEFISCU [PN II- RU-RC-2/2009]; NSERC; MITACs	Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL)); European Commission under a Marie Curie Excellence Grant; CNCSIS-UEFISCU(Consiliul National al Cercetarii Stiintifice (CNCS)); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); MITACs	We thank David Fleet, Allan Jepson, and James Elder for providing valuable advice as members of the thesis committee. We also thank Yuri Boykov and Vladimir Kolmogorov for providing their parametric maxow implementation. This research was sponsored in part by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-2-0060. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either express or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein. This work was also supported by the European Commission under a Marie Curie Excellence Grant MCEXT-025481 (Cristian Sminchisescu), CNCSIS-UEFISCU under project number PN II- RU-RC-2/2009 (Cristian Sminchisescu), NSERC (Alex Levinshtein, Sven Dickinson), MITACs (Alex Levinshtein).	BIEDERMAN I, 1985, COMPUT VISION GRAPH, V32, P29, DOI 10.1016/0734-189X(85)90002-7; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BINFORD TO, 1971, P IEEE C SYST CONTR; Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154; Borenstein E, 2002, LECT NOTES COMPUT SC, V2351, P109; BRADY M, 1984, INT J ROBOT RES, V3, P36, DOI 10.1177/027836498400300302; Carreira J., 2010, IEEE INT C COMP VIS; Carreira J., 2012, IEEE T PATTERN ANAL; Cham T.-J., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P385, DOI 10.1007/BFb0015552; CHAM TJ, 1995, IMAGE VISION COMPUT, V13, P439, DOI 10.1016/0262-8856(95)99731-F; CONNELL JH, 1987, ARTIF INTELL, V31, P159, DOI 10.1016/0004-3702(87)90018-X; CROWLEY JL, 1984, IEEE T PATTERN ANAL, V6, P156, DOI 10.1109/TPAMI.1984.4767500; CROWLEY JL, 1987, IEEE T PATTERN ANAL, V9, P113, DOI 10.1109/TPAMI.1987.4767876; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kolmogorov V, 2007, IEEE I CONF COMP VIS, P644; Levinshtein A, 2005, LECT NOTES COMPUT SC, V3757, P251, DOI 10.1007/11585978_17; Levinshtein A., 2010, ECCV; Levinshtein A, 2009, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2009.5459472; Lindeberg T, 1996, PROC CVPR IEEE, P465, DOI 10.1109/CVPR.1996.517113; Lindeberg T, 2003, LECT NOTES COMPUT SC, V2695, P148; Liu TL, 1998, INT C PATT RECOG, P994, DOI 10.1109/ICPR.1998.711856; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu YX, 2009, FOUND TRENDS COMPUT, V5, P1, DOI 10.1561/0600000008; Macrini D, 2011, COMPUT VIS IMAGE UND, V115, P1187, DOI 10.1016/j.cviu.2011.03.002; Macrini D, 2011, COMPUT VIS IMAGE UND, V115, P1044, DOI 10.1016/j.cviu.2010.12.011; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9; Mori G, 2005, IEEE I CONF COMP VIS, P1417; Mori G, 2004, PROC CVPR IEEE, P326; Munoz D, 2010, ECCV; Pelillo M, 1999, IEEE T PATTERN ANAL, V21, P1105, DOI 10.1109/34.809105; PENTLAND AP, 1990, INT J COMPUT VISION, V4, P107, DOI 10.1007/BF00127812; PENTLAND AP, 1986, ARTIF INTELL, V28, P293, DOI 10.1016/0004-3702(86)90052-4; PONCE J, 1990, COMPUT VISION GRAPH, V52, P328, DOI 10.1016/0734-189X(90)90079-B; Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10; SAINTMARC P, 1993, IEEE T PATTERN ANAL, V15, P1191, DOI 10.1109/34.244680; Sala P., 2008, P 6 IEEE COMP SOC WO; Sala P., 2010, P EUR C COMP VIS ECC; Sclaroff S, 2001, IEEE T PATTERN ANAL, V23, P475, DOI 10.1109/34.922706; Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shokoufandeh A, 2005, IEEE T PATTERN ANAL, V27, P1125, DOI 10.1109/TPAMI.2005.142; Shokoufandeh A, 1999, IMAGE VISION COMPUT, V17, P445, DOI 10.1016/S0262-8856(98)00124-3; Shokoufandeh A, 2006, COMPUT VIS IMAGE UND, V103, P139, DOI 10.1016/j.cviu.2006.05.001; Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703; Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8; Stahl JS, 2008, IEEE T PATTERN ANAL, V30, P395, DOI 10.1109/TPAMI.2007.1186; YlaJaaski A, 1996, COMPUT VIS IMAGE UND, V63, P399, DOI 10.1006/cviu.1996.0031	52	23	24	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2013	104	2					117	134		10.1007/s11263-013-0614-3	http://dx.doi.org/10.1007/s11263-013-0614-3			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	189FD		Green Submitted			2022-12-18	WOS:000322251800001
J	Huynh, CP; Robles-Kelly, A; Hancock, ER				Huynh, Cong Phuoc; Robles-Kelly, Antonio; Hancock, Edwin R.			Shape and Refractive Index from Single-View Spectro-Polarimetric Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Polarisation; Shape recovery; Refractive index; Spectro-polarimetric imagery; Multispectral imagery; Hyperspectral imagery; Fresnel reflection; Dispersion equations	MATERIAL CLASSIFICATION; POLARIZATION VISION; SURFACE; REFLECTION; RECOVERY; MODEL	In this paper, we address the problem of the simultaneous recovery of the shape and refractive index of an object from a spectro-polarimetric image captured from a single view. Here, we focus on the diffuse polarisation process occuring at dielectric surfaces due to subsurface scattering and transmission from the object surface into the air. The diffuse polarisation of the reflection process is modelled by the Fresnel transmission theory. We present a method for estimating the azimuth angle of surface normals from the spectral variation of the phase of polarisation. Moreover, we estimate the zenith angle of surface normals and index of refraction simultaneously in a well-posed optimisation framework. We achieve well-posedness by introducing two additional constraints to the problem, including the surface integrability and the material dispersion equation. This yields an iterative solution which is computationally efficient due to the use of closed-form solutions for both the zenith angle and the refractive index in each iteration. To demonstrate the effectiveness of our approach, we show results of shape recovery and surface rendering for both real-world and synthetic imagery.	[Huynh, Cong Phuoc; Robles-Kelly, Antonio] Natl ICT Australia NICTA, Canberra, ACT 2601, Australia; [Robles-Kelly, Antonio] Australian Natl Univ, Res Sch Engn, Canberra, ACT 0200, Australia; [Robles-Kelly, Antonio] UNSW ADFA, Sch Inf Tech & Elect Eng, Canberra, ACT 2600, Australia; [Hancock, Edwin R.] Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England	NICTA; Australian National University; Australian Defense Force Academy; University of York - UK	Huynh, CP (corresponding author), Natl ICT Australia NICTA, Locked Bag 8001, Canberra, ACT 2601, Australia.	cong.huynh@nicta.com.au	Hancock, Edwin/N-7548-2019; Robles-Kelly, Antonio/A-2459-2009; Huynh, Cong/V-5712-2019; Hancock, Edwin R/C-6071-2008	Hancock, Edwin/0000-0003-4496-2028; Robles-Kelly, Antonio/0000-0002-2465-5971; Hancock, Edwin R/0000-0003-4496-2028	Australian Government; Digital Economy; Australian Research Council through the ICT Centre of Excellence program	Australian Government(Australian GovernmentCGIAR); Digital Economy; Australian Research Council through the ICT Centre of Excellence program(Australian Research Council)	NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.	Atkinson G, 2005, LECT NOTES COMPUT SC, V3691, P162; Atkinson GA, 2005, IEEE I CONF COMP VIS, P309; Atkinson GA, 2007, IEEE T PATTERN ANAL, V29, P2001, DOI 10.1109/TPAMI.2007.1099; Atkinson GA, 2007, LECT NOTES COMPUT SC, V4673, P466; Atkinson GA, 2006, IEEE T IMAGE PROCESS, V15, P1653, DOI 10.1109/TIP.2006.871114; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Born M., 1999, PRINCIPLES OPTICS EL, DOI [10.1017/CBO9781139644181, DOI 10.1017/CBO9781139644181]; Chen H, 1998, INT J COMPUT VISION, V28, P73, DOI 10.1023/A:1008054731537; Coleman TF, 1996, SIAM J OPTIMIZ, V6, P1040, DOI 10.1137/S1052623494240456; Denes LJ, 1998, OPT ENG, V37, P1262, DOI 10.1117/1.601962; Drbohlav O, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P581, DOI 10.1109/ICCV.2001.937570; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; Gonzalez R C, 1992, DIGITAL IMAGE PROCES; Goudail F, 2004, APPL OPTICS, V43, P274, DOI 10.1364/AO.43.000274; Gupta N, 2002, OPT ENG, V41, P1033, DOI 10.1117/1.1467936; HALL JS, 1951, J OPT SOC AM, V41, P963, DOI 10.1364/JOSA.41.000963; HARRIS SE, 1969, J OPT SOC AM, V59, P744, DOI 10.1364/JOSA.59.000744; Hawryshyn CW, 2000, PHILOS T ROY SOC B, V355, P1187, DOI 10.1098/rstb.2000.0664; Hecht E., 2015, OPTICS; Kasarova SN, 2007, OPT MATER, V29, P1481, DOI 10.1016/j.optmat.2006.07.010; Mandel L., 1995, OPTICAL COHERENCE QU, DOI DOI 10.1017/CBO9781139644105; MARSHALL NJ, 1991, PHILOS T ROY SOC B, V334, P33, DOI 10.1098/rstb.1991.0096; Miyazaki D, 2002, J OPT SOC AM A, V19, P687, DOI 10.1364/JOSAA.19.000687; Miyazaki D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P982; Miyazaki D, 2004, IEEE T PATTERN ANAL, V26, P73, DOI 10.1109/TPAMI.2004.1261080; Nayar SK, 1997, INT J COMPUT VISION, V21, P163, DOI 10.1023/A:1007937815113; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684; Rahmann S, 2001, PROC CVPR IEEE, P149; Rahmann S, 1999, P SOC PHOTO-OPT INS, V3826, P22, DOI 10.1117/12.364333; Rahmann S, 2000, INT C PATT RECOG, P538, DOI 10.1109/ICPR.2000.903602; Sadjadi FA, 2004, OPT ENG, V43, P2283, DOI 10.1117/1.1782614; SAITO M, 1999, IEEE C COMP VIS PATT, V1, P1381; SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233; Sellmeier W., 1871, ANN PHYS CHEM, V143, DOI [DOI 10.1002/ANDP.18712190612, 10.1002/andp.18712190612]; SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969; Stiles W.S., 1959, OPTICA ACTA INT J OP, V6, P1, DOI [10.1080/713826267, DOI 10.1080/713826267]; Thilak V, 2007, APPL OPTICS, V46, P7527, DOI 10.1364/AO.46.007527; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; TORRANCE KE, 1966, J OPT SOC AM, V56, P916, DOI 10.1364/JOSA.56.000916; Wolff L. B., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P387, DOI 10.1109/CVPR.1989.37876; WOLFF LB, 1994, J OPT SOC AM A, V11, P2956, DOI 10.1364/JOSAA.11.002956; WOLFF LB, 1991, IEEE T PATTERN ANAL, V13, P635, DOI 10.1109/34.85655; WOLFF LB, 1990, IEEE T PATTERN ANAL, V12, P1059, DOI 10.1109/34.61705; WOLFF LB, 1995, IMAGE VISION COMPUT, V13, P497, DOI 10.1016/0262-8856(95)94383-B; Wolff LB, 1997, IEEE T ROBOTIC AUTOM, V13, P195, DOI 10.1109/70.563642; Wolff LB, 1997, IMAGE VISION COMPUT, V15, P81, DOI 10.1016/S0262-8856(96)01123-7; Zhu Q, 2006, P 2006 IEEE COMP SOC, V2, P1839	48	23	23	1	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	1					64	94		10.1007/s11263-012-0546-3	http://dx.doi.org/10.1007/s11263-012-0546-3			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	080XV		Green Submitted			2022-12-18	WOS:000314278500004
J	Binder, A; Muller, KR; Kawanabe, M				Binder, Alexander; Mueller, Klaus-Robert; Kawanabe, Motoaki			On Taxonomies for Multi-class Image Categorization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-class object categorization; Taxonomies; Support vector machine; Structure learning		We study the problem of classifying images into a given, pre-determined taxonomy. This task can be elegantly translated into the structured learning framework. However, despite its power, structured learning has known limits in scalability due to its high memory requirements and slow training process. We propose an efficient approximation of the structured learning approach by an ensemble of local support vector machines (SVMs) that can be trained efficiently with standard techniques. A first theoretical discussion and experiments on toy-data allow to shed light onto why taxonomy-based classification can outperform taxonomy-free approaches and why an appropriately combined ensemble of local SVMs might be of high practical use. Further empirical results on subsets of Caltech256 and VOC2006 data indeed show that our local SVM formulation can effectively exploit the taxonomy structure and thus outperforms standard multi-class classification algorithms while it achieves on par results with taxonomy-based structured algorithms at a significantly decreased computing time.	[Binder, Alexander; Mueller, Klaus-Robert; Kawanabe, Motoaki] Berlin Inst Technol, Machine Learning Grp, Dep Comp Sci, D-10587 Berlin, Germany; [Binder, Alexander; Kawanabe, Motoaki] Fraunhofer FIRST, Dep Intelligent Data Anal, D-12489 Berlin, Germany	Technical University of Berlin; Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Binder, A (corresponding author), Berlin Inst Technol, Machine Learning Grp, Dep Comp Sci, Franklinstr 28-29, D-10587 Berlin, Germany.	alexander.binder@tu-berlin.de; klaus-robert.mueller@tu-berlin.de; motoaki.kawanabe@first.fraunhofer.de	Mueller, Klaus-Robert/Y-3547-2019; Muller, Klaus R/C-3196-2013	Mueller, Klaus-Robert/0000-0002-3861-7685; Binder, Alexander/0000-0001-9605-6209	Federal Ministry of Economics and Technology of Germany (BMWi) [01MQ07018]; DFG	Federal Ministry of Economics and Technology of Germany (BMWi)(Federal Ministry for Economic Affairs and Energy (BMWi)); DFG(German Research Foundation (DFG))	We would like to thank Shinichi Nakajima and Ulf Brefeld for enlightening discussions. This work was supported in part by the Federal Ministry of Economics and Technology of Germany (BMWi) under the project THESEUS, grant 01MQ07018 and by DFG.	[Anonymous], 2007, PASCAL VISUAL OBJECT; Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214; Blaschko M. B., 2009, ADV NEURAL INFORM PR; Bosch A., 2007, THESIS U GIRONA; Cai L., 2004, P C INF KNOWL MAN; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dollar P, 2008, LECT NOTES COMPUT SC, V5303, P211, DOI 10.1007/978-3-540-88688-4_16; Everingham M., 2006, PASCAL VISUAL OBJECT; Everingham M., 2008, PASCAL VISUAL OBJECT; Everingham M, 2009, PASCAL VISUAL OBJECT; Fan XD, 2005, PROC CVPR IEEE, P716; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fergus R, 2007, INT J COMPUT VISION, V71, P273, DOI 10.1007/s11263-006-8707-x; Gehler P. V., 2009, ICCV; Griffin G., 2008, IEEE C COMP VIS PATT; Griffin Gregory, 2007, CALTECH 256 OBJECT C; Har-Peled S., 2002, ADV NEURAL INFORM PR; Joachims T., 1999, ADV KERNEL METHODS S; Kishida K, 2005, PROPERTY AVERAGE PRE; Lafferty J., 2004, P INT C MACH LEARN; Lampert C. H., 2008, P 30 DAGM S PATT REC; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Marszalek M., 2008, P EUR C COMP VIS; Marszalek M., 2007, P IEEE C COMP VIS PA; Moosmann F, 2008, IEEE T PATTERN ANAL, V30, P1632, DOI 10.1109/TPAMI.2007.70822; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; Ommer B., 2006, P IEEE INT C COMP VI, P194; Ommer B, 2010, IEEE T PATTERN ANAL, V32, P501, DOI 10.1109/TPAMI.2009.22; Platt JC, 2000, ADV NEUR IN, P61; Qi G. J., 2009, P 17 ACM INT C MULT, P243; Rothe R., 2014, ICCV, DOI DOI 10.1109/ICCV.2007.4409064; Scholkopf B., 2001, LEARNING KERNELS SUP; Shahbaz Khan F., 2009, IEEE C COMP VIS ICCV; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; Tahir M., 2008, SURREYUVA SRKDA METH; Taskar B., 2004, ADV NEURAL INFORM PR; Tibshirani R, 2007, J MACH LEARN RES, V8, P637; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219; Yang L., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/DYSPAN.2008.47; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4	46	23	25	1	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2012	99	3					281	301		10.1007/s11263-010-0417-8	http://dx.doi.org/10.1007/s11263-010-0417-8			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	950NI		Bronze			2022-12-18	WOS:000304655600003
J	Liu, HR; Yang, XW; Latecki, LJ; Yan, SC				Liu, Hairong; Yang, Xingwei; Latecki, Longin Jan; Yan, Shuicheng			Dense Neighborhoods on Affinity Graph	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Nearest neighbor; Affinity graph; Semi-supervised learning; Clustering		In this paper, we study the problem of how to reliably compute neighborhoods on affinity graphs. The k-nearest neighbors (kNN) is one of the most fundamental and simple methods widely used in many tasks, such as classification and graph construction. Previous research focused on how to efficiently compute kNN on vectorial data. However, most real-world data have no vectorial representations, and only have affinity graphs which may contain unreliable affinities. Since the kNN of an object o is a set of k objects with the highest affinities to o, it is easily disturbed by errors in pairwise affinities between o and other objects, and also it cannot well preserve the structure underlying the data. To reliably analyze the neighborhood on affinity graphs, we define the k-dense neighborhood (kDN), which considers all pairwise affinities within the neighborhood, i.e., not only the affinities between o and its neighbors but also between the neighbors. For an object o, its kDN is a set kDN(o) of k objects which maximizes the sum of all pairwise affinities of objects in the set {o}a(a)kDN(o). We analyze the properties of kDN, and propose an efficient algorithm to compute it. Both theoretic analysis and experimental results on shape retrieval, semi-supervised learning, point set matching and data clustering show that kDN significantly outperforms kNN on affinity graphs, especially when many pairwise affinities are unreliable.	[Liu, Hairong; Yan, Shuicheng] Natl Univ Singapore, Singapore 117548, Singapore; [Yang, Xingwei; Latecki, Longin Jan] Temple Univ, Philadelphia, PA 19122 USA	National University of Singapore; Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple University	Liu, HR (corresponding author), Natl Univ Singapore, Singapore 117548, Singapore.	lhrbss@gmail.com; happyyxw@gmail.com; latecki@temple.edu; eleyans@nus.eud.sg	Liu, Hairong/I-6695-2012; Yan, Shuicheng/HCI-1431-2022; Zhang, JinYuan/C-1542-2010	Latecki, Longin Jan/0000-0002-5102-8244				Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348; Asahiro Y, 2002, DISCRETE APPL MATH, V121, P15, DOI 10.1016/S0166-218X(01)00243-8; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Bomze IM, 2002, J GLOBAL OPTIM, V24, P163, DOI 10.1023/A:1020209017701; Caetano TS, 2006, IEEE T PATTERN ANAL, V28, P1646, DOI 10.1109/TPAMI.2006.207; Chan TM, 1998, DISCRETE COMPUT GEOM, V20, P359, DOI 10.1007/PL00009390; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chapelle O., 2006, BENCHMARK DATA SETS; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Clauset A, 2008, NATURE, V453, P98, DOI 10.1038/nature06830; Connor M, 2010, IEEE T VIS COMPUT GR, V16, P599, DOI 10.1109/TVCG.2010.9; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Crammer K., 2008, P 25 INT C MACH LEAR, P184; Cui Yu, 2001, Proceedings of the 27th International Conference on Very Large Data Bases, P421; Denoeux T, 2008, STUD FUZZ SOFT COMP, V219, P737; Fleishman S., 2005, ACM SPECIAL INTEREST, P552; Frank A., 2010, UCI MACHINE LEARNING; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Georgescu B, 2004, IEEE T PATTERN ANAL, V26, P674, DOI 10.1109/TPAMI.2004.2; Gibson D., 2005, P 31 INT C VER LARG, P732; GOLDSTEIN M, 1972, IEEE T INFORM THEORY, V18, P627, DOI 10.1109/TIT.1972.1054888; Gupta G., 2005, P 22 INT C MACH LEAR, P273; Han EH., 2001, PAC AS C KNOWL DISC, V2035, DOI [10.1007/3-540-45357-1_9, DOI 10.1007/3-540-45357-1_9]; Horton P, 1997, ISMB-97 - FIFTH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS FOR MOLECULAR BIOLOGY, PROCEEDINGS, P147; Hu HY, 2005, BIOINFORMATICS, V21, pI213, DOI 10.1093/bioinformatics/bti1049; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jebara T., 2009, P 26 ANN INT C MACHI, P441, DOI [10.1145/1553374.1553432, DOI 10.1145/1553374.1553432]; Jebara T, 2006, LECT NOTES COMPUT SC, V4212, P679; Kolahdouzan M., 2004, P 13 INT C VER LARG, P851; Korn F, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P215; Kuhn H., 1951, P 2 BERK S MATH STAT, P481, DOI DOI 10.1007/BF01582292; Kulis B., 2004, P 10 ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Li LP, 2001, BIOINFORMATICS, V17, P1131, DOI 10.1093/bioinformatics/17.12.1131; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; MOTZKIN TS, 1965, CANADIAN J MATH, V17, P533, DOI 10.4153/CJM-1965-053-6; Ng AY, 2002, ADV NEUR IN, V14, P849; Ouyang Q, 1997, SCIENCE, V278, P446, DOI 10.1126/science.278.5337.446; Pavan M, 2007, IEEE T PATTERN ANAL, V29, P167, DOI 10.1109/TPAMI.2007.250608; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Zaki M. J., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, P283; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhu X., 2003, INT C MACH LEARN	43	23	28	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2012	98	1					65	82		10.1007/s11263-011-0496-1	http://dx.doi.org/10.1007/s11263-011-0496-1			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	919PY					2022-12-18	WOS:000302341900004
J	Rudoy, D; Zelnik-Manor, L				Rudoy, Dmitry; Zelnik-Manor, Lihi			Viewpoint Selection for Human Actions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video analysis; Viewpoint selection; Human actions; Multiple viewpoints		In many scenarios a dynamic scene is filmed by multiple video cameras located at different viewing positions. Visualizing such multi-view data on a single display raises an immediate question-which cameras capture better views of the scene? Typically, (e.g. in TV broadcasts) a human producer manually selects the best view. In this paper we wish to automate this process by evaluating the quality of a view, captured by every single camera. We regard human actions as three-dimensional shapes induced by their silhouettes in the space-time volume. The quality of a view is then evaluated based on features of the space-time shape, which correspond with limb visibility. Resting on these features, two view quality approaches are proposed. One is generic while the other can be trained to fit any preferred action recognition method. Our experiments show that the proposed view selection provide intuitive results which match common conventions. We further show that it improves action recognition results.	[Rudoy, Dmitry; Zelnik-Manor, Lihi] Technion Israel Inst Technol, EE Dept, Haifa, Israel	Technion Israel Institute of Technology	Rudoy, D (corresponding author), Technion Israel Inst Technol, EE Dept, Haifa, Israel.	dmitryr@tx.technion.ac.il	Rudoy, Dmitry/T-3351-2019					[Anonymous], 2006, IXMAS; [Anonymous], 2002, LEARNING KERNELS; Assa J., 2010, EUROGRAPHICS; Assa J., 2008, INT C COMP GRAPH INT; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; Ballan L., 2010, ACM SIGGRAPH 2010, P1; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; Bordoloi UD, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P487; Christie M, 2008, COMPUT GRAPH FORUM, V27, P2197, DOI 10.1111/j.1467-8659.2008.01181.x; El-Alfy H, 2009, IEEE INT CONF ROBOT, P3623; Feldman J, 2005, PSYCHOL REV, V112, P243, DOI 10.1037/0033-295X.112.1.243; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Goshorn R., 2007, 1 INT C DISTR SMART; Johansson G., 1973, PERCEIVING EVENTS OB; Junejo IN, 2011, IEEE T PATTERN ANAL, V33, P172, DOI 10.1109/TPAMI.2010.68; Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414; Krahnstoever N., 2008, WORKSH MULT MULT SEN; Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244; Sokolov D., 2005, P 6 INT C VIRT REAL, P67; TRAN D, 2008, P 10 EUR C COMP VI 1, P561; Vazquez PP, 2003, COMPUT GRAPH FORUM, V22, P689, DOI 10.1111/j.1467-8659.2003.00717.x; Vieira T, 2009, COMPUT GRAPH FORUM, V28, P717, DOI 10.1111/j.1467-8659.2009.01412.x	22	23	23	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2012	97	3					243	254		10.1007/s11263-011-0484-5	http://dx.doi.org/10.1007/s11263-011-0484-5			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	907VP					2022-12-18	WOS:000301447600001
J	Vogiatzis, G; Hernandez, C				Vogiatzis, George; Hernandez, Carlos			Self-calibrated, Multi-spectral Photometric Stereo for 3D Face Capture	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Photometric stereo; Multi-spectral; Faces; Motion capture; Calibration	SHAPE	This paper addresses the problem of obtaining 3d detailed reconstructions of human faces in real-time and with inexpensive hardware. We present an algorithm based on a monocular multi-spectral photometric-stereo setup. This system is known to capture high-detailed deforming 3d surfaces at high frame rates and without having to use any expensive hardware or synchronized light stage. However, the main challenge of such a setup is the calibration stage, which depends on the lights setup and how they interact with the specific material being captured, in this case, human faces. For this purpose we develop a self-calibration technique where the person being captured is asked to perform a rigid motion in front of the camera, maintaining a neutral expression. Rigidity constrains are then used to compute the head's motion with a structure-from-motion algorithm. Once the motion is obtained, a multi-view stereo algorithm reconstructs a coarse 3d model of the face. This coarse model is then used to estimate the lighting parameters with a stratified approach: In the first step we use a RANSAC search to identify purely diffuse points on the face and to simultaneously estimate this diffuse reflectance model. In the second step we apply non-linear optimization to fit a non-Lambertian reflectance model to the outliers of the previous step. The calibration procedure is validated with synthetic and real data.	[Vogiatzis, George] Aston Univ, Birmingham B4 7ET, W Midlands, England; [Hernandez, Carlos] Google, Seattle, WA 98103 USA	Aston University; Google Incorporated	Vogiatzis, G (corresponding author), Aston Univ, Birmingham B4 7ET, W Midlands, England.	g.vogiatzis@aston.ac.uk; carloshernandez@google.com		Vogiatzis, George/0000-0002-3226-0603				Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Esteban CH, 2004, COMPUT VIS IMAGE UND, V96, P367, DOI 10.1016/j.cviu.2004.03.016; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; Furukawa Y., 2009, IEEE C COMP VIS PATT; Hartley R. I., 2000, MULTIPLE VIEW GEOMET; Hernandez C., 2010, P INT S 3D DAT PROC; Hernandez C., 2008, IEEE EUR C COMP VIS; Hernandez C, 2007, IEEE I CONF COMP VIS, P873; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Image Metrics, 2002, SIGGRAPH 2008 DEM SE; Kim H., 2010, P EUR C COMP VIS; Lin YP, 2010, PROC CVPR IEEE, P1490, DOI 10.1109/CVPR.2010.5539793; Ma W., ACM T GRAPHICS, V27; Ma W.-C., 2007, P 18 EUR C REND TECH, V2007, P10; Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226; Petrov A., 1987, COGNITIVE PROCESS, P350; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Weise T., 2007, IEEE C COMP VIS PATT; WOODHAM RJ, 1994, J OPT SOC AM A, V11, P3050, DOI 10.1364/JOSAA.11.003050; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759; Zhang S, 2006, OPT ENG, V45, DOI 10.1117/1.2336196	24	23	24	2	31	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2012	97	1					91	103		10.1007/s11263-011-0482-7	http://dx.doi.org/10.1007/s11263-011-0482-7			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	897SD		Green Accepted			2022-12-18	WOS:000300675300007
J	Shang, LM; Greenspan, M				Shang, Limin; Greenspan, Michael			Real-time Object Recognition in Sparse Range Images Using Error Surface Embedding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object recognition; Range image; Iterative closest point (ICP)	REPRESENTATION; REGISTRATION; SIGNATURES	A novel object recognition algorithm is introduced to identify objects and recover their pose from sparse range data. The method is based upon comparing the 7-D error surfaces of objects in various poses, which result from the registration error function between two convolved surfaces. The objects and their pose values are encoded by a small set of feature vectors extracted from the minima of the error surfaces. The problem of object recognition is thus reduced to comparing these feature vectors to find the corresponding error surfaces between the runtime data and a preprocessed database. The algorithm, called Potential Well Space Embedding (PWSE) has been implemented and tested on both simulated and real data. The experimental results show the technique to be both effective and efficient, executing at 122 frames per second on standard hardware and with recognition rates exceeding 97% for a database of 60 objects. The performance of PWSE on the large size database was also evaluated on the Princeton Shape Benchmark containing 1,814 objects. In addition, it functions well with very sparse data, possibly comprising only hundreds of points per image, and is shown to be robust to measurement error and outliers. With some small modifications, we applied PWSE to the problem of object class recognition. In experiments with the Princeton Shape Benchmark, PWSE is able to provides better classification rates than the previous methods in terms of nearest neighbour classification.	[Shang, Limin; Greenspan, Michael] Queens Univ, Dept Elect & Comp Engn, Sch Comp, Kingston, ON, Canada	Queens University - Canada	Shang, LM (corresponding author), Queens Univ, Dept Elect & Comp Engn, Sch Comp, Kingston, ON, Canada.	2ls2@qlink.queensu.ca; michael.greenspan@queensu.ca			Natural Sciences and Engineering Research Council of Canada; MDA Space Missions	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); MDA Space Missions	This work was supported by a grant from Natural Sciences and Engineering Research Council of Canada. The authors would like to thank: MDA Space Missions for their support of this work (both technical and financial), Dr. Piotr Jasiobedzki for his support and many suggestions on this work, and the reviewers for their helpful comments.	ABRAHAM M, 2001, P 6 INT S ART INT RO, P2235; BENTLEY JL, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P187, DOI 10.1145/98524.98564; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; BLAIS G, 1995, IEEE T PATTERN ANAL, V17, P820, DOI 10.1109/34.400574; Campbell R. J., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P505, DOI 10.1109/CVPR.1999.784728; Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186; Cyr CM, 2004, INT J COMPUT VISION, V57, P5, DOI 10.1023/B:VISI.0000013088.59081.4c; Demartines P, 1997, IEEE T NEURAL NETWOR, V8, P148, DOI 10.1109/72.554199; Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224; GODIN G, 1994, P SPIE VIDEOMETRIC, V3, P279; Hjaltason GR, 2003, IEEE T PATTERN ANAL, V25, P530, DOI 10.1109/TPAMI.2003.1195989; Huttenlocher DP, 1999, IEEE T PATTERN ANAL, V21, P951, DOI 10.1109/34.790437; Johnson AE, 1997, INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P121, DOI 10.1109/IM.1997.603857; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Jost T., 2002, THESIS U NEUCHATEL; Low KL, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P171; Low KL, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P73; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; LUCK J, 2000, P IEEE INT C ROB AUT, V4, P3739; Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213; Nene SA, 1997, IEEE T PATTERN ANAL, V19, P989, DOI 10.1109/34.615448; Pope AR, 2000, INT J COMPUT VISION, V40, P149, DOI 10.1023/A:1026502202780; POPE AR, 1993, P AAAI FALL WORKSH M, P35; Quinn J. M., 2003, PARALLEL PROGRAMMING; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Schapire RE, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1401; SCHIELE B, 1996, P 13 INT C PATT REC, V2, P50; Shan Y, 2004, PROC CVPR IEEE, P121; SHANG L, 2007, 6 INT C 3D DIG IM MO; Shang LM, 2007, IEEE T PATTERN ANAL, V29, P976, DOI 10.1109/TPAMI.2007.1088; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; Simon D., 1996, THESIS CARNEGIE MELL; Skocaj D, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P171, DOI 10.1109/IM.2001.924428; SUN Y, 2003, VACUUM, V3, P33; Taati B, 2007, IEEE I CONF COMP VIS, P15; TURK M, 1991, P IEEE C COMP VIS PA, P586, DOI DOI 10.1109/CVPR.1991.139758; Wu FC, 2006, PATTERN RECOGN, V39, P1799, DOI 10.1016/j.patcog.2006.03.019; Wu Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P275, DOI 10.1109/ICCV.2001.937529; Yamany SM, 2002, IEEE T PATTERN ANAL, V24, P1105, DOI 10.1109/TPAMI.2002.1023806	39	23	25	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2010	89	2-3			SI		211	228		10.1007/s11263-009-0276-3	http://dx.doi.org/10.1007/s11263-009-0276-3			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	594OS		Green Submitted			2022-12-18	WOS:000277547600006
J	Haro, G; Randall, G; Sapiro, G				Haro, Gloria; Randall, Gregory; Sapiro, Guillermo			Translated poisson mixture model for stratification learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						manifold learning; stratification learning; clustering; dimension estimation; density estimation; translated poisson; mixture models	DIMENSION; ALGORITHM	A framework for the regularized and robust estimation of non-uniform dimensionality and density in high dimensional noisy data is introduced in this work. This leads to learning stratifications, that is, mixture of manifolds representing different characteristics and complexities in the data set. The basic idea relies on modeling the high dimensional sample points as a process of translated Poisson mixtures, with regularizing restrictions, leading to a model which includes the presence of noise. The translated Poisson distribution is useful to model a noisy counting process, and it is derived from the noise-induced translation of a regular Poisson distribution. By maximizing the log-likelihood of the process counting the points falling into a local ball, we estimate the local dimension and density. We show that the sequence of all possible local countings in a point cloud formed by samples of a stratification can be modeled by a mixture of different translated Poisson distributions, thus allowing the presence of mixed dimensionality and densities in the same data set. With this statistical model, the parameters which best describe the data, estimated via expectation maximization, divide the points in different classes according to both dimensionality and density, together with an estimation of these quantities for each class. Theoretical asymptotic results for the model are presented as well. The presentation of the theoretical framework is complemented with artificial and real examples showing the importance of regularized stratification learning in high dimensional data analysis in general and computer vision and image analysis in particular.	[Haro, Gloria] Univ Politecn Cataluna, Dept Teoria Senyal & Comunicac, Barcelona, Spain; [Randall, Gregory] Univ Republica, Inst Ingn Elect, Montevideo, Uruguay; [Sapiro, Guillermo] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN USA	Universitat Politecnica de Catalunya; Universidad de la Republica, Uruguay; University of Minnesota System; University of Minnesota Twin Cities	Haro, G (corresponding author), Univ Politecn Cataluna, Dept Teoria Senyal & Comunicac, Barcelona, Spain.	gloria@gps.tsc.upc.edu; randall@fing.edu.uy; guille@umn.edu	Haro, Gloria/D-3394-2014	Haro, Gloria/0000-0002-8194-8092	ONR; DARPA; NSF; NGA; ARO; McKnight Foundation; Juan de la Cierva Program; Institute of Mathematics and its Applications, University of Minnesota, USA	ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); NGA; ARO; McKnight Foundation; Juan de la Cierva Program; Institute of Mathematics and its Applications, University of Minnesota, USA(University of Minnesota System)	This work has been supported by ONR, DARPA, NSF, NGA, ARO, the McKnight Foundation, and the Juan de la Cierva Program. GH was a postdoctoral associate at Institute of Mathematics and its Applications, University of Minnesota, USA, while performing part of this work. We like to thank the Yale Face Database Project, the Kanatani Lab and the people involved in the GPCA website,<SUP>8</SUP> for making their data and codes publicly available. We also thank Richard Souvenir for providing us some data and his code.<SUP>9</SUP>	Ambroise C, 1998, PATTERN RECOGN LETT, V19, P919, DOI 10.1016/S0167-8655(98)00076-2; AMBROISE C, 1996, GEOENV, V1; Barbara D., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P260, DOI 10.1145/347090.347145; BELKIN M, 2002, ADV NIPS, V14; Bendich P, 2007, ANN IEEE SYMP FOUND, P536, DOI 10.1109/FOCS.2007.45; BRAND M, 2002, ADV NIPS, V16; Cao WB, 2006, INT C PATT RECOG, P920; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dey TK, 2003, DISCRETE COMPUT GEOM, V29, P419, DOI 10.1007/s00454-002-2838-9; Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2; Gionis A., 2005, P 11 ACM SIGKDD INT, P51, DOI 10.1145/1081870.1081880; Goh A., 2007, P CVPR; HARO G, 2007, P 1 WORKSH COMP ANAL; HARO G, 2006, ADV NIPS, V19; HATHAWAY RJ, 1986, STAT PROBABIL LETT, V4, P53, DOI 10.1016/0167-7152(86)90016-7; Huang K, 2004, PROC CVPR IEEE, P631; Kanatani K., 2003, P AUSTR JAP ADV WORK, P25; KEGL B, 2002, ADV NIPS, V14; KUNG SY, 2004, BIOMETRIC AUTHENTICA; Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1784, DOI 10.1109/TPAMI.2006.223; Levina E., 2005, ADV NIPS, V17; LU L, 2006, P 23 INT C MACH LEAR, V148, P593; Ma Y, 2007, IEEE T PATTERN ANAL, V29, P1546, DOI 10.1109/TP'AMI.2007.1085; MORDOHAI P, 2005, IJCAI, P798; POLITO M, 2002, ADV NIPS, V14; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Snyder D, 1975, RANDOM POINT PROCESS; Snyder Donald Lee, 1991, RANDOM POINT PROCESS, DOI DOI 10.1007/978-1-4612-3166-0; Souvenir R, 2005, IEEE I CONF COMP VIS, P648; TAKENS F, 1985, LECT NOTES MATH, V1125, P99; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tishby N., 1999, P 37 ANN ALL C COMM, P368; Vidal R, 2003, PROC CVPR IEEE, P621; VIDAL R, 2004, IEEE T PATTERN ANAL, V27; Yang A., 2006, CVPR WORKSH 25 YEARS	37	23	23	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2008	80	3					358	374		10.1007/s11263-008-0144-6	http://dx.doi.org/10.1007/s11263-008-0144-6			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	350RJ		Green Submitted			2022-12-18	WOS:000259370500005
J	Lellmann, J; Balzer, J; Rieder, A; Beyerer, J				Lellmann, J.; Balzer, J.; Rieder, A.; Beyerer, J.			Shape from Specular Reflection and optical flow	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						specular surfaces; deflectometry; Shape from Shading; level sets; optical flow; constrained gradient descent		Inferring scene geometry from a sequence of camera images is one of the central problems in computer vision. While the overwhelming majority of related research focuses on diffuse surface models, there are cases when this is not a viable assumption: in many industrial applications, one has to deal with metal or coated surfaces exhibiting a strong specular behavior. We propose a novel and generalized constrained gradient descent method to determine the shape of a purely specular object from the reflection of a calibrated scene and additional data required to find a unique solution. This data is exemplarily provided by optical flow measurements obtained by small scale motion of the specular object, with camera and scene remaining stationary. We present a non-approximative general forward model to predict the optical flow of specular surfaces, covering rigid body motion as well as elastic deformation, and allowing for a characterization of problematic points. We demonstrate the applicability of our method by numerical experiments on synthetic and real data.	[Lellmann, J.; Balzer, J.; Beyerer, J.] Univ Karlsruhe, Dept Comp Sci, Inst Tech Informat, Lehrstuhl Interakt Echtzeitsyst, D-76128 Karlsruhe, Germany; [Rieder, A.] Univ Karlsruhe, Dept Math, Inst Angew & Numer Math, D-76128 Karlsruhe, Germany	Helmholtz Association; Karlsruhe Institute of Technology; Helmholtz Association; Karlsruhe Institute of Technology	Balzer, J (corresponding author), Univ Karlsruhe, Dept Comp Sci, Inst Tech Informat, Lehrstuhl Interakt Echtzeitsyst, Kaiserstr 12, D-76128 Karlsruhe, Germany.	lellmann@math.uni-heidelberg.de; jonathan.balzer@ies.uni-karlsruhe.de; andreas.rieder@math.uni-karlsruhe.de; juergen.beyerer@iitb.fraunhofer.de	Lellmann, Jan/AAL-4077-2021	Beyerer, Jurgen/0000-0003-3556-7181; Rieder, Andreas/0000-0002-3192-2847				Aubert G, 2003, SIAM J APPL MATH, V63, P2128, DOI 10.1137/S0036139902408928; BALZER J, 2006, P SPIE OPT E; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Berthold KP Horn, 1970, SHAPE SHADING METHOD, P1; Bonfort T, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P591; BONFORT T, 2006, THESIS I NATL POLYTE; Burger M, 2003, INTERFACE FREE BOUND, V5, P301; CHARPIAT G, 2005, ICCV, V2, P1403; Chen M, 2000, ACM T GRAPHIC, V19, P246, DOI 10.1145/380666.380670; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; GOLDLCKE M, 2004, ECCV, V2, P366; Hartley R., 2003, MULTIPLE VIEW GEOMET; HICKS RA, 2004, CVPR, V2, P143; Horn B., 1986, ROBOT VISION, P1; Jehan-Besson S, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P309, DOI 10.1007/0-387-28831-7_19; Jin HL, 2003, J SCI COMPUT, V19, P267, DOI 10.1023/A:1025308109816; Kammel S, 2004, THESIS U KARLSRUHE; Kickingereder R, 2004, PROCEEDINGS OF THE FOURTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P335; LELLMANN J, 2006, THESIS U KARLSRUHE; OREN M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P740, DOI 10.1109/ICCV.1995.466864; Osher S., 2002, APPL MATH SCI, V44, P685; PRADOS E, 2007, ESAIM-MATH MODEL NUM, V40, P393; ROTH S, 2006, P IEEE C COMP VIS PA, V2, P1869; ROZENFELD S, 2007, P INT C CVPR MINN; Savarese S, 2005, INT J COMPUT VISION, V64, P31, DOI 10.1007/s11263-005-1086-x; Sethian JA, 2005, LEVEL SET METHODS FA; SOLEM J, 2005, P WORKSH VAR GEOM LE, P332; Solem JE, 2005, LECT NOTES COMPUT SC, V3459, P419; SOLEM JE, 2004, 3DPVT, P26; WERLING S, 2007, P 8 INT C OPT 3 D ME; ZISSERMAN A, 1989, IMAGE VISION COMPUT, V7, P38, DOI 10.1016/0262-8856(89)90018-8; Zolesio J.- P., 2001, SHAPES GEOMETRIES; [No title captured]	33	23	25	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2008	80	2					226	241		10.1007/s11263-007-0123-3	http://dx.doi.org/10.1007/s11263-007-0123-3			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	348DA		Green Submitted			2022-12-18	WOS:000259190000004
J	Panin, G; Knoll, A				Panin, Giorgio; Knoll, Alois			Mutual information-based 3D object tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						surface-image alignment; mutual information; nonlinear optimization; B-spline interpolation; multiresolution; 3D tracking; template matching	SIGNAL; REGISTRATION	We propose a robust methodology for 3D model-based markerless tracking of textured objects in monocular image sequences. The technique is based on mutual information maximization, a widely known criterion for multimodal image registration, and employs an efficient multiresolution strategy in order to achieve robustness while keeping fast computational time, thus achieving near real-time performance for visual tracking of complex textured surfaces.	[Panin, Giorgio; Knoll, Alois] Tech Univ Munich, Chair Robot & Embedded Syst, D-85748 Garching, Germany	Technical University of Munich	Panin, G (corresponding author), Tech Univ Munich, Chair Robot & Embedded Syst, Boltzmannstr 3, D-85748 Garching, Germany.	panin@in.tum.de; knoll@in.tum.de	Knoll, Alois/AAN-8417-2021	Knoll, Alois/0000-0003-4840-076X				Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; BLACK MJ, 1996, ECCV, V1, P329; BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061; CASCIA M, 1999, FAST RELIABLE HEAD T; COOTES TF, 1998, LNCS, V1407, P484, DOI DOI 10.1007/BFB0054760; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Duda R.O., 1973, J ROYAL STAT SOC SER; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Goldberg DE, 1989, GENETIC ALGORITHMS S; Gonzalez R. C., 2006, DIGITAL IMAGE PROCES; GORODNICHY D, 2002, INT C VIS INT ALB CA, P383; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Huber P., 1981, ROBUST STAT; Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; LU L, 2004, P 2004 C COMP VIS PA, V5, P70; Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; MATTHEWS I, 2003, CMURITR0302; NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308; Park IK, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P49; Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867; PRINCIPE J, 1999, UNSUPERVISED ADAPTIV; Shi J., 1994, IEEE C COMP VIS PATT, P593; Skrypnyk I, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P110, DOI 10.1109/ISMAR.2004.53; Thevenaz P, 2000, IEEE T IMAGE PROCESS, V9, P2083, DOI 10.1109/83.887976; TOYAMA K, 1996, INT J COMPUT VISION, V35, P45; TOYAMA K, 1998, P WORKSH PERC US INT, P49; UNSER M, 1993, IEEE T SIGNAL PROCES, V41, P834, DOI 10.1109/78.193221; UNSER M, 1993, IEEE T SIGNAL PROCES, V41, P821, DOI 10.1109/78.193220; UNSER M, 1993, IEEE T PATTERN ANAL, V15, P364, DOI 10.1109/34.206956; Unser M, 1999, IEEE SIGNAL PROC MAG, V16, P22, DOI 10.1109/79.799930; Vacchetti L, 2004, IEEE T PATTERN ANAL, V26, P1385, DOI 10.1109/TPAMI.2004.92; Viola P, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P747; WELLS W, 1996, MULTIMODAL VOLUME RE; Xiao J, 2004, PROC CVPR IEEE, P535	36	23	24	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2008	78	1					107	118		10.1007/s11263-007-0083-7	http://dx.doi.org/10.1007/s11263-007-0083-7			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	270YE					2022-12-18	WOS:000253755300007
J	Carbonetto, P; Dorko, G; Schmid, C; Kuck, H; de Freitas, N				Carbonetto, Peter; Dorko, Gyuri; Schmid, Cordelia; Kuck, Hendrik; de Freitas, Nando			Learning to recognize objects with little supervision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object recognition; scale-invariant keypoints; weakly supervised learning; data association; Bayesian analysis; Markov Chain Monte Carlo	SPARSE; SCALE	This paper shows (i) improvements over state-of-the-art local feature recognition systems, (ii) how to formulate principled models for automatic local feature selection in object class recognition when there is little supervised data, and (iii) how to formulate sensible spatial image context models using a conditional random field for integrating local features and segmentation cues (superpixels). By adopting sparse kernel methods, Bayesian learning techniques and data association with constraints, the proposed model identifies the most relevant sets of local features for recognizing object classes, achieves performance comparable to the fully supervised setting, and obtains excellent results for image classification.	[Carbonetto, Peter; Kuck, Hendrik; de Freitas, Nando] Univ British Columbia, Vancouver, BC V5Z 1M9, Canada; [Dorko, Gyuri; Schmid, Cordelia] INRIA Rhone Alpes, Grenoble, France	University of British Columbia	Carbonetto, P (corresponding author), Univ British Columbia, Vancouver, BC V5Z 1M9, Canada.	pcarbo@cs.ubc.ca						Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108; Andrews S, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P943; Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Bernardo J. M., 2000, BAYESIAN THEORY; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; Boyd S, 2004, CONVEX OPTIMIZATION; CARBONETTO P, 2004, P ECCV, V1, P350; CARBONETTO P, 2003, P WORKSH ART INT STA; CARBONETTO P, 2004, BAYESIAN LEARNING WE; Celeux G, 2000, J AM STAT ASSOC, V95, P957, DOI 10.2307/2669477; CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568; Csurka Gabriella, 2004, P ECCV INT WORKSH ST; de Freitas N., 2005, P 21 C UNC ART INT, P332; Deselaers T, 2005, PROC CVPR IEEE, P157; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Dorko G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P634; Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97; Everingham M., 2006, PASCAL VISUAL OBJECT; Fergus R, 2003, PROC CVPR IEEE, P264; Hamze F., 2004, P 20 C UNC ART INT U, P243; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Kohn R, 2001, STAT COMPUT, V11, P313, DOI 10.1023/A:1011916902934; KUCK H, 2004, P 8 EUR C COMP VIS, V3, P1; Kumar S, 2006, INT J COMPUT VISION, V68, P179, DOI 10.1007/s11263-006-7007-9; LAFFERTY J, 2001, P 18 INT C MACHINE L; Lazebnik Svetlana, 2005, IEEE T PATTERN ANAL; Leibe B, 2005, PROC CVPR IEEE, P878; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Liu JS, 1999, J AM STAT ASSOC, V94, P1264, DOI 10.2307/2669940; LIU JS, 1994, BIOMETRIKA, V81, P27, DOI 10.1093/biomet/81.1.27; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MARSDEN JE, 1999, VECTOR CALCULUS; MCFADDEN D, 1989, ECONOMETRICA, V57, P995, DOI 10.2307/1913621; Mikolajczyk K, 2003, PROC CVPR IEEE, P257; Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561; MIKOLAJCZYK K, 2004, P EUR C COMP VIS PRA, V1, P69; MILLER T, 2004, P IEEE C COMP VIS PA, V2, P848; Opelt A, 2004, LECT NOTES COMPUT SC, V3022, P71; PASULA H, 2003, ADV NEURAL INFORM PR, V15; QUATTONI A, 2005, ADV NEURAL INFORM PR, V17, P1097; Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10; Robert C. P., 2004, M CARLO STAT METHODS; Robert C.P., 1994, BAYESIAN CHOICE; ROBERT CP, 1995, STAT COMPUT, V5, P121, DOI 10.1007/BF00143942; Serre T, 2005, PROC CVPR IEEE, P994; Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407; Sivic J, 2005, IEEE I CONF COMP VIS, P370; THAM S, 2002, P 19 INT C MACH LEAR; THAM S, 2002, THESIS U MELBOURNE; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; WILLIAMOWSKI J, 2004, P CVPR WORKSH LEARNI; Winn J., 2006, CVPR; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4; Zhu X., 2003, INT C MACH LEARN	57	23	23	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2008	77	1-3					219	237		10.1007/s11263-007-0067-7	http://dx.doi.org/10.1007/s11263-007-0067-7			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	267RE					2022-12-18	WOS:000253526100013
J	Holub, AD; Welling, M; Perona, P				Holub, Alex D.; Welling, Max; Perona, Pietro			Hybrid generative-discriminative visual categorization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						machine learning; object recognition; discriminative learning; support vector machines		Learning models for detecting and classifying object categories is a challenging problem in machine vision. While discriminative approaches to learning and classification have, in principle, superior performance, generative approaches provide many useful features, one of which is the ability to naturally establish explicit correspondence between model components and scene features this, in turn, allows for the handling of missing data and unsupervised learning in clutter. We explore a hybrid generative/discriminative approach, using 'Fisher Kernels' (Jaakola, T., et al. in Advances in neural information processing systems, Vol. 11, pp. 487-493, 1999), which retains most of the desirable properties of generative methods, while increasing the classification performance through a discriminative setting. Our experiments, conducted on a number of popular benchmarks, show strong performance improvements over the corresponding generative approach. In addition, we demonstrate how this hybrid learning paradigm can be extended to address several outstanding challenges within computer vision including how to combine multiple object models and learning with unlabeled data.	[Holub, Alex D.; Perona, Pietro] CALTECH, Pasadena, CA 91125 USA; [Welling, Max] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA	California Institute of Technology; University of California System; University of California Irvine	Holub, AD (corresponding author), CALTECH, MC 136-93, Pasadena, CA 91125 USA.	holub@vision.caltech.edu; welling@ics.uci.edu						BURL MC, 1996, CVPR, P223; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; CROWLEY JL, 1984, PATTERN RECOGNITION; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dorko G, 2005, RR5497 INRIA; Fei-Fei L., 2004, COMPUTER VISION PATT; Fergus R, 2003, PROC CVPR IEEE, P264; FERGUS R, 2005, THESIS U OXFORD DEP; GOLD C, 2005, NEURAL NETWORKS; HOLUB A, 2005, COMPUTER VISION PATT; HOLUB A, 2005, INT C COMP VIS ICCV; JAAKKOLA T, 1999, P 7 INT WORKSH ART I; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Leibe B, 2004, LECT NOTES COMPUT SC, V3175, P145; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; NG A, 2002, ADV NEURAL INFORM PR, V12; Opelt A, 2004, LECT NOTES COMPUT SC, V3022, P71; Opper M, 2000, ADV NEUR IN, P311; SCHNEIDERMAN H, 2004, CVPR, P639; Schoelkopf B., 2002, LEARNING KERNELS; Seeger M, 2002, ADV NEUR IN, V14, P905; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Torralba A., 2004, COMPUTER VISION PATT; TSUDA K, 2003, ASYMPTOTIC PROPERTIE; Ullman S, 2002, NAT NEUROSCI, V5, P682, DOI 10.1038/nn870; Vapnik V.N, 1998, STAT LEARNING THEORY; Vasconcelos N, 2004, LECT NOTES COMPUT SC, V3023, P430; Wallraven C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P257; Weber M, 2000, PROC CVPR IEEE, P101, DOI 10.1109/CVPR.2000.854754	30	23	25	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2008	77	1-3					239	258		10.1007/s11263-007-0084-6	http://dx.doi.org/10.1007/s11263-007-0084-6			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	267RE					2022-12-18	WOS:000253526100014
J	Sung, JW; Kanade, T; Kim, DJ				Sung, Jaewon; Kanade, Takeo; Kim, Daijin			A unified gradient-based approach for combining ASM into AAM	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						AAM; ASM; combining AAM into ASM; gradient-based optimization; facial expression recognition	MODELS	Active Appearance Model (AAM) framework is a very useful method that can fit the shape and appearance model to the input image for various image analysis and synthesis problems. However, since the goal of the AAM fitting algorithm is to minimize the residual error between the model appearance and the input image, it often fails to accurately converge to the landmark points of the input image. To alleviate this weakness, we have combined Active Shape Models (ASM) into AAMs, in which ASMs try to find correct landmark points using the local profile model. Since the original objective function of the ASM search is not appropriate for combining these methods, we derive a gradient based iterative method by modifying the objective function of the ASM search. Then, we propose a new fitting method that combines the objective functions of both ASM and AAM into a single objective function in a gradient based optimization framework. Experimental results show that the proposed fitting method reduces the average fitting error when compared with existing fitting methods such as ASM, AAM, and Texture Constrained-ASM (TC-ASM) and improves the performance of facial expression recognition significantly.	POSTECH, Dept Comp Sci & Engn, Pohang 790784, South Korea; Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Pohang University of Science & Technology (POSTECH); Carnegie Mellon University	Sung, JW (corresponding author), POSTECH, Dept Comp Sci & Engn, San 31,Hyoja Dong, Pohang 790784, South Korea.	jwsung@postech.ac.kr; tk@cs.cmu.edu; dkiiii@postech.ac.kr						BAKER S, 2003, CMURITR0305; COOTES T, 1999, BRIT MACH VIS C; Cootes TF, 2001, PROC CVPR IEEE, P1114; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; DORNAIKA F, 2003, BRIT MACH VIS C; JANG JSR, 1997, NEURO FUZZY SOFT COM; KANADE T, 1981, INT JOINT C ART INT, V1, P674; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KUILENBURG H, 2005, EUR C MACH LEARN; Lanitis A, 1997, IEEE T PATTERN ANAL, V19, P743, DOI 10.1109/34.598231; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Scott IM, 2003, LECT NOTES COMPUT SC, V2732, P258; STEGMANN MB, 2002, INT WORKSH GEN MOD B; THODBERG, 2001, BRIT MACH VIS C; van Ginneken B, 2006, MED IMAGE ANAL, V10, P19, DOI 10.1016/j.media.2005.02.002; van Ginneken B, 2002, IEEE T MED IMAGING, V21, P924, DOI 10.1109/TMI.2002.803121; XIAOJ, 2004, INT C COMP VIS PA RE; YAN S, 2002, EUR C COMP VIS	19	23	27	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2007	75	2					297	310		10.1007/s11263-006-0034-8	http://dx.doi.org/10.1007/s11263-006-0034-8			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	207LU					2022-12-18	WOS:000249253400006
J	Elder, JH; Prince, SJD; Hou, Y; Sizintsev, M; Olevskiy, E				Elder, J. H.; Prince, S. J. D.; Hou, Y.; Sizintsev, M.; Olevskiy, E.			Pre-attentive and attentive detection of humans in wide-field scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Early Cognitive Vision Workshop	MAY 29-JUN 01, 2004	Isle Skye, SCOTLAND	ECOVISION			SURVEILLANCE; RECOGNITION; INTEGRATION; MOTION	We address the problem of localizing and obtaining high-resolution footage of the people present in a scene. We propose a biologically-inspired solution combining pre-attentive, low-resolution sensing for detection with shiftable, high-resolution, attentive sensing for confirmation and further analysis. The detection problem is made difficult by the unconstrained nature of realistic environments and human behaviour, and the low resolution of pre-attentive sensing. Analysis of human peripheral vision suggests a solution based on integration of relatively simple but complementary cues. We develop a Bayesian approach involving layered probabilistic modeling and spatial integration using a flexible norm that maximizes the statistical power of both dense and sparse cues. We compare the statistical power of several cues and demonstrate the advantage of cue integration. We evaluate the Bayesian cue integration method for human detection on a labelled surveillance database and find that it outperforms several competing methods based on conjunctive combinations of classifiers (e.g., Adaboost). We have developed a real-time version of our pre-attentive human activity sensor that generates saccadic targets for an attentive foveated vision system. Output from high-resolution attentive detection algorithms and gaze state parameters are fed back as statistical priors and combined with pre-attentive cues to determine saccadic behaviour. The result is a closed-loop system that fixates faces over a 130 deg field of view, allowing high-resolution capture of facial video over a large dynamic scene.	York Univ, Ctr Vis Res, Toronto, ON M3J 1P3, Canada	York University - Canada	Elder, JH (corresponding author), York Univ, Ctr Vis Res, Toronto, ON M3J 1P3, Canada.							ABRAMSON Y, 2005, SEMIAUTOMATIC VISUAL; BOSE B, 2004, P CVPR, V2, P181; BUXTON H, 1995, ARTIF INTELL, V78, P431, DOI 10.1016/0004-3702(95)00041-0; COX DW, 1994, CYTOGENET CELL GENET, V66, P2; Elder James H., 2005, P624, DOI 10.1016/B978-012375731-9/50106-3; Elder JH, 2003, IEEE T PATTERN ANAL, V25, P661, DOI 10.1109/TPAMI.2003.1201818; Friedman N., 1997, P C UNC ART INT AUG, P175, DOI DOI 10.1016/J.CVIU.2007.08.003; GREEN DM, 1955, SIGNAL DETECTION THE; Greiffenhagen M, 2000, PROC CVPR IEEE, P335, DOI 10.1109/CVPR.2000.854840; Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683; Hayman E, 2002, LECT NOTES COMPUT SC, V2352, P469; Hess RF, 1997, NATURE, V390, P602, DOI 10.1038/37593; Ikeda H, 2005, VISION RES, V45, P1935, DOI 10.1016/j.visres.2005.02.001; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Itti L, 2005, VIS COGN, V12, P1093, DOI 10.1080/13506280444000661; IZENMAN AJ, 1991, J AM STAT ASSOC, V86, P205, DOI 10.2307/2289732; JOHNSTON A, 1985, VISION RES, V25, P179, DOI 10.1016/0042-6989(85)90111-7; Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198; KRUPPA H, 2003, JOINT IEEE INT WORKS, P157; Lienhart R, 2002, IEEE IMAGE PROC, P900; Marchesotti L, 2003, IEEE IMAGE PROC, P681; Miller MI, 1997, IEEE T IMAGE PROCESS, V6, P157, DOI 10.1109/83.552104; Nair V, 2004, PROC CVPR IEEE, P317; Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4; ROVAMO J, 1991, VISION RES, V31, P2227, DOI 10.1016/0042-6989(91)90175-5; Scassellati B, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P969; Schneiderman H, 2004, INT J COMPUT VISION, V56, P151, DOI 10.1023/B:VISI.0000011202.85607.00; Schneiderman H, 2004, PROC CVPR IEEE, P29; SHERRAH J, 2001, P INT C COMP VIS, V2, P42; Sidenbladh H, 2003, INT J COMPUT VISION, V54, P181, DOI 10.1023/A:1023765619733; SPENGLER M, 2001, LECT NOTES COMPUTER, V2095, P93; Sullivan J, 2001, INT J COMPUT VISION, V44, P111, DOI 10.1023/A:1011818912717; TOYAMA K, 2000, 4 AS C COMP VIS; Triesch J, 2001, IEEE T PATTERN ANAL, V23, P1449, DOI 10.1109/34.977568; Triesch J, 2001, NEURAL COMPUT, V13, P2049, DOI 10.1162/089976601750399308; VELISAVLJEVIC L, 2002, J VISION, V2, P493; VELISAVLJEVIC L, 2003, J VISION, V3, pA647; Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Xiong QR, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P191, DOI 10.1109/AVSS.2003.1217921; Zhao T, 2004, IEEE T PATTERN ANAL, V26, P1208, DOI 10.1109/TPAMI.2004.73	41	23	25	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2007	72	1					47	66		10.1007/s11263-006-8892-7	http://dx.doi.org/10.1007/s11263-006-8892-7			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	122QR					2022-12-18	WOS:000243242000004
J	Ardon, R; Cohen, LD				Ardon, Roberto; Cohen, Laurent D.			Fast constrained surface extraction by minimal paths	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	2nd IEEE Workshop on Variational, Geometric and Level Set Methods held in Conjunction with the IEEE International Conference on Computer Vision	OCT, 2003	Nice, FRANCE	IEEE, French Natl Inst Res Comp Sci & Control, Siemens Corp Res, Imaging & Visualizat Dept		active surfaces; active contours; minimal paths; level set method; object extraction	ACTIVE CONTOUR MODELS; SHAPE; ALGORITHMS	In this paper we consider a new approach for single object segmentation in 3D images. Our method improves the classical geodesic active surface model. It greatly simplifies the model initialization and naturally avoids local minima by incorporating user extra information into the segmentation process. The initialization procedure is reduced to introducing 3D curves into the image. These curves are supposed to belong to the surface to extract and thus, also constitute user given information. Hence. our model finds a surface that has these curves as boundary conditions and that minimizes the integral of a potential function that corresponds to the image features. Our goal is achieved by using globally minimal paths. We approximate the surface to extract by a discrete network of paths. Furthermore, an interpolation method is used to build a mesh or an implicit representation based on the information retrieved from the network of paths. Our paper describes a fast construction obtained by exploiting the Fast Marching algorithm and a fast analytical interpolation method. Moreover, a Level set method can be used to refine the segmentation when higher accuracy is required. The algorithm has been successfully applied to 3D medical images and synthetic images.	Philips France, MEDYSIS, F-92120 Suresnes, France; Univ Paris 09, CEREMADE, UMR 7534, F-75775 Paris 16, France	Philips; Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Mathematical Sciences (INSMI); UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine	Ardon, R (corresponding author), Philips France, MEDYSIS, 51 Rue Carnot, F-92120 Suresnes, France.	roberto.ardon@centraliens.net; cohen@ceremade.dauphine.fr						ARDON R, 2003, 2 IEEE WORKSH VAR GE, P233; BRUCKSTEIN AM, 1988, COMPUT VISION GRAPH, V44, P139, DOI 10.1016/S0734-189X(88)80002-1; Caselles V, 1997, IEEE T PATTERN ANAL, V19, P394, DOI 10.1109/34.588023; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; COHEN L, 2001, J MATH IMAGING VISIO, V14; Cohen L.D., 1997, CURVES SURFACES APPL, P77; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; Cremers D, 2003, IMAGE VISION COMPUT, V21, P77, DOI 10.1016/S0262-8856(02)00128-2; Deschamps T, 2001, MED IMAGE ANAL, V5, P281, DOI 10.1016/S1361-8415(01)00046-9; FRITSCH FN, 1980, SIAM J NUMER ANAL, V17, P238, DOI 10.1137/0717021; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; PARAGIOS NK, 2000, THESIS U NICE SOPHIA; SETHIAN J, 1999, LEVEL SET METHODS EV, P33018; SETHIAN JA, 1996, P NAT ACAD SCI, V93, P4; Tsai A, 2003, IEEE T MED IMAGING, V22, P137, DOI 10.1109/TMI.2002.808355; TSITSIKLIS JN, 1995, IEEE T AUTOMAT CONTR, V40, P1528, DOI 10.1109/9.412624; YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169; Zhao HK, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P194, DOI 10.1109/VLSM.2001.938900	20	23	25	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	1					127	136		10.1007/s11263-006-6850-z	http://dx.doi.org/10.1007/s11263-006-6850-z			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	063QH		Green Submitted			2022-12-18	WOS:000239034100010
J	Wills, J; Agarwal, S; Belongie, S				Wills, Josh; Agarwal, Sameer; Belongie, Serge			A feature-based approach for dense segmentation and estimation of large disparity motion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion segmentation; RANSAC; Markov Random Field; layer-based motion; metric labeling problem; graph cuts; periodic motion		We present a novel framework for motion segmentation that combines the concepts of layer-based methods and feature-based motion estimation. We estimate the initial correspondences by comparing vectors of filter outputs at interest points, from which we compute candidate scene relations via random sampling of minimal subsets of correspondences. We achieve a dense, piecewise smooth assignment of pixels to motion layers using a fast approximate graphcut algorithm based on a Markov random field formulation. We demonstrate our approach on image pairs containing large inter-frame motion and partial occlusion. The approach is efficient and it successfully segments scenes with inter-frame disparities previously beyond the scope of layer-based motion segmentation methods. We also present an extension that accounts for the case of non-planar motion, in which we use our planar motion segmentation results as an initialization for a regularized Thin Plate Spline fit. In addition, we present applications of our method to automatic object removal and to structure from motion.	Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Wills, J (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA.	josh@cs.ucsd.edu; sagarwal@cs.ucsd.edu; sjb@cs.ucsd.edu		Belongie, Serge/0000-0002-0388-5217				AYER S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P777, DOI 10.1109/ICCV.1995.466859; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BELONGIE S, 2004, SPAT COHERENCE VISUA; Black MJ, 1996, IEEE T PATTERN ANAL, V18, P972, DOI 10.1109/34.541407; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; BOYKOV Y, 1999, IEEE INT WORKSH EN M, P205; Branzei R., 2001, INT GAME THEORY REV, V3, P1; Chui HL, 2000, PROC CVPR IEEE, P44, DOI 10.1109/CVPR.2000.854733; Cutler R, 2000, IEEE T PATTERN ANAL, V22, P781, DOI 10.1109/34.868681; Darrell Trevor, 1991, P IEEE WORKSH VIS MO, P2; DONATO G, 2002, P 7 EUR C COMP VIS, V2, P531; DUCHON J, 1976, ANN SCI U CLERMONT F, V14, P19; Duchon Jean, 1977, LECT NOTES MATH, P2, DOI 10.1007/BFb0086566; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Forstner<spacing Wolfgang, 1987, ISPRS INT C FAST PRO, P2; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; HARTLEY R, 2000, MULTIPLE VIEW GENOME; IRANI M, 1999, VISION ALGORITHMS TH; JONES DG, 1992, IMAGE VISION COMPUT, V10, P699, DOI 10.1016/0262-8856(92)90015-U; KLEINBERG JM, 1999, P IEEE S FDN COMP SC; Lhuillier M, 2002, IEEE T PATTERN ANAL, V24, P1140, DOI 10.1109/TPAMI.2002.1023810; LIU Y, 2002, P 7 EUR C COMP VIS; Meinguet J., 1979, J APPL MATH PHYS, V5, P439; MIKOLAJCZYK K, 2002, ECCV, P128; Odobez JM, 1998, SIGNAL PROCESS, V66, P143, DOI 10.1016/S0165-1684(98)00003-6; POWELL MJD, 1995, COMPUTATIONAL TECHNI; SAWHNEY HS, 1993, INT J COMPUT VISION, V11, P237, DOI 10.1007/BF01469344; SEITZ SM, 1996, SIGGRAPH, P21; Smola A. J., 2000, ICML; SOATTO S, 2002, ECCV, P32; SZELISKI R, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P194, DOI 10.1109/CVPR.1994.323829; Szeliski R, 1996, IEEE T PATTERN ANAL, V18, P1199, DOI 10.1109/34.546257; TOMASI C, 1991, P IEEE WORKSH VIS MO; Torr PHS, 1998, PHILOS T R SOC A, V356, P1321, DOI 10.1098/rsta.1998.0224; TORR PHS, 1995, PROCEEDINGS OF EUROPE-CHINA WORKSHOP ON GEOMETRICAL MODELING & INVARIANTS FOR COMPUTER VISION, P118; TORR PHS, 1999, 7 INT C COMP VIS, V2, P983; Torresani L, 2001, PROC CVPR IEEE, P493; TORRESANI L, 2004, ECCV04, V2, P299; TORRESANI L, 2003, NIPS 2003; VIDAL R, 2004, P EUR C COMP VIS; Wahba G., 1990, SPLINE MODELS OBSERV; Wang J. Y. A., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P361, DOI 10.1109/CVPR.1993.341105; Weiss Y, 1997, PROC CVPR IEEE, P520, DOI 10.1109/CVPR.1997.609375; Wills J, 2003, PROC CVPR IEEE, P37; WILLS J, 2004, P EUR C COMP VIS PRA, V3, P170; XIAO J, 2004, CVPR04 WASH DC; XIAO J, 2004, P EUR C COMP VIS PRA	50	23	28	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2006	68	2					125	143		10.1007/s11263-006-6660-3	http://dx.doi.org/10.1007/s11263-006-6660-3			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	055CK		Green Submitted			2022-12-18	WOS:000238427400002
J	Pont, SC; Koenderink, JJ				Pont, SC; Koenderink, JJ			Bidirectional texture contrast function	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						texture; surface reflectance models; BRDF; BTF; contrast		Three dimensional surface corrugations on globally smooth surfaces give rise to brightness modulations of global shading patterns. We study systematic variations Of Such 3D image texture as a function of illumination and viewing geometry. The 3D texture is especially noticeable near the shadow terminator (for collimated illumination) or near the dark pole (for hemispherical diffuse illumination). We find that a simple micro-facet model, assuming locally Lambertian scattering, suffices to robustly describe texture contrast gradients of a large variety (measured and rendered textures; laboratory and field conditions) in a semi-quantitative manner. Robust statistical measures of the texture allows one to draw inferences concerning the nature of the light field (collimated to diffuse) and of surface roughness parameters, which can be used as input to the simplest BRDF models.	Helmholtz Inst, Dept Phys Man, NL-3584 CC Utrecht, Netherlands		Pont, SC (corresponding author), Helmholtz Inst, Dept Phys Man, Princetonpl 5, NL-3584 CC Utrecht, Netherlands.	s.c.pont@phys.uu.nl		Pont, Sylvia/0000-0002-9834-9600				Ashikhmin M, 2000, COMP GRAPH, P65, DOI 10.1145/344779.344814; Bomford D., 1990, ART MAKING IMPRESSIO; Born M., 1998, PRINCIPLES OPTICS; BRADY M, ANCIENT DOCUMENTS; CHANTLER MJ, 2002, P ECCV, P289; DANA KJ, 1977, P IEEE COMP SCI C CO; FLEY JD, 1990, COMPUTER GRAPHICS PR; GARDING J, 1993, IEEE T PATTERN ANAL, V15, P1202, DOI 10.1109/34.244682; Gershun A., 1939, J MATH PHYS, V18, P51, DOI [DOI 10.1002/SAPM193918151, 10.1002/sapm193918151]; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; Hogarth B., 1981, DYNAMIC LIGHT SHAPE; HORN BKP, 1989, SHAPE SHADNG; Hunter F, 1990, LIGHT SCI MAGIC INTR; Jacobs TS., 1988, LIGHT ARTIST; Koenderink JJ, 1996, J OPT SOC AM A, V13, P452, DOI 10.1364/JOSAA.13.000452; Koenderink JJ, 1999, INT J COMPUT VISION, V31, P129, DOI 10.1023/A:1008061730969; KOENDERINK JJ, 2002, UNPUB J OPTICAL SO A; KUBE P, 1988, IEEE T PATTERN ANAL, V10, P704, DOI 10.1109/34.6779; Lambert J.H., 1760, PHOTOMETRIA SIVE MEN; LEUNG T, 1997, IEEE C COMP VIS PATT; LU R, 2000, THESIS UTRECHT U; NAYAR SK, 1995, SCIENCE, V267, P1153, DOI 10.1126/science.7855592; SMITH BG, 1967, IEEE T ANTENN PROPAG, VAP15, P668, DOI 10.1109/TAP.1967.1138991; van Ginneken B, 1998, APPL OPTICS, V37, P130, DOI 10.1364/AO.37.000130; Wallert, 1999, STILL LIFES TECHNIQU; [No title captured]	26	23	24	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-MAY	2005	62	1-2					17	34						18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	866XV					2022-12-18	WOS:000224807600003
J	Nister, D				Nister, D			Untwisting a projective reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						self-calibration; auto-calibration; metric reconstruction; camera calibration; Euclidean reconstruction; cheirality; twisted pair; quasi-affine reconstruction; quasi-affine transformations; projective reconstruction	SELF-CALIBRATION	A method for upgrading a projective reconstruction to metric is presented. The method compares favourably to state of the art algorithms and has been found extremely reliable for both large and small reconstructions in a large number of experiments on real data. The notion of a twisted pair is generalized to the uncalibrated case. The reconstruction is first transformed by considering cheirality so that it does not contain any twisted pairs. It is argued that this is essential, since the method then proceeds by local perturbation. As is preferrable, we utilize a cost function that reflects the prior likelihood of calibration matrices well. It is shown that with any such cost function, there is little hope of untwisting a twisted pair by local perturbation. The results show that in practice it is most often sufficient to start with a reconstruction that is free from twisted pairs, provided that the minimized objective function is a geometrically meaningful quantity. When subjected to the common degeneracy of little or no rotation between the views, the proposed method still yields a very reasonable member of the family of possible solutions. Furthermore, the method is very fast and therefore suitable for the purpose of viewing reconstructions.	Sarnoff Corp, Princeton, NJ 08530 USA	Sarnoff Corporation	Nister, D (corresponding author), Sarnoff Corp, CN5300, Princeton, NJ 08530 USA.	dnister@sarnoff.com						Beardsley PA, 1997, INT J COMPUT VISION, V23, P235, DOI 10.1023/A:1007923216416; Bougnoux S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P790, DOI 10.1109/ICCV.1998.710808; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Hartley R. I., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P510, DOI 10.1109/ICCV.1999.791264; HARTLEY RI, 1994, LNCS SERIES, V825, P237; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hartman HJ, 1998, INSTR SCI, V26, P1, DOI 10.1023/A:1003023628307; HEYDEN A, 1999, P 7 INT C COMP VIS, V1, P350; LAVEAU S, 1996, SPRINGER LECT NOTES, V1064, P147; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; Nister D, 2003, PROC CVPR IEEE, P195; Nister D, 2000, LECT NOTES COMPUT SC, V1842, P649; Nister D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P116, DOI 10.1109/ICCV.2001.937612; Nister D., 2001, THESIS ROYAL I TECHN; NISTER D, 2001, SPRINGER LECT NOTES, V2018, P17; POLLEFEYS M, 1999, SELF CALIBRATION MET; Press WH, 1988, NUMERICAL RECIPES C; Robert L., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P540, DOI 10.1109/ICCV.1993.378165; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; Stolfi J., 1991, ORIENTED PROJECTIVE; STURM P, 1999, P BRIT MACH VIS C NO, P63; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; WERNER T, 1998, PATTERN RECOGN, P245; Zisserman A, 1998, PHILOS T R SOC A, V356, P1193, DOI 10.1098/rsta.1998.0217	27	23	25	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2004	60	2					165	183		10.1023/B:VISI.0000029667.76852.a1	http://dx.doi.org/10.1023/B:VISI.0000029667.76852.a1			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	841YE					2022-12-18	WOS:000222967700004
J	Tsotsos, JK				Tsotsos, JK			Motion understanding: Task-directed attention and representations that link perception with action	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion understanding; visual attention; task guidance; biological vision; representations	SELECTIVE VISUAL-ATTENTION; SPATIAL ATTENTION; AREA MT; EXTRASTRIATE CORTEX; FUNCTIONAL MRI; MECHANISMS; SEARCH; COMPLEXITY; MODULATION; NEURONS	This short paper outlines my position on a future direction for computational research on visual motion understanding. The direction combines motion perception, visual attention, action representation and computational vision. Due the breadth of literature in these areas, the paper cannot present a comprehensive review of any one topic. The review is a selective one, a selection that attempts to make some particular points. I claim that task-directed attentive processing is a largely unexplored dimension in the computational motion field. I recount in the context of motion understanding a past argument that in order to make vision systems general, attention is one of the components of the strategy. No matter how sophisticated the methods become for extracting motion information from image sequences, it will not be possible to achieve the goal of human-like performance without integrating the optimization of processing that attention provides. Virtually all past surveys of computational models of motion processing completely ignore attention. However, the concept has crept into work over the years in a variety of ways. A second claim is that the biology of attention offers some interesting insights to guide future development. Many computational authors had previously commented that too little is known about how biological vision systems use task-directed attention in motion processing; this is no longer true. Here, I briefly summarize biological evidence that attentive processing affects all aspects of visual perception including motion, and again emphasize that this paper does not do justice to the breadth and depth of the field. New findings provide a critical link between the perception of visual actions and their execution. Together these findings point to a strategy for motion understanding closely related to that presented more than two decades ago.	York Univ, Dept Comp Sci, Toronto, ON M3J 2R7, Canada; York Univ, Ctr Vis Res, Toronto, ON M3J 2R7, Canada	York University - Canada; York University - Canada	Tsotsos, JK (corresponding author), York Univ, Dept Comp Sci, Toronto, ON M3J 2R7, Canada.		Tsotsos, John K/G-3436-2011; Tsotsos, John/N-1131-2019	Tsotsos, John/0000-0002-8621-9147				Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744; Aggarwal JK, 1998, COMPUT VIS IMAGE UND, V70, P142, DOI 10.1006/cviu.1997.0620; ALBRIGHT TD, 1995, P NATL ACAD SCI USA, V92, P2433, DOI 10.1073/pnas.92.7.2433; Allport D. A., 1989, FDN COGNITIVE SCI, P631; ALOIMONOS Y, 1987, INT J COMPUT VISION, V1, P333; Aloimonos Y, 1993, ACTIVE PERCEPTION; BADLER N, 1975, THESIS U TORONTO; Bahcall DO, 1999, VISION RES, V39, P71, DOI 10.1016/S0042-6989(98)00090-X; Bajcsy R., 1985, P IEEE WORKSHOP COMP, P55; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; Baloch AA, 1997, VISION RES, V37, P3037, DOI 10.1016/S0042-6989(97)00103-X; Baluja S, 1997, ROBOT AUTON SYST, V22, P329, DOI 10.1016/S0921-8890(97)00046-8; Beauchamp MS, 1997, J NEUROPHYSIOL, V78, P516, DOI 10.1152/jn.1997.78.1.516; BLAKE A, 1992, HDB BRAIN THEORY NEU, P61; Brefczynski JA, 1999, NAT NEUROSCI, V2, P370, DOI 10.1038/7280; BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032; BUBICK A, 1997, ROYAL SOC WORKSH KNO; Buchel C, 1998, BRAIN, V121, P1281, DOI 10.1093/brain/121.7.1281; BURT PS, 1988, P 11 INT C PATT REC, P977; Caputo G, 1998, VISION RES, V38, P669, DOI 10.1016/S0042-6989(97)00189-2; CAVANAGH P, 1992, SCIENCE, V257, P1563, DOI 10.1126/science.1523411; CAVANAGH P, 2000, IN PRESS COGNITION; CEDRAS C, 1995, IMAGE VISION COMPUT, V13, P129, DOI 10.1016/0262-8856(95)93154-K; CEDRAS C, 1994, IEEE CVPR 94, P214; Chelazzi L, 1998, J NEUROPHYSIOL, V80, P2918, DOI 10.1152/jn.1998.80.6.2918; DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev.neuro.18.1.193; DICKMANNS E, 1992, ACTIVE VISION, P303; DICKMANNS ED, 1999, HDB COMPUTER VISION, V2; Dreschler L. S., 1982, Proceedings of the 6th International Conference on Pattern Recognition, P542; Fadiga L, 2000, INT J PSYCHOPHYSIOL, V35, P165, DOI 10.1016/S0167-8760(99)00051-3; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; Gandhi SP, 1999, P NATL ACAD SCI USA, V96, P3314, DOI 10.1073/pnas.96.6.3314; Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716; Goodale M.A., 1990, VISION ACTION CONTRO, P163; HILDRETH EC, 1995, HDB BRAIN THEORY NEU, P585; Hoffman JE, 1998, ATTENTION, P119; Humphreys GW, 2001, NAT NEUROSCI, V4, P84, DOI 10.1038/82940; Joseph JS, 1997, NATURE, V387, P805, DOI 10.1038/42940; Kalman RE., 1960, J BASIC ENG-T ASME, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kastner S, 2000, ANNU REV NEUROSCI, V23, P315, DOI 10.1146/annurev.neuro.23.1.315; Kastner S, 1998, SCIENCE, V282, P108, DOI 10.1126/science.282.5386.108; KOCH C, 1985, HUM NEUROBIOL, V4, P219; LU ZL, 1995, NATURE, V377, P237, DOI 10.1038/377237a0; Mann R, 1997, COMPUT VIS IMAGE UND, V65, P113, DOI 10.1006/cviu.1996.0576; Marr D., 1982, VISION COMPUTATIONAL; METZGER W, 1974, HDB PERCEPTION, V1, P109; MORAN J, 1985, SCIENCE, V229, P782, DOI 10.1126/science.4023713; NAGEL HH, 1988, IMAGE VISION COMPUT, V6, P59, DOI 10.1016/0262-8856(88)90001-7; NAGEL HH, 1981, IMAGE SEQUENCE ANAL, P19; NOWLAN SJ, 1995, J NEUROSCI, V15, P1195; OCraven KM, 1997, NEURON, V18, P591, DOI 10.1016/S0896-6273(00)80300-1; OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700; PAHLAVAN, 1993, ACTIVE PERCEPTION, P19; Parodi P, 1998, ARTIF INTELL, V105, P47, DOI 10.1016/S0004-3702(98)00077-0; PASHLER H, 1997, P9SYCHOL ATTENTION; PINHANEZ C, 1997, 423 MIT TR MED LAB; Raymond JE, 2000, TRENDS COGN SCI, V4, P42, DOI 10.1016/S1364-6613(99)01437-0; Rees G, 1997, SCIENCE, V278, P1616, DOI 10.1126/science.278.5343.1616; RENSINK R, 1989, 8922 TR U BRIT COL D; Reynolds JH, 1999, J NEUROSCI, V19, P1736; Rizzolatti G, 1998, TRENDS NEUROSCI, V21, P188, DOI 10.1016/S0166-2236(98)01260-0; Roelfsema PR, 1998, NATURE, V395, P376, DOI 10.1038/26475; SALIN PA, 1995, PHYSIOL REV, V75, P107, DOI 10.1152/physrev.1995.75.1.107; SANDINI TR, 1993, ACTIVE PERCEPTION; Seidemann E, 1999, J NEUROPHYSIOL, V81, P1783, DOI 10.1152/jn.1999.81.4.1783; SHAH M, 1997, MOTION BASED RECOGNI; SISKIND JM, 1995, ARTIF INTELL REV, V8, P371, DOI 10.1007/BF00849726; Sperling G., 1998, HIGH LEVEL MOTION PR, P153; Styles E. A., 1997, PSYCHOL ATTENTION; Treue S, 1999, NATURE, V399, P575, DOI 10.1038/21176; Treue S, 1996, NATURE, V382, P539, DOI 10.1038/382539a0; Tsotsos J.K., 1980, IEEE PATTERN ANAL MA, P563; Tsotsos J. K., 1987, P INT C COMP VIS LON; TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9; TSOTSOS JK, 1990, BEHAV BRAIN SCI, V13, P423, DOI 10.1017/S0140525X00079577; TSOTSOS JK, 1995, ARTIF INTELL, V75, P135, DOI 10.1016/0004-3702(94)00019-W; Tsotsos JK, 2001, VISUAL ATTENTION AND CORTICAL CIRCUITS, P285; TSOTSOS JK, 1992, INT J COMPUT VISION, V7, P127, DOI 10.1007/BF00128132; TSOTSOS JK, 1987, VISION BRAIN COOPERA, P361; TSOTSOS JK, 1985, COMPUT INTELL, V1, P16; TsoTsos JK, 1980, THESIS U TORONTO; TSOTSOS JK, IN PRESS 5 COURS INT; TSOTSOS JK, 1989, P INT JOINT C ART IN, P1571; TSOTSOS JK, 1977, P 5 INT JOINT C ART, P611; ULLMAN S, 1984, VIS COGN, P97; ULLMAN S, 1995, CEREB CORTEX, V1, P1; Valdes-Sosa M, 1998, J COGNITIVE NEUROSCI, V10, P137, DOI 10.1162/089892998563743; Vanduffel W, 2000, CEREB CORTEX, V10, P109, DOI 10.1093/cercor/10.2.109; Wachter S, 1999, COMPUT VIS IMAGE UND, V74, P174, DOI 10.1006/cviu.1999.0758; Watanabe Takeo, 1998, HIGH LEVEL MOTION PR, P95; Wertheimer M, 1912, Z PSYCHOL PHYSIOL SI, V61, P161; Ye YM, 1999, COMPUT VIS IMAGE UND, V73, P145, DOI 10.1006/cviu.1998.0736; Yeshurun Y, 1999, VISION RES, V39, P293, DOI 10.1016/S0042-6989(98)00114-X; YESHURUN Y, 1997, ARTIFICIAL VISION IM, P43; [No title captured]	95	23	24	0	4	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2001	45	3					265	280		10.1023/A:1013666302043	http://dx.doi.org/10.1023/A:1013666302043			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	487KH					2022-12-18	WOS:000171873900004
J	Ratan, AL; Grimson, WEL; Wells, WM				Ratan, AL; Grimson, WEL; Wells, WM			Object detection and localization by dynamic template warping	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object recognition; object detection; localization; dynamic programming; mutual information	RECOGNITION; ALGORITHM	A simple method is presented for detecting, localizing and recognizing instances of classes of objects, while accommodating a wide variation in an object's pose. The method utilizes a small two-dimensional template that is warped into an image, and converts localization to a one-dimensional sub-problem, with the search for a match between image and template executed by dynamic programming. For roughly cylindrical objects (like heads), the method recovers three of the six degrees of freedom of motion (2 translation, 1 rotation), and accommodates two more degrees of freedom in the search process (1 rotation, 1 translation). Experiments demonstrate that the method provides an efficient search strategy that outperforms normalized correlation. This is demonstrated in the example domain of face detection and localization, and can extended to more general detection tasks. An additional technique recovers rough object pose from the match results, and is used in a two stage recognition experiment in conjunction with maximization of mutual information.	MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Ratan, AL (corresponding author), MIT, Artificial Intelligence Lab, 545 Technol Sq, Cambridge, MA 02139 USA.	aparna@ai.mit.edu; welg@ai.mit.edu; sw@ai.mit.edu						BAKER HH, 1981, P 7 INT JOINT C ART, P631; Ballard D.H., 1982, COMPUTER VISION; BARROW HG, 1976, INTERACTIVE AIDS CAR; BETKE M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P523, DOI 10.1109/ICCV.1995.466895; Beymer DJ, 1993, 1461 MIT ART INT LAB; BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061; COOTES TF, 1993, P 4 INT C COMP VIS, P242; Corman T., 1990, INTRO ALGORITHMS; FORNEY GD, 1973, P IEEE, V61, P268, DOI 10.1109/PROC.1973.9030; HORNEGGER J, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P914, DOI 10.1109/ICCV.1995.466838; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; HUTTENLOCHER DP, 1996, P EUR C COMP VIS, P537; MAHMOOD STF, 1998, P CONT BAS ACC IM VI; MURASE H, 1959, AAAI FALL S SER WORK; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; PENTLAND A, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P84, DOI 10.1109/CVPR.1994.323814; Press W. H., 1990, NUMERICAL RECIPES C; ROMANO R, 1996, ARPA IU WORKSH, V1; ROWLEY H, 1998, P COMP VIS PATT REC; Rowley H. A., 1995, CITESEER; RUCKLIDGE WJ, 1994, P INT C COMP VIS, P457; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; SCHNEIDERMAN H, 1998, P COMP VIS PATT REC; SINHA P, 1994, INVESTIGATIVE OPTHAM; Sung K. - K., 1994, 1521 AI MIT; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; Vaillant R., 1994, IEEE P VIS IM SIGN P, V141; VIOLA P, 1995, P INT C COMP VIS CAM; VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010; YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169; [No title captured]	32	23	25	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2000	36	2					131	147		10.1023/A:1008147915077	http://dx.doi.org/10.1023/A:1008147915077			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	295FD					2022-12-18	WOS:000085955300003
J	Florack, L; Maas, R; Niessen, W				Florack, L; Maas, R; Niessen, W			Pseudo-linear scale-space theory	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						fuzzy dilation; erosion; linear scale-space; morphological scale-space; reaction-diffusion	IMAGE SEGMENTATION; DIFFUSION; SHAPES	It has been observed that linear, Gaussian scale-space, and nonlinear, morphological erosion and dilation scale-spaces generated by a quadratic structuring function have a lot in common. Indeed, far-reaching analogies have been reported, which seems to suggest the existence of an underlying isomorphism. However, an actual mapping appears to be missing. In the present work a one-parameter isomorphism is constructed in closed-form, which encompasses linear and both types of morphological scale-spaces as (non-uniform) limiting cases. The unfolding of the one-parameter family provides a means to transfer known results from one domain to the other. Moreover, for any fixed and non-degenerate parameter value one obtains a novel type of "pseudo-linear" multiscale representation that is, in a precise way, "in-between" the familiar ones. This is of interest in its own right, as it enables one to balance pros and cons of linear versus morphological scale-space representations in any particular situation.	Univ Utrecht, Dept Comp Sci, Image Sci Inst, NL-3584 CH Utrecht, Netherlands; Univ Utrecht Hosp, Image Sci Inst, NL-3584 CX Utrecht, Netherlands	Utrecht University; Utrecht University; Utrecht University Medical Center	Florack, L (corresponding author), Univ Utrecht, Dept Comp Sci, Image Sci Inst, Padualaan 14, NL-3584 CH Utrecht, Netherlands.			Niessen, Wiro/0000-0002-5822-1995				BLOCH I, 1995, PATTERN RECOGN, V28, P1341, DOI 10.1016/0031-3203(94)00312-A; DERICHE R, 1992, P 2 SING INT C IM PR, P10; DERICHE R, 1993, INRIARR1893; DORST L, 1994, SIGNAL PROCESS, V38, P79, DOI 10.1016/0165-1684(94)90058-2; GOUTSIAS J, 1995, COMPUT VIS IMAGE UND, V62, P326, DOI 10.1006/cviu.1995.1058; Heijmans H.J.A.M, 1992, NIEUW ARCH WISK, V10, P237; HEIJMANS HJAM, 1991, IEEE T PATTERN ANAL, V13, P568, DOI 10.1109/34.87343; Jackway PT, 1996, IEEE T PATTERN ANAL, V18, P38, DOI 10.1109/34.476009; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; Kimia BB, 1996, COMPUT VIS IMAGE UND, V64, P305, DOI 10.1006/cviu.1996.0062; Lax P. D., 1973, HYPERBOLIC SYSTEMS C; LINDEBERG T, 1994, KLUWER INT SERIES EN; MARAGOS P, 1997, COMPUTATIONAL IMAGIN, V5; Nielsen M, 1997, J MATH IMAGING VIS, V7, P291, DOI 10.1023/A:1008282127190; Niessen WJ, 1997, COMPUT VIS IMAGE UND, V66, P233, DOI 10.1006/cviu.1997.0614; Olver P.J., 1986, GRADUATE TEXTS MATH, V107; ROMENY BMT, 1997, LECT NOTES COMPUTER, V1252; SERRA J, 1997, COMPUTATIONAL IMAGIN, V2; SERRA J., 1994, COMPUTATIONAL IMAGIN, V2; Smoller J., 2012, SHOCK WAVES REACTION, DOI [10.1007/978-1-4612-0873-0, DOI 10.1007/978-1-4612-0873-0]; SPORRING J, 1997, COMPUTATIONAL IMAGIN, V8; SRRA J, 1982, IMAGE ANAL MATH MORP; van den Boomgaard R, 1992, NIEUW ARCH WISKD, V10, P219; VANDENBOOMGAARD R, 1994, IEEE T PATTERN ANAL, V16, P1101, DOI 10.1109/34.334389; vandenBoomgaard R, 1997, COMP IMAG VIS, V8, P203; VANDERBOOMGAARD R, 1992, THESIS U AMSTERDAM; VINCKEN KL, 1994, PATTERN RECOGN LETT, V15, P477, DOI 10.1016/0167-8655(94)90139-2; Vincken KL, 1997, IEEE T PATTERN ANAL, V19, P109, DOI 10.1109/34.574787; [No title captured]; [No title captured]	31	23	23	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1999	31	2-3					247	259		10.1023/A:1008026217765	http://dx.doi.org/10.1023/A:1008026217765			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	209NA					2022-12-18	WOS:000081053100010
J	Szeliski, R; Weiss, R				Szeliski, R; Weiss, R			Robust shape recovery from occluding contours using a linear smoother	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						multiframe stereo analysis; contour tracking and matching; occluding contours; reconstruction from profiles; Kalman filtering; Kalman soothing; surface reconstruction	PLANE IMAGE-ANALYSIS; EDGE-DETECTION; SEQUENCES; MODELS; SURFACE; MOTION	Recovering the shape of an object from two views fails at occluding contours of smooth objects because the extremal contours are view dependent. For three or more views, shape recovery is possible, and several algorithms have recently been developed for this purpose. We present a new approach to the multiframe stereo problem that does not depend on differential measurements in the image, which may be noise sensitive. Instead, we use a linear smoother to optimally combine all of the measurements available at the contours (and other edges) in all of the images. This allows us to extract a robust and reasonably dense estimate of surface shape, and to integrate shape information from both surface markings and occluding contours. Results are presented, which empirically show that in the presence of noise, smoothing over more than three views reduces the error even when the epipolar curve is nonplanar.	Microsoft Corp, Res, Redmond, WA 98052 USA; Digital Equipment Corp, Shrewsbury, MA 01545 USA	Microsoft	Szeliski, R (corresponding author), Microsoft Corp, Res, 1 Microsoft Way, Redmond, WA 98052 USA.							Albert A., 1972, REGRESSION MOORE PEN; ARBORGAST E, 1992, 2 EUR C COMP VIS ECC, P467; ARNOLD RD, 1983, AIM351 STANF U ART I; BAKER HH, 1989, INT J COMPUT VISION, V3, P33, DOI 10.1007/BF00054837; BAKER HH, 1989, INT J COMPUT VISION, V3, P50; Bar-Shalom Y., 1988, TRACKING DATA ASS; Bierman G.J., 1977, FACTORIZATION METHOD; BLAKE A, 1993, INT J COMPUT VISION, V11, P127, DOI 10.1007/BF01469225; BLAKE A, 1992, ACTIVE VISION, P175; BLAKE A, 1990, 1 EUR C COMP VIS ECC, P465; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; Boyer E, 1997, INT J COMPUT VISION, V22, P219, DOI 10.1023/A:1007978616082; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; CIPOLLA R, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P616; Clark J.J., 1990, DATA FUSION SENSORY; CURWEN R, 1992, 2 EUR C COMP VIS ECC, P879; FAUGERAS OD, 1987, 1ST P INT C COMP VIS, P25; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Gelb A., 1974, APPL OPTIMAL ESTIMAT; GIBLIN P, 1987, 1ST P INT C COMP VIS, P136; GIBLIN PJ, 1994, 3 EUR C COMP VIS ECC, V1, P14; GIBLIN PJ, 1992, MATH SURFACES, V5; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; Huber P., 1981, ROBUST STAT; LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; MATTHIES L, 1989, INT J COMPUT VISION, V3, P209, DOI 10.1007/BF00133032; Maybeck P. S., 1982, STOCHASTIC MODELS ES; MAYHEW JEW, 1980, PERCEPTION, V9, P69, DOI 10.1068/p090069; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P715, DOI 10.1109/34.85660; Press WH, 1986, NUMERICAL RECIPES C, V818; RIVES P, 1986, C INTELLIGENT AUTOMA, P522; SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029; SZELISKI R, 1993, IEEE COMP SOC C COMP, P82; SZELISKI R, 1989, BAYESIAN MODELING UN; SZELISKI R, 1991, IEEE C COMP VIS PATT, P625; TERZOPOULOS D, 1991, IEEE T PATTERN ANAL, V13, P703, DOI 10.1109/34.85659; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; Vaillant R., 1990, 1 EUR C COMP VIS ECC, P454; ZHENG JY, 1994, IEEE T PATTERN ANAL, V16, P163, DOI 10.1109/34.273734	43	23	23	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1998	28	1					27	44		10.1023/A:1008050630628	http://dx.doi.org/10.1023/A:1008050630628			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZX971					2022-12-18	WOS:000074573800002
J	Basri, R; Jacobs, DW				Basri, R; Jacobs, DW			Recognition using region correspondences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object recognition; occlusion; affine; perspective; regions; pose estimation; uniqueness; two-dimensional; three-dimensional; model	OBJECT RECOGNITION; MOMENT INVARIANTS; 3-D OBJECTS; IMAGE; AFFINE; ALIGNMENT; MODELS; MOTION	Recognition systems attempt to recover information about the identity of observed objects and their location in the environment. A fundamental problem in recognition is pose estimation. This is the problem of using a correspondence between some portions of an object model and some portions of an image to determine whether the image contains an instance of the object, and, in case it does, to determine the transformation that relates the model to the image. The current approaches to this problem are divided into methods that use ''global'' properties of the object (e.g., centroid and moments of inertia) and methods that use ''local'' properties of the object (e.g., corners and line segments). Global properties are sensitive to occlusion and, specifically, to self occlusion. Local properties are difficult to locate reliably, and their matching involves intensive computation. We present a novel method for recognition that uses region information. In our approach the model and the image are divided into regions. Given a match between subsets of regions (without any explicit correspondence between different pieces of the regions) the alignment transformation is computed. The method applies to planar objects under similarity, affine, and projective transformations and to projections of 3-D objects undergoing affine and projective transformations. The new approach combines many of the advantages of the previous two approaches, while avoiding some of their pitfalls. Like the global methods, our approach makes use of region information that reflects the true shape of the object. But like local methods, our approach can handle occlusion.	NEC RES INST,PRINCETON,NJ 08540	NEC Corporation	Basri, R (corresponding author), WEIZMANN INST SCI,DEPT APPL MATH,IL-76100 REHOVOT,ISRAEL.							ALTER TD, 1993, P INT C COMP VIS, P113; ALTER TD, 1994, IEEE C COMP VIS PATT, P892; Amenta N., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P340, DOI 10.1145/177424.178064; [Anonymous], 1985, PERCEPTUAL ORG VISUA; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BAIRD HS, 1985, MODEL BASED IMAGE MA; BAJCSY R, 1987, 1ST P INT C COMP VIS, P231; BASRI R, 1993, CVGIP-IMAG UNDERSTAN, V57, P331, DOI 10.1006/ciun.1993.1022; Basri R, 1996, INT J COMPUT VISION, V19, P169, DOI 10.1007/BF00055803; BASRI R, 1995, IEEE INT C COMP VIS, P8; BASRI R, 1994, CS9533 TR WEIZM I SC; BIEDERMAN I, 1985, COMPUT VISION GRAPH, V32, P29, DOI 10.1016/0734-189X(85)90002-7; Binford T.O., 1971, IEEE C SYST CONTR MI; BREUEL T, 1991, IEEE C COMP VIS PATT, P257; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; CANNY J, 1983, T PATTERN RECOGNITIO, V8, P679; CASS T, 1992, 2 EUR C COMP VIS, P834; CONWAY JB, 1990, COURSE FUNCTIONAL AN; Coxeter H.S.M., 1993, REAL PROJECTIVE PLAN, V3rd; DARRELL T, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P112; Duda R.O., 1973, J ROYAL STAT SOC SER; DUDANI SA, 1977, IEEE T COMPUT, V26, P39, DOI 10.1109/TC.1977.5009272; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2; HORAUD R, 1987, IEEE T PATTERN ANAL, V9, P401, DOI 10.1109/TPAMI.1987.4767922; HU MK, 1962, IEEE T INFORM THEORY, V8, P169; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; HUTTENLOCHER DP, 1993, P 4 INT C COMP VIS, P93; JACOBS D, 1996, IEEE T PAMI, P23; Jacobs D. W., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P439, DOI 10.1109/CVPR.1992.223153; JOSHI T, 1994, IEE C COMP VIS PATT, P876; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; LAMDAN Y, 1990, IEEE T ROBOTIC AUTOM, V6, P578, DOI 10.1109/70.62047; Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995; MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020; PAVLIDIS T, 1974, IEEE T COMPUT, VC 23, P860, DOI 10.1109/T-C.1974.224041; PENTLAND A, 1987, 1ST INT C COMP VIS, P612; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P534; POELMAN C, 1994, P EUR C COMP VIS; REEVES AP, 1988, IEEE T PATTERN ANAL, V10, P937, DOI 10.1109/34.9115; RICHARD CW, 1974, IEEE T SYST MAN CYB, VSMC4, P371, DOI 10.1109/TSMC.1974.5408458; ROTHWELL C, 1992, 2ND P EUR C COMP VIS, P757; Rothwell C. A., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P109, DOI 10.1109/CVPR.1992.223219; SADJADI FA, 1980, IEEE T PATTERN ANAL, V2, P127, DOI 10.1109/TPAMI.1980.4766990; Subrahmonia J, 1996, IEEE T PATTERN ANAL, V18, P505, DOI 10.1109/34.494640; Sugimoto A, 1996, INT J COMPUT VISION, V19, P181, DOI 10.1007/BF00055804; THOMPSON DW, 1987, IEEE J ROBOTIC AUTOM, P208; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; WEISS I, 1993, INT J COMPUT VISION, V10, P207, DOI 10.1007/BF01539536	53	23	25	0	5	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1997	25	2					145	166		10.1023/A:1007919917506	http://dx.doi.org/10.1023/A:1007919917506			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YG836					2022-12-18	WOS:A1997YG83600003
J	Shekarforoush, H; Berthod, M; Zerubia, J; Werman, M				Shekarforoush, H; Berthod, M; Zerubia, J; Werman, M			Sub-pixel Bayesian estimation of albedo and height	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							STATISTICAL-ANALYSIS; REGISTRATION; IMAGES	Given a set of low resolution camera images of a Lambertian surface, it is possible to reconstruct high resolution luminance and height information, when the relative displacements of the image frames are known. We have proposed iterative algorithms for recovering high resolution albedo with the knowledge of high resolution height and vice versa. The problem of surface reconstruction has been tackled in a Bayesian framework and has been formulated as one of minimizing an error function. Markov Random Fields (MRF) have been employed to characterize the a priori constraints on the solution space. As for the surface height, we have attempted a direct computation without refering to surface orientations, while increasing the resolution by camera jittering.	HEBREW UNIV JERUSALEM, INST COMP SCI, IL-91904 JERUSALEM, ISRAEL	Hebrew University of Jerusalem	Shekarforoush, H (corresponding author), INRIA, 2004 ROUTE LUCIOLES, F-06902 SOPHIA ANTIPOLIS, FRANCE.							Andrews H.C., 1977, DIGITAL IMAGE RESTOR; AZENCOTT R, 1987, P INT C IND APPL MAT; BERTHOD M, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P654, DOI 10.1109/CVPR.1994.323784; BERTHOD M, 1993, 2142 INRIA; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; BESAG J, 1986, J R STAT SOC B, V48, P259; BOX GEP, 1951, J R STAT SOC B, V13, P1, DOI 10.1111/j.2517-6161.1951.tb00067.x; BRUSS AR, 1982, J MATH PHYS, V23, P890, DOI 10.1063/1.525441; DECASTRO E, 1987, IEEE T PATTERN ANAL, V9, P700, DOI 10.1109/TPAMI.1987.4767966; FRANKOT RT, 1988, IEEE T PAMI, V10, P431; GEIGER D, 1991, IEEE T PAMI, P617; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GROSS D, 1986, THESIS TEL AVIV U; HORN BKP, 1977, ARTIF INTELL, V8, P201, DOI 10.1016/0004-3702(77)90020-0; HORN BKP, 1970, TR79; HORN BKP, 1989, P IM UND WORKSH DARP; HUTBER D, 1987, P SOC PHOTO-OPT INS, V849, P11; IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L; IRANI M, 1989, 897 HEB U JER DEP CO; KEREN D, 1993, IEEE T PATTERN ANAL, V15, P982, DOI 10.1109/34.254057; Keren D., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P742, DOI 10.1109/CVPR.1988.196317; LECLERC YG, 1991, P CVPR LAH MAUI HAW; MARROQUIN J, 1987, J AM STAT ASS; MOUSSOURIS J, 1974, J STAT PHYSICS, V10; PELEG S, 1987, PATTERN RECOGN LETT, V5, P223, DOI 10.1016/0167-8655(87)90067-5; PELEG S, 1989, IEEE T PAMI, V11, P139; PRAGER JM, 1983, COMPUT VISION GRAPH, V24, P271, DOI 10.1016/0734-189X(83)90057-9; Rosenfeld A., 1982, DIGITAL PICTURE PROC, V1; ROSENFELD A, 1982, DIGITAL PICTURE PROC, V2; TERZOPOULOS D, 1986, IEEE T PAMI, V8; Tikhonov A.N., 1977, SOLUTION ILL POSED P; Tsai, 1984, ADV COMPUTER VISION, V1, P317; WOLFF LB, 1992, RADIOMETRY PHYSICS B; ZERUBIA J, 1993, IEEE T NEURAL NETWOR, V4, P703, DOI 10.1109/72.238324	35	23	24	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1996	19	3					289	300		10.1007/BF00055148	http://dx.doi.org/10.1007/BF00055148			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VG016					2022-12-18	WOS:A1996VG01600004
J	GROSS, AD; BOULT, TE				GROSS, AD; BOULT, TE			ANALYZING SKEWED SYMMETRIES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							SHAPE	Symmetry is pervasive in both man-made objects and nature. Since symmetries project to skew symmetries, finding axes of skew symmetry is an important vision task. This paper presents a linear time algorithm for finding the axes of skew symmetry, where the degree of symmetry is known. First, we present a review and critique of current methods for finding the axes of skew symmetry. Next, we decompose the problem of finding skew symmetry into the subproblems of solving for the rotational parameter of a ''shear symmetry'' and recovering the shear parameter of a reflexive symmetry. Using this approach, the authors derive a direct, non-heuristic moment-based technique for finding the axes of skew symmetry. For skew symmetric figures with degree of symmetry less than five we obtain a closed-form solution. The method does not rely on continuous contours but assumes there is no occlusion and requires knowing the contour's degree of symmetry. It is the first algorithm to find the axes of skew symmetry in O(n) time, where n is the number of contour points. The method is especially suited to industrial applications where the degree of symmetry is often known a priori. Examples of the method are presented for both real and synthetic images, and an error analysis of the method is given.	CUNY QUEENS COLL,FLUSHING,NY 11367; LEHIGH UNIV,DEPT ELECT ENGN & COMP SCI,BETHLEHEM,PA 18015	City University of New York (CUNY) System; Queens College NY (CUNY); Lehigh University	GROSS, AD (corresponding author), CUNY,GRAD CTR,FLUSHING,NY 11367, USA.		Boult, Terrance E./AAT-2134-2021	Boult, Terrance E./0000-0001-5007-2529				BRADY JM, 1983, MITAIM711 AI LAB; BRADY M, 1984, INT J ROBOT RES, V3, P36, DOI 10.1177/027836498400300302; BUCHBERGER B, 1987, TRENDS COMPUTER ALGE, P52; FRIEDBERG SA, 1986, COMPUT VISION GRAPH, V34, P138, DOI 10.1016/S0734-189X(86)80055-X; GROSS AD, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P744; GROSS AD, 1994, 1992 P C COMP VIS PA; GROSS AD, 1990, CUCS06490 COL U DEP; GROSS AD, 1992, THESIS COLUMBIA U; HORAUD R, 1987, 1ST P INT C COMP VIS; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; MOHAN R, 1989, THESIS U SO CALIFORN; OH WG, 1987, 5TH P SCAND C IM AN, P191; PONCE J, 1988, 1988 P IM UND WORKSH, P1074; REISFELD D, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P62; ROSENFELD A, 1986, COMPUT VISION GRAPH, V33, P156, DOI 10.1016/0734-189X(86)90113-1; SAINTMARC P, 1990, B SPLINE CONTOUR REP; STEVENS KA, 1981, ARTIF INTELL, V17, P47, DOI 10.1016/0004-3702(81)90020-5; Ulupinar F., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P414, DOI 10.1109/CCV.1988.590018; Weyl H., 1952, SYMMETRY; YUEN SYK, 1989, SHAPE CONTOUR USING; ZABRODSKY H, 1993, 1993 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION : PROCEEDINGS, P678; ZABRODSKY H, 1992, 1992 P C COMP VIS PA, P703	22	23	23	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1994	13	1					91	111		10.1007/BF01420797	http://dx.doi.org/10.1007/BF01420797			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PK120					2022-12-18	WOS:A1994PK12000005
J	FAUGERAS, O; PAPADOPOULO, T				FAUGERAS, O; PAPADOPOULO, T			A THEORY OF THE MOTION FIELDS OF CURVES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OPTICAL-FLOW; IMAGE; VECTORS	This article reports a study of the motion field generated by moving 3-D curves that are observed by a camera. We first discuss the relationship between optical flow and motion field and show that the assumptions made in the computation of the optical flow are a bit difficult to defend. We then go ahead to study the motion field of a general curve. We first study the general case of a curve moving nonrigidly and introduce the notion of isometric motion. In order to do this, we introduce the notion of spatio-temporal surface and study its differential properties up to the second order. We show that, contrary to what is commonly believed, the full motion field of the curve (i.e., the component tangent to the curve) cannot be recovered from this surface. We also give the equations that characterize the spatio-temporal surface completely up to a rigid transformation. Those equations are the expressions of the first and second fundamental forms and the Gauss and Codazzi-Mainardi equations. We then relate those differential expressions computed on the spatio-temporal surface to quantities that can be computed from the images intensities. The actual values depend upon the choice of the edge detector. We then show that the hypothesis of a rigid 3-D motion allows in general to recover the structure and the motion of the curve, in fact without explicitly computing the tangential motion field, at the cost of introducing the three-dimensional accelerations. We first study the motion field generated by the simplest kind of rigid 3-D curves, namely lines. This study is illuminating in that it paves the way for the study of general rigid curves and because of the useful results which are obtained. We then extend the results obtained in the case of lines to the case of general curves and show that at each point of the image curve two equations can be written relating the kinematic screw of the moving 3-D curve and its time derivative to quantities defined in the study of the general nonrigid motion that can be measured from the spatio-temporal surface and therefore from the image. This shows that the structure and the motion of the curve can be recovered from six image points only, without establishing any point correspondences. Finally we study the cooperation between motion and stereo in the framework of this theory. The use of two cameras instead of one allows us to get rid of the three-dimensional accelerations and the relations between the two spatio-temporal surfaces of the same rigidly moving 3-D curve can be used to help disambiguate stereo correspondences.	INRIA SOPHIA ANTIPOLIS,F-06565 VALCONNE,FRANCE				Papadopoulo, Theodore/AAN-1245-2021	Papadopoulo, Theodore/0000-0002-1643-9988				AYACHE N, 1989, IEEE T ROBOTIC AUTOM, V5, P804, DOI 10.1109/70.88101; AYACHE N, 1987, 1ST P INT C COMP VIS, P422; AYACHE N, INRIA789 REP; BAKER HH, 1989, INT J COMPUT VISION, V3, P33, DOI 10.1007/BF00054837; BERGHOLM F, 1989, THESIS ROYAL I TECHN; BOUTHEMY P, 1989, IEEE T PATTERN ANAL, V11, P499, DOI 10.1109/34.24782; Bouthemy P., 1986, Eighth International Conference on Pattern Recognition. Proceedings (Cat. No.86CH2342-4), P651; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Cartan E, 1988, LECONS GEOMETRIE ESP; DERICHE R, 1987, INT J COMPUT VISION, V1, P167, DOI 10.1007/BF00123164; DERICHE R, 1990, IMAGE VISION COMPUT, V8, P261, DOI 10.1016/0262-8856(90)80002-B; DHAYER J, 1986, COMPUT VIS GRAPH IMA, V36, P166; Do Carmo M.P., 2016, DIFFERENTIAL GEOMETR, Vsecond; FAUGERAS O, 1991, CR ACAD SCI II, V312, P1279; FAUGERAS OD, 1990, INRIA1183 TECH REPT; FAUGERAS OD, 1990, 1ST P ECCV, P107; FAUGERAS OD, 1989, P IEEE RSJ INT WORKS, P646; FAUGERAS OD, 1987, 1ST P INT C COMP VIS, P25; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; Gong S., 1989, Proceedings. Workshop on Visual Motion (IEEE Cat. No.89CH2716-9), P73, DOI 10.1109/WVM.1989.47096; Hildreth E., 1984, MEASUREMENT VISUAL M; HILDRETH EC, 1983, COMPUT VISION GRAPH, V22, P1, DOI 10.1016/0734-189X(83)90093-2; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; KOENDERINK JJ, 1986, VISION RES, V26, P161, DOI 10.1016/0042-6989(86)90078-7; KOENDERINK JJ, 1978, KYBERNETIK 1978; KOENDERINK JJ, 1975, OPT ACTA, V22, P717; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; MAYBANK SJ, 1990, IMAGE VISION COMPUT, V8, P18, DOI 10.1016/0262-8856(90)90051-6; MEER P, 1989, CARTR424 U MAR CTR A; NAGEL HH, 1983, COMPUT VISION GRAPH, V21, P85, DOI 10.1016/S0734-189X(83)80030-9; ROBERT L, 1991, JUN P COMP VIS PATT, P57; SPIVAK M., 1979, COMPREHENSIVE INTRO, VI; TOSCANI G, 1988, OCT P IAPR WORKSH CO, P275; VIEVILLE T, 1992, 2ND EUR C COMP VIS, P203; WEISS I, 1991, CARTR545 U MAR CTR A	36	23	23	0	7	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1993	10	2					125	156		10.1007/BF01420734	http://dx.doi.org/10.1007/BF01420734			32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LP385					2022-12-18	WOS:A1993LP38500003
J	EDELMAN, S; FLASH, T; ULLMAN, S				EDELMAN, S; FLASH, T; ULLMAN, S			READING CURSIVE HANDWRITING BY ALIGNMENT OF LETTER PROTOTYPES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							RECOGNITION; PERCEPTION; MODEL; CHARACTERS; MOVEMENTS; CONTEXT	We describe a new approach to the visual recognition of cursive handwriting. An effort is made to attain human-like performance by using a method based on pictorial alignment and on a model of the process of handwriting. The alignment approach permits recognition of character instances that appear embedded in connected strings. A system embodying this approach has been implemented and tested on five different word sets. The performance was stable both across words and across writers. The system exhibited a substantial ability to interpret cursive connected strings without recourse to lexical knowledge.	WEIZMANN INST SCI,DEPT APPL MATH & COMP SCI,IL-76100 REHOVOT,ISRAEL; MIT,DEPT BRAIN & COGNIT SCI,CAMBRIDGE,MA 02139; MIT,ARTIFICIAL INTELLIGENCE LAB,CAMBRIDGE,MA 02139	Weizmann Institute of Science; Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)								Aho AV, 1974, DESIGN ANAL COMPUTER; [Anonymous], 2018, A A PRACT, V11, P321; BENISRAEL A, 1974, GENERALIZED INVERSES; BIEDERMAN I, 1985, COMPUT VISION GRAPH, V32, P1; BOOKSTEIN FL, 1978, LECTURE NOTES BIOMAT, V24; BOZINOVIC RM, 1985, TR8513 SUNY BUFF; BURR DJ, 1981, IEEE T PATTERN ANAL, V3, P708, DOI 10.1109/TPAMI.1981.4767176; BURR DJ, 1983, PHYSICAL BIOL PROCES, P260; CHEN SS, 1986, COMPUT VISION GRAPH, V36, P175, DOI 10.1016/0734-189X(86)90075-7; DEBOOR C, 1966, J MATH MECH, V15, P953; Duda R.O., 1973, J ROYAL STAT SOC SER; DUVERNOY J, 1979, PATTERN RECOGN, V11, P145, DOI 10.1016/0031-3203(79)90001-3; EDELMAN S, 1987, BIOL CYBERN, V57, P25, DOI 10.1007/BF00318713; EDELMAN S, 1988, THESIS WEIZMANN I SC; EDEN M, 1961, P S APPL MATH, V12, P83; FLASH T, 1985, J NEUROSCI, V5, P1688, DOI 10.1523/jneurosci.05-07-01688.1985; FLASH T, 1983, THESIS HARVARD MIT D; FOSTER DH, 1975, BIOL CYBERN, V17, P77, DOI 10.1007/BF00363947; Goad C., 1986, PIXELS PREDICATES, P371; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; GRIMSON WEL, 1981, IMAGES SURFACES; HALLE M, 1962, IRE T INFORM THEOR, V8, P155, DOI 10.1109/TIT.1962.1057686; HANSON AR, 1976, PATTERN RECOGN, V8, P35, DOI 10.1016/0031-3203(76)90027-3; HAYES KC, 1980, COMPUT VISION GRAPH, V14, P344, DOI 10.1016/0146-664X(80)90025-8; HOGAN N, 1984, J NEUROSCI, V4, P2745; HOGAN N, 1982, 1982 P AM CONTR C, P522; KAHAN S, 1987, IEEE T PATTERN ANAL, V9, P274, DOI 10.1109/TPAMI.1987.4767901; KARLIN S, 1969, APPROXIMATION SPECIA, P467; LOWE DG, 1986, PERCEPTUAL ORG VISUA; Maier M., 1986, Eighth International Conference on Pattern Recognition. Proceedings (Cat. No.86CH2342-4), P1056; Marr D., 1982, VISION; MCCLELLAND JL, 1981, PSYCHOL REV, V88, P375, DOI 10.1037/0033-295X.88.5.375; NEISSER U, 1960, INFORM CONTR, V3, P191; OKUDA T, 1976, IEEE T COMPUT, V25, P172, DOI 10.1109/TC.1976.5009232; PAIVIO A, 1978, HDB PERCEPTION, V8, P375; Pavlidis T., 1977, STRUCTURAL PATTERN R; Pearl Judea, 1984, HEURISTICS; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681; RITTER K, 1969, APPROXIMATION SPECIA, P75; Serra J, 1982, IMAGE ANAL MATH MORP; SRIHARI SN, 1987, ARTIF INTELL, V33, P217, DOI 10.1016/0004-3702(87)90035-X; STENTIFORD FWM, 1985, IEEE T PATTERN ANAL, V7, P349, DOI 10.1109/TPAMI.1985.4767665; SUEN CY, 1980, P IEEE, V68, P469, DOI 10.1109/PROC.1980.11675; SUEN CY, 1983, ACTA PSYCHOL, V54, P295, DOI 10.1016/0001-6918(83)90042-2; TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769; TRAVERS JR, 1978, AM J PSYCHOL, V91, P523, DOI 10.2307/1421698; ULLMAN S, 1986, MIT AI931 MEM; Yoshida M., 1973, 1st International Joint Conference on Pattern Recognition, P197; Zwikker C., 1963, ADV GEOMETRY PLANE C; [No title captured]	50	23	26	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1990	5	3					303	331		10.1007/BF00126503	http://dx.doi.org/10.1007/BF00126503			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EZ751		Green Submitted			2022-12-18	WOS:A1990EZ75100004
J	Xu, TY; Feng, ZH; Wu, XJ; Kittler, J				Xu, Tianyang; Feng, Zhenhua; Wu, Xiao-Jun; Kittler, Josef			Adaptive Channel Selection for Robust Visual Object Tracking with Discriminative Correlation Filters	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual Object Tracking; Discriminative Correlation Filters; Adaptive Channel Selection; Adaptive Elastic Net	REGRESSION	Discriminative Correlation Filters (DCF) have been shown to achieve impressive performance in visual object tracking. However, existing DCF-based trackers rely heavily on learning regularised appearance models from invariant image feature representations. To further improve the performance of DCF in accuracy and provide a parsimonious model from the attribute perspective, we propose to gauge the relevance of multi-channel features for the purpose of channel selection. This is achieved by assessing the information conveyed by the features of each channel as a group, using an adaptive group elastic net inducing independent sparsity and temporal smoothness on the DCF solution. The robustness and stability of the learned appearance model are significantly enhanced by the proposed method as the process of channel selection performs implicit spatial regularisation. We use the augmented Lagrangian method to optimise the discriminative filters efficiently. The experimental results obtained on a number of well-known benchmarking datasets demonstrate the effectiveness and stability of the proposed method. A superior performance over the state-of-the-art trackers is achieved using less than 10% deep feature channels.	[Xu, Tianyang; Feng, Zhenhua; Kittler, Josef] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England; [Feng, Zhenhua] Univ Surrey, Dept Comp Sci, Guildford GU2 7XH, Surrey, England; [Wu, Xiao-Jun] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China	University of Surrey; University of Surrey; Jiangnan University	Xu, TY (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.	tianyang.xu@surrey.ac.uk; z.feng@surrey.ac.uk; wu_xiaojun@jiangnan.edu.cn; j.kittler@surrey.ac.uk		Xu, Tianyang/0000-0002-9015-3128	UK EPSRC Programme Grant (FACER2VM) [EP/N007743/1]; EPSRC/dstl/MURI Project [EP/R018456/1]; National Natural Science Foundation of China [61672265, U1836218, 61902153, 61876072]	UK EPSRC Programme Grant (FACER2VM); EPSRC/dstl/MURI Project(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the UK EPSRC Programme Grant (FACER2VM) EP/N007743/1, in part by the EPSRC/dstl/MURI Project EP/R018456/1, and in part by the National Natural Science Foundation of China (61672265, U1836218, 61902153, 61876072).	Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374; Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53; Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226; Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881; Bay H., 2006, EUR C COMP VIS ECCV, P404, DOI [10.1007/11744023_32, DOI 10.1007/11744023_32]; Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bhat G., 2018, ARXIV PREPRINT ARXIV; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513; Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928; Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29; Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392; Feng ZH, 2015, IEEE T IMAGE PROCESS, V24, P3425, DOI 10.1109/TIP.2015.2446944; Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI 10.1109/ICCV.2017.129; Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006; Gundogdu E, 2018, IEEE T IMAGE PROCESS, V27, P2526, DOI 10.1109/TIP.2018.2806280; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He ZQ, 2017, IEEE INT CONF COMP V, P1992, DOI 10.1109/ICCVW.2017.233; Henriques J. F., 2012, EUR C COMP VIS, P702, DOI DOI 10.1007/978-3-642-33765-9_50; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1; Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230; Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79; Kristan Matej, 2016, VISUAL OBJECT TRACKI, P7; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935; Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515; Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905; Lin Z., 2010, ARXIV10095055, DOI DOI 10.1016/J.JSB.2012.10.010; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515; Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152; Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; Park E., 2018, ARXIV PREPRINT ARXIV; Petersen K. B., 2012, MATRIX COOKBOOK; Simonyan K., 2014, 3 INT C LEARN REPR I; Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937; Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279; Sun C, 2018, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2018.00058; Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934; Sun Y., 2019, IEEE C COMP VIS PATT; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158; Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531; van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376; Wang NY, 2015, IEEE I CONF COMP VIS, P3101, DOI 10.1109/ICCV.2015.355; Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142; Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Xu T., 2020, ARXIV PREPRINT ARXIV; Xu TY, 2020, IEEE T CIRC SYST VID, V30, P3727, DOI 10.1109/TCSVT.2019.2945068; Xu TY, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107172; Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI 10.1109/CVPR.2017.512; Zhang TZ, 2016, PROC CVPR IEEE, P3880, DOI 10.1109/CVPR.2016.421; Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0; Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34; Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z; Zhu Qiang, 2006, CVPR, DOI DOI 10.1109/CVPR.2006.119; Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	73	22	22	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1359	1375		10.1007/s11263-021-01435-1	http://dx.doi.org/10.1007/s11263-021-01435-1		FEB 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		hybrid			2022-12-18	WOS:000614668800002
J	Mordan, T; Thome, N; Henaff, G; Cord, M				Mordan, Taylor; Thome, Nicolas; Henaff, Gilles; Cord, Matthieu			End-to-End Learning of Latent Deformable Part-Based Representations for Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Fully convolutional network; Deep learning; Part-based representation; End-to-end latent part learning		Object detection methods usually represent objects through rectangular bounding boxes from which they extract features, regardless of their actual shapes. In this paper, we apply deformations to regions in order to learn representations better fitted to objects. We introduce DP-FCN, a deep model implementing this idea by learning to align parts to discriminative elements of objects in a latent way, i.e. without part annotation. This approach has two main assets: it builds invariance to local transformations, thus improving recognition, and brings geometric information to describe objects more finely, leading to a more accurate localization. We further develop both features in a new model named DP-FCN2.0 by explicitly learning interactions between parts. Alignment is done with an in-network joint optimization of all parts based on a CRF with custom potentials, and deformations are influencing localization through a bilinear product. We validate our models on PASCAL VOC and MS COCO datasets and show significant gains. DP-FCN2.0 achieves state-of-the-art results of 83.3 and 81.2% on VOC 2007 and 2012 with VOC data only.	[Mordan, Taylor; Cord, Matthieu] Sorbonne Univ, CNRS, Lab Informat Paris 6, LIP6, F-75005 Paris, France; [Mordan, Taylor; Henaff, Gilles] Thales Land & Air Syst, 2 Ave Gay Lussac, F-78990 Elancourt, France; [Thome, Nicolas] Conservatoire Natl Arts & Metiers, CEDRIC, 292 Rue St Martin, F-75003 Paris, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; Thales Group; heSam Universite; Conservatoire National Arts & Metiers (CNAM); Institut Polytechnique de Paris	Mordan, T (corresponding author), Sorbonne Univ, CNRS, Lab Informat Paris 6, LIP6, F-75005 Paris, France.; Mordan, T (corresponding author), Thales Land & Air Syst, 2 Ave Gay Lussac, F-78990 Elancourt, France.	taylor.mordan@lip6.fr; nicolas.thome@cnam.fr; gilles.henaff@fr.thalesgroup.com; matthieu.cord@lip6.fr		Mordan, Taylor/0000-0002-4775-9239				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Azizpour H, 2012, LECT NOTES COMPUT SC, V7572, P836, DOI 10.1007/978-3-642-33718-5_60; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Chandra S, 2017, IEEE I CONF COMP VIS, P5113, DOI 10.1109/ICCV.2017.546; Chen Liang-Chich, 2015, ABS14127062 CORR; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32; Dai Jifeng, 2016, P NIPS, P1; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Durand Thibaut, 2017, CVPR, DOI DOI 10.1109/CVPR.2017.631; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fidler S, 2013, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2013.423; Gidaris S., 2016, BMVC; Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135; Gidaris Spyros, 2016, CVPR; Girshick R., 2014, CVPR; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2015, PROC CVPR IEEE, P437, DOI 10.1109/CVPR.2015.7298641; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98; Krahenbuhl P., 2011, ADV NEURAL INF PROCE, V24, P109; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Li Yi, 2017, P IEEE C COMP VIS PA; Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mordan T., 2017, P BRIT MACH VIS C; Ott P, 2011, PROC CVPR IEEE, P1513, DOI 10.1109/CVPR.2011.5995357; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Savalle P.-A., 2014, P EUR C COMP VIS PAR; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Sicre R, 2017, PROC CVPR IEEE, P3116, DOI 10.1109/CVPR.2017.332; Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Wan L, 2015, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2015.7298686; Wang P, 2015, IEEE I CONF COMP VIS, P1573, DOI 10.1109/ICCV.2015.184; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhu L, 2010, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2010.5540096	54	22	23	2	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1659	1679		10.1007/s11263-018-1109-z	http://dx.doi.org/10.1007/s11263-018-1109-z			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY					2022-12-18	WOS:000492425300005
J	Yu, X; Porikli, F; Fernando, B; Hartley, R				Yu, Xin; Porikli, Fatih; Fernando, Basura; Hartley, Richard			Hallucinating Unaligned Face Images by Multiscale Transformative Discriminative Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face hallucination; Super-resolution; Multiscale; Transformative discriminative network	SUPERRESOLUTION; LIMITS	Conventional face hallucination methods heavily rely on accurate alignment of low-resolution (LR) faces before upsampling them. Misalignment often leads to deficient results and unnatural artifacts for large upscaling factors. However, due to the diverse range of poses and different facial expressions, aligning an LR input image, in particular when it is tiny, is severely difficult. In addition, when the resolutions of LR input images vary, previous deep neural network based face hallucination methods require the interocular distances of input face images to be similar to the ones in the training datasets. Downsampling LR input faces to a required resolution will lose high-frequency information of the original input images. This may lead to suboptimal super-resolution performance for the state-of-the-art face hallucination networks. To overcome these challenges, we present an end-to-end multiscale transformative discriminative neural network devised for super-resolving unaligned and very small face images of different resolutions ranging from 16 x 16 to 32 x 32 pixels in a unified framework. Our proposed network embeds spatial transformation layers to allow local receptive fields to line-up with similar spatial supports, thus obtaining a better mapping between LR and HR facial patterns. Furthermore, we incorporate a class-specific loss designed to classify upright realistic faces in our objective through a successive discriminative network to improve the alignment and upsampling performance with semantic information. Extensive experiments on a large face dataset show that the proposed method significantly outperforms the state-of-the-art.	[Yu, Xin; Porikli, Fatih; Hartley, Richard] Australian Natl Univ, Res Sch Engn, Canberra, ACT, Australia; [Fernando, Basura] A STAR Artificial Intelligence Initiat A AI, Human Centr AI Programme, Singapore, Singapore	Australian National University	Yu, X (corresponding author), Australian Natl Univ, Res Sch Engn, Canberra, ACT, Australia.	xin.yu@anu.edu.au; fatih.porikli@anu.edu.au; fernando_basura@scei.a-star.edu.sg; richard.hartley@anu.edu.au		Yu, Xin/0000-0002-0269-5649; Fernando, Basura/0000-0002-6920-9916	Australian Research Council's Discovery Project funding scheme [DP150104645]; Australian Research Council Centre of Excellence for Robotic Vision [CE140100016]	Australian Research Council's Discovery Project funding scheme(Australian Research Council); Australian Research Council Centre of Excellence for Robotic Vision(Australian Research Council)	This work was supported under the Australian Research Council's Discovery Project funding scheme (Project DP150104645) and Australian Research Council Centre of Excellence for Robotic Vision (Project CE140100016).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.596; Arandjelovic O, 2014, PATTERN RECOGN, V47, P2662, DOI 10.1016/j.patcog.2014.02.006; Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616; Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210; Bruna Joan, 2016, ICLR; Bulat A., 2017, PROC INT C COMP VIS; Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019; Bulat A, 2018, LECT NOTES COMPUT SC, V11210, P187, DOI 10.1007/978-3-030-01231-1_12; Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043; Dahl R, 2017, IEEE I CONF COMP VIS, P5449, DOI 10.1109/ICCV.2017.581; Denton Emily L, 2015, NEURIPS, V2, P4; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu S., 2015, P IEEE INT C COMP VI; Hennings-Yeomans PH, 2008, PROC CVPR IEEE, P3637; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129; Huang H., 2017, PROC INT C COMP VIS; Huang Jia-Bin, 2015, CVPR, DOI DOI 10.1109/CVPR.2015.7299156; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kolouri S., 2015, P IEEE COMP SOC C CO; Lai Wei-Sheng, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li YC, 2014, PATTERN RECOGN, V47, P1261, DOI 10.1016/j.patcog.2013.09.012; Lin ZC, 2006, IEEE T PATTERN ANAL, V28, P847, DOI 10.1109/TPAMI.2006.105; Lin ZC, 2008, INT J COMPUT VISION, V80, P406, DOI 10.1007/s11263-008-0148-2; Liu C, 2001, PROC CVPR IEEE, P192; Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Singh A, 2014, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2014.364; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Tappen MF, 2012, LECT NOTES COMPUT SC, V7578, P236, DOI 10.1007/978-3-642-33786-4_18; Tappen MF, 2003, P IEEE WORKSH STAT C, P1; van den Oord A, 2016, PR MACH LEARN RES, V48; Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9; Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171; Wei DM, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278040; Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36; Yang CY, 2018, INT J COMPUT VISION, V126, P597, DOI 10.1007/s11263-017-1044-4; Yang CY, 2013, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR.2013.146; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14; Yu X, 2020, IEEE T PATTERN ANAL, V42, P2926, DOI 10.1109/TPAMI.2019.2916881; Yu X, 2020, IEEE T PATTERN ANAL, V42, P2148, DOI 10.1109/TPAMI.2019.2914039; Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101; Yu X, 2017, AAAI CONF ARTIF INTE, P4327; Yu X, 2018, IEEE T IMAGE PROCESS, V27, P2747, DOI 10.1109/TIP.2018.2808840; Yu X, 2017, PROC CVPR IEEE, P5367, DOI 10.1109/CVPR.2017.570; Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20; Yu X, 2014, IEEE T MULTIMEDIA, V16, P1510, DOI 10.1109/TMM.2014.2321734; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957; Zhou EJ, 2015, AAAI CONF ARTIF INTE, P3871; Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	66	22	22	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					500	526		10.1007/s11263-019-01254-5	http://dx.doi.org/10.1007/s11263-019-01254-5		NOV 2019	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	KJ1GX					2022-12-18	WOS:000495044700002
J	Zhang, WC; Sun, CM				Zhang, Weichuan; Sun, Changming			Corner Detection Using Multi-directional Structure Tensor with Multiple Scales	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Corner detection; Image intensity variation extraction; Anisotropic Gaussian directional derivative filters; Multi-directional structure tensor with multiple scales	ACCURATE JUNCTION DETECTION; SPACE; IMAGES; EDGE	Corners are important features for image analysis and computer vision tasks. Local structure tensors with multiple scales are widely used in intensity-based corner detectors. In this paper, the properties of intensity variations of a step edge, L-type corner, Y- or T-type corner, X-type corner, and star-type corner are investigated. The properties that we obtained indicate that the image intensity variations of a corner are not always large in all directions. The properties also demonstrate that existing structure tensor-based corner detection methods cannot depict the differences of intensity variations well between edges and corners which result in wrong corner detections. We present a new technique to extract the intensity variations from input images using anisotropic Gaussian directional derivative filters with multiple scales. We prove that the new extraction technique on image intensity variation has the ability to accurately depict the characteristics of edges and corners in the continuous domain. Furthermore, the properties of the intensity variations of step edges and corners enable us to derive a new multi-directional structure tensor with multiple scales, which has the ability to depict the intensity variation differences well between edges and corners in the discrete domain. The eigenvalues of the multi-directional structure tensor with multiple scales are used to develop a new corner detection method. Finally, the criteria on average repeatability (under affine image transformation, JPEG compression, and noise degradation), region repeatability based on the Oxford dataset, repeatability metric based on the DTU dataset, detection accuracy, and localization accuracy are used to evaluate the proposed detector against ten state-of-the-art methods. The experimental results show that our proposed detector outperforms all the other tested detectors.	[Zhang, Weichuan] Xian Polytech Univ, Coll Elect & Informat, Xian 710048, Shaanxi, Peoples R China; [Sun, Changming] CSIRO Data61, POB 76, Epping, NSW 1710, Australia	Xi'an Polytechnic University; Commonwealth Scientific & Industrial Research Organisation (CSIRO)	Zhang, WC (corresponding author), Xian Polytech Univ, Coll Elect & Informat, Xian 710048, Shaanxi, Peoples R China.	zwc2003@163.com; changming.sun@csiro.au	Zhang, Weichuan/AAF-2811-2020; Sun, Changming/A-3276-2008	Sun, Changming/0000-0001-5943-1989	National Natural Science Foundation of China [61401347]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China (No. 61401347). We thank the anonymous reviewers for their detailed comments that substantially improved the paper.	Aanaes H, 2012, INT J COMPUT VISION, V97, P18, DOI 10.1007/s11263-011-0473-8; Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16; Awrangjeb M, 2008, IEEE T MULTIMEDIA, V10, P1059, DOI 10.1109/TMM.2008.2001384; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bowyer K, 2001, COMPUT VIS IMAGE UND, V84, P77, DOI 10.1006/cviu.2001.0931; Brox T, 2006, IMAGE VISION COMPUT, V24, P41, DOI 10.1016/j.imavis.2005.09.010; Cornelis N, 2008, PROC CVPR IEEE, P1013; DERICHE R, 1993, INT J COMPUT VISION, V10, P101, DOI 10.1007/BF01420733; DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060; Duval-Poo MA, 2015, IEEE T IMAGE PROCESS, V24, P3768, DOI 10.1109/TIP.2015.2451175; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; Gao XT, 2007, IEEE T CIRC SYST VID, V17, P868, DOI 10.1109/TCSVT.2007.897473; Garding J, 1996, INT J COMPUT VISION, V17, P163, DOI 10.1007/BF00058750; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Hartley R., 2004, ROBOTICA; Huang FC, 2012, IEEE T CIRC SYST VID, V22, P340, DOI 10.1109/TCSVT.2011.2162760; Kenney CS, 2003, IEEE T PATTERN ANAL, V25, P1437, DOI 10.1109/TPAMI.2003.1240118; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; LEE JS, 1995, IEEE T IMAGE PROCESS, V4, P100, DOI 10.1109/83.350810; Lenc K, 2016, LECT NOTES COMPUT SC, V9915, P100, DOI 10.1007/978-3-319-49409-8_11; Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Marimon D, 2010, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR.2010.5539936; Maver J, 2010, IEEE T PATTERN ANAL, V32, P1211, DOI 10.1109/TPAMI.2009.105; Miao ZW, 2013, PATTERN RECOGN, V46, P2890, DOI 10.1016/j.patcog.2013.03.024; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mokhtarian F, 1998, IEEE T PATTERN ANAL, V20, P1376, DOI 10.1109/34.735812; Moravec H.P., 1979, P 6 INT JOINT C ART, P598; NOBLE JA, 1988, IMAGE VISION COMPUT, V6, P121, DOI 10.1016/0262-8856(88)90007-8; Olson CF, 2000, IEEE T PATTERN ANAL, V22, P983, DOI 10.1109/34.877521; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; Rattarangsi A., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P923, DOI 10.1109/ICPR.1990.118242; Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275; Ruzon MA, 2001, IEEE T PATTERN ANAL, V23, P1281, DOI 10.1109/34.969118; Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446; Shui PL, 2013, IEEE T IMAGE PROCESS, V22, P3204, DOI 10.1109/TIP.2013.2259834; Shui PL, 2012, PATTERN RECOGN, V45, P806, DOI 10.1016/j.patcog.2011.07.020; Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Su R, 2012, PATTERN RECOGN, V45, P3695, DOI 10.1016/j.patcog.2012.04.013; TEH CH, 1989, IEEE T PATTERN ANAL, V11, P859, DOI 10.1109/34.31447; Pham TA, 2014, PATTERN RECOGN, V47, P282, DOI 10.1016/j.patcog.2013.06.027; Trujillo L, 2006, GECCO 2006: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOL 1 AND 2, P887; Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165; Wang JG, 2017, CHIN CONTR CONF, P1681, DOI 10.23919/ChiCC.2017.8027593; Wang YP, 1999, IEEE T IMAGE PROCESS, V8, P1757, DOI 10.1109/83.806621; Weickert J, 1998, IEEE T IMAGE PROCESS, V7, P398, DOI 10.1109/83.661190; Widynski N, 2014, IEEE T PATTERN ANAL, V36, P1922, DOI 10.1109/TPAMI.2014.2307856; Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5; Witkin A., 1984, P IEEE INT C AC SPEE, V9, P150, DOI DOI 10.1109/ICASSP.1984.1172729; Xia GS, 2014, INT J COMPUT VISION, V106, P31, DOI 10.1007/s11263-013-0640-1; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Zhang WC, 2015, PATTERN RECOGN, V48, P2785, DOI 10.1016/j.patcog.2015.03.021; Zhang WC, 2014, IET IMAGE PROCESS, V8, P639, DOI 10.1049/iet-ipr.2013.0641; Zhang WC, 2019, IEEE T IMAGE PROCESS, V28, P4444, DOI 10.1109/TIP.2019.2910655; Zhang WC, 2017, PATTERN RECOGN, V63, P193, DOI 10.1016/j.patcog.2016.10.008; Zhang XH, 2015, IEEE T PATTERN ANAL, V37, P2207, DOI 10.1109/TPAMI.2015.2396074; Zhong BJ, 2007, IEEE T PATTERN ANAL, V29, P508, DOI 10.1109/TPAMI.2007.50	62	22	25	4	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					438	459		10.1007/s11263-019-01257-2	http://dx.doi.org/10.1007/s11263-019-01257-2		OCT 2019	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KJ1GX					2022-12-18	WOS:000494067800001
J	Zhang, SF; Wen, LY; Shi, HL; Lei, Z; Lyu, SW; Li, SZ				Zhang, Shifeng; Wen, Longyin; Shi, Hailin; Lei, Zhen; Lyu, Siwei; Li, Stan Z.			Single-Shot Scale-Aware Network for Real-Time Face Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face detection; Single-shot; Scale-aware; Class imbalance		In this work, we describe a single-shot scale-aware convolutional neural network based face detector (SFDet). In comparison with the state-of-the-art anchor-based face detection methods, the main advantages of our method are summarized in four aspects. (1) We propose a scale-aware detection network using a wide scale range of layers associated with appropriate scales of anchors to handle faces with various scales, and describe a new equal density principle to ensure anchors with different scales to be evenly distributed on the image. (2) To improve the recall rates of faces with certain scales (e.g., the scales of the faces are quite different from the scales of designed anchors), we design a new anchor matching strategy with scale compensation. (3) We introduce an IoU-aware weighting scheme for each training sample in classification loss calculation to encode samples accurately in training process. (4) Considering the class imbalance issue, a max-out background strategy is used to reduce false positives. Several experiments are conducted on public challenging face detection datasets, i.e., WIDER FACE, AFW, PASCAL Face, FDDB, and MAFA, to demonstrate that the proposed method achieves the state-of-the-art results and runs at 82.1 FPS for the VGA-resolution images.	[Zhang, Shifeng; Lei, Zhen; Li, Stan Z.] Chinese Acad Sci, Inst Automat, CBSR, Beijing, Peoples R China; [Zhang, Shifeng; Lei, Zhen; Li, Stan Z.] Chinese Acad Sci, Inst Automat, NLPR, Beijing, Peoples R China; [Zhang, Shifeng; Lei, Zhen; Li, Stan Z.] Univ Chinese Acad Sci, Beijing, Peoples R China; [Wen, Longyin] JD Digits, Mountain View, CA USA; [Shi, Hailin] JD AI Res, Beijing, Peoples R China; [Lyu, Siwei] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; State University of New York (SUNY) System; State University of New York (SUNY) Albany	Lei, Z (corresponding author), Chinese Acad Sci, Inst Automat, CBSR, Beijing, Peoples R China.; Lei, Z (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing, Peoples R China.; Lei, Z (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.	shifeng.zhang@nlpr.ia.ac.cn; longyin.wen@jd.com; shihailin@jd.com; zlei@nlpr.ia.ac.cn; slyu@albany.edu; szli@nlpr.ia.ac.cn		Lyu, Siwei/0000-0002-0992-685X	Chinese National Natural Science Foundation [61876178, 61473291, 61806196]; National Key Research and Development Plan [2016YFC0801002]; JD Grapevine Plan; Authen Metric RD Funds	Chinese National Natural Science Foundation(National Natural Science Foundation of China (NSFC)); National Key Research and Development Plan; JD Grapevine Plan; Authen Metric RD Funds	This work was supported by the Chinese National Natural Science Foundation Projects #61876178, #61473291, #61806196, the National Key Research and Development Plan (Grant No. 2016YFC0801002), JD Grapevine Plan and Authen Metric R&D Funds.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.596; Barbu A., 2014, ARXIVABS140435968 CO; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Brubaker SC, 2008, INT J COMPUT VISION, V77, P65, DOI 10.1007/s11263-007-0060-1; Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen Y., 2017, ARXIVABS170905188 CO; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276; Farfade SS, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P643, DOI 10.1145/2671188.2749408; Fowlkes C. C., 2015, ARXIVABS150608347 CO; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Fu C., 2017, ARXIVABS170106659 CO; Ge S, 2017, IEEE CVPR, DOI DOI 10.1109/CVPR.2017.53; Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Glorot X., 2010, PROC MACH LEARN RES, P249; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25; Howard A. G, 2013, ARXIVABS13125402 COR; Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166; Huang J., 2016, ARXIVABS161110012 CO; Jain V., 2010, UMCS2010009; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang H., 2016, ARXIVABS160603473 CO; Kalal Z., 2008, P BRIT MACH VIS C, P1, DOI DOI 10.5244/C.22.42; Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557; Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98; Kumar V, 2015, IEEE I CONF COMP VIS, P1994, DOI 10.1109/ICCV.2015.231; Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79; Li HX, 2014, PROC CVPR IEEE, P1843, DOI 10.1109/CVPR.2014.238; Li JG, 2013, PROC CVPR IEEE, P3468, DOI 10.1109/CVPR.2013.445; Li YZ, 2016, LECT NOTES COMPUT SC, V9907, P420, DOI 10.1007/978-3-319-46487-9_26; Liao SC, 2016, IEEE T PATTERN ANAL, V38, P211, DOI 10.1109/TPAMI.2015.2448075; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu W., 2015, ARXIVABS150604579 CO; Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo WJ, 2016, ADV NEUR IN, V29; Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47; Ohn-Bar E., 2016, INT C PATT REC; Pham H, 2007, SPRINGER SER RELIAB, P1; QIN HW, 2016, PROC CVPR IEEE, P3456, DOI DOI 10.1109/CVPR.2016.376; Ranjan R., 2016, ARXIV160301249; Ranjana J. Shourie, 2015, 2015 40th International Conference on Infrared, Millimeter and Terahertz Waves (IRMMW-THz), P1, DOI 10.1109/IRMMW-THz.2015.7327917; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Redmon J., 2016, ARXIVABS161208242 CO; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen XH, 2013, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2013.444; Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212; Shrivastava A., 2016, ARXIVABS161206851 CO; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Shrivastava A, 2016, LECT NOTES COMPUT SC, V9905, P330, DOI 10.1007/978-3-319-46448-0_20; Sun X., 2017, ARXIVABS170108289 CO; Triantafyllidou D., 2016, INNS C BIG DATA, P61; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wan S., 2016, ARXIVABS160802236 CO; Wang H., 2017, ARXIVABS170601061 CO; Wang J., 2017, ARXIVABS171107246 CO; Wang X., 2017, P IEEE C COMPUTER VI, P2097, DOI 10.1109/CVPR.2017.369; Wang XD, 2018, APPL POWER ELECT CO, P992; Wang Y., 2017, ARXIVABS170905256 CO; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Yan JJ, 2014, PROC CVPR IEEE, P2497, DOI 10.1109/CVPR.2014.320; Yan JJ, 2014, IMAGE VISION COMPUT, V32, P790, DOI 10.1016/j.imavis.2013.12.004; Yang B., 2014, IEEE INT JOINT C BIO, P1; Yang B, 2015, IEEE I CONF COMP VIS, P82, DOI 10.1109/ICCV.2015.18; Yang S., 2017, ARXIVABS170602863 CO; Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419; Yue L, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON MECHANICAL, ELECTRONIC AND INFORMATION TECHNOLOGY ENGINEERING (ICMITE 2016), P51; Zeng XY, 2016, LECT NOTES COMPUT SC, V9911, P354, DOI 10.1007/978-3-319-46478-7_22; Zhang K., 2017, ICCV; Zhang S., 2017, ARXIVABS171106897 CO; Zhang SF, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P1, DOI 10.1109/BTAS.2017.8272675; Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhu C., 2016, ARXIVABS160605413 CO; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zhu YS, 2017, IEEE I CONF COMP VIS, P4146, DOI 10.1109/ICCV.2017.444; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	92	22	26	1	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		537	559		10.1007/s11263-019-01159-3	http://dx.doi.org/10.1007/s11263-019-01159-3			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD					2022-12-18	WOS:000468525900002
J	Sun, J; Ponce, J				Sun, Jian; Ponce, Jean			Learning Dictionary of Discriminative Part Detectors for Image Categorization and Cosegmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Discriminative parts; Discriminative learning; Image classification; Image cosegmentation	SPARSE	This paper proposes a novel approach to learning mid-level image models for image categorization and cosegmentation. We represent each image class by a dictionary of part detectors that best discriminate that class from the background. We learn category-specific part detectors in a weakly supervised setting in which the training images are only annotated with category labels without part/object location information. We use a latent SVM model regularized using the group sparsity norm to learn the part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the learned part detectors to image classification and cosegmentation, and present extensive comparative experiments with standard benchmarks.	[Sun, Jian] Xi An Jiao Tong Univ, 28 Xianning West Rd, Xian 710049, Shaanxi, Peoples R China; [Ponce, Jean] PSL Res Univ, Ecole Normale Super, 45 Rue Ulm, F-75005 Paris, France	Xi'an Jiaotong University; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Sun, J (corresponding author), Xi An Jiao Tong Univ, 28 Xianning West Rd, Xian 710049, Shaanxi, Peoples R China.	jiansun@mail.xjtu.edu.cn; jean.ponce@ens.fr			NSFC [61472313, 11131006, 61303121]; 973 program [2013CB329404]; European Research Council; Institut Universitaire de France;  [NCET-12-0442]	NSFC(National Natural Science Foundation of China (NSFC)); 973 program(National Basic Research Program of China); European Research Council(European Research Council (ERC)European Commission); Institut Universitaire de France; 	Jian Sun was supported by NSFC (No. 61472313, 11131006), the 973 program (2013CB329404), NCET-12-0442, and NSFC (No. 61303121). Jean Ponce's work was supported in part by European Research Council (VideoWorld project) and the Institut Universitaire de France.	Ahmed E., 2014, ECCV; [Anonymous], 2012, ECCV; Azizpour Hossein, 2012, ECCV; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bo L., 2013, CVPR; Bo L., 2009, NIPS; Bourdev L., 2009, ICCV; Bourdev L, 2010, LECT NOTES COMPUT SC, V6316, P168, DOI 10.1007/978-3-642-15567-3_13; Boureau Y, 2010, CVPR; Boureau Y.-L., 2011, ICCV; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chen X., 2013, ICCV; Chen X., 2015, P IEEE INT C COMP VI; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Cimpoi M., 2015, CVPR; Csurka G., 2004, ECCV WORKSHOPS; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Doersch C., 2013, NIPS; Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597; Duchenne O., 2011, ICCV; Duchi J., 2009, NIPS; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; FEIFEI L, 2004, CVPR WORKSH GEN MOD; Girshick R., 2015, CVPR; Gong Y., ECCV; Griffin G., 2007, PERONA P CALTECH 256; Hariharan B., 2012, ECCV; Jiang  Z., 2011, CVPR; Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88; Joulin A., 2010, CVPR; Joulin A., 2012, CVPR; Juneja M., 2013, CVPR; Kim G, 2012, CVPR; Kim G., 2011, ICCV; Kim J., 2013, CVPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuettel D., 2012, ECCV; Lazebnik S., 2006, P 2006 IEEE COMP VIS, P2169; Li L.-J., 2007, ICCV; Li L-J., 2010, ADV NEURAL INF PROCE, V23, P106; Lin D., 2014, CVPR; Liu L., 2011, ICCV; Lowe D. G., 1999, CVPR; Mairal J., 2008, CVPR; Mairal J., 2009, INT C MACH LEARN ICM, DOI 10.1145/1553374.1553463; Mukherjee L., 2012, ECCV; Mukherjee L., 2011, CVPR; Oliva A., 2010, INT J COMPUT VISION, V42, P145; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Pandey M., 2011, ICCV; Parizi S. N., 2015, ICLR; Parizi S. N., 2012, CVPR; Perronnin F., 2010, ECCV; Quattoni A., 2009, CVPR; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rubinstein M., 2013, CVPR; Sadeghi F., 2012, ECCV; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Santosh K., 2012, ECCV WORKSH PARTS AT; Seidenari L, 2014, IEEE T PATTERN ANAL, V36, P1033, DOI 10.1109/TPAMI.2013.232; Sharma G., 2012, CVPR; Singh S., 2012, ECCV; Su Y., 2011, ICCV; Sun J., 2013, ICCV; Todorovic S., 2008, CVPR; Vezhnevets A., 2012, CVPR; Vicente S., 2011, CVPR; Wang J., 2010, CVPR; Wang X., 2013, P 30 INT C INT C MAC; Xiao J., 2010, CVPR; Yan S., 2012, ECCV; Yang J., 2009, CVPR; Yao B., 2011, ICCV; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zeiler MD, 2014, ECCV; Zhang Ning, 2014, ICML; Zheng Y., 2012, ECCV; Zuo Z., 2014, ECCV	79	22	23	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2016	120	2					111	133		10.1007/s11263-016-0899-0	http://dx.doi.org/10.1007/s11263-016-0899-0			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DU3EF		Green Submitted			2022-12-18	WOS:000382092900001
J	Hu, Z; Yang, MH				Hu, Zhe; Yang, Ming-Hsuan			Learning Good Regions to Deblur Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image deblurring; Conditional random field; Kernel similarity; Region inference	RESTORATION	The goal of single image deblurring is to recover both a latent clear image and an underlying blur kernel from one input blurred image. Recent methods focus on exploiting natural image priors or additional image observations for deblurring, but pay less attention to the influence of image structure on estimating blur kernels. What is the useful image structure and how can one select good regions for deblurring? We formulate the problem of learning good regions for deblurring within the conditional random field framework. To better compare blur kernels, we develop an effective similarity metric for labeling training samples. The learned model is able to predict good regions from an input blurred image for deblurring without user guidance. Qualitative and quantitative evaluations demonstrate that good regions can be selected by the proposed algorithms for effective single image deblurring.	[Hu, Zhe; Yang, Ming-Hsuan] Univ Calif Merced, Sch Engn, Merced, CA 95343 USA	University of California System; University of California Merced	Yang, MH (corresponding author), Univ Calif Merced, Sch Engn, Merced, CA 95343 USA.	zhu@ucmerced.edu; mhyang@ucmerced.edu	Yang, Ming-Hsuan/AAE-7350-2019; Yang, Ming-Hsuan/T-9533-2019; Hu, Zhe/AAE-7207-2021	Yang, Ming-Hsuan/0000-0003-4848-2304; 				Bae H., 2012, P AS C COMP VIS, P322; Bardsley J, 2006, OPT EXPRESS, V14, P1767, DOI 10.1364/OE.14.001767; Ben-Ezra M, 2003, PROC CVPR IEEE, P657; Besag J., 2006, J ROYAL STAT SOC D, V24, P179; Cai JF, 2009, PROC CVPR IEEE, P104, DOI 10.1109/CVPRW.2009.5206743; Cho S., 2009, P ACM SIGGRAPH AS; Cho S., 2007, P IEEE INT C COMP VI; Cho TS, 2011, PROC CVPR IEEE, P241, DOI 10.1109/CVPR.2011.5995479; Cho TS, 2010, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2010.5540214; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; FLETCHER R, 1970, COMPUT J, V13, P317, DOI 10.1093/comjnl/13.3.317; Goldstein A, 2012, LECT NOTES COMPUT SC, V7576, P622, DOI 10.1007/978-3-642-33715-4_45; Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13; Hirsch M, 2011, IEEE I CONF COMP VIS, P463, DOI 10.1109/ICCV.2011.6126276; Hu W, 2012, IEEE T IMAGE PROCESS, V21, P386, DOI 10.1109/TIP.2011.2160073; Hu Z, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.136; Hu Z, 2012, LECT NOTES COMPUT SC, V7576, P59, DOI 10.1007/978-3-642-33715-4_5; JIA J, 2007, P IEEE C COMP VIS PA; Joshi N., 2008, P IEEE C COMP VIS PA; Joshi N., 2010, ACM T GRAPHIC, V29, P30, DOI DOI 10.1145/1778765.1778767; Kohler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3; Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521; Kumar S, 2006, INT J COMPUT VISION, V68, P179, DOI 10.1007/s11263-006-7007-9; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Levin A., 2006, ADV NEURAL INFORM PR, V19, P841; Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815; Liu S., 2014, VISUAL COMPUTER; Raskar R, 2006, ACM T GRAPHIC, V25, P795, DOI 10.1145/1141911.1141957; Shan Q., 2007, P IEEE INT C COMP VI; Shan Q., 2008, P ACM SIGGRAPH, P73; Tai Y. W., 2011, PAMI, V33, P1603; Tai Y. - W., 2008, P IEEE C COMP VIS PA; Whyte O, 2010, PROC CVPR IEEE, P491, DOI 10.1109/CVPR.2010.5540175; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157; Yedidia J., 2003, EXPLORING ARTIFICIAL, P236; Yitzhaky Y, 1998, J OPT SOC AM A, V15, P1512, DOI 10.1364/JOSAA.15.001512; Yousaf S, 2013, APPL MECH MATER, V436, P531, DOI 10.4028/www.scientific.net/AMM.436.531; Yuan L., 2007, P ACM SIGGRAPH; Zhong L., 2013, P IEEE C COMP VIS PA	40	22	24	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2015	115	3					345	362		10.1007/s11263-015-0821-1	http://dx.doi.org/10.1007/s11263-015-0821-1			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CW6EH					2022-12-18	WOS:000365089800007
J	Fawzi, A; Davies, M; Frossard, P				Fawzi, Alhussein; Davies, Mike; Frossard, Pascal			Dictionary Learning for Fast Classification Based on Soft-thresholding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dictionary learning; Soft-thresholding; Sparse coding; Rectifier linear units; Neural networks	FACE RECOGNITION; SPARSE; REPRESENTATIONS; ALGORITHM; LIBRARY	Classifiers based on sparse representations have recently been shown to provide excellent results in many visual recognition and classification tasks. However, the high cost of computing sparse representations at test time is a major obstacle that limits the applicability of these methods in large-scale problems, or in scenarios where computational power is restricted. We consider in this paper a simple yet efficient alternative to sparse coding for feature extraction. We study a classification scheme that applies the soft-thresholding nonlinear mapping in a dictionary, followed by a linear classifier. A novel supervised dictionary learning algorithm tailored for this low complexity classification architecture is proposed. The dictionary learning problem, which jointly learns the dictionary and linear classifier, is cast as a difference of convex (DC) program and solved efficiently with an iterative DC solver. We conduct experiments on several datasets, and show that our learning algorithm that leverages the structure of the classification problem outperforms generic learning procedures. Our simple classifier based on soft-thresholding also competes with the recent sparse coding classifiers, when the dictionary is learned appropriately. The adopted classification scheme further requires less computational time at the testing stage, compared to other classifiers. The proposed scheme shows the potential of the adequately trained soft-thresholding mapping for classification and paves the way towards the development of very efficient classification methods for vision problems.	[Fawzi, Alhussein; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland; [Davies, Mike] Univ Edinburgh, IDCOM, Edinburgh, Midlothian, Scotland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of Edinburgh	Fawzi, A (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.	alhussein.fawzi@epfl.ch; mike.davies@ed.ac.uk; pascal.frossard@epfl.ch	Frossard, Pascal/AAF-2268-2019; Davies, Michael/GWV-2527-2022; davies, mike/D-4973-2014	davies, mike/0000-0003-2327-236X	Engineering and Physical Sciences Research Council [EP/K032275/1] Funding Source: researchfish; EPSRC [EP/K032275/1] Funding Source: UKRI	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))		Akata Z, 2014, IEEE T PATTERN ANAL, V36, P507, DOI 10.1109/TPAMI.2013.146; An LTH, 2005, ANN OPER RES, V133, P23, DOI 10.1007/s10479-004-5022-1; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bishop, 1995, NEURAL NETWORKS PATT; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen CF, 2012, PROC CVPR IEEE, P2618, DOI 10.1109/CVPR.2012.6247981; Coates Adam, 2011, P 28 INT C MACH LEAR, P921; Denil M., 2012, ARXIV12080959; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Fadili MJ, 2009, COMPUT J, V52, P64, DOI 10.1093/comjnl/bxm055; Glorot X., 2011, P 14 INT C ART INT S, P315; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; Horst R., 2000, INTRO GLOBAL OPTIMIZ; Huang K., 2006, ADV NEURAL INFORM PR, P609, DOI DOI 10.7551/MITPRESS/7503.001.0001; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Kavukcuoglu K, 2010, ARXIV10103467; Kavukcuoglu Koray, 2010, ADV NEURAL INFORM PR, V23, P1090; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Larochelle H, 2012, J MACH LEARN RES, V13, P643; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ma L, 2012, PROC CVPR IEEE, P2586, DOI 10.1109/CVPR.2012.6247977; Maas A., 2013, ICML WORKSH DEEP LEA, P1; Mairal J., 2012, P 29 INT C INT C MAC, P1835; Mairal J, 2008, NIPS, P1033; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Mairal J, 2010, J MACH LEARN RES, V11, P19; Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592; RAMIREZ I, 2010, PROC CVPR IEEE, P3501, DOI DOI 10.1109/CVPR.2010.5539964; Sriperumbudur B. K., 2007, P 24 INT C MACH LEAR, P831; Tao PD, 1998, SIAM J OPTIMIZ, V8, P476, DOI 10.1137/S1052623494274313; Valkealahti K, 1998, IEEE T PATTERN ANAL, V20, P90, DOI 10.1109/34.655653; Ventura RMFI, 2006, IEEE T IMAGE PROCESS, V15, P726, DOI 10.1109/TIP.2005.860596; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123; Yuille AL, 2002, ADV NEUR IN, V14, P1033; Zeiler MD, 2013, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.2013.6638312; Zhang YMZ, 2013, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2013.93	42	22	24	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2015	114	2-3			SI		306	321		10.1007/s11263-014-0784-7	http://dx.doi.org/10.1007/s11263-014-0784-7			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CP7MJ		Green Submitted, Green Accepted			2022-12-18	WOS:000360071900012
J	Enqvist, O; Ask, E; Kahl, F; Astrom, K				Enqvist, Olof; Ask, Erik; Kahl, Fredrik; Astrom, Kalle			Tractable Algorithms for Robust Model Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Outliers; Geometry; Optimization; 3D reconstruction; Image registration	CONSENSUS	What is the computational complexity of geometric model estimation in the presence of noise and outliers? We show that the number of outliers can be minimized in polynomial time with respect to the number of measurements, although exponential in the model dimension. Moreover, for a large class of problems, we prove that the statistically more desirable truncated -norm can be optimized with the same complexity. In a similar vein, it is also shown how to transform a multi-model estimation problem into a purely combinatorial one-with worst-case complexity that is polynomial in the number of measurements but exponential in the number of models. We apply our framework to a series of hard fitting problems. It gives a practical method for simultaneously dealing with measurement noise and large amounts of outliers in the estimation of low-dimensional models. Experimental results and a comparison to random sampling techniques are presented for the applications rigid registration, triangulation and stitching.	[Enqvist, Olof; Kahl, Fredrik] Chalmers Univ Technol, S-41296 Gothenburg, Sweden; [Ask, Erik; Kahl, Fredrik; Astrom, Kalle] Lund Univ, Lund, Sweden	Chalmers University of Technology; Lund University	Enqvist, O (corresponding author), Chalmers Univ Technol, S-41296 Gothenburg, Sweden.	olof.enqvist@chalmers.se	Åström, Kalle/C-2836-2009	Åström, Kalle/0000-0002-8689-7810				Ask E., 2012, INT C PATT REC TSUK; Bazaraa MS., 2013, NONLINEAR PROGRAMMIN; Bazin J.-C., 2012, C COMP VIS PATT REC; Breuel TM, 2003, COMPUT VIS IMAGE UND, V90, P258, DOI 10.1016/S1077-3142(03)00026-2; Byrod M, 2009, INT J COMPUT VISION, V84, P237, DOI 10.1007/s11263-009-0235-z; CASS T, 1999, INT J COMPUT VISION, V21, P37; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; Enqvist O, 2008, LECT NOTES COMPUT SC, V5302, P141, DOI 10.1007/978-3-540-88682-2_12; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Hartley R., 2004, ROBOTICA; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127; Johnson D. S., 1978, Theoretical Computer Science, V6, P93, DOI 10.1016/0304-3975(78)90006-3; KAHL F, 2001, THESIS LUND I TECHNO; Kahl F, 2008, IEEE T PATTERN ANAL, V30, P1603, DOI 10.1109/TPAMI.2007.70824; Ke Q, 2007, IEEE T PATTERN ANAL, V29, P1834, DOI 10.1109/TPAMI.2007.1083; Lebeda K., 2012, BRIT MACH VIS C GUIL; Li H., 2009, INT C COMP VIS KYOT; Li H., 2007, C COMP VIS PATT REC; Mittal S, 2012, IEEE T PATTERN ANAL, V34, P2351, DOI 10.1109/TPAMI.2012.52; Olsson C., 2010, C COMP VIS PATT REC; Olsson C., 2008, C COMP VIS PATT REC, p[14, 17]; Sim Kristy, 2006, IEEE COMP SOC C COMP, P485; Toldo R., 2008, EUR C COMP VIS MARS; Tordoff B, 2002, LECT NOTES COMPUT SC, V2350, P82; Wills J, 2006, INT J COMPUT VISION, V68, P125, DOI 10.1007/s11263-006-6660-3; Yu J, 2011, PROC CVPR IEEE; Zuliani M., 2005, INT C IM PROC GEN IT	29	22	22	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2015	112	1					115	129		10.1007/s11263-014-0760-2	http://dx.doi.org/10.1007/s11263-014-0760-2			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CC4YC					2022-12-18	WOS:000350361500006
J	Hung, CH; Xu, L; Jia, JY				Hung, Chun Ho; Xu, Li; Jia, Jiaya			Consistent Binocular Depth and Scene Flow with Chained Temporal Profiles	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video depth estimation; Consistent scene flow; Chained temporal profiles; Stereo matching	MOTION ESTIMATION; STEREO; RECOVERY; ACCURACY; VIDEO	We propose a depth and image scene flow estimation method taking the input of a binocular video. The key component is motion-depth temporal consistency preservation, making computation in long sequences reliable. We tackle a number of fundamental technical issues, including connection establishment between motion and depth, structure consistency preservation in multiple frames, and long-range temporal constraint employment for error correction. We address all of them in a unified depth and scene flow estimation framework. Our main contributions include development of motion trajectories, which robustly link frame correspondences in a voting manner, rejection of depth/motion outliers through temporal robust regression, novel edge occurrence map estimation, and introduction of anisotropic smoothing priors for proper regularization.	[Hung, Chun Ho; Xu, Li; Jia, Jiaya] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China	Chinese University of Hong Kong	Jia, JY (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.	chhung@cse.cuhk.edu.hk; xuli@cse.cuhk.edu.hk; leojia@cse.cuhk.edu.hk	Jia, Jiaya/I-3251-2012		Research Grants Council of the Hong Kong SAR [413110]	Research Grants Council of the Hong Kong SAR(Hong Kong Research Grants Council)	The authors would like to thank the associate editor and all the anonymous reviewers for their time and effort. This work is supported by a grant from the Research Grants Council of the Hong Kong SAR (Project No. 413110).	Alvarez L, 2007, INT J COMPUT VISION, V75, P371, DOI 10.1007/s11263-007-0041-4; Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2; Basha T, 2010, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2010.5539791; Black M. J., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P138; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Brox T, 2009, PROC CVPR IEEE, P41, DOI 10.1109/CVPRW.2009.5206697; Bruhn A, 2005, IEEE I CONF COMP VIS, P749; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; Cech J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3129, DOI 10.1109/CVPR.2011.5995442; Furukawa Y, 2007, CVPR, V32, P1362, DOI DOI 10.1109/FDL.2016.7880384; Hadfield S, 2011, IEEE I CONF COMP VIS, P2290, DOI 10.1109/ICCV.2011.6126509; Irani M, 2002, INT J COMPUT VISION, V48, P173, DOI 10.1023/A:1016372015744; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Min D, 2006, INT C PATT RECOG, P74; OpenMP ARB, 2012, OP MULT; Patras I., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P359, DOI 10.1109/ICPR.1996.546049; Rabe C, 2010, LECT NOTES COMPUT SC, V6314, P582, DOI 10.1007/978-3-642-15561-1_42; Richardt C, 2010, LECT NOTES COMPUT SC, V6313, P510; Sand Peter, 2008, International Journal of Computer Vision, V80, P72, DOI 10.1007/s11263-008-0136-6; Sand P., 2006, CVPR, P2195; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Sun DQ, 2008, LECT NOTES COMPUT SC, V5304, P83; Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; University of Auckland, 2008, ENP IM SEQ AN TEST S; Valgaerts L, 2010, LECT NOTES COMPUT SC, V6314, P568, DOI 10.1007/978-3-642-15561-1_41; Vaudrey T., 2008, 23 INT C IM VIS COMP, P1; Vedula S, 2005, IEEE T PATTERN ANAL, V27, P475, DOI 10.1109/TPAMI.2005.63; Vogel C, 2011, IEEE I CONF COMP VIS, P1291, DOI 10.1109/ICCV.2011.6126381; Wedel A, 2008, LECT NOTES COMPUT SC, V5302, P739, DOI 10.1007/978-3-540-88682-2_56; Wedel A, 2011, INT J COMPUT VISION, V95, P29, DOI 10.1007/s11263-010-0404-0; Xiao JJ, 2006, LECT NOTES COMPUT SC, V3951, P211; Xu L, 2008, LECT NOTES COMPUT SC, V5302, P671; Xu L, 2010, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2010.5539820; Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70; Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52; Zhang L, 2003, PROC CVPR IEEE, P367; Zhang Y, 2001, PROC CVPR IEEE, P778; ZHANG ZY, 1992, IEEE T PATTERN ANAL, V14, P1141, DOI 10.1109/34.177380; Zimmer H, 2009, LECT NOTES COMPUT SC, V5681, P207, DOI 10.1007/978-3-642-03641-5_16	41	22	24	0	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					271	292		10.1007/s11263-012-0559-y	http://dx.doi.org/10.1007/s11263-012-0559-y			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO					2022-12-18	WOS:000315501800016
J	Tresadern, PA; Ionita, MC; Cootes, TF				Tresadern, P. A.; Ionita, M. C.; Cootes, T. F.			Real-Time Facial Feature Tracking on a Mobile Device	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Facial feature tracking; Active Appearance Model		This paper presents an implementation of the Active Appearance Model that is able to track a face on a mobile device in real-time. We achieve this performance by discarding an explicit texture model, using fixed-point arithmetic for much of the computation, applying a sequence of models with increasing complexity, and exploiting a sparse basis projection via Haar-like features. Our results show that the Haar-like feature basis achieves similar performance to more traditional approaches while being more suitable for a mobile device. Finally, we discuss mobile applications of the system such as face verification, teleconferencing and human-computer interaction.	[Tresadern, P. A.; Ionita, M. C.; Cootes, T. F.] Univ Manchester, Sch Canc & Enabling Sci, Manchester M13 9PT, Lancs, England	University of Manchester	Tresadern, PA (corresponding author), Univ Manchester, Sch Canc & Enabling Sci, Stopford Bldg, Manchester M13 9PT, Lancs, England.	philip.tresadern@manchester.ac.uk		Cootes, Timothy/0000-0002-2695-9063	European Union (EU) [214324]	European Union (EU)(CGIAREuropean Commission)	The MoBio Consortium would like to thank the European Union (EU) 7th Framework Research Programme for their support under grant agreement number 214324. We especially thank Visidon Ltd., Finland, for their invaluable work in developing the mobile phone user interface. For more information about the MoBio Consortium see http://www.mobioproject.org.	Batur AU, 2005, IEEE T IMAGE PROCESS, V14, P1707, DOI 10.1109/TIP.2005.854473; BERGEN JR, 1992, LECT NOTES COMPUT SC, V588, P237; Cootes T. F., 2001, P IEEE C COMP VIS PA; Cootes T. F., 1998, P BRIT MACH VIS C; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cootes Timothy F, 1998, P EUR C COMP VIS; Crow F. C, 1984, P C ACM SIGGRAPH, V18; Donner R, 2006, IEEE T PATTERN ANAL, V28, P1690, DOI 10.1109/TPAMI.2006.206; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gao XB, 2010, IEEE T SYST MAN CY C, V40, P145, DOI 10.1109/TSMCC.2009.2035631; Gao YJ, 2010, COMPUT ANIMAT VIRT W, V21, P343, DOI 10.1002/cav.340; GOODALL C, 1991, J ROY STAT SOC B MET, V53, P285, DOI 10.1111/j.2517-6161.1991.tb01825.x; Gross R, 2005, IMAGE VISION COMPUT, V23, P1080, DOI 10.1016/j.imavis.2005.07.009; Hou X., 2001, P IEEE C COMP VIS PA; Liang L., 2008, P EUR C COMP VIS; Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3; Liu XM, 2009, IEEE T PATTERN ANAL, V31, P1941, DOI 10.1109/TPAMI.2008.238; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Messer K., 1999, P 2 INT C AUDIO VIDE; Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y; Saragih J., 2007, P IEEE INT C COMP VI; Saragih J., 2006, P IEEE INT C PATT RE; Scott I. M., 2003, P INT C INF PROC MED; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wu H., 2008, P IEEE C COMP VIS PA	26	22	24	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2012	96	3					280	289		10.1007/s11263-011-0464-9	http://dx.doi.org/10.1007/s11263-011-0464-9			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	885HK					2022-12-18	WOS:000299769400002
J	Simon, L; Teboul, O; Koutsourakis, P; Paragios, N				Simon, Loic; Teboul, Olivier; Koutsourakis, Panagiotis; Paragios, Nikos			Random Exploration of the Procedural Space for Single-View 3D Modeling of Buildings	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Building modeling; Shape grammars; Supervised learning; Single-view; Image-based; Random walk		In this paper we tackle the problem of 3D modeling for urban environment using a modular, flexible and powerful approach driven from procedural generation. To this end, typologies of architectures are modeled through shape grammars that consist of a set of derivation rules and a set of shape/dictionary elements. Appearance (from statistical point of view with respect to the individual pixel's properties) of the dictionary elements is then learned using a set of training images. Image classifiers are trained towards recovering image support with respect to the semantics. Then, given a new image and the corresponding footprint, the modeling problem is formulated as a search of the space of shapes, that can be generated on-the-fly by deriving the grammar on the input axiom. Defining an image-based score function for the produced instances using the trained classifiers, the best rules are selected, making sure that we keep exploring the space by allowing some rules to be randomly selected. New rules are then generated by resampling around the selected rules. At the finest level, these rules define the 3D model of the building. Promising results on complex and varying architectural styles demonstrate the potential of the presented method.			Simon, L (corresponding author), Grande Voie Vignes, F-92295 Chatenay Malabry, France.	loic.simon@ecp.fr						Aichholzer O, 1995, J UNIVERS COMPUT SCI, V1, P752, DOI DOI 10.3217/JUCS-001-12-0752; Alegre F, 2004, INT WORKSH VIS TECHN; Bertsekas D. P., 2006, OR, P71; Bishop CM, 2006, PATTERN RECOGNITION; Blake A, 2004, LECT NOTES COMPUT SC, V3021, P428; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Cech J, 2009, INT J IMAG SYST TECH, V19, P69, DOI 10.1002/ima.20181; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Delaunoy A., 2008, BRIT MACH VIS C LEED; Dick AR, 2004, INT J COMPUT VISION, V60, P111, DOI 10.1023/B:VISI.0000029665.07652.61; Eppstein D, 1999, DISCRETE COMPUT GEOM, V22, P569, DOI 10.1007/PL00009479; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; Gargallo P., 2007, P INT C COMP VIS RIO; Gips J., 1975, SHAPE GRAMMARS THEIR; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Karantzalos K., 2010, IEEE T GEOS IN PRESS; Karantzalos K., 2009, INT C IM PROC; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Koutsourakis P., 2009, INT C COMPUTER VISIO; Labatut P, 2007, IEEE I CONF COMP VIS, P504; Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188; Lipp M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360701; Luong QT, 1997, INT J COMPUT VISION, V22, P261, DOI 10.1023/A:1007982716991; Muller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]; Muller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931; Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292; Pons JP, 2005, PROC CVPR IEEE, P822; REZNIK S., 2007, INT ARCH PHOTOGRAMME, V36, P173; Ripperda N., 2007, PHOTOGRAMMETRIC IMAG, P1; Ripperda N, 2006, LECT NOTES COMPUT SC, V4174, P750; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Stiny G., 1975, THESIS BIRKHAUSER; Sutton R.S., 1998, INTRO REINFORCEMENT, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Vu H., 2009, C COMP VIS PATT REC; Winn J., 2006, CVPR; Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324; Zaharescu A, 2007, LECT NOTES COMPUT SC, V4844, P166	39	22	22	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2011	93	2					253	271		10.1007/s11263-010-0370-6	http://dx.doi.org/10.1007/s11263-010-0370-6			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	740PD					2022-12-18	WOS:000288806000008
J	Sfikas, K; Theoharis, T; Pratikakis, I				Sfikas, Konstantinos; Theoharis, Theoharis; Pratikakis, Ioannis			ROSy+: 3D Object Pose Normalization Based on PCA and Reflective Object Symmetry with Application in 3D Object Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose normalization; Reflective symmetry; Principal component analysis; 3D object retrieval; Computational geometry		A novel pose normalization method based on 3D object reflective symmetry is presented. It is a general purpose global pose normalization method; in this paper it is used to enhance the performance of a 3D object retrieval pipeline. Initially, the axis-aligned minimum bounding box of a rigid 3D object is modified by requiring that the 3D object is also in minimum angular difference with respect to the normals to the faces of its bounding box. To estimate the modified axis-aligned bounding box, a set of predefined planes of symmetry are used and a combined spatial and angular distance, between the 3D object and its symmetric object, is calculated. By minimizing the combined distance, the 3D object fits inside its modified axis-aligned bounding box and alignment with the coordinate system is achieved. The proposed method is incorporated in a hybrid scheme, that serves as the alignment method in a 3D object retrieval system. The effectiveness of the 3D object retrieval system, using the hybrid pose normalization scheme, is evaluated in terms of retrieval accuracy and demonstrated using both quantitative and qualitative measures via an extensive consistent evaluation on standard benchmarks. The results clearly show performance boost against current approaches.	[Sfikas, Konstantinos; Theoharis, Theoharis] Univ Athens, Dept Informat & Telecommun, Comp Graph Lab, Athens, Greece; [Pratikakis, Ioannis] Democritus Univ Thrace, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece	National & Kapodistrian University of Athens; Democritus University of Thrace	Sfikas, K (corresponding author), Univ Athens, Dept Informat & Telecommun, Comp Graph Lab, Athens, Greece.	ksfikas@di.uoa.gr; theotheo@di.uoa.gr; ipratika@ee.duth.gr	PRATIKAKIS, IOANNIS/AAD-3387-2019; Theoharis, Theoharis/AAN-2555-2020	PRATIKAKIS, IOANNIS/0000-0002-4124-3688; Sfikas, Konstantinos/0000-0002-9173-4557	Greek State Scholarship Foundation	Greek State Scholarship Foundation(Greek Ministry of Development-GSRT)	This work has been partially funded by the Greek State Scholarship Foundation (I.K.Y.).	Ahn H. K., 2005, SCG 05, P356; Ahn HK, 2008, COMP GEOM-THEOR APPL, V40, P171, DOI 10.1016/j.comgeo.2007.08.001; Ankerst M, 1999, LECT NOTES COMPUT SC, V1651, P207; Bustos B, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P215, DOI 10.1109/TDPVT.2004.1335197; Chan CK, 2001, COMPUT STRUCT, V79, P1433, DOI 10.1016/S0045-7949(01)00046-3; Chaouch M., 2007, IEEE INT C MULT EXP; Chaouch M, 2009, GRAPH MODELS, V71, P63, DOI 10.1016/j.gmod.2008.12.006; Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669; Cornea ND, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P366, DOI 10.1109/SMI.2005.1; Daras P, 2006, IEEE T MULTIMEDIA, V8, P101, DOI 10.1109/TMM.2005.861287; Elad M., 2001, PROC 6 EUROPGRAPHICS, P97; Fang R, 2008, LECT NOTES COMPUT SC, V5358, P381, DOI 10.1007/978-3-540-89639-5_37; GOLDSMITH J, 1987, IEEE COMPUT GRAPH, V7, P14, DOI 10.1109/MCG.1987.276983; Goldstein H., 2001, CLASSICAL MECH; Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244; Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418; Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007; Kazhdan M, 2002, LECT NOTES COMPUT SC, V2351, P642; Kazhdan M., 2003, S GEOMETRY PROCESSIN; Kazhdan M, 2007, IEEE T PATTERN ANAL, V29, P1221, DOI 10.1109/TPAMI.2007.1032; Kim DH, 2004, LECT NOTES COMPUT SC, V3332, P238; Lian ZH, 2010, INT J COMPUT VISION, V89, P130, DOI 10.1007/s11263-009-0295-0; Martinet A, 2006, ACM T GRAPHIC, V25, P439, DOI 10.1145/1138450.1138462; MINOVIC P, 1993, IEEE T PATTERN ANAL, V15, P507, DOI 10.1109/34.211472; Mitchell E. E. L., 1965, SIMULATION, V4, P390, DOI [10.1177/003754976500400610, DOI 10.1177/003754976500400610]; Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924; Novotni M, 2004, COMPUT AIDED DESIGN, V36, P1047, DOI 10.1016/j.cad.2004.01.005; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Papadakis P., 2008, EUR WORKSH 3D OBJ RE, P8; Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026; Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6; Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5; Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923; Ricard J, 2005, PATTERN RECOGN LETT, V26, P2174, DOI 10.1016/j.patrec.2005.03.030; Rusinkiewicz S., 2004, P 2004 EUR ACM SIGGR, P115, DOI DOI 10.1145/1057432.1057448; Rustamov RM, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P13, DOI 10.1109/SMI.2007.6; Shilane P, 2004, P SHAP MOD INT; Sun CM, 1997, IEEE T PATTERN ANAL, V19, P164, DOI 10.1109/34.574800; Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609; Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0; Theodoridis S, 2006, PATTERN RECOGNITION, 3RD EDITION, P1; Theoharis T., 2008, GRAPHICS VISUALIZATI; van den Bergen G., 1997, J GRAPH TOOLS, V2, P1, DOI [10.1080/10867651.1997.10487480, DOI 10.1080/10867651.1997.10487480]; Vranic D., 2004, THESIS U LEIPZIG; Vranic D. V., 2005, CONTENT BASED CLASSI; Vranic DV, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P963; VRANIC DV, 2001, TOOLS 3D OBJECT RETR; Xiang P, 2007, LECT NOTES COMPUT SC, V4488, P25; Yu ZW, 2007, LECT NOTES ARTIF INT, V4571, P643; Zaharia T, 2001, PROC SPIE, V4304, P133, DOI 10.1117/12.424969; Zaharia T., 2004, SPIE C IM PROC ALG S; Zarpalas D, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/23912; Zhang J, 2005, LECT NOTES COMPUT SC, V3757, P285, DOI 10.1007/11585978_19; [No title captured]	54	22	24	5	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2011	91	3					262	279		10.1007/s11263-010-0395-x	http://dx.doi.org/10.1007/s11263-010-0395-x			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	711RY					2022-12-18	WOS:000286610300003
J	Bergtholdt, M; Kappes, J; Schmidt, S; Schnorr, C				Bergtholdt, Martin; Kappes, Joerg; Schmidt, Stefan; Schnoerr, Christoph			A Study of Parts-Based Object Class Detection Using Complete Graphs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Object class recognition; Graphical models; Conditional random fields; Classification; Multi-class; Learning; Inference; Optimization; Single view; Multiple view; 2D/3D pose	POSE ESTIMATION; RECOGNITION; CONFIGURATIONS; PROPAGATION; FEATURES; MODELS	Object detection is one of the key components in modern computer vision systems. While the detection of a specific rigid object under changing viewpoints was considered hard just a few years ago, current research strives to detect and recognize classes of non-rigid, articulated objects. Hampered by the omnipresent confusing information due to clutter and occlusion, the focus has shifted from holistic approaches for object detection to representations of individual object parts linked by structural information, along with richer contextual descriptions of object configurations. Along this line of research, we present a practicable and expandable probabilistic framework for parts-based object class representation, enabling the detection of rigid and articulated object classes in arbitrary views. We investigate learning of this representation from labelled training images and infer globally optimal solutions to the contextual MAP-detection problem, using A (*)-search with a novel lower-bound as admissible heuristic. An assessment of the inference performance of Belief-Propagation and Tree-Reweighted Belief Propagation is obtained as a by-product. The generality of our approach is demonstrated on four different datasets utilizing domain dependent information cues.	[Bergtholdt, Martin; Kappes, Joerg; Schmidt, Stefan; Schnoerr, Christoph] Heidelberg Univ, Dept Math & Comp Sci, D-69115 Heidelberg, Germany	Ruprecht Karls University Heidelberg	Bergtholdt, M (corresponding author), Heidelberg Univ, Dept Math & Comp Sci, Speyerer Str 4-6, D-69115 Heidelberg, Germany.	bergtholdt@web.de; kappes@math.uni-heidelberg.de; schmidt@math.uni-heidelberg.de; schnoerr@math.uni-heidelberg.de			European Marie Curie research training network VISIONTRAIN; Philips Research Europe, Hamburg	European Marie Curie research training network VISIONTRAIN(European Commission); Philips Research Europe, Hamburg	This work has been partially supported by the European Marie Curie research training network VISIONTRAIN and by Philips Research Europe, Hamburg. We also thank the anonymous reviewers for their detailed and constructive criticism that helped to improve the original version of the manuscript.	Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; BALAN A, 2007, ICCV; BECKER F, 2004, THESIS U MANNHEIM; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BENNETT K, 2006, INT JOINT C NEUR NET, V7, P1265; BERGTHOLDT M, 2006, ANN S GERM ASS PATT; BERGTHOLDT M, 2006, INT WORKS REPR US PR; Bray M, 2006, LECT NOTES COMPUT SC, V3952, P642; Chen M, 2006, NAT CLIN PRACT NEURO, V2, P2, DOI 10.1038/ncpneuro0087; COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104; COUGHLAN J, 2004, CVPRW, P180; Coughlan JM, 2002, LECT NOTES COMPUT SC, V2352, P453; Coughlan JM, 2002, NEURAL COMPUT, V14, P1929, DOI 10.1162/089976602760128072; Cowell R. G., 2003, PROBABILISTIC NETWOR; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; deGroot M.H., 1982, STATISTICIAN, V32, P12; Everingham M., 2006, PASCAL VISUAL OBJECT; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Fawcett T., 2004, ROC GRAPHS NOTES PRA; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Fergus R, 2007, INT J COMPUT VISION, V71, P273, DOI 10.1007/s11263-006-8707-x; Fergus R, 2003, PROC CVPR IEEE, P264; FERGUS R, 2001, EFFICIENT METHODS OB; FERGUS R, 2005, CVPR; FERRARI V, 2008, ICCV; Frey BJ, 2005, IEEE T PATTERN ANAL, V27, P1392, DOI 10.1109/TPAMI.2005.169; Gavrila DM, 2007, IEEE T PATTERN ANAL, V29, P1408, DOI 10.1109/TPAMI.2007.1062; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Gupta A, 2008, IEEE T PATTERN ANAL, V30, P493, DOI 10.1109/TPAMI.2007.1173; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; HARTLEY RI, 1992, LECT NOTES COMPUT SC, V588, P579; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; HOWE NR, 2007, CVPR EHUM2 2 WORKSH; Jiang H, 2008, PROC CVPR IEEE, P895; KOLMOGOROV V, 2006, ECCV; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kumar S, 2006, INT J COMPUT VISION, V68, P179, DOI 10.1007/s11263-006-7007-9; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Lee MW, 2006, IEEE T PATTERN ANAL, V28, P905, DOI 10.1109/TPAMI.2006.110; Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188; Levin A, 2006, LECT NOTES COMPUT SC, V3954, P581; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mikolajczyk K., 2004, ECCV; Mori G, 2006, IEEE T PATTERN ANAL, V28, P1052, DOI 10.1109/TPAMI.2006.149; Pearl J., 1984, INTELLIGENT SEARCH S; Pham TV, 2005, COMPUT VIS IMAGE UND, V99, P241, DOI 10.1016/j.cviu.2005.01.006; Platt JC, 2000, ADV NEUR IN, P61; PONCE J, 2006, LECT NOTES COMP SCI, V4170; Quattoni A, 2004, NIPS; Ramanan D, 2007, IEEE T PATTERN ANAL, V29, P65, DOI 10.1109/TPAMI.2007.250600; Rifkin R, 2004, J MACH LEARN RES, V5, P101; Roberts TJ, 2007, INT J COMPUT VISION, V73, P285, DOI 10.1007/s11263-006-9781-9; Rosenhahn B, 2007, INT J COMPUT VISION, V73, P243, DOI [10.1007/s11263-006-9965-3, 10.1007/S11263-006-9965-3]; Russell SJ, 1995, ARTIF INTELL, V4th; Schmidt S, 2007, LECT NOTES COMPUT SC, V4584, P122; Seemann E., 2006, CVPR; SIGAL L, 2006, CS0608 BROWN U DEP C; SIGAL L, 2006, CVPR, V2; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Sminchisescu C, 2007, IEEE T PATTERN ANAL, V29, P2030, DOI 10.1109/TPAMI.2007.1111; Sudderth EB, 2003, PROC CVPR IEEE, P605; Sutton C, 2007, J MACH LEARN RES, V8, P693; SZELISKI R, 2006, ECCV; WAINWRIGHT M, 2006, ADV NEUR INF PROC SY, P1425; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Weber M, 2000, LECT NOTES COMPUT SC, V1842, P18; Welk M, 2007, SIGNAL PROCESS, V87, P291, DOI 10.1016/j.sigpro.2005.12.013; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Winkler G., 2006, IMAGE ANAL RANDOM FI; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085; Yuille AL, 2000, PATTERN RECOGN, V33, P603, DOI 10.1016/S0031-3203(99)00075-8; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4; ZHANG L, 2007, ICCV	76	22	22	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2010	87	1-2			SI		93	117		10.1007/s11263-009-0209-1	http://dx.doi.org/10.1007/s11263-009-0209-1			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	539GP					2022-12-18	WOS:000273242300006
J	Yoon, KJ; Prados, E; Sturm, P				Yoon, Kuk-Jin; Prados, Emmanuel; Sturm, Peter			Joint Estimation of Shape and Reflectance using Multiple Images with Known Illumination Conditions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Workshop on Photometric Analysis for Computer Vision held in Conjunction with the 11th International Conference on Computer Vision Conference	OCT 04, 2007	Rio de Janeiro, BRAZIL			3D reconstruction; Reflectance estimation; Multiview stereo; Photometric stereo; Multiview shape from shading	MULTIVIEW STEREO RECONSTRUCTION	We propose a generative model based method for recovering both the shape and the reflectance of the surface(s) of a scene from multiple images, assuming that illumination conditions and cameras calibration are known in advance. Based on a variational framework and via gradient descents, the algorithm minimizes simultaneously and consistently a global cost functional with respect to both shape and reflectance. The motivations for our approach are threefold. (1) Contrary to previous works which mainly consider specific individual scenarios, our method applies indiscriminately to a number of classical scenarios; in particular it works for classical stereovision, multiview photometric stereo and multiview shape from shading. It works with changing as well as static illumination. (2) Our approach naturally combines stereo, silhouette and shading cues in a single framework. (3) Moreover, unlike most previous methods dealing with only Lambertian surfaces, the proposed method considers general dichromatic surfaces. We verify the method using various synthetic and real data sets.	[Yoon, Kuk-Jin] Gwangju Inst Sci & Technol, Comp Vis Lab, Dept Informat & Commun, Kwangju 500712, South Korea; [Prados, Emmanuel; Sturm, Peter] INRIA Grenoble, Percept Team, Montbonnot St Martin, Rhone Alpes, France	Gwangju Institute of Science & Technology (GIST)	Yoon, KJ (corresponding author), Gwangju Inst Sci & Technol, Comp Vis Lab, Dept Informat & Commun, Room C508,261 Cheomdan Gwagiro Oryong Dong, Kwangju 500712, South Korea.	kjyoon@gist.ac.kr; Emmanuel.Prados@inrialpes.fr; Peter.Sturm@inrialpes.fr	Yoon, Kuk-Jin/F-4329-2018					Bhat DN, 1998, INT J COMPUT VISION, V26, P91, DOI 10.1023/A:1007940725322; BIRKBECK N, 2006, EUR C COMP VIS, V1, P536; Blinn J. F., 1997, SIGGRAPH 77, P192; Charpiat G, 2007, INT J COMPUT VISION, V73, P325, DOI 10.1007/s11263-006-9966-2; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; GARGALLO P, 2008, THESIS I NATL POLYTE; Gargallo P, 2007, IEEE I CONF COMP VIS, P1364; Georghiades AS, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P816; GOESELE M, 2006, IEEE C COMP VIS PATT, V2, P2402; Goldman DB, 2005, IEEE I CONF COMP VIS, P341; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Hertzmann A, 2005, IEEE T PATTERN ANAL, V27, P1254, DOI 10.1109/TPAMI.2005.158; JIN H, 2008, INT J COMPUTER VISIO, V76; Jin HL, 2005, INT J COMPUT VISION, V63, P175, DOI 10.1007/s11263-005-6876-7; Jin HL, 2004, PROC CVPR IEEE, P36; Jin HL, 2003, J SCI COMPUT, V19, P267, DOI 10.1023/A:1025308109816; Jin HL, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P626, DOI 10.1109/TDPVT.2002.1024128; Kim J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1033, DOI 10.1109/ICCV.2003.1238463; KOLEV K, 2007, WORKSH PHOT AN COMP; Kolev K, 2007, LECT NOTES COMPUT SC, V4679, P441; LEE HC, 1990, IEEE T PATTERN ANAL, V12, P402, DOI 10.1109/34.50626; LU JP, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P80; Mallick SP, 2005, PROC CVPR IEEE, P619; NGAN A, 2005, EUR S REND, P117; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Osher S., 2002, LEVEL SET METHOD DYN; Paris S, 2006, INT J COMPUT VISION, V66, P141, DOI 10.1007/s11263-005-3953-x; Pons JP, 2007, INT J COMPUT VISION, V72, P179, DOI 10.1007/s11263-006-8671-5; Pons JP, 2005, PROC CVPR IEEE, P822; Powell MW, 2001, IEEE T PATTERN ANAL, V23, P1022, DOI 10.1109/34.955114; SCHULTZ H, 1994, IEEE T PATTERN ANAL, V16, P195, DOI 10.1109/34.273732; SEITZ SM, 2006, IEEE C COMP VIS PATT, P519; Sethian J.A., 1999, LEVEL SET METHODS FA, V2nd; Snow D, 2000, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2000.855839; Soatto S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P974; Solem JE, 2005, LECT NOTES COMPUT SC, V3459, P419; Stark MM, 2005, IEEE T VIS COMPUT GR, V11, P126, DOI 10.1109/TVCG.2005.26; TRAN S, 2006, EUR C COMP VIS, V2, P219; Vogiatzis G, 2005, IEEE I CONF COMP VIS, P228; Vogiatzis G, 2007, IEEE T PATTERN ANAL, V29, P2241, DOI 10.1109/TPAMI.2007.70712; Yang RG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P576, DOI 10.1109/ICCV.2003.1238399; Yezzi A, 2003, INT J COMPUT VISION, V53, P31, DOI 10.1023/A:1023079624234; Yoon KJ, 2006, LECT NOTES COMPUT SC, V3852, P761; Yu TL, 2007, INT J COMPUT VISION, V73, P123, DOI 10.1007/s11263-006-9373-8; Yu TL, 2004, PROC CVPR IEEE, P226; ZACH C, 2006, 3DPVT, P113; ZHOU W, 2002, EUR C COMP VIS, P206; ZICKLER T, 2006, IEEE C COMP VIS PATT, P1801; Zickler TE, 2002, INT J COMPUT VISION, V49, P215, DOI 10.1023/A:1020149707513; Zickler T, 2008, INT J COMPUT VISION, V79, P13, DOI 10.1007/s11I263-007-0087-3	50	22	24	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	2-3			SI		192	210		10.1007/s11263-009-0222-4	http://dx.doi.org/10.1007/s11263-009-0222-4			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	534NA		Green Submitted			2022-12-18	WOS:000272903200006
J	Hodneland, E; Tai, XC; Gerdes, HH				Hodneland, Erlend; Tai, Xue-Cheng; Gerdes, Hans-Hermann			Four-Color Theorem and Level Set Methods for Watershed Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Watershed; Level set; Four-Color theorem; Cell segmentation; Markers	ELLIPTIC INVERSE PROBLEMS; IMAGE SEGMENTATION; CELL SEGMENTATION; ACTIVE CONTOURS; EFFICIENT; MUMFORD; MODEL; ALGORITHM; MINIMIZATION; MICROSCOPY	A marker-controlled and regularized watershed segmentation is proposed for cell segmentation. Only a few previous studies address the task of regularizing the obtained watershed lines from the traditional marker-controlled watershed segmentation. In the present formulation, the topographical distance function is applied in a level set formulation to perform the segmentation, and the regularization is easily accomplished by regularizing the level set functions. Based on the well-known Four-Color theorem, a mathematical model is developed for the proposed ideas. With this model, it is possible to segment any 2D image with arbitrary number of phases with as few as one or two level set functions. The algorithm has been tested on real 2D fluorescence microscopy images displaying rat cancer cells, and the algorithm has also been compared to a standard watershed segmentation as it is implemented in MATLAB. For a fixed set of markers and a set of challenging images, the comparison of these two methods shows that the present level set formulation performs better than a standard watershed segmentation.	[Tai, Xue-Cheng] Nanyang Technol Univ, Div Math Sci, Sch Math & Phys Sci, Singapore 637616, Singapore; [Hodneland, Erlend; Gerdes, Hans-Hermann] Univ Bergen, Dept Biomed, N-5009 Bergen, Norway; [Tai, Xue-Cheng] Univ Bergen, Dept Math, N-5007 Bergen, Norway	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University of Bergen; University of Bergen	Tai, XC (corresponding author), Nanyang Technol Univ, Div Math Sci, Sch Math & Phys Sci, Singapore 637616, Singapore.	xctai@ntu.edu.sg	tai, xuecheng/L-9821-2013	tai, xuecheng/0000-0003-3359-9104	Norwegian Cancer Society [A05103/004]	Norwegian Cancer Society(Norwegian Cancer Society)	The authors wish to thank Steffen Gurke and Nickolay Bukhoresthliev for providing the majority of pictures in this work. Erlend Hodneland was supported by the Norwegian Cancer Society, project number A05103/004.	ADIGA P, 2003, MICROSC RES TECHNIQ, V54, P260; Adiga PSU, 1999, MICROSC RES TECHNIQ, V44, P49; Adiga U, 2006, IEEE T IMAGE PROCESS, V15, P2259, DOI 10.1109/TIP.2006.875205; APPEL K, 1977, ILLINOIS J MATH, V21, P429, DOI 10.1215/ijm/1256049011; Arbelaez PA, 2004, J MATH IMAGING VIS, V20, P43, DOI 10.1023/B:JMIV.0000011318.77653.44; Baggett D, 2005, CYTOM PART A, V67A, P137, DOI 10.1002/cyto.a.20162; Bamford P, 1998, SIGNAL PROCESS, V71, P203, DOI 10.1016/S0165-1684(98)00145-5; Bengtsson E., 2004, Pattern Recognition and Image Analysis, V14, P157; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; Chambolle A, 2004, J MATH IMAGING VIS, V20, P89; Chan TF, 2004, J COMPUT PHYS, V193, P40, DOI 10.1016/j.jcp.2003.08.003; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chan TF, 2006, IMAGE PROCESSING BAS, P175; Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1522, DOI 10.1109/83.862630; Chien SY, 2003, IEEE T CIRC SYST VID, V13, P453, DOI 10.1109/TCSVT.2003.811605; CHRISTIANSEN O, 2006, IMAGE PROCESSING BAS, P289; Chung G, 2005, LECT NOTES COMPUT SC, V3757, P439, DOI 10.1007/11585978_29; Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915; Darbon J, 2006, J MATH IMAGING VIS, V26, P261, DOI 10.1007/s10851-006-8803-0; De Solorzano CO, 2001, J MICROSC-OXFORD, V201, P404, DOI 10.1046/j.1365-2818.2001.00854.x; Dow AI, 1996, CYTOMETRY, V25, P71, DOI 10.1002/(SICI)1097-0320(19960901)25:1<71::AID-CYTO8>3.0.CO;2-H; Dufour A, 2005, IEEE T IMAGE PROCESS, V14, P1396, DOI 10.1109/TIP.2005.852790; Felkel P., 2002, COMPUT GRAPH FORUM, V20, P2001; Fok YL, 1996, IEEE T MED IMAGING, V15, P353, DOI 10.1109/42.500144; GAUTAMA S, 2004, INT ARCH PHOTOGRAMME, V34; Gonzalez R C, 1992, DIGITAL IMAGE PROCES; Grau V, 2004, IEEE T MED IMAGING, V23, P447, DOI 10.1109/TMI.2004.824224; Hodneland E, 2006, CYTOM PART A, V69A, P961, DOI 10.1002/cyto.a.20302; Jung YM, 2007, SIAM J APPL MATH, V67, P1213, DOI 10.1137/060662708; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Li HW, 2007, INT SER NUMER MATH, V154, P307; Li HW, 2007, INT J NUMER ANAL MOD, V4, P291; Lie J, 2006, IEEE T IMAGE PROCESS, V15, P1171, DOI 10.1109/TIP.2005.863956; Lie J, 2005, INT J NUMER ANAL MOD, V2, P422; Lie J, 2006, MATH COMPUT, V75, P1155, DOI 10.1090/S0025-5718-06-01835-7; LINDBLAD L, 2002, THESIS ACTA U UPSALI; LU T, 1992, RAIRO-MATH MODEL NUM, V26, P673; LU T, 1991, APPL MATH LETT, V4, P25, DOI 10.1016/0893-9659(91)90161-N; Malpica N, 1997, CYTOMETRY, V28, P289, DOI 10.1002/(SICI)1097-0320(19970801)28:4<289::AID-CYTO3>3.0.CO;2-7; Mattes J., 2001, INT C MED IM COMP CO, P1373; MEYER F, 1994, SIGNAL PROCESS, V38, P113, DOI 10.1016/0165-1684(94)90060-4; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; NAJMAN L, 1994, SIGNAL PROCESS, V38, P99, DOI 10.1016/0165-1684(94)90059-0; Nath SK, 2006, LECT NOTES COMPUT SC, V4190, P101; Nguyen HT, 2003, IEEE T PATTERN ANAL, V25, P330, DOI 10.1109/TPAMI.2003.1182096; Nielsen LK, 2007, INT J NUMER ANAL MOD, V4, P74; NIELSEN LK, 2006, IMAGE PROCESSING BAS, P403; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Osma-Ruiz V, 2007, PATTERN RECOGN, V40, P1078, DOI 10.1016/j.patcog.2006.06.025; Rambabu C, 2007, J SYST ARCHITECT, V53, P210, DOI 10.1016/j.sysarc.2005.12.005; SONG B, 2002, CAM0268 UCLA; Tai X.-C., 2004, INT J NUMER ANAL MOD, V1, P25; Tai XC, 2006, J COMPUT MATH, V24, P435; Tai XC, 2007, LECT NOTES COMPUT SC, V4485, P178; Tai XC, 2007, APPL NUMER MATH, V57, P686, DOI 10.1016/j.apnum.2006.07.010; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344; VINCENT L, 1994, DIGITAL IMAGE PROCES, P43; Wahlby C, 2004, J MICROSC-OXFORD, V215, P67, DOI 10.1111/j.0022-2720.2004.01338.x; WEI P, 2007, 7 WORLD C STRUCT MUL; Weickert J, 1998, IEEE T IMAGE PROCESS, V7, P398, DOI 10.1109/83.661190; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; [No title captured]; [No title captured]	64	22	23	1	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2009	82	3					264	283		10.1007/s11263-008-0199-4	http://dx.doi.org/10.1007/s11263-008-0199-4			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	411TF					2022-12-18	WOS:000263672800003
J	Karacali, B				Karacali, Bilge			Information theoretic deformable registration using local image information	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						deformable image registration; multi-modality images; information theoretic image similarity; mutual information; joint entropy; energy minimization	NONRIGID REGISTRATION; SIMILARITY MEASURES; MUTUAL-INFORMATION; MAXIMIZATION	We present a deformable registration algorithm for multi-modality images based on information theoretic similarity measures at the scale of individual image voxels. We derive analytical expressions for the mutual information, the joint entropy, and the sum of marginal entropies of two images over a small neighborhood in terms of image gradients. Using these expressions, we formulate image registration algorithms maximizing local similarity over the whole image domain in an energy minimization framework. This strategy produces highly elastic image alignment as the registration is driven by voxel similarities between the images, the algorithms are easily implementable using the closed-form expressions for the derivative of the optimization function with respect to the deformation, and avoid estimation of joint and marginal probability densities governing the image intensities essential to conventional information theoretic image registration methods.	Drexel Univ, Sch Biomed Engn, Sci & Hlth Syst, Ctr Integrated Bioinformat, Philadelphia, PA 19104 USA	Drexel University	Karacali, B (corresponding author), Drexel Univ, Sch Biomed Engn, Sci & Hlth Syst, Ctr Integrated Bioinformat, Philadelphia, PA 19104 USA.	bilge@drexel.edu	KARAÇALI, Bilge/A-1347-2018	KARAÇALI, Bilge/0000-0002-7765-6329				BREIMAN L, 1993, IEEE T INFORM THEORY, V39, P999, DOI 10.1109/18.256506; Collignon A., 1995, Computer Vision, Virtual Reality and Robotics in Medicine. First International Conference, CVRMed '95. Proceedings, P195; COLLIGNON A, 1995, COMP IMAG VIS, V3, P263; Crum WR, 2003, NEUROIMAGE, V20, P1425, DOI 10.1016/j.neuroimage.2003.07.014; Hallpike L, 2002, IMAGING, V4, P455, DOI DOI 10.1259/IMG.14.6.140455; KARACALT B, 2004, IEEE INT S BIOM IM, P1455; Kim J, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING, PROCEEDINGS, P353, DOI 10.1109/ISBI.2002.1029266; Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664; Maintz J B, 1996, Med Image Anal, V1, P151, DOI 10.1016/S1361-8415(96)80010-7; Maintz JBA, 1996, IEEE T PATTERN ANAL, V18, P353, DOI 10.1109/34.491617; Mattes D, 2003, IEEE T MED IMAGING, V22, P120, DOI 10.1109/TMI.2003.809072; Paillou P, 1999, IEEE T GEOSCI REMOTE, V37, P2099, DOI 10.1109/36.774720; Penney GP, 1998, IEEE T MED IMAGING, V17, P586, DOI 10.1109/42.730403; Pluim JPW, 2000, IEEE T MED IMAGING, V19, P809, DOI 10.1109/42.876307; Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867; Pluim JPW, 2003, IEEE T MED IMAGING, V22, P1341, DOI 10.1109/TMI.2003.819272; PLUIM JPW, 2001, P SOC PHOTO-OPT INS, V4322, P79; ROCHE A, 1998, MULTIMODAL REGISTRAT; Rogelj P, 2003, COMPUT VIS IMAGE UND, V92, P112, DOI 10.1016/S1077-3142(03)00116-4; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; SCHARSTEIN D, 1994, INT C PATT RECOG, P572, DOI 10.1109/ICPR.1994.576363; STUDHOLME C, 1995, COMP IMAG VIS, V3, P287; Toga AW, 2001, IMAGE VISION COMPUT, V19, P3, DOI 10.1016/S0262-8856(00)00055-X; VIOLA P, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P16, DOI 10.1109/ICCV.1995.466930; Wei GQ, 1998, IEEE T PATTERN ANAL, V20, P1143, DOI 10.1109/34.730551; WELLS MW, 1995, P 2 ANN INT S MED RO, P55; West J, 1997, J COMPUT ASSIST TOMO, V21, P554, DOI 10.1097/00004728-199707000-00007; WOODS RP, 1993, J COMPUT ASSIST TOMO, V17, P536, DOI 10.1097/00004728-199307000-00004	28	22	23	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2007	72	3					219	237		10.1007/s11263-006-8704-0	http://dx.doi.org/10.1007/s11263-006-8704-0			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	133MW					2022-12-18	WOS:000244018000001
J	Vogt, S; Khamene, A; Sauer, F				Vogt, Sebastian; Khamene, Ali; Sauer, Frank			Reality augmentation for medical procedures: System architecture, single camera marker tracking, and system evaluation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						augmented reality; in-situ visualization; computer assisted surgery; real-time system; single camera marker tracking; AR system architecture	GUIDED INTERVENTIONS	Augmented Reality is an emerging technology that seeks to enhance a user's view by overlaying graphical information. We developed a prototype AR system geared for medical applications. It is built around a stereoscopic head-mounted display of the video-see-through variety. The newest generation of this prototype system exhibits high performance on a standard PC platform. Stereoscopic video images are augmented with medical graphics in real-time at 30 frames per second and with XGA (1024 x 768) resolution. The system provides a compelling AR perception: the graphics appears firmly anchored in the scene-there is no time lag between video and graphics or any apparent jitter of the graphics. With the head-mounted display, the user has a natural and direct access to understanding the 3D structure of the scene, based on both stereo and kinetic depth cues. In the present paper, we describe in detail the architecture and several features of the AR prototype system. Head tracking is accomplished with a single-camera system, with the dedicated tracker camera placed on the head-mounted display. This configuration is the foundation of achieving a high-accuracy graphics overlay. We are now exploring the use of the prototype system for a variety of medical applications. This paper gives an overview over the pre-clinical tests that we have performed for interventional guidance. Overall, the feedback has been very positive and encouraging, and we are continuing to work towards realizing the clinical potential of the technology.	Siemens Corp Res, Imaging & Visualizat Dept, Princeton, NJ 08540 USA	Siemens AG	Vogt, S (corresponding author), Siemens Corp Res, Imaging & Visualizat Dept, 755 College Rd E, Princeton, NJ 08540 USA.	Sebastian.Vogt@siemens.com; Ali.Khamene@siemens.com; Sauer.Frank@siemens.com						Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459; Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355; BAJURA M, 1992, COMP GRAPH, V26, P203, DOI 10.1145/142920.134061; Birkfellner W, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P54, DOI 10.1109/ISAR.2000.880923; Edwards P J, 1995, J Image Guid Surg, V1, P172, DOI 10.1002/(SICI)1522-712X(1995)1:3<172::AID-IGS7>3.3.CO;2-U; Edwards PJ, 2000, IEEE T MED IMAGING, V19, P1082, DOI 10.1109/42.896784; Fuchs H, 1998, LECT NOTES COMPUT SC, V1496, P934, DOI 10.1007/BFb0056282; Grimson WEL, 1999, SCI AM, V280, P62; Haralick R. M., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P592, DOI 10.1109/CVPR.1991.139759; Haralick R.M., 1993, COMPUTER ROBOT VISIO; HOFF W, 1998, P 1 INT WORKSH AUGM, P167; Khamene A, 2003, LECT NOTES COMPUT SC, V2879, P344; King AP, 2000, PRESENCE-VIRTUAL AUG, V9, P360, DOI 10.1162/105474600566862; Quan L, 1999, IEEE T PATTERN ANAL, V21, P774, DOI 10.1109/34.784291; Rolland JP, 2000, PRESENCE-VIRTUAL AUG, V9, P287, DOI 10.1162/105474600566808; Rosenthal M, 2002, MED IMAGE ANAL, V6, P313, DOI 10.1016/S1361-8415(02)00088-9; Rubino GJ, 2000, NEUROSURGERY, V46, P643, DOI 10.1097/00006123-200003000-00023; Sauer F., 2002, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2002. 5th International Conference. Proceedings, Part II (Lecture Notes in Computer Science Vol.2489), P116; Sauer F, 2003, PROC SPIE, V5029, P384, DOI 10.1117/12.480383; Sauer F, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P47, DOI 10.1109/ISAR.2000.880922; State A., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P439, DOI 10.1145/237170.237283; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; Vogt S, 2002, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P127, DOI 10.1109/ISMAR.2002.1115082; VOGT S, 2004, P SPIE C MED IM 200J, V5637, P100; VOGT S, 2004, P 12 ANN MED MEETS V, P397; Wang RW, 2003, CHEM J CHINESE U, V24, P205	26	22	45	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2006	70	2					179	190		10.1007/s11263-006-7938-1	http://dx.doi.org/10.1007/s11263-006-7938-1			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	076SO					2022-12-18	WOS:000239978200006
J	Tu, Z; Zhu, SC				Tu, Zhuowen; Zhu, Song-Chun			Parsing images into regions, curves, and curve groups	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image segmentation; perceptual organization; curve grouping; graph partition; data-driven Markov chain Monte Carlo; Metropolized Gibbs sampler		In this paper, we present an algorithm for parsing natural images into middle level vision representations-regions, curves, and curve groups (parallel curves and trees). This algorithm is targeted for an integrated solution to image segmentation and curve grouping through Bayesian inference. The paper makes the following contributions. (1) It adopts a layered (or 2. 1 D-sketch) representation integrating both region and curve models which compete to explain an input image. The curve layer occludes the region layer and curves observe a partial order occlusion relation. (2) A Markov chain search scheme Metropolized Gibbs Samplers (MGS) is studied. It consists of several pairs of reversible jumps to traverse the complex solution space. An MGS proposes the next state within the jump scope of the current state according to a conditional probability like a Gibbs sampler and then accepts the proposal with a Metropolis-Hastings step. This paper discusses systematic design strategies of devising reversible jumps for a complex inference task. (3) The proposal probability ratios in jumps are factorized into ratios of discriminative probabilities. The latter are computed in a bottom-up process, and they drive the Markov chain dynamics in a data-driven Markov chain Monte Carlo framework. We demonstrate the performance of the algorithm in experiments with a number of natural images.	Univ Calif Los Angeles, Dept Neurol, Lab Neuroimaging, Los Angeles, CA 90095 USA; Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles; University of California System; University of California Los Angeles	Tu, Z (corresponding author), Univ Calif Los Angeles, Dept Neurol, Lab Neuroimaging, Los Angeles, CA 90095 USA.	ztu@stat.ucla.edu; sczhu@stat.ucla.edu						August J, 2003, IEEE T PATTERN ANAL, V25, P387, DOI 10.1109/TPAMI.2003.1190567; BARBU A, 2005, 409 UCLA DEP STAT; BARBU A, 2003, P INT C COMP VIS NIC; BREMAUD P, 1999, MARKOV CHAINS GIBBS, pCH6; BUBLEY R, 1997, P 38 ANN IEEE S FDN; CANDES EJ, 1998, THESIS YALE U; CARLSSON S, 1998, SIGNAL PROCESSING, V15; COMANICIU D, 1999, P INT C COMP VIS; Cooper C, 1999, RANDOM STRUCT ALGOR, V15, P242, DOI 10.1002/(SICI)1098-2418(199910/12)15:3/4<242::AID-RSA4>3.0.CO;2-C; Dellaert F, 2003, MACH LEARN, V50, P45, DOI 10.1023/A:1020245811187; DICK AR, 2002, P 6 EUR C COMP VIS; Forsyth DA, 2001, INT J COMPUT VISION, V41, P109, DOI 10.1023/A:1011165200654; GEMAN D, 1996, IEEE T PAMI, V18; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; GRENANDER U, 1994, J R STAT SOC B, V56, P549; HAN F, 2003, P INT WORKSHOP HIGH; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; ISARD M, 1996, P 4 EUR C COMP VIS; KAESS M, 2004, P 8 EUR C COMP VIS; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KHAN Z, 2004, P 8 EUR C COMP VIS; Lanterman AD, 2001, OPT ENG, V40, P1724, DOI 10.1117/1.1387279; LEE MW, 2004, P CVPR; Liu JS., 2001, MONTE CARLO STRATEGI, DOI DOI 10.1007/978-0-387-76371-2; MACIUCA R, 2005, IN PRESS J THEORETIC; MALIK J, 2001, INT J COMP VIS; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Nitzberg M., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P138, DOI 10.1109/ICCV.1990.139511; PARIDA L, 1998, IEEE T PAMI, V20; PESKUN PH, 1973, BIOMETRIKA, V60, P607; SCHAPIRE RE, 2000, MSRI WORKSHOP NONLIN; Shi J., 2000, IEEE T PAMI, V22; SKIENA S, 1990, IMPLEMENTING DISCRET, P203; Srivastava A, 2002, J STAT PLAN INFER, V103, P15, DOI 10.1016/S0378-3758(01)00195-1; SWENDSEN RH, 1987, PHYS REV LETT, V58, P86, DOI 10.1103/PhysRevLett.58.86; TU Z, 2002, P 7 EUR C COMP VIS C; Tu Z., 2003, P INT C COMP VIS NIC; TU ZW, 2002, IEEE T PAMI, V24; WANG J, 1994, IEEE T IMAGE PROCESS, V6; ZHAO T, 2004, P CVPR; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343; ZHU SC, 2000, P INT C COMP VIS PAT; Zimmer C, 2002, IEEE T MED IMAGING, V21, P1212, DOI 10.1109/TMI.2002.806292	47	22	23	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	2					223	249		10.1007/s11263-006-6995-9	http://dx.doi.org/10.1007/s11263-006-6995-9			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	065LW					2022-12-18	WOS:000239162400004
J	Feldman, J				Feldman, J			Perceptual grouping by selection of a logically minimal model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						perceptual grouping; perceptual organization; logic; nonaccidental properties	CURVE DETECTION; DOT LATTICES; REGULARITY; COMPLETION; PROXIMITY; PATTERNS; SIMILARITY; PRINCIPLE; OBJECTS; IMAGE	This paper presents a logic-based approach to grouping and perceptual organization, called Minimal Model theory, and presents efficient methods for computing interpretations in this framework. Grouping interpretations are first defined as logical structures, built out of atomic qualitative scene descriptors ("regularities") that are derived from considerations of non-accidentalness. These interpretations can then be partially ordered by their degree of regularity or constraint (measured numerically by their logical depth). The Genericity Constraint-the principle that interpretations should minimize coincidences in the observed configuration-dictates that the preferred interpretation will be the minimum in this partial order, i.e. the interpretation with maximum depth. This maximum-depth interpretation, also called the minimal model or minimal interpretation, is in a sense the "simplest" (algebraically minimal) interpretation available of the image configuration. As a side-effect, the "most salient" or most structured part of the scene can be identified, as the maximum-depth subtree of the minimal model. An efficient (O(n(2))) method for computing the minimal interpretation is presented, along with examples. Computational experiments show that the algorithm performs well under a wide range of parameter settings.	Rutgers State Univ, Dept Psychol, Ctr Cognit Sci, New Brunswick, NJ 08903 USA	Rutgers State University New Brunswick	Feldman, J (corresponding author), Rutgers State Univ, Dept Psychol, Ctr Cognit Sci, New Brunswick, NJ 08903 USA.							Amir A, 1998, IEEE T PATTERN ANAL, V20, P168, DOI 10.1109/34.659934; [Anonymous], 1990, INTRO LATTICES ORDER; ATTNEAVE F, 1968, AM J PSYCHOL, V81, P447, DOI 10.2307/1420645; BAYLIS GC, 1993, J EXP PSYCHOL HUMAN, V19, P451, DOI 10.1037/0096-1523.19.3.451; Bennett B. M., 1989, OBSERVER MECH FORMAL; BINFORD TO, 1981, ARTIF INTELL, V17, P205, DOI 10.1016/0004-3702(81)90025-4; BOSELIE F, 1989, PSYCHOL RES-PSYCH FO, V51, P93, DOI 10.1007/BF00309303; Bowyer K., 1998, EMPIRICAL EVALUATION; COMPTON BJ, 1993, PERCEPT PSYCHOPHYS, V53, P403, DOI 10.3758/BF03206783; COX IJ, 1993, INT J COMPUT VISION, V11, P5, DOI 10.1007/BF01420590; ELDER J, 1993, VISION RES, V33, P981, DOI 10.1016/0042-6989(93)90080-G; Feldman J, 2001, PERCEPT PSYCHOPHYS, V63, P1171, DOI 10.3758/BF03194532; Feldman J, 1996, PERCEPTION, V25, P335, DOI 10.1068/p250335; Feldman J, 1997, COMPUT INTELL, V13, P582, DOI 10.1111/0824-7935.00052; Feldman J, 1997, J MATH PSYCHOL, V41, P145, DOI 10.1006/jmps.1997.1154; Feldman J, 1999, ACTA PSYCHOL, V102, P137, DOI 10.1016/S0001-6918(98)00054-7; Feldman J, 1997, VISION RES, V37, P2835, DOI 10.1016/S0042-6989(97)00096-5; FELDMAN J, 2001, UNPUB BAYES SIMPLICI; GILCHRIST AL, 1989, PERCEPT PSYCHOPHYS, V45, P92; GLASS L, 1969, NATURE, V223, P578, DOI 10.1038/223578a0; GREEN DM, 1966, SIGNAL DETECTION THE; Guy G, 1996, INT J COMPUT VISION, V20, P113, DOI 10.1007/BF00144119; IVERSON LA, 1995, IEEE T PATTERN ANAL, V17, P982, DOI 10.1109/34.464562; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; Jepson A., 1992, SPATIAL VISION HUMAN, P89; JEPSON A, 1991, 43 MIT CTR COGN SCI; JEPSON A, 1999, P INT C COMP VIS, V2, P1123, DOI DOI 10.1109/ICCV.1999.790406; Kanizsa Gaetano, 1979, ORG VISION ESSAYS GE; KELLY AR, 2000, P 11 BRIT MACH VIS C, P586; Kitazaki M, 1996, PERCEPTION, V25, P797, DOI 10.1068/p250797; KOVACS I, 1993, P NATL ACAD SCI USA, V90, P7495, DOI 10.1073/pnas.90.16.7495; KUBOVY M, 1995, PSYCHOL SCI, V6, P225, DOI 10.1111/j.1467-9280.1995.tb00597.x; KUBOVY M, 1994, PSYCHON B REV, V1, P182, DOI 10.3758/BF03200772; Kubovy M, 1998, COGNITIVE PSYCHOL, V35, P71, DOI 10.1006/cogp.1997.0673; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; MARKMAN AB, 1993, COGNITIVE PSYCHOL, V25, P431, DOI 10.1006/cogp.1993.1011; MEDIN DL, 1990, PSYCHOL SCI, V1, P64, DOI 10.1111/j.1467-9280.1990.tb00069.x; Medioni G., 2000, COMPUTATIONAL FRAMEW; NAKAYAMA K, 1992, SCIENCE, V257, P1357, DOI 10.1126/science.1529336; PALMER SE, 1977, COGNITIVE PSYCHOL, V9, P441, DOI 10.1016/0010-0285(77)90016-0; PALMER SE, 1992, COGNITIVE PSYCHOL, V24, P436, DOI 10.1016/0010-0285(92)90014-S; PALMER SE, 1980, COGNITIVE PSYCHOL, V12, P285, DOI 10.1016/0010-0285(80)90012-2; Palmieri C, 2002, ENDOCR-RELAT CANCER, V9, P1, DOI 10.1677/erc.0.0090001; PARENT P, 1989, IEEE T PATTERN ANAL, V11, P823, DOI 10.1109/34.31445; Pizlo Z, 1997, VISION RES, V37, P1217, DOI 10.1016/S0042-6989(96)00220-9; Pomerantz J. R., 1986, PATTERN RECOGNITION, V2; POMERANTZ JR, 1977, J EXP PSYCHOL HUMAN, V3, P422, DOI 10.1037/0096-1523.3.3.422; POMERANTZ JR, 1989, J EXP PSYCHOL HUMAN, V15, P635, DOI 10.1037/0096-1523.15.4.635; Poston T., 2014, CATASTROPHE THEORY I; PRAZDNY K, 1984, PERCEPTION, V13, P469, DOI 10.1068/p130469; REITER R, 1989, ARTIF INTELL, V41, P125, DOI 10.1016/0004-3702(89)90008-8; REITER R, 1978, LOGIC DATA BASES; Rock I., 1983, LOGIC PERCEPTION; SEKULER AB, 1994, PSYCHOL SCI, V5, P260, DOI 10.1111/j.1467-9280.1994.tb00623.x; SEKULER AB, 1992, J EXP PSYCHOL GEN, V121, P95, DOI 10.1037/0096-3445.121.1.95; SHI J, 2000, IN PRESS IEEE T PATT; SMITH AR, 1986, J DRUG ISSUES, V16, P407, DOI 10.1177/002204268601600307; SMITS JTS, 1987, PERCEPTION, V16, P121, DOI 10.1068/p160121; STEVENS KA, 1978, BIOL CYBERN, V29, P19, DOI 10.1007/BF00365232; STEVENS KA, 1987, COMPUT VISION GRAPH, V37, P238, DOI 10.1016/S0734-189X(87)80004-X; TAKEICHI H, 1995, PERCEPTION, V24, P373, DOI 10.1068/p240373; Tenenbaum Jay M, 1983, HUMAN MACHINE VISION, P481; VANLIER RJ, 1995, PERCEPTION, V24, P727, DOI 10.1068/p240727; WAGEMANS J, 1993, J EXP PSYCHOL HUMAN, V19, P364, DOI 10.1037/0096-1523.19.2.364; WAGEMANS J, 1993, VISION RES, V33, P1067, DOI 10.1016/0042-6989(93)90241-N; Williams LR, 1999, INT J COMPUT VISION, V34, P81, DOI 10.1023/A:1008187804026; Williams LR, 1997, NEURAL COMPUT, V9, P837, DOI 10.1162/neco.1997.9.4.837; ZUCKER SW, 1983, PERCEPT PSYCHOPHYS, V34, P513, DOI 10.3758/BF03205904; ZUCKER SW, 1985, COMPUT VISION GRAPH, V32, P74, DOI 10.1016/0734-189X(85)90003-9; [No title captured]	71	22	23	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2003	55	1					5	25		10.1023/A:1024454423670	http://dx.doi.org/10.1023/A:1024454423670			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	694TM					2022-12-18	WOS:000183790500001
J	Srinivasan, S				Srinivasan, S			Extracting structure from optical flow using the fast error search technique	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; fast partial search; focus of expansion; optical flow	RECOVERING 3-D MOTION; 3-DIMENSIONAL MOTION; INHERENT AMBIGUITIES; PASSIVE NAVIGATION; FIELD; IMAGE	In this paper, we present a globally optimal and computationally efficient technique for estimating the focus of expansion (FOE) of an optical flow field, using fast partial search. For each candidate location on a discrete sampling of the image area, we generate a linear system of equations for determining the remaining unknowns, viz. rotation and inverse depth. We compute the least squares error of the system without actually solving the equations, to generate an error surface that describes the goodness of fit across the hypotheses. Using Fourier techniques, we prove that given an N x N flow field, the FOE, and subsequently rotation and structure, can be estimated in O(N-2 log N) operations. Since the resulting system is linear, bounded perturbations in the data lead to bounded errors. We support the theoretical development and proof of our technique with experiments on synthetic and real data. Through a series of experiments on synthetic data, we prove the correctness, robustness and operating envelope of our algorithm. We demonstrate the utility of our technique by applying it for detecting obstacles from a monocular sequence of images.	Microsoft Corp, Redmond, WA 98052 USA; Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA	Microsoft; University System of Maryland; University of Maryland College Park	Srinivasan, S (corresponding author), Microsoft Corp, 1 Microsoft Way, Redmond, WA 98052 USA.	sridhsri@microsoft.com						ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; ALOIMONOS Y, 1984, IEEE WORKSH COMP VIS, P72; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BRUSS AR, 1983, COMPUT VISION GRAPH, V21, P3, DOI 10.1016/S0734-189X(83)80026-7; Fejes S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P979, DOI 10.1109/ICCV.1998.710835; Fermuller C, 1997, INT J COMPUT VISION, V21, P223, DOI 10.1023/A:1007951901001; FERMULLER C, 1995, INT J COMPUT VISION, V15, P7, DOI 10.1007/BF01450848; Gibson J. J., 1955, PERCEPTION VISUAL WO; GIBSON JJ, 1957, PSYCHOL REV, V64, P288, DOI 10.1037/h0044277; GUPTA NC, 1995, ARTIF INTELL, V78, P45, DOI 10.1016/0004-3702(95)00031-3; HEEGER D, 1992, RBCVTR9240 U TOR; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; KOENDERINK JJ, 1976, J OPT SOC AM, V66, P717, DOI 10.1364/JOSA.66.000717; LAWTON DT, 1983, COMPUT VISION GRAPH, V22, P116, DOI 10.1016/0734-189X(83)90098-1; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; MITICHE A, 1987, IEEE WORKSH COMP VIS, P195; MITICHE A, 1994, COMPUTATIONAL ANAL V; NAKAYAMA K, 1974, PERCEPTION, V3, P63, DOI 10.1068/p030063; NEGAHDARIPOUR S, 1987, IEEE T PATTERN ANAL, V9, P168, DOI 10.1109/TPAMI.1987.4767884; OLIENSIS J, 1997, CRITIQUE STRUCTURE M; PRAZDNY K, 1981, COMPUT VISION GRAPH, V17, P238, DOI 10.1016/0146-664X(81)90004-6; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Simoncelli E, 1993, THESIS MIT; SRINIVASAN S, 1998, THESIS U MARYLAND CO; TSAI RY, 1981, IEEE T ACOUST SPEECH, V29, P1147, DOI 10.1109/TASSP.1981.1163710; ULLMAN S, 1979, PROC R SOC SER B-BIO, V203, P405, DOI 10.1098/rspb.1979.0006; WALLACH H, 1953, J EXP PSYCHOL, V45, P205, DOI 10.1037/h0056880; WAXMAN AM, 1987, INT J COMPUT VISION, V1, P239, DOI 10.1007/BF00127823; WAXMAN AM, 1985, INT J ROBOT RES, V4, P72, DOI 10.1177/027836498500400306; Weng J., 1991, MOTION STRUCTURE IMA; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903; Zhuang X., 1984, First Conference on Artificial Intelligence Applications (Cat. No. 84CH2107-1), P366; ZHUANG XH, 1988, COMPUT VISION GRAPH, V42, P334, DOI 10.1016/S0734-189X(88)80043-4	35	22	24	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	37	3					203	230		10.1023/A:1008111923880	http://dx.doi.org/10.1023/A:1008111923880			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	342GF					2022-12-18	WOS:000088636300001
J	Bruckstein, AM; Holt, RJ; Huang, TS; Netravali, AN				Bruckstein, AM; Holt, RJ; Huang, TS; Netravali, AN			Optimum fiducials under weak perspective projection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						pose recovery; weak perspective projection; fiducial design	OBJECT POSE; MOTION; IMAGE	We investigate how a given fixed number of points should be located in space so that the pose of a camera viewing them from unknown locations can be estimated with the greatest accuracy. We show that optimum solutions are obtained when the points form concentric complete regular polyhedra. For the case of optimal configurations we provide a worst-case error analysis and use it to analyze the effects of weak perspective approximation to true perspective viewing. Comprehensive computer simulations validate the theoretical results.	Lucent Technol, Bell Labs, Murray Hill, NJ 07974 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA; Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Alcatel-Lucent; Lucent Technologies; AT&T; University of Illinois System; University of Illinois Urbana-Champaign; Technion Israel Institute of Technology	Bruckstein, AM (corresponding author), Lucent Technol, Bell Labs, Murray Hill, NJ 07974 USA.		Holt, Robert J/B-5460-2009	Bruckstein, Alfred/0000-0001-5669-0037				ALTER TD, 1994, IEEE T PATTERN ANAL, V16, P802, DOI 10.1109/34.308475; CHAR BW, 1990, MAPLE 5 USERS GUIDE; Dahlquist G., 1974, NUMERICAL METHODS; DEMENTHON DF, 1995, INT J COMPUT VISION, V15, P123, DOI 10.1007/BF01450852; Horaud R, 1997, INT J COMPUT VISION, V22, P173, DOI 10.1023/A:1007940112931; HUANG TS, 1995, IEEE T PATTERN ANAL, V17, P1220, DOI 10.1109/34.476515; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P536, DOI 10.1109/34.24786; HUTTENLOCHER D, 1987, INT C COMP VIS LOND, P102; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234	11	22	22	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1999	35	3					223	244		10.1023/A:1008156210387	http://dx.doi.org/10.1023/A:1008156210387			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	265PK					2022-12-18	WOS:000084252400002
J	Aubert, G; Blanc-Feraud, L				Aubert, G; Blanc-Feraud, L			Some remarks on the equivalence between 2D and 3D classical snakes and geodesic active contours	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						geodesic active contours; active surfaces; Hamiltonian; snakes; optimization		Recently, Caselles et al. have shown the equivalence between a classical snake problem of Kass et al. and a geodesic active contour model. The PDE derived from the geodesic problem gives an evolution equation for active contours which is very powerfull for image segmentation since changes of topology are allowed using the level set implementation. However in Caselles' paper the equivalence with classical snake is only shown for 2D images and 1D curves, by using concepts of Hamiltonian theory which have no meanings for active surfaces. This paper propose to examine the notion of equivalence and to revisite Caselles et al. arguments. Then a notion equivalence is introduced and shown for classical snakes and geodesic active contours in the 2D (active contour) and 3D (active surface) case.	Univ Nice Sophia Antipolis, UMR 6621, Lab Math JA Dieudonne, F-06108 Nice 2, France; UNSA, INRIA, CNRS, Ariana Grp, F-06902 Sophia Antipolis, France	UDICE-French Research Universities; Universite Cote d'Azur; Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; Universite Cote d'Azur	Aubert, G (corresponding author), Univ Nice Sophia Antipolis, UMR 6621, Lab Math JA Dieudonne, Parc Valrose, F-06108 Nice 2, France.							Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Malladi R., 1995, IEEE T PATTERN ANAL, V17; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2	5	22	22	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1999	34	1					19	28		10.1023/A:1008168219878	http://dx.doi.org/10.1023/A:1008168219878			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	248WN					2022-12-18	WOS:000083299900002
J	Langer, MS				Langer, MS			When shadows become interreflections	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shading; shadows; interreflection; illumination; color	ILLUMINATION; SHAPE	Shadows and interreflections are present in all real scenes and provide a rich set of photometric cues for vision. In this paper, we show how shadows and interreflections are intrinsically related. Shadows tend to occur in those parts of a scene in which interreflections have the largest gain. We provide several basic results concerning this relationship in terms of the interreflection modes of a scene. We show that for a given scene, the interreflection mode having the largest gain is a physically realizable radiance function. We derive bounds on the gain of this mode and discuss how this mode is related to shadows. We analyze how well an n-bounce model of interreflections approximates an infinite-bounce model and how shadows affect this approximation. Finally, we introduce a novel method for inferring surface color in a uni-chromatic scene. The method is based on the relative contrast of the scene in different color channels.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Langer, MS (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.							ANSTIS S, 1992, NEURAL NETWORKS VISI; BAUM DR, 1989, P SIGGRAPH; BELHUMEUR P, 1997, P IEEE C COMP VIS PA; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P671, DOI 10.1109/34.85657; FUNT BV, 1993, IEEE T PATTERN ANAL, V15, P1319, DOI 10.1109/34.250838; FUNT BV, 1991, INT J COMPUT VISION, V6, P5, DOI 10.1007/BF00127123; GERSHON R, 1990, J OPT SOC AM, V7, P2041; GILCHRIST A, 1984, PERCEPTION, V13, P5, DOI 10.1068/p130005; GILCHRIST AL, 1993, INVEST OPH VIS SCI; HADDON J, 1998, P 6 INT C COMP VIS B; KERSTEN DK, 1996, INVEST OPH VIS SCI; KOENDERINK JJ, 1983, J OPT SOC AM, V73, P843, DOI 10.1364/JOSA.73.000843; LANGER MS, 1994, J OPT SOC AM A, V11, P467, DOI 10.1364/JOSAA.11.000467; Moon P, 1940, J OPT SOC AM, V30, P195, DOI 10.1364/JOSA.30.000195; MOON P, 1981, PHOTIC FIELD; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; NAYAR SK, 1992, IM UND WORKSH SAN DI, P333; Oren M., 1994, LECT NOTES COMPUTER, V801, P269; RUBIN JM, 1982, BIOL CYBERN, V45, P215, DOI 10.1007/BF00336194; Shafer S. A., 1985, SHADOWS SILHOUETTES; Siegel R., 1981, THERMAL RAD HEAT TRA; SPENCER DE, 1961, J FRANKLIN I, V252, P413; Waltz D., 1975, PSYCHOL COMPUTER VIS, P19	24	22	32	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1999	34	2-3					193	204		10.1023/A:1008131719047	http://dx.doi.org/10.1023/A:1008131719047			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	265NF					2022-12-18	WOS:000084249700007
J	Wallace, AM; Liang, B; Trucco, E; Clark, J				Wallace, AM; Liang, B; Trucco, E; Clark, J			Improving depth image acquisition using polarized light	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						depth sensing; triangulation; metallic reflections; polarization; vision	SURFACES	Control of the source and analysis of the polarization properties of the reflected light in a laser rangefinder based on triangulation offer a potential solution to the problem of distinguishing the primary laser stripe from unwanted inter-reflections caused by holes and concavities on metal surfaces. In this paper, the established polarization theory of first and subsequent inter-reflections from metallic surfaces is reviewed. This provides a point of comparison for ellipsometric measurements which verify the particular applicability of the microfacet surface model in our context. We demonstrate how a conventional laser rangefinder can be modified to discriminate between primary and secondary reflections. However, our experiments on third and subsequent reflections show that more complex models are required to provide complete resolution of the problem. Furthermore, error analysis demonstrates the requirement for very precise control of the source and receiving optoelectronics. We conclude by demonstrating the acquisition of a depth image with and without polarization optics and discuss the significance of our results for laser depth measurement.	Heriot Watt Univ, Dept Elect & Comp Engn, Edinburgh EH14 4AS, Midlothian, Scotland	Heriot Watt University	Wallace, AM (corresponding author), Heriot Watt Univ, Dept Elect & Comp Engn, Edinburgh EH14 4AS, Midlothian, Scotland.	andy@cee.hw.ac.uk; bojian@cee.hw.ac.uk; mtc@cee.hw.ac.uk; jclark@cee.hw.ac.uk		Trucco, Emanuele/0000-0002-5055-0794; Wallace, Andrew/0000-0003-4425-8591				Besl P. J., 1988, Machine Vision and Applications, V1, P127, DOI 10.1007/BF01212277; Born M., 1993, PRINCIPLES OPTICS, V6; BOUGUER P, 1961, GRADATION LIGHT TRAI; Boult T. E., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P656, DOI 10.1109/CVPR.1991.139769; Chen F, 1998, ANN CLIN LAB SCI, V28, P1; Clark J, 1997, IMAGE VISION COMPUT, V15, P107, DOI 10.1016/S0262-8856(96)01126-2; CLARK J, 1995, P IEE INT C IM PROC, P539; Collett E., 1993, POLARIZED LIGHT FUND; FRYER RJ, 1991, P BRIT MACH VIS C GL, P347; JONES BF, 1989, IMAGE VISION COMPUT, V7, P253, DOI 10.1016/0262-8856(89)90028-0; KLIGAR DS, 1990, POLARIZED LIGHT OPTI; Koshikawa K., 1987, Advanced Robotics, V2, P137, DOI 10.1163/156855387X00129; MENDELEEV VY, 1995, J OPT TECHNOL+, V62, P88; MERSCH SH, 1984, P C ROB VIS SENS PER, P440; MULLER V, 1996, P 4 EUR C COMP VIS C; Nayar SK, 1997, INT J COMPUT VISION, V21, P163, DOI 10.1023/A:1007937815113; NITZAN D, 1988, IEEE T PATTERN ANAL, V10, P291, DOI 10.1109/34.3895; PEZZANITI JL, 1993, THESIS U ALABAMA; PEZZANITI JL, 1990, P SPIE C POL RAD INF; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; TOURRE D, 1997, THESIS ST ANDREWS U; Trucco E, 1998, INT J COMPUT INTEG M, V11, P293, DOI 10.1080/095119298130642; VIDEEN G, 1992, J OPT SOC AM A, V9, P1111, DOI 10.1364/JOSAA.9.001111; Wolff L. B, 1987, PROC SPIE OPTICS ILL, V850, P110; WOLFF LB, 1989, P IEEE INT C COMP VI; WOLFF LB, 1990, THESIS COLUMBIA U; [No title captured]	27	22	24	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1999	32	2					87	109		10.1023/A:1008154415349	http://dx.doi.org/10.1023/A:1008154415349			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	229HC					2022-12-18	WOS:000082186600001
J	WU, JJ; RINK, RE; CAELLI, TM; GOURISHANKAR, VG				WU, JJ; RINK, RE; CAELLI, TM; GOURISHANKAR, VG			RECOVERY OF THE 3-D LOCATION AND MOTION OF A RIGID OBJECT THROUGH CAMERA IMAGE (AN EXTENDED KALMAN FILTER APPROACH)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									UNIV ALBERTA, DEPT ELECT ENGN, EDMONTON T6G 2E1, ALBERTA, CANADA; UNIV ALBERTA, ALBERTA CTR MACHINE INTELLIGENCE & ROBOT, EDMONTON T6G 2E1, ALBERTA, CANADA	University of Alberta; University of Alberta								Anderson B D., 1977, OPTIMAL FILTERING; AVID G, 1985, P IEEE C COMPUTER VI, P70; AVID G, 1985, IEEE T PAMI, V6, P384; BALAKRISHNAN AV, 1984, KALMAN FILTERING THE; Besl P. J., 1985, ACM COMPUTING SURVEY, V17; BROIDA TJ, 1986, IEEE T PATTERN ANAL, V8, P90, DOI 10.1109/TPAMI.1986.4767755; FANG JQ, 1984, IEEE T PATTERN ANAL, V6, P545, DOI 10.1109/TPAMI.1984.4767569; Faugeras O. D., 1986, Proceedings 1986 IEEE International Conference on Robotics and Automation (Cat. No.86CH2282-2), P1433; GENNERY DB, 1982, AAAI, V82, P13; HOHNE KH, 1983, IMAGE SEQUENCE PROCE, P603; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HUANG TS, 1983, 3RD P INT C ROB VIS, P229; KANATANI K, 1986, UNPUB COMPUTER VISIO; LEVIN JZ, 1979, COMPUT VISION GRAPH, V11, P73, DOI 10.1016/0146-664X(79)90077-7; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Maybeck P. S., 1982, STOCHASTIC MODELS ES, V2; Maybeck P. S., 1982, STOCHASTIC MODELS ES; MERI AZ, 1980, IEEE T PAMI, V2, P582; Nagel H.-H., 1983, IMAGE SEQUENCE PROCE, P2; NAGEL HH, 1981, AUG P IEEE C PATT RE, P103; NEGAHDARIPOUR S, 1985, DETERMINING 3-D MOTI; OROURKE J, 1981, AUG P IEEE C PATT RE, P82; Paul R. P., 1981, ROBOT MANIPULATORS M; Schunck B. G., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P560; STUELPNA.J, 1964, SIAM REV, V6, P422, DOI 10.1137/1006093; Tsai R.Y., 1986, P IEEE C COMP VIS PA, P364; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; ULLMAN S, 1981, COMPUTER, V14, P57, DOI 10.1109/C-M.1981.220564; Wittenburg J., 1977, DYNAMICS SYSTEMS RIG; WU JJ, 1988, THESIS U ALBERTA; Yasumoto Y., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P89	31	22	23	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1989	2	4					373	394		10.1007/BF00133556	http://dx.doi.org/10.1007/BF00133556			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC192					2022-12-18	WOS:A1989AC19200002
J	ALOIMONOS, J; SWAIN, M				ALOIMONOS, J; SWAIN, M			SHAPE FROM PATTERNS - REGULARIZATION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									UNIV MARYLAND,CTR AUTOMAT RES,COMP VIS LAB,COLLEGE PK,MD 20742; UNIV ROCHESTER,DEPT COMP SCI,ROCHESTER,NY 14627	University System of Maryland; University of Maryland College Park; University of Rochester								ALOIMONOS J, 1988, BIOL CYBERN, V58, P345, DOI 10.1007/BF00363944; ALOIMONOS J, 1987, P IMAGE UNDERSTANDIN, V2; ALOIMONOS J, 1986, P IEEE C COMPUTER VI; ALOIMONOS J, 1985, 9TH P INT JOINT C AR, P926; ALOIMONOS J, 1987, THESIS U ROCHESTER D; Ballard D.H., 1982, COMPUTER VISION; HADAMARD J, 1923, LECTURES CAUCHYS PRO; Horn B., 1986, ROBOT VISION, P1; Horn Berthold K. P., 1975, PSYCHOL COMPUTER VIS, P115; HORN BKP, 1977, ARTIF INTELL, V8, P201, DOI 10.1016/0004-3702(77)90020-0; IKEUCHI K, 1984, ARTIF INTELL, V22, P49, DOI 10.1016/0004-3702(84)90025-0; IKEUCHI K, 1981, ARTIF INTELL, V17, P147; KANADE T, 1980, AUG P WORKSH PICT DA; KANATANI K, 1984, ARTIF INTELL, V23, P213, DOI 10.1016/0004-3702(84)90010-9; KENDER J, 1979, LTH P INT JOINT C AR; KENDER JR, 1980, THESIS CARNEGIEMELLO; Lee D., 1985, P DARPA IMAGE UNDERS, P489; Marr D., 1982, VISION; Ohta T.I., 1981, P INT JOINT C ART IN, P746; POGGIO T, 1985, P ROY SOC LONDON B; POGGIO T, 1985, P IMAGE UNDERSTANDIN; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; SHAFER S, 1982, THESIS CARNEGIE MELL; SHULMAN D, 1988, 2011 U MAR DEP COMP; Smith G. D., 1978, NUMERICAL SOLUTION P, V2nd; STEVENS K, 1979, THESIS MIT; Tikhonov A., 1977, SOLUTIONS ILL POSED; WAHBA G, 1984, STATISTICS APPRAISAL; WALKER E, CMUCSTR84152 CARN ME; WITKIN AP, 1981, ARTIF INTELL, V17, P17, DOI 10.1016/0004-3702(81)90019-9	30	22	22	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1988	2	2					171	187		10.1007/BF00133699	http://dx.doi.org/10.1007/BF00133699			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC190					2022-12-18	WOS:A1988AC19000004
J	Yang, CY; Shen, YJ; Zhou, BL				Yang, Ceyuan; Shen, Yujun; Zhou, Bolei			Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generative model; Scene understanding; Image manipulation; Representation learning; Feature visualization	NETWORKS	Despite the great success of Generative Adversarial Networks (GANs) in synthesizing images, there lacks enough understanding of how photo-realistic images are generated from the layer-wise stochastic latent codes introduced in recent GANs. In this work, we show that highly-structured semantic hierarchy emerges in the deep generative representations from the state-of-the-art GANs like StyleGAN and BigGAN, trained for scene synthesis. By probing the per-layer representation with a broad set of semantics at different abstraction levels, we manage to quantify the causality between the layer-wise activations and the semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors that can be further used to steer the generation process, such as changing the lighting condition and varying the viewpoint of the scene. Extensive qualitative and quantitative results suggest that the generative representations learned by the GANs with layer-wise latent codes are specialized to synthesize various concepts in a hierarchical manner: the early layers tend to determine the spatial layout, the middle layers control the categorical objects, and the later layers render the scene attributes as well as the color scheme. Identifying such a set of steerable variation factors facilitates high-fidelity scene editing based on well-learned GAN models without any retraining (code and demo video are available at ).	[Yang, Ceyuan; Shen, Yujun; Zhou, Bolei] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China	Chinese University of Hong Kong	Yang, CY; Zhou, BL (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.	yc019@ie.cuhk.edu.hk; sy116@ie.cuhk.edu.hk; bzhou@ie.cuhk.edu.hk		Yang, Ceyuan/0000-0003-1417-1938	Early Career Scheme (ECS) through the Research Grants Council (RGC) of Hong Kong [24206219]; CUHK FoE RSFS [3133233]	Early Career Scheme (ECS) through the Research Grants Council (RGC) of Hong Kong; CUHK FoE RSFS	This work is supported by Early Career Scheme (ECS) through the Research Grants Council (RGC) of Hong Kong under Grant No.24206219 and CUHK FoE RSFS Grant (No. 3133233).	Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453; Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22; Alain G., 2016, INT C LEARN REPR WOR; Nguyen A, 2016, ADV NEUR IN, V29; [Anonymous], IEEE C COMP VIS PATT, P8296; Barrett D. G., 2018, INT C LEARN REPR; Bau D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323023; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Bau David, 2018, INT C LEARN REPR; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Cheng MM, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682628; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Goetschalckx L, 2019, IEEE I CONF COMP VIS, P5743, DOI 10.1109/ICCV.2019.00584; Gonzalez-Garcia A, 2018, INT J COMPUT VISION, V126, P476, DOI 10.1007/s11263-017-1048-0; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hensel M, 2017, ADV NEUR IN, V30; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jahanian Ali, 2019, INT C LEARN REPR, P3; Karacan L., 2016, ARXIV161200215; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101; Lehtinen J, 2017, INT C LEARN REPR; Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683; Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z; Radford A., 2015, P COMP C; Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467; Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267; Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092; Shen Yujun, 2020, P IEEE CVF C COMP VI; Simonyan K., 2018, INT C LEARN REPR; Nguyen-Phuoc T, 2019, IEEE I CONF COMP VIS, P7587, DOI 10.1109/ICCV.2019.00768; Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xiao TH, 2018, LECT NOTES COMPUT SC, V11214, P172, DOI 10.1007/978-3-030-01249-6_11; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Yao SY, 2018, ADV NEUR IN, V31; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Yu F., 2015, ARXIVABS150603365 CO; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang R., 2020, CORR ABS200700653; Zhang WD, 2020, IEEE T CYBERNETICS, V50, P2730, DOI 10.1109/TCYB.2019.2895837; Zhou B., 2015, INT C LEARN REPR; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhu J., 2020, EUR C COMP VIS; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zisserman A, 2014, WORKSH TRACK P 2 INT	52	21	21	1	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1451	1466		10.1007/s11263-020-01429-5	http://dx.doi.org/10.1007/s11263-020-01429-5		FEB 2021	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000616466600001
J	Fan, H; Bai, HX; Lin, LT; Yang, F; Chu, P; Deng, G; Yu, SJ; Harshit; Huang, MZ; Liu, JH; Xu, Y; Liao, CY; Yuan, L; Ling, HB				Fan, Heng; Bai, Hexin; Lin, Liting; Yang, Fan; Chu, Peng; Deng, Ge; Yu, Sijia; Harshit; Huang, Mingzhen; Liu, Juehuan; Xu, Yong; Liao, Chunyuan; Yuan, Lin; Ling, Haibin			LaSOT: A High-quality Large-scale Single Object Tracking Benchmark	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual tracking; Large-scale benchmark; High-quality dense annotation; Tracking evaluation		Despite great recent advances in visual tracking, its further development, including both algorithm design and evaluation, is limited due to lack of dedicated large-scale benchmarks. To address this problem, we presentLaSOT, a high-qualityLarge-scaleSingleObjectTracking benchmark. LaSOT contains a diverse selection of 85 object classes, and offers 1550 totaling more than 3.87 million frames. Each video frame is carefully and manually annotated with a bounding box. This makes LaSOT, to our knowledge, the largest densely annotated tracking benchmark. Our goal in releasing LaSOT is to provide a dedicated high quality platform for both training and evaluation of trackers. The average video length of LaSOT is around 2500 frames, where each video contains various challenge factors that exist in real world video footage,such as the targets disappearing and re-appearing. These longer video lengths allow for the assessment of long-term trackers. To take advantage of the close connection between visual appearance and natural language, we provide language specification for each video in LaSOT. We believe such additions will allow for future research to use linguistic features to improve tracking. Two protocols,full-overlapandone-shot, are designated for flexible assessment of trackers. We extensively evaluate 48 baseline trackers on LaSOT with in-depth analysis, and results reveal that there still exists significant room for improvement. The complete benchmark, tracking results as well as analysis are available at.	[Fan, Heng; Harshit; Huang, Mingzhen; Ling, Haibin] SUNY Stony Brook, Stony Brook, NY 11794 USA; [Bai, Hexin; Yang, Fan; Chu, Peng; Deng, Ge; Yu, Sijia; Liu, Juehuan] Temple Univ, Philadelphia, PA 19122 USA; [Lin, Liting; Xu, Yong] South China Univ Technol, Guangzhou, Peoples R China; [Lin, Liting; Xu, Yong] Peng Cheng Lab, Shenzhen, Peoples R China; [Liao, Chunyuan] HiScene Informat Technol, Shanghai, Peoples R China; [Yuan, Lin] Amazon Web Serv, Palo Alto, CA USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple University; South China University of Technology; Peng Cheng Laboratory; Amazon.com	Fan, H (corresponding author), SUNY Stony Brook, Stony Brook, NY 11794 USA.	hefan@cs.stonybrook.edu; hling@cs.stonybrook.edu		Fan, Heng/0000-0002-7033-3690	Amazon AWS Machine Learning Research Award	Amazon AWS Machine Learning Research Award	We thank the anonymous reviewers for insightful suggestions, and Jeremy Chu for proofreading the final draft. Ling was supported partially by the Amazon AWS Machine Learning Research Award.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2005, BMVC; Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737; Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881; Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057; Choi J, 2016, PROC CVPR IEEE, P4321, DOI 10.1109/CVPR.2016.468; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai K., 2019, COMPUT VIS PATTERN R; Dai K., 2020, CVPR; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928; Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29; Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490; Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143; Danelljan Martin, 2019, CVPR, V2, P6; Dave Achal, 2020, ECCV; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan H, 2019, P IEEECVF C COMPUTER, P5374; Fan H., 2020, ARXIV191107959; Fan HH, 2017, IEEE I CONF COMP VIS, P736, DOI 10.1109/ICCV.2017.86; Fan H, 2017, IEEE COMPUT SOC CONF, P2217, DOI 10.1109/CVPRW.2017.275; Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI 10.1109/ICCV.2017.129; Ganin Yaroslav, 2015, ICML; Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251; He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Henriques J. F., 2012, EUR C COMP VIS, P702, DOI DOI 10.1007/978-3-642-33765-9_50; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493; Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464; Huang LJ, 2020, AAAI CONF ARTIF INTE, V34, P11053; Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kittler J, 2019, ICCV; Kristan M, 2017, ICCVW; Kristan M., 2018, ECCVW; Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li AN, 2016, IEEE T PATTERN ANAL, V38, P335, DOI 10.1109/TPAMI.2015.2417577; Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935; Li BY, 2019, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2019.00111; Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515; Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626; Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007; Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551; Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039; Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18; Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124; Lukezic A, 2019, IEEE I CONF COMP VIS, P10012, DOI 10.1109/ICCV.2019.01011; Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515; Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352; Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177; Milan A., 2016, MOT16 BENCHMARK MULT; Muller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19; Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152; Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27; NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465; Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sclaroff S, 2020, WACV; Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230; Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937; Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158; Valmadre J, 2018, LECT NOTES COMPUT SC, V11207, P692, DOI 10.1007/978-3-030-01219-9_41; Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531; Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376; Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357; Wang N, 2013, ADV NEURAL INFORM PR, DOI DOI 10.5555/2999611.2999702; Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140; Wang QZ, 2019, PROC CVPR IEEE, P4190, DOI 10.1109/CVPR.2019.00432; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Yan Bin, 2019, ICCV; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang J., 2014, EUR C COMP VIS; Zhang K., 2014, ECCV; Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang Yulun, 2018, P EUROPEAN C COMPUTE, P286; Zhang ZY, 2019, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR.2019.00484; Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710	93	21	22	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					439	461		10.1007/s11263-020-01387-y	http://dx.doi.org/10.1007/s11263-020-01387-y		SEP 2020	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000574045400001
J	Shao, ZW; Liu, ZL; Cai, JF; Ma, LZ				Shao, Zhiwen; Liu, Zhilei; Cai, Jianfei; Ma, Lizhuang			J(A)over-capA-Net: Joint Facial Action Unit Detection and Face Alignment Via Adaptive Attention	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Joint learning; Facial AU detection; Face alignment; Adaptive attention learning	REPRESENTATION; 3D	Facial action unit (AU) detection and face alignment are two highly correlated tasks, since facial landmarks can provide precise AU locations to facilitate the extraction of meaningful local features for AU detection. However, most existing AU detection works handle the two tasks independently by treating face alignment as a preprocessing, and often use landmarks to predefine a fixed region or attention for each AU. In this paper, we propose a novel end-to-end deep learning framework for joint AU detection and face alignment, which has not been explored before. In particular, multi-scale shared feature is learned firstly, and high-level feature of face alignment is fed into AU detection. Moreover, to extract precise local features, we propose an adaptive attention learning module to refine the attention map of each AU adaptively. Finally, the assembled local features are integrated with face alignment feature and global feature for AU detection. Extensive experiments demonstrate that our framework (i) significantly outperforms the state-of-the-art AU detection methods on the challenging BP4D, DISFA, GFT and BP4D+ benchmarks, (ii) can adaptively capture the irregular region of each AU, (iii) achieves competitive performance for face alignment, and (iv) also works well under partial occlusions and non-frontal poses. The code for our method is available at https://github.com/ZhiwenShao/PyTorch-JAANet.	[Shao, Zhiwen] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Jiangsu, Peoples R China; [Shao, Zhiwen] Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Jiangsu, Peoples R China; [Shao, Zhiwen; Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China; [Liu, Zhilei] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China; [Cai, Jianfei] Monash Univ, Fac Informat Technol, Clayton, Vic 3800, Australia; [Ma, Lizhuang] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China	China University of Mining & Technology; Shanghai Jiao Tong University; Tianjin University; Monash University; East China Normal University	Shao, ZW (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Jiangsu, Peoples R China.; Shao, ZW (corresponding author), Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Jiangsu, Peoples R China.; Shao, ZW (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.	zhiwen_shao@cumt.edu.cn	Shao, Zhiwen/N-8985-2018; Liu, Zhilei/B-3733-2015	Shao, Zhiwen/0000-0002-9383-8384; Liu, Zhilei/0000-0003-1447-6256	School of Computer Science and Technology, China University of Mining and Technology; National Key R&D Program of China [2019YFC1521104]; National Natural Science Foundation of China [61972157, 61503277]; Zhejiang Lab [2020NB0AB01]; Data Science & Artificial Intelligence Research Centre@NTU (DSAIR); Monash FIT Start-up Grant	School of Computer Science and Technology, China University of Mining and Technology; National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Zhejiang Lab; Data Science & Artificial Intelligence Research Centre@NTU (DSAIR); Monash FIT Start-up Grant	This work is supported by the Start-up Grant, School of Computer Science and Technology, China University of Mining and Technology. It is also partially supported by the National Key R&D Program of China (No. 2019YFC1521104), the National Natural Science Foundation of China (No. 61972157 and No. 61503277), the Zhejiang Lab (No. 2020NB0AB01), the Data Science & Artificial Intelligence Research Centre@NTU (DSAIR), and the Monash FIT Start-up Grant.	Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606; Batista JC, 2017, IEEE INT CONF AUTOMA, P866, DOI 10.1109/FG.2017.111; Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Chu WS, 2017, IEEE INT CONF AUTOMA, P25, DOI 10.1109/FG.2017.13; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Corneanu C, 2018, LECT NOTES COMPUT SC, V11216, P309, DOI 10.1007/978-3-030-01258-8_19; De la Torre F, 2015, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2015.7163082; Ekman P., 2002, FACIAL ACTION CODING; Ekman P., 1978, FACIAL ACTION CODING; Ertugrul Itir Onal, 2020, IEEE Trans Biom Behav Identity Sci, V2, P158, DOI [10.1109/tbiom.2020.2977225, 10.1109/TBIOM.2020.2977225]; Girard JM, 2017, IEEE INT CONF AUTOMA, P581, DOI 10.1109/FG.2017.144; Gudi A., 2015, 2015 11 IEEE INT C W, V6, P1; He J, 2017, IEEE INT CONF AUTOMA, P848, DOI 10.1109/FG.2017.108; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Li W, 2018, IEEE T PATTERN ANAL, V40, P2583, DOI 10.1109/TPAMI.2018.2791608; Li W, 2017, PROC CVPR IEEE, P6766, DOI 10.1109/CVPR.2017.716; Li XR, 2017, IEEE INT CONF AUTOMA, P860, DOI 10.1109/FG.2017.110; Li Y, 2019, PROC CVPR IEEE, P10916, DOI 10.1109/CVPR.2019.01118; Li YQ, 2013, IEEE T IMAGE PROCESS, V22, P2559, DOI 10.1109/TIP.2013.2253477; Martinez B, 2019, IEEE T AFFECT COMPUT, V10, P325, DOI 10.1109/TAFFC.2017.2731763; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Nair V, 2010, P 27 INT C MACHINE L, P807; Niu XS, 2019, PROC CVPR IEEE, P11909, DOI 10.1109/CVPR.2019.01219; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233; Sanchez E., 2018, P BRIT MACH VIS C 20; Sankaran N, 2019, IEEE INT CONF AUTOMA, P107; Shao Z, 2016, GREEN CHEM SUSTAIN T, P1, DOI 10.1007/978-3-662-52936-2; Shao ZW, 2022, IEEE T AFFECT COMPUT, V13, P1274, DOI 10.1109/TAFFC.2019.2948635; Shao ZW, 2020, NEUROCOMPUTING, V396, P477, DOI 10.1016/j.neucom.2018.11.108; Shao ZW, 2018, LECT NOTES COMPUT SC, V11217, P725, DOI 10.1007/978-3-030-01261-8_43; Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Valstar M., 2006, COMP VIS PATT REC WO, P149; Valstar MF, 2017, IEEE INT CONF AUTOMA, P839, DOI 10.1109/FG.2017.107; Wang Jingdong, 2020, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2020.2983686, 10.1109/tpami.2020.2983686]; Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227; Wu Y, 2017, PROC CVPR IEEE, P5719, DOI 10.1109/CVPR.2017.606; Wu Y, 2016, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2016.370; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Zeng JB, 2015, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2015.413; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374; Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369; Zhao KL, 2016, IEEE T IMAGE PROCESS, V25, P3931, DOI 10.1109/TIP.2016.2570550; Zhao KL, 2015, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2015.7298833; Zhong L, 2015, IEEE T CYBERNETICS, V45, P1499, DOI 10.1109/TCYB.2014.2354351	56	21	22	5	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					321	340		10.1007/s11263-020-01378-z	http://dx.doi.org/10.1007/s11263-020-01378-z		SEP 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000568233000003
J	Chadebecq, F; Vasconcelos, F; Lacher, R; Maneas, E; Desjardins, A; Ourselin, S; Vercauteren, T; Stoyanov, D				Chadebecq, Francois; Vasconcelos, Francisco; Lacher, Rene; Maneas, Efthymios; Desjardins, Adrien; Ourselin, Sebastien; Vercauteren, Tom; Stoyanov, Danail			Refractive Two-View Reconstruction for Underwater 3D Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Underwater imaging; Two-view Refractive Structure-from-Motion; Flat refractive geometry	CALIBRATION; GEOMETRY; CAMERAS; MOTION; MODEL	Recovering 3D geometry from cameras in underwater applications involves the Refractive Structure-from-Motion problem where the non-linear distortion of light induced by a change of medium density invalidates the single viewpoint assumption. The pinhole-plus-distortion camera projection model suffers from a systematic geometric bias since refractive distortion depends on object distance. This leads to inaccurate camera pose and 3D shape estimation. To account for refraction, it is possible to use the axial camera model or to explicitly consider one or multiple parallel refractive interfaces whose orientations and positions with respect to the camera can be calibrated. Although it has been demonstrated that the refractive camera model is well-suited for underwater imaging, Refractive Structure-from-Motion remains particularly difficult to use in practice when considering the seldom studied case of a camera with a flat refractive interface. Our method applies to the case of underwater imaging systems whose entrance lens is in direct contact with the external medium. By adopting the refractive camera model, we provide a succinct derivation and expression for the refractive fundamental matrix and use this as the basis for a novel two-view reconstruction method for underwater imaging. For validation we use synthetic data to show the numerical properties of our method and we provide results on real data to demonstrate its practical application within laboratory settings and for medical applications in fluid-immersed endoscopy. We demonstrate our approach outperforms classic two-view Structure-from-Motion method relying on the pinhole-plus-distortion camera model.	[Chadebecq, Francois; Vasconcelos, Francisco; Lacher, Rene; Maneas, Efthymios; Desjardins, Adrien; Stoyanov, Danail] Wellcome EPSRC Ctr Intervent & Surg Sci WEISS, London, England; [Ourselin, Sebastien; Vercauteren, Tom] Kings Coll London, Sch Biomed Engn & Imaging Sci, London, England	UK Research & Innovation (UKRI); Engineering & Physical Sciences Research Council (EPSRC); University of London; King's College London	Chadebecq, F (corresponding author), Wellcome EPSRC Ctr Intervent & Surg Sci WEISS, London, England.	f.chadebecq@ucl.ac.uk	Vercauteren, Tom/I-7290-2013; Ourselin, Sebastien/K-6960-2015	Vercauteren, Tom/0000-0003-1794-0456; Ourselin, Sebastien/0000-0002-5694-5340	EPSRC [EP/P012841/1] Funding Source: UKRI	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))		Agrawal A, 2013, PROC CVPR IEEE, P1399, DOI 10.1109/CVPR.2013.184; Agrawal A, 2012, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2012.6248073; [Anonymous], THESIS; [Anonymous], INT SOC PHOT REM SEN; [Anonymous], 2016, CORR; [Anonymous], METHODS OCEANOGRAPHY; [Anonymous], INT SOC OPTICS PHOTO; [Anonymous], 2010, MED IMAGE COMPUTING; [Anonymous], NAT C COMP VIS PATT; [Anonymous], 2015, P OCEANS 2015 GEN IT; [Anonymous], 2017, IEEE INT C COMP VIS; [Anonymous], 2009, BMVC 20 BRIT MACHINE; Bouguet J. Y., 2008, CAMERA CALIBRATION T; Calkins H, 2017, J ARRYTHM, V33, P369, DOI 10.1016/j.joa.2017.08.001; Chang YJ, 2011, IEEE I CONF COMP VIS, P351, DOI 10.1109/ICCV.2011.6126262; Chen XD, 2014, PROC CVPR IEEE, P524, DOI 10.1109/CVPR.2014.74; Fitzgibbon AW, 2001, PROC CVPR IEEE, P125; FRYER JG, 1986, PHOTOGRAMM REC, V12, P73; Gallup D., 2007 IEEE C COMP VIS, P1; Glaeser G., 2000, J GEOM GRAPH, V4, P1; Grossberg MD, 2005, INT J COMPUT VISION, V61, P119, DOI 10.1023/B:VISI.0000043754.56350.10; Grossberg MD, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P108, DOI 10.1109/ICCV.2001.937611; Haner S, 2015, PROC CVPR IEEE, P1428, DOI 10.1109/CVPR.2015.7298749; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hecht E., 2017, OPTICS, V5th ed.; Jordt-Sedlazeck A, 2013, IEEE I CONF COMP VIS, P57, DOI 10.1109/ICCV.2013.14; Jordt-Sedlazeck A, 2013, LECT NOTES COMPUT SC, V8142, P333, DOI 10.1007/978-3-642-40602-7_36; Jordt-Sedlazeck A, 2012, LECT NOTES COMPUT SC, V7576, P846, DOI 10.1007/978-3-642-33715-4_61; Kang L, 2012, LECT NOTES COMPUT SC, V7575, P303, DOI 10.1007/978-3-642-33765-9_22; Kang L, 2012, APPL OPTICS, V51, P7591, DOI 10.1364/AO.51.007591; Lavest J.-M., 2000, EUR C COMPUT VISION, P654, DOI DOI 10.1007/3-540-45053-X_42; Li HF, 2008, CURR MED RES OPIN, V24, P1, DOI [10.1185/030079908X253933, 10.1088/0256-307X/24/3/072]; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Luczynski T, 2017, OCEAN ENG, V133, P9, DOI 10.1016/j.oceaneng.2017.01.029; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; Massot-Campos M, 2015, SENSORS-BASEL, V15, P31525, DOI 10.3390/s151229864; Mouragnon E, 2009, IMAGE VISION COMPUT, V27, P1178, DOI 10.1016/j.imavis.2008.11.006; Ramalingam S, 2006, COMPUT VIS IMAGE UND, V103, P218, DOI 10.1016/j.cviu.2006.06.006; Rizzini DL, 2015, INT J ADV ROBOT SYST, V12, DOI 10.5772/60526; Sedlazeck Anne, 2012, Outdoor and Large-Scale Real-World Scene Analysis. 15th International Workshop on Theoretical Foundations of Computer Vision. Revised Selected Papers, P212, DOI 10.1007/978-3-642-34091-8_10; Shibata A, 2015, 2015 IEEE/SICE INTERNATIONAL SYMPOSIUM ON SYSTEM INTEGRATION (SII), P540, DOI 10.1109/SII.2015.7405037; Shibata A, 2015, IEEE INT CONF ROBOT, P5239, DOI 10.1109/ICRA.2015.7139929; Steigman SA, 2010, FETAL DIAGN THER, V27, P87, DOI 10.1159/000262279; Sturm P, 2005, PROC CVPR IEEE, P206; Sturm P, 2004, LECT NOTES COMPUT SC, V3022, P1; Sturm P, 2008, LECT NOTES COMPUT SC, V5305, P609, DOI 10.1007/978-3-540-88693-8_45; Treibitz T, 2012, IEEE T PATTERN ANAL, V34, P51, DOI 10.1109/TPAMI.2011.105; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Yau T, 2013, PROC CVPR IEEE, P2499, DOI 10.1109/CVPR.2013.323; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	50	21	21	7	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1101	1117		10.1007/s11263-019-01218-9	http://dx.doi.org/10.1007/s11263-019-01218-9			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW	33343083	Green Published, hybrid			2022-12-18	WOS:000531431500003
J	Lenc, K; Vedaldi, A				Lenc, Karel; Vedaldi, Andrea			Understanding Image Representations by Measuring Their Equivariance and Equivalence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image representations; Geometric equivariance; Equivalent representations; Convolutional neural networks	CONVOLUTIONAL NEURAL-NETWORKS	Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aimed at filling this gap, we investigate two key mathematical properties of representations: equivariance and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parameterizations of a CNN, two different layers, or two different CNN architectures, share the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved and how various CNN architectures differ. We identify several predictors of geometric and architectural compatibility, including the spatial resolution of the representation and the complexity and depth of the models. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too.	[Lenc, Karel; Vedaldi, Andrea] Univ Oxford, Dept Engn Sci, Oxford, England	University of Oxford	Lenc, K (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	karel@robots.ox.ac.uk; vedaldi@robots.ox.ac.uk	Vedaldi, Andrea/B-9071-2015; Lenc, Karel/N-3273-2017	Vedaldi, Andrea/0000-0003-1374-2858; Lenc, Karel/0000-0001-6119-0045	ERC 677195-IDIU Oxford Engineering Science DTA	ERC 677195-IDIU Oxford Engineering Science DTA	We would like to thank Samuel Albanie for help in preparing this manuscript. Karel Lenc was supported by ERC 677195-IDIU Oxford Engineering Science DTA.	Albanie S, 2017, ESTIMATES MEMORY CON; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anselmi F, 2016, THEOR COMPUT SCI, V633, P112, DOI 10.1016/j.tcs.2015.06.048; Aubry M, 2015, IEEE I CONF COMP VIS, P2875, DOI 10.1109/ICCV.2015.329; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Canziani A, 2016, ARXIV; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen L, 2014, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2014.135; Cohen TS, 2016, PR MACH LEARN RES, V48; Cohen Taco S., 2017, 5 INT C LEARN REPR I, P2; Csurka G., 2004, P ECCV WORKSH STAT L; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dieleman S, 2016, PR MACH LEARN RES, V48; Dieleman S, 2015, MON NOT R ASTRON SOC, V450, P1441, DOI 10.1093/mnras/stv632; Donahue J, 2013, P 31 INT C MACH LEAR; Everingham M., 2007, PASCAL VISUAL OBIECT; Felzenszwalb P. F., 2009, OBJECT DETECTION DIS; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646, DOI DOI 10.5555/2984093.2984166; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laptev D, 2016, PROC CVPR IEEE, P289, DOI 10.1109/CVPR.2016.38; Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638; Li Y., 2015, FE NIPS, P196; Lindeberg T, 1998, 9814SE ISRN KTHNAP; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mahendran A, 2016, INT J COMPUT VISION, V120, P233, DOI 10.1007/s11263-016-0911-8; Mikolajczyk K., 2003, P CVPR; Parkhi O., 2011, P ICCV; Perronnin F., 2006, P CVPR; Ranzato Marc'Aurelio., 2007, PROC CVPR IEEE, P1, DOI [10.1109/CVPR.2007.383157, DOI 10.1109/CVPR.2007.383157]; Razavian A., 2014, CVPR DEEPVISION WORK; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schimdt U., 2012, P CVPR; Schmidt U, 2012, PROC CVPR IEEE, P2050, DOI 10.1109/CVPR.2012.6247909; Sifre L, 2013, P CVPR; Simonyan K., 2013, P NIPS; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Sjostrand K, 2018, J STAT SOFTW, V84, P1, DOI 10.18637/jss.v084.i10; Sohn K., 2012, ARXIV12066418 CORR; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Taskar B, 2004, ADV NEUR IN, V16, P25; Vedaldi A., 2010, P ACM INT C MULT; Vedaldi A., 2005, P ICCV; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Vondrick C., 2013, P ICCV; Wang J., 2010, P CVPR; Yang J., 2010, P CVPR; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhou X., 2010, P ECCV; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	68	21	22	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2019	127	5					456	476		10.1007/s11263-018-1098-y	http://dx.doi.org/10.1007/s11263-018-1098-y			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HO7XB	31148885	Green Published, hybrid, Green Submitted			2022-12-18	WOS:000461162200003
J	Zendel, O; Murschitz, M; Humenberger, M; Herzner, W				Zendel, Oliver; Murschitz, Markus; Humenberger, Martin; Herzner, Wolfgang			How Good Is My Test Data? Introducing Safety Analysis for Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	IEEE International Conference on Computer Vision (ICCV)	DEC 11-18, 2015	Santiago, CHILE	CPS, IEEE Comp Soc, Amazon, Microsoft, SENSETIME, Baidu, Intel, Facebook, Adobe, Panasonic, Google, OMRON, Blippar, iRobot, HISCENE, NVIDIA, Viscovery, AiCUre, M Tec, Inst Elect & Elect Engineers, Comp Vis Fdn		Test data; Testing; Validation; Safety analysis; Hazard analysis; Stereo vision		Good test data is crucial for driving new developments in computer vision (CV), but two questions remain unanswered: which situations should be covered by the test data, and how much testing is enough to reach a conclusion? In this paper we propose a new answer to these questions using a standard procedure devised by the safety community to validate complex systems: the hazard and operability analysis (HAZOP). It is designed to systematically identify possible causes of system failure or performance loss. We introduce a generic CV model that creates the basis for the hazard analysis and-for the first time-apply an extensive HAZOP to the CV domain. The result is a publicly available checklist with more than 900 identified individual hazards. This checklist can be utilized to evaluate existing test datasets by quantifying the covered hazards. We evaluate our approach by first analyzing and annotating the popular stereo vision test datasets Middlebury and KITTI. Second, we demonstrate a clearly negative influence of the hazards in the checklist on the performance of six popular stereo matching algorithms. The presented approach is a useful tool to evaluate and improve test datasets and creates a common basis for future dataset designs.	[Zendel, Oliver; Murschitz, Markus; Humenberger, Martin; Herzner, Wolfgang] AIT, Donau City Str 1, A-1220 Vienna, Austria	Austrian Institute of Technology (AIT)	Zendel, O (corresponding author), AIT, Donau City Str 1, A-1220 Vienna, Austria.	oliver.zendel@ait.ac.at	Humenberger, Martin/H-6499-2019	Humenberger, Martin/0000-0003-0600-9164	ARTEMIS [100233]; European Initiative to Enable Validation for Highly Automated Safe and Secure Systems (ENABLE-S3) Joint Undertaking [692455]; European Union's HORIZON research and innovation programme; Austrian Research Promotion Agency [848896]	ARTEMIS(European Commission); European Initiative to Enable Validation for Highly Automated Safe and Secure Systems (ENABLE-S3) Joint Undertaking; European Union's HORIZON research and innovation programme; Austrian Research Promotion Agency	Special thanks for their extensive CV-HAZOP contributions go to Lawitzky G., Wichert G., Feiten W. (Siemens Munich), Kothe U. (HCI Heidelberg), Fischer J. (Fraunhofer IPA), and Zinner C. (AIT). Thanks to Cho J.-H. (TU Wien) and Beham M. (AIT) for their help with the example chapter. The creation of the CV-HAZOP as well as this work have been funded by the ARTEMIS Project R3-COP, No. 100233 and the European Initiative to Enable Validation for Highly Automated Safe and Secure Systems (ENABLE-S3) Joint Undertaking under grant agreement grant agreement No. 692455. This joint undertaking receives support from the European Union's HORIZON 2020 research and innovation programme and Austria, Denmark, Germany, Finland, Czech Republic, Italy, Spain, Portugal, Poland, Ireland, Belgium, France, Netherlands, United Kingdom, Slovakia, Norway. Additional support was received by the project autoBAHN2020 funded by the Austrian Research Promotion Agency with Contract Number 848896.	ALOIMONOS J, 1989, INTEGRATION VISUAL M; [Anonymous], 1991, MILHDBK217F DEP DEF; Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14; Bowyer K, 2001, COMPUT VIS IMAGE UND, V84, P77, DOI 10.1006/cviu.2001.0931; Bowyer K., 1998, EMPIRICAL EVALUATION; Center for Chemical Process Safety, 1992, GUID HAZ EV PROC WOR; Demsar J, 2006, J MACH LEARN RES, V7, P1; Department of Defense, 1949, MILSTD1629A DEP DEF; Fenelon P., 1994, RISK MANAGEMENT CRIT, P11; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Geiger A., 2012, P IEEE COMP SOC C CO; Goseva-Popstojanova K, 2003, IEEE T SOFTWARE ENG, V29, P946, DOI 10.1109/TSE.2003.1237174; HAMPEL FR, 1971, ANN MATH STAT, V42, P1887, DOI 10.1214/aoms/1177693054; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Honauer K, 2015, IEEE I CONF COMP VIS, P2120, DOI 10.1109/ICCV.2015.245; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Humenberger M, 2010, COMPUT VIS IMAGE UND, V114, P1180, DOI 10.1016/j.cviu.2010.03.012; International Electrotechnical Commission, 2010, 615084 IEC; Kadiofsky T, 2012, LECT NOTES COMPUT SC, V7431, P404, DOI 10.1007/978-3-642-33179-4_39; Kajiya J.T., 1986, SIGGRAPH, P143, DOI [DOI 10.1145/15922.15902, 10.1145/15886.15902, DOI 10.1145/15886.15902]; Kletz T. A., 1983, HAZOP HAZAN NOTES ID; Kondermann D., 2015, P AS C COMP VIS; Kondermann D, 2013, P INT WORKSH VID IM; Konolige K., 1998, ROBOTICS RES; Laprie J., 1992, DEPENDABLE COMPUTING, V5; Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information; Matthias B., 2010, P 41 INT S ROB 6 GER; Mei X, 2013, PROC CVPR IEEE, P313, DOI 10.1109/CVPR.2013.47; Min J, 2004, IEEE T SYST MAN CY B, V34, P263, DOI 10.1109/TSMCB.2003.811118; Nyquist H., 1928, T AM I ELECT ENG, V47, P617, DOI [10.1109/T-AIEE.1928.5055024, DOI 10.1109/T-AIEE.1928.5055024]; Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027; Ponce J, 2006, LECT NOTES COMPUT SC, V4170, P29; Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; SCHARSTEIN D, 2003, COMPUTER VISION PATT; Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3; Schlick Rupert, 2011, Computer Safety, Reliability, and Security. Proceedings 30th International Conference, SAFECOMP 2011, P270, DOI 10.1007/978-3-642-24270-0_20; Shannon C.E., 1949, MATH THEORY COMMUNIC; SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969; Strecha C, 2008, PROC CVPR IEEE, P2838; Takanen A., 2008, FUZZING SOFTWARE SEC; Thacker NA, 2008, COMPUT VIS IMAGE UND, V109, P305, DOI 10.1016/j.cviu.2007.04.006; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Van der Spek R, 1997, KNOWLEDGE MANAGEMENT; Vesely W. E., 1981, SYSTEMS RELIABILITY; Von Bertalanffy L., 1968, GEN SYSTEM THEORY, V41973, P40; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; Zhang K., 2014, COMPUTER VISION PATT	48	21	22	3	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2017	125	1-3			SI		95	109		10.1007/s11263-017-1020-z	http://dx.doi.org/10.1007/s11263-017-1020-z			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	FL2TO		hybrid			2022-12-18	WOS:000414072800008
J	Ren, CY; Prisacariu, VA; Kahler, O; Reid, ID; Murray, DW				Ren, C. Y.; Prisacariu, V. A.; Kahler, O.; Reid, I. D.; Murray, D. W.			Real-Time Tracking of Single and Multiple Objects from Depth-Colour Imagery Using 3D Signed Distance Functions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-object tracking; Depth tracking; RGB-D imagery; Signed distance functions; Real-time	VISUAL TRACKING	We describe a novel probabilistic framework for real-time tracking of multiple objects from combined depth-colour imagery. Object shape is represented implicitly using 3D signed distance functions. Probabilistic generative models based on these functions are developed to account for the observed RGB-D imagery, and tracking is posed as a maximum a posteriori problem. We present first a method suited to tracking a single rigid 3D object, and then generalise this to multiple objects by combining distance functions into a shape union in the frame of the camera. This second model accounts for similarity and proximity between objects, and leads to robust real-time tracking without recourse to bolt-on or ad-hoc collision detection.	[Ren, C. Y.; Prisacariu, V. A.; Kahler, O.; Murray, D. W.] Univ Oxford, Dept Engn Sci, Oxford, England; [Reid, I. D.] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia	University of Oxford; University of Adelaide	Ren, CY (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	carl@robots.ox.ac.uk; victor@robots.ox.ac.uk; olaf@robots.ox.ac.uk; ian.reid@adelaide.edu.au; dwm@robots.ox.ac.uk		Reid, Ian/0000-0001-7790-6423	EU [287713]; UK's Engineering and Physical Science Research Council [EP/H050795, EP/J014990]; Australian Research Council [FL130100102]; Engineering and Physical Sciences Research Council [EP/J014990/1] Funding Source: researchfish; EPSRC [EP/J014990/1] Funding Source: UKRI	EU(European Commission); UK's Engineering and Physical Science Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Australian Research Council(Australian Research Council); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was funded by the Project REWIRE (Grant No. 287713) under the EU 7th Framework Programme, by Grants EP/H050795 and EP/J014990 from the UK's Engineering and Physical Science Research Council, and by a Laureate Fellowship (FL130100102) to IDR from the Australian Research Council.	Azad Pedram, 2011, IEEE International Conference on Robotics and Automation, P5204; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Choi C, 2013, IEEE INT C INT ROBOT, P1084, DOI 10.1109/IROS.2013.6696485; Choi C, 2010, IEEE INT CONF ROBOT, P4048, DOI 10.1109/ROBOT.2010.5509171; Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620; Fitzgibbon AW, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P662, DOI 10.1109/ICCV.2001.937584; GENNERY DB, 1992, INT J COMPUT VISION, V7, P243, DOI 10.1007/BF00126395; Harris C., 1990, P 1 BRIT MACH VIS C; Held RT, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P423; Kiyoung Kim, 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P193, DOI 10.1109/ISMAR.2010.5643569; Klein George, 2007, P1; Kyriazis N, 2013, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2013.9; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; LOWE DG, 1992, INT J COMPUT VISION, V8, P113, DOI 10.1007/BF00127170; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Oikonomidis I, 2012, PROC CVPR IEEE, P1862, DOI 10.1109/CVPR.2012.6247885; Oikonomidis I, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.101; Oikonomidis I, 2011, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2011.6126483; Prisacariu V. A., 2012, LNCS, P593, DOI DOI 10.1007/978-3-642-37331-2_45; Prisacariu VA, 2012, INT J COMPUT VISION, V98, P335, DOI 10.1007/s11263-011-0514-3; Prisacariu VA, 2013, INT SYM MIX AUGMENT, P89, DOI 10.1109/ISMAR.2013.6671768; Ren Carl Yuheng, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P47, DOI 10.1109/3DV.2014.39; Ren CY, 2013, IEEE I CONF COMP VIS, P1561, DOI 10.1109/ICCV.2013.197; Ren CY, 2012, LECT NOTES COMPUT SC, V7584, P72, DOI 10.1007/978-3-642-33868-7_8; SHUSTER MD, 1993, J ASTRONAUT SCI, V41, P439; SMITH R, 2006, OPEN DYNAMICS ENGINE; Sturm J, 2013, LECT NOTES COMPUT SC, V8142, P405, DOI 10.1007/978-3-642-40602-7_43; Ueda R., 2012, TRACKING 3D OBJECTS; Wuthrich M, 2013, IEEE INT C INT ROBOT, P3195, DOI 10.1109/IROS.2013.6696810	29	21	22	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2017	124	1					80	95		10.1007/s11263-016-0978-2	http://dx.doi.org/10.1007/s11263-016-0978-2			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FA2JD	32025093	Green Published, Green Submitted, hybrid			2022-12-18	WOS:000405265900005
J	Sagonas, C; Panagakis, Y; Zafeiriou, S; Pantic, M				Sagonas, Christos; Panagakis, Yannis; Zafeiriou, Stefanos; Pantic, Maja			Robust Statistical Frontalization of Human and Animal Faces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose normalization; Landmark localization; Face recognition; Low rank; Sparsity	RECOGNITION; ALIGNMENT	The unconstrained acquisition of facial data in real-world conditions may result in face images with significant pose variations, illumination changes, and occlusions, affecting the performance of facial landmark localization and recognition methods. In this paper, a novel method, robust to pose, illumination variations, and occlusions is proposed for joint face frontalization and landmark localization. Unlike the state-of-the-art methods for landmark localization and pose correction, where large amount of manually annotated images or 3D facial models are required, the proposed method relies on a small set of frontal images only. By observing that the frontal facial image of both humans and animals, is the one having the minimum rank of all different poses, a model which is able to jointly recover the frontalized version of the face as well as the facial landmarks is devised. To this end, a suitable optimization problem is solved, concerning minimization of the nuclear norm (convex surrogate of the rank function) and the matrix l(1) norm accounting for occlusions. The proposed method is assessed in frontal view reconstruction of human and animal faces, landmark localization, pose-invariant face recognition, face verification in unconstrained conditions, and video inpainting by conducting experiment on 9 databases. The experimental results demonstrate the effectiveness of the proposed method in comparison to the state-of-the-art methods for the target problems.	[Sagonas, Christos; Panagakis, Yannis; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England; [Pantic, Maja] Univ Twente, Fac Elect Engn Math & Comp Sci, Enschede, Netherlands	Imperial College London; University of Twente	Sagonas, C (corresponding author), Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England.	c.sagonas@imperial.ac.uk; i.panagakis@imperial.ac.uk; s.zafeiriou@imperial.ac.uk; m.pantic@imperial.ac.uk	Panagakis, Yannis/AAZ-8090-2020	Panagakis, Ioannis/0000-0003-0153-5210	EPSRC Project [EP/J017787/1, EP/N007743/1]; European Community Horizon 2020 [H2020] [688520, 645094]; Engineering and Physical Sciences Research Council [EP/J017787/1, EP/N007743/1, EP/H016988/1] Funding Source: researchfish; EPSRC [EP/N007743/1, EP/J017787/1, EP/H016988/1] Funding Source: UKRI	EPSRC Project(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Community Horizon 2020 [H2020]; Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The work of Christos Sagonas was funded by EPSRC Project EP/J017787/1 (4DFAB). The work of Stefanos Zafeiriou was partially funded by the European Community Horizon 2020 [H2020/2014-2020] under Grant Agreement No. 688520 (TeSLA) and by EPSRC project EP/N007743/1 (FACER2VM). The work of Yannis Panagakis and Maja Pantic is partially supported by the European Community Horizon 2020 [H2020/2014-2020] under Grant agreement no. 645094 (SEWA). The work of Maja Pantic is also partially supported by EPSRC Project EP/N007743/1 (FACER2VM).	Alabort-i-Medina J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P679, DOI 10.1145/2647868.2654890; Angelova A, 2013, IEEE WORK APP COMP, P39, DOI 10.1109/WACV.2013.6474997; Antonakos E, 2014, PROC CVPR IEEE, P1813, DOI 10.1109/CVPR.2014.234; Arashloo S. R., 2013, P IEEE 6 INT C BIOM, P1, DOI DOI 10.1109/BTAS.2013.6712721; Arashloo SR, 2014, IEEE T INF FOREN SEC, V9, P2100, DOI 10.1109/TIFS.2014.2359587; Ashraf A.B., 2008, CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587754; Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; Asthana A, 2011, IEEE I CONF COMP VIS, P937, DOI 10.1109/ICCV.2011.6126336; Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602; Bertsekas D.P., 2019, REINFORCEMENT LEARNI; Branson Steve, 2014, BRIT C MACH VIS; Cai J., 2010, 5 UCLA CAM; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chai XJ, 2007, IEEE T IMAGE PROCESS, V16, P1716, DOI 10.1109/TIP.2007.899195; Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49; Cheng XL, 2013, 2013 IEEE MILITARY COMMUNICATIONS CONFERENCE (MILCOM 2013), P1, DOI 10.1109/MILCOM.2013.9; Cheng X, 2013, IEEE I CONF COMP VIS, P577, DOI 10.1109/ICCV.2013.77; Cui Z, 2013, PROC CVPR IEEE, P3554, DOI 10.1109/CVPR.2013.456; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P907, DOI 10.1002/cpa.20131; Fazel M., 2002, MATRIX RANK MINIMIZA; Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215; Georgakis C, 2016, IEEE T IMAGE PROCESS, V25, P2021, DOI 10.1109/TIP.2016.2539502; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hassner T., 2015, P IEEE INT C COMP VI; Hu J., 2014, P AS C COMP VIS ACCV; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Huang G., 2012, ADV NEURAL INFORM PR, P764; Huang G.B., 2008, WORKSHOP FACESREAL L; Ho HT, 2013, IEEE T IMAGE PROCESS, V22, P1571, DOI 10.1109/TIP.2012.2233489; Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81; Juefei-Xu F, 2015, IEEE T IMAGE PROCESS, V24, P4780, DOI 10.1109/TIP.2015.2468173; Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Li AN, 2012, IEEE T IMAGE PROCESS, V21, P305, DOI 10.1109/TIP.2011.2160957; Li H., 2014, P AS C COMP VIS ACCV; Li HX, 2015, PROC CVPR IEEE, P4055, DOI 10.1109/CVPR.2015.7299032; Li HX, 2013, PROC CVPR IEEE, P3499, DOI 10.1109/CVPR.2013.449; Li SX, 2012, LECT NOTES COMPUT SC, V7572, P102, DOI 10.1007/978-3-642-33718-5_8; Lin YL, 2014, LECT NOTES COMPUT SC, V8692, P466, DOI 10.1007/978-3-319-10593-2_31; Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Nilsback M-E., 2006, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2006., DOI 10.1109/CVPR.2006.42]; Oh TH, 2015, PROC CVPR IEEE, P4484, DOI 10.1109/CVPR.2015.7299078; Panagakis Y., 2015, IEEE T PATTERN ANAL; Papamakarios G., 2014, P BRIT MACH VIS C BM, P11; Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092; Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282; Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Sagonas C, 2015, IEEE I CONF COMP VIS, P3871, DOI 10.1109/ICCV.2015.441; Sagonas C, 2014, PROC CVPR IEEE, P1789, DOI 10.1109/CVPR.2014.231; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tzimiropoulos Georgios, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P650, DOI 10.1007/978-3-642-37431-9_50; Tzimiropoulos G, 2012, IEEE T PATTERN ANAL, V34, P2454, DOI 10.1109/TPAMI.2012.40; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003; Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yi D, 2013, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2013.454; Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zhang WC, 2005, IEEE I CONF COMP VIS, P786; Zhang W, 2011, PROC CVPR IEEE, P513, DOI 10.1109/CVPR.2011.5995324; Zhang WW, 2008, LECT NOTES COMPUT SC, V5305, P802, DOI 10.1007/978-3-540-88693-8_59; Zhang ZD, 2012, INT J COMPUT VISION, V99, P1, DOI 10.1007/s11263-012-0515-x; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21	78	21	21	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		270	291		10.1007/s11263-016-0920-7	http://dx.doi.org/10.1007/s11263-016-0920-7			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS	32226226	Green Published, Green Accepted			2022-12-18	WOS:000398162200006
J	Niu, L; Li, W; Xu, D				Niu, Li; Li, Wen; Xu, Dong			Exploiting Privileged Information from Web Data for Action and Event Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Learning using privileged information; Multi-instance learning; Domain adaptation; Action recognition; Event recognition	DOMAIN ADAPTATION; KERNEL; IMAGES; KNOWLEDGE; OBJECTS; VIDEOS	In the conventional approaches for action and event recognition, sufficient labelled training videos are generally required to learn robust classifiers with good generalization capability on new testing videos. However, collecting labelled training videos is often time consuming and expensive. In this work, we propose new learning frameworks to train robust classifiers for action and event recognition by using freely available web videos as training data. We aim to address three challenging issues: (1) the training web videos are generally associated with rich textual descriptions, which are not available in test videos; (2) the labels of training web videos are noisy and may be inaccurate; (3) the data distributions between training and test videos are often considerably different. To address the first two issues, we propose a new framework called multi-instance learning with privileged information (MIL-PI) together with three new MIL methods, in which we not only take advantage of the additional textual descriptions of training web videos as privileged information, but also explicitly cope with noise in the loose labels of training web videos. When the training and test videos come from different data distributions, we further extend our MIL-PI as a new framework called domain adaptive MIL-PI. We also propose another three new domain adaptation methods, which can additionally reduce the data distribution mismatch between training and test videos. Comprehensive experiments for action and event recognition demonstrate the effectiveness of our proposed approaches.	[Niu, Li] Nanyang Technol Univ, Interdisciplinary Grad Sch, 50 Nanyang Ave, Singapore 639798, Singapore; [Li, Wen] ETH, Comp Vis Lab, Sternwartstr 7, CH-8092 Zurich, Switzerland; [Xu, Dong] Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Sydney	Li, W (corresponding author), ETH, Comp Vis Lab, Sternwartstr 7, CH-8092 Zurich, Switzerland.	lniu002@ntu.edu.sg; liwen@vision.ee.ethz.ch; dongxudongxu@gmail.com	Xu, Dong/A-3694-2011	Xu, Dong/0000-0003-2775-9730; Li, Wen/0000-0002-5559-8594	Faculty of Engineering & Information Technologies, The University of Sydney, under the Faculty Research Cluster Program; Singapore MoE Tier 2 Grant [ARC42/13]	Faculty of Engineering & Information Technologies, The University of Sydney, under the Faculty Research Cluster Program; Singapore MoE Tier 2 Grant(Ministry of Education, Singapore)	This research was supported by funding from the Faculty of Engineering & Information Technologies, The University of Sydney, under the Faculty Research Cluster Program. This work was also supported by the Singapore MoE Tier 2 Grant (ARC42/13).	Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653; Andrews S., 2002, SUPPORT VECTOR MACHI, P561; Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100; Bergamo A., 2010, ADV NEURAL INFORM PR, P181; Bobick AF, 1997, PHILOS T ROY SOC B, V352, P1257, DOI 10.1098/rstb.1997.0108; Bootkrajang J, 2014, PATTERN RECOGN, V47, P3641, DOI 10.1016/j.patcog.2014.05.007; Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57; Bunescu R.C., 2007, P ICML, P105, DOI DOI 10.1145/1273496.1273510; Chang S.-F., 2007, P INT WORKSH MULT IN, P255; Chen L, 2013, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2013.344; Chen XL, 2013, IEEE I CONF COMP VIS, P1409, DOI 10.1109/ICCV.2013.178; Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248; Chu WS, 2013, PROC CVPR IEEE, P3515, DOI 10.1109/CVPR.2013.451; Duan LX, 2012, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2012.6247819; Duan Lixin, 2012, IEEE Trans Neural Netw Learn Syst, V23, P504, DOI 10.1109/TNNLS.2011.2178556; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114; Duan LX, 2011, IEEE T IMAGE PROCESS, V20, P3280, DOI 10.1109/TIP.2011.2159227; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Farquhar Jason, 2005, ADV NEURAL INFORM PR, V18; Fergus R., 2005, ICCV; Fernando B., 2013, ICCV; Ferrari Vittorio, 2007, NIPS; Fouad S, 2013, IEEE T NEUR NET LEAR, V24, P1086, DOI 10.1109/TNNLS.2013.2251470; Gehler P., 2008, NIPS WORKSH KERN LEA; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Hu YX, 2009, IEEE I CONF COMP VIS, P128, DOI 10.1109/ICCV.2009.5459153; Hwang SJ, 2012, INT J COMPUT VISION, V100, P134, DOI 10.1007/s11263-011-0494-3; Jiang Y.G, 2011, PROC 1 ACM INT C MUL, P29; Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2; Kloft M, 2011, J MACH LEARN RES, V12, P953; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496; Leung T, 2011, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2011.6126479; Li QN, 2013, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2013.115; Li W., 2011, IEEE C COMP VIS PATT, P2368; Li W, 2014, LECT NOTES COMPUT SC, V8693, P437, DOI 10.1007/978-3-319-10602-1_29; Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167; Li W, 2012, IEEE DATA MINING, P419, DOI 10.1109/ICDM.2012.78; Li W, 2012, PROC CVPR IEEE, P2368, DOI 10.1109/CVPR.2012.6247949; Li Y.-F., 2009, P 12 INT C ART INT S, V5, P344; Liang LC, 2009, NEURAL NETWORKS, V22, P766, DOI 10.1016/j.neunet.2009.06.030; Lin Z, 2009, IEEE I CONF COMP VIS, P444; Morariu V. I., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3289, DOI 10.1109/CVPR.2011.5995386; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133; Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107; Shi YF, 2004, PROC CVPR IEEE, P862; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56; Tran SD, 2008, LECT NOTES COMPUT SC, V5303, P610, DOI 10.1007/978-3-540-88688-4_45; Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042; Vijayanarasimhan S., 2008, P IEEE C COMP VIS PA, p[1, 1], DOI DOI 10.1109/CVPR.2008.4587632; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang LA, 2011, INT J COMPUT VISION, V93, P162, DOI 10.1007/s11263-010-0393-z; Xu D, 2008, IEEE T PATTERN ANAL, V30, P1985, DOI 10.1109/TPAMI.2008.129; Yanagawa A, KODAK CONSUMER VIDEO; Yu T. H., 2010, BMVA PRESS, V52, P12, DOI DOI 10.5244/C.24.52; Zeng Z, 2010, LECT NOTES COMPUT SC, V6316, P532, DOI 10.1007/978-3-642-15567-3_39; Zhou Zhi-Hua, 2006, ADV NEURAL INFORM PR, P1609; Zhu G., 2009, P 17 ACM INT C MULT, P165, DOI DOI 10.1145/1631272.1631297	68	21	21	1	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2016	118	2			SI		130	150		10.1007/s11263-015-0862-5	http://dx.doi.org/10.1007/s11263-015-0862-5			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DO0OE					2022-12-18	WOS:000377477400003
J	Simo-Serra, E; Torras, C; Moreno-Noguer, F				Simo-Serra, Edgar; Torras, Carme; Moreno-Noguer, Francesc			DaLI: Deformation and Light Invariant Descriptor	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Local image descriptors; Diffusion equation; Heat kernel descriptors; Deformation and illumination invariance	SHAPE; RECOGNITION; FRAMEWORK	Recent advances in 3D shape analysis and recognition have shown that heat diffusion theory can be effectively used to describe local features of deforming and scaling surfaces. In this paper, we show how this description can be used to characterize 2D image patches, and introduce DaLI, a novel feature point descriptor with high resilience to non-rigid image transformations and illumination changes. In order to build the descriptor, 2D image patches are initially treated as 3D surfaces. Patches are then described in terms of a heat kernel signature, which captures both local and global information, and shows a high degree of invariance to non-linear image warps. In addition, by further applying a logarithmic sampling and a Fourier transform, invariance to photometric changes is achieved. Finally, the descriptor is compacted by mapping it onto a low dimensional subspace computed using Principal Component Analysis, allowing for an efficient matching. A thorough experimental validation demonstrates that DaLI is significantly more discriminative and robust to illuminations changes and image transformations than state of the art descriptors, even those specifically designed to describe non-rigid deformations.	[Simo-Serra, Edgar; Torras, Carme; Moreno-Noguer, Francesc] CSIC UPC, Inst Robot & Informat Ind, Barcelona 08028, Spain	Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i Informatica Industrial (IRII); Universitat Politecnica de Catalunya	Moreno-Noguer, F (corresponding author), CSIC UPC, Inst Robot & Informat Ind, Barcelona 08028, Spain.	esimo@iri.upc.edu; torras@iri.upc.edu; fmoreno@iri.upc.edu	Torras, Carme/M-1794-2014; Torras, Carme/AAG-3027-2019	Torras, Carme/0000-0002-2933-398X; Torras, Carme/0000-0002-2933-398X	EU Project IntellAct [FP7-ICT2009-6-269959]; Spanish Ministry of Economy and Competitiveness [PCIN-2013-047, DPI2011-27510]	EU Project IntellAct; Spanish Ministry of Economy and Competitiveness(Spanish Government)	This work has been partially funded by the Spanish Ministry of Economy and Competitiveness under Projects ERA-Net Chistera project ViSen PCIN-2013-047 and PAU+ DPI2011-27510, and by the EU Project IntellAct FP7-ICT2009-6-269959.	Aflalo Y., 2011, P SSVM; [Anonymous], 2011, INT C SCAL SPAC VAR; Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Berg AC, 2005, PROC CVPR IEEE, P26; Bronstein AM, 2008, INT J COMPUT VISION, V78, P67, DOI 10.1007/s11263-007-0078-4; Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838; Cai HP, 2011, IEEE T PATTERN ANAL, V33, P338, DOI 10.1109/TPAMI.2010.89; Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222; Chavel I., 1984, EIGENVALUES RIEMANNI; Cheng H., 2008, IEEE C COMP VIS PATT; Cho M, 2009, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2009.5459322; de Goes F, 2008, COMPUT GRAPH FORUM, V27, P1349, DOI 10.1111/j.1467-8659.2008.01274.x; Fan B, 2012, IEEE T PATTERN ANAL, V34, P2031, DOI 10.1109/TPAMI.2011.277; Feng Tang, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2631, DOI 10.1109/CVPRW.2009.5206550; Gebal K, 2009, COMPUT GRAPH FORUM, V28, P1405, DOI 10.1111/j.1467-8659.2009.01517.x; Gupta R., 2007, INT C COMP VIS; Gupta R., 2010, IEEE C COMP VIS PATT; Gupta R, 2008, LECT NOTES COMPUT SC, V5303, P265, DOI 10.1007/978-3-540-88688-4_20; Heikkila M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014; Ke Y, 2004, PROC CVPR IEEE, P506; Kokkinos I., 2012, RR7914 INRIA; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66; Ling H., 2010, EUR C COMP VIS; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; Ling HB, 2005, IEEE I CONF COMP VIS, P1466; Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730; Moreno-Noguer F, 2013, IEEE T PATTERN ANAL, V35, P463, DOI 10.1109/TPAMI.2012.102; Moreno-Noguer F, 2011, PROC CVPR IEEE, P1593, DOI 10.1109/CVPR.2011.5995529; Philbin J, 2010, LECT NOTES COMPUT SC, V6313, P677; Pinkall U., 1993, EXPT MATH, V2, P15, DOI DOI 10.1080/10586458.1993.10504266; Raviv D, 2011, PROC CVPR IEEE; Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011; Rustamov Raif M, 2007, P 5 EUR S GEOM PROC, P225, DOI DOI 10.2312/SGP/SGP07/225-233; Sanchez-Riera J, 2010, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2010.5539831; Serradell E., 2012, IEEE C COMP VIS PATT; Shi L, 2006, ACM T GRAPHIC, V25, P1108, DOI 10.1145/1141911.1142001; Sochen N, 1998, IEEE T IMAGE PROCESS, V7, P310, DOI 10.1109/83.661181; Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Torresani L, 2008, LECT NOTES COMPUT SC, V5303, P596, DOI 10.1007/978-3-540-88688-4_44; Trulls E., 2014, IEEE C COMP VIS PATT; Trulls E., 2013, IEEE C COMP VIS PATT; Vaxman A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778858; Vedaldi A, 2005, IEEE I CONF COMP VIS, P1474; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Wang ZH, 2011, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2011.6126294; Wesseling P, 2004, INTRO MULTIGRID METH; YANOWITZ SD, 1989, COMPUT VISION GRAPH, V46, P82, DOI 10.1016/S0734-189X(89)80017-9; Yezzi A, 1998, IEEE T IMAGE PROCESS, V7, P345, DOI 10.1109/83.661184	56	21	22	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2015	115	2					136	154		10.1007/s11263-015-0805-1	http://dx.doi.org/10.1007/s11263-015-0805-1			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CS7SL		Green Submitted			2022-12-18	WOS:000362285700004
J	Delong, A; Gorelick, L; Veksler, O; Boykov, Y				Delong, Andrew; Gorelick, Lena; Veksler, Olga; Boykov, Yuri			Minimizing Energies with Hierarchical Costs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Energy minimization; Hierarchical models; Graph cuts; Markov random fields (MRFs); Segmentation	CUTS	Computer vision is full of problems elegantly expressed in terms of energy minimization. We characterize a class of energies with hierarchical costs and propose a novel hierarchical fusion algorithm. Hierarchical costs are natural for modeling an array of difficult problems. For example, in semantic segmentation one could rule out unlikely object combinations via hierarchical context. In geometric model estimation, one could penalize the number of unique model families in a solution, not just the number of models-a kind of hierarchical MDL criterion. Hierarchical fusion uses the well-known alpha-expansion algorithm as a subroutine, and offers a much better approximation bound in important cases.	[Delong, Andrew; Gorelick, Lena; Veksler, Olga; Boykov, Yuri] Univ Western Ontario, Dept Comp Sci, London, ON N6A 5B7, Canada	Western University (University of Western Ontario)	Delong, A (corresponding author), Univ Western Ontario, Dept Comp Sci, London, ON N6A 5B7, Canada.	andrew.delong@gmail.com; lena.gorelick@gmail.com; olga@csd.uwo.ca; yuri@csd.uwo.ca	Boykov, Yuri/C-1718-2015; Veksler, Olga/B-6549-2015	Veksler, Olga/0000-0002-9664-6601	NSERC Discovery Grant [R3584A02]; Canadian Foundation for Innovation (CFI); Early Researcher Award (ERA) program	NSERC Discovery Grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canadian Foundation for Innovation (CFI)(Canada Foundation for Innovation); Early Researcher Award (ERA) program	We wish to thank the anonymous reviewers for careful reading and helpful comments. This work was supported by NSERC Discovery Grant R3584A02, the Canadian Foundation for Innovation (CFI), and the Early Researcher Award (ERA) program.	Aggarwal CC, 1997, OPER RES, V45, P226, DOI 10.1287/opre.45.2.226; Ahuja RK, 2002, DISCRETE APPL MATH, V123, P75, DOI 10.1016/S0166-218X(01)00338-9; Barinova O., 2010, IEEE C COMP VIS PATT; Bartal Y., 1998, ACM S THEOR COMP STO; Birchfield S., 1999, INT C COMP VIS ICCV; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P79, DOI 10.1007/0-387-28831-7_5; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Boykov Y.Y., 2001, INT C COMP VIS ICCV; Cunningham WH, 1999, LECT NOTES COMPUT SC, V1610, P114; Delong A., 2011, THESIS U W ONTARIO; Delong A., 2011, EN MIN METH COMP VIS; Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; FELZENSZWALB PF, 2010, PROC CVPR IEEE, P2241, DOI DOI 10.1109/CVPR.2010.5539906; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Givoni I. E., 2011, UNCERTAINTY ARTIFICI; GOLDBERG AV, 1988, J ACM, V35, P921, DOI 10.1145/48014.61051; Gorelick L., 2011, INT C COMP VIS ICCV; GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x; Guillaume L, 2005, ACM SIGGRAPH 2010 SI, V37, P975, DOI [DOI 10.1145/1778765.1778839, 10.1145/1778765.1778839]; Hartley R., 2003, MULTIPLE VIEW GEOMET; HOCHBAUM DS, 1982, MATH PROGRAM, V22, P148, DOI 10.1007/BF01581035; Isack H, 2012, INT J COMPUT VISION, V97, P123, DOI 10.1007/s11263-011-0474-7; Kantor E, 2009, J DISCRET ALGORITHMS, V7, P341, DOI 10.1016/j.jda.2008.11.006; Kleinberg J, 2002, J ACM, V49, P616, DOI 10.1145/585265.585268; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kumar M. P., 2009, P 25 C UNC ART INT, P313; Ladicky L., 2010, BRIT MACH VIS C BMVC; Ladicky L., 2010, EUR C COMP VIS ECCV; Lazic N., 2009, INT C COMP VIS ICCV; Lempitsky V, 2010, IEEE T PATTERN ANAL, V32, P1392, DOI 10.1109/TPAMI.2009.143; Li H., 2007, IEEE C COMP VIS PATT; Li S. Z., 1994, MARKOV RANDOM FIELD; Meyers C, 2007, LECT NOTES COMPUT SC, V3867, P24; Olsson C., 2009, INT C COMP VIS OCT 2; Pock T., 2008, EUR C COMP VIS ECCV; Pock T., 2009, IEEE C COMP VIS PATT; POTTS RB, 1952, P CAMB PHILOS SOC, V48, P106, DOI 10.1017/S0305004100027419; Rother C., 2007, IEEE C COMP VIS PATT; Rother C., 2005, IEEE C COMP VIS PATT; Sahin G, 2007, COMPUT OPER RES, V34, P2310, DOI [10.1016/j.cor.2005.09.005, 10.1016/j.cor.2006.09.005]; Sefer E., 2011, RES COMPUTATIONAL MO; Shmoys D. B., 1998, ACM S THEOR COMP STO, P265; Strandmark P., 2010, IEEE C COMP VIS PATT; Svitkina Z., 2006, ACM SIAM S DISCR ALG; Szeliski R, 2006, LECT NOTES COMPUT SC, V3952, P16; Torr P. H. S., 1994, EUR C COMP VIS ECCV; Torr PHS, 1998, PHILOS T R SOC A, V356, P1321, DOI 10.1098/rsta.1998.0224; VEKSLER O, 1999, THESIS CORNELL U; Werner T., 2008, IEEE C COMP VIS PATT; Woodford O. J., 2009, INT C COMP VIS ICCV; Yuan J., 2010, BRIT MACH VIS C BMVC; Zhou Q., 2011, INT J COMPUTER VISIO; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343	59	21	21	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2012	100	1					38	58		10.1007/s11263-012-0531-x	http://dx.doi.org/10.1007/s11263-012-0531-x			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	962SH					2022-12-18	WOS:000305564800003
J	Hahn, J; Tai, XC; Borok, S; Bruckstein, AM				Hahn, Jooyoung; Tai, Xue-Cheng; Borok, Sofia; Bruckstein, Alfred Marcel			Orientation-Matching Minimization for Image Denoising and Inpainting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Orientation-matching minimization; TV-Stokes equation; Image denoising; Image inpainting	DIRECTION DIFFUSION; EDGE-DETECTION; SPACE	In this paper, we propose an orientation-matching functional minimization for image denoising and image inpainting. Following the two-step TV-Stokes algorithm (Rahman et al. in Scale space and variational methods in computer vision, pp. 473-482, Springer, Heidelberg, 2007; Tai et al. in Image processing based on partial differential equations, pp. 3-22, Springer, Heidelberg, 2006; Bertalmio et al. in Proc. conf. comp. vision pattern rec., pp. 355-362, 2001), a regularized tangential vector field with zero divergence condition is first obtained. Then a novel approach to reconstruct the image is proposed. Instead of finding an image that fits the regularized normal direction from the first step, we propose to minimize an orientation matching cost measuring the alignment between the image gradient and the regularized normal direction. This functional yields a new nonlinear partial differential equation (PDE) for reconstructing denoised and inpainted images. The equation has an adaptive diffusivity depending on the orientation of the regularized normal vector field, providing reconstructed images which have sharp edges and smooth regions. The additive operator splitting (AOS) scheme is used for discretizing Euler-Lagrange equations. We present the results of various numerical experiments that illustrate the improvements obtained with the new functional.	[Hahn, Jooyoung; Tai, Xue-Cheng; Borok, Sofia; Bruckstein, Alfred Marcel] Nanyang Technol Univ, Div Math Sci, Sch Phys & Math Sci, Singapore, Singapore; [Tai, Xue-Cheng] Univ Bergen, Math Inst, Bergen, Norway; [Bruckstein, Alfred Marcel] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University of Bergen; Technion Israel Institute of Technology	Hahn, J (corresponding author), Nanyang Technol Univ, Div Math Sci, Sch Phys & Math Sci, Singapore, Singapore.	jyhahn@ntu.edu.sg; xctai@ntu.edu.sg; sofia.borok@gmail.com; freddy@cs.technion.ac.il	Hahn, Jooyoung/AGX-8714-2022; Hahn, Jooyoung/GPP-3243-2022; tai, xuecheng/L-9821-2013	tai, xuecheng/0000-0003-3359-9104; Bruckstein, Alfred/0000-0001-5669-0037; Hahn, Jooyoung/0000-0003-4357-1009	MOE (Ministry of Education) [T207N2202, NRF2007IDMIDM002-010]; SUG; School of Physical and Mathematical Sciences; Institute for Media Innovations	MOE (Ministry of Education); SUG; School of Physical and Mathematical Sciences; Institute for Media Innovations	The research is supported by MOE (Ministry of Education) Tier II project T207N2202 and IDM project NRF2007IDMIDM002-010. In addition, the support from SUG 20/07 is also gratefully acknowledged. Professor A. M. Bruckstein's work was supported in part by an NTU Joint visiting professorship at the School of Physical and Mathematical Sciences and the Institute for Media Innovations.	Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036; Bertalmio M, 2001, PROC CVPR IEEE, P355; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Bertalmio M, 2006, IEEE T IMAGE PROCESS, V15, P1934, DOI 10.1109/TIP.2006.877067; Bresson X, 2008, INVERSE PROBL IMAG, V2, P455, DOI 10.3934/ipi.2008.2.455; Brox T, 2006, IMAGE VISION COMPUT, V24, P41, DOI 10.1016/j.imavis.2005.09.010; CATTE F, 1992, SIAM J NUMER ANAL, V29, P182, DOI 10.1137/0729012; Chan RH, 2008, SIAM J IMAGING SCI, V1, P273, DOI 10.1137/070711499; Chan TF, 2005, COMMUN PUR APPL MATH, V58, P579, DOI 10.1002/cpa.20075; Chan TF, 2003, SIAM J APPL MATH, V63, P564; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487; Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844; CHESSEL A, 2006, ECCV06, V6, pR4; Dong FF, 2009, J MATH IMAGING VIS, V34, P89, DOI 10.1007/s10851-008-0132-z; Hahn J, 2009, J MATH IMAGING VIS, V34, P137, DOI 10.1007/s10851-009-0138-1; Kimmel R, 2003, INT J COMPUT VISION, V53, P225, DOI 10.1023/A:1023030907417; KORNPROBST P, 2006, 5905 INRIA; LINDENBAUM M, 1994, PATTERN RECOGN, V27, P1, DOI 10.1016/0031-3203(94)90013-2; Litvinov W.G., 2009, MODIFIED TV STOKES M, P2009; LU T, 1992, RAIRO-MATH MODEL NUM, V26, P673; Lysaker M, 2006, INT J COMPUT VISION, V66, P5, DOI 10.1007/s11263-005-3219-7; Lysaker M, 2004, IEEE T IMAGE PROCESS, V13, P1345, DOI 10.1109/TIP.2004.834662; Lysaker M, 2003, IEEE T IMAGE PROCESS, V12, P1579, DOI 10.1109/TIP.2003.819229; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Masnou S, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P259, DOI 10.1109/ICIP.1998.999016; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Perona P, 1998, IEEE T IMAGE PROCESS, V7, P457, DOI 10.1109/83.661195; Rahman T, 2007, LECT NOTES COMPUT SC, V4485, P473; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Sochen NA, 2004, SIAM J APPL MATH, V64, P1477, DOI 10.1137/S0036139902415518; Spira A, 2007, IEEE T IMAGE PROCESS, V16, P1628, DOI 10.1109/TIP.2007.894253; Steidl G, 2009, LECT NOTES COMPUT SC, V5567, P477, DOI 10.1007/978-3-642-02256-2_40; Tai X.C., 2006, IMAGE PROCESSING BAS, P3, DOI DOI 10.1007/978-3-540-33267-1_1; Tang B, 2000, INT J COMPUT VISION, V36, P149, DOI 10.1023/A:1008152115986; TERZOPOULOS D, 1988, IEEE T PATTERN ANAL, V10, P417, DOI 10.1109/34.3908; Tschumperle D, 2002, INT J COMPUT VISION, V50, P237, DOI 10.1023/A:1020870207168; Vese LA, 2003, SIAM J NUMER ANAL, V40, P2085; Weickert J, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P315, DOI 10.1007/3-540-31272-2_19; Weickert J, 1998, IEEE T IMAGE PROCESS, V7, P398, DOI 10.1109/83.661190; Weickert J, 1999, INT J COMPUT VISION, V31, P111, DOI 10.1023/A:1008009714131	41	21	24	3	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2011	92	3					308	324		10.1007/s11263-010-0371-5	http://dx.doi.org/10.1007/s11263-010-0371-5			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	729DS					2022-12-18	WOS:000287929400005
J	Vijayanarasimhan, S; Grauman, K				Vijayanarasimhan, Sudheendra; Grauman, Kristen			Cost-Sensitive Active Visual Category Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual category learning; Active learning; Multi-label; Multiple-instance learning; Cost prediction; Cost sensitive learning; Object recognition		We present an active learning framework that predicts the tradeoff between the effort and information gain associated with a candidate image annotation, thereby ranking unlabeled and partially labeled images according to their expected "net worth" to an object recognition system. We develop a multi-label multiple-instance approach that accommodates realistic images containing multiple objects and allows the category-learner to strategically choose what annotations it receives from a mixture of strong and weak labels. Since the annotation cost can vary depending on an image's complexity, we show how to improve the active selection by directly predicting the time required to segment an unlabeled image. Our approach accounts for the fact that the optimal use of manual effort may call for a combination of labels at multiple levels of granularity, as well as accurate prediction of manual effort. As a result, it is possible to learn more accurate category models with a lower total expenditure of annotation effort. Given a small initial pool of labeled data, the proposed method actively improves the category models with minimal manual intervention.	[Vijayanarasimhan, Sudheendra; Grauman, Kristen] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Vijayanarasimhan, S (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	svnaras@cs.utexas.edu; grauman@cs.utexas.edu			NSF [CAREER IIS-0747356, EIA-0303609]; Microsoft Research; DARPA VIRAT; Henry Luce Foundation	NSF(National Science Foundation (NSF)); Microsoft Research(Microsoft); DARPA VIRAT; Henry Luce Foundation	Many thanks to Alex Sorokin for helping us arrange the Mechanical Turk data collection. This research was supported in part by NSF CAREER IIS-0747356, Microsoft Research, DARPA VIRAT, NSF EIA-0303609, and the Henry Luce Foundation.	[Anonymous], 2008, CVPR; BACH FR, 2004, UCBCSD041307; Baldridge Jason, 2008, Natural Language Engineering, V14, P191, DOI 10.1017/S1351324906004396; BART E, 2005, CVPR05; Bunescu Razvan C., 2007, ICML; CAUWENBERGHS G, 2000, NIPS; Collins Brendan, 2008, ECCV; Dabbish L., 2004, CHI; Dietterich T. G., 1997, ARTIFICIAL INTELLIGE; FEIFEI L, 2003, ICCV; Fergus R., 2005, ICCV; Gartner T., 2002, ICML; Greiner R, 2002, ARTIF INTELL, V139, P137, DOI 10.1016/S0004-3702(02)00209-6; HAERTEL R, 2008, P WORKSH PARS GERM; Kapoor A., 2007, IJCAI; Kapoor A., 2007, ICCV, P2; KWOK JT, 2007, IJCAI; LANCKRIET G, 2004, J MACHINE LEARNING R, V5; LEE Y, 2008, BMVC; Li L.-J., 2007, CVPR; MARON O, 1998, ICML; Platt J., 1999, ADV LARGE MARGIN CLA; QI GJ, 2008, CVPR; Quelhas P., 2005, ICCV; Russell B., 2005, LABELME DATABASE WEB; Settles B., 2008, NIPS; Shotton J., 2006, ECCV; Sivic J., 2005, ICCV; Sorokin A., 2008, CVPR WORKSH; VERBEEK J, 2007, CVPR; VIJAYANARASIMHA.S, 2010, P IEEE C COMP VIS PA; Vijayanarasimhan Sudheendra, 2008, NIPS; Vijayanarasimhan Sudheendra, 2009, CVPR; WEBER M, 2000, ECCV; Winn J. M., 2005, ICCV; Wu TF, 2004, J MACH LEARN RES, V5, P975; Yan R., 2003, ICCV; YANG C, 2000, ICDE; ZHA ZJ, 2008, CVPR; Zhou Z.-H., 2006, NIPS	40	21	22	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2011	91	1					24	44		10.1007/s11263-010-0372-4	http://dx.doi.org/10.1007/s11263-010-0372-4			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	705HB					2022-12-18	WOS:000286118400002
J	Chantler, M; Petrou, M; Penirsche, A; Schmidt, M; McGunnigle, G				Chantler, M; Petrou, M; Penirsche, A; Schmidt, M; McGunnigle, G			Classifying surface texture while simultaneously estimating illumination direction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						texture classification; illumination estimation; surface texture		We propose a novel classifier that both classifies surface texture and simultaneously estimates the unknown illumination conditions. A new formal model of the dependency of texture features on lighting direction is developed which shows that their mean vectors are trigonometric functions of the illuminations' tilt and slant angles. This is used to develop a probabilistic description of feature behaviour which forms the basis of the new classifier. Given a feature set from an image of an unknown texture captured under unknown illumination conditions the algorithm first estimates the most likely illumination direction for each possible texture class. These estimates are used to calculate the class likelihoods and the classification is made accordingly. The ability of the classifier to estimate illuminant direction, and to assign the correct class, was tested on 55 real texture samples in two stages. The classifier was able to accurately estimate both the tilt and the slant angles of the light source for the majority of textures and gave a 98% classification rate.	Heriot Watt Univ, Sch Math & Comp Sci, Edinburgh EH14 4AS, Midlothian, Scotland; Univ Surrey, Dept Elect Engn, Surrey GU2 7XH, England	Heriot Watt University; University of Surrey	Chantler, M (corresponding author), Heriot Watt Univ, Sch Math & Comp Sci, Edinburgh EH14 4AS, Midlothian, Scotland.	ni.j.chantler@hw.ic.uk; M.Petrou@surrey.ac.A		Chantler, Mike/0000-0002-8381-1751	Engineering and Physical Sciences Research Council [GR/S12395/01] Funding Source: researchfish	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))		CHANTLER MJ, 1995, IEE P-VIS IMAGE SIGN, V142, P199, DOI 10.1049/ip-vis:19952065; CHANTLER MJ, 1995, P 5 INT C IM PROC IT, P767; CHANTLER MJ, 2002, ECCV 2002 EUR C COMP, V3, P289; CULA OG, 2001, P SPIE SAN JOS; Dana K. J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1061, DOI 10.1109/ICCV.1999.790389; Dana KJ, 1998, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.1998.698669; Dana KJ, 1997, PROC CVPR IEEE, P151, DOI 10.1109/CVPR.1997.609313; JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S; KUBE P, 1988, IEEE T PATTERN ANAL, V10, P704, DOI 10.1109/34.6779; Laws K, 1980, THESIS U SO CALIFORN; Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638; Leung T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1010, DOI 10.1109/ICCV.1999.790379; McGunnigle G, 2001, IEEE T IMAGE PROCESS, V10, P534, DOI 10.1109/83.913588; MCGUNNIGLE G, 1997, P BMVC 97 BRIT MACH, V2, P470; PENIRSCHKE A, 2002, RM024; Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261; Van Ginneken B, 1999, INT J COMPUT VISION, V31, P169, DOI 10.1023/A:1008018015948; Varma M, 2002, LECT NOTES COMPUT SC, V2352, P255; VARMA M, 2002, TEXT 2002 2 INT WORK, P139	19	21	30	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-MAY	2005	62	1-2					83	96		10.1023/B:VISI.0000046590.98379.19	http://dx.doi.org/10.1023/B:VISI.0000046590.98379.19			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	866XV					2022-12-18	WOS:000224807600006
J	Takao, N; Shi, JB; Baker, S				Takao, N; Shi, JB; Baker, S			Tele-Graffiti: A camera-projector based remote sketching system with hand-based user interface and automatic session summarization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						camera-projector based remote sketching systems; remote communication and collaboration; video compression and transmission; paper detection and tracking; hand-based user interfaces; automatic summarization; archiving; playback	COMPUTER	One way to build a remote sketching system is to use a video camera to image what each user draws at their site, transmit the video to the other sites, and display it there using an LCD projector. Such camera-projector based remote sketching systems date back to Paul Wellner's ( largely unimplemented) Xerox Double DigitalDesk. To make such a system usable, however, the users have to be able to move the paper on which they are drawing, they have to be able to interact with the system using a convenient interface, and sketching sessions must be stored in a compact format so that they can be replayed later. We have recently developed Tele-Graffiti, a remote sketching system with the following three features: ( 1) real-time paper tracking to allow the users to move their paper during system operation, ( 2) a hand based user interface, and ( 3) automatic session summarization and playback. In this paper, we describe the design, implementation, and performance of Tele-Graffiti.	Matsushita Elect Ind Co Ltd, Digital Network Dev Ctr, Kadoma, Osaka 5718501, Japan; Univ Penn, GRASP Lab, Philadelphia, PA 19104 USA; Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Panasonic; University of Pennsylvania; Carnegie Mellon University	Takao, N (corresponding author), Matsushita Elect Ind Co Ltd, Digital Network Dev Ctr, 1006 Kadoma, Kadoma, Osaka 5718501, Japan.							AGRAWALA M, 1997, P SIGGRAPH 97; CHIPOLLA R, 1998, COMPUTER VISION HUMA; CROWLEY J, 1995, P ENG HUM COMP INT; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Freeman WT, 1998, IEEE COMPUT GRAPH, V18, P42, DOI 10.1109/38.674971; HALL D, 1999, P 7 INT S INT ROB SY; Horn BKP, 1996, ROBOT VISION; KRUEGER W, 1995, IEEE COMPUT, V28, P42; LEIBE B, 2000, P IEEE VIRT REAL 200; Lien CC, 1998, IMAGE VISION COMPUT, V16, P121, DOI 10.1016/S0262-8856(97)00041-3; MYNATT ED, 1999, P ACM C HUM FACT COM; OKA K, 2002, P 5 INT C AUT FAC GE; Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226; QUEK F, 1995, P INT WORKSH AUT FAC; RASKAR R, 1998, P SIGGRAPH 98; REKIMOTO J., 1999, P ACM S US INT SOFTW; SATO Y, 2000, P 4 INT C AUT FAC GE; SUKTHANKAR R, 2001, P IEEE C COMP VIS PA; TAKAO N, 2002, CMURITR0210; TAKAO N, 2001, P 8 INT C COMP VIS D; *TEGR INC, 2002, HOM PAG TEGR INC; VONHARDENBURGH C, 2001, P ACM WORKSH PERC US; Wellner P., 1993, Communications of the ACM, V36, P86; *WOLFV, 2002, WOLFV VIS	24	21	27	0	5	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2003	53	2					115	133		10.1023/A:1023084706295	http://dx.doi.org/10.1023/A:1023084706295			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	674WZ					2022-12-18	WOS:000182661300001
J	Jalobeanu, A; Blanc-Feraud, L; Zerubia, J				Jalobeanu, A; Blanc-Feraud, L; Zerubia, J			Satellite image deblurring using complex wavelet packets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						deblurring; Bayesian estimation; complex wavelet packets; satellite and aerial images		The deconvolution of blurred and noisy satellite images is an ill-posed inverse problem. Direct inversion leads to unacceptable noise amplification. Usually the problem is regularized during the inversion process. Recently, new approaches have been proposed, in which a rough deconvolution is followed by noise filtering in the wavelet transform domain. Herein, we have developed this second solution, by thresholding the coefficients of a new complex wavelet packet transform; all the parameters are automatically estimated. The use of complex wavelet packets enables translational invariance and improves directional selectivity, while remaining of complexity O(N). A new hybrid thresholding technique leads to high quality results, which exhibit both correctly restored textures and a high SNR in homogeneous areas. Compared to previous algorithms, the proposed method is faster, rotationally invariant and better takes into account the directions of the details and textures of the image, improving restoration. The images deconvolved in this way can be used as they are (the restoration step proposed here can be inserted directly in the acquisition chain), and they can also provide a starting point for an adaptive regularization method, enabling one to obtain sharper edges.	NASA, Ames Res Ctr, USRA RIACS, Moffett Field, CA 94035 USA; INRIA, CNRS INRIA UNSA, Project Ariana, F-06902 Sophia Antipolis, France	National Aeronautics & Space Administration (NASA); NASA Ames Research Center; Inria	Jalobeanu, A (corresponding author), NASA, Ames Res Ctr, USRA RIACS, MS 269-4, Moffett Field, CA 94035 USA.							[Anonymous], 1998, P EUSIPCO, DOI DOI 10.5281/ZENODO.36900; BELGE M, 1998, IEEE P ICIP CHIC US; Bernardo J. M., 1994, BAYESIAN THEORY; CHANG SG, 1997, P ICIP; Charbonnier P, 1997, IEEE T IMAGE PROCESS, V6, P298, DOI 10.1109/83.551699; Coifman R. R., 1992, WAVELETS THEIR APPL, P153; Coifman R.R., 1995, 475 STANF U; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; DONOHO DL, 1995, APPL COMPUT HARMON A, V2, P101, DOI 10.1006/acha.1995.1008; FIGUEIREDO M, 1999, P SPIE C MATH MOD BA; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; HELSTROM CW, 1967, J OPT SOC AM; JALOBEANU A, 2000, P ICIP VANC; JALOBEANU A, 2000, 3955 INRIA; JALOBEANU A, 1999, LNCS EMMCVPR; JALOBEANU A, 2000, P ICPR BARC SPAIN; KALIFA J, 1998, WAVELET PACKET DECON; KALIFA J, 1999, THESIS ECOLE POLYTEC; Kingsbury N, 2001, APPL COMPUT HARMON A, V10, P234, DOI 10.1006/acha.2000.0343; Kingsbury N, 1999, PHILOS T R SOC A, V357, P2543, DOI 10.1098/rsta.1999.0447; Malfait M, 1997, IEEE T IMAGE PROCESS, V6, P549, DOI 10.1109/83.563320; MALLAT S, 1989, IEEE T PATTERN ANAL, V11, P7; Moulin P., 1993, Journal of Mathematical Imaging and Vision, V3, P123, DOI 10.1007/BF01248407; ROUGE B, 1995, IEEE 95 PHIL US; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Simoncelli E.P., 1999, BAYESIAN INFERENCE W; Tikhonov AN, 1963, SOV MATH DOKL, V4, P1624	28	21	33	0	6	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	2003	51	3					205	217		10.1023/A:1021801918603	http://dx.doi.org/10.1023/A:1021801918603			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	633TA					2022-12-18	WOS:000180297500003
J	McCane, B; Galvin, B; Novins, K				McCane, B; Galvin, B; Novins, K			Algorithmic fusion for more robust feature tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						feature tracking; motion analysis; combining multiple trackers; algorithmic fusion		We present a framework for merging the results of independent feature-based motion trackers using a classification based approach. We demonstrate the efficacy of the framework using corner trackers as an example. The major problem with such systems is generating ground truth data for training. We show how synthetic data can be used effectively to overcome this problem. Our combined system performs better in both dropouts and errors than a correspondence tracker, and had less than half the dropouts at the cost of moderate increase in error compared to a relaxation tracker.	Univ Otago, Dept Comp Sci, Dunedin, New Zealand	University of Otago	McCane, B (corresponding author), Univ Otago, Dept Comp Sci, Dunedin, New Zealand.	mccane@cs.otago.ac.nz		McCane, Brendan/0000-0001-8055-3331				Cox IJ, 1996, IEEE T PATTERN ANAL, V18, P138, DOI 10.1109/34.481539; COX IJ, 1993, INT J COMPUT VISION, V10, P53, DOI 10.1007/BF01440847; Foggia P, 1999, PATTERN RECOGN, V32, P1435, DOI 10.1016/S0031-3203(98)00169-1; GALVIN B, 1999, P 2 INT C 3 D DIG IM; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; HO TK, 1994, IEEE T PATTERN ANAL, V16, P66, DOI 10.1109/34.273716; Kang SB, 1997, COMPUT VIS IMAGE UND, V67, P296, DOI 10.1006/cviu.1996.0519; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Mazor E, 1998, IEEE T AERO ELEC SYS, V34, P103, DOI 10.1109/7.640267; McCane B, 2001, COMPUT VIS IMAGE UND, V84, P126, DOI 10.1006/cviu.2001.0930; Murphy RR, 1996, IEEE T SYST MAN CY A, V26, P42, DOI 10.1109/3468.477859; OTTE M, 1995, ARTIF INTELL, V78, P5, DOI 10.1016/0004-3702(95)00033-X; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; SMITH SM, 1995, IEEE T PATTERN ANAL, V17, P814, DOI 10.1109/34.400573; Tomasi C, 1991, CMUCS91132; XU L, 1992, IEEE T SYST MAN CYB, V22, P418, DOI 10.1109/21.155943	17	21	21	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2002	49	1					79	89		10.1023/A:1019833915960	http://dx.doi.org/10.1023/A:1019833915960			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	585UJ					2022-12-18	WOS:000177542700005
J	MAYBANK, SJ				MAYBANK, SJ			PROBABILISTIC ANALYSIS OF THE APPLICATION OF THE CROSS RATIO TO MODEL-BASED VISION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								The probability density function for the cross ratio is obtained under the hypothesis that the four image points have independent, identical, Gaussian distributions. The density function has six symmetries which are closely linked to the six different values of the cross ratio obtained by permuting the quadruple of points from which the cross ratio is calculated. The density function has logarithmic singularities corresponding to values of the cross ratio for which two of the four points are coincident. The cross ratio forms the basis of a simple system for recognising or classifying quadruples of collinear image points. The performance of the system depends on the choice of rule for deciding whether four image points have a given cross ratio sigma. A rule is stated which is computationally straightforward and which takes into account the effects on the cross ratio of small errors in locating the image points. Two key properties of the rule are the probability R of rejection, and the probability F of a false alarm. The probabilities R and F depend on a threshold t in the decision rule, There is a trade off between R and F obtained by varying t. It is shown that the trade off is insensitive to the given cross ratio sigma. Let F-w = max(sigma){F}. Then R, F-w are related approximately by root ln(R(-1)) = (root 2 epsilon r(F))F--1(w), provided epsilon-F-1(w) greater than or equal to 4. In the equation, epsilon is the accuracy with which image points can be located relative to the width of the image, and r(F) is a constant known as the normalised false alarm rate. In the range epsilon(-1)F(w) less than or equal to 4 the probabilities R and F-w are related approximately by R = 1 - root 2 pi(-1)epsilon(-1)r(F)(-1)F(w). The value of r(F) is 14.37. The consequences of these relations between R and F-w are discussed. It is conjectured that the above general form of the trade off between R and F-w holds for a wide class of scalar invariants that could be used for model based object recognition. Invariants with the same type of trade off between the probability of rejection and the probability of false alarm are said to be nondegenerate for model based vision.			MAYBANK, SJ (corresponding author), GEC LTD, MARCONI HIRST RES CTR, ELSTREE LANE, BOREHAMWOOD WD6 1RX, HERTS, ENGLAND.							Abramowitz M, 1965, HDB MATH FUNCTIONS F; ASTROM K, 1992, RT88IMAG14LIFIA RAPP; Cramer H., 1945, PRINCETON MATH SERIE, V9; Devijver PA, 1982, PATTERN RECOGNITION; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; GRADSHTEYN IS, 1983, TABLES INTEGRALS SER; Helstrom C., 1960, INT SERIES MONOGRAPH; Kanungo T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P247, DOI 10.1109/ICCV.1993.378211; MAYBANK SJ, 1995, INT J COMPUT VISION, V16, P5, DOI 10.1007/BF01428191; MAYBANK SJ, 1994, J APPLIED STATISTICS, V21, P439; MAYBANK SJ, 1994, LECTURE NOTES COMPUT, V825, P69; Mundy J., 1992, GEOMETRIC INVARIANCE; Semple J., 1979, ALGEBRAIC PROJECTIVE; WHITTAKER E, 1952, MODERN ANAL; Wolfram S., 1991, MATH SYSTEM DOING MA; [No title captured]	16	21	24	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1995	16	1					5	33		10.1007/BF01428191	http://dx.doi.org/10.1007/BF01428191			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RW852					2022-12-18	WOS:A1995RW85200001
J	Zheng, X; Guo, YQ; Huang, HB; Li, Y; He, R				Zheng, Xin; Guo, Yanqing; Huang, Huaibo; Li, Yi; He, Ran			A Survey of Deep Facial Attribute Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep neural networks; Deep facial attribute analysis; Facial attribute estimation; Facial attribute manipulation		Facial attribute analysis has received considerable attention when deep learning techniques made remarkable breakthroughs in this field over the past few years. Deep learning based facial attribute analysis consists of two basic sub-issues: facial attribute estimation (FAE), which recognizes whether facial attributes are present in given images, and facial attribute manipulation (FAM), which synthesizes or removes desired facial attributes. In this paper, we provide a comprehensive survey of deep facial attribute analysis from the perspectives of both estimation and manipulation. First, we summarize a general pipeline that deep facial attribute analysis follows, which comprises two stages: data preprocessing and model construction. Additionally, we introduce the underlying theories of this two-stage pipeline for both FAE and FAM. Second, the datasets and performance metrics commonly used in facial attribute analysis are presented. Third, we create a taxonomy of state-of-the-art methods and review deep FAE and FAM algorithms in detail. Furthermore, several additional facial attribute related issues are introduced, as well as relevant real-world applications. Finally, we discuss possible challenges and promising future research directions.	[Zheng, Xin; Guo, Yanqing] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China; [Huang, Huaibo; Li, Yi; He, Ran] CASIA, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Huang, Huaibo; Li, Yi; He, Ran] CASIA, Ctr Res Intelligent Percept & Comp, Beijing 100190, Peoples R China; [Huang, Huaibo; Li, Yi; He, Ran] Univ Chinese Acad Sci, Beijing 100190, Peoples R China; [He, Ran] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100190, Peoples R China	Dalian University of Technology; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences	Guo, YQ (corresponding author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.	zhengxin@mail.dlut.edu.cn; guoyq@dlut.edu.cn; huaibo.huang@cripac.ia.ac.cn; yi.li@cripac.ia.ac.cn; rhe@nlpr.ia.ac.cn			State Key Development Program [2016YFB1001001]; National Natural Science Foundation of China (NSFC) [U1736119]; Fundamental Research Funds for the Central Universities [DUT18JC06]	State Key Development Program; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	We thank the contributions of pioneer researchers in the field of deep learning based facial attribute analysis and other related fields. This work is supported in part by the State Key Development Program (Grant No. 2016YFB1001001), in part by the National Natural Science Foundation of China (NSFC) under Grant U1736119, and in part by the Fundamental Research Funds for the Central Universities under Grant DUT18JC06.	[Anonymous], P C ART INT AAAI; Belghazi M.I., 2018, INT C LEARN REPR, V1071; Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128; Berthelot David, 2019, INT C LEARN REPR; Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Cao C, 2019, IEEE T MULTIMEDIA, V21, P2750, DOI 10.1109/TMM.2019.2911457; Cao JJ, 2018, PROC CVPR IEEE, P4290, DOI 10.1109/CVPR.2018.00451; Cao J, 2019, IEEE T INF FOREN SEC, V14, P2028, DOI 10.1109/TIFS.2019.2891116; Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012; Chang XX, 2018, ADVANCED FUNCTIONAL MATERIALS (CMC 2017), P293, DOI 10.1007/978-981-13-0110-0_33; Chen JC, 2018, INT J COMPUT VISION, V126, P272, DOI 10.1007/s11263-017-1029-3; Chen L, 2014, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2014.135; Chen YC, 2019, PROC CVPR IEEE, P9851, DOI 10.1109/CVPR.2019.01009; Chhabra S, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P656; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dong Q, 2017, IEEE I CONF COMP VIS, P1869, DOI 10.1109/ICCV.2017.205; Dorta Garoe, 2018, ARXIV181112784; Duong CN, 2019, PROC CVPR IEEE, P10005, DOI 10.1109/CVPR.2019.01025; Egger B, 2018, INT J COMPUT VISION, V126, P1269, DOI 10.1007/s11263-018-1064-8; Fan QF, 2013, IEEE I CONF COMP VIS, P2736, DOI 10.1109/ICCV.2013.340; Fang YC, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0282-x; Fathy ME, 2015, INT CONF ACOUST SPEE, P1687, DOI 10.1109/ICASSP.2015.7178258; Fukui H, 2019, PROC CVPR IEEE, P10697, DOI 10.1109/CVPR.2019.01096; Gkioxari G, 2015, IEEE I CONF COMP VIS, P2470, DOI 10.1109/ICCV.2015.284; Gonzalez-Garcia A, 2018, INT J COMPUT VISION, V126, P476, DOI 10.1007/s11263-017-1048-0; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gunther M, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P90; Gunther M., 2013, BIOM ICB 2013 INT C, P1; Guo HX, 2017, EXPERT SYST APPL, V73, P220, DOI 10.1016/j.eswa.2016.12.035; Hadid A, 2007, 2007 FIRST ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS, P96; Han H, 2018, IEEE T PATTERN ANAL, V40, P2597, DOI 10.1109/TPAMI.2017.2738004; Hand EM, 2018, AAAI CONF ARTIF INTE, P6878; Hand EM, 2018, IEEE WINT CONF APPL, P84, DOI 10.1109/WACV.2018.00017; Hand EM, 2017, AAAI CONF ARTIF INTE, P4068; He Di, 2016, NEURAL INFORM PROCES, P2; He K., 2018, P INT JOINT C ARTIFI; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2017, ARXIV170708705; He R, 2019, IEEE T PATTERN ANAL, V41, P1761, DOI 10.1109/TPAMI.2018.2842770; He R, 2018, PATTERN RECOGN, V75, P4, DOI 10.1016/j.patcog.2017.02.005; He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hu Yibo, 2018, P IEEE C COMP VIS PA; Huang C, 2020, IEEE T PATTERN ANAL, V42, P2781, DOI 10.1109/TPAMI.2019.2914680; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Huang G.B., 2008, WORKSH FAC REAL LIF; Huang H., 2018, ARXIV180704099; Kalayeh MM, 2017, PROC CVPR IEEE, P4227, DOI 10.1109/CVPR.2017.450; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koepke A. Sophia, 2018, P BRIT MACH VIS C, P302; Kumar N, 2008, LECT NOTES COMPUT SC, V5305, P340, DOI 10.1007/978-3-540-88693-8_25; Kumar Neeraj, 2011, IEEE Trans Pattern Anal Mach Intell, V33, P1962, DOI 10.1109/TPAMI.2011.48; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lample Guillaume, 2017, ARXIV170600409; Larsen ABL, 2016, PR MACH LEARN RES, V48; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Li HY, 2018, J COMPUT SCI TECH-CH, V33, P511, DOI 10.1007/s11390-018-1835-2; Li JS, 2018, IEEE T IMAGE PROCESS, V27, P4651, DOI 10.1109/TIP.2018.2839521; Li M., 2016, ARXIV161005586 CORR; Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618; Li Y, 2015, IEEE I CONF COMP VIS, P3819, DOI 10.1109/ICCV.2015.435; Liu Alexander H, 2018, NIPS; Liu J, 2018, MECHANICS AND MATERIALS SCIENCE, P52; LIU MY, 2017, P ASM 36 INT C OC; Liu Y, 2019, IEEE C EVOL COMPUTAT, P2771, DOI 10.1109/CEC.2019.8790057; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126; Lu ZH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1083, DOI 10.1145/3240508.3240647; Luo P, 2013, IEEE I CONF COMP VIS, P2864, DOI 10.1109/ICCV.2013.356; Ma L., 2018, P INT C LEARN REPR I; Mahbub U, 2020, IEEE T AFFECT COMPUT, V11, P601, DOI 10.1109/TAFFC.2018.2820048; Meng Z., 2018, PROC EUR C COMPUT VI, P552; Miller T. L., 2007, NAMES FACES; Mirza M., 2014, ARXIV; Nguyen HM, 2018, LECT NOTES ARTIF INT, V10752, P539, DOI 10.1007/978-3-319-75420-8_51; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Parkhi Omkar M., 2015, BRIT MACH VIS C; Perarnau G, 2016, ARXIV161106355; Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233; Ranjan R, 2017, IEEE INT CONF AUTOMA, P17, DOI 10.1109/FG.2017.137; Rao YM, 2019, INT J COMPUT VISION, V127, P701, DOI 10.1007/s11263-018-1135-x; Rozsa A, 2019, PATTERN RECOGN LETT, V124, P100, DOI 10.1016/j.patrec.2017.10.024; Rozsa A, 2016, INT C PATT RECOG, P3121, DOI 10.1109/ICPR.2016.7900114; Rudd EM, 2016, IEEE COMPUT SOC CONF, P195, DOI 10.1109/CVPRW.2016.31; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308; Samangouei P, 2017, IMAGE VISION COMPUT, V58, P181, DOI 10.1016/j.imavis.2016.05.004; Sandeep RN, 2014, PROC CVPR IEEE, P3614, DOI 10.1109/CVPR.2014.462; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sethi A, 2019, PATTERN RECOGN LETT, V119, P157, DOI 10.1016/j.patrec.2018.03.010; Shen W, 2017, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2017.135; Shi HQ, 2020, IEEE T SYST MAN CY-S, V50, P2113, DOI 10.1109/TSMC.2018.2800092; Shi ZY, 2015, PROC CVPR IEEE, P4184, DOI 10.1109/CVPR.2015.7299046; Smith BM, 2013, PROC CVPR IEEE, P3484, DOI 10.1109/CVPR.2013.447; Song FY, 2014, COMPUT VIS IMAGE UND, V122, P143, DOI 10.1016/j.cviu.2014.02.010; Song L., 2018, P C ART INT AAAI; Song L., 2019, P C ART INT AAAI; Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612; Sun R., 2018, ARXIV180408882; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Suo JL, 2010, IEEE T PATTERN ANAL, V32, P385, DOI 10.1109/TPAMI.2009.39; Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1016/J.PATREC.2014.01.008; Szegedy Christian, 2014, P 2 INT C LEARNING R; Taherkhani F, 2018, IEEE COMPUT SOC CONF, P666, DOI 10.1109/CVPRW.2018.00097; Toderici G, 2010, INT J COMPUT VISION, V89, P382, DOI 10.1007/s11263-009-0300-7; Trokielewicz M, 2019, IEEE T INF FOREN SEC, V14, P1501, DOI 10.1109/TIFS.2018.2881671; Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030; Wang J, 2016, PROC CVPR IEEE, P2295, DOI 10.1109/CVPR.2016.252; Wang YL, 2018, IEEE WINT CONF APPL, P112, DOI 10.1109/WACV.2018.00019; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wolf L., 2011, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2011.5995566; Wu Y, 2019, INT J COMPUT VISION, V127, P115, DOI 10.1007/s11263-018-1097-z; Xiao FY, 2015, IEEE I CONF COMP VIS, P1458, DOI 10.1109/ICCV.2015.171; Xiao T., 2017, P INT C LEARN REPR W; Xiao TH, 2018, LECT NOTES COMPUT SC, V11214, P172, DOI 10.1007/978-3-030-01249-6_11; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271; Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26; Zhang J., 2017, P AS C MACH LEARN AC, P248; Zhang JC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P392, DOI 10.1145/3240508.3240594; Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212; Zhang S, 2018, IEEE T INF FOREN SEC, V13, P637, DOI 10.1109/TIFS.2017.2763119; Zhang SC, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P753, DOI 10.1145/2911451.2914732; Zhang Y, 2018, ARXIV180507277; Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463; Zhong Y, 2016, INT C PATT RECOG, P2264, DOI 10.1109/ICPR.2016.7899973; Zhong Y, 2016, IEEE IMAGE PROC, P3239, DOI 10.1109/ICIP.2016.7532958; Zhou Shuchang, 2017, ARXIV170504932; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhuang N, 2018, INT C PATT RECOG, P2069, DOI 10.1109/ICPR.2018.8545271; Zhuang N, 2018, PATTERN RECOGN, V80, P225, DOI 10.1016/j.patcog.2018.03.018	138	20	21	3	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2002	2034		10.1007/s11263-020-01308-z	http://dx.doi.org/10.1007/s11263-020-01308-z		MAR 2020	33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG		Green Submitted			2022-12-18	WOS:000521700000001
J	Shen, YM; Liu, L; Shao, L				Shen, Yuming; Liu, Li; Shao, Ling			Unsupervised Binary Representation Learning with Deep Variational Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hashing; Unsupervised learning; Deep learning; Image retrieval		Learning to hash is regarded as an efficient approach for image retrieval and many other big-data applications. Recently, deep learning frameworks are adopted for image hashing, suggesting an alternative way to formulate the encoding function other than the conventional projections. Although deep learning has been proved to be successful in supervised hashing, existing unsupervised deep hashing techniques still cannot produce leading performance compared with the non-deep methods, as it is hard to unveil the intrinsic structure of the whole sample space by simply regularizing the output codes within each single training batch. To tackle this problem, in this paper, we propose a novel unsupervised deep hashing model, named deep variational binaries (DVB). The conditional auto-encoding variational Bayesian networks are introduced in this work to exploit the feature space structure of the training data using the latent variables. Integrating the probabilistic inference process with hashing objectives, the proposed DVB model estimates the statistics of data representations, and thus produces compact binary codes. Experimental results on three benchmark datasets, i.e., CIFAR-10, SUN-397 and NUS-WIDE, demonstrate that DVB outperforms state-of-the-art unsupervised hashing methods with significant margins.	[Shen, Yuming; Liu, Li; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates		Shao, L (corresponding author), Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.	ymcidence@gmail.com; liuli1213@gmail.com; ling.shao@ieee.org		Shen, Yuming/0000-0003-4017-9140; Shao, Ling/0000-0002-8264-6117				Abadi M, 2015, P 12 USENIX S OPERAT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Belkin M, 2002, ADV NEUR IN, V14, P585; Cao Y., 2016, AAAI C ART INT AAAI; Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134; Carreira-Perpinan MA, 2015, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2015.7298654; Chaidaroon S., 2017, ACM C RES DEV INF RE; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Dai B., 2017, INT C MACH LEARN ICM; Eslami SM, 2016, NEURIPS, V1; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P1344, DOI 10.1109/TIP.2017.2652730; He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378; He X., 2003, ADV NEURAL INFORM PR, V16, P186; Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024; Hu G., 2017, IEEE INT C COMP VIS; Jiang Qing-Yuan, 2017, IEEE C COMP VIS PATT; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kong W., 2012, ADV NEURAL INFORM PR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Kulkarni TD, 2015, ADV NEUR IN, V28; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133; Liong VE, 2017, IEEE I CONF COMP VIS, P4097, DOI 10.1109/ICCV.2017.439; Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862; Liu L, 2017, INT J COMPUT VISION, V122, P439, DOI 10.1007/s11263-016-0931-4; Liu L, 2017, IEEE T IMAGE PROCESS, V26, P1289, DOI 10.1109/TIP.2017.2651390; Liu L, 2017, IEEE T IMAGE PROCESS, V26, P107, DOI 10.1109/TIP.2016.2619262; Liu L, 2016, IEEE T CYBERNETICS, V46, P2548, DOI 10.1109/TCYB.2015.2480966; Liu L, 2016, IEEE T NEUR NET LEAR, V27, P2526, DOI 10.1109/TNNLS.2015.2495345; Liu W., 2014, ADV NEURAL INFORM PR, V4, P3419; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Purushotham S., 2017, P ICLR, P1; Qin J, 2017, PROC CVPR IEEE, P6728, DOI 10.1109/CVPR.2017.712; Raginsky M., 2009, ADV NEURAL INFORM PR, P1509, DOI [10.5555/2984093.2984263, DOI 10.5555/2984093.2984263]; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Serban I. V., 2017, AAAI C ART INT AAAI; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Shen Y., 2017, BRIT MACH VIS C BMVC; Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210; Song J., 2013, ACM INT C MAN DAT SI; Do TT, 2016, LECT NOTES COMPUT SC, V9909, P219, DOI 10.1007/978-3-319-46454-1_14; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48; Weiss Y., 2009, NIPS; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Yu MY, 2016, IEEE T PATTERN ANAL, V38, P1651, DOI 10.1109/TPAMI.2015.2491925; Zhu H., 2016, AAAI C ART INT AAAI; Zhu XF, 2014, IEEE T IMAGE PROCESS, V23, P3737, DOI 10.1109/TIP.2014.2332764	62	20	20	5	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1614	1628		10.1007/s11263-019-01166-4	http://dx.doi.org/10.1007/s11263-019-01166-4			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY					2022-12-18	WOS:000492425300002
J	Dai, H; Pears, N; Smith, W; Duncan, C				Dai, Hang; Pears, Nick; Smith, William; Duncan, Christian			Statistical Modeling of Craniofacial Shape and Texture	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D morphable model; Statistical shape model; Craniofacial shape; Shape morphing	3D; REGISTRATION	We present a fully-automatic statistical 3D shape modeling approach and apply it to a large dataset of 3D images, the Headspace dataset, thus generating the first public shape-and-texture 3D morphable model (3DMM) of the full human head. Our approach is the first to employ a template that adapts to the dataset subject before dense morphing. This is fully automatic and achieved using 2D facial landmarking, projection to 3D shape, and mesh editing. In dense template morphing, we improve on the well-known Coherent Point Drift algorithm, by incorporating iterative data-sampling and alignment. Our evaluations demonstrate that our method has better performance in correspondence accuracy and modeling ability when compared with other competing algorithms. We propose a texture map refinement scheme to build high quality texture maps and texture model. We present several applications that include the first clinical use of craniofacial 3DMMs in the assessment of different types of surgical intervention applied to a craniosynostosis patient group.	[Dai, Hang; Pears, Nick; Smith, William] Univ York, Dept Comp Sci, York, N Yorkshire, England; [Dai, Hang] Incept Inst AI, Abu Dhabi, U Arab Emirates; [Duncan, Christian] Alder Hey Hosp, Liverpool, Merseyside, England	University of York - UK; Alder Hey Children's NHS Foundation Trust; Alder Hey Children's Hospital	Pears, N (corresponding author), Univ York, Dept Comp Sci, York, N Yorkshire, England.	hang.dai.cs@gmail.com; nick.pears@york.ac.uk; william.smith@york.ac.uk; Christian.Duncan@alderhey.nhs.uk	Smith, William/AAK-9101-2020	Smith, William/0000-0002-6047-0413; Pears, Nicholas Edwin/0000-0001-9513-5634	Google; Royal Academy of Engineering; Leverhulme Trust; QIDIS from the National Commissioning Group	Google(Google Incorporated); Royal Academy of Engineering(Royal Academy of Engineering - UK); Leverhulme Trust(Leverhulme Trust); QIDIS from the National Commissioning Group	We thank Google Faculty Awards and our Google sponsor, Forrester Cole, for supporting this research in 2017-2018. We thank the Royal Academy of Engineering and the Leverhulme Trust for priming this work in 2013-2014, via their Senior Research Fellowship awards. Headspace data collection was supported by QIDIS from the National Commissioning Group. We thank Rachel Armstrong, Headspace data collection coordinator.	ALBRECHT T, 2008, 2 MICCAI WORKSH MATH, P160; Amberg B, 2007, IEEE I CONF COMP VIS, P1326; An ZF, 2018, IEEE INT CONF AUTOMA, P416, DOI 10.1109/FG.2018.00067; BASSO C, 2007, J VIRTUAL REALITY BR, V4, P1; BEELER T, 2014, ACM T GRAPHIC, V33; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Bolkart T, 2015, IEEE I CONF COMP VIS, P3604, DOI 10.1109/ICCV.2015.411; Bolkart T, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P103, DOI 10.1109/3DV.2013.22; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7; Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598; Brunton A., 2011, 2011 Canadian Conference on Computer and Robot Vision (CRV), P347, DOI 10.1109/CRV.2011.53; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; COOTES TF, 1995, IMAGE VISION COMPUT, V13, P403, DOI 10.1016/0262-8856(95)99727-I; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Creusot C, 2013, INT J COMPUT VISION, V102, P146, DOI 10.1007/s11263-012-0605-9; Dai H, 2018, IEEE INT CONF AUTOMA, P91, DOI 10.1109/FG.2018.00023; Dai H, 2017, COMM COM INF SC, V723, P731, DOI 10.1007/978-3-319-60964-5_64; Dai H, 2017, IEEE I CONF COMP VIS, P3104, DOI 10.1109/ICCV.2017.335; DESMET M, 2010, P AS C COMP VIS, P276; Dryden I.L., 1998, STAT SHAPE ANAL; DUNCAN C, 2018, HEADSPACE DATASET; Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493; GERIG T, 2017, ARXIV17090839 CORR; Golovinskiy A, 2006, ACM T GRAPHIC, V25, P1025, DOI 10.1145/1141911.1141988; HARRISON CR, 2006, TECHNICAL REPORT; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Luthi M, 2018, IEEE T PATTERN ANAL, V40, P1860, DOI 10.1109/TPAMI.2017.2739743; Madsen D, 2018, PROC CVPR IEEE, P5295, DOI 10.1109/CVPR.2018.00555; Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Petr M, 2015, PROCEEDINGS SCCG: 2015 31ST SPRING CONFERENCE ON COMPUTER GRAPHICS, P151, DOI 10.1145/2788539.2788561; Salazar A, 2014, MACH VISION APPL, V25, P859, DOI 10.1007/s00138-013-0579-9; Saragih Jason M, 2011, Proc Int Conf Autom Face Gesture Recognit, P117, DOI 10.1109/FG.2011.5771400; Sorkine O., 2007, P 5 EUR S GEOM PROC, V4, P109, DOI [DOI 10.2312/SGP/SGP07/109-116, 10.2312/SGP/SGP07/109-116]; Styner MA, 2003, LECT NOTES COMPUT SC, V2732, P63; ter Haar FB, 2008, LECT NOTES COMPUT SC, V5305, P652, DOI 10.1007/978-3-540-88693-8_48; Thompson D. A. W, 1917, GROWTH FORM; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435; Wu Y, 2019, INT J COMPUT VISION, V127, P115, DOI 10.1007/s11263-018-1097-z; Yang F, 2012, PROC CVPR IEEE, P861, DOI 10.1109/CVPR.2012.6247759; Yang F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964955; Yin LJ, 2008, IEEE INT CONF AUTOMA, P116; Zhou YX, 2017, IEEE INT CONF AUTOMA, P626, DOI 10.1109/FG.2017.79; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	55	20	20	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					547	571		10.1007/s11263-019-01260-7	http://dx.doi.org/10.1007/s11263-019-01260-7		NOV 2019	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KJ1GX		hybrid			2022-12-18	WOS:000495287900002
J	Zhao, J; Xing, JL; Xiong, L; Yan, SC; Feng, JS				Zhao, Jian; Xing, Junliang; Xiong, Lin; Yan, Shuicheng; Feng, Jiashi			Recognizing Profile Faces by Imagining Frontal View	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose-invariant face recognition; Face frontalization; Cross-domain adversarial learning; Meta-learning; Learning to learn; Enforced cross-entropy optimization; Generative adversarial networks	RECOGNITION	Extreme pose variation is one of the key obstacles to accurate face recognition in practice. Compared with current techniques for pose-invariant face recognition, which either expect pose invariance from hand-crafted features or data-driven deep learning solutions, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose-Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network which simultaneously perceives global structures and local details, incorporating an unsupervised cross-domain adversarial training and a meta-learning ("learning to learn") strategy using siamese discriminator with dynamic convolution for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representations with large intra-class affinity and inter-class separability. Qualitative and quantitative experiments on both controlled and in-the-wild benchmark datasets demonstrate the superiority of the proposed model over the state-of-the-arts.	[Zhao, Jian] Inst North Elect Equipment, Beijing, Peoples R China; [Xing, Junliang] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Xiong, Lin] JD Digits, Silicon Valley, CA USA; [Yan, Shuicheng; Feng, Jiashi] Natl Univ Singapore, Singapore, Singapore; [Yan, Shuicheng] Yitu Technol, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; National University of Singapore	Xing, JL (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.	zhaojian90@u.nus.edu; jlxing@nlpr.ia.ac.cn; Lin.Xiong@jd.com; eleyans@nus.edu.sg; elefjia@nus.edu.sg	Xing, Junliang/HGE-9630-2022; Feng, Jiashi/AGX-6209-2022; Yan, Shuicheng/HCI-1431-2022	Xing, Junliang/0000-0001-6801-0510; Zhao, Jian/0000-0002-3508-756X	National Science Foundation of China [61672519]; NUS startup [R-263-000-C08-133]; NUS IDS [R-263-000-C67-646]; ECRA [R-263-000-C87-133]; MOE [R-263-000-C21-112]	National Science Foundation of China(National Natural Science Foundation of China (NSFC)); NUS startup; NUS IDS; ECRA; MOE(Ministry of Higher Education & Scientific Research (MHESR))	The work of Junliang Xing was partially supported by the National Science Foundation of China 61672519. The work of Jiashi Feng was partially supported by NUS startup R-263-000-C08-133, MOE Tier-I R-263-000-C21-112, NUS IDS R-263-000-C67-646 and ECRA R-263-000-C87-133.	Abdalmageed W., 2016, CORR, P1, DOI DOI 10.1109/WACV.2016.7477555; Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Bertinetto Luca, 2016, NIPS; Bowyer KW, 2006, COMPUT VIS IMAGE UND, V101, P1, DOI 10.1016/j.cviu.2005.05.005; Cao J, 2018, ADV NEUR IN, V31; Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389; Chen JC, 2016, IEEE IMAGE PROC, P2981, DOI 10.1109/ICIP.2016.7532906; Chen W., 2009, ADV NEURAL INFORM PR, P315; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160; Dave R., 2018, ARXIV180307288; Ding CX, 2017, PATTERN RECOGN, V66, P144, DOI 10.1016/j.patcog.2016.11.024; Freiwald WA, 2010, SCIENCE, V330, P845, DOI 10.1126/science.1194908; Ganin Y, 2016, J MACH LEARN RES, V17; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Hao SY, 2018, IEEE T GEOSCI REMOTE, V56, P2349, DOI 10.1109/TGRS.2017.2778343; HASSNER T, 2015, PROC CVPR IEEE, P4295, DOI DOI 10.1109/CVPR.2015.7299058; Hassner T., 2016, P IEEE C COMP VIS PA, P59; Hu YB, 2018, PROC CVPR IEEE, P8398, DOI 10.1109/CVPR.2018.00876; Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243; Kang BN, 2013, INT CONF UBIQ ROBOT, P346, DOI 10.1109/URAI.2013.6677383; Kasabov N, 2016, 2016 IEEE 8TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS (IS), P15, DOI 10.1109/IS.2016.7737434; Kingma D. P., 2013, AUTO ENCODING VARIAT; KLARE BF, 2015, PROC CVPR IEEE, P1931, DOI DOI 10.1109/CVPR.2015.7298803; LeCun Y, 1997, MNIST DATABASE HANDW; Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212; Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523; Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35; Mirza M., 2014, ARXIV; Ohayon S, 2012, NEURON, V74, P567, DOI 10.1016/j.neuron.2012.03.024; Parkhi Omkar M., 2015, BRIT MACH VIS C; Ranjan R., 2016, ARXIV161100851; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sagonas C, 2015, IEEE I CONF COMP VIS, P3871, DOI 10.1109/ICCV.2015.441; Sankaranarayanan S., 2016, P IEEE INT C BIOMETR, P1; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Sun Y., 2015, ARXIV PREPRINT ARXIV; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Taigman Y, 2015, PROC CVPR IEEE, P2746, DOI 10.1109/CVPR.2015.7298891; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tran L., 2017, CVPR; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang D., 2015, ARXIV150707242; Wang W, 2018, IEEE T PATTERN ANAL, V40, P2569, DOI 10.1109/TPAMI.2018.2810881; Wang W, 2019, IEEE T PATTERN ANAL, V41, P654, DOI 10.1109/TPAMI.2018.2803166; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; Xiao S., 2016, ACM MM, P691; Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4; Xiong C, 2015, IEEE I CONF COMP VIS, P3667, DOI 10.1109/ICCV.2015.418; Yim J, 2015, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2015.7298667; Yin X, 2018, IEEE T IMAGE PROCESS, V27, P964, DOI 10.1109/TIP.2017.2765830; Zhao J., 2018, T PAMI; Zhao J., 2017, ADV NEURAL INFORM PR, P65; Zhao J., 2019, ARXIV190204755; Zhao J, 2019, AAAI CONF ARTIF INTE, P9251; Zhao J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1184; Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679; Zhu Z., 2014, NIPS, P217; Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21	71	20	20	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					460	478		10.1007/s11263-019-01252-7	http://dx.doi.org/10.1007/s11263-019-01252-7		NOV 2019	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KJ1GX					2022-12-18	WOS:000494408100001
J	Micusik, B; Wildenauer, H				Micusik, Branislav; Wildenauer, Horst			Structure from Motion with Line Segments Under Relaxed Endpoint Constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Line segments; Line segment matching; Visual odometry	CORRESPONDENCES; REPRESENTATION; ALGORITHM	We present a novel structure from motion pipeline, which estimates motion and wiry 3D structure from imaged line segments across multiple views. Although the position and orientation of line segments can be determined more accurately than point features, the instability of their endpoints and the fact that lines are not constrained by epipolar geometry diverted most research focus away to point-based methods. In our approach, we tackle the problem of instable endpoints by utilizing relaxed constraints on their positions, both during matching and as well in the following bundle adjustment stage. Furthermore, we gain efficiency in estimating trifocal image relations by decoupling rotation and translation. To this end, a novel linear solver for relative translation estimation given rotations from five line correspondences in three views is introduced. Extensive experiments on long image sequences show that our line-based structure from motion pipeline advantageously complements point-based methods, giving more meaningful 3D representation for indoor scenarios.	[Micusik, Branislav] AIT, Digital Safety & Secur Dept, Donau City Str 1, A-1220 Vienna, Austria; [Wildenauer, Horst] Zeno Track GmbH, Barichgasse 40-42, A-1030 Vienna, Austria	Austrian Institute of Technology (AIT)	Micusik, B (corresponding author), AIT, Digital Safety & Secur Dept, Donau City Str 1, A-1220 Vienna, Austria.	Branislav.Micusik@ait.ac.at; Horst.Wildenauer@zenotrack.com			Austrian Research Promotion Agencys (FFG) [LOLOG 840168, LARAH 4586620, PAMON 835916]	Austrian Research Promotion Agencys (FFG)	This research received funding from the Austrian Research Promotion Agencys (FFG) Projects LOLOG 840168, LARAH 4586620 and PAMON 835916.	Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; [Anonymous], P BRIT MACH VIS C BM; Ansar A, 2002, P EUR C COMP VIS ECC; Antone ME, 2000, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.2000.854809; Barrow HG, 1977, P 5 INT JOINT C ART; Bartoli A, 2005, COMPUT VIS IMAGE UND, V100, P416, DOI 10.1016/j.cviu.2005.06.001; Bay H, 2005, PROC CVPR IEEE, P329; Bazin JC, 2010, COMPUT VIS IMAGE UND, V114, P254, DOI 10.1016/j.cviu.2009.04.006; Bazin JC, 2012, IEEE INT C INT ROBOT, P4282, DOI 10.1109/IROS.2012.6385802; BURNS JB, 1986, IEEE T PATTERN ANAL, V8, P425, DOI 10.1109/TPAMI.1986.4767808; CHEN HH, 1991, IEEE T PATTERN ANAL, V13, P530, DOI 10.1109/34.87340; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; Deriche R., 1990, P EUR C COMP VIS ECC; Elqursh A, 2011, PROC CVPR IEEE; Fan B, 2010, PROC CVPR IEEE, P390, DOI 10.1109/CVPR.2010.5540186; Faugeras O., 1987, P INT C COMP VIS ICC; Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135; Hartley RI, 1997, INT J COMPUT VISION, V22, P125, DOI 10.1023/A:1007936012022; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hirose K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.83; Hofer M, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.92; Huynh DQ, 2009, J MATH IMAGING VIS, V35, P155, DOI 10.1007/s10851-009-0161-2; Jain A., 2010, P IEEE C COMP VIS PA; Jiang N., 2013, P INT C COMP VIS ICC; LIU YC, 1988, COMPUT VISION GRAPH, V44, P35, DOI 10.1016/S0734-189X(88)80030-6; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Martinec D., 2003, P IEEE C COMP VIS PA; Micusik B, 2006, IEEE T PATTERN ANAL, V28, P1135, DOI 10.1109/TPAMI.2006.151; Micusik B., 2015, P IEEE C COMP VIS PA; Morris DD, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P696, DOI 10.1109/ICCV.1998.710793; Rother Carsten, 2003, THESIS; Schindler G., 2006, P INT S 3D DAT PROC; Sinha S.N., 2012, TRENDS TOPICS COMPUT, V6554, P267, DOI DOI 10.1007/978-3-642-35740-4; Sinha SN, 2009, IEEE I CONF COMP VIS, P1881, DOI 10.1109/ICCV.2009.5459417; SPETSAKIS ME, 1992, CVGIP-IMAG UNDERSTAN, V56, P230, DOI 10.1016/1049-9660(92)90040-A; Strecha C., 2008, P IEEE C COMP VIS PA; Subbarao R, 2009, INT J COMPUT VISION, V84, P1, DOI 10.1007/s11263-008-0195-8; TAYLOR CJ, 1995, IEEE T PATTERN ANAL, V17, P1021, DOI 10.1109/34.473228; Verhagen B, 2014, IEEE WINT CONF APPL, P493, DOI 10.1109/WACV.2014.6836061; VISCODA, 2012, VOOD CAM TRACK; Wang L., 2009, P INT C COMP VIS ICC; Weng J., 1993, SPRINGER SERIES INFO; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P318, DOI 10.1109/34.120327; Werner T., 2002, P EUR C COMP VIS ECC; Wildenauer H, 2012, PROC CVPR IEEE, P2831, DOI 10.1109/CVPR.2012.6248008; Wu Changchang, 2011, VISUALSFM VISUAL STR, P1; Zhang LL, 2014, J VIS COMMUN IMAGE R, V25, P904, DOI 10.1016/j.jvcir.2014.02.013; Zhang ZY, 1997, IMAGE VISION COMPUT, V15, P59, DOI 10.1016/S0262-8856(96)01112-2; ZHANG ZY, 1995, IEEE T PATTERN ANAL, V17, P1129	50	20	20	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2017	124	1					65	79		10.1007/s11263-016-0971-9	http://dx.doi.org/10.1007/s11263-016-0971-9			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FA2JD					2022-12-18	WOS:000405265900004
J	Li, Y; Song, YZ; Hospedales, TM; Gong, SG				Li, Yi; Song, Yi-Zhe; Hospedales, Timothy M.; Gong, Shaogang			Free-Hand Sketch Synthesis with Deformable Stroke Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stroke analysis; Perceptual grouping; Deformable stroke model; Sketch synthesis		We present a generative model which can automatically summarize the stroke composition of free-hand sketches of a given category. When our model is fit to a collection of sketches with similar poses, it discovers and learns the structure and appearance of a set of coherent parts, with each part represented by a group of strokes. It represents both consistent (topology) as well as diverse aspects (structure and appearance variations) of each sketch category. Key to the success of our model are important insights learned from a comprehensive study performed on human stroke data. By fitting this model to images, we are able to synthesize visually similar and pleasant free-hand sketches.	[Li, Yi; Song, Yi-Zhe; Hospedales, Timothy M.; Gong, Shaogang] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London, England; [Hospedales, Timothy M.] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of London; Queen Mary University London; University of Edinburgh	Li, Y (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, London, England.	yi.li@qmul.ac.uk; yizhe.song@qmul.ac.uk; t.hospedales@qmul.ac.uk; s.gong@qmul.ac.uk		Li, Yi/0000-0001-9400-2186				Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Berger I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461964; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chen H., 2002, P 10 ACM INT C MULT, P171; Chen H., 2006, PROC IEEE COMPUT SCI, V1, P943; Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]; Dai JF, 2013, IEEE I CONF COMP VIS, P1305, DOI 10.1109/ICCV.2013.165; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354; DUBUISSON MP, 1994, INT C PATT RECOG, P566, DOI 10.1109/ICPR.1994.576361; Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Fanti C, 2004, ADV NEUR IN, V16, P1603; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Fu HB, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024167; Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133; Grabli S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731056; Guo CE, 2007, COMPUT VIS IMAGE UND, V106, P5, DOI 10.1016/j.cviu.2005.09.004; Huang Z, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661228.2661280; Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470; Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43; Koffka K., 1935, PRINCIPLES GESTALT P; Kyprianidis J. E., 2008, P EG UK THEOR PRACT, P51, DOI DOI 10.2312/LOCALCHAPTEREVENTS/TPCG/TPCG08/051-058; Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882; Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406; Liu J, 2014, SPRINGERBRIEF PHYS, P15, DOI 10.1007/978-3-642-40549-5_2; Liu MY, 2010, PROC CVPR IEEE, P1696, DOI 10.1109/CVPR.2010.5539837; Minear M, 2004, BEHAV RES METH INS C, V36, P630, DOI 10.3758/BF03206543; OceanSITES, 2020, P 16 EUR C REND TECH, DOI DOI 10.2312/EGWR/EGSR05/183-192; Opelt A, 2006, LECT NOTES COMPUT SC, V3952, P575; Qi YG, 2015, PROC CVPR IEEE, P1856, DOI 10.1109/CVPR.2015.7298795; Qi YG, 2013, IEEE IMAGE PROC, P270, DOI 10.1109/ICIP.2013.6738056; Ren XF, 2008, INT J COMPUT VISION, V77, P47, DOI 10.1007/s11263-007-0092-6; SAUND E, 1992, ARTIF INTELL, V54, P71, DOI 10.1016/0004-3702(92)90088-F; Schneider RG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661231; Shotton J, 2008, IEEE T PATTERN ANAL, V30, P1270, DOI 10.1109/TPAMI.2007.70772; Thayananthan A, 2003, PROC CVPR IEEE, P127; Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930; Winnemoller H., 2011, P NONPH AN REND NPAR, P147; Wu YN, 2010, INT J COMPUT VISION, V90, P198, DOI 10.1007/s11263-009-0287-0; Xiaofeng R., 2012, P ADV NEUR INF PROC, P584, DOI DOI 10.1634/THEONCOLOGIST.8-3-252; Xu ZJ, 2008, IEEE T PATTERN ANAL, V30, P955, DOI 10.1109/TPAMI.2008.50; Yu Q, 2015, P BRIT MACH VIS C BM	47	20	20	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2017	122	1					169	190		10.1007/s11263-016-0963-9	http://dx.doi.org/10.1007/s11263-016-0963-9			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EL2AF		hybrid, Green Published, Green Submitted			2022-12-18	WOS:000394421800008
J	Urban, S; Wursthorn, S; Leitloff, J; Hinz, S				Urban, Steffen; Wursthorn, Sven; Leitloff, Jens; Hinz, Stefan			MultiCol Bundle Adjustment: A Generic Method for Pose Estimation, Simultaneous Self-Calibration and Reconstruction for Arbitrary Multi-Camera Systems	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Camera; Calibration; Accuracy; Precision; Geometric; Robotics; Bundle	O(N) SOLUTION; WIDE-ANGLE; MODEL	In this paper, we present a generic, modular bundle adjustment method for pose estimation, simultaneous self-calibration and reconstruction for multi-camera systems. In contrast to other approaches that use bearing vectors (camera rays) as observations, we extend the common collinearity equations with a general camera model and include the relative orientation of each camera w.r.t to the fixed multi-camera system frame yielding the extended collinearity equations that directly express all image observations as functions of all unknowns. Hence, we can either calibrate the camera system, the cameras, reconstruct the observed scene, and/or simply estimate the pose of the system by including the corresponding parameter block into the Jacobian matrix. Apart from evaluating the implementation with comprehensive simulations, we benchmark our method against recently published methods for pose estimation and bundle adjustment for multi-camera systems. Finally, all methods are evaluated using a 6 degree of freedom ground truth data set, that was recorded with a lasertracker.	[Urban, Steffen; Wursthorn, Sven; Leitloff, Jens; Hinz, Stefan] KIT, Inst Photogrammetry & Remote Sensing, Karlsruhe, Germany	Helmholtz Association; Karlsruhe Institute of Technology	Urban, S (corresponding author), KIT, Inst Photogrammetry & Remote Sensing, Karlsruhe, Germany.	steffen.urban@kit.edu	Hinz, Stefan/HDN-5781-2022	Hinz, Stefan/0000-0002-7323-9800; Leitloff, Jens/0000-0002-8405-0675	DFG research group FG 1546	DFG research group FG 1546(German Research Foundation (DFG))	This project was partially funded by the DFG research group FG 1546 "Computer-Aided Collaborative Subway Track Planning in Multi-Scale 3D City and Building Models".	Agarwal S., 2012, CERES SOLVER TUTORIA; Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; Baker P, 2000, IEEE WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P134, DOI 10.1109/OMNVIS.2000.853820; Barreto J., 2004, OMN 2004 ECCV 2004 W; Blanco-Claraco JL, 2014, INT J ROBOT RES, V33, P207, DOI 10.1177/0278364913507326; Bouguet J., 2014, CAMERA CALIBRATION T; BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855; Brown Duane C, 1966, PHOTOGRAMMETRIC ENG, P2, DOI DOI 10.1234/12345678; Bruckner M, 2014, MACH VISION APPL, V25, P389, DOI 10.1007/s00138-013-0541-x; Carrera G, 2011, IEEE INT CONF ROBOT, P2652, DOI 10.1109/ICRA.2011.5980294; Ceriani S, 2009, AUTON ROBOT, V27, P353, DOI 10.1007/s10514-009-9156-5; Clipp B, 2008, IEEE WORK APP COMP, P125; Davison A. J., 2004, P IFAC EURON S INT A; Ess A., 2007, BMVC, P1; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Forstner Wolfgang, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P619, DOI 10.1007/978-3-642-19309-5_48; Frahm JM, 2004, LECT NOTES COMPUT SC, V3175, P286; Frank O., 2007, BMVC, P1; Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Geyer C., 2000, EUR C COMP VIS, P445; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hesch JA, 2011, IEEE I CONF COMP VIS, P383, DOI 10.1109/ICCV.2011.6126266; Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153; Kneip L., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2969, DOI 10.1109/CVPR.2011.5995464; Kneip L, 2014, IEEE INT CONF ROBOT, P1, DOI 10.1109/ICRA.2014.6906582; Kneip L, 2014, LECT NOTES COMPUT SC, V8689, P127, DOI 10.1007/978-3-319-10590-1_9; Kneip L, 2013, IEEE INT CONF ROBOT, P3770, DOI 10.1109/ICRA.2013.6631107; Kumar R., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587676; Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607; Kurillo G., 2008, ACM IEEE INT C DISTR, P1, DOI [10.1109/ICDSC.2008.4635695, DOI 10.1109/ICDSC.2008.4635695]; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Li B, 2013, IEEE INT C INT ROBOT, P1301, DOI 10.1109/IROS.2013.6696517; Lourakis MIA, 2009, ACM T MATH SOFTWARE, V36, DOI 10.1145/1486525.1486527; Ly DS, 2014, MACH VISION APPL, V25, P1601, DOI 10.1007/s00138-014-0624-3; Maas HG, 1999, ISPRS J PHOTOGRAMM, V54, P352, DOI 10.1016/S0924-2716(99)00029-5; Mei C, 2007, IEEE INT CONF ROBOT, P3945, DOI 10.1109/ROBOT.2007.364084; Micusik B., 2004, THESIS; Muhle D, 2011, LECT NOTES COMPUT SC, V6952, P13, DOI 10.1007/978-3-642-24393-6_2; Puig L, 2012, COMPUT VIS IMAGE UND, V116, P120, DOI 10.1016/j.cviu.2011.08.003; Scaramuzza D., 2006, IEEE INT C COMP VIS, P45, DOI DOI 10.1109/ICVS.2006.3; Scaramuzza D, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P5695, DOI 10.1109/IROS.2006.282372; Scaramuzza Davide, 2014, OCAMCALIB OMNIDIRECT; Schneider J., 2012, ISPRS ANN PHOTOGRAMM, VI-3, P75; Schneider J, 2013, PHOTOGRAMM FERNERKUN, P309, DOI 10.1127/1432-8364/2013/0179; Schonbein M, 2014, IEEE INT CONF ROBOT, P4443, DOI 10.1109/ICRA.2014.6907507; Schonbein M., 2014, INT C INT ROB SYST I; Schwartz G, 2008, PUBLIC INVESTMENT AND PUBLIC-PRIVATE PARTNERSHIPS: ADDRESSING INFRASTRUCTURE CHALLENGES AND MANAGING FISCAL RISKS, P1; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Svoboda T., 2003, BIWITR26 ETH SWISS F; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; Urban S, 2015, ISPRS J PHOTOGRAMM, V108, P72, DOI 10.1016/j.isprsjprs.2015.06.005; Wu Changchang, 2011, VISUALSFM VISUAL STR, V9; Zheng YQ, 2013, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2013.291; Zhengyou Zhang, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P666, DOI 10.1109/ICCV.1999.791289	55	20	20	0	39	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	2					234	252		10.1007/s11263-016-0935-0	http://dx.doi.org/10.1007/s11263-016-0935-0			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK2LI					2022-12-18	WOS:000393758400003
J	Pickup, D; Sun, X; Rosin, PL; Martin, RR; Cheng, Z; Lian, Z; Aono, M; Ben Hamza, A; Bronstein, A; Bronstein, M; Bu, S; Castellani, U; Cheng, S; Garro, V; Giachetti, A; Godil, A; Isaia, L; Han, J; Johan, H; Lai, L; Li, B; Li, C; Li, H; Litman, R; Liu, X; Liu, Z; Lu, Y; Sun, L; Tam, G; Tatsuma, A; Ye, J				Pickup, D.; Sun, X.; Rosin, P. L.; Martin, R. R.; Cheng, Z.; Lian, Z.; Aono, M.; Ben Hamza, A.; Bronstein, A.; Bronstein, M.; Bu, S.; Castellani, U.; Cheng, S.; Garro, V.; Giachetti, A.; Godil, A.; Isaia, L.; Han, J.; Johan, H.; Lai, L.; Li, B.; Li, C.; Li, H.; Litman, R.; Liu, X.; Liu, Z.; Lu, Y.; Sun, L.; Tam, G.; Tatsuma, A.; Ye, J.			Shape Retrieval of Non-rigid 3D Human Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Benchmark; 3D shape retrieval; Non-rigid 3D shape retrieval; 3D humans	DESCRIPTOR; DISTANCE; FEATURES; POSE	3D models of humans are commonly used within computer graphics and vision, and so the ability to distinguish between body shapes is an important shape retrieval problem. We extend our recent paper which provided a benchmark for testing non-rigid 3D shape retrieval algorithms on 3D human models. This benchmark provided a far stricter challenge than previous shape benchmarks. We have added 145 new models for use as a separate training set, in order to standardise the training data used and provide a fairer comparison. We have also included experiments with the FAUST dataset of human scans. All participants of the previous benchmark study have taken part in the new tests reported here, many providing updated results using the new data. In addition, further participants have also taken part, and we provide extra analysis of the retrieval results. A total of 25 different shape retrieval methods are compared.	[Pickup, D.; Sun, X.; Rosin, P. L.; Martin, R. R.] Cardiff Univ, Cardiff, S Glam, Wales; [Cheng, Z.] Avatar Sci Hunan Co, Changsha, Hunan, Peoples R China; [Lian, Z.] Peking Univ, Beijing, Peoples R China; [Aono, M.; Tatsuma, A.] Toyohashi Univ Technol, Toyohashi, Aichi, Japan; [Ben Hamza, A.] Concordia Univ, Montreal, PQ, Canada; [Bronstein, A.; Litman, R.] Tel Aviv Univ, Tel Aviv, Israel; [Bronstein, M.] Univ Lugano, Lugano, Switzerland; [Bu, S.; Cheng, S.; Han, J.; Liu, Z.] Northwestern Polytech Univ, Fremont, Peoples R China; [Castellani, U.; Garro, V.; Giachetti, A.; Isaia, L.] Univ Verona, Verona, Italy; [Garro, V.] ISTI CNR, Pisa, Italy; [Godil, A.] NIST, Gaithersburg, MD 20899 USA; [Johan, H.] Fraunhofer IDM NTU, Singapore, Singapore; [Lai, L.; Li, H.; Liu, X.; Sun, L.] Beijing Technol & Business Univ, Beijing, Peoples R China; [Li, B.; Lu, Y.] Texas State Univ, San Marcos, TX USA; [Li, C.] Duke Univ, Durham, NC 27706 USA; [Tam, G.] Swansea Univ, Swansea, W Glam, Wales; [Ye, J.] Penn State Univ, State Coll, PA USA	Cardiff University; Peking University; Toyohashi University of Technology; Concordia University - Canada; Tel Aviv University; Universita della Svizzera Italiana; Northwestern Polytechnical University; University of Verona; Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR); National Institute of Standards & Technology (NIST) - USA; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Beijing Technology & Business University; Texas State University System; Texas State University San Marcos; Duke University; Swansea University; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University	Pickup, D (corresponding author), Cardiff Univ, Cardiff, S Glam, Wales.	pickupd@cardiff.ac.uk	Giachetti, Andrea/AAD-8247-2020; Sun, Xianfang/ABG-8970-2021; Ye, Jianbo/Q-6990-2017; Tam, Gary KL/E-5098-2011	LI, Haisheng/0000-0003-4861-0513; giachetti, andrea/0000-0002-7523-6806; Ye, Jianbo/0000-0003-4612-6429; Rosin, Paul/0000-0002-4965-3884; Tam, Gary KL/0000-0001-7387-5180	EPSRC [EP/J02211X/1]; Kayamori Foundation of Informational Science Advancement; JSPS KAKENHI [26280038, 15K12027, 15K15992]; National Natural Science Foundation of China [61202230, 61472015]; Grants-in-Aid for Scientific Research [15K15992] Funding Source: KAKEN; Engineering and Physical Sciences Research Council [EP/J02211X/1] Funding Source: researchfish	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Kayamori Foundation of Informational Science Advancement(Kayamori Foundation of Informational Science Advancement); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Grants-in-Aid for Scientific Research(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by EPSRC Research Grant EP/J02211X/1. Atsushi Tatsuma and Masaki Aono were supported by Kayamori Foundation of Informational Science Advancement and JSPS KAKENHI Grant Numbers 26280038, 15K12027 and 15K15992. Zhouhui Lian was supported by National Natural Science Foundation of China Grant Numbers 61202230 and 61472015.	Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Au O. K.-C., 2008, ACM SIGGRAPH 2008 SI; Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Bogo F., 2014, 2014 IEEE C COMP VIS; Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405; Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838; Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788; Bu SH, 2014, IEEE MULTIMEDIA, V21, P38, DOI 10.1109/MMUL.2014.52; Chen Y., 2013, P IEEE CAD GRAPH; Duda R.O., 2017, PATTERN CLASSIFICATI; Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902; Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45; Giachetti A, 2012, COMPUT GRAPH FORUM, V31, P1669, DOI 10.1111/j.1467-8659.2012.03172.x; Hasler N, 2009, COMPUT GRAPH FORUM, V28, P337, DOI 10.1111/j.1467-8659.2009.01373.x; Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; KAC M, 1966, AM MATH MON, V73, P1, DOI 10.2307/2313748; Li B, 2014, MULTIMED TOOLS APPL, V72, P1531, DOI 10.1007/s11042-013-1464-2; Li B, 2013, MULTIMED TOOLS APPL, V62, P821, DOI 10.1007/s11042-011-0873-3; Li C., 2013, THESIS; Li CY, 2013, INT J MULTIMED INF R, V2, P261, DOI 10.1007/s13735-013-0041-9; Li CY, 2014, MULTIMEDIA SYST, V20, P253, DOI 10.1007/s00530-013-0318-0; Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3; Lian Z., 2015, EUR WORKSH 3D OBJ RE; Lian Z., 2011, 3DOR, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088; Lian ZH, 2013, MACH VISION APPL, V24, P1685, DOI 10.1007/s00138-013-0501-5; Lian ZH, 2013, INT J COMPUT VISION, V102, P221, DOI 10.1007/s11263-012-0548-1; LIPMAN Y, 2010, ACM T GRAPHIC, V29; Litman R, 2014, COMPUT GRAPH FORUM, V33, P127, DOI 10.1111/cgf.12438; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Pickup D., 2016, P INT C COM IN PRESS; Pickup D, 2014, P 7 EUR WORKSH 3D OB; Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Rusu R. B., 2008, P 10 INT C INT AUT S; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Tam GKL, 2007, IEEE T VIS COMPUT GR, V13, P470, DOI 10.1109/TVCG.2007.1011; Valette S, 2004, COMPUT GRAPH FORUM, V23, P381, DOI 10.1111/j.1467-8659.2004.00769.x; Valette S, 2008, IEEE T VIS COMPUT GR, V14, P369, DOI 10.1109/TVCG.2007.70430; Van Der Heijden F., 2005, CLASSIFICATION PARAM; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wahl E, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P474, DOI 10.1109/IM.2003.1240284; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Yan HB, 2008, IEEE T VIS COMPUT GR, V14, P693, DOI 10.1109/TVCG.2008.28; Ye J, 2013, P 3 ACM C INT C MULT, P121; Ye JB, 2016, VISUAL COMPUT, V32, P553, DOI 10.1007/s00371-015-1071-5; ZHOU X., 2011, P 17 ACM SIGKDD INT, P877	53	20	22	0	26	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2016	120	2					169	193		10.1007/s11263-016-0903-8	http://dx.doi.org/10.1007/s11263-016-0903-8			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DU3EF		Green Accepted, hybrid, Green Published, Green Submitted			2022-12-18	WOS:000382092900004
J	Foi, A; Boracchi, G				Foi, Alessandro; Boracchi, Giacomo			Foveated Nonlocal Self-Similarity	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							NATURAL IMAGES; VIDEO COMPRESSION; EYE-MOVEMENTS; MICROSACCADES; STATISTICS; VISION; SENSITIVITY; TRANSFORM; QUALITY; FIELDS	When we gaze a scene, our visual acuity is maximal at the fixation point (imaged by the fovea, the central part of the retina) and decreases rapidly towards the periphery of the visual field. This phenomenon is known as foveation. We investigate the role of foveation in nonlocal image filtering, installing a different form of self-similarity: the foveated self-similarity. We consider the image denoising problem as a simple means of assessing the effectiveness of descriptive models for natural images and we show that, in nonlocal image filtering, the foveated self-similarity is far more effective than the conventional windowed self-similarity. To facilitate the use of foveation in nonlocal imaging algorithms, we develop a general framework for designing foveation operators for patches by means of spatially variant blur. Within this framework, we construct several parametrized families of operators, including anisotropic ones. Strikingly, the foveation operators enabling the best denoising performance are the radial ones, in complete agreement with the orientation preference of the human visual system.	[Foi, Alessandro] Tampere Univ Technol, Dept Signal Proc, Tampere, Finland; [Boracchi, Giacomo] Politecn Milan, Dipartimento Elettron Informaz & Bioingn, Milan, Italy	Tampere University; Polytechnic University of Milan	Foi, A (corresponding author), Tampere Univ Technol, Dept Signal Proc, Tampere, Finland.	alessandro.foi@tut.fi	Foi, Alessandro/D-6010-2012	Foi, Alessandro/0000-0001-8228-3187; Boracchi, Giacomo/0000-0002-1650-3054	Academy of Finland [252547]	Academy of Finland(Academy of Finland)	This work was supported by the Academy of Finland (Project No. 252547, Academy Research Fellow 2011-2016).	Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715; Alexander S. K., 2008, LNCS, V5112; Arias P, 2011, INT J COMPUT VISION, V93, P319, DOI 10.1007/s11263-010-0418-7; Awate SP, 2005, PROC CVPR IEEE, P44; Baddeley R, 1997, COGNITIVE SCI, V21, P351, DOI 10.1207/s15516709cog2103_4; Balas B, 2009, J VISION, V9, DOI [10.1167/9.2.16, 10.1167/9.12.13]; Barnsley M., 1993, FRACTALS EVERYWHERE; Basu A, 1998, IEEE T SYST MAN CY A, V28, P137, DOI 10.1109/3468.661143; Bigun J., 1986, P EUSIPCO, P883; Bigun J., 2011, SSBA P, P4; Boracchi G., 2012, P SPIE HVE, VI, P8291; Brockmole JR, 2005, PERCEPT PSYCHOPHYS, V67, P495, DOI 10.3758/BF03193327; Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024; Buades A, 2010, SIAM REV, V52, P113, DOI 10.1137/090773908; Buades A, 2011, IPOL2011; Buades A, 2008, INT J COMPUT VISION, V76, P123, DOI 10.1007/s11263-007-0052-1; Buades A, 2009, IEEE T IMAGE PROCESS, V18, P1192, DOI 10.1109/TIP.2009.2017171; Chang EC, 2000, APPL COMPUT HARMON A, V9, P312, DOI 10.1006/acha.2000.0324; CHATTERJEE P, 2008, P SPIE ELECT IMAG, V6814, P9; Chatterjee P, 2010, IEEE T IMAGE PROCESS, V19, P895, DOI 10.1109/TIP.2009.2037087; Chierchia G., 2014, ARXIV12105844V3; Chierchia G., 2014, SIGNAL IMAGE VIDEO P, P1; Chierchia G, 2014, IEEE T IMAGE PROCESS, V23, P5531, DOI 10.1109/TIP.2014.2364141; Ciresan D., 2012, ADV NEURAL INF PROCE, V25, P2843; Conn A.R., 2009, INTRO DERIVATIVE FRE; CURCIO CA, 1990, J COMP NEUROL, V292, P497, DOI 10.1002/cne.902920402; Dabov K., 2009, P SPARS; Dabov K., 2007, P EUSIPCO; Dabov K, 2008, PROC SPIE, V6812, DOI 10.1117/12.766355; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Daly S, 2001, J ELECTRON IMAGING, V10, P30, DOI 10.1117/1.1333679; Danielyan A., 2008, P LNLA; Danielyan A, 2012, IEEE T IMAGE PROCESS, V21, P1715, DOI 10.1109/TIP.2011.2176954; Darbon J, 2008, I S BIOMED IMAGING, P1331, DOI 10.1109/ISBI.2008.4541250; De Bonet J. S., 1997, NOISE REDUCTION DETE; Deledalle CA, 2012, INT J COMPUT VISION, V99, P86, DOI 10.1007/s11263-012-0519-6; Demeyer M, 2009, J VISION, V9, DOI 10.1167/9.4.28; Desbordes G., 2007, THESIS; Dong B, 2008, MULTISCALE MODEL SIM, V7, P589, DOI 10.1137/070705556; Donner K, 2007, VISION RES, V47, P1166, DOI 10.1016/j.visres.2006.11.024; Duncan RO, 2003, NEURON, V38, P659, DOI 10.1016/S0896-6273(03)00265-4; Ebrahimi M., 2008, P IPCV; Ebrahimi M, 2008, LECT NOTES COMPUT SC, V5112, P170, DOI 10.1007/978-3-540-69812-8_17; Ebrahimi M, 2007, LECT NOTES COMPUT SC, V4633, P117; Eckstein M. P., 2011, JOV, V11; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Elad M, 2009, COMPUT J, V52, P15, DOI 10.1093/comjnl/bxm008; Engbert R, 2004, PSYCHOL SCI, V15, P431, DOI 10.1111/j.0956-7976.2004.00697.x; Etienne-Cummings R, 2000, IEEE T CIRCUITS-II, V47, P504, DOI 10.1109/82.847066; Foi A., 2013, FROC SPARS 2013 SIGN, P1; Foi A, 2014, 2014 IEEE WORKSHOP ON STATISTICAL SIGNAL PROCESSING (SSP), P145, DOI 10.1109/SSP.2014.6884596; Foi A, 2013, IEEE IMAGE PROC, P464, DOI 10.1109/ICIP.2013.6738096; Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889; Freeman J, 2011, J NEUROSCI, V31, P4792, DOI 10.1523/JNEUROSCI.5160-10.2011; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Froment J, 2014, IMAGE PROCESS ON LIN, V4, P300, DOI 10.5201/ipol.2014.120; Garway-Heath DF, 2000, INVEST OPHTH VIS SCI, V41, P1774; Geisler WS, 2001, VISION RES, V41, P711, DOI 10.1016/S0042-6989(00)00277-7; Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120; Ghazel M, 2003, IEEE T IMAGE PROCESS, V12, P1560, DOI 10.1109/TIP.2003.818038; Gilboa G, 2008, MULTISCALE MODEL SIM, V7, P1005, DOI 10.1137/070698592; Grewenig S, 2011, J VIS COMMUN IMAGE R, V22, P117, DOI 10.1016/j.jvcir.2010.11.001; Haloi M, 2015, ARXIV PREPRINT ARXIV; Herwig A, 2014, J EXP PSYCHOL GEN, V143, P1903, DOI 10.1037/a0036781; Jacquin AE, 1992, IEEE T IMAGE PROCESS, V1, P18, DOI 10.1109/83.128028; Jennings JAM, 1997, VISION RES, V37, P697, DOI 10.1016/S0042-6989(96)00238-6; Ji ZX, 2009, INFORM PROCESS LETT, V109, P1238, DOI 10.1016/j.ipl.2009.09.007; Jin Q. Y., 2011, ARXIV11095640; Joselevitch Christina, 2008, Psychol. Neurosci., V1, P141, DOI 10.1590/S1983-32882008000200008; Katkovnik V, 2010, INT J COMPUT VISION, V86, P1, DOI 10.1007/s11263-009-0272-7; Kervrann C, 2007, LECT NOTES COMPUT SC, V4485, P520; Kervrann C, 2006, IEEE T IMAGE PROCESS, V15, P2866, DOI 10.1109/TIP.2006.877529; Kindermann S, 2005, MULTISCALE MODEL SIM, V4, P1091, DOI 10.1137/050622249; Kortum P, 1996, P SOC PHOTO-OPT INS, V2657, P350, DOI 10.1117/12.238732; Kowler E, 2011, VISION RES, V51, P1457, DOI 10.1016/j.visres.2010.12.014; Krishna BS, 2014, J VISION, V14, DOI 10.1167/14.1.6; La Torre D, 2009, SIAM J IMAGING SCI, V2, P470, DOI 10.1137/070696763; Lebrun M, 2013, SIAM J IMAGING SCI, V6, P1665, DOI 10.1137/120874989; Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092; LEVICK WR, 1982, J PHYSIOL-LONDON, V329, P243, DOI 10.1113/jphysiol.1982.sp014301; Levin A, 2012, LECT NOTES COMPUT SC, V7576, P73, DOI 10.1007/978-3-642-33715-4_6; Li YP, 2008, LECT NOTES COMPUT SC, V5304, P344; Lou YF, 2009, LECT NOTES COMPUT SC, V5716, P62; Louchet C, 2011, SIAM J IMAGING SCI, V4, P651, DOI 10.1137/100785855; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lyu SW, 2009, IEEE T PATTERN ANAL, V31, P693, DOI 10.1109/TPAMI.2008.107; Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324; Mahmoudi M, 2005, IEEE SIGNAL PROC LET, V12, P839, DOI 10.1109/LSP.2005.859509; Maleki A, 2013, APPL COMPUT HARMON A, V35, P452, DOI 10.1016/j.acha.2012.11.003; Manjon-Herrera J.V., 2008, NONLOCAL MEANS FILTE; Martinez-Conde S, 2013, NAT REV NEUROSCI, V14, P83, DOI 10.1038/nrn3405; McCamy MB, 2012, J NEUROSCI, V32, P9194, DOI 10.1523/JNEUROSCI.0515-12.2012; Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329; Moghaddam B, 2007, IEEE I CONF COMP VIS, P2073, DOI 10.1109/cvpr.2007.383092; Monaco JP, 2009, INT J COMPUT VISION, V85, P192, DOI 10.1007/s11263-009-0230-4; Mosseri I, 2013, IEEE INT CONF COMPUT; Olmedo-Paya Andres, 2013, Natural and Artificial Models in Computation and Biology. 5th International Work-Conference on the Interplay Between Natural and Artificial Computation, IWINAC 2013. Proceedings, Part I: LNCS 7930, P332, DOI 10.1007/978-3-642-38637-4_34; Orchard J, 2008, IEEE IMAGE PROC, P1732, DOI 10.1109/ICIP.2008.4712109; Osindero S, 2006, NEURAL COMPUT, V18, P381, DOI 10.1162/089976606775093936; Peyre G, 2009, J MATH IMAGING VIS, V34, P17, DOI 10.1007/s10851-008-0120-3; Poletti M, 2013, CURR BIOL, V23, P1691, DOI 10.1016/j.cub.2013.07.007; Postec S., 2012, THESIS; Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067; Ranzato M, 2013, IEEE T PATTERN ANAL, V35, P2206, DOI 10.1109/TPAMI.2013.29; Rosenholtz R., 2012, JOV, V12; Rosman G., 2013, COMPUTER GRAPHICS FO; Roth S, 2005, PROC CVPR IEEE, P860; Rucci M, 2007, NATURE, V447, P851, DOI 10.1038/nature05866; Salmon J, 2010, IEEE SIGNAL PROC LET, V17, P269, DOI 10.1109/LSP.2009.2038954; Sasaki Y., 2006, JOV, V6, P916; Sasaki Y, 2006, NEURON, V51, P661, DOI 10.1016/j.neuron.2006.07.021; Sigman M, 2001, P NATL ACAD SCI USA, V98, P1935, DOI 10.1073/pnas.031571498; Strasburger H., 2011, JOV, V11; Sutour C, 2014, IEEE T IMAGE PROCESS, V23, P3506, DOI 10.1109/TIP.2014.2329448; Tasdizen T, 2009, IEEE T IMAGE PROCESS, V18, P2649, DOI 10.1109/TIP.2009.2028259; Thaipanich T, 2010, IEEE T CONSUM ELECTR, V56, P2623, DOI 10.1109/TCE.2010.5681149; TOET A, 1992, VISION RES, V32, P1349, DOI 10.1016/0042-6989(92)90227-A; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; TONG F, 1995, IEEE T PATTERN ANAL, V17, P500, DOI 10.1109/34.391393; Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302; WALLACE RS, 1994, INT J COMPUT VISION, V13, P71, DOI 10.1007/BF01420796; Wandell B.A., 1995, FDN VISION; Wang J, 2006, IEEE IMAGE PROC, P1429, DOI 10.1109/ICIP.2006.312698; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang Z, 2006, SIGN PROC COMMUN SER, V28, P431; Wang Z, 2001, IEEE T IMAGE PROCESS, V10, P1397, DOI 10.1109/83.951527; Wei J, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P866, DOI 10.1109/IROS.1998.727309; Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009; WEIMAN CFR, 1979, COMPUT VISION GRAPH, V11, P197, DOI 10.1016/0146-664X(79)90089-3; Wertheim T., 1894, Z PSYCHOL, V7, P172; Williams DR, 1996, VISION RES, V36, P1103, DOI 10.1016/0042-6989(95)00182-4; WODNICKI R, 1995, PROCEEDINGS OF THE IEEE 1995 CUSTOM INTEGRATED CIRCUITS CONFERENCE, P357, DOI 10.1109/CICC.1995.518202; Yaroslavsky LP., 1985, DIGITAL PICTURE PROC; Zhang D, 2002, IEEE T CIRC SYST VID, V12, P331, DOI 10.1109/TCSVT.2002.1003472	136	20	22	0	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2016	120	1					78	110		10.1007/s11263-016-0898-1	http://dx.doi.org/10.1007/s11263-016-0898-1			33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DU3DX		Green Submitted			2022-12-18	WOS:000382092100006
J	Vondrick, C; Khosla, A; Pirsiavash, H; Malisiewicz, T; Torralba, A				Vondrick, Carl; Khosla, Aditya; Pirsiavash, Hamed; Malisiewicz, Tomasz; Torralba, Antonio			Visualizing Object Detection Features	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Feature visualization; Visual recognition; Object detection	IMAGE SUPERRESOLUTION; COUPLED DICTIONARY	We introduce algorithms to visualize feature spaces used by object detectors. Our method works by inverting a visual feature back to multiple natural images. We found that these visualizations allow us to analyze object detection systems in new ways and gain new insight into the detector's failures. For example, when we visualize the features for high scoring false alarms, we discovered that, although they are clearly wrong in image space, they often look deceptively similar to true positives in feature space. This result suggests that many of these false alarms are caused by our choice of feature space, and supports that creating a better learning algorithm or building bigger datasets is unlikely to correct these errors without improving the features. By visualizing feature spaces, we can gain a more intuitive understanding of recognition systems.	[Vondrick, Carl; Khosla, Aditya; Torralba, Antonio] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Pirsiavash, Hamed] Univ Maryland Baltimore Cty, Baltimore, MD 21250 USA; [Malisiewicz, Tomasz] Vis Ai, Burlington, VT 05401 USA	Massachusetts Institute of Technology (MIT); University System of Maryland; University of Maryland Baltimore County	Vondrick, C (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	vondrick@mit.edu; khosla@mit.edu; hpirsiav@umbc.edu; tom@vision.ai; torralba@mit.edu			NSF GRFP; Google Ph.D. Fellowship; Facebook fellowship; Google research award; ONR MURI [N000141010933]; NSF Career Award [0747120]	NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD)); Google Ph.D. Fellowship(Google Incorporated); Facebook fellowship(Facebook Inc); Google research award(Google Incorporated); ONR MURI(MURIOffice of Naval Research); NSF Career Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	We thank the CSAIL Vision Group for important discussions. Funding was provided by a NSF GRFP and Google Ph.D. Fellowship to CV, a Facebook fellowship to AK, and a Google research award, ONR MURI N000141010933 and NSF Career Award No. 0747120 to AT.	Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715; Biggio B., 2012, 29 INT C MACH LEARN, P1807; Bruckner D., 2014, MI O SCOPE DIAGNOSTI; Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56; Chen CY, 2014, PROC CVPR IEEE, P2011, DOI 10.1109/CVPR.2014.258; d'Angelo E, 2012, INT C PATT RECOG, P935; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Divvala SK, 2012, LECT NOTES COMPUT SC, V7585, P31, DOI 10.1007/978-3-642-33885-4_4; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Everingham M., 2005, PASCAL VISUAL OBJECT; FELZENSZWALB PF, 2010, PROC CVPR IEEE, P2241, DOI DOI 10.1109/CVPR.2010.5539906; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gosselin F, 2003, PSYCHOL SCI, V14, P505, DOI 10.1111/1467-9280.03452; Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33; Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25; Huang DA, 2013, IEEE I CONF COMP VIS, P2496, DOI 10.1109/ICCV.2013.310; Jain A. K., 1989, FUNDAMENTALS DIGITAL; Jia Y., 2013, CAFFE OPEN SOURCE CO; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kato H, 2014, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2014.127; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu LQ, 2012, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2012.6248103; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Nishimoto S, 2011, CURR BIOL, V21, P1641, DOI 10.1016/j.cub.2011.08.031; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Parikh D., 2011, NIPS WCSSWC, V2, P7; Parikh D, 2010, PROC CVPR IEEE, P2328, DOI 10.1109/CVPR.2010.5539920; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sadeghi M. A., 2013, P ADV NEUR INF PROC, P2949; Tatu A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1410, DOI 10.1109/ICCVW.2011.6130416; Vondrick C., 2015, ADV NEURAL INFORM PR, P289; Vondrick C, 2013, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2013.8; Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930; Weinzaepfel P, 2011, PROC CVPR IEEE, P337, DOI 10.1109/CVPR.2011.5995616; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang L, 2014, PROC CVPR IEEE, P1266, DOI 10.1109/CVPR.2014.165; Zhu XX, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.80; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	45	20	22	2	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	2					145	158		10.1007/s11263-016-0884-7	http://dx.doi.org/10.1007/s11263-016-0884-7			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FA		Green Published, Green Submitted			2022-12-18	WOS:000380269600003
J	Souly, N; Shah, M				Souly, Nasim; Shah, Mubarak			Visual Saliency Detection Using Group Lasso Regularization in Videos of Natural Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual saliency; Sparse coding; Super-voxels; Group lasso	SPARSE; ATTENTION; SELECTION; MODEL; REGRESSION; FRAMEWORK; PREDICT	Visual saliency is the ability of a vision system to promptly select the most relevant data in the scene and reduce the amount of visual data that needs to be processed. Thus, its applications for complex tasks such as object detection, object recognition and video compression have attained interest in computer vision studies. In this paper, we introduce a novel unsupervised method for detecting visual saliency in videos of natural scenes. For this, we divide a video into non-overlapping cuboids and create a matrix whose columns correspond to intensity values of these cuboids. Simultaneously, we segment the video using a hierarchical segmentation method and obtain super-voxels. A dictionary learned from the feature data matrix of the video is subsequently used to represent the video as coefficients of atoms. Then, these coefficients are decomposed into salient and non-salient parts. We propose to use group lasso regularization to find the sparse representation of a video, which benefits from grouping information provided by super-voxels and extracted features from the cuboids. We find saliency regions by decomposing the feature matrix of a video into low-rank and sparse matrices by using robust principal component analysis matrix recovery method. The applicability of our method is tested on four video data sets of natural scenes. Our experiments provide promising results in terms of predicting eye movement using standard evaluation methods. In addition, we show our video saliency can be used to improve the performance of human action recognition on a standard dataset.	[Souly, Nasim; Shah, Mubarak] Ctr Res Comp Vis, 4000 Cent Florida Blvd, Orlando, FL 32816 USA		Souly, N (corresponding author), Ctr Res Comp Vis, 4000 Cent Florida Blvd, Orlando, FL 32816 USA.	nsouly@eecs.ucf.edu; shah@crcv.ucf.edu		Shah, Mubarak/0000-0001-6172-5572	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center [D11PC20066]	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center	This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20066. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.	Bach FR, 2008, J MACH LEARN RES, V9, P1179; Borji A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.85; Borji A, 2013, VISION RES, V91, P62, DOI 10.1016/j.visres.2013.07.016; Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89; Bruce N. D. B., 2005, ADV NEURAL INF PROCE, P155; Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5; Dorr M, 2010, J VISION, V10, DOI 10.1167/10.10.28; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Frintrop S, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658355; Gao D., 2004, ADV NEURAL INFORM PR, V17, P481; Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27; Gao DS, 2009, NEURAL COMPUT, V21, P239, DOI 10.1162/neco.2009.11-06-391; Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893; Guo C., 2008, P CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587715; Hae Jong Seo, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P45, DOI 10.1109/CVPR.2009.5204207; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Itti L., 2005, IEEE C COMP VIS PATT; Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007; Jiewei Wang, 2011, Proceedings of the Sixth International Conference on Image and Graphics (ICIG 2011), P777, DOI 10.1109/ICIG.2011.43; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kastner S, 2000, ANNU REV NEUROSCI, V23, P315, DOI 10.1146/annurev.neuro.23.1.315; Kienzle W., 2007, ADV NEURAL INFORM PR; Kienzle W, 2007, LECT NOTES COMPUT SC, V4713, P405; Koch K, 2006, CURR BIOL, V16, P1428, DOI 10.1016/j.cub.2006.05.056; Lan T., 2011, INT C COMP VIS ICCV; Laptev I., 2008, IEEE C COMP VIS PATT; Lin Z., 2010, ARXIV PREPRINT ARXIV; Liu J., 2009, SLEP SPARSE LEARNING; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Ma Y. F., 2002, ACM INT C MULT MULTI; Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410; Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112; Mairal J, 2012, SPAMS SPARSE MODELIN; Mairal J, 2010, J MACH LEARN RES, V11, P19; MALLAT S, 2009, WAV TOUR SIGN PROC, P1; Marat S., 2007, EUR SIGN PROC C; Marat S, 2009, INT J COMPUT VISION, V82, P231, DOI 10.1007/s11263-009-0215-3; Marszalek M., 2009, IEEE C COMP VIS PATT; Mathe S., 2012, TECHNICAL REPORT; Mathe S., 2012, IEEE EUR C COMP VIS; Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x; Navalpakkam V., 2006, IEEE C COMP VIS PATT; Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Poirier FJAM, 2008, J VISION, V8, DOI 10.1167/8.15.14; Qin Z., 2010, MATH PROGRAMMING COM, V5, P143; Rensink RA, 1997, PSYCHOL SCI, V8, P368, DOI 10.1111/j.1467-9280.1997.tb00427.x; Rodriguez M. D., 2008, IEEE INT C COMP VIS; Roth V., 2008, INT C MACH LEARN, V104; Rubinstein R., 2010, P IEEE; Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477; Rudoy D, 2013, PROC CVPR IEEE, P1147, DOI 10.1109/CVPR.2013.152; Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Triesch J, 2003, J VIS, V3, P9, DOI DOI 10.1167/3.1.9; Vig E, 2012, IEEE T PATTERN ANAL, V34, P1080, DOI 10.1109/TPAMI.2011.198; Vig E, 2011, COGN COMPUT, V3, P79, DOI 10.1007/s12559-010-9061-4; Wang H, 2011, PROC CVPR IEEE; Wright Y., 2009, ADV NEURAL INFORM PR, V22, DOI DOI 10.5555/2984093.2984326; Xu CL, 2012, PROC CVPR IEEE, P1202, DOI 10.1109/CVPR.2012.6247802; Yan JC, 2010, IEEE SIGNAL PROC LET, V17, P739, DOI 10.1109/LSP.2010.2053200; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zhai Y., 2006, PROC14TH ACM INT C M, DOI [10.1145/1180639.1180824, DOI 10.1145/1180639.1180824]; Zhang L., 2009, P 31 ANN COGN SCI C, P2944; Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32; Zhong S. -H., 2013, AAAI; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	68	20	23	1	24	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2016	117	1					93	110		10.1007/s11263-015-0853-6	http://dx.doi.org/10.1007/s11263-015-0853-6			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DF2FG					2022-12-18	WOS:000371156500005
J	Baltieri, D; Vezzani, R; Cucchiara, R				Baltieri, Davide; Vezzani, Roberto; Cucchiara, Rita			Mapping Appearance Descriptors on 3D Body Models for People Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						People re-identification; 3D human model; SARC3D; 3D appearance model	PEDESTRIAN RECOGNITION; TRACKING	People Re-identification aims at associating multiple instances of a person's appearance acquired from different points of view, different cameras, or after a spatial or a limited temporal gap to the same identifier. The basic hypothesis is that the person's appearance is mostly constant. Many appearance descriptors have been adopted in the past, but they are often subject to severe perspective and view-point issues. In this paper, we propose a complete re-identification framework which exploits non-articulated 3D body models to spatially map appearance descriptors (color and gradient histograms) into the vertices of a regularly sampled 3D body surface. The matching and the shot integration steps are directly handled in the 3D body model, reducing the effects of occlusions, partial views or pose changes, which normally afflict 2D descriptors. A fast and effective model to image alignment is also proposed. It allows operation on common surveillance cameras or image collections. A comprehensive experimental evaluation is presented using the benchmark suite 3DPeS.	[Baltieri, Davide; Vezzani, Roberto; Cucchiara, Rita] Univ Modena & Reggio Emilia, Dipartimento Ingn Enzo Ferrari, I-41125 Modena, Italy	Universita di Modena e Reggio Emilia	Vezzani, R (corresponding author), Univ Modena & Reggio Emilia, Dipartimento Ingn Enzo Ferrari, Via Vignolese 905, I-41125 Modena, Italy.	davide.baltieri@gmail.com; roberto.vezzani@unimore.it; rita.cucchiara@unimore.it	Vezzani, Roberto/K-9070-2015; Cucchiara, Rita/L-3006-2015	Vezzani, Roberto/0000-0002-1046-6870; Cucchiara, Rita/0000-0002-2239-283X				Albu AB, 2006, INT C PATT RECOG, P924; Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156; Bak Slawomir, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P435, DOI 10.1109/AVSS.2010.34; Bak S, 2012, IMAGE VISION COMPUT, V30, P443, DOI 10.1016/j.imavis.2011.08.008; Baltieri D., 2011, P 2011 JOINT ACMWORK, P59, DOI DOI 10.1145/2072572.2072590; Baltieri D., 2012, P 12 EUR C COMP VIS; Baltieri D., 2010, P EUR IT CHAPT C 201; Baltieri D, 2011, LECT NOTES COMPUT SC, V6978, P197, DOI 10.1007/978-3-642-24085-0_21; Barbosa IB, 2012, LECT NOTES COMPUT SC, V7583, P433, DOI 10.1007/978-3-642-33863-2_43; Bird ND, 2005, IEEE T INTELL TRANSP, V6, P167, DOI 10.1109/TITS.2005.848370; Cai Q, 1999, IEEE T PATTERN ANAL, V21, P1241, DOI 10.1109/34.809119; Calderara S, 2008, IEEE T PATTERN ANAL, V30, P354, DOI 10.1109/TPAMI.2007.70814; Colombo C, 2008, PARALLEL COMPUT, V34, P718, DOI 10.1016/j.parco.2008.09.004; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dikmen M, 2011, LECT NOTES COMPUT SC, V6495, P501, DOI 10.1007/978-3-642-19282-1_40; Doretto G, 2011, J AMB INTEL HUM COMP, V2, P127, DOI 10.1007/s12652-010-0034-y; Enzweiler M, 2010, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2010.5540110; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Gandhi T., 2006, P IEEE INT C VID SIG, P78, DOI DOI 10.1109/AVSS.2006.90; Gandhi T, 2007, MACH VISION APPL, V18, P207, DOI 10.1007/s00138-006-0063-x; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224; Gilbert A, 2006, LECT NOTES COMPUT SC, V3952, P125; Gong S., 2014, ADV COMPUTER VISION, VXVIII; Grana C., 2007, CIVR 07, P302; Gray D., 2007, P 10 IEEE INT WORKSH; Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21; Gualdi G, 2010, LECT NOTES COMPUT SC, V6316, P196, DOI 10.1007/978-3-642-15567-3_15; Hartley R., 2004, ROBOTICA; Hellinger E, 1909, J REINE ANGEW MATH, V136, P210, DOI 10.1515/crll.1909.136.210; Hirzer M, 2012, LECT NOTES COMPUT SC, V7577, P780, DOI 10.1007/978-3-642-33783-3_56; Javed O, 2005, PROC CVPR IEEE, P26; Kai Jungling, 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P197, DOI 10.1109/AVSS.2011.6027319; Kotoulas L, 2003, IEE P-CIRC DEV SYST, V150, P387, DOI 10.1049/ip-cds:20030481; Krumm J, 2000, THIRD IEEE INTERNATIONAL WORKSHOP ON VISUAL SURVEILLANCE, PROCEEDINGS, P3, DOI 10.1109/VS.2000.856852; Monari E, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P134, DOI 10.1109/AVSS.2009.16; Nilski A, 2008, 42ND ANNUAL 2008 IEEE INTERNATIONAL CARNAHAN CONFERENCE ON SECURITY TECHNOLOGY, PROCEEDINGS, P277, DOI 10.1109/CCST.2008.4751314; Porikli F, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P133; Roullot E, 2008, INT CONF SYST SIGNAL, P97, DOI 10.1109/IWSSIP.2008.4604376; Schuegerl P, 2007, LECT NOTES COMPUT SC, V4577, P305; Schwartz W. R., 2009, P 22 BRAZ S COMP GRA; Vezzani R, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543596; Vezzani R, 2011, PATTERN RECOGN LETT, V32, P867, DOI 10.1016/j.patrec.2010.11.003; Vezzani R, 2010, MULTIMED TOOLS APPL, V50, P359, DOI 10.1007/s11042-009-0402-9; Vezzani Roberto, 2014, BENCHMARKING PERSON, P333, DOI [10.1007/978-1-4471-6296-4_16, DOI 10.1007/978-1-4471-6296-4_16]; Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598	46	20	21	0	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2015	111	3					345	364		10.1007/s11263-014-0747-z	http://dx.doi.org/10.1007/s11263-014-0747-z			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CB9XK					2022-12-18	WOS:000349987400005
J	Pal, CJ; Weinman, JJ; Tran, LC; Scharstein, D				Pal, Christopher J.; Weinman, Jerod J.; Tran, Lam C.; Scharstein, Daniel			On Learning Conditional Random Fields for Stereo Exploring Model Structures and Approximate Inference	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo; Learning; Structured prediction; Approximate inference	ENERGY MINIMIZATION; MAP ESTIMATION	Until recently, the lack of ground truth data has hindered the application of discriminative structured prediction techniques to the stereo problem. In this paper we use ground truth data sets that we have recently constructed to explore different model structures and parameter learning techniques. To estimate parameters in Markov random fields (MRFs) via maximum likelihood one usually needs to perform approximate probabilistic inference. Conditional random fields (CRFs) are discriminative versions of traditional MRFs. We explore a number of novel CRF model structures including a CRF for stereo matching with an explicit occlusion model. CRFs require expensive inference steps for each iteration of optimization and inference is particularly slow when there are many discrete states. We explore belief propagation, variational message passing and graph cuts as inference methods during learning and compare with learning via pseudolikelihood. To accelerate approximate inference we have developed a new method called sparse variational message passing which can reduce inference time by an order of magnitude with negligible loss in quality. Learning using sparse variational message passing improves upon previous approaches using graph cuts and allows efficient learning over large data sets when energy functions violate the constraints imposed by graph cuts.	[Pal, Christopher J.] Ecole Polytech, Montreal, PQ H3C 3A7, Canada; [Weinman, Jerod J.] Grinnell Coll, Dept Comp Sci, Grinnell, IA 50112 USA; [Tran, Lam C.] Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92103 USA; [Scharstein, Daniel] Middlebury Coll, Middlebury, VT 05753 USA	Universite de Montreal; Polytechnique Montreal; University of California System; University of California San Diego	Pal, CJ (corresponding author), Ecole Polytech, Montreal, PQ H3C 3A7, Canada.	christopher.pal@polymtl.ca; weinman@grinnell.edu; lat003@ucsd.edu; schar@middlebury.edu			Google; Microsoft; NSERC; NSF [0413169]	Google(Google Incorporated); Microsoft(Microsoft); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); NSF(National Science Foundation (NSF))	This research was supported in part by: a Google Research award, Microsoft Research through awards under the eScience and Memex funding programs, a gift from Kodak Research and an NSERC discovery award to C. P. Support was also provided in part by NSF grant 0413169 to D.S.	Alvarez L, 2002, J VIS COMMUN IMAGE R, V13, P3, DOI 10.1006/jvci.2001.0482; Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; [Anonymous], 2006, INTRO STAT RELATIONA; BARNARD ST, 1989, INT J COMPUT VISION, V3, P17, DOI 10.1007/BF00054836; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Blake A, 2004, LECT NOTES COMPUT SC, V3021, P428; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bleyer M., 2004, P ICIP; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Cowell R. G., 2003, PROBABILISTIC NETWOR; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; Frey B., 1997, P NIPS; Frey BJ, 2005, IEEE T PATTERN ANAL, V27, P1392, DOI 10.1109/TPAMI.2005.169; He XM, 2004, PROC CVPR IEEE, P695; Hong L, 2004, PROC CVPR IEEE, P74; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82; Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1480, DOI 10.1109/TPAMI.2006.193; Kong D., 2004, P BMVC; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Kumar S, 2006, INT J COMPUT VISION, V68, P179, DOI 10.1007/s11263-006-7007-9; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Liang P., 2008, P ICML; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Ng AY, 2002, P NIPS; PAL C, 2006, INT CONF ACOUST SPEE, P581; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scharstein D, 2003, PROC CVPR IEEE, P195; Scharstein D., 2007, P CVPR; Strecha C, 2004, PROC CVPR IEEE, P552; Strecha C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1194; Sun J, 2005, PROC CVPR IEEE, P399; Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509; Sutton C., 2004, P ICML; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tao H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P532, DOI 10.1109/ICCV.2001.937562; Tappen MF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P900; Trinh H., 2009, P BMVC; Vishwanathan S.V.N., 2006, P 23 INT C MACH LEAR, P969, DOI DOI 10.1145/1143844.1143966; Wainwright M., 2002, P NIPS; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Wainwright MJ, 2003, IEEE T INFORM THEORY, V49, P1120, DOI 10.1109/TIT.2003.810642; Wei YC, 2004, PROC CVPR IEEE, P106; Weinman J, 2004, MACHINE LEARN SIGN P, P549; Weinman J. J., 2007, UMCS2007054; Weinman J. J., 2008, P ECCV; Weinman JJ, 2009, IEEE T PATTERN ANAL, V31, P1733, DOI 10.1109/TPAMI.2009.38; Winn J, 2005, J MACH LEARN RES, V6, P661; Yang Q., 2006, P CVPR; Yedidia J., 2003, EXPLORING ARTIFICIAL, V8, P236; Zhang L, 2005, PROC CVPR IEEE, P288; Zhang Y, 2002, LECT NOTES COMPUT SC, V2351, P556; Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766	56	20	23	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2012	99	3					319	337		10.1007/s11263-010-0385-z	http://dx.doi.org/10.1007/s11263-010-0385-z			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	950NI					2022-12-18	WOS:000304655600005
J	Marszalek, M; Schmid, C				Marszalek, Marcin; Schmid, Cordelia			Accurate Object Recognition with Shape Masks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape masks; Object recognition; Object segmentation; Local features; Bag-of-features; Graz-02	SCALE	In this paper we propose an object recognition approach that is based on shape masks-generalizations of segmentation masks. As shape masks carry information about the extent (outline) of objects, they provide a convenient tool to exploit the geometry of objects. We apply our ideas to two common object class recognition tasks-classification and localization. For classification, we extend the orderless bag-of-features image representation. In the proposed setup shape masks can be seen as weak geometrical constraints over bag-of-features. Those constraints can be used to reduce background clutter and help recognition. For localization, we propose a new recognition scheme based on high-dimensional hypothesis clustering. Shape masks allow to go beyond bounding boxes and determine the outline (approximate segmentation) of the object during localization. Furthermore, the method easily learns and detects possible object viewpoints and articulations, which are often well characterized by the object outline. Our experiments reveal that shape masks can improve recognition accuracy of state-of-the-art methods while returning richer recognition answers at the same time. We evaluate the proposed approach on the challenging natural-scene Graz-02 object classes dataset.	[Marszalek, Marcin; Schmid, Cordelia] INRIA Grenoble, LEAR LJK, F-38330 Montbonnot St Martin, France		Marszalek, M (corresponding author), INRIA Grenoble, LEAR LJK, 665 Av Europe, F-38330 Montbonnot St Martin, France.	Marcin.Marszalek@inrialpes.fr; Cordelia.Schmid@inrialpes.fr			European Community; European Network of Excellence PASCAL	European Community(European Commission); European Network of Excellence PASCAL	M. Marszalek was supported by a grant from the European Community under the Marie-Curie project VISITOR. This work was supported by the European Network of Excellence PASCAL.	Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108; AGARWAL S, 2002, ECCV; Borenstein E., 2002, ECCV; Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646; Csurka G., 2004, ECCV WORKSHOPS; Dorko G., 2003, ICCV; EVERINGHAM M, 2006, SEL P 1 PASCAL CHALL; Everingham M, 2009, PASCAL VISUAL OBJECT; Everingham M., 2008, PASCAL VOC 08 CHALL; Fergus R, 2007, INT J COMPUT VISION, V71, P273, DOI 10.1007/s11263-006-8707-x; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; FUSSENEGGER M, 2006, ICPR; Galleguillos C., 2008, ECCV; Garding J, 1996, INT J COMPUT VISION, V17, P163, DOI 10.1007/BF00058750; Grauman K., 2005, ICCV; HAYMAN E, 2004, ECCV; Jing F., 2003, ICME; LAZEBNIK S, 2005, ICCV; Leibe B., 2005, CVPR; Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3; Li L., 2009, CVPR; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lyu S., 2005, CVPR; Marr D., 1982, VISION; Marszalek M., 2007, CVPR; MARSZALEK M, 2006, CVPR; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Opelt A, 2006, IEEE T PATTERN ANAL, V28, P416, DOI 10.1109/TPAMI.2006.54; Opelt A., 2005, SCIA; Opelt A., 2004, ECCV; OPELT A, 2004, TREMT200401 TU GRAZ; Peters J, 2017, ADAPT COMPUT MACH LE; PETERSON MA, 1994, CURR DIR PSYCHOL SCI, V3, P105, DOI 10.1111/1467-8721.ep10770552; Ramanan D., 2007, CVPR; Rothganger F., 2003, CVPR; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Russell B. C., 2006, CVPR; SEEMANN E, 2006, DAGM; Seemann E., 2006, CVPR; Shotton J., 2008, CVPR; Shotton J., 2005, ICCV; Sivic J., 2005, ICCV; Sivic J., 2003, ICCV; Thomas A., 2006, P IEEE C COMP VIS PA; Todorovic S., 2006, CVPR; Vecera SP, 1998, J EXP PSYCHOL HUMAN, V24, P441, DOI 10.1037/0096-1523.24.2.441; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Winn J. M., 2005, ICCV; Wu B., 2007, CVPR; YU S, 2003, CVPR; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4; [No title captured]	55	20	22	1	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2012	97	2					191	209		10.1007/s11263-011-0479-2	http://dx.doi.org/10.1007/s11263-011-0479-2			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	897RV					2022-12-18	WOS:000300674200004
J	Kokkinos, I; Yuille, A				Kokkinos, Iasonas; Yuille, Alan			Inference and Learning with Hierarchical Shape Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Inference; Learning; Hierarchy; Contours; Grouping; Deformable models; Shape; Parsing	IMAGE; SEGMENTATION; CONTOUR	In this work we introduce a hierarchical representation for object detection. We represent an object in terms of parts composed of contours corresponding to object boundaries and symmetry axes; these are in turn related to edge and ridge features that are extracted from the image. We propose a coarse-to-fine algorithm for efficient detection which exploits the hierarchical nature of the model. This provides a tractable framework to combine bottom-up and top-down computation. We learn our models from training images where only the bounding box of the object is provided. We automate the decomposition of an object category into parts and contours, and discriminatively learn the cost function that drives the matching of the object to the image using Multiple Instance Learning. Using shape-based information, we obtain state-of-the-art localization results on the UIUC and ETHZ datasets.	[Kokkinos, Iasonas] INRIA Saclay, Equipe Galen, Orsay, France; [Kokkinos, Iasonas] Ecole Cent Paris, Dept Appl Math, Paris, France; [Yuille, Alan] Univ Calif Los Angeles, Dept Comp Sci & Stat, Los Angeles, CA USA	UDICE-French Research Universities; Universite Paris Saclay; University of California System; University of California Los Angeles	Kokkinos, I (corresponding author), Ecole Cent Paris, Dept Appl Math, Paris, France.	iasonas.kokkinos@ecp.fr; yuille@stat.ucla.edu		Yuille, Alan L./0000-0001-5207-9249	W.M. Keck Foundation; NSF [613563]	W.M. Keck Foundation(W.M. Keck Foundation); NSF(National Science Foundation (NSF))	This work was supported by the W.M. Keck Foundation and NSF Grant 613563. We thank the Reviewers for their constructive feedback, which helped improve the presentation of the paper.	Agrawal S., 2002, ECCV; Ahuja N., 2007, ICCV; Amit Y, 1996, IEEE T PATTERN ANAL, V18, P225, DOI 10.1109/34.485529; Andrews S., 2002, NIPS; [Anonymous], 2008, CVPR; ARKIN EM, 1991, IEEE T PATTERN ANAL, V13, P209, DOI 10.1109/34.75509; Birkhoff, 1958, LATTICE THEORY, V64, P50; Borenstein E., 2002, ECCV; Chen Y., 2007, NIPS; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Collins M., 2002, EMNLP; Crandall D., 2005, CVPR; Csurka G., 2004, ECCV; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Dollar P., 2008, ECCV; Felzenszwalb P., 2008, CVPR; Felzenszwalb PF, 2007, J ARTIF INTELL RES, V29, P153, DOI 10.1613/jair.2187; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; FELZENSZWALB PF, 2007, CVPR; Fergus R., 2003, CVPR; FERGUS R, 2005, CVPR; Ferrari V., 2006, ECCV; Ferrari V., 2007, CVPR; Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144; Fidler S., 2008, CVPR; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Gehler P., 2007, AISTATS; Grimson E., 1991, OBJECT RECOGNITION C; Han F., 2005, ICCV; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Hopcroft J.E., 2006, INTRO AUTOMATA THEOR; Ioffe S, 2001, INT J COMPUT VISION, V43, P45, DOI 10.1023/A:1011179004708; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; Jiang T., 2009, CVPR; Jin Y., 2006, CVPR; Keselman Y, 2005, IEEE T PATTERN ANAL, V27, P1141, DOI 10.1109/TPAMI.2005.139; Kokkinos I., 2009, STOCH IM GRAMM WORKS; Kokkinos I., 2008, CVPR; Kokkinos I., 2006, CVPR; KOKKINOS I, 2007, ICCV; Kokkinos I, 2009, IEEE T PATTERN ANAL, V31, P1486, DOI 10.1109/TPAMI.2008.158; Lampert C., 2008, CVPR; Leibe B., 2004, ECCV SLCV WORKSH; Lempitsky V., 2008, ECCV; Leonardis A., 2007, CVPR; Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773; Lowe D. G., 1984, PERCEPTUAL ORG VISUA; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; Marr D., 1982, VISION; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; MOOSMANN F, 2006, NIPS; Moreels P, 2004, LECT NOTES COMPUT SC, V3021, P55; Mumford D, 1993, ALGEBRAIC GEOMETRY I, P507; Opelt A., 2006, CVPR; Parikh D., 2009, CVPR; Pearl Judea, 1984, HEURISTICS; Porway J., 2008, OBJECT CATEGORIZATIO; Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124; Ramanan D., 2006, CVPR; Russell B. C., 2006, CVPR; Russell G., 1979, IJCAI; Russell SJ, 1995, ARTIF INTELL, V4th; Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215; Sharon E, 2000, IEEE T PATTERN ANAL, V22, P1117, DOI 10.1109/34.879792; Shotton J., 2006, ECCV; Shotton J., 2005, ICCV; SIDDIQI K, 1995, IEEE T PATTERN ANAL, V17, P239, DOI 10.1109/34.368189; Sivic J., 2005, ICCV; Sudderth E. B., 2003, CVPR; Sudderth Erik B, 2005, ICCV; Taskar B., 2004, EMNLP04; Todorovic S., 2008, CVPR; Todorovic S., 2006, CVPR; Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x; Viola P., 2001, P 2001 IEEE COMP SOC, pI, DOI [10.1109/CVPR.2001.990517, DOI 10.1109/CVPR.2001.990517]; Welling M., 2000, ECCV; WU YN, 2007, ICCV; Zhang C., 2006, REPORTS GOING ABROAD; Zhu L., 2008, CVPR; Zhu L., 2008, ECCV; Zhu Q. H., 2008, ECCV; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018; [No title captured]	90	20	22	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2011	93	2					201	225		10.1007/s11263-010-0398-7	http://dx.doi.org/10.1007/s11263-010-0398-7			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	740PD		Bronze			2022-12-18	WOS:000288806000006
J	He, ZY; Chung, ACS				He, Zhenyu; Chung, Albert C. S.			3-D B-spline Wavelet-Based Local Standard Deviation (BWLSD): Its Application to Edge Detection and Vascular Segmentation in Magnetic Resonance Angiography	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Magnetic resonance angiography (MRA); Vascular segmentation; B-spline wavelet; Local standard deviation; FLUX	BLOOD-VESSELS; IMAGES; SCALE; PROPAGATION; SNAKES; SYSTEM	Extracting reliable image edge information is crucial for active contour models as well as vascular segmentation in magnetic resonance angiography (MRA). However, conventional edge detection techniques, such as gradient-based methods and wavelet-based methods, are incapable of returning reliable detection responses from low contrast edges in the images. In this paper, we propose a novel edge detection method by combining B-spline wavelet magnitude with standard deviation inside local region. It is proved theoretically and demonstrated experimentally in this paper that the new edge detection method, namely BWLSD, is able to give consistent and reliable strengths for edges with different image contrasts. Moreover, the relationship between the size of local region with non-zero wavelet magnitudes and the scale of wavelet function is established. This relationship indicates that if the scale of the adopted wavelet function is s, then the size of a local region, from which the standard deviation is estimated, should be 2s-1. The proposed edge detection technique is embedded in FLUX, namely, BWLSD-FLUX, for vascular segmentation in MRA image volumes. Experimental results on clinical images show that, as compared with the conventional FLUX, BWLSD-FLUX can achieve better segmentations of vasculatures in MRA images under same initial conditions.	[He, Zhenyu; Chung, Albert C. S.] Hong Kong Univ Sci & Technol, Lo Kwee Seong Med Image Anal Lab, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	He, ZY (corresponding author), Hong Kong Univ Sci & Technol, Lo Kwee Seong Med Image Anal Lab, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.	csezyhe@cse.ust.hk; achung@cse.ust.hk	Chung, Albert C. S./GZB-0224-2022					AIHW, 2004, HEART STROK VASC DIS; Ambrosio L, 1996, J DIFFER GEOM, V43, P693; Armande N, 1999, COMPUT VIS IMAGE UND, V73, P248, DOI 10.1006/cviu.1998.0658; Aylward SR, 2002, IEEE T MED IMAGING, V21, P61, DOI 10.1109/42.993126; Bombardier V, 1997, PROCEEDINGS OF THE SIXTH IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOLS I - III, P1057, DOI 10.1109/FUZZY.1997.622856; Bullitt E, 2001, MED IMAGE ANAL, V5, P157, DOI 10.1016/S1361-8415(01)00037-8; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; *CDC, 2008, CHR DIS OV; Chen J, 1998, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.1998.698625; Chwialkowski MP, 1996, COMPUT MED IMAG GRAP, V20, P365, DOI 10.1016/S0895-6111(96)00010-9; COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675; Duncan JS, 2000, IEEE T PATTERN ANAL, V22, P85, DOI 10.1109/34.824822; Eiho S, 1997, COMPUT CARDIOL, V24, P525, DOI 10.1109/CIC.1997.647950; FIGUEIREDO MAT, 1995, IEEE T MED IMAGING, V14, P162, DOI 10.1109/42.370413; Foracchia M, 2005, MED IMAGE ANAL, V9, P179, DOI 10.1016/j.media.2004.07.001; Frangi AF, 1998, LECT NOTES COMPUT SC, V1496, P130, DOI 10.1007/BFb0056195; Gooya A, 2008, IEEE T IMAGE PROCESS, V17, P1295, DOI 10.1109/TIP.2008.925378; Guo DB, 1998, COMPUT CARDIOL, V25, P441, DOI 10.1109/CIC.1998.731897; He J, 2005, NEW ENGL J MED, V353, P1124, DOI 10.1056/NEJMsa050467; Higgins WE, 1996, IEEE T MED IMAGING, V15, P377, DOI 10.1109/42.500146; Holtzman-Gazit M, 2006, IEEE T IMAGE PROCESS, V15, P354, DOI [10.1109/TIP.2005.860624, 10.1109/tip.2005.860624]; Hoover A, 2000, IEEE T MED IMAGING, V19, P203, DOI 10.1109/42.845178; Huang Q., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P104, DOI 10.1109/CVPR.1993.340972; Hunter IA, 1995, COMPUT CARDIOL, P741, DOI 10.1109/CIC.1995.482771; Ibanez L., 2005, ITK SOFTWARE GUIDE I; KASS M, 1988, INT J COMPUT VISION, V1, P1573; Kirbas C, 2004, ACM COMPUT SURV, V36, P81, DOI 10.1145/1031120.1031121; KRISSIAN K, 1996, P 1 C SCAL SPAC THEO, P345; Law MWK, 2007, IEEE T MED IMAGING, V26, P1224, DOI 10.1109/TMI.2007.903231; LEROY B, 1996, P 12 INT C AN OPT SY, P58; Lorigo LM, 2001, MED IMAGE ANAL, V5, P195, DOI 10.1016/S1361-8415(01)00040-8; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; MALLAT S, 1992, IEEE T PATTERN ANAL, V14, P710, DOI 10.1109/34.142909; MALLAT S, 1992, IEEE T INFORM THEORY, V38, P617, DOI 10.1109/18.119727; Mallat S., 1999, WAVELET TOUR SIGNAL; McInerney T, 2000, MED IMAGE ANAL, V4, P73, DOI 10.1016/S1361-8415(00)00008-6; Mendonca AM, 2006, IEEE T MED IMAGING, V25, P1200, DOI 10.1109/TMI.2006.879955; OBRIEN J, 1994, P C VIS BIOM COMP, V25, P25; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; PARVIN B, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P295, DOI 10.1109/CVPR.1994.323843; Poli R, 1997, COMPUT METH PROG BIO, V52, P1, DOI 10.1016/S0169-2607(96)01773-7; PRINET V, 1995, P IEEE 17 C ENG MED, P393; QUEK F, 1999, IEEE T INF TECHNOL B, V3, P139; REKOVEI R, 1995, IEEE T NEURAL NETWOR, V6, P64; Rinkel GJE, 1998, STROKE, V29, P251, DOI 10.1161/01.STR.29.1.251; Sato Y, 1998, Med Image Anal, V2, P143, DOI 10.1016/S1361-8415(98)80009-1; Schmitt H, 2002, IEEE T MED IMAGING, V21, P251, DOI 10.1109/42.996343; Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591; SMETS C, 1988, PATTERN RECOGN LETT, V8, P113, DOI 10.1016/0167-8655(88)90052-9; Sorantin E, 2002, IEEE T MED IMAGING, V21, P263, DOI 10.1109/42.996344; Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627; Tang YY, 2000, IEEE T SYST MAN CY B, V30, P93, DOI 10.1109/3477.826950; Vasilevskiy A, 2002, IEEE T PATTERN ANAL, V24, P1565, DOI 10.1109/TPAMI.2002.1114849; Wang YP, 1998, IEEE T PATTERN ANAL, V20, P1040, DOI 10.1109/34.722612; Whitaker RT, 1998, INT J COMPUT VISION, V29, P203, DOI 10.1023/A:1008036829907; Wong WCK, 2007, MED IMAGE ANAL, V11, P567, DOI 10.1016/j.media.2007.05.003; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; Yan P, 2006, MED IMAGE ANAL, V10, P317, DOI 10.1016/j.media.2005.12.002; Yim PJ, 2000, IEEE T MED IMAGING, V19, P568, DOI 10.1109/42.870662; ZIJDENBOS AP, 1994, IEEE T MED IMAGING, V13, P716, DOI 10.1109/42.363096	61	20	20	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	87	3					235	265		10.1007/s11263-009-0256-7	http://dx.doi.org/10.1007/s11263-009-0256-7			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	551KA		Green Submitted			2022-12-18	WOS:000274205700003
J	Joshi, N; Brady, M				Joshi, Niranjan; Brady, Michael			Non-Parametric Mixture Model Based Evolution of Level Sets and Application to Medical Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Non-parametric probability density functions; Finite mixture models; Curve evolution; Level sets	ACTIVE CONTOURS; SEGMENTATION; REGISTRATION; FRAMEWORK; MOTION	We present a novel region-based curve evolution algorithm which has three primary contributions: (i) non-parametric estimation of probability distributions using the recently developed NP windows method; (ii) an inequality-constrained least squares method to model the image histogram with a mixture of nonparametric probability distributions; and (iii) accommodation of the partial volume effect, which is primarily due to low resolution images, and which often poses a significant challenge in medical image analysis (our primary application area). We first approximate the image intensity histogram as a mixture of non-parametric probability density functions (PDFs), justifying its use with respect to medical image analysis. The individual densities in the mixture are estimated using the recent NP windows PDF estimation method, which builds a continuous representation of discrete signals. A Bayesian framework is then formulated in which likelihood probabilities are given by the non-parametric PDFs and prior probabilities are calculated using an inequality constrained least squares method. The non-parametric PDFs are then learnt and the segmentation solution is spatially regularised using a level sets framework. The log ratio of the posterior probabilities is used to drive the level set evolution. As background to our approach, we recall related developments in level set methods. Results are presented for a set of synthetic and natural images as well as simulated and real medical images of various anatomical organs. Results on a range of images show the effectiveness of the proposed algorithm.	[Joshi, Niranjan; Brady, Michael] Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	University of Oxford	Joshi, N (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	njoshi@robots.ox.ac.uk; jmb@robots.ox.ac.uk			Prof. Tony Hey and Prof. Andrew Blake of Microsoft Research	Prof. Tony Hey and Prof. Andrew Blake of Microsoft Research	The authors thank Wenjia Bai for providing 3D simulated PET data. They thank the anonymous reviewers for their comments, which have substantially improved the paper. They also thank Prof. Tony Hey and Prof. Andrew Blake of Microsoft Research for funding and for providing useful comments.	Bai WJ, 2009, PHYS MED BIOL, V54, P2719, DOI 10.1088/0031-9155/54/9/008; BOND S, 2007, P INF PROC MED IM IP; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Cremers D, 2007, INT J COMPUT VISION, V72, P195, DOI 10.1007/s11263-006-8711-1; Dowson N, 2008, IEEE T PATTERN ANAL, V30, P1841, DOI 10.1109/TPAMI.2007.70832; Felsberg M, 2001, IEEE T SIGNAL PROCES, V49, P3136, DOI 10.1109/78.969520; Ibanez L., 2005, INSIGHT SEGMENTATION; *ITK, 2005, INS SEGM REG TOOLK; IZENMAN AJ, 1991, J AM STAT ASSOC, V86, P205, DOI 10.2307/2289732; JOSHI N, 2007, THESIS U OXFORD; JOSHI NB, 2007, P INT C COMP THEOR A, P618; JOSHI NB, 2008, ADV INTELLIGENT INFO; JOSHI NB, 2005, P BRIT MACH VIS C BM; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kim JM, 2005, IEEE T IMAGE PROCESS, V14, P1486, DOI 10.1109/TIP.2005.854442; Michailovich O, 2007, IEEE T IMAGE PROCESS, V16, P2787, DOI 10.1109/TIP.2007.908073; MUMFORD D, 1990, IMAGE UNDERSTANDING, pCH2; Noe A, 2002, LECT NOTES COMPUT SC, V2488, P698; Papoulis A, 2002, PROBABILITY RANDOM V, V4th; Paragios N, 2002, J VIS COMMUN IMAGE R, V13, P249, DOI 10.1006/jvci.2001.0475; PARAGIOS NK, 2000, THESIS U NICE SOPHIA; RATHI Y, 2006, INT C SIGN IM PROC S; Reilhac A, 2004, IEEE T NUCL SCI, V51, P46, DOI 10.1109/TNS.2003.823011; Segars WP, 2002, IEEE T NUCL SCI, V49, P675, DOI 10.1109/TNS.2002.1039548; Sethian J. A., 1999, LEVEL SET METHODS FA; SIJBERS J, 1998, THESIS U ANTWERP BEL; Van Leemput K, 2003, IEEE T MED IMAGING, V22, P105, DOI 10.1109/TMI.2002.806587; Zeydabadi M, 2004, 2004 2ND IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: MACRO TO NANO, VOLS 1 and 2, P165; Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343	31	20	22	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	88	1					52	68		10.1007/s11263-009-0290-5	http://dx.doi.org/10.1007/s11263-009-0290-5			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	571LS		Green Submitted			2022-12-18	WOS:000275753900003
J	Xiang, T; Cheong, LF				Xiang, T; Cheong, LF			Understanding the behavior of SFM algorithms: A geometric approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; error analysis; epipolar constraint; inherent ambiguity; depth distortion	STRUCTURE-FROM-MOTION; RECOVERING 3-D MOTION; INHERENT AMBIGUITIES; LOCAL AMBIGUITIES; OPTICAL-FLOW	We put forth in this paper a geometrically motivated motion error analysis which is capable of supporting investigation of global effect such as inherent ambiguities. This is in contrast with the usual statistical kinds of motion error analyses which can only deal with local effect such as noise perturbations, and where much of the results regarding global ambiguities are empirical in nature. The error expression that we derive allows us to predict the exact conditions likely to cause ambiguities and how these ambiguities vary with motion types such as lateral or forward motion. Given the erroneous 3-D motion estimates caused by the inherent ambiguities, it is also important to study the behavior of the resultant distortion in depth recovered under different motion-scene configurations. Such an investigation may alert us to the occurrence of ambiguities under different conditions and be more careful in picking the solution. Our formulation, though geometrically motivated, was also put to use in modeling the effect of noise and in revealing the strong influence of feature distribution. Experiments on both synthetic and real image sequences were conducted to verify the various theoretical predictions.	Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119260, Singapore	National University of Singapore	Xiang, T (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, 10 Kent Ridge Crescent, Singapore 119260, Singapore.	txiang@dcs.qmul.ac.uk; eleclf@nus.edu.sg						ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; Brooks MJ, 1997, J OPT SOC AM A, V14, P2670, DOI 10.1364/JOSAA.14.002670; BROOKS MJ, 1998, P C ECCV, P283; Cheong L, 1998, COMPUT VIS IMAGE UND, V71, P356, DOI 10.1006/cviu.1997.0649; Cheong LF, 1999, INT J COMPUT VISION, V32, P195, DOI 10.1023/A:1008105012585; Cheong LF, 2001, INT J COMPUT VISION, V44, P199, DOI 10.1023/A:1012224215211; CHEONG LF, 2000, P C ECCV DUBL IR, V1, P664; Chiuso A, 2000, INT J COMPUT VISION, V39, P195, DOI 10.1023/A:1026563712076; Daniilidis Kostas, 1997, VISUAL NAVIGATION BI; Dutta R., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P106, DOI 10.1109/ICCV.1990.139504; Fermuller C, 2001, COMPUT VIS IMAGE UND, V82, P1, DOI 10.1006/cviu.2000.0900; Fermuller C, 2000, INT J COMPUT VISION, V37, P43, DOI 10.1023/A:1008177429387; Grossmann E, 2000, IMAGE VISION COMPUT, V18, P685, DOI 10.1016/S0262-8856(99)00072-4; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; Hoffman D., 1998, VISUAL INTELLIGENCE; HORN BKP, 1987, INT J COMPUT VISION, V1, P259, DOI 10.1007/BF00127824; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; KAHL F, 1995, P INT C COMP VIS, P469; KANATANI K, 1993, INT J COMPUT VISION, V11, P267, DOI 10.1007/BF01469345; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; KOENDERINK JJ, 1995, IMAGE VISION COMPUT, V13, P321, DOI 10.1016/0262-8856(95)99719-H; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LONGUETHIGGINS HC, 1984, PROC R SOC SER B-BIO, V223, P165, DOI 10.1098/rspb.1984.0088; Lucas B., 1984, THESIS CARNEGIE MELL; Ma Y, 2000, INT J COMPUT VISION, V36, P71, DOI 10.1023/A:1008124507881; Ma Y, 2001, INT J COMPUT VISION, V44, P219, DOI 10.1023/A:1012276232049; MAYBANK S, 1993, THEORY RECONSTRUCTIO; NEGAHDARIPOUR S, 1989, INT J COMPUT VISION, V3, P293, DOI 10.1007/BF00132601; Oliensis J, 2000, IEEE T PATTERN ANAL, V22, P685, DOI 10.1109/34.865186; Oliensis J, 2000, COMPUT VIS IMAGE UND, V80, P172, DOI 10.1006/cviu.2000.0869; OLIENSIS J, 2001, ERROR SURFACE STRUCT; PRAZDNY K, 1980, BIOL CYBERN, V36, P87, DOI 10.1007/BF00361077; RIEGER JH, 1985, J OPT SOC AM A, V2, P354, DOI 10.1364/JOSAA.2.000354; Soatto S, 1998, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.1998.698621; SPETSAKIS M, 1994, CVGIP-IMAG UNDERSTAN, V60, P300, DOI 10.1006/ciun.1994.1059; Sturm P, 1997, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.1997.609467; Szeliski R, 1997, IEEE T PATTERN ANAL, V19, P506, DOI 10.1109/34.589211; Weng J., 1991, MOTION STRUCTURE IMA; XIANG T, 2001, THESIS NATL U SINGAP; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903; ZHANG T, 1999, P IEEE C COMP VIS PA, P164; Zhang ZY, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P772, DOI 10.1109/ICCV.1998.710805	43	20	21	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2003	51	2					111	137		10.1023/A:1021627622971	http://dx.doi.org/10.1023/A:1021627622971			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	625GR					2022-12-18	WOS:000179809200002
J	Shum, HY; Szeliski, R				Shum, HY; Szeliski, R			Construction of panoramic image mosaics with global and local alignment (vol 36, pg 101, 2000)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									Microsoft Corp, Res, Redmond, WA 98052 USA	Microsoft	Shum, HY (corresponding author), Microsoft Corp, Res, Redmond, WA 98052 USA.							Shum HY, 2000, INT J COMPUT VISION, V36, P101, DOI 10.1023/A:1008195814169; Szeliski R, 1996, IEEE COMPUT GRAPH, V16, P22, DOI 10.1109/38.486677	2	20	22	0	24	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2002	48	2					151	152		10.1023/A:1016051024520	http://dx.doi.org/10.1023/A:1016051024520			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	567AZ		Bronze			2022-12-18	WOS:000176461600005
J	Betke, M; Makris, NC				Betke, M; Makris, NC			Recognition, resolution, and complexity of objects subject to affine transformations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object recognition; object complexity; coherence scale; coherence volume; recognition ambiguity; Fisher information; bandwidth; statistical estimation theory; lower bounds on estimation error; background models; traffic sign recognition; simulated annealing	INTENSITY; SCINTILLATION	The problem of recognizing objects subject to affine transformation in images is examined from a physical perspective using the theory of statistical estimation. Focusing first on objects that occlude zero-mean scenes with additive noise, we derive the Cramer-Rao lower bound on the mean-square error in an estimate of the six-dimensional parameter vector that describes an object subject to affine transformation and so generalize the bound on one-dimensional position error previously obtained in radar and sonar pattern recognition. We then derive two useful descriptors from the object's Fisher information that are independent of noise level. The first is a generalized coherence scale that has great practical value because it corresponds to the width of the object's autocorrelation peak under affine transformation and so provides a physical measure of the extent to which an object can be resolved under affine parameterization. The second is a scalar measure of an object's complexity that is invariant under affine transformation and can be used to quantitatively describe the ambiguity level of a general 6-dimensional affine recognition problem. This measure of complexity has a strong inverse relationship to the level of recognition ambiguity. We then develop a method for recognizing objects subject to affine transformation imaged in thousands of complex real-world scenes. Our method exploits the resolution gain made available by the brightness contrast between the object perimeter and the scene it partially occludes. The level of recognition ambiguity is shown to decrease exponentially with increasing object and scene complexity. Ambiguity is then avoided by conditioning the permissible range of template complexity above a priori thresholds. Our method is statistically optimal for recognizing objects that occlude scenes with zero-mean background.	Boston Univ, Boston, MA 02215 USA; MIT, Cambridge, MA 02139 USA	Boston University; Massachusetts Institute of Technology (MIT)	Betke, M (corresponding author), Boston Univ, Boston, MA 02215 USA.	betke@cs.bu.edu						Ballard D.H., 1982, COMPUTER VISION; Betke M, 1997, IEEE T ROBOTIC AUTOM, V13, P251, DOI 10.1109/70.563647; Betke M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P145, DOI 10.1109/ICCV.1998.710712; BETKE M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P523, DOI 10.1109/ICCV.1995.466895; BETKE M, 1997, CARTR858 U MAR; CERNUSCHIFRIAS B, 1989, IEEE T PATTERN ANAL, V11, P1028, DOI 10.1109/34.42835; CHIN RT, 1986, COMPUT SURV, V18, P67, DOI 10.1145/6462.6464; DIFRANCO JV, 1968, RADAR DETECTION; DOWNIE JD, 1994, J OPT SOC AM A, V11, P1599, DOI 10.1364/JOSAA.11.001599; FRIEDLAND NS, 1991, 2779 U MAR; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; Goodman J. W, 2000, STAT OPTICS; GOODMAN JW, 1965, PR INST ELECTR ELECT, V53, P1688, DOI 10.1109/PROC.1965.4341; Horn B., 1986, ROBOT VISION, P1; Jain R., 1995, MACHINE VISION; KASHIOKA S, 1976, IEEE T SYST MAN CYB, V6, P562, DOI 10.1109/TSMC.1976.4309551; Kay S.M, 1993, STAT SIGNAL PROCESSI; KELLEY RB, 1983, P IEEE, V71, P803, DOI 10.1109/PROC.1983.12680; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; LEVANON N, 1988, RADAR PRINCIPLES; MAKRIS NC, 1995, J ACOUST SOC AM, V97, P3547, DOI 10.1121/1.412440; Makris NC, 1996, J ACOUST SOC AM, V100, P769, DOI 10.1121/1.416239; MAKRIS NC, 1995, OPT LETT, V20, P2012, DOI 10.1364/OL.20.002012; METROPOLIS N, 1953, J CHEM PHYS C, V21, P734; POYNTON CA, 1993, SMPTE J, V102, P1099, DOI 10.5594/J01651; Rao C. R, 1973, LINEAR STAT INFERENC; ROSENFELD A, 1982, DIGITAL PICTURE PROC, V2; Strang G., 1976, LINEAR ALGEBRA APPL; STRENSKI PN, 1991, ALGORITHMICA, V6, P346, DOI 10.1007/BF01759050; SZU H, 1987, PHYS LETT A, V122, P157, DOI 10.1016/0375-9601(87)90796-1; Trucco E., 1998, INTRO TECHNIQUES 3D; Umbaugh SE., 1998, COMPUTER VISION IMAG; VAN HL, 1968, DETECTION ESTIMATI 1; YOSHIMURA S, 1994, P INT C INT ROB SYST; ZADEH LA, 1952, P IRE, V40, P1223, DOI 10.1109/JRPROC.1952.274117	35	20	20	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2001	44	1					5	40		10.1023/A:1011168302294	http://dx.doi.org/10.1023/A:1011168302294			36	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	464WT					2022-12-18	WOS:000170556900001
J	Callari, FG; Ferrie, FP				Callari, FG; Ferrie, FP			Active object recognition: Looking for differences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						active vision; control of perception; learning in computer vision	RANGE IMAGES; UNCERTAINTY; EXPLORATION; CONTEXT	This paper introduces an information-based methodology for view selection that actively exploits prior knowledge about the objects to be found in a scene. The methodology is used to implement an active recognition strategy which effectively puts prior constraints from the object database into the gaze control (planning) loop. Theoretical results are presented and discussed along with promising experimental data.	McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3X 2A7, Canada	McGill University	Callari, FG (corresponding author), McGill Univ, Ctr Intelligent Machines, 3480 Univ St, Montreal, PQ H3X 2A7, Canada.							ALOIMONOS J, 1987, P 1 INT C COMP VIS L; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; Barr A. H., 1981, IEEE COMPUT GRAPH, V1, P1, DOI [DOI 10.1109/MCG.1981.1673788, 10.1109/MCG.1981.1673788]; Bishop, 1995, NEURAL NETWORKS PATT; BRIDLE JS, 1990, NATO ASI F, V68; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DICKINSON S, 1994, P EUR C COMP VIS STO, pB3; Duda R.O., 1973, J ROYAL STAT SOC SER; Fedorov V.V., 1972, THEORY OPTIMAL EXPT; Golub G.H., 2013, MATRIX COMPUTATIONS, P357; GROSS A, 1988, P INT C COMP VIS, P690; HARALICK RM, 1983, IEEE T PATTERN ANAL, V5, P417, DOI 10.1109/TPAMI.1983.4767411; Lejeune A, 1996, COMPUT VIS IMAGE UND, V64, P230, DOI 10.1006/cviu.1996.0056; MACKAY DJC, 1992, NEURAL COMPUT, V4, P698; MAVER J, 1993, IEEE T PATTERN ANAL, V15, P417, DOI 10.1109/34.211463; Pito R., 1996, P 13 INT C PATT REC; Ripley BD., 1996; SOLINA F, 1990, IEEE T PATTERN ANAL, V12, P131, DOI 10.1109/34.44401; SOUCY G, 1996, COMPUTER VISION IMAG, V64, P1; STRAT TM, 1991, IEEE T PATTERN ANAL, V13, P1050, DOI 10.1109/34.99238; Whaite P, 1997, IEEE T PATTERN ANAL, V19, P193, DOI 10.1109/34.584097; Whaite P, 1997, IEEE T PATTERN ANAL, V19, P899, DOI 10.1109/34.608292; WHAITE P, 1991, IEEE T PATTERN ANAL, V13, P1038, DOI 10.1109/34.99237; WHAITE P, 1993, P 4 INT C COMP VIS B, P41; Wilkes D., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P136, DOI 10.1109/CVPR.1992.223215; [No title captured]	27	20	20	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2001	43	3					189	204		10.1023/A:1011135513777	http://dx.doi.org/10.1023/A:1011135513777			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	457WY					2022-12-18	WOS:000170161400004
J	Astrom, K; Cipolla, R; Giblin, P				Astrom, K; Cipolla, R; Giblin, P			Generalised epipolar constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						curved surface; epipolar geometry; frontier point; uncalibrated camera; apparent contour; silhouette; motion extraction	SURFACE; MOTION; CURVE	In this paper we will discuss structure and motion problems for curved surfaces. These will be studied using the silhouettes or apparent contours in the images. The problem of determining camera motion from the apparent contours of curved three-dimensional surfaces, is studied. It will be shown how special points, called epipolar tangency points or frontier points, can be used to solve this problem. A generalised epipolar constraint is introduced, which applies to points, curves, as well as to apparent contours of surfaces. The theory is developed for both continuous and discrete motion, known and unknown orientation, calibrated and uncalibrated, perspective, weak perspective and orthographic cameras. Results of an iterative scheme to recover the epipolar line structure from real image sequences using only the outlines of curved surfaces, is presented. A statistical evaluation is performed to estimate the stability of the solution. It is also shown how the motion of the camera from a sequence of images can be obtained from the relative motion between image pairs.	Lund Univ, Dept Math, Lund, Sweden; Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England; Univ Liverpool, Dept Pure Math, Liverpool L69 3BX, Merseyside, England	Lund University; University of Cambridge; University of Liverpool	Astrom, K (corresponding author), Lund Univ, Dept Math, Lund, Sweden.		Arandjelović, Ognjen/V-5255-2019; Åström, Kalle/C-2836-2009; Astrom, Kalle/AAT-9538-2020	Arandjelović, Ognjen/0000-0002-9314-194X; Åström, Kalle/0000-0002-8689-7810; Astrom, Kalle/0000-0002-8689-7810				[Anonymous], 1959, PHOTOGRAMM REC; ASTROM K, 1996, LECT NOTES COMPUTER, V1065, P97; ASTROM K, 1999, ADV APPL PROBABILITY; ASTROM K, 1996, THESIS LUND U SWEDEN; Blake A., 1992, ACTIVE VISION; Bruce J.W., 1992, CURVES SINGULARITIES; CARLSSON S, 1994, P 3 EUR C COMP VIS S, V1, P83; CIPOLLA R, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P269, DOI 10.1109/ICCV.1995.466775; Cipolla R., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P616, DOI 10.1109/ICCV.1990.139606; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; CIPOLLA R, 1995, LNCS, V1016; CIPOLLA R, 1991, THESIS U OXFORD; CIPOLLA R, 1996, INT J COMPUTER VISIO; Coxeter H.S.M., 1993, REAL PROJECTIVE PLAN, V3rd; Curwen R, 1992, ACTIVE VISION, P39; FAUGERAS OD, 1992, P 2 EUR C COMP VIS S, P563; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Fletcher G. J., 1996, THESIS LIVERPOOL U; Giblin P., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P136; GIBLIN PJ, 1994, J OPT SOC AM A, V11, P1976, DOI 10.1364/JOSAA.11.001976; GIBLIN PJ, 1995, IMAGE VISION COMPUT, V13, P33, DOI 10.1016/0262-8856(95)91466-Q; Heyden A, 1997, IMAGE VISION COMPUT, V15, P749, DOI 10.1016/S0262-8856(97)00005-X; JOSHI T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P290, DOI 10.1109/ICCV.1995.466927; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Koenderink J., 1990, SOLID SHAPE; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Luenberger D.G, 2016, LINEAR NONLINEAR PRO, DOI 10.1007/978-3-319-18842-3; Luong Q.-T., 1993, RR1894 INRIA; LUONG QT, 1994, P 3 EUR C COMP VIS S, P589; PORRILL J, 1991, IMAGE VISION COMPUT, V9, P45, DOI 10.1016/0262-8856(91)90048-T; RIEGER JH, 1986, OPT LETT, V11, P123, DOI 10.1364/OL.11.000123; Sinclair D., 1995, P SCIA, P181; Stefanovic P, 1973, ITC J, V3, P417; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; Vijayakumar B, 1996, PROC CVPR IEEE, P327, DOI 10.1109/CVPR.1996.517093; VIJAYAKUMAR B, 1995, P 5 INT C COMP VIS, P508	37	20	20	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1999	33	1					51	72		10.1023/A:1008113231241	http://dx.doi.org/10.1023/A:1008113231241			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	229HL					2022-12-18	WOS:000082187800003
J	Lu, JP; Little, JJ				Lu, JP; Little, JJ			Reflectance and shape from images using a collinear light source	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image sequence; surface reflectance; photometric and geometric constraints; integration of visual information; surface recovery	PHOTOMETRIC-STEREO; 3-DIMENSIONAL SHAPE; SURFACES; MODEL; MAP; GRADIENT; VISION	In this paper, a novel technique, called the photogeometric technique, is presented for surface reflectance extraction and surface recovery from an image sequence of a rotating object illuminated under a collinear light source (where the illuminant direction of the light source lies on or near the viewing direction of the camera). The rotation of the object is precisely controlled. The object surface is assumed to be smooth and uniform. The technique first computes the 3D locations of some surface points which give singular brightness values and builds the surface reflectance function by extracting the brightness values of these surface points from the image sequence. Then the technique uses the surface reflectance function and two images of the surface to recover surface depth and orientation simultaneously. The technique has been tested on real images of surfaces with different reflectance properties and geometric structures. The experimental results and comprehensive analysis show that the proposed technique is efficient and robust.	Univ British Columbia, Dept Comp Sci, Lab Computat Intelligence, Vancouver, BC V6T 1Z4, Canada	University of British Columbia	Lu, JP (corresponding author), Raytheon Syst Canada Ltd, Syst Div, 13951 Bridgeport Rd, Richmond, BC V6T 1J5, Canada.	jiping_lu@raytheon.com						Bichsel M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P459, DOI 10.1109/CVPR.1992.223150; BOYER E, 1996, P EUR C COMP VIS, P109; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; CHIARADIA MT, 1989, OPT ENG, V28, P935, DOI 10.1117/12.7977065; CHRISTENSEN PH, 1994, INT J COMPUT VISION, V13, P213, DOI 10.1007/BF01427152; Clark J.J., 1990, DATA FUSION SENSORY; COLEMAN EN, 1982, COMPUT VISION GRAPH, V18, P309, DOI 10.1016/0146-664X(82)90001-6; Dupuis P., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P453, DOI 10.1109/CVPR.1992.223151; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; FUA P, 1994, P 3 EUR C COMP VIS, P282; GRIMSON WEL, 1982, AI697 MIT; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1979, APPL OPTICS, V18, P1770, DOI 10.1364/AO.18.001770; HORN BKP, 1981, P IEEE, V69, P14, DOI 10.1109/PROC.1981.11918; HORN BKP, 1977, ARTIF INTELL, V8, P201, DOI 10.1016/0004-3702(77)90020-0; HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3; HORN BKP, 1990, INT J COMPUT VISION, V5, P37, DOI 10.1007/BF00056771; HOUGEN DR, 1993, P INT C COMP VIS, P148; IKEUCHI K, 1987, INT J ROBOT RES, V6, P15, DOI 10.1177/027836498700600102; Leclerc Y. G., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P552, DOI 10.1109/CVPR.1991.139752; LEE CH, 1985, ARTIF INTELL, V26, P125, DOI 10.1016/0004-3702(85)90026-8; LEE KM, 1993, IEEE T PATTERN ANAL, V15, P815, DOI 10.1109/34.236247; LEE S, 1991, IMAGE VISION COMPUT, V9, P39, DOI 10.1016/0262-8856(91)90047-S; LU J, 1997, TR9705 UBC DEP COMP; *MATL, 1993, HIGH PERF NUM COMP V; NAYAR SK, 1990, IEEE T ROBOTIC AUTOM, V6, P418, DOI 10.1109/70.59367; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; NAYAR SK, 1990, P IM UND WORKSH SEPT, P185; OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684; PANKANTI S, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P316, DOI 10.1109/CVPR.1994.323846; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P170, DOI 10.1109/TPAMI.1984.4767501; POGGIO T, 1988, SCIENCE, V242, P436, DOI 10.1126/science.3175666; Silver William M, 1980, THESIS MIT; Solomon F., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P466, DOI 10.1109/CVPR.1992.223149; Szeliski R., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P625, DOI 10.1109/CVPR.1991.139764; SZELISKI R, 1994, REAL TIME COMPUTER V, P141; Tagare H. D., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P340, DOI 10.1109/ICCV.1990.139546; TAGARE HD, 1991, IEEE T PATTERN ANAL, V13, P133, DOI 10.1109/34.67643; WADA T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P66, DOI 10.1109/ICCV.1995.466805; Wei GQ, 1997, IEEE T PATTERN ANAL, V19, P353, DOI 10.1109/34.588016; WOLFF L, 1996, P ECCV, P40; WOLFF LB, 1994, J OPT SOC AM A, V11, P2956, DOI 10.1364/JOSAA.11.002956; WOLFF LB, 1994, J OPT SOC AM A, V11, P3069, DOI 10.1364/JOSAA.11.003069; WOODHAM RJ, 1994, J OPT SOC AM A, V11, P3050, DOI 10.1364/JOSAA.11.003050; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; WOODHAM RJ, 1989, IEEE T ROBOTIC AUTOM, P36; Zheng J. Y., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P777, DOI 10.1109/CVPR.1992.223175; ZHENG JY, 1994, IEEE T PATTERN ANAL, V16, P163, DOI 10.1109/34.273734; ZHENG JY, 1995, P ICCV, P72; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658	50	20	23	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1999	32	3					213	240		10.1023/A:1008157029424	http://dx.doi.org/10.1023/A:1008157029424			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	229HK					2022-12-18	WOS:000082187500003
J	Gunn, SR; Nixon, MS				Gunn, SR; Nixon, MS			Global and local active contours for head boundary extraction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	13th ICPR Meeting	AUG, 1996	VIENNA, AUSTRIA			active contours; head boundary extraction; energy minimisation; dynamic programming	DEFORMABLE CONTOURS; RECOGNITION	Active contours are an attractive choice to extract the head boundary, for deployment within a face recognition or model-based coding scenario. However, conventional snake approaches can suffer difficulty in initialisation and parameterisation. A dual active contour configuration using dynamic programming has been developed to resolve these difficulties by using a global energy minimisation technique and a simplified parameterisation, to enable a global solution to be obtained. The merits of conventional gradient descent based snake (local) approaches, and search-based (global) approaches are discussed. In application to find head and face boundaries in front-view face images, the new technique employing dynamic programming is deployed to extract the inner face boundary, along with a conventional normal-driven contour to extract the outer (head) boundary. The extracted contours appear to offer sufficient discriminatory capability for inclusion within an automatic face recognition system.	Univ Southampton, Dept Elect & Comp Sci, ISIS Grp, Southampton SO17 1BJ, Hants, England	University of Southampton	Gunn, SR (corresponding author), Univ Southampton, Dept Elect & Comp Sci, ISIS Grp, Southampton SO17 1BJ, Hants, England.	S.R.Gunn@ecs.soton.ac.uk; msn@ecs.soton.ac.uk	Nixon, Mark S/F-7406-2014	Nixon, Mark/0000-0002-9174-5934				AMINI AA, 1990, IEEE T PATTERN ANAL, V12, P855, DOI 10.1109/34.57681; COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675; GEIGER D, 1995, IEEE T PATTERN ANAL, V17, P294, DOI 10.1109/34.368194; GUNN SR, 1994, BMVC94 - PROCEEDINGS OF THE 5TH BRITISH MACHINE VISION CONFERENCE, VOLS 1 AND 2, P305; HUANG CL, 1992, PATTERN RECOGN, V25, P1435, DOI 10.1016/0031-3203(92)90118-3; JIA XG, 1995, IEEE T PATTERN ANAL, V17, P1167, DOI 10.1109/34.476509; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; LAI KF, 1995, IEEE T PATTERN ANAL, V17, P1084, DOI 10.1109/34.473235; LAM KM, 1994, 1994 INT S SPEECH IM; MENET S, 1990, IEEE INT C SYST MAN, V212, P194; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681; Rueckert D, 1995, PROCEEDINGS OF THE 6TH BRITISH MACHINE VISION CONFERENCE 1995, VOLS 1 AND 2, P207; STORVIK G, 1994, IEEE T PATTERN ANAL, V16, P976, DOI 10.1109/34.329011; SUNG K, 1995, LECT NOTES COMPUTER, V970, P432; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; WAITE JB, 1990, BRIT TELECOM TECHNOL, V8, P127; WELSH WJ, 1990, BRIT TELECOM TECHNOL, V8, P94; WILLIAMS DJ, 1992, CVGIP-IMAG UNDERSTAN, V55, P14, DOI 10.1016/1049-9660(92)90003-L; XU G, 1994, PATTERN RECOGN, V27, P879, DOI 10.1016/0031-3203(94)90153-8	20	20	25	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1998	30	1					43	54		10.1023/A:1008065429466	http://dx.doi.org/10.1023/A:1008065429466			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	152CC					2022-12-18	WOS:000077757100003
J	Brodsky, T; Fermuller, C; Aloimonos, Y				Brodsky, T; Fermuller, C; Aloimonos, Y			Directions of motion fields are hardly ever ambiguous	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						qualitative vision; motion field; optic flow; egomotion		If instead of the full motion field, we consider only the direction of the motion field due to a rigid motion, what can we say about the three-dimensional motion information contained in it? This paper provides a geometric analysis of this question based solely on the constraint that the depth of the surfaces in view is positive. The motivation behind this analysis is to provide a theoretical foundation for image constraints employing only the sign of flow in various directions and justify their utilization for addressing 3D dynamic vision problems. It is shown that, considering as the imaging surface the whole sphere, independently of the scene in view, two different rigid motions cannot give rise to the same directional motion field. If we restrict the image to half of a sphere (or an infinitely large image plane) two different rigid motions with instantaneous translational and rotational velocities (t(1), omega(1)) and (t(2), omega(2)) cannot give rise to the same directional motion field unless the plane through t(1) and t(2) is perpendicular to the plane through omega(1) and omega(2) (i.e., (t(1) x t(2)) . (omega(1) x omega(2)) = 0). In addition, in order to give practical significance to these uniqueness results for the case of a limited field of view, we also characterize the locations on the image where the motion vectors due to the different motions must have different directions. If (omega(1) x omega(2)) . (t(1) x t(2)) = 0 and certain additional constraints are met, then the two rigid motions could produce motion fields with the same direction. For this to happen the depth of each corresponding surface has to be within a certain range, defined by a second and a third order surface. Similar more restrictive constraints are obtained for the case of multiple motions. Consequently, directions of motion fields are hardly ever ambiguous. A byproduct of the analysis is that full motion fields are never ambiguous with a half sphere as the imaging surface.	Univ Maryland, Ctr Automat Res, Comp Vis Lab, Dept Comp Sci, College Pk, MD 20742 USA; Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Brodsky, T (corresponding author), Univ Maryland, Ctr Automat Res, Comp Vis Lab, Dept Comp Sci, College Pk, MD 20742 USA.		Aloimonos, Yiannis/AAI-2969-2020	Aloimonos, Yiannis/0000-0002-8152-4281				ANCONA N, 1995, INT J COMPUT VISION, V14, P131, DOI 10.1007/BF01418979; Faugeras O. D., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P25; Fermuller C, 1997, INT J COMPUT VISION, V21, P223, DOI 10.1023/A:1007951901001; FERMULLER C, 1995, INT J COMPUT VISION, V15, P7, DOI 10.1007/BF01450848; FERMULLER C, 1995, INT J COMPUT VISION, V14, P147, DOI 10.1007/BF01418980; HORN BKP, 1987, INT J COMPUT VISION, V1, P259, DOI 10.1007/BF00127824; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; MAYBANK S, 1993, THEORY RECONSTRUCTIO; NEGAHDARIPOUR S, 1989, INT J COMPUT VISION, V3, P293, DOI 10.1007/BF00132601; NEGAHDARIPOUR S, 1987, IEEE T PATTERN ANAL, V9, P163; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; TISTARELLI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P108, DOI 10.1016/1049-9660(92)90089-L; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; VIEVILLE T, 1994, INT J COMPUT VISION, V13, P153, DOI 10.1007/BF01427150	15	20	20	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1998	26	1					5	24		10.1023/A:1007928406666	http://dx.doi.org/10.1023/A:1007928406666			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZC476					2022-12-18	WOS:000072583100001
J	SyedaMahmood, TF				SyedaMahmood, TF			Data and model-driven selection using color regions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							VISION; IMAGES	A key problem in model-based object recognition is selection, namely, the problem of determining which regions in the image are likely to come from a single object. In this paper we present an approach that uses color as a cue to perform selection either based solely on image-data (data-driven), or based on the knowledge of the color description of the model (model-driven). Specifically, the paper presents a method of color specification in terms of perceptual color categories and shows its relevance for the task of selection. The color categories are used to develop a fast region segmentation algorithm that extracts perceptual color regions in images. The color regions extracted form the basis for performing data and model-driven selection. Data-driven selection is achieved by selecting salient color regions as judged by a color-saliency measure that emphasizes attributes that are also important in human color perception. The approach to model-driven selection, on the other hand, exploits the color and other region information in the 3d model object to locate instances of the object in a given image. The approach presented tolerates some of the problems of occlusion, pose and illumination changes that make a model instance in an image appear different from its original description. Finally, the utility of color-based selection is demonstrated by showing the extent of search reduction possible when color-based selection is integrated with a recognition system.			SyedaMahmood, TF (corresponding author), XEROX CORP, WEBSTER RES CTR, 800 PHILLIPS RD, WEBSTER, NY 14580 USA.							[Anonymous], 2018, A A PRACT, V11, P321; [Anonymous], 1985, PERCEPTUAL ORG VISUA; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BASRI R, 1990, THESIS WEIZMANN I SC; Bolles R. C., 1982, INT J ROBOT RES, V1, P57; BOLLES RC, 1986, INT J ROBOT RES, V5, P3, DOI 10.1177/027836498600500301; BORNSTEIN MH, 1976, J EXP PSYCHOL HUMAN, V2, P115, DOI 10.1037/0096-1523.2.1.115; CARTERETTE E, 1978, PERCEPTUAL CODING; CLEMENS DT, 1991, IEEE T PATTERN ANAL, V13, P1007, DOI 10.1109/34.99235; CULHANE SM, 1992, P EUR C COMP VIS, P551; ENGEL FL, 1974, VISION RES, V14, P459, DOI 10.1016/0042-6989(74)90034-0; ENGEL FL, 1976, VISUAL CONSPICUITY E; Evans R.M., 1974, PERCEPTION COLOR; Foley J.D., 1984, FUNDAMENTALS INTERAC; FUNT B, 1988, P 2 IEEE INT C COMP, P2; GERSHON R, 1987, THESIS U TORONTO; Ghez C., 1985, VOLUNTARY MOVEMENT, P493; Grimson W. E. L., 1990, OBJECT RECOGNITION C; Horn B., 1986, ROBOT VISION, P1; HURLBERT A, 915 MIT AI; HURLBERT A, 1989, TR1154 MIT; HUTTENLOCHER DP, P INT C COMP VIS, P102; ILLINGWORTH J, 1988, COMPUT VISION GRAPH, V44, P87, DOI 10.1016/S0734-189X(88)80033-1; JACOBS DW, 1989, 1177 MIT AI; Jepson A.D., 1987, INT JOINT C ART INT, P755; KLINKER GJ, 1990, INT J COMPUT VISION, V4, P7, DOI 10.1007/BF00137441; KLINKER GJ, 1987, P INT C COMP VIS, P614; Land E.H., 1985, CENTRAL PERIPHERAL M, P5; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; LEE SW, 1992, P EUR C COMP VIS, P99; MALONEY LT, 1986, J OPT SOC AM A, V3, P29, DOI 10.1364/JOSAA.3.000029; Meyer G. W., 1980, Computer Graphics, V14, P254, DOI 10.1145/965105.807502; OHTA YI, 1980, COMPUT GRAPH IMAGE P, V13, P221; ROSCH E, 1975, J EXP PSYCHOL HUMAN, V1, P303, DOI 10.1037/0096-1523.1.4.303; SAGI D, 1985, SCIENCE, V228, P1217, DOI 10.1126/science.4001937; SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409; SHASHUA A, 1994, IEEE T PATTERN ANAL, V16, P778, DOI 10.1109/34.308472; STERNHEI.CE, 1966, J EXP PSYCHOL, V72, P770, DOI 10.1037/h0023739; Swain M. J., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P390, DOI 10.1109/ICCV.1990.139558; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487; SYEDAMAHMOOD TF, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P881, DOI 10.1109/CVPR.1994.323918; TREISMAN A, 1985, COMPUT VISION GRAPH, V31, P156, DOI 10.1016/S0734-189X(85)80004-9; TREISMAN A, 1983, PHYSICAL BIOL PROCES, P316; Tsukada M., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P385, DOI 10.1109/ICCV.1990.139557; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; ULLMAN S, 1984, COGNITION, V18, P97, DOI 10.1016/0010-0277(84)90023-4; ULLMAN S, 1989, 1152 MIT AI; WANDELL BA, 1987, IEEE T PATTERN ANAL, V9, P2, DOI 10.1109/TPAMI.1987.4767868; WIXON LE, 1989, SPIE SENSOR 2, V1198; Wyszecki Gunter, 1982, COLOR SCI, V8; [No title captured]; [No title captured]; [No title captured]	53	20	23	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1997	21	1-2					9	36		10.1023/A:1007919421801	http://dx.doi.org/10.1023/A:1007919421801			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WM797					2022-12-18	WOS:A1997WM79700002
J	Zerroug, M; Nevatia, R				Zerroug, M; Nevatia, R			Volumetric descriptions from a single intensity image	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							INTERPRETING LINE DRAWINGS; GENERALIZED CYLINDERS; CURVED OBJECTS; RECOGNITION; SURFACES; CONTOURS; SHAPE; VIEW	Since the early days of computer vision research, shape from contour has been one of the most challenging problems. Many researchers in the field have attempted to understand this problem and proposed different approaches to solve it. Shape from contour still remains one of the hardest problems in the field. The problem has two major difficulties. First, 2D properties of contours of viewed objects are generally not sufficient by themselves to uniquely determine 3D shape, as one dimension is lost in the projection. Second, real images produce imperfect contours that make their interpretation particularly difficult. The first problem has received some attention in the research community but in the context of perfect contours. The second one, however, has received very little. In this work, we propose a promising methodology to address this last problem for a large class of objects: generalized cylinders. It is based on exploiting mathematical invariant properties of the contours of generalized cylinders in a perceptual grouping approach. We show that using these properties greatly helps addressing the figure-ground problem in a more rigorous way than previous (intuitive) perceptual grouping methods. Our approach exploits the interplay between local and global features by handling different levels of the feature hierarchy. We have developed and implemented a method that handles SHGCs in complex scenes with markings and occlusion. We demonstrate the application of our method of shape description and scene segmentation on complex real images. We also demonstrate the usage of the obtained descriptions for recovery of complete 3-D object centered descriptions of viewed objects from a single intensity image.			Zerroug, M (corresponding author), UNIV SO CALIF,LOS ANGELES,CA 90089, USA.							[Anonymous], 1985, PERCEPTUAL ORG VISUA; BARROW HG, 1981, ARTIF INTELL, V17, P75, DOI 10.1016/0004-3702(81)90021-7; BARTELS DH, 1987, INTRO SPLINES USE CO; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Binford T.O., 1971, IEEE C SYST CONTR MI; BINFORD TO, 1981, ARTIF INTELL, V17, P205, DOI 10.1016/0004-3702(81)90025-4; BINFORD TO, 1991, P DARP ESPR WORKSH I; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; Do Carmo M.P., 2016, DIFFERENTIAL GEOMETR, Vsecond; DOLAN J, 1989, P IMAGE UNDERSTANDIN, P1135; GROSS A, 1990, P IM UND WORKSH PENN, P557; HORAUD R, 1988, ARTIF INTELL, V37, P333, DOI 10.1016/0004-3702(88)90059-8; HUMMEL JE, 1992, PSYCHOL REV; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; Koenderink J., 1990, SOLID SHAPE; MACKWORTH AK, 1973, ARTIF INTELL, V4, P121, DOI 10.1016/0004-3702(73)90003-9; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; MARR D, 1977, PROC R SOC SER B-BIO, V197, P441, DOI 10.1098/rspb.1977.0080; MOHAN R, 1989, JUN P IEEE C COMP VI, P333; NALWA VS, 1989, IEEE T PATTERN ANAL, V11, P1117, DOI 10.1109/34.42842; NEVATIA R, 1977, ARTIF INTELL, V8, P77, DOI 10.1016/0004-3702(77)90006-6; NEVATIA R, 1982, MACHINE PERCEPTION; NEVATIA R, 1992, P IM UND WORKSH SAN; PONCE J, 1989, IEEE T PATTERN ANAL, V11, P951, DOI 10.1109/34.35498; PONCE J, 1987, INT J COMPUT VISION, V1, P195, DOI 10.1007/BF00127820; Rao K., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P276, DOI 10.1109/CVPR.1988.196248; RICHETIN M, 1991, IEEE T PATTERN ANAL, V13, P185, DOI 10.1109/34.67647; SAINTMARC P, 1990, P EURO C COMPUT VISI, P604; SATO H, 1992, P IM UND WORKSH SAN; SHAFER SA, 1983, CS83131 CARN MELL U; SHAFER SA, 1983, CS083105 CARN MELL U; Tenenbaum Jay M, 1983, HUMAN MACHINE VISION, P481; Ulupinar F., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P674, DOI 10.1109/CVPR.1991.139777; Ulupinar F., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P582, DOI 10.1109/ICCV.1990.139600; ULUPINAR F, 1991, THESIS U SO CALIFORN; ULUPINAR F, 1992, P IM UND WORKSH SAN; ULUPINAR F, 1990, 10TH P INT C PATT RE, P147; WITKIN AP, 1981, ARTIF INTELL, V17, P17, DOI 10.1016/0004-3702(81)90019-9; ZERROUG M, 1993, P COMP VIS PATT REC; ZERROUG M, 1993, P 2 ARPA NSF ESPRIT; ZERROUG M, 1993, P 3 EUR C COMP VIS S; [No title captured]	44	20	20	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1996	20	1-2					11	42		10.1007/BF00144115	http://dx.doi.org/10.1007/BF00144115			32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VQ240					2022-12-18	WOS:A1996VQ24000002
J	Carlsson, S; Mohr, R; Moons, T; Morin, L; Rothwell, C; VanDiest, M; VanGool, L; Veillon, F; Zisserman, A				Carlsson, S; Mohr, R; Moons, T; Morin, L; Rothwell, C; VanDiest, M; VanGool, L; Veillon, F; Zisserman, A			Semi-local projective invariants for the recognition of smooth plane curves	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								Recently, several methods have been proposed for describing plane, non-algebraic curves in a projectively invariant fashion. These curve representations are invariant under changes in viewpoint and therefore ideally suited for recognition. We report the results of a study where the strengths and weaknesses of a number of semi-local methods are compared on the basis of the same images and edge data. All the methods define a distinguished or canonical projective frame for the curve segment which is used for projective normalisation. In this canonical frame the curve has a viewpoint invariant signature. Measurements on the signature are invariants. All the methods presented are designed to work on real images where extracted data will not be ideal, and parts of curves will be missing because of poor contrast or occlusion. We compare the stability and discrimination of the signatures and invariants over a number of example curves and viewpoints. The paper concludes with a discussion of how the various methods can be integrated within a recognition system.	INRIA, LIFIA, GRENOBLE, FRANCE; KATHOLIEKE UNIV LEUVEN, ESAT, MI2, B-3001 LOUVAIN, BELGIUM; UNIV OXFORD, DEPT ENGN SCI, OXFORD OX1 3PJ, ENGLAND	Inria; KU Leuven; University of Oxford	Carlsson, S (corresponding author), ROYAL INST TECHNOL, S-10044 STOCKHOLM, SWEDEN.							Astrom K., 1994, LECT NOTES COMPUTER, V825, P199; BRUCKSTEIN AM, 1992, VISUAL FORM, P89; CARLSSON S, 1992, ARTIF INT, P267; CARLSSON S, 1993, INT J COMPUT VISION, V17, P193; Duda R.O., 1973, J ROYAL STAT SOC SER; Gool LV, 1992, GEOMETRIC INVARIANCE, P193; KEMPENAERS P, 1992, SPIE, V1702, P41; LAMDAN Y, 1988, JUN P CVPR C ANN ARB, P335; ROTHWELL C, 1993, THESIS U OXFORD OXFO; ROTHWELL CA, 1995, INT J COMPUT VISION, V16, P57, DOI 10.1007/BF01428193; ROTHWELL CA, 1992, P 2 EUR C COMP VIS, P757; VANDIEST M, 1993, KULESATMI129305; VANDIEST M, 1994, P 3 EUR C COMP VIS S, V1, P527	13	20	20	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1996	19	3					211	236		10.1007/BF00055145	http://dx.doi.org/10.1007/BF00055145			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VG016					2022-12-18	WOS:A1996VG01600001
J	Vieville, T; Faugeras, O; Luong, QT				Vieville, T; Faugeras, O; Luong, QT			Motion of points and lines in the uncalibrated case	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								In the present paper we address the problem of computing structure and motion, given a set point and/or line correspondences, in a monocular image sequence, when the camera is not calibrated. Considering point correspondences first, we analyse how to parameterize the retinal correspondences, in function of the chosen geometry: Euclidean, affine or projective geometry. The simplest of these parameterizations is called the Fes-representation and is a composite projective representation. The main result is that considering N + 1 views in such a monocular image sequence, the retinal correspondences are parameterized by 11 N - 4 parameters in the general projective case. Moreover, 3 other parameters are required to work in the affine case and 5 additional parameters in the Euclidean case. These 8 parameters are ''calibration'' parameters and must be calculated considering at least 8 external informations or constraints. The method being constructive, all these representations are made explicit. Then, considering line correspondences, we show how the the same parameterizations can be used when we analyse the motion of lines, in the uncalibrated case. The case of three views is extensively studied and a geometrical interpretation is proposed, introducing the notion of trifocal geometry which generalizes the well known epipolar geometry. It is also discussed how to introduce line correspondences, in a framework based on point correspondences, using the same equations. Finally, considering the Fes-representation, one implementation is proposed as a ''motion module'', taking retinal correspondences as input, and providing and estimation of the 11 N - 4 retinal motion parameters. As discussed in this paper, this module can also estimate the 3D depth of the points up to an affine and projective transformation, defined by the 8 parameters identified in the first section. Experimental results are provided.			Vieville, T (corresponding author), INRIA,BP93,F-06902 VALBONNE,FRANCE.							Bar-Shalom Y., 1988, TRACKING DATA ASS; CROWLEY J, 1993, IMAGE VISION COMPUT, P11; DERICHE R, 1971, P 3 ICCV OS, P66; DERICHE R, 1990, 1ST P EUR C COMP VIS, P259; ENCISO R, 1993, 2071 INRIA; FAUGERAS O, 1992, 2 ECCV GEN; Faugeras O. D., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P25; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; GUIDUCCI A, 1988, PATTERN RECOGN LETT, V8, P311, DOI 10.1016/0167-8655(88)90080-3; Hartley R., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P549, DOI 10.1109/CVPR.1993.341076; Hartley R. I., 1993, P DARPA IM UND WORKS, P361; HEEL J, 1990, P 3 ICCV OS; HUANG T, 1990, SIGNAL PROCESSING 1; KANAL LN, 1988, UNCERTAINTY ARTIFICI; LAVEST J, 1993, INTELLIGENT AUTONOMO; Liu Y., 1986, Proceedings of the Workshop on Motion: Representation and Analysis (Cat. No.86CH2322-6), P47; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Luong Q.-T., 1993, RR1894 INRIA; LUONG QT, 1994, 3 ECCV STOCKH; LUONG T, 1992, THESIS U PARIS SUD O; MAYBANK S, 1992, INT J COMPUTER VISIO, P8; MITICHE A, 1986, OCT P INT C PATT REC, P1110; Mundy J., 1992, GEOMETRIC INVARIANCE; NAVAB N, 1993, IEEE P 4 INT C COMP, P713; Press W.H., 1988, NUMERICAL RECIPES AR; QUAN L, 1994, 3 ECCV STOCKH; ROBERT L, 1992, THESIS ECOLE POLYTEC; ROBERT L, 1993, 4 ICCV BERL; Ruymgaart P. A., 1985, MATH KALMAN BUCY FIL; STEPHENS M, 1989, COMPUTER VISION PATT, P556; THACKER NA, 1992, BRIT MACH VIS ASS M; TRIVEDI H, 1991, IMAGE VISION COMPUT, P9; TSAI RY, 1989, ROBOTICS REV, V1, P147; VIEVILLE T, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P517; VIEVILLE T, 1994, IMAGE VISION COMPUT, P12; VIEVILLE T, 1993, 4 ICCV BERL; VIEVILLE T, 1994, MACH VISION APPL, P8; VIEVILLE T, 1994, IN PRESS USING COLLI; WILLSON R, 1994, THESIS CARNEGIE MELL; WILLSON RG, 1993, IEEE P CVPR 93 NEW Y, P670; [No title captured]	41	20	20	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1996	17	1					7	41		10.1007/BF00127817	http://dx.doi.org/10.1007/BF00127817			35	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TX328					2022-12-18	WOS:A1996TX32800002
J	SINCLAIR, D; BLAKE, A; MURRAY, D				SINCLAIR, D; BLAKE, A; MURRAY, D			ROBUST ESTIMATION OF EGOMOTION FROM NORMAL FLOW	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							IMAGE SEQUENCES; OPTICAL-FLOW; MOTION	This paper investigates the robustness of an algorithm due to Horn and Weldon (1988) for recovery of egomotion from optic-flow. Assuming only normal components of flow vectors are available and that 3D angular velocity is known, tight constraints can be constructed on the direction of translational motion or, equivalently, on the focus of expansion. In practice however this is unacceptably restrictive. Some allowance must be made for uncertainty in angular velocity. We show that the algorithm can indeed be extended to cope with such uncertainty with graceful degradation in accuracy of estimated position of the focus of expansion. The shape of the error distribution depends on whether the focus of expansion is inside or outside the field of view. If it is inside, the error distribution is isotropic. As it moves outside the distribution becomes increasingly anisotropic. Results from an implementation of the algorithm confirm the validity of the error bounds.			SINCLAIR, D (corresponding author), UNIV OXFORD,DEPT ENGN SCI,PARKS RD,OXFORD OX1 3PJ,ENGLAND.							CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; DUTTA R, 1991, IEEE WORKSHOP VISUAL, P81; GAT E, 1991, P INT C ROBOTICS AUT; HARRIS CG, 1987, 3RD ALV VIS C, P189; HEEGER DJ, 1988, INT J COMPUT VISION, P279; HILDRETH EC, 1984, ARTIF INTELL, V23, P309, DOI 10.1016/0004-3702(84)90018-3; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1984, PROC R SOC SER B-BIO, V223, P165, DOI 10.1098/rspb.1984.0088; MATTHIES L, 1989, INT J COMPUT VISION, V3, P209, DOI 10.1007/BF00133032; MAYBANK SJ, 1985, P ROY SOC LOND A MAT, V401, P317, DOI 10.1098/rspa.1985.0101; MURRAY DW, 1990, EXPT MACHINE INTERPR; NAGEL HH, 1983, COMPUT VISION GRAPH, V21, P85, DOI 10.1016/S0734-189X(83)80030-9; PRAZDNY K, 1981, COMPUT VISION GRAPH, V17, P238, DOI 10.1016/0146-664X(81)90004-6; SUBBARAO M, 1986, COMPUT VISION GRAPH, V36, P208, DOI 10.1016/0734-189X(86)90076-9; VIEVILLE T, 1989, P NATO ADV RES WORKS, P339; WENG J, 1989, IEEE T PATTERN ANAL, V11; WONG JY, 1978, THEORY GROUND VEHICL	19	20	21	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1994	13	1					57	69		10.1007/BF01420795	http://dx.doi.org/10.1007/BF01420795			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PK120					2022-12-18	WOS:A1994PK12000003
J	WYATT, JL; KEAST, C; SEIDEL, M; STANDLEY, D; HORN, B; KNIGHT, T; SODINI, C; LEE, HS; POGGIO, T				WYATT, JL; KEAST, C; SEIDEL, M; STANDLEY, D; HORN, B; KNIGHT, T; SODINI, C; LEE, HS; POGGIO, T			ANALOG VLSI SYSTEMS FOR IMAGE ACQUISITION AND FAST EARLY VISION PROCESSING	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								This article describes a project to design and build prototype analog early vision systems that are remarkably low-power, small, and fast. Three chips are described in detail. A continuous-time CMOS imager and processor chip uses a fully parallel 2-D resistive grid to find an object's position and orientation at 5000 frames/second, using only 30 milliwatts of power. A CMOS/CCD imager and processor chip does high-speed image smoothing and segmentation in a clocked, fully parallel 2-D array. And a chip that merges imperfect depth and slope data to produce an accurate depth map is under development in switched-capacitor CMOS technology.	MIT,DEPT ELECT ENGN & COMP SCI,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)				/0000-0003-3434-391X				DeWeerth S. P., 1988, Advanced Research in VLSI. Proceedings of the Fifth MIT Conference, P259; DRON L, UNPUB INTERN J COMPU; DRUMHELLER M, 1986, P IEEE INT C ROB AUT; HAKKARAINEN M, 1991, SPIE S OPT ENG PHOTO, P173; HARRIS JG, 1988, NEURAL ARCH COMPUTER; HARRIS JG, 1991, THESIS CALTECH PASAD; HARRIS JG, 1989, ANALOG VLSI IMPLEMEN, P27; HARRIS JG, 1986, MIT TR908 TECHN REP; HORN B, 1991, VLSI91645 MICR TECHN; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; HORN BKP, 1986, ROBOT VISION, P48; HORN BKP, 1991, RES DIRECTIONS COMPU, P531; KEAST CL, 1992, IN PRESS VLSI CIRCUI; KEAST CL, 1990, FEB P SPIE CHARGE CO, P152; KEAST CL, 1992, THESIS MIT; LUMSDAINE A, 1991, J VLSI SIGN, V3, P53, DOI 10.1007/BF00927834; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; MCQUIRK IS, 1991, THESIS MIT CAMBRIDGE; Mead, 1989, ANALOG VLSI NEURAL S; SEIDEL MN, 1991, JUL P IMACS WORLD C, P1669; STANDLEY D, 1991, FEB P IEEE INT SOL S, P38; STANDLEY DL, 1991, IEEE J SOLID-ST CIRC, V26, P1853, DOI 10.1109/4.104177; STANDLEY DL, 1991, THESIS MIT CAMBRIDGE, pP2; UMMINGER CB, IN PRESS IEEE T CIRC; WALLMARK JT, 1957, P IRE, V45, P474, DOI 10.1109/JRPROC.1957.278435; WYATT JL, 1992, 1992 P IEEE INT S CI; YANG W, 1990, THESIS MIT CAMBRIDGE; YANG W, 1990, FEB ISSCC, P218; YANG W, 1991, SPIE INT S OPT ENG P, P114; YU PC, 1992, UNPUB 1992 EUR SOL S	30	20	24	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1992	8	3					217	230		10.1007/BF00055153	http://dx.doi.org/10.1007/BF00055153			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JU903					2022-12-18	WOS:A1992JU90300005
J	Li, DQ; Jiang, TT; Jiang, M				Li, Dingquan; Jiang, Tingting; Jiang, Ming			Unified Quality Assessment of in-the-Wild Videos with Mixed Datasets Training	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Content dependency; In-the-wild videos; Mixed datasets training; Temporal-memory effect; Video quality assessment	PREDICTION	Video quality assessment (VQA) is an important problem in computer vision. The videos in computer vision applications are usually captured in the wild. We focus on automatically assessing the quality of in-the-wild videos, which is a challenging problem due to the absence of reference videos, the complexity of distortions, and the diversity of video contents. Moreover, the video contents and distortions among existing datasets are quite different, which leads to poor performance of data-driven methods in the cross-dataset evaluation setting. To improve the performance of quality assessment models, we borrow intuitions from human perception, specifically, content dependency and temporal-memory effects of human visual system. To face the cross-dataset evaluation challenge, we explore a mixed datasets training strategy for training a single VQA model with multiple datasets. The proposed unified framework explicitly includes three stages: relative quality assessor, nonlinear mapping, and dataset-specific perceptual scale alignment, to jointly predict relative quality, perceptual quality, and subjective quality. Experiments are conducted on four publicly available datasets for VQA in the wild, i.e., LIVE-VQC, LIVE-Qualcomm, KoNViD-1k, and CVD2014. The experimental results verify the effectiveness of the mixed datasets training strategy and prove the superior performance of the unified model in comparison with the state-of-the-art models. For reproducible research, we make the PyTorch implementation of our method available at .	[Li, Dingquan; Jiang, Tingting] Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China; [Li, Dingquan] Peking Univ, Adv Inst Informat Technol, Hangzhou, Peoples R China; [Jiang, Tingting] Peking Univ, Dept Comp Sci, Beijing, Peoples R China; [Li, Dingquan; Jiang, Ming] Peking Univ, Sch Math Sci, Lab Math & Its Applicat, Beijing, Peoples R China; [Li, Dingquan] Peking Univ, Beijing Int Ctr Math Res, Beijing, Peoples R China; [Jiang, Tingting] Beijing Film Acad, Adv Innovat Ctr Future Visual Entertainment, Beijing, Peoples R China	Peking University; Peking University; Peking University; Peking University; Peking University	Jiang, TT (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China.; Jiang, TT (corresponding author), Peking Univ, Dept Comp Sci, Beijing, Peoples R China.; Jiang, TT (corresponding author), Beijing Film Acad, Adv Innovat Ctr Future Visual Entertainment, Beijing, Peoples R China.	dingquanli@pku.edu.cn; ttjiang@pku.edu.cn; ming-jiang@pku.edu.cn	Li, Dingquan/V-9254-2019	Li, Dingquan/0000-0002-5549-9027; Jiang, Tingting/0000-0002-5372-0656	Natural Science Foundation of China [61572042, 61520106004, 61527804]; National Key R&D Program of China [2018YFB1403900]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China	This work was partially supported by the Natural Science Foundation of China under contracts 61572042, 61520106004, and 61527804. This work was also supported in part by National Key R&D Program of China (2018YFB1403900). We acknowledge the High-Performance Computing Platform of Peking University for providing computational resources.	[Anonymous], 2018, P 10 INT C QUAL MULT; Bampis CG, 2017, IEEE T IMAGE PROCESS, V26, P5217, DOI 10.1109/TIP.2017.2729891; Barron JT, 2019, PROC CVPR IEEE, P4326, DOI 10.1109/CVPR.2019.00446; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Choi LK, 2018, SIGNAL PROCESS-IMAGE, V67, P182, DOI 10.1016/j.image.2018.06.009; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dingquan Li, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P789, DOI 10.1145/3394171.3413804; Dodge S, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX); Freitas PG, 2018, SIGNAL PROCESS-IMAGE, V64, P1, DOI 10.1016/j.image.2018.02.010; Ghadiyaram D, 2018, IEEE T CIRC SYST VID, V28, P2061, DOI 10.1109/TCSVT.2017.2707479; Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32; Nieto RG, 2019, INT CONF ACOUST SPEE, P2227, DOI 10.1109/ICASSP.2019.8683625; He H., 2019, ARXIV PREPRINT ARXIV; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hosu Vlad, 2017, PROC 9 INT C QUAL MU, P1; Isogawa M, 2019, INT J COMPUT VISION, V127, P1751, DOI 10.1007/s11263-018-1132-0; Juluri P, 2016, IEEE COMMUN SURV TUT, V18, P401, DOI 10.1109/COMST.2015.2401424; Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224; Kim W, 2018, LECT NOTES COMPUT SC, V11205, P224, DOI 10.1007/978-3-030-01246-5_14; King DB, 2015, ACS SYM SER, V1214, P1; Korhonen J, 2019, IEEE T IMAGE PROCESS, V28, P5923, DOI 10.1109/TIP.2019.2923051; Krasula L, 2020, IEEE T MULTIMEDIA, V22, P961, DOI 10.1109/TMM.2019.2935687; Lasinger K., ARXIV190701341; Li DQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2351, DOI 10.1145/3343031.3351028; Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354; Li XL, 2016, IEEE T IMAGE PROCESS, V25, P3329, DOI 10.1109/TIP.2016.2568752; Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801; Li YM, 2016, IEEE T CIRC SYST VID, V26, P1044, DOI 10.1109/TCSVT.2015.2430711; Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083; Liu WT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P546, DOI 10.1145/3240508.3240643; Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118; Lu W, 2019, INFORM SCIENCES, V478, P141, DOI 10.1016/j.ins.2018.11.003; Lv JM, 2018, PROC CVPR IEEE, P7948, DOI 10.1109/CVPR.2018.00829; Ma KD, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6732; Ma KD, 2016, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2016.184; Manasa K, 2016, IEEE IMAGE PROC, P2400, DOI 10.1109/ICIP.2016.7532789; Men H, 2017, INT WORK QUAL MULTIM; Mittal A, 2016, IEEE T IMAGE PROCESS, V25, P289, DOI 10.1109/TIP.2015.2502725; Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726; Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050; Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417; Nuutinen M, 2016, IEEE T IMAGE PROCESS, V25, P3073, DOI 10.1109/TIP.2016.2562513; Park J, 2013, IEEE T IMAGE PROCESS, V22, P610, DOI 10.1109/TIP.2012.2219551; Paszke A, 2019, ADV NEUR IN, V32; Rippel O, 2019, IEEE I CONF COMP VIS, P3453, DOI 10.1109/ICCV.2019.00355; Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154; Seshadrinathan K, 2011, INT CONF ACOUST SPEE, P1153; Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111; Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992; Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940; Siahaan E, 2018, SIGNAL PROCESS-IMAGE, V60, P237, DOI 10.1016/j.image.2017.10.009; Sinno Z, 2019, IEEE IMAGE PROC, P1750, DOI 10.1109/ICIP.2019.8803115; Sinno Z, 2019, IEEE T IMAGE PROCESS, V28, P612, DOI 10.1109/TIP.2018.2869673; Triantaphillidou S, 2007, J IMAGING SCI TECHN, V51, P259, DOI 10.2352/J.ImagingSci.Technol.(2007)51:3(259); Varga D, 2019, SIGNAL IMAGE VIDEO P, V13, P1569, DOI 10.1007/s11760-019-01510-8; Varga D, 2019, NEURAL PROCESS LETT, V50, P2595, DOI 10.1007/s11063-019-10036-6; VQEG, 2000, FINAL REPORT VIDEO Q; Wang HQ, 2017, J VIS COMMUN IMAGE R, V46, P292, DOI 10.1016/j.jvcir.2017.04.009; Wang Y, 2012, IEEE T CIRC SYST VID, V22, P989, DOI 10.1109/TCSVT.2012.2186745; Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xu JT, 2014, IEEE IMAGE PROC, P491, DOI 10.1109/ICIP.2014.7025098; Yan P, 2019, P SPIE, V11187, P74; Yang D, 2019, IEEE INT CONF COMP V, P3913, DOI 10.1109/ICCVW.2019.00485; Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789; You JY, 2019, IEEE IMAGE PROC, P2349, DOI 10.1109/ICIP.2019.8803395; You JY, 2014, IEEE T IMAGE PROCESS, V23, P200, DOI 10.1109/TIP.2013.2287611; Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410; Zhang Wei Emma, 2019, ABS190106796 CORR; Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319; Zhang Y, 2019, IEEE T CIRC SYST VID, V29, P2244, DOI 10.1109/TCSVT.2018.2868063; Zhang Y, 2020, IEEE T NEUR NET LEAR, V31, P2716, DOI 10.1109/TNNLS.2018.2890310	74	19	21	2	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1238	1257		10.1007/s11263-020-01408-w	http://dx.doi.org/10.1007/s11263-020-01408-w		JAN 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK		Green Submitted			2022-12-18	WOS:000608635800001
J	Shi, XS; Guo, ZH; Xing, FY; Liang, Y; Yang, L				Shi, Xiaoshuang; Guo, Zhenhua; Xing, Fuyong; Liang, Yun; Yang, Lin			Anchor-Based Self-Ensembling for Semi-Supervised Deep Pairwise Hashing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Anchor; Self-ensembling; Deep pairwise hasing; Semi-seupervised; Large-scale	SCENE	Deep hashing has attracted considerable attention to tackle large-scale retrieval tasks, because of automatic and powerful feature extraction of convolutional neural networks and the gain of hashing in computation and storage costs. Most current supervised deep hashing methods only utilize the semantic information of labeled data without exploiting unlabeled data. However, data annotation is expensive and thus only scarce labeled data are available, which are difficult to represent the true distribution of all data. In this paper, we propose a novel semi-supervised deep pairwise hashing method to leverage both labeled and unlabeled data to learn hash functions. Our method utilizes the transduction of anchors to preserve the pairwise similarity relationship among both labeled and unlabeled samples. Additionally, to explore the semantic similarity information hidden in unlabeled data, it adopts self-ensembling to create strong ensemble targets for latent binary vectors of training samples and form a consensus predicting similarity relationship to multiple anchors. Unlike previous pairwise based hashing methods without maintaining the relevance among similar neighbors, we further explain and exhibit the capability of our method on preserving their relevance through calculating their similarities to anchors. Finally, extensive experiments on benchmark databases demonstrate the superior performance of the proposed method over recent state-of-the-art hashing methods on multiple retrieval tasks. The source codes of the proposed method are available on: .	[Shi, Xiaoshuang; Liang, Yun; Yang, Lin] Univ Florida, Dept Biomed Engn, Gainesville, FL 32611 USA; [Guo, Zhenhua] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen, Peoples R China; [Xing, Fuyong] Univ Colorado, Denver, CO 80202 USA	State University System of Florida; University of Florida; Tsinghua University; University of Colorado System; University of Colorado Denver	Shi, XS (corresponding author), Univ Florida, Dept Biomed Engn, Gainesville, FL 32611 USA.	xsshi2015@ufl.edu; zhenhua.guo@sz.tsinghua.edu.cn; fuyong.xing@ucdenver.edu; yunliang@ufl.edu; Lin.Yang@bme.ufl.edu			Natural Science Foundation of China (NSFC) [61772296]; Shenzhen fundamental research fund [JCYJ20170412170438636]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Shenzhen fundamental research fund	The work is partially supported by the Natural Science Foundation of China (NSFC) (No. 61772296), Shenzhen fundamental research fund (No. JCYJ20170412170438636).	Broder A. Z., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P327, DOI 10.1145/276698.276781; Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134; Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248; Delalleau Olivier, 2005, P 10 INT WORKSH ART; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu HF, 2019, IEEE T IMAGE PROCESS, V28, P739, DOI 10.1109/TIP.2018.2860898; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jarvelin K., 2000, P 23 ANN INT ACM SIG, P41, DOI DOI 10.1145/345508.345545; JIANG QY, 2018, P AAAI C ART INT AAA; Kang WC, 2016, AAAI CONF ARTIF INTE, P1230; King DB, 2015, ACS SYM SER, V1214, P1; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; Laine Samuli, 2016, ARXIV161002242; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li W.J., 2015, P CVPR BOST MA US; Lin K, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301269; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu L, 2017, PROC CVPR IEEE, P5140, DOI 10.1109/CVPR.2017.546; Liu W., 2014, ADV NEURAL INFORM PR, V4, P3419; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu Wei, 2010, ICML; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842; Raginsky M., 2009, ADV NEURAL INFORM PR, P1509, DOI [10.5555/2984093.2984263, DOI 10.5555/2984093.2984263]; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sablayrolles A, 2017, INT CONF ACOUST SPEE, P1732, DOI 10.1109/ICASSP.2017.7952453; Sapkota M, 2019, IEEE J BIOMED HEALTH, V23, P805, DOI 10.1109/JBHI.2018.2827703; Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345; Shen FM, 2015, IEEE I CONF COMP VIS, P4148, DOI 10.1109/ICCV.2015.472; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Shi C, 2017, IEEE T KNOWL DATA EN, V29, P17, DOI 10.1109/TKDE.2016.2598561; Shi XS, 2018, PATTERN RECOGN, V81, P14, DOI 10.1016/j.patcog.2018.03.015; Shi XS, 2017, AAAI CONF ARTIF INTE, P2541; Shi XS, 2017, MED IMAGE ANAL, V42, P117, DOI 10.1016/j.media.2017.07.009; Shi XJ, 2016, 2016 4TH INTL CONF ON APPLIED COMPUTING AND INFORMATION TECHNOLOGY/3RD INTL CONF ON COMPUTATIONAL SCIENCE/INTELLIGENCE AND APPLIED INFORMATICS/1ST INTL CONF ON BIG DATA, CLOUD COMPUTING, DATA SCIENCE & ENGINEERING (ACIT-CSII-BCD), P419, DOI [10.1109/ACIT-CSII-BCD.2016.87, 10.1109/ACIT-CSII-BCD.2016.088]; Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344; Tarvainen A, 2017, ADV NEUR IN, V30; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang GA, 2018, LECT NOTES COMPUT SC, V11219, P491, DOI 10.1007/978-3-030-01267-0_29; Wang J, 2017, PROC SPIE, V10400, DOI 10.1117/12.2275222; Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48; Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; Xu K, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225060; Xu K, 2019, PROC INT CONF PARAL, DOI 10.1145/3337821.3337923; Yan X, 2017, JOVE-J VIS EXP, DOI 10.3791/56323; Zhang J., 2018, ARXIV180202488; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763; Zhu H, 2016, AAAI CONF ARTIF INTE, P2415; Zhuang BH, 2016, PROC CVPR IEEE, P5955, DOI 10.1109/CVPR.2016.641	63	19	19	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2307	2324		10.1007/s11263-020-01299-x	http://dx.doi.org/10.1007/s11263-020-01299-x		FEB 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG					2022-12-18	WOS:000516304300002
J	Zhang, L; Wang, P; Shen, CH; Liu, LQ; Wei, W; Zhang, YN; van den Hengel, A				Zhang, Lei; Wang, Peng; Shen, Chunhua; Liu, Lingqiao; Wei, Wei; Zhang, Yanning; van den Hengel, Anton			Adaptive Importance Learning for Improving Lightweight Image Super-Resolution Network	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Important learning; Single image super-resolution; Lightweight network enhancement		Deep neural networks have achieved remarkable success in single image super-resolution (SISR). The computing and memory requirements of these methods have hindered their application to broad classes of real devices with limited computing power, however. One approach to this problem has been lightweight network architectures that balance the super-resolution performance and the computation burden. In this study, we revisit this problem from an orthogonal view, and propose a novel learning strategy to maximize the pixel-wise fitting ability of a given lightweight network architecture. Considering that the initial performance of the lightweight network is very limited, we present an adaptive importance learning scheme for SISR that trains the network with an easy-to-complex paradigm by dynamically updating the importance of image pixels on the basis of the training loss. Specifically, we formulate the network training and the importance learning into a joint optimization problem. With a carefully designed importance penalty function, the importance of individual pixels can be gradually increased through solving a convex optimization problem. The training process thus begins with pixels that are easy to reconstruct, and gradually proceeds to more complex pixels as fitting improves. Furthermore, the proposed learning scheme is able to seamlessly assimilate knowledge from a more powerful teacher network in the form of importance initialization, thus obtaining better initial performance for the network. Through learning the network parameters, and updating pixel importance, the proposed learning scheme enables smaller, lightweight, networks to achieve better performance than has previously been possible. Extensive experiments on four benchmark datasets demonstrate the potential benefits of the proposed learning strategy in lightweight SISR network enhancement. In some cases, our learned network with only 25% of the parameters and computational complexity can produce comparable or even better results than the corresponding full-parameter network.	[Zhang, Lei] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Wang, Peng] Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW, Australia; [Shen, Chunhua; Liu, Lingqiao; van den Hengel, Anton] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia; [Wei, Wei; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian, Shaanxi, Peoples R China	University of Wollongong; University of Adelaide; Northwestern Polytechnical University	Shen, CH (corresponding author), Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia.	chunhua.shen@adelaide.edu.au	wang, peng/AAH-2781-2020; WANG, Peng/AHD-1399-2022	liu, lingqiao/0000-0003-3584-795X; Wang, Peng/0000-0002-5397-9115; van den Hengel, Anton/0000-0003-3027-8364; Shen, Chunhua/0000-0002-8648-8718	National Natural Science Foundation of China [61671385]; Australian Research Council [FT120100969]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Australian Research Council(Australian Research Council)	This work was supported in part by the National Natural Science Foundation of China (No. 61671385), and Australian Research Council Grant (FT120100969).	Basu S., 2013, AAAI; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Efrat N, 2013, IEEE I CONF COMP VIS, P2832, DOI 10.1109/ICCV.2013.352; Github, 2019, DRRN COD; Github, 2019, VDSR COD; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; Hinton G., 2015, ARXIV150302531; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang Jia-Bin, 2015, CVPR, DOI DOI 10.1109/CVPR.2015.7299156; Jiang L, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P547, DOI 10.1145/2647868.2654918; Jiang L, 2015, AAAI CONF ARTIF INTE, P2694; Khan Faisal, 2011, ADV NEURAL INFORM PR, P1449; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kumar A, 2010, ASIA PACIF MICROWAVE, P1189; Lai Wei-Sheng, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Liu LQ, 2017, IEEE T PATTERN ANAL, V39, P2335, DOI 10.1109/TPAMI.2017.2651061; Mao X.-J., 2016, ARXIV160608921; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Paszke Adam, 2017, PYTORCH TENSORS DYNA, P6; Romero Adriana, 2014, ARXIV14126550; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Timofte R, 2018, IEEE COMPUT SOC CONF, P965, DOI 10.1109/CVPRW.2018.00130; Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149; Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Wei W, 2017, IEEE T GEOSCI REMOTE, V55, P6860, DOI 10.1109/TGRS.2017.2735488; Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang L., 2017, ARXIV170801008; Zhang L, 2018, INT J COMPUT VISION, V126, P797, DOI 10.1007/s11263-018-1080-8; Zhang L, 2018, IEEE T IMAGE PROCESS, V27, P5969, DOI 10.1109/TIP.2018.2862629; Zhang Y., 2017, ARXIV170500384	41	19	19	2	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					479	499		10.1007/s11263-019-01253-6	http://dx.doi.org/10.1007/s11263-019-01253-6		NOV 2019	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KJ1GX		Green Accepted, Green Submitted			2022-12-18	WOS:000494782400001
J	Cherian, A; Gould, S				Cherian, Anoop; Gould, Stephen			Second-order Temporal Pooling for Action Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; Deep Learning; Kernel descriptors; Second-order statistics; Pooling; Image Representations; End-to-end learning; Region covariance descriptors		Deep learning models for video-based action recognition usually generate features for short clips (consisting of a few frames); such clip-level features are aggregated to video-level representations by computing statistics on these features. Typically zero-th (max) or the first-order (average) statistics are used. In this paper, we explore the benefits of using second-order statistics.Specifically, we propose a novel end-to-end learnable feature aggregation scheme, dubbed temporal correlation pooling that generates an action descriptor for a video sequence by capturing the similarities between the temporal evolution of clip-level CNN features computed across the video. Such a descriptor, while being computationally cheap, also naturally encodes the co-activations of multiple CNN features, thereby providing a richer characterization of actions than their first-order counterparts. We also propose higher-order extensions of this scheme by computing correlations after embedding the CNN features in a reproducing kernel Hilbert space. We provide experiments on benchmark datasets such as HMDB-51 and UCF-101, fine-grained datasets such as MPII Cooking activities and JHMDB, as well as the recent Kinetics-600. Our results demonstrate the advantages of higher-order pooling schemes that when combined with hand-crafted features (as is standard practice) achieves state-of-the-art accuracy.	[Cherian, Anoop; Gould, Stephen] Australian Natl Univ, Australian Ctr Robot Vis, Canberra, ACT, Australia	Australian Centre for Robotic Vision; Australian National University	Cherian, A (corresponding author), Australian Natl Univ, Australian Ctr Robot Vis, Canberra, ACT, Australia.	anoop.cherian@anu.edu.au; stephen.gould@anu.edu.au			Australian Research Council (ARC) through the Centre of Excellence for Robotic Vision [CE140100016]	Australian Research Council (ARC) through the Centre of Excellence for Robotic Vision(Australian Research Council)	This research was supported by the Australian Research Council (ARC) through the Centre of Excellence for Robotic Vision (CE140100016) and was undertaken with the resources from the National Computational Infrastructure (NCI) at the Australian National University. The authors also thank Mr. Edison Guo (ANU) for helpful discussions.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, WACV; Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965; Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4; Bilen Hakan, 2016, CVPR, DOI DOI 10.1109/CVPR.2016.331; Blank M, 2005, IEEE I CONF COMP VIS, P1395; Bojanowski P, 2014, LECT NOTES COMPUT SC, V8693, P628, DOI 10.1007/978-3-319-10602-1_41; Cai ZW, 2014, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2014.83; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Chaquet JM, 2013, COMPUT VIS IMAGE UND, V117, P633, DOI 10.1016/j.cviu.2013.01.013; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen X., 2014, P 27 ANN C NEURAL IN, P1736, DOI DOI 10.1109/CVPR.2018.00742; Cherian A., 2017, WACV; Cherian A, 2018, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2018.00234; Cherian A, 2017, PROC CVPR IEEE, P1581, DOI 10.1109/CVPR.2017.172; Cherian A, 2013, IEEE T PATTERN ANAL, V35, P2161, DOI 10.1109/TPAMI.2012.259; Cheron G., 2015, ARXIV150603607; Courtney PG, 2015, IEEE COMP SEMICON; Davis JW, 1997, PROC CVPR IEEE, P928, DOI 10.1109/CVPR.1997.609439; Donahue J., 2014, ARXIV14114389; Duchenne O., 2009, INT C COMP VIS; Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Feichtenhofer Christoph, 2016, NIPS; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70; Girdhar Rohit, 2017, PROC CVPR IEEE, P971, DOI DOI 10.1109/CVPR.2017.337; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; Gu C., 2017, ABS1705084214, P4; Guo K., 2013, TIP; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010; Hoeting JA, 1999, STAT SCI, V14, P382, DOI 10.1214/ss/1009212519; Huang Z., 2017, P 31 AAAI C ART INT; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Jebara T, 2003, LECT NOTES ARTIF INT, V2777, P57, DOI 10.1007/978-3-540-45167-9_6; Jegou H., 2009, CVPR; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kay W., 2017, ARXIV PREPRINT ARXIV; Klaser Alexander, 2008, BMVC; Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuehne H., 2011, P INT C COMP VIS, DOI DOI 10.1109/ICCV.2011.6126543; Lan T., 2015, ICCV; Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496; Lei J., 2012, ACM C UB COMP; Li P., 2013, ICCV; Monfort Mathew, 2018, ARXIV180103150; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Ni B., 2014, CVPR; Oneata D., 2013, ICCV; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013; Peng XJ, 2014, LECT NOTES COMPUT SC, V8691, P660, DOI 10.1007/978-3-319-10578-9_43; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Pirsiavash H, 2014, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2014.85; Pishchulin L, 2014, LECT NOTES COMPUT SC, V8753, P678, DOI 10.1007/978-3-319-11752-2_56; Prest A, 2012, IEEE T PATTERN ANAL, V34, P601, DOI 10.1109/TPAMI.2011.158; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rohrbach M., 2015, ARXIV150206648; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Ryoo M. S., 2006, CVPR; Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806; Soomro K., 2012, ARXIV; Sra Suvrit, 2011, TECHNICAL REPORT; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sun C., 2014, CVPR; Tang K., 2012, CVPR; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang J, 2018, PROC CVPR IEEE, P1149, DOI 10.1109/CVPR.2018.00126; Wang Lijun, 2015, CVPR; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wu Changchang, 2015, P IEEE C COMP VIS PA; Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67; Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386; Yao Bangpeng, 2012, ECCV; Yu K., 2017, ARXIV170306817; Yuan C., 2009, ACCV; Zhou Y., 2015, CVPR; Zhou Yipin, 2014, ECCV; Zuffi S., 2013, IJCV, V101, P437, DOI DOI 10.1007/s11263-012-0549-0	95	19	19	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2019	127	4					340	362		10.1007/s11263-018-1111-5	http://dx.doi.org/10.1007/s11263-018-1111-5			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HN4DR		Green Submitted			2022-12-18	WOS:000460135000002
J	Liu, B; Jing, LP; Li, J; Yu, J; Gittens, A; Mahoney, MW				Liu, Bo; Jing, Liping; Li, Jia; Yu, Jian; Gittens, Alex; Mahoney, Michael W.			Group Collaborative Representation for Image Set Classification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image set classification; Group collaborative representation; Point-to-sets representation; Set-to-sets representation	FACE RECOGNITION; SELECTION; POINT	With significant advances in imaging technology, multiple images of a person or an object are becoming readily available in a number of real-life scenarios. In contrast to single images, image sets can capture a broad range of variations in the appearance of a single face or object. Recognition from these multiple images (i.e., image set classification) has gained significant attention in the area of computer vision. Unlike many existing approaches, which assume that only the images in the same set affect each other, this work develops a group collaborative representation (GCR) model which makes no such assumption, and which can effectively determine the hidden structure among image sets. Specifically, GCR takes advantage of the relationship between image sets to capture the inter- and intra-set variations, and it determines the characteristic subspaces of all the gallery sets. In these subspaces, individual gallery images and each probe set can be effectively represented via a self-representation learning scheme, which leads to increased discriminative ability and enhances robustness and efficiency of the prediction process. By conducting extensive experiments and comparing with state-of-the-art, we demonstrated the superiority of the proposed method on set-based face recognition and object categorization tasks.	[Liu, Bo; Jing, Liping; Li, Jia; Yu, Jian] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China; [Liu, Bo] Agr Univ Hebei, Coll Informat Sci & Technol, Baoding 071000, Hebei, Peoples R China; [Gittens, Alex] Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA; [Mahoney, Michael W.] Univ Calif Berkeley, Int Comp Sci Inst, Berkeley, CA 94702 USA; [Mahoney, Michael W.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94702 USA	Beijing Jiaotong University; Hebei Agricultural University; Rensselaer Polytechnic Institute; University of California System; University of California Berkeley; University of California System; University of California Berkeley	Jing, LP (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.	boliu@hebau.edu.cn; lpjing@bjtu.edu.cn; jail@bjtu.edu.cn; jianyu@bjtu.edu.cn; gittea@rpi.edu; mmahoney@stat.berkeley.edu			National Natural Science Foundation of China [61632004, 61773050]; Opening Project of Beijing Key Lab of Traffic Data Analysis and Mining [BKLTDAM2017001]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Opening Project of Beijing Key Lab of Traffic Data Analysis and Mining	Funding was provided by National Natural Science Foundation of China (Grant Nos. 61632004, 61773050) and Opening Project of Beijing Key Lab of Traffic Data Analysis and Mining (Grant No. BKLTDAM2017001).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arandjelovic O, 2005, PROC CVPR IEEE, P581; Bach F, 2012, STAT SCI, V27, P450, DOI 10.1214/12-STS394; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Cevikalp H., 2010, PROC CVPR IEEE, P2567, DOI DOI 10.1109/CVPR.2010.5539965; Chen SK, 2013, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2013.65; DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909; Deng W, 2016, J SCI COMPUT, V66, P889, DOI 10.1007/s10915-015-0048-x; Gross Ralph, 2001, CMURITR0118; Harandi M. T., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2705, DOI 10.1109/CVPR.2011.5995564; Hayat M, 2015, IEEE T PATTERN ANAL, V37, P713, DOI 10.1109/TPAMI.2014.2353635; Hayat M, 2014, LECT NOTES COMPUT SC, V8694, P784, DOI 10.1007/978-3-319-10599-4_50; Hu YQ, 2012, IEEE T PATTERN ANAL, V34, P1992, DOI 10.1109/TPAMI.2011.283; Huang ZW, 2014, PROC CVPR IEEE, P1677, DOI 10.1109/CVPR.2014.217; Kim M, 2008, PROC CVPR IEEE, P1787; Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037; Lai K, 2011, IEEE INT CONF ROBOT, P1817; Lee KC, 2003, PROC CVPR IEEE, P313; Lin Z., 2011, PROC INT 25 C NEURAL, P612, DOI DOI 10.1007/S11263-013-0611-6; Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26; Lu JW, 2015, PROC CVPR IEEE, P1137, DOI 10.1109/CVPR.2015.7298717; Lu JW, 2013, IEEE I CONF COMP VIS, P329, DOI 10.1109/ICCV.2013.48; Mahmood A, 2014, PROC CVPR IEEE, P121, DOI 10.1109/CVPR.2014.23; Ng MK, 2011, SIAM J SCI COMPUT, V33, P1643, DOI 10.1137/100807697; Parkhi Omkar M., 2015, BRIT MACH VIS C; Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009; Saunders C., 1998, ICML 1998 P 15 INT C, P515; Shakhnarovich G, 2002, LECT NOTES COMPUT SC, V2352, P851, DOI 10.1007/3-540-47977-5_56; Tao M, 2011, SIAM J OPTIMIZ, V21, P57, DOI 10.1137/100781894; Uzair M., 2014, P AS C COMP VIS, P617; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang RP, 2008, PROC CVPR IEEE, P2940; Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965; Wang RP, 2009, PROC CVPR IEEE, P429, DOI 10.1109/CVPRW.2009.5206850; Wang W, 2015, PROC CVPR IEEE, P3395, DOI 10.1109/CVPR.2015.7298816; Wolf L., 2011, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2011.5995566; Yang M., 2013, PROC 10 IEEE INT C W, P1, DOI DOI 10.1109/FG.2013.6553727; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277; Zhu PF, 2013, IEEE I CONF COMP VIS, P2664, DOI 10.1109/ICCV.2013.331; Zhu PF, 2014, IEEE T INF FOREN SEC, V9, P1120, DOI 10.1109/TIFS.2014.2324277; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zou H, 2008, ANN APPL STAT, V2, P1290, DOI 10.1214/08-AOAS198	45	19	20	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2019	127	2					181	206		10.1007/s11263-018-1088-0	http://dx.doi.org/10.1007/s11263-018-1088-0			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HL5LJ					2022-12-18	WOS:000458768000004
J	Owens, A; Wu, JJ; McDermott, JH; Freeman, WT; Torralba, A				Owens, Andrew; Wu, Jiajun; McDermott, Josh H.; Freeman, William T.; Torralba, Antonio			Learning Sight from Sound: Ambient Sound Provides Supervision for Visual Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sound; Convolutional networks; Unsupervised learning		The sound of crashing waves, the roar of fast-moving cars-sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds. This paper extends an earlier conference paper, Owens et al. (in: European conference on computer vision, 2016b), with additional experiments and discussion.	[Owens, Andrew] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Owens, Andrew; Wu, Jiajun; McDermott, Josh H.; Freeman, William T.; Torralba, Antonio] MIT, Cambridge, MA 02139 USA	University of California System; University of California Berkeley; Massachusetts Institute of Technology (MIT)	Owens, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Owens, A (corresponding author), MIT, Cambridge, MA 02139 USA.	andrewo@mit.edu; jiajunwu@mit.edu; jhm@mit.edu; billf@mit.edu; torralba@csail.mit.edu	Wu, JiaJun/GQH-7885-2022		NSF [1524817, 1447476, 1212849]; McDonnell Scholar Award; Microsoft Ph.D. Fellowship; Shell Research	NSF(National Science Foundation (NSF)); McDonnell Scholar Award; Microsoft Ph.D. Fellowship(Microsoft); Shell Research	This work was supported by NSF Grants #1524817 to A.T; NSF Grants #1447476 and #1212849 to W.F.; a McDonnell Scholar Award to J.H.M.; and a Microsoft Ph.D. Fellowship to A.O. It was also supported by Shell Research, and by a donation of GPUs from NVIDIA. We thank Phillip Isola for the helpful discussions, and Carl Vondrick for sharing the data that we used in our experiments. We also thank the anonymous reviewers for their comments, which significantly improved the paper (in particular, for suggesting the comparison with texton features in Sect. 5).	Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13; Andrew Galen, 2013, ICML; Arandjelovic R., 2017, LOOK LISTEN LEARN; Bau D., 2017, NETWORK DISSECTION Q; Darrell T., 2016, ICLR; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DESA VR, 1994, PROCEEDINGS OF THE 1993 CONNECTIONIST MODELS SUMMER SCHOOL, P300; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766, DOI [DOI 10.1109/TPAMI.2015.2496141, 10.48550/arXiv.1406.6909]; Ellis D. P., 2011, IEEE INT C AC SPEECH; Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fisher III J.W., 2000, ADV NEURAL INFORM PR; GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1; Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261; Girshick R., 2015, ICCV; Goroshin R., 2015, ARXIV150402518; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Hershey J. R., 1999, ADV NEURAL INFORM PR; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Isola P., 2016, INT C LEARN REPR WOR; Isola P. J., 2015, THESIS; Jayaraman D, 2015, IEEE I CONF COMP VIS, P1413, DOI 10.1109/ICCV.2015.166; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kidron E, 2005, PROC CVPR IEEE, P88; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le Q., 2012, INT C MACH LEARN, DOI DOI 10.1109/MSP.2011.940881; Lee K., 2010, IEEE INT C AC SPEECH; Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; McDermott JH, 2011, NEURON, V71, P926, DOI 10.1016/j.neuron.2011.06.032; Mishkin Dmytro, 2015, ICLR; Mobahi Hossein, 2009, INT C MACH LEARN; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; OQUAB M, 2015, PROC CVPR IEEE, P685, DOI DOI 10.1109/CVPR.2015.7298668; Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264; Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48; Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Slaney M., 2000, ADV NEURAL INFORM PR; Smith L, 2005, ARTIF LIFE, V11, P13, DOI 10.1162/1064546053278973; Srivastava Nitish, 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thomee B., 2015, ARXIV150301817; Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652; Weiss Y., 2009, NIPS; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou Bolei, 2015, OBJECT DETECTORS EME, P2; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	55	19	20	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2018	126	10			SI		1120	1137		10.1007/s11263-018-1083-5	http://dx.doi.org/10.1007/s11263-018-1083-5			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GR8XD		Green Submitted			2022-12-18	WOS:000443018400004
J	Amer, MR; Shields, T; Siddiquie, B; Tamrakar, A; Divakaran, A; Chai, S				Amer, Mohamed R.; Shields, Timothy; Siddiquie, Behjat; Tamrakar, Amir; Divakaran, Ajay; Chai, Sek			Deep Multimodal Fusion: A Hybrid Approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Conditional Restricted Boltzmann Machines; Hybrid; Generative; Discriminative; Multimodal fusion; Gesture recognition; Social interaction modeling		We propose a novel hybrid model that exploits the strength of discriminative classifiers along with the representation power of generative models. Our focus is on detecting multimodal events in time varying sequences as well as generating missing data in any of the modalities. Discriminative classifiers have been shown to achieve higher performances than the corresponding generative likelihood-based classifiers. On the other hand, generative models learn a rich informative space which allows for data generation and joint feature representation that discriminative models lack. We propose a new model that jointly optimizes the representation space using a hybrid energy function. We employ a Restricted Boltzmann Machines (RBMs) based model to learn a shared representation across multiple modalities with time varying data. The Conditional RBMs (CRBMs) is an extension of the RBM model that takes into account short term temporal phenomena. The hybrid model involves augmenting CRBMs with a discriminative component for classification. For these purposes we propose a novel Multimodal Discriminative CRBMs (MMDCRBMs) model. First, we train the MMDCRBMs model using labeled data by training each modality, followed by training a fusion layer. Second, we exploit the generative capability of MMDCRBMs to activate the trained model so as to generate the lower-level data corresponding to the specific label that closely matches the actual input data. We evaluate our approach on ChaLearn dataset, audio-mocap, as well as the Tower Game dataset, mocap-mocap as well as three multimodal toy datasets. We report classification accuracy, generation accuracy, and localization accuracy and demonstrate its superiority compared to the state-of-the-art methods.	[Amer, Mohamed R.; Shields, Timothy; Siddiquie, Behjat; Tamrakar, Amir; Divakaran, Ajay; Chai, Sek] SRI Int, Princeton, NJ 08540 USA	SRI International	Amer, MR (corresponding author), SRI Int, Princeton, NJ 08540 USA.	mohamed.amer@sri.com; Timothy.Shields@sri.com; Behjat.Siddiquie@sri.com; Amir.Tamrakar@sri.com; Ajay.Divakaran@sri.com; Sek.Chai@sri.com			DARPA [W911NF-12-C-0001]; Air Force Research Laboratory (AFRL)	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Air Force Research Laboratory (AFRL)(United States Department of DefenseUS Air Force Research Laboratory)	We would like to thank Dr. Natalia Nevrova for providing the features preprocessing code for the ChaLearn dataset, and Dr. Graham Taylor for his insightful feedback and discussions. This work is supported by DARPA W911NF-12-C-0001 and the Air Force Research Laboratory (AFRL). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.	Amer M., 2014, WACV; Bengio Y., 2009, FTML; Camgoz N., 2014, ECCV; Chang J., 2014, ECCV W; Chen G., 2014, ECCV W; Cox S., 2008, AVSP; Druck G, 2010, ICML; Escalera S., 2014, ECCV W; Evangelidis G., 2014, ECCV W; Fujino A., 2008, TPAMI; Garg N., 2011, ACL; Glodek M., 2011, ACII; Gurban M, 2009, IEEE T SIGNAL PROCES, V57, P4765, DOI 10.1109/TSP.2009.2026513; Hausler C., 2012, CORR; Hinton G., 2007, AISTATS; Hinton G. E., 2002, NC; Hinton G. E., 2006, NC; Larochelle H., 2008, ICML; Lewandowski N. B., 2012, ICML; Li X., 2011, CVPR; Lucey P., 2006, HCSNET WORKSH US VIS; Matthews I, 2002, TPAMI; Memisevic R., 2007, CVPR; Mohamed A. R., 2009, ICASSP; Monnier C., 2014, ECCV W; Neverova N., 2014, PAMI; Neverova N., 2014, ECCV W; Ngiam J., 2011, ICML; Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4; Papandreou G., 2009, TASLP; Patterson E. K., 2002, ICASSP; Peng X., 2014, ECCV W; Perina A., 2012, TPAMI; Pigou L., 2014, ECCV W; Ramirez G., 2011, ACII; Ranzato Marc'Aurelio, 2011, CVPR; Rehg J., 2013, CVPR; Salakhutdinov R., 2006, SCIENCE; Salter D. A., 2015, ACII; Schuller B., 2011, ACII; Siddiquie B., 2013, ICME; Sminchisescu C., 2006, CVPR; Srivastava N., 2012, NIPS; Sun X., 2011, ACII; Sutskever I., 2008, NIPS; Taylor G.W., 2010, CVPR; Taylor GW, 2011, J MACH LEARN RES, V12, P1025; Wu D., 2014, ECCV W; Zanfir M., 2013, ICCV; Zeiler M. D., 2011, NIPS; Zeiler MD, 2014, ECCV; Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637	52	19	19	3	30	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		440	456		10.1007/s11263-017-0997-7	http://dx.doi.org/10.1007/s11263-017-0997-7			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA					2022-12-18	WOS:000425619100017
J	Hughes, B; Burghardt, T				Hughes, Benjamin; Burghardt, Tilo			Automated Visual Fin Identification of Individual Great White Sharks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Animal biometrics; Textureless object recognition; Shape analysis	PHOTOIDENTIFICATION; CARCHARIAS; SCALE	This paper discusses the automated visual identification of individual great white sharks from dorsal fin imagery. We propose a computer vision photo ID system and report recognition results over a database of thousands of unconstrained fin images. To the best of our knowledge this line of work establishes the first fully automated contour-based visual ID system in the field of animal biometrics. The approach put forward appreciates shark fins as textureless, flexible and partially occluded objects with an individually characteristic shape. In order to recover animal identities from an image we first introduce an open contour stroke model, which extends multi-scale region segmentation to achieve robust fin detection. Secondly, we show that combinatorial, scale-space selective fingerprinting can successfully encode fin individuality. We then measure the species-specific distribution of visual individuality along the fin contour via an embedding into a global 'fin space'. Exploiting this domain, we finally propose a non-linear model for individual animal recognition and combine all approaches into a fine-grained multi-instance framework. We provide a system evaluation, compare results to prior work, and report performance and properties in detail.	[Hughes, Benjamin] Save Our Seas Fdn, Rue Philippe Plantamour 20, CH-1201 Geneva, Switzerland; [Hughes, Benjamin; Burghardt, Tilo] Univ Bristol, Dept Comp Sci, Bristol BS8 1UB, Avon, England	University of Bristol	Hughes, B (corresponding author), Save Our Seas Fdn, Rue Philippe Plantamour 20, CH-1201 Geneva, Switzerland.; Hughes, B (corresponding author), Univ Bristol, Dept Comp Sci, Bristol BS8 1UB, Avon, England.	ben@saveourseas.com; tilo@cs.bris.ac.uk			EPSRC [EP/E501214/1]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	B. Hughes was supported by EPSRC Grant EP/E501214/1. We gratefully acknowledge Michael C. Scholl and the Save Our Seas Foundation for allowing the use of fin images and ground truth labels.	Anderson SD, 2011, MAR BIOL, V158, P1233, DOI 10.1007/s00227-011-1643-5; Arandjelovic O., 2012, BMVC 2012, P1; Arandjelovic R, 2011, IEEE I CONF COMP VIS, P375, DOI 10.1109/ICCV.2011.6126265; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Arbelaez P, 2012, PROC CVPR IEEE, P3378, DOI 10.1109/CVPR.2012.6248077; Baird RW, 2008, MAR MAMMAL SCI, V24, P591, DOI 10.1111/j.1748-7692.2008.00200.x; Baird RW, 2009, MAR MAMMAL SCI, V25, P251, DOI 10.1111/j.1748-7692.2008.00257.x; Berger-Wolf T. Y., 2015, DATA GOOD EXCHANGE; Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598; Bonfil R, 2005, SCIENCE, V310, P100, DOI 10.1126/science.1114898; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Carreira J, 2010, PROC CVPR IEEE, P3241, DOI 10.1109/CVPR.2010.5540063; Chapple TK, 2011, BIOL LETTERS, V7, P581, DOI 10.1098/rsbl.2011.0124; Crall JP, 2013, IEEE WORK APP COMP, P230, DOI 10.1109/WACV.2013.6475023; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Davis J., 2006, P 23 INT C MACH LEAR, V148, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]; Dollar P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231; Duyck J., 2015, PATTERN RECOGN, V48; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Freytag A., 2016, GERMAN CONFERENCE PA; Geusebroek JM, 2005, INT J COMPUT VISION, V61, P103, DOI 10.1023/B:VISI.0000042993.50813.60; Gu CH, 2009, PROC CVPR IEEE, P1030, DOI 10.1109/CVPRW.2009.5206727; Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20; Hillman G.R., 2003, Aquatic Mammals, V29, P117, DOI 10.1578/016754203101023960; Hughes B., 2015, 26 BRIT MACH VIS C B; Hughes B., 2015, MACHINE VISION ANIMA; Kelly MJ, 2001, J MAMMAL, V82, P440, DOI 10.1644/1545-1542(2001)082<0440:CAPMIS>2.0.CO;2; Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013; Li FX, 2010, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2010.5539839; Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49; Marshall AD, 2012, J FISH BIOL, V80, P1361, DOI 10.1111/j.1095-8649.2012.03244.x; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; McCann S, 2012, PROC CVPR IEEE, P3650, DOI 10.1109/CVPR.2012.6248111; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331; Ranguelova E, 2004, IEEE IMAGE PROC, P1727; Ravela S., 2013, LNCS, V7914, P10; Robbins R, 2012, MAR FRESHWATER RES, V63, P1215, DOI 10.1071/MF12208; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Scholl M., 2016, HIGH TECH FIN RECOGN; Simpson R., 2013, P 23 INT C WORLD WID, P1049; Speed Conrad W., 2007, Frontiers in Zoology, V4, P1; Stanley R., 1995, THESIS ECKERD COLL S; Stewman J, 2006, LECT NOTES COMPUT SC, V4141, P648; Towner AV, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0066035; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; Van Hoey N. E., 2013, PHOTOIDENTIFICATION; Van Tienhoven AM, 2007, J APPL ECOL, V44, P273, DOI 10.1111/j.1365-2664.2006.01273.x; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Zhang XH, 2009, PATTERN RECOGN LETT, V30, P449, DOI 10.1016/j.patrec.2008.11.002; Zheng YJ, 2009, IEEE I CONF COMP VIS, P889, DOI 10.1109/ICCV.2009.5459326	53	19	20	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2017	122	3			SI		542	557		10.1007/s11263-016-0961-y	http://dx.doi.org/10.1007/s11263-016-0961-y			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ES7OH	32103855	Green Published, hybrid, Green Submitted			2022-12-18	WOS:000399739200009
J	Simo-Serra, E; Torras, C; Moreno-Noguer, F				Simo-Serra, Edgar; Torras, Carme; Moreno-Noguer, Francesc			3D Human Pose Tracking Priors using Geodesic Mixture Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Probabilistic priors; Mixture modelling; Riemannian manifolds; 3D human pose; Human kinematics	HUMAN MOTION; RIEMANNIAN-MANIFOLDS; STATISTICS; CAPTURE	We present a novel approach for learning a finite mixture model on a Riemannian manifold in which Euclidean metrics are not applicable and one needs to resort to geodesic distances consistent with the manifold geometry. For this purpose, we draw inspiration on a variant of the expectation-maximization algorithm, that uses a minimum message length criterion to automatically estimate the optimal number of components from multivariate data lying on an Euclidean space. In order to use this approach on Riemannian manifolds, we propose a formulation in which each component is defined on a different tangent space, thus avoiding the problems associated with the loss of accuracy produced when linearizing the manifold with a single tangent space. Our approach can be applied to any type of manifold for which it is possible to estimate its tangent space. Additionally, we consider using shrinkage covariance estimation to improve the robustness of the method, especially when dealing with very sparsely distributed samples. We evaluate the approach on a number of situations, going from data clustering on manifolds to combining pose and kinematics of articulated bodies for 3D human pose tracking. In all cases, we demonstrate remarkable improvement compared to several chosen baselines.	[Simo-Serra, Edgar] Waseda Univ, Shinjuku Ku, Okubo 3-4-1, Tokyo 1698555, Japan; [Torras, Carme; Moreno-Noguer, Francesc] Inst Robt InformA tica Ind CSIC UPC, Barcelona 08028, Spain	Waseda University	Simo-Serra, E (corresponding author), Waseda Univ, Shinjuku Ku, Okubo 3-4-1, Tokyo 1698555, Japan.	esimo@aoni.waseda.jp; torras@iri.upc.edu; fmoreno@iri.upc.edu	Torras, Carme/AAG-3027-2019; Torras, Carme/M-1794-2014	Torras, Carme/0000-0002-2933-398X; Torras, Carme/0000-0002-2933-398X	Spanish MINECO project RobInstruct [TIN2014-58178-R]; ERA-net CHISTERA project I-DRESS [PCIN-2015-147]	Spanish MINECO project RobInstruct; ERA-net CHISTERA project I-DRESS	We would like to thank the three anonymous reviewers for their insights and comments that have significantly contributed to improving this manuscript. This work was partly funded by the Spanish MINECO project RobInstruct TIN2014-58178-R and by the ERA-net CHISTERA project I-DRESS PCIN-2015-147.	Andriluka M., 2010, IEEE C COMP VIS PATT; [Anonymous], 2003, INTRO DIFFERENTIABLE; Archambeau C, 2005, LECT NOTES COMPUT SC, V3512, P820; Banerjee A, 2005, J MACH LEARN RES, V6, P1345; Brand  M., 2003, ADV NEURAL INFORM PR, P961, DOI DOI 10.1109/34.682189; Brubaker M., 2012, P 15 INT C ART INT S, P161; Caseiro R., 2013, IEEE C COMP VIS PATT; Caseiro R, 2012, PATTERN RECOGN, V45, P3997, DOI 10.1016/j.patcog.2012.04.011; Chang J., 2013, ADV NEURAL INFORM PR, P620; Chen YL, 2010, IEEE T SIGNAL PROCES, V58, P5016, DOI 10.1109/TSP.2010.2053029; Darling RWR, 1996, ANN I H POINCARE-PR, V32, P431; Davis B. C., 2007, INT C COMP VIS; Dedieu JP, 2005, J COMPLEXITY, V21, P487, DOI 10.1016/j.jco.2004.09.010; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c; do Carmo M. P., 1992, RIEMANNIAN GEOMETRY; Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; Freifeld O., 2012, EUR C COMP VIS; Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1; Govindu V. M., 2013, INT C COMP VIS; Harandi M., 2012, EUR C COMP VIS; Harandi M.T., 2014, EUR C COMP VIS; Hauberg S, 2012, IMAGE VISION COMPUT, V30, P453, DOI 10.1016/j.imavis.2011.11.009; Huckemann S, 2010, STAT SINICA, V20, P1; Ionescu C., 2011, INT C COMP VIS; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jayasumana S., 2013, IEEE C COMP VIS PATT; Jayasumana S, 2015, IEEE T PATTERN ANAL; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; Lawrence N, 2005, J MACH LEARN RES, V6, P1783; Lawrence N., 2007, INT C MACH LEARN; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; Ledoit Olivier, 2011, 515 U ZUR I EMP RES; Lenglet C, 2006, J MATH IMAGING VIS, V25, P423, DOI 10.1007/s10851-006-6897-z; Li R, 2010, INT J COMPUT VISION, V87, P170, DOI 10.1007/s11263-009-0283-4; Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Muralidharan P., 2012, IEEE C COMP VIS PATT; Ozakin A., 2009, ADV NEURAL INFORM PR, P1375; Pelletier B, 2005, STAT PROBABIL LETT, V73, P297, DOI 10.1016/j.spl.2005.04.004; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Pennec X, 2009, LECT NOTES COMPUT SC, V5416, P347; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Said S., 2007, EUR SIGN PROC C; Sanin A., 2012, INT C IM PROC; Sasaki S., 1958, TOHOKU MATH J, V10, P338, DOI [10.2748/tmj/1178244668, DOI 10.2748/TMJ/1178244668]; Schafer J, 2005, STAT APPL GENET MO B, V4, DOI 10.2202/1544-6115.1175; Shirazi S., 2012, INT C IM PROC; Sigal L., 2004, IEEE C COMP VIS PATT; Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4; Simo-Serra E., 2015, INT C MACH VIS APPL; Simo-Serra E., 2012, IEEE C COMP VIS PATT; Simo-Serra E., 2014, BRIT MACH VIS C; Simo-Serra E., 2013, IEEE C COMP VIS PATT; Sivalingam R., 2010, EUR C COMP VIS; Sminchisescu C, 2003, INT J ROBOT RES, V22, P371, DOI 10.1177/0278364903022006003; Sommer S, 2014, ADV COMPUT MATH, V40, P283, DOI 10.1007/s10444-013-9308-1; Sommer S., 2010, EUR C COMP VIS; Sommer S., 2015, LECT NOTES COMPUTER; Straub J., 2015, INT C ART INT STAT; Taylor G. W., 2010, IEEE C COMP VIS PATT; Tosato D, 2013, IEEE T PATTERN ANAL, V35, P1972, DOI 10.1109/TPAMI.2012.263; Tosato D, 2010, LECT NOTES COMPUT SC, V6312, P378, DOI 10.1007/978-3-642-15552-9_28; Tournier M, 2009, COMPUT GRAPH FORUM, V28, P355, DOI 10.1111/j.1467-8659.2009.01375.x; Turaga P, 2011, IEEE T PATTERN ANAL, V33, P2273, DOI 10.1109/TPAMI.2011.52; Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75; Urtasun R., 2007, P 2 C HUM MOT UND MO; Urtasun Raquel, 2006, IEEE C COMP VIS PATT; Varol A., 2012, IEEE C COMP VIS PATT; WALLACE CS, 1987, J ROY STAT SOC B MET, V49, P240; Wang Jack M., 2005, NEURAL INFORM PROCES; Yao Angela, 2011, NEURAL INFORM PROCES; Zhang Miaomiao, 2013, ADV NEURAL INFORM PR, V26, P2	75	19	19	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		388	408		10.1007/s11263-016-0941-2	http://dx.doi.org/10.1007/s11263-016-0941-2			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS		Green Submitted			2022-12-18	WOS:000398162200011
J	Tzimiropoulos, G; Pantic, M				Tzimiropoulos, Georgios; Pantic, Maja			Fast Algorithms for Fitting Active Appearance Models to Unconstrained Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Active Appearance Models; Face alignment; In-the-wild		Fitting algorithms for Active Appearance Models (AAMs) are usually considered to be robust but slow or fast but less able to generalize well to unseen variations. In this paper, we look into AAM fitting algorithms and make the following orthogonal contributions: We present a simple "project-out" optimization framework that unifies and revises the most well-known optimization problems and solutions in AAMs. Based on this framework, we describe robust simultaneous AAM fitting algorithms the complexity of which is not prohibitive for current systems. We then go on one step further and propose a new approximate project-out AAM fitting algorithm which we coin Extended Project-Out Inverse Compositional (E-POIC). In contrast to current algorithms, E-POIC is both efficient and robust. Next, we describe a part-based AAM employing a translational motion model, which results in superior fitting and convergence properties. We also show that the proposed AAMs, when trained "in-the-wild" using SIFT descriptors, perform surprisingly well even for the case of unseen unconstrained images. Via a number of experiments on unconstrained human and animal face databases, we show that our combined contributions largely bridge the gap between exact and current approximate methods for AAM fitting and perform comparably with state-of-the-art face alignment systems.	[Tzimiropoulos, Georgios] Univ Nottingham, Sch Comp Sci, Nottingham, England; [Pantic, Maja] Imperial Coll London, Dept Comp, London, England; [Pantic, Maja] Univ Twente, Fac Elect Engn Math & Comp Sci, Enschede, Netherlands	University of Nottingham; Imperial College London; University of Twente	Tzimiropoulos, G (corresponding author), Univ Nottingham, Sch Comp Sci, Nottingham, England.	yorgos.tzimiropoulos@nottingham.ac.uk		Tzimiropoulos, Georgios/0000-0002-1803-5338	EPSRC [EP/M02153X/1]; European Community [645094]; Engineering and Physical Sciences Research Council [EP/M02153X/1] Funding Source: researchfish	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Community(European Commission); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The work of Georgios Tzimiropoulos was supported in part by the EPSRC project EP/M02153X/1 Facial Deformable Models of Animals. The work of Maja Pantic was supported in part by the European Community Horizon 2020 [H2020/2014-2020] under grant agreement no. 645094 (SEWA).	[Anonymous], 2012, CVPR; [Anonymous], 2012, CVPR; [Anonymous], 2013, CVPR; Asthana A., 2014, CVPR; Baker S., 2003, CMURITR0335; Batur AU, 2005, IEEE T IMAGE PROCESS, V14, P1707, DOI 10.1109/TIP.2005.854473; Belhumeur P. N., 2011, CVPR; Boyd S, 2004, CONVEX OPTIMIZATION; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cox Mark, 2008, CVPR; Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Gross R, 2005, IMAGE VISION COMPUT, V23, P1080, DOI 10.1016/j.imavis.2005.07.009; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Huang G. B., 2007, ICCV; Kazemi V., 2014, CVPR; Kossaifi J., 2014, ICIP; Kossaifi J., 2015, ICIP; Le V., 2012, ECCV; Liu C., 2008, EUR C COMP VIS ECCV; Lucas Bruce, 1981, IJCAI; Lucey S, 2009, IMAGE VISION COMPUT, V27, P1804, DOI 10.1016/j.imavis.2009.03.002; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Papandreou G., 2008, CVPR; Parkhi O. M., 2012, CVPR; Ren S., 2014, CVPR; Sagonas C., 2013, CVPR W; Saragih J, 2009, PATTERN RECOGN, V42, P2628, DOI 10.1016/j.patcog.2009.04.014; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Solem J. E., 2008, ICPR; Tzimiropoulos G., 2014, CVPR; Tzimiropoulos G., 2013, ICCV; Tzimiropoulos G., 2015, CVPR; Valstar M., 2010, CVPR; Xiong X., 2013, CVPR	37	19	21	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2017	122	1					17	33		10.1007/s11263-016-0950-1	http://dx.doi.org/10.1007/s11263-016-0950-1			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EL2AF	32269419	Green Published, Green Accepted			2022-12-18	WOS:000394421800002
J	Cirujeda, P; Cid, YD; Mateo, X; Binefa, X				Cirujeda, Pol; Dicente Cid, Yashin; Mateo, Xavier; Binefa, Xavier			A 3D Scene Registration Method via Covariance Descriptors and an Evolutionary Stable Strategy Game Theory Solver	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D scene registration; Covariance descriptor; Evolutionary game theory; Feature fusion	OBJECT RECOGNITION; SIGNATURES	In this paper we provide an integrated approach for matching patterns in scenes combining 3D and visual information. For local definition of points we propose a descriptor based on the notion of covariance of features for fusion of shape and color information of 3D surfaces, so-called multi-scale covariance descriptor (MCOV). The intrinsic properties of this descriptor are many: it is invariant to spatial rigid transformations, and robust to noise and resolution changes; it can also be used for characteristic point detection; and lies on top of a manifold topology which allows the use of analytical metric properties. This descriptor is complemented with a game theoretic approach for solving the matching correspondences under global geometric constraints. This layer offers a comprehensive understanding of the scene and avoids possible mismatches due to repeated areas or symmetries-which would be impossibly identified by the detector solely at a local level. Our solution is able to accurately match different views of a scene even under spatial transformations, high noise levels and with small overlap between views, outperforming state-of-the-art approaches. Results are validated by comparing MCOV against other state-of-the-art 3D point descriptor methods, and matching complex 3D and color scenes under several challenging conditions.	[Cirujeda, Pol; Dicente Cid, Yashin; Mateo, Xavier; Binefa, Xavier] Univ Pompeu Fabra, Dept Informat & Commun Technol, Barcelona 08018, Spain	Pompeu Fabra University	Cirujeda, P (corresponding author), Univ Pompeu Fabra, Dept Informat & Commun Technol, Tanger 122-140, Barcelona 08018, Spain.	pol.cirujeda@upf.edu; yashin.dicente@upf.edu; javier.mateo@upf.edu; xavier.binefa@upf.edu		Cirujeda Santolaria, Pol/0000-0001-5374-3519	Spanish Government Ministry of Economy and Competitivity [TIN2012-39203, IPT-2012-0630-020000]	Spanish Government Ministry of Economy and Competitivity	This work has been supported in part by the following research Project Grants: TIN2012-39203 and IPT-2012-0630-020000 awarded by the Spanish Government Ministry of Economy and Competitivity.	Albarelli A, 2010, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2010.5540183; Albarelli A, 2009, IEEE I CONF COMP VIS, P1319, DOI 10.1109/ICCV.2009.5459312; [Anonymous], 1997, CMURIT9747 ROB I CAR; Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Brusco N, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P262, DOI 10.1109/3DIM.2005.5; Chen CS, 1999, IEEE T PATTERN ANAL, V21, P1229, DOI 10.1109/34.809117; Cherian A, 2011, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2011.6126523; Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Chum O., 2004, P ACCV, V2, P812; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; Fehr D, 2012, IEEE INT CONF ROBOT, P1793, DOI 10.1109/ICRA.2012.6224740; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Flint A., 2007, P 9 BIENN C AUSTR PA, P182, DOI DOI 10.1109/DICTA.2007.4426794; Forstner W., 1999, QUO VADIS GEODESIA, V113, P113; Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224; Hartley R., 2003, MULTIPLE VIEW GEOMET; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kovnatsky A, 2012, LECT NOTES COMPUT SC, V6667, P616, DOI 10.1007/978-3-642-24785-9_52; Lai K, 2011, IEEE INT CONF ROBOT, P1817; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luebke DP, 2001, IEEE COMPUT GRAPH, V21, P24, DOI 10.1109/38.920624; Rodola E, 2013, INT J COMPUT VISION, V102, P129, DOI 10.1007/s11263-012-0568-x; Rusu R., 2009, IEEE INT C ROB AUT I, P3212, DOI DOI 10.1109/R0B0T.2009.5152473; Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Torsello A., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P290, DOI 10.1109/3DIMPVT.2011.43; Tuzel O., 2007, PROC CVPR IEEE, P1, DOI [DOI 10.1109/CVPR.2007.383197, 10.1109/CVPR.2007.383197]; Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Wilks SS, 1932, BIOMETRIKA, V24, P471; Yao J., 2008, INT WORKSH VIS SURV; Zaharescu A, 2012, INT J COMPUT VISION, V100, P78, DOI 10.1007/s11263-012-0528-5; Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748	36	19	21	0	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2015	115	3					306	329		10.1007/s11263-015-0820-2	http://dx.doi.org/10.1007/s11263-015-0820-2			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CW6EH					2022-12-18	WOS:000365089800005
J	Mille, J; Bougleux, S; Cohen, LD				Mille, Julien; Bougleux, Sebastien; Cohen, Laurent D.			Combination of Piecewise-Geodesic Paths for Interactive Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							IMAGE SEGMENTATION; ACTIVE CONTOURS; REGION MODELS; ALGORITHMS; EXTRACTION; DRIVEN	Minimum cost paths have been extensively studied theoretical tools for interactive image segmentation. The existing geodesically linked active contour (GLAC) model, which basically consists of a set of vertices connected by paths of minimal cost, blends the benefits of minimal paths and region-based active contours. This results in a closed piecewise-smooth curve, over which an edge or region energy functional can be formulated. As an important shortcoming, the GLAC in its initial formulation does not guarantee the curve to be simple, consistent with respect to the purpose of segmentation. In this paper, we draw our inspiration from the GLAC and other boundary-based interactive segmentation algorithms, in the sense that we aim to extract a contour given a set of user-provided points, by connecting these points using paths. The key idea is to select a combination among a set of possible paths, such that the resulting structure represents a relevant closed curve. Instead of considering minimal paths only, we switch to a more general formulation, which we refer to as admissible paths. These basically correspond to the roads travelling along the bottom of distinct valleys between given endpoints. We introduce a novel term to favor the simplicity of the generated contour, as well as a local search method to choose the best combination among possible paths.	[Mille, Julien] Univ Lyon 1, CNRS, LIRIS, UMR5205, F-69622 Villeurbanne, France; [Bougleux, Sebastien] Univ Caen Basse Normandie, UMR6072, GREYC, F-14050 Caen, France; [Cohen, Laurent D.] Univ Paris 09, UMR7534, CEREMADE, F-75016 Paris, France	Centre National de la Recherche Scientifique (CNRS); Institut National des Sciences Appliquees de Lyon - INSA Lyon; UDICE-French Research Universities; Universite Claude Bernard Lyon 1; Universite de Caen Normandie; UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine	Mille, J (corresponding author), Univ Lyon 1, CNRS, LIRIS, UMR5205, F-69622 Villeurbanne, France.	julien.mille@liris.cnrs.fr; sebastien.bougleux@unicaen.fr; cohen@ceremade.dauphine.fr	Bougleux, Sébastien/AAG-6982-2019	Bougleux, Sébastien/0000-0002-4581-7570				Adams C. C., 2004, KNOT BOOK ELEMENTARY; Appleton B, 2006, IEEE T PATTERN ANAL, V28, P106, DOI 10.1109/TPAMI.2006.12; Arnold V. I., 1994, ADV SOVIET MATH, V21, P33; Ben-Zadok N, 2009, I S BIOMED IMAGING, P1079, DOI 10.1109/ISBI.2009.5193243; Benmansour F, 2009, J MATH IMAGING VIS, V33, P209, DOI 10.1007/s10851-008-0131-0; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; Bresson X, 2007, J MATH IMAGING VIS, V28, P151, DOI 10.1007/s10851-007-0002-0; Brox T, 2009, INT J COMPUT VISION, V84, P184, DOI 10.1007/s11263-008-0153-5; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; CRANDALL MG, 1992, B AM MATH SOC, V27, P1, DOI 10.1090/S0273-0979-1992-00266-5; Cremers D, 2007, PROC SPIE, V6512, DOI 10.1117/12.708609; Eppstein D, 1998, SIAM J COMPUT, V28, P652, DOI 10.1137/S0097539795290477; Falcao AX, 2004, IEEE T PATTERN ANAL, V26, P19, DOI 10.1109/TPAMI.2004.1261076; Falcao AX, 2000, IEEE T MED IMAGING, V19, P55, DOI 10.1109/42.832960; Gao Y, 2012, MED IMAGE ANAL, V16, P1216, DOI 10.1016/j.media.2012.06.002; Gerard O, 2002, LECT NOTES COMPUT SC, V2352, P807; Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233; IVINS J, 1995, IMAGE VISION COMPUT, V13, P431, DOI 10.1016/0262-8856(95)99730-O; Karasev P, 2013, IEEE T MED IMAGING, V32, P2127, DOI 10.1109/TMI.2013.2274734; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kaul V, 2012, IEEE T PATTERN ANAL, V34, P1952, DOI 10.1109/TPAMI.2011.267; Kim JM, 2005, IEEE T IMAGE PROCESS, V14, P1486, DOI 10.1109/TIP.2005.854442; Lankton S, 2008, IEEE T IMAGE PROCESS, V17, P2029, DOI 10.1109/TIP.2008.2004611; Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; Michailovich O, 2007, IEEE T IMAGE PROCESS, V16, P2787, DOI 10.1109/TIP.2007.908073; Mille J., 2012, INT C COMP VIS THEOR; Mille J, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.133; Mille J, 2009, LECT NOTES COMPUT SC, V5567, P163, DOI 10.1007/978-3-642-02256-2_14; Miranda PAV, 2012, IEEE T IMAGE PROCESS, V21, P3042, DOI 10.1109/TIP.2012.2188034; Mortensen EN, 1998, GRAPH MODEL IM PROC, V60, P349, DOI 10.1006/gmip.1998.0480; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Paragios N, 2002, INT J COMPUT VISION, V46, P223, DOI 10.1023/A:1014080923068; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Sagiv C, 2006, IEEE T IMAGE PROCESS, V15, P1633, DOI 10.1109/TIP.2006.871133; Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591; SETHIAN JA, 1999, LEVEL SETS METHODS F; TSITSIKLIS JN, 1995, IEEE T AUTOMAT CONTR, V40, P1528, DOI 10.1109/9.412624; Unal G, 2005, INT J COMPUT VISION, V62, P199, DOI 10.1007/s11263-005-4880-6; Vicente S., 2008, IEEE COMPUTER VISION; Whitney H., 1937, COMPOS MATH, V4, P276; YEN JY, 1971, MANAGE SCI, V17, P712, DOI 10.1287/mnsc.17.11.712	45	19	19	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2015	112	1					1	22		10.1007/s11263-014-0751-3	http://dx.doi.org/10.1007/s11263-014-0751-3			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CC4YC		Green Submitted			2022-12-18	WOS:000350361500001
J	Biswas, A; Jacobs, D				Biswas, Arijit; Jacobs, David			Active Image Clustering with Pairwise Constraints from Humans	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Clustering; Active learning; Human in the loop; Pairwise constraints; Image labeling		We propose a method of clustering images that combines algorithmic and human input. An algorithm provides us with pairwise image similarities. We then actively obtain selected, more accurate pairwise similarities from humans. A novel method is developed to choose the most useful pairs to show a person, obtaining constraints that improve clustering. In a clustering assignment, elements in each data pair are either in the same cluster or in different clusters. We simulate inverting these pairwise relations and see how that affects the overall clustering. We choose a pair that maximizes the expected change in the clustering. The proposed algorithm has high time complexity, so we also propose a version of this algorithm that is much faster and exactly replicates our original algorithm. We further improve run-time by adding two heuristics, and show that these do not significantly impact the effectiveness of our method. We have run experiments in three different domains, namely leaf, face and scene images, and show that the proposed method improves clustering performance significantly.	[Biswas, Arijit; Jacobs, David] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Biswas, A (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	arijitbiswas87@gmail.com; djacobs@umiacs.umd.edu			Divn Of Social and Economic Sciences [0968546] Funding Source: National Science Foundation	Divn Of Social and Economic Sciences(National Science Foundation (NSF)NSF - Directorate for Social, Behavioral & Economic Sciences (SBE))		Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1023/A:1022821128753; Basu S., 2008, DATA MINING KNOWLEDG; Basu S., 2004, 4 SIAM INT C DAT MIN; Basu S., 2002, P 19 INT C MACH LEAR, P27; Biswas A., 2011, INT C MACH LEARN 201; Biswas A, 2012, PROC CVPR IEEE, P2152, DOI 10.1109/CVPR.2012.6247922; Branson S, 2011, IEEE I CONF COMP VIS, P1832, DOI 10.1109/ICCV.2011.6126450; Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32; Chellappa R, 1993, MARKOV RANDOM FIELDS; Dagli CK, 2006, INT C PATT RECOG, P506; Davidson I, 2009, DATA MIN KNOWL DISC, V18, P257, DOI 10.1007/s10618-008-0103-4; Felzenszwalb P, 2008, 2008 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2008.4587597; Gomes Ryan G, 2011, NEURAL INFORM PROCES, P558; Grira N., 2005, P 7 ACM SIGMM INT WO, P9, DOI DOI 10.1145/1101826.1101831; Guo YH, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P823; Holub Alex, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563068; Huang R., 2007, SDM SIAM; Huang RZ, 2009, DATA KNOWL ENG, V68, P49, DOI 10.1016/j.datak.2008.08.008; Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011; Jain P, 2009, PROC CVPR IEEE, P762, DOI 10.1109/CVPRW.2009.5206651; Joshi AJ, 2010, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2010.5540047; Kapoor A, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P877; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Macqueen J., 1967, P 5 BERK S MATH STAT, P281; Mallapragada PK, 2008, INT C PATT RECOG, P2376; Martinez A., 1998, 24 CVC, P24; Punera K, 2008, APPL ARTIF INTELL, V22, P780, DOI 10.1080/08839510802170546; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Settles B., 2010, TECHNICAL REPORT; Siddiquie B, 2010, PROC CVPR IEEE, P2979, DOI 10.1109/CVPR.2010.5540044; Sugar CA, 2003, J AM STAT ASSOC, V98, P750, DOI 10.1198/016214503000000666; Tan PN, 2016, INTRO DATA MINING; Vijayanarasimhan S, 2012, LECT NOTES COMPUT SC, V7576, P496, DOI 10.1007/978-3-642-33715-4_36; Vijayanarasimhan S, 2011, PROC CVPR IEEE, P1449, DOI 10.1109/CVPR.2011.5995430; Vijayanarasimhan S, 2010, PROC CVPR IEEE, P3035, DOI 10.1109/CVPR.2010.5540055; Wagstaff K., 2001, ICML, V1, P577, DOI DOI 10.1109/TPAMI.2002.1017616; Wang F, 2009, IEEE DATA MINING, P562, DOI 10.1109/ICDM.2009.66; Xu Q., 2005, DISCOVERY SCI, V3735; [No title captured]	41	19	19	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2014	108	1-2			SI		133	147		10.1007/s11263-013-0680-6	http://dx.doi.org/10.1007/s11263-013-0680-6			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AG7BT					2022-12-18	WOS:000335573700009
J	Calderero, F; Caselles, V				Calderero, Felipe; Caselles, Vicent			Recovering Relative Depth from Low-Level Features Without Explicit T-junction Detection and Interpretation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Low-level image features; Monocular depth; Relative depth order; Multi-scale analysis	BORDER OWNERSHIP; NATURAL IMAGES; MODEL; ALGORITHM; SHAPE; RECONSTRUCTION; PHOTOGRAPHY; INTEGRATION; COMPLETION; CONTOUR	This work presents a novel computational model for relative depth order estimation from a single image based on low-level local features that encode perceptual depth cues such as convexity/concavity, inclusion, and T-junctions in a quantitative manner, considering information at different scales. These multi-scale features are based on a measure of how likely is a pixel to belong simultaneously to different objects (interpreted as connected components of level sets) and, hence, to be occluded in some of them, providing a hint on the local depth order relationships. They are directly computed on the discrete image data in an efficient manner, without requiring the detection and interpretation of edges or junctions. Its behavior is clarified and illustrated for some simple images. Then the recovery of the relative depth order on the image is achieved by global integration of these local features applying a non-linear diffusion filtering of bilateral type. The validity of the proposed features and the integration approach is demonstrated by experiments on real images and comparison with state-of-the-art monocular depth estimation techniques.	[Calderero, Felipe; Caselles, Vicent] Univ Pompeu Fabra, Barcelona 08018, Spain	Pompeu Fabra University	Calderero, F (corresponding author), Univ Pompeu Fabra, Roc Boronat 138, Barcelona 08018, Spain.	felipe.calderero@upf.edu; vicent.caselles@upf.edu			MICINN [MTM2009-08171]; GRC [2009 SGR 773]; Generalitat de Catalunya	MICINN(Ministry of Science and Innovation, Spain (MICINN)Spanish GovernmentEuropean Commission); GRC(Science Foundation Ireland); Generalitat de Catalunya(Generalitat de Catalunya)	The authors would like to thank Mariella Dimiccoli, Guillem Palou, Philippe Salembier, and Michael Maire for their time and effort, and for kindly providing the state-of-the-art results shown in this work for comparison. We acknowledge partial support by MICINN project, reference MTM2009-08171, and by GRC reference 2009 SGR 773. VC also acknowledges partial support by "ICREA Academia" prize for excellence in research funded by the Generalitat de Catalunya.	Alvarez L, 1999, LECT NOTES COMPUT SC, V1682, P247; Alvarez L, 1999, ADV IMAG ELECT PHYS, V111, P167, DOI 10.1016/S1076-5670(08)70218-0; Amer M, 2010, P INT C IM PROC; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bordenave C, 2006, ADV APPL PROBAB, V38, P31, DOI 10.1239/aap/1143936138; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Buades A, 2010, IEEE T IMAGE PROCESS, V19, P1978, DOI 10.1109/TIP.2010.2046605; Caselles V, 2010, LECT NOTES MATH, V1984, P1, DOI 10.1007/978-3-642-04611-7; Caselles V., 1996, ICAOS'96. 12th International Conference on Analysis and Optimization of Systems. Images, Wavelets and PDEs, P356; Caselles V, 1999, INT J COMPUT VISION, V33, P5, DOI 10.1023/A:1008144113494; DARRELL T, 1995, IEEE T PATTERN ANAL, V17, P474, DOI 10.1109/34.391395; Dimiccoli M, 2009, IEEE IMAGE PROC, P3533, DOI 10.1109/ICIP.2009.5414079; Dimiccoli M, 2009, INT CONF ACOUST SPEE, P1229, DOI 10.1109/ICASSP.2009.4959812; Dimiccoli M, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P95, DOI 10.1109/ICVGIP.2008.97; Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778; Favaro P, 2008, IEEE T PATTERN ANAL, V30, P518, DOI 10.1109/TPAMI.2007.1175; Feldman D, 2008, IEEE T PATTERN ANAL, V30, P1171, DOI 10.1109/TPAMI.2007.70766; Fowlkes CC, 2007, J VISION, V7, DOI 10.1167/7.8.2; Froyen V, 2011, PERCEPTION, V40, P175; Froyen V, 2010, J VISION, V10, P1176; Froyen V, 2010, ADV NEURAL INFORM PR, V23, P631; Gao RX, 2007, LECT NOTES COMPUT SC, V4679, P213; Gibson J.J., 1979, ECOLOGICAL APPROACH, pp. 119; Goldstein E. B, 2002, SENSATION PERCEPTION; Gousseau Y, 2001, SIAM J MATH ANAL, V33, P634, DOI 10.1137/S0036141000371150; Hoiem D, 2011, INT J COMPUT VISION, V91, P328, DOI 10.1007/s11263-010-0400-4; Howard I.P., 2012, PERCEIVING DEPTH, V29; Kanizsa G., 1980, GRAMMATICA VEDERE SA; Kim SH, 2009, J VISION, V9, DOI 10.1167/9.10.8; Kogo N, 2011, J VISION, V11, P1100; Kogo N, 2011, VISION RES, V51, P2085, DOI 10.1016/j.visres.2011.08.010; Kogo N, 2010, PSYCHOL REV, V117, P406, DOI 10.1037/a0019076; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Lee SH, 2011, IEEE T CONSUM ELECTR, V57, P1018, DOI 10.1109/TCE.2011.6018850; Leichter I, 2009, IEEE I CONF COMP VIS, P9, DOI 10.1109/ICCV.2009.5459208; Lindeberg T., 1994, SCALE SPACE THEORY C; Liu BY, 2010, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2010.5539823; Maire M, 2010, LECT NOTES COMPUT SC, V6312, P450, DOI 10.1007/978-3-642-15552-9_33; Marr D., 1982, VISION COMPUTATIONAL; Metzger W., 1975, GESETZE SEHENS LEHRE; Namboodiri V, 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587779; Nitzberg M., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P138, DOI 10.1109/ICCV.1990.139511; Nitzberg M, 1993, FILTERING SEGMENTATI, V662; Palou G, 2011, INT CONF ACOUST SPEE, P1093; Parida L, 1998, IEEE T PATTERN ANAL, V20, P687, DOI 10.1109/34.689300; Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8; Peterson MA, 2008, J EXP PSYCHOL HUMAN, V34, P251, DOI 10.1037/0096-1523.34.2.251; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Pham T. Q., 2005, 2005 IEEE International Conference on Multimedia and Expo; Rensink RA, 1998, VISION RES, V38, P2489, DOI 10.1016/S0042-6989(98)00051-0; Rubin N, 2001, NAT NEUROSCI, V4, P857, DOI 10.1038/nn0901-857; Saxena A, 2008, INT J COMPUT VISION, V76, P53, DOI 10.1007/s11263-007-0071-y; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; Soille P., 2013, MORPHOLOGICAL IMAGE; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Torralba A, 2002, IEEE T PATTERN ANAL, V24, P1226, DOI 10.1109/TPAMI.2002.1033214; VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Williams LR, 1997, NEURAL COMPUT, V9, P837, DOI 10.1162/neco.1997.9.4.837; Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211; Yaroslavsky L, 1985, DIGITAL PICTURE PROC, V1; Yu SX, 2009, PROC CVPR IEEE, P2302, DOI 10.1109/CVPRW.2009.5206673; Zhou H, 2000, J NEUROSCI, V20, P6594, DOI 10.1523/JNEUROSCI.20-17-06594.2000	65	19	20	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2013	104	1					38	68		10.1007/s11263-013-0613-4	http://dx.doi.org/10.1007/s11263-013-0613-4			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	179LY					2022-12-18	WOS:000321523000003
J	Levinshtein, A; Sminchisescu, C; Dickinson, S				Levinshtein, Alex; Sminchisescu, Cristian; Dickinson, Sven			Optimal Image and Video Closure by Superpixel Grouping	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Closure; Segmentation; Perceptual grouping; Superpixels; Figure/ground segmentation; Motion segmentation	OBJECT SEGMENTATION; COMBINING BOUNDARY; REGION; TRACKING; REPRESENTATION; EXTRACTION; ROBUST; COLOR	Detecting independent objects in images and videos is an important perceptual grouping problem. One common perceptual grouping cue that can facilitate this objective is the cue of contour closure, reflecting the spatial coherence of objects in the world and their projections as closed boundaries separating figure from background. Detecting contour closure in images consists of finding a cycle of disconnected contour fragments that separates an object from its background. Searching the entire space of possible groupings is intractable, and previous approaches have adopted powerful perceptual grouping heuristics, such as proximity and co-curvilinearity, to constrain the search. We introduce a new formulation of the problem, by transforming the problem of finding cycles of contour fragments to finding subsets of superpixels whose collective boundary has strong edge support (few gaps) in the image. Our cost function, a ratio of a boundary gap measure to area, promotes spatially coherent sets of superpixels. Moreover, its properties support a global optimization procedure based on parametric maxflow. Extending closure detection to videos, we introduce the concept of spatiotemporal closure. Analogous to image closure, we formulate our spatiotemporal closure cost over a graph of spatiotemporal superpixels. Our cost function is a ratio of motion and appearance discontinuity measures on the boundary of the selection to an internal homogeneity measure of the selected spatiotemporal volume. The resulting approach automatically recovers coherent components in images and videos, corresponding to objects, object parts, and objects with surrounding context, providing a good set of multiscale hypotheses for high-level scene analysis. We evaluate both our image and video closure frameworks by comparing them to other closure detection approaches, and find that they yield improved performance.	[Levinshtein, Alex; Dickinson, Sven] Univ Toronto, Toronto, ON M5S 3G4, Canada; [Sminchisescu, Cristian] Univ Bonn, Inst Numer Simulat, D-53115 Bonn, Germany	University of Toronto; University of Bonn	Levinshtein, A (corresponding author), Univ Toronto, 6 Kings Coll Rd,Pratt Bldg, Toronto, ON M5S 3G4, Canada.	babalex@cs.toronto.edu; cristian.sminchisescu@ins.uni-bonn.de; sven@cs.toronto.edu			European Commission under Marie Curie Excellence Grant [MCEXT-025481]; CNCSIS-UEFISCU [PN II- RU-RC-2/2009]; NSERC; MITACS; DARPA	European Commission under Marie Curie Excellence Grant; CNCSIS-UEFISCU(Consiliul National al Cercetarii Stiintifice (CNCS)); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); MITACS; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We thank Allan Jepson for discussion about closure cost functions and optimization procedures, and Yuri Boykov and Vladimir Kolmogorov for providing their parametric maxflow implementation. This work was supported in part by the European Commission under a Marie Curie Excellence Grant MCEXT-025481 (Cristian Sminchisescu), CNCSIS-UEFISCU under project number PN II- RU-RC-2/2009 (Cristian Sminchisescu), CNCSIS-UEFISCU under project number PN II- RU-RC-2/2009 (Cristian Sminchisescu), NSERC (Alex Levinshtein, Sven Dickinson), MITACS (Alex Levinshtein), and DARPA (Sven Dickinson).	Alpert S, 2007, PROC CVPR IEEE, P359; [Anonymous], 1985, PERCEPTUAL ORG VISUA; [Anonymous], 1938, SOURCE BOOK GESTALT; BASCLE B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P302, DOI 10.1109/ICCV.1995.466925; Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436; Borenstein E, 2002, LECT NOTES COMPUT SC, V2351, P109; Boykov Yuri, 2001, INTERACTIVE GRAPH CU; BRADY M, 1984, INT J ROBOT RES, V3, P36, DOI 10.1177/027836498400300302; Carreira J, 2010, PROC CVPR IEEE, P3241, DOI 10.1109/CVPR.2010.5540063; Cham T.-J., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P385, DOI 10.1007/BFb0015552; Chung D, 2006, IMAGE VISION COMPUT, V24, P680, DOI 10.1016/j.imavis.2005.09.020; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; Cremers D, 2005, INT J COMPUT VISION, V62, P249, DOI 10.1007/s11263-005-4882-4; DeMenthon D., 2002, SMVP; Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985; Dinkelbach W., 1967, MANAGE SCI, V13, P492, DOI 10.1287/mnsc.13.7.492; ELDER J, 1994, VISION RES, V34, P3361, DOI 10.1016/0042-6989(94)90070-1; Elder J. H., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P399, DOI 10.1007/BFb0015553; Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42; Estrada F. J., 2006, COMP VIS PATT REC WO, P184; Estrada FJ, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334008; Fowlkes C, 2001, PROC CVPR IEEE, P231; Gelgon M, 2000, PATTERN RECOGN, V33, P725, DOI 10.1016/S0031-3203(99)00083-7; Greenspan H, 2004, IEEE T PATTERN ANAL, V26, P384, DOI 10.1109/TPAMI.2004.1262334; Huang YC, 2009, PROC CVPR IEEE, P1738, DOI 10.1109/CVPRW.2009.5206795; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; Jepson AD, 2002, LECT NOTES COMPUT SC, V2350, P692; Jermyn IH, 2001, IEEE T PATTERN ANAL, V23, P1075, DOI 10.1109/34.954599; Jojic N, 2001, PROC CVPR IEEE, P199; Kolmogorov V, 2007, IEEE I CONF COMP VIS, P644; Lempitsky V., 2009, IMAGE SEGMENTATION B; Levinshtein A., 2010, P AS C COMP VIS, P369; Levinshtein A, 2009, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2009.5459472; Levinshtein A, 2010, LECT NOTES COMPUT SC, V6312, P480, DOI 10.1007/978-3-642-15552-9_35; Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96; Li FX, 2010, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2010.5539839; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Maire M, 2008, PROC CVPR IEEE, P611; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Megret R., 2002, SURVEY SPATIO TEMPOR; MORI G, 2004, PROC CVPR IEEE, P326; Moscheni F, 1998, IEEE T PATTERN ANAL, V20, P897, DOI 10.1109/34.713358; Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758; Patras I, 2001, IEEE T PATTERN ANAL, V23, P326, DOI 10.1109/34.910886; Pritch Y, 2008, IEEE T PATTERN ANAL, V30, P1971, DOI 10.1109/TPAMI.2008.29; Ren X., 2005, ADV NEURAL INFORM PR; Ren XF, 2005, IEEE I CONF COMP VIS, P1214; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; SAINTMARC P, 1993, IEEE T PATTERN ANAL, V15, P1191, DOI 10.1109/34.244680; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shi JB, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1154, DOI 10.1109/ICCV.1998.710861; Stahl JS, 2008, IEEE T PATTERN ANAL, V30, P395, DOI 10.1109/TPAMI.2007.1186; Stahl JS, 2007, IEEE T IMAGE PROCESS, V16, P2590, DOI 10.1109/TIP.2007.904463; Stein A, 2007, IEEE I CONF COMP VIS, P110; Wang DM, 1998, IEEE T CIRC SYST VID, V8, P539, DOI 10.1109/76.718501; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Wang S, 2005, IEEE T PATTERN ANAL, V27, P546, DOI 10.1109/TPAMI.2005.84; Weiss Y, 1997, PROC CVPR IEEE, P520, DOI 10.1109/CVPR.1997.609375; Weiss Y, 1996, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.1996.517092; Williams LR, 1996, COMPUT VIS IMAGE UND, V64, P1, DOI 10.1006/cviu.1996.0043; WILLIAMS LR, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P408, DOI 10.1109/ICCV.1995.466910; YlaJaaski A, 1996, COMPUT VIS IMAGE UND, V63, P399, DOI 10.1006/cviu.1996.0031; Zhu QH, 2007, IEEE I CONF COMP VIS, P793	65	19	20	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2012	100	1					99	119		10.1007/s11263-012-0527-6	http://dx.doi.org/10.1007/s11263-012-0527-6			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	962SH					2022-12-18	WOS:000305564800006
J	Valenti, R; Sebe, N; Gevers, T				Valenti, Roberto; Sebe, Nicu; Gevers, Theo			What Are You Looking at?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						HCI; Gaze estimation; Saliency; Head pose; Eye location		In this paper we present a novel mechanism to obtain enhanced gaze estimation for subjects looking at a scene or an image. The system makes use of prior knowledge about the scene (e.g. an image on a computer screen), to define a probability map of the scene the subject is gazing at, in order to find the most probable location. The proposed system helps in correcting the fixations which are erroneously estimated by the gaze estimation device by employing a saliency framework to adjust the resulting gaze point vector. The system is tested on three scenarios: using eye tracking data, enhancing a low accuracy webcam based eye tracker, and using a head pose tracker. The correlation between the subjects in the commercial eye tracking data is improved by an average of 13.91%. The correlation on the low accuracy eye gaze tracker is improved by 59.85%, and for the head pose tracker we obtain an improvement of 10.23%. These results show the potential of the system as a way to enhance and self-calibrate different visual gaze estimation systems.	[Valenti, Roberto; Gevers, Theo] Univ Amsterdam, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands; [Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38100 Povo, Trento, Italy	University of Amsterdam; University of Trento	Valenti, R (corresponding author), Univ Amsterdam, Intelligent Syst Lab Amsterdam, Sci Pk 904, NL-1098 XH Amsterdam, Netherlands.	rvalenti@science.uva.nl; sebe@disi.unitn.it; th.gevers@uva.nl		Valenti, Roberto/0000-0002-3774-4092; Sebe, Niculae/0000-0002-6597-7248				BATES R, 2005, COGAIN C COMM GAZ IN; Cristinacce D., 2004, BMVC; Einhauser W, 2008, J VISION, V8, DOI 10.1167/8.14.18; GEISLER WS, 1995, HDB OPTICS, V1; Hansen D. W., 2010, PAMI, V32; Itti L., 1998, PAMI, V20; Judd T., 2009, ICCV; Kroon B., 2008, FG; Langton S. R., 2004, PERCEPTION PSYCHOPHY, V66; Liu T., 2007, CVPR; Ma Y-F, 2003, INT C MULTIMEDIA; Murphy-Chutorian E., 2009, PAMI, V31; Peters R. J., 2005, J VISION, V5; Rossi E. A., 2009, NATURE NEUROSCIENCE, V13; Smith K., 2008, PAMI, V30; Spain M., 2008, ECCV; Valenti R., 2008, CVPR; Valenti R., 2009, ICCV; Xiao J., 2002, FG; Zhu J., 2002, FGR	20	19	22	0	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2012	98	3					324	334		10.1007/s11263-011-0511-6	http://dx.doi.org/10.1007/s11263-011-0511-6			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	934NI		Bronze, Green Published			2022-12-18	WOS:000303450600005
J	Koppal, SJ; Yamazaki, S; Narasimhan, SG				Koppal, Sanjeev J.; Yamazaki, Shuntaro; Narasimhan, Srinivasa G.			Exploiting DLP Illumination Dithering for Reconstruction and Photography of High-Speed Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Active vision; DMD; DLP; High speed camera; Temporal dithering	STRUCTURED LIGHT	In this work, we recover fast moving scenes by exploiting the high-speed illumination "dithering" of cheap and easily available digital light processing (DLP) projectors. We first show how to reverse-engineer the temporal dithering for off-the-shelf projectors, using a high-speed camera. DLP dithering can produce temporal patterns commonly used in active vision techniques. Since the dithering occurs at a very high frame-rate, such illumination-based methods can be "speed up" for fast scenes. We demonstrate this with three applications, each of which only requires a single slide to be displayed by the DLP projector. The quality of the result is determined by the camera frame-rate available to the user. Pairing a high-speed camera and a DLP projector, we demonstrate structured light reconstruction at 100 Hz. With the same camera and three or more DLP projectors, we show photometric stereo and demultiplexing applications at 300 Hz. Finally, with a real-time (60 Hz) or still camera, we show that DLP illumination acts as a very fast flash, allowing strobe photography of high-speed scenes. We discuss, in depth, some characteristics of the temporal dithering with a case study of a particular projector. Finally, we describe limitations, trade-offs and other issues relating to this work.	[Koppal, Sanjeev J.; Narasimhan, Srinivasa G.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Yamazaki, Shuntaro] Natl Inst Adv Ind Sci & Technol, Tsukuba, Ibaraki, Japan	Carnegie Mellon University; National Institute of Advanced Industrial Science & Technology (AIST)	Koppal, SJ (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	sjkoppal@cmu.edu; shun-yamazaki@aist.go.jp; srinivas@cs.cmu.edu			ONR [N00014-08-1-0330]; NSF [IIS-0643628]; DURIP [N00014-06-1-0762]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); DURIP	This research was supported in parts by ONR grants N00014-08-1-0330 and DURIP N00014-06-1-0762, and NSF CAREER award IIS-0643628. The authors thank the anonymous reviewers for their useful comments.	Besl P. J., 1988, Machine Vision and Applications, V1, P127, DOI 10.1007/BF01212277; BITNER JR, 1976, COMMUN ACM, V19, P517, DOI 10.1145/360336.360343; BOYER KL, 1987, IEEE T PATTERN ANAL, V9, P14, DOI 10.1109/TPAMI.1987.4767869; Chen Q., 2005, 3D DIGITAL IMAGING M; COTTING D, 2004, ISMAR; Curless B., 1995, ICCV; DAVIS J, 2003, IEEE CVPR; Dudley D., 2003, P SPIE, V4985; Freeman W., 2003, CVPR; Hall-Holt O, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P359, DOI 10.1109/ICCV.2001.937648; HERTZMANN A, 2003, IEEE CVPR; Horn E, 1999, IMAGE VISION COMPUT, V17, P87, DOI 10.1016/S0262-8856(98)00113-9; Ishikawa K., 1999, DIPL WORKSH; Jarvenpaa T, 2005, J SOC INFORM DISPLAY; Jones Andrew, 2007, ACM SIGGRAPH; Kima S., 2007, DISPLAYS; Koppal S. J., 2009, PROCAMS; Koppal S. J., 2010, DLP PHOTOGRAPH WEBSI; Koppal S. J., 2010, TEMPORAL DITHERING W; Lee J., 2005, S US INT SOFTW TECHN; McDowall I., 2005, IEEE VR WORKSH EM DI; MIYASAKA T, 2000, P 19 C INT SOC PHOT, P65; Mori M., 1999, J SOC INFORM DISPLAY; Morita H., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P468, DOI 10.1109/CCV.1988.590025; Narasimhan S. G., 2008, P EUR C COMP VIS OCT; NAYAR SK, 2004, IEEE CVPR; NAYAR SK, 2006, ACM SIGGRAPH; Nii H., 2005, IEEE PROCAMS; Ogata M., 2005, DISPLAY TECHNOLOGY; Packer O., 2001, VISION RES; POSDAMER JL, 1982, COMPUT VISION GRAPH, V18, P1, DOI 10.1016/0146-664X(82)90096-X; Raskar R, 2001, PROC CVPR IEEE, P504; RASKAR R, 1998, ACM SIGGRAPH; Raskar R., 2006, ACM SIGGRAPH; Salvi J, 2004, PATTERN RECOGN, V37, P827, DOI 10.1016/j.patcog.2003.10.002; Sato K., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P657; Scharstein D., 2003, CVPR; Schechner Y. Y., 2003, ICCV, p[2, 5]; SEN P, 2005, ACM SIGGRAPH; Takhar D, 2006, PROC SPIE, V6065, DOI 10.1117/12.659602; Umezawa E., 2004, 45 ANN C JAP ERG ASS; Weise T., 2007, IEEE C COMPUTER VISI, P1; WENGER A, 2005, ACM SIGGRAPH; Will P. M., 1971, AI, V2; WOODHAM R, 1980, OPTENG, V19; You Y., 1996, IEEE T IMAGE PROCESS; YOUNG M, 2007, IEEE CVPR; Zhang L., 2002, 3DPVT; Zhang L., 2003, IEEE CVPR; ZHANG L, 2006, ACM SIGGRAPH; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; ZICKLER T, 2002, ECCV	52	19	19	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2012	96	1					125	144		10.1007/s11263-011-0454-y	http://dx.doi.org/10.1007/s11263-011-0454-y			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	872LF		Green Submitted			2022-12-18	WOS:000298810300007
J	Dong, YQ; Hintermuller, M; Rincon-Camacho, MM				Dong, Yiqiu; Hintermueller, Michael; Rincon-Camacho, M. Monserrat			A Multi-Scale Vectorial L (tau) -TV Framework for Color Image Restoration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-scale vectorial total variation; Color image; L-2-data fidelity; L-1-data fidelity; Gaussian noise; Salt-and-pepper noise	REGULARIZATION; MINIMIZATION	A general multi-scale vectorial total variation model with spatially adapted regularization parameter for color image restoration is introduced in this paper. This total variation model contains an L (tau) -data fidelity for any tau a[1,2]. The use of a spatial dependent regularization parameter improves the reconstruction of features in the image as well as an adequate smoothing for the homogeneous parts. The automated adaptation of this regularization parameter is made according to local statistical characteristics of the noise which contaminates the image. The corresponding multiscale vectorial total variation model is solved by Fenchel-duality and inexact semismooth Newton techniques. Numerical results are presented for the cases tau=1 and tau=2 which reconstruct images contaminated with salt-and-pepper noise and Gaussian noise, respectively.	[Dong, Yiqiu; Rincon-Camacho, M. Monserrat] Graz Univ, Inst Math & Sci Comp, A-8010 Graz, Austria; [Hintermueller, Michael] Humboldt Univ, Dept Math, D-10099 Berlin, Germany	University of Graz; Humboldt University of Berlin	Dong, YQ (corresponding author), Graz Univ, Inst Math & Sci Comp, Heinrichstr 36, A-8010 Graz, Austria.	yiqiu.dong@uni-graz.at; hint@math.hu-berlin.de; maria.rincon-camacho@uni-graz.at		Dong, Yiqiu/0000-0001-8363-9448; Hintermuller, Michael/0000-0001-9471-2479	Austrian Ministry of Science and Research [Y305]; Austrian Science Fund FWF through SFB; Austrian Science Fund (FWF) [F 3204, Y 305] Funding Source: researchfish	Austrian Ministry of Science and Research; Austrian Science Fund FWF through SFB(Austrian Science Fund (FWF)); Austrian Science Fund (FWF)(Austrian Science Fund (FWF))	This research was funded by the Austrian Ministry of Science and Research through START program Y305 "Interfaces and Free Boundaries" and the Austrian Science Fund FWF through SFB "Mathematical Optimization and Applications in Biomedical Sciences".	Almansa A, 2008, J SCI COMPUT, V34, P209, DOI 10.1007/s10915-007-9160-x; Andrews H.C., 1977, DIGITAL IMAGE RESTOR; Aubin J. P., 2006, APPL NONLINEAR ANAL; Blomgren P, 1998, IEEE T IMAGE PROCESS, V7, P304, DOI 10.1109/83.661180; Bresson X, 2008, INVERSE PROBL IMAG, V2, P455, DOI 10.3934/ipi.2008.2.455; Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258; Chan TF, 2005, SIAM J APPL MATH, V65, P1817, DOI 10.1137/040604297; Dong YQ, 2009, SIAM J IMAGING SCI, V2, P1168, DOI 10.1137/090758490; Dong YQ, 2009, LECT NOTES COMPUT SC, V5567, P271, DOI 10.1007/978-3-642-02256-2_23; DONG YQ, 2008, 22112008 IFB U GRAZ; Ekeland I, 1999, CONVEX ANAL VARIATIO; Fu HY, 2006, SIAM J SCI COMPUT, V27, P1881, DOI 10.1137/040615079; Galatsanos NP, 1992, IEEE T IMAGE PROCESS, V1, P322, DOI 10.1109/83.148606; GILBOA G, 2003, P VLSM 2003 NIC FRAN, P137; Hintermuller M, 2006, SIAM J SCI COMPUT, V28, P1, DOI 10.1137/040613263; Hintermuller M, 2004, SIAM J APPL MATH, V64, P1311, DOI 10.1137/S0036139903422784; HINTERMULLER M, 2010, INVERSE PRO IN PRESS; Mood A., 1974, INTRO THEORY STAT; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Nikolova M, 2002, SIAM J NUMER ANAL, V40, P965, DOI 10.1137/S0036142901389165; Nikolova M, 2004, J MATH IMAGING VIS, V20, P99, DOI 10.1023/B:JMIV.0000011920.58935.9c; Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412; PAPOULIS, 1991, PROBABILITY RANDOM V; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Strong D. M., 2005, SCALE RECOGNITION RE; Strong D.M, 1996, UCLA MATH DEP CAM RE; Tadmor E, 2004, MULTISCALE MODEL SIM, V2, P554, DOI 10.1137/030600448; Tadmor E., 2008, COMMUN MATH SCI, V6, P1; Tikhonov A.N., 1977, SOLUTION ILL POSED P; Vogel C. R., 2002, FRONTIERS APPL MATH, P23; Yin W, 2007, MULTISCALE MODEL SIM, V6, P190, DOI 10.1137/060663027; Yosida K, 1964, FUNCTIONAL ANAL	33	19	19	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2011	92	3					296	307		10.1007/s11263-010-0359-1	http://dx.doi.org/10.1007/s11263-010-0359-1			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	729DS		Green Submitted			2022-12-18	WOS:000287929400004
J	Wang, GH; Wu, QMJ				Wang, Guanghui; Wu, Q. M. Jonathan			Quasi-perspective Projection Model: Theory and Application to Structure and Motion Factorization from Uncalibrated Image Sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Computational models of vision; Quasi-perspective projection; Imaging geometry; Matrix factorization; Singular value decomposition; Euclidean reconstruction	STRUCTURE-FROM-MOTION; SELF-CALIBRATION; NONRIGID SHAPE; CAMERA; RECONSTRUCTION	This paper addresses the problem of factorization-based 3D reconstruction from uncalibrated image sequences. Previous studies on structure and motion factorization are either based on simplified affine assumption or general perspective projection. The affine approximation is widely adopted due to its simplicity, whereas the extension to perspective model suffers from recovering projective depths. To fill the gap between simplicity of affine and accuracy of perspective model, we propose a quasi-perspective projection model for structure and motion recovery of rigid and nonrigid objects based on factorization framework. The novelty and contribution of this paper are as follows. Firstly, under the assumption that the camera is far away from the object with small lateral rotations, we prove that the imaging process can be modeled by quasi-perspective projection, which is more accurate than affine model from both geometrical error analysis and experimental studies. Secondly, we apply the model to establish a framework of rigid and nonrigid factorization under quasi-perspective assumption. Finally, we propose an Extended Cholesky Decomposition to recover the rotation part of the Euclidean upgrading matrix. We also prove that the last column of the upgrading matrix corresponds to a global scale and translation of the camera thus may be set freely. The proposed method is validated and evaluated extensively on synthetic and real image sequences and improved results over existing schemes are observed.	[Wang, Guanghui; Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada; [Wang, Guanghui] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100080, Peoples R China	University of Windsor; Chinese Academy of Sciences; Institute of Automation, CAS	Wang, GH (corresponding author), Univ Windsor, Dept Elect & Comp Engn, 401 Sunset, Windsor, ON N9B 3P4, Canada.	ghwangca@gmail.com; jwu@uwindsor.ca	Wu, Q.M.Jonathan/O-3234-2017		Natural Sciences and Engineering Research Council of Canada; National Natural Science Foundation of China [60575015]	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The work is supported in part by Natural Sciences and Engineering Research Council of Canada, and the National Natural Science Foundation of China under Grant No. 60575015.	Bascle B, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P323, DOI 10.1109/ICCV.1998.710738; Brand M, 2005, PROC CVPR IEEE, P122; Brand M, 2001, PROC CVPR IEEE, P456; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Buchanan AM, 2005, PROC CVPR IEEE, P316; Chen P, 2008, INT J COMPUT VISION, V80, P125, DOI 10.1007/s11263-008-0135-7; Christy S, 1996, IEEE T PATTERN ANAL, V18, P1098, DOI 10.1109/34.544079; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; Del Bue A, 2006, P IEEE C COMP VIS PA, V1, P1191; DELBUE A, 2004, IEEE WORKSH ART NONR, P8; Frahm J.-M., 1996, EUR C COMP VIS CAMBR, P2; HAN M, 2000, P IEEE COMP SOC WORK; Hartley R, 2003, AUSTR JAP ADV WORKSH; Hartley R., 2004, ROBOTICA; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P133, DOI 10.1109/34.574792; Hartley R, 2008, LECT NOTES COMPUT SC, V5302, P276, DOI 10.1007/978-3-540-88682-2_22; Heyden A, 1999, IMAGE VISION COMPUT, V17, P981, DOI 10.1016/S0262-8856(99)00002-5; Heyden A, 1997, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.1997.609362; LI T, 2007, IEEE C COMP VIS PATT; Luong QT, 1997, INT J COMPUT VISION, V22, P261, DOI 10.1023/A:1007982716991; Mahamud S, 2000, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2000.854872; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; Oliensis J, 2007, IEEE T PATTERN ANAL, V29, P2217, DOI 10.1109/TPAMI.2007.1132; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; Quan L, 1996, INT J COMPUT VISION, V19, P93, DOI 10.1007/BF00131149; Rabaud V., 2008, IEEE C COMP VIS PATT; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torr PHS, 1998, COMPUT VIS IMAGE UND, V71, P312, DOI 10.1006/cviu.1997.0559; Torresani L, 2001, PROC CVPR IEEE, P493; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Triggs B, 1996, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.1996.517170; Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z; Vidal R, 2006, LECT NOTES COMPUT SC, V3952, P205; WANG G, 2008, IEEE C COMP VIS PATT; WANG G, 2006, HYBRID SYSTEM FEATUR; Wang GH, 2008, PROCEEDINGS OF THE FIFTH CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, P129, DOI 10.1109/CRV.2008.19; Wang GH, 2008, IEEE T SYST MAN CY B, V38, P90, DOI 10.1109/TSMCB.2007.910534; Wang G, 2008, PATTERN RECOGN LETT, V29, P72, DOI 10.1016/j.patrec.2007.09.004; Xiao J, 2005, IEEE I CONF COMP VIS, P1075; Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9; Yan JY, 2008, IEEE T PATTERN ANAL, V30, P865, DOI 10.1109/TPAMI.2007.70739; Yan JY, 2005, PROC CVPR IEEE, P815	43	19	21	1	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	87	3					213	234		10.1007/s11263-009-0267-4	http://dx.doi.org/10.1007/s11263-009-0267-4			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	551KA					2022-12-18	WOS:000274205700002
J	Bissacco, A; Soatto, S				Bissacco, Alessandro; Soatto, Stefano			Hybrid Dynamical Models of Human Motion for the Recognition of Human Gaits	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human motion estimation; Hybrid system identification; Tracking; Gait analysis; Synthesis; Recognition; Dynamical models	HIDDEN MARKOV-MODELS; HUMAN MOVEMENT	We propose a hybrid dynamical model of human motion and develop a classification algorithm for the purpose of analysis and recognition. We assume that some temporal statistics are extracted from the images, and use them to infer a dynamical model that explicitly represents ground contact events. Such events correspond to "switches" between symmetric sets of hidden parameters in an auto-regressive model. We propose novel algorithms to estimate switches and model parameters, and develop a distance between such models that explicitly factors out exogenous inputs that are not unique to an individual or his/her gait. We show that such a distance is more discriminative than the distance between simple linear systems for the task of gait recognition.	[Soatto, Stefano] Univ Calif Los Angeles, Los Angeles, CA 90095 USA; [Bissacco, Alessandro] Google INC, Santa Monica, CA 90401 USA	University of California System; University of California Los Angeles; Google Incorporated	Soatto, S (corresponding author), Univ Calif Los Angeles, 3531D Boelter Hall,405 Hilgard Ave, Los Angeles, CA 90095 USA.	soatto@ucla.edu			ONR [67F-1080868]; AFOSR [FA9550-06-1-0138]	ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	Research supported by ONR 67F-1080868 and AFOSR FA9550-06-1-0138.	Agarwal A., 2004, P EUR C COMP VIS, V3, P54; BENABDELKADER C, 2004, EURASIP J APPL SIG P, V4, P1; Bissacco A, 2005, PROC CVPR IEEE, P421; BISSACCO A, 2001, P IEEE INT C COMP VI; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; Bregler C, 1997, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.1997.609382; Bregler C., 1998, P CVPR; DECOCH K, 2000, P INT S MATH THEOR N; Del Vecchio D, 2003, AUTOMATICA, V39, P2085, DOI 10.1016/S0005-1098(03)00250-4; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Efros Alexei A, 2003, P ICCV; Garcia-Arraras JE, 1998, J EXP ZOOL, V281, P288, DOI 10.1002/(SICI)1097-010X(19980701)281:4<288::AID-JEZ5>3.0.CO;2-K; Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716; GHAHRAMANI Z, 1998, SWITCHING STATE SPAC; Greenspan H, 2004, COMPUT VIS IMAGE UND, V93, P86, DOI 10.1016/j.cviu.2003.08.004; Gustafsson F., 2000, ADAPTIVE FILTERING C; HE Q, 2000, IEEE WORKSH HUM MOT; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; JUANG BH, 1985, IEEE T ACOUST SPEECH, V33, P1404, DOI 10.1109/TASSP.1985.1164727; Kale A, 2004, IEEE T IMAGE PROCESS, V13, P1163, DOI 10.1109/TIP.2004.832865; KRISHNAPRASAD PS, 1979, IEEE T AUTOMATIC CON, V25, P197; LEE CS, 2004, P AUT FAC GEST REC S; Lee L, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P155, DOI 10.1109/AFGR.2002.1004148; Little J. J., 1998, Videre, V1; Ljung L., 1987, SYSTEM IDENTIFICATIO; MA Y, 2005, HYBRID SYSTEMS COMPU; Ma Y., 2003, INVITATION 3D VISION; Martin RJ, 2000, IEEE T SIGNAL PROCES, V48, P1164, DOI 10.1109/78.827549; MAZZARO C, 2002, P 3DPTV JUN 2002; North B, 2000, IEEE T PATTERN ANAL, V22, P1016, DOI 10.1109/34.877523; O'Brien JF, 2000, PROC GRAPH INTERF, P53; OH SM, 2005, INTL C COMP VIS, V2, P1161; Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684; PAVLOVIC V, 2000, P CVPR; RUBNER Y, 1998, P IEEE INT C COMP VI, P59; Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39; SHAH M, 1999, MOTION BASED RECOGNI; TUGNAIT JK, 1982, AUTOMATICA, V18, P607, DOI 10.1016/0005-1098(82)90012-7; VERES GV, 2004, P CVPR 04 JUN 2004; Wilson AD, 1999, IEEE T PATTERN ANAL, V21, P884, DOI 10.1109/34.790429; ZAMES G, 1980, P ALL C, P380; Zelnik-Manor L, 2006, IEEE T PATTERN ANAL, V28, P1530, DOI 10.1109/TPAMI.2006.194	43	19	19	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2009	85	1					101	114		10.1007/s11263-009-0248-7	http://dx.doi.org/10.1007/s11263-009-0248-7			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	470UY		Green Published			2022-12-18	WOS:000268008000005
J	Heitz, G; Elidan, G; Packer, B; Koller, D				Heitz, Geremy; Elidan, Gal; Packer, Benjamin; Koller, Daphne			Shape-Based Object Localization for Descriptive Classification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Probabilistic graphical models; Deformable shape models; Object recognition; Markov random fields	MODELS	Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. However, sometimes we are interested in a finer-grained characterization of the object's properties, such as its pose or articulation. In this paper we develop a probabilistic method (LOOPS) that can learn a shape and appearance model for a particular object class, and be used to consistently localize constituent elements (landmarks) of the object's outline in test images. This localization effectively projects the test image into an alternative representational space that makes it particularly easy to perform various descriptive tasks. We apply our method to a range of object classes in cluttered images and demonstrate its effectiveness in localizing objects and performing descriptive classification, descriptive ranking, and descriptive clustering.	[Heitz, Geremy; Packer, Benjamin; Koller, Daphne] Stanford Univ, Stanford, CA 94305 USA; [Elidan, Gal] Hebrew Univ Jerusalem, Dept Stat, IL-91905 Jerusalem, Israel	Stanford University; Hebrew University of Jerusalem	Heitz, G (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	gaheitz@cs.stanford.edu; galel@huji.ac.il; bpacker@cs.stanford.edu; koller@cs.stanford.edu			DARPA [FA8750-05-2-0249]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported by the DARPA Transfer Learning program under contract number FA8750-05-2-0249. We would also like to thank Vittorio Ferrari and Pawan Kumar for providing us code and helping us to get their methods working on our data.	Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Basri R, 1998, VISION RES, V38, P2365, DOI 10.1016/S0042-6989(98)00043-1; Belongie S, 2001, ADV NEUR IN, V13, P831; BERG A, 2005, IEEE COMP SCI C COMP; BORENSTEIN E, 2004, IEEE C COMP VIS PATT, P46; BORGEFORS G, 1988, IEEE T PATTERN ANAL, V10, P849, DOI 10.1109/34.9107; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; COOTES TF, 1998, P EUR C COMP VIS, V2, P484; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; CRANDALL D, 2005, P 2005 IEEE COMP SOC, V1; Crandall DJ, 2006, LECT NOTES COMPUT SC, V3951, P16; Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915; Dryden I.L., 1998, STAT SHAPE ANAL; ELIDAN G, 2006, P 22 C UNCERTAINTY A, P165, DOI DOI 10.48550/ARXIV.1206.6837; ELIDAN G, 2006, IEEE COMP SOC C COMP; FEIFEI L, 2004, IEEE COMP SOC C COMP; Felzenszwalb PF, 2000, PROC CVPR IEEE, P66, DOI 10.1109/CVPR.2000.854739; FELZENSZWALB PF, 2007, IEEE COMP SOC C COMP; Fergus R, 2005, PROC CVPR IEEE, P380; Fergus R, 2003, PROC CVPR IEEE, P264; FERRARI V, 2006, EUR C COMP VIS ECCV; Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144; Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468; Fink M, 2008, INT J COMPUT VISION, V77, P143, DOI 10.1007/s11263-007-0066-8; GRAUMAN K, 2005, INT C COMP VIS OCT; HILL A, 1996, P BRIT MACH VIS C; HILLEL AB, 2005, INT C COMP VIS WASH, P1762; KUMAR MP, 2005, IEEE COMP SOC C COMP; LEIBE B, 2004, ECCV WORKSH STAT LEA, P17; LEORDEANU M, 2007, IEEE COMP SOC C COMP; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MURPHY KP, 2006, CATEGORY LEVEL OBJEC; Opelt A., 2006, P IEEE C COMP VIS PA, P3; OPELT A, 2006, P BRIT MACH VIS C; PRASAD M, 2006, P CVPR, P1345; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924; Sethian J, 1998, LEVEL SET METHODS FA; Shotton J., 2005, INT C COMP VIS; THAYANANTHAN A, 2003, IEEE COMP SOC C COMP; TORRALBA A, 2005, ADV NEURAL INFORM PR, V17, P1401; Winn J., 2006, CVPR	45	19	19	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2009	84	1					40	62		10.1007/s11263-009-0228-y	http://dx.doi.org/10.1007/s11263-009-0228-y			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	445GH					2022-12-18	WOS:000266037900003
J	Destrero, A; De Mol, C; Odone, F; Verri, A				Destrero, Augusto; De Mol, Christine; Odone, Francesca; Verri, Alessandro			A Regularized Framework for Feature Selection in Face Detection and Authentication	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Feature selection; Learning from examples; Regularized methods; Lasso regression; Thresholded Landweber; Face detection; Face authentication; Real-time system	RECOGNITION; DISCRIMINANT; EIGENFACES; IMAGES	This paper proposes a general framework for selecting features in the computer vision domain-i.e., learning descriptions from data-where the prior knowledge related to the application is confined in the early stages. The main building block is a regularization algorithm based on a penalty term enforcing sparsity. The overall strategy we propose is also effective for training sets of limited size and reaches competitive performances with respect to the state-of-the-art. To show the versatility of the proposed strategy we apply it to both face detection and authentication, implementing two modules of a monitoring system working in real time in our lab. Aside from the choices of the feature dictionary and the training data, which require prior knowledge on the problem, the proposed method is fully automatic. The very good results obtained in different applications speak for the generality and the robustness of the framework.	[Destrero, Augusto; Odone, Francesca; Verri, Alessandro] Univ Genoa, DISI, I-16146 Genoa, Italy; [Verri, Alessandro] Imavis srl, I-16152 Genoa, Italy; [De Mol, Christine] Univ Libre Bruxelles, Dept Math, B-1050 Brussels, Belgium; [De Mol, Christine] Univ Libre Bruxelles, ECARES, B-1050 Brussels, Belgium	University of Genoa; Universite Libre de Bruxelles; Universite Libre de Bruxelles	Odone, F (corresponding author), Univ Genoa, DISI, Via Dodecaneso 35, I-16146 Genoa, Italy.	augusto.destrero@imavis.com; Christine.De.Mol@ulb.ac.be; odone@disi.unige.it; verri@disi.unige.it			FIRB Project LEAP [RBIN04PARL];  [ARC 02/07-281];  [GOA62]	FIRB Project LEAP; ; 	The authors are grateful to Ernesto De Vito for many useful discussions and hints, and to the anonymous reviewers for their constructive comments and insights. The work has been partially supported by the FIRB Project LEAP (RBIN04PARL). C. De Mol acknowledges the hospitality of DISI during a sabbatical semester in Genova and the support of the grants ARC 02/07-281 and GOA62(VUB).	Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Arca S, 2006, PATTERN RECOGN, V39, P432, DOI 10.1016/j.patcog.2005.06.015; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BROWN M, 2004, P 17 ICPR; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; DEMOL C, 2007, DISITR0704 U GEN DIP; DESTRERO A, 2007, IEEE T IMAGE PROCESS; Destrero A, 2007, LECT NOTES COMPUT SC, V4844, P881; Elisseeff A., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616; Etemad K, 1997, J OPT SOC AM A, V14, P1724, DOI 10.1364/JOSAA.14.001724; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; GIROSI F, 1998, NEURAL COMPUTATION, V10; Hadid A, 2007, LECT NOTES COMPUT SC, V4778, P1; Joachims T., 1999, ADV KERNEL METHODS S; Li SZ, 2004, IEEE T PATTERN ANAL, V26, P1112, DOI 10.1109/TPAMI.2004.68; Liu XM, 2003, PATTERN RECOGN, V36, P313, DOI 10.1016/S0031-3203(02)00033-X; MESSER K, 2004, LNCS, V3072; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; Mohan A, 2001, IEEE T PATTERN ANAL, V23, P349, DOI 10.1109/34.917571; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; OSUNA E, 1997, P IEEE INT C COMP VI; Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689; PENTLAND A, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P84, DOI 10.1109/CVPR.1994.323814; PENTLAND A, 1998, INT WORKSH VER LOW B, P101; Roth V, 2004, IEEE T NEURAL NETWOR, V15, P16, DOI 10.1109/TNN.2003.809398; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Schneiderman H., 2000, INT C COMP VIS; Tan XY, 2007, LECT NOTES COMPUT SC, V4778, P168; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Ullman S, 2002, NAT NEUROSCI, V5, P682, DOI 10.1038/nn870; Vapnik V.N, 1998, STAT LEARNING THEORY; VERSCHAE R, 2003, LECT NOTES COMPUTER, V2686; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang HJ, 2008, NEURAL COMPUT APPL, V17, P49, DOI 10.1007/s00521-006-0081-7; Weston J., 2003, Journal of Machine Learning Research, V3, P1439, DOI 10.1162/153244303322753751; Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235; Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883; ZHANG D, 2004, P ICPR; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zhu J., 2004, ADV NEURAL INFORM PR, V16	45	19	19	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2009	83	2					164	177		10.1007/s11263-008-0180-2	http://dx.doi.org/10.1007/s11263-008-0180-2			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	423UE		Green Submitted			2022-12-18	WOS:000264520400004
J	Riklin-Raviv, T; Sochen, N; Kiryati, N				Riklin-Raviv, Tammy; Sochen, Nir; Kiryati, Nahum			Shape-based mutual segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						mutual segmentation; level sets; shape; perspective transformation; planar projective homography	ACTIVE CONTOURS; FRAMEWORK; REGISTRATION; TEXTURE	We present a novel variational approach for simultaneous segmentation of two images of the same object taken from different viewpoints. Due to noise, clutter and occlusions, neither of the images contains sufficient information for correct object-background partitioning. The evolving object contour in each image provides a dynamic prior for the segmentation of the other object view. We call this process mutual segmentation. The foundation of the proposed method is a unified level-set framework for region and edge based segmentation, associated with a shape similarity term. The suggested shape term incorporates the semantic knowledge gained in the segmentation process of the image pair, accounting for excess or deficient parts in the estimated object shape. Transformations, including planar projectivities, between the object views are accommodated by a registration process held concurrently with the segmentation. The proposed segmentation algorithm is demonstrated on a variety of image pairs. The homography between each of the image pairs is estimated and its accuracy is evaluated.	[Riklin-Raviv, Tammy; Kiryati, Nahum] Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel; [Sochen, Nir] Tel Aviv Univ, Dept Appl Mech, IL-69978 Tel Aviv, Israel	Tel Aviv University; Tel Aviv University	Riklin-Raviv, T (corresponding author), Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel.	tammy@eng.tau.ac.il	Raviv, Tammy Riklin/A-3462-2013	Kiryati, Nahum/0000-0003-1436-2275				Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985; COLEMAN TF, 1994, OPTIMIZATION TOOLBOX; Cremers D, 2006, INT J COMPUT VISION, V66, P67, DOI 10.1007/s11263-005-3676-z; Cremers D, 2003, PATTERN RECOGN, V36, P1929, DOI 10.1016/S0031-3203(03)00056-6; CREMERS D, 2003, WORKSH VAR GEOM LEV, P169; Cremers D, 2007, INT J COMPUT VISION, V72, P195, DOI 10.1007/s11263-006-8711-1; DUCI A, 2002, P EUR C COMPUTER VIS, V3, P48; GOMES J, 2000, P 6 EUR C COMP VIS, P588; Holtzman-Gazit M, 2006, IEEE T IMAGE PROCESS, V15, P354, DOI [10.1109/TIP.2005.860624, 10.1109/tip.2005.860624]; HUANG X, 2004, MICCAI, V1, P60; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; Kimmel R, 2003, GEOMETRIC LEVEL SET METHODS IN IMAGING, VISION AND GRAPHICS, P59, DOI 10.1007/0-387-21810-6_4; Kimmel R, 2003, INT J COMPUT VISION, V53, P225, DOI 10.1023/A:1023030907417; Leventon ME, 2000, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P4, DOI 10.1109/MMBIA.2000.852354; Leventon ME, 2000, PROC CVPR IEEE, P316, DOI 10.1109/CVPR.2000.855835; Lorigo LM, 2000, PROC CVPR IEEE, P444, DOI 10.1109/CVPR.2000.855853; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Paragios N, 2002, J VIS COMMUN IMAGE R, V13, P249, DOI 10.1006/jvci.2001.0475; Riklin-Raviv T, 2005, IEEE I CONF COMP VIS, P204; Riklin-Raviv T, 2007, INT J COMPUT VISION, V72, P309, DOI 10.1007/s11263-006-9042-y; RIKLINRAVIV T, 2004, P EUR C COMP VIS, V4, P50; Rochery M, 2006, INT J COMPUT VISION, V69, P27, DOI 10.1007/s11263-006-6851-y; Rother C., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91; ROUSSON M, 2002, P EUR C COMP VIS, P78; Sagiv C, 2006, IEEE T IMAGE PROCESS, V15, P1633, DOI 10.1109/TIP.2006.871133; Sandberg B., 2002, LEVEL SET GABOR BASE; Soatto AV, 2006, P IEEE COMP SOC C CO, P1753; Strain J, 1999, J COMPUT PHYS, V152, P664, DOI 10.1006/jcph.1999.6259; Tsai A, 2003, IEEE T MED IMAGING, V22, P137, DOI 10.1109/TMI.2002.808355; Vasilevskiy A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P149, DOI 10.1109/ICCV.2001.937511; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Yezzi A, 2003, MED IMAGE ANAL, V7, P171, DOI 10.1016/S1361-8415(03)00004-5; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343	37	19	19	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2008	79	3					231	245		10.1007/s11263-007-0115-3	http://dx.doi.org/10.1007/s11263-007-0115-3			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	310KW		Green Submitted			2022-12-18	WOS:000256529500002
J	Lu, CL; Pizer, SM; Joshi, S; Jeong, JY				Lu, Conglin; Pizer, Stephen M.; Joshi, Sarang; Jeong, Ja-Yeon			Statistical multi-object shape models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						statistical shape models; multi-object shape models; deformable model; multi-scale shape analysis; medial representation	SEGMENTATION	The shape of a population of geometric entities is characterized by both the common geometry of the population and the variability among instances. In the deformable model approach, it is described by a probabilistic model on the deformations that transform a common template into various instances. To capture shape features at various scale levels, we have been developing an object-based multi-scale framework, in which a geometric entity is represented by a series of deformations with different locations and degrees of locality. Each deformation describes a residue from the information provided by previous steps. In this paper, we present how to build statistical shape models of multi-object complexes with such properties based on medial representations and explain how this may lead to more effective shape descriptions as well as more efficient statistical training procedures. We illustrate these ideas with a statistical shape model for a pair of pubic bones and show some preliminary results on using it as a prior in medical image segmentation.	Univ N Carolina, Med Image Display & Anal Grp, Chapel Hill, NC 27599 USA	University of North Carolina; University of North Carolina Chapel Hill	Lu, CL (corresponding author), Univ N Carolina, Med Image Display & Anal Grp, Chapel Hill, NC 27599 USA.	coa05lu@yahoo.com						BESAG J, 1986, J R STAT SOC B, V48, P259; BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0; CHANEY E, 2004, AM SOC THERAPEUTIC R; Cootes TF, 1999, LECT NOTES COMPUT SC, V1613, P322; COOTES TF, 1998, 5 EUR C COMP VIS, P484; Davatzikos C, 2003, IEEE T MED IMAGING, V22, P414, DOI 10.1109/TMI.2003.809688; David S, 2002, J SUSTAIN AGR, V21, P5, DOI 10.1300/J064v21n02_03; Fletcher PT, 2003, PROC CVPR IEEE, P95; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; FLETCHER PT, 2004, ECCV 2004 WORKSH COM; Gerig G, 2001, LNCS, V2208, P24; GOODALL C, 1991, J ROY STAT SOC B MET, V53, P285, DOI 10.1111/j.2517-6161.1991.tb01825.x; Grenander U., 1991, HANDS PATTERN THEORE; GRENANDER U, 1995, ELEMENTS PATTERN THE; Han Q, 2004, 2004 2ND IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: MACRO TO NANO, VOLS 1 and 2, P1251; Ho S, 2004, LECT NOTES COMPUT SC, V3216, P176; JEONG J, 2006, MICCAI C WORKSH MATH, P136; JOSHI S, 2002, IEEE TMI, V21; Joshi SC, 1997, THESIS WASHINGTON U; KAPUR T, 1998, MODEL BASED SEGMENTA; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kelemen A, 1999, IEEE T MED IMAGING, V18, P828, DOI 10.1109/42.811260; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; Lu CL, 2003, LECT NOTES COMPUT SC, V2695, P416; Lu CL, 2002, J VIS COMMUN IMAGE R, V13, P65, DOI 10.1006/jvci.2001.0476; MALLAT SG, 1989, IEEE T ACOUST SPEECH, V37, P2091, DOI 10.1109/29.45554; MERCK D, METHODOLOGY CONSTRUC; Pizer SM, 2005, LECT NOTES COMPUT SC, V3753, P60, DOI 10.1007/11577812_6; Pizer SM, 2003, IMAGE VISION COMPUT, V21, P5, DOI 10.1016/S0262-8856(02)00130-0; Pizer SM, 2003, INT J COMPUT VISION, V55, P155, DOI 10.1023/A:1026135101267; Pizer SM, 2003, INT J COMPUT VISION, V55, P85, DOI 10.1023/A:1026313132218; PIZER SM, 2006, MICCAI WORKSH STAT A, P43; Pohl KM, 2005, LECT NOTES COMPUT SC, V3749, P310; Pohl KM, 2005, LECT NOTES COMPUT SC, V3765, P489, DOI 10.1007/11569541_49; Siddiqi K, 2002, INT J COMPUT VISION, V48, P215, DOI 10.1023/A:1016376116653; STOUGH J, 2004, INT S BIOMEDICAL IMA, P436; STYNER M, 2001, LECT NOTES COMPUTER, V2082, P502; THALL A, 2004, THESIS U N CAROLINA; Tsai A, 2003, LECT NOTES COMPUT SC, V2732, P185; Tsai A, 2003, IEEE T MED IMAGING, V22, P137, DOI 10.1109/TMI.2002.808355; Unser M, 1996, P IEEE, V84, P626, DOI 10.1109/5.488704; Vaillant M, 1999, LECT NOTES COMPUT SC, V1613, P182; Yushkevich P., 2001, Information Processing in Medical Imaging. 17th International Conference, IPMI 2001. Proceedings (Lecture Notes in Computer Science Vol.2082), P402; ZHU G, 1999, NEUROSCI INTELL UNIT, V2, P1	45	19	19	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2007	75	3					387	404		10.1007/s11263-007-0045-0	http://dx.doi.org/10.1007/s11263-007-0045-0			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	211QS		Green Submitted			2022-12-18	WOS:000249539000005
J	Ohta, Y; Kitahara, I; Kameda, Y; Ishkawa, H; Koyama, T				Ohta, Yuichi; Kitahara, Itaru; Kameda, Yoshinari; Ishkawa, Hiroyuki; Koyama, Takayoshi			Live 3D video in soccer stadium	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						virtualized reality; 3D video; image-based rendering	MOTION; PEDESTRIANS; FRAMEWORK; MODELS	This paper proposes a method to realize a 3D video system that can capture video data from multiple cameras, reconstruct 3D models, transmit 3D video streams via the network, and display them on remote PCs. All processes are done in real time. We represent a player with a simplified 3D model consisting of a single plane and a live video texture extracted from multiple cameras. This 3D model is simple enough to be transmitted via a network. A prototype system has been developed and tested at actual soccer stadiums. A 3D video of a typical soccer scene, which includes more than a dozen players, was processed at video rate and transmitted to remote PCs through the internet at 15-24 frames per second.	Univ Tsukuba, Tsukuba, Ibaraki 305, Japan	University of Tsukuba	Ohta, Y (corresponding author), Univ Tsukuba, Tsukuba, Ibaraki 305, Japan.		Ohta, Yuichi/R-1471-2019	Ohta, Yuichi/0000-0001-7323-3532				AKENINEMOLLER T, 2002, REAL TIME RENDERING; Antonini G, 2006, INT J COMPUT VISION, V69, P159, DOI 10.1007/s11263-005-4797-0; Barnard M, 2004, INT C PATT RECOG, P610, DOI 10.1109/ICPR.2004.1334603; Chen WE, 2005, WIREL NETW MOB COMP, V1, P3; Cheung GKM, 2000, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2000.854944; Cheung KM, 2005, INT J COMPUT VISION, V63, P225, DOI 10.1007/s11263-005-6879-4; Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097; Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c; Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726; Figueroa P, 2004, INT C PATT RECOG, P787, DOI 10.1109/ICPR.2004.1333890; Iwase S, 2004, INT C PATT RECOG, P751, DOI 10.1109/ICPR.2004.1333881; Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394; Kim T, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P721, DOI 10.1109/ICCV.1998.710797; KITAHARA I, 2001, IEEE COMP SOC C COMP; Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951; Misu T, 2002, INT C PATT RECOG, P556, DOI 10.1109/ICPR.2002.1044792; Moezzi S, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P358, DOI 10.1109/MMCS.1996.534998; Narayanan PJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P3, DOI 10.1109/ICCV.1998.710694; Ohno Y, 2000, INT C PATT RECOG, P145, DOI 10.1109/ICPR.2000.905293; Pan Z, 2004, INT C PATT RECOG, P744, DOI 10.1109/ICPR.2004.1334366; POTMESIL M, 1987, COMPUT VISION GRAPH, V40, P1, DOI 10.1016/0734-189X(87)90053-3; Ramanan D, 2005, PROC CVPR IEEE, P271; Rosales R, 2006, INT J COMPUT VISION, V67, P251, DOI 10.1007/s11263-006-5165-4; Saito H., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P49, DOI 10.1109/CVPR.1999.784607; Seitz S. M., 1998, P IEEE C COMP VIS PA, P1067; Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637; SUGAWARA S, 1994, IEICE T INF SYST, VE77D, P1344; Sullivan S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P510, DOI 10.1109/ICCV.1998.710765; SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029; TAKEUCHI T, 2002, J VISION, V2, P377; Veit T, 2006, INT J COMPUT VISION, V68, P163, DOI 10.1007/s11263-006-6661-2; VELOSO M, 1998, LECT NOTES ARTIF INT, P242; Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8; Wada T, 2000, 5TH INTERNATIONAL WORKSHOP ON COMPUTER ARCHITECTURES FOR MACHINE PERCEPTION, PROCEEDINGS, P331, DOI 10.1109/CAMP.2000.875992; Wurmlin S, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P325, DOI 10.1109/PCCGA.2002.1167876; Yamada A, 2002, INT C PATT RECOG, P303, DOI 10.1109/ICPR.2002.1044697; YANG R, 2002, INT WORKSH IMM TEL I	37	19	22	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2007	75	1					173	187		10.1007/s11263-006-0030-z	http://dx.doi.org/10.1007/s11263-006-0030-z			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	197RM					2022-12-18	WOS:000248574200010
J	Bayerl, P; Neumann, H				Bayerl, Pierre; Neumann, Heiko			Disambiguating visual motion by form-motion interaction - a computational model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Early Cognitive Vision Workshop	MAY 29-JUN 01, 2004	Isle Skye, SCOTLAND	ECOVISION			NEURAL MECHANISMS; BAYESIAN-INFERENCE; INTEGRATION; SEGMENTATION; CONSTRAINTS; COHERENCE; OCCLUSION; RESPONSES	The neural mechanisms underlying motion segregation and integration still remain unclear to a large extent. Local motion estimates often are ambiguous in the lack of form features, such as corners or junctions. Furthermore, even in the presence of such features, local motion estimates may be wrong if they were generated near occlusions or from transparent objects. Here, a neural model of visual motion processing is presented that involves early stages of the cortical dorsal and ventral pathways. We investigate the computational mechanisms of V1-MT feedforward and feedback processing in the perception of coherent shape motion. In particular, we demonstrate how modulatory MT-V1 feedback helps to stabilize localized feature signals at, e.g. corners, and to disambiguate initial flow estimates that signal ambiguous movement due to the aperture problem for single shapes. In cluttered environments with multiple moving objects partial occlusions may occur which, in turn, generate erroneous motion signals at points of overlapping form. Intrinsic-extrinsic region boundaries are indicated by local T-junctions of possibly any orientation and spatial configuration. Such junctions generate strong localized feature tracking signals that inject erroneous motion directions into the integration process. We describe a simple local mechanism of excitatory form-motion interaction that modifies spurious motion cues at T-junctions. In concert with local competitive-cooperative mechanisms of the motion pathway the motion signals are subsequently segregated into coherent representations of moving shapes. Computer simulations demonstrate the competency of the proposed neural model.	Univ Ulm, Dept Neural Informat Proc, Ulm, Germany	Ulm University	Bayerl, P (corresponding author), Univ Ulm, Dept Neural Informat Proc, Ulm, Germany.	pierre.bayerl@um-ulm.de; heiko.neumann@um-ulm.de						ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; ADELSON EH, 1982, NATURE, V300, P523, DOI 10.1038/300523a0; BAEK K, 2003, IEEE WORKSH STAT COM; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Bayerl P, 2004, NEURAL COMPUT, V16, P2041, DOI 10.1162/0899766041732404; Brox T., 2004, P 8 EUR C COMP VIS P; Crick F, 1998, NATURE, V391, P245, DOI 10.1038/34584; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev.neuro.18.1.193; GROSSBERG S, 1980, PSYCHOL REV, V87, P1, DOI 10.1037/0033-295X.87.1.1; Grossberg S, 2001, VISION RES, V41, P2521, DOI 10.1016/S0042-6989(01)00131-6; Hansen T, 2004, NEURAL COMPUT, V16, P1013, DOI 10.1162/089976604773135087; Hough P., 1959, INT C HIGH EN ACC IN; Hupe JM, 2001, J NEUROPHYSIOL, V85, P134, DOI 10.1152/jn.2001.85.1.134; KALKAN S, 2004, P BRAIN INSP COGN SY; Kapadia MK, 2000, J NEUROPHYSIOL, V84, P2048, DOI 10.1152/jn.2000.84.4.2048; Koechlin E, 1999, BIOL CYBERN, V80, P25, DOI 10.1007/s004220050502; Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434; Liden L, 1999, VISION RES, V39, P3301, DOI 10.1016/S0042-6989(99)00055-3; Lukas BD, 1981, IM UND WORKSH; Marr D., 1982, VISION; McDermott J, 2001, PERCEPTION, V30, P905, DOI 10.1068/p3219; Medioni G., 2000, COMPUTATIONAL FRAMEW; METELLI F, 1974, SCI AM, V230, P91, DOI 10.1038/scientificamerican0474-90; NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833; Nestares O, 2001, PROC CVPR IEEE, P358; Neumann H, 1999, BIOL CYBERN, V81, P425, DOI 10.1007/s004220050573; Nicolescu M, 2003, PROC CVPR IEEE, P382; Pack CC, 2004, J NEUROSCI, V24, P3268, DOI 10.1523/JNEUROSCI.4387-03.2004; Sajda P, 2004, NEURAL NETWORKS, V17, P809, DOI 10.1016/j.neunet.2004.03.013; Shevelev IA, 1998, NEUROSCIENCE, V84, P713, DOI 10.1016/S0306-4522(97)00393-X; SHIMOJO S, 1989, VISION RES, V29, P619, DOI 10.1016/0042-6989(89)90047-3; Simoncelli EP, 1998, VISION RES, V38, P743, DOI 10.1016/S0042-6989(97)00183-1; SPORNS O, 1989, P NATL ACAD SCI USA, V86, P7265, DOI 10.1073/pnas.86.18.7265; STONER GR, 1990, NATURE, V344, P153, DOI 10.1038/344153a0; VANESSEN DC, 1994, NEURON, V13, P1, DOI 10.1016/0896-6273(94)90455-3; VONDERHEYDT R, 1984, SCIENCE, V224, P1260, DOI 10.1126/science.6539501; WEBER J, 1995, INT J COMPUT VISION, V14, P67, DOI 10.1007/BF01421489; Weiss Y, 2002, NAT NEUROSCI, V5, P598, DOI 10.1038/nn858; Weiss Y, 1996, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.1996.517092; WEISS Y, 1994, PERCEPTUALLY ORG EM; WEISS Y, 1997, MOTION SEGMENTATION; ZETZSCHE C, 1990, VISION RES, V30, P1111, DOI 10.1016/0042-6989(90)90120-A	43	19	20	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2007	72	1					27	45		10.1007/s11263-006-8891-8	http://dx.doi.org/10.1007/s11263-006-8891-8			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	122QR		Green Submitted			2022-12-18	WOS:000243242000003
J	Navab, N; Appel, M				Navab, Nassir; Appel, Mirko			Canonical representation and multi-view geometry of cylinders	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						cylinders; trilinear tensor; structure from motion; camera calibration; 3D reconstruction	LINE CORRESPONDENCES; AUGMENTED REALITY; MOTION; REGISTRATION	This paper first introduces a canonical representation for cylinders. The canonical representation introduced here is closely related to the Plucker line representation. In this paper, we show that this representation is an appropriate one for computer vision applications. In particular, it allows us to easily develop a series of mathematical methods for pose estimation, 3D reconstruction, and motion estimation. One of the major novelties in this paper is the introduction of the main equations dominating the three view geometry of cylinders. We show the relationship between cylinders' three-view geometry and that of lines (Spetsakis and Aloimonos, 1990; Weng et al., 1993) and points (Shashua, 1995) defined by the trilinear tensor (Hartley, 1997), and propose a linear method, which uses the correspondences between six cylinders across three views in order to recover the motion and structure. Cylindrical pipes and containers are the main components in the majority of chemical, water treatment and power plants, oil platforms, refineries and many other industrial installations. We have developed a professional software, called CyliCon, which allows efficient as-built reconstruction of such installations from a series of pre-calibrated images. Markers are used for this pre-calibration process. The theoretical and practical results in this paper represent the first steps towards marker-less calibration and reconstruction of such industrial sites. Here, the experimental results take advantage of the two-view and three-view geometry of cylinders introduced in this paper to provide initial camera calibration results.	Tech Univ Munich, Dept Comp Sci, Munich, Germany; Siemens Corp Technol, D-81730 Munich, Germany	Technical University of Munich; Siemens AG; Siemens Germany	Navab, N (corresponding author), Tech Univ Munich, Dept Comp Sci, Munich, Germany.	Nassir.Navab@scr.siemens.com; Mirko.Appel@siemens.com						Appel M, 2002, MACH VISION APPL, V13, P111, DOI 10.1007/s001380100066; APPEL M, 2002, INT C PATT REC QUEB; BAUER S, 1997, THESIS F AL U ERL NU; BENNING W, 1997, ALLGEMEINE VERMESSUN, V1, P16; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; CHANDLER JH, 1994, INT ARCH PHOTOGRAMME, V30, P28; DHOME M, 1990, P 1 EUR C COMP VIS, P4475; Goose S, 2003, IEEE PERVAS COMPUT, V2, P65, DOI 10.1109/MPRV.2003.1186727; HANEK R, 1999, P C COMP VIS PATT RE; Hartley RI, 1997, INT J COMPUT VISION, V22, P125, DOI 10.1023/A:1007936012022; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HORN B, 1987, J OPT SOC AM, P629; Jones M., 1996, INT ARCH PHOTOGRAMME, V31, P284; Navab N, 1997, INT J COMPUT VISION, V23, P17, DOI 10.1023/A:1007911807871; NAVAB N, 1999, IEEE INT WORKSH AUGM; NAVAB N, 1999, P 7 IEEE INT C EM TE; NAVAB N, 2003, P CVPR, V2, P607; SAYD P, 1996, P IEEE C ROB CYB CES, P560; SAYD P, 1997, P 19 SCAND C IM AN, P955; SHASHUA A, 1995, IEEE T PATTERN ANAL, V17, P779, DOI 10.1109/34.400567; SHIU YC, 1992, APPL ARTIFICIAL INTE, V10; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; TANGELDER JWH, 2000, P 19 C ISPRS AMST, V33; Veldhuis H, 1998, ISPRS J PHOTOGRAMM, V53, P6, DOI 10.1016/S0924-2716(97)00031-2; WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P318, DOI 10.1109/34.120327	26	19	20	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2006	70	2					133	149		10.1007/s11263-006-7935-4	http://dx.doi.org/10.1007/s11263-006-7935-4			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	076SO					2022-12-18	WOS:000239978200003
J	Han, M; Kanade, T				Han, M; Kanade, T			Reconstruction of a scene with multiple linearly moving objects	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; motion segmentation; dynamic scene reconstruction; computer vision	FACTORIZATION METHOD; DIRECT RECOVERY; MOTION; SHAPE; PARALLAX	In this paper we describe an algorithm to recover the scene structure, the trajectories of the moving objects and the camera motion simultaneously given a monocular image sequence. The number of the moving objects is automatically detected without prior motion segmentation. Assuming that the objects are moving linearly with constant speeds, we propose a unified geometrical representation of the static scene and the moving objects. This representation enables the embedding of the motion constraints into the scene structure, which leads to a factorization-based algorithm. We also discuss solutions to the degenerate cases which can be automatically detected by the algorithm. Extension of the algorithm to weak perspective projections is presented as well. Experimental results on synthetic and real images show that the algorithm is reliable under noise.	NEC Labs Amer, Melville, NY 11747 USA; Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	NEC Corporation; Carnegie Mellon University	Han, M (corresponding author), NEC Labs Amer, Melville, NY 11747 USA.	meihan@sv.nec-labs.com; tk@cs.cmu.edu						Avidan S, 2000, IEEE T PATTERN ANAL, V22, P348, DOI 10.1109/34.845377; AVIDAN S, 1999, CVPR99; Boult T. E., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P179, DOI 10.1109/WVM.1991.212809; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; CHRISTY S, 1996, ECCV96, P129; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gear CW, 1998, INT J COMPUT VISION, V29, P133, DOI 10.1023/A:1008026310903; HAN M, 1998, DARPA98, P154; HAN M, 1999, CMURITR9923 CARN MEL; HAN M, 2000, CVPR00, P542; Irani M, 2002, IEEE T PATTERN ANAL, V24, P1528, DOI 10.1109/TPAMI.2002.1046174; Irani M, 1997, IEEE T PATTERN ANAL, V19, P268, DOI 10.1109/34.584105; Irani M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P626, DOI 10.1109/ICCV.1999.791283; IRANI M, 1992, ICCV92, P282; IRANI M, 1998, EUR C COMP VIS, P829; KUMAR R, 1994, INT C PATT RECOG, P685, DOI 10.1109/ICPR.1994.576402; MANNING R, 1999, CVPR, P388; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; SAWHNEY H, 1999, ICCV99, P612; SHASHUA A, 1999, ICCV99, P330; SHASHUA A, 2000, ECCV00; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TORR PHS, 1993, P SOC PHOTO-OPT INS, V2059, P432, DOI 10.1117/12.150246; WEXLER Y, 2000, CVPR00, P576; Wolfe G, 2001, POPTRONICS, V2, P3; ZELNIKMANOR L, 1999, ICCV99, P710	27	19	22	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP-OCT	2004	59	3					285	300		10.1023/B:VISI.0000025801.70038.c7	http://dx.doi.org/10.1023/B:VISI.0000025801.70038.c7			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816HK		Green Submitted			2022-12-18	WOS:000221100800004
J	Sethi, A; Renaudie, D; Kriegman, D; Ponce, J				Sethi, A; Renaudie, D; Kriegman, D; Ponce, J			Curve and surface duals and the recognition of curved 3D objects from their silhouettes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						three-dimensional object recognition; invariants; duals; pedal curves	GENERALIZED CYLINDERS; 3-D OBJECTS; CONTOURS; SINGULARITIES; REVOLUTION; SHAPE; POSE	This article addresses the problem of recognizing a solid bounded by a smooth surface in a single image. The proposed approach is based on a new representation for two- and three-dimensional shapes, called their signature, that exploits the close relationship between the dual of a surface and the dual of its silhouette in weak-perspective images. Objects are modeled by rotating them in front of a camera without any knowledge of or constraints on their motion. The signatures of their silhouettes are concatenated into a single object signature. To recognize an object from novel viewpoint other than those used during modeling, the signature of the contours extracted from a test photograph is matched to the signatures of all modeled objects signatures. This approach has been implemented, and recognition examples are presented.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Sethi, A (corresponding author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.	asethi@uiuc.edu; david.renaudie@ensimag.imag.fr; kriegman@cs.ucsd.edu; ponce@cs.uiuc.edu		Sethi, Amit/0000-0002-8634-1804				Arbogast E., 1991, International Journal of Pattern Recognition and Artificial Intelligence, V5, P749, DOI 10.1142/S0218001491000430; Boyer E, 1997, INT J COMPUT VISION, V22, P219, DOI 10.1023/A:1007978616082; Bruce J.W., 1992, CURVES SINGULARITIES; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; CIPOLLA R, 1995, INT C COMP VIS, P269; Cipolla R., 2000, VISUAL MOTION CURVES; Forsyth DA, 2002, PRENT HALL PROF TECH; GIBLIN PJ, 1995, IMAGE VISION COMPUT, V13, P33, DOI 10.1016/0262-8856(95)91466-Q; GLACHET R, 1991, PATTERN RECOGN LETT, V12, P693, DOI 10.1016/0167-8655(91)90007-9; HUTTENLOCHER D, 1987, INT C COMP VIS LOND, P102; Joshi T, 1997, IMAGE VISION COMPUT, V15, P479, DOI 10.1016/S0262-8856(97)00001-2; KERGOSIEN YL, 1981, CR ACAD SCI I-MATH, V292, P929; KOENDERINK JJ, 1976, BIOL CYBERN, V24, P51, DOI 10.1007/BF00365595; KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321; KRIEGMAN DJ, 1990, INT J COMPUT VISION, V5, P119, DOI 10.1007/BF00054918; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; Liu J., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P123, DOI 10.1109/CVPR.1993.341000; LOCKWOOD EH, 1967, BOOK CURVES, P152; LOWE DG, 1987, INT J COMPUT VISION, V1, P57, DOI 10.1007/BF00128526; MACLAURIN C, 1718, PHILOS T, V30, P803; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; PONCE J, 1987, INT J COMPUT VISION, V1, P195, DOI 10.1007/BF00127820; PONCE J, 1992, CVGIP-IMAG UNDERSTAN, V55, P184, DOI 10.1016/1049-9660(92)90016-V; RENAUDIE D, 2000, P EUR C COMP VIS, V1, P784; RICHETIN M, 1991, IEEE T PATTERN ANAL, V13, P185, DOI 10.1109/34.67647; Sullivan S, 1998, IEEE T PATTERN ANAL, V20, P1091, DOI 10.1109/34.722621; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; VIJAYAKUMA B, 1998, COMPUTER VISION  DEC, P287; ZEROUG M, 1995, OBJECT REPRESENTATIO, P271	30	19	19	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2004	58	1					73	86		10.1023/B:VISI.0000016148.08046.fc	http://dx.doi.org/10.1023/B:VISI.0000016148.08046.fc			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	823OD					2022-12-18	WOS:000221621600006
J	Tek, H; Kimia, BB				Tek, H; Kimia, BB			Symmetry maps of free-form curve segments via wave propagation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Review						shape; medial axis; symmetry set; shock detection; perceptual organization	DISTANCE TRANSFORMATIONS; VORONOI-DIAGRAM; THINNING ALGORITHMS; DIFFUSION PROCESS; SHAPE; SKELETONS; EXTRACTION; SCHEMES; SET	This paper presents an approach for computing the symmetries (skeletons) of an edge map consisting of a collection of curve segments. This approach is a combination of analytic computations in the style of computational geometry and discrete propagations on a grid in the style of the numerical solutions of PDE's. Specifically, waves from each of the initial curve segments are initialized and propagated as a discrete wavefront along discrete directions. In addition, to avoid error built up due to the discrete nature of propagation, shockwaves are detected and explicitly propagated along a secondary dynamic grid. The propagation of shockwaves, integrated with the propagation of the wavefront along discrete directions, leads to an exact simulation of propagation by the Eikonal equation. The resulting symmetries are simply the collection of shockwaves formed in this process which can be manipulated locally, exactly, and efficiently under local changes in an edge map (gap completion, removal of spurious elements, etc). The ability to express grouping operations in the language of symmetry maps makes it an appropriate intermediate representation between low-level edge maps and high level object hypotheses.	Siemens Corp Res Inc, Imaging & Visualizat Dept, Princeton, NJ 08540 USA; Brown Univ, Div Engn, LEMS, Providence, RI 02912 USA	Siemens AG; Brown University	Tek, H (corresponding author), Siemens Corp Res Inc, Imaging & Visualizat Dept, 755 Coll Rd E, Princeton, NJ 08540 USA.	htek@scr.siemens.com; kimia@lems.brown.edu						AHUJA N, 1989, COMPUT VISION GRAPH, V48, P304, DOI 10.1016/0734-189X(89)90146-1; ALT H, 1995, ACM S COMP GEOM; ARCELLI C, 1993, IMAGE VISION COMPUT, V11, P163, DOI 10.1016/0262-8856(93)90055-L; ARCELLI C, 1992, PATTERN RECOGN LETT, V13, P237, DOI 10.1016/0167-8655(92)90074-A; ARCELLI C, 1985, IEEE T PATTERN ANAL, V7, P463, DOI 10.1109/TPAMI.1985.4767685; ARCELLI C, 1981, IEEE T PATTERN ANAL, V3, P134, DOI 10.1109/TPAMI.1981.4767071; ARCELLI C, 1989, IEEE T PATTERN ANAL, V11, P411, DOI 10.1109/34.19037; ARCELLI C, 1981, COMPUT VISION GRAPH, V17, P130, DOI 10.1016/0146-664X(81)90021-6; Arnold VI., 1989, MATH METHODS CLASSIC, V2, DOI [10.1007/978-1-4757-1693-1, 10.1007/978-1-4757-2063-1, DOI 10.1007/978-1-4757-2063-1]; AUGUST J, 1996, INT C PATT REC, P1; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; BORGEFORS G, 1984, COMPUT VISION GRAPH, V27, P321, DOI 10.1016/0734-189X(84)90035-5; BORGEFORS G, 1991, 7 SCIA; Brandt J. W., 1989, ICASSP-89: 1989 International Conference on Acoustics, Speech and Signal Processing (IEEE Cat. No.89CH2673-2), P1850, DOI 10.1109/ICASSP.1989.266813; BRANDT JW, 1991, J VISUAL COMMUNICATI, V2, P329; BRUCE JW, 1985, P ROY SOC EDINB A, V101, P163, DOI 10.1017/S0308210500026263; BURBECK CA, 1995, VISION RES, V35, P1917, DOI 10.1016/0042-6989(94)00286-U; BUTT MA, 1998, IEEE T IMAGE PROCESS; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chenitz W C, 1990, Issues Ment Health Nurs, V11, P1; Choi HI, 1997, GRAPH MODEL IM PROC, V59, P463, DOI 10.1006/gmip.1997.0444; CHOU JJ, 1995, IEEE COMPUT GRAPH, V15, P52, DOI 10.1109/38.365006; COURANT R, 1962, METHODS MATH PHYSICS, V2; DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4; DEMICHELI E, 1989, IEEE T PATTERN ANAL, V11, P1106, DOI 10.1109/34.42841; DIBAJA GS, 1994, J VIS COMMUN IMAGE R, V5, P107; DILL AR, 1987, IEEE T PATTERN ANAL, V9, P495, DOI 10.1109/TPAMI.1987.4767937; Eggers H, 1998, COMPUT VIS IMAGE UND, V69, P106, DOI 10.1006/cviu.1997.0596; ELBER G, 1997, TRCIS9707; FAROUKI RT, 1994, COMPUT AIDED GEOM D, V11, P117, DOI 10.1016/0167-8396(94)90029-9; FAROUKI RT, 1994, INST MATH C, P327; Ge YR, 1996, IEEE T PATTERN ANAL, V18, P1055, DOI 10.1109/34.544075; GIBLIN PJ, 2000, CVPR2000 CVPR 99 IEE; GIBLIN PJ, 1999, ICCV, P385; GIBLIN PJ, 1999, CVPR 1999, P79; GIBLIN PJ, 1998, LEMS170 BROWN U; Held M., 1991, COMPUTATIONAL GEOMET; HO SB, 1986, IEEE T PATTERN ANAL, V8, P512, DOI 10.1109/TPAMI.1986.4767815; Hoffmann CM, 1991, INT J COMPUT GEOM AP, V1, P47, DOI 10.1142/S0218195991000050; IGARASHI T, 1998, TEDDY 3D FREE FORM D; JANG BK, 1990, IEEE T PATTERN ANAL, V12, P541, DOI 10.1109/34.56190; JOHANNES MS, 2000, LEMS184 BROWN U; KELLY MF, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1016, DOI 10.1109/ICCV.1995.466823; Kelly P. J., 1979, GEOMETRY CONVEXITY; Kevorkian J., 1990, PARTIAL DIFFERENTIAL; Kimia BB, 1997, P SOC PHOTO-OPT INS, V3229, P288, DOI 10.1117/12.290349; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; KIMIA BB, 1990, P 1 EUR C COMP VIS A, P402; KIMIA BB, 1999, IMAGE DATABASES SEAR; Klein P, 2000, PROCEEDINGS OF THE ELEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P696; Klein P.N., 1998, P 6 ANN EUR S ALG ES, P91; LAM L, 1995, IEEE T PATTERN ANAL, V17, P914, DOI 10.1109/34.406659; LAM L, 1992, IEEE T PATTERN ANAL, V14, P869, DOI 10.1109/34.161346; Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850; LEE DT, 1982, IEEE T PATTERN ANAL, V4, P363, DOI 10.1109/TPAMI.1982.4767267; LeVeque RJ., 1992, LECT MATH ETH ZURICH, V2nd; LEYMARIE F, 1992, IEEE T PATTERN ANAL, V14, P56, DOI 10.1109/34.107013; LEYMARIE F, 1992, CVGIP-IMAG UNDERSTAN, V55, P84, DOI 10.1016/1049-9660(92)90008-Q; LIONS P.-L., 1981, GEN SOLUTIONS HAMILT; LIU T, 1999, ICCV, P456; Meyer F., 1990, Proceedings of the SPIE - The International Society for Optical Engineering, V1360, P251, DOI 10.1117/12.24212; MONTANARI U, 1968, J ACM, V15, P600, DOI 10.1145/321479.321486; MORSE B, 1993, INFORMATION PROCESSI; Ogniewicz R. L., 1993, DISCRETE VORONOI SKE; OGNIEWICZ RL, 1995, PATTERN RECOGN, V28, P343, DOI 10.1016/0031-3203(94)00105-U; OGNIEWICZ RL, 1992, CVPR92; OHYA T, 1984, INFORM PROCESS LETT, V18, P227, DOI 10.1016/0020-0190(84)90116-9; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; OSHER S, 1991, SIAM J NUMER ANAL, V28, P907, DOI 10.1137/0728049; PAVLIDIS T, 1980, COMPUT VISION GRAPH, V13, P142, DOI 10.1016/S0146-664X(80)80037-2; PIZER SM, 1987, IEEE T PATTERN ANAL, V9, P505, DOI 10.1109/TPAMI.1987.4767938; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; RAGNEMALM I, 1992, CVGIP-IMAG UNDERSTAN, V56, P399, DOI 10.1016/1049-9660(92)90050-D; RAGNEMALM I, 1990, THESIS LINKOPING U S; RAMAMURTHY R, 1998, J COMPUTATIONAL APPL; ROSENFEL.A, 1966, J ACM, V13, P471; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; SARKAR S, 1993, IEEE T SYST MAN CYB, V23, P382, DOI 10.1109/21.229452; SARKAR S, 1999, RECENT ADV PERCEPTUA; SCOTT GL, 1989, IMAGE VISION COMPUT, V7, P63, DOI 10.1016/0262-8856(89)90022-X; SEBASTIAN T, 2001, 12 ANN ACM SIAM S DI; Sethian J., 1996, LEVEL SET METHODS; Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591; Shaked D, 1998, COMPUT VIS IMAGE UND, V69, P156, DOI 10.1006/cviu.1997.0598; Sharvit D, 1998, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES - PROCEEDINGS, P56, DOI 10.1109/IVL.1998.694496; SHINGHAL R, 1982, IEEE T PATTERN ANAL, V4, P74, DOI 10.1109/TPAMI.1982.4767199; SHU CW, 1988, J COMPUT PHYS, V77, P439, DOI 10.1016/0021-9991(88)90177-5; Siddiqi K, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P222, DOI 10.1109/ICCV.1998.710722; Siddiqi K, 1996, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.1996.517119; Siddiqi K, 1997, GRAPH MODEL IM PROC, V59, P278, DOI 10.1006/gmip.1997.0438; SMITH S, 1997, UNPUB INT J COMP VIS; Smoller J., 1983, GRUNDLEHREN MATH WIS; SRINIVASAN V, IEEE P, V80, P1485; Styner M, 2000, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P235, DOI 10.1109/MMBIA.2000.852383; SUGIHARA K, 1992, P IEEE, V80, P1471, DOI 10.1109/5.163412; TARI S, 1996, IEEE WORKSH MATH MET; Tari ZSG, 1997, COMPUT VIS IMAGE UND, V66, P133, DOI 10.1006/cviu.1997.0612; TEICHMANN STM, 1998, SIGGRAPH; TEK H, 1997, P C COMP VIS PATT RE; TEK H, 1999, THESIS BROWN U PROVI; TEK H, 1999, 181 LEMS BROWN U; TEK H, 2001, IN PRESS J MATH IMAG; TEK H, 1997, P INT WORKSH VIS FOR; TEK H, 1999, CVPR1999, P471; TEK H, 1998, 4 INT S MATH MORPH I, P115; Tirthapura S, 1998, P SOC PHOTO-OPT INS, V3527, P25, DOI 10.1117/12.325825; Verwer B. J. H., 1988, 9th International Conference on Pattern Recognition (IEEE Cat. No.88CH2614-6), P137, DOI 10.1109/ICPR.1988.28189; Vincent L., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P520, DOI 10.1109/CVPR.1991.139746; WRIGHT MW, 1995, IMAGE VISION COMPUT, V13, P367, DOI 10.1016/0262-8856(95)99723-E; YAMADA H, 1984, P 7 INT C PATT REC M, P336; YAP CK, 1987, DISCRETE COMPUT GEOM, V2, P365, DOI 10.1007/BF02187890; YE QZ, 1988, P 9 INT C PATT REC, P495, DOI DOI 10.1109/ICPR.1988.28276; Zeleznik R. C., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P163, DOI 10.1145/237170.237238; Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1158, DOI 10.1109/34.809109; ZHU SC, 1996, INT J COMPUTER VISIO, V20; 1999, CVPR 99 IEEE COMP SO; 1999, ICCV 99 7 INT C COMP; 2000, CVPR 99 IEEE COMP SO	119	19	19	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG-SEP	2003	54	1-2					35	81		10.1023/A:1023753317008	http://dx.doi.org/10.1023/A:1023753317008			47	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	678EA					2022-12-18	WOS:000182851000003
J	Veksler, O				Veksler, O			Dense features for semi-dense stereo correspondence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						dense features; adaptive threshold; stereo; visual correspondence	ALGORITHM; WINDOW	We present a new feature based algorithm for stereo correspondence. Most of the previous feature based methods match sparse features like edge pixels, producing only sparse disparity maps. Our algorithm detects and matches dense features between the left and right images of a stereo pair, producing a semi-dense disparity map. Our dense feature is defined with respect to both images of a stereo pair, and it is computed during the stereo matching process, not a preprocessing step. In essence, a dense feature is a connected set of pixels in the left image and a corresponding set of pixels in the right image such that the intensity edges on the boundary of these sets are stronger than their matching error (which is the difference in intensities between corresponding boundary pixels). Our algorithm produces accurate semi-dense disparity maps, leaving featureless regions in the scene unmatched. It is robust, requires little parameter tuning, can handle brightness differences between images, nonlinear errors, and is fast (linear complexity).	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Veksler, O (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.	olga@research.nj.nec.com	Veksler, Olga/B-6549-2015	Veksler, Olga/0000-0002-9664-6601				AYACHE N, 1987, INT J COMPUT VISION, P1; BARNARD ST, 1989, INT J COMPUT VISION, V3, P17, DOI 10.1007/BF00054836; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Boykov Y, 1998, IEEE T PATTERN ANAL, V20, P1283, DOI 10.1109/34.735802; BOYKOV Y, 1999, ICCV, P377; COHEN SP, 1989, NEW YORK STATE J MED, V89, P416; Fusiello A, 1997, PROC CVPR IEEE, P858, DOI 10.1109/CVPR.1997.609428; GEIGER D, 1995, INT J COMPUT VISION, V14, P211, DOI 10.1007/BF01679683; GENNERY DB, 1980, MODELLING ENV EXPLOR; GRIMSON WEL, 1981, PHILOS T ROY SOC B, V292, P217, DOI 10.1098/rstb.1981.0031; ISHIKAWA H, 1998, EUR C COMP VIS, P232; JERMYN I, 1999, ICCV99, P904; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; MA J, 2000, CVPR00, P637; Maas R., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P106, DOI 10.1109/CVPR.1999.786925; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; MEDIONI G, 1985, COMPUT VISION GRAPH, V31, P2, DOI 10.1016/S0734-189X(85)80073-6; Mori K., 1973, COMPUT GRAPHICS IMAG, V2, P393; OKUTOMI M, 2001, CVPR01, V2, P138; PANTON DJ, 1978, PHOTOGRAMM ENG REM S, V44, P1499; Robert L., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P57, DOI 10.1109/CVPR.1991.139661; Roy S, 1999, INT J COMPUT VISION, V34, P147, DOI 10.1023/A:1008192004934; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; SZELISKI R, 1999, IEEE WORKSH VIS ALG; Tao H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P532, DOI 10.1109/ICCV.2001.937562; Tao H, 2000, FIFTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P246, DOI 10.1109/WACV.2000.895429; Veksler O, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P540, DOI 10.1109/ICCV.2001.937563; Wang S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P517, DOI 10.1109/ICCV.2001.937560; Zitnick CL, 2000, IEEE T PATTERN ANAL, V22, P675, DOI 10.1109/34.865184	30	19	19	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					247	260		10.1023/A:1014506211316	http://dx.doi.org/10.1023/A:1014506211316			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN					2022-12-18	WOS:000174354700018
J	Yacoob, Y; Davis, LS				Yacoob, Y; Davis, LS			Learned models for estimation of rigid and articulated human motion from stationary or moving camera	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						tracking; optical flow; camera motion; non-rigid motion; motion learning; human motion	PARAMETERIZED MODELS; IMAGE SEQUENCES; OPTICAL-FLOW; TRACKING; RECOGNITION; OBJECTS	We propose an approach for modeling, measurement and tracking of rigid and articulated motion as viewed from a stationary or moving camera. We first propose an approach for learning temporal-flow models from exemplar image sequences. The temporal-flow models are represented as a set of orthogonal temporal-flow bases that are learned using principal component analysis of instantaneous flow measurements. Spatial constraints on the temporal-flow are then incorporated to model the movement of regions of rigid or articulated objects. These spatio-temporal flow models are subsequently used as the basis for simultaneous measurement and tracking of brightness motion in image sequences. Then we address the problem of estimating composite independent object and camera image motions. We employ the spatio-temporal flow models learned through observing typical movements of the object from a stationary camera to decompose image motion into independent object and camera motions. The performance of the algorithms is demonstrated on several long image sequences of rigid and articulated bodies in motion.	Univ Maryland, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Yacoob, Y (corresponding author), Univ Maryland, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA.	yaser@umiacs.umd.edu; lsd@umiacs.umd.edu						ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; BAUMBERG A, 1994, IEEE WORKSH MOT NONR, P194; BERGEN JR, 1992, LECT NOTES COMPUT SC, V588, P237; Black MJ, 1997, PROC CVPR IEEE, P561, DOI 10.1109/CVPR.1997.609381; Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; Boult T. E., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P179, DOI 10.1109/WVM.1991.212809; Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581; COSTEIRA J, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1071, DOI 10.1109/ICCV.1995.466815; Fejes S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P979, DOI 10.1109/ICCV.1998.710835; FERMULLER C, 1995, INT J COMPUT VISION, V15, P7, DOI 10.1007/BF01450848; Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056; GEMAN S, 1987, B INT STAT I, V4, P5; GONCALVES L, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P764, DOI 10.1109/ICCV.1995.466861; Haritaoglu I, 1998, INT C PATT RECOG, P77, DOI 10.1109/ICPR.1998.711084; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Irani M, 1996, IMAGE UNDERSTANDING WORKSHOP, 1996 PROCEEDINGS, VOLS I AND II, P707; JU X, 1996, INT C FAC GEST, P561; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; MACLEAN WJ, 1994, BRIT MACH VIS C, P13; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P730, DOI 10.1109/34.85661; ROHR K, 1994, CVGIP-IMAG UNDERSTAN, V59, P94, DOI 10.1006/ciun.1994.1006; Tian TY, 1997, IEEE T PATTERN ANAL, V19, P1178, DOI 10.1109/34.625131; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Yacoob Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P446, DOI 10.1109/ICCV.1998.710757; YACOOB Y, 1999, INT J COMPUT VISION, V2, P1; YACOOB Y, 1998, J COMPUTER VISION IM, V73, P232; Yamamoto M, 1998, PROC CVPR IEEE, P2, DOI 10.1109/CVPR.1998.698580	30	19	19	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2000	36	1					5	30		10.1023/A:1008173322902	http://dx.doi.org/10.1023/A:1008173322902			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	280DJ					2022-12-18	WOS:000085086700001
J	Hespanha, JP; Dodds, Z; Hager, GD; Morse, AS				Hespanha, JP; Dodds, Z; Hager, GD; Morse, AS			What tasks can be performed with an uncalibrated stereo vision system?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						vision-based control; uncalibrated visual servoing; geometric stereo vision; weak calibration	HAND-EYE COORDINATION; MODULAR SYSTEM; FEEDBACK; ROBOTS	This article studies the following question: "When is it possible to decide, on the basis of images of point-features observed by an imprecisely modeled two-camera stereo vision system, whether or not a prescribed robot positioning task has been precisely accomplished?" Results are shown for three camera model classes: injective cameras, weakly calibrated projective cameras, and uncalibrated projective cameras. In particular, given a weakly calibrated stereo pair, it is shown that a positioning task can be precisely accomplished if and only if the task specification is invariant to projective transformations. It is shown that injective and uncalibrated projective cameras can accomplish fewer tasks, but are still able to accomplish tasks involving point coincidences. The same formal framework is applied to the problem of determining the set of tasks which can be precisely accomplished with the well-known position-based control architecture. It is shown that, for any class of camera models, the set of tasks which can be precisely accomplished using a position-based control architecture is a subset of the complete set of tasks which can be decided on the set, but includes all positioning tasks based on point coincidences. Two ways of extending the idea of position-based control to accomplish more tasks are also presented.	Univ So Calif, Los Angeles, CA 90089 USA; Yale Univ, Ctr computat Vis & Control, Dept Comp Sci, New Haven, CT 06520 USA	University of Southern California; Yale University	Hespanha, JP (corresponding author), Univ So Calif, Los Angeles, CA 90089 USA.	hespanha@usc.edu; zachary.dodds@yale.edu; gregory.hager@yale.edu; as.morse@yale.edu	Hager, Gregory D/A-3222-2010; Hespanha, Joao P/C-2569-2008	Hespanha, Joao P/0000-0003-2809-4718				Boufama B, 1998, IMAGE VISION COMPUT, V16, P27, DOI 10.1016/S0262-8856(97)00047-4; CASTANO A, 1994, IEEE T ROBOTIC AUTOM, V10, P334, DOI 10.1109/70.294208; Chang WC, 1997, IEEE DECIS CONTR P, P48, DOI 10.1109/CDC.1997.650586; CHANG WC, 1996, P 13 WORLD C IFAC, VA, P343; CHANG WC, 1997, THESIS YALE U; CHAUMETTE F, 1997, P IROS WORKSH NEW TR, P43; CHAUMETTE F, 1994, VISUAL SERVOING, P199; Corke P. I., 1994, VISUAL SERVOING, P1; Dickmanns E.D., 1988, MACH VISION APPL, V1, P241; ESPIAU B, 1992, IEEE T ROBOTIC AUTOM, V8, P313, DOI 10.1109/70.143350; FAGERER C, 1994, AUTONOMOUS ROBOTS, V1; FAUGERAS OD, 1992, P 2 EUR C COMP VIS S, P563; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; HAGER G, 1997, P IEEE RSJ INRIA WOR, P71; Hager GD, 1997, IEEE T ROBOTIC AUTOM, V13, P582, DOI 10.1109/70.611326; HAGER GD, 1995, IEEE CONTR SYST MAG, V15, P30, DOI 10.1109/37.341862; HAGER GD, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1009, DOI 10.1109/ICCV.1995.466824; Hartley RI, 1997, INT J COMPUT VISION, V22, P5, DOI 10.1023/A:1007957826135; Hartman HJ, 1998, INSTR SCI, V26, P1, DOI 10.1023/A:1003023628307; HESPANHA JP, 1998, THESIS YALE U NEW HA; HESPANHA JP, 1998, P 37 C DEC CONTR; HESPANHA JP, 1998, P NOLCOS; HOLLINGHURST N, 1994, IMAGE VISION COMPUT, V12, P187, DOI 10.1016/0262-8856(94)90071-X; HOSODA K, 1994, IROS '94 - INTELLIGENT ROBOTS AND SYSTEMS: ADVANCED ROBOTIC SYSTEMS AND THE REAL WORLD, VOLS 1-3, P186, DOI 10.1109/IROS.1994.407392; HUTCHINSON S, 1996, IEEE T ROBOT AUTOM, V12; Jagersand M, 1997, IEEE INT CONF ROBOT, P2874, DOI 10.1109/ROBOT.1997.606723; Kelly R, 1996, IEEE T ROBOTIC AUTOM, V12, P759, DOI 10.1109/70.538980; KRIEGMAN D, 1998, LECT NOTES CONTROL I, V237; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Maybank SJ, 1998, IMAGE VISION COMPUT, V16, P13, DOI 10.1016/S0262-8856(97)00048-6; Mundy J., 1992, GEOMETRIC INVARIANCE; NELSON BJ, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P829, DOI 10.1109/CVPR.1994.323907; PONCE J, 1998, IMAGE VISION COMPUT, V16, P223; RIZZI A, 1995, THESIS YALE U; ROBERT L, 1993, P INT C COMP VIS BER, P540; ROBERT L, 1995, 2584 INRIA; SAMSON C, 1992, ROBOT CONTROL TASK F; Sastry S., 1989, ADAPTIVE CONTROL STA; Seelinger MJ, 1997, P SOC PHOTO-OPT INS, V3209, P133, DOI 10.1117/12.287632; Sharma R, 1997, IEEE T ROBOTIC AUTOM, V13, P61, DOI 10.1109/70.554347; SKAAR SB, 1990, INT J ROBOT RES, V9, P22, DOI 10.1177/027836499000900402; Toyama K, 1996, IEEE INT CONF ROBOT, P2636, DOI 10.1109/ROBOT.1996.506560; WEISS LE, 1987, IEEE T ROBOTIC AUTOM, V3, P404, DOI 10.1109/JRA.1987.1087115; WIJESOMA SW, 1993, INT J ROBOT RES, V12, P65, DOI 10.1177/027836499301200105; YOSHIMI BH, 1994, IEEE INT CONF ROBOT, P156, DOI 10.1109/ROBOT.1994.350995; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561	46	19	41	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1999	35	1					65	85		10.1023/A:1008111128520	http://dx.doi.org/10.1023/A:1008111128520			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	265NY					2022-12-18	WOS:000084251300005
J	Jacobs, D; Basri, R				Jacobs, D; Basri, R			3-D to 2-D pose determination with regions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object recognition; pose determination; linear programming; line traversal; occlusion; regions; parts; convexity	OBJECT RECOGNITION; FACTORIZATION METHOD; GLOBAL DEFORMATIONS; HAUSDORFF DISTANCE; MOMENT INVARIANTS; MODELS; IMAGES; MOTION; SUPERQUADRICS; ALIGNMENT	This paper presents a novel approach to parts-based object recognition in the presence of occlusion. We focus on the problem of determining the pose of a 3-D object from a single 2-D image when convex parts of the object have been matched to corresponding regions in the image. We consider three types of occlusions: self-occlusion, occlusions whose locus is identified in the image, and completely arbitrary occlusions. We show that in the first two cases this is a convex optimization problem, derive efficient algorithms, and characterize their performance. For the last case, we prove that the problem of finding valid poses is computationally hard, but provide an efficient, approximate algorithm. This work generalizes our previous work on region-based object recognition, which focused on the case of planar models.	NEC Res Inst, Princeton, NJ 08540 USA; Weizmann Inst Sci, Dept Appl Math, IL-76100 Rehovot, Israel	NEC Corporation; Weizmann Institute of Science	Jacobs, D (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.	dwj@research.nj.nec.com; ronen@wisdom.weizmann.ac.il						Alter TD, 1998, INT J COMPUT VISION, V27, P127, DOI 10.1023/A:1007989016491; Amenta N., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P340, DOI 10.1145/177424.178064; AMENTA N, 1992, PROCEEDINGS OF THE THIRD ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P66; AMENTA N, COMMUNICATION; [Anonymous], 1985, PERCEPTUAL ORG VISUA; Basri R, 1996, INT J COMPUT VISION, V19, P169, DOI 10.1007/BF00055803; Basri R, 1997, INT J COMPUT VISION, V25, P145, DOI 10.1023/A:1007919917506; BASRI R, 1995, CS9533 WEIZM I SCI; BASRI R, 1996, 1 ACM WORKSH APPL CO, P57; BERG JL, 1993, COMP STAND INTER, V15, P1, DOI 10.1016/0920-5489(93)90022-J; BIEDERMAN I, 1985, COMPUT VISION GRAPH, V32, P29, DOI 10.1016/0734-189X(85)90002-7; Binford T.O., 1971, IEEE C SYST CONTR MI; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; CLEMENS D, 1991, TR1307 MIT AI; Conway J.B., 1990, GRADUATE TEXTS MATH; Duda R.O., 1973, J ROYAL STAT SOC SER; DUDANI SA, 1977, IEEE T COMPUT, V26, P39, DOI 10.1109/TC.1977.5009272; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FORSYTH D, 1992, EUR C COMP VIS, P639; Forsyth DA, 1996, INT J COMPUT VISION, V18, P21, DOI 10.1007/BF00126138; GROSS A, 1990, P IM UND WORKSH PENN, P557; HORAUD R, 1987, IEEE T PATTERN ANAL, V9, P401, DOI 10.1109/TPAMI.1987.4767922; HU MK, 1962, IEEE T INFORM THEORY, V8, P169; Huttenlocher D. P., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P93, DOI 10.1109/ICCV.1993.378231; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; JACOBS D, 1992, 1416 MIT AI; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; Jacobs DW, 1997, INT J COMPUT VISION, V21, P123, DOI 10.1023/A:1007927623619; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995; MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020; MEGIDDO N, 1996, 10018 RJ IBM ALM RES; NAGAO K, 1994, 12 INT C PATT REC, P861; Nayar SK, 1996, INT J COMPUT VISION, V17, P219, DOI 10.1007/BF00128232; PELLEGRINI M, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P177, DOI 10.1145/98524.98563; Pentland A. P., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P612; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; PONCE J, 1989, IEEE T PATTERN ANAL, V11, P951, DOI 10.1109/34.35498; REEVES AP, 1984, P 7 INT C PATT REC M, P447; RICHARD CW, 1974, IEEE T SYST MAN CYB, VSMC4, P371, DOI 10.1109/TSMC.1974.5408458; RIVLIN E, 1995, COMPUT VIS IMAGE UND, V62, P164, DOI 10.1006/cviu.1995.1048; Rock I., 1983, LOGIC PERCEPTION; ROTHWELL C, 1993, 3 INT C COMP VIS, P573; Rucklidge WJ, 1997, INT J COMPUT VISION, V24, P251, DOI 10.1023/A:1007975324482; SADJADI FA, 1980, IEEE T PATTERN ANAL, V2, P127, DOI 10.1109/TPAMI.1980.4766990; SEIDEL R, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P211, DOI 10.1145/98524.98570; SHAFER SA, 1983, CS083105 CARN MELL U; SOLINA F, 1990, IEEE T PATTERN ANAL, V12, P131, DOI 10.1109/34.44401; Sugimoto A, 1996, INT J COMPUT VISION, V19, P181, DOI 10.1007/BF00055804; TERZOPOULOS D, 1991, IEEE T PATTERN ANAL, V13, P703, DOI 10.1109/34.85659; Thompson D. W., 1987, Proceedings of the 1987 IEEE International Conference on Robotics and Automation (Cat. No.87CH2413-3), P208; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; ULUPINAR F, 1995, IEEE T PATTERN ANAL, V17, P120, DOI 10.1109/34.368175; VIJAYAKUMAR B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P508, DOI 10.1109/ICCV.1995.466897; ZERROUG M, 1994, APPL INVARIANCE COMP	60	19	21	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1999	34	2-3					123	145		10.1023/A:1008135819955	http://dx.doi.org/10.1023/A:1008135819955			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	265NF					2022-12-18	WOS:000084249700004
J	Williams, LR				Williams, LR			Topological reconstruction of a smooth manifold-solid from its occluding contour	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								This paper describes a simple construction for building a combinatorial model of a smooth manifold-solid from a labeled-figure representing its occluding contour. The motivation is twofold. First, deriving the combinatorial model is an essential intermediate step in the visual reconstruction of solid-shape from image contours. A description of solid-shape consists of a metric and a topological component. Both are necessary: the metric component specifies how the topological component is embedded in three-dimensional space. The paneling construction described in this paper is a procedure for generating the topological component from a labeled-figure representing the occluding contour. Second, the existence of this construction establishes the sufficiency of a labeling scheme for line-drawings of smooth solid-objects originally proposed by Huffman (1971). By sufficiency, it is meant that every set of closed plane-curves satisfying this labeling scheme is shown to correspond to a generic view of a manifold-solid. Together with the Whitney theorem (Whitney, 1955), this confirms that Huffman's labeling scheme correctly distinguishes possible from impossible smooth solid-objects.			Williams, LR (corresponding author), NEC RES INST,4 INDEPENDENCE WAY,PRINCETON,NJ 08540, USA.							[Anonymous], 1975, PSYCHOL COMPUTER VIS; Arnold V. I, 1991, THEORY SINGULARITIES; Callahan J., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P240; DRAPER SW, 1980, THESIS SUSSEX U; Francis G. K., 1977, HOUSTON J MATH, V3, P165; Griffiths H.B., 1981, SURFACES; Henle M., 1979, COMBINATORIAL INTRO; Huffman D, 1971, MACHINE INTELLIGENCE; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; Kanizsa G., 1979, ORG VISION; KOENDERINK JJ, 1976, BIOL CYBERNETICS, V24; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; Paul G., 1988, PREDATORY DINOSAURS; PIGNONI R, 1991, MANUSCRIPTA MATH, V72, P223, DOI 10.1007/BF02568277; Richards W., 1988, NATURAL COMPUTATION; Seifert H., 1980, TXB TOPOLOGY; SZELISKI R, 1993, IEEE C COMP VIS PATT; TERZOPOULOS, 1987, P 1 INT C COMP VIS L; Williams LR, 1996, COMPUT VIS IMAGE UND, V64, P1, DOI 10.1006/cviu.1996.0043; WILLIAMS LR, 1994, THESIS U MASSACHUSET	20	19	20	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	1997	23	1					93	108		10.1023/A:1007967925618	http://dx.doi.org/10.1023/A:1007967925618			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XK559					2022-12-18	WOS:A1997XK55900005
J	HORIUCHI, T; BAIR, W; BISHOFBERGER, B; MOORE, A; KOCH, C; LAZZARO, J				HORIUCHI, T; BAIR, W; BISHOFBERGER, B; MOORE, A; KOCH, C; LAZZARO, J			COMPUTING MOTION USING ANALOG VLSI VISION CHIPS - AN EXPERIMENTAL COMPARISON AMONG DIFFERENT APPROACHES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							VISUAL-MOTION; OPTICAL-FLOW; COMPUTATIONAL THEORY; PERCEPTION; VELOCITY; MODELS; FIELD	We have designed, built and tested a number of analog CMOS VLSI circuits for computing 1-D motion from the time-varying intensity values provided by an array of on-chip phototransistors. We present experimental data for two such circuits and discuss their relative performance. One circuit approximates the correlation model while a second chip uses resistive grids to compute zero-crossings to be tracked over time by a separate digital processor. Both circuits integrate image acquisition with image processing functions and compute velocity in real time. For comparison, we also describe the performance of a simple motion algorithm using off-the-shelf digital components. We conclude that analog circuits implementing various correlation-like motion algorithms are more robust than our previous analog circuits implementing gradient-like motion algorithms.	CALTECH,COMPUTAT & NEURAL SYST PROGRAM,PASADENA,CA 91125; UNIV CALIF BERKELEY,BERKELEY,CA 94720	California Institute of Technology; University of California System; University of California Berkeley				Koch, Christof/0000-0001-6482-8067				ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; BAIR W, 1991, ADV NEURAL INFORMATI, V3, P399; BULTHOFF H, 1989, NATURE, V337, P549, DOI 10.1038/337549a0; DELBRUCK T, 1989, NEURAL INFORMATION P, V1, P720; EGELHAAF M, 1989, J OPT SOC AM A, V6, P1070, DOI 10.1364/JOSAA.6.001070; FENNEMA CL, 1979, COMPUT VISION GRAPH, V9, P301, DOI 10.1016/0146-664X(79)90097-2; GRZYWACZ NM, 1990, PROC R SOC SER B-BIO, V239, P129, DOI 10.1098/rspb.1990.0012; HASSENSTEIN B, 1956, Z NATURFORSCH PT B, V11, P513; Hildreth E., 1984, MEASUREMENT VISUAL M; HILDRETH EC, 1987, ANNU REV NEUROSCI, V10, P477, DOI 10.1146/annurev.ne.10.030187.002401; HORIUCHI J, 1991, ADV NEURAL INFORMATI, V3, P406; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1989, MIT1071 INT LAB MEM; KOCH C, 1990, ADV NEURAL INFORMATI, V2, P750; KOCH C, 1991, OCT IEEE WORKSH VIS, P312; KOCH C, 1989, MAR IEEE P WORKSH VI, P62; Koch C, 1989, NEURAL COMPUT, V1, P184, DOI 10.1162/neco.1989.1.2.184; KONISHI M, 1986, TRENDS NEUROSCI, V9, P163, DOI 10.1016/0166-2236(86)90053-6; LAZZARO JP, 1989, ANALOG VLSI IMPLEMEN, P85; LULLMAN S, 1981, IEEE COMPUT, V14, P57; MARR D, 1981, PROC R SOC SER B-BIO, V211, P151, DOI 10.1098/rspb.1981.0001; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; MATHUR B, 1991, P SOC PHOTO-OPT INS, P1473; Mead, 1989, ANALOG VLSI NEURAL S; MEAD C, 1985, 1985 CHAP HILL C VER; MOORE A, 1991, VISUAL INFORMATION PROCESSING : FROM NEURONS TO CHIPS, V1473, P66; NAGEL HH, 1978, 4TH P INT C PATT REC; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; POGGIO T, 1989, COMP NEUR S, P355; POGGIO T, 1973, KYBERNETIK, V13, P223, DOI 10.1007/BF00274887; REICHARDT W, 1988, NATURWISSENSCHAFTEN, V75, P313, DOI 10.1007/BF00367326; TANNER J, 1989, ANALOG VLSI NEURAL S, P229; TANNER J, 1986, VLSI SIGNAL PROCESSI, P59; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; VANSANTEN JPH, 1984, J OPT SOC AM A, V1, P451, DOI 10.1364/JOSAA.1.000451; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; WANG HT, 1989, NEURAL COMPUT, V1, P92; WATSON AB, 1985, J OPT SOC AM A, V2, P322, DOI 10.1364/JOSAA.2.000322; YUILLE AL, 1988, NATURE, V333, P71, DOI 10.1038/333071a0; [No title captured]	42	19	22	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1992	8	3					203	216		10.1007/BF00055152	http://dx.doi.org/10.1007/BF00055152			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JU903		Green Accepted			2022-12-18	WOS:A1992JU90300004
J	BAKER, HH				BAKER, HH			BUILDING SURFACES OF EVOLUTION - THE WEAVING WALL	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									SRI INT, CTR ARTIFICIAL INTELLIGENCE, MENLO PK, CA 94025 USA	SRI International								ARTZY E, 1981, COMPUT VISION GRAPH, V15, P1, DOI 10.1016/0146-664X(81)90103-9; BAKER HH, 1988, INT J COM V; BANCHOFF TF, 1986, STATISTICAL IMAGE PR; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; GOLDWASSER SM, 1987, COMPUT VISION GRAPH, V39, P1, DOI 10.1016/S0734-189X(87)80200-1; HANSON AJ, 1988, COMPUT VISION GRAPH, V44, P191, DOI 10.1016/S0734-189X(88)80005-7; HUERTAS A, 1986, IEEE T PATTERN ANAL, V8, P651, DOI 10.1109/TPAMI.1986.4767838; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; PENTLAND AP, 1986, ARTIF INTELL, V28, P293, DOI 10.1016/0004-3702(86)90052-4; SANDER PT, 1987, 1ST P INT C COMP VIS, P241; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346; ZUCKER SW, 1981, IEEE T PATTERN ANAL, V3, P324, DOI 10.1109/TPAMI.1981.4767105; 1988, DATAMATION       FEB, P85; [No title captured]	16	19	19	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	1989	3	1					51	71		10.1007/BF00054838	http://dx.doi.org/10.1007/BF00054838			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AE167					2022-12-18	WOS:A1989AE16700004
J	Zaffar, M; Garg, S; Milford, M; Kooij, J; Flynn, D; McDonald-Maier, K; Ehsan, S				Zaffar, Mubariz; Garg, Sourav; Milford, Michael; Kooij, Julian; Flynn, David; McDonald-Maier, Klaus; Ehsan, Shoaib			VPR-Bench: An Open-Source Visual Place Recognition Evaluation Framework with Quantifiable Viewpoint and Appearance Change	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual place recognition; SLAM; Autonomous robotics; Robotic vision	LARGE-SCALE; IMAGE FEATURES; LOCALIZATION; SINGLE; MODEL; SCENE; SLAM	Visual place recognition (VPR) is the process of recognising a previously visited place using visual information, often under varying appearance conditions and viewpoint changes and with computational constraints. VPR is related to the concepts of localisation, loop closure, image retrieval and is a critical component of many autonomous navigation systems ranging from autonomous vehicles to drones and computer vision systems. While the concept of place recognition has been around for many years, VPR research has grown rapidly as a field over the past decade due to improving camera hardware and its potential for deep learning-based techniques, and has become a widely studied topic in both the computer vision and robotics communities. This growth however has led to fragmentation and a lack of standardisation in the field, especially concerning performance evaluation. Moreover, the notion of viewpoint and illumination invariance of VPR techniques has largely been assessed qualitatively and hence ambiguously in the past. In this paper, we address these gaps through a new comprehensive open-source framework for assessing the performance of VPR techniques, dubbed "VPR-Bench". VPR-Bench (Open-sourced at: ) introduces two much-needed capabilities for VPR researchers: firstly, it contains a benchmark of 12 fully-integrated datasets and 10 VPR techniques, and secondly, it integrates a comprehensive variation-quantified dataset for quantifying viewpoint and illumination invariance. We apply and analyse popular evaluation metrics for VPR from both the computer vision and robotics communities, and discuss how these different metrics complement and/or replace each other, depending upon the underlying applications and system requirements. Our analysis reveals that no universal SOTA VPR technique exists, since: (a) state-of-the-art (SOTA) performance is achieved by 8 out of the 10 techniques on at least one dataset, (b) SOTA technique in one community does not necessarily yield SOTA performance in the other given the differences in datasets and metrics. Furthermore, we identify key open challenges since: (c) all 10 techniques suffer greatly in perceptually-aliased and less-structured environments, (d) all techniques suffer from viewpoint variance where lateral change has less effect than 3D change, and (e) directional illumination change has more adverse effects on matching confidence than uniform illumination change. We also present detailed meta-analyses regarding the roles of varying ground-truths, platforms, application requirements and technique parameters. Finally, VPR-Bench provides a unified implementation to deploy these VPR techniques, metrics and datasets, and is extensible through templates.	[Zaffar, Mubariz; McDonald-Maier, Klaus; Ehsan, Shoaib] Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England; [Zaffar, Mubariz; Kooij, Julian] Delft Univ Technol, Cognit Robot, NL-2628 CD Delft, Netherlands; [Garg, Sourav; Milford, Michael] Queensland Univ Technol, Sch Elect Engn & Comp Sci, Brisbane, Qld 4000, Australia; [Flynn, David] Heriot Watt Univ, Sch Engn & Phys Sci, Smart Syst Grp, Edinburgh EH14 4AS, Midlothian, Scotland	University of Essex; Delft University of Technology; Queensland University of Technology (QUT); Heriot Watt University	Zaffar, M (corresponding author), Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.; Zaffar, M (corresponding author), Delft Univ Technol, Cognit Robot, NL-2628 CD Delft, Netherlands.	mubariz.zaffar@essex.ac.uk; s.garg@qut.edu.au; michael.milford@qut.edu.au; j.f.p.kooij@tudelft.nl; D.Flynn@hw.ac.uk; kdm@essex.ac.uk; sehsan@essex.ac.uk	Garg, Sourav/AAT-6300-2020; Milford, Michael/J-1304-2012	Garg, Sourav/0000-0001-6068-3307; Zaffar, Mubariz/0000-0002-9368-2391; Flynn, David/0000-0002-1024-3618; Milford, Michael/0000-0002-5162-1793; McDonald-Maier, Klaus/0000-0002-6412-8519; Kooij, Julian/0000-0001-9919-0710	UK Engineering and Physical Sciences Research Council [EP/R02572X/1, EP/P017487/1, EP/R026173/1]; RICE project - National Centre for Nuclear Robotics Flexible Partnership Fund; TU Delft AI Labs programme; ARC [FT140101229, CE140100016]; QUT Centre for Robotics	UK Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); RICE project - National Centre for Nuclear Robotics Flexible Partnership Fund; TU Delft AI Labs programme; ARC(Australian Research Council); QUT Centre for Robotics	Our work was supported by the UK Engineering and Physical Sciences Research Council through Grants EP/R02572X/1, EP/P017487/1 and EP/R026173/1 and in part by the RICE project funded by the National Centre for Nuclear Robotics Flexible Partnership Fund. This research has also been supported by the TU Delft AI Labs programme. Michael Milford was supported by ARC Grants FT140101229, CE140100016 and the QUT Centre for Robotics.	Aanaes H, 2012, INT J COMPUT VISION, V97, P18, DOI 10.1007/s11263-011-0473-8; Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; Agrawal M, 2008, LECT NOTES COMPUT SC, V5305, P102, DOI 10.1007/978-3-540-88693-8_8; Angeli A, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P1031, DOI 10.1109/IROS.2008.4650675; Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Arandjelovic R, 2015, LECT NOTES COMPUT SC, V9006, P188, DOI 10.1007/978-3-319-16817-3_13; Arandjelovic R, 2015, LECT NOTES COMPUT SC, V9003, P178, DOI 10.1007/978-3-319-16865-4_12; Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38; Badino H, 2012, IEEE INT CONF ROBOT, P1635, DOI 10.1109/ICRA.2012.6224716; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Bingyi Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P726, DOI 10.1007/978-3-030-58565-5_43; Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754; Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222; Camara L. G., 2019, HIGHLY ROBUST VISUAL; Camara LG, 2019, 2019 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR); Chancan M, 2020, IEEE ROBOT AUTOM LET, V5, P993, DOI 10.1109/LRA.2020.2967324; Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610; Chen Z., 2014, ARXIV14111509, V2, P1; Chen ZT, 2017, IEEE INT C INT ROBOT, P9; Chen ZT, 2018, IEEE ROBOT AUTOM LET, V3, P4015, DOI 10.1109/LRA.2018.2859916; Cheron, 2018, THESIS TRINITY COLL; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Cieslewski T, 2018, IEEE INT CONF ROBOT, P2466; Cieslewski T, 2017, 2017 INTERNATIONAL SYMPOSIUM ON MULTI-ROBOT AND MULTI-AGENT SYSTEMS (MRS); Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; Demir M, 2018, IEEE INT C SEMANT CO, P71, DOI 10.1109/ICSC.2018.00019; DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060; Duckett, 2004, IFAC P VOLUMES, V37, P36; Durand, 2019, 2019 IEEE INT C COMP; Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828; Falliat D, 2007, IEEE INT CONF ROBOT, P3921; Ferrarini B, 2020, IEEE ROBOT AUTOM LET, V5, P1688, DOI 10.1109/LRA.2020.2969197; Fraundorfer F, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P3878; GARDNER MA, 2017, ACM T GRAPHIC, V36, P1; Garg S., 2020, FOUNDAT TRENDS ROB, V8, P1, DOI [DOI 10.1561/9781680837698, 10.1561/2300000059, DOI 10.1561/2300000059, 10.1561/9781680837698]; Garg S., 2021, ARXIV PREPRINT ARXIV; Garg S, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Garg S, 2018, IEEE INT CONF ROBOT, P3645; Girdhar Y, 2010, IEEE INT CONF ROBOT, P5035, DOI 10.1109/ROBOT.2010.5509464; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15; Hausler S, 2019, IEEE ROBOT AUTOM LET, V4, P1924, DOI 10.1109/LRA.2019.2898427; Ho KL, 2007, INT J COMPUT VISION, V74, P261, DOI 10.1007/s11263-006-0020-1; Hold-Geoffroy Y, 2017, PROC CVPR IEEE, P2373, DOI 10.1109/CVPR.2017.255; Hou Y, 2018, J INTELL ROBOT SYST, V92, P505, DOI 10.1007/s10846-017-0735-y; Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Jenicek T, 2019, IEEE I CONF COMP VIS, P9695, DOI 10.1109/ICCV.2019.00979; Jin YH, 2021, INT J COMPUT VISION, V129, P517, DOI 10.1007/s11263-020-01385-0; Johns E, 2011, IEEE I CONF COMP VIS, P874, DOI 10.1109/ICCV.2011.6126328; Konolige K, 2008, IEEE T ROBOT, V24, P1066, DOI 10.1109/TRO.2008.2004832; Kopitkov D, 2018, IEEE INT C INT ROBOT, P7795, DOI 10.1109/IROS.2018.8594506; Kosecka J, 2005, ROBOT AUTON SYST, V52, P27, DOI 10.1016/j.robot.2005.03.008; Kostavelis I, 2015, ROBOT AUTON SYST, V66, P86, DOI 10.1016/j.robot.2014.12.006; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larsson M, 2019, PROC CVPR IEEE, P9524, DOI 10.1109/CVPR.2019.00976; Lategahn H, 2013, IEEE INT VEH SYM, P285, DOI 10.1109/IVS.2013.6629483; Li LX, 2014, INT CONF ACOUST SPEE; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823; Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498; Maddern W, 2012, INT J ROBOT RES, V31, P429, DOI 10.1177/0278364912438273; Masone C, 2021, IEEE ACCESS, V9, P19516, DOI 10.1109/ACCESS.2021.3054937; Matas J., 2015, PCVPR 2015 WORKSH VI, V30; McDonald-Maier M., 2019, IEEE T ROBOT; McManus C., 2014, ROB SCI SYST C; Mei C, 2009, P BRIT MACH VIS C, V1; Merrill N., 2018, ARXIV180507703; Milford M, 2013, INT J ROBOT RES, V32, P766, DOI 10.1177/0278364913490323; Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623; Mohan A, 2007, IEEE T VIS COMPUT GR, V13, P652, DOI 10.1109/TVCG.2007.1008; Mount J, 2016, IEEE INT CONF ROBOT, P4822, DOI 10.1109/ICRA.2016.7487686; Mousavian A, 2015, IEEE INT CONF ROBOT, P4882, DOI 10.1109/ICRA.2015.7139877; Murillo AC, 2007, IEEE INT CONF ROBOT, P3901, DOI 10.1109/ROBOT.2007.364077; Murillo A. C., 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2196, DOI 10.1109/ICCVW.2009.5457552; Murmann L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980219; Nardi L, 2015, IEEE INT CONF ROBOT, P5783, DOI 10.1109/ICRA.2015.7140009; Naseer Tayyab, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2614, DOI 10.1109/ICRA.2017.7989305; Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374; Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2; Paul R, 2014, IEEE INT CONF ROBOT, P1304, DOI 10.1109/ICRA.2014.6907021; Pepperell E, 2015, IEEE INT CONF ROBOT, P1118, DOI 10.1109/ICRA.2015.7139316; Pepperell E, 2014, IEEE INT CONF ROBOT, P1612, DOI 10.1109/ICRA.2014.6907067; Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009; Philbin J, 2008, PROC CVPR IEEE, P2285; Porav H, 2018, IEEE INT CONF ROBOT, P1011; Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566; Radenovic F, 2018, PROC CVPR IEEE, P5706, DOI 10.1109/CVPR.2018.00598; Ranganathan A., 2013, US Patent, Patent No. [8,559,717, 8559717]; Revaud J, 2019, IEEE I CONF COMP VIS, P5106, DOI 10.1109/ICCV.2019.00521; Revaud Jerome, 2019, ADV NEURAL INFORM PR, V32, P12405; Robertson D. P., 2004, BMVC, V19, P165, DOI DOI 10.5244/C.18.84); Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Rosten E., 2006, ECCV, P430, DOI DOI 10.1007/11744023_; Sandev R, 2016, 2016 13TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P53, DOI 10.1109/CRV.2016.38; Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300; Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897; Sattler T, 2016, PROC CVPR IEEE, P1582, DOI 10.1109/CVPR.2016.175; Schonberger JL, 2018, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR.2018.00721; Se S, 2002, INT J ROBOT RES, V21, P735, DOI 10.1177/027836402761412467; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Simeoni O, 2019, PROC CVPR IEEE, P11643, DOI 10.1109/CVPR.2019.01192; Singh G., 2010, ICRA OMN VIS WORKSH, P4042; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Skinner J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P2737, DOI 10.1109/IROS.2016.7759425; Skrede S, 2013, NORDLAND DATASET; Stenborg E, 2018, IEEE INT CONF ROBOT, P6484; Stumm E, 2013, IEEE INT C INT ROBOT, P4158, DOI 10.1109/IROS.2013.6696952; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986; Sunderhauf N, 2011, IEEE INT C INT ROBOT, P1234, DOI 10.1109/IROS.2011.6048590; Sunderhauf Niko, 2013, IEEE INT C ROB AUT W; Talbot B, 2018, IEEE INT C INT ROBOT, P7758, DOI 10.1109/IROS.2018.8593761; Tipaldi GD, 2013, IEEE INT CONF ROBOT, P2693, DOI 10.1109/ICRA.2013.6630947; Tolias G, 2016, INT J COMPUT VISION, V116, P247, DOI 10.1007/s11263-015-0810-4; Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177; Tolias Giorgos, 2016, P ICLR; Tomita M.A., 2021, ARXIV PREPRINT ARXIV; Tomita M.A., 2020, ARXIV PREPRINT ARXIV; Topp EA, 2008, IEEE INT CONF ROBOT, P2564, DOI 10.1109/ROBOT.2008.4543599; Torii A, 2021, IEEE T PATTERN ANAL, V43, P814, DOI 10.1109/TPAMI.2019.2941876; Torii A, 2015, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2015.7298790; Torii A, 2013, PROC CVPR IEEE, P883, DOI 10.1109/CVPR.2013.119; Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470; Vorstius, 2020, 15 INT JOINT C COMP, pE; Wang JL, 2005, Proceedings of 2005 International Conference on Machine Learning and Cybernetics, Vols 1-9, P1256; Warburg Frederik, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P2623, DOI 10.1109/CVPR42600.2020.00270; Weyand T, 2020, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR42600.2020.00265; Ye Y., 2017, BRIT MACH VIS C BMVC; Zaffar, ARXIV PREPRINT ARXIV; Zaffar M., 2018, ARXIV PREPRINT ARXIV; Zaffar M., 2019, IEEE INT C ROBOTICS, P1; Zaffar M, 2020, IEEE ROBOT AUTOM LET, V5, P1835, DOI 10.1109/LRA.2020.2969917; Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303; Zeng F, 2019, IEEE INT CONF ROBOT, P1444, DOI 10.1109/ICRA.2019.8794453; Zetao Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3223, DOI 10.1109/ICRA.2017.7989366; Zhang XW, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107760	138	18	18	5	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2136	2174		10.1007/s11263-021-01469-5	http://dx.doi.org/10.1007/s11263-021-01469-5		MAY 2021	39	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW		Green Published, Green Accepted, hybrid			2022-12-18	WOS:000648223000003
J	Rahman, S; Khan, SH; Porikli, F				Rahman, Shafin; Khan, Salman H.; Porikli, Fatih			Zero-Shot Object Detection: Joint Recognition and Localization of Novel Concepts	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Zero-shot learning; Zero-shot object detection; Deep learning; Loss function	CLASSIFICATION; ATTRIBUTES	Zero shot learning (ZSL) identifies unseen objects for which no training images are available. Conventional ZSL approaches are restricted to a recognition setting where each test image is categorized into one of several unseen object classes. We posit that this setting is ill-suited for real-world applications where unseen objects appear only as a part of a complete scene, warranting both 'recognition' and 'localization' of the unseen category. To address this limitation, we introduce a new'Zero-Shot Detection'(ZSD) problem setting, which aims at simultaneously recognizing and locating object instances belonging to novel categories, without any training samples. We introduce an integrated solution to the ZSD problem that jointly models the complex interplay between visual and semantic domain information. Ours is an end-to-end trainable deep network for ZSD that effectively overcomes the noise in the unsupervised semantic descriptions. To this end, we utilize the concept of meta-classes to design an original loss function that achieves synergy between max-margin class separation and semantic domain clustering. In order to set a benchmark for ZSD, we propose an experimental protocol for the large-scale ILSVRC dataset that adheres to practical challenges, e.g., rare classes are more likely to be the unseen ones. Furthermore, we present a baseline approach extended from conventional recognition to the ZSD setting. Our extensive experiments show a significant boost in performance (in terms of mAP and Recall) on the imperative yet difficult ZSD problem on ImageNet detection, MSCOCO and FashionZSD datasets.	[Rahman, Shafin] North South Univ, Dhaka, Bangladesh; [Rahman, Shafin] CSIRO, Data61, Canberra, ACT 2601, Australia; [Rahman, Shafin; Porikli, Fatih] Australian Natl Univ, Canberra, ACT 0200, Australia; [Khan, Salman H.] Mohamed bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Porikli, Fatih] Huawei, San Diego, CA USA	North South University (NSU); Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University; Mohamed Bin Zayed University of Artificial Intelligence; Huawei Technologies	Rahman, S (corresponding author), North South Univ, Dhaka, Bangladesh.; Rahman, S (corresponding author), CSIRO, Data61, Canberra, ACT 2601, Australia.; Rahman, S (corresponding author), Australian Natl Univ, Canberra, ACT 0200, Australia.	shafin.rahman@northsouth.edu; salman.khan@mbzuai.ac.ae; fatih.porikli@anu.edu.au	Rahman, Shafin/N-1939-2019; Khan, Salman Hameed/M-4834-2016	Rahman, Shafin/0000-0001-7169-0318; Khan, Salman Hameed/0000-0002-9502-1749				Akata Z, 2016, PROC CVPR IEEE, P59, DOI 10.1109/CVPR.2016.14; Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Al-Halah Z, 2016, PROC CVPR IEEE, P5975, DOI 10.1109/CVPR.2016.643; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8; Buchmann J, 2017, QUANTUM SCI TECHNOL, V2, DOI 10.1088/2058-9565/aa69cd; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; Demirel B., 2018, BRIT MACH VIS C, P56; Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4; Deutsch S, 2017, PROC CVPR IEEE, P5292, DOI 10.1109/CVPR.2017.562; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Frome Andrea, 2013, NEURIPS; Fu Y., 2015, ABS150307790 CORR; Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493; Jayaraman D, 2014, ADV NEUR IN, V27; Jetley S., 2016, ARXIV161107932; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Li XR, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P879, DOI 10.1145/2766462.2767773; Li Yi, 2017, P IEEE C COMP VIS PA; Li ZY, 2017, PROC CVPR IEEE, P7350, DOI 10.1109/CVPR.2017.777; Li ZY, 2014, LECT NOTES COMPUT SC, V8694, P350, DOI 10.1007/978-3-319-10599-4_23; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Maxime Bucher S. H., 2016, P 14 EUR C COMP VIS; Mikolov T, 2013, P 26 INT C NEURAL IN, P3111; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220; Palatucci Mark, 2009, ADV NEURAL INFORM PR, P1410; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Rahman S., 2020, ARXIV200307003; Rahman S, 2020, AAAI CONF ARTIF INTE, V34, P11932; Rahman S, 2019, IEEE I CONF COMP VIS, P6081, DOI 10.1109/ICCV.2019.00618; Rahman S, 2019, LECT NOTES COMPUT SC, V11361, P547, DOI 10.1007/978-3-030-20887-5_34; Rahman S, 2018, IEEE T IMAGE PROCESS, V27, P5652, DOI 10.1109/TIP.2018.2861573; Rahman Shafin, 2018, ARXIV181108982; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schonfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844; Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Wah C., 2011, TECH REP; Wang XY, 2013, IEEE I CONF COMP VIS, P2120, DOI 10.1109/ICCV.2013.264; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Xiao H., 2017, FASHION MNIST NOVEL; Ye M, 2017, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR.2017.542; Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang Y, 2016, PROC CVPR IEEE, P5985, DOI 10.1109/CVPR.2016.644; Zhang ZM, 2016, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2016.649; Zhang ZY, 2015, IEEE I CONF COMP VIS, P2614, DOI 10.1109/ICCV.2015.300; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhu P., 2018, ARXIV180307113	66	18	18	2	26	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2979	2999		10.1007/s11263-020-01355-6	http://dx.doi.org/10.1007/s11263-020-01355-6		JUL 2020	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ					2022-12-18	WOS:000552163800001
J	Kong, B; Supancic, J; Ramanan, D; Fowlkes, CC				Kong, Bailey; Supancic, James, III; Ramanan, Deva; Fowlkes, Charless C.			Cross-Domain Image Matching with Deep Feature Maps	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Normalized cross-correlation; Similarity metric; Cross-domain image matching	FOURIER-TRANSFORM	We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene. This recognition problem is made difficult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns. We find that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for this specialized domains. However, the choice of similarity measure for matching exemplars to a query image is essential to good performance. For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness. Our proposed metric significantly improves performance in matching crime scene shoeprints to laboratory test impressions. We also show its effectiveness in other cross-domain image retrieval problems: matching facade images to segmentation labels and aerial photos to map images. Finally, we introduce a discriminatively trained variant and fine-tune our system through our proposed metric, obtaining state-of-the-art performance.	[Kong, Bailey; Supancic, James, III; Fowlkes, Charless C.] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92617 USA; [Ramanan, Deva] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA	University of California System; University of California Irvine; Carnegie Mellon University	Kong, B (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92617 USA.	bhkong@ics.uci.edu; jsupanci@ics.uci.edu; deva@cs.cmu.edu; fowlkes@ics.uci.edu		Kong, Bailey/0000-0001-6556-8017	Center for Statistics and Applications in Forensic Evidence (CSAFE) through NIST [70NANB15H176]	Center for Statistics and Applications in Forensic Evidence (CSAFE) through NIST	We thank Sarena Wiesner andYaron Shor for providing access to their dataset. This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through NIST Cooperative Agreement #70NANB15H176.	Bodziak W. J., 1999, FOOTWEAR IMPRESSION; Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470; Chia-Hung Wei, 2014, 2014 International Conference on Information Science, Electronics and Electrical Engineering (ISEEE), P1069, DOI 10.1109/InfoSEEE.2014.6947833; Costea D., 2016, ARXIV160508323; Dardi F, 2009, LECT NOTES COMPUT SC, V5716, P384, DOI 10.1007/978-3-642-04146-4_42; de Chazal P, 2005, IEEE T PATTERN ANAL, V27, P341, DOI 10.1109/TPAMI.2005.48; Divecha M, 2016, 24TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2016), DOI 10.1145/2996913.2996980; Fisher R. B., 1995, P BRIT MACH VIS C BM; GEISS S, 1991, ANAL CHIM ACTA, V242, P5, DOI 10.1016/0003-2670(91)87040-E; Gueham M, 2008, 2008 INTERNATIONAL MACHINE VISION AND IMAGE PROCESSING CONFERENCE, PROCEEDINGS, P37, DOI 10.1109/IMVIP.2008.25; Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33; Isola P, 2017, P 2017 IEEE C COMP V; Kong B., 2017, BRIT MACH VIS C BMVC; Kortylewski A., 2016, BRIT MACH VIS C; Kortylewski A., 2017, THESIS; Kortylewski A, 2015, LECT NOTES COMPUT SC, V9008, P644, DOI 10.1007/978-3-319-16628-5_46; Lee HC, 2001, ADV FINGERPRINT TECH; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Mardia K. V., 1980, MULTIVARIATE ANAL PR; Martin N., 1979, MULTIVARIATE ANAL; Parkhi O. M., 2015, BMVC, V1; Patil PM, 2009, PATTERN RECOGN, V42, P1308, DOI 10.1016/j.patcog.2008.11.008; Pavlou M, 2006, LECT NOTES COMPUT SC, V4224, P721; Radim Tyleek R.S, 2013, P GCPR SAARBR GERM; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Richetelli N, 2017, FORENSIC SCI INT, V275, P102, DOI 10.1016/j.forsciint.2017.02.030; Russell B. C., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P545, DOI 10.1109/ICCVW.2011.6130291; Senlet T, 2014, INT C PATT RECOG, P2990, DOI 10.1109/ICPR.2014.516; SHAFFER JP, 1974, EDUC PSYCHOL MEAS, V34, P521; Shrivastava A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024188; Tang B, 2010, 2010 INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE AND ENGINEERING (MSE 2010), VOL 5, P88; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Yekutieli Y., 2012, TP3211 NAT I JUST; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767; Zhang L., 2005, UK WORKSH COMP INT	37	18	19	2	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1738	1750		10.1007/s11263-018-01143-3	http://dx.doi.org/10.1007/s11263-018-01143-3			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY		Green Submitted			2022-12-18	WOS:000492425300010
J	Zhong, LS; Zhang, L				Zhong, Leisheng; Zhang, Li			A Robust Monocular 3D Object Tracking Method Combining Statistical and Photometric Constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D object tracking; Region-based method; Direct method; Statistical constraints; Photometric constraints	VISUAL TRACKING; POSE ESTIMATION; SEGMENTATION	Both region-based methods and direct methods have become popular in recent years for tracking the 6-dof pose of an object from monocular video sequences. Region-based methods estimate the pose of the object by maximizing the discrimination between statistical foreground and background appearance models, while direct methods aim to minimize the photometric error through direct image alignment. In practice, region-based methods only care about the pixels within a narrow band of the object contour due to the level-set-based probabilistic formulation, leaving the foreground pixels beyond the evaluation band unused. On the other hand, direct methods only utilize the raw pixel information of the object, but ignore the statistical properties of foreground and background regions. In this paper, we find it beneficial to combine these two kinds of methods together. We construct a new probabilistic formulation for 3D object tracking by combining statistical constraints from region-based methods and photometric constraints from direct methods. In this way, we take advantage of both statistical property and raw pixel values of the image in a complementary manner. Moreover, in order to achieve better performance when tracking heterogeneous objects in complex scenes, we propose to increase the distinctiveness of foreground and background statistical models by partitioning the global foreground and background regions into a small number of sub-regions around the object contour. We demonstrate the effectiveness of the proposed novel strategies on a newly constructed real-world dataset containing different types of objects with ground-truth poses. Further experiments on several challenging public datasets also show that our method obtains competitive or even superior tracking results compared to previous works. In comparison with the recent state-of-art region-based method, the proposed hybrid method is proved to be more stable under silhouette pose ambiguities with a slightly lower tracking accuracy.	[Zhong, Leisheng; Zhang, Li] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China	Tsinghua University	Zhong, LS (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.	zls13@mails.tsinghua.edu.cn; chinazhangli@mail.tsinghua.edu.cn			National Natural Science Foundation of China [U1533132]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is partly supported by the National Natural Science Foundation of China under Grant U1533132.	Alismail H, 2016, INT CONF 3D VISION, P389, DOI 10.1109/3DV.2016.48; [Anonymous], [No title captured]; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Bibby C, 2008, LECT NOTES COMPUT SC, V5303, P831, DOI 10.1007/978-3-540-88688-4_61; Caron G, 2014, IMAGE VISION COMPUT, V32, P54, DOI 10.1016/j.imavis.2013.10.007; Choi C, 2010, IEEE INT CONF ROBOT, P4048, DOI 10.1109/ROBOT.2010.5509171; Crivellaro A, 2014, PROC CVPR IEEE, P3414, DOI 10.1109/CVPR.2014.436; Dambreville S, 2008, LECT NOTES COMPUT SC, V5303, P169, DOI 10.1007/978-3-540-88688-4_13; Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577; Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54; Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005; Hexner J, 2016, INT J COMPUT VISION, V118, P95, DOI 10.1007/s11263-015-0873-2; Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326; Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169; Kehl W, 2017, PROC CVPR IEEE, P465, DOI 10.1109/CVPR.2017.57; Kerl C, 2013, IEEE INT CONF ROBOT, P3748, DOI 10.1109/ICRA.2013.6631104; Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001; Lima J., 2010, SBC, V1, P2; Lin Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4429, DOI 10.1109/ICRA.2017.7989512; Loesch A, 2015, IEEE INT C INT ROBOT, P6059, DOI 10.1109/IROS.2015.7354240; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Park Y, 2008, INT SYM MIX AUGMENT, P117, DOI 10.1109/ISMAR.2008.4637336; Pauwels K, 2013, PROC CVPR IEEE, P2347, DOI 10.1109/CVPR.2013.304; Petit A, 2013, IEEE INT C INT ROBOT, P3719, DOI 10.1109/IROS.2013.6696887; Prisacariu VA, 2012, INT J COMPUT VISION, V98, P335, DOI 10.1007/s11263-011-0514-3; Prisacariu VA, 2013, INT SYM MIX AUGMENT, P89, DOI 10.1109/ISMAR.2013.6671768; Ren CY, 2017, INT J COMPUT VISION, V124, P80, DOI 10.1007/s11263-016-0978-2; Ren Carl Yuheng, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P47, DOI 10.1109/3DV.2014.39; Scandaroli GG, 2012, LECT NOTES COMPUT SC, V7577, P442, DOI 10.1007/978-3-642-33783-3_32; Seo BK, 2016, LECT NOTES COMPUT SC, V9915, P551, DOI 10.1007/978-3-319-49409-8_48; Seo BK, 2014, IEEE T VIS COMPUT GR, V20, P99, DOI 10.1109/TVCG.2013.94; Singhal P., 2016, ARXIV160304117; Tjaden H, 2017, IEEE I CONF COMP VIS, P124, DOI 10.1109/ICCV.2017.23; Tjaden H, 2016, LECT NOTES COMPUT SC, V9908, P423, DOI 10.1007/978-3-319-46493-0_26; Zhao S, 2014, IEEE IMAGE PROC, P486, DOI 10.1109/ICIP.2014.7025097; Zhong LS, 2018, IEEE T CIRC SYST VID, V28, P2302, DOI 10.1109/TCSVT.2017.2731519	36	18	19	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					973	992		10.1007/s11263-018-1119-x	http://dx.doi.org/10.1007/s11263-018-1119-x			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM					2022-12-18	WOS:000474559000001
J	Ruder, M; Dosovitskiy, A; Brox, T				Ruder, Manuel; Dosovitskiy, Alexey; Brox, Thomas			Artistic Style Transfer for Videos and Spherical Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Style transfer; Deep networks; Artistic videos; Video stylization		Manually re-drawing an image in a certain artistic style takes a professional artist a long time. Doing this for a video sequence single-handedly is beyond imagination. We present two computational approaches that transfer the style from one image (for example, a painting) to a whole video sequence. In our first approach, we adapt to videos the original image style transfer technique by Gatys et al. based on energy minimization. We introduce new ways of initialization and new loss functions to generate consistent and stable stylized video sequences even in cases with large motion and strong occlusion. Our second approach formulates video stylization as a learning problem. We propose a deep network architecture and training procedures that allow us to stylize arbitrary-length videos in a consistent and stable way, and nearly in real time. We show that the proposed methods clearly outperform simpler baselines both qualitatively and quantitatively. Finally, we propose a way to adapt these approaches also to 360 images and videos as they emerge with recent virtual reality hardware.	[Ruder, Manuel; Dosovitskiy, Alexey; Brox, Thomas] Univ Freiburg, BIOSS Ctr Biol Signalling Studies, Dept Comp Sci, Freiburg, Germany	University of Freiburg	Ruder, M (corresponding author), Univ Freiburg, BIOSS Ctr Biol Signalling Studies, Dept Comp Sci, Freiburg, Germany.	rudera@cs.uni-freiburg.de; dosovits@cs.uni-freiburg.de; brox@cs.uni-freiburg.de	Li, Mengqi/AAG-6804-2021					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126; Collobert R., 2011, NIPS; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Ghiasi Golnaz, 2017, BMVC; Gupta A, 2017, IEEE I CONF COMP VIS, P4087, DOI 10.1109/ICCV.2017.438; Hays J., 2004, NPAR, P113; Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Litwinowicz P, 1997, P SIGGRAPH 97, P407; Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740; Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557; Nikulin Y., 2016, ARXIVABS160207188 CO; O'Donovan P, 2012, IEEE T VIS COMPUT GR, V18, P475, DOI 10.1109/TVCG.2011.51; Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720; Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3; Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Ulyanov D., 2016, ARXIVABS160708022 CO; Ulyanov D, 2016, PR MACH LEARN RES, V48; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Yu F., 2016, P ICLR 2016; Zhang H., 2017, ARXIVABS170306953 CO	29	18	18	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2018	126	11					1199	1219		10.1007/s11263-018-1089-z	http://dx.doi.org/10.1007/s11263-018-1089-z			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GT3IE		Green Submitted			2022-12-18	WOS:000444394200003
J	Kadambi, A; Taamazyan, V; Shi, BX; Raskar, R				Kadambi, Achuta; Taamazyan, Vage; Shi, Boxin; Raskar, Ramesh			Depth Sensing Using Geometrically Constrained Polarization Normals	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	IEEE International Conference on Computer Vision (ICCV)	DEC 11-18, 2015	Santiago, CHILE	CPS, IEEE Comp Soc, Amazon, Microsoft, SENSETIME, Baidu, Intel, Facebook, Adobe, Panasonic, Google, OMRON, Blippar, iRobot, HISCENE, NVIDIA, Viscovery, AiCUre, M Tec, Inst Elect & Elect Engineers, Comp Vis Fdn		Computational photography; Light transport; Depth sensing; Shape from polarization	SURFACE ORIENTATION	Analyzing the polarimetric properties of reflected light is a potential source of shape information. However, it is well-known that polarimetric information contains fundamental shape ambiguities, leading to an underconstrained problem of recovering 3D geometry. To address this problem, we use additional geometric information, from coarse depth maps, to constrain the shape information from polarization cues. Our main contribution is a framework that combines surface normals from polarization (hereafter polarization normals) with an aligned depth map. The additional geometric constraints are used to mitigate physics-based artifacts, such as azimuthal ambiguity, refractive distortion and fronto-parallel signal degradation. We believe our work may have practical implications for optical engineering, demonstrating a new option for state-of-the-art 3D reconstruction.	[Kadambi, Achuta; Taamazyan, Vage; Shi, Boxin; Raskar, Ramesh] MIT, Media Lab, 75 Amherst St, Cambridge, MA 02139 USA; [Taamazyan, Vage] Moscow Inst Phys & Technol, Skoltech MIT Initiat, Moscow, Russia; [Taamazyan, Vage] Tardis 3D Technol LLC, Moscow, Russia; [Shi, Boxin] Natl Inst Adv Ind Sci & Technol, Artificial Intelligence Res Ctr, Tokyo, Japan	Massachusetts Institute of Technology (MIT); Moscow Institute of Physics & Technology; National Institute of Advanced Industrial Science & Technology (AIST)	Kadambi, A; Shi, BX (corresponding author), MIT, Media Lab, 75 Amherst St, Cambridge, MA 02139 USA.; Shi, BX (corresponding author), Natl Inst Adv Ind Sci & Technol, Artificial Intelligence Res Ctr, Tokyo, Japan.	achoo@mit.edu; vaheta@gmail.com; shiboxin@media.mit.edu; raskar@media.mit.edu			Charles Draper Doctoral Fellowship; Qualcomm Innovation Fellowship; New Energy and Industrial Technology Development Organization (NEDO)	Charles Draper Doctoral Fellowship; Qualcomm Innovation Fellowship; New Energy and Industrial Technology Development Organization (NEDO)(New Energy and Industrial Technology Development Organization (NEDO))	The authors thank G. Atkinson, T. Boult, S. Izadi, D. Miyazaki, G. Satat, N. Naik, I.K. Park, H. Zhao for valuable feedback. Achuta Kadambi is supported by the Charles Draper Doctoral Fellowship and the Qualcomm Innovation Fellowship. Boxin Shi is supported by a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO). The work of the MIT-affiliated coauthors was supported by the Media Lab Consortium members.	Ackermann J., 2012, IEEE C COMP VIS PATT; Agrawal A., 2006, ECCV; [Anonymous], 2016, CVPR; Atkinson G.A., 2005, ICCV; Atkinson GA, 2006, IEEE T IMAGE PROCESS, V15, P1653, DOI 10.1109/TIP.2006.871114; Basri R., 2007, IJCV; Chen Tongbo, 2007, CVPR; Cula O. G., 2007, IEEE TPAMI; Dickson CN, 2015, J OPT SOC AM A, V32, P2307, DOI 10.1364/JOSAA.32.002307; Fraile R., 2006, ICPR; Ghosh A., 2010, SIGGRAPH ASIA; Ghosh A., 2011, SIGGRAPH ASIA; Ghosh A., 2009, EGSR; Guarnera G. C., 2012, ECCV WORKSH; Gupta M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2735702; Han Y., 2013, ICCV; Haque S. M., 2014, CVPR; Hecht E., 2015, OPTICS; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Holroyd M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409086; Huynh CP, 2013, INT J COMPUT VISION, V101, P64, DOI 10.1007/s11263-012-0546-3; Izadi S., 2011, ACM UIST; Jayasuriya S, 2015, OPT LETT, V40, P2433, DOI 10.1364/OL.40.002433; Joshi Neel, 2007, ICCV; Kadambi A., 2015, SIGGRAPH 2015 STUDIO, P23; Kadambi A, 2015, INT CONF ACOUST SPEE, P1166, DOI 10.1109/ICASSP.2015.7178153; Kadambi Achuta, 2015, ICCV; Klasing K., 2011, ICRA; Lanman D., 2011, SIGGRAPH ASIA; Ma W.-C., 2007, EUROGRAPHICS; Manakov A., 2013, SIGGRAPH; Mitra N. J., 2003, GEOM EUR S COMP; Miyazaki D., 2003, ELECT IMAGING; Miyazaki D., 2012, 3DIMPVT; Miyazaki D., 2004, TPAMI; Miyazaki Daisuke, 2003, ICCV; Naik N., 2015, SIGGRAPH; Naik N., 2015, CVPR; NAYAR SK, 1997, IJCV; Ngo Thanh T., 2015, SHAPE LIGHT DIRECTIO; NIESSNER M, 2013, SIGGRAPH ASIA; Oliensis J., 1991, UNIQUENESS SHAPE SHA; Or-el R., 2015, IEEE TPAMI; Or-el R., 2015, RGBD FUSION REAL TIM; OTOOLE M, 2014, SIGGRAPH; Rahmann S., 2001, RECONSTRUCTION SPECU; Robles-Kelly A., 2012, IMAGING SPECTROSCOPY; SAITO M, 1999, CVPR; Schechner Y. Y., 2015, ICCP; Schechner Y. Y., 2001, CVPR; Shi B., 2014, TPAMI; Shi B., 2014, CVPR; Shi B., 2014, IEEE; Smith W. A., 2016, LINEAR DEPTH ESTIMAT; Stolz C., 2016, SPIE PHOTONICS EUROP, P98; TREIBITZ T, 2009, IEEE TPAMI; Wallace AM, 1999, INT J COMPUT VISION, V32, P87, DOI 10.1023/A:1008154415349; Wolff LB, 1997, IMAGE VISION COMPUT, V15, P81, DOI 10.1016/S0262-8856(96)01123-7; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu C., 2014, SIGGRAPH; WU C, 2011, CVPR; Yeung S.K., 2014, TPAMI; Yu L.-F., 2013, CVPR; Yuille A., 1997, SHAPE; Zappa CJ, 2008, MEAS SCI TECHNOL, V19, DOI 10.1088/0957-0233/19/5/055503; ZHANG L, 2003, ICCV; Zhang LC, 2012, INT C PATT RECOG, P3791; Zhao Y, 2016, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-662-49373-1; Zickler TE, 2002, INT J COMPUT VISION, V49, P215, DOI 10.1023/A:1020149707513; Zickler T, 2006, IEEE T PATTERN ANAL, V28, P1287, DOI 10.1109/TPAMI.2006.170	70	18	20	2	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2017	125	1-3			SI		34	51		10.1007/s11263-017-1025-7	http://dx.doi.org/10.1007/s11263-017-1025-7			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	FL2TO		Green Submitted			2022-12-18	WOS:000414072800004
J	Shen, XY; Zhou, C; Xu, L; Jia, JY				Shen, Xiaoyong; Zhou, Chao; Xu, Li; Jia, Jiaya			Mutual-Structure for Joint Filtering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	IEEE International Conference on Computer Vision (ICCV)	DEC 11-18, 2015	Santiago, CHILE	CPS, IEEE Comp Soc, Amazon, Microsoft, SENSETIME, Baidu, Intel, Facebook, Adobe, Panasonic, Google, OMRON, Blippar, iRobot, HISCENE, NVIDIA, Viscovery, AiCUre, M Tec, Inst Elect & Elect Engineers, Comp Vis Fdn		Image filter; Mutual structure; Joint estimation; Depth refinement; Stereo matching		Previous joint/guided filters directly transfer structural information from the reference to the target image. In this paper, we analyze the major drawback-that is, there may be completely different edges in the two images. Simply considering all patterns could introduce significant errors. To address this issue, we propose the concept of mutual-structure, which refers to the structural information that is contained in both images and thus can be safely enhanced by joint filtering. We also use an untraditional objective function that can be efficiently optimized to yield mutual structure. Our method results in important edge preserving property, which greatly benefits depth completion, optical flow estimation, image enhancement, stereo matching, to name a few.	[Shen, Xiaoyong; Zhou, Chao; Jia, Jiaya] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China; [Xu, Li] SenseTime Grp Ltd, Shatin, Hong Kong, Peoples R China	Chinese University of Hong Kong	Shen, XY (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.	xyshen@cse.cuhk.edu.hk; zhouc@cse.cuhk.edu.hk; xuli@sensetime.com; leojia@cse.cuhk.edu.hk	Jia, Jiaya/I-3251-2012		Research Grant Council of the Hong Kong Special Administrative Region [413113]	Research Grant Council of the Hong Kong Special Administrative Region(Hong Kong Research Grants Council)	This research is supported by the Research Grant Council of the Hong Kong Special Administrative Region under Grant Number 413113.	Arbelaez Pablo, 2014, CVPR; Carlo T., 1998, ICCV; Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1239451.1239554, 10.1145/1276377.1276506]; Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910; Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574; Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171; Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666; Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328; Gastal ESL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185529; Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964; He Kaiming, 2010, ECCV; Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156; Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402; Lu S., 2014, CVPR; Ma Z., 2013, ICCV; PARIS S, 2006, ECCV; Paris S, 2008, FOUND TRENDS COMPUT, V4, P1, DOI 10.1561/0600000020; Park Jaesik, 2011, ICCV; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Raskar R., 2004, NPAR; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Shen X., 2014, ECCV; Shen XY, 2015, IEEE I CONF COMP VIS, P3406, DOI 10.1109/ICCV.2015.389; Shih YC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508419; van de Weijer J., 2001, CVPR; Xiao J., 2006, ECCV; Xie S., 2015, ICCV; YANG Q, 2009, CVPR; Yang Q., 2012, ECCV; Yang QX, 2015, IEEE T PATTERN ANAL, V37, P834, DOI 10.1109/TPAMI.2014.2353642; Zhang Q., 2014, ECCV; Zhang Q., 2014, CVPR; Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146	35	18	20	3	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2017	125	1-3			SI		19	33		10.1007/s11263-017-1021-y	http://dx.doi.org/10.1007/s11263-017-1021-y			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	FL2TO					2022-12-18	WOS:000414072800003
J	Hayat, M; Khan, SH; Bennamoun, M				Hayat, Munawar; Khan, Salman H.; Bennamoun, Mohammed			Empowering Simple Binary Classifiers for Image Set Based Face Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image set classification; Binary to multi-class classification; Video based face recognition; Object recognition	APPEARANCE	Face recognition from image sets has numerous real-life applications including recognition from security and surveillance systems, multi-view camera networks and personal albums. An image set is an unordered collection of images (e.g., video frames, images acquired over long term observations and personal albums) which exhibits a wide range of appearance variations. The main focus of the previously developed methods has therefore been to find a suitable representation to optimally model these variations. This paper argues that such a representation could not necessarily encode all of the information contained in the set. The paper, therefore, suggests a different approach which does not resort to a single representation of an image set. Instead, the images of the set are retained in their original form and an efficient classification strategy is developed which extends well-known simple binary classifiers for the task of multi-class image set classification. Unlike existing binary to multi-class extension strategies, which require multiple binary classifiers to be trained over a large number of images, the proposed approach is efficient since it trains only few binary classifiers on very few images. Extensive experiments and comparisons with existing methods show that the proposed approach achieves state of the art performance for image set classification based face and object recognition on a number of challenging datasets.	[Hayat, Munawar] Univ Canberra, Human Ctr Technol Res Ctr, Bruce, ACT 2617, Australia; [Khan, Salman H.] CSIRO, Data61, Canberra, ACT, Australia; [Khan, Salman H.] Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT, Australia; [Bennamoun, Mohammed] Univ Western Australia, Sch Comp Sci & Software Engn, Crawley, WA 6009, Australia	University of Canberra; Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University; University of Western Australia	Hayat, M (corresponding author), Univ Canberra, Human Ctr Technol Res Ctr, Bruce, ACT 2617, Australia.	munawar.hayat@canberra.edu.au; salman.khan@data61.csiro.au; mohammed.bennamoun@uwa.edu.au	Hayat, Munawar/AAU-8658-2020; Bennamoun, Mohammed/C-2789-2013; Khan, Salman Hameed/M-4834-2016	Hayat, Munawar/0000-0002-2706-5985; Bennamoun, Mohammed/0000-0002-6603-3257; Khan, Salman Hameed/0000-0002-9502-1749				An SJ, 2015, IEEE I CONF COMP VIS, P2515, DOI 10.1109/ICCV.2015.289; Arandjelovic O, 2005, PROC CVPR IEEE, P581; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chien JT, 2002, IEEE T PATTERN ANAL, V24, P1644, DOI 10.1109/TPAMI.2002.1114855; Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11; Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458; Goldberger J., 2004, ADV NEURAL INFORM PR, V17; Gross Ralph, 2001, CMURITR0118; Harandi M. T., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2705, DOI 10.1109/CVPR.2011.5995564; Hayat M., 2014, 2014 IEEE C COMP VIS; Hayat M., 2013, 2013 IEEE WORKSH APP; Hayat M, 2015, IEEE T PATTERN ANAL, V37, P713, DOI 10.1109/TPAMI.2014.2353635; Hayat M, 2014, IEEE T AFFECT COMPUT, V5, P301, DOI 10.1109/TAFFC.2014.2330580; Hayat M, 2014, LECT NOTES COMPUT SC, V8694, P784, DOI 10.1007/978-3-319-10599-4_50; Hu YQ, 2012, IEEE T PATTERN ANAL, V34, P1992, DOI 10.1109/TPAMI.2011.283; Huang Z., 2014, LEARNING EUCLIDEAN R; Jia Y., 2014, P 22 ACM INT C MULT, P675; Khan SH, 2014, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2014.249; Khan SH, 2016, IEEE T IMAGE PROCESS, V25, P3372, DOI 10.1109/TIP.2016.2567076; Kim M, 2008, PROC CVPR IEEE, P1787; Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Lee KC, 2003, PROC CVPR IEEE, P313; Leibe B, 2003, PROC CVPR IEEE, P409; Li BYL, 2013, IEEE WORK APP COMP, P186, DOI 10.1109/WACV.2013.6475017; Lu J., 2013, 2013 IEEE C INT C CO; Ng H. W., 2014, IEEE INT C IM PROC P; Oja E., 1983, SUBSPACE METHODPAT, V4; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Ortiz EG, 2013, PROC CVPR IEEE, P3531, DOI 10.1109/CVPR.2013.453; Parkhi Omkar M., 2015, BRIT MACH VIS C; Razavian Ali Sharif, 2014, P IEEE C COMP VIS PA, P806, DOI DOI 10.1109/CVPRW.2014.131; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shakhnarovich G, 2002, LECT NOTES COMPUT SC, V2352, P851, DOI 10.1007/3-540-47977-5_56; Sharma A., 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247923; Sugiyama M, 2007, J MACH LEARN RES, V8, P1027; Uzair M, 2013, INT CONF BIOMETR; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Vincent P, 2002, ADV NEUR IN, V14, P985; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang RP, 2008, PROC CVPR IEEE, P2940; Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965; Wang RP, 2009, PROC CVPR IEEE, P429, DOI 10.1109/CVPRW.2009.5206850; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Yamaguchi O, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P318, DOI 10.1109/AFGR.1998.670968; Yang M., 2013, PROC 10 IEEE INT C W, P1, DOI DOI 10.1109/FG.2013.6553727; Yang P, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P356; Yin LJ, 2008, IEEE INT CONF AUTOMA, P116; Zhiwu Huang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P589, DOI 10.1007/978-3-642-37444-9_46; Zhu P., 2013, 2013 IEEE C INT C CO	57	18	22	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2017	123	3					479	498		10.1007/s11263-017-1000-3	http://dx.doi.org/10.1007/s11263-017-1000-3			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EX9EO					2022-12-18	WOS:000403559600009
J	Li, B; Xiong, WH; Hu, WM; Funt, B; Xing, JL				Li, Bing; Xiong, Weihua; Hu, Weiming; Funt, Brian; Xing, Junliang			Multi-Cue Illumination Estimation via a Tree-Structured Group Joint Sparse Representation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Color constancy; Illumination estimation; Automatic white balancing; Tree-structured group; Sparse representation	COLOR CONSTANCY ALGORITHMS; CHROMATICITY; STATISTICS	A multi-cue illumination estimation method based on tree-structured group joint sparse representation is proposed. Tests show that the proposed method works better than existing methods, most of which are based on using only a single cue type, for example, a binarized color histogram or simple image statistic such as the mean RGB. Most existing illumination estimation methods make their estimates using only one of three kinds of cues. They differ in which cue type they use, but the chosen cue is either based on (1) properties of the low-level RGB color distribution, (2) mid-level initial illuminant estimates provided by subordinate methods, or (3) high-level knowledge of scene content (e.g., indoor versus outdoor scene). The proposed multi-cue method combines the information provided by cues of all three of these types within the framework of a tree-structured group joint sparse representation (TGJSR). In TGJSR, the training data is grouped into a tree of subgroups. A test image under an unknown illuminant has its features reconstructed in terms of a joint sparse representation model derived from the grouped training data. The test image's illumination is then estimated based on the weights involved in the joint sparse representation model. As a general framework, the proposed TGJSR framework can also easily be extended to incorporate any new features or cues that might be discovered in the future for illumination estimation.	[Li, Bing; Xiong, Weihua; Hu, Weiming; Xing, Junliang] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Funt, Brian] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada	Chinese Academy of Sciences; Institute of Automation, CAS; Simon Fraser University	Hu, WM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.	wmhu@nlpr.ia.ac.cn	Xing, Junliang/HGE-9630-2022; Li, Bing/AAX-5919-2021	Xing, Junliang/0000-0001-6801-0510; 	National Nature Science Foundation of China [61370038, 61472421]; 973 Basic Research Program of China [2014CB349303]; Chinese National Programs for High Technology Research and Development (863 Program) [2012AA012503, 2012AA012504]; Natural Sciences and Engineering Research Council of Canada	National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); 973 Basic Research Program of China(National Basic Research Program of China); Chinese National Programs for High Technology Research and Development (863 Program)(National High Technology Research and Development Program of China); Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR)	This work was supported by the National Nature Science Foundation of China (Nos. 61370038, and 61472421), the 973 Basic Research Program of China (No. 2014CB349303), and Chinese National Programs for High Technology Research and Development (863 Program) (Nos. 2012AA012503 and 2012AA012504), as well as the Natural Sciences and Engineering Research Council of Canada.	Barnard K, 2002, IEEE T IMAGE PROCESS, V11, P972, DOI 10.1109/TIP.2002.802531; Barnard K, 2002, IEEE T IMAGE PROCESS, V11, P985, DOI 10.1109/TIP.2002.802529; Beigpour S, 2014, IEEE T IMAGE PROCESS, V23, P83, DOI 10.1109/TIP.2013.2286327; Bengio S., 2009, P INT C NEUR INF PRO, P82; Bianco S, 2010, PATTERN RECOGN, V43, P695, DOI 10.1016/j.patcog.2009.08.007; Bianco S, 2008, IEEE T IMAGE PROCESS, V17, P2381, DOI 10.1109/TIP.2008.2006661; Bianco S, 2008, J ELECTRON IMAGING, V17, DOI 10.1117/1.2921013; Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393; BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7; Cardei V. C., 1999, COL IM C, V1, P311; Cardei VC, 2002, J OPT SOC AM A, V19, P2374, DOI 10.1364/JOSAA.19.002374; Chakrabarti A, 2012, IEEE T PATTERN ANAL, V34, P1509, DOI 10.1109/TPAMI.2011.252; Chen X, 2009, IEEE DATA MINING, P746, DOI 10.1109/ICDM.2009.128; Ciurea F, 2003, ELEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING - SYSTEMS, TECHNOLOGIES, APPLICATIONS, P160; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Finayson GD, 2001, IEEE T PATTERN ANAL, V23, P1209, DOI 10.1109/34.969113; Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37; Finlayson GD, 2013, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2013.239; FORSYTH DA, 1990, INT J COMPUT VISION, V5, P5, DOI 10.1007/BF00056770; Foster DH, 2011, VISION RES, V51, P674, DOI 10.1016/j.visres.2010.09.006; Funt B, 2012, COLOR IMAG CONF, P105; Funt B, 2012, J IMAGING SCI TECHN, V56, DOI 10.2352/J.ImagingSci.Technol.2012.56.2.020501; Gao SB, 2013, IEEE I CONF COMP VIS, P929, DOI 10.1109/ICCV.2013.119; Gehler PV, 2008, PROC CVPR IEEE, P3291; Geusebroek JM, 2005, INT J COMPUT VISION, V62, P7, DOI 10.1007/s11263-005-4632-7; Gijsenij A, 2012, IEEE T PATTERN ANAL, V34, P918, DOI 10.1109/TPAMI.2011.197; Gijsenij A, 2012, IEEE T IMAGE PROCESS, V21, P697, DOI 10.1109/TIP.2011.2165219; Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224; Gijsenij A, 2011, IEEE T PATTERN ANAL, V33, P687, DOI 10.1109/TPAMI.2010.93; Gijsenij A, 2010, INT J COMPUT VISION, V86, P127, DOI 10.1007/s11263-008-0171-3; Gijsenij A, 2009, J OPT SOC AM A, V26, P2243, DOI 10.1364/JOSAA.26.002243; Jenatton R, 2011, J MACH LEARN RES, V12, P2777; Joze HRV, 2014, IEEE T PATTERN ANAL, V36, P860, DOI 10.1109/TPAMI.2013.169; LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108; Li B, 2013, PROC CVPR IEEE, P1423, DOI 10.1109/CVPR.2013.187; Li B, 2014, IEEE T IMAGE PROCESS, V23, P1194, DOI 10.1109/TIP.2013.2277943; Li B, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857898; Liu J.-S., 2010, EURASIP J WIREL COMM, V2010, P1, DOI DOI 10.1002/MEET.14504701173/FULL; Logvinenko AD, 2014, IEEE T IMAGE PROCESS, V23, P34, DOI 10.1109/TIP.2013.2283148; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu R, 2009, IEEE I CONF COMP VIS, P1749, DOI 10.1109/ICCV.2009.5459391; Luo ZQ, 2007, SIAM J OPTIMIZ, V18, P1, DOI 10.1137/050642691; Nedovic V, 2010, IEEE T PATTERN ANAL, V32, P1673, DOI 10.1109/TPAMI.2009.174; Nesterov Y., 2007, CORE DISCUSSION PAPE; Parraga C, 2009, PERCEPTION, V38, P180; Ruderman DL, 1998, J OPT SOC AM A, V15, P2036, DOI 10.1364/JOSAA.15.002036; Shenghua Gao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2809, DOI 10.1109/CVPR.2011.5995454; Shi L., 2012, COMMUNICATION; Shi L., 2011, REPROCESSED VERSION; Tan RT, 2004, J OPT SOC AM A, V21, P321, DOI 10.1364/JOSAA.21.000321; Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808; van de Weijer J, 2007, IEEE I CONF COMP VIS, P2197; Vazquez-Corral J, 2012, IEEE T IMAGE PROCESS, V21, P1997, DOI 10.1109/TIP.2011.2171353; Vazquez-Corral J, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.031105; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Xiong WH, 2006, J IMAGING SCI TECHN, V50, P341, DOI 10.2352/J.ImagingSci.Technol.(2006)50:4(341); Yang AY, 2013, IEEE T IMAGE PROCESS, V22, P3234, DOI 10.1109/TIP.2013.2262292; Yosida K, 1964, FUNCTIONAL ANAL; Yuan XT, 2010, PROC CVPR IEEE, P3493, DOI 10.1109/CVPR.2010.5539967	59	18	24	0	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2016	117	1					21	47		10.1007/s11263-015-0844-7	http://dx.doi.org/10.1007/s11263-015-0844-7			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DF2FG					2022-12-18	WOS:000371156500002
J	Raviv, D; Kimmel, R				Raviv, Dan; Kimmel, Ron			Affine Invariant Geometry for Non-rigid Shapes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							RIEMANNIAN-MANIFOLDS; RECOGNITION; SIGNATURES; FRAMEWORK; CURVES	Shape recognition deals with the study geometric structures. Modern surface processing methods can cope with non-rigidity-by measuring the lack of isometry, deal with similarity or scaling-by multiplying the Euclidean arc-length by the Gaussian curvature, and manage equi-affine transformations-by resorting to the special affine arc-length definition in classical equi-affine differential geometry. Here, we propose a computational framework that is invariant to the full affine group of transformations (similarity and equi-affine). Thus, by construction, it can handle non-rigid shapes. Technically, we add the similarity invariant property to an equi-affine invariant one and establish an affine invariant pseudo-metric. As an example, we show how diffusion geometry can encapsulate the proposed measure to provide robust signatures and other analysis tools for affine invariant surface matching and comparison.	[Raviv, Dan] MIT, Media Lab, Cambridge, MA 02139 USA; [Kimmel, Ron] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Massachusetts Institute of Technology (MIT); Technion Israel Institute of Technology	Raviv, D (corresponding author), MIT, Media Lab, Cambridge, MA 02139 USA.	darav@mit.edu; ron@cs.technion.ac.il	Raviv, Dan/AAZ-2851-2020		Office of Naval Research (ONR) [N00014-12-1-0517]; Israel Science Foundation (ISF) [1031/12]	Office of Naval Research (ONR)(Office of Naval Research); Israel Science Foundation (ISF)(Israel Science Foundation)	We thank the editor and the reviewers for their valuable comments that helped us improve the presentation and writeup of the paper. This research was supported by the Office of Naval Research (ONR) award number N00014-12-1-0517 and by Israel Science Foundation (ISF) Grant Number 1031/12.	Aflalo Y, 2013, SIAM J IMAGING SCI, V6, P1579, DOI 10.1137/120888107; ALVAREZ L, 1993, ARCH RATION MECH AN, V123, P199, DOI 10.1007/BF00375127; Andrade M, 2012, COMPUT AIDED GEOM D, V29, P162, DOI 10.1016/j.cagd.2011.11.002; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Ben Hamza A, 2006, IEEE T IMAGE PROCESS, V15, P2249, DOI 10.1109/TIP.2006.875250; BERARD P, 1994, GEOM FUNCT ANAL, V4, P373, DOI 10.1007/BF01896401; Blaschke W., 1923, VORLESUNGEN DIFFEREN; Bronstein A. M., 2010, P WORKSH 3D OBJ RETR; Bronstein AM, 2006, SIAM J SCI COMPUT, V28, P1812, DOI 10.1137/050639296; Bronstein AM, 2010, INT J COMPUT VISION, V89, P266, DOI 10.1007/s11263-009-0301-6; Bronstein M. M., 2010, P COMP VIS PATT REC; Brook A, 2005, LECT NOTES COMPUT SC, V3459, P456; BRUCKSTEIN AM, 1993, CVGIP-IMAG UNDERSTAN, V58, P49, DOI 10.1006/ciun.1993.1031; Bruckstein AM, 1998, PATTERN RECOGN, V31, P181, DOI 10.1016/S0031-3203(97)00018-6; BRUCKSTEIN AM, 1995, ANN MATH ARTIF INTEL, V13, P227, DOI 10.1007/BF01530829; Bruckstein AM, 1997, IMAGE VISION COMPUT, V15, P335, DOI 10.1016/S0262-8856(96)01140-7; BRUCKSTEIN AM, 1992, INT J COMPUT VISION, V7, P271, DOI 10.1007/BF00126396; Calabi E, 1998, INT J COMPUT VISION, V26, P107, DOI 10.1023/A:1007992709392; Carlsson S, 1996, INT J COMPUT VISION, V19, P211, DOI 10.1007/BF00055145; Chazal F, 2009, COMPUT GRAPH FORUM, V28, P1393, DOI 10.1111/j.1467-8659.2009.01516.x; COHIGNAC T, 1994, INT C PATT RECOG, P164, DOI 10.1109/ICPR.1994.576250; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102; Davies RH, 2002, IEEE T MED IMAGING, V21, P525, DOI 10.1109/TMI.2002.1009388; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; DZIUK G, 1988, LECT NOTES MATH, V1357, P142; Elad A, 2001, PROC CVPR IEEE, P168; Fletcher PT, 2003, LECT NOTES COMPUT SC, V2732, P450; Gray A., 2006, MODERN DIFFERENTIAL, V3rd edition; Huang H., 2005, P MED IM COMP COMP A; Kimmel R., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P45, DOI 10.1109/ICPR.1996.545989; Kovnatsky A., 2012, P EUR WORKSH 3D OBJ; Ling HB, 2005, PROC CVPR IEEE, P719; Lipman Y., 2009, P ACM T GRAPH SIGGRA, V28; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Memoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y; Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI [DOI 10.1007/978-3-662-05105-4_2, 10.1007/978-3-662-05105-4_2]; MOONS T, 1995, INT J COMPUT VISION, V14, P25, DOI 10.1007/BF01421487; Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730; Olver P. J., 1999, FOUND COMPUT MATH, V1, P3; Olver PJ, 2005, LECT NOTES COMPUT SC, V3519, P105; Ovsjanikov M., 2009, P NONR SHAP AN DEF I; Ovsjanikov M, 2010, COMPUT GRAPH FORUM, V29, P1555, DOI 10.1111/j.1467-8659.2010.01764.x; PAUWELS EJ, 1995, INT J COMPUT VISION, V14, P49, DOI 10.1007/BF01421488; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Polthier K., 1998, MATH VISUALIZATION A, P135; Qiu HJ, 2007, IEEE T PATTERN ANAL, V29, P1873, DOI 10.1109/TPAMI.2007.1103; Raviv D., 2013, J MATH IMAGING VISIO; Raviv D, 2011, PROC CVPR IEEE; Raviv D, 2011, COMPUT GRAPH-UK, V35, P692, DOI 10.1016/j.cag.2011.03.030; Reuter M, 2010, NEUROIMAGE, V53, P1181, DOI 10.1016/j.neuroimage.2010.07.020; Rugis J, 2006, LECT NOTES COMPUT SC, V4319, P138; Rustamov Raif M, 2007, P 5 EUR S GEOM PROC, P225, DOI DOI 10.2312/SGP/SGP07/225-233; Sapiro G., 1993, THESIS TECHNION IIT; Su B., 1983, AFFINE DIFFERENTIAL; Sun J., 2009, P S GEOM PROC SGP; VANGOOL LJ, 1992, ARTIF INT, P293; Wang Y, 2008, INT J COMPUT VISION, V76, P283, DOI 10.1007/s11263-007-0063-y; Weiss I., 1988, CARTR339 U MAR	59	18	20	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2015	111	1					1	11		10.1007/s11263-014-0728-2	http://dx.doi.org/10.1007/s11263-014-0728-2			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AZ6RC		Green Submitted			2022-12-18	WOS:000348345400001
J	Ho, HT; Gopalan, R				Huy Tho Ho; Gopalan, Raghuraman			Model-Driven Domain Adaptation on Product Manifolds for Unconstrained Face Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Domain adaptation; Unconstrained face recognition; Manifold learning; Tensor computation	TEXTURE CLASSIFICATION; ROBUST; ILLUMINATION; BLUR; KERNEL	Many classification algorithms see a reduction in performance when tested on data with properties different from that used for training. This problem arises very naturally in face recognition where images corresponding to the source domain (gallery, training data) and the target domain (probe, testing data) are acquired under varying degree of factors such as illumination, expression, blur and alignment. In this paper, we account for the domain shift by deriving a latent subspace or domain, which jointly characterizes the multifactor variations using appropriate image formation models for each factor. We formulate the latent domain as a product of Grassmann manifolds based on the underlying geometry of the tensor space, and perform recognition across domain shift using statistics consistent with the tensor geometry. More specifically, given a face image from the source or target domain, we first synthesize multiple images of that subject under different illuminations, blur conditions and 2D perturbations to form a tensor representation of the face. The orthogonal matrices obtained from the decomposition of this tensor, where each matrix corresponds to a factor variation, are used to characterize the subject as a point on a product of Grassmann manifolds. For cases with only one image per subject in the source domain, the identity of target domain faces is estimated using the geodesic distance on product manifolds. When multiple images per subject are available, an extension of kernel discriminant analysis is developed using a novel kernel based on the projection metric on product spaces. Furthermore, a probabilistic approach to the problem of classifying image sets on product manifolds is introduced. We demonstrate the effectiveness of our approach through comprehensive evaluations on constrained and unconstrained face datasets, including still images and videos.	[Huy Tho Ho] Univ Maryland, UMIACS, Dept Elect & Comp Engn, College Pk, MD 20742 USA; [Huy Tho Ho] Univ Maryland, UMIACS, Ctr Automat Res, College Pk, MD 20742 USA; [Gopalan, Raghuraman] AT&T Labs Res, Dept Video & Multimedia Technol Res, Middletown, NJ 07748 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park; AT&T	Ho, HT (corresponding author), Univ Maryland, UMIACS, Dept Elect & Comp Engn, College Pk, MD 20742 USA.	huytho@umd.edu; raghuram@research.att.com						[Anonymous], 2007, P 15 ACM INT C MULTI; Arandjelovic O, 2005, PROC CVPR IEEE, P581; Arandjelovic Ognjen, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P203; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Begelfor E., 2006, 2006 IEEE COMPUTER S, V2, P2087, DOI DOI 10.1109/CVPR.2006.50; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Biswas S, 2009, IEEE T PATTERN ANAL, V31, P884, DOI 10.1109/TPAMI.2007.12.0830; BJORCK A, 1973, MATH COMPUT, V27, P579, DOI 10.2307/2005662; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; BROOKS MJ, 1985, P INT JOINT C ART IN, P932; Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965; Chen YC, 2012, LECT NOTES COMPUT SC, V7577, P766, DOI 10.1007/978-3-642-33783-3_55; Duan L., 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gopalan R, 2012, IEEE T PATTERN ANAL, V34, P1220, DOI 10.1109/TPAMI.2012.15; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Hamm J., 2008, P INT C MACH LEARN I, P376, DOI DOI 10.1145/1390156.1390204; Hoffman J, 2012, LECT NOTES COMPUT SC, V7573, P702, DOI 10.1007/978-3-642-33709-3_50; Hu Y., 2011, IEEE C COMP VIS PATT, P27; Huang G., 2007, P ICCV; Huang G.B., 2008, WORKSHOP FACESREAL L; Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924; Jia H., 2008, AUT FAC GEST REC 200, P1; Jia HJ, 2009, PROC CVPR IEEE, P136, DOI 10.1109/CVPRW.2009.5206862; Joliffe I. T., 1986, PRINCIPAL COMPONENT; Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Lee J. M., 2010, INTRO TOPOLOGICAL MA; Lee KC, 2005, COMPUT VIS IMAGE UND, V99, P303, DOI 10.1016/j.cviu.2005.02.002; Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92; Li H., 2013, P CVPR; Li Y, 2005, IEEE I CONF COMP VIS, P114; Liu J, 2007, LECT NOTES COMPUT SC, V4778, P205; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu J., 2011, P ICCV, P1943; Lui YM, 2008, LECT NOTES COMPUT SC, V5303, P44, DOI 10.1007/978-3-540-88688-4_4; Lui YM, 2012, IMAGE VISION COMPUT, V30, P380, DOI 10.1016/j.imavis.2011.08.002; Lui YM, 2010, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2010.5540131; Martinez A., 1998, AR FACE DATABASE; Martinez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382; Ni J., 2013, P CVPR; Nishiyama M, 2011, IEEE T PATTERN ANAL, V33, P838, DOI 10.1109/TPAMI.2010.203; Nowak E., 2007, P CVPR; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27; Park SW, 2010, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2010.5539980; Pinto N., 2009, P CVPR; Qiu Q, 2012, LECT NOTES COMPUT SC, V7575, P631, DOI 10.1007/978-3-642-33765-9_45; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Sanderson C, 2009, LECT NOTES COMPUT SC, V5558, P199, DOI 10.1007/978-3-642-01793-3_21; Shakhnarovich G, 2002, LECT NOTES COMPUT SC, V2352, P851, DOI 10.1007/3-540-47977-5_56; Shekhar S., 2013, P CVPR; Shi Y., 2012, P ICML; Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154; Sung Won Park, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2817, DOI 10.1109/CVPR.2011.5995397; Sung Won Park, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P464, DOI 10.1109/FG.2011.5771443; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Vageeswaran P, 2013, IEEE T IMAGE PROCESS, V22, P1362, DOI 10.1109/TIP.2012.2228498; Vapnik V.N, 1998, STAT LEARNING THEORY; Vasilescu M., 2007, P ICCV, P1; Vasilescu M.A.O., 2002, P EUR C COMP VIS, P447; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wang RP, 2009, PROC CVPR IEEE, P429, DOI 10.1109/CVPRW.2009.5206850; Wolf L., 2008, FAC REAL LIF IM WORK; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zheng JJ, 2012, INT C PATT RECOG, P2095	70	18	19	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2014	109	1-2			SI		110	125		10.1007/s11263-014-0720-x	http://dx.doi.org/10.1007/s11263-014-0720-x			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AI7QY					2022-12-18	WOS:000337091700008
J	Shekhovtsov, A; Hlavac, V				Shekhovtsov, Alexander; Hlavac, Vaclav			A Distributed Mincut/Maxflow Algorithm Combining Path Augmentation and Push-Relabel	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Maximum flow; Minimum cut; Distributed; Parallel; Push-relable; Augmenting path; Large scale	MARKOV RANDOM-FIELDS; GRAPH CUTS; IMPLEMENTATION	We propose a novel distributed algorithm for the minimum cut problem. Motivated by applications like volumetric segmentation in computer vision, we aim at solving large sparse problems. When the problem does not fully fit in the memory, we need to either process it by parts, looking at one part at a time, or distribute across several computers. Many mincut/maxflow algorithms are designed for the shared memory architecture and do not scale to this setting. We consider algorithms that work on disjoint regions of the problem and exchange messages between the regions. We show that the region push-relabel algorithm of Delong and Boykov (A scalable graph-cut algorithm for N-D grids, in CVPR, 2008) uses I similar to(n (2)) rounds of message exchange, where n is the number of vertices. Our new algorithm performs path augmentations inside the regions and push-relabel style updates between the regions. It uses asymptotically less message exchanges, , where is the set of boundary vertices. The sequential and parallel versions of our algorithm are competitive with the state-of-the-art in the shared memory model. By achieving a lower amount of message exchanges (even asymptotically lower in our synthetic experiments), they suit better for solving large problems using a disk storage or a distributed system.	[Shekhovtsov, Alexander; Hlavac, Vaclav] Czech Tech Univ, Ctr Machine Percept, Prague 16627 6, Czech Republic	Czech Technical University Prague	Shekhovtsov, A (corresponding author), Czech Tech Univ, Ctr Machine Percept, Prague 16627 6, Czech Republic.	shekhole@fel.cvut.cz; hlavac@fel.cvut.cz	Shekhovtsov, Alexander/A-7436-2013; Hlavac, Vaclav/ABI-7009-2020; Hlavac, Vaclav/D-9415-2014	Hlavac, Vaclav/0000-0002-8472-3147; Hlavac, Vaclav/0000-0002-8472-3147; Shekhovtsov, Alexander/0000-0003-0678-8954	EU [FP7-ICT-247870 NIFTi, FP7-ICT-247525 HUMAVIPS, GACR P103/10/0783]	EU(European Commission)	This work was supported by the EU project FP7-ICT-247870 NIFTi, FP7-ICT-247525 HUMAVIPS and GACR P103/10/0783.	ANDERSON R, 1995, J PARALLEL DISTR COM, V29, P17, DOI 10.1006/jpdc.1995.1103; Boros E., 1991, 171991 RRR RUTCOR; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Boykov Y., 1999, ICCV; BOYKOV Y, 1998, CVPR; Boykov Y., 2003, ICCV; BOYKOV Y, 2006, BMVC; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; Cherkassky B. V., 1994, IMPLEMENTING PUSHREL; Delong A., 2008, CVPR; GOLDBERG A, 1998, J ACM; Goldberg A.V., 1987, THESIS MASSACHUSETTS; Goldberg A. V., 2008, P 16 ANN EUR S ALG; GOLDBERG AV, 1991, INFORM PROCESS LETT, V38, P179, DOI 10.1016/0020-0190(91)90097-2; GOLDBERG AV, 1988, J ACM, V35; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Jancosek M., 2011, CVPR; Jolly M., 2001, ICCV; Kohli P, 2005, IEEE I CONF COMP VIS, P922; Kolmogorov V., 2001, ICCV; Kolmogorov V., 2004, THESIS ITHACA; Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031; Kovtun I., 2004, THESIS; Labatut P, 2009, COMPUT GRAPH FORUM, V28, P2275, DOI 10.1111/j.1467-8659.2009.01530.x; Lempitsky V., 2007, CVPR; Lempitsky V., 2006, ECCV; Lempitsky V, 2010, IEEE T PATTERN ANAL, V32, P1392, DOI 10.1109/TPAMI.2009.143; Liu J., 2010, CVPR, P2; Schlesinger D., 2006, RES REPORT; Shekhovtsov A., 2011, LECT NOTES COMPUTER, P14; STRANDMARK P, 2010, CVPR; University of Western Ontario, 2008, MAX FLOW PROBL INST; [No title captured]	33	18	18	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2013	104	3			SI		315	342		10.1007/s11263-012-0571-2	http://dx.doi.org/10.1007/s11263-012-0571-2			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	190VT		Green Submitted			2022-12-18	WOS:000322371100006
J	Reilly, V; Solmaz, B; Shah, M				Reilly, Vladimir; Solmaz, Berkan; Shah, Mubarak			Shadow Casting Out Of Plane (SCOOP) Candidates for Human and Vehicle Detection in Aerial Imagery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human detection; Vehicle detection; Aerial surveillance; UAV; Shadow; Metadata		In this paper, we propose a method for detecting humans and vehicles in imagery taken from a UAV. This is a challenging problem due to a limited number of pixels on target, which makes it more difficult to distinguish objects from background clutter, and results in much larger search space. We propose a method for constraining the search based on a number of geometric constraints obtained from the metadata. Specifically, we obtain the orientation of ground plane normal, the orientation of shadows cast by out of plane objects in the scene, and the relationship between object heights and the size of their corresponding shadows. We use the aforementioned information in a geometry-based shadow, and ground-plane normal blob detector, which provides an initial estimation for locations of shadow casting out of plane (SCOOP) objects in the scene. These SCOOP candidate locations are then classified as either human or clutter using a combination of wavelet features and a Support Vector Machine. To detect vehicles, we similarly find potential vehicle candidates by combining SCOOP and inverted-SCOOP candidates and then classify them using wavelet features and SVM. Our method works on a single frame, and unlike motion detection based methods, it bypasses the entire pipeline of registration, motion detection, and tracking. This method allows for detection of stationary and slowly moving humans and vehicles while avoiding the search across the entire image, allowing accurate and fast localization. We show impressive results on sequences from VIVID and CLIF datasets and provide comparative analysis.	[Reilly, Vladimir; Solmaz, Berkan; Shah, Mubarak] Univ Cent Florida, Orlando, FL 32816 USA	State University System of Florida; University of Central Florida	Reilly, V (corresponding author), Univ Cent Florida, Orlando, FL 32816 USA.	vsreilly@eecs.ucf.edu; bsolmaz@eecs.ucf.edu; shah@eecs.ucf.edu		Shah, Mubarak/0000-0001-6172-5572	Harris corporation; Defense Advanced Research Projects Agency (DARPA) [HR0011-10-C-0112]	Harris corporation; Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This research was partially supported by the Harris corporation and Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-10-C-0112. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the Harris corporation or DARPA.	Bi S., 2007, ICAL; Bissacco A., 2007, NIPS; BOSE B, 2004, CVPR; Breckon T., 2009, UAVS; Breckon T., 2010, UAVS; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chang J. C., 2002, ICPR; Chen Y. T., 2009, ICCV; CHENG H, 2006, CVPR; Dalai N., 2005, CVPR, V1; Doherty P., 2008, IEEE AER; Felzenszwalb P, 2008, PROC CVPR IEEE, P1984; Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18; Gaszczak A., 2011, REAL TIME PEOPLE VEH; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hsieh J. W., 2004, ICPR; Hu H., 2010, ICACIA; Jay Kuo C. C., 2003, AVSS; Kembhavi A., 2011, IEEE T PATTERN ANAL; Kluckner S., 2009, ACCV; Leibe B., 2005, CVPR; Liu Z., 2010, WCICA; Martel-Brisson N., 2005, CVPR; Mikolajczyk K., 2004, ECCV; Miller A., 2007, CLEAR; Panagopoulos A., 2009, CVPR; Porikli F., 2005, ICCV; Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520; Quaritsch M., 2010, ELEKTROTECHNIK INFOR, V127; Reda I, 2003, NRELTP56034302; Sabzmeydani P., 2007, CVPR; Tian T. P., 2010, ECCV; Tuzel O., 2008, PATTERN ANAL MACHINE, P30; Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207; Wu B., 2005, ICCV; Wu Q., 2010, SWUATC; Xiao J., 2008, MULTIMODAL TECHNOLOG; XIAO J, 2008, CVPR; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; XU L, 2006, INTELLIGENT SYSTEMS, V2; Yahyanejad S., 2010, AVSS; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355	42	18	20	0	30	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	2					350	366		10.1007/s11263-012-0580-1	http://dx.doi.org/10.1007/s11263-012-0580-1			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	081BW					2022-12-18	WOS:000314291600007
J	Wolf, L; Littman, R; Mayer, N; German, T; Dershowitz, N; Shweka, R; Choueka, Y				Wolf, Lior; Littman, Rotem; Mayer, Naama; German, Tanya; Dershowitz, Nachum; Shweka, Roni; Choueka, Yaacov			Identifying Join Candidates in the Cairo Genizah	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cairo Genizah; Document analysis; Similarity learning		A join is a set of manuscript-fragments that are known to originate from the same original work. The Cairo Genizah is a collection containing approximately 350,000 fragments of mainly Jewish texts discovered in the late 19th century. The fragments are today spread out in libraries and private collections worldwide, and there is an ongoing effort to document and catalogue all extant fragments. The task of finding joins is currently conducted manually by experts, and presumably only a small fraction of the existing joins have been discovered. In this work, we study the problem of automatically finding candidate joins, so as to streamline the task. The proposed method is based on a combination of local descriptors and learning techniques. To evaluate the performance of various join-finding methods, without relying on the availability of human experts, we construct a benchmark dataset that is modeled on the Labeled Faces in the Wild benchmark for face recognition. Using this benchmark, we evaluate several alternative image representations and learning techniques. Finally, a set of newly-discovered join-candidates have been identified using our method and validated by a human expert.	[Wolf, Lior; Littman, Rotem; Mayer, Naama; German, Tanya; Dershowitz, Nachum] Tel Aviv Univ, Blavatnik Sch Comp Sci, IL-69978 Tel Aviv, Israel; [Shweka, Roni; Choueka, Yaacov] Friedberg Genizah Project, Jerusalem, Israel	Tel Aviv University	Wolf, L (corresponding author), Tel Aviv Univ, Blavatnik Sch Comp Sci, IL-69978 Tel Aviv, Israel.	wolf@cs.tau.ac.il			Friedberg Genizah Project	Friedberg Genizah Project	We would like to thank Dr.Dov Friedberg for a generous grant that enabled us to start this research, and the Friedberg Genizah Project for matching funds that enabled its continuation.	BAIRD HS, 1992, P IEEE, V80, P1059, DOI 10.1109/5.156469; Bensefia A, 2003, PROC INT CONF DOC, P946; Bilenko M, 2004, ICML; BRES S, 2006, 10 INT WORKSH FRONT; Bulacu M, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P279, DOI 10.1109/ICIAP.2007.4362792; Casey RG, 1996, IEEE T PATTERN ANAL, V18, P690, DOI 10.1109/34.506792; DANCE C, 2004, ECCV WORKSH STAT LEA; DINSTEIN I, 1982, IEEE T SYST MAN CYB, V12, P405; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; GUILLAUMIN M, 2009, INT C COMP VIS SEP; HERTZ T, 2004, INT C MACH LEARN ICM; HUANG G, 2008, ECCV FACES REAL LIFE; HUANG GB, 2007, 0749 TR UMASS; Ke Y, 2004, PROC CVPR IEEE, P506; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; LAZEBNIK S, 2006, 2006 IEEE COMP SOC C, V2, P2169, DOI DOI 10.1109/CVPR.2006.68; Leedham G, 2002, EIGHTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION: PROCEEDINGS, P244, DOI 10.1109/IWFHR.2002.1030917; Lerner HG, 2006, CAT CLASSIF Q, V42, P21, DOI 10.1300/J104v42n01_04; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Panagopoulos M, 2009, IEEE T PATTERN ANAL, V31, P1404, DOI 10.1109/TPAMI.2008.201; Pinto Nicolas, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2591, DOI 10.1109/CVPRW.2009.5206605; Reif Stefan C., 2000, JEWISH ARCH OLD CAIR; Serre T, 2005, PROC CVPR IEEE, P994; SHENTAL N, 2006, COMPUTER VISION ECCV, P181; SIMAKOV D, 2008, IEEE C COMP VIS PATT, P1; Srihari S. N., 1989, Machine Vision and Applications, V2, P141, DOI 10.1007/BF01212455; Taigman Y., 2009, BRIT MACH VIS C; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; WOLF L, 2006, 2006 IEEE COMP SOC C, V2, P2153; Wolf L., 2008, FAC REAL LIF IM WORK; WOLF L, 2009, POST ICCV WORKSH EH; WOLF L, 2009, AS COMP VIS C ACCV S; Wolf L., 2009, IEEE INT C COMP VIS; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; Xing E. P., 2003, NIPS; LFW BENCHMARK RESULT	38	18	18	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2011	94	1					118	135		10.1007/s11263-010-0389-8	http://dx.doi.org/10.1007/s11263-010-0389-8			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	760JM		Green Submitted			2022-12-18	WOS:000290320600009
J	Guan, L; Franco, JS; Pollefeys, M				Guan, Li; Franco, Jean-Sebastien; Pollefeys, Marc			Multi-view Occlusion Reasoning for Probabilistic Silhouette-Based Dynamic Scene Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-view 3D reconstruction; Bayesian inference; Graphical model; Shape-from-silhouette; Occlusion		In this paper, we present an algorithm to probabilistically estimate object shapes in a 3D dynamic scene using their silhouette information derived from multiple geometrically calibrated video camcorders. The scene is represented by a 3D volume. Every object in the scene is associated with a distinctive label to represent its existence at every voxel location. The label links together automatically-learned view-specific appearance models of the respective object, so as to avoid the photometric calibration of the cameras. Generative probabilistic sensor models can be derived by analyzing the dependencies between the sensor observations and object labels. Bayesian reasoning is then applied to achieve robust reconstruction against real-world environment challenges, such as lighting variations, changing background etc. Our main contribution is to explicitly model the visual occlusion process and show: (1) static objects (such as trees or lamp posts), as parts of the pre-learned background model, can be automatically recovered as a byproduct of the inference; (2) ambiguities due to inter-occlusion between multiple dynamic objects can be alleviated, and the final reconstruction quality is drastically improved. Several indoor and outdoor real-world datasets are evaluated to verify our framework.	[Guan, Li; Pollefeys, Marc] Univ N Carolina, Chapel Hill, NC 27515 USA; [Franco, Jean-Sebastien] Univ Bordeaux, LaBRI, INRIA Sud Ouest, Talence, France; [Pollefeys, Marc] Swiss Fed Inst Technol, Zurich, Switzerland	University of North Carolina; University of North Carolina Chapel Hill; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite de Bordeaux; Swiss Federal Institutes of Technology Domain; ETH Zurich	Guan, L (corresponding author), Univ N Carolina, Chapel Hill, NC 27515 USA.	lguan@cs.unc.edu; jean-sebastien.franco@labri.fr; marc@cs.unc.edu	Pollefeys, Marc/I-7607-2013		David and Lucille Packard Foundation; NSF [IIS-0237533]	David and Lucille Packard Foundation(The David & Lucile Packard Foundation); NSF(National Science Foundation (NSF))	We would like to thank A. Gupta et al. (2007), Mittal and Davis (2003) for providing us the 16-camera dataset. We also gratefully acknowledge the support of David and Lucille Packard Foundation Fellowship and NSF Career award IIS-0237533.	Apostoloff N, 2005, PROC CVPR IEEE, P553; Baumgart B. G., 1974, Geometric modeling for computer vision; Broadhurst A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P388, DOI 10.1109/ICCV.2001.937544; Brostow G. J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P8, DOI 10.1109/ICCV.1999.791190; DEBONET JS, 1999, ICCV, V1, P418; Diebel J., CVPR, V1, P519; ELFES A, 1989, COMPUTER, V22, P46, DOI 10.1109/2.30720; FAVARO P, 2003, ICCV, V1, P479; Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174; Franco J.-S., 2003, BRIT MACH VIS C BMVC, V1, P329, DOI [DOI 10.5244/C.17.32, 10.5244/C.17.32]; FRANCO JS, 2005, ICCV, V2, P1747; FURUKAWA Y, 2006, EUR C COMP VIS, V1, P564; Grauman K, 2003, PROC CVPR IEEE, P187; Guan L., 2007, IEEE C COMP VIS PATT, P1; GUAN L, 2008, CVPR, P1; GUAN L, 2006, 3DPVT, V1, P413; Gupta A, 2007, IEEE I CONF COMP VIS, P118; Ilie A, 2005, IEEE I CONF COMP VIS, P1268, DOI 10.1109/ICCV.2005.88; Joshi N., 2005, CS20050821 UCSD CSE; KECK M, 2008, CVPR, P1; KIM K, 2005, ISVC, V1, P337; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; LAZEBNIK S, 2001, COMPUTER VISION PATT, V1, P156; MARGARITIS D, 1998, ICML, V1, P332; MATUSIK W, 2000, SIGGRAPH, V1, P369; MATUSIK W, 2001, P EUR WORKSH REND, V1, P115; Mittal A, 2003, INT J COMPUT VISION, V51, P189, DOI 10.1023/A:1021849801764; Otsuka K, 2004, PROC CVPR IEEE, P90; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Sinha SN, 2005, IEEE I CONF COMP VIS, P349; Slabaugh GG, 2004, INT J COMPUT VISION, V57, P179, DOI 10.1023/B:VISI.0000013093.45070.3b; Snow D, 2000, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2000.855839; Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637; Stein A, 2007, IEEE I CONF COMP VIS, P110; Takamatsu J., 2008, PROC IEEE C COMPUT V, P1; Yang DB, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P122; ZIEGLER R, 2003, EUR ACM SIGGRAPH S G, V1, P248	39	18	21	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2010	90	3					283	303		10.1007/s11263-010-0341-y	http://dx.doi.org/10.1007/s11263-010-0341-y			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	662BV		Green Submitted			2022-12-18	WOS:000282782700002
J	Akgul, CB; Sankur, B; Yemez, Y; Schmitt, F				Akgul, Ceyhun Burak; Sankur, Buelent; Yemez, Yuecel; Schmitt, Francis			Similarity Learning for 3D Object Retrieval Using Relevance Feedback and Risk Minimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Relevance feedback; Similarity models and learning; Empirical ranking risk; Support vector machines; 3D object retrieval	IMAGE RETRIEVAL	We introduce a similarity learning scheme to improve the 3D object retrieval performance in a relevance feedback setting. The proposed algorithm relies on a score fusion approach that linearly combines elementary similarity scores originating from different shape descriptors into a final similarity function. Each elementary score is modeled in terms of the posterior probability of a database item being relevant to the user-provided query. The posterior parameters are learned via off-line discriminative training, while the optimal combination of weights to generate the final similarity function is obtained by on-line empirical ranking risk minimization. This joint use of on-line and off-line learning methods in relevance feedback not only improves the retrieval performance significantly as compared to the totally unsupervised case, but also outperforms the standard support vector machines based approach. Experiments on several 3D databases, including the Princeton Shape Benchmark, show also that the proposed algorithm has a better small sample behavior.	[Akgul, Ceyhun Burak] Philips Res, Video Proc & Anal Grp, Eindhoven, Netherlands; [Sankur, Buelent] Bogazici Univ, Elect Elect Engn Dept, Istanbul, Turkey; [Yemez, Yuecel] Koc Univ, Dept Comp Engn, Istanbul, Turkey; [Schmitt, Francis] Telecom ParisTech, Dept Signal Images, Paris, France	Philips; Philips Research; Bogazici University; Koc University; IMT - Institut Mines-Telecom; Institut Polytechnique de Paris	Akgul, CB (corresponding author), Philips Res, Video Proc & Anal Grp, Eindhoven, Netherlands.	cb.akgul@gmail.com	Sankur, Bulent/N-4663-2017					AKGUL CB, 2008, P EUR WORKSH 3D OBJ; AKGUL CB, 2007, THESIS TELECOM PARIS; Akgul CB, 2009, IEEE T PATTERN ANAL, V31, P1117, DOI 10.1109/TPAMI.2009.25; Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; CHEN Y, 2001, P IEEE INT C IM PROC, P815; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Datta AK, 2008, OPT LASER TECHNOL, V40, P1, DOI 10.1016/j.optlastec.2007.04.006; Giacinto G, 2004, PATTERN RECOGN, V37, P1499, DOI 10.1016/j.patcog.2004.01.005; GIORGI D, 2007, SHREC2007 3D SHAPE R, P5; Goodall S, 2004, LECT NOTES COMPUT SC, V3115, P638; Guo GD, 2002, IEEE T NEURAL NETWOR, V13, P811, DOI 10.1109/TNN.2002.1021882; Hastie T, 2009, ELEMENTS STAT LEARNI; Huang J, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P325, DOI 10.1145/266180.266383; Ishikawa Y., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P218; Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007; LAAKSONEN J, 1999, P INNS IEEE INT JOIN, P1199; Lei Z, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P721, DOI 10.1109/ICIP.2001.958595; Leifman G, 2005, VISUAL COMPUT, V21, P865, DOI 10.1007/s00371-005-0341-z; Lin HT, 2007, MACH LEARN, V68, P267, DOI 10.1007/s10994-007-5018-6; Nastar C, 1998, PROC CVPR IEEE, P547, DOI 10.1109/CVPR.1998.698659; NOVOTNI M, 2005, P INT WORKSH CONT BA; Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026; Peng J, 1999, COMPUT VIS IMAGE UND, V75, P150, DOI 10.1006/cviu.1999.0770; PICARD RW, 1999, P IEEE INT C IM PROC, P777; Platt JC, 2000, ADV NEUR IN, P61; Porkaew K, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P747, DOI 10.1109/MMCS.1999.778578; ROCCHIO JJ, 1966, THESIS HARVARD U CAM; Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510; Rui Y, 2000, PROC CVPR IEEE, P236, DOI 10.1109/CVPR.2000.855825; Santini S, 2000, IEEE MULTIMEDIA, V7, P26, DOI 10.1109/93.879766; Schettini R., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P75, DOI 10.1109/ICIP.1999.817072; Scholkopf B., 2002, LEARNING KERNELS; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0; Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134; Tieu K, 2000, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2000.855824; Tong S., 2001, PROC ACM INT C MULTI, V9, P107; Vasconcelos N, 2000, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2000.855822; VASCONCELOS N, 1999, P NEUR INF PROC SYST, V12; Vranic D., 2004, THESIS U LEIPZIG; Wu Y, 2000, PROC CVPR IEEE, P222, DOI 10.1109/CVPR.2000.855823; Zhou XS, 2001, PROC CVPR IEEE, P11; Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3	45	18	18	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2010	89	2-3			SI		392	407		10.1007/s11263-009-0294-1	http://dx.doi.org/10.1007/s11263-009-0294-1			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	594OS					2022-12-18	WOS:000277547600016
J	Jian, YD; Chen, CS				Jian, Yong-Dian; Chen, Chu-Song			Two-View Motion Segmentation with Model Selection and Outlier Removal by RANSAC-Enhanced Dirichlet Process Mixture Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion segmentation; Mixture of Dirichlet process	STRUCTURE-AND-MOTION; FACTORIZATION METHOD; MULTIBODY; SHAPE	We propose a novel motion segmentation algorithm based on mixture of Dirichlet process (MDP) models. In contrast to previous approaches, we consider motion segmentation and its model selection regarding to the number of motion models as an inseparable problem. Our algorithm can simultaneously infer the number of motion models, estimate the cluster memberships of correspondences, and identify the outliers. The main idea is to use MDP models to fully exploit the geometric consistencies before making premature decisions about the number of motion models. To handle outliers, we incorporate RANSAC into the inference process of MDP models. In the experiments, we compare the proposed algorithm with naive RANSAC, GPCA and Schindler's method on both synthetic data and real image data. The experimental results show that we can handle more motions and have satisfactory performance in the presence of various levels of noise and outlier.	[Chen, Chu-Song] Acad Sinica, Inst Informat Sci, Taipei 11529, Taiwan; [Jian, Yong-Dian] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	Academia Sinica - Taiwan; University System of Georgia; Georgia Institute of Technology	Chen, CS (corresponding author), Acad Sinica, Inst Informat Sci, Taipei 11529, Taiwan.	ydjian@gatech.edu; song@iis.sinica.edu.tw			 [98-EC-17-A-02-S1-032];  [NSC98-2221-E-001-012-MY3]	; 	This work was supported in part under Grants 98-EC-17-A-02-S1-032 and NSC98-2221-E-001-012-MY3.	ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871; BALLARD DH, 1983, COMPUT VISION GRAPH, V22, P95, DOI 10.1016/0734-189X(83)90097-X; BOBER M, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P947, DOI 10.1109/CVPR.1994.323931; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; GRUBER A, 2006, P EUR C COMP VIS; Hartley R., 2004, ROBOTICA; Horn B., 1986, ROBOT VISION, P1; ILLINGWORTH J, 1987, IEEE T PATTERN ANAL, V9, P690, DOI 10.1109/TPAMI.1987.4767964; JIAN YD, 2007, P INT C COMP VIS; Kanatani K, 2002, LECT NOTES COMPUT SC, V2352, P335; Kumar MP, 2005, IEEE I CONF COMP VIS, P33; LI HW, 1986, COMPUT VISION GRAPH, V36, P139, DOI 10.1016/0734-189X(86)90073-3; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MacEachern SN, 1998, J COMPUT GRAPH STAT, V7, P223, DOI 10.2307/1390815; MAKADIA A, 2005, P IEEE C COMP VIS PA; Morita T, 1997, IEEE T PATTERN ANAL, V19, P858, DOI 10.1109/34.608289; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; ORBANZ P, 2006, P EUR C COMP VIS; Schindler K, 2006, IEEE T PATTERN ANAL, V28, P983, DOI 10.1109/TPAMI.2006.130; Schindler K, 2008, INT J COMPUT VISION, V79, P159, DOI 10.1007/s11263-007-0111-7; SHASHUA A, 2006, P EUR C COMP VIS; Stewenius H, 2006, ISPRS J PHOTOGRAMM, V60, P284, DOI 10.1016/j.isprsjprs.2006.03.005; SUDDERTH E, 2006, P IEEE C COMP VIS PA, V2, P2410; Sugaya Y, 2004, IEICE T INF SYST, VE87D, P1935; Tian TY, 1997, IEEE T PATTERN ANAL, V19, P1178, DOI 10.1109/34.625131; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torr PHS, 1998, PHILOS T R SOC A, V356, P1321, DOI 10.1098/rsta.1998.0224; TRON R, 2007, P IEEE C COMP VIS PA; Tuzel O, 2005, IEEE I CONF COMP VIS, P18; VIDAL R, 2004, P IEEE C COMP VIS PA; Vidal R, 2006, INT J COMPUT VISION, V68, P7, DOI 10.1007/s11263-005-4839-7; Wills J, 2006, INT J COMPUT VISION, V68, P125, DOI 10.1007/s11263-006-6660-3; Wolf L, 2001, PROC CVPR IEEE, P263; Xiao JJ, 2005, IEEE T PATTERN ANAL, V27, P1644, DOI 10.1109/TPAMI.2005.202; Yan J., 2006, P EUR C COMP VIS; YANG A, 2006, P IEEE WORKSH 25 YEA; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	43	18	18	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2010	88	3					489	501		10.1007/s11263-010-0317-y	http://dx.doi.org/10.1007/s11263-010-0317-y			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	580EL		Green Submitted			2022-12-18	WOS:000276429900008
J	Ferencz, A; Learned-Miller, EG; Malik, J				Ferencz, Andras; Learned-Miller, Erik G.; Malik, Jitendra			Learning to locate informative features for visual identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object recognition; object identification; parametric models; interclass transfer; learning from new examples; one-shot learning	RECOGNITION	Object identification is a specialized type of recognition in which the category (e. g. cars) is known and the goal is to recognize an object's exact identity (e. g. Bob's BMW). Two special challenges characterize object identification. First, inter-object variation is often small (many cars look alike) and may be dwarfed by illumination or pose changes. Second, there may be many different instances of the category but few or just one positive "training" examples per object instance. Because variation among object instances may be small, a solution must locate possibly subtle object-specific salient features, like a door handle, while avoiding distracting ones such as specular highlights. With just one training example per object instance, however, standard modeling and feature selection techniques cannot be used. We describe an on-line algorithm that takes one image from a known category and builds an efficient "same" versus "different" classification cascade by predicting the most discriminative features for that object instance. Our method not only estimates the saliency and scoring function for each candidate feature, but also models the dependency between features, building an ordered sequence of discriminative features specific to the given image. Learned stopping thresholds make the identifier very efficient. To make this possible, category-specific characteristics are learned automatically in an off-line training procedure from labeled image pairs of the category. Our method, using the same algorithm for both cars and faces, outperforms a wide variety of other methods.	[Ferencz, Andras] Mobileye Vis Technol, Princeton, NJ USA; [Learned-Miller, Erik G.] UMass Amherst, Comp Sci, Amherst, MA USA; [Malik, Jitendra] Univ Calif Berkeley, Comp Sci, Berkeley, CA USA	University of Massachusetts System; University of Massachusetts Amherst; University of California System; University of California Berkeley	Ferencz, A (corresponding author), Mobileye Vis Technol, Princeton, NJ USA.	ferencz@cs.berkeley.edu; elm@cs.umass.edu; malik@cs.berkeley.edu						Amit Y, 1999, NEURAL COMPUT, V11, P1691, DOI 10.1162/089976699300016197; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Belongie S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P454, DOI 10.1109/ICCV.2001.937552; Berg AC, 2005, PROC CVPR IEEE, P26; BERG TL, 2004, CVPR, V2, P848; Bernstein EJ, 2005, PROC CVPR IEEE, P734; Blanz V, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P202, DOI 10.1109/AFGR.2002.1004155; BOLME D, 2003, ICVS, P128; DIAMOND R, 1986, J EXP PSYCHOL GEN, V115, P107, DOI 10.1037/0096-3445.115.2.107; DORK G, 2005, RR5497 INRIA; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Fei-Fei L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1134, DOI 10.1109/ICCV.2003.1238476; Freund Y, 1996, P 13 INT C MACH LEAR, P148, DOI DOI 10.5555/3091696.3091715; HEISELE B, 2000, AI MEMO, V521; JAIN V, 2006, BRIT MACH VIS C, V1, P357; John G.H., 1994, MACHINE LEARNING P 1, DOI 10.1016/B978-1-55860-335-6.50023-4; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Kibble WF, 1941, SANKHYA, V5, P137; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; McCullagh P., 1989, GEN LINEAR MODELS, V2nd; Miller EG, 2000, PROC CVPR IEEE, P464, DOI 10.1109/CVPR.2000.855856; Moghaddam B, 2000, PATTERN RECOGN, V33, P1771, DOI 10.1016/S0031-3203(99)00179-X; MORI G, 2001, CVPR, V1, P723; Schneiderman H, 2000, PROC CVPR IEEE, P746, DOI 10.1109/CVPR.2000.855895; Shental N., 2003, NIPS; Shental N., 2002, ECCV; Tarr MJ, 2000, NAT NEUROSCI, V3, P764, DOI 10.1038/77666; Thrun S., 1996, EXPLANATION BASED NE; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Vidal-Naquet M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P281; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Weber M, 2000, LECT NOTES COMPUT SC, V1842, P18; WISKOTT L, 1997, 7 INT C COMP AN IM P, V19, P775; Xing E., 2002, P ADV NEUR INF PROC, V15, P1; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342	36	18	20	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2008	77	1-3					3	24		10.1007/s11263-007-0093-5	http://dx.doi.org/10.1007/s11263-007-0093-5			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	267RE		Green Submitted			2022-12-18	WOS:000253526100002
J	Guilbert, N; Bartoli, A; Heyden, A				Guilbert, Nicolas; Bartoli, Adrien; Heyden, Anders			Affine approximation for direct batch recovery of euclidian structure and motion from sparse data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; batch recovery; closure constraints; affine camera model; autocalibration; contraction mapping	STRUCTURE-FROM-MOTION; FACTORIZATION METHOD; SELF-CALIBRATION; SHAPE; RECONSTRUCTION; PERSPECTIVE; CAMERA; VIEWS	We present a batch method for recovering Euclidian camera motion from sparse image data. The main purpose of the algorithm is to recover the motion parameters using as much of the available information and as few computational steps as possible. The algorithm thus places itself in the gap between factorisation schemes, which make use of all available information in the initial recovery step, and sequential approaches which are able to handle sparseness in the image data. Euclidian camera matrices are approximated via the affine camera model, thus making the recovery direct in the sense that no intermediate projective reconstruction is made. Using a little known closure constraint, the FA-closure, we are able to formulate the camera coefficients linearly in the entries of the affine fundamental matrices. The novelty of the presented work is twofold: Firstly the presented formulation allows for a particularly good conditioning of the estimation of the initial motion parameters but also for an unprecedented diversity in the choice of possible regularisation terms. Secondly, the new autocalibration scheme presented here is in practice guaranteed to yield a Least Squares Estimate of the calibration parameters. As a bi-product, the affine camera model is rehabilitated as a useful model for most cameras and scene configurations. e.g. wide an-le lenses observing a scene at close range. Experiments on real and synthetic data demonstrate the ability to reconstruct scenes which are very problematic for previous structure from motion techniques due to local ambiguities and error accumulation.	Lund Univ, Ctr Math Sci, S-22100 Lund, Sweden; Univ Clermont Ferrand, CNRS, LASMEA, Clermont Ferrand, France; Malmo Univ Hosp, Div Math, Malmo, Sweden	Lund University; Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA); Lund University; Skane University Hospital	Guilbert, N (corresponding author), Lund Univ, Ctr Math Sci, S-22100 Lund, Sweden.							Aanaes H, 2002, IEEE T PATTERN ANAL, V24, P1215, DOI 10.1109/TPAMI.2002.1033213; Beardsley PA, 1997, INT J COMPUT VISION, V23, P235, DOI 10.1023/A:1007923216416; BJORCK AKE, 1996, NUMERICAL METHODS LE; Christy S, 1996, IEEE T PATTERN ANAL, V18, P1098, DOI 10.1109/34.544079; FAUGERAS O, 1993, 3 DIMENSIONAL VOMPUY; FAUGERAS OD, 1992, LECTURE NOTES COMPUT, V588, P563; Griffel D. H., 1985, APPL FUNCTIONAL ANAL; GUILBERT N, 2004, P AS C COMP VIS JEJ; GUILBERT N, 2003, P BRIT MACH VIS C, V1, P63; GUILBERT N, 2006, STRUCTURE MOTION SYS; HARTLEY RI, 1994, LNCS SERIES, V825, P237; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HEYDEN A, 1999, P 7 INT C COMP VIS K; Jacobs D, 1997, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.1997.609321; Jianbo S., 1994, IEEE C COMP VIS PATT; KAHL F, 1999, INT J COMPUTER VISIO; Kanade T, 1998, PHILOS T R SOC A, V356, P1153, DOI 10.1098/rsta.1998.0215; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; MARTINEC D, 2002, P 7 EUR C COMP VIS C; Nister D., 2001, THESIS ROYAL I TECHN; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; Quan L, 1996, INT J COMPUT VISION, V19, P93, DOI 10.1007/BF00131149; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Triggs B, 1997, IMAGE VISION COMPUT, V15, P617, DOI 10.1016/S0262-8856(97)00016-4; Xu A., 2018, KINETIC THEORY, DOI [10.1007/978-94-015-8668-9, DOI 10.1007/978]; Zhang ZY, 1998, J MATH IMAGING VIS, V9, P213, DOI 10.1023/A:1008341803636	27	18	18	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2006	69	3					317	333		10.1007/s11263-006-8113-4	http://dx.doi.org/10.1007/s11263-006-8113-4			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	067YM					2022-12-18	WOS:000239338700004
J	Aubert, G; Aujol, JF; Blanc-Feraud, L				Aubert, G; Aujol, JF; Blanc-Feraud, L			Detecting codimension - Two objects in an image with Ginzburg-Landau models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Ginzburg-Landau model; points detection; segmentation; PDE; biological images; SAR images	GEODESIC ACTIVE CONTOURS	In this paper, we propose a new mathematical model for detecting in an image singularities of codimension greater than or equal to two. This means we want to detect isolated points in a 2-D image or points and curves in a 3-D image. We drew one's inspiration from Ginzburg-Landau (G-L) models which have proved their efficiency for modeling many phenomena in physics. We introduce the model, state its mathematical properties and give some experimental results demonstrating its capability in image processing.	Univ Nice, CNRS, UMR 6621, Lab JA Dieudonne, F-06108 Nice, France; INRIA Sophia Antipolis, Project Commun CNRS INRIA UNSA, ARIANA, F-06902 Sophia Antipolis, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Cote d'Azur	Aubert, G (corresponding author), Univ Nice, CNRS, UMR 6621, Lab JA Dieudonne, Parc Valrose, F-06108 Nice, France.	gaubert@math.unice.fr; aujol@cmla.ens-cachan.fr; Laure.Blanc-Feraud@sophia.inria.fr	Aujol, Jean-Francois/AHC-7262-2022	Aujol, Jean-Francois/0000-0001-6716-0509				ALBERTI G, 2003, VARIATIONAL CONVERGE; AMBROSIO L, 1996, J DIFFERENTIAL GEOME, V43; Aubert G, 1999, INT J COMPUT VISION, V34, P19, DOI 10.1023/A:1008168219878; AUBERT G, 2004, IN PRESS J NUMERICAL; Aubert G., 2002, APPL MATH SCI; Bethuel F., 1994, GINZBURG LANDAU VORT; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; CHEN XY, 1998, SIAM J MATH ANAL; Gilboa G, 2004, IEEE T PATTERN ANAL, V26, P1020, DOI 10.1109/TPAMI.2004.47; GILBOA G, 2001, LECT NOTES COMPUTER, V2106; Ginzburg V, 1950, ZHEKSPER TEO FIZ, V20; GROSSAUER H, 2003, LECT NOTES COMPUTER, V1682; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; HENDERSON L, 1998, PRINCIPLE APPL IMAGI, V2; Lorigo L. M., 1999, INT C INF PROC MED I; Osher S, 2001, J COMPUT PHYS, V169, P463, DOI 10.1006/jcph.2000.6636; RUUTH SJ, 1998, 9847 UCLA; SUSSMAN M, 1994, J COMPUT PHYS, V114, P146, DOI 10.1006/jcph.1994.1155; [No title captured], DOI DOI 10.1007/3-540-57956-7_1	20	18	19	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2005	65	1-2					29	42		10.1007/s11263-005-3847-y	http://dx.doi.org/10.1007/s11263-005-3847-y			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	994NU		Green Submitted			2022-12-18	WOS:000234039300002
J	Van de Weijer, J; Van den Boomgaard, R				Van de Weijer, J; Van den Boomgaard, R			Least squares and robust estimation of local image structure	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	4th International Conference on Scale Space Methods in Computer Vision	JUN 10-12, 2003	ISLE SKYE, SCOTLAND	British Machine Vis Assoc, Kings Coll London, IT Univ Copenhagen		scale-space theory; non-linear filtering; color image filtering; robust estimation		Linear scale space methodology uses Gaussian probes at scale s to observe the differential structure. In observing the differential image structure through the Gaussian derivative probes at scale s we implicitly construct the Taylor series expansion of the smoothed image. The Gaussian facet model, as a generalization of the classic Haralick facet model, constructs a polynomial approximation of the unsmoothed image. The measured differential structure therefore is closer to the 'real' structure than the differential structure measured using Gaussian derivatives. At the points in an image where the differential structure changes abruptly (because of discontinuities in the imaging conditions, e.g. a material change, or a depth discontinuity) both the Gaussian derivatives and the Gaussian facet model diffuse the information from both sides of the discontinuity (smoothing across the edge). Robust estimators that are classically meant to deal with statistical outliers can also be used to deal with these mixed model distributions'. In this paper we introduce the robust estimators of local image structure. Starting with the Gaussian facet model where we replace the quadratic error norm with a robust (Gaussian) error norm leads to a robust Gaussian facet model. We will show examples of using the robust differential structure estimators for luminance and color images, for zero and higher order differential structure. Furthermore we look at a 'robustified' structure tensor that forms the basis of robust orientation estimation.	Univ Amsterdam, Dept Comp Sci, Amsterdam, Netherlands	University of Amsterdam	Van de Weijer, J (corresponding author), Univ Amsterdam, Dept Comp Sci, Kruislaan 403, Amsterdam, Netherlands.	joostw@science.uva.nl; rein@science.uva.nl		van de Weijer, Joost/0000-0002-9656-9706				BAKKER P, 1999, ASCI99 P 5 ANN C ADV, P207; BIGUN J, 1991, IEEE T PATTERN ANAL, V13, P775, DOI 10.1109/34.85668; Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192; FARNEBACK G, 1999, THESIS LINKOPING U; HARALICK R, 1983, TOPOGRAPHIC PRIMAL S, P50; HUECKEL MH, 1971, J ACM, V18, P113, DOI 10.1145/321623.321635; KASS M, 1987, COMPUTER GRAPHICS IM, V37, P363; KUWAHARA M, DIGITAL PROCESSING B; Lindeberg T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P683, DOI 10.1109/ICCV.1993.378146; NAGAO M, 1979, COMPUT VISION GRAPH, V9, P394, DOI 10.1016/0146-664X(79)90102-3; Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710; TOMASI C, P 6 INT C COMP VIS B, P839; van de Weijer J, 2001, PROC CVPR IEEE, P428; Van den Boomgaard R, 2002, MATHEMATICAL MORPHOLOGY, PROCEEDINGS, P283; van den Boomgaard R, 2002, INT C PATT RECOG, P927, DOI 10.1109/ICPR.2002.1048187; WEICKER J, 1997, ANISOTROPIC DIFFUSIO; WEICKERT J, 2002, INVERSE PROBL IMAG, V313, P252; [No title captured]	18	18	18	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2005	64	2-3					143	155						13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	961YC					2022-12-18	WOS:000231696700005
J	Hanek, R; Beetz, M				Hanek, R; Beetz, M			The contracting curve density algorithm: Fitting parametric curve models to images using local self-adapting separation criteria	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						deformable models; optimization; model-based image segmentation; 3-D pose estimation; color; texture; image cue integration; automatic scale selection; sub-pixel accuracy	RANDOM-FIELD MODELS; BAYESIAN-APPROACH; SEGMENTATION; COLOR; TRACKING; INTEGRATION; SNAKES	The task of fitting parametric curve models to the boundaries of perceptually meaningful image regions is a key problem in computer vision with numerous applications, such as image segmentation, pose estimation, object tracking, and 3-D reconstruction. In this article, we propose the Contracting Curve Density (CCD) algorithm as a solution to the curve-fitting problem. The CCD algorithm extends the state-of-the-art in two important ways. First, it applies a novel likelihood function for the assessment of a fit between the curve model and the image data. This likelihood function can cope with highly inhomogeneous image regions, because it is formulated in terms of local image statistics. The local image statistics are learned on the fly from the vicinity of the expected curve. They provide therefore locally adapted criteria for separating the adjacent image regions. These local criteria replace often used predefined fixed criteria that rely on homogeneous image regions or specific edge properties. The second contribution is the use of blurred curve models as efficient means for iteratively optimizing the posterior density over possible model parameters. These blurred curve models enable the algorithm to trade-off two conflicting objectives, namely heaving a large area of convergence and achieving high accuracy. We apply the CCD algorithm to several challenging image segmentation and 3-D pose estimation problems. Our experiments with RGB images show that the CCD algorithm achieves a high level of robustness and sub-pixel accuracy even in the presence of severe texture, shading, clutter, partial occlusion, and strong changes of illumination.	Tech Univ Munich, D-85748 Garching, Germany	Technical University of Munich	Hanek, R (corresponding author), Tech Univ Munich, Boltzmannstr 3, D-85748 Garching, Germany.	hanek@in.tum.de; beetzm@in.tum.de		Beetz, Michael/0000-0002-7888-7444				AMINI AA, 1990, IEEE T PATTERN ANAL, V12, P855, DOI 10.1109/34.57681; Baker SS, 1998, J PEDIATR GASTR NUTR, V27, P1, DOI 10.1097/00005176-199807000-00001; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; Belongie S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P675, DOI 10.1109/ICCV.1998.710790; Bennett J, 1998, IEEE T PATTERN ANAL, V20, P327, DOI 10.1109/34.667889; Blake A., 1998, ACTIVE CONTOURS, DOI [10.1007/978-1-4471-1555-7, DOI 10.1007/978-1-4471-1555-7]; BONGIOVANNI G, 1995, COMPUT VIS IMAGE UND, V61, P60, DOI 10.1006/cviu.1995.1005; Bournan C, 1993, IEEE T IMAGE PROCESS, V2, P296, DOI 10.1109/83.236536; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; Chakraborty A, 1999, IEEE T PATTERN ANAL, V21, P12, DOI 10.1109/34.745730; Chellappa R., 1985, PATTERN RECOGNITION, V2, P79; Chesnaud C, 1999, IEEE T PATTERN ANAL, V21, P1145, DOI 10.1109/34.809108; Chuang YY, 2001, PROC CVPR IEEE, P264; COOTES TF, 1994, IMAGE VISION COMPUT, V12, P355, DOI 10.1016/0262-8856(94)90060-4; COOTES TF, 1993, FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION : PROCEEDINGS, P242; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; COX I, 1996, P IEEE INT C PATT RE, pB557; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dubuisson-Jolly MP, 2001, COMPUT VIS IMAGE UND, V81, P26, DOI 10.1006/cviu.2000.0883; Felzenszwalb PF, 1998, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.1998.698594; GEIGER D, 1995, IEEE T PATTERN ANAL, V17, P294, DOI 10.1109/34.368194; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Hanek R, 2001, PROC CVPR IEEE, P797; Hanek R, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P116, DOI 10.1109/IRDS.2002.1041374; HANEK R, 2002, ROBOCUP INT S; HANEK R, 2003, THESIS TU MUNCHEN; HERMES L, 2002, P EUR C COMP VIS, V3, P577; Hough P.V., 1962, US Patent, Patent No. [US3069654A, 3069654, 3,069,654]; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Jones TN, 1998, PROC CVPR IEEE, P330, DOI 10.1109/CVPR.1998.698627; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KOLLNIG H, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P569, DOI 10.1109/ICCV.1995.466888; LEVENTON ME, 2000, PROC CVPR IEEE, P316, DOI DOI 10.1109/CVPR.2000.855835; Li PH, 2003, IMAGE VISION COMPUT, V21, P111, DOI 10.1016/S0262-8856(02)00133-6; Li S. Z., 2001, COMP SCI W; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043; Luo H, 2000, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2000.855854; MACCORMICK J, 2000, P EUR C COMP VIS, P3; Malik J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P918, DOI 10.1109/ICCV.1999.790346; Malishevskii AS, 2001, PHYS SOLID STATE+, V43, P1, DOI 10.1134/1.1340176; Manduchi R., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P956, DOI 10.1109/ICCV.1999.790351; McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7; Mirmehdi M, 2000, IEEE T PATTERN ANAL, V22, P142, DOI 10.1109/34.825753; Mortensen EN, 1998, GRAPH MODEL IM PROC, V60, P349, DOI 10.1006/gmip.1998.0480; NALWA VS, 1986, IEEE T PATTERN ANAL, V8, P699, DOI 10.1109/TPAMI.1986.4767852; PANJWANI DK, 1995, IEEE T PATTERN ANAL, V17, P939, DOI 10.1109/34.464559; PARAGIOS N, 2000, P EUR C COMP VIS, P224; Pece AV, 2002, LECT NOTES COMPUT SC, V2350, P3; Press W. H., 1996, NUMERICAL RECIPES C; Robert L, 1996, COMPUT VIS IMAGE UND, V63, P314, DOI 10.1006/cviu.1996.0021; RONFARD R, 1994, INT J COMPUT VISION, V13, P229, DOI 10.1007/BF01427153; Ruzon MA, 2001, IEEE T PATTERN ANAL, V23, P1281, DOI 10.1109/34.969118; Ruzon MA, 2000, PROC CVPR IEEE, P18, DOI 10.1109/CVPR.2000.855793; Schmitt T, 2002, IEEE T ROBOTIC AUTOM, V18, P670, DOI 10.1109/TRA.2002.804499; Sclaroff S, 2001, IEEE T PATTERN ANAL, V23, P475, DOI 10.1109/34.922706; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Steger C., 2000, INT ARCH PHOTOGRA B3, V33, P141; STORVIK G, 1994, IEEE T PATTERN ANAL, V16, P976, DOI 10.1109/34.329011; Sullivan S, 1998, IEEE T PATTERN ANAL, V20, P1091, DOI 10.1109/34.722621; Terzopoulos D., 1992, ACTIVE VISION, P3; Thirion B, 2000, PROC CVPR IEEE, P349, DOI 10.1109/CVPR.2000.854845; ULRICH M, 2001, PHOTOGRAMM FERNERKUN, P571; Vijayakumar B, 1998, COMPUT VIS IMAGE UND, V72, P287, DOI 10.1006/cviu.1998.0701; WU Z, 1993, IEEE T PATTERN ANAL, V15, P1101, DOI 10.1109/34.244673; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; ZEOLLER T, 2002, P IEEE INT C PATT RE, V2, P627; Zhong Y, 2000, IEEE T PATTERN ANAL, V22, P544, DOI 10.1109/34.857008; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343	69	18	18	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP-OCT	2004	59	3					233	258		10.1023/B:VISI.0000025799.44214.29	http://dx.doi.org/10.1023/B:VISI.0000025799.44214.29			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816HK					2022-12-18	WOS:000221100800002
J	Ma, Y; Huang, K; Vidal, R; Kosecka, J; Sastry, S				Ma, Y; Huang, K; Vidal, R; Kosecka, J; Sastry, S			Rank conditions on the multiple-view matrix	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						multiple-view matrix; rank conditions; structure from motion; factorization	LINE CORRESPONDENCES; FACTORIZATION METHOD; AFFINE STRUCTURE; MOTION	Geometric relationships governing multiple images of points and lines and associated algorithms have been studied to a large extent separately in multiple-view geometry. The previous studies led to a characterization based on multilinear constraints, which have been extensively used for structure and motion recovery, feature matching and image transfer. In this paper we present a universal rank condition on the so-called multiple-view matrix M for arbitrarily combined point and line features across multiple views. The condition gives rise to a complete set of constraints among multiple images. All previously known multilinear constraints become simple instantiations of the new condition. In particular, the relationship between bilinear, trilinear and quadrilinear constraints can be clearly revealed from this new approach. The theory enables us to carry out global geometric analysis for multiple images, as well as systematically characterize all degenerate configurations, without breaking image sequence into pairwise or triple-wise sets of views. This global treatment allows us to utilize all incidence conditions governing all features in all images simultaneously for a consistent recovery of motion and structure from multiple views. In particular, a rank-based multiple-view factorization algorithm for motion and structure recovery is derived from the rank condition. Simulation results are presented to validate the multiple-view matrix based approach.	Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; Johns Hopkins Univ, Dept Biomed Engn, Baltimore, MD 21218 USA; George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA; Univ Calif Berkeley, Berkeley, CA 94720 USA	University of Illinois System; University of Illinois Urbana-Champaign; Johns Hopkins University; George Mason University; University of California System; University of California Berkeley	Ma, Y (corresponding author), Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA.	yima@uiuc.edu; kunhuang@uiuc.edu; rvidal@cis.jhu.edu; kosecka@cs.gmu.edu; sastry@eecs.berkeley.edu	Huang, Kun/E-3272-2011					AVIDAN S, 1998, IEEE T VISUALIZATION, V4; FAUGERAS O, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P951, DOI 10.1109/ICCV.1995.466832; FOSSUM R, 2001, 012222DC203 UIUC CSL; HARTLEY R, 1994, P 1994 IM UND WORKSH, P1006; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Heyden A, 1997, MATH METHOD APPL SCI, V20, P1135, DOI 10.1002/(SICI)1099-1476(19970910)20:13<1135::AID-MMA908>3.0.CO;2-9; Kahl F, 1999, INT J COMPUT VISION, V33, P163, DOI 10.1023/A:1008192713051; KOSECKA J, 2002, TYRRH INT WORKSH DIG; Kruppa E., 1913, SITZ BER AKAD WIS MN, V122, P1939; LIU Y, 1990, IEEE T PATTERN ANAL, P28; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Ma Y., 2003, INVITATION 3 D VISIO; MA Y, 2001, 012214DC220 UIUC CSL; Quan L, 1996, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.1996.517164; SHAHUA A, 1994, P ECCV, V1, P479; SHASHUA A, 2000, P ECCV, V1, P711; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; TAYLOR CJ, 1995, IEEE T PATTERN ANAL, V17, P1021, DOI 10.1109/34.473228; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Triggs B, 1996, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.1996.517170; TRIGGS B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P338, DOI 10.1109/ICCV.1995.466920	22	18	23	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2004	59	2					115	137		10.1023/B:VISI.0000022286.53224.3d	http://dx.doi.org/10.1023/B:VISI.0000022286.53224.3d			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	842DR					2022-12-18	WOS:000222983800001
J	Langer, MS; Mann, R				Langer, MS; Mann, R			Optical snow	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion analysis; Fourier transform; optical flow; layered motion; transparency; motion field; spatiotemporal filters	ROBUST ESTIMATION; MOTION; FLOW; MODEL; PERCEPTION; VELOCITY	Classical methods for measuring image motion by computer have concentrated on the cases of optical flow in which the motion field is continuous, or layered motion in which the motion field is piecewise continuous. Here we introduce a third natural category which we call optical snow. Optical snow arises in many natural situations such as camera motion in a highly cluttered 3-D scene, or a passive observer watching a snowfall. Optical snow yields dense motion parallax with depth discontinuities occurring near all image points. As such, constraints on smoothness or even smoothness in layers do not apply. In the Fourier domain, optical snow yields a one-parameter family of planes which we call a bowtie. We present a method for measuring the parameters of the direction and range of speeds of the motion for the special case of parallel optical snow. We demonstrate the effectiveness of the method for both synthetic and real image sequences.	McGill Univ, Sch Comp Sci, Montreal, PQ H3A 2A7, Canada; Univ Waterloo, Sch Comp Sci, Waterloo, ON N2L 3G1, Canada	McGill University; University of Waterloo	Langer, MS (corresponding author), McGill Univ, Sch Comp Sci, Montreal, PQ H3A 2A7, Canada.	langer@cim.mcgill.ca; mannr@uwaterloo.ca						ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Beauchemin SS, 2000, IEEE T PATTERN ANAL, V22, P200, DOI 10.1109/34.825758; BERGEN JR, 1992, IEEE T PATTERN ANAL, V14, P886, DOI 10.1109/34.161348; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; DARRELL T, 1995, IEEE T PATTERN ANAL, V17, P474, DOI 10.1109/34.391395; Davis J, 2000, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2000.855878; FENNEMA CL, 1979, COMPUT VISION GRAPH, V9, P301, DOI 10.1016/0146-664X(79)90097-2; FLEET DJ, 1994, VISION RES, V34, P3057, DOI 10.1016/0042-6989(94)90278-X; Fleet DJ, 1992, MEASUREMENT IMAGE VE; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; GRZYWACZ NM, 1990, PROC R SOC SER B-BIO, V239, P129, DOI 10.1098/rspb.1990.0012; HEEGER DJ, 1987, 1ST P INT C COMP VIS, P181; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HuangHellinger FR, 1995, HUM BRAIN MAPP, V3, P13, DOI 10.1002/hbm.460030103; IRANI M, 1994, INT J COMPUT VISION, V12, P5, DOI 10.1007/BF01420982; JEPSON A, 1993, IEEE C COMP VIS PATT, P760; Ju SX, 1996, PROC CVPR IEEE, P307, DOI 10.1109/CVPR.1996.517090; KOENDERINK JJ, 1986, VISION RES, V26, P161, DOI 10.1016/0042-6989(86)90078-7; Langer MS, 2002, LECT NOTES COMPUT SC, V2525, P181; Langer MS, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P155, DOI 10.1109/ICCV.2001.937512; LAPPE M, 1993, NEURAL COMPUT, V5, P374, DOI 10.1162/neco.1993.5.3.374; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Mann R, 2002, INT C PATT RECOG, P264, DOI 10.1109/ICPR.2002.1047447; MARR D, 1981, PROC R SOC SER B-BIO, V211, P151, DOI 10.1098/rspb.1981.0001; Marr D., 1982, VISION COMPUTATIONAL; Milanfar P, 1996, J OPT SOC AM A, V13, P2151, DOI 10.1364/JOSAA.13.002151; QIAN N, 1994, J NEUROSCI, V14, P7357; Sawhney HS, 1996, IEEE T PATTERN ANAL, V18, P814, DOI 10.1109/34.531801; Shizawa M., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P289, DOI 10.1109/CVPR.1991.139704; Simoncelli EP, 1998, VISION RES, V38, P743, DOI 10.1016/S0042-6989(97)00183-1; TRUCCO E, 1998, INTRO TECHNIQUES 3 D; WANG J, 1993, IEEE C COMP VIS PATT, P361; WARREN WH, 1990, J OPT SOC AM A, V7, P160, DOI 10.1364/JOSAA.7.000160; WATSON AB, 1985, J OPT SOC AM A, V2, P322, DOI 10.1364/JOSAA.2.000322; WAXMAN AM, 1985, INT J ROBOT RES, V4, P95, DOI 10.1177/027836498500400307; Weiss Y, 1997, PROC CVPR IEEE, P520, DOI 10.1109/CVPR.1997.609375; WILDES R, 2000, 6 EUR C COMP VIS DUB, P768; ZEMEL RS, 1999, ADV NEURAL INFORMATI, V11, P768; ZUCKER SW, 1987, COMPUT VISION GRAPH, V37, P196, DOI 10.1016/S0734-189X(87)80002-6	41	18	18	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2003	55	1					55	71		10.1023/A:1024440524579	http://dx.doi.org/10.1023/A:1024440524579			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	694TM					2022-12-18	WOS:000183790500003
J	Li, YM; Gong, SG; Liddell, H				Li, YM; Gong, SG; Liddell, H			Constructing facial identity surfaces for recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						face recognition; Kernel Discriminant Analysis; dynamic face models; facial identity surfaces	OBJECT RECOGNITION; FACE IMAGES; MODELS; SHAPE	We present a novel approach to face recognition by constructing facial identity structures across views and over time, referred to as identity surfaces, in a Kernel Discriminant Analysis (KDA) feature space. This approach is aimed at addressing three challenging problems in face recognition: modelling faces across multiple views, extracting non-linear discriminatory features, and recognising faces over time. First, a multi-view face model is designed which can be automatically fitted to face images and sequences to extract the normalised facial texture patterns. This model is capable of dealing with faces with large pose variation. Second, KDA is developed to compute the most significant non-linear basis vectors with the intention of maximising the between-class variance and minimising the within-class variance. We applied KDA to the problem of multi-view face recognition, and a significant improvement has been achieved in reliability and accuracy. Third, identity surfaces are constructed in a pose-parameterised discriminatory feature space. Dynamic face recognition is then performed by matching the object trajectory computed from a video input and model trajectories constructed on the identity surfaces. These two types of trajectories encode the spatio-temporal dynamics of moving faces.	BTexact Technol, Content & Coding Lab, Ipswich IP5 3RE, Suffolk, England; Univ London, Queen Mary, Dept Comp Sci, London E1 4NS, England	University of London; Queen Mary University London	Li, YM (corresponding author), BTexact Technol, Content & Coding Lab, PP2 Ross Bldg,Adastral Pk, Ipswich IP5 3RE, Suffolk, England.	Yongmin.Li@bt.com; sgg@dcs.qmul.ac.uk; heather@dcs.qmul.ac.uk						[Anonymous], 2 INT WORKSH STAT CO; Atick JJ, 1996, NEURAL COMPUT, V8, P1321, DOI 10.1162/neco.1996.8.6.1321; Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980; BENNETT AD, 1991, BRIT MACHINE VISION, P233; Bishop, 1995, NEURAL NETWORKS PATT; BRAMMER K, 1989, KALMANBUCY FILTERS; Bruce V, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P408, DOI 10.1109/AFGR.1998.670983; BRUCE V, 1998, FACE RECOGNITION THE, P51; BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061; Burges CJC, 1997, ADV NEUR IN, V9, P375; BURGES CJC, 1996, P 13 INT C MACH LEAR, P71; CHOUDHURY T, 1999, INT C AUD VID BAS PE, P176; Cootes T., 1994, P BRIT MACH VIS C, V1, P327; Cootes T. F., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P227, DOI 10.1109/AFGR.2000.840639; COOTES TF, 1998, P EUR C COMP VIS, V2, P484; CRAW I, 1992, LECT NOTES COMPUT SC, V588, P92; DeCarlo D, 2000, INT J COMPUT VISION, V38, P99, DOI 10.1023/A:1008122917811; EDWARDS G, 1996, BRIT MACH VIS C ED S, V2, P765; Edwards G. J., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P486, DOI 10.1109/CVPR.1999.786982; Edwards GJ, 1998, IMAGE VISION COMPUT, V16, P203, DOI 10.1016/S0262-8856(97)00069-3; Edwards GJ, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P260, DOI 10.1109/AFGR.1998.670958; Ezzat T, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P116, DOI 10.1109/AFGR.1996.557252; Fisher RA, 1938, ANN EUGENIC, V8, P376, DOI 10.1111/j.1469-1809.1938.tb02189.x; Fukunaga K., 1972, INTRO STAT PATTERN R; Gong S., 1994, EUR WORKSH COMB REAL, P96; Howell AJ, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P224, DOI 10.1109/AFGR.1996.557268; Jebara T., 1997, IEEE C COMP VIS PATT; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Knight B, 1997, VIS COGN, V4, P265, DOI 10.1080/713756764; LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173; Lanitis A, 1997, IEEE T PATTERN ANAL, V19, P743, DOI 10.1109/34.598231; LI HB, 1993, IEEE T PATTERN ANAL, V15, P545, DOI 10.1109/34.216724; LI Y, 2000, IEEE INT C AUT FAC G, P300; LI Y, 2000, BRIT MACH VIS C BRIS, P242; MCKENNA S, 1998, FACE RECOGNITION THE, P578; McKenna S. J., 2000, SURVEY FACE RECOGNIT, V58; Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; Moghaddam B, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P30, DOI 10.1109/AFGR.1998.670921; Moghaddam B., 1994, AUTOMATIC SYSTEMS ID, V2277; MURASE H, 1994, IEEE T PATTERN ANAL, V16, P1219, DOI 10.1109/34.387485; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; Okubo M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P187, DOI 10.1109/AFGR.1998.670947; Osuna E, 1997, PROC CVPR IEEE, P130, DOI 10.1109/CVPR.1997.609310; OSUNA EE, 1997, 1602 MIT AI; Romdhani S., 1999, BMVC99. Proceedings of the 10th British Machine Vision Conference, P483; Roth V, 2000, ADV NEUR IN, V12, P568; Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583, DOI 10.1007/BFb0020217; Scholkopf Bernhard, 1997, SUPPORT VECTOR LEARN; Shakunaga T, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P94, DOI 10.1109/AFGR.1998.670931; SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519; Steffens J, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P516, DOI 10.1109/AFGR.1998.671000; Sung K. -K., 1994, 1521 MIT AI; Swets DL, 1996, IEEE T PATTERN ANAL, V18, P831, DOI 10.1109/34.531802; TURK M, 1989, INTELLIGENT ROBOTS C, V8, P22; VANDERBEI R, 1994, 9415 SOR PRINC U; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vetter T, 1998, INT J COMPUT VISION, V28, P103, DOI 10.1023/A:1008058932445; VETTER T, 1998, FACE RECOGNITION THE, P310; Waite J. B., 1990, BMVC90 Proceedings of the British Machine Vision Conference, P407; Wen Yi Zhao, 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P285, DOI 10.1109/AFGR.2000.840648; Wu HY, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P345, DOI 10.1109/AFGR.1996.557289; Yamaguchi O, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P318, DOI 10.1109/AFGR.1998.670968; Yokoyama T, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P254, DOI 10.1109/AFGR.1998.670957; YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169; Zhao W., 1998, FACE RECOGNITION THE, P73	67	18	19	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2003	53	1					71	92		10.1023/A:1023083725143	http://dx.doi.org/10.1023/A:1023083725143			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	668CV					2022-12-18	WOS:000182273500004
J	Tao, H; Huang, TS				Tao, H; Huang, TS			Visual estimation and compression of facial motion parameters - Elements of a 3D model-based video coding system	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						nonrigid motion analysis; facial animation; model-based coding; MPEG4		The MPEG4 standard supports the transmission and composition of facial animation with natural video by including a facial animation parameter (FAP) set that is defined based on the study of minimal facial actions and is closely related to muscle actions. The FAP set enables model-based representation of natural or synthetic talking head sequences and allows intelligible visual reproduction of facial expressions, emotions, and speech pronunciations at the receiver. This paper describes two key components we have developed for building a model-based video coding system: (1) a method for estimating FAP parameters based on our previously proposed piecewise Bezier volume deformation model (PBVD), and (2) various methods for encoding FAP parameters. PBVD is a linear deformation model suitable for both the synthesis and the analysis of facial images. Each FAP parameter is a basis function in this model. Experimental results on PBVD-based animation, model-based tracking, and spatial-temporal compression of FAP parameters are demonstrated in this paper.	Univ Calif Santa Cruz, Dept Comp Engn, Santa Cruz, CA 95064 USA; Univ Illinois, Beckman Inst, Image Proc & Format Lab, Urbana, IL 61801 USA	University of California System; University of California Santa Cruz; University of Illinois System; University of Illinois Urbana-Champaign	Tao, H (corresponding author), Univ Calif Santa Cruz, Dept Comp Engn, Santa Cruz, CA 95064 USA.							CHOI CS, 1994, IEEE T CIRC SYST VID, V4, P257, DOI 10.1109/76.305871; DeCarlo D, 1996, PROC CVPR IEEE, P231, DOI 10.1109/CVPR.1996.517079; Duda R.O., 1973, J ROYAL STAT SOC SER; Essa IA, 1997, IEEE T PATTERN ANAL, V19, P757, DOI 10.1109/34.598232; *ISO, 1997, ISOIECJTC1SC29WG11; Jain A.K., 1989, FUNDAMENTALS DIGITAL; KALRA P, 1992, P EUR 92, P59; Lee Y., 1995, P 22 ANN C COMP GRAP, P55, DOI [10.1145/218380.218407, DOI 10.1145/218380.218407]; LI HB, 1993, IEEE T PATTERN ANAL, V15, P545, DOI 10.1109/34.216724; PARKE FI, 1982, IEEE COMPUT GRAPH, V2, P61; SEDERBERG TW, 1986, P 13 ANN C COMP GRAP, P151, DOI DOI 10.1145/15922.15903; SIROVICH L, 1992, INT J SUPERCOMPUT AP, V6, P50, DOI 10.1177/109434209200600104; Tao H, 1999, IEEE T CIRC SYST VID, V9, P264, DOI 10.1109/76.752094; TAO H, 1997, ISOIECJTC1SC29WG11; TAO H, 1999, P COMP VIS PATT REC, V1, P611; WILLIAMS L, 1990, P SIGGRAPH 90, P235	16	18	18	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2002	50	2					111	125		10.1023/A:1020389714861	http://dx.doi.org/10.1023/A:1020389714861			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	597DX					2022-12-18	WOS:000178207800002
J	Ying, ZR; Castanon, D				Ying, ZR; Castanon, D			Partially occluded object recognition using statistical models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						one-to-one correspondence; occlusion; statistical model; Markov random field; model-based recognition	AUTOMATIC TARGET RECOGNITION; HAUSDORFF DISTANCE; ALGORITHMS	In this paper, we present a new Bayesian framework for partially occluded object recognition based on matching extracted local features on a one-to-one basis with object features. We introduce two different statistical models for occlusion: one model assumes that each feature in the model can be occluded independent of whether any other features are occluded, whereas the second model uses spatially correlated occlusion to represent the extent of occlusion. Using these models, the object recognition problem reduces to finding the object hypothesis with largest generalized likelihood. We develop fast algorithms for finding the optimal one-to-one correspondence between scene features and object features to compute the generalized likelihoods under both models. We conduct experiments illustrating the differences between the two occlusion models using different quantitative metrics. We also evaluate the recognition performance of our algorithms using examples extracted from object silhouettes and synthetic aperture radar imagery, and illustrate the performance advantages of our approach over alternative algorithms.	Boston Univ, Dept Elect & Comp Engn, Boston, MA 02215 USA	Boston University	Ying, ZR (corresponding author), Aware Inc, 40 Middlesex Turnpike, Bedford, MA 01730 USA.							ANSARI N, 1990, IEEE T PATTERN ANAL, V12, P470, DOI 10.1109/34.55107; BAIRD HS, 1985, MODEL BASED IMAGE MA; Bertsekas D. P., 1992, Computational Optimization and Applications, V1, P277, DOI 10.1007/BF00249638; Boshra M, 2000, IEEE T PATTERN ANAL, V22, P956, DOI 10.1109/34.877519; Boykov Y., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P517, DOI 10.1109/CVPR.1999.784730; Breuel T. M., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P257, DOI 10.1109/CVPR.1991.139698; Chung PC, 1998, IEEE T SIGNAL PROCES, V46, P1991, DOI 10.1109/78.700970; COSTA MS, 1992, P SOC PHOTO-OPT INS, V1662, P21, DOI 10.1117/12.58489; Der SZ, 1997, IEEE T IMAGE PROCESS, V6, P92, DOI 10.1109/83.552099; Gold S, 1998, PATTERN RECOGN, V31, P1019, DOI 10.1016/S0031-3203(98)80010-1; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; HUMMEL R, 1988, DARPA IM UND WORKSH; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710; Lamdan Y., 1988, P IEEE INT C COMP VI, P238; Li S., 1995, MARKOV RANDOM FIELD, P1; Nemhauser G.L., 1988, INTEGER COMBINATORIA; Olson CF, 1998, PROC CVPR IEEE, P150, DOI 10.1109/CVPR.1998.698602; Ratches JA, 1997, IEEE T PATTERN ANAL, V19, P1004, DOI 10.1109/34.615449; REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177; RICHARDS JA, 2000, SPIE ALGORITHM SAR I, V7; RIGOUTSOS I, 1995, COMPUT VIS IMAGE UND, V62, P11, DOI 10.1006/cviu.1995.1038; Ross T, 1998, P SOC PHOTO-OPT INS, V3370, P566, DOI 10.1117/12.321859; Sim DG, 1999, IEEE T IMAGE PROCESS, V8, P425, DOI 10.1109/83.748897; WEBER M, 2000, EUR C COMP VIS; Wells WM, 1997, INT J COMPUT VISION, V21, P63, DOI 10.1023/A:1007923522710; Ying Z., 1999, Proceedings 1999 International Conference on Information Intelligence and Systems (Cat. No.PR00446), P324, DOI 10.1109/ICIIS.1999.810284; YING Z, 1999, P 1999 IEEE INT C IN, P560; YING Z, 2002, INT C COMP VIS PATT; YING Z, 2002, THESIS BOSTON U; YING Z, 2001, P INT C COMP VIS VAN	31	18	24	0	5	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2002	49	1					57	78		10.1023/A:1019881831890	http://dx.doi.org/10.1023/A:1019881831890			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	585UJ					2022-12-18	WOS:000177542700004
J	Lee, SH; Kanatsugu, Y; Park, JI				Lee, SH; Kanatsugu, Y; Park, JI			MAP-based stochastic diffusion for stereo matching and line fields estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						disparity estimation; Markov random field; stochastic diffusion; cooperative line field; hierarchical approaches	MOTION ESTIMATION; IMAGE SEQUENCES; SEGMENTATION; TRACKING; MODELS; GENERATION; ALGORITHM; OBJECTS	This paper proposes a stochastic approach to estimate the disparity field combined with line field. In the maximum a posteriori (MAP) method based on Markov random field (MRF) model, it is important to optimize and converge the Gibbs potential function corresponding to the perturbed disparity field. The proposed optimization method, stochastic diffusion, takes advantage of the probabilistic distribution of the neighborhood fields to diffuse the Gibbs potential space iteratively. By using the neighborhood distribution in the non-random and non-deterministic diffusion, both the estimation accuracy and the convergence speed are improved. In the paper, the hierarchical stochastic diffusion is also applied to the disparity field. The hierarchical approach reduces the memory and computational load, and increases the convergence speed of the potential space. The paper also proposes an effective configuration of the neighborhood to be suitable for the hierarchical disparity structure. According to the experiments, the stochastic diffusion shows good estimation performance. The line field improves the estimation at the object boundary, and coincides with the object boundary with the useful contours. The stochastic diffusion is applicable to any kind of field estimation given the appropriate definition of the field and MRF models.	NHK Engn Serv, Object Based Broadcasting Syst Res Lab, Kinuta Setagaya Ku, Tokyo 1578510, Japan; Hanyang Univ, Div Elect & Comp Engn, Seoul 133791, South Korea	NHK Japan Broadcasting Corp; Hanyang University	Lee, SH (corresponding author), NHK Engn Serv, Object Based Broadcasting Syst Res Lab, Kinuta Setagaya Ku, 1-10-11 NHK STRL, Tokyo 1578510, Japan.							BARNARD ST, 1989, INT J COMPUT VISION, V3, P17, DOI 10.1007/BF00054836; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; Black MJ, 2000, INT J COMPUT VISION, V38, P231, DOI 10.1023/A:1008195307933; BOYKOV Y, 1999, P CVPR 99; Chang MM, 1997, IEEE T IMAGE PROCESS, V6, P1326, DOI 10.1109/83.623196; Chang NL, 1997, IEEE T IMAGE PROCESS, V6, P584, DOI 10.1109/83.563323; Chellappa R, 1993, MARKOV RANDOM FIELDS; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GONZALEZ, 1995, DIGITAL IMAGE PROCES; GRAFFIGNE C, 1995, PROC SPIE, V2568, P2, DOI 10.1117/12.216341; Haralick R.M., 1993, COMPUTER ROBOT VISIO; HEITZ F, 1993, IEEE T PATTERN ANAL, V15, P1217, DOI 10.1109/34.250841; HEITZ F, 1994, CVGIP-IMAG UNDERSTAN, V59, P125, DOI 10.1006/ciun.1994.1008; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; *ISO IEC, 1994, 13813 ISOIEC; Jebara T, 1999, IEEE SIGNAL PROC MAG, V16, P66, DOI 10.1109/79.768574; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; Kim M, 1999, IEEE T CIRC SYST VID, V9, P1216, DOI 10.1109/76.809157; KONRAD J, 1992, IEEE T PATTERN ANAL, V14, P910, DOI 10.1109/34.161350; Kreyzig Erwin, 1978, INTRO FUNCTIONAL ANA; Lee SH, 1999, IEICE T FUND ELECTR, VE82A, P1367; LEE SH, 2000, P ICIP 2000, P2213; LEE SH, 1998, P ICASSP 98 SEATTL U, V5, P2769; LEE SH, 2001, P PICT COD S PCS 200; Li ZN, 1996, IEEE T IMAGE PROCESS, V5, P1493, DOI 10.1109/83.541420; MARROQUIN J, 1987, J AM STAT ASSOC, V82, P76, DOI 10.2307/2289127; Meier T, 1999, IEEE T CIRC SYST VID, V9, P1190, DOI 10.1109/76.809155; MEYER FG, 1994, CVGIP-IMAG UNDERSTAN, V60, P119, DOI 10.1006/ciun.1994.1042; Moellenhoff MS, 1998, IEEE T IMAGE PROCESS, V7, P804, DOI 10.1109/83.679421; *MPEG 4, 1997, ISOIECJTC1SC29WG11 M; Nadabar SG, 1996, IEEE T PATTERN ANAL, V18, P326, DOI 10.1109/34.485560; Redert A, 1999, IEEE SIGNAL PROC MAG, V16, P29, DOI 10.1109/79.768571; REDERT PA, 1997, P ICASSP97, P2749; Scharstein D, 1998, INT J COMPUT VISION, V28, P155, DOI 10.1023/A:1008015117424; SCHARSTEIN D, 2002, IJCV; Stiller C, 1997, IEEE T IMAGE PROCESS, V6, P234, DOI 10.1109/83.551695; STILLER C, 1993, P IEEE ICASSP 93, V5, P193; SZELISKI R, 1999, P VIS ALG THEOR PRAC; Tekalp AM, 1995, DIGITAL VIDEO PROCES; TORR PHS, 1999, P ICCV; Woo WT, 1996, P SOC PHOTO-OPT INS, V2727, P28, DOI 10.1117/12.233256; ZHANG J, 1995, IEEE T IMAGE PROCESS, V4, P19, DOI 10.1109/83.350816; ZHANG J, 1992, IEEE T SIGNAL PROCES, V40, P2570, DOI 10.1109/78.157297; Zitnick CL, 2000, IEEE T PATTERN ANAL, V22, P675, DOI 10.1109/34.865184	47	18	20	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					195	218		10.1023/A:1014550009499	http://dx.doi.org/10.1023/A:1014550009499			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN					2022-12-18	WOS:000174354700015
J	Ma, Y; Soatto, S; Kosecka, J; Sastry, S				Ma, Y; Soatto, S; Kosecka, J; Sastry, S			Euclidean reconstruction and reprojection up to subgroups	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	7th IEEE International Conference on Computer Vision	SEP 20-27, 1999	KERKYRA, GREECE	IEEE		camera self-calibration; multilinear constraints; reconstruction ambiguity; reprojection	MULTILINEAR CONSTRAINTS; SELF-CALIBRATION; MOVING CAMERA	The necessary and sufficient conditions for being able to estimate scene structure, motion and camera calibration from a sequence of images are very rarely satisfied in practice. What exactly can be estimated in sequences of practical importance, when such conditions are not satisfied ? In this paper we give a complete answer to this question. For every camera motion that fails to meet the conditions, we give explicit formulas for the ambiguities in the reconstructed scene, motion and calibration. Such a characterization is crucial both for designing robust estimation algorithms (that do not try to recover parameters that cannot be recovered), and for generating novel views of the scene by controlling the vantage point. To this end, we characterize explicitly all the vantage points that give rise to a valid Euclidean reprojection regardless of the ambiguity in the reconstruction. We also characterize vantage points that generate views that are altogether invariant to the ambiguity. All the results are presented using simple notation that involves no tensors nor complex projective geometry, and should be accessible with basic background in linear algebra.	Univ Calif Berkeley, Berkeley, CA 94720 USA; Washington Univ, St Louis, MO 63130 USA	University of California System; University of California Berkeley; Washington University (WUSTL)	Ma, Y (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.							Avidan S, 1997, PROC CVPR IEEE, P1034, DOI 10.1109/CVPR.1997.609457; BEARDSLEY PA, 1995, EUR CHIN WORKSH GEOM; BOUFAMA B, 1993, ICCV BERL GERM, P466; CARLSSON S, 1994, APPL INVARIANCE COMP; Christy S, 1996, IEEE T PATTERN ANAL, V18, P1098, DOI 10.1109/34.544079; Faugeras O., 1997, 3225 INRIA; FAUGERAS OD, 1995, P IEEE WORKSH REPR V; HARTLEY R, 1994, P IM UND WORKSH; Heyden A, 1997, MATH METHOD APPL SCI, V20, P1135, DOI 10.1002/(SICI)1099-1476(19970910)20:13<1135::AID-MMA908>3.0.CO;2-9; Heyden A., 1997, Algebraic Frames for the Perception-Action Cycle. International Workshop, AFPAC'97. Proceedings, P54, DOI 10.1007/BFb0017860; Heyden A, 1998, INT J COMPUT VISION, V30, P5, DOI 10.1023/A:1008020228557; Luong QT, 1997, INT J COMPUT VISION, V22, P261, DOI 10.1023/A:1007982716991; LUONG QT, 1994, ECCV, P589; MA Y, 1999, UNPUB IEEE T PAMI; MAYBANK S, 1988, ICCV BOMB IND, P703; MAYBANK S, 1993, THEORY RECONSTRUCTIO; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; MOONS T, 1993, APPL INVARIANCE COMP, P297; STURM P, 1997, C COMP VIS PATT REC, P1100; TRIGGS B, IN PRESS INT J COMPU; WERMAN M, 1995, PROCEEDINGS OF EUROPE-CHINA WORKSHOP ON GEOMETRICAL MODELING & INVARIANTS FOR COMPUTER VISION, P94	21	18	18	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2000	38	3					219	229		10.1023/A:1008143307025	http://dx.doi.org/10.1023/A:1008143307025			11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	352VV					2022-12-18	WOS:000089239400003
J	Duvdevani-Bar, S; Edelman, S				Duvdevani-Bar, S; Edelman, S			Visual recognition and categorization on the basis of similarities to multiple class prototypes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						representation; similarity; visual shape recognition; categorization; view space; shape space	OBJECT RECOGNITION; FACE RECOGNITION; 3-D OBJECTS; REPRESENTATION; SHAPE; IMAGE; APPEARANCE; EXAMPLES; SURFACE; SPACES	One of the difficulties of object recognition stems from the need to overcome the variability in object appearance caused by pose and other factors, such as illumination. The influence of these factors can be countered by learning to interpolate between stored views of the target object, taken under representative combinations of viewing conditions. Difficulties of another kind arise in daily life situations that require categorization, rather than recognition, of objects. Although categorization cannot rely on interpolation between stored examples, we show that knowledge of several representative members, or prototypes, of each of the categories of interest can provide the necessary computational substrate for the categorization of new instances. We describe a system that represents input shapes by their similarities to several prototypical objects, and show that it can recognize new views of the familiar objects, discriminate among views of previously unseen shapes, and attribute the latter to familiar categories.	Weizmann Inst Sci, Dept Appl Math, IL-76100 Rehovot, Israel; Cornell Univ, Dept Psychol, Ithaca, NY 14853 USA	Weizmann Institute of Science; Cornell University	Duvdevani-Bar, S (corresponding author), Weizmann Inst Sci, Dept Appl Math, IL-76100 Rehovot, Israel.	sharon@wisdom.weizmann.ac.il; shimone@cogs.susx.ac.uk						Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229; Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545; Basri R, 1996, INT J COMPUT VISION, V19, P147, DOI 10.1007/BF00055802; BAXTER J, 1997, P 14 INT C MACH LEAR, P39; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BIEDERMAN I, 1988, COGNITIVE PSYCHOL, V20, P38, DOI 10.1016/0010-0285(88)90024-2; BREUEL TM, 1992, THESIS MIT; Broomhead D. S., 1988, Complex Systems, V2, P321; BURGE M, 1997, J COMPUTING INFORMAT, V4, P39; BURL M, 1998, RECOGNITION VISUAL O; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; DEVERDIERE VC, 1998, LNCS SERIES, V1406, P640; Duda R.O., 1973, J ROYAL STAT SOC SER; Duvdevani-Bar S, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P118, DOI 10.1109/AFGR.1998.670935; DUVDEVANIBAR S, 1997, THESIS WEIZMANN I SC; Edelman S, 1998, BEHAV BRAIN SCI, V21, P449, DOI 10.1017/S0140525X98001253; EDELMAN S, 1995, MIND MACH, V5, P45, DOI 10.1007/BF00974189; EDELMAN S, 1992, LECT NOTES COMPUT SC, V588, P787; EDELMAN S, 1993, IEEE T PATTERN ANAL, V15, P833, DOI 10.1109/34.236244; Edelman S, 1997, NEURAL COMPUT, V9, P701, DOI 10.1162/neco.1997.9.4.701; EDELMAN S, 1997, MECH PERCEPTUAL LEAR, P353; Edelman S., 1997, P SIM CAT WORKSH, P75; FILLENBAUM S, 1979, STRUCTURES SUBJECTIV; Gersho A., 1992, VECTOR QUANTIZATION; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P330, DOI 10.1109/34.485561; JOLICOEUR P, 1984, COGNITIVE PSYCHOL, V16, P243, DOI 10.1016/0010-0285(84)90009-4; Kanatani Kenichi, 1990, GROUP THEORETICAL ME, P4; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; LANDO M, 1995, NETWORK-COMP NEURAL, V6, P551, DOI 10.1088/0954-898X/6/4/003; LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; LOWE DG, 1986, PERCEPTUAL ORG VISUA; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020; Mel BW, 1997, NEURAL COMPUT, V9, P777, DOI 10.1162/neco.1997.9.4.777; Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; Nelson RC, 1998, VISION RES, V38, P2469, DOI 10.1016/S0042-6989(98)00030-3; Palmer S. E., 1981, ATTENTION PERFORM, V9, P135; POGGIO T, 1990, SCIENCE, V247, P978, DOI 10.1126/science.247.4945.978; POGGIO T, 1990, NATURE, V343, P263, DOI 10.1038/343263a0; POGGIO T, 1992, 1347 MIT ART INT LAB; POGGIO T., 1989, 1140 MIT ART INT LAB; PRICE CJ, 1989, Q J EXP PSYCHOL-A, V41, P797, DOI 10.1080/14640748908402394; RIESENHUBER M, 1998, IN PRESS ADV NEURAL, V10; Rosch E., 1978, COGNITION CATEGORIZA, P27, DOI DOI 10.1037/0012-1649.16.5.391; SAS Institute Inc., 1989, US GUID VERS 6; SCHIELE B, 1996, LECT NOTES COMPUTER, V1, P610; Shapira Y., 1991, P IJCAI, P1257; SHEPARD RN, 1980, SCIENCE, V210, P390, DOI 10.1126/science.210.4468.390; SMITH I, 1990, SOLID STATE TECHNOL, V33, P53; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; ULLMAN S, 1989, COGNITION, V32, P193, DOI 10.1016/0010-0277(89)90036-X; Ullman S., 1996, HIGH LEVEL VISION OB; Vetter T, 1997, IEEE T PATTERN ANAL, V19, P733, DOI 10.1109/34.598230; VETTER T, 1995, CEREB CORTEX, V5, P261, DOI 10.1093/cercor/5.3.261; WEISS Y, 1995, NETWORK-COMP NEURAL, V6, P19, DOI 10.1088/0954-898X/6/1/002	58	18	18	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1999	33	3					201	228		10.1023/A:1008102413960	http://dx.doi.org/10.1023/A:1008102413960			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	248WM					2022-12-18	WOS:000083299800003
J	Gupta, N; Kanal, L				Gupta, N; Kanal, L			Gradient based image motion estimation without computing gradients	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						optical flow; Gauss's Divergence theorem; non-local constraint; numerical differentiation	OPTICAL-FLOW; VISUAL-MOTION; INFORMATION; ALGORITHM; SEQUENCES; VELOCITY	Computing an optical how field using the classical image motion constraint equation I(x)u+I(y)v+I-t-0, is difficult owing to the aperture problem and the need to compute the image intensity derivatives via numerical differentiation-an extremely unstable operation. We integrate the above constraint equation over a significant spatio-temporal support and use Gauss's Divergence theorem to replace the volume integrals by surface integrals, thereby eliminating the intensity derivatives and numerical differentiation. We tackle the aperture problem by fitting an affine flow field model to a small space-time window. Using this affine model our new integral motion constraint approach leads to a robust and accurate algorithm to compute the optical flow field. Extensive experimentation confirms that the algorithm is indeed robust and accurate.	LNK CORP INC,RIVERDALE,MD 20737		Gupta, N (corresponding author), UNIV MARYLAND,DEPT COMP SCI,COLLEGE PK,MD 20742, USA.							ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BURDEN R. L., 1978, NUMERICAL ANAL; FENNEMA CL, 1979, COMPUT VISION GRAPH, V9, P301, DOI 10.1016/0146-664X(79)90097-2; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; GLAZER F, 1987, THESIS U MASSACHUSET; GUPTA N, 1993, P AS C COMP VIS OS J; GUPTA N, 1993, THESIS U MARYLAND CO; GUPTA NC, 1995, ARTIF INTELL, V78, P45, DOI 10.1016/0004-3702(95)00031-3; HAY JC, 1960, PSYCHOL REV, V73, P550; HEEGER DJ, 1988, INT J COMPUT VISION, V1, P273; HILDRETH EC, 1984, ARTIF INTELL, V23, P309, DOI 10.1016/0004-3702(84)90018-3; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Kreyszig E., 1967, ADV ENG MATH; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Lucas B., 1984, THESIS CARNEGIE MELL; NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833; NAGEL NN, 1981, IEEE COMPUT, P29; NEGAHDARIPOUR S, 1992, INT J COMPUT VISION, V9, P163, DOI 10.1007/BF00133700; POGGIO T, 1976, Quarterly Reviews of Biophysics, V9, P377; RAGHAVAN S, 1992, P INT C PATT REC HAG; SCHUNCK BG, 1984, IEEE WORKSHOP COMPUT, P58; TSAI RY, 1981, IEEE T ACOUST SPEECH, V29, P1147, DOI 10.1109/TASSP.1981.1163710; VERRI A, 1987, 1ST P INT C COMP VIS, P171; WAXMAN AM, 1985, INT J ROBOT RES, V4, P95, DOI 10.1177/027836498500400307	27	18	20	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	1997	22	1					81	101		10.1023/A:1007931911114	http://dx.doi.org/10.1023/A:1007931911114			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WT108					2022-12-18	WOS:A1997WT10800004
J	Bruce, JW; Giblin, PJ; Tari, E				Bruce, JW; Giblin, PJ; Tari, E			Parabolic curves of evolving surfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						parabolic curve; surface; morphogenetic script		In this article we show how certain geometric structures which are also associated with a smooth surface evolve as the shape of the surface changes in a 1-parameter family. We concentrate on the parabolic set and its image under the Gauss map, but the same techniques also classify the changes in the dual of the surface. All these have significance for computer vision, for example through their connection with specularities and apparent contours. With the aid of our complete classification, which includes all the phenomena associated with multi-contact tangent planes as well as those associated with parabolic sets, we re-examine examples given by J. Koenderink in his book (1990) under the title of Morphological Scripts. We also explain some of the connections between parabolic sets and 'ridges' of a surface, where principal curvatures achieve turning values along lines of curvature. The point of view taken is the analysis of the contact between surfaces and their tangent planes. A systematic investigation of this yields the results using singularity theory. The mathematical details are suppressed here and appear in Bruce et al. (1993).			Bruce, JW (corresponding author), UNIV LIVERPOOL,DEPT PURE MATH,POB 147,LIVERPOOL L69 3BX,MERSEYSIDE,ENGLAND.							Arnol'd V.I., 1985, SINGULARITIES DIFFER, V1; Banchoff T., 1982, CUSPS GAUSS MAPPINGS; Bruce J.W., 1992, CURVES SINGULARITIES; BRUCE JW, 1994, J LOND MATH SOC, V49, P183, DOI 10.1112/jlms/49.1.183; BRUCE JW, 1986, P ROY SOC EDINB A, V104, P179, DOI 10.1017/S030821050001917X; BRUCE JW, 1995, REAL COMPLEX SINGULA, V333, P148; Hilbert D., 1952, GEOMETRY IMAGINATION; Koenderink J., 1990, SOLID SHAPE; PORTEOUS IR, 1994, IN PRESS GEOMETRIC D; RIEGER JH, 1993, GEOMETRIAE DEDICATA, V48, P211, DOI 10.1007/BF01264068	10	18	18	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1996	17	3					291	306		10.1007/BF00128235	http://dx.doi.org/10.1007/BF00128235			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	UB510					2022-12-18	WOS:A1996UB51000004
J	NAGEL, HH				NAGEL, HH			OPTICAL-FLOW ESTIMATION AND THE INTERACTION BETWEEN MEASUREMENT ERRORS AT ADJACENT PIXEL POSITIONS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							IMAGE SEQUENCES; MOVING-OBJECTS; MOTION; RECOVERY	In order to estimate both components of optical flow as well as their first spatio-temporal derivatives, it is postulated that the Optical Flow Constraint Equation (OFCE) is valid in a spatio-temporal neighborhood of pixels. So far, it has been tacitly assumed that the partial derivatives of the gray value distribution-which are required for this approach at the pixel positions involved-are independent from each other. It is shown how dropping this assumption affects the estimation procedure, based on well established approaches of estimation theory. The insight gained thereby is used to develop an approach towards merging image regions based on the compatibility of optical flow estimates obtained within the regions considered for merger.	FRAUNHOFER INST INFORMAT & DATENVERARBEITUNG IITB, D-76131 KARLSRUHE, GERMANY	Fraunhofer Gesellschaft	NAGEL, HH (corresponding author), UNIV KARLSRUHE, FAK INFORMAT, INST ALGORITHMEN & KOGNIT SYST, FRAUNHOFERSTR 1, D-76131 KARLSRUHE, GERMANY.							Barron J. L., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P236, DOI 10.1109/CVPR.1992.223269; Bergen J. R., 1991, Artificial Intelligence and Computer Vision. Proceedings of the Seventh Israeli Conference, P147; BOUTHEMY P, 1993, INT J COMPUT VISION, V10, P157, DOI 10.1007/BF01420735; BOUTHEMY P, 1987, 1ST P INT C COMP VIS, P463; BURT PJ, 1989, IEEE MOTION WORKSHOP, P2; BURT PJ, 1991, OCT IEEE WORKSH VIS, P187; CAMPANI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P90, DOI 10.1016/1049-9660(92)90088-K; CAMPANI M, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P22; CHOU WS, 1993, PATTERN RECOGN, V26, P351, DOI 10.1016/0031-3203(93)90043-V; DEMICHELI E, 1993, IEEE T PATTERN ANAL, V15, P434, DOI 10.1109/34.211464; Etoh M., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P192, DOI 10.1109/ICCV.1993.378220; Fleet DJ, 1992, MEASUREMENT IMAGE VE; Gu H., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P367, DOI 10.1109/CVPR.1993.341104; HARALICK RM, 1980, COMPUT VISION GRAPH, V12, P60, DOI 10.1016/0146-664X(80)90004-0; HEEGER DJ, 1987, INT J COMPUT VISION, V1, P279, DOI 10.1007/BF00133568; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HSU YZ, 1984, COMPUT VISION GRAPH, V26, P73, DOI 10.1016/0734-189X(84)90131-2; IRANI M, 1992, LECT NOTES COMPUT SC, V588, P282; Letang J. M., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P21, DOI 10.1109/ICCV.1993.378239; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; NAGEL HH, 1992, ARTIFICIAL BIOL VISI, P193; NAGEL HH, 1994, LECTURE NOTES COMPUT, V801, P305; Negahdaripour S., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P2, DOI 10.1109/ICCV.1993.378241; NEGAHDARIPOUR S, 1992, INT J COMPUT VISION, V9, P163, DOI 10.1007/BF00133700; PELEG S, 1990, 10TH P INT C PATT RE, V1, P109; Simoncelli E. P., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P310, DOI 10.1109/CVPR.1991.139707; SOCHER G, 1993, THESIS U KARLSRUHE; SPOERRI A, 1987, JUN P INT C COMP VIS, P209; SZELISKI R, 1990, INT J COMPUT VISION, V5, P271, DOI 10.1007/BF00126502; SZELISKI R, 1989, BAYESIAN MODELING UN; THOMPSON WB, 1990, INT J COMPUT VISION, V4, P39, DOI 10.1007/BF00137442; TORR PHS, 1993, IMAGE VISION COMPUT, V11, P180, DOI 10.1016/0262-8856(93)90034-E; TORR PHS, 1992, P BRIT MACHINE VISIO, P79; Wang J. Y. A., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P361, DOI 10.1109/CVPR.1993.341105; Weber J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P12, DOI 10.1109/ICCV.1993.378240	36	18	18	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	1995	15	3					271	288		10.1007/BF01451744	http://dx.doi.org/10.1007/BF01451744			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RM520					2022-12-18	WOS:A1995RM52000005
J	THEODORACATOS, VE; CALKINS, DE				THEODORACATOS, VE; CALKINS, DE			A 3-D VISION SYSTEM MODEL FOR AUTOMATIC OBJECT SURFACE SENSING	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							MACHINE VISION; CALIBRATION; CAMERA	The development of a noncontact ''light striping'' (structured light) based three-dimensional, six-degrees-of-freedom vision system for automatic object surface sensing is reported. The system modeling and world-point reconstruction methodology involve homogeneous-coordinate system transformations applied in two independent stages: the video imaging stage using three-dimensional perspective transformations and the mechanical scanning stage using three-dimensional affine transformations. Concatenation of the two independent matrix models leads to a robust four-by-four, six-degrees-of-freedom system model. The reconstructed sectional contours are registered automatically in real time with respect to a common world-coordinate system as a control net for Non-Uniform Rational B-spline (NURBS) surface approximation. The reconstruction process is demonstrated by measuring the surface of a 19.5 x 2 ft rowing shell. A detailed statistical accuracy and precision analysis shows an average error of 0.2% (0.002) of the camera's field-of-view. System sensitivity analysis reveals a nonlinear increase of sensitivity for angles higher than 45-degrees between the normals of the image and laser planes.	UNIV WASHINGTON,DEPT MECH ENGN,SEATTLE,WA 98195	University of Washington; University of Washington Seattle	THEODORACATOS, VE (corresponding author), UNIV OKLAHOMA,SCH AEROSP & MECH ENGN,NORMAN,OK 73019, USA.							MANSBACH P, 1986, COMPUT VISION GRAPH, V35, P200, DOI 10.1016/0734-189X(86)90027-7; POPPLESTONE RJ, 1975, 4TH P INT JOINT C AR, P664; POTMESIL M, 1979, P IEEE, V67, P553; POTMESIL M, 1983, 8TH P INT JOINT C AI, P1089; PRATT V, 1987, ACM SIGGRAPH, V21, P145; Sato Y., 1982, IEEE T PATTERN ANAL, p641~646; SATO Y, 1979, SYST COMPUT CONT, V10, P1; SOBEL I, 1973, 3RD INT JOINT C ART, P648; THEODORACATOS VE, 1968, THESIS U WASHINGTON; THEORDORACATOS VE, 1991, 1991 P ASME DES AUT; Tio J.B.K., 1983, P RPIP, p52~96; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; WILL PM, 1972, PR INST ELECTR ELECT, V60, P669, DOI 10.1109/PROC.1972.8726; WILL PM, 1971, ARTIF INTELL, V2, P319, DOI 10.1016/0004-3702(71)90015-4; Wolf P.R., 1974, ELEMENTS PHOTOGRAMME; [No title captured]; 1965, BASIC OPTICS OPTICAL	17	18	20	0	4	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1993	11	1					75	99		10.1007/BF01420594	http://dx.doi.org/10.1007/BF01420594			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LW796					2022-12-18	WOS:A1993LW79600005
J	Yuille, AL; Liu, CX				Yuille, Alan L.; Liu, Chenxi			Deep Nets: What have They Ever Done for Vision?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep neural networks; Computer vision; Success; Limitation; Cognitive science; Neuroscience	HIERARCHICAL-MODELS; BAYESIAN-INFERENCE; INTELLIGENCE; RECOGNITION; EXTRACTION; GRADIENTS; RESPONSES; SHAPE; CODE	This is an opinion paper about the strengths and weaknesses of Deep Nets for vision. They are at the heart of the enormous recent progress in artificial intelligence and are of growing importance in cognitive science and neuroscience. They have had many successes but also have several limitations and there is limited understanding of their inner workings. At present Deep Nets perform very well on specific visual tasks with benchmark datasets but they are much less general purpose, flexible, and adaptive than the human visual system. We argue that Deep Nets in their current form are unlikely to be able to overcome the fundamental problem of computer vision, namely how to deal with the combinatorial explosion, caused by the enormous complexity of natural images, and obtain the rich understanding of visual scenes that the human visual achieves. We argue that this combinatorial explosion takes us into a regime where "big data is not enough" and where we need to rethink our methods for benchmarking performance and evaluating vision algorithms. We stress that, as vision algorithms are increasingly used in real world applications, that performance evaluation is not merely an academic exercise but has important consequences in the real world. It is impractical to review the entire Deep Net literature so we restrict ourselves to a limited range of topics and references which are intended as entry points into the literature. The views expressed in this paper are our own and do not necessarily represent those of anybody else in the computer vision community.	[Yuille, Alan L.; Liu, Chenxi] Johns Hopkins Univ, Baltimore, MD 21218 USA	Johns Hopkins University	Liu, CX (corresponding author), Johns Hopkins Univ, Baltimore, MD 21218 USA.	alan.l.yuille@gmail.com; cxliu@jhu.edu		Yuille, Alan L./0000-0001-5207-9249	Center for Brains, Minds and Machines (CBMM) - NSF STC Award [CCF-1231216, ONR N00014-15-1-2356]	Center for Brains, Minds and Machines (CBMM) - NSF STC Award	This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC Award CCF-1231216 and ONR N00014-15-1-2356. We thank Kyle Rawlins, Tal Linzen, Wei Shen, and Adam Kortylewski for providing feedback and Weichao Qiu, Daniel Kersten, Ed Connor, Chaz Firestone, Vicente Ordonez, and Greg Hager for discussions on some of these topics. We thank the reviewers for some very helpful feedback which greatly improved the paper.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Alcorn M. A., 2019, PROC CVPR IEEE, P4845; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbib MA, 2016, COMPUT NEUROSCI-MIT, P1; Arterberry M.E., 2016, DEV PERCEPTION INFAN; Athalye A, 2018, PR MACH LEARN RES, V80; Barlow H, 1997, J NEUROSCI, V17, P7954; Barrett D.G.T., 2018, ARXIV180704225; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Bland Lucy, 2010, OXFORD HDB HIST EUGE, P215; Bowyer KW., 1999, CVPR, P1354; Boyden ES, 2005, NAT NEUROSCI, V8, P1263, DOI 10.1038/nn1525; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chang Angel X., 2015, ARXIV151203012CSGR P; Changizi M, 2010, VISION REVOLUTION LA; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen X., 2014, P 27 ANN C NEURAL IN, P1736, DOI DOI 10.1109/CVPR.2018.00742; Chen XJ, 2015, PROC CVPR IEEE, P3945, DOI 10.1109/CVPR.2015.7299020; Chen Y., 2007, NIPS, V20, P289; Chomsky N., 1965, ASPECTS THEORY SYNTA; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cichy RM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27755; Clune J, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2012.2863; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Darwiche A, 2018, COMMUN ACM, V61, P56, DOI 10.1145/3271625; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Eigen David, 2014, NEURIPS; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Firestone C, 2020, P NATL ACAD SCI USA, V117, P26562, DOI 10.1073/pnas.1905334117; Gao C, 2018, PLANT BIOLOGY, V20, P3, DOI 10.1111/plb.12633; Gao S, 2019, 2019 2ND IEEE INTERNATIONAL CONFERENCE ON INFORMATION COMMUNICATION AND SIGNAL PROCESSING (ICICSP), P185, DOI 10.1109/ICICSP48821.2019.8958569; Geisler WS, 2011, VISION RES, V51, P771, DOI 10.1016/j.visres.2010.09.027; Geman S, 2007, GRAMMAR VISION PROBA; George D, 2017, SCIENCE, V358, DOI 10.1126/science.aag2612; Gibson J.J., 1979, ECOLOGICAL APPROACH; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopnik A., 1999, SCI CRIB MINDS BRAIN; Green D. M., 1966, SIGNAL DETECTION THE, DOI DOI 10.1086/405615; Gregoriou GG, 2014, NAT NEUROSCI, V17, P1003, DOI 10.1038/nn.3742; GREGORY RL, 1973, EYE BRAIN PSYCHOL SE; Grenander U., 1993, GEN PATTERN THEORY M; Guu K, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1051, DOI 10.18653/v1/P17-1097; Guzman, 1968, P DEC 9 11 1968 FA 1, P291, DOI [10.1145/1476589.1476631, DOI 10.1145/1476589.1476631]; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; He Kaiming, 2019, ABS191105722 ARXIV; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Jabr F, 2012, CONNECTOME DEBATE IS; Jegou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156; Julesz B., 1971, FDN CYCLOPEAN PERCEP; Kaushik Divyansh, 2020, ICLR; Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579; Konishi S., 1999, CVPR, P1573; Kortylewski A., 2020, ABS200304490 CORR; Kortylewski A, 2020, IEEE WINT CONF APPL, P1322, DOI 10.1109/WACV45572.2020.9093560; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434; Lin X., 2017, INT C LEARN REPR; Liu C., 2020, ABS200312056 CORR; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; LIU ZL, 1995, VISION RES, V35, P549, DOI 10.1016/0042-6989(94)00150-K; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu H., 2005, NIPS, V18, P827; Lyu J., 2019, ABS191206314 CORR; Madry Aleksander, 2017, ARXIV; Mao JH, 2015, IEEE I CONF COMP VIS, P2533, DOI 10.1109/ICCV.2015.291; Marcus Gary, 2018, ARXIV; Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; McManus JNJ, 2011, P NATL ACAD SCI USA, V108, P9739, DOI 10.1073/pnas.1105855108; Mengistu H, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004829; Mirza M., 2014, ARXIV; Mu J., 2019, ABS191208265 CORR; MUMFORD D, 1994, PROG MATH, V119, P187; Mumford D., 2010, PATTERN THEORY STOCH; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pearl J., 2009, CAUSALITY, DOI DOI 10.1017/CBO9780511803161; Penn DC, 2008, BEHAV BRAIN SCI, V31, P109, DOI 10.1017/S0140525X08003543; Pham H, 2018, PR MACH LEARN RES, V80; Poirazi P, 2001, NEURON, V29, P779, DOI 10.1016/S0896-6273(01)00252-5; Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755; Qiu WC, 2016, LECT NOTES COMPUT SC, V9915, P909, DOI 10.1007/978-3-319-49409-8_75; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren Z, 2017, AAAI CONF ARTIF INTE, P1495; Rensink RA, 1997, PSYCHOL SCI, V8, P368, DOI 10.1111/j.1467-9280.1997.tb00427.x; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Rosenfeld A., 2018, ABS180803305 CORR; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russell S., 2010, ARTIF INTELL, DOI DOI 10.1136/gutjnl-2018-317500; Sabour Sara, 2017, PROC 31 INT C NEURAL; Salakhutdinov R., 2012, ICML WORKSH UNS TRAN; Seung H. S., 2012, CONNECTOME BRAINS WI; Shen W, 2017, IEEE T IMAGE PROCESS, V26, P5298, DOI 10.1109/TIP.2017.2735182; Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212; Shu M, 2020, AAAI CONF ARTIF INTE, V34, P11998; Simons DJ, 1999, PERCEPTION, V28, P1059, DOI 10.1068/p2952; Smirnakis SM, 1995, NEUROBIOLOGY OF COMPUTATION, P427; Smith L, 2005, ARTIF LIFE, V11, P13, DOI 10.1162/1064546053278973; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; TJAN BS, 1995, VISION RES, V35, P3053, DOI 10.1016/0042-6989(95)00070-G; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tsipras Dimitris, 2019, ROBUSTNESS MAY BE OD, V1, P2; Tu ZW, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P18, DOI 10.1109/ICCV.2003.1238309; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Uesato J, 2018, PR MACH LEARN RES, V80; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang J., 2015, ABS151106855 CORR; Wang P, 2016, LECT NOTES COMPUT SC, V9905, P545, DOI 10.1007/978-3-319-46448-0_33; Wang TL, 2019, IEEE I CONF COMP VIS, P5309, DOI 10.1109/ICCV.2019.00541; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Wen HG, 2018, CEREB CORTEX, V28, P4136, DOI 10.1093/cercor/bhx268; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Xia FT, 2016, LECT NOTES COMPUT SC, V9909, P648, DOI 10.1007/978-3-319-46454-1_39; Xia Y., 2020, MOL PSYCHIATR; Xie CY, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2809731; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; XU L, 1994, NEURAL NETWORKS, V7, P609, DOI 10.1016/0893-6080(94)90040-X; Yamane Y, 2008, NAT NEUROSCI, V11, P1352, DOI 10.1038/nn.2202; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Yang C., 2020, ABS200405682 CORR; Yosinski J., 2015, ICML DEEP LEARN WORK; YUILLE A, 2016, J MACHINE LEARNING R, V17, P292; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002; Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zendel O, 2015, IEEE I CONF COMP VIS, P2066, DOI 10.1109/ICCV.2015.239; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang Y, 2018, INT CONF 3D VISION, P228, DOI 10.1109/3DV.2018.00035; Zhang ZS, 2020, IEEE WINT CONF APPL, P1350, DOI 10.1109/WACV45572.2020.9093445; Zhou Bolei, 2015, OBJECT DETECTORS EME, P2; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zhou ZL, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08931-6; Zhu H., 2019, COGSCI, P3213; Zhu L, 2010, PROC CVPR IEEE, P1919; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018; Zhu ZT, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3609; Zitnick CL, 2016, AI MAG, V37, P63, DOI 10.1609/aimag.v37i1.2647; Zoph B., 2017, P1	159	17	18	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					781	802		10.1007/s11263-020-01405-z	http://dx.doi.org/10.1007/s11263-020-01405-z		NOV 2020	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	QT6QC		Green Submitted			2022-12-18	WOS:000593419400001
J	Teng, SZ; Zhang, SL; Huang, QM; Sebe, N				Teng, Shangzhi; Zhang, Shiliang; Huang, Qingming; Sebe, Nicu			Viewpoint and Scale Consistency Reinforcement for UAV Vehicle Re-Identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Vehicle re-identification; UAV; Viewpoint; Scale		This paper studies vehicle ReID in aerial videos taken by Unmanned Aerial Vehicles (UAVs). Compared with existing vehicle ReID tasks performed with fixed surveillance cameras, UAV vehicle ReID is still under-explored and could be more challenging, e.g., aerial videos have dynamic and complex backgrounds, different vehicles show similar appearance, and the same vehicle commonly show distinct viewpoints and scales. To facilitate the research on UAV vehicle ReID, this paper contributes a novel dataset called UAV-VeID. UAV-VeID contains 41,917 images of 4601 vehicles captured by UAVs, where each vehicle has multiple images taken from different viewpoints. UAV-VeID also includes a large-scale distractor set to encourage the research on efficient ReID schemes. Compared with existing vehicle ReID datasets, UAV-VeID exhibits substantial variances in viewpoints and scales of vehicles, thus requires more robust features. To alleviate the negative effects of those variances, this paper also proposes a viewpoint adversarial training strategy and a multi-scale consensus loss to promote the robustness and discriminative power of learned deep features. Extensive experiments on UAV-VeID show our approach outperforms recent vehicle ReID algorithms. Moreover, our method also achieves competitive performance compared with recent works on existing vehicle ReID datasets including VehicleID, VeRi-776 and VERI-Wild.	[Teng, Shangzhi; Huang, Qingming] Univ Chinese Acad Sci, Beijing, Peoples R China; [Zhang, Shiliang] Peking Univ, Dept Comp Sci, Beijing, Peoples R China; [Sebe, Nicu] Univ Trento, Trento, Italy	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Peking University; University of Trento	Zhang, SL (corresponding author), Peking Univ, Dept Comp Sci, Beijing, Peoples R China.	shangzhi.teng@vipl.ict.ac.cn; slzhang.jdl@pku.edu.cn; qmhuang@ucas.ac.cn; sebe@disi.unitn.it	Teng, Shangzhi/AAN-9030-2021	Sebe, Niculae/0000-0002-6597-7248	National Natural Science Foundation of China [61620106009, U20B2052, 61936011, 61931008, 61836002]; Italy-China collaboration project TALENT [2018YFE0118400]; Beijing Natural Science Foundation [JQ18012]; Key Research Program of Frontier Sciences, CAS [QYZDJ-SSW-SYS013]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Italy-China collaboration project TALENT; Beijing Natural Science Foundation(Beijing Natural Science Foundation); Key Research Program of Frontier Sciences, CAS	This work is supported in part by National Natural Science Foundation of China under Grant Nos. 61620106009, U20B2052, 61936011, 61931008 and 61836002, in part by the Italy-China collaboration project TALENT 2018YFE0118400, in part by Beijing Natural Science Foundation under Grant No. JQ18012, in part by Key Research Program of Frontier Sciences, CAS: QYZDJ-SSW-SYS013.	Avola D, 2020, IEEE T SYST MAN CY-S, V50, P2139, DOI 10.1109/TSMC.2018.2804766; Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240; Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Du DW, 2018, LECT NOTES COMPUT SC, V11214, P375, DOI 10.1007/978-3-030-01249-6_23; Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10; Ganin Yaroslav, 2015, ICML; Girisha S, 2019, IEEE ACCESS, V7, P136239, DOI 10.1109/ACCESS.2019.2941026; Guo HY, 2018, AAAI CONF ARTIF INTE, P6853; HAN B, 2018, PROC CVPR IEEE, V31; He B., 2019, CVPR; He Junjun, 2019, ICCV; Hsieh MR, 2017, IEEE I CONF COMP VIS, P4165, DOI 10.1109/ICCV.2017.446; Huang Gao, 2018, ICLR; Kanac A., 2017, BMVC; Kanac A., 2018, GCPR; Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801; Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186; Liu WY, 2016, PR MACH LEARN RES, V48; Liu X., 2018, ICME; Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53; Liu X, 2019, PROCEEDINGS OF THE ASME PRESSURE VESSELS AND PIPING CONFERENCE, 2019, VOL 3; Long MS, 2018, ADV NEUR IN, V31; Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335; Lu JS, 2016, ADV NEUR IN, V29; Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27; Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934; Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Robicquet A, 2016, LECT NOTES COMPUT SC, V9912, P549, DOI 10.1007/978-3-319-46484-8_33; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Selvaraju Ramprasaath R, 2017, ICCV; Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan Weimin, 2018, CVPR; Teng S., 2018, PCM; Teng S., 2020, TCSVT; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Wang D, 2020, PROC CVPR IEEE, P3950, DOI 10.1109/CVPR42600.2020.00401; Wang XG, 2019, PROC CVPR IEEE, P8868, DOI [10.1109/CVPR.2019.00908, 10.1109/CVPR.2019.00267]; Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Yan K, 2017, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2017.68; Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023; Yao HT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P342, DOI 10.1145/3123266.3123278; Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94; Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380; Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679; Zhu Jun-Yan, 2017, ICCV; Zhu P., 2020, ABS200106303 CORR, P1; Zhu Pengfei, 2018, ARXIV180407437, P2; Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064	60	17	17	8	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					719	735		10.1007/s11263-020-01402-2	http://dx.doi.org/10.1007/s11263-020-01402-2		NOV 2020	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC					2022-12-18	WOS:000592144700001
J	Nie, D; Shen, D				Nie, Dong; Shen, Dinggang			Adversarial Confidence Learning for Medical Image Segmentation and Synthesis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Adversarial confidence learning; Medical image analysis; Segmentation; Image synthesis		Generative adversarial networks (GAN) are widely used in medical image analysis tasks, such as medical image segmentation and synthesis. In these works, adversarial learning is directly applied to the original supervised segmentation (synthesis) networks. The usage of adversarial learning is effective in improving visual perception performance since adversarial learning works as realistic regularization for supervised generators. However, the quantitative performance often cannot improve as much as the qualitative performance, and it can even become worse in some cases. In this paper, we explore how we can take better advantage of adversarial learning in supervised segmentation (synthesis) models and propose an adversarial confidence learning framework to better model these problems. We analyze the roles of discriminator in the classic GANs and compare them with those in supervised adversarial systems. Based on this analysis, we propose adversarial confidence learning, i.e., besides the adversarial learning for emphasizing visual perception, we use the confidence information provided by the adversarial network to enhance the design of supervised segmentation (synthesis) network. In particular, we propose using a fully convolutional adversarial network for confidence learning to provide voxel-wise and region-wise confidence information for the segmentation (synthesis) network. With these settings, we propose a difficulty-aware attention mechanism to properly handle hard samples or regions by taking structural information into consideration so that we can better deal with the irregular distribution of medical data. Furthermore, we investigate the loss functions of various GANs and propose using the binary cross entropy loss to train the proposed adversarial system so that we can retain the unlimited modeling capacity of the discriminator. Experimental results on clinical and challenge datasets show that our proposed network can achieve state-of-the-art segmentation (synthesis) accuracy. Further analysis also indicates that adversarial confidence learning can both improve the visual perception performance and the quantitative performance.	[Nie, Dong] Univ North Carolina Chapel Hill, Dept Comp Sci, Chapel Hill, NC 27514 USA; [Nie, Dong; Shen, Dinggang] Univ North Carolina Chapel Hill, Dept Radiol, Chapel Hill, NC 27514 USA; [Nie, Dong; Shen, Dinggang] Univ North Carolina Chapel Hill, BRIC, Chapel Hill, NC 27514 USA; [Shen, Dinggang] Korea Univ, Dept Brain & Cognit Engn, Seoul 02841, South Korea	University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina School of Medicine; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina School of Medicine; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina School of Medicine; Korea University	Nie, D (corresponding author), Univ North Carolina Chapel Hill, Dept Comp Sci, Chapel Hill, NC 27514 USA.; Nie, D; Shen, D (corresponding author), Univ North Carolina Chapel Hill, Dept Radiol, Chapel Hill, NC 27514 USA.; Nie, D; Shen, D (corresponding author), Univ North Carolina Chapel Hill, BRIC, Chapel Hill, NC 27514 USA.; Shen, D (corresponding author), Korea Univ, Dept Brain & Cognit Engn, Seoul 02841, South Korea.	dongnie@cs.unc.edu; dgshen@med.unc.edu	Shen, Dinggang/ABF-6812-2020	Shen, Dinggang/0000-0002-7934-5698	National Institutes of Health [R01 CA206100]	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was supported by the National Institutes of Health under Grant R01 CA206100.	Alexandre-Lecalir L, 2018, PROC INT CON GENDER, P10; [Anonymous], 2019, ARXIV190208128; Arjovsky M, 2017, PR MACH LEARN RES, V70; Chaitanya K, 2019, LECT NOTES COMPUT SC, V11492, P29, DOI 10.1007/978-3-030-20351-1_3; Chen C, 2018, LECT NOTES COMPUT SC, V11046, P143, DOI 10.1007/978-3-030-00919-9_17; Chen H, 2016, PROC CVPR IEEE, P2487, DOI 10.1109/CVPR.2016.273; Chen LX, 2019, NURS CRIT CARE, V24, P362, DOI 10.1111/nicc.12357; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Costa P, 2018, IEEE T MED IMAGING, V37, P781, DOI 10.1109/TMI.2017.2759102; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo YR, 2016, IEEE T MED IMAGING, V35, P1077, DOI 10.1109/TMI.2015.2508280; Han X, 2017, MED PHYS, V44, P1408, DOI 10.1002/mp.12155; Hardy C., 2018, ARXIV181103850; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang YW, 2017, PROC CVPR IEEE, P5787, DOI 10.1109/CVPR.2017.613; Hung Wei-Chih, 2018, ARXIV180207934; Huo YK, 2018, I S BIOMED IMAGING, P1217; Gulrajani I, 2017, ADV NEUR IN, V30; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kohl S., 2017, ARXIV170208014; Kumar P, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P48, DOI 10.1109/SSCI.2018.8628895; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin Tsung-Yi, 2017, ARXIV170802002, P2980, DOI [DOI 10.1109/ICCV.2017.324, DOI 10.1109/TPAMI.2018.2858826]; Litjens G, 2014, MED IMAGE ANAL, V18, P359, DOI 10.1016/j.media.2013.12.002; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luc P., 2016, NIPS WORKSHOP ADVERS; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mathieu Michael, 2015, ARXIV151105440; Menze Bjoern H, 2015, IEEE Trans Med Imaging, V34, P1993, DOI 10.1109/TMI.2014.2377694; Merkow Jameson, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9902, P371, DOI 10.1007/978-3-319-46726-9_43; Mescheder L, 2018, PR MACH LEARN RES, V80; Metz Luke, 2016, ARXIV161102163; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Moeskops P., 2017, ARXIV170703195; Mordido G., 2018, ARXIV180711346; Nie D., 2019, AAAI; Nie D, 2019, IEEE T NEUR NET LEAR, V30, P1552, DOI 10.1109/TNNLS.2018.2870182; Nie D, 2018, IEEE T BIO-MED ENG, V65, P2720, DOI 10.1109/TBME.2018.2814538; Nie Dong, 2017, Med Image Comput Comput Assist Interv, V10435, P417, DOI 10.1007/978-3-319-66179-7_48; Nie D, 2016, LECT NOTES COMPUT SC, V10008, P170, DOI 10.1007/978-3-319-46976-8_18; Oktay O., 2018, ARXIV E PRINTS ARXIV, DOI [10.48550/arXiv.1804.03999, DOI 10.48550/ARXIV.1804.03999]; Pan TX, 2017, AAAI CONF ARTIF INTE, P4240; Qi GJ, 2020, INT J COMPUT VISION, V128, P1118, DOI 10.1007/s11263-019-01265-2; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roy A. G., 2015, MICCAI; Sabokrou M., 2018, ACCV; Shi F, 2015, IEEE T MED IMAGING, V34, P2459, DOI 10.1109/TMI.2015.2437894; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262; Wolterink JM, 2017, IEEE T MED IMAGING, V36, P2536, DOI 10.1109/TMI.2017.2708987; Xiao H., 2017, ARXIV171106828; Xue Y, 2019, LECT NOTES COMPUT SC, V11764, P387, DOI 10.1007/978-3-030-32239-7_43; Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462; Yang X, 2017, AAAI CONF ARTIF INTE, P1633; Yu LQ, 2017, AAAI CONF ARTIF INTE, P66; Zhang Y., 2017, PROC INT C MED IMAGE, P408; Zhang ZZ, 2018, PROC CVPR IEEE, P9242, DOI 10.1109/CVPR.2018.00963; Zhou Xiao-Yun, 2017, ARXIV171101506; Zhu WT, 2018, I S BIOMED IMAGING, P847	68	17	17	1	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2494	2513		10.1007/s11263-020-01321-2	http://dx.doi.org/10.1007/s11263-020-01321-2		MAR 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY	34149167	Green Accepted			2022-12-18	WOS:000564965100001
J	Changpinyo, S; Chao, WL; Gong, BQ; Sha, F				Changpinyo, Soravit; Chao, Wei-Lun; Gong, Boqing; Sha, Fei			Classifier and Exemplar Synthesis for Zero-Shot Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Zero-shot learning; Generalized zero-shot learning; Transfer learning; Object recognition; Semantic embeddings; Evaluation metrics	DATABASE	Zero-shot learning (ZSL) enables solving a task without the need to see its examples. In this paper, we propose two ZSL frameworks that learn to synthesize parameters for novel unseen classes. First, we propose to cast the problem of ZSL as learning manifold embeddings from graphs composed of object classes, leading to a flexible approach that synthesizes "classifiers" for the unseen classes. Then, we define an auxiliary task of synthesizing "exemplars" for the unseen classes to be used as an automatic denoising mechanism for any existing ZSL approaches or as an effective ZSL model by itself. On five visual recognition benchmark datasets, we demonstrate the superior performances of our proposed frameworks in various scenarios of both conventional and generalized ZSL. Finally, we provide valuable insights through a series of empirical analyses, among which are a comparison of semantic representations on the full ImageNet benchmark as well as a comparison of metrics used in generalized ZSL. Our code and data are publicly available at .	[Changpinyo, Soravit] Google AI, Los Angeles, CA 90095 USA; [Chao, Wei-Lun] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA; [Gong, Boqing] Google, Seattle, WA USA; [Sha, Fei] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90007 USA	Cornell University; Google Incorporated; University of Southern California	Changpinyo, S (corresponding author), Google AI, Los Angeles, CA 90095 USA.	schangpi@google.com; weilunchao760414@gmail.com; boqinggo@outlook.com; feisha@usc.edu		Changpinyo, Soravit/0000-0002-4013-1190	USC Graduate Fellowships; NSF [IIS-1065243, 1451412, 1513966/1632803/1833137, 1208500, CCF-1139148]; Google Research Award; Alfred P. Sloan Research Fellowship; ARO [W911NF-12-1-0241, W911NF-15-1-0484]	USC Graduate Fellowships; NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated); Alfred P. Sloan Research Fellowship(Alfred P. Sloan Foundation); ARO	This work is partially supported by USC Graduate Fellowships, NSF IIS-1065243, 1451412, 1513966/1632803/1833137, 1208500, CCF-1139148, a Google Research Award, an Alfred P. Sloan Research Fellowship, gifts from Facebook and Netflix, and ARO#W911NF-12-1-0241 and W911NF-15-1-0484.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Akata Z., 2013, P IEEE C COMP VIS PA; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Al-Halah Z., 2015, WACV; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2018, PAMI; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bucher M., 2018, RFIAP; Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Chen C.-Y., 2014, CVPR; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duan Kun, 2012, CVPR; Elhoseiny M., 2013, ICCV; Farhadi A., 2009, CVPR; Frome Andrea, 2013, NEURIPS; Fu Y., 2014, ECCV; Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Gan C, 2015, AAAI CONF ARTIF INTE, P3769; Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17; Garcia S, 2008, J MACH LEARN RES, V9, P2677; Gavves E., 2015, ICCV; Hinton GE., 2002, NIPS, V15, P833; Jayaraman D., 2014, NIPS; Jayaraman D., 2014, CVPR; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kampffmeyer Michael, 2019, CVPR, P11487; Karessli N., 2017, CVPR; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Li X., 2015, ICCV; Long Y., 2017, IEEE C COMP VIS PATT; Lu Y., 2016, IJCAI; Mansimov E., 2016, P INT C LEARN REPR; Mensink T., 2014, IEEE C COMP VIS PATT; Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83; Mikolov T., 2013, ARXIV; Mikolov T., 2013, 9 INT C LEARN REPR, P1; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Morgado P., 2017, CVPR; Norouzi Mohammad, 2014, ICLR; Palatucci M., 2009, ADV NEURAL INFORM PR, V22; Parikh D., 2011, CVPR; Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Peters J, 2017, ADAPT COMPUT MACH LE; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Reed S. E., 2016, CVPR; Reed S, 2016, PR MACH LEARN RES, V48; Ristin M, 2016, IEEE T PATTERN ANAL, V38, P490, DOI 10.1109/TPAMI.2015.2459678; Rohrbach M., 2010, CVPR; Rohrbach M, 2011, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2011.5995627; Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salakhutdinov R., 2011, CVPR; Socher Richard, 2013, NEURIPS; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Horn Grant, 2017, ARXIV170901450; Verma V.K., 2018, CVPR; Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wang Q, 2017, INT J COMPUT VISION, V124, P356, DOI 10.1007/s11263-017-1027-5; Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717; Xian Y, 2017, P IEEE C COMP VIS PA, P4582; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xu X, 2015, IEEE IMAGE PROC, P63, DOI 10.1109/ICIP.2015.7350760; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Yang Y., 2015, ICLR; Yu F. X., 2013, CVPR; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang Z., 2015, ICCV; Zhang Ziyu, 2016, P CVPR; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhu X., 2014, CVPR; Zhu Yizhe, 2018, CVPR	90	17	19	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2020	128	1					166	201		10.1007/s11263-019-01193-1	http://dx.doi.org/10.1007/s11263-019-01193-1			36	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KI6WA		Green Submitted			2022-12-18	WOS:000511490100007
J	Liu, ZC; Luo, WH; Wu, BY; Yang, X; Liu, W; Cheng, KT				Liu, Zechun; Luo, Wenhan; Wu, Baoyuan; Yang, Xin; Liu, Wei; Cheng, Kwang-Ting			Bi-Real Net: Binarizing Deep Network Towards Real-Network Performance	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						1-Bit CNNs; Binary convolution; Shortcut; 1-Layer-per-block	TRACKING	In this paper, we study 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the lacking of a representational capability and the training difficulty impede 1-bit CNNs from performing as well as real-valued networks. To this end, we propose Bi-Real net with a novel training algorithm to tackle these two challenges. To enhance the representational capability, we propagate the real-valued activations generated by each 1-bit convolution via a parameter-free shortcut. To address the training difficulty, we propose a training algorithm using a tighter approximation to the derivative of the sign function, a magnitude-aware binarization for weight updating, a better initialization method, and a two-step scheme for training a deep network. Experiments on ImageNet show that an 18-layer Bi-Real net with the proposed training algorithm achieves 56.4% top-1 classification accuracy, which is 10% higher than the state-of-the-arts (e.g., XNOR-Net), with a greater memory saving and a lower computational cost. Bi-Real net is also the first to scale up 1-bit CNNs to an ultra-deep network with 152 layers, and achieves 64.5% top-1 accuracy on ImageNet. A 50-layer Bi-Real net shows comparable performance to a real-valued network on the depth estimation task with merely a 0.3% accuracy gap.	[Liu, Zechun; Cheng, Kwang-Ting] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China; [Luo, Wenhan; Wu, Baoyuan; Liu, Wei] Tencent AI Lab, Shenzhen, Peoples R China; [Yang, Xin] Huazhong Univ Sci & Technol, Wuhan, Peoples R China	Hong Kong University of Science & Technology; Tencent; Huazhong University of Science & Technology	Yang, X (corresponding author), Huazhong Univ Sci & Technol, Wuhan, Peoples R China.	liuzechun0216@gmail.com; whluo.china@gmail.com; wubaoyuan1987@gmail.com; xinyang2014@hust.edu.cn; wl2223@columbia.edu; timcheng@ust.hk	Luo, Wenhan/GZL-0535-2022	Cheng, Kwang-Ting Tim/0000-0002-3885-4912; Liu, Wei/0000-0002-3865-8145	HKSAR RGC's funding [GRF-16203918]	HKSAR RGC's funding	The authors would like to acknowledge HKSAR RGC's funding support under Grant GRF-16203918. We also would like to thank Zhuoyi Bai, Tian Xia, Prof. Zhenyan Wang and Xiaofeng Hu from Huazhong University of Science and Technology for their efforts in implementing Bi-Real net on FPGA and carrying out the on-board speed estimation.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bagherinezhad H, 2018, ARXIV180502641; Baskin Chaim, 2018, ARXIV180410969; Chollet F, 2017, 161002357 ARXIV, P1610; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Ge CY, 2018, INT CONF CLOUD COMPU, P1072, DOI 10.1109/CCIS.2018.8691376; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Guo YW, 2016, ADV NEUR IN, V29; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hou L., 2017, P INT C LEARN REPR; Howard A.G., 2017, MOBILENETS EFFICIENT; Hubara I, 2018, J MACH LEARN RES, V18; Hubara I, 2016, ADV NEUR IN, V29; Iandola F.N., 2016, ARXIV; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lai L., 2017, ARXIV170303073; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Luo WH, 2018, PR MACH LEARN RES, V80; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740; Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1016/J.PATREC.2014.01.008; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tang W, 2017, AAAI CONF ARTIF INTE, P2625; Wu BY, 2017, PATTERN RECOGN, V64, P361, DOI 10.1016/j.patcog.2016.10.022; Wu BY, 2013, IEEE I CONF COMP VIS, P2856, DOI 10.1109/ICCV.2013.355; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xilinx, 2012, CISC VIS NETW IND GL, V5, P1; Yin JJ, 2018, IEEE INT C INT ROBOT, P365, DOI 10.1109/ICRIS.2018.00097; Zhang HW, 2017, IEEE I CONF COMP VIS, P4243, DOI 10.1109/ICCV.2017.454; Zhang Hanwang, 2017, PROC CVPR IEEE, P5532, DOI [DOI 10.1109/CVPR.2017.331, DOI 10.1109/CVPR.2018.00611]; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhao Y, 2017, ADVANCES IN ENERGY AND ENVIRONMENT RESEARCH, P345; Zhou A, 2017, INCREMENTAL NETWORK; Zhou AJ, 2018, PROC CVPR IEEE, P9426, DOI 10.1109/CVPR.2018.00982; Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58; Zhou S., 2016, ARXIV160606160; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23; Zhuang BH, 2018, PROC CVPR IEEE, P7920, DOI 10.1109/CVPR.2018.00826	51	17	17	3	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2020	128	1					202	219		10.1007/s11263-019-01227-8	http://dx.doi.org/10.1007/s11263-019-01227-8			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KI6WA		Green Submitted			2022-12-18	WOS:000511490100008
J	Zhao, Y; Xiong, YJ; Wang, LM; Wu, ZR; Tang, XO; Lin, DH				Zhao, Yue; Xiong, Yuanjun; Wang, Limin; Wu, Zhirong; Tang, Xiaoou; Lin, Dahua			Temporal Action Detection with Structured Segment Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Temporal action detection; Temporal action localization; Temporal action proposals; Human action recognition	ACTION RECOGNITION	This paper addresses an important and challenging task, namely detecting the temporal intervals of actions in untrimmed videos. Specifically, we present a framework called structured segment network (SSN). It is built on temporal proposals of actions. SSN models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and precise localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end manner. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping is devised to generate high quality action proposals. We further study the importance of the decomposed discriminative model and discover a way to achieve similar accuracy using a single classifier, which is also complementary with the original SSN design. On two challenging benchmarks, THUMOS'14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures.	[Zhao, Yue; Xiong, Yuanjun; Wu, Zhirong; Tang, Xiaoou; Lin, Dahua] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China; [Xiong, Yuanjun] Amazon Rekognit, Seattle, WA USA; [Wang, Limin] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China; [Wu, Zhirong] Microsoft Res Asia, Beijing, Peoples R China	Chinese University of Hong Kong; Nanjing University; Microsoft; Microsoft Research Asia	Zhao, Y (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.	zy317@ie.cuhk.edu.hk; bitxiong@gmail.com; lmwang.nju@gmail.com; xavibrowu@gmail.com; xtang@ie.cuhk.edu.hk; dhlin@ie.cuhk.edu.hk	Zhao, Yue/AGU-0409-2022	Zhao, Yue/0000-0003-2753-5921	Big Data Collaboration Research Grant from SenseTime Group (CUHK) [TS1610626]; General Research Fund (GRF) of Hong Kong [14236516]; Early Career Scheme (ECS) of Hong Kong [24204215]; National Science Foundation of China [61921006, 61321491]; Collaborative Innovation Center of Novel Software Technology and Industrialization	Big Data Collaboration Research Grant from SenseTime Group (CUHK); General Research Fund (GRF) of Hong Kong; Early Career Scheme (ECS) of Hong Kong; National Science Foundation of China(National Natural Science Foundation of China (NSFC)); Collaborative Innovation Center of Novel Software Technology and Industrialization	This work is partially supported by the Big Data Collaboration Research Grant from SenseTime Group (CUHK Agreement No. TS1610626), the General Research Fund (GRF) of Hong Kong (No. 14236516) and the Early Career Scheme (ECS) of Hong Kong (No. 24204215), the National Science Foundation of China (No. 61921006, No. 61321491), and Collaborative Innovation Center of Novel Software Technology and Industrialization. We thank Tianwei Lin for kindly providing the BSN proposals on THUMOS14.	Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754; [Anonymous], 2017, IEEE INT C COMP VIS; [Anonymous], 2017, P BMVC; [Anonymous], THUMOS ACTION RECOGN; [Anonymous], 2014, THUMOS ACTION RECOGN; [Anonymous], ABS160701979 CORR; [Anonymous], ACITIVITYNET LARGE S; [Anonymous], EUR C COMP VIS ECCV; [Anonymous], IEEE T PATTERN ANAL; [Anonymous], NIPS WORKSH LARGE SC; Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; De Geest R, 2016, LECT NOTES COMPUT SC, V9909, P269, DOI 10.1007/978-3-319-46454-1_17; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65; Gao J, 2017, BRIT MACH VIS C BMVC; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211; Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5; Hosang Jan, 2016, IEEE Trans Pattern Anal Mach Intell, V38, P814, DOI 10.1109/TPAMI.2015.2465908; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100; Jiang SS, 2019, IEEE INT C SEMANT CO, P7, DOI 10.1109/ICOSC.2019.8665522; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kratky H, 2018, INTERNATIONAL LOW IMPACT DEVELOPMENT CONFERENCE 2018: GETTING IN TUNE WITH GREEN INFRASTRUCTURE, P90; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343; Meng FS, 2018, PROCEEDINGS OF THE ASME TURBO EXPO: TURBOMACHINERY TECHNICAL CONFERENCE AND EXPOSITION, 2018, VOL 5C, P717; Mettes P, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P427, DOI 10.1145/2671188.2749404; Mettes P, 2016, LECT NOTES COMPUT SC, V9909, P437, DOI 10.1007/978-3-319-46454-1_27; Hoai M, 2011, PROC CVPR IEEE; Murakami Y, 2018, COGN TECHNOL, P3, DOI 10.1007/978-981-10-7793-7_1; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228; Pedregosa F., 2011, J MACH LEARN RES, V12, P2825; Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706; Pirsiavash H, 2014, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2014.85; Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richard A, 2016, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2016.341; Roerdink J. B. T. M., 2000, Fundamenta Informaticae, V41, P187; Schindler K, 2008, PROC CVPR IEEE, P3025; Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10; Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Simonyan K, 2014, ADV NEUR IN, V27; Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216; Soomro K., 2012, CRCVTR1201; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tang KV, 2013, IEEE I CONF COMP VIS, P2696, DOI 10.1109/ICCV.2013.335; van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456; vanGemert J., 2015, BMVC; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Wang LM, 2016, PROC CVPR IEEE, P2708, DOI 10.1109/CVPR.2016.296; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang LM, 2014, IEEE T IMAGE PROCESS, V23, P810, DOI 10.1109/TIP.2013.2295753; Wang P, 2017, IEEE T CIRC SYST VID, V27, P2613, DOI 10.1109/TCSVT.2016.2576761; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362; Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293; Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297; Zhang D., 2018, P BRIT MACH VIS C BM; Zhao Y, 2017, ARXIV171008011, V8; Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	88	17	18	8	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2020	128	1					74	95		10.1007/s11263-019-01211-2	http://dx.doi.org/10.1007/s11263-019-01211-2			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KI6WA		Green Submitted			2022-12-18	WOS:000511490100004
J	Khowaja, SA; Lee, SL				Khowaja, Sunder Ali; Lee, Seok-Lyong			Semantic Image Networks for Human Action Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion representation; Semantic information; Convolutional neural networks; Human action recognition; Long-short-term memory networks; Frame ranking		In this paper, we propose the use of a semantic image, an improved representation for video analysis, principally in combination with Inception networks. The semantic image is obtained by applying localized sparse segmentation using global clustering prior to the approximate rank pooling, which summarizes the motion characteristics in single or multiple images. It incorporates the background information by overlaying a static background from the window onto the subsequent segmented frames. The idea is to improve the action-motion dynamics by focusing on the region, which is important for action recognition and encoding the temporal variances using the frame ranking method. We also propose the sequential combination of Inception-ResNetv2 and long-short-term memory network (LSTM) to leverage the temporal variances for improved recognition performance. Extensive analysis has been carried out on UCF101 and HMDB51 datasets, which are widely used in action recognition studies. We show that (1) the semantic image generates better activations and converges faster than its original variant, (2) using segmentation prior to approximate rank pooling yields better recognition performance, (3) the use of LSTM leverages the temporal variance information from approximate rank pooling to model the action behavior better than the base network, (4) the proposed representations are adaptive as they can be used with existing methods such as temporal segment and I3D ImageNet+Kinetics network to improve the recognition performance, and (5) the four-stream network architecture pre-trained on ImageNet+Kinetics and fine-tuned using the proposed representation achieves the state-of-the-art performance, 99.1% and 83.7% recognition accuracy on UCF101 and HMDB51, respectively.	[Khowaja, Sunder Ali; Lee, Seok-Lyong] Hankuk Univ Foreign Studies, Dept Ind & Management Engn, Global Campus, Yongin, South Korea	Hankuk University Foreign Studies	Lee, SL (corresponding author), Hankuk Univ Foreign Studies, Dept Ind & Management Engn, Global Campus, Yongin, South Korea.	sandar.ali@usindh.edu.pk; sllee@hufs.ac.kr	Khowaja, Sunder Ali/N-2065-2019	Khowaja, Sunder Ali/0000-0002-4586-4131; Lee, Seok-Lyong/0000-0002-8630-5395	Hankuk University of Foreign Studies Research Fund [2019]; Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education [2018R1D1A1B07049113]	Hankuk University of Foreign Studies Research Fund; Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education(National Research Foundation of KoreaMinistry of Education (MOE), Republic of KoreaNational Research Council for Economics, Humanities & Social Sciences, Republic of Korea)	This research was supported by Hankuk University of Foreign Studies Research Fund (Grant No. 2019), and also supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2018R1D1A1B07049113).	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Ali S, 2010, IEEE T PATTERN ANAL, V32, P288, DOI 10.1109/TPAMI.2008.284; [Anonymous], 2017, DISTILL, DOI [10.23915/distill.00007, DOI 10.23915/DISTILL.00007]; [Anonymous], 2015, EXPLOITING IMAGE TRA; [Anonymous], VISUALIZING WHAT CON; [Anonymous], 2018, CIKM18 P 27 ACM INT, DOI DOI 10.1145/3269206.3271759; [Anonymous], VECTOR QUANTIZATION; [Anonymous], ADV NEURAL INFORM PR; [Anonymous], P BRIT MACH VIS C BR, DOI [10.5244/C.22.88, DOI 10.5244/C.22.88]; Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349; Bilen H, 2018, IEEE T PATTERN ANAL, V40, P2799, DOI 10.1109/TPAMI.2017.2769085; Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen YP, 2018, ADV NEUR IN, V31; Cheron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Diba A, 2017, PROC CVPR IEEE, P1541, DOI 10.1109/CVPR.2017.168; Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174; Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Feichtenhofer Christoph, 2016, P ADV NEUR INF PROC; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2015.7299173; Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Ke Y, 2005, IEEE I CONF COMP VIS, P166; King DB, 2015, ACS SYM SER, V1214, P1; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496; Likas A, 2003, PATTERN RECOGN, V36, P451, DOI 10.1016/S0031-3203(02)00060-2; Ma CY, 2019, SIGNAL PROCESS-IMAGE, V71, P76, DOI 10.1016/j.image.2018.09.003; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Pirsiavash H., 2009, P ADV NEUR INF PROC, P1482; de Souza CR, 2016, LECT NOTES COMPUT SC, V9911, P697, DOI 10.1007/978-3-319-46478-7_43; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Shechtman E, 2005, PROC CVPR IEEE, P405; Simons KA, 2014, ADV ACC EDUC-TEACH, V15, P1, DOI 10.1108/S1085-462220140000015001; Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Storath M, 2014, SIAM J IMAGING SCI, V7, P1826, DOI 10.1137/130950367; Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Tseng ZL, 2018, INT SYMP NEXTGEN, P369; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155; Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang LM, 2016, INT J COMPUT VISION, V119, P254, DOI 10.1007/s11263-015-0859-0; Wang Y., 2016, BIG DATA ANAL, V1, P12, DOI [10.1186/s41044-016-0010-4, DOI 10.1186/S41044-016-0010-4]; Wang YB, 2017, PROC CVPR IEEE, P2097, DOI 10.1109/CVPR.2017.226; Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231; Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110; Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219	76	17	17	1	38	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					393	419		10.1007/s11263-019-01248-3	http://dx.doi.org/10.1007/s11263-019-01248-3		OCT 2019	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KJ1GX		Green Submitted			2022-12-18	WOS:000494207200001
J	Tesfaye, YT; Zemene, E; Prati, A; Pelillo, M; Shah, M				Tesfaye, Yonatan Tariku; Zemene, Eyasu; Prati, Andrea; Pelillo, Marcello; Shah, Mubarak			Multi-target Tracking in Multiple Non-overlapping Cameras Using Fast-Constrained Dominant Sets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-target multi-camera tracking; Constrained dominant sets; Standard quadratic optimization	PERSON REIDENTIFICATION; ASSOCIATION	In this paper, a unified three-layer hierarchical approach for solving tracking problem in a multiple non-overlapping cameras setting is proposed. Given a video and a set of detections (obtained by any person detector), we first solve within-camera tracking employing the first two layers of our framework and then, in the third layer, we solve across-camera tracking by associating tracks of the same person in all cameras simultaneously. To best serve our purpose, we propose fast-constrained dominant set clustering (FCDSC), a novel method which is several orders of magnitude faster (close to real time) than existing methods. FCDSC is a parameterized family of quadratic programs that generalizes the standard quadratic optimization problem. In our method, we first build a graph where nodes of the graph represent short-tracklets, tracklets and tracks in the first, second and third layer of the framework, respectively. The edge weights reflect the similarity between nodes. FCDSC takes as input a constrained set, a subset of nodes from the graph which need to be included in the extracted cluster. Given a constrained set, FCDSC generates compact clusters by selecting nodes from the graph which are highly similar to each other and with elements in the constrained set. We have tested this approach on a very large and challenging dataset (namely, MOTchallenge DukeMTMC) and show that the proposed framework outperforms the state-of-the-art approaches. Even though the main focus of this paper is on multi-target tracking in non-overlapping cameras, the proposed approach can also be applied to solve video-based person re-identification problem. We show that when the re-identification problem is formulated as a clustering problem, FCDSC can be used in conjunction with state-of-the-art video-based re-identification algorithms, to increase their already good performances. Our experiments demonstrate the general applicability of the proposed framework for multi-target multi-camera tracking and person re-identification tasks.	[Tesfaye, Yonatan Tariku; Zemene, Eyasu; Shah, Mubarak] Univ Cent Florida, CRCV, Orlando, FL 32816 USA; [Tesfaye, Yonatan Tariku] IUAV Univ Venice, Venice, Italy; [Zemene, Eyasu] Ca Foscari Univ Venice, Venice, Italy; [Pelillo, Marcello] Ca Foscari Univ Venice, DAIS ECLT, Venice, Italy; [Prati, Andrea] Univ Parma, Dept Engn & Architecture, Parma, Italy	State University System of Florida; University of Central Florida; IUAV University Venice; Universita Ca Foscari Venezia; Universita Ca Foscari Venezia; University of Parma	Tesfaye, YT (corresponding author), Univ Cent Florida, CRCV, Orlando, FL 32816 USA.; Tesfaye, YT (corresponding author), IUAV Univ Venice, Venice, Italy.	yonitare@gmail.com; eyasu201011@gmail.com; andrea.prati@unipr.it; pelillo@unive.it; shah@eecs.ucf.edu			U.S. Army Research Laboratory; U.S. Army Research Office (ARO) [W911NF-14-1-0294]; Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA RD Contract [D17PC00345]	U.S. Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL)); U.S. Army Research Office (ARO); Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA RD Contract	This research is based upon work supported in parts by the U.S. Army Research Laboratory and the U.S. Army Research Office (ARO) under Contract/Grant No. W911NF-14-1-0294; and the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. D17PC00345. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.	[Anonymous], 2007, WILEY ENCY COMPUTER; [Anonymous], 2014, P AS C COMP VIS; Bulo SR, 2011, COMPUT VIS IMAGE UND, V115, P984, DOI 10.1016/j.cviu.2010.12.004; Cai YH, 2014, IEEE WINT CONF APPL, P761, DOI 10.1109/WACV.2014.6836026; Chen XT, 2014, PATTERN RECOGN, V47, P1126, DOI 10.1016/j.patcog.2013.06.011; Cheng D, 2017, NEUROCOMPUTING, V230, P30, DOI 10.1016/j.neucom.2016.11.038; D'Orazio T, 2009, 2009 THIRD ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS, P365; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; DEHGHAN A, 2015, PROC CVPR IEEE, P4091, DOI DOI 10.1109/CVPR.2015; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Gao Y, 2014, IEEE T CIRC SYST VID, V24, P1122, DOI 10.1109/TCSVT.2014.2302366; Gilbert A, 2006, LECT NOTES COMPUT SC, V3952, P125; Gong DNT, 2009, LECT NOTES COMPUT SC, V5716, P179, DOI 10.1007/978-3-642-04146-4_21; Grossman R, 2005, ACM SIGKDD INT C KNO; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Javed O, 2008, COMPUT VIS IMAGE UND, V109, P146, DOI 10.1016/j.cviu.2007.01.003; Klaser A., 2008, SPATIO TEMPORAL DESC, DOI 10.5244/C.22.99; Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939; Kuo CH, 2010, LECT NOTES COMPUT SC, V6311, P383; Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Liu HR, 2013, IEEE T PATTERN ANAL, V35, P2131, DOI 10.1109/TPAMI.2013.16; Ma BP, 2014, IMAGE VISION COMPUT, V32, P379, DOI 10.1016/j.imavis.2014.04.002; Maksai A, 2016, PROC CVPR IEEE, P972, DOI 10.1109/CVPR.2016.111; McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148; Pavan M., 2004, ADV NEURAL INFORM PR, V17, P1057; Pavan M, 2007, IEEE T PATTERN ANAL, V29, P167, DOI 10.1109/TPAMI.2007.250608; Prosser B., 2008, BRIT MACHINE VISION, P1; Rezatofighi SH, 2015, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2015.349; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Smeulders A. W. M., 2014, IEEE T PATTERN ANAL, V37, DOI DOI 10.1109/TPAMI.2013.230; Smith JM, 1988, DID DARWIN GET IT RI, P202, DOI [10.1007/978-1-4684-7862-4_22, DOI 10.1007/978-1-4684-7862-4_22]; Solera F, 2017, IEEE T CIRC SYST VID, V27, P441, DOI 10.1109/TCSVT.2016.2607378; Srivastava S, 2011, INT CONF ACOUST SPEE, P1821; Thomas A, 2007, IEEE I CONF COMP VIS, P23; Wang B, 2014, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2014.161; Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45; Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Yoon K, 2018, IET IMAGE PROCESS, V12, P1175, DOI 10.1049/iet-ipr.2017.1244; You JJ, 2016, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2016.150; YU SI, 2016, PROC CVPR IEEE, P3871, DOI DOI 10.1109/CVPR.2016.420; Zemene E, 2016, LECT NOTES COMPUT SC, V9912, P278, DOI 10.1007/978-3-319-46484-8_17; Zemene E, 2015, LECT NOTES COMPUT SC, V9279, P150, DOI 10.1007/978-3-319-23231-7_14; Zhang S, 2015, COMPUT VIS IMAGE UND, V134, P64, DOI 10.1016/j.cviu.2015.01.002; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133	51	17	19	2	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2019	127	9					1303	1320		10.1007/s11263-019-01180-6	http://dx.doi.org/10.1007/s11263-019-01180-6			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IL9YV					2022-12-18	WOS:000477642300007
J	Masi, I; Tran, AT; Hassner, T; Sahin, G; Medioni, G				Masi, Iacopo; Anh Tuan Tran; Hassner, Tal; Sahin, Gozde; Medioni, Gerard			Face-Specific Data Augmentation for Unconstrained Face Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face recognition; Deep learning; Data augmentation	VERIFICATION	We identify two issues as key to developing effective face recognition systems: maximizing the appearance variations of training images and minimizing appearance variations in test images. The former is required to train the system for whatever appearance variations it will ultimately encounter and is often addressed by collecting massive training sets with millions of face images. The latter involves various forms of appearance normalization for removing distracting nuisance factors at test time and making test faces easier to compare. We describe novel, efficient face-specific data augmentation techniques and show them to be ideally suited for both purposes. By using knowledge of faces, their 3D shapes, and appearances, we show the following: (a) We can artificially enrich training data for face recognition with face-specific appearance variations. (b) This synthetic training data can be efficiently produced online, thereby reducing the massive storage requirements of large-scale training sets and simplifying training for many appearance variations. Finally, (c) The same, fast data augmentation techniques can be applied at test time to reduce appearance variations and improve face representations. Together, with additional technical novelties, we describe a highly effective face recognition pipeline which, at the time of submission, obtains state-of-the-art results across multiple benchmarks. Portions of this paper were previously published by Masi et al.(European conference on computer vision, Springer, pp 579-596, 2016b, International conference on automatic face and gesture recognition, 2017).	[Masi, Iacopo] USC, ISI, Marina Del Rey, CA 90292 USA; [Anh Tuan Tran; Sahin, Gozde; Medioni, Gerard] USC, Inst Robot & Intelligent Syst, Los Angeles, CA USA; [Hassner, Tal] Open Univ Israel, Raanana, Israel	University of Southern California; University of Southern California; Open University Israel	Masi, I (corresponding author), USC, ISI, Marina Del Rey, CA 90292 USA.	iacopo@isi.edu; anhttran@usc.edu; talhassner@gmail.com; gsahin@usc.edu; medioni@usc.edu	Tran, Anh Tuan/GZB-0391-2022		Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) [2014-14071600011]; NVIDIA Corporation	Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA); NVIDIA Corporation	The authors wish to thank Jongmoo Choi for his help in this project. This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA 2014-14071600011. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon. Moreover, we gratefully acknowledge the support of NVIDIA Corporation with the donation of the NVIDIA Titan X GPU used for this research.	AbdAlmageed W, 2016, IEEE WINT CONF APPL; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2014, ARXIV PREPRINT ARXIV; Baltrusaitis T., 2013, P INT C COMP VIS WOR; Bansal A., 2017, INT JOINT C BIOM; Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544; Chandraker, 2018, ARXIV180309014; Chang Feng ju, 2017, 7 IEEE INT WORKSH AN; Chang FJ, 2019, INT J COMPUT VISION, V127, P930, DOI 10.1007/s11263-019-01151-x; Chang FJ, 2018, IEEE INT CONF AUTOMA, P122, DOI 10.1109/FG.2018.00027; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen J, 2015, IEEE ICC, P1801, DOI 10.1109/ICC.2015.7248586; Chen JC, 2016, IEEE WINT CONF APPL; Chowdhury Animesh R., 2016, 2016 IEEE International Conference on Plasma Science (ICOPS), DOI 10.1109/PLASMA.2016.7534285; Crispell D. E., 2016, APPL IM PATT REC WOR; Crosswhite N, 2018, IMAGE VISION COMPUT, V79, P35, DOI 10.1016/j.imavis.2018.09.002; Crosswhite N, 2017, IEEE INT CONF AUTOMA, P1, DOI 10.1109/FG.2017.11; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Ferrari C., 2017, P C COMP VIS PATT RE; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gunther M., 2017, INTERNATIONAL CONFER; Guo Y., 2016, ELECT IMAGING, V11; Hassner T., 2016, P INT C COMP VIS REC; Hassner T., 2015, P INT C COMP VIS REC; Hassner T, 2014, MACH VISION APPL, V25, P971, DOI 10.1007/s00138-013-0571-4; Hassner T, 2013, IEEE I CONF COMP VIS, P3607, DOI 10.1109/ICCV.2013.448; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hu J., 2015, COMPUTER VISION ACCV, P252; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Huang GB, 2007, 07 UMASS TR; Hughes John F, 2014, COMPUTER GRAPHICS PR, P5; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kemelmacher-Shlizerman I., 2016, P INT C COMP VIS PAT; KEMELMACHERSHLIZER, 2014, PROC CVPR IEEE, P3334, DOI DOI 10.1109/CVPR.2014.426; Kim KG, 2018, IEEE WINT CONF APPL, P39, DOI 10.1109/WACV.2018.00011; KLARE BF, 2015, PROC CVPR IEEE, P1931, DOI DOI 10.1109/CVPR.2015.7298803; Klontz J., 2013, INT C BIOM THEOR APP; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195; Levi G., 2015, P INT C COMP VIS PAT; Li H., 2014, AS C COMP VIS ACCV; Liu W., 2017, P INT C COMP VIS PAT; Liu X, 2017, IEEE INT CONF AUTOMA, P111, DOI 10.1109/FG.2017.22; Masi I., 2016, P INT C COMP VIS PAT; Masi I., 2013, P INT C COMP VIS PAT; Masi I., 2017, INT C AUT FAC GEST R; Masi I, 2019, IEEE T PATTERN ANAL, V41, P379, DOI 10.1109/TPAMI.2018.2792452; Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35; McLaughlin N., 2015, INT C ADV VID SIGN B; Mokhayeri F., 2018, ARXIV180101974; Neves J, 2019, IEEE T INF FOREN SEC, V14, P151, DOI 10.1109/TIFS.2018.2846617; Nguyen MH, 2008, COMPUT GRAPH FORUM, V27, P627, DOI 10.1111/j.1467-8659.2008.01160.x; Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024; Parkhi O. M., 2014, P INT C COMP VIS PAT; Parkhi O. M., 2015, P BRIT MACHN VIS C; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Ranjan R., 2018, ARXIV180907586 CORR; Ranjan R, 2017, IEEE INT CONF AUTOMA, P17, DOI 10.1109/FG.2017.137; Rashedi E, 2019, NEUROCOMPUTING, V329, P311, DOI 10.1016/j.neucom.2018.10.041; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Sankaranarayanan S., 2016, ARXIV160203418; Sankaranarayanan S, 2016, INT CONF BIOMETR THE; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092; Sun Y., 2015, ARXIV PREPRINT ARXIV; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0; Taigman Y., 2015, P C COMP VIS PATT RE; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tran A. T., 2018, P C COMP VIS PATT RE; Tran AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163; Wang D., 2015, ARXIV150707242; Wen YD, 2019, INT J COMPUT VISION, V127, P668, DOI 10.1007/s11263-018-01142-4; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87; Wolf L., 2011, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2011.5995566; Wolf L, 2011, IEEE T PATTERN ANAL, V33, P1978, DOI 10.1109/TPAMI.2010.230; Xie L., 2016, P C COMP VIS PATT RE; Xie S., 2015, P INT C COMP VIS; Xie SN, 2015, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2015.7298880; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Xu Z, 2015, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2015.290; Yager N, 2010, IEEE T PATTERN ANAL, V32, P220, DOI 10.1109/TPAMI.2008.291; Yang H., 2015, P C COMP VIS PATT RE; Yang J., 2017, P C COMP VIS PATT RE; Yang ZH, 2016, INT C PATT RECOG, P633, DOI 10.1109/ICPR.2016.7899705; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yin Q., 2015, NAIVE DEEP FACE RECO; Zhao J, 2017, ADV NEUR IN, V30; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	92	17	17	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		642	667		10.1007/s11263-019-01178-0	http://dx.doi.org/10.1007/s11263-019-01178-0			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD					2022-12-18	WOS:000468525900007
J	Maurer, D; Ju, YC; Breuss, M; Bruhn, A				Maurer, Daniel; Ju, Yong Chul; Breuss, Michael; Bruhn, Andres			Combining Shape from Shading and Stereo: A Joint Variational Method for Estimating Depth, Illumination and Albedo	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo reconstruction; Shape from Shading; Variational methods; Illumination estimation; Albedo estimation; Joint reasoning	HIGH-QUALITY SHAPE; SURFACE RECONSTRUCTION; 3-D RECONSTRUCTION; PERSPECTIVE SHAPE; IMAGE; OBJECTS; MODEL	Shape from shading (SfS) and stereo are two fundamentally different strategies for image-based 3-D reconstruction. While approaches for SfS infer the depth solely from pixel intensities, methods for stereo are based on a matching process that establishes correspondences across images. This difference in approaching the reconstruction problem yields complementary advantages that are worthwhile being combined. So far, however, most joint approaches are based on an initial stereo mesh that is subsequently refined using shading information. In this paper we follow a completely different approach. We propose a joint variational method that combines both cues within a single minimisation framework. To this end, we fuse a Lambertian SfS approach with a robust stereo model and supplement the resulting energy functional with a detail-preserving anisotropic second-order smoothness term. Moreover, we extend the resulting model in such a way that it jointly estimates depth, albedo and illumination. This in turn makes the approach applicable to objects with non-uniform albedo as well as to scenes with unknown illumination. Experiments for synthetic and real-world images demonstrate the benefits of our combined approach: They not only show that our method is capable of generating very detailed reconstructions, but also that joint approaches are feasible in practice.	[Maurer, Daniel; Ju, Yong Chul; Bruhn, Andres] Univ Stuttgart, Inst Visualizat & Interact Syst, Stuttgart, Germany; [Breuss, Michael] BTU Cottbus Senftenberg, Inst Appl Math & Sci Comp, Cottbus, Germany	University of Stuttgart; Brandenburg University of Technology Cottbus	Maurer, D (corresponding author), Univ Stuttgart, Inst Visualizat & Interact Syst, Stuttgart, Germany.	maurer@vis.uni-stuttgart.de; ju@vis.uni-stuttgart.de; breuss@b-tu.de; bruhn@vis.uni-stuttgart.de		Ju, Yong Chul/0000-0002-2454-282X	German Research Foundation (DFG) [BR 2245/3-1, BR 4372/1-1]; DFG [SFB/Transregio 161]	German Research Foundation (DFG)(German Research Foundation (DFG)); DFG(German Research Foundation (DFG))	This work has been partly funded by the German Research Foundation (DFG) within the joint Project BR 2245/3-1 and BR 4372/1-1. Moreover, we thank the DFG for financial support within Project B04 of SFB/Transregio 161.	Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148; Baillard C, 1999, COMPUT VIS IMAGE UND, V76, P244, DOI 10.1006/cviu.1999.0793; Basha T, 2013, INT J COMPUT VISION, V101, P6, DOI 10.1007/s11263-012-0542-7; Black M. J., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P296, DOI 10.1109/CVPR.1991.139705; BLAKE A, 1985, IMAGE VISION COMPUT, V3, P183, DOI 10.1016/0262-8856(85)90006-X; Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14; Boulanger J, 2011, SPRINGER SER OPTIM A, V49, P131, DOI 10.1007/978-1-4419-9569-8_8; Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521; Breuss M, 2012, SIAM J IMAGING SCI, V5, P311, DOI 10.1137/100815104; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Bruhn A, 2005, IEEE I CONF COMP VIS, P749; Camilli F, 2017, SIAM J IMAGING SCI, V10, P26, DOI 10.1137/16M1066397; Charbonnier P, 1997, IEEE T IMAGE PROCESS, V6, P298, DOI 10.1109/83.551699; Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37; CRYER JE, 1995, PATTERN RECOGN, V28, P1033, DOI 10.1016/0031-3203(94)00183-M; DIZENZO S, 1986, COMPUT VISION GRAPH, V33, P116, DOI 10.1016/0734-189X(86)90223-9; Forstner<spacing Wolfgang, 1987, ISPRS INT C FAST PRO, P2; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106; Gelfand I. M., 2000, CALCULUS VARIATIONS; Graber G, 2015, PROC CVPR IEEE, P511, DOI 10.1109/CVPR.2015.7298649; Hafner D, 2015, LECT NOTES COMPUT SC, V9358, P79, DOI 10.1007/978-3-319-24947-6_7; Haines T. S. F., 2007, P BRIT MACH VIS C; Han Y, 2013, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2013.204; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Horn B, 1970, THESIS; HOUGEN DR, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P991; Immel D. S., 1986, Computer Graphics, V20, P133, DOI 10.1145/15886.15901; Jin HL, 2008, INT J COMPUT VISION, V76, P245, DOI 10.1007/s11263-007-0055-y; Ju YC, 2016, MATH VIS, P43, DOI 10.1007/978-3-319-24726-7_3; Kajiya J.T., 1986, SIGGRAPH, P143, DOI [DOI 10.1145/15922.15902, 10.1145/15886.15902, DOI 10.1145/15886.15902]; Langguth F, 2016, LECT NOTES COMPUT SC, V9907, P469, DOI 10.1007/978-3-319-46487-9_29; Leclerc Y. G., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P552, DOI 10.1109/CVPR.1991.139752; Liu-Yin Q., 2016, BRIT MACH VIS C BMVC; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; Maurer D., 2016, P BRIT VIS C; Maurer D, 2015, LECT NOTES COMPUT SC, V9358, P249, DOI 10.1007/978-3-319-24947-6_20; Mecca R, 2016, SIAM J IMAGING SCI, V9, P1858, DOI 10.1137/16M1068177; Memin E, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P933, DOI 10.1109/ICCV.1998.710828; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Okatani T, 1997, COMPUT VIS IMAGE UND, V66, P119, DOI 10.1006/cviu.1997.0613; Or-Ell R, 2016, PROC CVPR IEEE, P4378, DOI 10.1109/CVPR.2016.474; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Prados E, 2005, PROC CVPR IEEE, P870; Queau Y., 2015, PROC INT C SCALE SPA, P498; Ranftl R, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P401, DOI 10.1109/IVS.2012.6232171; Robert L., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P439, DOI 10.1007/BFb0015556; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; Samaras D, 2000, PROC CVPR IEEE, P480, DOI 10.1109/CVPR.2000.855858; Schroers Christopher, 2015, Scale Space and Variational Methods in Computer Vision. 5th International Conference, SSVM 2015. Proceedings: LNCS 9087, P551, DOI 10.1007/978-3-319-18461-6_44; Semerjian B, 2014, LECT NOTES COMPUT SC, V8694, P719, DOI 10.1007/978-3-319-10599-4_46; Simoncelli E. P., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P310, DOI 10.1109/CVPR.1991.139707; Strecha C., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587706; Valgaerts L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366206; van Diggelen J, 1951, B ASTRON I NETH, V1, P283; Vogel O, 2007, LECT NOTES COMPUT SC, V4485, P871; Volz S, 2011, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2011.6126359; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; Weickert J., 2013, INT C SCALE SPACE VA, P380; Wu CL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661232; Wu CL, 2011, PROC CVPR IEEE, P969, DOI 10.1109/CVPR.2011.5995388; Wu CY, 2010, INT J COMPUT VISION, V86, P211, DOI 10.1007/s11263-009-0207-3; Xu D, 2014, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2014.198; Yoon KJ, 2010, INT J COMPUT VISION, V86, P192, DOI 10.1007/s11263-009-0222-4; Young David M., 1971, ITERATIVE SOLUTION L, P63; Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186; Yu TL, 2007, INT J COMPUT VISION, V73, P123, DOI 10.1007/s11263-006-9373-8; Yu TL, 2004, PROC CVPR IEEE, P226; Zimmer H, 2011, INT J COMPUT VISION, V93, P368, DOI 10.1007/s11263-011-0422-6; Zollhofer M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766887	71	17	18	0	30	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2018	126	12			SI		1342	1366		10.1007/s11263-018-1079-1	http://dx.doi.org/10.1007/s11263-018-1079-1			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GZ3KT					2022-12-18	WOS:000449286200006
J	Hattori, H; Lee, N; Boddeti, VN; Beainy, F; Kitani, KM; Kanade, T				Hattori, Hironori; Lee, Namhoon; Boddeti, Vishnu Naresh; Beainy, Fares; Kitani, Kris M.; Kanade, Takeo			Synthesizing a Scene-Specific Pedestrian Detector and Pose Estimator for Static Video Surveillance	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Training with synthetic data; Pedestrian detection; Pose estimation		We consider scenarios where we have zero instances of real pedestrian data (e.g., a newly installed surveillance system in a novel location in which no labeled real data or unsupervised real data exists yet) and a pedestrian detector must be developed prior to any observations of pedestrians. Given a single image and auxiliary scene information in the form of camera parameters and geometric layout of the scene, our approach infers and generates a large variety of geometrically and photometrically accurate potential images of synthetic pedestrians along with purely accurate ground-truth labels through the use of computer graphics rendering engine. We first present an efficient discriminative learning method that takes these synthetic renders and generates a unique spatially-varying and geometry-preserving pedestrian appearance classifier customized for every possible location in the scene. In order to extend our approach to multi-task learning for further analysis (i.e., estimating pose and segmentation of pedestrians besides detection), we build a more generalized model employing a fully convolutional neural network architecture for multi-task learning leveraging the "free" ground-truth annotations that can be obtained from our pedestrian synthesizer. We demonstrate that when real human annotated data is scarce or non-existent, our data generation strategy can provide an excellent solution for an array of tasks for human activity analysis including detection, pose estimation and segmentation. Experimental results show that our approach (1) outperforms classical models and hybrid synthetic-real models, (2) outperforms various combinations of off-the-shelf state-of-the-art pedestrian detectors and pose estimators that are trained on real data, and (3) surprisingly, our method using purely synthetic data is able to outperform models trained on real scene-specific data when data is limited.	[Hattori, Hironori; Lee, Namhoon; Boddeti, Vishnu Naresh; Kitani, Kris M.; Kanade, Takeo] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Beainy, Fares] Volvo Construct Equipment, Gothenburg, Sweden; [Hattori, Hironori] Univ Tokyo, Inst Ind Sci, Tokyo, Japan; [Lee, Namhoon] Univ Oxford, Engn Sci Dept, Oxford, England; [Boddeti, Vishnu Naresh] Michigan State Univ, Comp Sci & Engn, Lansing, MI USA	Carnegie Mellon University; Volvo; University of Tokyo; University of Oxford; Michigan State University	Hattori, H (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.; Hattori, H (corresponding author), Univ Tokyo, Inst Ind Sci, Tokyo, Japan.	hattorih@iis.u-tokyo.ac.jp; namhoon.lee@eng.ox.ac.uk; vishnu@msu.edu; fares.beainy@volvo.com; kkitani@cs.cmu.edu		Hattori, Hironori/0000-0001-9892-3601				[Anonymous], BMVC; Athitsos V., 2008, P 1 INT C PERV TECHN, P30; Athitsos V, 2010, PERS UBIQUIT COMPUT, V14, P511, DOI 10.1007/s00779-009-0276-x; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Benfold B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3457, DOI 10.1109/CVPR.2011.5995667; Boddeti VN, 2013, PROC CVPR IEEE, P2291, DOI 10.1109/CVPR.2013.297; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Broggi A., 2005, P 2005 IEEE COMP SOC, P1; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; Cai ZW, 2015, IEEE I CONF COMP VIS, P3361, DOI 10.1109/ICCV.2015.384; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dollar P, 2009, BRIT MACHINE VISION, DOI [10.5244/C.23.91, DOI 10.5244/C.23.91]; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Enzweiler M, 2009, IEEE T PATTERN ANAL, V31, P2179, DOI 10.1109/TPAMI.2008.260; Ess A, 2007, IEEE I CONF COMP VIS, P2065; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Fischer Philipp, 2015, ARXIV150406852, P2; Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470; Girshick R., 2011, ADV NEURAL INFORM PR, V24, P442; Girshick R., 2015, ICCV; Grauman K, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P641; Grimson E, 2004, COMP VIS PATT REC 20, V2, pII; Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006; Hattori K., 2014, TECHNICAL REPORT; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hejrati M, 2014, PROC CVPR IEEE, P2449, DOI 10.1109/CVPR.2014.314; Henriques JF, 2013, IEEE I CONF COMP VIS, P2760, DOI 10.1109/ICCV.2013.343; Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5; Huang SY, 2017, PROC CVPR IEEE, P4664, DOI 10.1109/CVPR.2017.496; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lai K., 2012, ICRA; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Marin J, 2010, PROC CVPR IEEE, P137, DOI 10.1109/CVPR.2010.5540218; Matikainen P, 2012, LECT NOTES COMPUT SC, V7583, P209, DOI 10.1007/978-3-642-33863-2_21; Meng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3401, DOI 10.1109/CVPR.2011.5995698; Movshovitz-Attias Y., 2014, BMVC; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257; Pepik B, 2012, PROC CVPR IEEE, P3362, DOI 10.1109/CVPR.2012.6248075; Pishchulin L, 2013, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2013.433; Pishchulin L, 2012, PROC CVPR IEEE, P3178, DOI 10.1109/CVPR.2012.6248052; Pishchulin L, 2011, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2011.5995574; Ramakrishna V, 2014, LECT NOTES COMPUT SC, V8690, P33, DOI 10.1007/978-3-319-10605-2_3; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Romero J, 2010, IEEE INT CONF ROBOT, P458, DOI 10.1109/ROBOT.2010.5509753; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Roth P. M., 2009, IEEE C COMP VIS PATT, P2727; Sangineto E, 2014, LECT NOTES COMPUT SC, V8691, P456, DOI 10.1007/978-3-319-10578-9_30; Satkin S, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.128; Shotton J, 2013, IEEE T PATTERN ANAL, V35, P2821, DOI 10.1109/TPAMI.2012.241; Stalder S., 2009, P PETS; Stalder S, 2010, LECT NOTES COMPUT SC, V6311, P369, DOI 10.1007/978-3-642-15549-9_27; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sun B., 2014, BMVC, V1, P3; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Taylor Geoffrey R., 2007, IEEE COMPUTER SOC C, DOI [DOI 10.1109/CVPR.2007.383518, 10.1109/CVPR.2007.383518]; Thirde D, 2006, P 9 IEEE INT WORKSH, P47; Tian YL, 2015, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2015.221; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Triggs B, 2006, ACCV; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163; Wang M, 2012, PROC CVPR IEEE, P3274, DOI 10.1109/CVPR.2012.6248064; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wojek C, 2009, PROC CVPR IEEE, P794, DOI 10.1109/CVPRW.2009.5206638; Xu JL, 2013, IEEE COMPUT SOC CONF, P688, DOI 10.1109/CVPRW.2013.104; Yang W, 2016, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2016.335; Yang Y, 2013, PROC CVPR IEEE, P1650, DOI 10.1109/CVPR.2013.216; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Zhang SS, 2015, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2015.7298784	75	17	18	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2018	126	9			SI		1027	1044		10.1007/s11263-018-1077-3	http://dx.doi.org/10.1007/s11263-018-1077-3			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GQ3HQ					2022-12-18	WOS:000441553300009
J	Sui, Y; Tang, YF; Zhang, L; Wang, GH				Sui, Yao; Tang, Yafei; Zhang, Li; Wang, Guanghui			Visual Tracking via Subspace Learning: A Discriminative Approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual tracking; Discriminative subspace; Joint learning; Sparse representation; Low-rank approximation	SPARSE APPEARANCE MODEL; ROBUST OBJECT TRACKING; THRESHOLDING ALGORITHM; SHRINKAGE; SELECTION	Good tracking performance is in general attributed to accurate representation over previously obtained targets and/or reliable discrimination between the target and the surrounding background. In this work, a robust tracker is proposed by integrating the advantages of both approaches. A subspace is constructed to represent the target and the neighboring background, and their class labels are propagated simultaneously via the learned subspace. In addition, a novel criterion is proposed, by taking account of both the reliability of discrimination and the accuracy of representation, to identify the target from numerous target candidates in each frame. Thus, the ambiguity in the class labels of neighboring background samples, which influences the reliability of the discriminative tracking model, is effectively alleviated, while the training set still remains small. Extensive experiments demonstrate that the proposed approach outperforms most state-of-the-art trackers.	[Sui, Yao] Harvard Univ, Harvard Med Sch, Boston, MA 02115 USA; [Tang, Yafei] China Unicom Res Inst, Beijing 100032, Peoples R China; [Zhang, Li] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China; [Wang, Guanghui] Univ Kansas, Dept Elect Engn & Comp Sci, Lawrence, KS 66045 USA	Harvard University; Harvard Medical School; Tsinghua University; University of Kansas	Sui, Y (corresponding author), Harvard Univ, Harvard Med Sch, Boston, MA 02115 USA.	suiyao@gmail.com; tyf8807@gmail.com; chinazhangli@tsinghua.edu.cn; ghwang@ku.edu			National Natural Science Foundation of China (NSFC) [61132007, 61573351]; National Natural Science Foundation of China (NSFC); Civil Aviation Administration [U1533132]; National Aeronautics and Space Administration (NASA) LEARN II Program [NNX15AN94N]	National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Civil Aviation Administration; National Aeronautics and Space Administration (NASA) LEARN II Program	This work is supported by the National Natural Science Foundation of China (NSFC) under Grants 61132007 and 61573351, the joint fund of Civil Aviation Research by the National Natural Science Foundation of China (NSFC) and Civil Aviation Administration under Grant U1533132, and the National Aeronautics and Space Administration (NASA) LEARN II Program under Grant No. NNX15AN94N.	[Anonymous], 2016, CVPR; Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374; Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53; Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35; Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Belhumeur PN, 1996, PROC CVPR IEEE, P270, DOI 10.1109/CVPR.1996.517085; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Danelljan M, 2014, BRIT MACHINE VISIO, P1; Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733; Grabner H, 2006, IEEE C COMP VIS PATT, P260; Hager GD, 1996, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.1996.517104; Han B, 2016, IEEE COMP SOC C COMP; Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251; Henriques J. F., 2012, EUR C COMP VIS, P702, DOI DOI 10.1007/978-3-642-33765-9_50; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231; Kwon J, 2014, IEEE T PATTERN ANAL, V36, P1428, DOI 10.1109/TPAMI.2013.213; Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369; Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821; Lasserre J.A., 2006, IEEE COMP SOC C COMP, P87, DOI DOI 10.1109/CVPR.2006.227; Lin Z., 2010, ARXIV10095055, DOI DOI 10.1016/J.JSB.2012.10.010; Liu BY, 2013, IEEE T PATTERN ANAL, V35, P2968, DOI 10.1109/TPAMI.2012.215; Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730; Liu S., 2016, IEEE COMP SOC C COMP; Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352; Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177; Mairal J., 2008, IEEE COMP SOC C COMP; Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66; Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292; Pati YC, 1993, SIGN SYST COMP 1993, P40, DOI DOI 10.1109/ACSSC.1993.342465; Pham DS, 2008, PROC CVPR IEEE, P517; Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466; Raina Rajat, 2007, INT C MACH LEARN ICM; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891; Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230; Sui Y., 2017, IEEE T CYBERNETICS T; Sui Y., 2016, EUR C COMP VIS ECCV; Sui Y, 2016, INT J COMPUT VISION, V119, P110, DOI 10.1007/s11263-016-0881-x; Sui Y, 2015, IEEE I CONF COMP VIS, P3002, DOI 10.1109/ICCV.2015.344; Sui Y, 2015, IEEE T IMAGE PROCESS, V24, P4686, DOI 10.1109/TIP.2015.2462076; Sui Y, 2015, PATTERN RECOGN, V48, P2872, DOI 10.1016/j.patcog.2015.03.007; Sui Y, 2015, IEEE SIGNAL PROC LET, V22, P1331, DOI 10.1109/LSP.2015.2402313; Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang D., 2014, IEEE COMP SOC C COMP; Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307; Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677; Wang D, 2012, IEEE SIGNAL PROC LET, V19, P711, DOI 10.1109/LSP.2012.2215320; Wang LJ, 2016, PROC CVPR IEEE, P1373, DOI 10.1109/CVPR.2016.153; Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357; Wang Q., 2012, IEEE WINT C APPL COM; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Zhang CC, 2014, NEUROCOMPUTING, V131, P237, DOI 10.1016/j.neucom.2013.10.020; Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283; Zhang KH, 2013, IEEE T IMAGE PROCESS, V22, P4664, DOI 10.1109/TIP.2013.2277800; Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62; Zhang SP, 2013, PATTERN RECOGN, V46, P1772, DOI 10.1016/j.patcog.2012.10.006; Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0; Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34; ZHANG TZ, 2015, PROC CVPR IEEE, P150, DOI DOI 10.1109/CVPR.2015.7298610; Zhong W, 2014, IEEE T IMAGE PROCESS, V23, P2356, DOI 10.1109/TIP.2014.2313227; Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	74	17	18	0	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2018	126	5					515	536		10.1007/s11263-017-1049-z	http://dx.doi.org/10.1007/s11263-017-1049-z			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FZ0UP					2022-12-18	WOS:000427289200004
J	Sindagi, V; Srivastava, S				Sindagi, Vishwanath A.; Srivastava, Sumit			Domain Adaptation for Automatic OLED Panel Defect Detection Using Adaptive Support Vector Data Description	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Domain adaptation; Adaptive-SVDD; Defect detection	INSPECTION; CLASSIFICATION; RECOGNITION; WAVELET	Detection of surface defects on organic light emitting diode (OLED) panels pose challenges such as irregular shapes and sizes along with varying textures and patterns on the panels. These challenges can be addressed by designing invariant features and training an anomaly detection algorithm such as support vector data description (SVDD). However, these hand designed features may not be capable of handling test datasets that have undergone distributional shift due to changes in lighting configuration or panel specification. This leads to a degradation of the classifier performance. In this paper, we propose a domain adaptation technique for outlier detection called as adaptive support vector data description (A-SVDD) to tackle distributional change in OLED panel datasets. The proposed method aims to learn an incremental classifier based on the existing classifier using an objective function similar to SVDD. We also investigate the application of features called as local inlier-outlier ratios augmented with modified local binary pattern (LBP) for detection of OLED panel defects in the context of SVDD and A-SVDD. In the experiments, the proposed domain adaptation technique is compared with baseline methods and existing approaches to demonstrate its effectiveness. A detailed evaluation of the features was performed in the context of A-SVDD and SVDD on several defects like scratch, spot, stain and pit to demonstrate that the combination of local inlier-outlier ratios and modified LBP significantly increases the detection accuracy.	[Sindagi, Vishwanath A.; Srivastava, Sumit] Samsung R Inst Bangalore, Bangalore, Karnataka, India; [Sindagi, Vishwanath A.; Srivastava, Sumit] Samsung R&D Inst Bangalore, Bangalore, Karnataka, India		Sindagi, V (corresponding author), Samsung R&D Inst Bangalore, Bangalore, Karnataka, India.	vishwanath.a@samsung.com; sumit.sri@samsung.com						Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Aiger D, 2010, PROC CVPR IEEE, P295, DOI 10.1109/CVPR.2010.5540198; [Anonymous], 2007, P 15 ACM INT C MULTI; Banerjee A, 2007, IEEE IMAGE PROC, P1797; Benmoussat M., 2012, Proceedings of the 2012 IEEE International Conference on Imaging Systems and Techniques (IST), P228, DOI 10.1109/IST.2012.6295527; Benmoussat MS, 2013, INFRARED PHYS TECHN, V61, P68, DOI 10.1016/j.infrared.2013.07.007; Blitzer J., 2006, P 2006 C EMP METH NA, P120, DOI DOI 10.3115/1610075.1610094; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chang W.C., 2013, TECHNICAL REPORT; Chen L.C., 2008, MEASUREMENT SCI TECH, V19, P1; Chen SL, 2008, J ADV MECH DES SYST, V2, P441, DOI 10.1299/jamdsm.2.441; Choi J., 2015, IEEE INT C IM PROC, P1037; Crammer K, 2008, J MACH LEARN RES, V9, P1757; Daume III Hal, 2007, P 45 ANN M ASS COMP, P256, DOI DOI 10.48550/ARXIV.0907.1815; Duan L., 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411; Ghouti Lahouari, 2005, 2005 13th European Signal Processing Conference, P1; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750; Jiang BC, 2005, INT J PROD RES, V43, P67, DOI 10.1080/00207540412331285832; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kumar A, 2008, IEEE T IND ELECTRON, V55, P348, DOI 10.1109/TIE.1930.896476; Lee SW, 2006, PATTERN RECOGN, V39, P1809, DOI 10.1016/j.patcog.2006.04.033; Li L, 2013, CHIN OPT LETT, V11, DOI 10.3788/COL201311.021102; Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167; Liu YH, 2008, MEAS SCI TECHNOL, V19, DOI 10.1088/0957-0233/19/9/095501; Lu CJ, 2004, INT J PROD RES, V42, P4331, DOI 10.1080/00207540410001716480; Maenpaa T, 2003, PATTERN ANAL APPL, V6, P169, DOI 10.1007/s10044-002-0179-1; Nguyen H. V., 2013, IEEE T PATTERN ANAL, V24, P5479; Ni J, 2013, PROC CVPR IEEE, P692, DOI 10.1109/CVPR.2013.95; Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Park J, 2007, NEURAL COMPUT, V19, P1919, DOI 10.1162/neco.2007.19.7.1919; Patel V. M., 2015, IEEE SIGNAL PROCESSI, V32, P63; Rostamizadeh A., 2009, ADV NEURAL INFORM PR, P1041; Scheirer WJ, 2011, IEEE T PATTERN ANAL, V33, P1689, DOI 10.1109/TPAMI.2011.54; Shekhar S, 2013, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.2013.53; Shi Yuan, 2012, ICML; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Silven I, 2003, MACH VISION APPL, V13, P275, DOI 10.1007/s00138-002-0084-z; Sindagi VA, 2015, 2015 14th IAPR International Conference on Machine Vision Applications (MVA), P214, DOI 10.1109/MVA.2015.7153170; Tajeripour F, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/783898; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Tsai DM, 2005, INT J PROD RES, V43, P4589, DOI 10.1080/00207540500140732; Tsai DM, 2006, PATTERN RECOGN, V39, P1679, DOI 10.1016/j.patcog.2006.03.005; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wu P., 2004, PROC 21 INT C MACH L, P110; Zadrozny  B., 2004, INT C MACH LEARN ICM, DOI 10.1145/1015330.1015425	47	17	17	2	50	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		193	211		10.1007/s11263-016-0953-y	http://dx.doi.org/10.1007/s11263-016-0953-y			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS					2022-12-18	WOS:000398162200002
J	Spyropoulos, A; Mordohai, P				Spyropoulos, Aristotle; Mordohai, Philippos			Correctness Prediction, Accuracy Improvement and Generalization of Stereo Matching Using Supervised Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo matching; 3D reconstruction; Ground control points; Confidence estimation		Machine learning has been instrumental in most areas of computer vision, but has not been applied to the problem of stereo matching with similar frequency or success. In this paper, we present a supervised learning approach by defining a set of features that capture various forms of information about each pixel, and then by using them to predict the correctness of stereo matches based on a random forest. We show highly competitive results in predicting the correctness of matches and in confidence estimation, which allows us to rank pixels according to the reliability of their assigned disparities. Moreover, we show how these confidence values can be used to improve the accuracy of disparity maps by integrating them with an MRF-based stereo algorithm. This is an important distinction from current literature that has mainly focused on sparsification by removing potentially erroneous disparities to generate quasi-dense disparity maps. Finally, we demonstrate domain generalization of our method by applying classifiers to datasets different than those they were trained on with minimal loss of accuracy.	[Spyropoulos, Aristotle; Mordohai, Philippos] Stevens Inst Technol, Dept Comp Sci, Hoboken, NJ 07030 USA	Stevens Institute of Technology	Spyropoulos, A (corresponding author), Stevens Inst Technol, Dept Comp Sci, Hoboken, NJ 07030 USA.	aspyropo@stevens.edu; Philippos.Mordohai@stevens.edu			National Science Foundation [1217797, 1527294]	National Science Foundation(National Science Foundation (NSF))	The authors are grateful to Haeusler et al. (2013), Park and Yoon (2015) and Wang and Yang (2011) for sharing data and providing guidance on how to implement their algorithms. This research has been supported in part by the National Science Foundation Awards #1217797 and #1527294.	Alahari K, 2010, PROC CVPR IEEE, P895, DOI 10.1109/CVPR.2010.5540123; Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Bobick AF, 1999, INT J COMPUT VISION, V33, P181, DOI 10.1023/A:1008150329890; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603; Criminisi A., 2013, DECISION FORESTCOM; CRUZ JM, 1995, PATTERN RECOGN LETT, V16, P933, DOI 10.1016/0167-8655(95)00028-F; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Haeusler R., 2013, CVPR; Haeusler R, 2012, LECT NOTES COMPUT SC, V7584, P158, DOI 10.1007/978-3-642-33868-7_16; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Kim JC, 2005, PROC CVPR IEEE, P1075; Komodakis N., 2007, P 2007 IEEE C COMP V, P1, DOI [10.1109/CVPR.2007.383095, DOI 10.1109/CVPR.2007.383095]; Kong D., 2004, BMVC; Kong D., 2006, BMVC; LEW MS, 1994, IEEE T PATTERN ANAL, V16, P869, DOI 10.1109/34.310682; Li Y., 2008, CVPR; Liang Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3033, DOI 10.1109/CVPR.2011.5995480; Mac Aodha O., 2012, PAMI, V35, P1107; Merrell Paul, 2007, ICCV; Motten A., 2012, IEEE IFIP INT C VLSI; Pal CJ, 2012, INT J COMPUT VISION, V99, P319, DOI 10.1007/s11263-010-0385-z; Park MG, 2015, PROC CVPR IEEE, P101, DOI 10.1109/CVPR.2015.7298605; Pfeiffer D, 2013, PROC CVPR IEEE, P297, DOI 10.1109/CVPR.2013.45; Reynolds M, 2011, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2011.5995550; Sabater N, 2012, IEEE T PATTERN ANAL, V34, P930, DOI 10.1109/TPAMI.2011.207; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; Spyropoulos A, 2014, PROC CVPR IEEE, P1621, DOI 10.1109/CVPR.2014.210; Trinh H., 2009, BMVC; Xun Sun, 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P132, DOI 10.1109/3DIMPVT.2011.24; Yamaguchi K, 2012, LECT NOTES COMPUT SC, V7576, P45, DOI 10.1007/978-3-642-33715-4_4; Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767; Zhang K, 2009, IEEE T CIRC SYST VID, V19, P1073, DOI 10.1109/TCSVT.2009.2020478; Zhang L, 2007, IEEE T PATTERN ANAL, V29, P331, DOI 10.1109/TPAMI.2007.36	39	17	17	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2016	118	3					300	318		10.1007/s11263-015-0877-y	http://dx.doi.org/10.1007/s11263-015-0877-y			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DP9AM					2022-12-18	WOS:000378789100002
J	Cho, J; Lee, M; Oh, S				Cho, Jungchan; Lee, Minsik; Oh, Songhwai			Complex Non-rigid 3D Shape Recovery Using a Procrustean Normal Distribution Mixture Model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D reconstruction; Shape analysis; Non-rigid structure from motion; Non-rigid shape recovery	MOTION	Recovering the 3D shape of a non-rigid object is a challenging problem. Existing methods make the low-rank assumption and do not scale well with the increased degree of freedom found in complex non-rigid deformations or shape variations. Moreover, in general, the degree of freedom of deformation is assumed to be known in advance, which limits the applicability of non-rigid structure from motion algorithms in a practical situation. In this paper, we propose a method for handling complex shape variations based on the assumption that complex shape variations can be represented probabilistically by a mixture of primitive shape variations. The proposed model is a generative probabilistic model, called a Procrustean normal distribution mixture model, which can model complex shape variations without rank constraints. Experimental results show that the proposed method significantly outperforms existing methods.	[Cho, Jungchan; Oh, Songhwai] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea; [Cho, Jungchan; Oh, Songhwai] Seoul Natl Univ, ASRI, Seoul, South Korea; [Lee, Minsik] Hanyang Univ, Div Elect Engn, Ansan, South Korea	Seoul National University (SNU); Seoul National University (SNU); Hanyang University	Oh, S (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.; Oh, S (corresponding author), Seoul Natl Univ, ASRI, Seoul, South Korea.	cjc83@snu.ac.kr; mleepaper@hanyang.ac.kr; songhwai@snu.ac.kr	Lee, Minsik/S-7959-2017	Lee, Minsik/0000-0003-4941-4311	Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT & Future Planning [NRF-2013R1A1A2065551]	Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT & Future Planning	This work was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF-2013R1A1A2065551).	Akhter I., 2009, PATTERN RECOGN, DOI [10.1109/CVPR.2009.5206620., DOI 10.1109/CVPR.2009.5206620]; Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201; Bregler C., 2000, P IEEE C COMP VIS PA, DOI 10.1109/CVPR.2000.854941.; Celeux G, 2001, J COMPUT GRAPH STAT, V10, P697, DOI 10.1198/106186001317243403; Cho J, 2013, COMPUT VIS IMAGE UND, V117, P1549, DOI 10.1016/j.cviu.2013.07.009; Dai YC, 2012, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2012.6247905; Fayad J., 2010, P EUR C COMP VIS, DOI [10.1007/978-3-642-15561-1_22., DOI 10.1007/978-3-642-15561-1_22]; Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138; Forsyth DA, 2002, PRENT HALL PROF TECH; Gotardo P. F., 2011, P IEEE C COMP VIS PA, DOI [10.1109/TPAMI.2007.70752., DOI 10.1109/TPAMI.2007.70752]; Lee M., 2013, P IEEE C COMP VIS PA, DOI [10.1109/TPAMI.2007.70752., DOI 10.1109/TPAMI.2007.70752]; Lee M, 2014, PROC CVPR IEEE, P1550, DOI 10.1109/CVPR.2014.201; Lin Z., 2010, ARXIV PREPRINT ARXIV; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Paladini M., 2009, P IEEE C COMP VIS PA, DOI [10.1109/CVPR.2009.5206602., DOI 10.1109/CVPR.2009.5206602]; PizarroD ., 2011, P IEEE C COMP VIS PA, DOI [10.1109/CVPR.2011.59955677., DOI 10.1109/CVPR.2011.59955677]; Salzmann M., 2008, P IEEE C COMP VIS PA, DOI [10.1109/CVPR.2008.4587499, DOI 10.1109/CVPR.2008.4587499]; TAYLOR J, 2010, PROC CVPR IEEE, P2761, DOI DOI 10.1109/CVPR.2010.5540002; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; van der Aa N., 2011, P WORKSH HUM INT COM, DOI [10.1109/ICCVW.2011.6130396, DOI 10.1109/ICCVW.2011.6130396]; Varol A., 2009, P IEEE INT C COMP VI, DOI [10.1109/ICCV.2009.5459403, DOI 10.1109/ICCV.2009.5459403]; Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9; Zelditch ML, 2012, GEOMETRIC MORPHOMETRICS FOR BIOLOGISTS: A PRIMER, 2ND EDITION, P1; Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280; Zhu Y., 2013, P IEEE C COMP VIS PA, DOI [10.1109/CVPR.2011.5995650, DOI 10.1109/CVPR.2011.5995650]; Zhu YY, 2014, PROC CVPR IEEE, P1542, DOI 10.1109/CVPR.2014.200; Zhu YY, 2015, IEEE T PATTERN ANAL, V37, P529, DOI 10.1109/TPAMI.2013.2295311	28	17	17	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2016	117	3					226	246		10.1007/s11263-015-0860-7	http://dx.doi.org/10.1007/s11263-015-0860-7			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DL1TF					2022-12-18	WOS:000375414600002
J	Xu, C; Nanjappa, A; Zhang, XW; Cheng, L				Xu, Chi; Nanjappa, Ashwin; Zhang, Xiaowei; Cheng, Li			Estimate Hand Poses Efficiently from Single Depth Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hand pose estimation; Depth images; GPU acceleration; Regression forests; Consistency analysis; Annotated hand image dataset	RANDOM FORESTS	This paper aims to tackle the practically very challenging problem of efficient and accurate hand pose estimation from single depth images. A dedicated two-step regression forest pipeline is proposed: given an input hand depth image, step one involves mainly estimation of 3D location and in-plane rotation of the hand using a pixel-wise regression forest. This is utilized in step two which delivers final hand estimation by a similar regression forest model based on the entire hand image patch. Moreover, our estimation is guided by internally executing a 3D hand kinematic chain model. For an unseen test image, the kinematic model parameters are estimated by a proposed dynamically weighted scheme. As a combined effect of these proposed building blocks, our approach is able to deliver more precise estimation of hand poses. In practice, our approach works at 15.6 frame-per-second (FPS) on an average laptop when implemented in CPU, which is further sped-up to 67.2 FPS when running on GPU. In addition, we introduce and make publicly available a data-glove annotated depth image dataset covering various hand shapes and gestures, which enables us conducting quantitative analyses on real-world hand images. The effectiveness of our approach is verified empirically on both synthetic and the annotated real-world datasets for hand pose estimation, as well as related applications including part-based labeling and gesture classification. In addition to empirical studies, the consistency property of our approach is also theoretically analyzed.	[Xu, Chi; Nanjappa, Ashwin; Zhang, Xiaowei; Cheng, Li] ASTAR, Bioinformat Inst, Singapore, Singapore; [Cheng, Li] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore	Agency for Science Technology & Research (A*STAR); A*STAR - Bioinformatics Institute (BII); National University of Singapore	Cheng, L (corresponding author), ASTAR, Bioinformat Inst, Singapore, Singapore.; Cheng, L (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.	xuchi@bii.a-star.edu.sg; ashwinn@bii.a-star.edu.sg; zhangxw@bii.a-star.edu.sg; chengli@bii.a-star.edu.sg	Cheng, Li/AAU-6734-2020	Cheng, Li/0000-0003-3261-3533; Xu, Chi/0000-0002-5301-9376	A*STAR JCO; IAF	A*STAR JCO(Agency for Science Technology & Research (A*STAR)); IAF	This research was partially supported by A*STAR JCO and IAF grants. We would like to thank Vaghul Aditya Balaji for helping with the data collection and website design processes during his intern attachment at BII.	ANDREWS HC, 1976, IEEE T COMPUT, V25, P196, DOI 10.1109/TC.1976.5009235; Ballan L, 2012, LECT NOTES COMPUT SC, V7577, P640, DOI 10.1007/978-3-642-33783-3_46; Biau G, 2012, J MACH LEARN RES, V13, P1063; Biau G, 2008, J MACH LEARN RES, V9, P2015; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2004, CONSISTENCY SIMPLE R; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; de La Gorce M, 2011, IEEE T PATTERN ANAL, V33, P1793, DOI 10.1109/TPAMI.2011.33; Denil M., 2014, ICML; Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012; Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458; Gall J, 2013, DECISION FORESTCOM, DOI [DOI 10.1007/978-1-4471-4929-3_11, 10.1007/978-1-4471-4929-3_11]; Girshick R, 2011, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2011.6126270; Gustus A, 2012, BIOL CYBERN, V106, P741, DOI 10.1007/s00422-012-0532-4; Gyrofi L., 2002, DISTRIBUTION FREE TH; Hackenberg G, 2011, P IEEE VIRT REAL ANN, P19, DOI 10.1109/VR.2011.5759431; Hamming R. W, 1997, DIGITAL FILTERS; Hinterstoisser S., 2010, CVPR; Keskin C, 2012, LECT NOTES COMPUT SC, V7577, P852, DOI 10.1007/978-3-642-33783-3_61; Lewis J. P., 1995, VIS INTERFACE, V10, P120; Mao Ye, 2013, Time-Of-Flight and Depth Imaging. Sensors, Algorithms and Applications. Dagstuhl 2012 Seminar on Time-of-Flight Imaging and GCPR 2013 Workshop on Imaging New Modalities: LNCS 8200, P149, DOI 10.1007/978-3-642-44964-2_8; Melax S., 2013, GRAPHICS INTERFACE; Oikonomidis I., 2014, CVPR; Oikonomidis N., 2011, BMVC; Peachey D, 1990, TEXTURE ON DEMAND; Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305; Sueda S., 2008, SIGGRAPH; Tang DH, 2014, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2014.490; Taylor J., 2014, CVPR; Tzionas D., 2013, GERM C PATT REC; UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573; Wang R.Y., 2009, SIGGRAPH 09, P63; Xu C., 2013, ICCV; Zhao W., 2012, EUR S COMP AN	38	17	20	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2016	116	1					21	45		10.1007/s11263-015-0826-9	http://dx.doi.org/10.1007/s11263-015-0826-9			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7TE		hybrid			2022-12-18	WOS:000369422500002
J	Zheng, Y; Zemel, RS; Zhang, YJ; Larochelle, H				Zheng, Yin; Zemel, Richard S.; Zhang, Yu-Jin; Larochelle, Hugo			A Neural Autoregressive Approach to Attention-based Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Attention-based recognition; Neural networks; Neural autoregressive distribution estimator		Tasks that require the synchronization of perception and action are incredibly hard and pose a fundamental challenge to the fields of machine learning and computer vision. One important example of such a task is the problem of performing visual recognition through a sequence of controllable fixations; this requires jointly deciding what inference to perform from fixations and where to perform these fixations. While these two problems are challenging when addressed separately, they become even more formidable if solved jointly. Recently, a restricted Boltzmann machine (RBM) model was proposed that could learn meaningful fixation policies and achieve good recognition performance. In this paper, we propose an alternative approach based on a feed-forward, auto-regressive architecture, which permits exact calculation of training gradients (given the fixation sequence), unlike for the RBM model. On a problem of facial expression recognition, we demonstrate the improvement gained by this alternative approach. Additionally, we investigate several variations of the model in order to shed some light on successful strategies for fixation-based recognition.	[Zheng, Yin; Zhang, Yu-Jin] Tsinghua Univ, Dept Elect Engn, Beijing 10084, Peoples R China; [Zemel, Richard S.] Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada; [Larochelle, Hugo] Univ Sherbrooke, Dept Informat, Sherbrooke, PQ J1K 2R1, Canada	Tsinghua University; University of Toronto; University of Sherbrooke	Zheng, Y (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 10084, Peoples R China.	yzheng3xg@gmail.com; zemel@cs.toronto.edu; zhang-yj@tsinghua.edu.cn; hugo.larochelle@usherbrooke.ca			Natural Sciences and Engineering Research Council of Canada; National Natural Science Foundation [NNSF-61171118]; Ministry of Education of China [SRFDP-20110002110057]	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); National Natural Science Foundation(National Natural Science Foundation of China (NSFC)); Ministry of Education of China(Ministry of Education, China)	This work was partially supported by the Natural Sciences and Engineering Research Council of Canada, the National Natural Science Foundation under Grants NNSF-61171118 and the Ministry of Education under Grants SRFDP-20110002110057 of China.	Bazzani L, 2011, P 28 INT C MACH LEAR, P937; Butko NJ, 2010, IEEE T AUTON MENT DE, V2, P91, DOI 10.1109/TAMD.2010.2051029; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312; Erez T., 2011, AAAI; Fazl A, 2009, COGNITIVE PSYCHOL, V58, P1, DOI 10.1016/j.cogpsych.2008.05.001; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hinton GE, 2012, IMPROVING NEURAL NET; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kanan C., 2010, CVPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle H., 2012, ADV NEURAL INFORM PR, P2708; Larochelle H., 2011, INT C ART INT STAT; Larochelle H., 2010, ADV NEURAL INFORM PR, P1243; Lazebnik S., 2006, P 2006 IEEE COMP VIS, P2169; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mathe S, 2013, ADV NEURAL INFORM PR, P1923; Nair V., 2010, ICML, P807; Najemnik J, 2005, NATURE, V434, P387, DOI 10.1038/nature03390; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Rifai Salah, 2011, ICML; Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X; Southall J. P. C., 1962, HELMHOLTZS TREATISE, V2; Susskind J. M., 2010, TORONTO FACE DATABAS, V3; Uria Benigno, 2013, P 26 INT C NEURAL IN; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Yang J., 2009, CVPR	31	17	17	0	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2015	113	1			SI		67	79		10.1007/s11263-014-0765-x	http://dx.doi.org/10.1007/s11263-014-0765-x			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CH6GX					2022-12-18	WOS:000354135700006
J	Amer, MR; Yousefi, S; Raich, R; Todorovic, S				Amer, Mohamed R.; Yousefi, Siavash; Raich, Raviv; Todorovic, Sinisa			Monocular Extraction of 2.1D Sketch Using Constrained Convex Optimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						2.1D sketch; Figure-ground assignment; Image segmentation; Convex quadratic optimization	FEEDBACK ARC SET; DEPTH; SHAPE; IMAGE	This paper presents an approach to estimating the 2.1D sketch from monocular, low-level visual cues. We use a low-level segmenter to partition the image into regions, and, then, estimate their 2.1D sketch, subject to figure-ground and similarity constraints between neighboring regions. The 2.1D sketch assigns a depth ordering to image regions which are expected to correspond to objects and surfaces in the scene. This is cast as a constrained convex optimization problem, and solved within the optimization transfer framework. The optimization objective takes into account the curvature and convexity of parts of region boundaries, appearance, and spatial layout properties of regions. Our new optimization transfer algorithm admits a closed-form expression of the duality gap, and thus allows explicit computation of the achieved accuracy. The algorithm is efficient with quadratic complexity in the number of constraints between image regions. Quantitative and qualitative results on challenging, real-world images of Berkeley segmentation, Geometric Context, and Stanford Make3D datasets demonstrate our high accuracy, efficiency, and robustness.	[Amer, Mohamed R.; Raich, Raviv; Todorovic, Sinisa] Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA; [Yousefi, Siavash] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA	Oregon State University; University of Washington; University of Washington Seattle	Amer, MR (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA.	amerm@onid.orst.edu; siavash@uw.edu; raich@eecs.oregonstate.edu; sinisa@eecs.oregonstate.edu			 [NSF RI 1302700];  [DARPA MSEE FA 8650-11-1-7149]	; 	This work was supported in part by Grants NSF RI 1302700 and DARPA MSEE FA 8650-11-1-7149.	Adelson E.H., 1995, P IEEE WORKSH REPR V; Afonso MV, 2010, IEEE T IMAGE PROCESS, V19, P2345, DOI 10.1109/TIP.2010.2047910; Ahuja N., 2008, P IEEE C COMP VIS PA; Alon N, 2006, SIAM J DISCRETE MATH, V20, P137, DOI 10.1137/050623905; Amer M., 2010, P INT C IM PROC ICIP; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; BARNOY A, 1990, SIAM J DISCRETE MATH, V3, P7, DOI 10.1137/0403002; Boyd S, 2004, CONVEX OPTIMIZATION; Charbit P, 2007, COMB PROBAB COMPUT, V16, P1, DOI 10.1017/S0963548306007887; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Dark Channel Prior Dehazing, 2015, PYTH IMPL SINGL IM H; DARRELL T, 1995, IEEE T PATTERN ANAL, V17, P474, DOI 10.1109/34.391395; Darrell T., 1988, P C CVPR 88 COMP SOC; Dimiccoli M., 2009, IEEE INT C IM PROC I; Dimiccoli M, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P95, DOI 10.1109/ICVGIP.2008.97; Esedoglu S, 2003, J MATH IMAGING VIS, V18, P7, DOI 10.1023/A:1021837026373; Favaro P, 2008, IEEE T PATTERN ANAL, V30, P518, DOI 10.1109/TPAMI.2007.1175; Fowlkes CC, 2007, J VISION, V7, DOI 10.1167/7.8.2; Fragkiadaki K., 2010, P EUR C COMP VIS ECC; Gao R., 2007, P EUR C COMP VIS EMM; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; Gu C., 2009, P IEEE C COMP VIS AP; Guo CE, 2007, COMPUT VIS IMAGE UND, V106, P5, DOI 10.1016/j.cviu.2005.09.004; Hoiem D, 2005, IEEE I CONF COMP VIS, P654; Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232; Hoiem D., 2008, P IEEE C COMP VIS PA; Hoiem D., 2010, IJCV, V91, P328; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; Hwang T.-l., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P476, DOI 10.1109/CVPR.1989.37890; Jia Z., 2012, P IEEE C COMP VIS PA; Kenyon-Mathieu C, 2007, ACM S THEORY COMPUT, P95, DOI 10.1145/1250790.1250806; Krishnan A, 1996, INT J COMPUT VISION, V20, P169, DOI 10.1007/BF00208718; Leichter I, 2009, IEEE I CONF COMP VIS, P9, DOI 10.1109/ICCV.2009.5459208; MALIK J, 1989, IEEE T PATTERN ANAL, V11, P555, DOI 10.1109/34.24791; Marr D., 1979, P INT JOINT C ART IN; Martin D., 2001, P IEEE INT C COMP VI; Mathieu C., 2011, J ACM, P95; Nitzberg M., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P138, DOI 10.1109/ICCV.1990.139511; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; Rajagopalan AN, 1997, IEEE T PATTERN ANAL, V19, P1158, DOI 10.1109/34.625126; Ren XF, 2006, LECT NOTES COMPUT SC, V3952, P614; Roy-Chowdhury AK, 2005, IEEE T IMAGE PROCESS, V14, P1057, DOI 10.1109/TIP.2005.849775; Saund E., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P624, DOI 10.1109/CVPR.1999.784988; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Scharstein D, 2007, P IEEE C COMP VIS PA; Sipahi, 2010, 2010 IEEE WORLD C CO, DOI [10.1109/CEC.2010.5585957, DOI 10.1109/CEC.2010.5585957]; Stamm H, 1991, FEEDBACK PROBLEMS PL; Sun M, 2010, LECT NOTES COMPUT SC, V6315, P658, DOI 10.1007/978-3-642-15555-0_48; Varma M., 2007, P INT C COMP VIS ICC; Vecera SP, 2002, J EXP PSYCHOL GEN, V131, P194, DOI 10.1037//0096-3445.131.2.194; Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892	51	17	17	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2015	112	1					23	42		10.1007/s11263-014-0752-2	http://dx.doi.org/10.1007/s11263-014-0752-2			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CC4YC					2022-12-18	WOS:000350361500002
J	Yamada, M; Sigal, L; Chang, Y				Yamada, Makoto; Sigal, Leonid; Chang, Yi			Domain Adaptation for Structured Regression	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D pose estimation; Semi-supervised domain adaptation; Covariate shift adaptation	HUMAN POSE	Discriminative regression models have proved effective for many vision applications (here we focus on 3D full-body and head pose estimation from image and depth data). However, dataset bias is common and is able to significantly degrade the performance of a trained model on target test sets. As we show, covariate shift, a form of unsupervised domain adaptation (USDA), can be used to address certain biases in this setting, but is unable to deal with more severe structural biases in the data. We propose an effective and efficient semi-supervised domain adaptation (SSDA) approach for addressing such more severe biases in the data. Proposed SSDA is a generalization of USDA, that is able to effectively leverage labeled data in the target domain when available. Our method amounts to projecting input features into a higher dimensional space (by construction well suited for domain adaptation) and estimating weights for the training samples based on the ratio of test and train marginals in that space. The resulting augmented weighted samples can then be used to learn a model of choice, alleviating the problems of bias in the data; as an example, we introduce SSDA twin Gaussian process regression (SSDA-TGP) model. With this model we also address the issue of data sharing, where we are able to leverage samples from certain activities (e.g., walking, jogging) to improve predictive performance on very different activities (e.g., boxing). In addition, we analyze the relationship between domain similarity and effectiveness of proposed USDA versus SSDA methods. Moreover, we propose a computationally efficient alternative to TGP (Bo and Sminchisescu 2010), and it's variants, called the direct TGP. We show that our model outperforms a number of baselines, on two public datasets: HumanEva and ETH Face Pose Range Image Dataset. We can also achieve 8-15 times speedup in computation time, over the traditional formulation of TGP, using the proposed direct formulation, with little to no loss in performance.	[Yamada, Makoto; Chang, Yi] Yahoo Labs, Sunnyvale, CA 94089 USA; [Sigal, Leonid] Disney Res Pittsburgh, Pittsburgh, PA 15213 USA		Yamada, M (corresponding author), Yahoo Labs, 701 1st Ave, Sunnyvale, CA 94089 USA.	makotoy@yahoo-inc.com; lsigal@disneyresearch.com; yichang@yahoo-inc.com		Chang, Yi/0000-0003-2697-8093				Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; ALI SM, 1966, J ROY STAT SOC B, V28, P131; Black M. J., 2007, NIPS; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Breitenstein MD, 2008, PROC CVPR IEEE, P3613; Chen Y.-Y., 2011, P 20 INT C COMP WORL, P25; Daume III Hal, 2007, P 45 ANN M ASS COMP, P256, DOI DOI 10.48550/ARXIV.0907.1815; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458; Girshick R, 2011, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2011.6126270; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Jiang J. J., 2007, LIT SURVEY DOMAIN AD; Kanaujia A., 2007, P IEEE C COMP VIS PA, P1; Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Lim J.J., 2011, ADV NEURAL INFORM PR, P118; Miller EG, 2000, PROC CVPR IEEE, P464, DOI 10.1109/CVPR.2000.855856; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; SIGAL L, 2006, CS0608 BROWN U; Sinno J.P., 2009, IEEE T KNOWL DATA EN, V22, P1345, DOI [10.1109/TKDE.2009.191, DOI 10.1109/TKDE.2009.191]; Sminchisescu Cristian, 2006, P INT C COMP VIS PAT, P1743; Sugiyama M., 2008, NIPS, P1433; Sun M, 2012, PROC CVPR IEEE, P3394, DOI 10.1109/CVPR.2012.6248079; Torralba A, 2004, PROC CVPR IEEE, P762; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Urtasun R, 2008, PROC CVPR IEEE, P149; Weise T., 2007, IEEE C COMPUTER VISI, P1; Yamada M, 2013, NEURAL COMPUT, V25, P1324, DOI 10.1162/NECO_a_00442; Yamada M, 2012, LECT NOTES COMPUT SC, V7575, P674, DOI 10.1007/978-3-642-33765-9_48	33	17	17	2	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2014	109	1-2			SI		126	145		10.1007/s11263-013-0689-x	http://dx.doi.org/10.1007/s11263-013-0689-x			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AI7QY					2022-12-18	WOS:000337091700009
J	Poling, B; Lerman, G				Poling, Bryan; Lerman, Gilad			A New Approach to Two-View Motion Segmentation Using Global Dimension Minimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Global dimension; Empirical dimension; Subspace clustering; Hybrid-linear modeling; Motion segmentation; Outliers; Robust statistics	MODEL	We present a new approach to rigid-body motion segmentation from two views. We use a previously developed nonlinear embedding of two-view point correspondences into a 9-dimensional space and identify the different motions by segmenting lower-dimensional subspaces. In order to overcome nonuniform distributions along the subspaces, whose dimensions are unknown, we suggest the novel concept of global dimension and its minimization for clustering subspaces with some theoretical motivation. We propose a fast projected gradient algorithm for minimizing global dimension and thus segmenting motions from 2-views. We develop an outlier detection framework around the proposed method, and we present state-of-the-art results on outlier-free and outlier-corrupted two-view data for segmenting motion.	[Poling, Bryan; Lerman, Gilad] Univ Minnesota, Sch Math, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Lerman, G (corresponding author), Univ Minnesota, Sch Math, 127 Vincent Hall,206 Church St SE, Minneapolis, MN 55455 USA.	poli0048@math.umn.edu; lerman@umn.edu		Lerman, Gilad/0000-0003-4624-3115	NSF [DMS-09-15064, DMS-09-56072]; IMA; Division Of Mathematical Sciences [0956072] Funding Source: National Science Foundation	NSF(National Science Foundation (NSF)); IMA; Division Of Mathematical Sciences(National Science Foundation (NSF)NSF - Directorate for Mathematical & Physical Sciences (MPS))	This work was supported by NSF Grants DMS-09-15064 and DMS-09-56072. GL was partially supported by the IMA during their annual program on the mathematics of information (2011-2012) and BP benefited from participating in parts of this program and even presented an initial version of this work at an IMA seminar in Spring 2012. We thank the anonymous reviewers for their thoughtful comments, Shankar Rao for sharing with us the RAS database, and Tom Lou for his helpful suggestions in regards to our algorithm for minimizing global dimension. A very preliminary version of this work was submitted to CVPR 2012, we thank one of the anonymous reviewers for some insightful comments that made us modify the GDM algorithm and its theoretical support.	Aldroubi A., 2013, ISRN SIGNAL PROCESS, P1; Arias-Castro E., 2013, ARXIV E PRINTS; Arias-Castro E, 2011, ELECTRON J STAT, V5, P1537, DOI 10.1214/11-EJS651; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Barbara D., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P260, DOI 10.1145/347090.347145; Bertsekas D. P., 1995, ATHENA SCI; Bhatia R., 1997, P IEEE WORKSH VIS MO; Boult T. E., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P179, DOI 10.1109/WVM.1991.212809; Bradley PS, 2000, J GLOBAL OPTIM, V16, P23, DOI 10.1023/A:1008324625522; Chen GL, 2011, PROC CVPR IEEE; Chen GL, 2009, FOUND COMPUT MATH, V9, P517, DOI 10.1007/s10208-009-9043-7; Chen GL, 2009, INT J COMPUT VISION, V81, P317, DOI 10.1007/s11263-008-0178-9; Ciptadi A, 2009, IEEE I CONF COMP VIS, P1765, DOI 10.1109/ICCV.2009.5459394; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Feng XL, 1998, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.1998.698613; Gionis A., 2005, P 11 ACM SIGKDD INT, P51, DOI 10.1145/1081870.1081880; Grafakos L., 2004, CLASSICAL MODERN FOU; Haro G., 2006, NEURAL INFORM PROCES; Haro G, 2008, INT J COMPUT VISION, V80, P358, DOI 10.1007/s11263-008-0144-6; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Ho J, 2003, PROC CVPR IEEE, P11, DOI 10.1109/cvpr.2003.1211332; Kanatani K, 2002, LECT NOTES COMPUT SC, V2352, P335; Kanatani K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P586, DOI 10.1109/ICCV.2001.937679; Lerman G, 2011, ANN STAT, V39, P2686, DOI 10.1214/11-AOS914; Levina E., 2005, ADV NEURAL INFORM PR, V17, P777, DOI DOI 10.5555/2976040.2976138; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Ma Y., 2004, INVITATION 3 D VISIO; Ma Y, 2008, SIAM REV, V50, P413, DOI 10.1137/060655523; Ma Y, 2007, IEEE T PATTERN ANAL, V29, P1546, DOI 10.1109/TP'AMI.2007.1085; Ozay N, 2010, PROC CVPR IEEE, P3209, DOI 10.1109/CVPR.2010.5540075; Papadopoulo T, 2000, LECT NOTES COMPUT SC, V1842, P554; Rao SR, 2010, INT J COMPUT VISION, V88, P425, DOI 10.1007/s11263-009-0314-1; Roy Olivier, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P606; Soltanolkotabi M., 2013, ARXIV E PRINTS; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; Torr PHS, 1998, PHILOS T R SOC A, V356, P1321, DOI 10.1098/rsta.1998.0224; Tron R, 2007, PROC CVPR IEEE, P41, DOI 10.1109/cvpr.2007.382974; Tseng P, 2000, J OPTIMIZ THEORY APP, V105, P249, DOI 10.1023/A:1004678431677; Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244; Vidal R, 2006, INT J COMPUT VISION, V68, P7, DOI 10.1007/s11263-005-4839-7; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94; Yang A. Y., 2006, P IEEE C COMP VIS PA, P99, DOI DOI 10.1109/CVPRW.2006.178; Zhang L, 2009, PROCEEDINGS OF 2009 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS TECHNOLOGY AND APPLICATIONS, P212, DOI 10.1109/ICCOMTA.2009.5349209; Zhang T, 2012, INT J COMPUT VISION, V100, P217, DOI 10.1007/s11263-012-0535-6; Zhang T, 2010, PROC CVPR IEEE, P1927, DOI 10.1109/CVPR.2010.5539866	51	17	17	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2014	108	3					165	185		10.1007/s11263-013-0694-0	http://dx.doi.org/10.1007/s11263-013-0694-0			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AH8NO		Green Submitted			2022-12-18	WOS:000336394900001
J	Ramalingam, S; Taguchi, Y				Ramalingam, Srikumar; Taguchi, Yuichi			A Theory of Minimal 3D Point to 3D Plane Registration and Its Generalization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D-to-3D registration; Pose estimation; Point-to-plane registration; Minimal solution; Correspondence problem	RELATIVE POSE	Registration of 3D data is a key problem in many applications in computer vision, computer graphics and robotics. This paper provides a family of minimal solutions for the 3D-to-3D registration problem in which the 3D data are represented as points and planes. Such scenarios occur frequently when a 3D sensor provides 3D points and our goal is to register them to a 3D object represented by a set of planes. In order to compute the 6 degrees-of-freedom transformation between the sensor and the object, we need at least six points on three or more planes. We systematically investigate and develop pose estimation algorithms for several configurations, including all minimal configurations, that arise from the distribution of points on planes. We also identify the degenerate configurations in such registrations. The underlying algebraic equations used in many registration problems are the same and we show that many 2D-to-3D and 3D-to-3D pose estimation/registration algorithms involving points, lines, and planes can be mapped to the proposed framework. We validate our theory in simulations as well as in three real-world applications: registration of a robotic arm with an object using a contact sensor, registration of planar city models with 3D point clouds obtained using multi-view reconstruction, and registration between depth maps generated by a Kinect sensor.	[Ramalingam, Srikumar; Taguchi, Yuichi] MERL, Cambridge, MA USA		Ramalingam, S (corresponding author), MERL, Cambridge, MA USA.	ramalingam@merl.com						BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; CHEN HH, 1991, IEEE T PATTERN ANAL, V13, P530, DOI 10.1109/34.87340; CHEN Y, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P2724, DOI 10.1109/ROBOT.1991.132043; DHOME M, 1989, IEEE T PATTERN ANAL, V11, P1265, DOI 10.1109/34.41365; Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108; Enqvist O, 2009, IEEE I CONF COMP VIS, P1295, DOI 10.1109/ICCV.2009.5459319; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fitzgibbon AW, 2003, IMAGE VISION COMPUT, V21, P1145, DOI 10.1016/j.imavis.2003.09.004; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599; GEYER C, 2007, P IEEE C COMP VIS PA; Grimson W. E. L., 1983, MODEL BASED RECOGNIT; HARALICK RM, 1994, INT J COMPUT VISION, V13, P331, DOI 10.1007/BF02028352; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Kukelova Zuzana, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P216, DOI 10.1007/978-3-642-19309-5_17; Kukelova Z., 2007, P IEEE C COMP VIS PA; Kukelova Z, 2008, LECT NOTES COMPUT SC, V5304, P302, DOI 10.1007/978-3-540-88690-7_23; Li H., 2005, P OMNIVIS; Naroditsky Oleg, 2011, IEEE International Conference on Robotics and Automation, P3429; Naroditsky O, 2012, IEEE T PATTERN ANAL, V34, P818, DOI 10.1109/TPAMI.2011.226; Nister D, 2004, PROC CVPR IEEE, P560; Nister D, 2003, PROC CVPR IEEE, P195; Nister D, 2006, INT J COMPUT VISION, V67, P211, DOI 10.1007/s11263-005-4265-x; Olsson C., 2006, P IEEE COMP SOC C CO, V1, P1206, DOI DOI 10.1109/CVPR.2006.307; Raguram R, 2008, LECT NOTES COMPUT SC, V5303, P500, DOI 10.1007/978-3-540-88688-4_37; Ramalingam S, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P203, DOI 10.1109/IM.2003.1240251; Ramalingam S, 2010, LECT NOTES COMPUT SC, V6315, P436, DOI 10.1007/978-3-642-15555-0_32; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Stewenius H, 2005, PROC CVPR IEEE, P789; Stewenius H., 2005, P OMNIVIS; Tu P, 1999, LECT NOTES COMPUT SC, V1681, P246; Yu L, 2007, IEEE INT C BIOINFORM, P9, DOI 10.1109/BIBM.2007.19	34	17	20	1	58	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					73	90		10.1007/s11263-012-0576-x	http://dx.doi.org/10.1007/s11263-012-0576-x			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO		Green Submitted			2022-12-18	WOS:000315501800006
J	Bandouch, J; Jenkins, OC; Beetz, M				Bandouch, Jan; Jenkins, Odest Chadwicke; Beetz, Michael			A Self-Training Approach for Visual Tracking and Recognition of Complex Human Activity Patterns	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Markerless human motion capture; Probabilistic state estimation; Self-trained models of human motion; Activity recognition	HUMAN MOTION; HUMAN MOVEMENT; CAPTURE; PEOPLE	Automatically observing and understanding human activities is one of the big challenges in computer vision research. Among the potential fields of application are areas such as robotics, human computer interaction or medical research. In this article we present our work on unintrusive observation and interpretation of human activities for the precise recognition of human fullbody motions. The presented system requires no more than three cameras and is capable of tracking a large spectrum of motions in a wide variety of scenarios. This includes scenarios where the subject is partially occluded, where it manipulates objects as part of its activities, or where it interacts with the environment or other humans. Our system is self-training, . it is capable of learning models of human motion over time. These are used both to improve the prediction of human dynamics and to provide the basis for the recognition and interpretation of observed activities. The accuracy and robustness obtained by our system is the combined result of several contributions. By taking an anthropometric human model and optimizing it towards use in a probabilistic tracking framework we obtain a detailed biomechanical representation of human shape, posture and motion. Furthermore, we introduce a sophisticated hierarchical sampling strategy for tracking that is embedded in a probabilistic framework and outperforms state-of-the-art Bayesian methods. We then show how to track complex manipulation activities in everyday environments using a combination of learned human appearance models and implicit environment models. Finally, we discuss a locally consistent representation of human motion that we use as a basis for learning environment- and task-specific motion models. All methods presented in this article have been subject to extensive experimental evaluation on today's benchmarks and several challenging sequences ranging from athletic exercises to ergonomic case studies to everyday manipulation tasks in a kitchen environment.	[Bandouch, Jan; Beetz, Michael] Tech Univ Munich, Intelligent Autonomous Syst Grp, D-85748 Garching, Germany; [Jenkins, Odest Chadwicke] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA	Technical University of Munich; Brown University	Bandouch, J (corresponding author), Tech Univ Munich, Intelligent Autonomous Syst Grp, Boltzmannstr 3, D-85748 Garching, Germany.	bandouch@cs.tum.edu; cjenkins@cs.brown.edu; beetz@cs.tum.edu		Beetz, Michael/0000-0002-7888-7444	DFG (Deutsche Forschungsgemeinschaft) cluster of excellence CoTeSys (Cognition for Technical Systems) at the Technische Universitat Munchen; DFG	DFG (Deutsche Forschungsgemeinschaft) cluster of excellence CoTeSys (Cognition for Technical Systems) at the Technische Universitat Munchen(German Research Foundation (DFG)); DFG(German Research Foundation (DFG))	This work was supported by the DFG (Deutsche Forschungsgemeinschaft) cluster of excellence CoTeSys (Cognition for Technical Systems) at the Technische Universitat Munchen and is partly funded by the DFG project MEMOMAN.	Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Anguelov D., 2004, 20 C UNC ART INT AUA; Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374; Balan A. O., 2008, EUR C COMP VIS ECCV; Bandouch J., 2008, 19 BRIT MACH VIS C B; Beetz M., 2008, IEEE 17 INT S ROB HU; Bo L., 2008, COMPUTER VISION PATT; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; Bray M, 2007, COMPUT VIS IMAGE UND, V106, P116, DOI 10.1016/j.cviu.2005.09.013; Bregler C, 2004, INT J COMPUT VISION, V56, P179, DOI 10.1023/B:VISI.0000011203.00237.9b; Bubb H., 1997, 13 TRIENN C INT ERG; Cham T.-J., 1999, COMPUTER VISION PATT; Cheung K. M., 2003, C COMP VIS PATT REC; Datta A., 2009, IEEE INT WORKSH TRAC; Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c; Efros A. A., 2003, INT C COMP VIS ICCV; Engstler F., 2009, 17 WORLD C ERG INT E; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1; Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Grauman K., 2003, INT C COMP VIS ICCV; Grest D, 2007, LECT NOTES COMPUT SC, V4814, P28; Herda L., 2004, EUR C COMP VIS ECCV; Horaud R, 2009, IEEE T PATTERN ANAL, V31, P158, DOI 10.1109/TPAMI.2008.108; Ivekovic V., 2008, EVOLUTIONARY COMPUTA, V16; Jenkins O. C., 2004, INT C MACH LEARN ICM; Ju S. X., 1996, INT C AUT FAC GEST R; Kehl R, 2006, COMPUT VIS IMAGE UND, V104, P190, DOI 10.1016/j.cviu.2006.07.010; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Knoop S, 2009, ROBOT AUTON SYST, V57, P321, DOI 10.1016/j.robot.2008.10.017; Knossow D, 2008, INT J COMPUT VISION, V79, P247, DOI 10.1007/s11263-007-0116-2; Kovar L., 2002, 29 ANN C COMP GRAPH; Kruger V, 2007, ADV ROBOTICS, V21, P1473; Kulic D, 2008, INT J ROBOT RES, V27, P761, DOI 10.1177/0278364908091153; MacCormick J, 2000, INT J COMPUT VISION, V39, P57, DOI 10.1023/A:1008122218374; MacCormick J., 2000, EUR C COMP VIS ECCV; Mikic I., 2001, COMPUTER VISION PATT; Mitchelson J., 2003, BRIT MACH VIS C BMVC; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Oikonomopoulos A, 2006, IEEE T SYST MAN CY B, V36, P710, DOI 10.1109/TSMCB.2005.861864; Pellegrini S., 2008, BRIT MACH VIS C BMVC; Plankers R, 2001, COMPUT VIS IMAGE UND, V81, P285, DOI 10.1006/cviu.2000.0891; Poppe R, 2007, COMPUT VIS IMAGE UND, V108, P4, DOI 10.1016/j.cviu.2006.10.016; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Ramanan D, 2007, IEEE T PATTERN ANAL, V29, P65, DOI 10.1109/TPAMI.2007.250600; Rosenhahn B., 2006, KUNSTLICHE INTELLIGE, V20, P45; Seidl A., 1994, THESIS TU MUNCHEN; Seitz T., 2005, SAE 2005 WORLD C; Sheikh Y., 2005, INT C COMP VIS ICCV; Sidenbladh H., 2002, EUR C COMP VIS ECCV; Sigal L., 2006, HUMANEVA SYNCHRONIZE; Sigal L., 2006, INT C ART MOT DEF OB; Sminchisescu C, 2003, INT J ROBOT RES, V22, P371, DOI 10.1177/0278364903022006003; Taylor G. W., 2010, COMPUTER VISION PATT; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tenorth M., 2009, INTELLIGENT ROBOTS S; Tenorth M., 2009, IEEE INT WORKSH TRAC; Urtasun R., 2004, EUR C COMP VIS ECCV; Urtasun R., 2006, COMPUTER VISION PATT; Vondrak M., 2008, COMPUTER VISION PATT; Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013; Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236; Yilmaz A., 2005, COMPUTER VISION PATT	65	17	19	0	37	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2012	99	2					166	189		10.1007/s11263-012-0522-y	http://dx.doi.org/10.1007/s11263-012-0522-y			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	943RU					2022-12-18	WOS:000304143700003
J	Rogez, G; Rihan, J; Orrite-Urunuela, C; Torr, PHS				Rogez, Gregory; Rihan, Jonathan; Orrite-Urunuela, Carlos; Torr, Philip H. S.			Fast Human Pose Detection Using Randomized Hierarchical Cascades of Rejectors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human detection; Pose estimation; Cascade classifiers	IMAGE; TRACKING; MOTION	This paper addresses human detection and pose estimation from monocular images by formulating it as a classification problem. Our main contribution is a multi-class pose detector that uses the best components of state-of-the-art classifiers including hierarchical trees, cascades of rejectors as well as randomized forests. Given a database of images with corresponding human poses, we define a set of classes by discretizing camera viewpoint and pose space. A bottom-up approach is first followed to build a hierarchical tree by recursively clustering and merging the classes at each level. For each branch of this decision tree, we take advantage of the alignment of training images to build a list of potentially discriminative HOG (Histograms of Orientated Gradients) features. We then select the HOG blocks that show the best rejection performances. We finally grow an ensemble of cascades by randomly sampling one of these HOG-based rejectors at each branch of the tree. The resulting multi-class classifier is then used to scan images in a sliding window scheme. One of the properties of our algorithm is that the randomization can be applied on-line at no extra-cost, therefore classifying each window with a different ensemble of randomized cascades. Our approach, when compared to other pose classifiers, gives fast and efficient detection performances with both fixed and moving cameras. We present results using different publicly available training and testing data sets.	[Rogez, Gregory; Orrite-Urunuela, Carlos] Univ Zaragoza, Aragon Inst Engn Res I3A, Comp Vis Lab, Zaragoza, Spain; [Rihan, Jonathan; Torr, Philip H. S.] Oxford Brookes Univ, Dept Comp, Oxford OX33 1HX, England	University of Zaragoza; Oxford Brookes University	Rogez, G (corresponding author), Univ Zaragoza, Aragon Inst Engn Res I3A, Comp Vis Lab, Zaragoza, Spain.	grogez@unizar.es			EPSRC [GR/T21790/01(P)]; Sony Entertainment Europe (SCEE); Departamento de Ciencia, Tecnologia y Universidad del Gobierno de Aragon; Fondo Social Europeo; Ministerio de Ciencia e Innovacion [TIN2010-20177]; Royal Society	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Sony Entertainment Europe (SCEE); Departamento de Ciencia, Tecnologia y Universidad del Gobierno de Aragon(Gobierno de Aragon); Fondo Social Europeo(European Social Fund (ESF)European Commission); Ministerio de Ciencia e Innovacion(Ministry of Science and Innovation, Spain (MICINN)Instituto de Salud Carlos IIISpanish Government); Royal Society(Royal Society of London)	Part of this work was conducted while the first author was a research fellow at Oxford Brookes University. This work was partly supported by the EPSRC grant GR/T21790/01(P) and by Sony Entertainment Europe (SCEE). G. Rogez and C. Orrite would like to acknowledge support provided by: "Departamento de Ciencia, Tecnologia y Universidad del Gobierno de Aragon", "Fondo Social Europeo" and "Ministerio de Ciencia e Innovacion (TIN2010-20177)". Prof. Torr is in receipt of a Royal Society Wolfson Research Merit Award.	Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156; Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754; Bergtholdt M, 2010, INT J COMPUT VISION, V87, P93, DOI 10.1007/s11263-009-0209-1; Bissacco A., 2007, 2007 IEEE C COMP VIS, P1, DOI [10.1109/CVPR.2007.383129, DOI 10.1109/CVPR.2007.383129]; Bissacco A., 2006, NIPS, P169; Bookstein F. L., 1991, MORPHOMETRIC TOOLS L; Bosch A, 2007, IEEE I CONF COMP VIS, P1863; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Brostow Gabriel J., 2008, ECCV, P44, DOI [10.1007/978-3-540-88682-2_5, DOI 10.1007/978-3-540-88682-2_5]; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Collins R., 2003, ICCV; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; DESELAERS T, 2007, CVPR; Dimitrijevic M, 2006, COMPUT VIS IMAGE UND, V104, P127, DOI 10.1016/j.cviu.2006.07.007; Elgammal A, 2009, IEEE T PATTERN ANAL, V31, P520, DOI 10.1109/TPAMI.2008.101; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; FELZENSZWALB PF, 2010, PROC CVPR IEEE, P2241, DOI DOI 10.1109/CVPR.2010.5539906; Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468; Fossati A., 2007, CVPR; Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1; Gavrila DM, 2007, IEEE T PATTERN ANAL, V29, P1408, DOI 10.1109/TPAMI.2007.1062; Gross Ralph, 2001, CMURITR0118; Jaeggli T, 2009, INT J COMPUT VISION, V83, P121, DOI 10.1007/s11263-008-0158-0; Kanade Takeo, 2000, P 4 IEEE INT C AUT F, P1, DOI [10.1109/AFGR.2000.840611, DOI 10.1109/AFGR.2000.840611]; Laptev I, 2009, IMAGE VISION COMPUT, V27, P535, DOI 10.1016/j.imavis.2008.08.010; Lee CS, 2010, INT J COMPUT VISION, V87, P118, DOI 10.1007/s11263-009-0266-5; Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188; Lin Z, 2010, IEEE T PATTERN ANAL, V32, P604, DOI 10.1109/TPAMI.2009.204; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma Yong, 2005, Tsinghua Science and Technology, V10, P152, DOI 10.1016/S1007-0214(05)70047-X; Moosmann F, 2008, IEEE T PATTERN ANAL, V30, P1632, DOI 10.1109/TPAMI.2007.70822; Mori G, 2006, IEEE T PATTERN ANAL, V28, P1052, DOI 10.1109/TPAMI.2006.149; Navaratnam R., 2005, BMVC; Okada R, 2008, LECT NOTES COMPUT SC, V5303, P434, DOI 10.1007/978-3-540-88688-4_32; Okada R, 2008, IEICE T INF SYST, VE91D, P1855, DOI 10.1093/ietisy/e91-d.7.1855; Orrite C, 2009, LECT NOTES COMPUT SC, V5524, P176, DOI 10.1007/978-3-642-02172-5_24; Roberts TJ, 2004, LECT NOTES COMPUT SC, V2034, P291; Rogez G., 2008, PATTERN RECOGNITION; Rogez G, 2008, PROC CVPR IEEE, P2142; Sabzmeydani P., 2007, CVPR07; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; SMINCHISESCU C, 2006, IEEE COMP SOC C COMP, V2, P1743; Stenger B.D.R., 2004, THESIS U CAMBRIDGE; Sugano H, 2007, LECT NOTES COMPUT SC, V4872, P932; THAYANANTHAN A, 2006, P EUR C COMP VIS, P124; Toyama K, 2002, INT J COMPUT VISION, V48, P9, DOI 10.1023/A:1014899027014; Villamizar M, 2009, LECT NOTES COMPUT SC, V5524, P128, DOI 10.1007/978-3-642-02172-5_18; Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8; Viola P., 2002, INT J COMPUTER VISIO; Wu B, 2005, IEEE I CONF COMP VIS, P90; Zehnder P, 2005, LECT NOTES COMPUT SC, V3361, P329; Zhang Jingjun, 2007, Proceedings. IEEE SoutheastCon 2007 (IEEE Cat. No.07CH37882); Zhang ZQ, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P149, DOI 10.1109/AFGR.2002.1004147; Zhu Qiang, 2006, CVPR, DOI DOI 10.1109/CVPR.2006.119	60	17	17	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2012	99	1					25	52		10.1007/s11263-012-0516-9	http://dx.doi.org/10.1007/s11263-012-0516-9			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	935NG					2022-12-18	WOS:000303525200002
J	Gupta, M; Tian, YD; Narasimhan, SG; Zhang, L				Gupta, Mohit; Tian, Yuandong; Narasimhan, Srinivasa G.; Zhang, Li			A Combined Theory of Defocused Illumination and Global Light Transport	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Defocused illumination; Global light transport; Depth recovery; Direct-global component separation; Projectors; Physics-based vision	SHAPE; MEDIA; DEPTH	Projectors are increasingly being used as light-sources in computer vision applications. In several applications, they are modeled as point light sources, thus ignoring the effects of illumination defocus. In addition, most active vision techniques assume that a scene point is illuminated only directly by the light source, thus ignoring global light transport effects. Since both defocus and global illumination co-occur in virtually all scenes illuminated by projectors, ignoring them can result in strong, systematic biases in the recovered scene properties. To make computer vision techniques work for general real world scenes, it is thus important to account for both these effects. In this paper, we study the interplay between defocused illumination and global light transport. We show that both these seemingly disparate effects can be expressed as low pass filters on the incident illumination. Using this observation, we derive an invariant between the two effects, which can be used to separate the two. This is directly useful in scenarios where limited depth-of-field devices (such as projectors) are used to illuminate scenes with global light trans-port and significant depth variations. We show applications in two scenarios: (a) accurate depth recovery in the presence of global light transport, and (b) factoring out the effects of illumination defocus for correct direct-global component separation. We demonstrate our approach using scenes with complex shapes, reflectance properties, textures and translucencies.	[Gupta, Mohit; Tian, Yuandong; Narasimhan, Srinivasa G.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Zhang, Li] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA	Carnegie Mellon University; University of Wisconsin System; University of Wisconsin Madison	Gupta, M (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	mohitg@cs.cmu.edu; tydsh@cs.cmu.edu; srinivas@cs.cmu.edu; lizhang@cs.wisc.edu			ONR [N00014-08-1-0330]; NSF [CCF-0541307, IIS-0643628, IIS-0845916]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This research is supported in parts by an ONR grant N00014-08-1-0330 and NSF awards CCF-0541307, IIS-0643628 and IIS-0845916. Parts of the results presented in this paper have previously appeared in Gupta et al. (2009).	ATCHESON B, 2008, SIGGRAPH; Bai J., 2010, ECCV; Bouguet J.-Y., 1998, P IEEE ICCV; Chen T., 2008, CVPR, p[1, 2]; Chen Tongbo, 2007, CVPR; Fuchs C, 2008, COMPUT GRAPH FORUM, V27, P1245, DOI 10.1111/j.1467-8659.2008.01263.x; GARG G, 2006, EGSR; GODIN G, 2001, 5 C OPT 3 D MEAS TEC; GU J, 2008, ECCV; Gupta M., 2009, P IEEE CVPR; HARDY AC, 1967, J OPT SOC AM, V57, P44, DOI 10.1364/JOSA.57.000044; Hasinoff S. W., 2006, ECCV 1; Hawkins T., 2005, SIGGRAPH; Horn B., 1975, PSYCHOL COMPUTER VIS, V19, P115; Hullin M. B., 2008, SIGGRAPH; Kutulakos KN, 2008, INT J COMPUT VISION, V76, P13, DOI 10.1007/s11263-007-0049-9; Levin A., 2007, SIGGRAPH; LEVOY M, 2004, SIGGRAPH; Liu S., 2010, ECCV; Morris N. J. W., 2007, ICCV; MUKAIGAWA Y, 2010, CVPR; Narasimhan SG, 2005, IEEE I CONF COMP VIS, P420; Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723; Narasimhan SG, 2006, ACM T GRAPHIC, V25, P1003, DOI 10.1145/1141911.1141986; Nayar S. K., 2006, SIGGRAPH; NAYAR SK, 1994, IEEE T PATTERN ANAL, V16, P824, DOI 10.1109/34.308479; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; O'Toole M., 2010, P SIGGRAPH AS; Schechner YY, 2000, INT J COMPUT VISION, V39, P141, DOI 10.1023/A:1008175127327; SEITZ SM, 2005, ICCV; SEN P, 2005, SIGGRAPH; Subbarao M., 1992, MACH VISION APPL, P277; TREIBITZ T, 2006, P IEEE CVPR, V2, P1861; Watanabe M, 1998, INT J COMPUT VISION, V27, P203, DOI 10.1023/A:1007905828438; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Zhang L., 2006, SIGGRAPH	36	17	17	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2012	98	2					146	167		10.1007/s11263-011-0500-9	http://dx.doi.org/10.1007/s11263-011-0500-9			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	934NH		Green Submitted			2022-12-18	WOS:000303450500002
J	Crivelli, T; Bouthemy, P; Cernuschi-Frias, B; Yao, JF				Crivelli, Tomas; Bouthemy, Patrick; Cernuschi-Frias, Bruno; Yao, Jian-feng			Simultaneous Motion Detection and Background Reconstruction with a Conditional Mixed-State Markov Random Field	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion detection; Background reconstruction; Mixed-state Markov models; Conditional random fields	STATISTICAL-ANALYSIS; SEGMENTATION; CLASSIFICATION; MODELS	In this work we present a new way of simultaneously solving the problems of motion detection and background image reconstruction. An accurate estimation of the background is only possible if we locate the moving objects. Meanwhile, a correct motion detection is achieved if we have a good available background model. The key of our joint approach is to define a single random process that can take two types of values, instead of defining two different processes, one symbolic (motion detection) and one numeric (background intensity estimation). It thus allows to exploit the (spatio-temporal) interaction between a decision (motion detection) and an estimation (intensity reconstruction) problem. Consequently, the meaning of solving both tasks jointly, is to obtain a single optimal estimate of such a process. The intrinsic interaction and simultaneity between both problems is shown to be better modeled within the so-called mixed-state statistical framework, which is extended here to account for symbolic states and conditional random fields. Experiments on real sequences and comparisons with existing motion detection methods support our proposal. Further implications for video sequence inpainting will be also discussed.	[Crivelli, Tomas; Cernuschi-Frias, Bruno] Univ Buenos Aires, Buenos Aires, DF, Argentina; [Crivelli, Tomas; Bouthemy, Patrick] INRIA, F-35042 Rennes, France; [Cernuschi-Frias, Bruno] Consejo Nacl Invest Cient & Tecn, RA-1033 Buenos Aires, DF, Argentina; [Yao, Jian-feng] Univ Rennes 1, IRMAR, F-35042 Rennes, France	University of Buenos Aires; Inria; Consejo Nacional de Investigaciones Cientificas y Tecnicas (CONICET); Universite de Rennes	Crivelli, T (corresponding author), Univ Buenos Aires, Buenos Aires, DF, Argentina.	tomas.crivelli@gmail.com; patrick.bouthemy@inria.fr; bcf@ieee.org; jian-feng.yao@univ-rennes1.fr	Cernuschi-Frias, Bruno/G-9177-2012; Yao, Jianfeng/B-3428-2011	Cernuschi-Frias, Bruno/0000-0001-5335-9402; Yao, Jianfeng/0000-0003-3147-2775				Benboudjema D, 2007, IEEE T PATTERN ANAL, V29, P1367, DOI 10.1109/TPAMI.2007.1059; BENEDEK C, 2007, IEEE INT C IM PROC, V6, P141; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; BESAG J, 1986, J R STAT SOC B, V48, P259; Black MJ, 1996, INT J COMPUT VISION, V19, P57, DOI 10.1007/BF00131148; Blanchet J, 2008, IEEE T PATTERN ANAL, V30, P1055, DOI 10.1109/TPAMI.2008.27; Bouthemy P, 2006, J MATH IMAGING VIS, V25, P387, DOI 10.1007/s10851-006-7251-1; BOUTHEMY P, 1993, OPT ENG, V32, P1205, DOI 10.1117/12.134183; BUGEAU A, 2007, CVPR 07; CAILLOL H, 1993, IEEE T GEOSCI REMOTE, V31, P801, DOI 10.1109/36.239902; Carincotte C, 2006, IEEE T GEOSCI REMOTE, V44, P432, DOI 10.1109/TGRS.2005.861007; CARINCOTTE C, 2004, ICASSP 04, P17; CERNUSCHIFRIAS B, 2007, 6255 INRIA; Chellappa R., 1985, PATTERN RECOGNITION, V2, P79; Chen J, 2007, PROC CVPR IEEE, P2400; Collet C, 2004, PATTERN RECOGN, V37, P2337, DOI 10.1016/j.patcog.2004.03.017; Criminisi A., 2006, IEEE C COMP VIS PATT, V1, P53, DOI DOI 10.1109/CVPR.2006.69; Crivelli T, 2006, IEEE IMAGE PROC, P1857, DOI 10.1109/ICIP.2006.312842; CRIVELLI T, 2009, MLVMA 09; Crivelli T, 2008, LECT NOTES COMPUT SC, V5302, P113, DOI 10.1007/978-3-540-88682-2_10; ELFADEL IM, 1994, IEEE T PATTERN ANAL, V16, P24, DOI 10.1109/34.273719; Fablet R, 2003, IEEE T PATTERN ANAL, V25, P1619, DOI 10.1109/TPAMI.2003.1251155; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Guyon X., 1995, RANDOM FIELDS NETWOR; Hardouin C, 2008, BIOMETRIKA, V95, P335, DOI 10.1093/biomet/asn016; Harwood D., 2000, EUR C COMP VIS, P751, DOI DOI 10.1007/3-540-45053-X_48; HEITZ F, 1993, IEEE T PATTERN ANAL, V15, P1217, DOI 10.1109/34.250841; Jojic N, 2001, PROC CVPR IEEE, P199; Kasetkasem T, 2002, IEEE T GEOSCI REMOTE, V40, P1815, DOI 10.1109/TGRS.2002.802498; Ko T, 2008, LECT NOTES COMPUT SC, V5304, P276, DOI 10.1007/978-3-540-88690-7_21; Koller D, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P324; Kumar S, 2006, INT J COMPUT VISION, V68, P179, DOI 10.1007/s11263-006-7007-9; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; LI Y, 2008, ECCV, P379; Lorette A, 2000, INT J COMPUT VISION, V36, P221, DOI 10.1023/A:1008129103384; Lu WL, 2009, IEEE T GEOSCI REMOTE, V47, P2913, DOI 10.1109/TGRS.2009.2017738; MIGDAL J, 2005, WMVC, P58; Mittal A, 2004, PROC CVPR IEEE, P302; Monnet A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1305; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P457; PARAG T, 2006, CVPR 2006, P1916; PIECZYNSKI W, 2000, MACHINE GRAPHICS VIS, P705; Salzenstein F, 1997, GRAPH MODEL IM PROC, V59, P205, DOI 10.1006/gmip.1997.0431; Salzenstein F, 2006, IEEE T PATTERN ANAL, V28, P1753, DOI 10.1109/TPAMI.2006.228; Sheikh Y, 2005, IEEE T PATTERN ANAL, V27, P1778, DOI 10.1109/TPAMI.2005.213; Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677; Sun J, 2006, LECT NOTES COMPUT SC, V3952, P628; Veit T, 2006, INT J COMPUT VISION, V68, P163, DOI 10.1007/s11263-006-6661-2; WANG T, 2006, CVPRW 06; Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236; Wright J., 2009, NIPS 2009; Wu J, 2007, IEEE T IMAGE PROCESS, V16, P241, DOI 10.1109/TIP.2006.884933; Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005; Zivkovic Z, 2004, IEEE T PATTERN ANAL, V26, P651, DOI 10.1109/TPAMI.2004.1273970	54	17	19	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2011	94	3					295	316		10.1007/s11263-011-0429-z	http://dx.doi.org/10.1007/s11263-011-0429-z			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816AC					2022-12-18	WOS:000294570100003
J	Forsyth, DA				Forsyth, D. A.			Variable-Source Shading Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision; Shape from shading; Interreflections; Object recognition	VISCOSITY SOLUTIONS; SHAPE; AMBIGUITY; LIGHT	The shading on curved surfaces is a cue to shape. Current computer vision methods for analyzing shading use physically unrealistic models, have serious mathematical problems, cannot exploit geometric information if it is available, and are not reliable in practice. We introduce a novel method of accounting for variations in irradiance resulting from interreflections, complex sources and the like. Our approach uses a spatially varying source model with a local shading model. Fast spatial variation in the source is penalised, consistent with the rendering community's insight that interreflections are spatially slow. This yields a physically plausible shading model. Because modern cameras can make accurate reports of observed radiance, our method compels the reconstructed surface to have shading exactly consistent with that of the image. For inference, we use a variational formulation, with a selection of regularization terms which guarantee that a solution exists. Our method is evaluated on physically accurate renderings of virtual objects, and on images of real scenes, for a variety of different kinds of boundary condition. Reconstructions for single sources compare well with photometric stereo reconstructions and with ground truth.	U Illinois Urbana Champaign, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Forsyth, DA (corresponding author), U Illinois Urbana Champaign, Urbana, IL 61801 USA.	daf@uiuc.edu			Office of Naval Research, MURI [N00014-01-1-0890]	Office of Naval Research, MURI(MURIOffice of Naval Research)	The idea of an effective source was suggested by Ryan White, and much of the model is the result of discussions with him. I thank Andrew Zisserman and Derek Hoiem for comments on a draft version. This work was supported in part by the Office of Naval Research under N00014-01-1-0890 as part of the MURI program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect those of the National Science Foundation or the Office of Naval Research. Careful, detailed and comprehensive reviews have been extremely helpful in preparing this version.	Arikan O, 2005, ACM T GRAPHIC, V24, P1108, DOI 10.1145/1073204.1073319; Bar M, 2003, J COGNITIVE NEUROSCI, V15, P600, DOI 10.1162/089892903321662976; Bar M, 2004, NAT REV NEUROSCI, V5, P617, DOI 10.1038/nrn1476; Battu B, 2007, PERCEPTION, V36, P1290, DOI 10.1068/p5591; Berthold KP Horn, 1970, SHAPE SHADING METHOD, P1; BLAKE A, 1985, IMAGE VISION COMPUT, V3, P183, DOI 10.1016/0262-8856(85)90006-X; BLAKE A, 1985, COMPUT VISION GRAPH, V32, P314, DOI 10.1016/0734-189X(85)90054-4; Brelstaff G., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P297, DOI 10.1109/CCV.1988.590004; BROOKS MJ, 1992, INT J COMPUT VISION, V7, P119, DOI 10.1007/BF00128131; BRUSS AR, 1982, J MATH PHYS, V23, P890, DOI 10.1063/1.525441; Chandraker M., 2007, P IEEE C COMPUTER VI, P1; Cohen M.F., 1993, RADIOSITY REALISTIC; CRANDALL MG, 1983, T AM MATH SOC, V277, P1, DOI 10.2307/1999343; Dacorogna B., 2008, APPL MATH SCI, V78; Dacorogna B., DIRECT METHODS CALCU, V78; Daniel P., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P418, DOI 10.1109/ICIAP.1999.797631; Daniel P, 2000, P 4 AS C COMP VIS TA, P187; Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884; Durou JD, 2008, COMPUT VIS IMAGE UND, V109, P22, DOI 10.1016/j.cviu.2007.09.003; DUTRE P, 2002, ADV GLOBAL ILLUMINAT; Falcone M., 1997, Image Analysis and Processing. 9th International Conference, ICIAP '97 Proceedings, P596; Fenske M. J., 2005, J VISION, V5, P851; Forsyth D., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P466, DOI 10.1109/CVPR.1989.37889; FORSYTH D, 1990, IMAGE VISION COMPUT, V8, P42, DOI 10.1016/0262-8856(90)90055-A; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P671, DOI 10.1109/34.85657; Georghiades A. S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P277, DOI 10.1109/AFGR.2000.840647; Goral C. M., 1984, Computers & Graphics, V18, P213; Hernandez C, 2008, LECT NOTES COMPUT SC, V5302, P290, DOI 10.1007/978-3-540-88682-2_23; Ho J, 2006, LECT NOTES COMPUT SC, V3953, P239, DOI 10.1007/11744078_19; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; Horn B. K. P, 1970, SHAPE SHADING METHOD; HORN BKP, 1993, IEEE T PATTERN ANAL, V15, P166, DOI 10.1109/34.192489; HORN BKP, 1977, ARTIF INTELL, V8, P201, DOI 10.1016/0004-3702(77)90020-0; HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; Jin HL, 2004, PROC CVPR IEEE, P36; Johnson M.K., 2005, P ACM MULTIMEDIA SEC, P1, DOI [DOI 10.1145/1073170.1073171, DOI 10.1145/0731701073171]; Koenderink JJ, 2001, PERCEPTION, V30, P431, DOI 10.1068/p3030; Koenderink JJ, 1998, PHILOS T R SOC A, V356, P1071, DOI 10.1098/rsta.1998.0211; Kreyszig E., 1989, INTRO FUNCTIONAL ANA; Krivanek J, 2005, IEEE T VIS COMPUT GR, V11, P550, DOI 10.1109/TVCG.2005.83; LARSON G. W., 1998, RENDERING RADIANCE A; Lax P. D., 2002, FUNCTIONAL ANAL; Mitsunaga T., P 1999 IEEE COMP SOC, P374; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; Nocedal J., 1999, SPRINGER SER OPER RE; OLIENSIS J, 1991, INT J COMPUT VISION, V6, P75, DOI 10.1007/BF00128151; PENTLAND AP, 1982, J OPT SOC AM, V72, P448, DOI 10.1364/JOSA.72.000448; Pharr M., 2004, PHYS BASED RENDERING; Potetz B., 2007, IEEE C COMP VIS PATT; Prados E, 2005, INT J COMPUT VISION, V65, P97, DOI 10.1007/s11263-005-3844-1; Prados E, 2005, PROC CVPR IEEE, P870; Prados E, 2002, LECT NOTES COMPUT SC, V2351, P790; Ragheb H, 2005, PATTERN RECOGN, V38, P1574, DOI 10.1016/j.patcog.2005.03.025; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; Raskar R., 2002, THESIS UNC CHAPEL HI; Robles-Kelly A, 2007, IEEE T IMAGE PROCESS, V16, P7, DOI 10.1109/TIP.2006.884945; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; Shimshoni I, 1997, COMPUT VIS IMAGE UND, V65, P296, DOI 10.1006/cviu.1996.0569; Sillion F. X., 1994, RADIOSITY GLOBAL ILL; Tankus A, 2005, INT J COMPUT VISION, V63, P21, DOI 10.1007/s11263-005-4945-6; TSAI PS, 1994, IMAGE VISION COMPUT, V12, P487, DOI 10.1016/0262-8856(94)90002-7; Wada T, 1997, INT J COMPUT VISION, V24, P125, DOI 10.1023/A:1007906904009; Ward G. J., 1988, Computer Graphics, V22, P85, DOI 10.1145/378456.378490; Worthington PL, 1999, IEEE T PATTERN ANAL, V21, P1250, DOI 10.1109/34.817406; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284	66	17	17	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2011	91	3					280	302		10.1007/s11263-010-0396-9	http://dx.doi.org/10.1007/s11263-010-0396-9			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	711RY					2022-12-18	WOS:000286610300004
J	Rasche, C				Rasche, Christoph			An Approach to the Parameterization of Structure for Fast Categorization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Contour description; Curve partitioning; Symmetric-axis transform; Image classification; Basic-level categorization	SHAPE REPRESENTATION; OBJECT RECOGNITION; EYE-MOVEMENTS; PERCEPTION; MULTISCALE; CONTOUR; SCALE; SCENE; FEATURES; MEMORY	A decomposition is described, which parameterizes the geometry and appearance of contours and regions of gray-scale images with the goal of fast categorization. To express the contour geometry, a contour is transformed into a local/global space, from which parameters are derived classifying its global geometry (arc, inflexion or alternating) and describing its local aspects (degree of curvature, edginess, symmetry). Regions are parameterized based on their symmetric axes, which are evolved with a wave-propagation process enabling to generate the distance map for fragmented contour images. The methodology is evaluated on three image sets, the Caltech 101 set and two sets drawn from the Corel collection. The performance nearly reaches the one of other categorization systems for unsupervised learning.	Univ Giessen, Abt Allgemeine Psychol, D-35394 Giessen, Germany	Justus Liebig University Giessen	Rasche, C (corresponding author), Univ Giessen, Abt Allgemeine Psychol, Otto Behagel Str 10,F1, D-35394 Giessen, Germany.	rasche15@gmail.com	Rasche, Christoph/C-1392-2011		European Commission [IST-C-033816]	European Commission(European CommissionEuropean Commission Joint Research Centre)	The study is supported by the Gaze-Based Communication Project (European Commission within the Information Society Technologies, contract no. IST-C-033816). The author wishes to thank Nadine Hartig for help with categorization, and Karl Gegenfurtner for lab support.	AMADASUN M, 1989, IEEE T SYST MAN CYB, V19, P1264, DOI 10.1109/21.44046; Amit Y, 2003, VISION RES, V43, P2073, DOI 10.1016/S0042-6989(03)00306-7; [Anonymous], 1985, PERCEPTUAL ORG VISUA; ASADA H, 1986, IEEE T PATTERN ANAL, V8, P2, DOI 10.1109/TPAMI.1986.4767747; Basri R, 1997, INT J COMPUT VISION, V25, P145, DOI 10.1023/A:1007919917506; BENGTSSON A, 1991, IEEE T PATTERN ANAL, V13, P85, DOI 10.1109/34.67634; Berengolts A, 2006, IEEE T PATTERN ANAL, V28, P1973, DOI 10.1109/TPAMI.2006.249; Berretti S, 2000, IEEE T MULTIMEDIA, V2, P225, DOI 10.1109/6046.890058; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; BLUM H, 1967, PERSPECT BIOL MED, V10, P381; Brady TF, 2008, P NATL ACAD SCI USA, V105, P14325, DOI 10.1073/pnas.0803390105; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Demirci MF, 2006, INT J COMPUT VISION, V69, P203, DOI 10.1007/s11263-006-6993-y; Draper BA, 1996, P IEEE, V84, P1625, DOI 10.1109/5.542412; Dror R, 2004, J VISION, V4, P821, DOI 10.1167/4.9.11; Dudek G, 1997, COMPUT VIS IMAGE UND, V68, P170, DOI 10.1006/cviu.1997.0533; Elder JH, 1999, INT J COMPUT VISION, V34, P97, DOI 10.1023/A:1008183703117; Elder JH, 2003, IEEE T PATTERN ANAL, V25, P661, DOI 10.1109/TPAMI.2003.1201818; Fabbri R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1322432.1322434; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; FELZENSZWALB PF, 2007, IEEE C COMP VIS PATT, P17; Fergus R, 2007, INT J COMPUT VISION, V71, P273, DOI 10.1007/s11263-006-8707-x; Fergus R, 2004, LECT NOTES COMPUT SC, V3021, P242; FISCHLER M, 1983, P 10 INT JOINT C ART, V2; FONSECA M, 2006, GRAPHICS RECOGNITION; Fu K. S., 1968, SEQUENTIAL METHODS P, V240, P241; Gregory RL, 1997, PHILOS T ROY SOC B, V352, P1121, DOI 10.1098/rstb.1997.0095; GUNTHER O, 1990, COMPUT VISION GRAPH, V51, P313, DOI 10.1016/0734-189X(90)90006-H; Hansen T, 2004, NEURAL COMPUT, V16, P1013, DOI 10.1162/089976604773135087; HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328; Heidemann G, 2005, IMAGE VISION COMPUT, V23, P861, DOI 10.1016/j.imavis.2005.05.016; Heitz G, 2009, INT J COMPUT VISION, V84, P40, DOI 10.1007/s11263-009-0228-y; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; JOLICOEUR P, 1984, COGNITIVE PSYCHOL, V16, P243, DOI 10.1016/0010-0285(84)90009-4; Joubert OR, 2007, VISION RES, V47, P3286, DOI 10.1016/j.visres.2007.09.013; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Keselman Y, 2005, IEEE T PATTERN ANAL, V27, P1141, DOI 10.1109/TPAMI.2005.139; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; LEIBE B, 2003, IEEE COMP SOC C COMP; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; Marr D., 1982, VISION; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Mel BW, 1997, NEURAL COMPUT, V9, P777, DOI 10.1162/neco.1997.9.4.777; Minsky M., 1975, PSYCHOL COMPUTER VIS, P211; Mojsilovic A, 2004, INT J COMPUT VISION, V56, P79, DOI 10.1023/B:VISI.0000004833.39906.33; MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591; MOKHTARIAN F, 1986, IEEE T PATTERN ANAL, V8, P34, DOI 10.1109/TPAMI.1986.4767750; Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220; Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724; NELSON R, 1998, 6 INT C COMP VIS; Niethammer M, 2004, INT J COMPUT VISION, V60, P203, DOI 10.1023/B:VISI.0000036835.28674.d0; NOTON D, 1971, SCIENCE, V171, P308, DOI 10.1126/science.171.3968.308; NOVAK D, 2008, IEEE CBMI 2008, P446; Ogniewicz RL, 1995, PATTERN RECOGN, V28, P1839, DOI 10.1016/0031-3203(95)00059-3; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; OPELT A, 2006, LNCS, V3952; Palmer S. E., 1981, ATTENTION PERFORM, V9, P135; Palmer S.E., 1999, VISION SCI PHOTONS P; PARENT P, 1989, IEEE T PATTERN ANAL, V11, P823, DOI 10.1109/34.31445; Pelillo M, 1999, IEEE T PATTERN ANAL, V21, P1105, DOI 10.1109/34.809105; POTTER MC, 1976, J EXP PSYCHOL-HUM L, V2, P509, DOI 10.1037/0278-7393.2.5.509; Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520; Rajashekar U, 2008, IEEE T IMAGE PROCESS, V17, P564, DOI 10.1109/TIP.2008.917218; RASCHE C, 2005, MAKING NEUROMORPHIC; Rasche C, 2007, IEEE T NEURAL NETWOR, V18, P520, DOI 10.1109/TNN.2006.884679; RAVISHANKAR RAO, 1993, CVGIP-GRAPH MODEL IM, V55, P218; Renninger LW, 2004, VISION RES, V44, P2301, DOI 10.1016/j.visres.2004.04.006; Rolls E.T., 2002, COMPUTATIONAL NEUROS; ROSCH E, 1976, COGNITIVE PSYCHOL, V8, P382, DOI 10.1016/0010-0285(76)90013-X; ROSENFELD A, 1968, PATTERN RECOGN, V1, P33, DOI 10.1016/0031-3203(68)90013-7; Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56; Shotton J, 2005, IEEE I CONF COMP VIS, P503; Shotton J, 2008, IEEE T PATTERN ANAL, V30, P1270, DOI 10.1109/TPAMI.2007.70772; Siddiqi K, 1999, IMAGE VISION COMPUT, V17, P365, DOI 10.1016/S0262-8856(98)00130-9; STANDING L, 1970, PSYCHON SCI, V19, P73, DOI 10.3758/BF03337426; Sudderth EB, 2008, INT J COMPUT VISION, V77, P291, DOI 10.1007/s11263-007-0069-5; TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999; Tenenbaum Jay M, 1983, HUMAN MACHINE VISION, P481; Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0; Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766; TREISMAN A, 1988, PSYCHOL REV, V95, P15, DOI 10.1037/0033-295X.95.1.15; Tu Z, 2006, INT J COMPUT VISION, V69, P223, DOI 10.1007/s11263-006-6995-9; VanRullen R, 2002, VISION RES, V42, P2593, DOI 10.1016/S0042-6989(02)00298-5; Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1; Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109; Wang S, 2007, INT J COMPUT VISION, V71, P337, DOI 10.1007/s11263-006-8427-2; Yuille A, 2004, ADV NEUR IN, V16, P1459; Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008; Zhong BJ, 2007, IEEE T PATTERN ANAL, V29, P508, DOI 10.1109/TPAMI.2007.50; Zhu SC, 1996, INT J COMPUT VISION, V20, P187; Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1158, DOI 10.1109/34.809109	95	17	17	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	87	3					337	356		10.1007/s11263-009-0286-1	http://dx.doi.org/10.1007/s11263-009-0286-1			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	551KA		Green Submitted			2022-12-18	WOS:000274205700008
J	Peursum, P; Venkatesh, S; West, G				Peursum, Patrick; Venkatesh, Svetha; West, Geoff			A Study on Smoothing for Particle-Filtered 3D Human Body Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Articulated human body tracking; Particle filtering; Smoothing	MOTION CAPTURE; MODELS	Stochastic models have become the dominant means of approaching the problem of articulated 3D human body tracking, where approximate inference is employed to tractably estimate the high-dimensional (similar to 30D) posture space. Of these approximate inference techniques, particle filtering is the most commonly used approach. However filtering only takes into account past observations-almost no body tracking research employs smoothing to improve the filtered inference estimate, despite the fact that smoothing considers both past and future evidence and so should be more accurate. In an effort to objectively determine the worth of existing smoothing algorithms when applied to human body tracking, this paper investigates three approximate smoothed-inference techniques: particle-filtered backwards smoothing, variational approximation and Gibbs sampling. Results are quantitatively evaluated on both the HumanEva dataset as well as a scene containing occluding clutter. Surprisingly, it is found that existing smoothing techniques are unable to provide much improvement on the filtered estimate, and possible reasons as to why are explored and discussed.	[Peursum, Patrick; Venkatesh, Svetha; West, Geoff] Curtin Univ Technol, Dept Comp, Perth, WA, Australia	Curtin University	Peursum, P (corresponding author), Curtin Univ Technol, Dept Comp, GPO Box U1987, Perth, WA, Australia.	P.Peursum@curtin.edu.au; S.Venkatesh@curtin.edu.au; G.West@curtin.edu.au		venkatesh, svetha/0000-0001-8675-6631	Australian Research Council [LP0561867]	Australian Research Council(Australian Research Council)	This research is supported by Australian Research Council grant LP0561867. We would also like to thank the anonymous reviewers for their valuable comments and suggestions.	Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Balan A. O., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P349; Barber D., 1998, Neural Networks and Machine Learning. Proceedings, P215; Black M. J., 2006, CS0608 BROWN U; BRUBAKER M, 2006, WORKSH EV ART HUM MO; BRUBAKER MA, 2007, P IEEE C COMP VIS PA; Caillette F., 2005, BRIT MACH VIS C, P469; Cheng S. Y., 2007, P IEEE COMP VIS PATT, P1; Corduneanu A., 2001, P 8 INT C ART INT ST, P27, DOI DOI 10.1016/J.CSDA.2006.07.020; Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c; Deutscher J, 2000, PROC CVPR IEEE, P126, DOI 10.1109/CVPR.2000.854758; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; DOUCET A, 2002, P INT C AUD SPEECH S, V2, P701; Elgammal A, 2004, PROC CVPR IEEE, P681; Godsill SJ, 2004, J AM STAT ASSOC, V99, P156, DOI 10.1198/016214504000000151; Gupta A, 2008, IEEE T PATTERN ANAL, V30, P493, DOI 10.1109/TPAMI.2007.1173; Hua G, 2007, COMPUT VIS IMAGE UND, V108, P272, DOI 10.1016/j.cviu.2006.11.020; HUSZ Z, 2007, WORKSH EV ART HUM MO; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Kanaujia A., 2007, P IEEE C COMP VIS PA; Klaas M., 2006, P 23 INT C MACH LEAR, P481, DOI DOI 10.1145/1143844.1143905; LEE CS, 2006, WORKSH EV ART HUM MO; Lee MW, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P159, DOI 10.1109/MOTION.2002.1182229; LEE MW, 2005, P JOINT WORKSH VIS S; Mikic I, 2001, PROC CVPR IEEE, P455; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Mundermann L., 2007, P IEEE C COMP VIS PA; PEURSUM P, 2008, VARIATIONAL GIBBS IN; PEURSUM P, 2006, BEHAV ANNEALED PARTI; PEURSUM P, 2007, P IEEE C COMP VIS PA; Poon E, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P151, DOI 10.1109/MOTION.2002.1182228; Sidenbladh H., 2000, LNCS, V2, P702; Sigal L, 2004, PROC CVPR IEEE, P421; Sminchisescu C, 2004, PROC CVPR IEEE, P608; Sminchisescu C, 2003, PROC CVPR IEEE, P69; Sminchisescu C, 2001, PROC CVPR IEEE, P447; Sminchisescu C, 2006, COMPUT VIS IMAGE UND, V104, P210, DOI 10.1016/j.cviu.2006.07.014; Smith AWB, 2006, INT C PATT RECOG, P789; Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677; TAYCHER L, 2006, P IEEE C COMP VIS PA, P222; Urtasun R, 2005, IEEE I CONF COMP VIS, P403; Urtasun R, 2006, COMPUT VIS IMAGE UND, V104, P157, DOI 10.1016/j.cviu.2006.08.006; Vermaak J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1110; VONDRAK M, 2008, P IEEE C COMP VIS PA; Winn J, 2005, J MACH LEARN RES, V6, P661	48	17	19	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2010	87	1-2			SI		53	74		10.1007/s11263-009-0205-5	http://dx.doi.org/10.1007/s11263-009-0205-5			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	539GP		Green Submitted			2022-12-18	WOS:000273242300004
J	Flusser, J; Kautsky, J; Sroubek, F				Flusser, Jan; Kautsky, Jaroslav; Sroubek, Filip			Implicit Moment Invariants	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Invariants; Implicit invariants; Moments; Orthogonal polynomials; Polynomial image deformation	PATTERN-RECOGNITION; IMAGE-ANALYSIS; IDENTIFICATION; NORMALIZATION; REGISTRATION; NOISE	The use of traditional moment invariants in object recognition is limited to simple geometric transforms, such as rotation, scaling and affine transformation of the image. This paper introduces so-called implicit moment invariants. Implicit invariants measure the similarity between two images factorized by admissible image deformations. For many types of image deformations traditional invariants do not exist but implicit invariants can be used as features for object recognition. In the paper we present implicit moment invariants with respect to polynomial transform of spatial coordinates, describe their stable and efficient implementation by means of orthogonal moments, and demonstrate their performance in artificial as well as real experiments.	[Flusser, Jan; Sroubek, Filip] ASCR, Inst Informat Theory & Automat, Prague 18208 8, Czech Republic; [Kautsky, Jaroslav] Flinders Univ S Australia, Adelaide, SA, Australia	Czech Academy of Sciences; Institute of Information Theory & Automation of the Czech Academy of Sciences; Flinders University South Australia	Sroubek, F (corresponding author), ASCR, Inst Informat Theory & Automat, Pod Vodarenskou Vezi 4, Prague 18208 8, Czech Republic.	flusser@utia.cas.cz; jarka@infoeng.flinders.edu.au; sroubekf@utia.cas.cz	Sroubek, Filip/G-6882-2014; Flusser, Jan/F-6209-2014	Sroubek, Filip/0000-0001-6835-4911; Flusser, Jan/0000-0003-3747-9214				ABUMOSTAFA YS, 1985, IEEE T PATTERN ANAL, V7, P46, DOI 10.1109/TPAMI.1985.4767617; ABUMOSTAFA YS, 1984, IEEE T PATTERN ANAL, V6, P698, DOI 10.1109/TPAMI.1984.4767594; Bailey RR, 1996, IEEE T PATTERN ANAL, V18, P389, DOI 10.1109/34.491620; BELKASIM SO, 1991, PATTERN RECOGN, V24, P1117, DOI 10.1016/0031-3203(91)90140-Z; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; DUDANI SA, 1977, IEEE T COMPUT, V26, P39, DOI 10.1109/TC.1977.5009272; ELKHALY F, 1990, PATTERN RECOGN, V23, P1207, DOI 10.1016/0031-3203(90)90116-3; FLUSSER J, 1994, IEEE T GEOSCI REMOTE, V32, P382, DOI 10.1109/36.295052; FLUSSER J, 1994, PATTERN RECOGN LETT, V15, P433, DOI 10.1016/0167-8655(94)90092-2; Flusser J, 2000, PATTERN RECOGN, V33, P1405, DOI 10.1016/S0031-3203(99)00127-2; FLUSSER J, 1993, PATTERN RECOGN, V26, P167, DOI 10.1016/0031-3203(93)90098-H; Flusser J, 2002, PATTERN RECOGN, V35, P3015, DOI 10.1016/S0031-3203(02)00093-6; Flusser J, 2006, IEEE T IMAGE PROCESS, V15, P3784, DOI 10.1109/TIP.2006.884913; Geusebroek JM, 2005, INT J COMPUT VISION, V61, P103, DOI 10.1023/B:VISI.0000042993.50813.60; GOLUB GH, 1983, NUMER MATH, V41, P147, DOI 10.1007/BF01390210; GOSHTASBY A, 1988, IEEE T GEOSCI REMOTE, V26, P60, DOI 10.1109/36.3000; Gruber M, 1997, IEEE T PATTERN ANAL, V19, P136, DOI 10.1109/34.574793; Gurevich G.B., 1964, FDN THEORY ALGEBRAIC; Hilbert D., 1993, THEORY ALGEBRAIC INV; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; HUPKENS TM, 1995, PATTERN RECOGN LETT, V16, P371, DOI 10.1016/0167-8655(94)00110-O; KAUTSKY J, 1983, LINEAR ALGEBRA APPL, V52-3, P439; KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109; Kybic J, 2003, IEEE T IMAGE PROCESS, V12, P1427, DOI 10.1109/TIP.2003.813139; LI YJ, 1992, PATTERN RECOGN, V25, P723, DOI 10.1016/0031-3203(92)90135-6; Liao SX, 1996, IEEE T PATTERN ANAL, V18, P254, DOI 10.1109/34.485554; MAITRA S, 1979, P IEEE, V67, P697, DOI 10.1109/PROC.1979.11309; Mamistvalov AG, 1998, IEEE T PATTERN ANAL, V20, P819, DOI 10.1109/34.709598; MUKUNDAN R, 1993, PATTERN RECOGN LETT, V14, P199, DOI 10.1016/0167-8655(93)90072-L; Mukundan R, 1996, PATTERN RECOGN LETT, V17, P1279, DOI 10.1016/0167-8655(96)00099-2; Mundy J., 1992, GEOMETRIC INVARIANCE; PAWLAK M, 1992, IEEE T INFORM THEORY, V38, P1698, DOI 10.1109/18.165444; Pawlak M., 2006, IMAGE ANAL MOMENTS R; PIZLO Z, 1992, CVGIP-IMAG UNDERSTAN, V56, P330, DOI 10.1016/1049-9660(92)90046-6; PROKOP RJ, 1992, CVGIP-GRAPH MODEL IM, V54, P438, DOI 10.1016/1049-9652(92)90027-U; REISS TH, 1991, IEEE T PATTERN ANAL, V13, P830, DOI 10.1109/34.85675; Schur I, 1968, VORLESUNGEN INVARIAN; SLUZEK A, 1995, PATTERN RECOGN LETT, V16, P687, DOI 10.1016/0167-8655(95)00021-8; Suk T, 2004, INT C PATT RECOG, P192, DOI 10.1109/ICPR.2004.1334093; Suk T, 2004, IEEE T PATTERN ANAL, V26, P1364, DOI 10.1109/TPAMI.2004.89; TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920; TEH CH, 1988, IEEE T PATTERN ANAL, V10, P496, DOI 10.1109/34.3913; TSIRIKOLIAS K, 1993, PATTERN RECOGN, V26, P877, DOI 10.1016/0031-3203(93)90053-Y; VANGOOL L, 1995, IMAGE VISION COMPUT, V13, P259, DOI 10.1016/0262-8856(95)99715-D; WALLIN A, 1995, IEEE T PATTERN ANAL, V17, P1106, DOI 10.1109/34.473239; Wang LZ, 1998, IEEE T IMAGE PROCESS, V7, P196, DOI 10.1109/83.660996; WEISS I, 1988, P IMAGE UNDERSTANDIN, P1125; WONG RY, 1978, COMPUT VISION GRAPH, V8, P16, DOI 10.1016/S0146-664X(78)80028-8; WONG WH, 1995, PATTERN RECOGN LETT, V16, P115, DOI 10.1016/0167-8655(94)00089-L; Yang LR, 1996, PATTERN RECOGN, V29, P1061, DOI 10.1016/0031-3203(95)00147-6	50	17	19	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	1					72	86		10.1007/s11263-009-0259-4	http://dx.doi.org/10.1007/s11263-009-0259-4			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	534MZ					2022-12-18	WOS:000272903100004
J	Gritai, A; Sheikh, Y; Rao, C; Shah, M				Gritai, Alexei; Sheikh, Yaser; Rao, Cen; Shah, Mubarak			Matching Trajectories of Anatomical Landmarks Under Viewpoint, Anthropometric and Temporal Transforms	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Applications; Trajectory matching; Human Information Processing; Motion	PERCEPTION; RECOGNITION	An approach is presented to match imaged trajectories of anatomical landmarks (e.g. hands, shoulders and feet) using semantic correspondences between human bodies. These correspondences are used to provide geometric constraints for matching actions observed from different viewpoints and performed at different rates by actors of differing anthropometric proportions. The fact that the human body has approximate anthropometric proportion allows innovative use of the machinery of epipolar geometry to provide constraints for analyzing actions performed by people of different sizes, while ensuring that changes in viewpoint do not affect matching. In addition, for linear time warps, a novel measure, constructed only from image measurements of the locations of anatomical landmarks across time, is proposed to ensure that similar actions performed at different rates are accurately matched as well. An additional feature of this new measure is that two actions from cameras moving at constant (and possibly different) velocities can also be matched. Finally, we describe how dynamic time warping can be used in conjunction with the proposed measure to match actions in the presence of nonlinear time warps. We demonstrate the versatility of our algorithm in a number of challenging sequences and applications, and report quantitative evaluation of the matching approach presented.	[Gritai, Alexei] Cernium Corp, Reston, VA USA; [Sheikh, Yaser] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Rao, Cen] PVI Virtual Media Serv, New York, NY USA; [Shah, Mubarak] Univ Cent Florida, Sch Elect Engn & Comp Sci, Orlando, FL 32816 USA	Carnegie Mellon University; State University System of Florida; University of Central Florida	Gritai, A (corresponding author), Cernium Corp, Reston, VA USA.	agritai@cernium.com; yaser@cs.cmu.edu; cen.rao@gmail.com		Shah, Mubarak/0000-0001-6172-5572				AGGARWAL J, 2004, 2 INT S 3 DAT PROC V; Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744; AKITA K, 1984, PATTERN RECOGN, V17, P73, DOI 10.1016/0031-3203(84)90036-0; Ayers D, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P42, DOI 10.1109/ACV.1998.732856; BADLER NI, 1993, SIMULATING HUMANS; BLACK M, 1998, EUR C COMP VIS, P63; BLACK MJ, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P374, DOI 10.1109/ICCV.1995.466915; Blakemore SJ, 2001, NAT REV NEUROSCI, V2, P561, DOI 10.1038/35086023; Blank M, 2005, IEEE I CONF COMP VIS, P1395; Bobick AF, 1998, PROC CVPR IEEE, P196, DOI 10.1109/CVPR.1998.698609; Boiman O., 2005, P IEEE INT C COMP VI; Breggin P. R., 2000, INT J RISK SAFETY ME, V13, P15; Brian Burns J., 1992, GEOMETRIC INVARIANCE; BRIDGER R, 1982, HUMAN PERFORMANCE EN; Bridger R.S., 1995, INTRO ERGONOMICS; Buxton H, 2003, IMAGE VISION COMPUT, V21, P125, DOI 10.1016/S0262-8856(02)00127-0; Campbell LW, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P157, DOI 10.1109/AFGR.1996.557258; Caspi Y, 2000, PROC CVPR IEEE, P682, DOI 10.1109/CVPR.2000.854940; CEDRAS C, 1995, IMAGE VISION COMPUT, V13, P129, DOI 10.1016/0262-8856(95)93154-K; Daems A, 1999, VIS COGN, V6, P665, DOI 10.1080/135062899394894; Darrell TJ, 1996, IEEE T PATTERN ANAL, V18, P1236, DOI 10.1109/34.546259; DAVIS J, 1994, P AS C SIGN SYST COM; Davis JW, 1997, PROC CVPR IEEE, P928, DOI 10.1109/CVPR.1997.609439; Decety J, 1999, TRENDS COGN SCI, V3, P172, DOI 10.1016/S1364-6613(99)01312-1; Easterby R., 1982, ANTHROPOMETRY BIOMEC; Farnell B, 1999, ANNU REV ANTHROPOL, V28, P341, DOI 10.1146/annurev.anthro.28.1.341; Fine S, 1998, MACH LEARN, V32, P41, DOI 10.1023/A:1007469218079; Galata A, 2001, COMPUT VIS IMAGE UND, V81, P398, DOI 10.1006/cviu.2000.0894; Gallese V, 1996, BRAIN, V119, P593, DOI 10.1093/brain/119.2.593; Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716; Goldman A., 1970, THEORY HUMAN ACTION; Gould K., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P79, DOI 10.1109/CVPR.1989.37831; GRITAI A, 2006, IEEE INT C MULT EXP; Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HERMAN M, 1979, THESIS U MARYLAND; HOGG DC, 1984, THESIS U SUSSEX; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Ju SX, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P38, DOI 10.1109/AFGR.1996.557241; KROEMER K, 1982, ANTHROPOMETRY BIOMEC; LAPTEV I, 2005, P IEEE INT C COMP VI; Li H, 2005, IEEE I CONF COMP VIS, P236; LIAO W, 1994, WORKSH MOT NONR ART; Mises Ludwig von, 1966, HUMAN ACTION TREATIS, V3rd; Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897; Nishikawa A, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P552, DOI 10.1109/AFGR.1998.671006; NIYOGI SA, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P469, DOI 10.1109/CVPR.1994.323868; Oliver N, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P3, DOI 10.1109/ICMI.2002.1166960; OLIVER N, 1999, P ICVS99 GRAN CAN SP; OROURKE J, 1980, IEEE T PATTERN ANAL, V2, P522, DOI 10.1109/TPAMI.1980.6447699; Parameswaran V, 2002, INT C PATT RECOG, P307, DOI 10.1109/ICPR.2002.1044699; PARAMESWARAN V, 2009, INT J COMPUTER VISIO; PARAMESWARAN V, 2003, P IEEE INT C COMP VI; Polana R., 1994, Journal of Visual Communication and Image Representation, V5, P172, DOI 10.1006/jvci.1994.1016; Prinz W, 1997, EUR J COGN PSYCHOL, V9, P129, DOI 10.1080/713752551; Rao C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P939; RAO C, 2001, P IEEE INT C COMP VI; RASHID RF, 1980, IEEE T PATTERN ANAL, V2, P574, DOI 10.1109/TPAMI.1980.6447705; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; Seitz SM, 1997, INT J COMPUT VISION, V25, P231, DOI 10.1023/A:1007928103394; SHAH M, 2004, INT C PATT REC; Shechtrnan E., 2005, IEEE C COMP VIS PATT; SHEIKH Y, 2007, P IEEE INT C COMP VI; STARNER T, 1996, COMPUTATIONAL IMAGIN; SUKTHANKAR R, 2005, P IEEE INT C COMP VI; VENKATESH S, 2005, P IEEE INT C COMP VI; VERFAILLIE K, 1992, CAN J PSYCHOL, V46, P215, DOI 10.1037/h0084322; Wang LA, 2003, PATTERN RECOGN, V36, P585, DOI 10.1016/S0031-3203(02)00100-0; YAMATO J, 1995, P IEEE C COMP VIS PA, P624; Yang J, 1997, IEEE T SYST MAN CY A, V27, P34, DOI 10.1109/3468.553220; Yang MH, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P10, DOI 10.1109/AFGR.1998.670918; YILMAZ A, 2005, IEEE P INT C COMP VI; Zatsiorsky VM, 2002, KINEMATICS HUMAN MOT; Zelnik-Manor L., 2001, IEEE C COMP VIS PATT	75	17	17	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2009	84	3					325	343		10.1007/s11263-009-0239-8	http://dx.doi.org/10.1007/s11263-009-0239-8			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	458MS					2022-12-18	WOS:000267028600006
J	Dahlkamp, H; Nagel, HH; Ottlik, A; Reuter, P				Dahlkamp, Hendrik; Nagel, Hans-Hellmut; Ottlik, Artur; Reuter, Paul			A framework for model-based tracking experiments in image sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						model-based tracking; Kalman-Filter; edge elements; optical flow; EM Contour Algorithm	REAL-TIME TRACKING; MOTION ESTIMATION; VEHICLE TRACKING; MULTIPLE PEOPLE; REGION TRACKING; OBJECT TRACKING; SEGMENTATION; RECOGNITION; POINT; COMBINATION	Motris, an integrated system for model-based tracking research, has been designed modularly to study the effects of algorithmic variations on tracking results. Motris attempts to avoid introducing bias into the relative assessment of alternative approaches. Such a bias may be caused by differences of implementation and parameterization if the component approaches are evaluated in separate testing environments. Tracking results are evaluated automatically on a significant test sample in order to quantify the effects of different combinations of alternatives. The Motris system environment thus allows an in-depth comparison between the so-called 'Edge-Element Association' approach documented in Haag and Nagel (1999) and the more recent 'Expectation-Maximization' approach reported by Pece and Worrall (2002).	Univ Karlsruhe, Inst Algorithmen & Kognit Syst, D-76128 Karlsruhe, Germany	Helmholtz Association; Karlsruhe Institute of Technology	Nagel, HH (corresponding author), Univ Karlsruhe, Inst Algorithmen & Kognit Syst, Postfach 6980, D-76128 Karlsruhe, Germany.	nagel@iaks.uni-karlsruhe.de						Badenas J, 2001, PATTERN RECOGN, V34, P661, DOI 10.1016/S0031-3203(00)00014-5; Brand M, 2000, IEEE T PATTERN ANAL, V22, P844, DOI 10.1109/34.868685; Collins RT, 2000, IEEE T PATTERN ANAL, V22, P745, DOI 10.1109/TPAMI.2000.868676; Cortelazzo GM, 2004, COMPUT VIS IMAGE UND, V96, P269, DOI 10.1016/j.cviu.2004.05.001; Cremers D, 2003, IMAGE VISION COMPUT, V21, P77, DOI 10.1016/S0262-8856(02)00128-2; Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909; Dahlkamp H, 2004, LECT NOTES COMPUT SC, V3175, P71; Dahlkamp H, 2006, LECT NOTES COMPUT SC, V3667, P38; DALKAMP H, 2004, 76128 U KARLSR I ALG; DeCarlo D., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P132, DOI 10.1109/CVPR.1999.784620; Duric Z, 2002, PATTERN RECOGN, V35, P1339, DOI 10.1016/S0031-3203(01)00119-4; ECARLO, 1996, P IEEE C COMP VIS PA, P231; Ferryman JM, 1998, 1998 IEEE WORKSHOP ON VISUAL SURVEILLANCE, PROCEEDINGS, P73; Ferryman JM, 2000, INT J COMPUT VISION, V37, P187, DOI 10.1023/A:1008155721192; FERRYMAN JM, 2003, P JOINT IEEE INT WOR; FITZGIBBON AW, 2000, VERNON 2000, P891; Fua P, 2000, INT J COMPUT VISION, V38, P153, DOI 10.1023/A:1008105802790; Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716; Haag M, 1999, INT J COMPUT VISION, V35, P295, DOI 10.1023/A:1008112528134; HAAG M, 1998, DISSERTATIONEN KUNST, V193; Horn B., 1986, ROBOT VISION, P1; *IEEE COMP SOC, ICCV 2001 8 INT C CO, V1; Jang DS, 2000, PATTERN RECOGN, V33, P1135, DOI 10.1016/S0031-3203(99)00100-4; Jurie F, 2002, PATTERN RECOGN, V35, P317, DOI 10.1016/S0031-3203(01)00031-0; Kakadiaris I, 2000, IEEE T PATTERN ANAL, V22, P1453, DOI 10.1109/34.895978; Kang DJ, 2003, PATTERN RECOGN, V36, P79, DOI 10.1016/S0031-3203(02)00046-8; Kang HG, 2005, PATTERN RECOGN, V38, P1045, DOI 10.1016/j.patcog.2004.12.008; Kastrinaki V, 2003, IMAGE VISION COMPUT, V21, P359, DOI 10.1016/S0262-8856(03)00004-0; Kato J, 2002, IEEE T PATTERN ANAL, V24, P1291, DOI 10.1109/TPAMI.2002.1033221; KIM Z, 2003, P 9 INT C COMP VIS, V1; KOLLNIG H, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P569, DOI 10.1109/ICCV.1995.466888; KOLLNIG H, 1996, LECT NOTES COMPUTER, V1064; Lee L, 2000, IEEE T PATTERN ANAL, V22, P758, DOI 10.1109/34.868678; Lerdsudwichai C, 2005, PATTERN RECOGN, V38, P1059, DOI 10.1016/j.patcog.2004.11.022; Magee DR, 2004, IMAGE VISION COMPUT, V22, P143, DOI 10.1016/S0262-8856(03)00145-8; Mansouri AR, 2002, IEEE T PATTERN ANAL, V24, P947, DOI 10.1109/TPAMI.2002.1017621; MIDDENDORF M, 2001, ICCV 2001; MIDDENDORF M, 2004, AUSWERTUNG LOKALER G; Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897; Nagel HH, 2004, AI MAG, V25, P31; Ning HZ, 2004, PATTERN RECOGN, V37, P1423, DOI 10.1016/j.patcog.2004.01.011; Nummiaro K, 2003, IMAGE VISION COMPUT, V21, P99, DOI 10.1016/S0262-8856(02)00129-4; Pai CJ, 2004, PATTERN RECOGN, V37, P1025, DOI 10.1016/j.patcog.2003.10.005; PAJDLA T, 2004, LECT NOTES COMPUTER, V3021; Pece AEC, 2003, IMAGE VISION COMPUT, V21, P1, DOI 10.1016/S0262-8856(02)00124-5; PECE AEC, 2002, LECT NOTES COMPUTER, V2350; PECE AEC, 2003, P 3 WORKSH STAT COMP; Pei SC, 1998, PATTERN RECOGN, V31, P333, DOI 10.1016/S0031-3203(97)00044-7; Polat E, 2003, PATTERN RECOGN, V36, P2127, DOI 10.1016/S0031-3203(03)00041-4; Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520; Remagnino P, 2004, PATTERN RECOGN, V37, P675, DOI 10.1016/j.patcog.2003.09.017; Ricquebourg Y, 2000, IEEE T PATTERN ANAL, V22, P797, DOI 10.1109/34.868682; SHASHUA A, 2001, ICCV 2001; Sminchisescu C, 2005, IEEE T PATTERN ANAL, V27, P727, DOI 10.1109/TPAMI.2005.104; Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677; Tan TN, 1998, INT J COMPUT VISION, V27, P5, DOI 10.1023/A:1007924428535; Tissainayagam P, 2005, PATTERN RECOGN, V38, P105, DOI 10.1016/j.patcog.2004.05.011; Tsap LV, 2000, IEEE T PATTERN ANAL, V22, P526, DOI 10.1109/34.857007; Vacchetti L, 2004, IEEE T PATTERN ANAL, V26, P1385, DOI 10.1109/TPAMI.2004.92; Veenman CJ, 2003, PATTERN RECOGN, V36, P2049, DOI 10.1016/S0031-3203(03)00037-2; VERNON D, 2000, LECT NOTES COMPUTER, V1842; Wang FL, 2003, CHINESE LAW GOV, V36, P3, DOI 10.2753/CLG0009-460936043; WESTPHAL H, 1986, COMPUT VISION GRAPH, V34, P302, DOI 10.1016/S0734-189X(86)80045-7; XIAO J, 2004, COMPUTER VISION IMAG, V96, P294; Yam CY, 2004, PATTERN RECOGN, V37, P1057, DOI 10.1016/j.patcog.2003.09.012; Zhang YW, 2000, LECT NOTES COMPUT SC, V1888, P339; Zhu ZG, 2004, COMPUT VIS IMAGE UND, V96, P294, DOI 10.1016/j.cviu.2004.03.011	68	17	18	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2007	73	2					139	157		10.1007/s11263-006-9786-4	http://dx.doi.org/10.1007/s11263-006-9786-4			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	146OI					2022-12-18	WOS:000244943500002
J	Ramirez-Manzanares, A; Rivera, N				Ramirez-Manzanares, Alonso; Rivera, Niariano			Basis tensor decomposition for restoring intra-voxel structure and stochastic walks for inferring brain connectivity in DT-MRI	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	2nd IEEE Workshop on Variational, Geometric and Level Set Methods held in Conjunction with the IEEE International Conference on Computer Vision	OCT, 2003	Nice, FRANCE	IEEE, French Natl Inst Res Comp Sci & Control, Siemens Corp Res, Imaging & Visualizat Dept		DT-MRI; DTI; fiber tractography; intra-voxel structure; high angular resolution diffusion imaging; multi-tensor; stochastic particle walks	DIFFUSION; FEATURES	We present a regularized method for solving an inverse problem in Diffusion Tensor Magnetic Resonance Imaging (DT-MRI) data. Ill the case of brain images. DT-MR imagery technique produces a tensor field that indicates the local orientation of nerve bundles. Now days. the spatial resolution of this technique is limited by the partial volume effect produced in voxels that contain fiber crossings or bifurcations. In this paper. we proposed a method for recovering the intra-voxel information and inferring the brain connectivity. We assume that the observed tensor is a linear combination of a given tensor basis, therefore, the aim of our approach is the computation of the unknown coefficients of this linear combination. By regularizing the problem. we introduce the needed prior information about the piecewise smoothness of nerve bundles orientation. As a result, we recover a multi-tensor field. Moreover, for estimating the nerve bundles trajectory, we propose a method based on stochastic walks of particles through the computed multi-tensor field. The performance of the method is demonstrated by experiments in both synthetic and real data.	Ctr Invest Matemat AC, Guanajuato 36000, Mexico		Ramirez-Manzanares, A (corresponding author), Ctr Invest Matemat AC, Apdo Postal 402, Guanajuato 36000, Mexico.	alram@cimat.mx; mrivera@cimat.mx		Rivera, Mariano/0000-0002-3211-2467				Basser PJ, 2000, MAGNET RESON MED, V44, P625, DOI 10.1002/1522-2594(200010)44:4<625::AID-MRM17>3.0.CO;2-O; Basser PJ, 2002, IEICE T INF SYST, VE85D, P15; Basser PJ, 1996, J MAGN RESON SER B, V111, P209, DOI [10.1006/jmrb.1996.0086, 10.1016/j.jmr.2011.09.022]; BJORNEMO M, 2002, WHITE MATTER FIBER T; Buxton R., 2002, INTRO FUNCTIONAL MAG; Gee JC, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING, PROCEEDINGS, P477, DOI 10.1109/ISBI.2002.1029298; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Li S. Z., 2001, COMP SCI W; LUTKENHONER B, 1991, ACTA OTO-LARYNGOL, P52; ODONNELL L, 2002, 5 INT C MED IM COMP; Pajevic S, 2002, J MAGN RESON, V154, P85, DOI 10.1006/jmre.2001.2452; Parker GJM, 2002, IEEE T MED IMAGING, V21, P505, DOI 10.1109/TMI.2002.1009386; Poldrack RA, 2001, DYSLEXIA, FLUENCY, AND THE BRAIN, P213; Poupon C, 2000, NEUROIMAGE, V12, P184, DOI 10.1006/nimg.2000.0607; RAMIREZMANZANAR.A, 2004, P IEEE MED IM C 2004, V7, P4207; RAMIREZMANZANAR.A, 2003, 2 WORKSH VAR LEV SET, P71; Ruiz-Alzola J, 2000, LECT NOTES COMPUT SC, V1935, P541; Tschumperle D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P116; TSCHUMPERLE D, 2003, VLSM 03, P255; TSCHUMPERLE D, 2002, IJCV INT J COMPUTER; TUCH D, 2002, THESIS HARVARD MIT; Tuch DS, 2002, MAGNET RESON MED, V48, P577, DOI 10.1002/mrm.10268; TUCH DS, 1999, P 7 ANN M ISMRM, V321; VIGUERAS F, 2001, THESIS CIMAT; Wang Z, 2003, LECT NOTES COMPUT SC, V2732, P660; Weinstein AR, 2003, J GEN INTERN MED, V18, P306; Westin CF, 2002, MED IMAGE ANAL, V6, P93, DOI 10.1016/S1361-8415(02)00053-1; Westin CF, 1999, LECT NOTES COMPUT SC, V1679, P441; WESTIN CF, 2002, P INT SOC MAGN RES M; Wiegell MR, 2000, RADIOLOGY, V217, P897, DOI 10.1148/radiology.217.3.r00nv43897; ZHANG S, 2000, IN PRESS VISUALIZATI; Zhukov L, 2003, J ELECTRON IMAGING, V12, P125, DOI 10.1117/1.1527628; Zhukov L, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P387, DOI 10.1109/VISUAL.2002.1183799	34	17	17	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	1					77	92		10.1007/s11263-006-6855-7	http://dx.doi.org/10.1007/s11263-006-6855-7			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	063QH					2022-12-18	WOS:000239034100006
J	Zelnik-Manor, L; Machline, M; Irani, M				Zelnik-Manor, Lihi; Machline, Moshe; Irani, Michal			Multi-body factorization with uncertainty: Revisiting motion consistency	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Workshop on Vision and Modelling of Dynamic Scenes held in Conjuction with the European Conference on Computer Vision	2002	Copenhagen, DENMARK			multi-body factorization; dense segmentation; directional uncertainty		Dynamic analysis of video sequences often relies on the segmentation of the sequence into regions of consistent motions. Approaching this problem requires a definition of which motions are regarded as consistent. Common approaches to motion segmentation usually group together points or image regions that have the same motion between successive frames (where the same motion can be 2D, 3D, or non-rigid). In this paper we define a new type of motion consistency, which is based on temporal consistency of behaviors across multiple frames in the video sequence. Our definition of consistent "temporal behavior" is expressed in terms of multi-frame linear subspace constraints. This definition applies to 2D, 3D, and some non-rigid motions without requiring prior model selection. We further show that our definition of motion consistency extends to data with directional uncertainty, thus leading to a dense segmentation of the entire image. Such segmentation is obtained by applying the new motion consistency constraints directly to covariance-weighted image brightness measurements. This is done without requiring prior correspondence estimation nor feature tracking.	CALTECH, Pasadena, CA 91125 USA; Weizmann Inst Sci, Rehovot, Israel	California Institute of Technology; Weizmann Institute of Science	Zelnik-Manor, L (corresponding author), CALTECH, Pasadena, CA 91125 USA.	lihi@vision.caltech.edu; michal.irani@weizmann.ac.il						Anandan P, 2002, INT J COMPUT VISION, V49, P101, DOI 10.1023/A:1020137420717; AYER S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P777, DOI 10.1109/ICCV.1995.466859; Boult T. E., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P179, DOI 10.1109/WVM.1991.212809; Brand M, 2001, PROC CVPR IEEE, P456; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; COSTEIRA J, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1071, DOI 10.1109/ICCV.1995.466815; Gear CW, 1998, INT J COMPUT VISION, V29, P133, DOI 10.1023/A:1008026310903; Ichimura N, 2000, PROC CVPR IEEE, P446, DOI 10.1109/CVPR.2000.854877; Irani M, 2002, INT J COMPUT VISION, V48, P173, DOI 10.1023/A:1016372015744; IRANI M, 1992, LECT NOTES COMPUT SC, V588, P282; KANATANI K, 2001, INT C COMP VIS VANC, V1, P301; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; MACHLINE M, 2002, WORKSH VIS MOD DYN S; Morris DD, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P696, DOI 10.1109/ICCV.1998.710793; TORR L, 1998, PHILOS T R SOC A, P1321; TORR PHS, 1998, ECCV98, V1, P511; Torresani L, 2001, PROC CVPR IEEE, P493; WANG J, 1993, IEEE C COMP VIS PATT, P361	18	17	17	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2006	68	1					27	41		10.1007/s11263-005-4840-1	http://dx.doi.org/10.1007/s11263-005-4840-1			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	052JB		Green Submitted			2022-12-18	WOS:000238228900003
J	Valdes, A; Ronda, JI; Gallego, G				Valdes, A; Ronda, JI; Gallego, G			The absolute line quadric and camera autocalibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CALIBRATION	We introduce a geometrical object providing the same information as the absolute conic: the absolute line quadric (ALQ). After the introduction of the necessary exterior algebra and Grassmannian geometry tools, we analyze the Grassmannian of lines of P-3 from both the projective and Euclidean points of view. The exterior algebra setting allows then to introduce the ALQ as a quadric arising very naturally from the dual absolute quadric. We fully characterize the ALQ and provide clean relationships to solve the inverse problem. i.e., recovering the Euclidean structure of space from the ALQ. Finally we show how the ALQ turns Out to be particularly suitable to address the Euclidean autocalibration of a set of cameras with square pixels and otherwise varying intrinsic parameters, providing new linear and non-linear algorithms for this problem. We also provide experimental results showing the good performance of the techniques.	Univ Complutense Madrid, Dept Geometria & Topol, E-28040 Madrid, Spain; Univ Politecn Madrid, Grp Tratamiento Imagenes, E-28040 Madrid, Spain	Complutense University of Madrid; Universidad Politecnica de Madrid	Valdes, A (corresponding author), Univ Complutense Madrid, Dept Geometria & Topol, E-28040 Madrid, Spain.	Antonio_Valdes@mat.ucm.es; jir@gti.ssr.upm.es; ggb@gti.ssr.upm.es	Gallego, Guillermo/I-7131-2012; Ronda, José Ignacio/T-7334-2018	Gallego, Guillermo/0000-0002-2672-9241; Ronda, José Ignacio/0000-0003-1430-1835; Valdes, Antonio/0000-0001-5930-8307				BARNABEI M, 1985, J ALGEBRA, V96, P120, DOI 10.1016/0021-8693(85)90043-2; Bayro-Corrochano E, 2002, PATTERN RECOGN, V35, P169, DOI 10.1016/S0031-3203(00)00182-5; Bourbaki N., 1970, ELEMENTS MATH; CARLSSON S, 1993, COMPUTER VISION JOIN, P145; DUMMIT D, 1999, ABSTRACT ALGEBRA; Gill P. E., 1981, PRACTICAL OPTIMIZATI; Harris J., 2013, ALGEBRAIC GEOMETRY 1, V133, P4; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HEYDEN A, 1998, EUR C COMP VIS FREIB; HEYDEN A, 1998, AS C COMP VIS HONG K; KAMINSKI JY, 2001, COMPUTER VISION 2001, P181; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; POLLEFEYS M, 1998, P ICCV 98; PONCE J, 2000, P SMILE; Pottmann Helmut, 2001, MATH VISUAL, V2; Semple J. G., 1998, ALGEBRAIC PROJECTIVE; SEO Y, ICCV 00; TRIGGS W, 1997, P IEEE C COMP VIS PA, P609; TRIGGS W, 1995, IEEE INT C COMP VIS; Valdes A, 2005, J MATH IMAGING VIS, V23, P167, DOI 10.1007/s10851-005-6464-z; Wells R. O., 1980, DIFFERENTIAL ANAL CO	22	17	17	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2006	66	3					283	303		10.1007/s11263-005-3677-y	http://dx.doi.org/10.1007/s11263-005-3677-y			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	028GP		Green Submitted, Green Accepted			2022-12-18	WOS:000236475400004
J	Karacali, B; Snyder, W				Karacali, B; Snyder, W			Noise reduction in surface reconstruction from a given gradient field	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						surface reconstruction; gradient field; noise reduction; thresholding; projection; gradient space; orthonormal representation; multiscale analysis	PHOTOMETRIC STEREO; LIGHT-SOURCE; LEAP-FROG; SHAPE; ALGORITHM	We present a gradient space technique for noise reduction in surfaces reconstructed from a noisy gradient field. We first analyze the error sources in the recovered gradient field of a surface using a three-image photometric stereo method. Based on this analysis, we propose an additive noise model to describe the errors in the surface gradient estimates. We then use a vector space formulation and construct a multiscale orthonormal expansion for gradient fields. Using the sparse representation properties of this expansion, we develop techniques for reducing the gradient field noise by coefficient selection with thresholding. The simulation results indicate that the proposed technique provides significant improvement on the noise levels of both the estimated gradient fields and the reconstructed surfaces under heavy noise levels. Furthermore, the experiments using noisy photometric stereo image triplets of real range data suggest that the additive model remains viable after the nonlinear photometric stereo operation to provide accurate noise removal.	Univ Penn, Dept Radiol, Sect Biomed Image Anal, Philadelphia, PA 19104 USA; N Carolina State Univ, Dept Elect & Comp Engn, Raleigh, NC 27695 USA	University of Pennsylvania; University of North Carolina; North Carolina State University	Karacali, B (corresponding author), Univ Penn, Dept Radiol, Sect Biomed Image Anal, 3600 Market St Suite 380, Philadelphia, PA 19104 USA.	bilge@rad.upenn.edu; wes@eos.ncsu.edu	KARAÇALI, Bilge/A-1347-2018	KARAÇALI, Bilge/0000-0002-7765-6329				BUZBEE BL, 1970, SIAM J NUMER ANAL, V7, P627, DOI 10.1137/0707049; Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1532, DOI 10.1109/83.862633; Courant R., 1962, METHODS MATH PHYS, VII; DAUBECHIES I, 1988, COMMUN PUR APPL MATH, V41, P909, DOI 10.1002/cpa.3160410705; DELFT P, 1981, J MATH ANAL APPL, V34, P235; Donoho D. L., 1994, COMPTES RENDUS ACA 1, VI, P319; EMBRECHTS P, 1997, APPL MATH STOCHASTIC; Frankot R. T., 1989, SHAPE SHADING; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; GALO M, 1996, IEEE INT C IM PROC, V2, P309; Grossberg MD, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P108, DOI 10.1109/ICCV.2001.937611; HAYAKAWA H, 1994, J OPT SOC AM A, V11, P3079, DOI 10.1364/JOSAA.11.003079; HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3; HORN BKP, 1906, ROBOT VISION; HORN KP, 1989, SHAPE SHADING; HSIEH JW, 1995, GRAPH MODEL IM PROC, V57, P343, DOI 10.1006/gmip.1995.1030; HURT NE, 1991, ACTA APPL MATH, V23, P163, DOI 10.1007/BF00048804; IKEUCHI K, 1981, IEEE T PATTERN ANAL, V3, P661, DOI 10.1109/TPAMI.1981.4767167; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; IWAHORI Y, 1994, IEICE T INF SYST, VE77D, P498; Iwahori Y, 1997, IEICE T INF SYST, VE80D, P948; Jain A. K., 1989, FUNDAMENTALS DIGITAL; JONES AG, 1994, IMAGE VISION COMPUT, V12, P411, DOI 10.1016/0262-8856(94)90025-6; Karacali B, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P525; KARACALI B, 1999, 9913 TR N CAR STAT U; KARACALI B, 2002, THESIS N CAROLINA ST; Kiang RK, 1997, P SOC PHOTO-OPT INS, V3077, P525, DOI 10.1117/12.271513; Kim BH, 1997, COMPUT VIS IMAGE UND, V66, P255, DOI 10.1006/cviu.1997.0515; KOZERA R, 1991, APPL MATH COMPUT, V44, P1, DOI 10.1016/0096-3003(91)90001-4; KRIM H, 1995, P IEEE INT C AC SPEE; Lambert J.H., 1760, PHOTOMETRIA SIVE MEN; LEE KM, 1994, CVGIP-IMAG UNDERSTAN, V59, P202, DOI 10.1006/ciun.1994.1013; Mallat S., 1999, WAVELET TOUR SIGNAL; MALLAT S, 1989, IEEE T PATTERN ANAL, V11, P7; Noakes L, 2003, LECT NOTES COMPUT SC, V2616, P419; Noakes L, 1999, PROC SPIE, V3811, P305, DOI 10.1117/12.364106; Noakes L, 1999, PROC SPIE, V3811, P317, DOI 10.1117/12.364108; Noakes L, 2003, J MATH IMAGING VIS, V18, P119, DOI 10.1023/A:1022104332058; Noakes L., 2001, LNCS, V2243, P352, DOI DOI 10.1007/3-540-45576-0; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; RAJARAM KV, 1995, IEEE T SYST MAN CYB, V25, P1289, DOI 10.1109/21.400507; SHEVLIN F, 1994, IEEE IMAGE PROC, P1007, DOI 10.1109/ICIP.1994.413507; SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; TAGARE HD, 1991, IEEE T PATTERN ANAL, V13, P133, DOI 10.1109/34.67643; Woodham R. J., 1978, Proceedings of the Society of Photo-Optical Instrumentation Engineers, vol.155. Image Understanding Systems and Industrial Applications, P136; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; Zheng Q, 1997, INT J REMOTE SENS, V18, P197, DOI 10.1080/014311697219367	48	17	23	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2004	60	1					25	44		10.1023/B:VISI.0000027788.50090.b6	http://dx.doi.org/10.1023/B:VISI.0000027788.50090.b6			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	820LW					2022-12-18	WOS:000221391600005
J	Kuijper, A; Florack, LMJ				Kuijper, A; Florack, LMJ			The relevance of non-generic events in scale space models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						scale space; catastrophe theory; critical points; topology; deep structure; multi-scale segmentation	CRITICAL-POINTS; SEGMENTATION; IMAGES; ORDER	In order to investigate the deep structure of Gaussian scale space images, one needs to understand the behaviour of spatial critical points under the influence of blurring. We show how the mathematical framework of catastrophe theory can be used to describe and model the behaviour of critical point trajectories when various different types of generic events, viz. annihilations and creations of pairs of spatial critical points, (almost) coincide. Although such events are non-generic in mathematical sense, they are not unlikely to be encountered in practice due to numerical limitations. Furthermore, the behaviour of these trajectories leads to the observation that fine-to-coarse tracking of critical points doesn't suffice, since they can form closed loops in scale space. The modelling of the trajectories include these loops. We apply the theory to an artificial image and a simulated MR image and show the occurrence of the described behaviour.	IT Univ Copenhagen, Dept Innovat, DK-2400 Copenhagen, Denmark; Tech Univ Eindhoven, Dept Biomed Engn, NL-5600 MB Eindhoven, Netherlands	IT University Copenhagen; Eindhoven University of Technology	Kuijper, A (corresponding author), IT Univ Copenhagen, Dept Innovat, Glentevej 67, DK-2400 Copenhagen, Denmark.	arjan@itu.dk; l.m.j.florack@tue.nl	Kuijper, Arjan/A-7814-2012	Kuijper, Arjan/0000-0002-6413-0061				Argyris J, 2001, CHAOS SOLITON FRACT, V12, P1, DOI 10.1016/S0960-0779(99)00161-7; Arnold V.I., 1984, CATASTROPHE THEORY; Arnold V I, 1994, ENCY MATH SCI; Arnold V. I., 1993, ENCY MATH SCI, V39; Arnold V. I., 1983, GRUNDLEHREN MATH WIS, V250; ARNOLD VI, 1992, ORDINARY DIFFERENTIA; Arnoux P, 2001, NONL PHEN COMPL SYST, V6, P1; Berkner K, 1999, P AM MATH SOC, V127, P425, DOI 10.1090/S0002-9939-99-04532-3; Bruce JW, 1984, CURVES SINGULARITIES; CLARK JJ, 1988, SINGULARITY THEORY P; Cocosco C.A., 1997, NEUROIMAGE, V5, P425, DOI DOI 10.1016/S1053-8119(97)80018-3; Damon J, 1997, ARCH RATION MECH AN, V140, P353, DOI 10.1007/s002050050071; DAMON J, 1995, J DIFFER EQUATIONS, V115, P368, DOI 10.1006/jdeq.1995.1019; Damon J, 1997, COMP IMAG VIS, V8, P147; Duncan JS, 2000, IEEE T PATTERN ANAL, V22, P85, DOI 10.1109/34.824822; Eberly D., 1994, Journal of Mathematical Imaging and Vision, V4, P353, DOI 10.1007/BF01262402; Florack L, 1996, INT J COMPUT VISION, V18, P61, DOI 10.1007/BF00126140; Florack L, 2000, J MATH IMAGING VIS, V12, P65, DOI 10.1023/A:1008304909717; Florack L., 1997, COMP IMAG VIS, V10; Florack L. M. J., 1994, Journal of Mathematical Imaging and Vision, V4, P171, DOI 10.1007/BF01249895; Florack L. M. J., 1993, Journal of Mathematical Imaging and Vision, V3, P327, DOI 10.1007/BF01664793; Florack L. M. J., 1994, Journal of Mathematical Imaging and Vision, V4, P325, DOI 10.1007/BF01262401; Florack L.M.J., 2001, UUCS200123; FLORACK LMJ, 1992, IMAGE VISION COMPUT, V10, P376, DOI 10.1016/0262-8856(92)90024-W; Fomenko A. T., 1997, TOPOLOGICAL MODELING; Gilmore R., 1981, CATASTROPHE THEORY S; Griffin L, 1997, COMP IMAG VIS, V8, P165; GRIFFIN LD, 1992, IMAGE VISION COMPUT, V10, P389, DOI 10.1016/0262-8856(92)90025-X; GRIFFIN LD, 1995, IMAGE VISION COMPUT, V13, P543, DOI 10.1016/0262-8856(95)91145-4; HENKEL R. D., 1995, LECT NOTES COMPUTER, V970, P41; Iijima T., 1962, BULL ELECT LAB, V26, P368; Johansen P, 2000, J MATH IMAGING VIS, V13, P193, DOI 10.1023/A:1011241531216; Johansen P., 1994, Journal of Mathematical Imaging and Vision, V4, P57, DOI 10.1007/BF01250004; Johansen P, 1997, COMP IMAG VIS, V8, P139; JOHANSEN P, 1996, P COP WORKSH GAUSS S; JOHANSEN P, 1997, 972 U COP DEP COM SC; JOHANSEN P, 1986, P 8 INT C PATT REC P, P215; Kalitzin S, 1997, COMP IMAG VIS, V8, P181; Kalitzin SN, 2001, IEEE T PATTERN ANAL, V23, P447, DOI 10.1109/34.922704; Kalitzin SN, 1998, J MATH IMAGING VIS, V9, P253, DOI 10.1023/A:1008376504545; KELLER R, 1999, THESIS U N CAROLINA; KERCKHOVE M, 2001, LECT NOTES COMPUTER, V2106; Kuijper A, 2003, J MATH IMAGING VIS, V18, P169, DOI 10.1023/A:1022168617945; Kuijper A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P487, DOI 10.1109/ICCV.2001.937665; Kuijper A, 1999, LECT NOTES COMPUT SC, V1682, P318; KUIJPER A, 2002, IEEE T IMAGE PROCESS; LIFSHITZ LM, 1990, IEEE T PATTERN ANAL, V12, P529, DOI 10.1109/34.56189; LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346; Lindeberg T., 1992, Journal of Mathematical Imaging and Vision, V1, P65, DOI 10.1007/BF00135225; LINDEBERG T, 1994, KLUWER INT SERIES EN; LU Y.-C., 1976, SINGULARITY THEORY I; Maslov EM, 2001, PHYSICA D, V152, P769, DOI 10.1016/S0167-2789(01)00210-X; Moss IG, 2000, PHYS LETT B, V491, P203, DOI 10.1016/S0370-2693(00)00966-7; NIELSEN M, 1999, LECT NOTES COMPUTER, V1682; Olsen OF, 1997, LECT NOTES COMPUT SC, V1252, P101; Olsen OF, 1997, COMP IMAG VIS, V8, P191; OLSEN OF, 2000, THESIS U COPENHAGEN; OTSU N, 1981, THESIS ELECT LAB IBA; Poston T., 2014, CATASTROPHE THEORY I; Rieger J. H., 1995, Journal of Mathematical Imaging and Vision, V5, P207, DOI 10.1007/BF01248372; Rieger JH, 1997, INT J COMPUT VISION, V23, P79, DOI 10.1023/A:1007915908780; RIEGER JH, 1992, BIOL CYBERN, V66, P497, DOI 10.1007/BF00204114; ROMENY BMT, 1997, LECT NOTES COMPUTER, V1252; SALDEN AH, 1996, THESIS UTRECHT U; Simmons A, 1998, IEEE T MED IMAGING, V17, P371, DOI 10.1109/42.712127; SPORRING J, 1997, COMPUTATIONAL IMAGIN, V8; Thom R., 2018, STRUCTURAL STABILITY; Thom R., 1983, PARABOLES CATASTROPH; WADA T, 1990, ICPR90, V2, P103; Weickert J, 1997, COMP IMAG VIS, V8, P45; Weickert J, 1999, J MATH IMAGING VIS, V10, P237, DOI 10.1023/A:1008344623873; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; Woodcock A., 1978, CATASTROPHE THEORY; Zeeman EC, 1977, CATASTROPHE THEORY S; ZHAO N, 1985, IECE JAPAN T D, V68, P1125; ZHAO NY, 1985, IECE JAPAN T D, V68, P508; ZHAO NY, 1985, THESIS TOKYO I TECHN	78	17	18	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2004	57	1					67	84		10.1023/B:VISI.0000013091.14851.24	http://dx.doi.org/10.1023/B:VISI.0000013091.14851.24			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	766DP		Green Submitted			2022-12-18	WOS:000188330400004
J	Rivlin, E; Rotstein, H				Rivlin, E; Rotstein, H			Control of a camera for active vision: Foveal vision, smooth tracking and saccade	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						active vision; smooth pursuit; saccade	SAMPLED-DATA CONTROL; LINEAR PERIODIC-SYSTEMS; PURSUIT; DELAYS	Several characteristics of the human oculomotor system have been suggested to be useful also for active vision mechanisms. Among others, foveal vision and a tracking scheme based on two different modes, called smooth pursuit and saccade have often been postulated or implemented. The purpose of this paper is to formulate a setup in which the benefit of implementing these schemes can be evaluated in a systematic manner, based on control considerations but incorporating image processing constraints. First, the advantage of using foveal vision is evaluated by computing the size of the foveal window which will allow tracking of the largest possible class of signals. By using linear optimal control theory, this problem can be formulated as a one-variable maximization. Second, foveal vision leads naturally to smooth pursuit, defined as the performance that can be achieved by the controller resulting in the optimal size of the foveal window. This controller is relatively simple (i.e., linear, time-invariant) as is to be expected for this control loop. Finally, when smooth pursuit fails a corrective action must be performed to re-center the target on the fovea. Recent results in linear optimal control, provide the necessary tools for addressing this challenging problem in a systematic manner.	Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel; RAFAEL, Armament Dev Author, IL-32000 Haifa, Israel; Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology; RAFAEL ADVANCED DEFENSE SYSTEMS; Technion Israel Institute of Technology	Rivlin, E (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.							BAJCSY R, 1988, P IEEE, V76; BAMIEH B, 1991, SYST CONTROL LETT, V17, P79, DOI 10.1016/0167-6911(91)90033-B; BAMIEH BA, 1992, IEEE T AUTOMAT CONTR, V37, P418, DOI 10.1109/9.126576; Bar-Shalom Y, 1988, TRACKING DATA ASS MA; BLANCHINI F, 1995, IEEE T AUTOMAT CONTR, V40, P1127, DOI 10.1109/9.388699; BROWN C, 1990, BIOL CYBERN, V63, P61, DOI 10.1007/BF00202454; BROWN C, 1990, IEEE T SYST MAN CYB, V20, P518, DOI 10.1109/21.52563; BROWN C, 1993, ARTIF INTELL, P123; Carpenter RHS, 1988, MOVEMENTS EYES; CHRISTENSEN HI, 1995, COMMUNICATION; CHRISTIAN JL, 1993, GENE DEV, V7, P1; CLARK J, 1993, ACTIVE VISION, P137; COOMBE R, 1991, STUDIES LAW POLITICS, V11, P3; COOMBS D, 1993, INT J COMPUT VISION, V11, P147, DOI 10.1007/BF01469226; DEVLIEGER JH, 1982, AUTOMATICA, V18, P239, DOI 10.1016/0005-1098(82)90111-X; DULLERUD G, 1992, IEEE T AUTOMATIC CON, P436; Ferrier N. J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P9, DOI 10.1142/S0218001493000029; FIALA JC, 1994, INT J COMPUT VISION, V12, P231, DOI 10.1007/BF01421204; KROTOV E, INT J COMPUTER VISIO, V11, P187; Milios E., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P51, DOI 10.1142/S0218001493000042; Mirkin L, 1997, AUTOMATICA, V33, P1997, DOI 10.1016/S0005-1098(97)00135-0; MIRKIN L, 1998, INT J CONTROL; MURRAY DW, 1995, INT J COMPUT VISION, V16, P205, DOI 10.1007/BF01539627; MURRAY DW, 1993, ACTIVE VISION, P155; Pahlavan K., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P33, DOI 10.1142/S0218001493000030; RIVLIN E, 1997, IEEE T AUTOMATIC CON, V43, P833; ROBINSON DA, 1968, PR INST ELECTR ELECT, V56, P1032, DOI 10.1109/PROC.1968.6455; Sharkey PM, 1996, IEE P-CONTR THEOR AP, V143, P436, DOI 10.1049/ip-cta:19960542; SWAIN MJ, 1993, INT J COMPUT VISION, V11, P109, DOI 10.1007/BF01469224; YAMAMOTO Y, 1994, IEEE T AUTOMAT CONTR, V39, P703, DOI 10.1109/9.286247	30	17	18	0	3	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2000	39	2					81	96		10.1023/A:1008166825510	http://dx.doi.org/10.1023/A:1008166825510			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	359FD					2022-12-18	WOS:000089600100002
J	Molton, N; Brady, M				Molton, N; Brady, M			Practical structure and motion from stereo when motion is unconstrained	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; stereo vision; unconstrained motion; map building; obstacle avoidance; walker-mounted	3-D	This paper describes a system which robustly estimates motion, and the 3D structure of a rigid environment, as a stereo vision platform moves through it. The system can cope with any camera motion, and any scene structure and is successful even in the presence of large jumps in camera position between the capture of successive image pairs, and when point matching is ambiguous. The system was developed to provide robust obstacle avoidance for a partially sighted person. The process described attempts to maximise use of the abundant information present in a stereo sequence. Key features include the use of multiple stereo match hypotheses, efficient motion computation from three images, and the use of this motion to ensure reliable matching, and to eliminate multiple stereo matches. Points are reconstructed in 3D space and tracked in a static coordinate frame with a Kalman Filter. This results in good 3D scene reconstructions. Structure which is impossible to match with certainty is absent, rather than being incorrectly reconstructed. As a result, the system is appropriate for obstacle detection. The results of processing some indoor and outdoor scenes, are given in the paper, and practical issues are highlighted throughout.	Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	University of Oxford	Molton, N (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.							BOUGUET JY, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P645, DOI 10.1109/ICCV.1995.466877; Ferrari F., 1990, Proceedings. IROS '90. IEEE International Workshop on Intelligent Robots and Systems '90. Towards a New Frontier of Applications (Cat. No.90TH0332-7), P703, DOI 10.1109/IROS.1990.262486; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; HARRIS C, 1987, P ALV C, P189; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; HIGHNAM RP, 1999, MAMMOGRAPHIC IMAGE P; Konolige K., 1998, Robotics Research. Eighth International Symposium, P203; LI F, 1995, P ISRR 95 GERM; MATTHIES L, 1987, IEEE T ROBOTIC AUTOM, V3, P239, DOI 10.1109/JRA.1987.1087097; MATTHIES L, 1994, IEEE T ROBOTIC AUTOM, V10, P783, DOI 10.1109/70.338533; Matthies L., 1996, ROBOTICS RES, P475; Molton N, 1999, ROBOT AUTON SYST, V26, P185, DOI 10.1016/S0921-8890(98)00068-2; MOLTON ND, IN PRESS ROBOTICS AU; MOLTON ND, 1999, THESIS U OXFORD; MORAVEC HP, 1983, P IEEE, V71, P872, DOI 10.1109/PROC.1983.12684; MURRAY D, 1998, P IEEE WORKSH PERC M, P19; POLLARD SB, 1991, 3D MODEL RECOGNITION, P33; SE S, 1998, THESIS U OXFORD; SE S, 1998, P 3 AS C COMP VIS AC, P152; Stein GP, 1998, PROC CVPR IEEE, P211, DOI 10.1109/CVPR.1998.698611; Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; WANG H, 1995, IMAGE VISION COMPUT, V13, P695, DOI 10.1016/0262-8856(95)98864-P; WOLFE WJ, 1991, IEEE T PATTERN ANAL, V13, P66, DOI 10.1109/34.67632; YOUNG GSJ, 1990, IEEE T PATTERN ANAL, V12, P735, DOI 10.1109/34.57666; ZHANG ZY, 1992, IEEE T PATTERN ANAL, V14, P1141, DOI 10.1109/34.177380	26	17	18	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2000	39	1					5	23		10.1023/A:1008191416557	http://dx.doi.org/10.1023/A:1008191416557			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	352WA					2022-12-18	WOS:000089241300001
J	Thompson, WB				Thompson, WB			Exploiting discontinuities in optical flow	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						visual motion; optical flow; discontinuities; edges	VISUAL-MOTION; FIELDS; OPTIMIZATION; SEGMENTATION; IMAGES; SCENES	Most optical flow estimation techniques have substantial difficulties dealing with flow discontinuities. Methods which simultaneously detect flow boundaries and use the detected boundaries to aid in flow estimation can produce significantly improved results. Current approaches to implementing these methods still have important limitations, however. We demonstrate three such problems: errors due to the mixture of image properties across boundaries, an intrinsic ambiguity in boundary location when only short sequences are considered, and difficulties insuring that the motion of a boundary aids in flow estimation for the surface to which it is attached without corrupting the flow estimates for the occluded surface on the other side. The first problem can be fixed by basing flow estimation only on image changes at edges. The second requires an analysis of longer time intervals. The third can be aided by using a boundary detection mechanism which classifies the sides of boundaries as occluding and occluded at the same time as the boundaries are detected.	Univ Utah, Dept Comp Sci, Salt Lake City, UT 84112 USA	Utah System of Higher Education; University of Utah	Thompson, WB (corresponding author), Univ Utah, Dept Comp Sci, Salt Lake City, UT 84112 USA.	thompson@cs.utah.edu						Adelson E. H., 1986, Proceedings of the Workshop on Motion: Representation and Analysis (Cat. No.86CH2322-6), P151; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; BARNARD ST, 1980, IEEE T PATTERN ANAL, V2, P333, DOI 10.1109/TPAMI.1980.4767032; BARRY AR, 1994, BIOPHARM-TECHNOL BUS, V7, P43; Bergen J. R., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P27, DOI 10.1109/ICCV.1990.139486; Black M. J., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P33, DOI 10.1109/ICCV.1990.139487; FENNEMA CL, 1979, COMPUT VISION GRAPH, V9, P301, DOI 10.1016/0146-664X(79)90097-2; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; GAMBLE E, 1987, 970 AI MIT; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Heeger D. J., 1988, INT J COMPUT VISION, V1, P279; HEITZ F, 1993, IEEE T PATTERN ANAL, V15, P1217, DOI 10.1109/34.250841; HILDRETH E, 1983, MEASUREMENT VISUAL M; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HUTCHINSON J, 1988, COMPUTER, V21, P52, DOI 10.1109/2.31; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; KAPLAN GA, 1969, PERCEPT PSYCHOPHYS, V6, P193, DOI 10.3758/BF03207015; Koch C., 1989, Proceedings. Workshop on Visual Motion (IEEE Cat. No.89CH2716-9), P62, DOI 10.1109/WVM.1989.47095; KONRAD J, 1992, IEEE T PATTERN ANAL, V14, P910, DOI 10.1109/34.161350; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; MURRAY DW, 1987, IEEE T PATTERN ANAL, V9, P220, DOI 10.1109/TPAMI.1987.4767896; MUTCH KM, 1985, IEEE T PATTERN ANAL, V7, P133, DOI 10.1109/TPAMI.1985.4767638; MUTCH KM, 1983, MULTIRESOLUTION IMAG, P343; NAGEL HH, 1995, INT J COMPUT VISION, V15, P273; PRAZDNY K, 1985, BIOL CYBERN, V52, P93, DOI 10.1007/BF00363999; RANGARAJAN K, 1988, P 2 INT C COMP VIS, P90; SCHUNCK BG, 1989, IEEE T PATTERN ANAL, V11, P1010, DOI 10.1109/34.42834; SCOTT GL, 1988, LOCAL GLOBAL INTERPR; SMITLEY DL, 1984, P 7 INT C PATT REC, P433; THOMPSON WB, 1985, IEEE T PATTERN ANAL, V7, P374, DOI 10.1109/TPAMI.1985.4767677; THOMPSON WB, 1981, COMPUTER, V14, P20, DOI 10.1109/C-M.1981.220559; THOMPSON WB, 1995, UUCS95015 U UT DEP C; THOMPSON WB, 1992, COMPUTER VISION GRAP, P69; THOMPSON WB, 1992, BIOL CYBERN, P327; Waxman A. M., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P717, DOI 10.1109/CVPR.1988.196313; YONAS A, 1990, ABSTR INV OPHTH VIS, V31, P524	38	17	17	1	5	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1998	30	3					163	173		10.1023/A:1008026031844	http://dx.doi.org/10.1023/A:1008026031844			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	156FL					2022-12-18	WOS:000077990900001
J	Metaxas, D; Koh, E; Badler, NI				Metaxas, D; Koh, E; Badler, NI			Multi-level shape representation using global deformations and locally adaptive finite elements	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						multi-level shape representation; adaptive finite elements; deformable models; physics-based modeling	DEFORMABLE SURFACES	We present a model-based method for the multi-level shape, pose estimation and abstraction of an object's surface from range data. The surface shape is estimated based on the parameters of a superquadric that is subjected to global deformations (tapering and bending) and a varying number of levels of local deformations. Local deformations are implemented using locally adaptive finite elements whose shape functions are piecewise cubic functions with C-1 continuity. The surface pose is estimated based on the model's translational and rotational degrees of freedom. The algorithm first does a coarse fit, solving for a first approximation to the translation, rotation and global deformation parameters and then does several passes of mesh refinement, by locally subdividing triangles based on the distance between the given datapoints and the model. The adaptive finite element algorithm ensures that during subdivision the desirable finite element mesh generation properties of conformity, non-degeneracy and smoothness are maintained. Each pass of the algorithm uses physics-based modeling techniques to iteratively adjust the global and local parameters of the model in response to forces that are computed from approximation errors between the model and the data. We present results demonstrating the multi-level shape representation for both sparse and dense range data.			Metaxas, D (corresponding author), UNIV PENN, DEPT COMP & INFORMAT SCI, 200 S 33RD ST, PHILADELPHIA, PA 19104 USA.							BAJAJ CL, 1995, P SIGGRAPH, P109; Barr A. H., 1981, IEEE Computer Graphics and Applications, V1, P11, DOI 10.1109/MCG.1981.1673799; BRAMBLE JH, 1991, MATH COMPUT, V56, P1, DOI 10.1090/S0025-5718-1991-1052086-4; COHEN I, 1992, CVGIP-IMAG UNDERSTAN, V56, P242, DOI 10.1016/1049-9660(92)90041-Z; COHEN L, 1991, PATTERN ANAL MACHINE, V11; DECARLO D, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P834, DOI 10.1109/ICCV.1995.466851; DECARLO D, 1996, IEEE PAMI, V18, P834; DELINGETTE H, 1992, IMAGE VISION COMPUT, V10, P132, DOI 10.1016/0262-8856(92)90065-B; DELINGETTE H, 1993, P INT C COMP VIS BER; DELINGETTE H, 1994, 2214 INRLA S ANT; Dhatt G., 1984, FINITE ELEMENT METHO; Hoppe H., 1994, P SIGGRAPH 94, P295, DOI DOI 10.1145/192161.192233; Huang W.-C., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P833, DOI 10.1109/CVPR.1992.223246; KOH E, 1994, P 3 EUR C COMP VIS E, P441; METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727; METAXAS D, 1996, P EUR C COMP VIS ECC, P550; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P715, DOI 10.1109/34.85660; RIVARA MC, 1984, INT J NUMER METH ENG, V20, P745, DOI 10.1002/nme.1620200412; ROSENBERG IG, 1975, MATH COMPUT, V29, P390, DOI 10.1090/S0025-5718-1975-0375068-5; SCHMITT FJM, 1986, P SIGGRAPH 86, P179; STYNES M, 1980, MATH COMPUT, V35, P1195, DOI 10.1090/S0025-5718-1980-0583497-1; Tanaka H. T., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P88, DOI 10.1109/CVPR.1993.340974; TERZOPOULOS D, 1991, IEEE T PATTERN ANAL, V13, P703, DOI 10.1109/34.85659; TERZOPOULOS D, 1988, ARTIF INTELL, V36, P91, DOI 10.1016/0004-3702(88)90080-X; Vasilescu M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P829, DOI 10.1109/CVPR.1992.223247; Vemuri B. C., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P307, DOI 10.1109/CVPR.1993.340964; Zienkiewicz OC, 1977, FINITE ELEMENT METHO; [No title captured]; [No title captured]	29	17	17	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1997	25	1					49	61		10.1023/A:1007929702347	http://dx.doi.org/10.1023/A:1007929702347			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YB326					2022-12-18	WOS:A1997YB32600003
J	Pahlavan, K; Uhlin, T; Eklundh, JO				Pahlavan, K; Uhlin, T; Eklundh, JO			Dynamic fixation and active perception	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								Fixation is the link between the physical environment and the visual observer, both of which can be dynamic. That is, dynamic fixation serves the task of preserving a reference point in the world, despite relative motion. In this respect, fixation is dynamical in two senses: in response to voluntary changes of fixation point or attentive cues-gaze shiftings, and in response to the desire to compensate for the retinal slip-gaze holding. The work presented here, addresses the vergence movement and preservation of binocular fixation during smooth pursuit. This movement is a crucial component of fixation. The two vergence processes, disparity vergence and accommodative vergence, are described; a novel algorithm for robust disparity vergence and an active approach for blur detection and depth from defocus are presented. The main characteristics of the disparity vergence technique are the simplicity of the algorithm, the influence of both left and right images in the course of fixation and the agreement with the fixation model of primates. The major characteristic of the suggested algorithm for blur detection is its active approach which makes it suitable for achieving qualitative and reasonable depth estimations without unrealistic assumptions about the structures in the images. The paper also covers the integration of the two processes disparity vergence and accommodation vergence which are in turn accomplished by an integration of the disparity and blur stimuli. This integration is accounted for in both static and dynamic experiments.			Pahlavan, K (corresponding author), ROYAL INST TECHNOL,DEPT NUMER ANAL & COMP SCI,CVAP,S-10044 STOCKHOLM,SWEDEN.							Abbott A. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P532, DOI 10.1109/CCV.1988.590034; ALPERN M, 1956, AM J OPHTHALMOL, V42, P289, DOI 10.1016/0002-9394(56)90380-4; ALPERN M, 1957, ARCH OPHTHALMOL-CHIC, V57, P345; CALWAY AD, 1992, P BMVC, P237; Carpenter RHS, 1988, MOVEMENTS EYES; Clark J. J., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P514, DOI 10.1109/CCV.1988.590032; COOMBS DJ, 1992, THESIS U ROCHESTER; Fry G., 1937, AM J OPTOM PHYS OPT, V14, P402, DOI [10.1097/00006324-193711000-00001, DOI 10.1097/00006324-193711000-00001]; Fry G. A., 1939, AM J OPTOM, V16, P325; HORII A, 1992, TRITANAP16 ROYAL I T; HORII A, 1992, TRITANAP9215 ROYAL I; HORN BKP, 1989, ROBOT VISION, P25; JARVIS RA, 1976, MICROSCOPE, V24, P163; Jepson A. D., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P398, DOI 10.1109/CVPR.1989.37877; JULESZ B, 1971, F CYCLOPEAN PERCEPTI; KNOLL HENRY A., 1949, AMER JOUR OPTOM AND ARCH AMER ACAD OPTOM, V26, P346; Krotkov EP, 1989, ACTIVE COMPUTER VISI; LUNEBURG R, 1948, MATH ANAL BINOCULAR; Maddox E E, 1886, J Anat Physiol, V20, P475; MAKI A, 1993, P 8 SCAND C IM AN TO; MARG E, 1949, AM J OPTOMETRY, V27, P217; MARG ELWIN, 1949, AMER JOUR OPTOM AND ARCH AMER ACAD OPTOM, V26, P183; Muller J., 1826, VERGLEICHENDEN PHYSL; PAHLAVAN K, 1992, CVGIP-IMAG UNDERSTAN, V56, P41, DOI 10.1016/1049-9660(92)90084-G; PAHLAVAN K, 1994, MECHATRONICS ACTIVE, P113; PAHLAVAN K, 1993, TRITANAP9316 ROY I T; PAHLAVAN K, 1992, 2ND P EUR C COMP VIS, P526; Pahlavan K., 1993, ACTIVE PERCEPTION, P19; Pentland A., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P256, DOI 10.1109/CVPR.1989.37858; Pentland A. P., 1987, P IEEE T PAMI, P523; SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114; Subbarao M., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P149, DOI 10.1109/CCV.1988.589986; Tenenbaum J.M., 1970, THESIS STANFORD U; Yarbus A. L., 1967, EYE MOVEMENTS VISION, P171; ZHANG W, 1993, P 4 INT C COMP VIS B, P183	35	17	17	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1996	17	2					113	135		10.1007/BF00058748	http://dx.doi.org/10.1007/BF00058748			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TZ498					2022-12-18	WOS:A1996TZ49800002
J	FIALA, JC; LUMIA, R; ROBERTS, KJ; WAVERING, AJ				FIALA, JC; LUMIA, R; ROBERTS, KJ; WAVERING, AJ			TRICLOPS - A TOOL FOR STUDYING ACTIVE VISION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							EYE	The design, performance, and application of The Real-time, Intelligently ControLled, Optical Positioning (TRICLOPS) are described in this article. TRICLOPS is a multiresolution trinocular camera-pointing system which provides a center wide-angle view camera and two higher-resolution vergence cameras. It is a direct-drive system that exhibits dynamic performance comparable to the human visual system. The mechanical design and performance of various active vision systems are discussed and compared to those of TRICLOPS. The multiprocessor control system for TRICLOPS is described. The kinematics of the device are also discussed and calibration methods are given. Finally, as an example of real-time visual control, a problem in visual tracking with TRICLOPS is examined. In this example, TRICLOPS is shown to be capable of tracking a ball moving at 3 m/s, which results in rotational velocities of the vergence cameras in excess of 6 rad/s (344 deg/s).			FIALA, JC (corresponding author), NATL INST STAND & TECHNOL,DIV ROBOT SYST,GAITHERSBURG,MD 20899, USA.							ABBOTT AL, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P489; ALBUS JS, 1987, 1235 NAT I STAND TEC; ALLEN P, 1990, P DARA IMAGE UNDERST; ALOIMONOS J, 1989, INTEGRATION VISUAL M; ALOIMONOS J, 1987, 1ST P INT C COMP VIS, P35; Bajcsy R., 1988, Proceedings of the IEEE, V76, P966, DOI 10.1109/5.5968; Ballard D. H., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P524, DOI 10.1109/CCV.1988.590033; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; Bederson B. B., 1992, Proceedings. 1992 IEEE International Conference on Robotics And Automation (Cat. No.92CH3140-1), P658, DOI 10.1109/ROBOT.1992.220292; BROWN CM, 1989, P OWRKSHOP INTERPRET, P145; Carpenter RHS, 1988, MOVEMENTS EYES; CHACONAS K, 1990, 904286 NAT I STAND T; Clark J. J., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P514, DOI 10.1109/CCV.1988.590032; FIALA J, 1992, P AAS C GUID CONTR K; FIALA J, 1989, 894215 NAT I STAND T; GEIGER D, 1989, BIOL CYBERN, V62, P117, DOI 10.1007/BF00203000; JAMES ML, 1977, APPLIED NUMERICAL ME; KROTKOV E, 1988, IEEE T ROBOTIC AUTOM, V4, P108, DOI 10.1109/56.782; OLSON TJ, 1991, INT J COMPUT VIS, V7, P667; PAHLAVAN K, 1992, CVGIP-IMAG UNDERSTAN, V56, P41, DOI 10.1016/1049-9660(92)90084-G; POGGIO T, 1988, P DARPA IMAGE UNDERS, P177; ROBINSON DA, 1987, VISION BRAIN COOPERA; SCHWAB EC, 1986, PATERN RECOGNITION H, V2; SWAIN MJ, 1991, PROMISINO DIRECTIONS; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; WAVERING AJ, 1993, SPIE, V2056, P86; Wurtz R. H, 1989, NEUROBIOLOGY SACCADI; 1987, PIPE INTRO PIPE SYST; 1978, NASA REFERENCE PUBLI, V1024	29	17	17	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1994	12	2-3					231	250		10.1007/BF01421204	http://dx.doi.org/10.1007/BF01421204			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NL558					2022-12-18	WOS:A1994NL55800005
J	WANG, CH; SRIHARI, SN				WANG, CH; SRIHARI, SN			A FRAMEWORK FOR OBJECT RECOGNITION IN A VISUALLY COMPLEX ENVIRONMENT AND ITS APPLICATION TO LOCATING ADDRESS BLOCKS ON MAIL PIECES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									SUNY BUFFALO,DEPT COMP SCI,BUFFALO,NY 14260	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo			Srihari, Sargur N/E-8100-2011					BALLARD DH, 1978, COMPUTER VISION SYST, P271; BARNETT JA, 1981, 7TH P INT JOINT C AR, P868; Binford T. O., 1982, INT J ROBOT RES, V1, P18; Brooks R., 1984, MODEL BASED COMPUTER; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; CAVIGLIONE M, 1986, OCT P USPS ADV TECHN, P85; ERMAN LD, 1980, COMPUT SURV, V12, P213, DOI 10.1145/356810.356816; Forgy C.L., 1981, CMUCS81135; GARVEY TD, 1981, 7TH P INT JOINT C AR, P319; Hanson A., 1978, COMPUTER VISION SYST, P303; HULL JJ, 1984, 214 STAT U NEW YORK; LEHAR AF, 1986, OCT P USPS ADV TECHN, P132; LEVINE MD, 1981, IEEE T PATTERN ANAL, V3, P540, DOI 10.1109/TPAMI.1981.4767147; LITCHER A, 1986, OCT P USPS ADV TECHN, P66; MATSUYAMA T, 1985, 9TH P INT JOINT C AR, P908; MCKEOWN DM, 1985, IEEE T PATTERN ANAL, V7, P570, DOI 10.1109/TPAMI.1985.4767704; Nagao M., 1980, STRUCTURAL ANAL COMP; NII HP, 1986, AI MAG, V7, P38; NULGAONKAR PG, 1986, OCT P USPS ADV TECHN, P161; OHTA Y, 1985, KNOWLEDGE BASED INTE; ORROCK JE, 1986, OCT P USPS ADV TECHN, P104; Shafer G., 1976, MATH THEORY EVIDENCE, VVolume 1; Shapiro L. G., 1985, Second Conference on Artificial Intelligence Applications: The Engineering of Knowledge-Based Systems (Cat. No.85CH2215-2), P76; SRIHARI SN, 1987, AI MAG, V8, P25; SRIHARI SN, 1986, SUNY8609 DEP COMP SC; WANG CH, 1986, 5TH P AAAI NAT C PHI, P1133; WESLEY LP, 1982, AUG P WORKSH COMP VI, P14; YEH PS, 1987, PATTERN RECOGN, V20, P213, DOI 10.1016/0031-3203(87)90055-0; 1985, GTRI AUTOMATED PROCE; 1984, ENG REPORT OCR READA	30	17	18	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1988	2	2					125	151		10.1007/BF00133697	http://dx.doi.org/10.1007/BF00133697			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC190					2022-12-18	WOS:A1988AC19000002
J	Kortylewski, A; Liu, Q; Wang, AT; Sun, YH; Yuille, A				Kortylewski, Adam; Liu, Qing; Wang, Angtian; Sun, Yihong; Yuille, Alan			Compositional Convolutional Neural Networks: A Robust and Interpretable Model for Object Recognition Under Occlusion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Compositional models; Robustness to partial occlusion; Image classification; Object detection; Out-of-distribution generalization		Computer vision systems in real-world applications need to be robust to partial occlusion while also being explainable. In this work, we show that black-box deep convolutional neural networks (DCNNs) have only limited robustness to partial occlusion. We overcome these limitations by unifying DCNNs with part-based models into Compositional Convolutional Neural Networks (CompositionalNets)-an interpretable deep architecture with innate robustness to partial occlusion. Specifically, we propose to replace the fully connected classification head of DCNNs with a differentiable compositional model that can be trained end-to-end. The structure of the compositional model enables CompositionalNets to decompose images into objects and context, as well as to further decompose object representations in terms of individual parts and the objects' pose. The generative nature of our compositional model enables it to localize occluders and to recognize objects based on their non-occluded parts. We conduct extensive experiments in terms of image classification and object detection on images of artificially occluded objects from the PASCAL3D+ and ImageNet dataset, and real images of partially occluded vehicles from the MS-COCO dataset. Our experiments show that CompositionalNets made from several popular DCNN backbones (VGG-16, ResNet50, ResNext) improve by a large margin over their non-compositional counterparts at classifying and detecting partially occluded objects. Furthermore, they can localize occluders accurately despite being trained with class-level supervision only. Finally, we demonstrate that CompositionalNets provide human interpretable predictions as their individual components can be understood as detecting parts and estimating an objects' viewpoint.	[Kortylewski, Adam; Liu, Qing; Wang, Angtian; Sun, Yihong; Yuille, Alan] Johns Hopkins Univ, Baltimore, MD 21218 USA	Johns Hopkins University	Kortylewski, A (corresponding author), Johns Hopkins Univ, Baltimore, MD 21218 USA.	akortyl1@jhu.edu; qingliu@jhu.edu; angtianwang@jhu.edu; ysun86@jhu.edu; ayuille1@jhu.edu		Yuille, Alan L./0000-0001-5207-9249; Kortylewski, Adam/0000-0002-9146-4403	Swiss National Science Foundation [P2BSP2.181713]; Office of Naval Research [N00014-18-1-2119, N00014-20-1-2206]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); Office of Naval Research(Office of Naval Research)	Funding was provided by the Swiss National Science Foundation (P2BSP2.181713) and the Office of Naval Research (Grant No. N00014-18-1-2119 and N00014-20-1-2206).	Alain Guillaume, 2016, ARXIV161001644; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2016, ADV NEUR IN, V29; [Anonymous], 1980, PRINCIPLES ARTIFICIA; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Banerjee A, 2005, J MACH LEARN RES, V6, P1345; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Blaschko MB, 2008, PROC CVPR IEEE, P93, DOI 10.1109/cvpr.2008.4587586; Brendel Wieland, 2019, INT C LEARN REPR; Chen Y., 2008, ADV NEURAL INFORM PR, P289; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Dai JF, 2014, PROC CVPR IEEE, P2505, DOI 10.1109/CVPR.2014.321; Dechter R, 2007, ARTIF INTELL, V171, P73, DOI 10.1016/j.artint.2006.11.003; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Economist T., 2017, WHY SELF DRIVING CAR; Fawzi A., 2016, TECHNICAL REPORT; Fidler S., 2014, ARXIV14085516; Fong R, 2018, PROC CVPR IEEE, P8730, DOI 10.1109/CVPR.2018.00910; George D, 2017, SCIENCE, V358, DOI 10.1126/science.aag2612; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2015, PROC CVPR IEEE, P437, DOI 10.1109/CVPR.2015.7298641; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu ZT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2410, DOI 10.18653/v1/p16-1228; Huber P., 1981, ROBUST STAT; Jin Y., 2006, CVPR, V2, P2145; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kingma D.P, P 3 INT C LEARNING R; Kortylewski A., 2016, BRIT MACH VIS C; Kortylewski A., 2020, P IEEE C COMP VIS PA; Kortylewski A., 2017, THESIS; Kortylewski A, 2020, IEEE WINT CONF APPL, P1322, DOI 10.1109/WACV45572.2020.9093560; Kortylewski A, 2019, PROC CVPR IEEE, P11604, DOI 10.1109/CVPR.2019.01188; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343; Li A., 2018, BRIT MACH VIS C; Li X., 2019, P IEEE C COMP VIS PA, P6220; Li Y, 2013, IEEE T INTELL TRANSP, V14, P984, DOI 10.1109/TITS.2013.2250501; Liao R., 2016, ADV NEURAL INFORM PR, P5076; Lin L, 2015, IEEE T PATTERN ANAL, V37, P959, DOI 10.1109/TPAMI.2014.2359888; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011; Narasimhan N. D. R. M. V. S. G, 2019, IEEE C COMP VIS PATT; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren WH, 2018, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2018.00561; Sabour S, 2017, ADV NEUR IN, V30; Song X, 2013, PROC CVPR IEEE, P3278, DOI 10.1109/CVPR.2013.421; Stone A, 2017, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2017.85; Tabernik D, 2016, INT C PATT RECOG, P3470, DOI 10.1109/ICPR.2016.7900171; Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12; Tang W, 2017, IEEE I CONF COMP VIS, P2803, DOI 10.1109/ICCV.2017.303; Wang A., 2020, P IEEE C COMP VIS PA; Wang J., 2017, ARXIV171104451; Wang J., 2017, ARXIV170707819; Wang J., 2015, ARXIV151106855; Wu TF, 2016, IEEE T PATTERN ANAL, V38, P1829, DOI 10.1109/TPAMI.2015.2497699; Xia FT, 2016, AAAI CONF ARTIF INTE, P3632; Xiang Y., 2013, OBJECT DETECTION 3D; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Xiao M., 2019, ARXIV190903879; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yan S., 2015, SIGNAL PROCESSING, V110; Yuille A. L., 2018, ARXIV180504025; Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang QS, 2018, FRONT INFORM TECH EL, V19, P27, DOI 10.1631/FITEE.1700808; Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920; Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442; Zhang ZS, 2018, PROC CVPR IEEE, P1372, DOI 10.1109/CVPR.2018.00149; Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644; Zhou Bolei, 2015, OBJECT DETECTORS EME, P2; Zhu HH, 2019, CONF TECHNOL APPL; Zhu L., 2008, IEEE C COMP VIS PATT, P1; Zhu L, 2008, LECT NOTES COMPUT SC, V5303, P759	77	16	17	1	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					736	760		10.1007/s11263-020-01401-3	http://dx.doi.org/10.1007/s11263-020-01401-3		NOV 2020	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC		Green Submitted			2022-12-18	WOS:000592014000001
J	Zhang, MM; Fletcher, PT				Zhang, Miaomiao; Fletcher, P. Thomas			Fast Diffeomorphic Image Registration via Fourier-Approximated Lie Algebras	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Fourier-approximated Lie algebras; Geodesic shooting; Diffeomorphic image registration	STATISTICS; FLOWS	This paper introduces Fourier-approximated Lie algebras for shooting (FLASH), a fast geodesic shooting algorithm for diffeomorphic image registration. We approximate the infinite-dimensional Lie algebra of smooth vector fields, i.e., the tangent space at the identity of the diffeomorphism group, with a low-dimensional, bandlimited space. We show that most of the computations for geodesic shooting can be carried out entirely in this low-dimensional space. Our algorithm results in dramatic savings in time and memory over traditional large-deformation diffeomorphic metric mapping algorithms, which require dense spatial discretizations of vector fields. To validate the effectiveness of FLASH, we run pairwise image registration on both 2D synthetic data and real 3D brain images and compare with the state-of-the-art geodesic shooting methods. Experimental results show that our algorithm dramatically reduces the computational cost and memory footprint of diffemorphic image registration with little or no loss of accuracy.	[Zhang, Miaomiao] Lehigh Univ, 113 Res Dr, Bethlehem, PA 18015 USA; [Fletcher, P. Thomas] Univ Utah, 72 S Cent Campus Dr, Salt Lake City, UT USA	Lehigh University; Utah System of Higher Education; University of Utah	Zhang, MM (corresponding author), Lehigh Univ, 113 Res Dr, Bethlehem, PA 18015 USA.	miaomiao@cse.lehigh.edu; fletcher@sci.utah.edu			NIH [5R01EB007688]; NSF CAREER [1054057]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported by NIH Grant 5R01EB007688 and NSF CAREER Grant 1054057.	ARNOLD V, 1966, ANN I FOURIER, V16, P319, DOI 10.5802/aif.233; Arsigny V, 2006, LECT NOTES COMPUT SC, V4190, P924; Ashburner J, 2007, NEUROIMAGE, V38, P95, DOI 10.1016/j.neuroimage.2007.07.007; Ashburner J, 2011, NEUROIMAGE, V55, P954, DOI 10.1016/j.neuroimage.2010.12.049; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Bullo F, 1995, INVARIANT AFFINE CON; Christensen GE, 1996, IEEE T IMAGE PROCESS, V5, P1435, DOI 10.1109/83.536892; Dupuis P, 1998, Q APPL MATH, V56, P587, DOI 10.1090/qam/1632326; Durrleman S, 2011, LECT NOTES COMPUT SC, V6801, P123, DOI 10.1007/978-3-642-22092-0_11; Hinkle J, 2014, J MATH IMAGING VIS, V50, P32, DOI 10.1007/s10851-013-0489-5; Hromatka M, 2015, LECT NOTES COMPUT SC, V9350, P372, DOI 10.1007/978-3-319-24571-3_45; Joshi S, 2004, NEUROIMAGE, V23, pS151, DOI 10.1016/j.neuroimage.2004.07.068; Marcus DS, 2007, J COGNITIVE NEUROSCI, V19, P1498, DOI 10.1162/jocn.2007.19.9.1498; Miller MI, 2006, J MATH IMAGING VIS, V24, P209, DOI 10.1007/s10851-005-3624-0; Niethammer M, 2011, LECT NOTES COMPUT SC, V6892, P655, DOI 10.1007/978-3-642-23629-7_80; Singh N, 2013, INT S BIOM IM ISBI; Trouve A, 1998, INT J COMPUT VISION, V28, P213, DOI 10.1023/A:1008001603737; Vaillant M, 2004, NEUROIMAGE, V23, pS161, DOI 10.1016/j.neuroimage.2004.07.023; Vercauteren T, 2009, NEUROIMAGE, V45, pS61, DOI 10.1016/j.neuroimage.2008.10.040; Vialard FX, 2012, INT J COMPUT VISION, V97, P229, DOI 10.1007/s11263-011-0481-8; Younes L, 2009, NEUROIMAGE, V45, pS40, DOI 10.1016/j.neuroimage.2008.10.050; Zhang M., P INT C INF C INF P; Zhang MM, 2017, LECT NOTES COMPUT SC, V10265, P559, DOI 10.1007/978-3-319-59050-9_44; Zhang Miaomiao, 2016, Med Image Comput Comput Assist Interv, V9902, P166, DOI 10.1007/978-3-319-46726-9_20; Zhang MM, 2014, LECT NOTES COMPUT SC, V8675, P121, DOI 10.1007/978-3-319-10443-0_16; Zhang Miaomiao, 2013, Inf Process Med Imaging, V23, P37, DOI 10.1007/978-3-642-38868-2_4	26	16	18	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2019	127	1					61	73		10.1007/s11263-018-1099-x	http://dx.doi.org/10.1007/s11263-018-1099-x			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HK6AD					2022-12-18	WOS:000458050000004
J	Bregier, R; Devernay, F; Leyrit, L; Crowley, JL				Bregier, Romain; Devernay, Frederic; Leyrit, Laetitia; Crowley, James L.			Defining the Pose of Any 3D Rigid Object and an Associated Distance	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose; 3D rigid object; Symmetry; Distance; Metric; Average; Rotation; SE(3); SO(3); Object recognition	METRICS; MOTION; DISPLACEMENTS	The pose of a rigid object is usually regarded as a rigid transformation, described by a translation and a rotation. However, equating the pose space with the space of rigid transformations is in general abusive, as it does not account for objects with proper symmetries-which are common among man-made objects. In this article, we define pose as a distinguishable static state of an object, and equate a pose to a set of rigid transformations. Based solely on geometric considerations, we propose a frame-invariant metric on the space of possible poses, valid for any physical rigid object, and requiring no arbitrary tuning. This distance can be evaluated efficiently using a representation of poses within a Euclidean space of at most 12 dimensions depending on the object's symmetries. This makes it possible to efficiently perform neighborhood queries such as radius searches or k-nearest neighbor searches within a large set of poses using off-the-shelf methods. Pose averaging considering this metric can similarly be performed easily, using a projection function from the Euclidean space onto the pose space. The practical value of those theoretical developments is illustrated with an application of pose estimation of instances of a 3D rigid object given an input depth map, via a Mean Shift procedure.	[Bregier, Romain; Leyrit, Laetitia] Sileane, St Etienne, France; [Bregier, Romain; Devernay, Frederic; Crowley, James L.] Univ Grenoble Alpes, Inria Grenoble Rhone Alpes, Grenoble, France	Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA)	Bregier, R (corresponding author), Sileane, St Etienne, France.; Bregier, R (corresponding author), Univ Grenoble Alpes, Inria Grenoble Rhone Alpes, Grenoble, France.	r.bregier@sileane.com; frederic.devernay@inria.fr; l.leyrit@sileane.com; james.crowley@inria.fr	Devernay, Frédéric/B-4629-2009	Devernay, Frédéric/0000-0002-7061-4898	Association Nationale de la Recherche et de la Technologie [CIFRE 2014/0173]	Association Nationale de la Recherche et de la Technologie(French National Research Agency (ANR))	Funding is provided by Association Nationale de la Recherche et de la Technologie (CIFRE 2014/0173).	Angeles J, 2006, MECH MACH THEORY, V41, P884, DOI 10.1016/j.mechmachtheory.2006.03.010; Belta C, 2002, IEEE T ROBOTIC AUTOM, V18, P334, DOI 10.1109/TRA.2002.1019463; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Blender Online Community, 2016, BLEND 3D MOD REND PA; Bregier R, 2017, IEEE INT CONF COMP V, P2209, DOI 10.1109/ICCVW.2017.258; Chirikjian GS, 2015, J COMPUT INF SCI ENG, V15, DOI 10.1115/1.4028941; Chirikjian GS, 1998, J MECH DESIGN, V120, P252, DOI 10.1115/1.2826966; CURTIS WD, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P377, DOI 10.1109/VRAIS.1993.380755; Di Gregorio R, 2008, ADVANCES IN ROBOT KINEMATICS: ANALYSIS AND DESIGN, P361; Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108; Eberharter JK, 2004, J MECH DESIGN, V126, P805, DOI 10.1115/1.1767816; Etzel KR, 1996, IEEE INT CONF ROBOT, P3185; Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Gramkow C, 2001, J MATH IMAGING VIS, V15, P7, DOI 10.1023/A:1011217513455; Gupta KC, 1997, J MECH DESIGN, V119, P346, DOI 10.1115/1.2826354; Hinterstoisser S., 2012, LNCS; Kazerounian K., 1992, 47 ASME DE, V47, P271; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Lin Q, 2000, INT J ROBOT RES, V19, P612, DOI 10.1177/027836490001900605; MARTINEZ JMR, 1995, J MECH DESIGN, V117, P41, DOI 10.1115/1.2826115; Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331; PARK FC, 1995, J MECH DESIGN, V117, P48, DOI 10.1115/1.2826116; Pelletier B, 2005, STAT PROBABIL LETT, V73, P297, DOI 10.1016/j.spl.2005.04.004; Pennec X., 1998, COMPUTING MEAN GEOME; Purwar A, 2009, ASME 2009 INT DES EN, P1295; Rodrigues JJ, 2012, IEEE INT C INT ROBOT, P3334, DOI 10.1109/IROS.2012.6385680; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Sharf I, 2010, MECH MACH THEORY, V45, P1239, DOI 10.1016/j.mechmachtheory.2010.05.002; Subbarao R., 2006, CVPR, V1, P1168; Sucan IA, 2012, IEEE ROBOT AUTOM MAG, V19, P72, DOI 10.1109/MRA.2012.2205651; Tejani A, 2014, LECT NOTES COMPUT SC, V8694, P462, DOI 10.1007/978-3-319-10599-4_30; Tjaden H, 2016, LECT NOTES COMPUT SC, V9908, P423, DOI 10.1007/978-3-319-46493-0_26; Tuzel O, 2005, IEEE I CONF COMP VIS, P18; UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573; VAINSTHEIN BK, 1994, FUNDAMENTALS CRYSTAL; Zefran M, 1996, IEEE INT CONF ROBOT, P121, DOI 10.1109/ROBOT.1996.503583	37	16	16	3	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2018	126	6					571	596		10.1007/s11263-017-1052-4	http://dx.doi.org/10.1007/s11263-017-1052-4			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GC3EO		Green Submitted			2022-12-18	WOS:000429667300001
J	Kong, Y; Fu, Y				Kong, Yu; Fu, Yun			Max-Margin Heterogeneous Information Machine for RGB-D Action Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; RGB-D videos; Heterogeneous data; Feature learning		We propose a novel approach, max-margin heterogeneous information machine (MMHIM), for human action recognition from RGB-D videos. MMHIM fuses heterogeneous RGB visual features and depth features, and learns effective action classifiers using the fused features. Rich heterogeneous visual and depth data are effectively compressed and projected to a learned shared space and independent private spaces, in order to reduce noise and capture useful information for recognition. Knowledge from various sources can then be shared with others in the learned space to learn cross-modal features. This guides the discovery of valuable information for recognition. To capture complex spatiotemporal structural relationships in visual and depth features, we represent both RGB and depth data in a matrix form. We formulate the recognition task as a low-rank bilinear model composed of row and column parameter matrices. The rank of the model parameter is minimized to build a low-rank classifier, which is beneficial for improving the generalization power. We also extend MMHIM to a structured prediction model that is capable of making structured outputs. Extensive experiments on a new RGB-D action dataset and two other public RGB-D action datasets show that our approaches achieve state-of-the-art results. Promising results are also shown if RGB or depth data are missing in training or testing procedure.	[Kong, Yu; Fu, Yun] Northeastern Univ, Dept ECE, Boston, MA 02115 USA; [Fu, Yun] Northeastern Univ, Coll CIS, Boston, MA 02115 USA	Northeastern University; Northeastern University	Kong, Y (corresponding author), Northeastern Univ, Dept ECE, Boston, MA 02115 USA.	yukong@ece.neu.edu; yunfu@ece.neu.edu	Kong, Yu/N-3387-2017	Kong, Yu/0000-0001-6271-4082	NSF IIS [1651902]; NSF CNS [1314484]; ONR [N00014-12-1-1028, N00014-14-1-0484]; U.S. Army Research Office Young Investigator Award [W911NF-14-1-0218]	NSF IIS(National Science Foundation (NSF)); NSF CNS(National Science Foundation (NSF)); ONR(Office of Naval Research); U.S. Army Research Office Young Investigator Award	This work is supported in part by the NSF IIS Award 1651902, NSF CNS Award 1314484, ONR Award N00014-12-1-1028, ONR Young Investigator Award N00014-14-1-0484, and U.S. Army Research Office Young Investigator Award W911NF-14-1-0218.	Andrew Galen, 2013, ICML; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Argyriou A., 2008, IJCV; Bo L., 2011, CVPR; Chen L.-C., 2014, ARXIV PREPRINT ARXIV; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Do T.M.T., 2009, ICML; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; El R. O., 2015, CVPR; Fernando B, 2016, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR.2016.212; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Hadfield S., 2013, CVPR; HU JF, 2015, PROC CVPR IEEE, P5344; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jia C., 2014, ACM MULTIMEDIA; Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Klaser Alexander, 2008, BMVC; Kobayashi T., 2014, IJCV; Kong Y., 2015, CVPR; Kong Y., 2014, ECCV; Kong Y., 2014, PAMI; Koppula H., 2013, INT C MACHINE LEARNI, P792, DOI DOI 10.1177/0278364913478446; Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Lin Y.Y., 2014, CVPR; Liu J., 2008, EURASIP J ADV SIG PR, V2008, P1, DOI DOI 10.1074/JBC.M802695200; Liu L., 2013, P 23 INT JOINT C ART; Lu C., 2014, CVPR; Luo J., 2013, ICCV; Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214; Marszalek M., 2009, P IEEE C COMP VIS PA; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Ni B., 2015, CVPR; Ni B., 2011, ICCV WORKSH CD3CV; Ofli F., 2013, P IEEE WORKSH APPL C; Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98; Pirsiavash H., 2009, P ADV NEUR INF PROC, P1482; Raptis M., 2010, ECCV; Raptis M., 2013, CVPR; Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shotton J., 2013, PAMI; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591; Tang K., 2012, CVPR; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Teo C. H., 2007, KDD; Tishby N., 1999, P 37 ANN ALL C COMM, P368; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang J, 2012, LECT NOTES COMPUT SC, V7573, P872, DOI 10.1007/978-3-642-33709-3_62; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897; Wang Weiran, 2015, ICML; Wolf L., 2007, CVPR; Wu Changchang, 2015, P IEEE C COMP VIS PA; Xia L, 2013, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2013.365; Xie P., 2013, IJCAI; Xu C., 2013, ICCV; Xu C, 2014, IEEE T PATTERN ANAL, V36, P1559, DOI 10.1109/TPAMI.2013.2296528; Yang X., 2012, P 20 ACM INT C MULTI, P1057, DOI [10.1145/2393347.2396382, DOI 10.1145/2393347.2396382]; Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108; Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342; Zhang J, 2013, IEEE I CONF COMP VIS, P1273, DOI 10.1109/ICCV.2013.161; Zhou Y., 2015, CVPR	69	16	18	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2017	123	3					350	371		10.1007/s11263-016-0982-6	http://dx.doi.org/10.1007/s11263-016-0982-6			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EX9EO					2022-12-18	WOS:000403559600003
J	Wu, S; Yang, H; Zheng, SB; Su, H; Fan, YW; Yang, MH				Wu, Shuang; Yang, Hua; Zheng, Shibao; Su, Hang; Fan, Yawen; Yang, Ming-Hsuan			Crowd Behavior Analysis via Curl and Divergence of Motion Trajectories	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Crowd behavior analysis; Curl; Divergence; Motion trajectories; Motion coding; Path integration		In the field of crowd behavior analysis, existing methods mainly focus on using local representations inspired by models found in other disciplines (e.g., fluid dynamics and social dynamics) to describe motion patterns. However, less attention is paid to exploiting motion structures (e.g., visual information contained in trajectories) for behavior analysis. In this paper, we consider both local characteristics and global structures of a motion vector field, and propose the Curl and Divergence of motion Trajectories (CDT) descriptors to describe collective motion patterns. To this end, a trajectory-based motion coding algorithm is designed to extract the CDT descriptors. For each motion vector field we construct its conjugate field, in which each vector is perpendicular to the counterpart in the original vector field. The trajectories in the motion and corresponding conjugate fields indicate the tangential and radial motion structures, respectively. By integrating curl (and divergence, respectively) along the tangential paths (and the radial paths, respectively), the CDT descriptors are extracted. We show that the proposed motion descriptors are scale- and rotation-invariant for effective crowd behavior analysis. For concreteness, we apply the CDT descriptors to identify five typical crowd behaviors (lane, clockwise arch, counterclockwise arch, bottleneck and fountainhead) with a pipeline including motion decomposition. Extensive experimental results on two benchmark datasets demonstrate the effectiveness of the CDT descriptors for describing and classifying crowd behaviors.	[Wu, Shuang; Yang, Hua; Zheng, Shibao] Shanghai Jiao Tong Univ, Inst Image Proc & Network Engn, Dept Elect Engn, Shanghai, Peoples R China; [Su, Hang] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China; [Fan, Yawen] Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing, Jiangsu, Peoples R China; [Yang, Ming-Hsuan] Univ Calif Merced, Sch Engn, Merced, CA 95343 USA	Shanghai Jiao Tong University; Tsinghua University; Nanjing University of Posts & Telecommunications; University of California System; University of California Merced	Yang, MH (corresponding author), Univ Calif Merced, Sch Engn, Merced, CA 95343 USA.	shuangwu@sjtu.edu.cn; hyang@sjtu.edu.cn; sbzh@sjtu.edu.cn; suhangss@mail.tsinghua.edu.cn; ywfan@njupt.edu.cn; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019; Yang, Ming-Hsuan/AAE-7350-2019	Yang, Ming-Hsuan/0000-0003-4848-2304; 	NSFC [61671289, 61171172, 61102099, 61571261, 61521062]; STCSM [15DZ1207403]; NSF CAREER [1149783]	NSFC(National Natural Science Foundation of China (NSFC)); STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	We thank Dr. Berkan Solmaz for sharing the UCF crowd dataset and discussing technical details, as well as Jing Shao and Bolei Zhou for providing the CUHK crowd dataset. This work is supported in part by NSFC 61671289, 61171172, 61102099, 61571261 and 61521062, STCSM Grant 15DZ1207403, and NSF CAREER Grant 1149783.	Ali S, 2010, IEEE T PATTERN ANAL, V32, P288, DOI 10.1109/TPAMI.2008.284; Ali S, 2007, PROC CVPR IEEE, P65; Ali S, 2013, IEEE INT NEW CIRC; Brostow G.J., 2006, P IEEE INT C COMP VI, P594, DOI DOI 10.1109/CVPR.2006.320; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cheriyadat AM, 2008, IEEE J-STSP, V2, P568, DOI 10.1109/JSTSP.2008.2001306; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Cong Y, 2013, PATTERN RECOGN, V46, P1851, DOI 10.1016/j.patcog.2012.11.021; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Hu M., 2008, IEEE INT C PATT REC; Li F, 2015, IEEE COMPUT SOC CONF; Li RN, 2010, PROC CVPR IEEE, P2038, DOI 10.1109/CVPR.2010.5539880; Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029; Lin DH, 2009, PROC CVPR IEEE, P747, DOI 10.1109/CVPRW.2009.5206660; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Marsden J.E., 2003, VECTOR CALCULUS; Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641; Saleemi I, 2010, PROC CVPR IEEE, P2069, DOI 10.1109/CVPR.2010.5539884; Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285; Jacques JCS, 2010, IEEE SIGNAL PROC MAG, V27, P66, DOI 10.1109/MSP.2010.937394; Solmaz B, 2012, IEEE T PATTERN ANAL, V34, P2064, DOI 10.1109/TPAMI.2012.123; Su H, 2013, IEEE T INF FOREN SEC, V8, P1575, DOI 10.1109/TIFS.2013.2277773; Wang H, 2011, PROC CVPR IEEE; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang W., 2014, EUR C COMP VIS; Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882; Wu S, 2012, IEEE T SYST MAN CY B, V42, P1443, DOI 10.1109/TSMCB.2012.2192267; Yi S., 2015, IEEE C COMP VIS PATT; Yi S., 2014, IEEE C COMP VIS PATT; Zhan BB, 2008, MACH VISION APPL, V19, P345, DOI 10.1007/s00138-008-0132-4; Zhao X., 2011, IEEE INT C COMP VIS; Zhou B., 2012, EUR C COMP VIS; Zhou BL, 2013, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2013.392; Zhou BL, 2011, PROC CVPR IEEE	35	16	16	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2017	123	3					499	519		10.1007/s11263-017-1005-y	http://dx.doi.org/10.1007/s11263-017-1005-y			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EX9EO					2022-12-18	WOS:000403559600010
J	Aksoy, EE; Orhan, A; Worgotter, F				Aksoy, Eren Erdal; Orhan, Adil; Woergoetter, Florentin			Semantic Decomposition and Recognition of Long and Complex Manipulation Action Sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic decomposition; Temporal segmentation; Action recognition; Manipulation action; Semantic event chain	CLASSIFICATION; MODELS	Understanding continuous human actions is a non-trivial but important problem in computer vision. Although there exists a large corpus of work in the recognition of action sequences, most approaches suffer from problems relating to vast variations in motions, action combinations, and scene contexts. In this paper, we introduce a novel method for semantic segmentation and recognition of long and complex manipulation action tasks, such as "preparing a breakfast" or "making a sandwich". We represent manipulations with our recently introduced "Semantic Event Chain" (SEC) concept, which captures the underlying spatiotemporal structure of an action invariant to motion, velocity, and scene context. Solely based on the spatiotemporal interactions between manipulated objects and hands in the extracted SEC, the framework automatically parses individual manipulation streams performed either sequentially or concurrently. Using event chains, our method further extracts basic primitive elements of each parsed manipulation. Without requiring any prior object knowledge, the proposed framework can also extract object-like scene entities that exhibit the same role in semantically similar manipulations. We conduct extensive experiments on various recent datasets to validate the robustness of the framework.	[Aksoy, Eren Erdal; Orhan, Adil] Karlsruhe Inst Technol, High Performance Humanoid Technol H2T, Inst Anthropomat & Robot, Karlsruhe, Germany; [Woergoetter, Florentin] Georg August Univ Gottingen, BCCN, Inst Phys 3, Friedrich Hund Pl 1, D-37077 Gottingen, Germany	Helmholtz Association; Karlsruhe Institute of Technology; University of Gottingen	Aksoy, EE (corresponding author), Karlsruhe Inst Technol, High Performance Humanoid Technol H2T, Inst Anthropomat & Robot, Karlsruhe, Germany.	eren.aksoy@kit.edu		Aksoy, Eren Erdal/0000-0002-5712-6777	European Community [ICT-2011.2.1, 600578]	European Community(European Commission)	The research leading to these results has received funding from the European Communitys Seventh Framework Programme FP7/2007-2013 (Programme and Theme: ICT-2011.2.1, Cognitive Systems and Robotics) under Grant Agreement No. 600578, ACAT. We thank Seongyong Koo for sharing with us the MOT dataset (Koo et al. 2014).	Abramov A., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P457, DOI 10.1109/WACV.2012.6163000; Abramov A., 2010, 5 INT S 3D DAT PROC, P1; Ahad Md. Atiqur Rahman, 2011, COMPUTER VISION ACTI; Aksoy E. E., 2013, P 3 JOINT INT C DEV, P1; Aksoy EE, 2015, ROBOT AUTON SYST, V71, P118, DOI 10.1016/j.robot.2014.11.003; Aksoy EE, 2011, INT J ROBOT RES, V30, P1229, DOI 10.1177/0278364911410459; Aksoy EE, 2010, IEEE INT CONF ROBOT, P398, DOI 10.1109/ROBOT.2010.5509319; Anscombe G. E. M., 1963, INTENTION; Badler N. I., 1975, THESIS; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; Bobick AF, 1998, PROC CVPR IEEE, P196, DOI 10.1109/CVPR.1998.698609; Brand M, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P94, DOI 10.1109/AFGR.1996.557249; Brand M., 1997, P AAAI97, P12; Bullock IM, 2013, IEEE T HAPTICS, V6, P129, DOI [10.1109/ToH.2012.53, 10.1109/TOH.2012.53]; Chen H.S., 2006, P 4 ACM INT WORKSHOP, P171, DOI [10.1145/1178782.1178808, DOI 10.1145/1178782.1178808]; CUTKOSKY MR, 1989, IEEE T ROBOTIC AUTOM, V5, P269, DOI 10.1109/70.34763; Danelljan M., 2006, COMPUTER VISION PATT, P1090; Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726; Ekvall S, 2005, IEEE INT CONF ROBOT, P748; ELLIOTT JM, 1984, DEV MED CHILD NEUROL, V26, P283, DOI 10.1111/j.1469-8749.1984.tb04445.x; Fathi A, 2011, IEEE I CONF COMP VIS, P407, DOI 10.1109/ICCV.2011.6126269; Feix T., 2009, ROBOTICS SCI SYSTEMS, P407; Fern A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P159; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Guoliang Luo, 2011, 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011), P2028, DOI 10.1109/IROS.2011.6048175; Gupta A, 2007, PROC CVPR IEEE, P2564; Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Thuc HLU, 2012, PROC INT CONF ADV, P110, DOI 10.1109/ATC.2012.6404241; Ke Y, 2007, IEEE I CONF COMP VIS, P1424; Kjellstrom H, 2011, COMPUT VIS IMAGE UND, V115, P81, DOI 10.1016/j.cviu.2010.08.002; Koo S, 2014, J VIS COMMUN IMAGE R, V25, P108, DOI 10.1016/j.jvcir.2013.03.020; Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446; Kruger N, 2011, ROBOT AUTON SYST, V59, P740, DOI 10.1016/j.robot.2011.05.009; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Lee K, 2013, ROBOT AUTON SYST, V61, P1323, DOI 10.1016/j.robot.2013.08.003; Li Y, 2015, PROC CVPR IEEE, P287, DOI 10.1109/CVPR.2015.7298625; Liu J, 2016, SPRINGER TRAC ADV RO, V111, P263, DOI 10.1007/978-3-319-25739-6_12; Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359; Martinez D, 2014, IEEE INT CONF ROBOT, P5671, DOI 10.1109/ICRA.2014.6907693; Mele AR., 1992, SPRINGS ACTION UNDER; Minh Hoai, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3265, DOI 10.1109/CVPR.2011.5995470; Minnen D, 2003, PROC CVPR IEEE, P626; Nagahama K, 2013, IEEE INT CONF ROBOT, P1303, DOI 10.1109/ICRA.2013.6630739; Papon J., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P361, DOI 10.1109/WACV.2012.6163002; Pardowitz Michael, 2008, 2008 8th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2008), P347, DOI 10.1109/ICHR.2008.4756003; Pauwels K, 2010, J VISION, V10, DOI 10.1167/10.10.18; Pei MT, 2013, COMPUT VIS IMAGE UND, V117, P1369, DOI 10.1016/j.cviu.2012.12.003; Peursum P, 2004, INT C PATT RECOG, P440, DOI 10.1109/ICPR.2004.1333797; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Ramirez-Amaro K., 2013, IEEE RAS INT C HUM R; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Rui Y, 2000, PROC CVPR IEEE, P111, DOI 10.1109/CVPR.2000.855807; Ryoo M. S., 2000, P IEEE C COMP VIS PA, P1709; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Scovanner P., 2007, ACM MM, P357; Shi QF, 2011, INT J COMPUT VISION, V93, P22, DOI 10.1007/s11263-010-0384-0; Siskind J. M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P347; SISKIND JM, 1995, ARTIF INTELL REV, V8, P371, DOI 10.1007/BF00849726; Sminchisescu C, 2006, COMPUT VIS IMAGE UND, V104, P210, DOI 10.1016/j.cviu.2006.07.014; Sridhar M, 2008, FRONT ARTIF INTEL AP, V178, P606, DOI 10.3233/978-1-58603-891-5-606; Vitaladevuni S. N. P., 2008, IEEE COMP SOC C COMP; Vuga R., 2013, INT WORKSH ROB ALP A; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang ZW, 2012, PROC CVPR IEEE, P1330, DOI 10.1109/CVPR.2012.6247818; Wei P., 2006, IEEE T PATTERN ANAL; Weinland D, 2006, P IEEE C COMP VIS PA, P1639; Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002; Wimmer R, 2011, TEI 2011: PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON TANGIBLE EMBEDDED AND EMBODIED INTERACTION, P221; Worgotter F, 2009, ROBOT AUTON SYST, V57, P420, DOI 10.1016/j.robot.2008.06.011; Worgotter F, 2015, IEEE T AUTON MENT DE, V7, P140, DOI 10.1109/TAMD.2015.2427233; Worgotter F, 2013, IEEE T AUTON MENT DE, V5, P117, DOI 10.1109/TAMD.2012.2232291; Worn H, 2010, 2 INT C ADV COGN TEC, P7; Yamato J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P379, DOI 10.1109/CVPR.1992.223161; Yang S., 2016, 15 ANN C N AM CHAPT; Yang YZ, 2013, PROC CVPR IEEE, P2563, DOI 10.1109/CVPR.2013.331; Zhong H, 2004, PROC CVPR IEEE, P819; Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137	80	16	16	1	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2017	122	1					84	115		10.1007/s11263-016-0956-8	http://dx.doi.org/10.1007/s11263-016-0956-8			32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EL2AF		Green Submitted			2022-12-18	WOS:000394421800005
J	Choe, G; Park, J; Tai, YW; Kweon, IS				Choe, Gyeongmin; Park, Jaesik; Tai, Yu-Wing; Kweon, In So			Refining Geometry from Depth Sensors using IR Shading Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						RGB-D sensor; Kinect; Infrared; IR; Geometry refinement; Shading image; Shape from shading		We propose a method to refine geometry of 3D meshes from a consumer level depth camera, e.g. Kinect, by exploiting shading cues captured from an infrared (IR) camera. A major benefit to using an IR camera instead of an RGB camera is that the IR images captured are narrow band images that filter out most undesired ambient light, which makes our system robust against natural indoor illumination. Moreover, for many natural objects with colorful textures in the visible spectrum, the subjects appear to have a uniform albedo in the IR spectrum. Based on our analyses on the IR projector light of the Kinect, we define a near light source IR shading model that describes the captured intensity as a function of surface normals, albedo, lighting direction, and distance between light source and surface points. To resolve the ambiguity in our model between the normals and distances, we utilize an initial 3D mesh from the Kinect fusion and multi-view information to reliably estimate surface details that were not captured and reconstructed by the Kinect fusion. Our approach directly operates on the mesh model for geometry refinement. We ran experiments on our algorithm for geometries captured by both the Kinect I and Kinect II, as the depth acquisition in Kinect I is based on a structured-light technique and that of the Kinect II is based on a time-of-flight technology. The effectiveness of our approach is demonstrated through several challenging real-world examples. We have also performed a user study to evaluate the quality of the mesh models before and after our refinements.	[Choe, Gyeongmin; Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea; [Park, Jaesik] Intel Labs, Santa Clara, CA USA; [Tai, Yu-Wing] SenseTime Grp Ltd, Hong Kong, Hong Kong, Peoples R China	Korea Advanced Institute of Science & Technology (KAIST); Intel Corporation	Kweon, IS (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	gmchoe@rcv.kaist.ac.kr; jaesik.park@intel.com; yuwing@sensetime.com; iskweon77@kaist.ac.kr	Kweon, In So/C-2023-2011; Tai, Yu Wing/C-2047-2011	Tai, Yu Wing/0000-0002-3148-0380; PARK, JAESIK/0000-0001-5541-409X	National Research Foundation of Korea (NRF) - Korea government (MSIP) [2010-0028680]	National Research Foundation of Korea (NRF) - Korea government (MSIP)	This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2010-0028680).	Bellia L, 2011, BUILD ENVIRON, V46, P1984, DOI 10.1016/j.buildenv.2011.04.007; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bohme M, 2010, COMPUT VIS IMAGE UND, V114, P1329, DOI 10.1016/j.cviu.2010.08.001; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chatterjee A, 2015, PROC CVPR IEEE, P933, DOI 10.1109/CVPR.2015.7298695; Choe GM, 2016, PROC CVPR IEEE, P2452, DOI 10.1109/CVPR.2016.269; Choe GM, 2014, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2014.501; Deguchi K., 2012, P IEEE C COMP VIS PA; Delaunoy Amael, 2014, P IEEE C COMP VIS PA; Dolson J., 2010, P IEEE C COMP VIS PA; Fanello SR, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601223; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Grossberg MD, 2004, IEEE T PATTERN ANAL, V26, P1272, DOI 10.1109/TPAMI.2004.88; Han Y., 2013, P IEEE C COMP VIS IC; Haque S., 2014, P IEEE C COMP VIS PA; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Higo T, 2009, IEEE I CONF COMP VIS, P1234, DOI 10.1109/ICCV.2009.5459331; Horn B. K. P., 1978, MIT AI MEMO; Horn B.K.P., 1989, SHAPE SHADING; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; Izadi S., 2011, P 24 ANN ACM S US IN; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Kerl C., 2013, P INT C INT ROB SYST; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Lensch HPA, 2003, ACM T GRAPHIC, V22, P234, DOI 10.1145/636886.636891; Leyvand T, 2011, COMPUTER, V44, P94, DOI 10.1109/MC.2011.114; Liao Miao, 2007, IEEE C COMP VIS PATT, P1; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Lu Z., 2010, P IEEE C COMP VIS PA; Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226; Or-El R, 2015, PROC CVPR IEEE, P5407, DOI 10.1109/CVPR.2015.7299179; Park J., 2011, P IEEE C COMP VIS IC; Park J., 2013, P IEEE C COMP VIS IC; Park J, 2014, IEEE T IMAGE PROCESS, V23, P5559, DOI 10.1109/TIP.2014.2361034; Salamati N., 2009, P IS T SID 17 COL IM; Seitz Steven M., 2006, IEEE C COMP VIS PATT; Shan Q., 2013, P INT C 3D VIS 3DV; Shen J., 2013, P IEEE C COMP VIS PA; Shi B., 2014, P INT C 3D VIS 3DV; Shotton J., 2011, P IEEE C COMP VIS PA; Surazhsky Vitaly, 2003, P 2003 EUR ACM SIGGR; Suwajanakorn S., 2014, P EUR C COMP VIS ECC; Vlasic D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618520; Wu C., 2011, P IEEE C COMP VIS PA; Wu C., 2014, P SIGGRAPH AS; YANG Q, 2007, P IEEE C COMP VIS PA; Yu L-F, 2013, P IEEE C COMP VIS PA; Zhang Q., 2012, P IEEE C COMP VIS PA; Zhou QY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601134; Zollhofer M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766887	50	16	17	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2017	122	1					1	16		10.1007/s11263-016-0937-y	http://dx.doi.org/10.1007/s11263-016-0937-y			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EL2AF		Green Submitted			2022-12-18	WOS:000394421800001
J	Jemec, J; Pernus, F; Likar, B; Burmen, M				Jemec, Jurij; Pernus, Franjo; Likar, Bostjan; Buermen, Miran			2D Sub-pixel Point Spread Function Measurement Using a Virtual Point-Like Source	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Virtual point-like source; Two-dimensional sub-pixel point spread function; 2D PSF; Modulation transfer function; Imaging system quality assessment; ISO 12233	MODULATION TRANSFER-FUNCTION; CALIBRATION; FOCUS; LENS	2D point spread function (PSF) is a commonly used measure to assess the quality of various imaging systems. The most convenient way of 2D PSF measurement is taking an image of a light source with its size well below the diffraction limit of the imaging system. In this paper, we present a novel method that allows formation of such a virtual point-like source by a simple setup with a convex spherical mirror and a collimated light source. Sub-pixel 2D PSF measurements are possible by displacing the setup in sub-pixel steps. Comparison of the 1D modulation transfer functions estimated by the proposed method and the International Organization for Standardization (ISO) 12233 standard shows that the proposed method presents a viable alternative to the ISO 12233 standard. Furthermore, future work on calibration patterns and algorithms for sub-pixel 2D PSF estimation from a single image could benefit from the presented method, which provides ground truth sub-pixel 2D PSF measurements for real imaging systems.	[Jemec, Jurij; Pernus, Franjo; Likar, Bostjan; Buermen, Miran] Univ Ljubljana, Lab Imaging Technol, Fac Elect Engn, Trzaska 25, Ljubljana 1000, Slovenia; [Pernus, Franjo; Likar, Bostjan] SENSUM, Comp Vis Syst, Tehnoloski Pk 21, Ljubljana 1000, Slovenia	University of Ljubljana	Jemec, J (corresponding author), Univ Ljubljana, Lab Imaging Technol, Fac Elect Engn, Trzaska 25, Ljubljana 1000, Slovenia.	jurij.jemec@fe.uni-lj.si			Slovenian Research Agency [J7-6781, J2-5473, L2-5472, L2-4072]	Slovenian Research Agency(Slovenian Research Agency - Slovenia)	This research was supported by the Slovenian Research Agency, under Grants J7-6781, J2-5473, L2-5472 and L2-4072.	Arnison MR, 2011, APPL OPTICS, V50, P2158, DOI 10.1364/AO.50.002158; BOONE JM, 1994, MED PHYS, V21, P1541, DOI 10.1118/1.597264; BOREMAN GD, 1995, APPL OPTICS, V34, P8050, DOI 10.1364/AO.34.008050; Burns P. D., 2015, SFRMAT3; Burns PA, 2000, PICS 2000: IMAGE PROCESSING, IMAGE QUALITY, IMAGE CAPTURE, SYSTEMS CONFERENCE, PROCEEDINGS, P135; Burns PD, 2002, PICS 2002: IMAGE PROCESSING, IMAGE QUALITY, IMAGE CAPTURE, SYSTEMS CONFERENCE, PROCEEDINGS, P193; Claxton CD, 2008, J OPT SOC AM A, V25, P159, DOI 10.1364/JOSAA.25.000159; Cracknell AP, 1998, INT J REMOTE SENS, V19, P2025, DOI 10.1080/014311698214848; Delbracio M, 2012, IMAGE PROCESS ON LIN, V2, P8, DOI 10.5201/ipol.2012.admm-nppsf; Delbracio M, 2012, SIAM J IMAGING SCI, V5, P1234, DOI 10.1137/110848335; Delbracio M, 2012, INT J COMPUT VISION, V96, P175, DOI 10.1007/s11263-011-0460-0; Du H, 2004, APPL OPTICS, V43, P665, DOI 10.1364/AO.43.000665; Fetterly KA, 2002, MED PHYS, V29, P913, DOI 10.1118/1.1472498; Frenkel A, 1997, APPL OPTICS, V36, P5288, DOI 10.1364/AO.36.005288; GUENTHER BD, 1990, MODERN OPTICS; Imatest LLC, 2004, SHARPN WHAT IS IT IS; Joshi N., 2008, CVPR, P1; Katrasnik J, 2013, APPL OPTICS, V52, P3526, DOI 10.1364/AO.52.003526; Kee E., 2011, 2011 IEEE INT C COMP, P1; KIM KS, 1994, P SOC PHOTO-OPT INS, V2225, P185, DOI 10.1117/12.179697; LAMBERTS RL, 1958, J OPT SOC AM, V48, P487, DOI 10.1364/JOSA.48.000487; Lehr J, 1998, IEEE T IMAGE PROCESS, V7, P258, DOI 10.1109/83.661006; Lenhard K, 2015, IEEE T GEOSCI REMOTE, V53, P6085, DOI 10.1109/TGRS.2015.2431743; Mahajan V. N., 2010, HDB OF OPTICS, VI; MARKHAM BL, 1985, IEEE T GEOSCI REMOTE, V23, P864, DOI 10.1109/TGRS.1985.289472; Milanfar P., 2010, SUPER RESOLUTION IMA; Mosleh A, 2015, PROC CVPR IEEE, P4961, DOI 10.1109/CVPR.2015.7299130; Navas-Moya F. A., 2013, J EUROPEAN OPTICAL S, V8; Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207; Quabis S, 2001, APPL PHYS B-LASERS O, V72, P109, DOI 10.1007/s003400000451; REICHENBACH SE, 1991, OPT ENG, V30, P170, DOI 10.1117/12.55783; Ryu Z. M., 1991, ORLANDO 91, P469; SAMBRIDGE M, 1995, GEOPHYS J INT, V122, P837, DOI 10.1111/j.1365-246X.1995.tb06841.x; Samei E, 1998, MED PHYS, V25, P102, DOI 10.1118/1.598165; Shih YC, 2012, LECT NOTES COMPUT SC, V7575, P42, DOI 10.1007/978-3-642-33765-9_4; Szeliski R., 2010, COMPUTER VISION ALGO, DOI DOI 10.1007/978-3-030-34372-9; Takenaga T, 2015, RADIOL PHYS TECHNOL, V8, P53, DOI 10.1007/s12194-014-0286-x; Torkildsen H, 2014, SPIE DEFENSE SECURIT; TZANNES AP, 1995, OPT ENG, V34, P1808, DOI 10.1117/12.203133; Ye M, 2012, APPL OPTICS, V51, P7630, DOI 10.1364/AO.51.007630; Zandhuis JA, 1997, IEE P-VIS IMAGE SIGN, V144, P285, DOI 10.1049/ip-vis:19971307; Zong Y., 2007, P SPIE L, V6744L, P1	42	16	16	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2017	121	3					391	402		10.1007/s11263-016-0948-8	http://dx.doi.org/10.1007/s11263-016-0948-8			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK9UY					2022-12-18	WOS:000394270600004
J	Sui, Y; Zhang, L				Sui, Yao; Zhang, Li			Robust Tracking via Locally Structured Representation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual tracking; Low-rank approximation; Sparse representation; Greedy algorithm; Appearance model	SPARSE APPEARANCE MODEL; VISUAL TRACKING; THRESHOLDING ALGORITHM; OBJECT TRACKING	Representation method is critical to visual tracking. A robust representation describes the target accurately, leading to good tracking performance. In this work, a novel representation is proposed, which is designed to be simultaneously low-rank and joint sparse for the local patches within a target region. In this representation, the subspace structure is exploited by the low-rank constraint to reflect the global information of all the patches, and the sparsity structure is captured by the joint sparsity restriction to describe the locally intimate relationship between the neighboring patches. Importantly, to make the representation computationally applicable to visual tracking, a novel fast algorithm based on greedy strategy is proposed, and the performance analysis of this algorithm is also presented. Thus, the tracking in this work is formulated as a locally low-rank and joint sparse matching problem within particle filtering framework. Alarge number of experimental results show that the tracking drift problem is effectively alleviated in various challenging situations by using the proposed representation method. Both qualitative and quantitative evaluations demonstrate that the proposed tracker performs favorably against many other state-of-the-art trackers. Benefitting from the good adaptive capability of the representation, all the parameters of the proposed tracking algorithm are fixed in all the experiments.	[Sui, Yao; Zhang, Li] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China	Tsinghua University	Sui, Y (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.	suiyao@gmail.com; chinazhangli@tsinghua.edu.cn			National Natural Science Foundation of China (NSFC) [61172125, 61132007]	National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China (NSFC) under Grant 61172125 and Grant 61132007.	Adam A., 2006, IEEE C COMP VIS PATT; Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374; Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226; Bach F, 2012, OPTIMIZATION FOR MACHINE LEARNING, P19; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Belhumeur PN, 1996, PROC CVPR IEEE, P270, DOI 10.1109/CVPR.1996.517085; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes E., 2009, ARXIV09123599V1; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Coates A., 2011, INT C MACH LEARN ICM; Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733; Grabner H, 2006, IEEE C COMP VIS PATT, P260; Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19; Hager GD, 1996, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.1996.517104; Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251; Henriques J. F., 2012, EUR C COMP VIS, P702, DOI DOI 10.1007/978-3-642-33765-9_50; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hong ZB, 2013, IEEE I CONF COMP VIS, P649, DOI 10.1109/ICCV.2013.86; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231; Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369; Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821; Li X, 2013, IEEE T PATTERN ANAL, V35, P863, DOI 10.1109/TPAMI.2012.166; Lin Z., 2010, ARXIV10095055V2 UIUC; Liu BY, 2013, IEEE T PATTERN ANAL, V35, P2968, DOI 10.1109/TPAMI.2012.215; Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730; Liu Bb., 2010, P IEEE INT WORKSH IN, P1, DOI DOI 10.1109/INFCOM.2010.5462262; Mei X, 2011, PROC CVPR IEEE, P1257; Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66; Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292; Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895; Perez P, 2002, LECT NOTES COMPUT SC, V2350, P661; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891; Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230; Sui Y, 2015, IEEE I CONF COMP VIS, P3002, DOI 10.1109/ICCV.2015.344; Sui Y, 2015, IEEE T IMAGE PROCESS, V24, P4686, DOI 10.1109/TIP.2015.2462076; Sui Y, 2015, PATTERN RECOGN, V48, P2872, DOI 10.1016/j.patcog.2015.03.007; Sui Y, 2015, IEEE SIGNAL PROC LET, V22, P1331, DOI 10.1109/LSP.2015.2402313; Wang D., 2014, IEEE COMP SOC C COMP; Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307; Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677; Wang D, 2012, IEEE SIGNAL PROC LET, V19, P711, DOI 10.1109/LSP.2012.2215320; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Xiangyuan L., 2014, IEEE COMP SOC C COMP; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Yuan XT, 2012, IEEE T IMAGE PROCESS, V21, P4349, DOI 10.1109/TIP.2012.2205006; Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62; Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34; Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908; Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882	54	16	21	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	2					110	144		10.1007/s11263-016-0881-x	http://dx.doi.org/10.1007/s11263-016-0881-x			35	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FA					2022-12-18	WOS:000380269600002
J	Shrivastava, A; Patel, VM; Pillai, JK; Chellappa, R				Shrivastava, Ashish; Patel, Vishal M.; Pillai, Jaishanker K.; Chellappa, Rama			Generalized Dictionaries for Multiple Instance Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sparse coding; Multiple instance learning; Dictionary learning; Object recognition; Pain detection	KERNEL SPARSE REPRESENTATION; CLASSIFICATION	We present a multi-class multiple instance learning (MIL) algorithm using the dictionary learning framework where the data is given in the form of bags. Each bag contains multiple samples, called instances, out of which at least one belongs to the class of the bag. We propose a noisy-OR model and a generalized mean-based optimization framework for learning the dictionaries in the feature space. The proposed method can be viewed as a generalized dictionary learning algorithm since it reduces to a novel discriminative dictionary learning framework when there is only one instance in each bag. Various experiments using popular vision-related MIL datasets as well as the UNBC-McMaster Pain Shoulder Archive database show that the proposed method performs significantly better than the existing methods.	[Shrivastava, Ashish; Patel, Vishal M.; Chellappa, Rama] Univ Maryland, Ctr Automat Res, UMIACS, College Pk, MD 20742 USA; [Pillai, Jaishanker K.] Google, Mountain View, CA 94043 USA; [Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; Google Incorporated; University System of Maryland; University of Maryland College Park	Shrivastava, A (corresponding author), Univ Maryland, Ctr Automat Res, UMIACS, College Pk, MD 20742 USA.	ashish@umiacs.umd.edu	Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAV-8690-2020		ONR [N00014-12-1-0124]	ONR(Office of Naval Research)	This work was partially supported by an ONR Grant N00014-12-1-0124.	Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; Amores J, 2013, ARTIF INTELL, V201, P81, DOI 10.1016/j.artint.2013.06.003; Andrews S., 2003, NEURAL INFORM PROCES; [Anonymous], 2007, P 24 ANN INT C MACH; [Anonymous], 2009, IEEE C COMP VIS PATT; Ashraf AB, 2009, IMAGE VISION COMPUT, V27, P1788, DOI 10.1016/j.imavis.2009.05.007; Babenko B., 2009, MULTIPLE INSTANCE LE; Cao L., 2010, IEEE C COMP VIS PATT; Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248; Chen YX, 2004, J MACH LEARN RES, V5, P913; Csurka G., 2004, ECCV, P1; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1; Galleguillos C., 2008, P 10 EUR C COMP VIS; Gao SH, 2010, LECT NOTES COMPUT SC, V6314, P1; Gehler P.V., 2007, PROCEEDINGS OF THE 1; Harandi M., 2012, P 12 EUR C COMP VIS; Nguyen HV, 2013, IEEE T IMAGE PROCESS, V22, P5123, DOI 10.1109/TIP.2013.2282078; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Huo J., 2012, INT C INT DAT ENG AU; Jiang H, 2011, PROC CVPR IEEE; Kokiopoulou E, 2008, IEEE T MULTIMEDIA, V10, P806, DOI 10.1109/TMM.2008.922806; Laptev I., 2008, IEEE C COMP VIS PATT; Leistner C., 2010, P 11 EUR C COMP VIS; Leung T., 2011, IEEE INT C COMP VIS; Lucey P., 2008, INT C AUD VIS SPEECH; Lucey P., 2011, INT C AUT FAC GEST R; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Maron O., 1998, ADV NEURAL INFORM PR; Mohan A, 2001, IEEE T PATTERN ANAL, V23, P349, DOI 10.1109/34.917571; Nguyen H.V., 2012, P 12 EUR C COMP VIS; Nguyen H. V., 2012, IEEE INT C AC SPEECH; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Patel V. M., 2013, SPARSE REPRESENTATIO; Patel V.M., 2011, AS C PATT REC; Phillips PJ, 1998, IEEE T IMAGE PROCESS, V7, P1150, DOI 10.1109/83.704308; Qiu Q, 2014, IEEE T PATTERN ANAL, V36, P2173, DOI 10.1109/TPAMI.2014.2316824; Ray S., 2005, P 22 ANN INT C MACH; Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551; Sapiro G., 2007, SPARSE REPRESENTATIO; SCHMIDT M, 2007, P 18 EUR C MACH LEAR; Schmidt M., 2009, TR200919 UBC, V19; Scholkopf B., 2001, LEARNING KERNELS SUP; Shrivastava A., 2012, 11 AS C COMP VIS; Shrivastava A., 2014, IEEE INT C IM PROC; Shrivastava A, 2014, IEEE T IMAGE PROCESS, V23, P3013, DOI 10.1109/TIP.2014.2324290; Sikka K, 2013, IEEE INT CONF AUTOMA; Song H.O., 2012, P 12 EUR C COMP VIS; Vinh N.X., 2009, P 26 ICML, P1073; Viola P. A., 2005, ADV NEURAL INFORM PR; Wang H.Y., 2008, P 25 ANN INT C MACH; Wang X., 2013, P 30 ANN INT C MACH; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Yang M., 2011, IEEE C COMP VIS PATT; Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539; Zhang Q., 2001, ADV NEURAL INFORM PR; Zhang Q., 2010, IEEE C COMP VIS PATT; Zhou Z.H., 2004, TECHNICAL REPORT	58	16	19	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2015	114	2-3			SI		288	305		10.1007/s11263-015-0831-z	http://dx.doi.org/10.1007/s11263-015-0831-z			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CP7MJ					2022-12-18	WOS:000360071900011
J	Yan, JZ; Lin, S; Kang, SB; Tang, XO				Yan, Jianzhou; Lin, Stephen; Kang, Sing Bing; Tang, Xiaoou			Change-Based Image Cropping with Exclusion and Compositional Features	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image cropping; Image composition; Photo enhancement	PHOTO	Image cropping is a common operation used to improve the visual quality of photographs. In this paper, we present an automatic cropping technique that accounts for the two primary considerations of people when they crop: removal of distracting content, and enhancement of overall composition. Our approach utilizes a large training set consisting of photos before and after cropping by expert photographers to learn how to evaluate these two factors in a crop. In contrast to the many methods that exist for general assessment of image quality, ours specifically examines differences between the original and cropped photo in solving for the crop parameters. To this end, several novel image features are proposed to model the changes in image content and composition when a crop is applied. The effectiveness of each feature is empirically analyzed in determining a final feature set for crop computation. Our experiments demonstrate improvements of our method over recent cropping algorithms on a broad range of images.	[Yan, Jianzhou; Tang, Xiaoou] Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China; [Lin, Stephen; Kang, Sing Bing] Microsoft Res, Redmond, WA USA	Chinese University of Hong Kong; Microsoft	Yan, JZ (corresponding author), Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.	yjz011@alumni.ie.cuhk.edu.hk						Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cheng B., 2010, P 18 ACM INT C MULTI, P291; Chong Cao, 2013, 2013 IEEE 4th International Conference on Software Engineering and Service Science (ICSESS), P955, DOI 10.1109/ICSESS.2013.6615464; Ciocca G, 2007, IEEE T CONSUM ELECTR, V53, P1622, DOI 10.1109/TCE.2007.4429261; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Endres I, 2014, IEEE T PATTERN ANAL, V36, P222, DOI 10.1109/TPAMI.2013.122; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Ke Y., 2006, P IEEE C COMP VIS PA; Kennedy L., 2011, ACM INT C MULT RETR, P30; Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x; Luo JB, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P2218; Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498; Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386; Ma M, 2004, CCNC 2004: 1ST IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE, PROCEEDINGS, P710, DOI 10.1109/CCNC.2004.1286964; Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467; Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847; Nielsen F, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P417, DOI 10.1109/ICME.2006.262525; Nishiyama M., 2009, ACM MULTIMEDIA; Park J, 2012, IEEE IMAGE PROC, P2741, DOI 10.1109/ICIP.2012.6467466; Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159; Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771; Stentiford F., 2007, P 5 INT C COMP VIS S; STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308; Suh B, 2003, P 16 ANN ACM S USER, P95, DOI [10.1145/964696.964707, DOI 10.1145/964696.964707]; Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899; Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071; Xiao R., 2007, P INT C COMP VIS ICC; Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949; Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226; Zhang M., 2005, IEEE INT C MULT EXP	34	16	16	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2015	114	1					74	87		10.1007/s11263-015-0801-5	http://dx.doi.org/10.1007/s11263-015-0801-5			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CO1UF					2022-12-18	WOS:000358940300005
J	Ni, BB; Moulin, P; Yan, SC				Ni, Bingbing; Moulin, Pierre; Yan, Shuicheng			Pose Adaptive Motion Feature Pooling for Human Action Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Adaptive feature pooling; Human pose; Action recognition		Ineffective spatial-temporal motion feature pooling has been a fundamental bottleneck for human action recognition/detection for decades. Previous pooling schemes such as global, spatial-temporal pyramid, or human and object centric pooling fail to capture discriminative motion patterns because informative movements only occur in specific regions of the human body, that depend on the type of action being performed. Global (holistic) motion feature pooling methods therefore often result in an action representation with limited discriminative capability. To address this fundamental limitation, we propose an adaptive motion feature pooling scheme that utilizes human poses as side information. Such poses can be detected for instance in assisted living and indoor smart surveillance scenarios. Taking both video sub-volumes for pooling and human pose types as hidden variables, we formulate the motion feature pooling problem as a latent structural learning problem where the relationship between the discriminative pooling video sub-volumes and the pose types is learned. The resulting pose adaptive motion feature pooling scheme is extensively tested on assisted living and smart surveillance datasets and on general action recognition benchmarks. Improved action recognition and detection performances are demonstrated.	[Ni, Bingbing] Adv Digital Sci Ctr, Singapore, Singapore; [Moulin, Pierre] Univ Illinois, Urbana, IL USA; [Yan, Shuicheng] Natl Univ Singapore, Singapore 117548, Singapore	University of Illinois System; University of Illinois Urbana-Champaign; National University of Singapore	Ni, BB (corresponding author), Adv Digital Sci Ctr, Singapore, Singapore.	bingbing.ni@adsc.com.sg; moulin@ifp.uiuc.edu; eleyans@nus.edu.sg	Yan, Shuicheng/HCI-1431-2022					Andrews S., 2002, SUPPORT VECTOR MACHI, P561; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen Q., 2011, INT C COMP VIS PATT; Choi J., 2008, ACM MULTIMEDIA INFOR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Duchenne O, 2009, IEEE I CONF COMP VIS, P1491, DOI 10.1109/ICCV.2009.5459279; Girshick RB, 2012, DISCRIMINATIVELY TRA; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31; Kanan C., 2010, INT C COMP VIS PATT; Klaser Alexander, 2008, BMVC; Kuehne H., 2011, P INT C COMP VIS, DOI DOI 10.1109/ICCV.2011.6126543; Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lv F., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383131; Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557; Ni BB, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Raptis M., 2012, INT C COMP VIS PATT; Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342; Russakovsky O, 2012, LECT NOTES COMPUT SC, V7573, P1, DOI 10.1007/978-3-642-33709-3_1; Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361; Satkin S, 2010, LECT NOTES COMPUT SC, V6311, P536, DOI 10.1007/978-3-642-15549-9_39; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shi QF, 2011, INT J COMPUT VISION, V93, P22, DOI 10.1007/s11263-010-0384-0; Shimada Atsushi, 2013, Advances in Depth Image Analysis and Applications. International Workshop, WDIA 2012. Selected and Invited Papers: LNCS 7854, P168, DOI 10.1007/978-3-642-40303-3_18; Tang K., 2012, CVPR; Vahdat A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1729, DOI 10.1109/ICCVW.2011.6130458; Wang G., 2009, INT C COMP VIS; WANG H, 2013, INT C COMP VIS, DOI DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214; Wolf C., 2012, RRLIRIS2012004; Yakhnenko O., 2011, TECHNICAL REPORT; Yamato J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P379, DOI 10.1109/CVPR.1992.223161; Yang Jianchao, 2009, INT C COMP VIS PATT; Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPR.2009.5206671, 10.1109/CVPRW.2009.5206671]	42	16	17	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2015	111	2					229	248		10.1007/s11263-014-0742-4	http://dx.doi.org/10.1007/s11263-014-0742-4			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AZ6RD					2022-12-18	WOS:000348345500006
J	Ma, AJ; Yuen, PC				Ma, Andy J.; Yuen, Pong C.			Reduced Analytic Dependency Modeling: Robust Fusion for Visual Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Robustness; Score-level fusion; Probabilistic constraints; Dependency modeling; Visual recognition	SCORE LEVEL FUSION; FACE-RECOGNITION; CLASSIFICATION; FEATURES; COMBINATION; CLASSIFIERS; SCALE	This paper addresses the robustness issue of information fusion for visual recognition. Analyzing limitations in existing fusion methods, we discover two key factors affecting the performance and robustness of a fusion model under different data distributions, namely (1) data dependency and (2) fusion assumption on posterior distribution. Considering these two factors, we develop a new framework to model dependency based on probabilistic properties of posteriors without any assumption on the data distribution. Making use of the range characteristics of posteriors, the fusion model is formulated as an analytic function multiplied by a constant with respect to the class label. With the analytic fusion model, we give an equivalent condition to the independent assumption and derive the dependency model from the marginal distribution property. Since the number of terms in the dependency model increases exponentially, the Reduced Analytic Dependency Model (RADM) is proposed based on the convergent property of analytic function. Finally, the optimal coefficients in the RADM are learned by incorporating label information from training data to minimize the empirical classification error under regularized least square criterion, which ensures the discriminative power. Experimental results from robust non-parametric statistical tests show that the proposed RADM method statistically significantly outperforms eight state-of-the-art score-level fusion methods on eight image/video datasets for different tasks of digit, flower, face, human action, object, and consumer video recognition.	[Ma, Andy J.; Yuen, Pong C.] Hong Kong Baptist Univ, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China; [Yuen, Pong C.] BNU HKBU United Int Coll, Zhuhai, Peoples R China	Hong Kong Baptist University; Beijing Normal University - Hong Kong Baptist University United International College	Yuen, PC (corresponding author), Hong Kong Baptist Univ, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.	jhma@comp.hkbu.edu.hk; pcyuen@comp.hkbu.edu.hk	Jinhua, Andy/Y-9408-2019	Yuen, Pong Chi/0000-0002-9343-2202	Science Faculty Research Grant of Hong Kong Baptist University; Hong Kong Research Grants Council General Research Fund [212313]; National Science Foundation of China [61172136]	Science Faculty Research Grant of Hong Kong Baptist University; Hong Kong Research Grants Council General Research Fund(Hong Kong Research Grants Council); National Science Foundation of China(National Natural Science Foundation of China (NSFC))	This project was partially supported by the Science Faculty Research Grant of Hong Kong Baptist University, Hong Kong Research Grants Council General Research Fund 212313, National Science Foundation of China Research Grant 61172136. The authors would like to thank the editor and reviewers for their helpful comments which improve the quality of this paper.	Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469; Awais M., 2011, BRIT MACH VIS C, V60, P11; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Canu S., 2005, SVM KERNEL METHODS M; Chen HF, 2005, IEEE T SYST MAN CY B, V35, P578, DOI 10.1109/TSMCB.2005.846659; Comaniciu D, 2003, FUSION 2003: PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE OF INFORMATION FUSION, VOLS 1 AND 2, P1303; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dass SC, 2005, LECT NOTES COMPUT SC, V3546, P1049; Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092; Demsar J, 2006, J MACH LEARN RES, V7, P1; DUNN OJ, 1961, J AM STAT ASSOC, V56, P52, DOI 10.2307/2282330; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Feller W., 1968, INTRO PROBABILITY TH, V1; Fernando B, 2012, PROC CVPR IEEE, P3434, DOI 10.1109/CVPR.2012.6248084; Friedman M, 1937, J AM STAT ASSOC, V32, P675, DOI 10.2307/2279372; Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; GUILLAUMIN M, 2010, PROC CVPR IEEE, P902, DOI DOI 10.1109/CVPR.2010.5540120; He HB, 2012, IEEE T NEUR NET LEAR, V23, P1100, DOI 10.1109/TNNLS.2012.2198227; He MX, 2010, PATTERN RECOGN, V43, P1789, DOI 10.1016/j.patcog.2009.11.018; He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55; Huber P., 2009, ROBUST STAT; Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012; Jiang Y.-G., 2011, ICMR, P1; Kim D, 2013, IEEE IPCCC; Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881; Krantz SG., 2002, PRIMER REAL ANAL FUN, V2, DOI [10.1007/978-0-8176-8134-0, DOI 10.1007/978-0-8176-8134-0]; Kuncheva L., 2014, COMBINING PATTERN CL; Lan X., 2014, IEEE C COMP VIS PATT; Liu D, 2013, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.2013.109; Liu JC, 2012, LECT NOTES COMPUT SC, V7576, P397, DOI [10.1007/978-3-642-33715-4_29, 10.1007/978-3-642-33167-1_23]; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luenberger D.G., 2008, LINEAR NONLINEAR PRO; Ma AJ, 2013, IEEE T CIRC SYST VID, V23, P1447, DOI 10.1109/TCSVT.2013.2248494; Ma AJ, 2013, IEEE T PATTERN ANAL, V35, P1135, DOI 10.1109/TPAMI.2012.198; Ma AJ, 2012, LECT NOTES COMPUT SC, V7574, P792, DOI 10.1007/978-3-642-33712-3_57; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mittal A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.75; Nandakumar K, 2008, IEEE T PATTERN ANAL, V30, P342, DOI 10.1109/TPAMI.2007.70796; Natarajan P, 2012, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2012.6247814; Nilsback M-E., 2006, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2006., DOI 10.1109/CVPR.2006.42]; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Oh S, 2014, MACH VISION APPL, V25, P49, DOI 10.1007/s00138-013-0525-x; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790; Prabhakar S, 2002, PATTERN RECOGN, V35, P861, DOI 10.1016/S0031-3203(01)00103-0; Terrades OR, 2009, IEEE T PATTERN ANAL, V31, P1630, DOI 10.1109/TPAMI.2008.224; Ross A.A., 2006, HDB MULTIBIOMETRICS; Rudin W, 1976, PRINCIPLES MATH ANAL, V3rd; Scheirer W, 2010, LECT NOTES COMPUT SC, V6313, P481; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Sheskin D. J., 2011, HDB PARAMETRIC NONPA, V5th; Silverman B. W, 1986, DENSITY ESTIMATION S, DOI 10.1201/9781315140919; Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154; Toh KA, 2004, IEEE T PATTERN ANAL, V26, P740, DOI 10.1109/TPAMI.2004.3; Toh KA, 2004, IEEE T CIRC SYST VID, V14, P224, DOI 10.1109/TCSVT.2003.821974; Ueda N, 2000, IEEE T PATTERN ANAL, V22, P207, DOI 10.1109/34.825759; van Breukelen M, 1998, KYBERNETIKA, V34, P381; Wang H, 2013, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2013.398; Wang J, 2012, IEEE T SIGNAL PROCES, V60, P6202, DOI 10.1109/TSP.2012.2218810; Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032; Yuan XT, 2012, IEEE T IMAGE PROCESS, V21, P4349, DOI 10.1109/TIP.2012.2205006; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4	65	16	16	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2014	109	3					233	251		10.1007/s11263-014-0723-7	http://dx.doi.org/10.1007/s11263-014-0723-7			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AN1QL					2022-12-18	WOS:000340358600004
J	Paramanand, C; Rajagopalan, AN				Paramanand, C.; Rajagopalan, A. N.			Shape from Sharp and Motion-Blurred Image Pair	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion blur; Blur kernel; Transformation spread function; Belief propagation; Depth map		Motion blur due to camera shake is a common occurrence. During image capture, the apparent motion of a scene point in the image plane varies according to both camera motion and scene structure. Our objective is to infer the camera motion and the depth map of static scenes using motion blur as a cue. To this end, we use an unblurred-blurred image pair. Initially, we develop a technique to estimate the transformation spread function (TSF) which symbolizes the camera shake. This technique uses blur kernels estimated at different points across the image. Based on the estimated TSF, we recover the complete depth map of the scene within a regularization framework.	[Paramanand, C.; Rajagopalan, A. N.] Indian Inst Technol, Dept Elect Engn, Madras 600036, Tamil Nadu, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras	Paramanand, C (corresponding author), Indian Inst Technol, Dept Elect Engn, Madras 600036, Tamil Nadu, India.	paramanand@gmail.com; raju@ee.iitm.ac.in		Ambasamudram, Rajagopalan/0000-0002-0006-6961	Department of Atomic Energy Science Research Council, India	Department of Atomic Energy Science Research Council, India	The authors would like to thank the anonymous reviewers for their useful comments and suggestions. They gratefully acknowledge the support given by Department of Atomic Energy Science Research Council, India.	ALVERTOS N, 1989, IEEE T PATTERN ANAL, V11, P897, DOI 10.1109/34.35494; Babacan SD, 2010, IEEE T IMAGE PROCESS, V19, P2874, DOI 10.1109/TIP.2010.2052263; Bhavsar AV, 2012, INT J COMPUT VISION, V97, P167, DOI 10.1007/s11263-011-0476-5; Boracchi G, 2009, PATTERN RECOGN LETT, V30, P671, DOI 10.1016/j.patrec.2009.02.002; Brodatz P., 1966, TEXTURES PHOTOGRAPHI; Caglioti V., 2008, INT J COMPUT VISION, V86, P243; Canon U.S.A., 2012, LENS IM STAB; Chaudhuri S., 1999, DEPTH DEFOCUS REAL A; Dai S., 2008, P COMP VIS PATT REC; Dannilidis K., 1993, P COMP VIS PATT REC; Favaro P., 2006, 3 D SHAPE ESTIMATION; Favaro P., 2004, P EUR C COMP VIS ECC; Favaro P., 2004, P COMP VIS PATT REC; Favaro P, 2008, IEEE T PATTERN ANAL, V30, P518, DOI 10.1109/TPAMI.2007.1175; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Fox J. S., 1988, P COMP VIS PATT REC; Gupta A., 2010, P EUR C COMP VIS ECC; Hartley R., 2004, ROBOTICA; Hirsch M., 2011, P INT C COMP VIS ICC; Hu Z, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.136; Joshi N, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731050; Klein G.S.W, 2005, P BRIT MACH VIS C BM; Levin A., 2009, P COMP VIS PATT REC; Lin H.Y., 2006, P INT C PATT REC ICP; Liu J., 2009, SLEP SPARSE LEARNING; Liu J., 2009, P INT C MACH LEARN I; Paramanand C, 2012, IEEE T IMAGE PROCESS, V21, P2798, DOI 10.1109/TIP.2011.2179664; Paramanand C., 2010, P IEEE WORKSH 3 DIM; Paramanand C., 2010, P BRIT MACH VIS C BM; Portz T., 2012, P COMP VIS PATT REC; Sellent A, 2011, IEEE T PATTERN ANAL, V33, P1577, DOI 10.1109/TPAMI.2010.218; Shan Q., 2008, ACM T GRAPHICS, V27; Sorel M., 2009, P INT C IM PROC ICIP; Sorel M, 2008, IEEE T IMAGE PROCESS, V17, P105, DOI 10.1109/TIP.2007.912928; Tai Y., 2010, P COMP VIS PATT REC; Tai YW, 2011, IEEE T PATTERN ANAL, V33, P1603, DOI 10.1109/TPAMI.2010.222; Tai YW, 2010, IEEE T PATTERN ANAL, V32, P1012, DOI 10.1109/TPAMI.2009.97; Watanabe M, 1998, INT J COMPUT VISION, V27, P203, DOI 10.1023/A:1007905828438; Whyte O., 2010, P COMP VIS PATT REC; Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7; Xu L., 2012, P INT C COMP PHOT IC; Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452; Zheng Y., 2011, P COMP VIS PATT REC	45	16	16	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2014	107	3					272	292		10.1007/s11263-013-0685-1	http://dx.doi.org/10.1007/s11263-013-0685-1			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AD6IU					2022-12-18	WOS:000333362600004
J	Chen, ZH; Wong, KYK; Matsushita, Y; Zhu, XL				Chen, Zhihu; Wong, Kwan-Yee K.; Matsushita, Yasuyuki; Zhu, Xiaolong			Depth from Refraction Using a Transparent Medium with Unknown Pose and Refractive Index	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Refraction; Pose estimation; Refractive index; Depth; Reconstruction		In this paper, we introduce a novel method for depth acquisition based on refraction of light. A scene is captured directly by a camera and by placing a transparent medium between the scene and the camera. A depth map of the scene is then recovered from the displacements of scene points in the images. Unlike other existing depth from refraction methods, our method does not require prior knowledge of the pose and refractive index of the transparent medium, but instead can recover them directly from the input images. By analyzing the displacements of corresponding scene points in the images, we derive closed form solutions for recovering the pose of the transparent medium and develop an iterative method for estimating the refractive index of the medium. Experimental results on both synthetic and real-world data are presented, which demonstrate the effectiveness of the proposed method.	[Chen, Zhihu; Wong, Kwan-Yee K.; Zhu, Xiaolong] Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China; [Matsushita, Yasuyuki] Microsoft Res Asia, Beijing, Peoples R China	University of Hong Kong; Microsoft; Microsoft Research Asia	Chen, ZH (corresponding author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.	zhchen04@gmail.com	; Wong, Kenneth Kwan Yee/C-1577-2009	Matsushita, Yasuyui/0000-0002-1935-4752; Wong, Kenneth Kwan Yee/0000-0001-8560-9007				Agarwal S, 2004, LECT NOTES COMPUT SC, V3022, P483; Bouguet J. Y., 2008, CAMERA CALIBRATION T; Chen ZH, 2011, IEEE I CONF COMP VIS, P635, DOI 10.1109/ICCV.2011.6126298; Gao C, 2004, INT C PATT RECOG, P108; Gao C., 2006, 2006 IEEE COMP SOC C, V2, P2316; Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233; Hartley R., 2004, ROBOTICA; Ishigure T, 1996, APPL OPTICS, V35, P2048, DOI 10.1364/AO.35.002048; Lee D, 2000, IEEE T ROBOTIC AUTOM, V16, P528, DOI 10.1109/70.880803; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Maas H. G., 1995, OPTICALK 3 D MEASURE; Morris NJW, 2011, IEEE T PATTERN ANAL, V33, P1518, DOI 10.1109/TPAMI.2011.24; MURASE H, 1992, IEEE T PATTERN ANAL, V14, P1045, DOI 10.1109/34.159906; Nishimoto Y., 1987, Proceedings of the International Workshop on Industrial Applications of Machine Vision and Machine Intelligence. Seiken Symposium (Cat. no. 87TH0166-9), P192; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Sellmeier W., 1871, ANN PHYS CHEM, V143, DOI [DOI 10.1002/ANDP.18712190612, 10.1002/andp.18712190612]; SHIMIZU M, 2006, P CAN C COMP ROB VIS, P14; Shimizu M., 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587525; Shimizu M, 2007, FOURTH CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P347, DOI 10.1109/CRV.2007.46; Subbarao M., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P498, DOI 10.1109/CVPR.1988.196281; Surya G., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P61, DOI 10.1109/CVPR.1993.340978; ZHANG X, 1994, EXP FLUIDS, V17, P225; Zhou CY, 2009, IEEE I CONF COMP VIS, P325, DOI 10.1109/ICCV.2009.5459268; Zhou CY, 2010, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2010.5540090	26	16	18	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					3	17		10.1007/s11263-012-0590-z	http://dx.doi.org/10.1007/s11263-012-0590-z			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO		Green Published, hybrid			2022-12-18	WOS:000315501800002
J	MacKinnon, D; Carrier, B; Beraldin, JA; Cournoyer, L				MacKinnon, David; Carrier, Benjamin; Beraldin, Jean-Angelo; Cournoyer, Luc			GD&T-Based Characterization of Short-Range Non-contact 3D Imaging Systems	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Characterization; GD&T; 3D imaging systems; Quality analysis; Spatial metrology; 3D metrology	ROUGHNESS; SURFACES; SPECKLE; FORM	We present a series of test metrics, artifacts, and procedures for characterizing and verifying the operating limits of a short-range non-contact three-dimensional imaging system. These metrics have been designed to correspond to dimensioning and tolerancing metrics that are widely used in industry (e.g. automotive, aerospace, etc.). We introduce operating limit metrics that correspond with the geometric dimensioning and tolerancing (GD&T) metrics of Form (Flatness and Circularity), Orientation (Angularity), Location (Sphere, Corner, and Hole Position Errors), and Size (Diameter, Sphere-spacing, Plane-spacing and Angle Errors). An example is presented to illustrate how these metrics, artifacts, and associated test procedures can be used in practice.	[MacKinnon, David; Carrier, Benjamin; Beraldin, Jean-Angelo; Cournoyer, Luc] Natl Res Council Canada, Ottawa, ON, Canada	National Research Council Canada	MacKinnon, D (corresponding author), Natl Res Council Canada, Ottawa, ON, Canada.	david.mackinnon@nrc-cnrc.gc.ca						[Anonymous], 1998, ISO142531; [Anonymous], 2002, 2634 VDI 2; [Anonymous], 2009, Y1452009 ASME; [Anonymous], 2008, GUM 1995 MINOR CORRE; [Anonymous], 2011, 1036072011 ISO; [Anonymous], 2005, 2617 VDI; [Anonymous], 2008, 2634 VDI 3; [Anonymous], 2011, E254411A ASTM INT; [Anonymous], 2004, Y1451M1994 ASME; [Anonymous], 2012, 3D SCANN SOFTW; [Anonymous], 1994, 572541994 ISO; Anthony GT, 1996, PRECIS ENG, V19, P28, DOI 10.1016/0141-6359(96)00005-0; BARIBEAU R, 1991, APPL OPTICS, V30, P2873, DOI 10.1364/AO.30.002873; Carrier B, 2011, PROC SPIE, V7864, DOI 10.1117/12.871942; Concheri G., 2001, P 12 ADM INT C, P37; Dietzsch M, 2008, WEAR, V264, P411, DOI 10.1016/j.wear.2006.08.042; El-Hakim S. F., 2007, SENSOR INTEGRATION V, P259; Hausler G, 2011, OPTICAL MEASUREMENT OF SURFACE TOPOGRAPHY, P23; Janecki D, 2012, PRECIS ENG, V36, P128, DOI 10.1016/j.precisioneng.2011.08.001; Joint Committee for Guides in Metrology (JCGM), 2012, 2002008 JCGM; MacKinnon D., 2010, SPIE NEWSROOM, DOI [10.1117/2.1201102.003393, DOI 10.1117/2.1201102.003393]; MacKinnon D, 2011, PROC SPIE, V7864, DOI 10.1117/12.872124; Mendenhall W., 1986, HYPOTHESIS TESTING, P382; Nieciqg H., 2003, P 17 IMEKO WORLD C M, P1810; PEDERSEN HM, 1974, OPT COMMUN, V12, P156, DOI 10.1016/0030-4018(74)90380-0; Poniatowska M, 2011, METROL MEAS SYST, V18, P199, DOI 10.2478/v10178-011-0003-z; Raja J, 2002, PRECIS ENG, V26, P222, DOI 10.1016/S0141-6359(02)00103-4; Rose N., 2009, QUALITY MAGAZINE, V48, P42; Witzgall C., 2006, P IEEE APPL IM PATT, P8, DOI [10.1109/AIPR.2006.33, DOI 10.1109/AIPR.2006.33]	29	16	16	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					56	72		10.1007/s11263-012-0570-3	http://dx.doi.org/10.1007/s11263-012-0570-3			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO					2022-12-18	WOS:000315501800005
J	Joshi, SH; Srivastava, A				Joshi, Shantanu H.; Srivastava, Anuj			Intrinsic Bayesian Active Contours for Extraction of Object Boundaries in Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape extraction; Segmentation; Bayesian shape extraction; Tangent PCA; Intrinsic shape analysis; Elastic shapes; Riemannian metric	SHAPE-ANALYSIS; MODELS; SNAKES	We present a framework for incorporating prior information about high-probability shapes in the process of contour extraction and object recognition in images. Here one studies shapes as elements of an infinite-dimensional, non-linear quotient space, and statistics of shapes are defined and computed intrinsically using differential geometry of this shape space. Prior models on shapes are constructed using probability distributions on tangent bundles of shape spaces. Similar to the past work on active contours, where curves are driven by vector fields based on image gradients and roughness penalties, we incorporate the prior shape knowledge in the form of vector fields on curves. Through experimental results, we demonstrate the use of prior shape models in the estimation of object boundaries, and their success in handling partial obscuration and missing data. Furthermore, we describe the use of this framework in shape-based object recognition or classification.	[Joshi, Shantanu H.] Univ Calif Los Angeles, Lab Neuroimaging, Los Angeles, CA 90024 USA; [Srivastava, Anuj] Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA	University of California System; University of California Los Angeles; State University System of Florida; Florida State University	Joshi, SH (corresponding author), Univ Calif Los Angeles, Lab Neuroimaging, Los Angeles, CA 90024 USA.	sjoshi@loni.ucla.edu	Srivastava, Anuj/F-7417-2011; Joshi, Sudhanshu/L-2284-2019; Srivastava, Anuj/L-4705-2019	Joshi, Sudhanshu/0000-0003-4748-5001; 	Army Research Office [W911NF-04-01-0268, W911-NF-04-1-0113]; Air Force Office of Scientific Research [FA9550-06-1-0324]; Northrop Grumman Corporation; NATIONAL CENTER FOR RESEARCH RESOURCES [U54RR021813] Funding Source: NIH RePORTER	Army Research Office; Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Northrop Grumman Corporation; NATIONAL CENTER FOR RESEARCH RESOURCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR))	We thank Prof. David Wilson of University of Florida for providing us the ultrasound images, Dr. Richard Sims of USAMRDEC for the tank data, and Albert Prieto- Marquez of Department of Biological Sciences, Florida State University for the use of the hadrosaurid bone images. Additionally, we thank Dr. Ian Jermyn of INRIA, Sophia Antipolis, France for several interesting discussions. This research was support in part by the Army Research Office awards W911NF-04-01-0268 and W911-NF-04-1-0113, the Air Force Office of Scientific Research award FA9550-06-1-0324, and an Innovation Alliance Award from the Northrop Grumman Corporation.	AMIT Y, 1991, J AM STAT ASSOC, V86, P376, DOI 10.2307/2290581; Bhattacharya R, 2003, ANN STAT, V31, P1; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Charpiat G, 2006, MODEL SIMUL SCI ENG, P363; Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985; COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675; Cremers D, 2003, PATTERN RECOGN, V36, P1929, DOI 10.1016/S0031-3203(03)00056-6; Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915; Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5; Dryden I.L., 1998, STAT SHAPE ANAL, DOI [DOI 10.5555/1046920.1088707, 10.1002/9781119072492]; GAGE M, 1986, J DIFFER GEOM, V23, P69; GRAYSON MA, 1987, J DIFFER GEOM, V26, P285; Grenander U, 2000, IEEE T INFORM THEORY, V46, P1658, DOI 10.1109/18.850712; Grenander U., 1991, HANDS PATTERN THEORE; Grenander U., 1981, ABSTRACT INFERENCE; Grenander U., 1993, GEN PATTERN THEORY; Jermyn IH, 2005, ANN STAT, V33, P583, DOI 10.1214/009053604000001273; Joshi S., 2007, IEEE C COMP VIS PATT; Joshi SH, 2005, LECT NOTES COMPUT SC, V3565, P541; Joshi SH, 2007, LECT NOTES COMPUT SC, V4679, P387; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; Klassen E, 2006, LECT NOTES COMPUT SC, V3951, P95; Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333; Leventon ME, 2000, PROC CVPR IEEE, P316, DOI 10.1109/CVPR.2000.855835; Li CM, 2005, PATTERN RECOGN, V38, P1947, DOI 10.1016/j.patcog.2004.12.015; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Miller MI, 2001, INT J COMPUT VISION, V41, P61, DOI 10.1023/A:1011161132514; Mio W, 2004, PROC CVPR IEEE, P10; Mio W, 2007, INT J COMPUT VISION, V73, P307, DOI [10.1007/s11263-006-9968-0, 10.1007/s11263-006-996S-0]; ROUSSON M, 2002, ECCV, V2, P78; Samir C, 2006, IEEE T PATTERN ANAL, V28, P1858, DOI 10.1109/TPAMI.2006.235; SHAH J, 2005, H0 TYPE RIEMANNIAN M; Small C.G., 1996, STAT THEORY SHAPE; Srivastava A, 2005, IEEE T PATTERN ANAL, V27, P590, DOI 10.1109/TPAMI.2005.86; SRIVASTAVA A, 2005, LNCS, V3851, P612; Vaillant M, 2004, NEUROIMAGE, V23, pS161, DOI 10.1016/j.neuroimage.2004.07.023; VANTREES HL, 1971, DETECTION ESTIMATION, V1; Vemuri B, 2003, GEOMETRIC LEVEL SET METHODS IN IMAGING, VISION AND GRAPHICS, P251, DOI 10.1007/0-387-21810-6_14; Wilson D, 2000, PROCEEDINGS OF THE SECOND NSF INTERNATIONAL CONFERENCE ON FOOD SAFETY, P128; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; Xu CY, 1998, SIGNAL PROCESS, V71, P131, DOI 10.1016/S0165-1684(98)00140-6; Yezzi A, 2005, IEEE I CONF COMP VIS, P913; Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685	46	16	16	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2009	81	3					331	355		10.1007/s11263-008-0179-8	http://dx.doi.org/10.1007/s11263-008-0179-8			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	394GK	21076692	Green Accepted			2022-12-18	WOS:000262433800007
J	Massot, C; Herault, J				Massot, Corentin; Herault, Jeanny			Model of frequency analysis in the visual cortex and the shape from texture problem	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Conference on Computer Vision and Pattern Recognition	JUN 20-25, 2005	San Diego, CA	IEEE Comp Soc		model of V1; frequency estimation; log-normal filters; shape from texture; biological model	SURFACE ORIENTATION; PERCEPTION; COMPUTATION; SELECTIVITY; RETINA; DEPTH; CELLS	This paper addresses the question: at the level of cortical cells present in the primary area V1, is the information sufficient to extract the local perspective from the texture? Starting from a model of complex cells in visual area V1, we propose a biologically plausible algorithm for frequency analysis applied to the shape from texture problem. First, specific log-normal filters are designed in replacement of the classical Gabor filters because of their theoretical properties and of their biological plausibility. These filters are separable in frequency and orientation and they better sample the image spectrum which makes them appropriate for any pattern analysis technique. A method to estimate the local frequency in the image, which discards the need to choose the best local scale, is designed. Based on this frequency analysis model, a local decomposition of the image into patches leads to the estimation of the local frequency variation which is used to solve the problem of recovering the shape from the texture. From the analytical relation between the local frequency and the geometrical parameters, under perspective projection, it is possible to recover the orientation and the shape of the original image. The accuracy of the method is evaluated and discussed on different kind of textures, both regular and irregular, with planar and curved surfaces and also on natural scenes and psychophysical stimuli. It compares favorably to the best existing methods, with in addition, a low computational cost. The biological plausibility of the model is finally discussed.	[Massot, Corentin; Herault, Jeanny] GIPSA Lab, F-3800 Grenoble, France	UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS)	Massot, C (corresponding author), GIPSA Lab, 46 Ave Felix Viallet, F-3800 Grenoble, France.	corentinmassot@yahoo.fr; Jeanny.Herault@inpg.fr		Massot, Corentin/0000-0002-3649-0078				ATICK JJ, 1992, NEURAL COMPUT, V4, P196, DOI 10.1162/neco.1992.4.2.196; BEAUDOT WH, 1994, THESIS TIRF LABORATO; BLACK MJ, 1995, IEEE INT S COMP VIS, P485; Carandini M, 2005, J NEUROSCI, V25, P10577, DOI 10.1523/JNEUROSCI.3726-05.2005; CARANDINI M, 1999, CEREBRAL CORTEX, V13; Clerc M, 2002, IEEE T PATTERN ANAL, V24, P536, DOI 10.1109/34.993560; DAUGMAN JG, 1980, VISION RES, V20, P847, DOI 10.1016/0042-6989(80)90065-6; DEVALOIS RL, 1982, VISION RES, V22, P545, DOI 10.1016/0042-6989(82)90113-4; DEVALOIS RL, 1991, PIGMENTS PRECEPTION; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; Fleming RW, 2004, J VISION, V4, P798, DOI 10.1167/4.9.10; Garding J, 1996, INT J COMPUT VISION, V17, P163, DOI 10.1007/BF00058750; Gautama T, 2003, NATO SCI SER I LIFE, V334, P282; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; GUERINDUGUE A, 1999, SCIA, P533; HEEGER DJ, 1993, J NEUROPHYSIOL, V70, P1885, DOI 10.1152/jn.1993.70.5.1885; Herault J, 1996, NEUROCOMPUTING, V12, P113, DOI 10.1016/0925-2312(95)00114-X; HUBEL DH, 1974, J COMP NEUROL, V158, P267, DOI 10.1002/cne.901580304; Hwang WL, 1998, IEEE T IMAGE PROCESS, V7, P773, DOI 10.1109/83.668032; JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233; Knill DC, 1998, VISION RES, V38, P1683, DOI 10.1016/S0042-6989(97)00325-8; KNUTSSON H, 1994, IEEE INT C IM PROC; KOVESI P, 2003, P AUSTR JAP ADV WORK; LANDY MS, 1995, VISION RES, V35, P389, DOI 10.1016/0042-6989(94)00176-M; Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151; Lelandais S, 2005, INT J IMAGE GRAPH, V5, P329, DOI 10.1142/S0219467805001781; Li A, 2004, J VISION, V4, P860, DOI 10.1167/4.10.3; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Liu Y, 2004, J NEUROSCI, V24, P3795, DOI 10.1523/JNEUROSCI.0150-04.2004; LOH A, 2005, UWACSSE05001; Malik J, 1997, INT J COMPUT VISION, V23, P149, DOI 10.1023/A:1007958829620; Ning QA, 1997, NEURON, V18, P359, DOI 10.1016/S0896-6273(00)81238-6; Ribeiro E, 2001, IEEE T PATTERN ANAL, V23, P1459, DOI 10.1109/34.977570; Sakai K, 1997, NETWORK-COMP NEURAL, V8, P335, DOI 10.1088/0954-898X/8/3/007; Spillmann L., 1990, VISUAL PERCEPTION NE; SUPER BJ, 1995, PATTERN RECOGN, V28, P729, DOI 10.1016/0031-3203(94)00140-H; SUPER BJ, 1995, IEEE T PATTERN ANAL, V17, P333, DOI 10.1109/34.385983; Torralba AB, 1999, IEEE T CIRCUITS-I, V46, P269, DOI 10.1109/81.747199; Tsutsui KI, 2002, SCIENCE, V298, P409, DOI 10.1126/science.1074128; Wallis G, 2001, SPATIAL VISION, V14, P237, DOI 10.1163/156856801753253573	40	16	16	1	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2008	76	2					165	182		10.1007/s11263-007-0048-x	http://dx.doi.org/10.1007/s11263-007-0048-x			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	255VH		Green Submitted			2022-12-18	WOS:000252685400007
J	Arnaud, E; Memin, E				Arnaud, Elise; Memin, Etienne			Partial linear Gaussian models for tracking in image sequences using Sequential Monte Carlo methods	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sequential Monte Carlo methods; optimal importance function; Rao-Blackwellization; validation gate; point tracking; planar structure tracking	VISUAL TRACKING; FILTERS; POINT; INFERENCE	The recent development of Sequential Monte Carlo methods (also called particle filters) has enabled the definition of efficient algorithms for tracking applications in image sequences. The efficiency of these approaches depends on the quality of the state-space exploration, which may be inefficient due to a crude choice of the function used to sample in the associated probability space. A careful study of this issue led us to consider the modeling of the tracked dynamic system with partial linear Gaussian models. Such models are characterized by a non linear dynamic equation, a linear measurement equation and additive Gaussian noises. They allow inferring an analytic expression of the optimal importance function used in the diffusion process of the particle filter, and enable building a relevant approximation of a validation gate. Despite of these potential advantages partial linear Gaussian models have not been investigated. The aim of this paper is therefore to demonstrate that such models can be of real interest facing difficult usual issues such as occlusions, ambiguities due to cluttered backgrounds and large state space. Three instances of these models are proposed. After a theoretical analysis, their significance is demonstrated by their performance for tracking points and planar objects in challenging real-world image sequences.	Univ Grenoble 1, Inria Rhone Alpes, F-38330 Montbonnot St Martin, France; Univ Rennes 1, IRISA, F-35042 Rennes, France	UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Universite Grenoble Alpes (UGA); Universite de Rennes	Arnaud, E (corresponding author), Univ Grenoble 1, Inria Rhone Alpes, 655 Ave Europe, F-38330 Montbonnot St Martin, France.	elise.arnaud@inrialpes.fr; memin@irisa.fr						Anderson B. D. O., 1979, OPTIMAL FILTERING; Arnaud E, 2005, IEEE T IMAGE PROCESS, V14, P63, DOI 10.1109/TIP.2004.838707; ARNAUD E, 2004, EUR C COMP VIS, V3, P302; Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374; Aschwanden P., 1992, ROBUST COMPUTER VISI, P268; Bar-Shalom Y., 1995, MULTITARGET MULTISEN; Bilmes J., 1997, GENTLE TUTORIAL ALGO; BLACK M, 1999, INT C COMP VIS, V2, P551; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; Blake A, 1999, ADV NEUR IN, V11, P389; BRAND M, 2001, C COMP VIS PATT REC; Breidt FJ, 2000, IEEE T AERO ELEC SYS, V36, P47, DOI 10.1109/7.826311; Casella G, 1996, BIOMETRIKA, V83, P81, DOI 10.1093/biomet/83.1.81; Chen R, 2000, J R STAT SOC B, V62, P493, DOI 10.1111/1467-9868.00246; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; CUZOL A, 2005, INT C COMP VIS; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; Doucet A., 2001, STAT ENG INFORM SCI; FAUGERAS O, 1988, MOTION STRUCTURE MOT; Gordon N, 1997, IEEE T AERO ELEC SYS, V33, P353, DOI 10.1109/7.570826; Gustafsson F, 2002, IEEE T SIGNAL PROCES, V50, P425, DOI 10.1109/78.978396; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; HINKLEY DV, 1971, BIOMETRIKA, V58, P509, DOI 10.1093/biomet/58.3.509; Isard M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P107, DOI 10.1109/ICCV.1998.710707; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Jepson AD, 2001, PROC CVPR IEEE, P415; Khan Z, 2004, PROC CVPR IEEE, P980; KONG A, 1994, J AM STAT ASSOC, V89, P278, DOI 10.2307/2291224; Kwok C, 2004, P IEEE, V92, P469, DOI 10.1109/JPROC.2003.823144; Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847; MA B, 2004, P MED IM COMP COMP A; MARRS A, 2002, SPIE C SIGN DAT PROC; Merwe Rudolph van der, 2000, 380 CUEDFINFENGTR U; Nguyen HT, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P678, DOI 10.1109/ICCV.2001.937587; NORTH B, 1998, INT C COMP VIS; ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029; Okuma Kenji, 2004, EUR C COMP VIS; Perez P, 2004, P IEEE, V92, P495, DOI 10.1109/JPROC.2003.823147; PEREZ P, 2002, EUR C COMP VIS, P661; Rittscher J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P634, DOI 10.1109/ICCV.1999.791284; RUI Y, 2001, CVPR, V2, P786; Russell S., 2000, C UNC ART INT; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; SIDENBLADH H, 2002, EUR C COMP VIS, V1, P784; SIVIC J, 2004, EUR C COMP VIS, P85; SMINCHISESCU C, 2002, EUR C COMP VIS; Sullivan J, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P323, DOI 10.1109/ICCV.2001.937536; Tissainayagam P, 2004, IMAGE VISION COMPUT, V22, P663, DOI 10.1016/j.imavis.2004.02.001; TORRESANI L, 2002, EUR C COMP VIS; Vermaak J, 2002, IEEE T SPEECH AUDI P, V10, P173, DOI 10.1109/TSA.2002.1001982; Vermaak J, 2005, IEEE T AERO ELEC SYS, V41, P309, DOI 10.1109/TAES.2005.1413764; VERMAAK J, 2002, EUR C COMP VIS, V1, P645; WAN E, 2000, IEEE S AD SYST SIGN; Wu Y, 2004, INT J COMPUT VISION, V58, P55, DOI 10.1023/B:VISI.0000016147.97880.cd	55	16	17	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2007	74	1					75	102		10.1007/s11263-006-0003-2	http://dx.doi.org/10.1007/s11263-006-0003-2			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	165PX					2022-12-18	WOS:000246318800007
J	Yalcin, H; Unel, M; Wolovich, W				Yalcin, H; Unel, M; Wolovich, W			Implicitization of parametric curves by matrix annihilation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						free-form models; implicitization; Fourier descriptors; implicit polynomials	COMPLETE-SETS; REPRESENTATION; POLYNOMIALS; INVARIANTS; SURFACES; OBJECTS; IMAGES; FORMS	Object recognition is a central problem in computer vision. When objects are defined by boundary curves, they can be represented either explicitly or implicitly. Implicit polynomial (IP) equations have long been known to offer certain advantages over more traditional parametric methods. However, the lack of general procedures for obtaining IP models of higher degree has prevented their general use in many practical applications. In most cases today, parametric equations are used to model curves and surfaces. One such parametric representation, elliptic Fourier Descriptors (EFD), has been widely used to represent 2D and 3D curves, as well as 3D surfaces. Although EFDs can represent nearly all curves, it is often convenient to have an implicit algebraic description F(x, y) = 0, for several reasons. Algebraic curves and surfaces have proven very useful in many model-based applications. Various algebraic and geometric invariants obtained from these implicit models have been studied rather extensively, since implicit polynomials are well-suited to computer vision tasks, especially for single computation pose estimation, shape tracking, 3D surface estimation from multiple images and efficient geometric indexing into large pictorial databases. In this paper, we present a new non-symbolic implicitization technique called the matrix annihilation method, for converting parametric Fourier representations to algebraic ( implicit polynomial) representations, thereby benefiting from the features of both.	Brown Univ, Div Engn, Providence, RI 02912 USA; Yale Univ, Ctr Computat Vis & Control, New Haven, CT 06520 USA	Brown University; Yale University	Yalcin, H (corresponding author), Brown Univ, Div Engn, Providence, RI 02912 USA.	hulya_yalcin@brown.edu; mu@sysc.eng.yale.edu; waw@lems.brown.edu	Unel, Mustafa/AAY-3881-2020; Rohlf, F J/A-8710-2008	Unel, Mustafa/0000-0002-2907-3233; 				BAJAJ C, 1988, CSDTR826 PURD U; Bloomenthal J., 1997, INTRO IMPLICIT SURFA; Buchberger B., 1985, MULTIDIMENSIONAL SYS, P184; Calabi E, 1998, INT J COMPUT VISION, V26, P107, DOI 10.1023/A:1007992709392; CANNY J, 1990, J SYMB COMPUT, V9, P241, DOI 10.1016/S0747-7171(08)80012-0; CHIONH E, 1990, THESIS U WATERLOO CA; CHIONH EW, 1992, VISUAL COMPUT, V8, P171; Dixon AL, 1908, P LOND MATH SOC, V6, P468; GRANLUND GH, 1972, IEEE T COMPUT, VC 21, P195, DOI 10.1109/TC.1972.5008926; HOBBY JD, 1991, ACM T GRAPHIC, V10, P255, DOI 10.1145/108541.108546; Hoffmann C.M., 1989, GEOMETRIC SOLID MODE; HOFFMANN CM, 1993, IEEE COMPUT GRAPH, V13, P79, DOI 10.1109/38.180121; Hong H, 1995, LECT NOTES COMPUT SC, V948, P285; Huang ZH, 1996, IEEE T IMAGE PROCESS, V5, P1473, DOI 10.1109/83.536895; KEREN D, 1994, IEEE T PATTERN ANAL, V16, P38, DOI 10.1109/34.273718; KUHL FP, 1982, COMPUT VISION GRAPH, V18, P236, DOI 10.1016/0146-664X(82)90034-X; Lei Z, 1996, THIRD IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV '96, PROCEEDINGS, P148, DOI 10.1109/ACV.1996.572044; Lestrel PE., 1997, FOURIER DESCRIPTORS; LIN CS, 1987, PATTERN RECOGN, V20, P535, DOI 10.1016/0031-3203(87)90080-X; MA S, 1993, IJCV, V10; Macaulay F.S., 1916, ALGEBRAIC THEORY MOD; Manocha D., 1992, Computer-Aided Geometric Design, V9, P25, DOI 10.1016/0167-8396(92)90051-P; MANOCHA D, 1992, J SYMB COMPUT, V13, P485, DOI 10.1016/S0747-7171(10)80008-2; MONGA O, 1995, COMPUT VIS IMAGE UND, V61, P171, DOI 10.1006/cviu.1995.1014; Mundy J., 1992, GEOMETRIC INVARIANCE; Rutter JW., 2000, CH MATH SER, V1st; SALMON G, 1915, TREATISE ANAL GEOMET, P264; Sederberg T, 1983, THESIS PURDUE U W LA; Sederberg T. W., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P301, DOI 10.1145/218380.218460; SEDERBERG TW, 1986, IEEE COMPUT GRAPH, V6, P52, DOI 10.1109/MCG.1986.276742; SEDERBERG TW, 1984, COMPUT VISION GRAPH, V28, P72, DOI 10.1016/0734-189X(84)90140-3; SOLINA F, 1990, IEEE T PATTERN ANAL, V12, P131, DOI 10.1109/34.44401; Subrahmonia J, 1996, IEEE T PATTERN ANAL, V18, P505, DOI 10.1109/34.494640; Tasdizen T, 2000, IEEE T IMAGE PROCESS, V9, P405, DOI 10.1109/83.826778; TAUBIN G, 1994, IEEE T PATTERN ANAL, V16, P287, DOI 10.1109/34.276128; TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273; Unel M, 2000, ADV APPL MATH, V24, P65, DOI 10.1006/aama.1999.0679; Unel M, 1999, INT J PATTERN RECOGN, V13, P1137, DOI 10.1142/S0218001499000641; UNEL M, 1998, PATTERN ANAL APPL J, V1; Unsalan C, 2001, COMPUT VIS IMAGE UND, V81, P1, DOI 10.1006/cviu.2000.0881; WALLACE TP, 1980, IEEE T PATTERN ANAL, V2, P583, DOI 10.1109/TPAMI.1980.6447707; WOLOVICH W, 2002, J MANUFACTURING SCI; Wolovich WA, 1998, IEEE T PATTERN ANAL, V20, P1080, DOI 10.1109/34.722620; Wu MF, 1998, IEEE T PATTERN ANAL, V20, P858, DOI 10.1109/34.709610; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949; [No title captured]	46	16	17	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG-SEP	2003	54	1-2					105	115		10.1023/A:1023757417916	http://dx.doi.org/10.1023/A:1023757417916			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	678EA					2022-12-18	WOS:000182851000005
J	Seitz, SM; Kutulakos, KN				Seitz, SM; Kutulakos, KN			Plenoptic image editing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image-based modeling and rendering; plenoptic function; image editing; view synthesis; space carving	SHAPE; SET	This paper presents a new class of interactive image editing operations designed to maintain consistency between multiple images of a physical 3D scene. The distinguishing feature of these operations is that edits to any one image propagate automatically to all other images as if the (unknown) 3D scene had itself been modified. The modified scene can then be viewed interactively from any other camera viewpoint and under different scene illuminations. The approach is useful first as a power-assist that enables a user to quickly modify many images by editing just a few, and second as a means for constructing and editing image-based scene representations by manipulating a set of photographs. The approach works by extending operations like image painting, scissoring, and morphing so that they alter a scene's plenoptic function in a physically-consistent way, thereby affecting scene appearance from all viewpoints simultaneously. A key element in realizing these operations is a new volumetric decomposition technique for reconstructing an scene's plenoptic function from an incomplete set of camera viewpoints.	Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA; Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada; Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA; Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA; Univ Rochester, Dept Dermatol, Rochester, NY 14627 USA	University of Washington; University of Washington Seattle; University of Toronto; University of Wisconsin System; University of Wisconsin Madison; University of Rochester; University of Rochester	Seitz, SM (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.	seitz@cs.washington.edu; kyros@cs.toronto.edu						*AD SYST INC, 1998, PHOT TM 5 0 COMP PRO; Adelson E. H., 1991, COMPUTATION MODELS V; AVIDAN S, 1998, T VISUALIZATION COMP, V4, P1034; Basri R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P383, DOI 10.1109/ICCV.2001.937651; BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003; Belhumeur PN, 1998, INT J COMPUT VISION, V28, P245, DOI 10.1023/A:1008005721484; BENTON S, 1983, PROCESSING DISPLAY 3; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097; Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191; EPSTEIN R, 1996, OBJECT REPRESENTATIO, V2, P179; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; FITZGIBBON AW, 1998, P EUR C COMP VIS, P311; Gleicher M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P183, DOI 10.1145/218380.218441; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; Hanrahan P., 1990, Computer Graphics, V24, P215, DOI 10.1145/97880.97903; KATAYAMA A, 1995, P SPIE A, V2409, P21; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Kutulakos KN, 1997, PROC CVPR IEEE, P53, DOI 10.1109/CVPR.1997.609297; KUTULAKOS KN, 1994, INT J COMPUT VISION, V12, P113, DOI 10.1007/BF01421200; KUTULAKOS KN, 2000, P EUR C COMP VIS, P67; LAVEAU S, 1994, INT C PATT RECOG, P689, DOI 10.1109/ICPR.1994.576404; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Lichtenbelt B., 1998, INTRO VOLUME RENDERI; McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398; Moezzi S, 1996, IEEE COMPUT GRAPH, V16, P58, DOI 10.1109/38.544073; MORTENSEN EN, 1995, P 22 ANN C COMP GRAP, P191, DOI DOI 10.1145/218380.218442; Narayanan PJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P3, DOI 10.1109/ICCV.1998.710694; NIELSON GM, 1993, IEEE COMPUT GRAPH, V13, P60, DOI 10.1109/38.180119; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; SATO Y, 1997, P SIGGRAPH 97, P379; Schroder P., 2019, CIRCULAR EC GLOBAL S, P43, DOI DOI 10.1145/218380.218439; Seitz S. M., 1995, Proceedings IEEE Workshop on Representation of Visual Scenes (In Conjunction with ICCV'95) (Cat. No.95TB8126), P18, DOI 10.1109/WVRS.1995.476848; Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526; SEITZ SM, 1996, P SIGGRAPH 96, P21; SHADE J, P SIGGRAPH 98, P231; Shashua A, 1992, THESIS MIT; SILLION FX, 1991, COMP GRAPH, V25, P187, DOI 10.1145/127719.122739; Stollnitz E.J., 1996, WAVELETS COMPUTER GR; SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029; Wilkes D., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P136, DOI 10.1109/CVPR.1992.223215; Wood DN, 2000, COMP GRAPH, P287, DOI 10.1145/344779.344925; WOODHAM RJ, 1991, 9118 U BRIT COL LAB; Zeleznik R. C., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P163, DOI 10.1145/237170.237238	47	16	27	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2002	48	2					115	129		10.1023/A:1016046923611	http://dx.doi.org/10.1023/A:1016046923611			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	567AZ					2022-12-18	WOS:000176461600003
J	Forsyth, DA; Haddon, J; Ioffe, S				Forsyth, DA; Haddon, J; Ioffe, S			The joy of sampling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Markov chain Monte Carlo; colour constancy; structure from motion	COLOR; MODEL; RESTORATION	A standard method for handling Bayesian models is to use Markov chain Monte Carlo methods to draw samples from the posterior. We demonstrate this method on two core problems in computer vision-structure from motion and colour constancy. These examples illustrate a samplers producing useful representations for very large problems. We demonstrate that the sampled representations are trustworthy, using consistency checks in the experimental design. The sampling solution to structure from motion is strictly better than the factorisation approach, because: it reports uncertainty on structure and position measurements in a direct way; it can identify tracking errors; and its estimates of covariance in marginal point position are reliable. Our colour constancy solution is strictly better than competing approaches, because: it reports uncertainty on surface colour and illuminant measurements in a direct way; it incorporates all available constraints on surface reflectance and on illumination in a direct way; and it integrates a spatial model of reflectance and illumination distribution with a rendering model in a natural way. One advantage of a sampled representation is that it can be resampled to take into account other information. We demonstrate the effect of knowing that, in our colour constancy example, a surface viewed in two different images is in fact the same object. We conclude with a general discussion of the strengths and weaknesses of the sampling paradigm as a tool for computer vision.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Forsyth, DA (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.	daf@cs.berkeley.edu; haddon@cs.berkeley.edu; ioffe@cs.berkeley.edu						AMIT Y, 1991, J AM STAT ASSOC, V86, P376, DOI 10.2307/2290581; Beardsley PA, 1997, INT J COMPUT VISION, V23, P235, DOI 10.1023/A:1007923216416; BESAG J, 1995, STAT SCI, V10, P3, DOI 10.1214/ss/1177010123; Binford T. O., 1994, Image Understanding Workshop. Proceedings, P149; Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393; BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7; Carlin B., 1996, BAYES EMPIRICAL BAYE; Carpenter J, 1999, IEE P-RADAR SON NAV, V146, P2, DOI 10.1049/ip-rsn:19990255; CHOU PB, 1990, INT J COMPUT VISION, V4, P185, DOI 10.1007/BF00054995; COLLINS NE, 1988, SIMULATED ANNEALING; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191; Dellaert F, 2000, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2000.854916; Doucet A., 2001, SEQUENTIAL MONTE CAR; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Evans M., 2000, APPROXIMATING INTEGR; Faugeras O, 1996, INT J COMPUT VISION, V18, P5, DOI 10.1007/BF00126137; Faugeras O, 1998, COMPUT VIS IMAGE UND, V69, P292, DOI 10.1006/cviu.1998.0665; Finlayson GD, 1996, IEEE T PATTERN ANAL, V18, P1034, DOI 10.1109/34.541413; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FORSYTH DA, 1990, INT J COMPUT VISION, V5, P5, DOI 10.1007/BF00056770; Funt B., 1998, EUR C COMP VIS SPRIN, P445; Gammerman D., 1997, MARKOV CHAIN MONTE C; Gelman A, 2013, BAYESIAN DATA ANAL, P16; Gelman A, 1992, STAT SCI, V7, P136, DOI 10.1214/ss/1177011136; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Geman  S., 1986, P INT C MATH; Geweke J., 1992, BAYESIAN STAT, V4; Gilks WR, 1996, MARKOV CHAIN MONTE C; GOLDEN BL, 1986, NAV RES LOG, V33, P261, DOI 10.1002/nav.3800330209; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; GREEN PJ, 1996, MARKOV CHAIN MONTE C, P381; GRENANDER U, 1993, TUTORIAL PATTERN THE; GRENANDER U, 1983, TUTORIAL PATTERN THE; Grenander U., 1993, GEN PATTERN THEORY; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HUANG T, 1994, AAAI, P966; IOFFE S, 1999, ICCV, P1092; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Jacobs D., 1997, IEEE C COMP VIS PATT; Jerrum M., 1996, MARKOV CHAIN MONTE C; Jolly MPD, 1996, IEEE T PATTERN ANAL, V18, P293, DOI 10.1109/34.485557; KANAZAWA K, 1995, P UNC AI; KITAGAWA G, 1987, J AM STAT ASSOC, V82, P1032, DOI 10.2307/2289375; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; LEE HC, 1986, J OPT SOC AM A, V3, P1694, DOI 10.1364/JOSAA.3.001694; Li S., 1995, MARKOV RANDOM FIELD, P1; MALONEY LT, 1986, J OPT SOC AM A, V3, P29, DOI 10.1364/JOSAA.3.000029; MARIMONT DH, 1992, J OPT SOC AM A, V9, P1905, DOI 10.1364/JOSAA.9.001905; MAYBANK SJ, 1999, IEE C APPL STAT PATT, P9; McLachlan GJ., 1996, WILEY SERIES PROBABI; MOLLER J, 1999, STOCHASTIC GEOMETRY; Morris DD, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P696, DOI 10.1109/ICCV.1998.710793; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Neal R., 1993, CRGTR931 U TOR COMP; NOBLE JA, 1993, IEEE C COMP VIS PATT, P246; Pavlovic V., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P94, DOI 10.1109/ICCV.1999.791203; Pavlovic V., 1999, IEEE COMPUTER SOC C, P609, DOI [10.1016/j.neunet.2005.06.042, DOI 10.1016/J.NEUNET.2005.06.042]; Phillips DB, 1996, MARKOV CHAIN MONTE C; POEMAN C, 1993, CS93219 CMU; RICHARDSON S, 1987, P ROY STAT SOC B, V59, P731; Ripley B.D., 1987, STOCHASTIC SIMULATIO; Ripley BD., 1996; Roberts G. O., 1996, MARKOV CHAIN MONTE C, P45; ROBERTS GO, 1992, BAYESIAN STAT, V4; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; Sanz-Serna J., 1994, NUMERICAL HAMILTONIA; SARKAR S, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P98, DOI 10.1109/CVPR.1994.323816; SARKAR S, 1992, IEEE COMPUTER VISION, P251; Sullivan J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1068, DOI 10.1109/ICCV.1999.790391; Tierney L., 1996, MARKOV CHAIN MONTE C, P61; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torr P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P485, DOI 10.1109/ICCV.1998.710762; Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552; TRAUB JF, 1999, COMPLEXITY INFORMATI; TRIGG SB, 1995, IEEE C COMP VIS PATT, P845; van Lieshout, 1999, STOCHASTIC GEOMETRY; VanGool L, 1997, EUR T TELECOMMUN, V8, P369; YUILLE AL, 1999, IEEE C COMP VIS PATT, P631; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zhu SC, 2000, PROC CVPR IEEE, P738, DOI 10.1109/CVPR.2000.855894; Zhu SC, 1998, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.1998.698590	83	16	16	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		2001	41	1-2					109	134		10.1023/A:1011165200654	http://dx.doi.org/10.1023/A:1011165200654			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	427YL					2022-12-18	WOS:000168434300007
J	SONG, DM				SONG, DM			CONICS-BASED STEREO, MOTION ESTIMATION, AND POSE DETERMINATION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							RIGID PLANAR PATCH; LINE CORRESPONDENCES; 3-D OBJECTS; PARAMETERS; ALGORITHM; VISION; REPRESENTATIONS; UNIQUENESS; SURFACES; CURVES	Stereo vision, motion and structure parameter estimation, and pose determination are three important problems in 3-D computer vision. The first step in all of these problems is to choose and to extract primitives and their features in images. In most of the previous work, people usually use edge points or straight line segments as primitives and their local properties as features. Few methods have been presented in the literature using more compact primitives and their global features. This article presents an approach using conics as primitives. For stereo vision, a closed-form solution is provided for both establishing the correspondence of conics in images and the reconstruction of conics in space. With this method, the correspondence is uniquely determined and the reconstruction is global. It is shown that the method can be extended for higher degree (degree greater-than-or-equal-to 3) planar curves. For motion and structure parameter estimation, it is shown that, in general, two sequential images of at least three conics are needed in order to determine the camera motion. A complicated nonlinear system must be solved in this case. In particular, if we are given two images of a pair of coplanar conics, a closed-form solution of camera motion is presented. In a CAD-based vision system, the object models are available, and this makes it possible to recognize 3-D objects and to determine their poses from a single image. For pose determination, it is shown that if there exist two conics on the surface of an object, the object's pose can be determined by an efficient one-dimensional search. In particular, if two conics are coplanar, a closed-form solution of the object's pose is presented. Uniqueness analysis and some experiments with real or synthesized data are presented in this article.	CHINESE ACAD SCI,INST AUTOMAT,NATL PATTERN RECOGNIT LAB,BEIJING 100080,PEOPLES R CHINA	Chinese Academy of Sciences; Institute of Automation, CAS								AYACHE N, 1991, IEEE T PATTERN ANAL, V13, P73, DOI 10.1109/34.67633; AYACHE N, 1987, INT J COMPUT VISION, V1, P107, DOI 10.1007/BF00123161; BAKER HH, 1981, 7TH P INT JOINT C AR, P631; BORGEFORS G, 1988, IEEE T PATTERN ANAL, V10, P849, DOI 10.1109/34.9107; BRINT AT, 1990, IMAGE VISION COMPUT, V8, P50, DOI 10.1016/0262-8856(90)90056-B; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CHIONH EW, 1991, ACM T GRAPHIC, V10, P378, DOI 10.1145/116913.116917; DAVIS LS, 1983, COMPUT VISION GRAPH, V23, P313, DOI 10.1016/0734-189X(83)90029-4; DERICHE R, 1987, INT J COMPUT VISION, V1, P167, DOI 10.1007/BF00123164; DHOME M, 1989, IEEE T PATTERN ANAL, V11, P1265, DOI 10.1109/34.41365; Faugeras O. D., 1988, International Journal of Pattern Recognition and Artificial Intelligence, V2, P485, DOI 10.1142/S0218001488000285; FAUGERAS OD, 1990, INT J COMPUT VISION, V4, P225, DOI 10.1007/BF00054997; FORSYTH D, 1991, IMAGE VISION COMPUT, V9, P130, DOI 10.1016/0262-8856(91)90023-I; FORSYTH D, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P598; GRIMSON WEL, 1985, IEEE T PATTERN ANAL, V7, P17, DOI 10.1109/TPAMI.1985.4767615; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; HARALICK RM, 1984, PATTERN RECOGN, V17, P637, DOI 10.1016/0031-3203(84)90017-7; HORAUD R, 1989, IEEE T PATTERN ANAL, V11, P1168, DOI 10.1109/34.42855; Horn R. A., 1986, MATRIX ANAL; Hough P.V., 1962, US Patent, Patent No. [US3069654A, 3069654, 3,069,654]; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; LIU YC, 1986, OCT P INT C PATT REC, P306; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LONGUETHIGGINS HC, 1987, STUDIES COGNITIVE SC, P395; LU Y, 1989, IEEE T PATTERN ANAL, V11, P337, DOI 10.1109/34.19032; MA SD, 1992, P IAPR C HAGUE; MA SD, 1992, 2ND PAC RIM INT C AI; Macaulay F.S., 1916, ALGEBRAIC THEORY MOD; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MEDIONI G, 1985, COMPUT VISION GRAPH, V31, P2, DOI 10.1016/S0734-189X(85)80073-6; Merritt EL, 1949, PHOTOGRAMM ENG, V15, P649; Nagel H., 1986, Eighth International Conference on Pattern Recognition. Proceedings (Cat. No.86CH2342-4), P1174; Nasrabadi N. M., 1988, Proceedings of the 1988 IEEE International Conference on Robotics and Automation (Cat. No.88CH2555-1), P1804, DOI 10.1109/ROBOT.1988.12327; POGGIO GF, 1984, ANNU REV NEUROSCI, V7, P379, DOI 10.1146/annurev.ne.07.030184.002115; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; PORRILL J, 1991, IMAGE VISION COMPUT, V9, P45, DOI 10.1016/0262-8856(91)90048-T; Robert L., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P57, DOI 10.1109/CVPR.1991.139661; SALMON G, 1866, MODERN HIGHER ALGEBR; SAWHNEY HS, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P494; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; SHI SH, 1991, THESIS CHINESE ACAD; SHUN J, 1986, P C COMPUT VIS PATT, P109; SILBERBERG TM, 1986, COMPUT VISION GRAPH, V35, P47, DOI 10.1016/0734-189X(86)90125-8; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; SRINIVASAN R, 1987, 1ST P INT C COMP VIS, P677; SVALBE ID, 1989, IEEE T PATTERN ANAL, V11, P941, DOI 10.1109/34.35497; TANG J, 1991, STRAIGHT LINE CONICS; TSAI RY, 1982, IEEE T ACOUST SPEECH, V30, P525, DOI 10.1109/TASSP.1982.1163931; TSAI RY, 1981, IEEE T ACOUST SPEECH, V29, P1147, DOI 10.1109/TASSP.1981.1163710; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; Wei G.-Q., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P2017, DOI 10.1109/ROBOT.1990.126302; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P318, DOI 10.1109/34.120327; YEN BL, 1983, IMAGE SEQUENCE PROCE; [No title captured]	54	16	16	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1993	10	1					7	25						19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LP384					2022-12-18	WOS:A1993LP38400001
J	RIEGER, JH				RIEGER, JH			GLOBAL BIFURCATION SETS AND STABLE PROJECTIONS OF NONSINGULAR ALGEBRAIC-SURFACES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							MODEL-BASED RECOGNITION; GEOMETRY; PLANE; CLASSIFICATION; SINGULARITIES; DECOMPOSITION; ALGORITHM; SYSTEMS; OBJECTS	The view graph of a surface is a planar graph whose nodes are the stable views (projections) of the surface and whose edges represent transitional views of codimension one. The space of all directions of orthogonal projection can be identified with the projective plane. The set of "bad" projection directions, associated with the degenerate views of positive codimension, forms a graph in the projective plane (the view bifurcation set). This graph is dual to the view graph and divides the projective plane into a certain number of connected regions whose representatives are the nodes of the view graph. We assume that the projected surface is nonsingular and parameterized by polynomials of degree d. We present an estimate for the number of nodes in the view graph in terms of d and describe symbolic algorithms for computing the bifurcation set and the view graph of a surface from a parametrization.	UNIV KARLSRUHE,FACHBEREICH INFORMAT,W-2000 HAMBURG 50,GERMANY	Helmholtz Association; Karlsruhe Institute of Technology								ARNOLD VI, 1983, RUSS MATH SURV+, V38, P87, DOI 10.1070/RM1983v038n02ABEH003471; ARNON DS, 1984, SIAM J COMPUT, V13, P878, DOI 10.1137/0213055; ARNON DS, 1984, SIAM J COMPUT, V13, P865, DOI 10.1137/0213054; Banchoff T., 1982, CUSPS GAUSS MAPPINGS; Buchberger B., 1985, MULTIDIMENSIONAL SYS, P184; Buchberger B., 1970, AEQUATIONES MATH, V4, P374, DOI 10.1007/BF01844169; Callahan J., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P240; CHAKRAVARTY I, 1982, P SOC PHOTO-OPT INST, V336, P37, DOI 10.1117/12.933609; CHIN RT, 1986, COMPUT SURV, V18, P67, DOI 10.1145/6462.6464; Collins G.E., 1975, LECT NOTES COMPUT SC, V33, P134, DOI [10.1007/3-540-07407-4_17, DOI 10.1007/3-540-07407-4_17]; EGGERT D, 1989, NOV P IEEE WORKSH IN, P102; Fekete G., 1984, Proceedings of the Workshop on Computer Vision: Representation and Control, P192; GAFFNEY T, 1983, P SYMP PURE MATH, V40, P409; Giblin P. J., 1977, GRAPHS SURFACES HOMO; Gigus Z., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P30, DOI 10.1109/CCV.1988.589969; Greenberg M.J., 1981, ALGEBRAIC TOPOLOGY 1; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; KERGOSIEN YL, 1981, CR ACAD SCI I-MATH, V292, P929; KERGOSIEN YL, 1980, CR ACAD SCI A MATH, V290, P705; KOENDERINK JJ, 1976, BIOL CYBERN, V24, P51, DOI 10.1007/BF00365595; KRIEGMAN DJ, 1990, INT J COMPUT VISION, V5, P119, DOI 10.1007/BF00054918; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; MATHER JN, 1973, ANN MATH, V98, P226, DOI 10.2307/1970783; Milnor J., 1964, P AM MATH SOC, V15, P275, DOI DOI 10.1090/S0002-9939-1964-0161339-9; MISHRA B, 1989, INFORM SCIENCES, V48, P219, DOI 10.1016/0020-0255(89)90037-6; PRILL D, 1986, SIAM J COMPUT, V15, P972, DOI 10.1137/0215069; RIEGER JH, 1987, IMAGE VISION COMPUT, V5, P91, DOI 10.1016/0262-8856(87)90033-3; RIEGER JH, 1987, J LOND MATH SOC, V36, P351, DOI 10.1112/jlms/s2-36.2.351; RIEGER JH, 1990, MATH PROC CAMBRIDGE, V107, P127, DOI 10.1017/S0305004100068420; RIEGER JH, 1990, ARTIF INTELL, V44, P1, DOI 10.1016/0004-3702(90)90097-J; Roy, 1987, GEOMETRIE ALGEBRIQUE; Stewman J., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P494, DOI 10.1109/CCV.1988.590029; TRINKS W, 1978, J NUMBER THEORY, V10, P475, DOI 10.1016/0022-314X(78)90019-7; Walker R.J., 1950, ALGEBRAIC CURVES; WHITNEY H, 1955, ANN MATH, V62, P374, DOI 10.2307/1970070; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]	47	16	16	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1992	7	3					171	194		10.1007/BF00126392	http://dx.doi.org/10.1007/BF00126392			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HX401					2022-12-18	WOS:A1992HX40100001
J	BARRON, JL; JEPSON, AD; TSOTSOS, JK				BARRON, JL; JEPSON, AD; TSOTSOS, JK			THE FEASIBILITY OF MOTION AND STRUCTURE FROM NOISY TIME-VARYING IMAGE VELOCITY INFORMATION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							3-DIMENSIONAL MOTION; PASSIVE NAVIGATION; SEQUENCES; FLOW; UNIQUENESS; ALGORITHMS; PARAMETERS; SURFACES; SCENE	This research addresses the problem of noise sensitivity inherent in motion and structure algorithms. The motion and structure paradigm is a two-step process. First, we measure image velocities and, perhaps, their spatial and temporal derivatives, are obtained from time-varying image intensity data and second, we use these data to compute the motion of a moving monocular observer in a stationary environment under perspective projection, relative to a single 3-D planar surface. The first contribution of this article is an algorithm that uses time-varying image velocity information to compute the observer's translation and rotation and the normalized surface gradient of the 3-D planar surface. The use of time-varying image velocity information is an important tool in obtaining a more robust motion and structure calculation. The second contribution of this article is an extensive error analysis of the motion and structure problem. Any motion and structure algorithm that uses image velocity information as its input should exhibit error sensitivity behavior compatible with the results reported here. We perform an average and worst case error analysis for four types of image velocity information: full and normal image velocities and full and normal sets of image velocity and its derivatives. (These derivatives are simply the coefficients of a truncated Taylor series expansion about some point in space and time.) The main issues we address here are: just how sensitive is a motion and structure computation in the presence of noisy input, or alternately, how accurate must our image velocity information be, how much and what type of input data is needed, and under what circumstances is motion and structure feasible? That is, when can we be sure that a motion and structure computation will produce usable results? We base our answers on a numerical error analysis we conduct for a large number of motions.	UNIV WESTERN ONTARIO,DEPT COMP SCI,LONDON N6A 5B7,ONTARIO,CANADA; UNIV TORONTO,DEPT COMP SCI,TORONTO M5S 1A4,ONTARIO,CANADA	Western University (University of Western Ontario); University of Toronto			Tsotsos, John K/G-3436-2011; Tsotsos, John/N-1131-2019	Tsotsos, John/0000-0002-8621-9147				Adiv G., 1985, PAMI, V7, P384; ADIV G, 1984, COINS8407 U MASS TEC; ALOIMONOS J, 1984, 2ND P WORKSH COMP VI, P72; ALOIMONOS J, 1986, MAY P WORKSH MOT REP; ALOIMONOS J, 1987, 1ST P INT C COMP VIS, P35; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; [Anonymous], 1986, EVASION DIVISAS HIST; BANDYOPADHYAY, 1986, TR211 U ROCH DEP COM; BANDYOPADHYAY A, 1985, TR157 U ROCH DEP COM; Barron J. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P651; BARRON JL, 1987, 6TH P AM ASS ART INT, P700; BARRON JL, 1987, SEP P ICIAP SIC, P425; BARRON JL, 1984, RBCVTR845 U TOR DEP; BARRON JL, 1989, SEP P ICIAP POS, P399; BARRON JL, 1988, THESIS U TORONTO; BARRON JL, 1987, 10TH P INT JOINT C A, P822; BARRON JL, TBCVTR8824 U TOR DEP; Broida T. J., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P176; BRUSS AR, 1983, COMPUT VISION GRAPH, V21, P3, DOI 10.1016/S0734-189X(83)80026-7; BUXTON BF, 1984, 6TH P EUR C ART INT, P631; DENNIS JR, 1983, NUMERICAL METHODS UN; DRESCHLER L, 1982, COMPUT VISION GRAPH, V20, P199, DOI 10.1016/0146-664X(82)90081-8; DUTTA R, 1989, JUN IEEE P CVPR SAN, P159; EAGLESON R, 1989, JUN P VIS INT 89 LON, P61; FANG JQ, 1984, COMPUT VISION GRAPH, V26, P183, DOI 10.1016/0734-189X(84)90182-8; FANG JQ, 1984, IEEE T PATTERN ANAL, V6, P545, DOI 10.1109/TPAMI.1984.4767569; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; FLEET DJ, 1989, RBCVTR8927 U TOR DEP; GIBSON JJ, 1957, PSYCHOL REV, V64, P288, DOI 10.1037/h0044277; HAY JC, 1966, PSYCHOL REV, V73, P550, DOI 10.1037/h0023863; Heeger D. J., 1988, INT J COMPUT VISION, V1, P279; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; KANATANI K, 1985, 9TH P INT JOINT C AR, P886; LAWTON DT, 1983, COMPUT VISION GRAPH, V22, P116, DOI 10.1016/0734-189X(83)90098-1; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; MARR D, 1981, PROC R SOC SER B-BIO, V211, P151, DOI 10.1098/rspb.1981.0001; MATTHIES L, 1989, INT J COMPUT VISION, V3, P209, DOI 10.1007/BF00133032; NEGAHDARIPOUR S, 1987, IEEE T PATTERN ANAL, V9, P168, DOI 10.1109/TPAMI.1987.4767884; PRAZDNY K, 1979, 6TH P INT JOINT C AR, P702; REGAN D, 1982, SCIENCE, V215, P194, DOI 10.1126/science.7053572; ROACH JW, 1980, IEEE T PATTERN ANAL, V2, P554, DOI 10.1109/TPAMI.1980.6447703; Shariat H., 1986, Proceedings of the Workshop on Motion: Representation and Analysis (Cat. No.86CH2322-6), P119; SHARIAT H, 1986, P DARPA IAMGE UNDERS, P694; SPETSAKIS ME, 1988, CARTR389CSTR2101 U M; SUBBARAO M, 1985, CARTR114CSTR1485 U M; SUBBARAO M, 1986, CARTR221 U MAR CTR A; SUBBARAO M, 1985, 3RD WORKSH COMP VIS, P129; SYNDER MA, 1986, ACCURACY 3D PARAMETE; TSAI RY, 1982, IEEE T ACOUST SPEECH, V30, P525, DOI 10.1109/TASSP.1982.1163931; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; VERRI A, 1987, 1ST P INT C COMP VIS, P171; WAXMAN A, 1984, CARTR58 U MAR CTR AU; WAXMAN AM, 1985, INT J ROBOT RES, V4, P72, DOI 10.1177/027836498500400306; WAXMAN AM, 1985, INT J ROBOT RES, V4, P95, DOI 10.1177/027836498500400307; WAXMAN AM, 1988, P C COMPUT VISION PA; WAXMAN AM, 1983, CARTR24 U MAR CTR AU; WAXMAN AM, 1987, 1ST P INT C COMP VIS, P12; WEBB JA, 1981, IEEE COMPUT, V14, P40; WU JJ, 1989, INT J COMPUT VISION, V2, P373, DOI 10.1007/BF00133556	63	16	18	0	5	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1990	5	3					239	269		10.1007/BF00126501	http://dx.doi.org/10.1007/BF00126501			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EZ751		Green Submitted			2022-12-18	WOS:A1990EZ75100002
J	RAO, K; NEVATIA, R				RAO, K; NEVATIA, R			COMPUTING VOLUME DESCRIPTIONS FROM SPARSE 3-D DATA	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									UNIV SO CALIF,INST ROBOT & INTELLIGENT SYST,DEPT ELECT ENGN,LOS ANGELES,CA 90089; UNIV SO CALIF,INST ROBOT & INTELLIGENT SYST,DEPT COMP SCI,LOS ANGELES,CA 90089	University of Southern California; University of Southern California								BAJCSY R, 1987, 1ST P INT C COMP VIS, P231; BINFORD TO, 1971, UNPUB IEEE C SYSTEMS; BOLLES RC, 1986, INT J ROBOT RES, V5, P3, DOI 10.1177/027836498600500301; BROOKS RA, 1981, AIM343 STANF ART INT; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; GRIMSON WEL, 1981, IMAGES SURFACES; MACKWORTH AK, 1973, ARTIF INTELL, V4, P121, DOI 10.1016/0004-3702(73)90003-9; Malik J., 1985, THESIS STANFORD U ST; MARR D, 1977, PROC R SOC SER B-BIO, V197, P441, DOI 10.1098/rspb.1977.0080; NEVATIA R, 1982, MACHINE PERCEPTION; PENTLAND A, 1987, 1ST INT C COMP VIS, P612; SHAFER SA, 1983, CS83131 CARN MELL U; SMITH DR, 1985, COMPUTER VISION GRAP, V35, P322; TERZOPOULOS D, 1984, THESIS MIT; WALTZ DL, 1972, MIT AITR271 ART INT	15	16	16	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1988	2	1					33	50		10.1007/BF00836280	http://dx.doi.org/10.1007/BF00836280			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC189					2022-12-18	WOS:A1988AC18900002
J	PONCE, J; CHELBERG, D				PONCE, J; CHELBERG, D			FINDING THE LIMBS AND CUSPS OF GENERALIZED CYLINDERS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									STANFORD UNIV,ROBOT LAB,STANFORD,CA 94305	Stanford University								BINFORD TO, 1981, ARTIF INTELL, V17, P205, DOI 10.1016/0004-3702(81)90025-4; BINFORD TO, 1971, DEC P IEEE C SYST CO; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; KOENDERINK JJ, 1976, BIOL CYBERN, V24, P51, DOI 10.1007/BF00365595; MALIK J, 1985, P IM UND WORKSH MIAM, P209; MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020; MARR D, 1977, PROC R SOC SER B-BIO, V197, P441, DOI 10.1098/rspb.1977.0080; NEVATIA R, 1982, MACHINE PERCEPTION; PONCE J, 1987, 1ST P INT C COMP VIS; PONCE J, 1987, IN PRESS FINDING LIM; PONCE J, 1987, 4TH P IEEE ROB C RAL; SCOTT R, 1984, P IMAGE UNDERSTANDIN, P98; SHAFER SA, 1983, CMUCS083105 CARN MEL	13	16	16	0	3	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		1987	1	3					195	210						16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	M2050					2022-12-18	WOS:A1987M205000001
J	Varol, G; Laptev, I; Schmid, C; Zisserman, A				Varol, Gul; Laptev, Ivan; Schmid, Cordelia; Zisserman, Andrew			Synthetic Humans for Action Recognition from Unseen Viewpoints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Synthetic humans; Action recognition	REPRESENTATIONS	Although synthetic training data has been shown to be beneficial for tasks such as human pose estimation, its use for RGB human action recognition is relatively unexplored. Our goal in this work is to answer the question whether synthetic humans can improve the performance of human action recognition, with a particular focus on generalization to unseen viewpoints. We make use of the recent advances in monocular 3D human body reconstruction from real action sequences to automatically render synthetic training videos for the action labels. We make the following contributions: (1) we investigate the extent of variations and augmentations that are beneficial to improving performance at new viewpoints. We consider changes in body shape and clothing for individuals, as well as more action relevant augmentations such as non-uniform frame sampling, and interpolating between the motion of individuals performing the same action; (2) We introduce a new data generation methodology, SURREACT, that allows training of spatio-temporal CNNs for action classification; (3) We substantially improve the state-of-the-art action recognition performance on the NTU RGB+D and UESTC standard human action multi-view benchmarks; Finally, (4) we extend the augmentation approach to in-the-wild videos from a subset of the Kinetics dataset to investigate the case when only one-shot training data is available, and demonstrate improvements in this case as well.	[Varol, Gul] Univ Gustave Eiffel, CNRS, Ecole Ponts, LIGM, Champs Sur Marne, France; [Laptev, Ivan; Schmid, Cordelia] INRIA, Paris, France; [Zisserman, Andrew] Univ Oxford, Visual Geometry Grp, Oxford, England	Centre National de la Recherche Scientifique (CNRS); Ecole des Ponts ParisTech; Universite Gustave-Eiffel; ESIEE Paris; Inria; University of Oxford	Varol, G (corresponding author), Univ Gustave Eiffel, CNRS, Ecole Ponts, LIGM, Champs Sur Marne, France.	gul.varol@enpc.fr			Google Research; EPSRC grant ExTol; Louis Vuitton ENS Chair on Artificial Intelligence; DGA project DRAAF; French government [ANR-19-P3IA-0001]	Google Research(Google Incorporated); EPSRC grant ExTol(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Louis Vuitton ENS Chair on Artificial Intelligence; DGA project DRAAF; French government	This work was supported in part by Google Research, EPSRC grant ExTol, Louis Vuitton ENS Chair on Artificial Intelligence, DGA project DRAAF, and the French government under management of Agence Nationale de la Recherche as part of the Investissements d'Avenir program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). We thank Angjoo Kanazawa, Fabien Baradel, andMax Bain for helpful discussions, PhilippeWeinzaepfel and Nieves Crasto for providing pre-trained models.	[Anonymous], CARNEGIE MELLON MOCA; [Anonymous], SURREACT PROJECT; [Anonymous], 2017, ABS170310106 CORR; [Anonymous], 2016, P ECCV; Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Farhadi A, 2008, LECT NOTES COMPUT SC, V5302, P154, DOI 10.1007/978-3-540-88682-2_13; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Ghezelghieh MF, 2016, INT CONF 3D VISION, P685, DOI 10.1109/3DV.2016.75; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hu JF, 2017, IEEE T PATTERN ANAL, V39, P2186, DOI 10.1109/TPAMI.2016.2640292; Hu P., 2017, CVPR; Ji Y., 2018, ACMMM; Junejo IN, 2011, IEEE T PATTERN ANAL, V33, P172, DOI 10.1109/TPAMI.2010.68; Kanazawa A., 2019, CVPR; Kankanhalli M, 2018, NEURIPS; Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463; Kong Y., 2018, ABS180611230 CORR; Kong Y, 2017, IEEE T IMAGE PROCESS, V26, P3028, DOI 10.1109/TIP.2017.2696786; Kuipers B., 2011, CVPR; Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624; Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718; Liu C., 2018, ABS180504025 CORR; Liu J., 2019, ABS190600161 CORR; Liu J, 2019, INT J COMPUT VISION, V127, P1545, DOI 10.1007/s11263-019-01192-2; Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50; Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127; Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030; Liu Y, 2019, IEEE T CIRC SYST VID, V29, P2416, DOI 10.1109/TCSVT.2018.2868123; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Luo Z., 2018, ECCV; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; Lv F., 2007, CVPR; Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554; Marin J, 2010, PROC CVPR IEEE, P137, DOI 10.1109/CVPR.2010.5540218; Masi I, 2019, INT J COMPUT VISION, V127, P642, DOI 10.1007/s11263-019-01178-0; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Pena A.M, 2017, CVPR; Pishchulin L, 2012, PROC CVPR IEEE, P3178, DOI 10.1109/CVPR.2012.6248052; Price W, 2019, IEEE INT CONF COMP V, P1371, DOI 10.1109/ICCVW.2019.00173; Puig X., 2018, CVPR; Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40; Rahmani H, 2018, IEEE T PATTERN ANAL, V40, P667, DOI 10.1109/TPAMI.2017.2691768; Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167; Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389; Rahmani H, 2015, PROC CVPR IEEE, P2458, DOI 10.1109/CVPR.2015.7298860; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; Schmid C, 2019, CVPR; Schniter P., 2020, ARXIV PREPRINT ARXIV; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132; Simonyan K, 2014, ADV NEUR IN, V27; Soomro K., 2012, COMPUT SCI; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sun C., 2017, ECCV; Sun J, 2015, CVPR; Tang S, 2019, GCPR; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Tung H.Y.F., 2017, NEURIPS; Varol G., 2018, ECCV; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Varol Gul, 2018, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2017.2712608; Vetter T, 2018, CVPRW; Vijayanarasimhan S, 2017, ABS170506950 CORR; Wang D., 2018, ECCV; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; Webber BL, 1993, SIMULATING HUMANS CO, DOI [10.1093/oso/9780195073591.001.0001, DOI 10.1093/OSO/9780195073591.001.0001]; Weinland D, 2007, IEEE I CONF COMP VIS, P170; Xiao J, 2015, ABS150603365 CORR; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Zhang DW, 2018, PROC CVPR IEEE, P6762, DOI 10.1109/CVPR.2018.00707; Zhang JT, 2018, IEEE T IMAGE PROCESS, V27, P4709, DOI 10.1109/TIP.2018.2836323; Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2017.233; Zheng JJ, 2016, IEEE T IMAGE PROCESS, V25, P2542, DOI 10.1109/TIP.2016.2548242; Zheng JJ, 2013, IEEE I CONF COMP VIS, P3176, DOI 10.1109/ICCV.2013.394; Zhu Y., 2018, ACCV; Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525; Zisserman A.=, 2019, ABS190702499 CORR; Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43; Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316	98	15	15	5	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2264	2287		10.1007/s11263-021-01467-7	http://dx.doi.org/10.1007/s11263-021-01467-7		MAY 2021	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW		Green Submitted, hybrid			2022-12-18	WOS:000650112400001
J	Chen, Z; Zhang, J; Tao, DC				Chen, Zhe; Zhang, Jing; Tao, Dacheng			Recursive Context Routing for Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Context modeling; Computer vision; Deep learning	IMPLICIT; IDENTIFICATION; INFORMATION; VISION; SCENES	Recent studies have confirmed that modeling contexts is important for object detection. However, current context modeling approaches still have limited expressive capacity and dynamics to encode contextual relationships and model contexts, deteriorating their effectiveness. In this paper, we instead seek to recast the current context modeling framework and perform more dynamic context modeling for object detection. In particular, we devise a novel Recursive Context Routing (ReCoR) mechanism to encode contextual relationships and model contexts more effectively. The ReCoR progressively models more contexts through a recursive structure, providing a more feasible and more comprehensive method to utilize complicated contexts and contextual relationships. For each recursive stage, we further decompose the modeling of contexts and contextual relationships into a spatial modeling process and a channel-wise modeling process, avoiding the need for exhaustive modeling of all the potential pair-wise contextual relationships with more dynamics in a single pass. The spatial modeling process focuses on spatial contexts and gradually involves more spatial contexts according to the recursive architecture. In the channel-wise modeling process, we introduce a context routing algorithm to improve the efficacy of modeling channel-wise contextual relationships dynamically. We perform a comprehensive evaluation of the proposed ReCoR on the popular MS COCO dataset and PASCAL VOC dataset. The effectiveness of the ReCoR can be validated on both datasets according to the consistent performance gains of applying our method on different baseline object detectors. For example, on MS COCO dataset, our approach can respectively deliver around 10% relative improvements for a Mask RCNN detector on the bounding box task, and 7% relative improvements on the instance segmentation task, surpassing existing context modeling approaches with a great margin. State-of-the-art detection performance can also be accessed by applying the ReCoR on the Cascade Mask RCNN detector, illustrating the great benefits of our method for improving context modeling and object detection.	[Chen, Zhe; Zhang, Jing; Tao, Dacheng] Univ Sydney, Sch Comp Sci, Fac Engn, 6 Cleveland St, Darlington, NSW 2008, Australia	University of Sydney	Zhang, J; Tao, DC (corresponding author), Univ Sydney, Sch Comp Sci, Fac Engn, 6 Cleveland St, Darlington, NSW 2008, Australia.	zhe.chen1@sydney.edu.au; jing.zhang1@sydney.edu.au; dacheng.tao@sydney.edu.au	Chen, Zhe/AAT-9716-2020	Chen, Zhe/0000-0001-5004-8975; ZHANG, JING/0000-0001-6595-7661	Australian Research Council [FL-170100117, DP-180103424, IH-180100002, IC-190100031, LE-200100049]	Australian Research Council(Australian Research Council)	This work was supported by Australian Research Council Projects FL-170100117, DP-180103424, IH-180100002, IC-190100031, LE-200100049.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Auckland ME, 2007, PSYCHON B REV, V14, P332, DOI 10.3758/BF03194073; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X; BIEDERMAN I, 1974, J EXP PSYCHOL, V103, P597, DOI 10.1037/h0037158; BOYCE SJ, 1989, J EXP PSYCHOL HUMAN, V15, P556, DOI 10.1037/0096-1523.15.3.556; Brockmole JR, 2006, J EXP PSYCHOL LEARN, V32, P699, DOI 10.1037/0278-7393.32.4.699; Brockmole JR, 2008, Q J EXP PSYCHOL, V61, P1886, DOI 10.1080/17470210701781155; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Cao Yue, 2019, P IEEE CVF INT C COM; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440; Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052; Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5; Chun MM, 1998, COGNITIVE PSYCHOL, V36, P28, DOI 10.1006/cogp.1998.0681; Chun MM, 2003, J EXP PSYCHOL LEARN, V29, P224, DOI 10.1037/0278-7393.29.2.224; Chun MM, 1999, PSYCHOL SCI, V10, P360, DOI 10.1111/1467-9280.00168; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Davenport JL, 2004, PSYCHOL SCI, V15, P559, DOI 10.1111/j.0956-7976.2004.00719.x; DEGRAEF P, 1992, CAN J PSYCHOL, V46, P489, DOI 10.1037/h0084324; Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Galleguillos C, 2008, PROC CVPR IEEE, P3552; Galleguillos C, 2010, COMPUT VIS IMAGE UND, V114, P712, DOI 10.1016/j.cviu.2010.02.004; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Ghiasi G, 2018, ADV NEUR IN, V31; Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Harjuniemi E, 2018, ISWC'18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P254, DOI 10.1145/3267242.3267303; Harzallah H, 2009, IEEE I CONF COMP VIS, P237, DOI 10.1109/ICCV.2009.5459257; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K, 2017, P IEEE INT C COMPUTE, DOI DOI 10.1109/ICCV.2017.322; Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4; Henderson JM, 1999, ANNU REV PSYCHOL, V50, P243, DOI 10.1146/annurev.psych.50.1.243; Hinton Geoffrey E., 2018, INT C LEARN REPR; Hollingworth A, 1998, J EXP PSYCHOL GEN, V127, P398, DOI 10.1037/0096-3445.127.4.398; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang YP, 2018, IEEE INT ULTRA SYM; Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98; Li HY, 2019, INT J COMPUT VISION, V127, P225, DOI 10.1007/s11263-018-1101-7; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu L, 2019, J COMPUTER VISION, V128, P261; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Liu Y, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278091; Lu M, 2017, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2017.270; Modolo D., 2015, BMVC, V1, P6; Mordan T, 2019, INT J COMPUT VISION, V127, P1659, DOI 10.1007/s11263-018-1109-z; Ouyang W., 2017, ICCV; Ouyang WL, 2016, INT J COMPUT VISION, V120, P14, DOI 10.1007/s11263-016-0890-9; PALMER SE, 1975, MEM COGNITION, V3, P519, DOI 10.3758/BF03197524; Perera R, 2018, IEEE INT ULTRA SYM; Qiao Siyuan, 2019, ARXIV190310520, P2; Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren Jimmy, 2017, CVPR, DOI DOI 10.1109/CVPR.2017.87; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sabour Sara, 2017, PROC 31 INT C NEURAL; Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212; Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273; Vondrick C, 2016, INT J COMPUT VISION, V119, P145, DOI 10.1007/s11263-016-0884-7; Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu R. R., 2010, IEEE T PATTERN ANAL, V32, P1627; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zeng XY, 2018, IEEE T PATTERN ANAL, V40, P2109, DOI 10.1109/TPAMI.2017.2745563; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747	80	15	15	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2021	129	1								10.1007/s11263-020-01370-7	http://dx.doi.org/10.1007/s11263-020-01370-7		AUG 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	PU0KV					2022-12-18	WOS:000560985300001
J	Bellavia, F; Colombo, C				Bellavia, Fabio; Colombo, Carlo			Is There Anything New to Say About SIFT Matching?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						SIFT; sGLOH2; Quantization; Binary descriptors; Symmetric matching; Hierarchical cascade filtering; Deep descriptors; Keypoint patch orientation; Approximated overlap error	DESCRIPTORS; FEATURES; REPRESENTATION; PERFORMANCE; TEXTURE; WORLD	SIFT is a classical hand-crafted, histogram-based descriptor that has deeply influenced research on image matching for more than a decade. In this paper, a critical review of the aspects that affect SIFT matching performance is carried out, and novel descriptor design strategies are introduced and individually evaluated. These encompass quantization, binarization and hierarchical cascade filtering as means to reduce data storage and increase matching efficiency, with no significant loss of accuracy. An original contextual matching strategy based on a symmetrical variant of the usual nearest-neighbor ratio is discussed as well, that can increase the discriminative power of any descriptor. The paper then undertakes a comprehensive experimental evaluation of state-of-the-art hand-crafted and data-driven descriptors, also including the most recent deep descriptors. Comparisons are carried out according to several performance parameters, among which accuracy and space-time efficiency. Results are provided for both planar and non-planar scenes, the latter being evaluated with a new benchmark based on the concept of approximated patch overlap. Experimental evidence shows that, despite their age, SIFT and other hand-crafted descriptors, once enhanced through the proposed strategies, are ready to meet the future image matching challenges. We also believe that the lessons learned from this work will inspire the design of better hand-crafted and data-driven descriptors.	[Bellavia, Fabio] Univ Palermo, Dept Math & Comp Sci, Via Archirafi 34, I-90123 Palermo, Italy; [Colombo, Carlo] Univ Firenze, Dept Informat Engn, Via S Marta 3, I-50139 Florence, Italy	University of Palermo; University of Florence	Colombo, C (corresponding author), Univ Firenze, Dept Informat Engn, Via S Marta 3, I-50139 Florence, Italy.	fabio.bellavia@unipa.it; carlo.colombo@unifi.it	Bellavia, Fabio/N-6790-2018	Bellavia, Fabio/0000-0002-1688-8476	Air Force Research Laboratory; Defense Advanced Research Projects Agency [FA8750-16-2-0188]; European Social Fund (ESF) [CUP B74I18000220006, AIM 1875400]; Italian Ministry of Education and Research (MIUR) under the programPONRicerca e Innovazione 2014-2020	Air Force Research Laboratory; Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); European Social Fund (ESF)(European Social Fund (ESF)); Italian Ministry of Education and Research (MIUR) under the programPONRicerca e Innovazione 2014-2020(Ministry of Education, Universities and Research (MIUR))	The Titan Xp used for this research was generously donated by the NVIDIA Corporation. This work is based on research partially sponsored by the Air Force Research Laboratory and the Defense Advanced Research Projects Agency under agreement number FA8750-16-2-0188. The U.S. Government is authorized to reproduce and distribute reprints forGovernmental purposes notwithstanding any copyright notation thereon. F. Bellavia is currently funded by the Italian Ministry of Education and Research (MIUR) under the programPONRicerca e Innovazione 2014-2020, cofunded by the European Social Fund (ESF), CUP B74I18000220006, id. proposta AIM 1875400, linea di attivita 2, Area Cultural Heritage.	Aanaes H, 2012, INT J COMPUT VISION, V97, P18, DOI 10.1007/s11263-011-0473-8; Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715; [Anonymous], 2018, ARXIV180101466; Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018; Baber J, 2014, IMAGE VISION COMPUT, V32, P940, DOI 10.1016/j.imavis.2014.08.006; Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410; Balntas V, 2015, PROC CVPR IEEE, P2367, DOI 10.1109/CVPR.2015.7298850; Balntas Vassileios, 2016, BMVC, V2, DOI DOI 10.5244/C.30.119; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Bellavia F., 2019, INT C COMP AN IM PAT, P299; Bellavia F, 2018, IEEE T PATTERN ANAL, V40, P931, DOI 10.1109/TPAMI.2017.2697849; Bellavia F, 2013, LECT NOTES COMPUT SC, V8156, P270; Bian J. W., 2018, ARXIV180802267; Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54; Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56; Chandrasekhar V, 2012, INT J COMPUT VISION, V96, P384, DOI 10.1007/s11263-011-0453-z; Chi-Yi Tsai, 2013, Journal of Software, V8, P2197, DOI 10.4304/jsw.8.9.2197-2201; Cui Y, 2009, LECT NOTES COMPUT SC, V5627, P258; Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145; Fan B, 2019, IEEE T IMAGE PROCESS, V28, P4774, DOI 10.1109/TIP.2019.2909640; Fan B, 2014, IEEE T IMAGE PROCESS, V23, P2583, DOI 10.1109/TIP.2014.2317981; Fan B, 2012, IEEE T PATTERN ANAL, V34, P2031, DOI 10.1109/TPAMI.2011.277; Fanfani M, 2016, MACH VISION APPL, V27, P833, DOI 10.1007/s00138-016-0793-3; Fraundorfer F., 2005, IEEE COMP SOC C COMP, P33, DOI [10.1109/CVPR.2005.393, DOI 10.1109/CVPR.2005.393]; Haibin L., 2006, P IEEE C COMP VIS PA, P246, DOI DOI 10.1109/CVPR.2006.99; Hassner T, 2012, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2012.6247842; He K, 2018, PROC CVPR IEEE, P4023, DOI 10.1109/CVPR.2018.00423; Heikkila M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014; Hua GC, 2007, LECT NOTES COMPUT SC, V4808, P1; Jin ZM, 2014, IEEE T CYBERNETICS, V44, P2167, DOI 10.1109/TCYB.2014.2302018; Ke Y, 2004, PROC CVPR IEEE, P506; Khan N, 2015, MACH VISION APPL, V26, P819, DOI 10.1007/s00138-015-0689-7; Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151; Lenc K., 2014, P COMP VIS WINT WORK, P67; Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542; Levi G, 2016, IEEE WINT CONF APPL; Lin K, 2019, IEEE T PATTERN ANAL, V41, P1501, DOI 10.1109/TPAMI.2018.2833865; Lin WY, 2018, IEEE T PATTERN ANAL, V40, P34, DOI 10.1109/TPAMI.2017.2652468; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo ZX, 2018, LECT NOTES COMPUT SC, V11213, P170, DOI 10.1007/978-3-030-01240-3_11; Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI 10.1007/s11263-018-1117-z; Malkov YA, 2020, IEEE T PATTERN ANAL, V42, P824, DOI 10.1109/TPAMI.2018.2889473; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mishchuk Anastasiya, 2017, ADV NEURAL INFORM PR; Moreels P, 2007, INT J COMPUT VISION, V73, P263, DOI 10.1007/s11263-006-9967-1; Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; Mukundan A., 2017, BRIT MACH VIS C BMVC; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Schonberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736; Simo-Serra E, 2015, ICCV; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Strecha C, 2008, PROC CVPR IEEE, P2838; Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103; Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Treen G., 2009, P WORKSH APPL COMP V, P1, DOI DOI 10.1109/WACV.2009.5403099; Trzcinski T, 2013, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2013.370; Trzcinski Tomasz, 2012, ADV NEURAL INFORM PR, P269, DOI DOI 10.1177/1753193411419945; Tuytelaars T., 2007, P IEEE INT C COMP VI; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Wang ZH, 2016, IEEE T PATTERN ANAL, V38, P2198, DOI 10.1109/TPAMI.2015.2513396; Wei X, 2018, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2018.00200; Winder S, 2009, PROC CVPR IEEE, P178, DOI 10.1109/CVPRW.2009.5206839; Yang TY, 2016, PROC CVPR IEEE, P327, DOI 10.1109/CVPR.2016.42; Yi KM, 2016, PROC CVPR IEEE, P107, DOI 10.1109/CVPR.2016.19; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4; Zhang W, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P854; Ziegler A, 2012, ADV NEURAL INFORM PR, P1	72	15	16	2	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2020	128	7					1847	1866		10.1007/s11263-020-01297-z	http://dx.doi.org/10.1007/s11263-020-01297-z		MAR 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MC3NG		Green Submitted			2022-12-18	WOS:000520662000004
J	Ranjan, A; Hoffmann, DT; Tzionas, D; Tang, SY; Romero, J; Black, MJ				Ranjan, Anurag; Hoffmann, David T.; Tzionas, Dimitrios; Tang, Siyu; Romero, Javier; Black, Michael J.			Learning Multi-human Optical Flow	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Optical; Flow; Deep; Learning; Human; Bodies; Synthetic; Dataset; Humanflow	CAPTURING HANDS; MOTION	The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single- and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research.	[Ranjan, Anurag; Hoffmann, David T.; Tzionas, Dimitrios; Tang, Siyu; Black, Michael J.] Max Planck Inst Intelligent Syst, Tubingen, Germany; [Romero, Javier] Amazon Body Labs, Barcelona, Spain	Max Planck Society	Ranjan, A (corresponding author), Max Planck Inst Intelligent Syst, Tubingen, Germany.	anurag.ranjan@tue.mpg.de; david.hoffmann@tue.mpg.de; dimitris.tzionas@tue.mpg.de; siyu.tang@tue.mpg.de; javier@amazon.com; black@tue.mpg.de	Tzionas, Dimitrios/H-9513-2018	Tzionas, Dimitrios/0000-0002-7963-2582	Max Planck Society	Max Planck Society(Max Planck SocietyFoundation CELLEX)	Open access funding provided by Max Planck Society. We thank Yiyi Liao for helping us with optical flow evaluation. We thank Sergi Pujades for helping us with collision detection of meshes. We thank Cristian Sminchisescu for the Human3.6M MoCap marker data.	Abdulla W., 2017, GITHUB REPOSITORY, DOI DOI 10.1109/CVPR.2017.106; Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457; Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Barbosa IB, 2018, COMPUT VIS IMAGE UND, V167, P50, DOI 10.1016/j.cviu.2017.12.002; BASTIONI M, 2007, MAKEHUMAN OPEN SOURC; Black MJ, 1997, PROC CVPR IEEE, P561, DOI 10.1109/CVPR.1997.609381; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Brox T, 2009, PROC CVPR IEEE, P41, DOI 10.1109/CVPRW.2009.5206697; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Charles J, 2016, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR.2016.334; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Davis JW, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P39, DOI 10.1109/EVENT.2001.938864; Dong XW, 2018, IEEE CONF COMPUT; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Fablet R, 2002, LECT NOTES COMPUT SC, V2350, P476; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Fragkiadaki K, 2013, PROC CVPR IEEE, P2059, DOI 10.1109/CVPR.2013.268; Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075; GAIDON A, 2016, PROC CVPR IEEE, P4340, DOI DOI 10.1109/CVPR.2016.470; Gaidon A, 2014, INT J COMPUT VISION, V107, P219, DOI 10.1007/s11263-013-0677-1; Geiger A., 2012, P IEEE COMP SOC C CO; Geman D, 2016, P NATL ACAD SCI USA, V113, P9384, DOI 10.1073/pnas.1609793113; Ghezelghieh MF, 2016, INT CONF 3D VISION, P685, DOI 10.1109/3DV.2016.75; Green R., 2003, ARCH GAME DEVELOPERS, V56, P4; Gross Ralph, 2001, CMURITR0118; GUNEY F, 2016, AS C COMP VIS ACCV; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Kingma D.P, P 3 INT C LEARNING R; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Loper M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661273; Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Menze Moritz, 2015, CVPR; Milan A., 2016, MOT16 BENCHMARK MULT; OBINETTE KM, 2002, TECHNICAL REPORT, V1; Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Ranjan Anurag, 2018, BRIT MACH VIS C BMVC; Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720; Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883; Romero J, 2015, LECT NOTES COMPUT SC, V9358, P412, DOI 10.1007/978-3-319-24947-6_34; SHUGRINA M, 2019, IEEE C COMP VIS PATT; Soomro K., 2012, ARXIV; Sun DQ, 2008, LECT NOTES COMPUT SC, V5304, P83; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; TESCHNER M, 2004, EUROGRAPHICS STATE O, P119; Tran D, 2016, IEEE COMPUT SOC CONF, P402, DOI 10.1109/CVPRW.2016.57; Tzionas D, 2016, INT J COMPUT VISION, V118, P172, DOI 10.1007/s11263-016-0895-4; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Wulff J, 2015, PROC CVPR IEEE, P120, DOI 10.1109/CVPR.2015.7298607; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zuffi S, 2013, IEEE I CONF COMP VIS, P3312, DOI 10.1109/ICCV.2013.411	61	15	16	3	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		873	890		10.1007/s11263-019-01279-w	http://dx.doi.org/10.1007/s11263-019-01279-w		JAN 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		hybrid, Green Submitted			2022-12-18	WOS:000505388600002
J	Rogez, G; Schmid, C				Rogez, Gregory; Schmid, Cordelia			Image-Based Synthesis for Deep 3D Human Pose Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human 3D pose estimation; Data augmentation; CNN; Data synthesis		This paper addresses the problem of 3D human pose estimation in the wild. A significant challenge is the lack of training data, i.e., 2D images of humans annotated with 3D poses. Such data is necessary to train state-of-the-art CNN architectures. Here, we propose a solution to generate a large set of photorealistic synthetic images of humans with 3D pose annotations. We introduce an image-based synthesis engine that artificially augments a dataset of real images with 2D human pose annotations using 3D motion capture data. Given a candidate 3D pose, our algorithm selects for each joint an image whose 2D pose locally matches the projected 3D pose. The selected images are then combined to generate a new synthetic image by stitching local image patches in a kinematically constrained manner. The resulting images are used to train an end-to-end CNN for full-body 3D pose estimation. We cluster the training data into a large number of pose classes and tackle pose estimation as a K-way classification problem. Such an approach is viable only with large training sets such as ours. Our method outperforms most of the published works in terms of 3D pose estimation in controlled environments (Human3.6M) and shows promising results for real-world images (LSP). This demonstrates that CNNs trained on artificial images generalize well to real images. Compared to data generated from more classical rendering engines, our synthetic images do not require any domain adaptation or fine-tuning stage.	[Rogez, Gregory; Schmid, Cordelia] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,Inst Engn,LJK, F-38000 Grenoble, France	Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria	Rogez, G (corresponding author), Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,Inst Engn,LJK, F-38000 Grenoble, France.	gregory.rogez@inria.fr			European Commission under FP7 Marie Curie IOF Grant [PIOF-GA-2012-328288]; ERC advanced Grant ALLEGRO; Amazon Academic Research Award (AARA); NVIDIA	European Commission under FP7 Marie Curie IOF Grant; ERC advanced Grant ALLEGRO; Amazon Academic Research Award (AARA); NVIDIA	This work was supported by the European Commission under FP7 Marie Curie IOF Grant (PIOF-GA-2012-328288) and partially supported by the ERC advanced Grant ALLEGRO and an Amazon Academic Research Award (AARA). We acknowledge the support of NVIDIA with the donation of the GPUs used for this research. We thank Dr. Philippe Weinzaepfel for his help. We also thank the anonymous reviewers for their comments and suggestions that helped improve the paper.	Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; [Anonymous], 2016, ECCV; Bissacco A., 2006, NIPS; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Chen X., 2014, P 27 ANN C NEURAL IN, P1736, DOI DOI 10.1109/CVPR.2018.00742; de Souza Cesar Roberto, 2017, CVPR, DOI DOI 10.1109/CVPR.2017.278; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Du Y, 2016, LECT NOTES COMPUT SC, V9908, P20, DOI 10.1007/978-3-319-46493-0_2; Elhayek A, 2015, PROC CVPR IEEE, P3810, DOI 10.1109/CVPR.2015.7299005; Enzweiler M, 2008, PROC CVPR IEEE, P1944; Fan XC, 2014, LECT NOTES COMPUT SC, V8689, P174, DOI 10.1007/978-3-319-10590-1_12; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006; Hornung A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186645; Huang SY, 2017, PROC CVPR IEEE, P4664, DOI 10.1109/CVPR.2017.496; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318; Johnson Sam, 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12]; Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381; Kostrikov I., 2014, PROC BRIT MACH VIS C; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li S., 2016, IJCV; Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Mori G, 2006, IEEE T PATTERN ANAL, V28, P1052, DOI 10.1109/TPAMI.2006.149; Okada R, 2008, LECT NOTES COMPUT SC, V5303, P434, DOI 10.1007/978-3-540-88688-4_32; Park Dennis, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P58, DOI 10.1109/CVPRW.2015.7301337; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Pishchulin L, 2012, PROC CVPR IEEE, P3178, DOI 10.1109/CVPR.2012.6248052; Ramakrishna Varun, 2012, ECCV, P573, DOI DOI 10.1007/978-3-642-33765-9; Rogez G, 2016, ADV NEUR IN, V29; Rogez G, 2017, PROC CVPR IEEE, P1216, DOI 10.1109/CVPR.2017.134; Rogez G, 2015, PROC CVPR IEEE, P4325, DOI 10.1109/CVPR.2015.7299061; Rogez G, 2012, INT J COMPUT VISION, V99, P25, DOI 10.1007/s11263-012-0516-9; Romero J, 2010, IEEE INT CONF ROBOT, P458, DOI 10.1109/ROBOT.2010.5509753; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Sigal L, 2006, LECT NOTES COMPUT SC, V4069, P185; Simo-Serra E, 2013, PROC CVPR IEEE, P3634, DOI 10.1109/CVPR.2013.466; Simo-Serra E., 2012, CVPR; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Taylor J. C., 2000, CVPR; Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113; Tekin Bugra, 2016, BRIT MACH VIS C 2016, DOI DOI 10.5244/C.30.130; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Wang CY, 2014, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2014.303; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xu JL, 2014, IEEE T PATTERN ANAL, V36, P2367, DOI 10.1109/TPAMI.2014.2327973; Yang W, 2016, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2016.335; Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535; Zhou F, 2014, LECT NOTES COMPUT SC, V8694, P62, DOI 10.1007/978-3-319-10599-4_5; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51; Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17; Zuffi S, 2015, PROC CVPR IEEE, P3537, DOI 10.1109/CVPR.2015.7298976	72	15	15	2	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2018	126	9			SI		993	1008		10.1007/s11263-018-1071-9	http://dx.doi.org/10.1007/s11263-018-1071-9			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GQ3HQ		Green Submitted			2022-12-18	WOS:000441553300007
J	Varadarajan, J; Subramanian, R; Bulo, SR; Ahuja, N; Lanz, O; Ricci, E				Varadarajan, Jagannadan; Subramanian, Ramanathan; Bulo, Samuel Rota; Ahuja, Narendra; Lanz, Oswald; Ricci, Elisa			Joint Estimation of Human Pose and Conversational Groups from Social Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Head and body pose estimation; F-formation estimation; Semi-supervised learning; Convex optimization; Conversational groups; Video surveillance	HEAD POSE; VISUAL FOCUS; ATTENTION; TRACKING	Despite many attempts in the last few years, automatic analysis of social scenes captured by wide-angle camera networks remains a very challenging task due to the low resolution of targets, background clutter and frequent and persistent occlusions. In this paper, we present a novel framework for jointly estimating (i) head, body orientations of targets and (ii) conversational groups called F-formations from social scenes. In contrast to prior works that have (a) exploited the limited range of head and body orientations to jointly learn both, or (b) employed the mutual head (but not body) pose of interactors for deducing F-formations, we propose a weakly-supervised learning algorithm for joint inference. Our algorithm employs body pose as the primary cue for F-formation estimation, and an alternating optimization strategy is proposed to iteratively refine F-formation and pose estimates. We demonstrate the increased efficacy of joint inference over the state-of-the-art via extensive experiments on three social datasets.	[Varadarajan, Jagannadan; Ahuja, Narendra] Adv Digital Sci Ctr, Singapore, Singapore; [Subramanian, Ramanathan] Int Inst Informat Technol, Hyderabad, Andhra Prades, India; [Subramanian, Ramanathan] Univ Glasgow, Glasgow, Lanark, Scotland; [Bulo, Samuel Rota] Mapillary Res, Graz, Austria; [Bulo, Samuel Rota; Lanz, Oswald; Ricci, Elisa] Fdn Bruno Kessler, Trento, Italy; [Ahuja, Narendra] Univ Illinois, Champaign, IL USA; [Ricci, Elisa] Univ Perugia, Dept Engn, Perugia, Italy	International Institute of Information Technology Hyderabad; University of Glasgow; Fondazione Bruno Kessler; University of Illinois System; University of Illinois Urbana-Champaign; University of Perugia	Varadarajan, J (corresponding author), Adv Digital Sci Ctr, Singapore, Singapore.	vjagan@gmail.com; ramanathan.Subramanian@glasgow.ac.uk; rotabulo@fbk.eu; n-ahuja@illinois.edu; lanz@fbk.eu; eliricci@fbk.eu	Lanz, Oswald/AAW-7865-2021	Ricci, Elisa/0000-0002-0228-1147; Lanz, Oswald/0000-0003-4793-4276; Subramanian, Ramanathan/0000-0001-9441-7074	Singapore's Agency for Science, Technology and Research (A*STAR)	Singapore's Agency for Science, Technology and Research (A*STAR)(Agency for Science Technology & Research (A*STAR))	This work is supported by the research grant for the Human-Centered Cyber-physical Systems Programme at the Advanced Digital Sciences Center from Singapore's Agency for Science, Technology and Research (A*STAR). We thank NVIDIA for GPU donation.	Alameda-Pineda X., 2015, ACM MULTIMEDIA; Alameda-Pineda X, 2016, IEEE T PATTERN ANAL, V38, P1707, DOI 10.1109/TPAMI.2015.2496269; Alletto S., 2014, WORKSH EG VIS; Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754; Ba S., 2008, IEEE INT C AC SPEECH; Ba SO, 2006, LECT NOTES COMPUT SC, V4299, P75; Bazzani L, 2013, EXPERT SYST, V30, P115, DOI 10.1111/j.1468-0394.2012.00622.x; Benfold B., 2011, INT C COMP VIS; Butko T, 2011, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2011-1; Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28; Chamveha I, 2013, COMPUT VIS IMAGE UND, V117, P1502, DOI 10.1016/j.cviu.2013.06.005; Chen C., 2011, IEEE ICCV SISM; Chen C., 2012, COMPUTER VISION PATT; Chi EC, 2015, J COMPUT GRAPH STAT, V24, P994, DOI 10.1080/10618600.2014.948181; Choi  W., 2014, EUR C COMP VIS; CIOLEK TM, 1980, SOCIOL INQ, V50, P237, DOI 10.1111/j.1475-682X.1980.tb00022.x; Cristani M., 2011, BRIT MACH VIS C; Demirkus M., 2014, EUR C COMP VIS; Eichner M., 2010, EUR C COMP VIS; Gan T, 2013, ACM MULTIMEDIA; Heili A., 2014, INT C IM PROC; Hocking T., 2011, INT C MACH LEARN; Hu T, 2015, COMPUT VIS IMAGE UND, V134, P89, DOI 10.1016/j.cviu.2015.02.007; Krahnstoever N., 2011, IEEE ADV VIDEO SIGNA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Leal-Taixe L., 2014, COMPUTER VISION PATT; Liem MC, 2014, IMAGE VISION COMPUT, V32, P728, DOI 10.1016/j.imavis.2014.04.007; Marin-Jimenez MJ, 2014, INT J COMPUT VISION, V106, P282, DOI 10.1007/s11263-013-0655-7; Mathias M., 2013, INT C COMP VIS; Meyer G. P., 2015, INT C COMP VIS; Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106; Patron-Perez A, 2012, IEEE T PATTERN ANAL, V34, P2441, DOI 10.1109/TPAMI.2012.24; Pellegrini S., 2010, EUR C COMP VIS; Rajagopal AK, 2014, INT J COMPUT VISION, V109, P146, DOI 10.1007/s11263-013-0692-2; Ricci E, 2015, INT C COMP VIS ICCV; Robertson Neil, 2006, EUR C COMP VIS; Setti F., 2015, PLOS ONE, V10; Setti F., 2013, INT C IM PROC; Setti F, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123783; Setti F, 2013, 2013 14TH INTERNATIONAL WORKSHOP ON IMAGE ANALYSIS FOR MULTIMEDIA INTERACTIVE SERVICES (WIAMIS); Smith K, 2008, IEEE T PATTERN ANAL, V30, P1212, DOI 10.1109/TPAMI.2007.70773; Tompson J.J., 2014, ADV NEURAL INF PROCE, P1799, DOI DOI 10.5555/2968826.2969027; Toshev A., 2014, COMPUTER VISION PATT; Tran K. N., 2013, INT JOINT C COMP VIS; Vascon S., 2014, P AS C COMP VIS; Vascon S, 2016, COMPUT VIS IMAGE UND, V143, P11, DOI 10.1016/j.cviu.2015.09.012; Voit M, 2009, LECT NOTES COMPUT SC, V5815, P415, DOI 10.1007/978-3-642-04667-4_42; Wojek C., 2011, COMPUTER VISION PATT; Yan SC, 2009, IEEE T IMAGE PROCESS, V18, P202, DOI 10.1109/TIP.2008.2006400; Yan Y, 2013, INT C COMP VIS; Yan Y, 2016, IEEE T PATTERN ANAL, V38, P1070, DOI 10.1109/TPAMI.2015.2477843; Zen G., 2010, ACM MULT WORKSH MULT; Zhu X., 2005, 1530 U WISC; Zhu X., 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI [10.2200/S00196ED1V01Y200906AIM006, DOI 10.2200/S00196ED1V01Y200906AIM006]	55	15	15	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		410	429		10.1007/s11263-017-1026-6	http://dx.doi.org/10.1007/s11263-017-1026-6			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA					2022-12-18	WOS:000425619100015
J	Mondal, A; Ghosh, S; Ghosh, A				Mondal, Ajoy; Ghosh, Susmita; Ghosh, Ashish			Partially Camouflaged Object Tracking using Modified Probabilistic Neural Network and Fuzzy Energy based Active Contour	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Camouflage; Multi-cue; Probabilistic neural network; Fuzzy energy and visual similarity	VISUAL SURVEILLANCE; INTEGRATING COLOR; TEXTURE FEATURES; SEGMENTATION; MODEL; CLASSIFICATION; APPEARANCE; TARGETS; MOTION; SPEED	Various problems in object detection and tracking have attracted researchers to develop methodologies for solving these problems. Occurrence of camouflage is one of such challenges that makes object detection and tracking problems more complex. However, less attention has been given to detect and track camouflaged objects due to complexity of the problem. In this article, we propose a tracking-by-detection algorithm to detect and track camouflaged objects. To increase separability between the camouflaged object and the background, we propose to integrate features (CIELab, histogram of orientation gradients and locally adaptive ternary pattern) from multi-cue (color, shape and texture) to represent a camouflaged object. A probabilistic neural network (PNN) is modified to construct an efficient discriminative appearance model for detecting camouflaged objects in video sequences. A large number of training patterns (many could be redundant) are reduced based on motion of the object in the modified PNN. The modified PNN makes the detection process faster and also increases the detection accuracy. Due to high visual similarity among the camouflaged object and the background, the boundary of camouflaged object is not well defined (i.e., boundary may be smooth and/or discontinuous). In this context, a robust fuzzy energy based active contour model using both global and local information is proposed to extract contour (boundary) of the detected camouflaged object for tracking. We show a realization of the proposed method and demonstrate its performance (both quantitatively and qualitatively) with respect to state-of-the-art techniques on several challenging sequences. Analysis of results concludes that the proposed technique can track camouflaged (fully or partially) objects as well as objects in various complex environments in a better way as compare to the existing ones.	[Mondal, Ajoy; Ghosh, Ashish] Indian Stat Inst, Machine Intelligence Unit, Kolkata 700108, India; [Ghosh, Susmita] Jadavpur Univ, Dept Comp Sci & Engn, Kolkata 700032, India	Indian Statistical Institute; Indian Statistical Institute Kolkata; Jadavpur University	Ghosh, A (corresponding author), Indian Stat Inst, Machine Intelligence Unit, Kolkata 700108, India.	ajoy.mondal83@gmail.com; susmitaghoshju@gmail.com; ash@isical.ac.in		GHOSH, ASHISH/0000-0003-1548-5576; GHOSH, SUSMITA/0000-0002-1691-761X	U.S. Army through the project "Processing and Analysis of Aircraft Images with Machine Learning Techniques for Locating Objects of Interest" [FA5209-08-P-0241]	U.S. Army through the project "Processing and Analysis of Aircraft Images with Machine Learning Techniques for Locating Objects of Interest"	The authors like to thank the editor and the reviewers for their thorough and constructive comments, which helped a lot to enhance the quality of the manuscript. Funding by U.S. Army through the project "Processing and Analysis of Aircraft Images with Machine Learning Techniques for Locating Objects of Interest" (Contract No. FA5209-08-P-0241) is also gratefully acknowledged.	Akhloufi MA, 2010, IEEE SYS MAN CYBERN, P3308, DOI 10.1109/ICSMC.2010.5642391; Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35; Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226; Bandouch J, 2012, INT J COMPUT VISION, V99, P166, DOI 10.1007/s11263-012-0522-y; Bishop, 1995, NEURAL NETWORKS PATT; Boult TE, 2001, P IEEE, V89, P1382, DOI 10.1109/5.959337; BURRASCANO P, 1991, IEEE T NEURAL NETWOR, V2, P458, DOI 10.1109/72.88165; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Challa S., 2011, FUNDAMENTALS OBJECT; Chan T., 2002, CAM UCLA, V68; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chandesa T., 2009, 2009 IEEE International Conference on Signal and Image Processing Applications (ICSIPA 2009), P468, DOI 10.1109/ICSIPA.2009.5478700; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; Conte D, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P340, DOI 10.1109/AVSS.2009.83; Copeland AC, 1997, P SOC PHOTO-OPT INS, V3070, P194, DOI 10.1117/12.281557; Cremers D, 2007, INT J COMPUT VISION, V72, P195, DOI 10.1007/s11263-006-8711-1; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Desai C, 2011, INT J COMPUT VISION, V95, P1, DOI 10.1007/s11263-011-0439-x; Di Lascio R, 2013, COMPUT VIS IMAGE UND, V117, P892, DOI 10.1016/j.cviu.2013.04.004; Du H, 2012, COMPUT GRAPH FORUM, V31, P2203, DOI 10.1111/j.1467-8659.2012.03213.x; FOX C, 1988, INTRO CALCULUS VARIA; Freeman W T, 1995, P INT WORKSH AUT FAC, P296; Gonzalez R.C., 2008, DIGITAL IMAGE PROCES; Gretzmacher FM, 1998, P SOC PHOTO-OPT INS, V3375, P58, DOI 10.1117/12.327177; Hao W, 2007, MEAS SCI TECHNOL, V18, P1999, DOI 10.1088/0957-0233/18/7/028; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251; Harville M, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P3, DOI 10.1109/EVENT.2001.938860; He L, 2007, LECT NOTES COMPUT SC, V4485, P777; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Huang Z.Q., 2005, P DIG IM COMP TECHN, P24; Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880; KaewTrakulPong P, 2003, IMAGE VISION COMPUT, V21, P913, DOI 10.1016/S0262-8856(03)00076-3; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kasturi R, 2009, IEEE T PATTERN ANAL, V31, P319, DOI 10.1109/TPAMI.2008.57; Krinidis M., 2012, 8 INT C ART INT APPL, P27; Krinidis S, 2009, IEEE T IMAGE PROCESS, V18, P2747, DOI 10.1109/TIP.2009.2030468; Kusy M, 2013, LECT NOTES ARTIF INT, V7894, P118, DOI 10.1007/978-3-642-38658-9_11; Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821; Lan XY, 2014, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2014.156; Lankton S, 2008, IEEE T IMAGE PROCESS, V17, P2029, DOI 10.1109/TIP.2008.2004611; Levi K, 2004, PROC CVPR IEEE, P53; Li CM, 2008, IEEE T IMAGE PROCESS, V17, P1940, DOI 10.1109/TIP.2008.2002304; Li CM, 2010, IEEE T IMAGE PROCESS, V19, P3243, DOI 10.1109/TIP.2010.2069690; Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039; Maggio E., 2011, VIDEO TRACKING OR; Malathi T, 2013, ANNU IEEE IND CONF; Mao KZ, 2000, IEEE T NEURAL NETWOR, V11, P1009, DOI 10.1109/72.857781; METZ CE, 1978, SEMIN NUCL MED, V8, P283, DOI 10.1016/S0001-2998(78)80014-2; Mondal A., 2014, SOFT COMPUT, V20, P1; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; MUSAVI MT, 1994, IEEE T PATTERN ANAL, V16, P659, DOI 10.1109/34.295911; Ning JF, 2009, INT J PATTERN RECOGN, V23, P1245, DOI 10.1142/S0218001409007624; Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Pan YS, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P350, DOI 10.1109/MMSP.2006.285328; Raghu PP, 1998, IEEE T NEURAL NETWOR, V9, P516, DOI 10.1109/72.668893; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Santner J, 2010, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2010.5540145; Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891; Shyu KK, 2012, NONLINEAR DYNAM, V67, P1559, DOI 10.1007/s11071-011-0088-1; Singh SK, 2013, IERI PROC, V4, P351, DOI 10.1016/j.ieri.2013.11.050; SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q; Suard F, 2006, 2006 IEEE INTELLIGENT VEHICLES SYMPOSIUM, P207; Talu MF, 2013, EXPERT SYST APPL, V40, P6233, DOI 10.1016/j.eswa.2013.05.056; Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645; Tran TT, 2014, J VIS COMMUN IMAGE R, V25, P1732, DOI 10.1016/j.jvcir.2014.06.006; TRAVEN HGC, 1991, IEEE T NEURAL NETWOR, V2, P366, DOI 10.1109/72.97913; Wang JQ, 2008, IEEE T IMAGE PROCESS, V17, P235, DOI 10.1109/TIP.2007.914150; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Wu Y, 2015, APPL SOFT COMPUT, V34, P301, DOI 10.1016/j.asoc.2015.04.058; Yan WJ, 2011, J AMB INTEL SMART EN, V3, P237, DOI 10.3233/AIS-2011-0111; Yang B, 2014, INT J COMPUT VISION, V107, P203, DOI 10.1007/s11263-013-0666-4; Yang F, 2014, IEEE T IMAGE PROCESS, V23, P1639, DOI 10.1109/TIP.2014.2300823; Yezzi A, 1997, IEEE T MED IMAGING, V16, P199, DOI 10.1109/42.563665; Yilmaz A., 2006, ACM COMPUT SURV, V38, P1264; Yin JQ, 2011, PROCEDIA ENGINEER, V15, DOI 10.1016/j.proeng.2011.08.412; Zhang KH, 2010, IMAGE VISION COMPUT, V28, P668, DOI 10.1016/j.imavis.2009.10.009; Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z; Zhang XF, 2015, J INEQUAL APPL, DOI 10.1186/1029-242X-2015-1; Zhong M, 2007, NEURAL COMPUT, V19, P2840, DOI 10.1162/neco.2007.19.10.2840; Zhong W, 2014, IEEE T IMAGE PROCESS, V23, P2356, DOI 10.1109/TIP.2014.2313227	84	15	18	3	29	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2017	122	1					116	148		10.1007/s11263-016-0959-5	http://dx.doi.org/10.1007/s11263-016-0959-5			33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EL2AF					2022-12-18	WOS:000394421800006
J	Alabort-i-Medina, J; Zafeiriou, S				Alabort-i-Medina, Joan; Zafeiriou, Stefanos			A Unified Framework for Compositional Fitting of Active Appearance Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Active appearance models; Non-linear optimization; Compositional gradient descent; Bayesian inference; Asymmetric and bidirectional composition; Schur complement; Wiberg algorithm		Active appearance models (AAMs) are one of the most popular and well-established techniques for modeling deformable objects in computer vision. In this paper, we study the problem of fitting AAMs using compositional gradient descent (CGD) algorithms. We present a unified and complete view of these algorithms and classify them with respect to three main characteristics: (i) cost function; (ii) type of composition; and (iii) optimization method. Furthermore, we extend the previous view by: (a) proposing a novel Bayesian cost function that can be interpreted as a general probabilistic formulation of the well-known project-out loss; (b) introducing two new types of composition, asymmetric and bidirectional, that combine the gradients of both image and appearance model to derive better convergent and more robust CGD algorithms; and (c) providing new valuable insights into existent CGD algorithms by reinterpreting them as direct applications of the Schur complement and the Wiberg method. Finally, in order to encourage open research and facilitate future comparisons with our work, we make the implementation of the algorithms studied in this paper publicly available as part of the Menpo Project (http://www.menpo.org).	[Alabort-i-Medina, Joan; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England	Imperial College London	Alabort-i-Medina, J (corresponding author), Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England.	ja310@imperial.ac.uk; s.zafeiriou@imperial.ac.uk			DTA studentship from Imperial College London; Qualcomm Innovation Fellowship; EPSRC project Adaptive Facial Deformable Models for Tracking (ADAManT) [EP/L026813/1]; EPSRC [EP/J017787/1, EP/J013501/1, EP/H016988/1, EP/H001379/1, EP/N007743/1, EP/L026813/1, EP/N010868/1] Funding Source: UKRI; Engineering and Physical Sciences Research Council [EP/J013501/1, EP/J017787/1, EP/N007743/1, EP/L026813/1, EP/H001379/1, EP/H016988/1, EP/N010868/1] Funding Source: researchfish	DTA studentship from Imperial College London; Qualcomm Innovation Fellowship; EPSRC project Adaptive Facial Deformable Models for Tracking (ADAManT)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The work of Joan Alabort-i-Medina is funded by a DTA studentship from Imperial College London and by the Qualcomm Innovation Fellowship. The work of S. Zafeiriou has been partly funded by the EPSRC project Adaptive Facial Deformable Models for Tracking (ADAManT), EP/L026813/1.	Alabort-i-Medina J, 2014, IEEE C COMP VIS PATT; Alabort-i-Medina J, 2015, IEEE C COMP VIS PATT; Alabort-i-Medina J, 2014, ACM INT C MULT ACMM; Amberg B., 2009, IEEE C COMP VIS PATT; Antonakos E., 2014, IEEE T IMAGE PROCESS; Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; Autheserre J.B., 2009, IEEE INT C AC SPEECH; Bach Francis R, 2005, TECHNICAL REPORT; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Batur AU, 2005, IEEE T IMAGE PROCESS, V14, P1707, DOI 10.1109/TIP.2005.854473; Belhumeur P. N., 2011, C COMP VIS PATT REC; Benhimane Selim, 2004, IEEE INT C INT ROB S; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Bradski G., 2000, DOBBS J SOFTWARE TOO, V3; Cootes T. F., 2001, IEEE C COMP VIS PATT; Cootes T. F., 2004, TECHNICAL REPORT; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; De la Torre F, 2012, IEEE T PATTERN ANAL, V34, P1041, DOI 10.1109/TPAMI.2011.184; Donner R, 2006, IEEE T PATTERN ANAL, V28, P1690, DOI 10.1109/TPAMI.2006.206; Gross R, 2005, IMAGE VISION COMPUT, V23, P1080, DOI 10.1016/j.imavis.2005.07.009; Hou X., 2001, IEEE C COMP VIS PATT; Kossaifi J., 2014, IEEE INT C IM PROC I; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Li P, 2012, IEEE T PATTERN ANAL, V34, P144, DOI 10.1109/TPAMI.2011.104; Liu XM, 2009, IEEE T PATTERN ANAL, V31, P1941, DOI 10.1109/TPAMI.2008.238; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Lucey S, 2013, IEEE T PATTERN ANAL, V35, P1383, DOI 10.1109/TPAMI.2012.220; Malis E., 2004, INT C ROB AUT ICRA; Martins P., 2010, BRIT MACH VIS C BMVC; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Megret R., 2008, EUR C COMP VIS ECCV; Megret R, 2010, IEEE T IMAGE PROCESS, V19, P2369, DOI 10.1109/TIP.2010.2048406; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; Munoz E, 2015, INT J COMPUT VISION, V112, P354, DOI 10.1007/s11263-014-0769-6; Nicolaou M. A., 2014, MACHINE LEARNING KNO; Okatani T, 2007, INT J COMPUT VISION, V72, P329, DOI 10.1007/s11263-006-9785-5; Papandreou G, 2008, PROC CVPR IEEE, P1539; Roweis S, 1998, ADV NEUR IN, V10, P626; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Saragih J, 2009, PATTERN RECOGN, V42, P2628, DOI 10.1016/j.patcog.2009.04.014; Sauer P, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.30; Strelow D., 2012, EUR C COMP VIS ECCV; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Tresadern P.A., 2010, BRIT MACH VIS C BMVC; Tzimiropoulos G., 2012, IEEE AS C COMP VIS A; Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989; Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239; Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79; van der Maaten L., 2010, IEEE C COMP VIS PATT; Vedaldi A., 2010, VLFEAT OPEN PORTABLE; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Woodbury M. A., 1950, 42 PRINC U STAT RES; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Zhu X., 2012, C COMP VIS PATT REC	56	15	15	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	1					26	64		10.1007/s11263-016-0916-3	http://dx.doi.org/10.1007/s11263-016-0916-3			39	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EI3HN	32355408	Green Submitted, Green Published, hybrid			2022-12-18	WOS:000392380900002
J	Douze, M; Revaud, J; Verbeek, J; Jegou, H; Schmid, C				Douze, Matthijs; Revaud, Jerome; Verbeek, Jakob; Jegou, Herve; Schmid, Cordelia			Circulant Temporal Encoding for Video Retrieval and Temporal Alignment	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video retrieval; Video synchronization; Fourier transform		We address the problem of specific video event retrieval. Given a query video of a specific event, e.g., a concert of Madonna, the goal is to retrieve other videos of the same event that temporally overlap with the query. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to efficiently compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. The descriptors can be compressed in the frequency domain with a product quantizer adapted to complex numbers. In this case, video retrieval is performed without decompressing the descriptors. We also consider the temporal alignment of a set of videos. We exploit the matching confidence and an estimate of the temporal offset computed for all pairs of videos by our retrieval approach. Our robust algorithm aligns the videos on a global timeline by maximizing the set of temporally consistent matches. The global temporal alignment enables synchronous playback of the videos of a given scene.	[Douze, Matthijs; Revaud, Jerome; Verbeek, Jakob; Schmid, Cordelia] INRIA Grenoble, Montbonnot St Martin, France; [Douze, Matthijs; Jegou, Herve] Facebook Artificial Intelligence Res, Paris, France; [Revaud, Jerome] Xerox Res Ctr Europe, Meylan, France; [Jegou, Herve] INRIA Rennes, Rennes, France	Facebook Inc; Xerox	Douze, M (corresponding author), INRIA Grenoble, Montbonnot St Martin, France.; Douze, M (corresponding author), Facebook Artificial Intelligence Res, Paris, France.	matthijs@fb.com; jerome.revaud@xrce.xerox.com; jakob.verbeek@inria.fr; rvj@fb.com; cordelia.schmid@inria.fr			European integrated project AXES; MSR/INRIA joint project; ERC advanced grant ALLEGRO	European integrated project AXES; MSR/INRIA joint project; ERC advanced grant ALLEGRO	We are grateful to the team members who participated as cameramen or actors in the shooting of the Climbing dataset. This work was supported by the European integrated project AXES, the MSR/INRIA joint project and the ERC advanced grant ALLEGRO.	Arandjelovic R, 2012, CVPR; Ballan L., 2010, ACM SIGGRAPH; Bishop C. M., 2006, J ELECT IMAG, V16, P140; Bolme D. S., 2010, CVPR; BROWN LG, 1992, COMPUT SURV, V24, P325, DOI 10.1145/146370.146374; Caspi Y, 2002, IEEE T PATTERN ANAL, V24, P1409, DOI 10.1109/TPAMI.2002.1046148; Chu W.-S., 2012, ECCV; Douze M., 2010, ECCV; Dubout C., 2012, ECCV; Evangelidis GD, 2013, IEEE T PATTERN ANAL, V35, P2371, DOI 10.1109/TPAMI.2013.56; Franco JS, 2009, IEEE T PATTERN ANAL, V31, P414, DOI 10.1109/TPAMI.2008.104; Hasler Nils, 2009, CVPR; Henriques J. F., 2012, ECCV; Henriques J. F., 2013, ICCV; Hoai M., 2012, AISTATS; Jain M., 2012, ICMR; Jegou H., 2012, ECCV; Jegou H., 2008, ECCV; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jiang H., 2012, ECCV; Kalker T., 1999, SPIE C SEC WAT MULT; Karpenko A, 2011, IEEE T PATTERN ANAL, V33, P618, DOI 10.1109/TPAMI.2010.118; Kennedy L., 2009, WWW; Kuehne H., 2011, ICCV; Kumar BV., 2005, CORRELATION PATTERN, DOI 10.1017/CBO9780511541087; LAWTO J, 2007, CIVR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mallat S, 2008, WAVELET TOUR SINGAL; Nowak E., 2006, ECCV; Oneata D., 2013, ICCV; Over P., 2014, P TRECVID 2014; Petit B, 2010, INT J DIGIT MULTIMED, V2010, DOI 10.1155/2010/247108; Revaud J., 2013, CVPR; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Smeaton A. F., 2006, MIR; Song J., 2011, P 19 ACM INT C MULT; Soomro K., 2012, COMPUT SCI; Tuytelaars T., 2004, CVPR; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang O, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601208; Wu Xiao, 2007, PRACTICAL ELIMINATIO; Xiong C, 2012, INT WORKSH MULT DAT; Yeh M.C., 2009, CIVR	43	15	15	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	3			SI		291	306		10.1007/s11263-015-0875-0	http://dx.doi.org/10.1007/s11263-015-0875-0			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FE		Green Submitted			2022-12-18	WOS:000380270000006
J	Janan, F; Brady, M				Janan, Faraz; Brady, Michael			Shape Description and Matching Using Integral Invariants on Eccentricity Transformed Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape matching; Shape correspondence; Integral invariant; Eccentricity transform; Fast Marching Algorithm; CAD; Breast cancer	OBJECT RECOGNITION; REPRESENTATION; CLASSIFICATION; MULTISCALE; RETRIEVAL; GEOMETRY; ALGORITHMS; SIGNATURES; CURVATURE; CURVES	Matching occluded and noisy shapes is a problem frequently encountered in medical image analysis and more generally in computer vision. To keep track of changes inside the breast, for example, it is important for a computer aided detection system to establish correspondences between regions of interest. Shape transformations, computed both with integral invariants (II) and with geodesic distance, yield signatures that are invariant to isometric deformations, such as bending and articulations. Integral invariants describe the boundaries of planar shapes. However, they provide no information about where a particular feature lies on the boundary with regard to the overall shape structure. Conversely, eccentricity transforms (Ecc) can match shapes by signatures of geodesic distance histograms based on information from inside the shape; but they ignore the boundary information. We describe a method that combines the boundary signature of a shape obtained from II and structural information from the Ecc to yield results that improve on them separately.	[Janan, Faraz] Univ Oxford, Biomed Engn, Oxford, England; [Janan, Faraz] Matakina UK Ltd, Ely, Cambs, England; [Brady, Michael] Univ Oxford, Oxford, England	University of Oxford; University of Oxford	Janan, F (corresponding author), Univ Oxford, Biomed Engn, Oxford, England.	faraz.janan@some.ox.ac.uk; mike.brady@oncology.ox.ac.uk	Janan, Faraz/AAF-4538-2019	Janan, Faraz/0000-0001-7479-6708				Alferez R, 1999, IEEE T PATTERN ANAL, V21, P505, DOI 10.1109/34.771318; Amanatiadis A, 2011, IET IMAGE PROCESS, V5, P493, DOI 10.1049/iet-ipr.2009.0246; Arrebola F, 2005, PATTERN RECOGN, V38, P1596, DOI 10.1016/j.patcog.2005.03.006; Arun K. S., 2011, INT J COMPUTATIONAL, V15, P41; Bauer M., 2011, ARXIV11074257; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Belongie S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P454, DOI 10.1109/ICCV.2001.937552; Ben Hamza A, 2006, IEEE T IMAGE PROCESS, V15, P2249, DOI 10.1109/TIP.2006.875250; BENGTSSON A, 1991, IEEE T PATTERN ANAL, V13, P85, DOI 10.1109/34.67634; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING; Bo Yu, 2010, 2010 IEEE International Conference on Intelligent Computing and Intelligent Systems (ICIS 2010), P669, DOI 10.1109/ICICISYS.2010.5658385; Boue M, 1999, SIAM J NUMER ANAL, V36, P667, DOI 10.1137/S0036142997323521; Brandt RD, 1996, PATTERN RECOGN LETT, V17, P1001, DOI 10.1016/0167-8655(96)00062-1; Bronstein AM, 2008, INT J COMPUT VISION, V78, P67, DOI 10.1007/s11263-007-0078-4; Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103; Bruckstein AM, 1997, IMAGE VISION COMPUT, V15, P335, DOI 10.1016/S0262-8856(96)01140-7; Byung-Woo Hong, 2003, International Conference on Visual Information Engineering (VIE 2003) (IEE Conf. Publ.No.495), P157, DOI 10.1049/cp:20030511; Calabi E, 1998, INT J COMPUT VISION, V26, P107, DOI 10.1023/A:1007992709392; Cao WG, 2013, SCI CHINA INFORM SCI, V56, DOI 10.1007/s11432-011-4453-y; Chen YW, 2009, PATTERN RECOGN LETT, V30, P799, DOI 10.1016/j.patrec.2008.04.015; Chetverikov D, 1999, LECT NOTES COMPUT SC, V1689, P367; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; COHIGNAC T, 1994, INT C PATT RECOG, P164, DOI 10.1109/ICPR.1994.576250; COLE JB, 1991, PATTERN RECOGN LETT, V12, P519, DOI 10.1016/0167-8655(91)90091-Y; Davidovic T., 2010, P INT S OP RES TAR S, P389; Davies Edward R., 2004, MACHINE VISION THEOR; DAVIS LS, 1977, IEEE T COMPUT, V26, P236, DOI 10.1109/TC.1977.1674812; Dijkstra E. W., 1976, DISCIPLINE PROGRAMMI; Dijkstra EW, 1959, NUMER MATH, V1, P269, DOI 10.1007/BF01386390; Dijkstra EW, 1968, COOPERATING SEQUENTI; Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110; Duci A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P656; Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902; Fidler T., 2007, INVERSE PROBLEMS INT; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; Forsyth D., 1990, PROJECTIVELY INVARIA; Frenkel M, 2003, LECT NOTES COMPUT SC, V2683, P35; Gdalyahu Y, 1999, IEEE T PATTERN ANAL, V21, P1312, DOI 10.1109/34.817410; Gorelick L, 2006, IEEE T PATTERN ANAL, V28, P1991, DOI 10.1109/TPAMI.2006.253; Gu YH, 2000, PATTERN RECOGN, V33, P1411, DOI 10.1016/S0031-3203(99)00131-4; Hadley G., 1964, NONLINEAR DYNAMIC PR; Hann CE, 2002, ACTA APPL MATH, V74, P177, DOI 10.1023/A:1020617228313; HELGASON S, 1984, GROUPS GEOMETRIC ANA; Helmsen J, 1996, P SOC PHOTO-OPT INS, V2726, P253, DOI 10.1117/12.240959; Highnam R, 2010, LECT NOTES COMPUT SC, V6136, P342, DOI 10.1007/978-3-642-13666-5_46; Hong B. W., 2004, IMAGE SEGMENTATION S; Huang CL, 1998, IMAGE VISION COMPUT, V16, P149, DOI 10.1016/S0262-8856(97)00062-0; Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925; Huang Si-ming, 2011, 2011 IEEE International Conference on Computer Science and Automation Engineering (CSAE), P16, DOI 10.1109/CSAE.2011.5952625; Huang ZH, 1996, IEEE T IMAGE PROCESS, V5, P1473, DOI 10.1109/83.536895; Ion Adrian, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563032; Ion A., 2007, 31 ANN WORKSH AUSTR, P97; Ion A, 2011, COMPUT VIS IMAGE UND, V115, P817, DOI 10.1016/j.cviu.2011.02.006; Janan Faraz, 2012, Breast Imaging. Proceedings 11th International Workshop, IWDM 2012, P173, DOI 10.1007/978-3-642-31271-7_23; Jeffreys M., 2010, DIGITAL MAMMOGRAPHY; Kanatani Kenichi, 1990, GROUP THEORETICAL ME, P4; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kimmel R, 2001, J MATH IMAGING VIS, V14, P237, DOI 10.1023/A:1011234012449; KIMMEL R, 2004, NUMERICAL GEOMETRY I; Kimmel R, 1996, FAST MARCHING METHOD, P8; Kliot M., 1998, COMP VIS ECCV 98; Lasenby J, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P313, DOI 10.1109/ICIP.1996.560819; Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850; Latto A., 1984, Proceedings of the Workshop on Computer Vision: Representation and Control, P183; Lenz R., 1990, GROUP THEORETICAL ME; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Li S. Z., 1999, PROGR NEURAL NETWORK, V6, P203; LI SZ, 1992, PATTERN RECOGN, V25, P583, DOI 10.1016/0031-3203(92)90075-T; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; Maciel J, 2003, IEEE T PATTERN ANAL, V25, P187, DOI 10.1109/TPAMI.2003.1177151; Manay S., 2004, INTEGRAL INVARIANT S; Manay S, 2006, IEEE T PATTERN ANAL, V28, P1602, DOI 10.1109/TPAMI.2006.208; Mansoory M. S., 2011, MODERN APPL SCI, V5; MARDIA KV, 1989, ADV APPL PROBAB, V21, P742, DOI 10.2307/1427764; Mateus D., 2008, PROC CVPR IEEE, P1, DOI DOI 10.1109/CVPR.2008.4587538; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591; Mokhtarian F, 1998, SER SOFTW ENGN KNOWL, V8, P51; MOKHTARIAN F, 1986, IEEE T PATTERN ANAL, V8, P34, DOI 10.1109/TPAMI.1986.4767750; Mumford D, 1991, MATH THEORIES SHAPE; Nasreddine K, 2009, IEEE IMAGE PROC, P405, DOI 10.1109/ICIP.2009.5414454; NIELSEN L, 1991, CVGIP-IMAG UNDERSTAN, V54, P145, DOI 10.1016/1049-9660(91)90079-5; Olver P. J., 1995, EQUIVALENCE INVARIAN, DOI DOI 10.1017/CBO9780511609565; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Ozcan E, 1997, PATTERN RECOGN LETT, V18, P987, DOI 10.1016/S0167-8655(97)00123-2; Petrakis EGM, 2002, IEEE T PATTERN ANAL, V24, P1501, DOI 10.1109/TPAMI.2002.1046166; Peyre G, 2011, COMPUT SCI ENG, V13, P68, DOI 10.1109/MCSE.2011.90; Peyre G, 2009, FOUND TRENDS COMPUT, V5, DOI 10.1561/0600000029; Pottmann H, 2009, COMPUT AIDED GEOM D, V26, P37, DOI 10.1016/j.cagd.2008.01.002; Qu ZG, 2011, OPT ENG, V50, DOI 10.1117/1.3647520; Rafajlowicz E, 2007, 2007 INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL SYSTEMS, P69, DOI 10.1109/NDS.2007.4509548; Reiss T.H., 1993, RECOGNIZING PLANAR O; Reuter M., 2005, P ACM S SOL PHYS MOD, P101, DOI [10.1145/1060244.1060256, DOI 10.1145/1060244.1060256]; Rezai-Rad G., 2006, INF COMM TECHN 2006, V1, P1103; Rosin PL, 2011, LECT NOTES COMPUT SC, V6854, P253, DOI 10.1007/978-3-642-23672-3_31; ROTHWELL CA, 1995, INT J COMPUT VISION, V16, P57, DOI 10.1007/BF01428193; ROTHWELL CA, 1992, LECT NOTES COMPUT SC, V588, P757; Ruggeri MR, 2010, INT J COMPUT VISION, V89, P248, DOI 10.1007/s11263-009-0250-0; Rusinol M., 2007, PATTERN RECOGNITION; Sampat MP, 2005, HANDBOOK OF IMAGE AND VIDEO PROCESSING, 2ND EDITION, P1195, DOI 10.1016/B978-012119792-6/50130-3; Sato J, 1997, IMAGE VISION COMPUT, V15, P627, DOI 10.1016/S0262-8856(97)00011-5; Sato J., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P915, DOI 10.1109/ICPR.1996.546157; Sebastian T. B., 2001, VISUAL FORM 2001; Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951; Sethian J. A., 1999, LEVEL SET METHODS FA; Sharma A, 2010, P NORDIA WORKSH CVPR, P29; Sharon E, 2006, INT J COMPUT VISION, V70, P55, DOI 10.1007/s11263-006-6121-z; Shashua A, 1996, IEEE T PATTERN ANAL, V18, P873, DOI 10.1109/34.537342; Shi JL, 2013, APPL SOFT COMPUT, V13, P3060, DOI 10.1016/j.asoc.2012.04.029; Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703; Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710; Sniedovich M., 2010, DYNAMIC PROGRAMMING, DOI 10.1201/EBK0824740993; Sonka M., 2014, IMAGE PROCESSING ANA; Squire DM, 2000, COMPUT VIS IMAGE UND, V77, P284, DOI 10.1006/cviu.2000.0809; Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609; Taubin G., 1991, OBJECT RECOGNITION B; Teodorovic D., 2011, ACM T COMPUT LOG, V1529, P3785; Thomas T.Y., 1934, DIFFERENTIAL INVARIA; Tian J., 2011, T COMPUTATIONAL COLL; Tian J, 2011, EXPERT SYST APPL, V38, P12514, DOI 10.1016/j.eswa.2011.04.037; Torresani L., 2008, COMP VIS ECCV 2008; Trucco E, 1995, AI COMMUN, V8, P193; Tsai YHR, 2003, SIAM J NUMER ANAL, V41, P673, DOI 10.1137/S0036142901396533; TSITSIKLIS JN, 1995, IEEE T AUTOMAT CONTR, V40, P1528, DOI 10.1109/9.412624; Und Bildverarbeitung A.M., 2009, ECCENTRICITY TRANSFO; Van Gool L., 1996, COMP VIS ECCV 96; van Kaick O, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P271, DOI 10.1109/PG.2007.56; van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x; Veltkamp R. C., 2001, STATE ART SHAPE MATC; Veltkamp RC, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P188, DOI 10.1109/SMA.2001.923389; Wang S, 2007, IEEE T PATTERN ANAL, V29, P1209, DOI 10.1109/TPAMI.2007.1050; Wang Y, 2004, IEEE IMAGE PROC, P409; Wang Y, 2004, IMAGE VISION COMPUT, V22, P269, DOI 10.1016/j.imavis.2003.10.003; WEISS I, 1993, IEEE T PATTERN ANAL, V15, P943, DOI 10.1109/34.232081; White R., 2004, MATCHING SHAPES USIN; Xingfang Y., 2010, IEEE 2010 2 INT C SI, V2; Xu CF, 2010, PATTERN RECOGN LETT, V31, P1759, DOI 10.1016/j.patrec.2009.11.018; Xu CJ, 2009, IEEE T PATTERN ANAL, V31, P180, DOI 10.1109/TPAMI.2008.199; Xu JN, 2008, IEEE IMAGE PROC, P2596, DOI 10.1109/ICIP.2008.4712325; Xu SH, 2006, ISDA 2006: SIXTH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS DESIGN AND APPLICATIONS, VOL 2, P720; Xu Y., 2010, COMP VIS ACCV 2009; Yang Y.-L., 2006, S GEOM PROC, P223; Yue W, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P769, DOI 10.1109/ICIP.2001.958607; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949; Zeng Jun, 2011, Computer Engineering and Applications, V47, P194, DOI 10.3778/j.issn.1002-8331.2011.15.052; Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008; Zhang SJ, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P343, DOI 10.1109/ICME.2000.869611; Zhao HK, 2005, MATH COMPUT, V74, P603; Zhou D., 2011, IEEE 2011 4 INT C BI, V1; ZISSERMAN A, 1995, ARTIF INTELL, V78, P239, DOI 10.1016/0004-3702(95)00023-2	150	15	17	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2015	113	2					92	112		10.1007/s11263-014-0773-x	http://dx.doi.org/10.1007/s11263-014-0773-x			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CI1FE		Green Accepted			2022-12-18	WOS:000354487300002
J	Torralba, A; Freeman, WT				Torralba, Antonio; Freeman, William T.			Accidental Pinhole and Pinspeck Cameras Revealing the Scene Outside the Picture	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Accidental cameras; Pinhole; Anti pinhole		We identify and study two types of "accidental" images that can be formed in scenes. The first is an accidental pinhole camera image. The second class of accidental images are "inverse" pinhole camera images, formed by subtracting an image with a small occluder present from a reference image without the occluder. Both types of accidental cameras happen in a variety of different situations. For example, an indoor scene illuminated by natural light, a street with a person walking under the shadow of a building, etc. The images produced by accidental cameras are often mistaken for shadows or interreflections. However, accidental images can reveal information about the scene outside the image, the lighting conditions, or the aperture by which light enters the scene.	[Torralba, Antonio; Freeman, William T.] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Torralba, A (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.	torralba@csail.mit.edu; billf@mit.edu			NSF [0747120, CGV 1111415, CGV 0964004]; ONR MURI [N000141010933]	NSF(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research)	Funding for this work was provided by NSF Career award 0747120 and ONR MURI N000141010933 to A. Torralba, and NSF CGV 1111415 and NSF CGV 0964004 to W.T.Freeman. We thank Tomasz Malisiewicz for suggesting the configuration of Fig. 30 and Agata Lapedriza for comments on the manuscript.	ADELSON EH, 1992, IEEE T PATTERN ANAL, V14, P99, DOI 10.1109/34.121783; Baker S, 1999, INT J COMPUT VISION, V35, P175, DOI 10.1023/A:1008128724364; Barron JT, 2011, PROC CVPR IEEE; Barrow H. G., 1978, 157 AI CTR SRI INT; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; COHEN AL, 1982, OPT ACTA, V29, P63, DOI 10.1080/713820733; Criminisi A, 2000, INT J COMPUT VISION, V40, P123, DOI 10.1023/A:1026598000963; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; Hasinoff SW, 2011, IEEE I CONF COMP VIS, P185, DOI 10.1109/ICCV.2011.6126241; KRISHNAN D, 2011, IEEE C COMP VIS PATT; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; LEVIN A, 2007, P SIGGRAPH ACM T GRA; Liu C, 2008, IEEE T PATTERN ANAL, V30, P299, DOI [10.1109/TPAMI.2007.1176, 10.1109/TPAMI.20071176]; Minnaert Marcel, 1954, LIGHT COLOUR OPEN AI; MORELL A, 1995, A CAMERA IN A ROOM; NAYAR S, 2006, P SIGGRAPH ACM T GRA; Nishino K, 2006, INT J COMPUT VISION, V70, P23, DOI 10.1007/s11263-006-6274-9; O'Brien JF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077345; Russell BC, 2009, PROC CVPR IEEE, P2703; Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185; Torralba A, 2012, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2012.6247698; Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606; YOUNG AT, 1974, ICARUS, V21, P262, DOI 10.1016/0019-1035(74)90042-6; ZERMENO A, 1978, Patent No. 4085324; Zomet A., 2006, LENSLESS IMAGING CON	25	15	16	3	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2014	110	2			SI		92	112		10.1007/s11263-014-0697-5	http://dx.doi.org/10.1007/s11263-014-0697-5			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AY5UE		Green Published, hybrid			2022-12-18	WOS:000347636400001
J	Wu, D; Wetzstein, G; Barsi, C; Willwacher, T; Dai, QH; Raskar, R				Wu, Di; Wetzstein, Gordon; Barsi, Christopher; Willwacher, Thomas; Dai, Qionghai; Raskar, Ramesh			Ultra-fast Lensless Computational Imaging through 5D Frequency Analysis of Time-resolved Light Transport	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computational photography; Light transport; Frequency analysis; Lensless imaging		Light transport has been analyzed extensively, in both the primal domain and the frequency domain. Frequency analyses often provide intuition regarding effects introduced by light propagation and interaction with optical elements; such analyses encourage optimal designs of computational cameras that efficiently capture tailored visual information. However, previous analyses have relied on instantaneous propagation of light, so that the measurement of the time dynamics of light-scene interaction, and any resulting information transfer, is precluded. In this paper, we relax the common assumption that the speed of light is infinite. We analyze free space light propagation in the frequency domain considering spatial, temporal, and angular light variation. Using this analysis, we derive analytic expressions for information transfer between these dimensions and show how this transfer can be exploited for designing a new lensless imaging system. With our frequency analysis, we also derive performance bounds for the proposed computational camera architecture and provide a mathematical framework that will also be useful for future ultra-fast computational imaging systems.	[Wu, Di; Wetzstein, Gordon; Barsi, Christopher; Raskar, Ramesh] MIT Media Lab, Cambridge, MA 02139 USA; [Wu, Di; Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China; [Willwacher, Thomas] Harvard Univ, Dept Math, Cambridge, MA 02138 USA; [Wu, Di] Tsinghua Univ, Grad Sch Shenzhen, Beijing 100084, Peoples R China	Massachusetts Institute of Technology (MIT); Tsinghua University; Harvard University; Tsinghua University; University Town of Shenzhen; Tsinghua Shenzhen International Graduate School	Wetzstein, G (corresponding author), MIT Media Lab, Cambridge, MA 02139 USA.	gordonw@media.mit.edu	Dai, Qionghai/ABD-5298-2021	Dai, Qionghai/0000-0001-7043-3061; Wetzstein, Gordon/0000-0002-9243-6885	Media Lab Consortium; NSERC; China National Basic Research Project [2010CB731800]; NSFC [61120106003, 61035002]	Media Lab Consortium; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); China National Basic Research Project; NSFC(National Natural Science Foundation of China (NSFC))	The work of the MIT affiliated coauthors was funded by the Media Lab Consortium Members. Gordon Wetzstein was supported by an NSERC Postdoctoral Fellowship. Tsinghua University affiliated coauthors were supported by China National Basic Research Project (No. 2010CB731800) and the Key Project of NSFC (Nos. 61120106003 and 61035002).	Andersen G, 2005, OPT LETT, V30, P2976, DOI 10.1364/OL.30.002976; Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932; Chandrasekaran V, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P563; DOWSKI ER, 1995, APPL OPTICS, V34, P1859, DOI 10.1364/AO.34.001859; Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320; Gill PR, 2011, OPT LETT, V36, P2949, DOI 10.1364/OL.36.002949; Hamamatsu, 2012, HAM STREAK CAM; Herman G. T., 1995, Real-Time Imaging, V1, P3, DOI 10.1006/rtim.1995.1002; Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929; Isikman S. O., 2011, P NATL ACAD SCI; Kajiya J.T., 1986, SIGGRAPH, P143, DOI [DOI 10.1145/15922.15902, 10.1145/15886.15902, DOI 10.1145/15886.15902]; Kak A. C., 2001, PRINCIPLES COMPUTERI; Koppal SJ, 2011, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.2011.5995338; Levin A., 2010, P IEEE CVPR, P1; Levin A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531403; Naik N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024205; Ng R, 2005, ACM T GRAPHIC, V24, P735, DOI 10.1145/1073204.1073256; Pandharkar R, 2011, PROC CVPR IEEE, P265, DOI 10.1109/CVPR.2011.5995465; Raskar R, 2008, MIT TECH REP, P7; Saleh B., 2011, INTRO SUBSURFACE IMA; Smith Adam, 2008, UCSCSOE0826; Veeraraghavan A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239520; Velten A, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1747; Wu D, 2012, IEEE INT CONF COMMUN, P43, DOI 10.1109/ICCCW.2012.6316472; Zomet A., 2006, COMP VIS PATT REC 20, V1, P339	25	15	15	2	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2014	110	2			SI		128	140		10.1007/s11263-013-0686-0	http://dx.doi.org/10.1007/s11263-013-0686-0			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AY5UE		Green Submitted			2022-12-18	WOS:000347636400004
J	Verdie, Y; Lafarge, F				Verdie, Yannick; Lafarge, Florent			Detecting parametric objects in large scenes by Monte Carlo sampling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stochastic modeling; Monte Carlo sampling; Object detection; Large scenes; Energy minimization; Point processes; Markov random fields	MARKED POINT PROCESS; ENERGY MINIMIZATION; IMAGE SEGMENTATION; EXTRACTION	Point processes constitute a natural extension of Markov random fields (MRF), designed to handle parametric objects. They have shown efficiency and competitiveness for tackling object extraction problems in vision. Simulating these stochastic models is however a difficult task. The performances of the existing samplers are limited in terms of computation time and convergence stability, especially on large scenes. We propose a new sampling procedure based on a Monte Carlo formalism. Our algorithm exploits the Markovian property of point processes to perform the sampling in parallel. This procedure is embedded into a data-driven mechanism so that the points are distributed in the scene in function of spatial information extracted from the input data. The performances of the sampler are analyzed through a set of experiments on various object detection problems from large scenes, including comparisons to the existing algorithms. The sampler is also tested as optimization algorithm for MRF-based labeling problems.	[Verdie, Yannick; Lafarge, Florent] INRIA, Sophia Antipolis, France	Inria	Lafarge, F (corresponding author), INRIA, Sophia Antipolis, France.	Florent.Lafarge@inria.fr			European Research Council (ERC) [257474]	European Research Council (ERC)(European Research Council (ERC))	This work was partially funded by the European Research Council (ERC Starting Grant "Robust Geometry Processing", Grant agreement 257474). The authors thank A. Lehmussola, V. Lempitsky, H. Bischof, R. Ehrich, the French Mapping Agency (IGN), the Tour du Valat, and the BRGM for providing the datasets, as well as the reviewers for their valuable comments.	Baddeley A.J., 1993, J APPL STAT, V20, P231, DOI [10.1080/02664769300000065, DOI 10.1080/02664769300000065]; Benchmark, 2013, DATASETS RESULTS EVA; BESAG J, 1986, J R STAT SOC B, V48, P259; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Byrd J., 2010, IEEE INT S PAR DISTR; Chai D., 2013, RECOVERING LINE NETW; Chai D., 2012, INT SOC PHOT REM SEN; Descombes X, 2011, STOCHASTIC GEOMETRY; Descombes X, 2009, J MATH IMAGING VIS, V33, P347, DOI 10.1007/s10851-008-0117-y; Earl DJ, 2005, PHYS CHEM CHEM PHYS, V7, P3910, DOI 10.1039/b509983h; Ge W., 2009, COMPUTER VISION PATT; Gonzalez J., 2011, P 14 INT C ART INT S, V15, P324; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; GRENANDER U, 1994, J R STAT SOC B, V56, P549; Han F, 2004, IEEE T PATTERN ANAL, V26, P1138, DOI 10.1109/TPAMI.2004.70; Harkness M., 2000, BRIT MACH VIS C BRIS; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Lacoste C, 2005, IEEE T PATTERN ANAL, V27, P1568, DOI 10.1109/TPAMI.2005.206; Lafarge F, 2012, INT J COMPUT VISION, V99, P69, DOI 10.1007/s11263-012-0517-8; Lafarge F, 2010, IEEE T PATTERN ANAL, V32, P1597, DOI 10.1109/TPAMI.2009.152; Lehmussola A, 2007, IEEE T MED IMAGING, V26, P1010, DOI 10.1109/TMI.2007.896925; Lempitsky V., 2010, C NEUR INF PROC SYST; Li S. Z., 2001, COMP SCI W; Liu JS., 2001, MONTE CARLO STRATEGI, DOI DOI 10.1007/978-0-387-76371-2; Mallet C, 2010, IEEE T IMAGE PROCESS, V19, P3204, DOI 10.1109/TIP.2010.2052825; Nguyen H.-G., 2010, EUR C COMP VIS HER G; Ortner M, 2008, IEEE T PATTERN ANAL, V30, P105, DOI 10.1109/TPAMI.2007.1159; Rochery M., 2006, INT J COMPUT VISION, V69, P335; SALAMON P, 2002, SIAM MONOGRAPHS MATH; Srivastava A, 2002, J STAT PLAN INFER, V103, P15, DOI 10.1016/S0378-3758(01)00195-1; Stoica RS, 2007, J R STAT SOC C-APPL, V56, P459, DOI 10.1111/j.1467-9876.2007.00587.x; Sun K., 2007, ENERGY MINIMIZATION; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239; Utasi A., 2011, C COMP VIS PATT REC; van Lieshout MNM, 2008, IEEE T PATTERN ANAL, V30, P1308, DOI 10.1109/TPAMI.2008.45; Verdie Y., 2012, EUR C COMP VIS FIR I; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; Zhu SC, 2005, INT J COMPUT VISION, V62, P121, DOI 10.1007/s11263-005-4638-1	39	15	18	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2014	106	1					57	75		10.1007/s11263-013-0641-0	http://dx.doi.org/10.1007/s11263-013-0641-0			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	288QM		Green Submitted			2022-12-18	WOS:000329626800003
J	Lian, ZH; Godil, A; Xiao, JG				Lian, Zhouhui; Godil, Afzal; Xiao, Jianguo			Feature-Preserved 3D Canonical Form	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Canonical form; Multidimensional scaling; 3D shape retrieval; Non-rigid	RECOGNITION; RETRIEVAL	Measuring the dissimilarity between non-rigid objects is a challenging problem in 3D shape retrieval. One potential solution is to construct the models' 3D canonical forms (i.e., isometry-invariant representations in 3D Euclidean space) on which any rigid shape matching algorithm can be applied. However, existing methods, which are typically based on embedding procedures, result in greatly distorted canonical forms, and thus could not provide satisfactory performance to distinguish non-rigid models. In this paper, we present a feature-preserved canonical form for non-rigid 3D watertight meshes. The basic idea is to naturally deform original models against corresponding initial canonical forms calculated by Multidimensional Scaling (MDS). Specifically, objects are first segmented into near-rigid subparts, and then, through properly-designed rotations and translations, original subparts are transformed into poses that correspond well with their positions and directions on MDS canonical forms. Final results are obtained by solving nonlinear minimization problems for optimal alignments and smoothing boundaries between subparts. Experiments on two non-rigid 3D shape benchmarks not only clearly verify the advantages of our algorithm against existing approaches, but also demonstrate that, with the help of the proposed canonical form, we can obtain significantly better retrieval accuracy compared to the state of the art.	[Lian, Zhouhui; Xiao, Jianguo] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China; [Godil, Afzal] NIST, Gaithersburg, MD 20899 USA	Peking University; National Institute of Standards & Technology (NIST) - USA	Lian, ZH (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.	lianzhouhui@yahoo.com.cn; godil@nist.gov; xiaojianguo@pku.edu.cn	Xiao, Jian/GYU-4351-2022		China Post-doctoral Science Foundation [2012M510274]; SIMA program; Shape Metrology IMS	China Post-doctoral Science Foundation(China Postdoctoral Science Foundation); SIMA program; Shape Metrology IMS	This work has been supported by China Post-doctoral Science Foundation (Grant No.: 2012M510274), the SIMA program and the Shape Metrology IMS. We would like to thank the anonymous reviewers for their constructive comments, and Xu-Lei Wang for providing his results that have been compared in this paper.	Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859; [Anonymous], 2009, 3DOR EUR; Borg I., 1997, MODERN MULTIDIMENSIO; Bronstein AM, 2006, SIAM J SCI COMPUT, V28, P1812, DOI 10.1137/050639296; Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1; Bronstein AM, 2010, INT J COMPUT VISION, V89, P266, DOI 10.1007/s11263-009-0301-6; Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669; Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Cox M. A. A., 1994, MULTIDIMENSIONAL SCA; Davis TA, 2011, ACM T MATH SOFTWARE, V38, DOI [10.1145/2049662.2049670, 10.1145/2049662.2049663]; Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902; Faloutsos C., 1995, P 1995 ACM SIGMOD IN, P163; Funkhouser T., 2006, PROC EUROGRAPHICS S, P131; Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45; Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282; Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x; Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kazhdan M., 2003, Symposium on Geometry Processing, P156; Lai YK, 2008, SPM 2008: PROCEEDINGS OF THE ACM SOLID AND PHYSICAL MODELING SYMPOSIUM, P183; Lian ZH, 2010, IEEE IMAGE PROC, P3181, DOI 10.1109/ICIP.2010.5654226; Lian ZH, 2010, INT J COMPUT VISION, V89, P130, DOI 10.1007/s11263-009-0295-0; Liu Y., 2006, COMP VIS PATT REC 20, V2, P2025, DOI DOI 10.1109/CVPR.2006.278; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mahmoudi M, 2009, GRAPH MODELS, V71, P22, DOI 10.1016/j.gmod.2008.10.002; Mateus D, 2007, IEEE I CONF COMP VIS, P39; Memoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y; MEMOLI F., 2007, S POINT BAS GRAPH, P81, DOI DOI 10.2312/SPBG/SPBG07/081-090; Morris D., 2006, VOXELIZER FLOODFILLI; Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Ovsjanikov Maks, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P320, DOI 10.1109/ICCVW.2009.5457682; Philipp-Foliguet S, 2011, PATTERN RECOGN, V44, P588, DOI 10.1016/j.patcog.2010.09.016; Reuter M., 2005, P ACM S SOL PHYS MOD, P101, DOI [10.1145/1060244.1060256, DOI 10.1145/1060244.1060256]; Rustamov Raif M, 2007, P 5 EUR S GEOM PROC, P225, DOI DOI 10.2312/SGP/SGP07/225-233; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8; Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609; Tam GKL, 2007, IEEE T VIS COMPUT GR, V13, P470, DOI 10.1109/TVCG.2007.1011; Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0; Wang X.L., 2010, PROCDPVT, V10, P17; Wuhrer S, 2010, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2010.5540188; Zhouhui Lian, 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P116, DOI 10.1109/3DIMPVT.2011.22; Zhouhui Lian, 2010, Proceedings of the Shape Modeling International (SMI 2010), P25, DOI 10.1109/SMI.2010.20; Zunic J, 2004, IEEE T PATTERN ANAL, V26, P923, DOI 10.1109/TPAMI.2004.19	48	15	19	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					221	238		10.1007/s11263-012-0548-1	http://dx.doi.org/10.1007/s11263-012-0548-1			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO					2022-12-18	WOS:000315501800013
J	Bartoli, A; Pizarro, D; Loog, M				Bartoli, Adrien; Pizarro, Daniel; Loog, Marco			Stratified Generalized Procrustes Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape; Registration; Procrustes; Theseus; Generalized	CLOSED-FORM SOLUTION; ROTATION; PARAMETERS; MOTION; SHAPE	Generalized procrustes analysis computes the best set of transformations that relate matched shape data. In shape analysis the transformations are usually chosen as similarities, while in general statistical data analysis other types of transformation groups such as the affine group may be used. Generalized procrustes analysis has a nonlinear and nonconvex formulation. The classical approach alternates the computation of a so-called reference shape and the computation of transformations relating this reference shape to each shape datum in turn. We propose the stratified approach to generalized procrustes analysis. It first uses the affine transformation group to analyze the data and then upgrades the solution to the sought after group, whether Euclidean or similarity. We derive a convex formulation for each of these two steps, and efficient practical algorithms that gracefully handle missing data (incomplete shapes). Extensive experimental results show that our approaches perform well on simulated and real data. In particular our closed-form solution gives very accurate results for generalized procrustes analysis of Euclidean data.	[Bartoli, Adrien] Univ Auvergne, CNRS, UMR 6284, ISIT, Clermont Ferrand, France; [Pizarro, Daniel] Univ Alcala de Henares, Alcala De Henares, Spain; [Loog, Marco] Delft Univ Technol, Delft, Netherlands	Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA); Universidad de Alcala; Delft University of Technology	Bartoli, A (corresponding author), Univ Auvergne, CNRS, UMR 6284, ISIT, Clermont Ferrand, France.	adrien.bartoli@gmail.com		Pizarro, Daniel/0000-0003-0622-4884	Spanish Ministry of Science and Innovation [TIN2009-08984]	Spanish Ministry of Science and Innovation(Ministry of Science and Innovation, Spain (MICINN)Spanish Government)	Daniel Pizarro has been supported by the Spanish Ministry of Science and Innovation under project VISNU (ref. TIN2009-08984).	Aguiar P., 2008, INT C IM PROC; ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965; Bartoli A., 2008, INT C COMP VIS PATT; Bartoli A., 2010, BRIT MACH VIS C; Black M. J., 2006, CS0608 BROWN U; BOGGS PT, 1989, ACM T MATH SOFTWARE, V15, P348, DOI 10.1145/76909.76913; Bregler C., 2000, INT C COMP VIS PATT; Dryden I.L., 1998, STAT SHAPE ANAL, DOI [DOI 10.5555/1046920.1088707, 10.1002/9781119072492]; Eggert DW, 1997, MACH VISION APPL, V9, P272, DOI 10.1007/s001380050048; GORYN D, 1995, IEEE T PATTERN ANAL, V17, P1219, DOI 10.1109/34.476514; Gower J. C., 2004, PROCRUSTES PROBLEMS; GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478; Gurdjos P., 2009, INT C COMP VIS; Hartley R., 2003, MULTIPLE VIEW GEOMET; HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Jacobs DW, 2001, COMPUT VIS IMAGE UND, V82, P57, DOI 10.1006/cviu.2001.0906; KANATANI K, 1994, IEEE T PATTERN ANAL, V16, P543, DOI 10.1109/34.291441; Kanatani K., 1998, EUR C COMP VIS; Krishnan S., 2005, EUR S GEOM PROC; Mahamud S., 2001, INT C COMP VIS PATT; Meer P., 1999, INT C COMP VIS PATT; Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a; Quan L, 1996, INT J COMPUT VISION, V19, P93, DOI 10.1007/BF00131149; Ramos J. A., 1997, C DEC CONTR; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; SCHONEMANN PH, 1970, PSYCHOMETRIKA, V35, P245, DOI 10.1007/BF02291266; Slama CC., 1980, MANUAL PHOTOGRAMMETR, V4th edn; STEWART GW, 1993, SIAM REV, V35, P551, DOI 10.1137/1035134; Ten Berge JMF, 1977, PSYCHOMETRIKA, V42, P267, DOI 10.1007/BF02294053; Triggs B., 2000, P INT WORKSH VIS ALG; UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573; WALKER MW, 1991, CVGIP-IMAG UNDERSTAN, V54, P358, DOI 10.1016/1049-9660(91)90036-O; Wen G., 2005, COMP GRAPH INT; Xiao J., 2006, INT C COMP VIS PATT; Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9; Yezzi AJ, 2003, INT J COMPUT VISION, V53, P153, DOI 10.1023/A:1023048024042	37	15	16	2	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	2					227	253		10.1007/s11263-012-0565-0	http://dx.doi.org/10.1007/s11263-012-0565-0			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	081BW		Green Submitted			2022-12-18	WOS:000314291600001
J	Myaskouvskey, A; Gousseau, Y; Lindenbaum, M				Myaskouvskey, Artiom; Gousseau, Yann; Lindenbaum, Michael			Beyond Independence: An Extension of the A Contrario Decision Procedure	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						A contrario decision; Object recognition; Significance test; Background model; Number of false alarms; Meaningful matches	EDGE-DETECTION; CLASSIFICATION; MATCHES; MODELS	The a contrario approach is a principled method for making algorithmic decisions that has been applied successfully to many tasks in image analysis. The method is based on a background model (or null hypothesis) for the image. This model relies on independence assumptions and characterizes images in which no detection should be made. It is often image dependent, relying on statistics gathered from the image, and therefore adaptive. In this paper we propose a generalization for background models which relaxes the independence assumption and instead uses image dependent second order properties. The second order properties are accounted for thanks to graphical models. The modified a contrario technique is applied to two tasks: line segment detection and part-based object detection, and its advantages are demonstrated. In particular, we show that the proposed method enables reasonably accurate prediction of the false detection rate with no need for training data.	[Myaskouvskey, Artiom; Lindenbaum, Michael] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel; [Gousseau, Yann] LTCI CNRS, Telecom ParisTech, F-75634 Paris, France	Technion Israel Institute of Technology; Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris	Myaskouvskey, A (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.	artiom@cs.technion.ac.il; mic@cs.technion.ac.il; gousseau@telecom-paristech.fr						CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cormen T.H., 1990, INTRO ALGORITHMS 2 V; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Desolneux A, 2001, J MATH IMAGING VIS, V14, P271, DOI 10.1023/A:1011290230196; Desolneux A, 2000, INT J COMPUT VISION, V40, P7, DOI 10.1023/A:1026593302236; Desolneux A., 2008, GESTALT THEORY IMAGE, V1st ed.; Fergus R, 2007, INT J COMPUT VISION, V71, P273, DOI 10.1007/s11263-006-8707-x; Flenner A, 2011, SIAM J IMAGING SCI, V4, P243, DOI 10.1137/090772344; Galerne B, 2011, IEEE T IMAGE PROCESS, V20, P257, DOI 10.1109/TIP.2010.2052822; Gousseau Y, 2007, MULTISCALE MODEL SIM, V6, P105, DOI 10.1137/060659041; Grimmett G. R., 2001, PROBABILITY RANDOM P; Grosjean B, 2009, J MATH IMAGING VIS, V33, P313, DOI 10.1007/s10851-008-0111-4; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Moisan L, 2004, INT J COMPUT VISION, V57, P201, DOI 10.1023/B:VISI.0000013094.38752.54; Muse P, 2006, INT J COMPUT VISION, V69, P295, DOI 10.1007/s11263-006-7546-0; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rabin J, 2009, SIAM J IMAGING SCI, V2, P931, DOI 10.1137/090751359; Rajashekar U, 2006, J VISION, V6, P379, DOI 10.1167/6.4.7; RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006; Sabater N, 2012, IEEE T PATTERN ANAL, V34, P930, DOI 10.1109/TPAMI.2011.207; Sivic J, 2005, P INT C COMP VIS; VANTREES HL, 1965, DETECTION ESTIMATI 1; Vidal-Naquet M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P281; von Gioi RG, 2008, J MATH IMAGING VIS, V32, P313, DOI 10.1007/s10851-008-0102-5; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; Weber M, 2000, LECT NOTES COMPUT SC, V1842, P18; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4	28	15	15	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	1					22	44		10.1007/s11263-012-0543-6	http://dx.doi.org/10.1007/s11263-012-0543-6			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	080XV					2022-12-18	WOS:000314278500002
J	Hua, G; Fu, Y; Turk, M; Pollefeys, M; Zhang, ZY				Hua, Gang; Fu, Yun; Turk, Matthew; Pollefeys, Marc; Zhang, Zhengyou			Introduction to the Special Issue on Mobile Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Hua, Gang] Stevens Inst Technol, Hoboken, NJ 07030 USA; [Fu, Yun] SUNY Buffalo, Buffalo, NY 14260 USA; [Turk, Matthew] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA; [Pollefeys, Marc] Swiss Fed Inst Technol, Zurich, Switzerland; [Zhang, Zhengyou] Microsoft Res, Redmond, WA USA	Stevens Institute of Technology; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; University of California System; University of California Santa Barbara; Swiss Federal Institutes of Technology Domain; ETH Zurich; Microsoft	Hua, G (corresponding author), Stevens Inst Technol, Hoboken, NJ 07030 USA.	ghua@stevens.edu; yunfu@buffalo.edu; mturk@cs.ucsb.edu; marc.pollefeys@inf.ethz.ch; zhang@microsoft.com	zhang, zheng/HCH-9684-2022; Pollefeys, Marc/I-7607-2013					Catarci T., 2004, LECT NOTES COMPUTER, V2973, P25; Hua G, 2007, LECT NOTES COMPUT SC, V4796, P39; Wang J., 2006, ACM S US INT SOFTW T	3	15	18	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2012	96	3					277	279		10.1007/s11263-011-0506-3	http://dx.doi.org/10.1007/s11263-011-0506-3			3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	885HK					2022-12-18	WOS:000299769400001
J	Thirthala, S; Pollefeys, M				Thirthala, SriRam; Pollefeys, Marc			Radial Multi-focal Tensors Applications to Omnidirectional Camera Calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Calibration; Non-standard cameras; Auto-calibration; Radial distortion; Multi-view tensors; Quadrifocal tensor; Trifocal tensor; Fish-eye lenses; Catadioptric cameras	DISTORTION	The 1D radial camera maps all points on a plane, containing the principal axis, onto the radial line which is the intersection of that plane and the image plane. It is a sufficiently general model to express both central and non-central cameras, since the only assumption it makes is of known center of distortion. In this paper, we study the multi-focal tensors arising out of 1D radial cameras. There exist no two-view constraints (like the fundamental matrix) for 1D radial cameras. However, the 3-view and 4-view cases are interesting. For the 4-view case we have the radial quadrifocal tensor, which has 15 d.o.f and 2 internal constraints. For the 3-view case, we have the radial trifocal tensor, which has 7 d.o.f and no internal constraints. Under the assumption of a purely rotating central camera, this can be used to do a non-parametric estimation of the radial distortion of a 1D camera. Even in the case of a non-rotating camera it can be used to do parametric estimation, assuming a planar scene. Finally we examine the mixed trifocal tensor, which models the case of two 1D radial cameras and one standard pin-hole camera. Of the above radial multifocal tensors, only the radial trifocal tensor is useful practically, since it doesn't require any knowledge of the scene and is extremely robust. We demonstrate results based on real-images for this. For the quadrifocal tensor, too, we present a way to do a metric reconstruction of the scene and to undistort the image (given a sufficiently dense set of point-correspondences). We also show results on synthetic images. However, it must be noted that currently the quadrifocal and mixed trifocal tensors are useful only from a theoretical stand-point.	[Thirthala, SriRam; Pollefeys, Marc] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC USA; [Pollefeys, Marc] ETH, Inst Computat Sci, Zurich, Switzerland	University of North Carolina; University of North Carolina Chapel Hill; Swiss Federal Institutes of Technology Domain; ETH Zurich; Universita della Svizzera Italiana	Thirthala, S (corresponding author), Google Engn, Bangalore, Karnataka, India.	tvnsriram18@gmail.com; marc.pollefeys@inf.ethz.ch	Pollefeys, Marc/I-7607-2013					Baker S, 1999, INT J COMPUT VISION, V35, P175, DOI 10.1023/A:1008128724364; Bakstein H, 2002, THIRD WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P60, DOI 10.1109/OMNVIS.2002.1044492; Benosman R. B., 2001, PANORAMIC VISION SEN; BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855; De Agapito L., 1999, CVPR; Devernay F, 2001, MACH VISION APPL, V13, P14, DOI 10.1007/PL00013269; Espuny F., 2007, VISAPP 2007. Second International Conference on Computer Vision Theory and Applications, P26; Espuny F., 2008, GENERIC SELF CALIBRA; Faugeras O, 2000, IEEE T PATTERN ANAL, V22, P1179, DOI 10.1109/34.879801; Fitzgibbon AW, 2001, PROC CVPR IEEE, P125; GEYER C, 2001, CVPR; GOSHTASBY A, 1989, COMPUT VISION GRAPH, V47, P385, DOI 10.1016/0734-189X(89)90120-5; Grossberg Michael D., 2001, ICCV; Grossmann E., 2006, CVPR06, P1222; Grossmann E, 2010, COMPUT VIS IMAGE UND, V114, P198, DOI 10.1016/j.cviu.2009.03.009; Hartley R. I., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P471; Hartley R. I, 1993, DARPA93, P737; Hartley RI, 2004, LECT NOTES COMPUT SC, V3021, P363; Hartley R, 2007, IEEE T PATTERN ANAL, V29, P1309, DOI 10.1109/TPAMI.2007.1147; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hughes C, 2010, IEEE T PATTERN ANAL, V32, P2289, DOI 10.1109/TPAMI.2010.159; Hughes C, 2010, IMAGE VISION COMPUT, V28, P538, DOI 10.1016/j.imavis.2009.09.001; Kang S B, 2000, IAPR WORKSH MACH VIS, P603; Kannala J., 2004, ICPR; Kannala J, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P28; Li HD, 2006, LECT NOTES COMPUT SC, V3851, P21; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Micusik B, 2003, PROC CVPR IEEE, P485; Nister D, 2005, IEEE I CONF COMP VIS, P120; POLLEFEYS M, 1999, IJCV; QUAN L, 1997, PAMI; Ramalingam S., 2005, P ICCV WORKSH OMN VI; Ramalingam S, 2010, COMPUT VIS IMAGE UND, V114, P210, DOI 10.1016/j.cviu.2009.07.007; Rufli M, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3121, DOI 10.1109/IROS.2008.4650703; Sagawa R., 2008, OMNIVIS08; Scaramuzza D., 2006, P IEEE INT C COMP VI; Shah S, 1996, PATTERN RECOGN, V29, P1775, DOI 10.1016/0031-3203(96)00038-6; Stein GP, 1997, PROC CVPR IEEE, P602, DOI 10.1109/CVPR.1997.609387; Sturm P, 2004, LECT NOTES COMPUT SC, V3022, P1; Swaminathan R, 2000, IEEE T PATTERN ANAL, V22, P1172, DOI 10.1109/34.879797; Tardif Jean-Philippe, 2007, 2007 11th IEEE International Conference on Computer Vision, P1; Tardif JP, 2006, LECT NOTES COMPUT SC, V3954, P186; Thirthala S., 2005, Proceedings. Tenth IEEE International Conference on Computer Vision, P1539; Thirthala S., 2005, OMNIVIS05; Thirthala S., 2005, CVPR; Tisseur F., 2000, QUADRATIC EIGENVALUE; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388; TRIGGS B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P338, DOI 10.1109/ICCV.1995.466920; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P965, DOI 10.1109/34.159901; Xiong YL, 1997, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.1997.609326	51	15	16	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2012	96	2					195	211		10.1007/s11263-011-0463-x	http://dx.doi.org/10.1007/s11263-011-0463-x			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	876AW					2022-12-18	WOS:000299080200004
J	Ellis, L; Dowson, N; Matas, J; Bowden, R				Ellis, Liam; Dowson, Nicholas; Matas, Jiri; Bowden, Richard			Linear Regression and Adaptive Appearance Models for Fast Simultaneous Modelling and Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Regression tracking; Online appearance modelling		This work proposes an approach to tracking by regression that uses no hard-coded models and no offline learning stage. The Linear Predictor (LP) tracker has been shown to be highly computationally efficient, resulting in fast tracking. Regression tracking techniques tend to require offline learning to learn suitable regression functions. This work removes the need for offline learning and therefore increases the applicability of the technique. The online-LP tracker can simply be seeded with an initial target location, akin to the ubiquitous Lucas-Kanade algorithm that tracks by registering an image template via minimisation. A fundamental issue for all trackers is the representation of the target appearance and how this representation is able to adapt to changes in target appearance over time. The two proposed methods, LP-SMAT and LP-MED, demonstrate the ability to adapt to large appearance variations by incrementally building an appearance model that identifies modes or aspects of the target appearance and associates these aspects to the Linear Predictor trackers to which they are best suited. Experiments comparing and evaluating regression and registration techniques are presented along with performance evaluations favourably comparing the proposed tracker and appearance model learning methods to other state of the art simultaneous modelling and tracking approaches.	[Ellis, Liam] Linkoping Univ, CVL, Linkoping, Sweden; [Ellis, Liam; Bowden, Richard] Univ Surrey, CVSSP, Guildford GU2 5XH, Surrey, England; [Dowson, Nicholas] Royal Brisbane & Womens Hosp, AEHRC, Brisbane, Qld, Australia; [Matas, Jiri] Czech Tech Univ, CMP, CR-16635 Prague, Czech Republic	Linkoping University; University of Surrey; Royal Brisbane & Women's Hospital; Czech Technical University Prague	Ellis, L (corresponding author), Linkoping Univ, CVL, Linkoping, Sweden.	liam@isy.liu.se; nicholas.dowson@csiro.au; matas@cmp.felk.cvut.cz; R.Bowden@Surrey.ac.uk	Bowden, Richard/AAF-8283-2019; Dowson, Nicholas D H/B-7621-2017; Dowson, Nicholas/A-7537-2011; , Matas/AAW-3282-2020	Bowden, Richard/0000-0003-3285-8020; Dowson, Nicholas D H/0000-0003-4694-5459; Dowson, Nicholas/0000-0003-4694-5459; 	DIPLECS, Dynamic Interactive Perception-action LEarning in Cognitive Systems; European Commission [215078]; Czech Science Foundation [102/07/1317]	DIPLECS, Dynamic Interactive Perception-action LEarning in Cognitive Systems; European Commission(European CommissionEuropean Commission Joint Research Centre); Czech Science Foundation(Grant Agency of the Czech Republic)	This work is supported by DIPLECS, Dynamic Interactive Perception-action LEarning in Cognitive Systems, funded as contract number 215078 by the European Commission under FP7, and with support by Czech Science Foundation Project 102/07/1317.	Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Bischof H., 2006, BMVC, P47; Bray M, 2006, LECT NOTES COMPUT SC, V3952, P642; Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205; DOWSON N, 2006, N TIER SIMULTANEOUS, V2, P569; Dowson N, 2008, IEEE T PATTERN ANAL, V30, P180, DOI 10.1109/TPAMI.2007.70757; Ellis L., 2008, BMVC, P33; Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; HANSEN BB, 1999, IEEE COMP SOC C COMP, V2, P2202; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Jepson AD, 2001, PROC CVPR IEEE, P415; JURIE F, 2002, BRIT MACH VIS C, P123; Kalal Z., 2010, CVPR; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Marchand E., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P262, DOI 10.1109/ICCV.1999.791229; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; MATAS J, 2006, ICVGIP, P445; Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16; Mayol WW, 2008, MACH VISION APPL, V19, P65, DOI 10.1007/s00138-007-0087-x; ONG EJ, 2009, ROBUST FACIAL FEATUR, P1483; Paquin D, 2006, MATH BIOSCI ENG, V3, P389; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; SHEIKH YA, 2007, 11 IEEE INT C COMP V, P141; Shum HY, 2000, INT J COMPUT VISION, V36, P101, DOI 10.1023/A:1008195814169; Triggs B., 1998, P 5 EUROPEAN C COMPU; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Williams O, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P353; ZIMMERMANN K, 2008, THESIS CZECH TU PRAG; Zimmermann K, 2009, IEEE T PATTERN ANAL, V31, P677, DOI 10.1109/TPAMI.2008.119	31	15	17	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2011	95	2					154	179		10.1007/s11263-010-0364-4	http://dx.doi.org/10.1007/s11263-010-0364-4			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	815YN		Green Submitted			2022-12-18	WOS:000294566000004
J	Kolomenkin, M; Shimshoni, I; Tal, A				Kolomenkin, Michael; Shimshoni, Ilan; Tal, Ayellet			Prominent Field for Shape Processing and Analysis of Archaeological Artifacts	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape analysis; Shape processing; Archaeological models	SURFACES; DESIGN	Archaeological artifacts are an essential element of archaeological research. They provide evidence of the past and enable archaeologists to obtain qualified conclusion. Nowadays, many artifacts are scanned by 3D scanners. While convenient in many aspects, the 3D representation is often unsuitable for further analysis, due to flaws in the scanning process or defects in the original artifacts. We propose a new approach for automatic processing of scanned artifacts. It is based on the definition of a new direction field on surfaces (a normalized vector field), termed the prominent field. The prominent field is oriented with respect to the prominent feature curves of the surface. We demonstrate the applicability of the prominent field in two applications. The first is surface enhancement of archaeological artifacts, which helps enhance eroded features and remove scanning noise. The second is artificial coloring that can replace manual artifact illustration in archaeological reports.	[Shimshoni, Ilan] Univ Haifa, IL-31999 Haifa, Israel; [Kolomenkin, Michael; Tal, Ayellet] Technion Israel Inst Technol, Haifa, Israel	University of Haifa; Technion Israel Institute of Technology	Shimshoni, I (corresponding author), Univ Haifa, IL-31999 Haifa, Israel.	michkol@tx.technion.ac.il; ishimshoni@mis.haifa.ac.il; ayellet@ee.technion.ac.il		Shimshoni, Ilan/0000-0002-5276-0242	Israel Science Foundation (ISF) [628/08]; Olendorff foundation; joint Technion University of Haifa research foundation; Goldbers fund for electronics research	Israel Science Foundation (ISF)(Israel Science Foundation); Olendorff foundation; joint Technion University of Haifa research foundation; Goldbers fund for electronics research	This research was supported in part by the Israel Science Foundation (ISF) 628/08, Olendorff foundation, the joint Technion University of Haifa research foundation, and the Goldbers fund for electronics research. We thank Dr. A. Gilboa and Zinman Institute of Archeology for supplying us the models which were excavated at Tel Dor and for fruitful discussions on the topic.	Bajaj CL, 2003, ACM T GRAPHIC, V22, P4, DOI 10.1145/588272.588276; Brown BJ, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360683; COHEN J, 2004, S VIRT REAL ARCH CUL, P135; Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; Eigensatz M, 2008, COMPUT GRAPH FORUM, V27, P241, DOI 10.1111/j.1467-8659.2008.01121.x; Fisher M, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239507; Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368; Gooch B., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P31, DOI 10.1145/300523.300526; Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074; Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414; KOLLER D, 2006, J ROMAN ARCHAEOLOG S, V61, P237; Kolomenkin M., 2009, CVPR, P2767; KOLOMENKIN M, 2009, IEEE WORKSH EH DIG A; Kolomenkin M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409110; MEYER M, 2002, VISMATH, VB, P187; Ohtake Y, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P203; Ray N, 2006, ACM T GRAPHIC, V25, P1460, DOI 10.1145/1183287.1183297; RUSHMEIER H, 2006, RECORDING MODELING V, P22; Rusinkiewicz S, 2006, ACM T GRAPHIC, V25, P1199, DOI 10.1145/1141911.1142015; Stam J, 2003, ACM T GRAPHIC, V22, P724, DOI 10.1145/882262.882338; STERN E, 1995, EXCAVATIONS DOR; Sun XF, 2007, IEEE T VIS COMPUT GR, V13, P925, DOI 10.1109/TVCG.2007.1065; Tasdizen T, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P125, DOI 10.1109/VISUAL.2002.1183766; Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473; Toler-Franklin C, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P111; TURK G, 2001, ACM T GRAPHIC, V19, P347; von Funck W, 2006, ACM T GRAPHIC, V25, P1118, DOI 10.1145/1141911.1142002; Vrubel Alexandre, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2687, DOI 10.1109/CVPRW.2009.5206586; WEI L, 2001, ACM T GRAPHIC, V19, P355; Yoshizawa S, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P38; Zhang E, 2006, ACM T GRAPHIC, V25, P1294, DOI 10.1145/1183287.1183290	32	15	15	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2011	94	1					89	100		10.1007/s11263-010-0386-y	http://dx.doi.org/10.1007/s11263-010-0386-y			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)	Computer Science	760JM		Green Submitted			2022-12-18	WOS:000290320600007
J	Hinterstoisser, S; Lepetit, V; Benhimane, S; Fua, P; Navab, N				Hinterstoisser, Stefan; Lepetit, Vincent; Benhimane, Selim; Fua, Pascal; Navab, Nassir			Learning Real-Time Perspective Patch Rectification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Patch rectification; Tracking by detection; Object recognition; Online learning; Real-time learning; Pose estimation		We propose two learning-based methods to patch rectification that are faster and more reliable than state-of-the-art affine region detection methods. Given a reference view of a patch, they can quickly recognize it in new views and accurately estimate the homography between the reference view and the new view. Our methods are more memory-consuming than affine region detectors, and are in practice currently limited to a few tens of patches. However, if the reference image is a fronto-parallel view and the internal parameters known, one single patch is often enough to precisely estimate an object pose. As a result, we can deal in real-time with objects that are significantly less textured than the ones required by state-of-the-art methods. The first method favors fast run-time performance while the second one is designed for fast real-time learning and robustness. However, they follow the same general approach: First, a classifier provides for every keypoint a first estimate of its transformation. Then, the estimate allows carrying out an accurate perspective rectification using linear predictors. The last step is a fast verification-made possible by the accurate perspective rectification-of the patch identity and its sub-pixel precision position estimation. We demonstrate the advantages of our approach on real-time 3D object detection and tracking applications.	[Hinterstoisser, Stefan; Benhimane, Selim; Navab, Nassir] Tech Univ Munich, D-85748 Munich, Germany; [Lepetit, Vincent; Fua, Pascal] Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland	Technical University of Munich; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Hinterstoisser, S (corresponding author), Tech Univ Munich, Boltzmannstr 3, D-85748 Munich, Germany.	hinterst@in.tum.de; vincent.lepetit@epfl.ch; benhiman@in.tum.de; pascal.fua@epfl.ch; navab@in.tum.de	Fua, Pascal/H-3928-2011	Fua, Pascal/0000-0002-6702-9970	BMBF [01IM08002]; Bayrische Forschungsstiftung BFS	BMBF(Federal Ministry of Education & Research (BMBF)); Bayrische Forschungsstiftung BFS	This project was granted partly by BMBF (AVILUSplus: 01IM08002) and by the Bayrische Forschungsstiftung BFS.	Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Baker S., 2006, CMURITR0611; Bay H., 2006, EUR C COMP VIS, P417; Beis JS, 1997, PROC CVPR IEEE, P1000, DOI 10.1109/CVPR.1997.609451; Benhimane S, 2007, INT J ROBOT RES, V26, P661, DOI 10.1177/0278364907080252; BERG A, 2002, C COMP VIS PATT REC; Chum O., 2006, P COMP VIS PATT REC, V1, P879; Frey BJ, 2003, IEEE T PATTERN ANAL, V25, P1, DOI 10.1109/TPAMI.2003.1159942; Goedeme T., 2004, C COMP VIS PATT REC; Grabner H., 2006, C COMP VIS PATT REC; GRABNER M, 2007, C COMP VIS PATT REC; Hinterstoisser S., 2008, C COMP VIS PATT REC; HINTERSTOISSER S, 2009, C COMP VIS PATT REC; Jurie F, 2002, IEEE T PATTERN ANAL, V24, P996, DOI 10.1109/TPAMI.2002.1017625; LEPETIT V, 2005, C COMP VIS PATT REC; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; MIKOLAJCZYK K, 2004, INT J COMPUTER VISIO; MOLTON N, 2004, BRIT MACH VIS C; Obdrzalek S., 2006, CATEGORY LEVEL OBJEC, P85; OZUYSAL M, 2009, IEEE T PATTERN ANAL; Ozuysal M, 2006, EUR C COMP VIS; Philbin J, 2007, IEEE C COMP VIS PATT; Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1; Salzmann M., 2007, IEEE T PATTERN ANAL; Taylor S., 2009, BRIT MACH VIS C; TRIGGS B, 2004, EUR C COMP VIS; Williams B., 2007, INT C COMP VIS	29	15	16	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2011	91	1					107	130		10.1007/s11263-010-0379-x	http://dx.doi.org/10.1007/s11263-010-0379-x			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	705HB		Green Submitted			2022-12-18	WOS:000286118400006
J	Chandraker, M; Agarwal, S; Kriegman, D; Belongie, S				Chandraker, Manmohan; Agarwal, Sameer; Kriegman, David; Belongie, Serge			Globally Optimal Algorithms for Stratified Autocalibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Autocalibration; Multiple view geometry; Global optimization; Convex relaxations	OPTIMIZATION; POLYNOMIALS; PROGRAMS; MATLAB	We present practical algorithms for stratified autocalibration with theoretical guarantees of global optimality. Given a projective reconstruction, we first upgrade it to affine by estimating the position of the plane at infinity. The plane at infinity is computed by globally minimizing a least squares formulation of the modulus constraints. In the second stage, this affine reconstruction is upgraded to a metric one by globally minimizing the infinite homography relation to compute the dual image of the absolute conic (DIAC). The positive semidefiniteness of the DIAC is explicitly enforced as part of the optimization process, rather than as a post-processing step. For each stage, we construct and minimize tight convex relaxations of the highly non-convex objective functions in a branch and bound optimization framework. We exploit the inherent problem structure to restrict the search space for the DIAC and the plane at infinity to a small, fixed number of branching dimensions, independent of the number of views. Chirality constraints are incorporated into our convex relaxations to automatically select an initial region which is guaranteed to contain the global minimum. Experimental evidence of the accuracy, speed and scalability of our algorithm is presented on synthetic and real data.	[Chandraker, Manmohan; Kriegman, David; Belongie, Serge] Univ Calif San Diego, Dept Comp Sci & Engn, San Diego, CA 92103 USA; [Agarwal, Sameer] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA	University of California System; University of California San Diego; University of Washington; University of Washington Seattle	Chandraker, M (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, San Diego, CA 92103 USA.	mkchandraker@cs.ucsd.edu; sagarwal@cs.washington.edu; kriegman@cs.ucsd.edu; sjb@cs.ucsd.edu	Chandraker, Manmohan/AAU-4762-2021	Belongie, Serge/0000-0002-0388-5217	NSF [EIA-0303622, EIA-0321235, 0448615]; UW Animation Research Labs; Washington Research Foundation; Adobe; Microsoft; Sloan Research Fellowship	NSF(National Science Foundation (NSF)); UW Animation Research Labs; Washington Research Foundation; Adobe; Microsoft(Microsoft); Sloan Research Fellowship(Alfred P. Sloan Foundation)	The authors would like to thank Fredrik Kahl for several helpful discussions and providing data for the experiments with real images. Manmohan Chandraker and David Kriegman were supported by NSF EIA-0303622. Sameer Agarwal was supported by NSF EIA-0321235, UW Animation Research Labs, Washington Research Foundation, Adobe and Microsoft. Serge Belongie was supported by NSF Career #0448615 and the Sloan Research Fellowship.	Agarwal S., 2008, IEEE C COMP VIS PATT; AGARWAL S, 2006, EUR C COMP VIS, P592; AGRAWAL M, 2004, INT C IM PROC; ALKHAYYAL FA, 1983, MATH OPER RES, V8, P273, DOI 10.1287/moor.8.2.273; Boyd S, 2004, CONVEX OPTIMIZATION; Breuel TM, 2002, LECT NOTES COMPUT SC, V2352, P837; CHANDRAKER M, 2007, INT C COMP VIS; CHANDRAKER M, 2007, IEEE C COMP VIS PATT; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; Frahm J.-M., 1996, EUR C COMP VIS CAMBR, P2; Freedman D, 2003, IEEE T PATTERN ANAL, V25, P604, DOI 10.1109/TPAMI.2003.1195994; Fusiello A, 2004, IEEE T PATTERN ANAL, V26, P1633, DOI 10.1109/TPAMI.2004.125; GAT Y, 2003, COMP VIS PATT REC WO; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; Hartley R., 2004, ROBOTICA; Hartley R. I., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P510, DOI 10.1109/ICCV.1999.791264; Hartley RI, 1998, INT J COMPUT VISION, V26, P41, DOI 10.1023/A:1007984508483; Hartley R, 2007, LECT NOTES COMPUT SC, V4843, P13; Henrion D, 2003, ACM T MATH SOFTWARE, V29, P165, DOI 10.1145/779359.779363; HEYDEN A, 1996, ICPR, P339; Horst R., 2006, GLOBAL OPTIMIZATION; Kahl F., 2005, INT C COMP VIS; Lampert C.H., 2008, IEEE C COMP VIS PATT; LAND AH, 1960, ECONOMETRICA, V28, P497, DOI 10.2307/1910129; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Levenberg K., 1944, Q APPL MATH, V2, P164, DOI 10.1090/qam/10666; Manning RA, 2001, PROC CVPR IEEE, P590; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; MCCORMICK GP, 1976, MATH PROGRAM, V10, P147, DOI 10.1007/BF01580665; Moore R. E., 1996, METHOD APPL INTERVAL; Nister D, 2004, INT J COMPUT VISION, V60, P165, DOI 10.1023/B:VISI.0000029667.76852.a1; Olsson C, 2009, IEEE T PATTERN ANAL, V31, P783, DOI 10.1109/TPAMI.2008.131; Pollefeys M, 1999, IEEE T PATTERN ANAL, V21, P707, DOI 10.1109/34.784285; Pollefeys M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P90, DOI 10.1109/ICCV.1998.710705; Pollefeys M, 2002, LECT NOTES COMPUT SC, V2351, P837; PRAJNA S, 2002, IEEE C DEC CONTR; Schaffalitzky F., 2000, IND C COMP VIS GRAPH, P314; SIM K, 2006, IEEE C COMP VIS PATT, P1230, DOI DOI 10.1109/CVPR.2006.247; Stewenius H, 2005, IEEE I CONF COMP VIS, P686; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; Sturm P, 2000, IEEE T PATTERN ANAL, V22, P1199, DOI 10.1109/34.879804; Tawarmalani M, 2002, MATH PROGRAM, V93, P247, DOI 10.1007/s10107-002-0308-z; Tawarmalani M, 2001, J GLOBAL OPTIM, V20, P137; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388; ZONGKER D, 1996, INT C PATT REC ICPR, P18	47	15	16	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2010	90	2					236	254		10.1007/s11263-009-0305-2	http://dx.doi.org/10.1007/s11263-009-0305-2			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	640XQ		Green Published, Bronze			2022-12-18	WOS:000281087900006
J	Liu, J; Huang, HY; Huan, ZD; Zhang, HL				Liu, Jun; Huang, Haiyang; Huan, Zhongdan; Zhang, Haili			Adaptive Variational Method for Restoring Color Images with High Density Impulse Noise	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Color image; Deblurring and denoising; Impulse noise; Variational regularization methods; EM algorithm; Split Bregman method	MAXIMUM-LIKELIHOOD; RESTORATION; MINIMIZATION; ALGORITHM; REMOVAL	In this paper, a new variational framework of restoring color images with impulse noise is presented. The novelty of this work is the introduction of an adaptively weighting data-fidelity term in the cost functional. The fidelity term is derived from statistical methods and contains two weighting functions as well as some statistical control parameters of noise. This method is based on the fact that impulse noise can be approximated as an additive noise with probability density function (PDF) being the finite mixture model. A Bayesian framework is then formulated in which likelihood functions are given by the mixture model. Inspired by the expectation-maximization (EM) algorithm, we present two models with variational framework in this study. The superiority of the proposed models is that: the weighting functions can effectively detect the noise in the image; with the noise information, the proposed algorithm can automatically balance the regularity of the restored image and the fidelity term by updating the weighting functions and the control parameters. These two steps ensure that one can obtain a good restoration even though the degraded color image is contaminated by impulse noise with large ration (90% or more). In addition, the numerical implementation of this algorithm is very fast by using a split algorithm. Some numerical experimental results and comparisons with other methods are provided to show the significant effectiveness of our approach.	[Liu, Jun; Huang, Haiyang; Huan, Zhongdan] Beijing Normal Univ, Sch Math Sci, Lab Math & Complex Syst, Beijing 100875, Peoples R China; [Zhang, Haili] Univ Florida, Dept Math, Gainesville, FL 32611 USA	Beijing Normal University; State University System of Florida; University of Florida	Huan, ZD (corresponding author), Beijing Normal Univ, Sch Math Sci, Lab Math & Complex Syst, Beijing 100875, Peoples R China.	liujn121@gmail.com; hhywsg@bnu.edu.cn; bnumath217@gmail.com; hlzhang@ufl.edu			National Science Foundation of China (NSFC) [10531040]	National Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	We would like to thank Dr. Jian-Feng Cai for his kind offer the source codes of (Cai et al. 2008, 2009) and the authors of (Yang et al. 2009) for their open source codes FTVd, we also thank the reviewers for their valuable comments. The research has been supported by National Science Foundation of China (NSFC, No. 10531040).	Bar L, 2007, IEEE T IMAGE PROCESS, V16, P1101, DOI 10.1109/TIP.2007.891805; Bar L, 2006, INT J COMPUT VISION, V70, P279, DOI 10.1007/s11263-006-6468-1; Bilmes J.A., 1998, INT COMPUT SCI I, V4, P126; Blomgren P, 1998, IEEE T IMAGE PROCESS, V7, P304, DOI 10.1109/83.661180; Bresson X, 2008, INVERSE PROBL IMAG, V2, P455, DOI 10.3934/ipi.2008.2.455; Bresson X, 2007, J MATH IMAGING VIS, V28, P151, DOI 10.1007/s10851-007-0002-0; Cai JF, 2008, INVERSE PROBL IMAG, V2, P187; Cai JF, 2010, J MATH IMAGING VIS, V36, P46, DOI 10.1007/s10851-009-0169-7; Chambolle A, 2004, J MATH IMAGING VIS, V20, P89; Chan RH, 2005, IEEE T IMAGE PROCESS, V14, P1479, DOI 10.1109/TIP.2005.852196; Chan TF, 1999, SIAM J SCI COMPUT, V20, P1964, DOI 10.1137/S1064827596299767; Chan TF, 2005, IMAGE PROCESSING AND ANALYSIS, P207; Chen T, 2001, IEEE T CIRCUITS-II, V48, P784, DOI 10.1109/82.959870; Darbon J, 2005, LECT NOTES COMPUT SC, V3522, P351; Douglas J., 1956, T AM MATH SOC, V82, P421, DOI DOI 10.1090/S0002-9947-1956-0084194-4; Eng HL, 2001, IEEE T IMAGE PROCESS, V10, P242, DOI 10.1109/83.902289; Esser E, 2009, APPL LAGRANGIAN BASE; Gilboa G, 2007, MULTISCALE MODEL SIM, V6, P595, DOI 10.1137/060669358; Gilboa G, 2008, MULTISCALE MODEL SIM, V7, P1005, DOI 10.1137/070698592; Goldstein T., 2008, 0829 UCLA CAM; Huang YM, 2009, IEEE SIGNAL PROC LET, V16, P457, DOI 10.1109/LSP.2009.2016835; JALOBEANU A, 2005, IEEE T IMAGE PROCESS, V14, P1469; LAGENDIJK RL, 1990, OPT ENG, V29, P422, DOI 10.1117/12.55611; Liu J, 2009, INT J COMPUT VISION, V85, P182, DOI 10.1007/s11263-009-0254-9; Michael K. N., 1999, SIAM J SCI COMPUT, V21, P851; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Nikolova M, 2004, J MATH IMAGING VIS, V20, P99, DOI 10.1023/B:JMIV.0000011920.58935.9c; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Rudin L. I., 1994, Proceedings ICIP-94 (Cat. No.94CH35708), P31, DOI 10.1109/ICIP.1994.413269; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Setzer S, 2009, LECT NOTES COMPUT SC, V5567, P464, DOI 10.1007/978-3-642-02256-2_39; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; Tai X., 2009, 0905 UCLA CAM; TSCHUMPERLE D, 2002, THESIS U NICE SOPHIA; VOGEL C, 2002, COMPUTATIONAL METHOD, P53; Vogel CR, 1998, IEEE T IMAGE PROCESS, V7, P813, DOI 10.1109/83.679423; Vogel CR, 1996, SIAM J SCI COMPUT, V17, P227, DOI 10.1137/0917016; Yang JF, 2009, SIAM J SCI COMPUT, V31, P2842, DOI 10.1137/080732894; Zhao WY, 2007, IEEE SIGNAL PROC LET, V14, P401, DOI 10.1109/LSP.2006.887843	40	15	17	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2010	90	2					131	149		10.1007/s11263-010-0351-9	http://dx.doi.org/10.1007/s11263-010-0351-9			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	640XQ					2022-12-18	WOS:000281087900001
J	Malgouyres, F; Zeng, T				Malgouyres, F.; Zeng, T.			A Predual Proximal Point Algorithm Solving a Non Negative Basis Pursuit Denoising Model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Basis Pursuit; Algorithm; Sparse representation; Proximal point algorithm; ell1 minimization	THRESHOLDING ALGORITHM; SPARSE; REPRESENTATIONS; RECONSTRUCTION; COORDINATE	This paper develops an implementation of a Predual Proximal Point Algorithm (PPPA) solving a Non Negative Basis Pursuit Denoising model. The model imposes a constraint on the l (2) norm of the residual, instead of penalizing it. The PPPA solves the predual of the problem with a Proximal Point Algorithm (PPA). Moreover, the minimization that needs to be performed at each iteration of PPA is solved with a dual method. We can prove that these dual variables converge to a solution of the initial problem. Our analysis proves that we turn a constrained non differentiable convex problem into a short sequence of nice concave maximization problems. By nice, we mean that the functions which are maximized are differentiable and their gradient is Lipschitz. The algorithm is easy to implement, easier to tune and more general than the algorithms found in the literature. In particular, it can be applied to the Basis Pursuit Denoising (BPDN) and the Non Negative Basis Pursuit Denoising (NNBPDN) and it does not make any assumption on the dictionary. We prove its convergence to the set of solutions of the model and provide some convergence rates. Experiments on image approximation show that the performances of the PPPA are at the current state of the art for the BPDN.	[Malgouyres, F.] Univ Paris 13, CNRS, LAGA, UMR 7539, F-93430 Villetaneuse, France; [Zeng, T.] Hong Kong Baptist Univ, Dept Math, Kowloon Tong, Hong Kong, Peoples R China; [Zeng, T.] Hong Kong Baptist Univ, Inst Computat Math, Kowloon Tong, Hong Kong, Peoples R China	Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Mathematical Sciences (INSMI); Universite Paris 13; Hong Kong Baptist University; Hong Kong Baptist University	Malgouyres, F (corresponding author), Univ Paris 13, CNRS, LAGA, UMR 7539, 99 Ave Jean Baptiste Clement, F-93430 Villetaneuse, France.	malgouy@math.univ-paris13.fr; zeng@hkbu.edu.hk	Zeng, Tieyong/B-7147-2009	ZENG, Tieyong/0000-0002-0688-202X				BECT J, 2004, LECT NOTES COMPUTER; Berg E., 2007, TR200720 U BRIT COL; Bertsekas DP, 2003, NONLINEAR PROGRAMMIN; Bioucas-Dias JM, 2006, IEEE T IMAGE PROCESS, V15, P937, DOI 10.1109/TIP.2005.863972; Brown M, 2005, PATTERN RECOGN LETT, V26, P1907, DOI 10.1016/j.patrec.2005.03.012; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Ciarlet P.G., 1989, INTRO NUMERICAL LINE, DOI [10.1017/9781139171984, DOI 10.1017/9781139171984]; Combettes PL, 2008, SIAM J OPTIMIZ, V18, P1351, DOI 10.1137/060669498; Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; DONOHO D, 2006, 200618 DEP STAT; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Donoho DL, 2005, P NATL ACAD SCI USA, V102, P9446, DOI 10.1073/pnas.0502269102; DONOHO DL, 2005, 200504 STANF U DEP S; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Elad M, 2007, APPL COMPUT HARMON A, V23, P346, DOI 10.1016/j.acha.2007.02.002; Elad M, 2006, IEEE T INFORM THEORY, V52, P5559, DOI 10.1109/TIT.2006.885522; FIGUEIREDO M, 2007, GPSR 5 0 MATLAB TOOL; FIGUEIREDO M, 2005, P IEEE INT C IM PROC, V2; Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; GULER O, 1991, SIAM J CONTROL OPTIM, V29, P403, DOI 10.1137/0329022; Hale ET, 2007, TR0707 CAAM RIC U; Kim SJ, 2007, IEEE J-STSP, V1, P606, DOI 10.1109/JSTSP.2007.910971; Lemarechal C, 1997, SIAM J OPTIMIZ, V7, P367, DOI 10.1137/S1052623494267127; Malgouyres F, 2007, SIGNAL PROCESS, V87, P2695, DOI 10.1016/j.sigpro.2007.04.019; MALGOUYRES F, 2008, CODES SCRIPTS BASIS; MALGOUYRES F, 2006, 20061 U PAR 13; Maria S.I., 2006, PROC IEEE INT C ACOU, P14; Nesterov Y., 2018, APPL OPTIMIZATION; Rockafellar R. T., 1970, CONVEX ANAL; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; Sardy S, 2000, J COMPUT GRAPH STAT, V9, P361, DOI 10.2307/1390659; Starck JL, 2005, IEEE T IMAGE PROCESS, V14, P1570, DOI 10.1109/TIP.2005.852206; ZENG T, 2007, THESIS U PAR 13	37	15	17	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2009	83	3					294	311		10.1007/s11263-009-0227-z	http://dx.doi.org/10.1007/s11263-009-0227-z			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	424EI		Green Submitted			2022-12-18	WOS:000264547900005
J	Shah, SK				Shah, Shishir K.			Performance modeling and algorithm characterization for robust image segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image segmentation; performance modeling; algorithm characterization; algorithm selection	PREDICTION	This paper presents a probabilistic framework based on Bayesian theory for the performance prediction and selection of an optimal segmentation algorithm. The framework models the optimal algorithm selection process as one that accounts for the information content of an input image as well as the behavioral properties of a particular candidate segmentation algorithm. The input image information content is measured in terms of image features while the candidate segmentation algorithm's behavioral characteristics are captured through the use of segmentation quality features. Gaussian probability distribution models are used to learn the required relationships between the extracted image and algorithm features and the framework tested on the Berkeley Segmentation Dataset using four candidate segmentation algorithms.	Univ Houston, Dept Comp Sci, Houston, TX 77204 USA; [Shah, Shishir K.] Univ Houston, Quantitat Imaging lab, Houston, TX 77204 USA	University of Houston System; University of Houston; University of Houston System; University of Houston	Shah, SK (corresponding author), Univ Houston, Dept Comp Sci, Houston, TX 77204 USA.	shah@cs.uh.edu						Borsotti M, 1998, PATTERN RECOGN LETT, V19, P741, DOI 10.1016/S0167-8655(98)00052-X; Brox T, 2004, LECT NOTES COMPUT SC, V3175, P415; Chalmond B, 2001, IEEE T IMAGE PROCESS, V10, P1039, DOI 10.1109/83.931098; Chiang HC, 1999, P SOC PHOTO-OPT INS, V3721, P785, DOI 10.1117/12.357693; CHIANG HC, 2000, P SOC PHOTO-OPT INS, V4053, P7546; Cho K, 1997, IEEE T PATTERN ANAL, V19, P1185, DOI 10.1109/34.632979; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Duda R.O., 2000, PATTERN CLASSIFICATI; Ettinger GJ, 1996, P SOC PHOTO-OPT INS, V2757, P318, DOI 10.1117/12.242044; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; FOGEL I, 1989, BIOL CYBERN, V61, P103, DOI 10.1007/BF00204594; Freixenet J, 2002, LECT NOTES COMPUT SC, V2352, P408, DOI 10.1007/3-540-47977-5_27; FU KS, 1981, PATTERN RECOGN, V13, P3, DOI 10.1016/0031-3203(81)90028-5; HARALICK RM, 1992, COMPUTER ROBOT VISIO, V1, P303; Konishi S, 2000, PROC CVPR IEEE, P125, DOI 10.1109/CVPR.2000.855809; Laws KI., 1980, THESIS U SO CALIFORN; LIU JQ, 1994, IEEE T PATTERN ANAL, V16, P689, DOI 10.1109/34.297949; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Meila M., 2005, P 22 INT C MACH LEAR, P577; NAIR D, 1996, 4 EUR C COMP VIS CAM, V1, P579; PAL NR, 1993, PATTERN RECOGN, V26, P1277, DOI 10.1016/0031-3203(93)90135-J; Pavlidis T., 1977, STRUCTURAL PATTERN R; Pernet A, 1999, THEOR APPL GENET, V99, P524, DOI 10.1007/s001220051266; Puzicha J, 1999, PATTERN RECOGN LETT, V20, P899, DOI 10.1016/S0167-8655(99)00056-2; Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261; Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10; REYNOLDS RG, 1995, P 1 INT S INT NEUR B, V240; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shufelt JA, 1999, IEEE T PATTERN ANAL, V21, P311, DOI 10.1109/34.761262; SPANN M, 1988, PATTERN RECOGN LETT, V8, P251, DOI 10.1016/0167-8655(88)90032-3; SPANN M, 1994, PATTERN RECOGN, V27, P1717, DOI 10.1016/0031-3203(94)90089-2; Tourassi GD, 2000, COMPUT BIOMED RES, V33, P161, DOI 10.1006/cbmr.2000.1542; Tuceryan M., 1993, HDB PATTERN RECOGNIT, P235, DOI DOI 10.1142/9789814343138_0010; van Rijsbergen C., 1979, INFORM RETRIEVAL, V2nd; YANG AY, 2006, UCBEECS2006195; ZHANG H, 2006, IEEE C COMP VIS PATT, P1138; ZHANG X, 1993, BRIT MACH VIS C; Zhang YJ, 1996, PATTERN RECOGN, V29, P1335, DOI 10.1016/0031-3203(95)00169-7	38	15	15	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2008	80	1					92	103		10.1007/s11263-008-0130-z	http://dx.doi.org/10.1007/s11263-008-0130-z			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	344CB					2022-12-18	WOS:000258901900007
J	Jackson, JD; Yezzi, AJ; Soatto, S				Jackson, Jeremy D.; Yezzi, Anthony J.; Soatto, Stefano			Dynamic shape and appearance modeling via moving and deforming layers	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shape modeling; appearance modeling; non-rigid registration; rigid registration; active contours	OPTICAL-FLOW; COMPUTATION	We propose a model of the shape, motion and appearance of a scene, seen through a sequence of images, that captures occlusions, scene deformations, unconstrained viewpoint variations and changes in its radiance. This model is based on a collection of overlapping layers that can move and deform, each supporting an intensity function that can change over time. We discuss the generality and limitations of this model in relation to existing ones such as traditional optical flow or motion segmentation, layers, deformable templates and deformotion. We then illustrate how this model can be used for inference of shape, motion, deformation and appearance of the scene from a collection of images. The layering structure allows for automatic inpainting of partially occluded regions. We illustrate the model on synthetic and real sequences where existing schemes fail, and show how suitable choices of constants in the model yield existing schemes, from optical flow to motion segmentation and inpainting.	[Jackson, Jeremy D.; Yezzi, Anthony J.] Georgia Inst Technol, Sch Elect Engn, Atlanta, GA 30332 USA; [Soatto, Stefano] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University System of Georgia; Georgia Institute of Technology; University of California System; University of California Los Angeles	Jackson, JD (corresponding author), Georgia Inst Technol, Sch Elect Engn, 777 Atlantic Dr NW, Atlanta, GA 30332 USA.	jeremydjackson@gmail.com	Yezzi, Anthony/AAB-4235-2020	Jackson, Jeremy/0000-0003-4072-548X				Alvarez L, 1999, LECT NOTES COMPUT SC, V1682, P235; BAKER S, 2003, IMAGE CODING ACTIVE; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Blake A., 1998, ACTIVE CONTOURS, DOI [10.1007/978-1-4471-1555-7, DOI 10.1007/978-1-4471-1555-7]; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; COOTES TF, 1998, P EUR C COMP VIS, V2, P484; CREMERS D, 2003, INT C SCALE SPACE TH, P599; DERICHE R, 1995, P 2 AS C COMP VIS AC, V2, P290; Grenander U., 1993, GEN PATTERN THEORY; Haussecker HW, 2001, IEEE T PATTERN ANAL, V23, P661, DOI 10.1109/34.927465; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HSU S, 1992, P IEEE C COMP VISION, P1621; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; MARKS T, 2005, ADV NEURAL INFORM PR, V17; MILLER MI, 1999, P SCTV; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y; Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758; Paragios N, 2003, COMPUT VIS IMAGE UND, V89, P142, DOI 10.1016/S1077-3142(03)00010-9; SCHNORR C, 1992, INT J COMPUT VISION, V8, P153, DOI 10.1007/BF00127172; SOATTO S, 2002, P EUR C COMP VIS, V3, P32; Trouve A, 2005, FOUND COMPUT MATH, V5, P173, DOI 10.1007/s10208-004-0128-z; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Yezzi AJ, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P59, DOI 10.1109/ICCV.2001.937499	24	15	16	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2008	79	1					71	84		10.1007/s11263-007-0097-1	http://dx.doi.org/10.1007/s11263-007-0097-1			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	291JQ		Green Submitted			2022-12-18	WOS:000255193500005
J	Wu, YH; Li, YF; Hu, ZY				Wu, Yihong; Li, Youfu; Hu, Zhanyi			Detecting and handling unreliable points for camera parameter estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						bracket algebra; invariant; camera calibration	RECONSTRUCTION	The popularly used DLT method sometimes fails to give reliable camera parameter estimation. It is therefore important to detect the unreliability and provide the corresponding solutions. Based on a complete framework of invariance for six points, we construct two evaluation functions to detect the unreliability. The two evaluation functions do not involve any computations for the camera projective matrix or optical center and thus are efficient to perform the detection. Then, the guidelines corresponding to the different detection results are presented. In particular, a filtering RANSAC method to remove the detected unreliable points is provided. The filtering RANSAC proves to be successful in removing the unreliable points even if these points are of a large proportion.	[Wu, Yihong; Li, Youfu] City Univ Hong Kong, Dept Manufacturing Engn & Engn Management, Kowloon, Hong Kong, Peoples R China; [Wu, Yihong; Hu, Zhanyi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100080, Peoples R China	City University of Hong Kong; Chinese Academy of Sciences; Institute of Automation, CAS	Wu, YH (corresponding author), City Univ Hong Kong, Dept Manufacturing Engn & Engn Management, 83 Tat Chee Ave, Kowloon, Hong Kong, Peoples R China.	yhwu@nlpr.ia.ac.cn		LI, You Fu/0000-0002-5227-1326				ABDELAZIZ YI, 1971, P ASP UI S CLOS RANG, P1; Bayro-Corrochano E, 2002, J MATH IMAGING VIS, V16, P131, DOI 10.1023/A:1013947415006; BUCHANAN T, 1992, PROJECTION 6 POINTS; Carlsson S., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P249, DOI 10.1007/BFb0055671; CARLSSON S, 1995, KTHNAP9522 SE ISRN; Coble AB, 1922, T AM MATH SOC, V24, P1; Duda R.O., 2000, PATTERN CLASSIFICATI; FORSTNER W, 1987, COMPUT VISION GRAPH, V40, P273, DOI 10.1016/S0734-189X(87)80144-5; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Kahl F, 2007, INT J COMPUT VISION, V74, P3, DOI 10.1007/s11263-006-0015-y; QUAN L, 1995, IEEE T PATTERN ANAL, V17, P34; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; Slama CC., 1980, MANUAL PHOTOGRAMMETR, V4th edn; Sutherland Ivan E., 1963, 296 MIT LINC LAB; WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779; Wu YH, 2005, LECT NOTES COMPUT SC, V3519, P403; Wu YH, 2003, IEEE T PATTERN ANAL, V25, P1329, DOI 10.1109/TPAMI.2003.1233907; WU YH, 2001, THESIS CHIN AC SCI I; Wu YH, 2006, IMAGE VISION COMPUT, V24, P1313, DOI 10.1016/j.imavis.2006.04.010	19	15	16	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2008	79	2					209	223		10.1007/s11263-007-0114-4	http://dx.doi.org/10.1007/s11263-007-0114-4			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	302ZV					2022-12-18	WOS:000256009700006
J	Ramnath, K; Koterba, S; Xiao, J; Hu, CB; Matthews, I; Baker, S; Cohn, J; Kanade, T				Ramnath, Krishnan; Koterba, Seth; Xiao, Jing; Hu, Changbo; Matthews, Iain; Baker, Simon; Cohn, Jeffrey; Kanade, Takeo			Multi-view AAM fitting and construction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Conference on Computer Vision and Pattern Recognition	JUN 20-25, 2005	San Diego, CA	IEEE Comp Soc		active appearance models; multi-view 3D face model construction; multi-view AAM fitting; non-rigid structure-from-motion; motion-stereo; camera calibration	ACTIVE APPEARANCE MODELS; IMAGE; AMBIGUITIES; TRACKING; MOTION; SHAPE	Active Appearance Models (AAMs) are generative, parametric models that have been successfully used in the past to model deformable objects such as human faces. The original AAMs formulation was 2D, but they have recently been extended to include a 3D shape model. A variety of single-view algorithms exist for fitting and constructing 3D AAMs but one area that has not been studied is multi-view algorithms. In this paper we present multi-view algorithms for both fitting and constructing 3D AAMs. Fitting an AAM to an image consists of minimizing the error between the input image and the closest model instance; i.e. solving a nonlinear optimization problem. In the first part of the paper we describe an algorithm for fitting a single AAM to multiple images, captured simultaneously by cameras with arbitrary locations, rotations, and response functions. This algorithm uses the scaled orthographic imaging model used by previous authors, and in the process of fitting computes, or calibrates, the scaled orthographic camera matrices. In the second part of the paper we describe an extension of this algorithm to calibrate weak perspective (or full perspective) camera models for each of the cameras. In essence, we use the human face as a (non-rigid) calibration grid. We demonstrate that the performance of this algorithm is roughly comparable to a standard algorithm using a calibration grid. In the third part of the paper, we show how camera calibration improves the performance of AAM fitting. A variety of non-rigid structure-from-motion algorithms, both single-view and multi-view, have been proposed that can be used to construct the corresponding 3D non-rigid shape models of a 2D AAM. In the final part of the paper, we show that constructing a 3D face model using non-rigid structure-from-motion suffers from the Bas-Relief ambiguity and may result in a "scaled" (stretched/compressed) model. We outline a robust non-rigid motion-stereo algorithm for calibrated multi-view 3D AAM construction and show how using calibrated multi-view motion-stereo can eliminate the Bas-Relief ambiguity and yield face models with higher 3D fidelity.	[Ramnath, Krishnan] Objectvideo Inc, Reston, VA 20191 USA; [Koterba, Seth; Hu, Changbo; Matthews, Iain; Kanade, Takeo] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Xiao, Jing] Epson Res & Dev, Epson Palo Alto Lab, San Jose, CA 95131 USA; [Baker, Simon] Microsoft Corp, Microsoft Res, Redmond, WA 98052 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA	Carnegie Mellon University; Microsoft; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Ramnath, K (corresponding author), Objectvideo Inc, 11600 Sunrise Valley Dr,Suite 290, Reston, VA 20191 USA.	kramnath@objectvideo.com; skoterba@cs.cmu.edu; xiaoj@erd.epson.com; changbo@cs.cmu.edu; iainm@cs.cmu.edu; sbaker@microsoft.com; jeffcohn@cs.cmu.edu; tk@cs.cmu.edu			NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH051435] Funding Source: NIH RePORTER; NIMH NIH HHS [R01 MH051435, R01 MH051435-13] Funding Source: Medline	NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); NIMH NIH HHS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))		Ahlberg J, 2001, IEEE ICCV WORKSHOP ON RECOGNITION, ANALYSIS AND TRACKING OF FACES AND GESTURES IN REAL-TIME SYSTEMS, PROCEEDINGS, P68, DOI 10.1109/RATFG.2001.938912; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Baker S, 2004, IEEE T PATTERN ANAL, V26, P1380, DOI 10.1109/TPAMI.2004.77; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; BOUGUET JY, 2005, CAMERA CALIBRATION T; Brandt Tobias, 2001, Curr Treat Options Neurol, V3, P463, DOI 10.1007/s11940-001-0034-5; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; COOTES T, 2000, P BRIT MACH VIS C, V1, P52; COOTES T, 1998, P BRIT MACH VIS C, V2, P680; Cootes TF, 1996, IMAGE VISION COMPUT, V14, P581, DOI 10.1016/0262-8856(96)01099-2; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; COOTES TF, 2002, P BMVC, V2, P837; COOTES TF, 1998, P EUR C COMP VIS, V2, P484; Dornaika F, 2004, IEEE T SYST MAN CY B, V34, P1838, DOI 10.1109/TSMCB.2004.829135; EDWARDS GJ, 1999, THESIS U MANCHESTER; GOKTURK S, 2001, P INT C COMP VIS, P7101; Gross R, 2004, IEEE T PATTERN ANAL, V26, P449, DOI 10.1109/TPAMI.2004.1265861; Gross R, 2006, IMAGE VISION COMPUT, V24, P593, DOI 10.1016/j.imavis.2005.08.001; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1064; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hu CG, 2004, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM ON INSTRUMENTATION SCIENCE AND TECHNOLOGY, VOL 3, P437; Jones MJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P683, DOI 10.1109/ICCV.1998.710791; Koterba S, 2005, IEEE I CONF COMP VIS, P511; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Matthews I, 2007, INT J COMPUT VISION, V75, P93, DOI 10.1007/s11263-007-0043-2; Pighin F., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P143, DOI 10.1109/ICCV.1999.791210; Romdhani S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P59; Sclaroff S, 2003, COMPUT VIS IMAGE UND, V89, P197, DOI 10.1016/S1077-3142(03)00003-1; Sclaroff S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1146, DOI 10.1109/ICCV.1998.710860; Soatto S, 1998, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.1998.698621; Sung J, 2004, IEEE IMAGE PROC, P3363; Szeliski R, 1997, IEEE T PATTERN ANAL, V19, P506, DOI 10.1109/34.589211; Torresani L, 2001, PROC CVPR IEEE, P493; Vetter T, 1997, IEEE T PATTERN ANAL, V19, P733, DOI 10.1109/34.598230; WAXMAN AM, 1986, IEEE T PATTERN ANAL, V8, P715, DOI 10.1109/TPAMI.1986.4767853; Wen Z, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1343, DOI 10.1109/ICCV.2003.1238646; Xiao J, 2005, IEEE I CONF COMP VIS, P1075; Xiao J, 2004, LECT NOTES COMPUT SC, V2034, P573; ZHANG Z, 1992, 3D DYNAMIC SCENA ANA; ZHANG ZY, 1992, IEEE T PATTERN ANAL, V14, P1141, DOI 10.1109/34.177380	41	15	16	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2008	76	2					183	204		10.1007/s11263-007-0050-3	http://dx.doi.org/10.1007/s11263-007-0050-3			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	255VH	19838316	Green Accepted			2022-12-18	WOS:000252685400008
J	Yu, TL; Xu, N; Ahuja, N				Yu, Tianli; Xu, Ning; Ahuja, Narendra			Shape and View Independent Reflectance Map from multiple views	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						reflectance model; 3d reconstruction; shape from shading; illumination model; BRDF	ILLUMINATION; STEREO; MODELS	We consider the problem of estimating the 3D shape and reflectance properties of an object made of a single material from a set of calibrated views. To model the reflectance, we propose to use the View Independent Reflectance Map (VIRM), which is a representation of the joint effect of the diffuse+specular Bidirectional Reflectance Distribution Function (BRDF) and the environment illumination. The object shape is parameterized using a triangular mesh. We pose the estimation problem as minimizing the cost of matching input images, and the images synthesized using the shape and VIRM estimates. We show that by enforcing a constant value of VIRM as a global constraint, we can minimize the cost function by iterating between the VIRM and shape estimation. Experimental results on both synthetic and real objects show that our algorithm can recover both the 3D shape and the diffuse/specular reflectance information. Our algorithm does not require the light sources to be known or calibrated. The estimated VIRM can be used to predict the appearances of objects with the same material from novel viewpoints and under transformed illumination.	Univ Illinois, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Yu, TL (corresponding author), Univ Illinois, Urbana, IL 61801 USA.	tianli.yu@motorola.com		YU, TIAN-LI/0000-0002-0273-9330				[Anonymous], MATHWORLD WOLFRAM WE; Bhat DN, 1998, INT J COMPUT VISION, V26, P91, DOI 10.1023/A:1007940725322; DEBEVEC P, 2004, HIGH DYNAMIC RANGE I; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; GEORGHIADES A, 2003, CVPR 2001 P 2001 IEE, V2, P816; Hertzmann A, 2003, PROC CVPR IEEE, P533; IN H, 2003, P 2003 IEEE COMP SOC, V1, P171; JIN H, 2004, P IEEE C COMP VIS PA, V1, P36; Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Lee KM, 1997, COMPUT VIS IMAGE UND, V67, P143, DOI 10.1006/cviu.1997.0522; LIN S, 2002, P EUR C COMP VIS, P210; *MATHW INC, 2004, MATL OPT TOOLB; Miller G. S., 1984, COURSE NOTES ADV COM; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; Samaras D, 2003, IEEE T PATTERN ANAL, V25, P247, DOI 10.1109/TPAMI.2003.1177155; Samaras D, 2000, PROC CVPR IEEE, P480, DOI 10.1109/CVPR.2000.855858; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Treuille A, 2004, LECT NOTES COMPUT SC, V3022, P457; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; Yu YZ, 1999, COMP GRAPH, P215; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; Zickler TE, 2002, INT J COMPUT VISION, V49, P215, DOI 10.1023/A:1020149707513	25	15	16	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2007	73	2					123	138		10.1007/s11263-006-9373-8	http://dx.doi.org/10.1007/s11263-006-9373-8			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	146OI					2022-12-18	WOS:000244943500001
J	Oliensis, J				Oliensis, J			The least-squares error for structure from infinitesimal motion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; optimization; least-squares error; error analysis; noise sensitivity; ambiguity; local minima; optical flow; motion estimation; shape from x	RECOVERING 3-D MOTION; INHERENT AMBIGUITIES; ALGORITHMS	We analyze the least-squares error for structure from motion with a single infinitesimal motion ("structure from optical flow"). We present asymptotic approximations to the noiseless error over two, complementary regions of motion estimates: roughly forward and non-forward translations. Our approximations are powerful tools for understanding the error. Experiments show that they capture its detailed behavior over the entire range of motions. We illustrate the use of our approximations by deriving new properties of the least-squares error. We generalize the earlier results of Jepson/Heeger/Maybank on the bas-relief ambiguity and of Oliensis on the reflected minimum. We explain the error's complexity and its multiple local minima for roughly forward translation estimates (epipoles within the field of view) and identify the factors that make this complexity likely. For planar scenes, we clarify the effects of the two-fold ambiguity, show the existence of a new, double bas-relief ambiguity, and analyze the error's local minima. For nonplanar scenes, we derive simplified error approximations for reasonable assumptions on the image and scene. For example, we show that the error tends to have a simpler form when many points are tracked. We show experimentally that our analysis for zero image noise gives a good model of the error for large noise. We show theoretically and experimentally that the error for projective structure from motion is simpler but flatter than the error for calibrated images.	Stevens Inst Technol, Dept Comp Sci, Hoboken, NJ 07030 USA	Stevens Institute of Technology	Oliensis, J (corresponding author), Stevens Inst Technol, Dept Comp Sci, Castle Point Hudson, Hoboken, NJ 07030 USA.	oliensis@cs.stevens-tech.edu						ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; CHIUSO A, 2000, IJCV, V39, P95; DANIILIDIS K, 1990, IMAGE VISION COMPUT, V8, P297, DOI 10.1016/0262-8856(90)80006-F; DANIILIDIS K, 1993, VISUAL NAVIGATION, P61; DUTTA R, 1989, CVPR, P159; Erdelyi A., 1956, ASYMPTOTIC EXPANSION; Fermuller C, 2000, INT J COMPUT VISION, V37, P43, DOI 10.1023/A:1008177429387; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HOMEGGER J, 1999, ICCV, P640; JEPSON AD, 1990, RBCVTR9036 U TOR; Kanatani K., 1993, GEOMETRIC COMPUTATIO; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; KUMAR R, 1990, ICCV, P365; Ma Y, 2001, INT J COMPUT VISION, V44, P219, DOI 10.1023/A:1012276232049; MAYBANK S, 1993, THEORY RECONSTRUCTIO; MAYBANK SJ, 1987, THESIS U LONDON; Oliensis J, 1998, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.1998.698610; Oliensis J, 2000, IEEE T PATTERN ANAL, V22, P685, DOI 10.1109/34.865186; Oliensis J, 2001, IEEE T PATTERN ANAL, V23, P546, DOI 10.1109/34.927457; OLINESIS J, 1999, CVPR, P185; Soatto S, 1998, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.1998.698621; Soatto S, 1997, INT J COMPUT VISION, V22, P235, DOI 10.1023/A:1007930700152; SPETSAKIS ME, 1992, IEEE T PATTERN ANAL, V14, P959, DOI 10.1109/34.161355; Szeliski R, 1997, IEEE T PATTERN ANAL, V19, P506, DOI 10.1109/34.589211; Tian TY, 1996, PROC CVPR IEEE, P315, DOI 10.1109/CVPR.1996.517091; Tomasi C., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P422, DOI 10.1109/CVPR.1993.341096; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; Vieville T, 1996, COMPUT VIS IMAGE UND, V64, P128, DOI 10.1006/cviu.1996.0049; WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074; WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779; Xiang T, 2003, INT J COMPUT VISION, V51, P111, DOI 10.1023/A:1021627622971; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903	33	15	17	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	2005	61	3					259	299						41	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	896IP					2022-12-18	WOS:000226928100003
J	Sminchisescu, C; Triggs, B				Sminchisescu, C; Triggs, B			Building roadmaps of minima and transitions in visual models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						model based vision; global optimization; saddle points; 3D human tracking	MOLECULAR-DYNAMICS; ENERGY; RELAXATION; WALKING; MOTION	Becoming trapped in suboptimal local minima is a perennial problem when optimizing visual models, particularly in applications like monocular human body tracking where complicated parametric models are repeatedly fitted to ambiguous image measurements. We show that trapping can be significantly reduced by building 'roadmaps' of nearby minima linked by transition pathways-paths leading over low 'mountain passes' in the cost surface-found by locating the transition state (codimension-1 saddle point) at the top of the pass and then sliding downhill to the next minimum. We present two families of transition-state-finding algorithms based on local optimization. In eigenvector tracking, unconstrained Newton minimization is modified to climb uphill towards a transition state, while in hypersurface sweeping, a moving hypersurface is swept through the space and moving local minima within it are tracked using a constrained Newton method. These widely applicable numerical methods, which appear not to be known in vision and optimization, generalize methods from computational chemistry where finding transition states is critical for predicting reaction parameters. Experiments on the challenging problem of estimating 3D human pose from monocular images show that our algorithms find nearby transition states and minima very efficiently, but also underline the disturbingly large numbers of minima that can exist in this and similar model based vision problems.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada; INRIA, CNRS, GRAVIR, F-38330 Montbonnot St Martin, France	University of Toronto; Centre National de la Recherche Scientifique (CNRS); Inria	Sminchisescu, C (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.	crismin@cs.toronto.edu; Bill.Triggs@inrialpes.fr						ABASHKIN Y, 1994, INT J QUANTUM CHEM; ABASHKIN Y, 1994, J CHEM PHYS; ANDERSON N, 1986, J OPTIMIZATION THEOR; Barkema GT, 1996, PHYS REV LETT, V77, P4358, DOI 10.1103/PhysRevLett.77.4358; BOFILL JM, 1994, J COMPUT CHEM, V15, P1, DOI 10.1002/jcc.540150102; BRANIN F, 1972, NUMERICAL METHODS NO; CERJAN CJ, 1981, J CHEM PHYS, V75, P2800, DOI 10.1063/1.442352; Chiuso A, 2000, INT J COMPUT VISION, V39, P195, DOI 10.1023/A:1026563712076; Choo K., 2001, IEEE INT C COMP VIS; CRIPPEN GM, 1971, ARCH BIOCHEM BIOPHYS, V144, P462, DOI 10.1016/0003-9861(71)90349-3; CULOT P, 1992, THEOR CHIM ACTA, V82, P189, DOI 10.1007/BF01113251; Deutscher J., 2000, IEEE INT C COMP VIS; DEUTSCHER J, 2001, IEEE INT C COMP VIS; FIODOROVA I, 1978, OPTIMAL DECISION THE; Fletcher Roger, 1987, PRACTICAL METHODS OP, DOI 10.1002/9781118723203; Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056; GE R, 1987, J COMP MATH; Glover F., 1989, ORSA Journal on Computing, V1, P190, DOI [10.1287/ijoc.2.1.4, 10.1287/ijoc.1.3.190]; GOLDSTEIN A, 1971, MATH COMPUTATION; GRIEWANK A, 1981, J OPTIMIZATION THEOR; HELGAKER T, 1991, CHEM PHYS LETT, V182, P503, DOI 10.1016/0009-2614(91)90115-P; Henkelman G, 1999, J CHEM PHYS, V111, P7010, DOI 10.1063/1.480097; HILDERBRANDT RL, 1977, COMPUT CHEM, V1, P179, DOI 10.1016/0097-8485(77)85008-0; JENSEN F, 1995, J CHEM PHYS, V102, P6706, DOI 10.1063/1.469144; JORGENSEN P, 1988, THEOR CHIM ACTA, V73, P55, DOI 10.1007/BF00526650; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; LEE HJ, 1985, COMPUT VISION GRAPH, V30, P148, DOI 10.1016/0734-189X(85)90094-5; LEVY A, 1985, SIAM J STAT COMP; Morris DD, 1998, PROC CVPR IEEE, P289, DOI 10.1109/CVPR.1998.698622; Mousseau N, 1998, PHYS REV E, V57, P2419, DOI 10.1103/PhysRevE.57.2419; Munro LJ, 1999, PHYS REV B, V59, P3969, DOI 10.1103/PhysRevB.59.3969; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; NICHOLS J, 1990, J CHEM PHYS, V92, P340, DOI 10.1063/1.458435; OLIENSIS J, 2001, ERROR SURFACE STRUCT; SEVICK EM, 1993, J CHEM PHYS, V98, P3196, DOI 10.1063/1.464093; SIDENBLADH H, 2002, EUR C COMP VIS; SIDENBLADH H, 2000, EUR C COMP VIS; SIMONS J, 1983, J PHYS CHEM-US, V87, P2745, DOI 10.1021/j100238a013; Sminchisescu C, 2004, PROC CVPR IEEE, P608; Sminchisescu C, 2003, INT J ROBOT RES, V22, P371, DOI 10.1177/0278364903022006003; Sminchisescu C, 2003, PROC CVPR IEEE, P69; Sminchisescu C, 2002, LECT NOTES COMPUT SC, V2350, P566; Sminchisescu C, 2001, PROC CVPR IEEE, P447; Sminchisescu C, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P27, DOI 10.1109/AFGR.2002.1004125; Sminchisescu C., 2004, ICML; SMINCHISESCU C, 2002, THESIS INRIA; Sminchisescu C., 2003, CSRG478 U TOR; SUN JQ, 1993, J CHEM PHYS, V98, P9707, DOI 10.1063/1.464349; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Voter AF, 1997, PHYS REV LETT, V78, P3908, DOI 10.1103/PhysRevLett.78.3908; Voter AF, 1997, J CHEM PHYS, V106, P4665, DOI 10.1063/1.473503; WALES DJ, 1989, J CHEM PHYS, V91; WALES DJ, 1996, J CHEM PHYS, V105	53	15	15	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2005	61	1					81	101		10.1023/B:VISI.0000042935.43630.46	http://dx.doi.org/10.1023/B:VISI.0000042935.43630.46			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	866XN		Green Submitted			2022-12-18	WOS:000224806800004
J	Chowdhury, AKR; Chellappa, R				Chowdhury, AKR; Chellappa, R			Stochastic approximation and rate-distortion analysis for robust structure and motion estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure and motion estimation; error analysis; Robbins-Monro stochastic approximation; rate distortion theory	3-D MOTION; STATISTICAL-ANALYSIS; SEQUENCE; PARAMETERS; ALGORITHMS	Recent research on structure and motion recovery has focused on issues related to sensitivity and robustness of existing techniques. One possible reason is that in practical applications, the underlying assumptions made by existing algorithms are often violated. In this paper, we propose a framework for 3D reconstruction from short monocular video sequences taking into account the statistical errors in reconstruction algorithms. Detailed error analysis is especially important for this problem because the motion between pairs of frames is small and slight perturbations in its estimates can lead to large errors in 3D reconstruction. We focus on the following issues: physical sources of errors, their experimental and theoretical analysis, robust estimation techniques and measures for characterizing the quality of the final reconstruction. We derive a precise relationship between the error in the reconstruction and the error in the image correspondences. The error analysis is used to design a robust, recursive multi-frame fusion algorithm using "stochastic approximation" as the framework since it is capable of dealing with incomplete information about errors in observations. Rate-distortion analysis is proposed for evaluating the quality of the final reconstruction as a function of the number of frames and the error in the image correspondences. Finally, to demonstrate the effectiveness of the algorithm, examples of depth reconstruction are shown for different video sequences.	Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA; Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Chowdhury, AKR (corresponding author), Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA.	amitrc@cfar.umd.edu; rama@cfar.umd.edu	Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/AAJ-1504-2020	Roy-Chowdhury, Amit/0000-0001-6690-9725				AZARBAYEJANI A, 1995, IEEE T PATTERN ANAL, V17, P562, DOI 10.1109/34.387503; Benveniste A., 1987, ADAPTIVE ALGORITHMS; Black MJ, 1996, INT J COMPUT VISION, V19, P57, DOI 10.1007/BF00131148; BROIDA T, 1985, THESIS; BROIDA TJ, 1991, IEEE T PATTERN ANAL, V13, P497, DOI 10.1109/34.87338; BROIDA TJ, 1990, IEEE T AERO ELEC SYS, V26, P639, DOI 10.1109/7.55557; BROIDA TJ, 1989, J OPT SOC AM A, V6, P879, DOI 10.1364/JOSAA.6.000879; Cho K, 1997, IEEE T PATTERN ANAL, V19, P1185, DOI 10.1109/34.632979; CHOWDHURY AR, 2003, CVPR WORKSH STAT AN; CHOWDHURY AR, 2002, THESIS U MARYLAND CO; CHOWDHURY AR, 2002, INT C AC SPEECH SIGN; CHOWDHURY AR, 2003, IN PRESS COMPUTER VI; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DANIILIDIS K, 1990, IMAGE VISION COMPUT, V8, P297, DOI 10.1016/0262-8856(90)80006-F; DANIILIDIS K, 1993, VISNAV93; DANIILIDIS K, 1993, CVPR 93, P188; Efron B., 1994, MONOGR STAT APPL PRO, DOI DOI 10.1007/978-1-4899-4541-9; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FERMULLER C, 2001, FDN IMAGE UNDERSTAND, pCH14; Fessler JA, 1996, IEEE T IMAGE PROCESS, V5, P493, DOI 10.1109/83.491322; Fua P, 2000, INT J COMPUT VISION, V38, P153, DOI 10.1023/A:1008105802790; GENNERY DB, 1992, INT J COMPUT VISION, V7, P243, DOI 10.1007/BF00126395; Golub G. H., 1996, MATRIX COMPUTATIONS; Goodman I., 2013, MATH DATA FUSION, V37; HARALICK R, 1996, ECCV WORKSH PERF CHA; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; KANATANI K, 1993, IEEE T PATTERN ANAL, V15, P37, DOI 10.1109/34.184773; Kanatani K., 1996, STAT OPTIMIZATION GE; Ljung L., 1987, THEORY PRACTICE RECU; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Ma Y, 2000, INT J COMPUT VISION, V36, P71, DOI 10.1023/A:1008124507881; MEER P, 1992, C COMP VIS PATT REC, P621; MORRIS D, 2000, 3D MODEL ACCURACY GA; Nalwa V. S., 1993, GUIDED TOUR COMPUTER; Oliensis J, 1999, INT J COMPUT VISION, V34, P163, DOI 10.1023/A:1008139920864; Oliensis J, 2001, IEEE T PATTERN ANAL, V23, P546, DOI 10.1109/34.927457; Oliensis J., 2000, CRITIQUE STRUCTURE M; Papoulis A., 1991, COMMUNICATIONS SIGNA, V3; Poor H. V., 1988, INTRO SIGNAL DETECTI; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718; SARIDIS G, 1974, IEEE T AUTOMATIC CON, V19; Shan Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P644, DOI 10.1109/ICCV.2001.937687; Shao J., 1998, MATH STAT; Soatto S, 1998, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.1998.698621; SPALL J, 2000, INTRO STOCHASTIC SEA; Srinivasan S, 2000, INT J COMPUT VISION, V37, P203, DOI 10.1023/A:1008111923880; Sun ZH, 2001, COMPUT VIS IMAGE UND, V82, P110, DOI 10.1006/cviu.2001.0910; Szeliski R., 1994, Journal of Visual Communication and Image Representation, V5, P10, DOI 10.1006/jvci.1994.1002; Thomas JI, 1999, COMPUT VIS IMAGE UND, V76, P109, DOI 10.1006/cviu.1999.0779; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Triggs B., 2000, VISION ALGORITHMS TH; TSAI RY, 1981, IEEE T ACOUST SPEECH, V29, P1147, DOI 10.1109/TASSP.1981.1163710; Walter R., 1976, PRINCIPLES MATH ANAL; WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074; WENG JY, 1987, IEEE T PATTERN ANAL, V9, P370, DOI 10.1109/TPAMI.1987.4767920; WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779; YOUNG GSJ, 1990, IEEE T PATTERN ANAL, V12, P735, DOI 10.1109/34.57666; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903; ZHANG Z, 1992, 3D DYNAMIC SCENE ANA; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561	62	15	15	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2003	55	1					27	53		10.1023/A:1024488407740	http://dx.doi.org/10.1023/A:1024488407740			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	694TM					2022-12-18	WOS:000183790500002
J	Aggarwal, M; Ahuja, N				Aggarwal, M; Ahuja, N			A pupil-centric model of image formation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image model; pin-hole; thin-lens; Gaussian thick-lens; aperture position; camera design; depth from (de)focus; entrance end exit pupils	VISION	This paper has been prompted by observations of some anomalies in the performance of the standard imaging models (pin-hole, thin-lens and Gaussian thick-lens), in the context of composing omnifocus images and estimating depth maps from a sequence of images. A closer examination of the models revealed that they assume a position of the aperture that conflicts with the designs of many available lenses. We have shown in this paper that the imaging geometry and photometric properties of an image are significantly influenced by the position of the aperture. This is confirmed by the discrepancies between observed mappings and those predicted by the models. We have therefore concluded that the current imaging models do not adequately represent practical imaging systems. We have proposed a pupil-centric model of image formation, which overcomes these deficiencies and have given the associated mappings. The impact of this model on some common imaging scenarios is described, along with experimental verification of the better performance of the model on three real lenses.	Sarnoff Corp, Princeton, NJ 08540 USA; Univ Illinois, Urbana, IL 61801 USA	Sarnoff Corporation; University of Illinois System; University of Illinois Urbana-Champaign	Aggarwal, M (corresponding author), Sarnoff Corp, 201 Washington Rd, Princeton, NJ 08540 USA.	maggarwal@sarnoff.com; ahuja@vision.ai.uiuc.edu						Aggarwal M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P472, DOI 10.1109/ICCV.2001.937554; Aggarwal M, 2000, INT C PATT RECOG, P876, DOI 10.1109/ICPR.2000.905559; CASTANO A, 1998, THESIS U ILLINOIS UR; FAUGERAS O, 1995, J OPT SOC AM A, V12, P465, DOI 10.1364/JOSAA.12.000465; FORSYTH D, 2000, UNPUB COMPUTER VISIO; Hecht E., 1974, OPTICS; Horn B., 1986, ROBOT VISION, P1; Kingslake R., 1983, OPTICAL SYSTEM DESIG; KINGSLAKE R, 1978, LENS DESIGN FUNDAMAN; KOLB C, 1995, P 22 ANN C COMP GRAP, P317; Krishnan A, 1996, INT J COMPUT VISION, V20, P169, DOI 10.1007/BF00208718; KRISHNAN A, 1997, THESIS U ILLINOIS UR; Smith W.J., 1992, OPTICAL ELECTRO OPTI; TRIGGS B, 1998, EUR C COMP VIS, V1, P89; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; TSAI RY, 1986, CVPR, P364; Watanabe M, 1997, IEEE T PATTERN ANAL, V19, P1360, DOI 10.1109/34.643894; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	18	15	16	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	2002	48	3					195	214		10.1023/A:1016324132583	http://dx.doi.org/10.1023/A:1016324132583			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	573BJ					2022-12-18	WOS:000176809200003
J	Peer, P; Solina, F				Peer, P; Solina, F			Panoramic depth imaging: Single standard camera approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						stereo vision; reconstruction; panoramic image; mosaicing		In this paper we present a panoramic depth imaging system. The system is mosaic-based which means that we use a single rotating camera and assemble the captured images in a mosaic. Due to a setoff of the camera's optical center from the rotational center of the system we are able to capture the motion parallax effect which enables stereo reconstruction. The camera is rotating on a circular path with a step defined by the angle, equivalent to one pixel column of the captured image. The equation for depth estimation can be easily extracted from the system geometry. To find the corresponding points on a stereo pair of panoramic images the epipolar geometry needs to be determined. It can be shown that the epipolar geometry is very simple if we are doing the reconstruction based on a symmetric pair of stereo panoramic images. We get a symmetric pair of stereo panoramic images when we take symmetric pixel columns on the left and on the right side from the captured image center column. Epipolar lines of the symmetrical pair of panoramic images are image rows. The search space on the epipolar line can be additionaly constrained. The focus of the paper is mainly on the system analysis. Results of the stereo reconstruction procedure and quality evaluation of generated depth images are quite promissing. The system performs well for reconstruction of small indoor spaces. Our finall goal is to develop a system for automatic navigation of a mobile robot in a room.	Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana 1001, Slovenia	University of Ljubljana	Peer, P (corresponding author), Univ Ljubljana, Fac Comp & Informat Sci, Trzaska 25, Ljubljana 1001, Slovenia.	peter.peer@fri.uni-lj.si; franc.solina@fri.uni-lj.si	Peer, Peter/A-2653-2008; Solina, Franc/F-7411-2011	Peer, Peter/0000-0001-9744-4035; Solina, Franc/0000-0002-9268-6825				Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Gupta R, 1997, IEEE T PATTERN ANAL, V19, P963, DOI 10.1109/34.615446; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Heung-Yeung Shum, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P14, DOI 10.1109/ICCV.1999.791191; Huang F, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P560, DOI 10.1109/ICCV.2001.937566; HUANG F, 2000, CTUCMP200007 CZECH T; ISHIGURO H, 1992, IEEE T PATTERN ANAL, V14, P257, DOI 10.1109/34.121792; PAAR G, 1992, IEEE INT C PATT REC, V1, P738; Peleg S, 2001, IEEE T PATTERN ANAL, V23, P279, DOI 10.1109/34.910880; Peleg S, 2000, IEEE T PATTERN ANAL, V22, P1144, DOI 10.1109/34.879794; Peleg S, 2000, PROC CVPR IEEE, P208, DOI 10.1109/CVPR.2000.855821; Peleg S., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P395, DOI 10.1109/CVPR.1999.786969; Prihavec B, 1998, J NETW COMPUT APPL, V21, P219, DOI 10.1006/ma990074; Rademacher P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P199, DOI 10.1145/280814.280871; Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251, DOI 10.1145/258734.258861; Wood D. N., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P243, DOI 10.1145/258734.258859	17	15	16	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					149	160		10.1023/A:1014541807682	http://dx.doi.org/10.1023/A:1014541807682			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN					2022-12-18	WOS:000174354700011
J	Cheong, LF; Xiang, T				Cheong, LF; Xiang, T			Characterizing depth distortion under different generic motions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; visual perception; uncalibrated motion analysis; depth distortion; shape representation	RECOVERING 3-D MOTION; INHERENT AMBIGUITIES; GEOMETRY	Given that errors in the estimates for the intrinsic and extrinsic camera parameters are inevitable, it is important to understand the behaviour of the resultant distortion in depth recovered under different motion-scene configurations. The main interest in this study is to look for generic motion type that can render depth recovery more robust and reliable. To this end, lateral and forward motions are compared both under calibrated and uncalibrated scenarios. For lateral motion, we found that although Euclidean reconstruction is difficult, ordinal depth information is obtainable; while for forward motion, depth information (even partial one) is difficult to recover. In the uncalibrated case, with fixed intrinsic parameters, the preceding statements still hold. However, if intrinsic parameter variations are allowed, then for lateral motion, depth relief can only be preserved locally. In general, lateral motion yields a distortion relationship that belongs to the projective transformation of a very simple type, while the distortion transformations for general motions including forward motion belong to the Cremona transformation. As an aside, we also provide an analysis of the distortion in the depth recovered using the least square procedure as compared to the epipolar reconstruction approach.	Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119260, Singapore	National University of Singapore	Cheong, LF (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, 10 Kent Ridge Crescent, Singapore 119260, Singapore.	eleclf@nus.edu.sg						ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; Bougnoux S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P790, DOI 10.1109/ICCV.1998.710808; CHAUMETTE F, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P347, DOI 10.1109/CVPR.1994.323850; Cheong L, 1998, COMPUT VIS IMAGE UND, V71, P356, DOI 10.1006/cviu.1997.0649; Cheong LF, 1999, INT J COMPUT VISION, V32, P195, DOI 10.1023/A:1008105012585; CHEONG LF, 2000, IN PRESS 6 EUR C COM; Coombs D., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P440, DOI 10.1109/CVPR.1993.341093; DANIILIDIS K, 1995, VISUAL NAVIGATION BI; Darrell T., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P173, DOI 10.1109/WVM.1991.212810; Dutta R., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P106, DOI 10.1109/ICCV.1990.139504; FERMULLER C, 1995, INT J COMPUT VISION, V14, P147, DOI 10.1007/BF01418980; Grossmann E, 2000, IMAGE VISION COMPUT, V18, P685, DOI 10.1016/S0262-8856(99)00072-4; HORN BKP, 1987, INT J COMPUT VISION, V1, P259, DOI 10.1007/BF00127824; KAHL F, 1995, P INT C COMP VIS, P469; KOENDERINK JJ, 1995, IMAGE VISION COMPUT, V13, P321, DOI 10.1016/0262-8856(95)99719-H; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LUCAS BD, 1984, THESIS CARNEGIEMELLO; NEGAHDARIPOUR S, 1989, INT J COMPUT VISION, V3, P293, DOI 10.1007/BF00132601; Oliensis J., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P185, DOI 10.1109/CVPR.1999.786937; Oliensis J., 2000, CRITIQUE STRUCTURE M; Santos-Victor J., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P434, DOI 10.1109/CVPR.1993.341094; Soatto S, 1998, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.1998.698621; Sturm P, 1997, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.1997.609467; Szeliski R, 1997, IEEE T PATTERN ANAL, V19, P506, DOI 10.1109/34.589211; THOMAS JI, 1993, P DARPA IM UND WORKS, P691; TODD JT, 1989, PSYCHOL REV, V96, P643, DOI 10.1037/0033-295X.96.4.643; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; Weng J., 1991, MOTION STRUCTURE IMA; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903	29	15	15	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP-OCT	2001	44	3					199	217		10.1023/A:1012224215211	http://dx.doi.org/10.1023/A:1012224215211			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	496NM					2022-12-18	WOS:000172402900003
