PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
J	Ru, LX; Du, B; Zhan, YB; Wu, C				Ru, Lixiang; Du, Bo; Zhan, Yibing; Wu, Chen			Weakly-Supervised Semantic Segmentation with Visual Words Learning and Hybrid Pooling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Weakly-supervised semantic segmentation; Visual words learning; Hybrid pooling; Semantic segmentation	NEURAL-NETWORK	Weakly-supervised semantic segmentation (WSSS) methods with image-level labels generally train a classification network to generate the Class Activation Maps (CAMs) as the initial coarse segmentation labels. However, current WSSS methods still perform far from satisfactorily because their adopted CAMs (1) typically focus on partial discriminative object regions and (2) usually contain useless background regions. These two problems are attributed to the sole image-level supervision and aggregation of global information when training the classification networks. In this work, we propose the visual words learning module and hybrid pooling approach, and incorporate them in classification network to mitigate the above problems. In visual words learning module, we counter the first problem by enforcing the classification network to learn fine-grained visual word labels so that more object extents could be discovered. Specifically, the visual words are learned with a codebook, which could be updated via two proposed strategies, i.e. learning-based strategy and memory-bank strategy. The second drawback of CAMs is alleviated with the proposed hybrid pooling, which incorporates the global average and local discriminative information to simultaneously ensure object completeness and reduce background regions. We evaluated our methods on PASCAL VOC 2012 and MS COCO 2014 datasets. Without any extra saliency prior, our method achieved 70.6% and 70.7% mIoU on the val and test set of PASCAL VOC dataset, respectively, and 36.2% mIoU on the val set of MS COCO dataset, which significantly surpassed the performance of state-of-the-art WSSS methods.	[Ru, Lixiang; Du, Bo] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Inst Artificial Intelligence Sch Comp Sci, Wuhan, Peoples R China; [Ru, Lixiang; Du, Bo] Wuhan Univ, Hubei Key Lab Multimedia & Network Commun Engn, Wuhan, Peoples R China; [Zhan, Yibing] JD Explore Acad, JDcom, Beijing, Peoples R China; [Wu, Chen] Wuhan Univ, LIESMARS, Wuhan, Peoples R China	Wuhan University; Wuhan University; Wuhan University	Du, B (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Inst Artificial Intelligence Sch Comp Sci, Wuhan, Peoples R China.; Du, B (corresponding author), Wuhan Univ, Hubei Key Lab Multimedia & Network Commun Engn, Wuhan, Peoples R China.	rulixiang@whu.edu.cn; dubo@whu.edu.cn; zhanyibing@jd.com; chen.wu@whu.edu.cn		Wu, Chen/0000-0001-6461-8377	National Natural Science Foundation of China [62141112, T2122014, 41871243, 61971317]; Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies) [2019AEA170]; Natural Science Foundation of Hubei Province [2020CFB594]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies); Natural Science Foundation of Hubei Province(Natural Science Foundation of Hubei Province)	This work was supported in part by the National Natural Science Foundation of China under Grants 62141112, T2122014, 41871243 and 61971317, the Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies) under Grant 2019AEA170, and Natural Science Foundation of Hubei Province under Grant 2020CFB594.	ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913; Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231; Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523; Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Araslanov N, 2020, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR42600.2020.00431; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Chang Y. T., 2020, BRIT MACH VIS C BMVC; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cogswell M., 2017, INT C LEARN REPR; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Di Lin, 2016, Arxiv, DOI arXiv:1604.05144; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; Gidaris S, 2020, PROC CVPR IEEE, P6926, DOI 10.1109/CVPR42600.2020.00696; Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563; Hou QB, 2018, ADV NEUR IN, V31; Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733; Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216; Jo S., 2021, 2021 IEEE INT C IM P, P639; Ke Tsung-Wei, 2021, INT C LEARN REPR; Kim B, 2021, AAAI CONF ARTIF INTE, V35, P1754; Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee J, 2021, PROC CVPR IEEE, P2643; Lee Jungbeom, 2021, P IEEE CVF C COMP VI, P4071; Lee S, 2021, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR46437.2021.00545; Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1984; Li Yi, 2021, P IEEE CVF INT C COM, P6964; Lin H., 2019, AMBIENT TEMPERATURE; Lin M, 2013, 13124400 ARXIV; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z; Liu Y, 2022, IEEE T PATTERN ANAL, V44, P1415, DOI 10.1109/TPAMI.2020.3023152; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Oh Y, 2021, PROC CVPR IEEE, P6909, DOI 10.1109/CVPR46437.2021.00684; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Passalis N, 2017, IEEE I CONF COMP VIS, P5766, DOI 10.1109/ICCV.2017.614; Paszke A, 2019, ADV NEUR IN, V32; Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780; Roy A, 2017, PROC CVPR IEEE, P7282, DOI 10.1109/CVPR.2017.770; Ru L., 2021, INT JOINT C ART INT; Rubin Donald B, 2019, BIOSTAT EPIDEMIOL, V3, P140, DOI DOI 10.1080/24709360.2019.1670513; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Sculley D., 2010, P 19 INT C WORLD WID, P1177, DOI [10.1145/1772690.1772862, DOI 10.1145/1772690.1772862]; Song CF, 2019, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2019.00325; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315; Wang X, 2020, INT J COMPUT VISION, V128, P1736, DOI 10.1007/s11263-020-01293-3; Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687; Wu T, 2021, PROC CVPR IEEE, P16760, DOI 10.1109/CVPR46437.2021.01649; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Xu Lian, 2021, P IEEE CVF INT C COM, P6984; Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265; Yu-Ting Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8988, DOI 10.1109/CVPR42600.2020.00901; Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229; Zhang BF, 2020, AAAI CONF ARTIF INTE, V34, P12765; Zhang Dong, 2020, ADV NEURAL INFORM PR, P655; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhuang CX, 2019, IEEE I CONF COMP VIS, P6001, DOI 10.1109/ICCV.2019.00610	67	0	0	5	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					1127	1144		10.1007/s11263-022-01586-9	http://dx.doi.org/10.1007/s11263-022-01586-9		MAR 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ		Green Submitted			2022-12-18	WOS:000766416300001
J	Zins, M; Simon, G; Berger, MO				Zins, Matthieu; Simon, Gilles; Berger, Marie-Odile			Object-Based Visual Camera Pose Estimation From Ellipsoidal Model and 3D-Aware Ellipse Prediction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual Localization; Pose from objects; Ellipse prediction; Ellipsoidal model		In this paper, we propose a method for initial camera pose estimation from just a single image which is robust to viewing conditions and does not require a detailed model of the scene. This method meets the growing need of easy deployment of robotics or augmented reality applications in any environments, especially those for which no accurate 3D model nor huge amount of ground truth data are available. It exploits the ability of deep learning techniques to reliably detect objects regardless of viewing conditions. Previous works have also shown that abstracting the geometry of a scene of objects by an ellipsoid cloud allows to compute the camera pose accurately enough for various application needs. Though promising, these approaches use the ellipses fitted to the detection bounding boxes as an approximation of the imaged objects. In this paper, we go one step further and propose a learning-based method which detects improved elliptic approximations of objects which are coherent with the 3D ellipsoids in terms of perspective projection. Experiments prove that the accuracy of the computed pose significantly increases thanks to our method. This is achieved with very little effort in terms of training data acquisition-a few hundred calibrated images of which only three need manual object annotation. Code and models are released at https://gitlabinria.fritangram/3d-aware-ellipses- for-visual-localization.	[Zins, Matthieu; Simon, Gilles; Berger, Marie-Odile] Univ Lorraine, CNRS, LORIA, Inria, Nancy, France	Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Lorraine	Zins, M (corresponding author), Univ Lorraine, CNRS, LORIA, Inria, Nancy, France.	matthieu.zins@inria.fr; gilles.simon@loria.fr; marie-odile.berger@inria.fr		Simon, Gilles/0000-0002-5909-6524; Zins, Matthieu/0000-0001-6238-4271				Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38; Berger M.-O, 2019, 18 IEEE INT S MIX AU, P19; Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489; Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267; Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366; Bui M., 2018, BRIT MACH VIS C 2018, P3; Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Delhumeau Jonathan, 2013, ACM MM, P653, DOI DOI 10.1145/2502081.2502171; DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060; Dong WB, 2021, IEEE T IMAGE PROCESS, V30, P2193, DOI 10.1109/TIP.2021.3050673; Gaudilliere V, 2020, IEEE ROBOT AUTOM LET, V5, P5189, DOI 10.1109/LRA.2020.3005387; Hinterstoisser Stefan, 2012, P AS C COMP VIS, P2, DOI DOI 10.1007/978-3-642-37331-2_42; Hodan T, 2017, IEEE WINT CONF APPL, P880, DOI 10.1109/WACV.2017.103; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Kaiming He, 2020, IEEE Transactions on Pattern Analysis and Machine Intelligence, V42, P386, DOI 10.1109/TPAMI.2018.2844175; Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169; Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694; Kendall A, 2016, IEEE INT CONF ROBOT, P4762, DOI 10.1109/ICRA.2016.7487679; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; King DB, 2015, ACS SYM SER, V1214, P1; Li YP, 2012, LECT NOTES COMPUT SC, V7572, P15, DOI 10.1007/978-3-642-33718-5_2; Li ZG, 2019, IEEE I CONF COMP VIS, P7677, DOI 10.1109/ICCV.2019.00777; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Melekhov I, 2017, IEEE INT CONF COMP V, P870, DOI 10.1109/ICCVW.2017.107; Mousavian A., 2017, PROC CVPR IEEE, P7074, DOI DOI 10.1109/CVPR.2017.597; Nicholson L, 2019, IEEE ROBOT AUTOM LET, V4, P1, DOI 10.1109/LRA.2018.2866205; Nister David, 2006, CVPR, P2161, DOI DOI 10.1109/CVPR.2006.264; Pan SY, 2021, IEEE WINT CONF APPL, P3891, DOI 10.1109/WACV48630.2021.00394; Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776; Paschalidou D, 2019, PROC CVPR IEEE, P10336, DOI 10.1109/CVPR.2019.01059; Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469; PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009; Piasco N., 2019, 30 BRIT MACH VIS C 2, P14; Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rosenhahn B, 2006, LECT NOTES COMPUT SC, V4040, P263; Rubino C, 2018, IEEE T PATTERN ANAL, V40, P1281, DOI 10.1109/TPAMI.2017.2701373; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499; Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342; Sattler T, 2017, IEEE T PATTERN ANAL, V39, P1744, DOI 10.1109/TPAMI.2016.2611662; Sattler T, 2012, LECT NOTES COMPUT SC, V7572, P752, DOI 10.1007/978-3-642-33718-5_54; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43; Taira H, 2018, PROC CVPR IEEE, P7199, DOI 10.1109/CVPR.2018.00752; Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038; Walch F, 2017, IEEE I CONF COMP VIS, P627, DOI 10.1109/ICCV.2017.75; Wang H, 2019, PROC CVPR IEEE, P2637, DOI 10.1109/CVPR.2019.00275; Weinzaepfel P, 2019, PROC CVPR IEEE, P5617, DOI 10.1109/CVPR.2019.00578; Yang C, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20113045; Yang SC, 2019, IEEE T ROBOT, V35, P925, DOI 10.1109/TRO.2019.2909168; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Zakharov S, 2019, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2019.00203; Zins M., 2020, INT VIRT C 3D VIS 3D	58	0	0	3	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					1107	1126		10.1007/s11263-022-01585-w	http://dx.doi.org/10.1007/s11263-022-01585-w		MAR 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ		Green Submitted			2022-12-18	WOS:000765665600001
J	Shao, R; Perera, P; Yuen, PC; Patel, VM				Shao, Rui; Perera, Pramuditha; Yuen, Pong C.; Patel, Vishal M.			Open-Set Adversarial Defense with Clean-Adversarial Mutual Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Adversarial defense; Open-set recognition; Feature denoising; Mutual learning		Open-set recognition and adversarial defense study two key aspects of deep learning that are vital for real-world deployment. The objective of open-set recognition is to identify samples from open-set classes during testing, while adversarial defense aims to robustify the network against images perturbed by imperceptible adversarial noise. This paper demonstrates that open-set recognition systems are vulnerable to adversarial samples. Furthermore, this paper shows that adversarial defense mechanisms trained on known classes are unable to generalize well to open-set samples. Motivated by these observations, we emphasize the necessity of an Open-Set Adversarial Defense (OSAD) mechanism. This paper proposes an Open-Set Defense Network with Clean-Adversarial Mutual Learning (OSDN-CAML) as a solution to the OSAD problem. The proposed network designs an encoder with dual-attentive feature-denoising layers coupled with a classifier to learn a noise-free latent feature representation, which adaptively removes adversarial noise guided by channel and spatial-wise attentive filters. Several techniques are exploited to learn a noise-free and informative latent feature space with the aim of improving the performance of adversarial defense and open-set recognition. First, we incorporate a decoder to ensure that clean images can be well reconstructed from the obtained latent features. Then, self-supervision is used to ensure that the latent features are informative enough to carry out an auxiliary task. Finally, to exploit more complementary knowledge from clean image classification to facilitate feature denoising and search for a more generalized local minimum for open-set recognition, we further propose clean-adversarial mutual learning, where a peer network (classifying clean images) is further introduced to mutually learn with the classifier (classifying adversarial images). We propose a testing protocol to evaluate OSAD performance and show the effectiveness of the proposed method on white-box attacks, black-box attacks, as well as the rectangular occlusion attack in multiple object classification datasets.	[Shao, Rui; Yuen, Pong C.] Hong Kong Baptist Univ, Hong Kong, Peoples R China; [Perera, Pramuditha] AWS AI Labs, Seattle, WA USA; [Patel, Vishal M.] Johns Hopkins Univ, Baltimore, MD USA	Hong Kong Baptist University; Johns Hopkins University	Yuen, PC (corresponding author), Hong Kong Baptist Univ, Hong Kong, Peoples R China.	ruishao@comp.hkbu.edu.hk; pramudi@amazon.com; pcyuen@comp.hkbu.edu.hk; vpatel36@jhu.edu			Research Grants Council, Hong Kong [RGC/HKBU12200820]; ARO [W911NF-21-1-0135]	Research Grants Council, Hong Kong(Hong Kong Research Grants Council); ARO	This work is partially supported by Research Grants Council (RGC/HKBU12200820), Hong Kong. Vishal M. Patel was supported by an ARO Grant W911NF-21-1-0135.	Alex Krizhevsky V. N., 2010, CIFAR 10 CANADIAN I; Baweja Y, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020); Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Evtimov I., 2018, CVPR; Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175; Ge ZongYuan, 2017, BMVC; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gupta P, 2019, IEEE I CONF COMP VIS, P6707, DOI 10.1109/ICCV.2019.00681; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hendrycks D, 2019, ADV NEUR IN, V32; Jang Y, 2019, IEEE I CONF COMP VIS, P2740, DOI 10.1109/ICCV.2019.00283; Kingma D.P, P 3 INT C LEARNING R; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618; Liang S, 2017, AEBMR ADV ECON, V34, P1; Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191; Liu Yanpei, 2017, ICLR; Madry Aleksander, 2017, ARXIV; Neal L, 2018, LECT NOTES COMPUT SC, V11210, P620, DOI 10.1007/978-3-030-01231-1_38; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Oza P., 2020, ECCV; Oza P, 2019, PROC CVPR IEEE, P2302, DOI 10.1109/CVPR.2019.00241; Oza P, 2019, IEEE SIGNAL PROC LET, V26, P277, DOI 10.1109/LSP.2018.2889273; Perera P, 2019, PROC CVPR IEEE, P2893, DOI 10.1109/CVPR.2019.00301; Perera P, 2019, PROC CVPR IEEE, P11536, DOI 10.1109/CVPR.2019.01181; Perera P, 2019, IEEE T IMAGE PROCESS, V28, P5450, DOI 10.1109/TIP.2019.2917862; Perera Pramuditha, 2020, CVPR; Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256; Shao R., 2020, ECCV; Shao R, 2020, AAAI CONF ARTIF INTE, V34, P11974; Shao R, 2019, PROC CVPR IEEE, P10015, DOI 10.1109/CVPR.2019.01026; Shao R, 2019, IEEE T INF FOREN SEC, V14, P923, DOI 10.1109/TIFS.2018.2868230; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Woo S, 2018, ADV NEUR IN, V31; Wu Tong, 2020, ICLR; Xie Chulin, 2020, ICLR; Xie C, 2020, PROC CVPR IEEE, P816, DOI 10.1109/CVPR42600.2020.00090; Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059; Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775; Ye M, 2022, IEEE T PATTERN ANAL, V44, P924, DOI 10.1109/TPAMI.2020.3013379; Ye Mang, 2019, CVPR, P6210; Yoshihashi R, 2019, PROC CVPR IEEE, P4011, DOI 10.1109/CVPR.2019.00414; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang H, 2017, IEEE T PATTERN ANAL, V39, P1690, DOI 10.1109/TPAMI.2016.2613924; Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454	57	0	0	10	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					1070	1087		10.1007/s11263-022-01581-0	http://dx.doi.org/10.1007/s11263-022-01581-0		MAR 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ		Green Submitted			2022-12-18	WOS:000764971600001
J	Fazlali, H; Shirani, S; BradforSd, M; Kirubarajan, T				Fazlali, Hamidreza; Shirani, Shahram; BradforSd, Michael; Kirubarajan, Thia			Atmospheric Turbulence Removal in Long-Range Imaging Using a Data-Driven-Based Approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Atmospheric turbulence; Long-range imaging; Autoencoder; Deep learning; K-means; Binary search; Fusion; Saliency map	TO-NOISE-RATIO; VIDEO STABILIZATION	Atmospheric turbulence is one of the causes of quality degradation in long-range imaging and its removal from degraded frame sequences is considered an ill-posed problem. There have been numerous attempts to address this problem. However, there is still room for improving the quality of the restored images. In contrast to the previous approaches to address this problem, in this paper, we propose a data-driven approach. First, an end-to-end deep convolutional autoencoder is trained using natural images and its encoder part is exploited to extract high-level features from all the frames in a sequence that are distorted by atmospheric turbulence. Then, the binary search algorithm and the unsupervised K-means clustering method are jointly exploited to analyze these high-level features to find the best set of frames that can help accurately reconstruct the original high-quality image. After removing the geometric distortion from the selected frames, the saliency map of the average set of the selected frames is calculated and used with the original selected frames to train an end-to-end multi-scale saliency-guided deep convolutional autoencoder network to fuse the registered frames. This network uses different scales of the input frames and their saliency maps for better fusion performance. Specifically, the fusion network learns how to fuse these sets of frames and also exploit information from their saliency map to generate an image with more details of the scene. Finally, this fused image is post-processed to boost the visual quality of the output fused image. The experimental results on both synthetically and naturally distorted sequences show the superiority of our method compared to the state-of-the-art methods.	[Fazlali, Hamidreza; Shirani, Shahram; BradforSd, Michael; Kirubarajan, Thia] McMaster Univ, ITB A-111,1280 Main St West, Hamilton, ON L8S 4K1, Canada	McMaster University	Fazlali, H (corresponding author), McMaster Univ, ITB A-111,1280 Main St West, Hamilton, ON L8S 4K1, Canada.	fazlalih@mcmaster.ca						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Aittala M, 2018, LECT NOTES COMPUT SC, V11212, P748, DOI 10.1007/978-3-030-01237-3_45; Anantrasirichai N, 2013, IEEE T IMAGE PROCESS, V22, P2398, DOI 10.1109/TIP.2013.2249078; Aubailly M., 2009, P SPIE, V7463; Aubailly M, 2008, PROC SPIE, V7090, DOI 10.1117/12.799874; Baig MH, 2017, ADV NEUR IN, V30; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681; Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820; Chen GP, 2020, APPL SOFT COMPUT, V89, DOI 10.1016/j.asoc.2020.106131; Chen HZ, 2012, IEEE T IMAGE PROCESS, V21, P262, DOI 10.1109/TIP.2011.2160958; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; FRIED DL, 1978, J OPT SOC AM, V68, P1651, DOI 10.1364/JOSA.68.001651; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Gao Junyu, 2019, ARXIV PREPRINT ARXIV; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gonzalez R.C., 2002, DIGITAL IMAGE PROCES, V455; Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940; Halder KK, 2015, OPT EXPRESS, V23, P5091, DOI 10.1364/OE.23.005091; Harmeling S, 2009, IEEE INT C COMPUTATI, P1, DOI DOI 10.1109/ICCPHOT.2009.5559014; John S, 2005, IEEE T IMAGE PROCESS, V14, P577, DOI 10.1109/TIP.2005.846022; Kim JS, 2014, IEEE T CIRC SYST VID, V24, P198, DOI 10.1109/TCSVT.2013.2270366; Kim TH, 2017, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2017.435; Kingma D.P, P 3 INT C LEARNING R; Lau CP, 2020, IEEE INT CONF AUTOMA, P32, DOI 10.1109/FG47880.2020.00012; Lau CP, 2019, INVERSE PROBL, V35, DOI 10.1088/1361-6420/ab0e4b; Li BY, 2018, AAAI CONF ARTIF INTE, P7016; Lou YF, 2013, INVERSE PROBL IMAG, V7, P839, DOI 10.3934/ipi.2013.7.839; MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708; Noroozi M, 2017, LECT NOTES COMPUT SC, V10496, P65, DOI 10.1007/978-3-319-66709-6_6; PEARSON JE, 1976, APPL OPTICS, V15, P622, DOI 10.1364/AO.15.000622; ROGGEMANN MC, 1994, OPT ENG, V33, P3254, DOI 10.1117/12.181250; Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; Shimizu M, 2008, PROC CVPR IEEE, P1420; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tyson R. K, 2000, INTRO ADAPTIVE OPTIC, V41; Wang MH, 2018, MULTIMED TOOLS APPL, V77, P25905, DOI 10.1007/s11042-018-5825-8; Wang W, 2010, PROC CVPR IEEE, P2368, DOI 10.1109/CVPR.2010.5539927; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xie Y, 2016, IEEE T IMAGE PROCESS, V25, P4943, DOI 10.1109/TIP.2016.2598638; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xue BD, 2016, OPT EXPRESS, V24, P28092, DOI 10.1364/OE.24.028092; Yan RM, 2016, IEEE T IMAGE PROCESS, V25, P1910, DOI 10.1109/TIP.2016.2535273; Yitzhaky Y, 1997, OPT ENG, V36, P3064, DOI 10.1117/1.601526; Zhang C, 2018, IEEE ACCESS, V6, P75855, DOI 10.1109/ACCESS.2018.2883489; Zhu X, 2013, IEEE T PATTERN ANAL, V35, P157, DOI 10.1109/TPAMI.2012.82; Zhu X, 2010, PROC SPIE, V7543, DOI 10.1117/12.840127; Zhu Xin, 2011, China Vegetables, P1	52	0	0	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					1031	1049		10.1007/s11263-022-01584-x	http://dx.doi.org/10.1007/s11263-022-01584-x		FEB 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ					2022-12-18	WOS:000762134200001
J	Kataoka, H; Okayasu, K; Matsumoto, A; Yamagata, E; Yamada, R; Inoue, N; Nakamura, A; Satoh, Y				Kataoka, Hirokatsu; Okayasu, Kazushige; Matsumoto, Asato; Yamagata, Eisuke; Yamada, Ryosuke; Inoue, Nakamasa; Nakamura, Akio; Satoh, Yutaka			Pre-Training Without Natural Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Formula-driven supervised learning; Image recognition; Representation learning	LACUNARITY; DIMENSIONS; OBJECT	Is it possible to use convolutional neural networks pre-trained without any natural images to assist natural image understanding? The paper proposes a novel concept, Formula-driven Supervised Learning (FDSL). We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinitely large dataset of labeled images. The proposed framework is similar yet different from Self-Supervised Learning because the FDSL framework enables the creation of image patterns based on any mathematical formulas in addition to self-generated labels. Further, unlike pre-training with a synthetic image dataset, a dataset under the framework of FDSL is not required to define object categories, surface texture, lighting conditions, and camera viewpoint. In the experimental section, we find a better dataset configuration through an exploratory study, e.g., increase of #category/#instance, patch rendering, image coloring, and training epoch. Although models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, do not necessarily outperform models pre-trained with human annotated datasets in all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models. The FractalDB pre-trained CNN also outperforms other pre-trained models on auto-generated datasets based on FDSL such as Bezier curves and Perlin noise. This is reasonable since natural objects and scenes existing around us are constructed according to fractal geometry. Image representation with the proposed FractalDB captures a unique feature in the visualization of convolutional layers and attentions.	[Kataoka, Hirokatsu; Okayasu, Kazushige; Matsumoto, Asato; Yamada, Ryosuke; Satoh, Yutaka] Natl Inst Adv Ind Sci & Technol, Tsukuba, Ibaraki, Japan; [Yamagata, Eisuke; Inoue, Nakamasa] Tokyo Inst Technol, Tokyo, Japan; [Nakamura, Akio] Tokyo Denki Univ, Tokyo, Japan	National Institute of Advanced Industrial Science & Technology (AIST); Tokyo Institute of Technology; Tokyo Denki University	Kataoka, H (corresponding author), Natl Inst Adv Ind Sci & Technol, Tsukuba, Ibaraki, Japan.	hirokatsu.kataoka@aist.go.jp			New Energy and Industrial Technology Development Organization (NEDO) [JPNP20006]; JSPS KAKENHI [JP19H01134]	New Energy and Industrial Technology Development Organization (NEDO)(New Energy and Industrial Technology Development Organization (NEDO)); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This paper is based on results obtained from a project, JPNP20006, commissioned by the New Energy and Industrial Technology Development Organization (NEDO). This work was supported by JSPS KAKENHI Grant No. JP19H01134. AI Bridging Cloud Infrastructure (ABCI) provided by the National Institute of Advanced Industrial Science and Technology (AIST) was used.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Asano Yuki Markus, 2020, P ICLR; Barnsley MF., 1988, FRACTALS EVERYWHERE; Birhane A, 2021, IEEE WINT CONF APPL, P1536, DOI 10.1109/WACV48630.2021.00158; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Chen T, 2020, PR MACH LEARN RES, V119; Chen YQ, 1997, COMPUT GRAPH-UK, V21, P367, DOI 10.1016/S0097-8493(97)00014-9; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Donahue J, 2014, PR MACH LEARN RES, V32; Donahue Jeff, 2019, ADV NEURAL INFORM PR; Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Falconer K.J., 2014, FRACTAL GEOMETRY MAT, V3rd ed.; Farin G, 1990, CURVES SURFACES COMP; Fellbaum C, 1998, LANG SPEECH & COMMUN, P1; Gidaris Spyros, 2018, ARXIV180307728; He K., 2020, IEEE INT C COMP VIS; He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Howard A.G., 2017, MOBILENETS EFFICIENT; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G., 2017, P IEEE C COMPUTER VI, P4700, DOI DOI 10.1109/CVPR.2017.243; Huh M., 2016, ADV NEURAL INFORM PR; Kay W., 2017, ARXIV PREPRINT ARXIV; Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277; Krasin Ivan, 2017, OPENIMAGES PUBLIC DA, V2, P18; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; LANDINI G, 1995, INVEST OPHTH VIS SCI, V36, P2749; Larsson G., 2017, INT C LEARN REPR ICL; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12; Mandelbrot B.B., 1983, FRACTAL GEOMETRY NAT, P468; Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464; MONRO DM, 1995, IEEE COMPUT GRAPH, V15, P32, DOI 10.1109/38.364961; Movshovitz-Attias Y, 2016, LECT NOTES COMPUT SC, V9915, P202, DOI 10.1007/978-3-319-49409-8_18; Noroozi M, 2018, PROC CVPR IEEE, P9359, DOI 10.1109/CVPR.2018.00975; Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P661, DOI 10.1109/TPAMI.1984.4767591; Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269; Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636; Remez T, 2018, LECT NOTES COMPUT SC, V11211, P39, DOI 10.1007/978-3-030-01234-2_3; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Smith TG, 1996, J NEUROSCI METH, V69, P123, DOI 10.1016/S0165-0270(96)00080-5; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; van den Oord Aaron, 2018, ARXIV180703748; Varma M, 2007, IEEE I CONF COMP VIS, P369; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu Y, 2009, INT J COMPUT VISION, V83, P85, DOI 10.1007/s11263-009-0220-6; Yang Kaiyu, 2020, C FAIRN ACC TRANSP F; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009	64	0	0	5	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					990	1007		10.1007/s11263-021-01555-8	http://dx.doi.org/10.1007/s11263-021-01555-8		FEB 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ		hybrid, Green Submitted			2022-12-18	WOS:000760698800001
J	Zhang, ZX; Pan, C; Peng, JR				Zhang, Zhaoxiang; Pan, Cong; Peng, Junran			Delving into the Effectiveness of Receptive Fields: Learning Scale-Transferrable Architectures for Practical Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision; Object detection; Effective receptive fields; Hardware acceleration		Scale-sensitive object detection remains a challenging task, where most of the existing methods could not learn it explicitly and are not robust. Besides, they are less efficient during training or slow during inference, which is not friendly to real-time applications. In this paper, we propose a scale-transferrable architecture for practical object detection based on the analysis of the connection between dilation rate and effective receptive field. Our method firstly predicts a global continuous scale, which is shared by all positions, for each convolution filter of each network stage. Secondly, we average the spatial features and distill the scale from channels to effectively learn the scale. Thirdly, for fast-deployment, we propose a scale decomposition method that transfers the robust fractional scale into the combination of fixed integral scales for each convolution filter, which exploits the dilated convolution. Moreover, to overcome the shortcomings of our method for large-scale object detection, we modify the Feature Pyramid Network structure. Finally, we illustrate the orthogonality role of our method for sampling strategy. We demonstrate the effectiveness of our method on one-stage and two-stage algorithms under different configurations and compare them with different dilated convolution blocks. For practical applications, the training strategy of our method is simple and efficient, avoiding complex data sampling or optimization strategy. During inference, we reduce the latency of the proposed method by using the hardware accelerator TensorRT without extra operation. On the COCO test-dev, our model achieves 41.7% mAP on one-stage detector and 42.5% mAP on two-stage detector based on ResNet-101, and outperforms baselines by 3.2% and 3.1% mAP, respectively.	[Zhang, Zhaoxiang; Pan, Cong] Univ Chinese Acad Sci, Sch Future Technol, Beijing, Peoples R China; [Zhang, Zhaoxiang; Pan, Cong] Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; [Zhang, Zhaoxiang; Pan, Cong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Zhang, Zhaoxiang] Chinese Acad Sci, Hong Kong Inst Sci & Innovat, Ctr Artificial Intelligence & Robot, Hong Kong, Peoples R China; [Peng, Junran] Huawei Cloud & AI, Beijing, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Huawei Technologies	Peng, JR (corresponding author), Huawei Cloud & AI, Beijing, Peoples R China.	zhaoxiang.zhang@ia.ac.cn; pancong2018@ia.ac.cn; pengjunran@huawei.com			Major Project for New Generation of AI [2018AAA0100400]; NationalNatural Science Foundation of China [61836014, U21B 2042]	Major Project for New Generation of AI; NationalNatural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the Major Project for New Generation of AI (No.2018AAA0100400) and the NationalNatural Science Foundation of China (No.61836014, No.U21B 2042).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen K., 2019, ARXIV PREPRINT ARXIV; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen Q, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107334; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Fu C. -Y., 2017, ARXIV170106659; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jaderberg M, 2015, ADV NEUR IN, V28; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI 10.1007/978-3-030-01240-3_21; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Peng JR, 2019, IEEE I CONF COMP VIS, P9606, DOI 10.1109/ICCV.2019.00970; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shrivastava Abhinav, 2016, ARXIV161206851; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tychsen-Smith L, 2018, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR.2018.00719; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu HY, 2018, LECT NOTES COMPUT SC, V11215, P827, DOI 10.1007/978-3-030-01252-6_49; Xu J, 2020, PATTERN RECOGN, V99, DOI 10.1016/j.patcog.2019.107098; Yu F., 2016, P ICLR 2016; Zhang R, 2017, IEEE I CONF COMP VIS, P2050, DOI 10.1109/ICCV.2017.224; Zhang S, 2021, EDUC MANAG ADM LEAD, V49, P768, DOI 10.1177/1741143220915925; Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442; Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953	47	0	0	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					970	989		10.1007/s11263-021-01573-6	http://dx.doi.org/10.1007/s11263-021-01573-6		FEB 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ					2022-12-18	WOS:000759289300002
J	Song, X; Yang, GR; Zhu, XG; Zhou, H; Ma, YX; Wang, Z; Shi, JP				Song, Xiao; Yang, Guorun; Zhu, Xinge; Zhou, Hui; Ma, Yuexin; Wang, Zhe; Shi, Jianping			AdaStereo: An Efficient Domain-Adaptive Stereo Matching Approach (vol 130, pg 226, 2022)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Song, Xiao; Zhou, Hui; Wang, Zhe; Shi, Jianping] Sense Time Grp Ltd, Hong Kong, Peoples R China; [Yang, Guorun] Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China; [Zhu, Xinge] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Ma, Yuexin] Shanghai Tech Univ, Shanghai, Peoples R China	Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; Chinese University of Hong Kong; ShanghaiTech University	Song, X (corresponding author), Sense Time Grp Ltd, Hong Kong, Peoples R China.	songxiao@sensetime.com; yangguorun91@gmail.com; zhuxinge123@gmail.com; zhouhui@sensetime.com; mayuexin@shanghaitech.edu.cn; wangzhe@sensetime.com; shijianping@sensetime.com						Song X, 2022, INT J COMPUT VISION, V130, P226, DOI 10.1007/s11263-021-01549-6	1	0	0	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					884	884		10.1007/s11263-022-01580-1	http://dx.doi.org/10.1007/s11263-022-01580-1		FEB 2022	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI		Bronze			2022-12-18	WOS:000753230200001
J	Qiao, XT; Zheng, QL; Cao, Y; Lau, RWH				Qiao, Xiaotian; Zheng, Quanlong; Cao, Ying; Lau, Rynson W. H.			Instance-Aware Scene Layout Forecasting (Jan, 10.1007/s11263-021-01560-x, 2022)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Qiao, Xiaotian] Xidian Univ, Xian, Peoples R China; [Qiao, Xiaotian; Zheng, Quanlong; Cao, Ying; Lau, Rynson W. H.] City Univ Hong Kong, Kowloon, Hong Kong, Peoples R China	Xidian University; City University of Hong Kong	Cao, Y; Lau, RWH (corresponding author), City Univ Hong Kong, Kowloon, Hong Kong, Peoples R China.	qiaoxt1992@gmail.com; xiaolong921001@gmail.com; caoying59@gmail.com; Rynson.Lau@cityu.edu.hk						Qiao XT, 2022, INT J COMPUT VISION, V130, P504, DOI 10.1007/s11263-021-01560-x	1	0	0	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					883	883		10.1007/s11263-022-01577-w	http://dx.doi.org/10.1007/s11263-022-01577-w		FEB 2022	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI		Bronze			2022-12-18	WOS:000752161200001
J	Chen, ZS; Xie, LX; Niu, JW; Liu, XF; Wei, LH; Tian, Q				Chen, Zhengsu; Xie, Lingxi; Niu, Jianwei; Liu, Xuefeng; Wei, Longhui; Tian, Qi			Network Adjustment: Channel and Block Search Guided by Resource Utilization Ratio	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Network adjustment; Model compression; Neural architecture search; Network architecture design; Resource utilization ratio		It is an important problem to design resource-efficient neural architectures. One solution is adjusting the number of channels in each layer and the number of blocks in each network stage. This paper presents a novel framework named network adjustment which considers accuracy as a function of the computational resource (e.g., FLOPs or parameters), so that architecture design becomes an optimization problem and can be solved with the gradient-based optimization method. The gradient is defined as the resource utilization ratio (RUR) of each changeable module (layer or block) in a network and is accurate only in a small neighborhood of the current status. Therefore, we estimate it using Dropout, a probabilistic operation, and optimize the network architecture iteratively. The computational overhead of the entire process is comparable to that of re-training the final model from scratch. We investigate two versions of RUR where the resource usage is measured by FLOPs and latency. Experiments on standard image classification datasets and a few base networks including ResNet and EfficientNet demonstrate the effectiveness of our approach, which consistently outperforms the pruning-based counterparts.	[Chen, Zhengsu; Niu, Jianwei; Liu, Xuefeng] Beihang Univ, Beijing, Peoples R China; [Xie, Lingxi] Johns Hopkins Univ, Baltimore, MD USA; [Wei, Longhui] Peking Univ, Beijing, Peoples R China; [Tian, Qi] Xidian Univ, Xian, Peoples R China	Beihang University; Johns Hopkins University; Peking University; Xidian University	Liu, XF (corresponding author), Beihang Univ, Beijing, Peoples R China.	danczs@buaa.edu.cn; 198808xc@gmail.com; niujianwei@buaa.edu.cn; liu_xuefeng@buaa.edu.cn; longhuiwei@pku.edu.cn; wywqtian@gmail.com			National Key R&D Program of China [2017YFB1301100]; National Natural Science Foundation of China [61772060, U1536107, 61472024, 61572060, 61976012, 61602024]; CERNET Innovation Project [NGII20160316]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CERNET Innovation Project	This work was supported by the National Key R&D Program of China (2017YFB1301100), National Natural Science Foundation of China (61772060, U1536107, 61472024, 61572060, 61976012, 61602024), and the CERNET Innovation Project (NGII20160316). We would like to thank Dr. Xin Chen for the helpful discussions.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Cai H., 2018, ARXIV PREPRINT ARXIV; Chen K., 2019, ARXIV190607155; CHEN Y, 1903, ARXIV190310979; Chen Z, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01370-7; CHU X, 1907, ARXIV190701845; DONG X, 1905, ARXIV190509717; Dong XY, 2017, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR.2017.205; Frankle Jonathan, 2018, ARXIV180303635; Ghiasi G, 2018, ADV NEUR IN, V31; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gordon A, 2018, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2018.00171; GUO Z, 1904, ARXIV190400420; HAN S, 2015, ADV NEU RAL INFORM P; HAN SY, 2016, IEEE ICC; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447; Hinton G., 2015, ARXIV150302531; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang ZH, 2018, LECT NOTES COMPUT SC, V11220, P317, DOI 10.1007/978-3-030-01270-0_19; Jin XX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901353; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008; Larsson G., 2016, ARXIV PREPRINT ARXIV; Li X., 2019, COMPUTER VISION PATT; Liu Hanxiao, 2018, ARXIV180609055; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Liu Zhuang, 2018, ARXIV181005270; LONG J, 2015, COMPUTERVISION PATTE; LYM S, 1901, ARXIV190109290; Mao HZ, 2017, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2017.241; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pham H, 2018, PR MACH LEARN RES, V80; Qiao SY, 2019, PROC CVPR IEEE, P61, DOI 10.1109/CVPR.2019.00015; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; STAMOULIS D, 1904, ARXIV190402877; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tan MX, 2019, PR MACH LEARN RES, V97; Tan Mingxing, 2021, ARXIV210400298; Veit A, 2016, ADV NEUR IN, V29; Wan Alvin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12962, DOI 10.1109/CVPR42600.2020.01298; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wang YL, 2020, AAAI CONF ARTIF INTE, V34, P12273; Wen W, 2016, ADV NEUR IN, V29; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; XU Y, 2001, ARXIV200106392; YU J, 1903, ARXIV190311728; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	64	0	0	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					820	835		10.1007/s11263-021-01566-5	http://dx.doi.org/10.1007/s11263-021-01566-5		FEB 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI					2022-12-18	WOS:000749450600001
J	Luvizon, DC; Picard, D; Tabia, H				Luvizon, Diogo C.; Picard, David; Tabia, Hedi			Consensus-Based Optimization for 3D Human Pose Estimation in Camera Coordinates	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human pose estimation; Motion capture; Neural networks; Deep learning		3D human pose estimation is frequently seen as the task of estimating 3D poses relative to the root body joint. Alternatively, we propose a 3D human pose estimation method in camera coordinates, which allows effective combination of 2D annotated data and 3D poses and a straightforward multi-view generalization. To that end, we cast the problem as a view frustum space pose estimation, where absolute depth prediction and joint relative depth estimations are disentangled. Final 3D predictions are obtained in camera coordinates by the inverse camera projection. Based on this, we also present a consensus-based optimization algorithm for multi-view predictions from uncalibrated images, which requires a single monocular training procedure. Although our method is indirectly tied to the training camera intrinsics, it still converges for cameras with different intrinsic parameters, resulting in coherent estimations up to a scale factor. Our method improves the state of the art on well known 3D human pose datasets, reducing the prediction error by 32% in the most common benchmark. We also reported our results in absolute pose position error, achieving 80 mm for monocular estimations and 51 mm for multi-view, on average. Source code is available at https://github.com/dluvizon/3d-pose-consensus.	[Luvizon, Diogo C.] CY Cergy Paris Univ, CNRS, UMR 8051, ETIS,ENSEA, F-95000 Cergy, France; [Picard, David] Univ Gustave Eiffel, CNRS, Ecole Ponts, LIGM,IMAGINE, F-77455 Marne La Vallee, France; [Tabia, Hedi] Univ Paris Saclay, Univ Evry, IBISC, F-91025 Evry, France	Centre National de la Recherche Scientifique (CNRS); CY Cergy Paris Universite; Centre National de la Recherche Scientifique (CNRS); Ecole des Ponts ParisTech; Universite Gustave-Eiffel; ESIEE Paris; UDICE-French Research Universities; Universite Paris Saclay	Luvizon, DC (corresponding author), CY Cergy Paris Univ, CNRS, UMR 8051, ETIS,ENSEA, F-95000 Cergy, France.	diogo.luvizon@ensea.fr; david.picard@enpc.fr; hedi.tabia@univ-evry.fr			CNPq (Brazil) [233342/2014-1]	CNPq (Brazil)(Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPQ))	This work was partially funded by CNPq (Brazil)-Grant 233342/2014-1.	Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; Amin S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.45; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Belagiannis V, 2016, IEEE T PATTERN ANAL, V38, P1929, DOI 10.1109/TPAMI.2015.2509986; Belagiannis V, 2014, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR.2014.216; Burenius M, 2013, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2013.464; Nunez JC, 2019, NEUROCOMPUTING, V323, P335, DOI 10.1016/j.neucom.2018.10.009; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Chen Weifeng, 2016, NEURIPS, P2, DOI DOI 10.5555/3157096.3157178; Eigen D, 2014, ADV NEUR IN, V27; Gunel S, 2019, IEEE INT CONF COMP V, P1819, DOI 10.1109/ICCVW.2019.00226; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hofmann M, 2012, INT J COMPUT VISION, V96, P103, DOI 10.1007/s11263-011-0451-1; Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Ionescu C, 2011, IEEE I CONF COMP VIS, P2220, DOI 10.1109/ICCV.2011.6126500; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Kadkhodamohammadi A, 2020, MACH VISION APPL, V32, DOI 10.1007/s00138-020-01120-2; Kazemi V, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.48; Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8; Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326; Luvizon DC, 2019, COMPUT GRAPH-UK, V85, P15, DOI 10.1016/j.cag.2019.09.002; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Mehta D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392410; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Micusik B, 2010, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2010.5539786; Moon G, 2019, IEEE I CONF COMP VIS, P10132, DOI 10.1109/ICCV.2019.01023; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nie BX, 2017, IEEE I CONF COMP VIS, P3467, DOI 10.1109/ICCV.2017.373; Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139; Popa AI, 2017, PROC CVPR IEEE, P4714, DOI 10.1109/CVPR.2017.501; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Rhodin H, 2018, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2018.00880; Rogez G, 2017, PROC CVPR IEEE, P1216, DOI 10.1109/CVPR.2017.134; Sarafianos N, 2016, COMPUT VIS IMAGE UND, V152, P1, DOI 10.1016/j.cviu.2016.09.002; Shi Y., 2018, ARXIV180609241; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284; Sun XL, 2018, 2018 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC); Tekin Bugra, 2016, BRIT MACH VIS C BMVC, P2; Tome D, 2018, INT CONF 3D VISION, P474, DOI 10.1109/3DV.2018.00061; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Trumble M., 2017, 2017 BRIT MACH VIS C; Trumble M, 2018, LECT NOTES COMPUT SC, V11214, P800, DOI 10.1007/978-3-030-01249-6_48; van Dijk T, 2019, IEEE I CONF COMP VIS, P2183, DOI 10.1109/ICCV.2019.00227; Vasiljevic Igor, 2019, ARXIV190800463; Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551; Zanfir A, 2018, ADV NEUR IN, V31; Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229; Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303; Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51; Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	60	0	0	6	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					869	882		10.1007/s11263-021-01570-9	http://dx.doi.org/10.1007/s11263-021-01570-9		FEB 2022	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI		Green Submitted			2022-12-18	WOS:000749450600004
J	Zhang, ZX; Luo, CC; Wu, HP; Chen, YT; Wang, NY; Song, CF				Zhang, Zhaoxiang; Luo, Chuanchen; Wu, Haiping; Chen, Yuntao; Wang, Naiyan; Song, Chunfeng			From Individual to Whole: Reducing Intra-class Variance by Feature Aggregation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Feature aggregation; Deep learning; Intra-class variance; Person re-identification; Video object detection	PERSON REIDENTIFICATION; ATTENTION; NETWORK; ACCURATE	The recording process of observation is influenced by multiple factors, such as viewpoint, illumination, and state of the object-of-interest etc.Thus, the image observation of the same object may vary a lot under different conditions. This leads to severe intra-class variance which greatly challenges the discrimination ability of the vision model. However, the current prevailing softmax loss for visual recognition only pursues perfect inter-class separation in the feature space. Without considering the intra-class compactness, the learned model easily collapses when it encounters the instances that deviate a lot from their class centroid. To resist the intra-class variance, we start by organizing the input instances as a graph. From this viewpoint, we find that the normalized cut on the graph is a favorable surrogate metric of the intra-class variance within the training batch. Inspired by the equivalence between the normalized cut and random walk, we propose a feature aggregation scheme using transition probabilities as guidance. By imposing supervision on the aggregated features, we can constrain the transition probabilities to form a graph partition consistent with the given labels. Thus, the normalized cut as well as intra-class variance can be well suppressed. To validate the effectiveness of this idea, we instantiate it in spatial, temporal, and spatial-temporal scenarios. Experimental results on corresponding benchmarks demonstrate that the proposed feature aggregation leads to significant improvement in performance. Our method is on par with, or even better than current state-of-the-arts in both tasks.	[Zhang, Zhaoxiang; Luo, Chuanchen; Song, Chunfeng] Chinese Acad Sci CASIA, Ctr Res Intelligent Percept & Comp CRIPAC, Natl Lab Pattern Recognit NLPR, Inst Automat, Beijing 100190, Peoples R China; [Zhang, Zhaoxiang; Luo, Chuanchen; Song, Chunfeng] Univ Chinese Acad Sci UCAS, Beijing 100190, Peoples R China; [Zhang, Zhaoxiang] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol C, Beijing 100190, Peoples R China; [Zhang, Zhaoxiang] Chinese Acad Sci HKISICAS, Hong Kong Inst Sci & Innovat, Ctr Artificial Intelligence & Robot, Hong Kong, Peoples R China; [Wu, Haiping; Chen, Yuntao] Tusimple, San Diego, CA USA; [Wang, Naiyan] TuSimple, Beijing, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences	Zhang, ZX (corresponding author), Chinese Acad Sci CASIA, Ctr Res Intelligent Percept & Comp CRIPAC, Natl Lab Pattern Recognit NLPR, Inst Automat, Beijing 100190, Peoples R China.; Zhang, ZX (corresponding author), Univ Chinese Acad Sci UCAS, Beijing 100190, Peoples R China.; Zhang, ZX (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol C, Beijing 100190, Peoples R China.; Zhang, ZX (corresponding author), Chinese Acad Sci HKISICAS, Hong Kong Inst Sci & Innovat, Ctr Artificial Intelligence & Robot, Hong Kong, Peoples R China.	zhaoxiang.zhang@ia.ac.cn; luochuanchen2017@ia.ac.cn; haipingwoo@gmail.com; chenyuntao08@gmail.com; winsty@gmail.com; chunfeng.song@ia.ac.cn	Song, Chunfeng/AGI-3684-2022	Song, Chunfeng/0000-0003-1223-3242	Major Project for New Generation of AI [2018AAA0100400]; National Natural Science Foundation of China [61836014, U21B2042, 61773375, 62006231, 62072457]; National Youth Talent Support Program	Major Project for New Generation of AI; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Youth Talent Support Program	This work was supported in part by the Major Project for New Generation of AI (No. 2018AAA0100400), the National Natural Science Foundation of China (No. 61836014, No. U21B2042, No. 61773375, No. 62006231, No. 62072457), the National Youth Talent Support Program.	Carreiraperpinan M.A, 2006, P 23 INT C MACHINE L, V148, P153, DOI [DOI 10.1145/1143844.1143864, 10.1145/1143844.1143864]; Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225; Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696; Chen GY, 2019, IEEE I CONF COMP VIS, P9546, DOI 10.1109/ICCV.2019.00964; Chen K, 2018, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2018.00425; Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304; Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5; Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Fu Y, 2019, AAAI CONF ARTIF INTE, P8287; Gu XQ, 2019, IEEE I CONF COMP VIS, P9646, DOI 10.1109/ICCV.2019.00974; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; Han Wei, 2016, ARXIV160208465; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hermans Alexander, 2017, ARXIV170307737; Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Jegou H, 2007, PROC CVPR IEEE, P9; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95; Li JN, 2019, IEEE I CONF COMP VIS, P3957, DOI 10.1109/ICCV.2019.00406; Li JN, 2019, AAAI CONF ARTIF INTE, P8618; Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006; Liu C.-T., 2019, BMVC; Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Lu YY, 2017, IEEE I CONF COMP VIS, P2363, DOI 10.1109/ICCV.2017.257; Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508; Meila M., 2001, P 8 INT WORKSH ART I; Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47; Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577; Qin DF, 2011, PROC CVPR IEEE, P777, DOI 10.1109/CVPR.2011.5995373; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241; Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30; Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562; Sohn K, 2016, ADV NEUR IN, V29; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Subramaniam A, 2019, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2019.00065; Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tripathi S., 2016, ARXIV160704648; Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23; Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang SY, 2018, LECT NOTES COMPUT SC, V11217, P557, DOI 10.1007/978-3-030-01261-8_33; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wang YL, 2018, PROC CVPR IEEE, P8906, DOI 10.1109/CVPR.2018.00928; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279; Wei ZX, 2017, IEEE GLOB COMM CONF; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wu HP, 2019, IEEE I CONF COMP VIS, P9216, DOI 10.1109/ICCV.2019.00931; Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148; Yu R., 2017, ARXIV170804169; Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31; Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52	86	0	0	3	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					800	819		10.1007/s11263-021-01569-2	http://dx.doi.org/10.1007/s11263-021-01569-2		FEB 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI					2022-12-18	WOS:000749450600003
J	Zheng, Q; Gong, MM; You, XG; Tao, DC				Zheng, Qi; Gong, Mingming; You, Xinge; Tao, Dacheng			A Unified B-Spline Framework for Scale-Invariant Keypoint Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Keypoint detection; B-splines; Scale-space; Scale invariance; Image filtering	CONTINUOUS WAVELET TRANSFORM; FEATURES; SELECTION	Scale-invariant keypoint detection is a fundamental problem in low-level vision. To accelerate keypoint detectors (e.g. DoG, Harris-Laplace, Hessian-Laplace) that are developed in Gaussian scale-space, various fast detectors (e.g., SURF, CenSurE, and BRISK) have been developed by approximating Gaussian filters with simple box filters. However, there is no principled way to design the shape and scale of the box filters. Additionally, the involved integral image technique makes it difficult to figure out the continuous kernels that correspond to the discrete ones used in these detectors, so there is no guarantee that those good properties such as causality in the original Gaussian space can be inherited. To address these issues, in this paper, we propose a unified B-spline framework for scale-invariant keypoint detection. Owing to an approximate relationship to Gaussian kernels, the B-spline framework provides a mathematical interpretation of existing fast detectors based on integral images. In addition, from B-spline theories, we illustrate the problem in repeated integration, which is the generalized version of the integral image technique. Finally, following the dominant measures for keypoint detection and automatic scale selection, we develop B-spline determinant of Hessian (B-DoH) and B-spline Laplacian-of-Gaussian (B-LoG) as two instantiations within the unified B-spline framework. For efficient computation, we propose to use repeated running-sums to convolve images with B-spline kernels with fixed orders, which avoids the problem of integral images by introducing an extra interpolation kernel. Our B-spline detectors can be designed in a principled way without the heuristic choice of kernel shape and scales and naturally extend the popular SURF and CenSurE detectors with more complex kernels. Extensive experiments on the benchmark dataset demonstrate that the proposed detectors outperform the others in terms of repeatability and efficiency.	[Zheng, Qi; Tao, Dacheng] Univ Sydney, Sch Comp Sci, Sydney, NSW 2006, Australia; [Gong, Mingming] Univ Melbourne, Sch Math & Stat, Melbourne, Vic 3010, Australia; [Zheng, Qi; You, Xinge] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan, Peoples R China	University of Sydney; University of Melbourne; Huazhong University of Science & Technology	You, XG (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan, Peoples R China.	youxg@hust.edu.cn		Zheng, Qi/0000-0002-4351-9537; Gong, Mingming/0000-0001-7147-5589	NSFC [61772220, 62172177]	NSFC(National Natural Science Foundation of China (NSFC))	This research was supported by the NSFC (No. 61772220 and 62172177).	Afonso MV, 2014, IEEE T MULTIMEDIA, V16, P1, DOI 10.1109/TMM.2013.2281023; Agrawal M, 2008, LECT NOTES COMPUT SC, V5305, P102, DOI 10.1007/978-3-540-88693-8_8; Awrangjeb M, 2012, IEEE T IMAGE PROCESS, V21, P4167, DOI 10.1109/TIP.2012.2200493; BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749; Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410; Barroso-Laguna A, 2019, IEEE I CONF COMP VIS, P5835, DOI 10.1109/ICCV.2019.00593; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Benbihi A, 2019, IEEE I CONF COMP VIS, P7939, DOI 10.1109/ICCV.2019.00803; Bouma H, 2007, LECT NOTES COMPUT SC, V4485, P406; Bretzner L, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P423, DOI 10.1109/AFGR.2002.1004190; Brown M, 2002, BRIT MACH VIS C, DOI DOI 10.5244/C.16.23; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chaudhury KN, 2010, IEEE T IMAGE PROCESS, V19, P2290, DOI 10.1109/TIP.2010.2046953; Crow F. C., 1984, Computers & Graphics, V18, P207; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Deselaers T, 2008, INFORM RETRIEVAL, V11, P77, DOI 10.1007/s10791-007-9039-3; DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060; Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828; Fauqueur J, 2007, IEEE I CONF COMP VIS, P2309; Goh SS, 2007, ADV COMPUT MATH, V26, P231, DOI 10.1007/s10444-004-8007-3; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Heckbert P. S., 1986, Computer Graphics, V20, P315, DOI 10.1145/15886.15921; Herman G, 2013, PATTERN RECOGN, V46, P3315, DOI 10.1016/j.patcog.2013.04.021; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Kienzle W, 2007, ADV NEURAL INFORM PR, P689; Krig S., 2016, COMPUTER VISION METR, DOI [10.1007/978-3-319-33762-3, DOI 10.1007/978-3-319-33762-3]; Lagiewka M, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS (INISTA), P45, DOI 10.1109/INISTA.2017.8001130; Lawton W., 1995, ADV COMPUT MATH, V3, P137, DOI 10.1007/BF03028364; Ledwich L., 2004, AUSTR C ROB AUT CANB, P3; Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542; Li YL, 2015, NEUROCOMPUTING, V149, P736, DOI 10.1016/j.neucom.2014.08.003; Lindeberg T, 2003, LECT NOTES COMPUT SC, V2695, P148; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Lindeberg T., 2009, ENCY COMPUTER SCI EN, P2495; Lindeberg T., 2014, COMPUTER VISION REFE, P701; Lindeberg T, 2018, J MATH IMAGING VIS, V60, P525, DOI 10.1007/s10851-017-0766-9; Lindeberg T, 2015, J MATH IMAGING VIS, V52, P3, DOI 10.1007/s10851-014-0541-0; Lindeberg T, 2013, J MATH IMAGING VIS, V46, P177, DOI 10.1007/s10851-012-0378-3; Lorenz C, 1997, LECT NOTES COMPUT SC, V1205, P233, DOI 10.1007/BFb0029242; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mair E, 2010, LECT NOTES COMPUT SC, V6312, P183, DOI 10.1007/978-3-642-15552-9_14; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mikolajczyk K., 2002, THESIS I NATL POLYTE; MORTENSEN EN, 2005, PROC CVPR IEEE, P184, DOI DOI 10.1109/CVPR.2005.45; Munoz A, 2002, SIGNAL PROCESS, V82, P749, DOI 10.1016/S0165-1684(02)00140-8; Munoz-Barrutia A, 2010, IEEE T IMAGE PROCESS, V19, P11, DOI 10.1109/TIP.2009.2031235; Ono Yuki, 2018, ADV NEURAL INFORM PR, P6237; Revaud J, 2019, ADV NEUR IN, V32; Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Savinov N, 2017, PROC CVPR IEEE, P3929, DOI 10.1109/CVPR.2017.418; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017; UNSER M, 1994, IEEE T SIGNAL PROCES, V42, P3519, DOI 10.1109/78.340787; UNSER M, 1992, IEEE T INFORM THEORY, V38, P864, DOI 10.1109/18.119742; UNSER M, 1993, IEEE T SIGNAL PROCES, V41, P821, DOI 10.1109/78.193220; UNSER M, 1993, IEEE T PATTERN ANAL, V15, P364, DOI 10.1109/34.206956; van denBoomgaard R., 2006, SCALE SPACE MORPHOLO, P205; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wang YP, 1998, IEEE T PATTERN ANAL, V20, P1040, DOI 10.1109/34.722612; Wang Z, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P1361, DOI 10.1109/ICMA.2013.6618111; Witkin AP, 1987, READINGS COMPUTER VI, P329; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4	67	0	0	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					777	799		10.1007/s11263-021-01568-3	http://dx.doi.org/10.1007/s11263-021-01568-3		FEB 2022	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI					2022-12-18	WOS:000749450600002
J	Arrigoni, F; Ricci, E; Pajdla, T				Arrigoni, Federica; Ricci, Elisa; Pajdla, Tomas			Multi-frame Motion Segmentation by Combining Two-Frame Results	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion segmentation; Multibody structure from motion; Synchronization	SYNCHRONIZATION	In this paper we consider the motion segmentation problem on sparse and unstructured datasets involving rigid motions, motivated by multibody structure from motion. In particular, we assume only two-frame correspondences as input without prior knowledge about trajectories. Inspired by the success of synchronization methods, we address this problem by introducing a two-stage approach: first, motion segmentation is addressed on image pairs independently; then, two-frame results are combined in a robust way to compute the final multi-frame segmentation. Our synthetic and real experiments demonstrate that the proposed approach is very effective in reducing the errors among two-frame results and it can cope with a large amount of mismatches. Moreover, our method can be profitably used to build a multibody structure from motion pipeline.	[Arrigoni, Federica; Ricci, Elisa] Univ Trento, Dept Informat Engn & Comp Sci DISI, Trento, Italy; [Ricci, Elisa] Fdn Bruno Kessler, Deep Visual Learning Grp, Povo, Italy; [Pajdla, Tomas] Czech Tech Univ, Czech Inst Informat Robot & Cybernet CIIRC, Prague, Czech Republic	University of Trento; Fondazione Bruno Kessler; Czech Technical University Prague	Arrigoni, F (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci DISI, Trento, Italy.	federica.anigoni@unitn.it; e.ricci@unitn.it; pajdla@cvut.cz						[Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; Arrigoni Federica, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P514, DOI 10.1007/978-3-030-58565-5_31; Arrigoni F., 2020, PROC INT CONF PATT R; Arrigoni F, 2020, INT J COMPUT VISION, V128, P26, DOI 10.1007/s11263-019-01240-x; Arrigoni F, 2016, SIAM J IMAGING SCI, V9, P1963, DOI 10.1137/16M1060248; Arrigoni Federica, 2019, IEEE INT C COMP VIS; Barath D, 2018, LECT NOTES COMPUT SC, V11220, P229, DOI 10.1007/978-3-030-01270-0_14; Bernard F, 2019, IEEE I CONF COMP VIS, P10283, DOI 10.1109/ICCV.2019.01038; Bernard F, 2015, PROC CVPR IEEE, P2161, DOI 10.1109/CVPR.2015.7298828; Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302; Bideau P, 2019, LECT NOTES COMPUT SC, V11134, P715, DOI 10.1007/978-3-030-11024-6_55; Bideau P, 2018, PROC CVPR IEEE, P508, DOI 10.1109/CVPR.2018.00060; Bideau P, 2016, LECT NOTES COMPUT SC, V9912, P433, DOI 10.1007/978-3-319-46484-8_26; Birdal T, 2019, PROC CVPR IEEE, P11097, DOI 10.1109/CVPR.2019.01136; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Caelles Sergi, 2019, ARXIV190500737; Carlone L, 2015, IEEE INT CONF ROBOT, P4597, DOI 10.1109/ICRA.2015.7139836; Chen Y, 2020, PROC CVPR IEEE, P4154, DOI 10.1109/CVPR42600.2020.00421; Chin TJ, 2010, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2010.5539931; Cortes, 2009, NEURAL INF PROCESS S; Dave A, 2019, IEEE INT CONF COMP V, P1493, DOI 10.1109/ICCVW.2019.00187; Delong A, 2012, INT J COMPUT VISION, V100, P38, DOI 10.1007/s11263-012-0531-x; Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z; Dragon R, 2013, LECT NOTES COMPUT SC, V8142, P425, DOI 10.1007/978-3-642-40602-7_45; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Geiger A., 2012, P IEEE COMP SOC C CO; Govindu VM, 2004, PROC CVPR IEEE, P684; Gower J. C., 2004, PROCRUSTES PROBLEMS; Hartley R., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3041, DOI 10.1109/CVPR.2011.5995745; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Isack H, 2012, INT J COMPUT VISION, V97, P123, DOI 10.1007/s11263-011-0474-7; Ji P, 2015, IEEE I CONF COMP VIS, P4687, DOI 10.1109/ICCV.2015.532; Ji P, 2014, LECT NOTES COMPUT SC, V8694, P204, DOI 10.1007/978-3-319-10599-4_14; Jung H, 2014, PROC CVPR IEEE, P1210, DOI 10.1109/CVPR.2014.158; Keuper M, 2020, IEEE T PATTERN ANAL, V42, P140, DOI 10.1109/TPAMI.2018.2876253; Keuper M, 2017, IEEE I CONF COMP VIS, P4252, DOI 10.1109/ICCV.2017.455; Keuper M, 2015, IEEE I CONF COMP VIS, P3271, DOI 10.1109/ICCV.2015.374; Kohli P., 2007, P IEEE C COMP VIS PA, P1; Kuang D, 2015, J GLOBAL OPTIM, V62, P545, DOI 10.1007/s10898-014-0247-2; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Kumar Abhishek, 2011, NEURIPS, P2, DOI DOI 10.5555/2986459.2986617; Lai TT, 2017, IEEE T INTELL TRANSP, V18, P973, DOI 10.1109/TITS.2016.2596296; Lamdouar H, 2020, PROC ASIAN C COMPUT, P1; Li CG, 2015, PROC CVPR IEEE, P277, DOI 10.1109/CVPR.2015.7298624; Li ZW, 2013, IEEE I CONF COMP VIS, P1369, DOI 10.1109/ICCV.2013.173; Lin K., 2018, PROC ASIAN C COMPUT, P163; Lin Z, 2010, ARXIV10095055, DOI [DOI 10.1016/J.JSB.2012.10.010, 10.1016/j.jsb.2012.10.010]; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Magri L., 2015, BMVC, V12, P20, DOI [10.5244/C.29.20, DOI 10.5244/C.29.20]; Magri L, 2016, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2016.361; Magri L, 2014, PROC CVPR IEEE, P3954, DOI 10.1109/CVPR.2014.505; Maset E, 2017, IEEE I CONF COMP VIS, P4578, DOI 10.1109/ICCV.2017.489; Mattheus J., 2020, 2020 2 INT MULT INF, P1, DOI DOI 10.1109/IMITEC50163.2020.9334076; Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242; Olsson C, 2011, LECT NOTES COMPUT SC, V6688, P524, DOI 10.1007/978-3-642-21227-7_49; Pachauri D., 2013, ADV NEURAL INFORM PR, V26, P1860; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Pham TT, 2014, IEEE T PATTERN ANAL, V36, P1658, DOI 10.1109/TPAMI.2013.2296310; Poling B, 2014, INT J COMPUT VISION, V108, P165, DOI 10.1007/s11263-013-0694-0; Pont-Tuset J., 2017, ARXIV170400675; Rao S, 2010, IEEE T PATTERN ANAL, V32, P1832, DOI 10.1109/TPAMI.2009.191; Rosen D., 2017, MITCSAILTR2017002; Rubino C, 2018, IEEE INT CONF ROBOT, P1879; Sabzevari R, 2016, IEEE T ROBOT, V32, P638, DOI 10.1109/TRO.2016.2552548; Santellani E., 2018, ISPRS ANN PHOTOGRAMM, V4, P247; Saputra MRU, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3177853; Schindler K, 2005, PROC CVPR IEEE, P676; Schroeder Pierre, 2011, 2011 IEEE WORKSH APP, P650, DOI DOI 10.1109/WACV.2011.5711566; Shen Y., 2016, ADV NEURAL INFORM PR, V29, P4925; Singer A, 2011, APPL COMPUT HARMON A, V30, P20, DOI 10.1016/j.acha.2010.02.001; Tokmakov P, 2019, INT J COMPUT VISION, V127, P282, DOI 10.1007/s11263-018-1122-2; Toldo R, 2008, LECT NOTES COMPUT SC, V5302, P537, DOI 10.1007/978-3-540-88682-2_41; Tombari F., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P212, DOI 10.1109/3DIMPVT.2011.34; Torsello A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2441, DOI 10.1109/CVPR.2011.5995565; Tron R, 2017, IEEE I CONF COMP VIS, P4077, DOI 10.1109/ICCV.2017.437; Tron R, 2007, PROC CVPR IEEE, P41, DOI 10.1109/cvpr.2007.382974; Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244; Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z; Vidal R, 2006, INT J COMPUT VISION, V68, P7, DOI 10.1007/s11263-005-4839-7; Vincent E, 2001, ISPA 2001: PROCEEDINGS OF THE 2ND INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P182, DOI 10.1109/ISPA.2001.938625; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wang X, 2014, DATA MIN KNOWL DISC, V28, P1, DOI 10.1007/s10618-012-0291-9; Wang YX, 2018, IEEE SIGNAL PROC LET, V25, P145, DOI 10.1109/LSP.2017.2773636; Xiaoxiao Shi, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P1043, DOI 10.1109/ICDM.2010.64; XU L, 1990, PATTERN RECOGN LETT, V11, P331, DOI 10.1016/0167-8655(90)90042-Z; Xu X, 2018, PROC CVPR IEEE, P2859, DOI 10.1109/CVPR.2018.00302; Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94; Yao R, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3391743; Yuan S., 2019, SPATIALLY CONSTRAINE; Zhang W, 2007, LECT NOTES COMPUT SC, V4358, P60; Zhao Q, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20133806; Zuliani M, 2005, IEEE IMAGE PROC, P2969	94	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					696	728		10.1007/s11263-021-01544-x	http://dx.doi.org/10.1007/s11263-021-01544-x		JAN 2022	33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI		hybrid, Green Published			2022-12-18	WOS:000750293700001
J	Jeon, HG; Im, S; Choe, J; Kang, MJ; Lee, JY; Hebert, M				Jeon, Hae-Gon; Im, Sunghoon; Choe, Jaesung; Kang, Minjun; Lee, Joon-Young; Hebert, Martial			CMSNet: Deep Color and Monochrome Stereo	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo matching; Disparity estimation; Image enhancement; Convolutional neural network	IMAGE	In this paper, we propose an end-to-end convolutional neural network for stereo matching with color and monochrome cameras, called CMSNet (Color and Monochrome Stereo Network). Both cameras have the same structure except for the presence of a Bayer filter, but have a fundamental trade-off. The Bayer filter allows capturing chrominance information of scenes, but limits a quantum efficiency of cameras, which causes severe image noise. It seems ideal if we can take advantage of both the cameras so that we obtain noise-free images with their corresponding disparity maps. However, image luminance recorded from a color camera is not consistent with that from a monochrome camera due to spatially-varying illumination and different spectral sensitivities of the cameras. This degrades the performance of stereo matching. To solve this problem, we design CMSNet for disparity estimation from noisy color and relatively clean monochrome images. CMSNet also infers a noise-free image with the estimated disparity map. We leverage a data augmentation to simulate realistic signal-dependent noise and various radiometric distortions between input stereo pairs to train CMSNet effectively. CMSNet is evaluated using various datasets and the performance of our disparity estimation and image enhancement consistently outperforms state-of-the-art methods.	[Jeon, Hae-Gon] GIST, AI Grad Sch, Gwangju, South Korea; [Jeon, Hae-Gon] GIST, Sch Elect Engn & Comp Sci, Gwangju, South Korea; [Im, Sunghoon] DGIST, Dept Elect Engn & Comp Sci, Deagu, South Korea; [Choe, Jaesung; Kang, Minjun] Korea Adv Inst Sci & Technol, Daejeon, South Korea; [Lee, Joon-Young] Adobe Res, San Jose, CA USA; [Hebert, Martial] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA	Gwangju Institute of Science & Technology (GIST); Gwangju Institute of Science & Technology (GIST); Daegu Gyeongbuk Institute of Science & Technology (DGIST); Korea Advanced Institute of Science & Technology (KAIST); Adobe Systems Inc.; Carnegie Mellon University	Im, S (corresponding author), DGIST, Dept Elect Engn & Comp Sci, Deagu, South Korea.	haegonj@gist.ac.kr; sunghoonim@dgist.ac.kr; jaesung.choe@kaist.ac.kr; kmmj2005@kaist.ac.kr; jolee@adobe.com; hebert@ri.cmu.edu			Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT) [2019-0-01842, 2014-3-00077, 2021-0-02068, 2020-0-00231]; Vehicles AI Convergence Research & Development Program through the National IT Industry Promotion Agency of Korea (NIPA) - Ministry of Science and ICT [S1602-20-1001]; International Collaborative Research and Development Program - Korea Institute for Advancement of Technology (KIAT) [P146500035]; National Research Foundation of Korea (NRF) - Korea government (MSIT) [2020 R1C1C1012635, 2020R1C1C1013210]; Ministry of Trade, Industry and Energy (MOTIE); Korea Institute for Advancement of Technology(KIAT) through the International Cooperative RD program [P0019797]	Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Vehicles AI Convergence Research & Development Program through the National IT Industry Promotion Agency of Korea (NIPA) - Ministry of Science and ICT(National IT Industry Promotion Agency (NIPA), Republic of Korea); International Collaborative Research and Development Program - Korea Institute for Advancement of Technology (KIAT); National Research Foundation of Korea (NRF) - Korea government (MSIT)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Ministry of Trade, Industry and Energy (MOTIE)(Ministry of Trade, Industry & Energy (MOTIE), Republic of Korea); Korea Institute for Advancement of Technology(KIAT) through the International Cooperative RD program	This work is partly supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-01842, Artificial Intelligence Graduate School Program (GIST), No.2014-3-00077, AI National Strategy Project, No.2021-0-02068, Artificial Intelligence Innovation Hub, No.2020-0-00231, Development of Low Latency VR/AR Streaming Technology based on 5G edge cloud), Vehicles AI Convergence Research & Development Program through the National IT Industry Promotion Agency of Korea (NIPA) funded by the Ministry of Science and ICT (No. S1602-20-1001), the International Collaborative Research and Development Program funded by the Korea Institute for Advancement of Technology (KIAT) (No. P146500035), and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No.2020 R1C1C1012635, 2020R1C1C1013210). In addition, this research was financially supported by the Ministry of Trade, Industry and Energy(MOTIE) and Korea Institute for Advancement of Technology(KIAT) through the International Cooperative R&D program in part (P0019797).	Abadi M, 2015, P 12 USENIX S OPERAT; Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; [Anonymous], 2019, SAMSUNG GALAXY S10; [Anonymous], 2018, IPHONE XS; [Anonymous], 2016, HUAWEI P9; [Anonymous], 2019, LG V50; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Chakrabarti A, 2014, IEEE INT CONF COMPUT; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Che CQ, 2020, IEEE INT CONF COMPUT; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dong X, 2019, AAAI CONF ARTIF INTE, P8255; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Flea3, 2017, GIGE IM PERF SPEC; Gallo O., 2009, COMP PHOT ICCP 2009, P1; Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964; Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254; He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365; Heo YS, 2013, IEEE T PATTERN ANAL, V35, P1094, DOI 10.1109/TPAMI.2012.167; Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136; Hirschmuller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221; Holloway J, 2015, IEEE T IMAGE PROCESS, V24, P823, DOI [10.1109/TIP.2014.2383315, 10.1109/TIP.2015.2413291]; Hu J, 2013, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2013.154; Im S., 2019, ARXIV190500538; Im S, 2019, IEEE T IMAGE PROCESS, V28, P2451, DOI 10.1109/TIP.2018.2886777; Immerkaer J, 1996, COMPUT VIS IMAGE UND, V64, P300, DOI 10.1006/cviu.1996.0060; Ironi R., 2005, P 16 EUROGRAPHICS C, P201; Jaderberg M, 2015, ADV NEUR IN, V28; JEON HG, 2016, PROC CVPR IEEE, P4086, DOI DOI 10.1109/CVPR.2016.443; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim S, 2015, PROC CVPR IEEE, P2103, DOI 10.1109/CVPR.2015.7298822; Kimmel R, 1999, IEEE T IMAGE PROCESS, V8, P1221, DOI 10.1109/83.784434; Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780; Li A., 2018, PROC ASIAN C COMPUT, P197; Liang MY, 2019, AAAI CONF ARTIF INTE, P8706; Liu XH, 2013, IEEE T IMAGE PROCESS, V22, P5226, DOI 10.1109/TIP.2013.2283400; Liu ZW, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661277; Malvar HS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P485; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Menze Moritz, 2015, CVPR; Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x; Owen AB, 2007, CONTEMP MATH, V443, P59; Pinggera P, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.26; Reinhard E., 2010, HIGH DYNAMIC RANGE I, V2; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schechner YY, 2007, IEEE T PATTERN ANAL, V29, P1339, DOI 10.1109/TPAMI.2007.1151; Shin C, 2018, PROC CVPR IEEE, P4748, DOI 10.1109/CVPR.2018.00499; Tosi F, 2019, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR.2019.01003; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wu Y, 2017, IEEE ICC; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhao P, 2019, IEEE IC COMP COM NET; Zhi D, 2018, IEEE INT CONF COMP	58	0	0	2	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					652	668		10.1007/s11263-021-01565-6	http://dx.doi.org/10.1007/s11263-021-01565-6		JAN 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI					2022-12-18	WOS:000745601700002
J	Wald, J; Navab, N; Tombari, F				Wald, Johanna; Navab, Nassir; Tombari, Federico			Learning 3D Semantic Scene Graphs with Instance Embeddings	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene graphs; 3D scene understanding; Semantic segmentation		A 3D scene is more than the geometry and classes of the objects it comprises. An essential aspect beyond object-level perception is the scene context, described as a dense semantic network of interconnected nodes. Scene graphs have become a common representation to encode the semantic richness of images, where nodes in the graph are object entities connected by edges, so-called relationships. Such graphs have been shown to be useful in achieving state-of-the-art performance in image captioning, visual question answering and image generation or editing. While scene graph prediction methods so far focused on images, we propose instead a novel neural network architecture for 3D data, where the aim is to learn to regress semantic graphs from a given 3D scene. With this work, we go beyond object-level perception, by exploring relations between object entities. Our method learns instance embeddings alongside a scene segmentation and is able to predict semantics for object nodes and edges. We leverage 3DSSG, a large scale dataset based on 3RScan that features scene graphs of changing 3D scenes. Finally, we show the effectiveness of graphs as an intermediate representation on a retrieval task.	[Wald, Johanna; Navab, Nassir; Tombari, Federico] Tech Univ Munich, CAMP, Munich, Germany; [Tombari, Federico] Google Inc, Zurich, Switzerland	Technical University of Munich; Google Incorporated	Wald, J (corresponding author), Tech Univ Munich, CAMP, Munich, Germany.	johanna.wald@tum.de; navab@in.tum.de; tombari@in.tum.de			Projekt DEAL	Projekt DEAL	Open Access funding enabled and organized by Projekt DEAL.	Abadi M., TENSORFLOW LARGE SCA; Abdul-Rashid H., 2018, EUR WORKSH 3D OBJ RE; Abdul-Rashid H., 2019, EUROGRAPHICS WORKSHO; Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI 10.1109/ICRA.2019.8794387; ARANDJELOVIC R, 2016, C COMP VIS PATT REC; Armeni I, 2019, IEEE I CONF COMP VIS, P5663, DOI 10.1109/ICCV.2019.00576; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Ashual O, 2019, IEEE I CONF COMP VIS, P4560, DOI 10.1109/ICCV.2019.00466; Avetisyan Armen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P596, DOI 10.1007/978-3-030-58542-6_36; Avetisyan A, 2019, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2019.00272; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dahnert M, 2019, IEEE I CONF COMP VIS, P8748, DOI 10.1109/ICCV.2019.00884; Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dhamo H, 2020, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR42600.2020.00526; Engelmann F, 2017, IEEE INT CONF COMP V, P716, DOI 10.1109/ICCVW.2017.90; Engelmann Francis, 2020, P IEEE CVF C COMP VI, P9031, DOI DOI 10.1109/CVPR42600.2020.00905; Fellbaum C, 1998, LANG SPEECH & COMMUN, P1; Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929; Funkhouser, 2020, EUR C COMP VIS; Galvez-Lopez D, 2011, IEEE INT C INT ROBOT, P51, DOI 10.1109/IROS.2011.6048525; Gay P, 2019, LECT NOTES COMPUT SC, V11363, P330, DOI 10.1007/978-3-030-20893-6_21; Gibson J.J., 1979, ECOLOGICAL APPROACH; Glocker B, 2015, IEEE T VIS COMPUT GR, V21, P571, DOI 10.1109/TVCG.2014.2360403; Google Research, 2021, TF3D; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Han L, 2020, PROC CVPR IEEE, P2937, DOI 10.1109/CVPR42600.2020.00301; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Herzig R, 2018, ADV NEUR IN, V31; Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455; Huang JW, 2019, PROC CVPR IEEE, P4435, DOI 10.1109/CVPR.2019.00457; Huang SY, 2018, ADV NEUR IN, V31; Izadinia H, 2017, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2017.260; Jiang CFF, 2018, INT J COMPUT VISION, V126, P920, DOI 10.1007/s11263-018-1103-5; Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990; Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Kulkarni N, 2019, IEEE I CONF COMP VIS, P2212, DOI 10.1109/ICCV.2019.00230; Lahoud J, 2019, IEEE I CONF COMP VIS, P9255, DOI 10.1109/ICCV.2019.00935; Li MY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3303766; Li YY, 2018, ADV NEUR IN, V31; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu TQ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661243; Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045; Liu Yunze, 2020, ABS201213089 CORR; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lu G., 2016, MULTIMEDIA MODELING, P218; Ma R, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275035; Mittal G., 2019, ICLR DEEP GEN MOD HI; Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527; Najibi Mahyar, 2020, P IEEE CVF C COMP VI, P11913; Narita G, 2019, IEEE INT C INT ROBOT, P4205, DOI 10.1109/IROS40897.2019.8967890; Newell A, 2017, ADV NEUR IN, V30; Nie YY, 2020, PROC CVPR IEEE, P52, DOI 10.1109/CVPR42600.2020.00013; Peyre J, 2017, IEEE I CONF COMP VIS, P5189, DOI 10.1109/ICCV.2017.554; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Qi MS, 2019, PROC CVPR IEEE, P3952, DOI 10.1109/CVPR.2019.00408; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rethage D, 2018, LECT NOTES COMPUT SC, V11208, P625, DOI 10.1007/978-3-030-01225-0_37; Ronneberger O, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rosinol A, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI; Rosinol A, 2020, IEEE INT CONF ROBOT, P1689, DOI 10.1109/ICRA40945.2020.9196885; Shi YF, 2019, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2019.00187; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Sohn K, 2016, ADV NEUR IN, V29; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314; Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621; Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Torii A, 2015, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2015.7298790; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wald Johanna, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P467, DOI 10.1007/978-3-030-58571-6_28; Wald J, 2020, PROC CVPR IEEE, P3960, DOI 10.1109/CVPR42600.2020.00402; Wald J, 2019, IEEE I CONF COMP VIS, P7657, DOI 10.1109/ICCV.2019.00775; Wang M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661278; Xia F, 2018, PROC CVPR IEEE, P9068, DOI 10.1109/CVPR.2018.00945; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094; Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407; Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611; Zhang J, 2019, PROC CVPR IEEE, P11527, DOI 10.1109/CVPR.2019.01180; Zhao Y., 2011, ADV NEURAL INFORM PR, P73; Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110; Zhou J., 2016, NEUROCOMPUTING; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472	98	0	0	10	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					630	651		10.1007/s11263-021-01546-9	http://dx.doi.org/10.1007/s11263-021-01546-9		JAN 2022	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI		hybrid			2022-12-18	WOS:000745601700001
J	Talon, T; Pellegrino, S				Talon, Thibaud; Pellegrino, Sergio			Inextensible Surface Reconstruction Under Small Relative Deformations from Distributed Angle Measurements	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Inextensible; Surface; Reconstruction; Distributed sensors; Angle measurements; Metric tensor	NONRIGID 3D SHAPE; TEMPLATE; SMOOTH; DESIGN; SENSOR	A mathematical model to measure the shape of a 3D surface using angle measurements from embedded sensors is presented. The surface is known in a reference configuration and is assumed to have deformed inextensibly to its current shape. An inextensibility condition is enforced through a discretization of the metric tensor generating a finite number of constraints. This model allows to parameterize the shape of the surface using a small number of unknowns which leads to a small number of sensors. We study the singularities of the equations and derive necessary conditions for the problem to be well-posed as well as limitations of the algorithm. Simulations and experiments are performed on developable surfaces under relatively small deformations to analyze the performance of the method and to show the influence of the parameters used in our algorithm. Overall, the proposed method outperforms the current state-of-the-art by almost an order of magnitude.	[Talon, Thibaud] Grad Aerosp Labs, Pasadena, CA 91125 USA; [Pellegrino, Sergio] CALTECH, Grad Aerosp Labs, 1200 E Calif Blvd MS 105-05, Pasadena, CA 91125 USA	California Institute of Technology	Talon, T (corresponding author), Grad Aerosp Labs, Pasadena, CA 91125 USA.	thibaud.talon@gmail.com; sergiop@caltech.edu			Space Solar Power Project at Caltech	Space Solar Power Project at Caltech	Financial support from the Space Solar Power Project at Caltech is gratefully acknowledged.	Abbena E., 2017, MODERN DIFFERENTIAL; Abramowitz M., 1948, HDB MATH FUNCTIONS F, V55; Bartoli A, 2015, IEEE T PATTERN ANAL, V37, P2099, DOI 10.1109/TPAMI.2015.2392759; Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802; Boden RC, 2018, J SPACECRAFT ROCKETS, V55, P415, DOI 10.2514/1.A33941; Brunet F, 2014, COMPUT VIS IMAGE UND, V125, P138, DOI 10.1016/j.cviu.2014.04.003; Brunet F, 2011, LECT NOTES COMPUT SC, V6494, P52, DOI 10.1007/978-3-642-19318-7_5; Chhatkuli A, 2017, IEEE T PATTERN ANAL, V39, P833, DOI 10.1109/TPAMI.2016.2562622; Coons S, 1974, COMPUT AIDED GEOM D, P1; Dierckx Paul, 1995, CURVE SURFACE FITTIN, P1; Floudas C., 2001, ENCY OPTIMIZATION; Fornberg B, 2007, COMPUT MATH APPL, V54, P379, DOI 10.1016/j.camwa.2007.01.028; Gallardo M, 2017, IEEE I CONF COMP VIS, P3904, DOI 10.1109/ICCV.2017.419; Goldenthal R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276438; GOSHTASBY A, 1993, INT J COMPUT VISION, V10, P233, DOI 10.1007/BF01539537; Hermanis A, 2016, IEEE SENS J, V16, P1271, DOI 10.1109/JSEN.2015.2496283; Hoshi Takayuki, 2008, SICE 2008 - 47th Annual Conference of the Society of Instrument and Control Engineers of Japan, P915, DOI 10.1109/SICE.2008.4654785; Huard M, 2013, NUMER ALGORITHMS, V63, P483, DOI 10.1007/s11075-012-9633-3; Khatamian A, 2016, J INF PROCESS SYST, V12, P338, DOI 10.3745/JIPS.01.0010; Kreyszig E., 1968, INTRO DIFFERENTIAL G; Malvern L.E., 1969, INTRO MECH CONTINUOU; Metaxas D., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P337, DOI 10.1109/CVPR.1991.139712; Nielson GM, 2004, IEEE T VIS COMPUT GR, V10, P224, DOI 10.1109/TVCG.2004.1260774; Perriollat M, 2013, COMPUT ANIMAT VIRT W, V24, P457, DOI 10.1002/cav.1478; Perriollat M, 2011, INT J COMPUT VISION, V95, P124, DOI 10.1007/s11263-010-0352-8; Ricci MG, 1901, MATH ANN, V54, P125; Saguin-Sprynski N., 2014, P 7 EUR WORKSH STRUC, P702; Salzmann M, 2008, LECT NOTES COMPUT SC, V5305, P581, DOI 10.1007/978-3-540-88693-8_43; Salzmann M, 2007, IEEE T PATTERN ANAL, V29, P1481, DOI 10.1109/TPAMI.2007.1080; Sorkine O., 2007, P 5 EUR S GEOM PROC, V4, P109, DOI [DOI 10.2312/SGP/SGP07/109-116, 10.2312/SGP/SGP07/109-116]; Sprynski N, 2011, PECCS 2011: PROCEEDINGS OF THE 1ST INTERNATIONAL CONFERENCE ON PERVASIVE AND EMBEDDED COMPUTING AND COMMUNICATION SYSTEMS, P421; Stanko T, 2017, COMPUT GRAPH-UK, V66, P74, DOI 10.1016/j.cag.2017.05.013; Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531; Talon T., 2020, THESIS CALIFORNIA I; Talon T., 2017, 4 AIAA SPAC STRUCT C, P1116; Talon T, 2021, ACTA ASTRONAUT, V180, P328, DOI 10.1016/j.actaastro.2020.12.056; Talon T, 2018, IEEE METROL AEROSPAC, P581; Torresani L, 2004, ADV NEUR IN, V16, P1555; Trebi-Ollennu A, 2001, IEEE T ROBOTIC AUTOM, V17, P939, DOI 10.1109/70.976028; Varol A, 2012, IEEE T PATTERN ANAL, V34, P1118, DOI 10.1109/TPAMI.2011.196	40	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					594	614		10.1007/s11263-021-01552-x	http://dx.doi.org/10.1007/s11263-021-01552-x		JAN 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS					2022-12-18	WOS:000741215100001
J	Mao, Q; Tseng, HY; Lee, HY; Huang, JB; Ma, SW; Yang, MH				Mao, Qi; Tseng, Hung-Yu; Lee, Hsin-Ying; Huang, Jia-Bin; Ma, Siwei; Yang, Ming-Hsuan			Continuous and Diverse Image-to-Image Translation via Signed Attribute Vectors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image-to-image translation; Generative adversarial networks; Image synthesis	GENERATIVE ADVERSARIAL NETWORKS	Recent image-to-image (I2I) translation algorithms focus on learning the mapping from a source to a target domain. However, the continuous translation problem that synthesizes intermediate results between two domains has not been well-studied in the literature. Generating a smooth sequence of intermediate results bridges the gap of two different domains, facilitating the morphing effect across domains. Existing I2I approaches are limited to either intra-domain or deterministic inter-domain continuous translation. In this work, we present an effectively signed attribute vector, which enables continuous translation on diverse mapping paths across various domains. In particular, we introduce a unified attribute space shared by all domains that utilize the sign operation to encode the domain information, thereby allowing the interpolation on attribute vectors of different domains. To enhance the visual quality of continuous translation results, we generate a trajectory between two sign-symmetrical attribute vectors and leverage the domain information of the interpolated results along the trajectory for adversarial training. We evaluate the proposed method on a wide range of I2I translation tasks. Both qualitative and quantitative results demonstrate that the proposed framework generates more high-quality continuous translation results against the state-of-the-art methods.	[Mao, Qi] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China; [Tseng, Hung-Yu] Meta, Menlo Pk, CA 94025 USA; [Lee, Hsin-Ying] Snap Res, Santa Monica, CA 90405 USA; [Huang, Jia-Bin] Univ Maryland, Comp Sci, College Pk, MD 20742 USA; [Ma, Siwei] Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China; [Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95343 USA; [Yang, Ming-Hsuan] Yonsei Univ, Seoul, South Korea; [Yang, Ming-Hsuan] Google, Mountain View, CA 94043 USA	Communication University of China; University System of Maryland; University of Maryland College Park; Peking University; University of California System; University of California Merced; Yonsei University; Google Incorporated	Yang, MH (corresponding author), Univ Calif Merced, Merced, CA 95343 USA.; Yang, MH (corresponding author), Yonsei Univ, Seoul, South Korea.; Yang, MH (corresponding author), Google, Mountain View, CA 94043 USA.	qimao@cuc.edu.cn; hungyutseng@fb.com; hlee5@snap.com; jbhuang0604@gmail.com; swma@pku.edu.cn; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304	National Natural Science Foundation of China [62025101]; China Scholarship Council; High-performance Computing Platform of Peking University; State Key Laboratory of Media Convergence and Communication (Communication University of China); Fundamental Research Funds for the Central Universities; NSF CAREER grant [1149783]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Scholarship Council(China Scholarship Council); High-performance Computing Platform of Peking University; State Key Laboratory of Media Convergence and Communication (Communication University of China); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); NSF CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD))	Q. Mao and S. Ma are supported in part by the National Natural Science Foundation of China (62025101), China Scholarship Council for 1 year visiting at the University of California at Merced, and High-performance Computing Platform of Peking University, State Key Laboratory of Media Convergence and Communication (Communication University of China), and Fundamental Research Funds for the Central Universities. H.-Y. Lee, H.-Y. Tseng andM.-H. Yang are supported in part by NSF CAREER grant 1149783.	Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453; Abdal Rameen, 2020, P IEEE CVF C COMP VI, P8296, DOI 10.1109/CVPR42600.2020.00832; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Brock A., 2018, ICLR, P1; Burkov E., 2020, CVPR, P13786; Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hensel M, 2017, ADV NEUR IN, V30; Hinton, 2016, ARXIV PREPRINT ARXIV; Hoffman J, 2018, PR MACH LEARN RES, V80; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Kotovenko D, 2019, IEEE I CONF COMP VIS, P4421, DOI 10.1109/ICCV.2019.00452; Lample G, 2017, ADV NEUR IN, V30; Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Liao J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629494; Lira Wallace, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P363, DOI 10.1007/978-3-030-58574-7_22; Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065; Liu MY, 2017, ADV NEUR IN, V30; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152; Mescheder L, 2018, PR MACH LEARN RES, V80; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Paszke A., 2017, AUTOMATIC DIFFERENTI; Saito Kuniaki, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P382, DOI 10.1007/978-3-030-58580-8_23; Ulyanov D., 2016, ARXIV160708022; Voynov Andrey, 2020, ICML, P9786; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wolberg G, 1998, VISUAL COMPUT, V14, P360, DOI 10.1007/s003710050148; Wu PW, 2019, IEEE I CONF COMP VIS, P5913, DOI 10.1109/ICCV.2019.00601; Wu WY, 2019, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2019.00820; Yu Xiaoming, 2019, ADV NEURAL INFORM PR; Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhao SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3490; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	47	0	0	5	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					517	549		10.1007/s11263-021-01557-6	http://dx.doi.org/10.1007/s11263-021-01557-6		JAN 2022	33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		Green Submitted			2022-12-18	WOS:000739783300002
J	Shin, HY; Oh, HS				Shin, Ha-Young; Oh, Hee-Seok			Robust Geodesic Regression	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Geodesic regression; Manifold statistics; M-type estimators; Riemannian manifolds; Robust statistics		This paper studies robust regression for data on Riemannian manifolds. Geodesic regression is the generalization of linear regression to a setting with a manifold-valued dependent variable and one or more real-valued independent variables. The existing work on geodesic regression uses the sum-of-squared errors to find the solution, but as in the classical Euclidean case, the least-squares method is highly sensitive to outliers. In this paper, we use M-type estimators, including the L-1, Huber and Tukey biweight estimators, to perform robust geodesic regression, and describe how to calculate the tuning parameters for the latter two. We show that, on compact symmetric spaces, all M-type estimators are maximum likelihood estimators, and argue in favor of a general preference for the L-1 estimator over the L-2 and Huber estimators on high-dimensional spaces. A derivation of the Riemannian normal distribution on S-n and H-n is also included. Results from numerical examples, including analysis of real neuroimaging data, demonstrate the promising empirical properties of the proposed approach.	[Shin, Ha-Young; Oh, Hee-Seok] Seoul Natl Univ, Dept Stat, Seoul 08826, South Korea	Seoul National University (SNU)	Oh, HS (corresponding author), Seoul Natl Univ, Dept Stat, Seoul 08826, South Korea.	heeseok@stats.snu.ac.kr			National Research Foundation of Korea (NRF) - Korea government [2020R1A4A1018207, 2021R1A2C1091357]	National Research Foundation of Korea (NRF) - Korea government(National Research Foundation of Korea)	This research was supported by the National Research Foundation of Korea (NRF) funded by the Korea government (2020R1A4A1018207; 2021R1A2C1091357).	Banerjee M, 2016, PROC CVPR IEEE, P4424, DOI 10.1109/CVPR.2016.479; Cheng G, 2013, SIAM J IMAGING SCI, V6, P592, DOI 10.1137/110853376; Cornea E, 2017, J R STAT SOC B, V79, P463, DOI 10.1111/rssb.12169; Cotton A., 2002, INT J MATH MATH SCI, V32, P641, DOI DOI 10.1155/S0161171202207188; Davis BC, 2010, INT J COMPUT VISION, V90, P255, DOI 10.1007/s11263-010-0367-1; do Carmo M. P, 1992, RIEMANNIAN GEOMETRY; Du J, 2014, NEUROIMAGE, V87, P416, DOI 10.1016/j.neuroimage.2013.06.081; Fletcher PT, 2013, INT J COMPUT VISION, V105, P171, DOI 10.1007/s11263-012-0591-y; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; Fletcher T., 2020, RIEMANNIAN GEOMETRIC, P39, DOI [10.1016/B978-0-12-814725-2.00009-1, DOI 10.1016/B978-0-12-814725-2.00009-1]; Hein, 2008, P ADV NEUR INF PROC, V21, P1561; Hein M., 2009, ADV NEURAL INFORM PR, V22; Hinkle J, 2014, J MATH IMAGING VIS, V50, P32, DOI 10.1007/s10851-013-0489-5; Hong Y, 2016, IEEE T PATTERN ANAL, V38, P2284, DOI 10.1109/TPAMI.2016.2516533; Kim HJ, 2014, PROC CVPR IEEE, P2705, DOI 10.1109/CVPR.2014.352; Mortici C, 2012, APPL MATH LETT, V25, P717, DOI 10.1016/j.aml.2011.10.008; Shin H.-Y., 2020, THESIS SEOUL NATL U; Steinke F, 2010, SIAM J IMAGING SCI, V3, P527, DOI 10.1137/080744189; Zhang XW, 2019, IEEE T PATTERN ANAL, V41, P444, DOI 10.1109/TPAMI.2017.2776260	19	0	0	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					478	503		10.1007/s11263-021-01561-w	http://dx.doi.org/10.1007/s11263-021-01561-w		JAN 2022	26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		Green Submitted			2022-12-18	WOS:000739265400001
J	Guo, CA; Zuo, XX; Wang, S; Liu, XS; Zou, SH; Gong, ML; Cheng, L				Guo, Chuan; Zuo, Xinxin; Wang, Sen; Liu, Xinshuang; Zou, Shihao; Gong, Minglun; Cheng, Li			Action2video: Generating Videos of Human 3D Actions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D human motion generation; Video motion synthesis; Lie algebraic human representation; Temporal variational autoencoders	ACTION RECOGNITION; MOTION	We aim to tackle the interesting yet challenging problem of generating videos of diverse and natural human motions from prescribed action categories. The key issue lies in the ability to synthesize multiple distinct motion sequences that are realistic in their visual appearances. It is achieved in this paper by a two-step process that maintains internal 3D pose and shape representations, action2motion and motion2video. Action2motion stochastically generates plausible 3D pose sequences of a prescribed action category, which are processed and rendered by motion2video to form 2D videos. Specifically, the Lie algebraic theory is engaged in representing natural human motions following the physical law of human kinematics; a temporal variational auto-encoder is developed that encourages diversity of output motions. Moreover, given an additional input image of a clothed human character, an entire pipeline is proposed to extract his/her 3D detailed shape, and to render in videos the plausible motions from different views. This is realized by improving existing methods to extract 3D human shapes and textures from single 2D images, rigging, animating, and rendering to form 2D videos of human motions. It also necessitates the curation and reannotation of 3D human motion datasets for training purpose. Thorough empirical experiments including ablation study, qualitative and quantitative evaluations manifest the applicability of our approach, and demonstrate its competitiveness in addressing related tasks, where components of our approach are compared favorably to the state-of-the-arts.	[Guo, Chuan; Zuo, Xinxin; Wang, Sen; Zou, Shihao; Cheng, Li] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB, Canada; [Liu, Xinshuang] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China; [Gong, Minglun] Univ Guelph, Sch Comp Sci, Guelph, ON, Canada	University of Alberta; Tsinghua University; University of Guelph	Guo, CA (corresponding author), Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB, Canada.	cguo2@ualberta.ca; xzuo@ualberta.ca; sen9@ualberta.ca; liuxs17@mails.tsinghua.edu.cn; szou2@ualberta.ca; minglun@uoguelph.ca; lcheng5@ualberta.ca		Zuo, Xinxin/0000-0002-7116-9634; Wang, Sen/0000-0002-1808-5239; guo, chuan/0000-0002-4539-0634	NSERC; UAHJIC	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); UAHJIC	This work is partly supported by the NSERC Discovery Grant and the UAHJIC grants. We would like to thank the Amazon MTurk workers for their efforts in user studies, and the anonymous reviewers for helping to improve the presentation of our manuscript. We also want to thank Chungyi Weng, Yuanlu Xu and Zerong Zheng for their great help on reproducing the works of Photo Wake-Up, ARCH and PaMIR.	Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392462; Adeli V, 2020, IEEE ROBOT AUTOM LET, V5, P6033, DOI 10.1109/LRA.2020.3010742; Ahn H, 2018, IEEE INT CONF ROBOT, P5915; Ahuja C, 2019, INT CONF 3D VISION, P719, DOI 10.1109/3DV.2019.00084; Aksan E, 2019, IEEE I CONF COMP VIS, P7143, DOI 10.1109/ICCV.2019.00724; Aliakbarian S, 2020, PROC CVPR IEEE, P5222, DOI 10.1109/CVPR42600.2020.00527; Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875; Chaaraoui AA, 2014, EXPERT SYST APPL, V41, P786, DOI 10.1016/j.eswa.2013.08.009; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bengio S, 2015, ADV NEUR IN, V28; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Cai HY, 2018, LECT NOTES COMPUT SC, V11206, P374, DOI 10.1007/978-3-030-01216-8_; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603; Chung J, 2015, ADV NEUR IN, V28; CMU, 2003, CMU GRAPH LAB MOT CA; de Souza CR, 2020, INT J COMPUT VISION, V128, P1505, DOI 10.1007/s11263-019-01222-z; Denton E, 2018, PR MACH LEARN RES, V80; Gao H, 2019, IEEE I CONF COMP VIS, P9005, DOI 10.1109/ICCV.2019.00910; Gavrila D, 1995, INT WORKSH AUT FAC G, P272; Geman S, 1987, B INT STAT I, V4, P5; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Guo CA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2021, DOI 10.1145/3394171.3413635; Habibie Ikhsanul, 2017, 28 BRIT MACH VIS C; Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011; He JW, 2018, LECT NOTES COMPUT SC, V11209, P466, DOI 10.1007/978-3-030-01228-1_28; Higgins I, 2016, BETA VAE LEARNING BA; Hornung A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186645; Huang R, 2021, INT C LEARN REPR; Huang Z, 2020, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR42600.2020.00316; Huang ZW, 2017, PROC CVPR IEEE, P1243, DOI 10.1109/CVPR.2017.137; Hussein Mohamed E, 2013, 23 INT JOINT C ART I; Jingwei Xu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P178, DOI 10.1007/978-3-030-58621-8_11; Kim Y, 2019, ADV NEUR IN, V32; Kingma D.P, P 3 INT C LEARNING R; Kingma DP, 2014, ADV NEUR IN, V27; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lazova V, 2019, INT CONF 3D VISION, P643, DOI 10.1109/3DV.2019.00076; Lee HY, 2019, ADV NEUR IN, V32; Lee Jun-Tae, 2020, INT C LEARN REPR; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Lin Angela S, 2018, NEURIPS WORKSH VIS G, V1; Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873; Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600; Liu ZG, 2019, PROC CVPR IEEE, P9996, DOI 10.1109/CVPR.2019.01024; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Marwah T, 2017, IEEE I CONF COMP VIS, P1435, DOI 10.1109/ICCV.2017.159; Muller M., 2007, INFORM RETRIEVAL MUS, V3, P69, DOI [10.1007/978-3-540- 74048-3_4, DOI 10.1007/978-3-540-74048-3]; Muller M., 2007, MOCAP DATABASE HDM05; Murray R. M., 1994, MATH INTRO ROBOTIC M, V1; Pavllo D, 2020, INT J COMPUT VISION, V128, P855, DOI 10.1007/s11263-019-01245-6; Plappert M, 2018, ROBOT AUTON SYST, V109, P13, DOI 10.1016/j.robot.2018.07.006; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016; Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239; Schonfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844; Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312; Shlizerman E, 2018, PROC CVPR IEEE, P7574, DOI 10.1109/CVPR.2018.00790; Siarohin A, 2019, PROC CVPR IEEE, P2372, DOI 10.1109/CVPR.2019.00248; Sohn K, 2015, ADV NEUR IN, V28; Sorkine O., 2007, P 5 EUR S GEOM PROC, V4, P109, DOI [DOI 10.2312/SGP/SGP07/109-116, 10.2312/SGP/SGP07/109-116]; Stoll S, 2020, INT J COMPUT VISION, V128, P891, DOI 10.1007/s11263-019-01281-2; Tang TR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1598, DOI 10.1145/3240508.3240526; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Villegas R, 2018, PROC CVPR IEEE, P8639, DOI 10.1109/CVPR.2018.00901; Vondrick C, 2017, PROC CVPR IEEE, P2992, DOI 10.1109/CVPR.2017.319; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang TC, 2019, ADV NEUR IN, V32; Wang TC, 2018, ADV NEUR IN, V31; Wang TH, 2019, IEEE I CONF COMP VIS, P10490, DOI 10.1109/ICCV.2019.01059; Wang ZY, 2020, AAAI CONF ARTIF INTE, V34, P12281; Weng CY, 2019, PROC CVPR IEEE, P5901, DOI 10.1109/CVPR.2019.00606; Wu Y, 2020, PROC CVPR IEEE, P5538, DOI 10.1109/CVPR42600.2020.00558; Xu C, 2017, INT J COMPUT VISION, V123, P454, DOI 10.1007/s11263-017-0998-6; Yacoob Y, 1999, COMPUT VIS IMAGE UND, V73, P232, DOI 10.1006/cviu.1998.0726; Yamada T, 2018, IEEE ROBOT AUTOM LET, V3, P3441, DOI 10.1109/LRA.2018.2852838; Yan SJ, 2019, IEEE I CONF COMP VIS, P4393, DOI 10.1109/ICCV.2019.00449; Yan XC, 2018, LECT NOTES COMPUT SC, V11209, P276, DOI 10.1007/978-3-030-01228-1_17; Yang CH, 2018, PROCEEDINGS OF 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION SYSTEMS (ICCCS), P201; Yen-Chi Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P159, DOI 10.1007/978-3-030-58571-6_10; Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582; Zhang YP, 2012, IEEE VTS VEH TECHNOL; Zhao R, 2020, PROC CVPR IEEE, P6224, DOI 10.1109/CVPR42600.2020.00626; Zhao R, 2018, AAAI CONF ARTIF INTE, P2636; Zheng Ding, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7917, DOI 10.1109/CVPR42600.2020.00794; Zheng Zerong, 2021, IEEE T PATTERN ANAL, V2; Zhou Z., 2012, SA 12 ASS COMPUTING, P33, DOI DOI 10.1145/2407746.2407779; Zhu YZ, 2020, PROC CVPR IEEE, P6537, DOI 10.1109/CVPR42600.2020.00657; Zuo XX, 2021, IEEE T MULTIMEDIA, V23, P1617, DOI 10.1109/TMM.2020.3001506	97	0	0	2	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					285	315		10.1007/s11263-021-01550-z	http://dx.doi.org/10.1007/s11263-021-01550-z		JAN 2022	31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		Green Submitted			2022-12-18	WOS:000738511800006
J	Rameau, F; Bailo, O; Park, J; Joo, K; Kweon, IS				Rameau, Francois; Bailo, Oleksandr; Park, Jinsun; Joo, Kyungdon; Kweon, In So			Real-Time Multi-Car Localization and See-Through System	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Simultaneous localization and mapping; Collaborative SLAM; Connected vehicles; See-through	SLAM; ACCURACY	In this paper, we propose a multi-vehicle localization approach relying exclusively on cameras installed on connected cars (e.g. vehicles with Internet access). The proposed method is designed to perform in real-time while requiring a low bandwidth connection as a result of an efficient distributed architecture. Hence, our approach is compatible with both LTE Internet connection and local Wi-Fi networks. To reach this goal, the vehicles share small portions of their respective 3D maps to estimate their relative positions. The global consistency between multiple vehicles is enforced via a novel graph-based strategy. The efficiency of our system is highlighted through a series of real experiments involving multiple vehicles. Moreover, the usefulness of our technique is emphasized by an innovative and unique multi-car see-through system resolving the inherent limitations of the previous approaches. A video demonstration is available via: https://youtu.be/GD7Z95bWP6k.	[Rameau, Francois; Bailo, Oleksandr; Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea; [Park, Jinsun] Pusan Natl Univ, Sch Comp Sci & Engn, Busan, South Korea; [Joo, Kyungdon] UNIST, Artificial Intelligence Grad Sch, Ulsan, South Korea; [Joo, Kyungdon] UNIST, Dept Comp Sci & Engn, Ulsan, South Korea	Korea Advanced Institute of Science & Technology (KAIST); Pusan National University; Ulsan National Institute of Science & Technology (UNIST); Ulsan National Institute of Science & Technology (UNIST)	Rameau, F (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	frameau@kaist.ac.kr; sasha94@kaist.ac.kr; jspark@pusan.ac.kr; kdjoo369@gmail.com; iskweon77@kaist.ac.kr		Rameau, Francois/0000-0001-5031-7653	Shared Sensing for Cooperative Cars Project - Bosch (China) Investment Ltd.; Korea Research Fellowship (KRF) Program through the NRF - Ministry of Science, ICT and Future Planning [2015H1D3A1066564]; Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education [NRF-2021R1I1A1A01060267]; Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT) [2020-0-01336]; National Research Foundation of Korea (NRF) - Korea government (MSIT) [NRF-2021R1C1C1005723]	Shared Sensing for Cooperative Cars Project - Bosch (China) Investment Ltd.; Korea Research Fellowship (KRF) Program through the NRF - Ministry of Science, ICT and Future Planning(National Research Foundation of Korea); Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education(National Research Foundation of KoreaMinistry of Education (MOE), Republic of KoreaNational Research Council for Economics, Humanities & Social Sciences, Republic of Korea); Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); National Research Foundation of Korea (NRF) - Korea government (MSIT)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This research was supported by the Shared Sensing for Cooperative Cars Project funded by Bosch (China) Investment Ltd. The first author was supported by Korea Research Fellowship (KRF) Program through the NRF funded by the Ministry of Science, ICT and Future Planning (2015H1D3A1066564). The third author (Jinsun Park) was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF-2021R1I1A1A01060267). The fourth author (Kyungdon Joo) was supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2020-0-01336, Artificial Intelligence Graduate School Program (UNIST)) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2021R1C1C1005723).	Bailo O., 2017, ARXIV171007434; Bailo O, 2018, PATTERN RECOGN LETT, V106, P53, DOI 10.1016/j.patrec.2018.02.020; Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033; Chen HI, 2015, IEEE I CONF COMP VIS, P3110, DOI 10.1109/ICCV.2015.356; Chen XYL, 2018, IEEE ANN INT CONF CY, P73, DOI 10.1109/CYBER.2018.8688219; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Cieslewski T., 2017, ARXIV171005772; Dubois R, 2019, IEEE INT C INT ROBOT, P2123, DOI 10.1109/IROS40897.2019.8967617; Feilner M., 2006, OPENVPN BUILDING INT; Forster C, 2013, IEEE INT C INT ROBOT, P3963; Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158; Geiger A., 2012, P IEEE COMP SOC C CO; Geiger A, 2011, LECT NOTES COMPUT SC, V6492, P25, DOI 10.1007/978-3-642-19315-6_3; Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405; Golodetz S, 2018, IEEE T VIS COMPUT GR, V24, P2895, DOI 10.1109/TVCG.2018.2868533; Ha H., 2015, PSIVT; Ha H, 2017, IEEE I CONF COMP VIS, P5354, DOI 10.1109/ICCV.2017.571; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Ikeda S, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P354, DOI 10.1109/ISMAR-Adjunct.2018.00103; Karrer M, 2018, IEEE ROBOT AUTOM LET, V3, P2762, DOI 10.1109/LRA.2018.2837226; Klein G, 2009, INT SYM MIX AUGMENT, P83, DOI 10.1109/ISMAR.2009.5336495; Kneip L, 2011, PROC CVPR IEEE; Levinson J, 2011, IEEE INT VEH SYM, P163, DOI 10.1109/IVS.2011.5940562; Liu J, 2017, IEEE T INTELL TRANSP, V18, P2111, DOI 10.1109/TITS.2016.2633999; Liu JY, 2012, INT C PATT RECOG, P2055; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Miksik O, 2012, INT C PATT RECOG, P2681; Mohanarajah G, 2015, IEEE T AUTOM SCI ENG, V12, P423, DOI 10.1109/TASE.2015.2408456; Moratuwage MDP, 2010, I C CONT AUTOMAT ROB, P1422, DOI 10.1109/ICARCV.2010.5707778; Morrison JG, 2016, SPRINGER TRAC ADV RO, V112, P119, DOI 10.1007/978-4-431-55879-8_9; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Naja R., 2013, WIRELESS VEHICULAR N, V2013; Nettleton EW, 2000, P SOC PHOTO-OPT INS, V4196, P337, DOI 10.1117/12.403733; Olaverri-Monreal C, 2010, IEEE INT VEH SYM, P123, DOI 10.1109/IVS.2010.5548020; Pereira FI, 2018, IEEE T INTELL TRANSP, V19, P3584, DOI 10.1109/TITS.2018.2853579; Perron J. M., 2015, WORKSH MULT GEOM ROB, P1339; Persson M, 2015, IEEE INT VEH SYM, P686, DOI 10.1109/IVS.2015.7225764; Piasco N, 2018, PATTERN RECOGN, V74, P90, DOI 10.1016/j.patcog.2017.09.013; Pire T, 2017, ROBOT AUTON SYST, V93, P27, DOI 10.1016/j.robot.2017.03.019; Qin T., 2019, ARXIV PREPRINT ARXIV; Rameau F., 2016, ECCV WORKSHOP; Rameau F., 2018, IW FCV; Rameau F, 2016, IEEE T VIS COMPUT GR, V22, P2395, DOI 10.1109/TVCG.2016.2593768; Rezaei S, 2007, IEEE T CONTR SYST T, V15, P1080, DOI 10.1109/TCST.2006.886439; Riazuelo L, 2014, ROBOT AUTON SYST, V62, P401, DOI 10.1016/j.robot.2013.11.007; Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Schmuck Patrik, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3863, DOI 10.1109/ICRA.2017.7989445; Schmuck P, 2019, J FIELD ROBOT, V36, P763, DOI 10.1002/rob.21854; Shen MC, 2019, IEEE T INTELL TRANSP, V20, P2255, DOI 10.1109/TITS.2018.2866232; Silpa-Anan C, 2008, PROC CVPR IEEE, P2308; Steiner AK, 2013, ATMOS CHEM PHYS, V13, P1469, DOI 10.5194/acp-13-1469-2013; Tao Z, 2013, IEEE INT C INTELL TR, P1509, DOI 10.1109/ITSC.2013.6728444; Van Opdenbosch D, 2018, IEEE WINT CONF APPL, P992, DOI 10.1109/WACV.2018.00114; Wei L., 2014, ECCV WORKSHOP; WHO, 2015, LOB STAT REP ROAD SA; Wimmer P, 2005, P SOC PHOTO-OPT INS, V5664, P400, DOI 10.1117/12.585481; Wing MG, 2005, J FOREST, V103, P169, DOI 10.1093/jof/103.4.169; Wiseman Y., 2018, INT J CONTROL AUTOM, V11, P151, DOI [10.14257/ijca.2018.11.2.13, DOI 10.14257/IJCA.2018.11.2.13]; Xu Z, 2017, J ADV TRANSPORT, V2017, P10; Yu, 2019, VIRTUAL REALITY INTE, V1, P461, DOI [10.1016/j.vrih.2019.09.002, DOI 10.1016/J.VRIH.2019.09.002]; Zheng YQ, 2013, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2013.291; Zou DP, 2013, IEEE T PATTERN ANAL, V35, P354, DOI 10.1109/TPAMI.2012.104	63	0	0	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					384	404		10.1007/s11263-021-01558-5	http://dx.doi.org/10.1007/s11263-021-01558-5		JAN 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS					2022-12-18	WOS:000738511800004
J	Zhao, JH; Xu, S; Zhang, BC; Gu, JX; Doermann, D; Guo, GD				Zhao, Junhe; Xu, Sheng; Zhang, Baochang; Gu, Jiaxin; Doermann, David; Guo, Guodong			Towards Compact 1-bit CNNs via Bayesian Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Quantization; Binarized network; Bayesian learning; Pruning	NEURAL-NETWORKS; SQUEEZE; MODEL	Deep convolutional neural networks (DCNNs) have dominated as the best performers on almost all computer vision tasks over the past several years. However, it remains a major challenge to deploy these powerful DCNNs in resource-limited environments, such as embedded devices and smartphones. To this end, 1-bit CNNs have emerged as a feasible solution as they are much more resource-efficient. Unfortunately, they often suffer from a significant performance drop compared to their full-precision counterparts. In this paper, we propose a novel Bayesian Optimized compact 1-bit CNNs (BONNs) model, which has the advantage of Bayesian learning, to improve the performance of 1-bit CNNs significantly. BONNs incorporate the prior distributions of full-precision kernels, features, and filters into a Bayesian framework to construct 1-bit CNNs in a comprehensive end-to-end manner. The proposed Bayesian learning algorithms are well-founded and used to optimize the network simultaneously in different kernels, features, and filters, which largely improves the compactness and capacity of 1-bit CNNs. We further introduce a new Bayesian learning-based pruning method for 1-bit CNNs, which significantly increases the model efficiency with very competitive performance. This enables our method to be used in a variety of practical scenarios. Extensive experiments on the ImageNet, CIFAR, and LFW datasets show that BONNs achieve the best in classification performance compared to a variety of state-of-the-art 1-bit CNN models. In particular, BONN achieves a strong generalization performance on the object detection task.	[Zhao, Junhe; Xu, Sheng; Zhang, Baochang] Beihang Univ, Beijing, Peoples R China; [Gu, Jiaxin] Tencent, Youtu Lab, Shanghai, Peoples R China; [Doermann, David] Univ Buffalo, Buffalo, NY USA; [Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing, Peoples R China; [Guo, Guodong] Natl Engn Lab Deep Learning Technol & Applicat, Beijing, Peoples R China	Beihang University; Tencent; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; Baidu	Zhang, BC (corresponding author), Beihang Univ, Beijing, Peoples R China.	jhzhao@buaa.edu.cn; shengxu@buaa.edu.cn; bezhang@buaa.edu.cn; jxgu1016@gmail.com; doermann@buffalo.edu; guoguodong01@baidu.com			National Natural Science Foundation of China [62076016]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The work was supported by the National Natural Science Foundation of China 62076016.	Adam, 2017, MOBILENETS EFFICIENT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bailin Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P639, DOI 10.1007/978-3-030-58536-5_38; Bishop C, 1997, J BRAZILIAN COMPUTER, V4, P61, DOI [10.1590/S0104-65001997000200006, DOI 10.1590/S0104-65001997000200006]; Blei DM, 2007, ANN APPL STAT, V1, P17, DOI 10.1214/07-AOAS114; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Courbariaux M, 2015, ADV NEUR IN, V28; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding XH, 2019, PROC CVPR IEEE, P4938, DOI 10.1109/CVPR.2019.00508; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Dong Y, 2014, COMPUTER SCI; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gu JX, 2019, IEEE I CONF COMP VIS, P4908, DOI 10.1109/ICCV.2019.00501; Gu JX, 2019, AAAI CONF ARTIF INTE, P8344; Guo YW, 2016, ADV NEUR IN, V29; Han S, 2015, ADV NEUR IN, V28; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Hastie T, 2009, ELEMENTS STAT LEARNI; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hu H., 2016, ARXIV PREPRINT ARXIV; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G.B., 2008, WORKSH FAC REAL LIF; Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang ZH, 2018, LECT NOTES COMPUT SC, V11220, P317, DOI 10.1007/978-3-030-01270-0_19; Hubara I, 2018, J MACH LEARN RES, V18; Jun Fang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P69, DOI 10.1007/978-3-030-58536-5_5; Jung S, 2019, PROC CVPR IEEE, P4345, DOI 10.1109/CVPR.2019.00448; Kingma DP, 2015, ADV NEUR IN, V28; Krizhevsky A., 2010, THE CIFAR 10 DATASET; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lampinen J, 2001, NEURAL NETWORKS, V14, P257, DOI 10.1016/S0893-6080(00)00098-8; Lemaire C, 2019, PROC CVPR IEEE, P9100, DOI 10.1109/CVPR.2019.00932; Leng C, 2018, AAAI CONF ARTIF INTE, P3466; [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1; Li H., 2017, P INT C LEARNING REP; Li YC, 2019, PROC CVPR IEEE, P2795, DOI 10.1109/CVPR.2019.00291; Li ZF, 2017, IEEE I CONF COMP VIS, P2603, DOI 10.1109/ICCV.2017.282; Liang FM, 2018, J AM STAT ASSOC, V113, P955, DOI 10.1080/01621459.2017.1409122; Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160; Lin SH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2425; Lin SH, 2019, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR.2019.00290; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin XF, 2017, ADV NEUR IN, V30; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu CL, 2019, PROC CVPR IEEE, P2686, DOI 10.1109/CVPR.2019.00280; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Liu H, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM - EDUCATIONAL RESEARCH AND EDUCATIONAL TECHNOLOGY, 2019, P3; Liu JC, 2021, 2021 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS 2021), DOI 10.1109/IWS52775.2021.9499583; Liu ZC, 2020, INT J COMPUT VISION, V128, P202, DOI 10.1007/s11263-019-01227-8; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Louizos C, 2017, ADV NEUR IN, V30; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Molchanov D, 2017, PR MACH LEARN RES, V70; Molchanov P., 2017, P INT C LEARNING REP, P1; Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250; Novikov A, 2015, ADV NEUR IN, V28; Phan H, 2020, IEEE WINT CONF APPL, P3442, DOI 10.1109/WACV45572.2020.9093444; Qin HT, 2020, PROC CVPR IEEE, P2247, DOI 10.1109/CVPR42600.2020.00232; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sengupta S, 2016, IEEE WINT CONF APPL; Shoukai Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P1, DOI 10.1007/978-3-030-58610-2_1; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Sun S., 2019, INT C LEARN REPR; Sun SY, 2017, PR MACH LEARN RES, V54, P1283; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Ullrich Karen, 2017, ICLR; Wan DW, 2018, LECT NOTES COMPUT SC, V11206, P322, DOI 10.1007/978-3-030-01216-8_20; Wan F, 2019, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2019.00230; Wang XD, 2018, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2018.00094; Wang ZW, 2020, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR42600.2020.00212; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wu SH, 2018, 2018 52ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS), DOI 10.1109/CISS.2018.8362280; Xu S., 2021, P IEEECVF C COMPUTER, P5682; Xu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5178, DOI 10.1109/ICCV48922.2021.00515; Yang TJ, 2017, PROC CVPR IEEE, P6071, DOI 10.1109/CVPR.2017.643; Yang Zhaohui, 2020, NEURIPS; Yaohui Cai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13166, DOI 10.1109/CVPR42600.2020.01318; Yoon J, 2017, PR MACH LEARN RES, V70; ZAGORUYKO S, 2015, 2015 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2015.7299064; Zechun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P143, DOI 10.1007/978-3-030-58568-6_9; Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579; Zheng Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P625, DOI 10.1007/978-3-030-58598-3_37; Zhou HP, 2019, PR MACH LEARN RES, V97; Zhou S., 2016, ARXIV160606160; Zhu Chenzhuo, 2017, ICLR; Zhuang BH, 2018, PROC CVPR IEEE, P7920, DOI 10.1109/CVPR.2018.00826; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	101	0	0	4	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					201	225		10.1007/s11263-021-01543-y	http://dx.doi.org/10.1007/s11263-021-01543-y		DEC 2021	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS					2022-12-18	WOS:000724649400001
J	Zhang, HK; Li, Y; Chen, H; Gong, CR; Bai, ZW; Shen, CH				Zhang, Haokui; Li, Ying; Chen, Hao; Gong, Chengrong; Bai, Zongwen; Shen, Chunhua			Memory-Efficient Hierarchical Neural Architecture Search for Image Restoration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Neural architecture search; Hierarchical search space; Image denosing; Super-resolution		Recently, much attention has been spent on neural architecture search (NAS), aiming to outperform those manually-designed neural architectures on high-level vision recognition tasks. Inspired by the success, here we attempt to leverage NAS techniques to automatically design efficient network architectures for low-level image restoration tasks. In particular, we propose a memory-efficient hierarchical NAS (termed HiNAS) and apply it to two such tasks: image denoising and image super-resolution. HiNAS adopts gradient based search strategies and builds a flexible hierarchical search space, including the inner search space and outer search space. They are in charge of designing cell architectures and deciding cell widths, respectively. For the inner search space, we propose a layer-wise architecture sharing strategy, resulting in more flexible architectures and better performance. For the outer search space, we design a cell-sharing strategy to save memory, and considerably accelerate the search speed. The proposed HiNAS method is both memory and computation efficient. With a single GTX1080Ti GPU, it takes only about 1 h for searching for denoising network on the BSD-500 dataset and 3.5 h for searching for the super-resolution structure on the DIV2K dataset. Experiments show that the architectures found by HiNAS have fewer parameters and enjoy a faster inference speed, while achieving highly competitive performance compared with state-of-the-art methods. Code is available at: https://github.com/hkzhang91/HiNAS	[Zhang, Haokui; Li, Ying; Gong, Chengrong] Northwestern Polytech Univ, Sch Comp Sci, Xian, Peoples R China; [Zhang, Haokui] Intellifus, Shenzhen, Peoples R China; [Bai, Zongwen] Yanan Univ, Shaanxi Key Lab Intelligent Proc Big Energy Data, Yanan, Peoples R China; [Chen, Hao; Shen, Chunhua] Zhejiang Univ, Hangzhou, Peoples R China	Northwestern Polytechnical University; Yanan University; Zhejiang University	Zhang, HK (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian, Peoples R China.; Zhang, HK (corresponding author), Intellifus, Shenzhen, Peoples R China.	hkzhang1991@mail.nwpu.edu.cn		Zhang, Haokui/0000-0002-4336-5558	National Natural Science Foundation of China [61871460, 61941112, 61761042]; Shaanxi Provincial Key RD Program [2020KW-003]; Natural Science Foundation of China of Shaanxi [2020JM-556]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shaanxi Provincial Key RD Program; Natural Science Foundation of China of Shaanxi	Part of this work was done when H. Zhang, H. Chen and C. Shen were with The University of Adelaide. This work was in part supported by National Natural Science Foundation of China (61871460, 61941112, 61761042), Shaanxi Provincial Key RD Program (2020KW-003) and Natural Science Foundation of China of Shaanxi (2020JM-556). Y. Li and C. Shen are the corresponding authors.	Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16; Bender Gabriel, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14311, DOI 10.1109/CVPR42600.2020.01433; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cai H, 2019, IEEE ICC; Chatterjee P, 2010, IEEE T IMAGE PROCESS, V19, P895, DOI 10.1109/TIP.2009.2037087; Chen S, 2019, ARXIV PREPRINT ARXIV; Chu X., 2019, FAST ACCURATE LIGHTW; Chu X., 2019, MULTIOBJECTIVE REINF; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong XY, 2019, ADV NEUR IN, V32; Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186; Elsken Thomas, 2018, ARXIV180409081; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084; Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082; Jaroensri R., 2019, GENERATING TRAINING; Jiemin Fang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10625, DOI 10.1109/CVPR42600.2020.01064; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Lai Wei-Sheng, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618; Lee W., 2020, LEARNING PRIVILEGED; Li L, 2020, PR MACH LEARN RES, V115, P367; LIANG H, 2019, DARTS IMPROVED DIFFE; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Liu D, 2018, ADV NEUR IN, V31; Liu Hanxiao, 2018, ICLR; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243; Liu P, 2019, MED IMAGE ANAL, V54, P306, DOI 10.1016/j.media.2019.03.004; Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717; Loshchilov I., 2017, P INT C LEARNING REP; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Mao XJ, 2016, ADV NEUR IN, V29; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Ning Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11940, DOI 10.1109/CVPR42600.2020.01196; Pham H, 2018, PR MACH LEARN RES, V80; Plotz T, 2018, ADV NEUR IN, V31; Suganuma M, 2018, PR MACH LEARN RES, V80; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Wan Alvin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12962, DOI 10.1109/CVPR42600.2020.01298; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xiaohe Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P352, DOI 10.1007/978-3-030-58548-8_21; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xu YH, 2021, IEEE T PATTERN ANAL, V43, P2953, DOI 10.1109/TPAMI.2021.3059510; Zhang C., 2018, INT C LEARN REPR; Zhang HK, 2020, PROC CVPR IEEE, P3654, DOI 10.1109/CVPR42600.2020.00371; ZHANG K, 2017, PROC CVPR IEEE, P2808, DOI DOI 10.1109/CVPR.2017.300; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang W, 2018, IEEE CONF COMPUT; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	68	0	0	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					157	178		10.1007/s11263-021-01537-w	http://dx.doi.org/10.1007/s11263-021-01537-w		NOV 2021	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC		Green Submitted			2022-12-18	WOS:000721645600001
J	Liu, T; Sun, JJ; Zhao, L; Zhao, JP; Yuan, LZ; Wang, YX; Chen, LC; Schroff, F; Adam, H				Liu, Ting; Sun, Jennifer J.; Zhao, Long; Zhao, Jiaping; Yuan, Liangzhe; Wang, Yuxiao; Chen, Liang-Chieh; Schroff, Florian; Adam, Hartwig			View-Invariant, Occlusion-Robust Probabilistic Embedding for Human Pose	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human pose embedding; Probabilistic embedding; View-invariant pose retrieval; Action retrieval; Occlusion Robustness		Recognition of human poses and actions is crucial for autonomous systems to interact smoothly with people. However, cameras generally capture human poses in 2D as images and videos, which can have significant appearance variations across viewpoints that make the recognition tasks challenging. To address this, we explore recognizing similarity in 3D human body poses from 2D information, which has not been well-studied in existing works. Here, we propose an approach to learning a compact view-invariant embedding space from 2D body joint keypoints, without explicitly predicting 3D poses. Input ambiguities of 2D poses from projection and occlusion are difficult to represent through a deterministic mapping, and therefore we adopt a probabilistic formulation for our embedding space. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 3D pose estimation models. We also show that by training a simple temporal embedding model, we achieve superior performance on pose sequence retrieval and largely reduce the embedding dimension from stacking frame-based embeddings for efficient large-scale retrieval. Furthermore, in order to enable our embeddings to work with partially visible input, we further investigate different keypoint occlusion augmentation strategies during training. We demonstrate that these occlusion augmentations significantly improve retrieval performance on partial 2D input poses. Results on action recognition and video alignment demonstrate that using our embeddings without any additional training achieves competitive performance relative to other models specifically trained for each task.	[Liu, Ting; Zhao, Long; Zhao, Jiaping; Yuan, Liangzhe; Wang, Yuxiao; Chen, Liang-Chieh; Schroff, Florian; Adam, Hartwig] Google Res, Los Angeles, CA 92091 USA; [Sun, Jennifer J.] CALTECH, Pasadena, CA 91125 USA; [Zhao, Long] Rutgers State Univ, Piscataway, NJ 08854 USA	Google Incorporated; California Institute of Technology; Rutgers State University New Brunswick	Liu, T (corresponding author), Google Res, Los Angeles, CA 92091 USA.	liuti@google.com; jjsun@caltech.edu; lz311@cs.rutgers.edu; jiapingz@google.com; lzyuan@google.com; yuxiaow@google.com; lcchen@google.com; fschroff@google.com; hadam@google.com		Zhao, Long/0000-0001-8921-8564; Sun, Jennifer/0000-0002-0906-6589	NSERC [PGSD3-532647-2019]; Caltech	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Caltech	We would like to thank Debidatta Dwibedi, Kree Cole-McLaughlin and Andrew Gallagher from Google Research, Xiao Zhang from University of Chicago, and Yisong Yue from Caltech for the helpful discussions. We appreciate the support of Pietro Perona and the Computational Vision Lab at Caltech for making this collaboration possible. The author Jennifer J. Sun is supported by NSERC (funding number PGSD3-532647-2019) and Caltech.	Abadi M, 2015, P 12 USENIX S OPERAT; Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Bojchevski Aleksandar, 2018, INT C LEARN REPR; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Cao CQ, 2018, IEEE T CYBERNETICS, V48, P1095, DOI 10.1109/TCYB.2017.2756840; Chen CH, 2019, PROC CVPR IEEE, P5707, DOI 10.1109/CVPR.2019.00586; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Chen Ting, 2020, ICML, P1597; Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742; Cheng Y, 2020, AAAI CONF ARTIF INTE, V34, P10631; Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081; Du WB, 2017, IEEE I CONF COMP VIS, P3745, DOI 10.1109/ICCV.2017.402; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Dwibedi D, 2019, PROC CVPR IEEE, P1801, DOI 10.1109/CVPR.2019.00190; Garcia-Salguero M, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19224943; Gu RS, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P163, DOI 10.1109/MIPR.2019.00036; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; He Kaiming, 2017, CVPR; He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208; Hermans Alexander, 2017, ARXIV170307737; Ho CH, 2019, PROC CVPR IEEE, P12369, DOI 10.1109/CVPR.2019.01266; Hu WZ, 2010, PROC CVPR IEEE, P2273, DOI 10.1109/CVPR.2010.5539910; Huang C., 2016, ADV NEURAL INFORM PR, P1262; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Iqbal U, 2017, IEEE INT CONF AUTOMA, P438, DOI 10.1109/FG.2017.61; Iscen A, 2018, PROC CVPR IEEE, P7642, DOI 10.1109/CVPR.2018.00797; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Jammalamadaka N., 2012, ACM MM; Ji XF, 2008, LECT NOTES ARTIF INT, V5177, P741; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117; LeCun Y, 2004, PROC CVPR IEEE, P97; Li JN, 2018, ADV NEUR IN, V31; Li SC, 2020, PROC CVPR IEEE, P6172, DOI 10.1109/CVPR42600.2020.00621; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127; Luvizon DC, 2021, IEEE T PATTERN ANAL, V43, P2752, DOI 10.1109/TPAMI.2020.2976014; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32; Mori Greg, 2015, ARXIV150700302; Nie BX, 2015, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2015.7298734; Oh S.J., 2019, ICLR; Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17; Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395; Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794; Qiu HB, 2019, IEEE I CONF COMP VIS, P4341, DOI 10.1109/ICCV.2019.00444; Rao C, 2001, PROC CVPR IEEE, P316; Rayat I. H. M., 2018, ECCV; Rhodin H, 2018, LECT NOTES COMPUT SC, V11214, P765, DOI 10.1007/978-3-030-01249-6_46; Rhodin H, 2018, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2018.00880; Ronchi M. R., 2016, ICDM; Sarandi I., 2018, ARXIV180904987V3; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sermanet P, 2018, IEEE INT CONF ROBOT, P1134; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Sun J. J., 2020, ECCV; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425; Tome D, 2018, INT CONF 3D VISION, P474, DOI 10.1109/3DV.2018.00061; van den Oord Aaron, 2018, ARXIV180703748; Vilnis Luke, 2015, INT C LEARN REPR; von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37; Wang JJ, 2014, IEEE CONF COMPU INTE; Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Zeng A., 2020, ECCV; Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280; Zhang YP, 2012, IEEE VTS VEH TECHNOL; Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354; Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51	81	0	0	3	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					111	135		10.1007/s11263-021-01529-w	http://dx.doi.org/10.1007/s11263-021-01529-w		NOV 2021	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC		Green Submitted, Green Accepted			2022-12-18	WOS:000719170500001
J	Yu, LT; Li, ZB; Xu, M; Gao, YS; Luo, JB; Zhang, J				Yu, Litao; Li, Zhibin; Xu, Min; Gao, Yongsheng; Luo, Jiebo; Zhang, Jian			Distribution-Aware Margin Calibration for Semantic Segmentation in Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic segmentation; Margin calibration; IoU		The Jaccard index, also known as Intersection-over-Union (IoU), is one of the most critical evaluation metrics in image semantic segmentation. However, direct optimization of IoU score is very difficult because the learning objective is neither differentiable nor decomposable. Although some algorithms have been proposed to optimize its surrogates, there is no guarantee provided for the generalization ability. In this paper, we propose a margin calibration method, which can be directly used as a learning objective, for an improved generalization of IoU over the data-distribution, underpinned by a rigid lower bound. This scheme theoretically ensures a better segmentation performance in terms of IoU score. We evaluated the effectiveness of the proposed margin calibration method on seven image datasets, showing substantial improvements in IoU score over other learning objectives using deep segmentation models.	[Yu, Litao; Xu, Min; Zhang, Jian] Univ Technol Sydney, Sydney, NSW, Australia; [Li, Zhibin] CSIRO, Brisbane, Qld, Australia; [Gao, Yongsheng] Griffith Univ, Brisbane, Qld, Australia; [Luo, Jiebo] Rochester Univ, Rochester, NY USA	University of Technology Sydney; Commonwealth Scientific & Industrial Research Organisation (CSIRO); Griffith University; University of Rochester	Zhang, J (corresponding author), Univ Technol Sydney, Sydney, NSW, Australia.	Litao.Yu@uts.edu.au; Zhibin.Li@csiro.au; Min.Xu@uts.edu.au; yongsheng.gao@griffith.edu.au; jluo@cs.rochester.edu; Jian.Zhang@uts.edu.au		Luo, Jiebo/0000-0002-4516-9729; Zhang, Jian/0000-0002-7240-3541	Multimedia Data Analytics Lab of Global Big Data Technologies Centre (GBDTC), University of Technology Sydney (UTS); UTS; CSIRO's Machine Learning and Artificial Intelligence Future Science Platform	Multimedia Data Analytics Lab of Global Big Data Technologies Centre (GBDTC), University of Technology Sydney (UTS); UTS; CSIRO's Machine Learning and Artificial Intelligence Future Science Platform	This research work is supported by Multimedia Data Analytics Lab of Global Big Data Technologies Centre (GBDTC), University of Technology Sydney (UTS). The experimental environment is provided by UTS Tech Lab. Zhibin Li acknowledges the support from UTS to conduct this research, and the continued support from CSIRO's Machine Learning and Artificial Intelligence Future Science Platform.	Abraham N, 2019, I S BIOMED IMAGING, P683, DOI 10.1109/ISBI.2019.8759329; Ahmed F, 2015, IEEE I CONF COMP VIS, P1850, DOI 10.1109/ICCV.2015.215; Allan, 2017, ROBOTIC INSTRUMENT S; Ayed, 2019, INT C MED IM DEEP LE, P285; Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464; Blaschko MB, 2008, LECT NOTES COMPUT SC, V5302, P2, DOI 10.1007/978-3-540-88682-2_2; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Cadena C, 2014, IEEE INT CONF ROBOT, P2639, DOI 10.1109/ICRA.2014.6907237; Cao KD, 2019, ADV NEUR IN, V32; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254; Eelbode T, 2020, IEEE T MED IMAGING, V39, P3679, DOI 10.1109/TMI.2020.3002417; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Karimi D, 2020, IEEE T MED IMAGING, V39, P499, DOI 10.1109/TMI.2019.2930068; Ke TW, 2018, LECT NOTES COMPUT SC, V11205, P605, DOI 10.1007/978-3-030-01246-5_36; Khan S, 2019, PROC CVPR IEEE, P103, DOI 10.1109/CVPR.2019.00019; Li XH, 2018, PR MACH LEARN RES, V80; Li Y., 2002, PROC ICML, V2, P379; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Loshchilov Ilya, 2019, INT C LEARN REPR; Ma J, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102035; Mohri M., 2018, FDN MACHINE LEARNING; Nagendar G., 2018, BRIT MACH VIS C NEWC, P278; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Neyshabur Behnam, 2018, INT C LEARN REPR; Nowozin S, 2014, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2014.77; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Rahman Md Atiqur, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P234, DOI 10.1007/978-3-319-50835-1_22; Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salehi SSM, 2017, LECT NOTES COMPUT SC, V10541, P379, DOI 10.1007/978-3-319-67389-9_44; Schmidt-Thieme L, 2019, LEARNING SURROGATE L; Shen D., 2020, NIPS, V33, P1; Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Sungha Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9370, DOI 10.1109/CVPR42600.2020.00939; Wang GT, 2020, IEEE T MED IMAGING, V39, P2653, DOI 10.1109/TMI.2020.3000314; Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265; Wang L, 2020, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR42600.2020.00383; Xiao JX, 2009, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2009.5459249; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077; Yu F., 2016, P ICLR 2016; Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271; Zhang R, 2019, ADV NEUR IN, V32; Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0	59	0	0	4	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					95	110		10.1007/s11263-021-01533-0	http://dx.doi.org/10.1007/s11263-021-01533-0		NOV 2021	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC		Green Submitted			2022-12-18	WOS:000716278700001
J	Chen, M; Zhang, C; Ikeuchi, K				Chen, Mei; Zhang, Cha; Ikeuchi, Katsushi			Editorial for Special Issue on Computer Vision in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material												Mei.Chen@microsoft.com							0	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					1	2		10.1007/s11263-021-01538-9	http://dx.doi.org/10.1007/s11263-021-01538-9		NOV 2021	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC		Bronze			2022-12-18	WOS:000714988400001
J	Chen, ZR; Huang, Y; Yu, HY; Wang, L				Chen, Zerui; Huang, Yan; Yu, Hongyuan; Wang, Liang			Learning a Robust Part-Aware Monocular 3D Human Pose Estimator via Neural Architecture Search	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Monocular 3D human pose estimation; Heterogeneous human body parts; Neural architecture search		Even though most existing monocular 3D human pose estimation methods achieve very competitive performance, they are limited in estimating heterogeneous human body parts with the same decoder architecture. In this work, we present an approach to build a part-aware 3D human pose estimator to better deal with these heterogeneous human body parts. Our proposed method consists of two learning stages: (1) searching suitable decoder architectures for specific parts and (2) training the part-aware 3D human pose estimator built with these optimized neural architectures. Consequently, our searched model is very efficient and compact and can automatically select a suitable decoder architecture to estimate each human body part. In comparison with previous state-of-the-art models built with ResNet-50 network, our method can achieve better performance and reduce 64.4% parameters and 8.5% FLOPs (multiply-adds). We validate the robustness and stability of our searched models by conducting extensive and rigorous ablation experiments. Our method can advance state-of-the-art accuracy on both the single-person and multi-person 3D human pose estimation benchmarks with affordable computational cost.	[Chen, Zerui; Huang, Yan; Yu, Hongyuan; Wang, Liang] CASIA, Ctr Res Intelligent Percept & Comp, NLPR, Beijing, Peoples R China; [Chen, Zerui; Yu, Hongyuan; Wang, Liang] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China; [Wang, Liang] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China; [Wang, Liang] Chinese Acad Sci, Artificial Intelligence Res, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Chinese Academy of Sciences	Wang, L (corresponding author), CASIA, Ctr Res Intelligent Percept & Comp, NLPR, Beijing, Peoples R China.; Wang, L (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; Wang, L (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.; Wang, L (corresponding author), Chinese Acad Sci, Artificial Intelligence Res, Beijing, Peoples R China.	zerui.chen@cripac.ia.ac.cn; yhuang@nlpr.ia.ac.cn; hongyuan.yu@cripac.ia.ac.cn; wangliang@nlpr.ia.ac.cn	Huang, Yan/HCH-6526-2022		National Key Research and Development Program of China [2018AAA0100400]; National Natural Science Foundation of China [61633021, 61721004, 61806194, U1803261, 61976132]; Beijing Nova Program [Z201100006820079]; Shandong Provincial Key Research andDevelopment Program [2019JZZY010119]; Key Research Program of Frontier Sciences CAS [ZDBS-LY-JSC032]; CAS-AIR	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission); Shandong Provincial Key Research andDevelopment Program; Key Research Program of Frontier Sciences CAS; CAS-AIR	This work was jointly supported by National Key Research and Development Program of China Grant No. 2018AAA0100400, National Natural Science Foundation of China (61633021, 61721004, 61806194, U1803261, and 61976132), Beijing Nova Program (Z201100006820079), Shandong Provincial Key Research andDevelopment Program (2019JZZY010119), Key Research Program of Frontier Sciences CAS Grant No.ZDBS-LY-JSC032, and CAS-AIR.	Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Baek S, 2018, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR.2018.00869; Baker Bowen, 2017, ICLR; Belagiannis V, 2014, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR.2014.216; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Burenius M, 2013, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2013.464; Cai Han, 2019, INT C LEARN REPR; Cai Y, 2019, IEEE ICC; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Chen Y, 2019, 25TH AMERICAS CONFERENCE ON INFORMATION SYSTEMS (AMCIS 2019); Chen Z., 2019, BRIT MACH VIS C BMVC; Chen Z., 2020, EUR C COMP VIS ECCV; Ci H, 2019, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2019.00235; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Divvala SK, 2012, LECT NOTES COMPUT SC, V7585, P31, DOI 10.1007/978-3-642-33885-4_4; Fabbri M, 2020, PROC CVPR IEEE, P7202, DOI 10.1109/CVPR42600.2020.00723; Fang H.S., 2018, AAAI C ART INT AAAI; GANAPATHI V, 2010, PROC CVPR IEEE, P755, DOI DOI 10.1109/CVPR.2010.5540141; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Gupta A, 2014, PROC CVPR IEEE, P2601, DOI 10.1109/CVPR.2014.333; Hatcliff J, 2019, IEEE SYMP PROD COMPL; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Jiang H., 2010, INT C PATTERN RECOGN, P1674, DOI DOI 10.1109/ICPR.2010.414; Jiang W, 2020, CONSUM COMM NETWORK; Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381; Kingma D.P, P 3 INT C LEARNING R; Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117; Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463; LEE HJ, 1985, COMPUT VISION GRAPH, V30, P148, DOI 10.1016/0734-189X(85)90094-5; Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2019, PROC CVPR IEEE, P10978, DOI 10.1109/CVPR.2019.01124; Liu H, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM - EDUCATIONAL RESEARCH AND EDUCATIONAL TECHNOLOGY, 2019, P3; Loshkarev IY, 2019, J PHYS CONF SER, V1333, DOI 10.1088/1742-6596/1333/4/042019; MacKay DJ, 2003, INFORM THEORY INFERE; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Moon G, 2019, IEEE I CONF COMP VIS, P10132, DOI 10.1109/ICCV.2019.01023; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Mueller F, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322958; Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Nibali A, 2019, IEEE WINT CONF APPL, P1477, DOI 10.1109/WACV.2019.00162; Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062; Park S, 2016, LECT NOTES COMPUT SC, V9915, P156, DOI 10.1007/978-3-319-49409-8_15; Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Qiu HB, 2019, IEEE I CONF COMP VIS, P4341, DOI 10.1109/ICCV.2019.00444; Rhodin H, 2018, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2018.00880; Rogez G, 2016, ADV NEUR IN, V29; Rogez G, 2017, PROC CVPR IEEE, P1216, DOI 10.1109/CVPR.2017.134; Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883; Sarandi I., 2018, ARXIV180904987V3; SARANDI I, 2020, IEEE INT C AUTOMATIC; SHOTTON J, 2011, PROC CVPR IEEE, P1297, DOI DOI 10.1109/CVPR.2011.5995316; Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12; Tang W, 2019, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2019.00120; Taylor CJ, 2000, PROC CVPR IEEE, P677, DOI 10.1109/CVPR.2000.855885; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Tu H., 2020, EUR C COMP VIS ECCV; Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Wang J, 2019, IEEE I CONF COMP VIS, P8200, DOI 10.1109/ICCV.2019.00829; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wu XK, 2018, LECT NOTES COMPUT SC, V11220, P246, DOI 10.1007/978-3-030-01270-0_15; Xiong F, 2019, IEEE I CONF COMP VIS, P793, DOI 10.1109/ICCV.2019.00088; Xu Y, 2020, PLANT SOIL, V449, P133, DOI 10.1007/s11104-020-04435-1; Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551; Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535; Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783; Zhou K, 2019, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2019.00243; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou Xiaowei, 2018, IEEE T PATTERN ANAL; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51; Zoph B., 2017, P1	92	0	0	1	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					56	75		10.1007/s11263-021-01525-0	http://dx.doi.org/10.1007/s11263-021-01525-0		OCT 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC					2022-12-18	WOS:000711300700001
J	Kandukuri, RK; Achterhold, J; Moeller, M; Stueckler, J				Kandukuri, Rama Krishna; Achterhold, Jan; Moeller, Michael; Stueckler, Joerg			Physical Representation Learning and Parameter Identification from Video Using Differentiable Physics	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Physical scene understanding; Video representation learning; Differentiable physics	FRICTION	Representation learning for video is increasingly gaining attention in the field of computer vision. For instance, video prediction models enable activity and scene forecasting or vision-based planning and control. In this article, we investigate the combination of differentiable physics and spatial transformers in a deep action conditional video representation network. By this combination our model learns a physically interpretable latent representation and can identify physical parameters. We propose supervised and self-supervised learning methods for our architecture. In experiments, we consider simulated scenarios with pushing, sliding and colliding objects, for which we also analyze the observability of the physical properties. We demonstrate that our network can learn to encode images and identify physical properties like mass and friction from videos and action sequences. We evaluate the accuracy of our training methods, and demonstrate the ability of our method to predict future video frames from input images and actions.	[Kandukuri, Rama Krishna; Achterhold, Jan; Stueckler, Joerg] Max Planck Inst Intelligent Syst, Tubingen, Germany; [Moeller, Michael] Univ Siegen, Siegen, Germany	Max Planck Society; Universitat Siegen	Kandukuri, RK (corresponding author), Max Planck Inst Intelligent Syst, Tubingen, Germany.	rama.kandukuri@tue.mpg.de; jan.achterhold@tue.mpg.de; michael.moeller@uni-siegen.de; joerg.stueckler@tue.mpg.de			Cyber Valley; Max Planck Society; German Federal Ministry of Education and Research (BMBF) through the Tuebingen AI Center [FKZ: 01IS18039B]; InternationalMax Planck Research School for Intelligent Systems (IMPRS-IS)	Cyber Valley; Max Planck Society(Max Planck SocietyFoundation CELLEX); German Federal Ministry of Education and Research (BMBF) through the Tuebingen AI Center(Federal Ministry of Education & Research (BMBF)); InternationalMax Planck Research School for Intelligent Systems (IMPRS-IS)	We acknowledge support from Cyber Valley, the Max Planck Society, and the German Federal Ministry of Education and Research (BMBF) through the Tuebingen AI Center (FKZ: 01IS18039B). The authors thank the InternationalMax Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Jan Achterhold.	Amos B, 2017, PR MACH LEARN RES, V70; Anitescu M, 1997, NONLINEAR DYNAM, V14, P231, DOI 10.1023/A:1008292328909; Babaeizadeh Mohammad, 2018, ICLR; Belbute-Peres FD, 2018, ADV NEUR IN, V31; Chen T.Q., 2018, NEURIPS, P2610; Cline Michael Bradley, 2002, THESIS; Degrave J, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00006; Djork-Arn, ICLR 2016; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Greydanus S, 2019, ADV NEUR IN, V32; Hafner D, 2019, PR MACH LEARN RES, V97; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaderberg M, 2015, ADV NEUR IN, V28; Jaques M., 2020, P INT C LEARN REPR; Kandukuri R., 2020, P 42 GERM C PATT REC; Kloss A., 2017, ARXIV171004102; Mottaghi R, 2016, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2016.383; Mottaghi R, 2016, LECT NOTES COMPUT SC, V9908, P269, DOI 10.1007/978-3-319-46493-0_17; Runia T.F.H., 2020, P IEEE C COMP VIS PA; Shi XJ, 2015, ADV NEUR IN, V28; Smith R., 2008, OPEN DYNAMICS ENGINE; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Stewart DE, 2000, SIAM REV, V42, P3, DOI 10.1137/S0036144599360110; Watters N, 2017, ADV NEUR IN, V30; Ye T, 2018, LECT NOTES COMPUT SC, V11216, P89, DOI 10.1007/978-3-030-01258-8_6; Zhu DY, 2019, LECT NOTES COMPUT SC, V11824, P595, DOI 10.1007/978-3-030-33676-9_42	28	0	0	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					3	16		10.1007/s11263-021-01493-5	http://dx.doi.org/10.1007/s11263-021-01493-5		OCT 2021	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC		hybrid			2022-12-18	WOS:000707947400001
J	Akata, Z; Geiger, A; Sattler, T				Akata, Zeynep; Geiger, Andreas; Sattler, Torsten			Computer Vision and Pattern Recognition 2020	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Akata, Zeynep; Geiger, Andreas] Univ Tubingen, D-72076 Tubingen, Germany; [Sattler, Torsten] Czech Tech Univ, Prague 16000, Czech Republic	Eberhard Karls University of Tubingen; Czech Technical University Prague	Akata, Z (corresponding author), Univ Tubingen, D-72076 Tubingen, Germany.	zeynep.akata@uni-tuebingen.de; a.geiger@uni-tuebingen.de; torsten.sattler@cvut.cz	Sattler, Torsten/AAM-3155-2021		Projekt DEAL	Projekt DEAL	Open Access funding enabled and organized by Projekt DEAL.		0	0	0	2	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3169	3170		10.1007/s11263-021-01522-3	http://dx.doi.org/10.1007/s11263-021-01522-3		OCT 2021	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN		hybrid			2022-12-18	WOS:000707277700001
J	Xu, D; Chellappa, R; Van Gool, L; Lu, G				Xu, Dong; Chellappa, Rama; Van Gool, Luc; Lu, Guo			Guest Editorial: Special Issue on Deep Learning for Video Analysis and Compression	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Xu, Dong] Univ Sydney, Sch Elect & Informat Engn, Bldg J03, Sydney, NSW 2006, Australia; [Chellappa, Rama] Johns Hopkins Univ JHU, Dept Elect & Comp Engn, 3400 North Charles St, Baltimore, MD 21218 USA; [Chellappa, Rama] Johns Hopkins Univ JHU, Dept Biomed Engn, 3400 North Charles St, Baltimore, MD 21218 USA; [Van Gool, Luc] ETF C 117, Dept Informat Technol & Elect Engn, Sternwartstr 7, CH-8092 Zurich, Switzerland; [Van Gool, Luc] Katholieke Univ Leuven, ESAT Dept, Kardinaal Mercierlaan 1, B-3001 Leuven, Belgium; [Lu, Guo] Beijing Inst Technol, Sch Compute Sci & Technol, Room 305,Software Bldg, Beijing 100081, Peoples R China	University of Sydney; Johns Hopkins University; Johns Hopkins University; KU Leuven; Beijing Institute of Technology	Xu, D (corresponding author), Univ Sydney, Sch Elect & Informat Engn, Bldg J03, Sydney, NSW 2006, Australia.	dong.xu@sydney.edu.au							0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3171	3173		10.1007/s11263-021-01530-3	http://dx.doi.org/10.1007/s11263-021-01530-3		OCT 2021	3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN		Bronze			2022-12-18	WOS:000707538700006
J	Losch, M; Fritz, M; Schiele, B				Losch, Max; Fritz, Mario; Schiele, Bernt			Semantic Bottlenecks: Quantifying and Improving Inspectability of Deep Representations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Explainable AI; Inspectability; Interpretability; Semantic segmentation; Representation learning		Today's deep learning systems deliver high performance based on end-to-end training but are notoriously hard to inspect. We argue that there are at least two reasons making inspectability challenging: (i) representations are distributed across hundreds of channels and (ii) a unifying metric quantifying inspectability is lacking. In this paper, we address both issues by proposing Semantic Bottlenecks (SB), which can be integrated into pretrained networks, to align channel outputs with individual visual concepts and introduce the model agnostic Area Under inspectability Curve (AUiC) metric to measure the alignment. We present a case study on semantic segmentation to demonstrate that SBs improve the AUiC up to six-fold over regular network outputs. We explore two types of SB-layers in this work. First, concept-supervised SB-layers (SSB), which offer inspectability w.r.t. predefined concepts that the model is demanded to rely on. And second, unsupervised SBs (USB), which offer equally strong AUiC improvements by restricting distributedness of representations across channels. Importantly, for both SB types, we can recover state of the art segmentation performance across two different models despite a drastic dimensionality reduction from 1000s of non aligned channels to 10s of semantics-aligned channels that all downstream results are based on.	[Losch, Max; Schiele, Bernt] Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany; [Fritz, Mario] CISPA Helmholtz Ctr Informat Secur, Saarbrucken, Germany	Max Planck Society	Losch, M (corresponding author), Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany.	mlosch@mpi-inf.mpg.de			Projekt DEAL	Projekt DEAL	Open Access funding enabled and organized by Projekt DEAL.	Adebayo J, 2018, ADV NEUR IN, V31; Al-Shedivat M, 2020, J MACH LEARN RES, V21; Alvarez-Melis D, 2018, ADV NEUR IN, V31; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Bau David, 2019, INT C LEARN REPR ICL; Bell S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462002; Burgess Christopher P, 2019, ARXIV190111390; Chen CF, 2019, ADV NEUR IN, V32; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen L, 2014, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2014.135; Chen RJ, 2019, IEEE I CONF COMP VIS, P9186, DOI 10.1109/ICCV.2019.00928; Chu, 2020, ARXIV PREPRINT ARXIV; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Esser Patrick, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9220, DOI 10.1109/CVPR42600.2020.00924; Fong R, 2019, IEEE I CONF COMP VIS, P2950, DOI 10.1109/ICCV.2019.00304; Fong R, 2018, PROC CVPR IEEE, P8730, DOI 10.1109/CVPR.2018.00910; Greff K, 2019, PR MACH LEARN RES, V97; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Higgins I, 2016, BETA VAE LEARNING BA; Hooker S, 2019, ADV NEUR IN, V32; Jacobsen J orn-Henrik, 2018, P ICLR; jia Li L., 2010, NIPS, DOI [10.1184/R1/6475985.v1, DOI 10.1184/R1/6475985.V1]; Kingma D. P., 2013, AUTO ENCODING VARIAT; Li O, 2018, AAAI CONF ARTIF INTE, P3530; Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775; Lipton ZC, 2018, COMMUN ACM, V61, P36, DOI 10.1145/3233231; Liu H, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM - EDUCATIONAL RESEARCH AND EDUCATIONAL TECHNOLOGY, 2019, P3; Lundberg SM, 2017, ADV NEUR IN, V30; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Marcos D, 2019, IEEE INT CONF COMP V, P4207, DOI 10.1109/ICCVW.2019.00518; Mu Jesse, 2020, P C ADV NEUR INF PRO; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Petsiuk V., 2018, P BRIT MACH VIS C 20, P151; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Srinivas S, 2019, ADV NEUR IN, V32; Sundararajan M, 2017, PR MACH LEARN RES, V70; Tao Andrew, 2020, ARXIV200510821; Wada K., 2016, LABELME IMAGE POLYGO, DOI 10.5281/zenodo.5711226; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Xie Sirui, 2019, ICLR, V1, P13; Yeh, 2019, ARXIV PREPRINT ARXIV; Yosinski J., 2015, ICML DEEP LEARN WORK; Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zintgraf Luisa M., 2017, P ICLR	56	0	0	2	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					3136	3153		10.1007/s11263-021-01498-0	http://dx.doi.org/10.1007/s11263-021-01498-0		SEP 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH8ZM		hybrid			2022-12-18	WOS:000695432000003
J	Sitenko, D; Boll, B; Schnorr, C				Sitenko, Dmitrij; Boll, Bastian; Schnoerr, Christoph			Assignment Flow for Order-Constrained OCT Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Assignment flow; Assignment manifold; Optical coherence tomography; Information geometry; Covariance descriptor	COHERENCE TOMOGRAPHY IMAGES; RETINAL LAYER SEGMENTATION; AUTOMATIC SEGMENTATION; SURFACE; REGION	At the present time optical coherence tomography (OCT) is among the most commonly used non-invasive imaging methods for the acquisition of large volumetric scans of human retinal tissues and vasculature. The substantial increase of accessible highly resolved 3D samples at the optic nerve head and the macula is directly linked to medical advancements in early detection of eye diseases. To resolve decisive information from extracted OCT volumes and to make it applicable for further diagnostic analysis, the exact measurement of retinal layer thicknesses serves as an essential task be done for each patient separately. However, manual examination of OCT scans is a demanding and time consuming task, which is typically made difficult by the presence of tissue-dependent speckle noise. Therefore, the elaboration of automated segmentation models has become an important task in the field of medical image processing. We propose a novel, purely data driven geometric approach to order-constrained 3D OCT retinal cell layer segmentation which takes as input data in any metric space and can be implemented using only simple, highly parallelizable operations. As opposed to many established retinal layer segmentation methods, we use only locally extracted features as input and do not employ any global shape prior. The physiological order of retinal cell layers and membranes is achieved through the introduction of a smoothed energy term. This is combined with additional regularization of local smoothness to yield highly accurate 3D segmentations. The approach thereby systematically avoid bias pertaining to global shape and is hence suited for the detection of anatomical changes of retinal tissue structure. To demonstrate its robustness, we compare two different choices of features on a data set of manually annotated 3D OCT volumes of healthy human retina. The quality of computed segmentations is compared to the state of the art in automatic retinal layer segmention as well as to manually annotated ground truth data in terms of mean absolute error and Dice similarity coefficient. Visualizations of segmented volumes are also provided.	[Sitenko, Dmitrij; Boll, Bastian] Heidelberg Univ, Image & Pattern Anal Grp IPA, Heidelberg, Germany; [Sitenko, Dmitrij; Boll, Bastian] Heidelberg Univ, Heidelberg Collaboratory Image Proc HCI, Heidelberg, Germany; [Schnoerr, Christoph] Heidelberg Univ, Image & Pattern Anal Grp, Heidelberg, Germany	Ruprecht Karls University Heidelberg; Ruprecht Karls University Heidelberg; Ruprecht Karls University Heidelberg	Sitenko, D (corresponding author), Heidelberg Univ, Image & Pattern Anal Grp IPA, Heidelberg, Germany.; Sitenko, D (corresponding author), Heidelberg Univ, Heidelberg Collaboratory Image Proc HCI, Heidelberg, Germany.	dmitrij.sitenko@iwr.uni-heidelberg.de; bastian.boll@iwr.uni-heidelberg.de; schnoerr@math.uni-heidelberg.de		Sitenko, Dmitrij/0000-0002-0022-3891; Boll, Bastian/0000-0002-3490-3350	Deutsche Forschungsgemeinschaft (DFG) [EXC-2181/1 - 390900948]	Deutsche Forschungsgemeinschaft (DFG)(German Research Foundation (DFG))	We thank Dr. Stefan Schmidt and Julian Weichsel for sharing with us their expertise on OCT sensors, data acquisition and processing. In addition, we thank Fred Hamprecht and Alberto Bailoni for their guidance in training deep networks for feature extraction from 3D data. This work is supported by Deutsche Forschungsgemeinschaft (DFG) under Germany's Excellence Strategy EXC-2181/1 - 390900948 (the Heidelberg STRUCTURES Excellence Cluster).	Abramoff Michael D, 2010, IEEE Rev Biomed Eng, V3, P169, DOI 10.1109/RBME.2010.2084567; Amari S., 2000, METHODS INFORM GEOME; Antony BJ, 2010, PROC SPIE, V7626, DOI 10.1117/12.843928; Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996; Astrom F, 2017, J MATH IMAGING VIS, V58, P211, DOI 10.1007/s10851-016-0702-4; Bauschke H. H., 1997, J CONVEX ANAL, V4, P27; Bhatia, 2013, RIEMANNIAN MEAN POSI, P35; Bhatia R, 2007, PRINC SER APPL MATH, P1; Bini DA, 2013, LINEAR ALGEBRA APPL, V438, P1700, DOI 10.1016/j.laa.2011.08.052; Bridson M., 1999, METRIC SPACES NONPOS, DOI [10.1007/978-3-662-12494-9, DOI 10.1007/978-3-662-12494-9]; Censor Y., 1997, PARALLEL OPTIMIZATIO; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Cherian A, 2016, ADV COMPUT VIS PATT, P93, DOI 10.1007/978-3-319-45026-1_4; Chiu SJ, 2015, BIOMED OPT EXPRESS, V6, P1172, DOI 10.1364/BOE.6.001172; Congedo M, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121423; Crum WR, 2006, IEEE T MED IMAGING, V25, P1451, DOI 10.1109/TMI.2006.880587; Depeursinge A, 2014, MED IMAGE ANAL, V18, P176, DOI 10.1016/j.media.2013.10.005; DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409; Duan JM, 2015, PHYS MED BIOL, V60, P8901, DOI 10.1088/0031-9155/60/22/8901; Dufour PA, 2013, IEEE T MED IMAGING, V32, P531, DOI 10.1109/TMI.2012.2225152; Fang LY, 2017, BIOMED OPT EXPRESS, V8, P2732, DOI 10.1364/BOE.8.002732; Garvin M.D., IOWA REFERENCE ALGOR; Garvin MK, 2009, IEEE T MED IMAGING, V28, P1436, DOI 10.1109/TMI.2009.2016958; Haeker M, 2007, LECT NOTES COMPUT SC, V4791, P244; HASHIMOTO M, 1987, COMPUT VISION GRAPH, V39, P28, DOI 10.1016/S0734-189X(87)80201-3; He YF, 2019, BIOMED OPT EXPRESS, V10, P5042, DOI 10.1364/BOE.10.005042; Higham NJ, 2008, OTHER TITL APPL MATH, V104, P1; HUANG D, 1991, SCIENCE, V254, P1178, DOI 10.1126/science.1957169; Huhnerbein R, 2021, J MATH IMAGING VIS, V63, P186, DOI 10.1007/s10851-020-00977-2; Jaccard P., 1908, B SOC SCI NAT, V44, P223, DOI [DOI 10.5169/SEALS-268384, 10.5169/ seals-268384]; Jost J., 2017, RIEMANNIAN GEOMETRY, DOI 10.1007/978-3-319-61860-9; Kafieh R, 2013, MED IMAGE ANAL, V17, P907, DOI 10.1016/j.media.2013.05.006; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Kjpargeter, MUSCLES HEAD; Lee J.M., 2013, INTRO SMOOTH MANIFOL, V2nd, DOI 10.1007/978-1-4419-9982-5; Li K, 2006, IEEE T PATTERN ANAL, V28, P119, DOI 10.1109/TPAMI.2006.19; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Liu XM, 2019, IEEE ACCESS, V7, P3046, DOI 10.1109/ACCESS.2018.2889321; Moakher M, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P285, DOI 10.1007/3-540-31272-2_17; Novosel J, 2017, IEEE T MED IMAGING, V36, P1276, DOI 10.1109/TMI.2017.2666045; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Quellec G, 2010, IEEE T MED IMAGING, V29, P1321, DOI 10.1109/TMI.2010.2047023; Rathke F., 2017, INT C MED IM COMP CO, P177, DOI [DOI 10.1007/978-3-319-66182-7_21, 10.1007/978-3-319-66182-7_21]; Rathke F, 2014, MED IMAGE ANAL, V18, P781, DOI 10.1016/j.media.2014.03.004; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roy AG, 2017, BIOMED OPT EXPRESS, V8, P3627, DOI 10.1364/BOE.8.003627; Schnorr C., 2020, VARIATIONAL METHODS, P235; Sirinukunwattana K, 2015, PROC SPIE, V9420, DOI 10.1117/12.2082010; Sitenko D., 2020, GCPR, P58; Song Q, 2013, IEEE T MED IMAGING, V32, P376, DOI 10.1109/TMI.2012.2227120; Sra S, 2016, P AM MATH SOC, V144, P2787, DOI 10.1090/proc/12953; Turaga P. K., 2016, RIEMANNIAN COMPUTING, P145; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Yazdanpanah A, 2011, IEEE T MED IMAGING, V30, P484, DOI 10.1109/TMI.2010.2087390; Zeilmann A, 2020, INVERSE PROBL, V36, DOI 10.1088/1361-6420/ab2772; Zern A., ARXIVABS200211571; Zern A, 2020, J MATH IMAGING VIS, V62, P982, DOI 10.1007/s10851-019-00935-7; Zisler M, 2020, SIAM J IMAGING SCI, V13, P1113, DOI 10.1137/19M1298639	58	0	0	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					3088	3118		10.1007/s11263-021-01520-5	http://dx.doi.org/10.1007/s11263-021-01520-5		SEP 2021	31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH8ZM		Green Submitted, hybrid			2022-12-18	WOS:000692902100003
J	Frintrop, S; Fink, GA; Jiang, XY				Frintrop, Simone; Fink, Gernot A.; Jiang, Xiaoyi			Guest Editorial: Special Issue: Computer Vision and Pattern Recognition (DAGM GCPR 2019)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Frintrop, Simone] Univ Hamburg, Hamburg, Germany; [Fink, Gernot A.] TU Dortmund, Dortmund, Germany; [Jiang, Xiaoyi] Univ Munster, Munster, Germany	University of Hamburg; Dortmund University of Technology; University of Munster	Frintrop, S (corresponding author), Univ Hamburg, Hamburg, Germany.	simone.frintrop@uni-hamburg.de	Jiang, Xiaoyi/AAA-3532-2022	Jiang, Xiaoyi/0000-0001-7678-9528	Projekt DEAL	Projekt DEAL	Open Access funding enabled and organized by Projekt DEAL.		0	0	0	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					3004	3005		10.1007/s11263-021-01509-0	http://dx.doi.org/10.1007/s11263-021-01509-0		AUG 2021	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH8ZM		hybrid			2022-12-18	WOS:000690351900003
J	Huang, D; Zhang, R; Zhang, XS; Wu, F; Wang, XZ; Jin, PW; Liu, SL; Li, L; Chen, YJ				Huang, Di; Zhang, Rui; Zhang, Xishan; Wu, Fan; Wang, Xianzhuo; Jin, Pengwei; Liu, Shaoli; Li, Ling; Chen, Yunji			A Decomposable Winograd Method for N-D Convolution Acceleration in Video Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Convolution neural networks; Model acceleration; Winograd algorithm; Video analysis		Winograd's minimal filtering algorithm has been widely used in 2-D Convolutional Neural Networks (CNNs) to reduce the number of multiplications for faster processing. However, it is only effective on convolutions with kernel size as 3 and stride as 1, because it suffers from significantly increased FLOPs and numerical accuracy problems for kernel size larger than 3 and fails on convolution with stride larger than 1. Worse, the extension to N-D convolution will intensify the numerical accuracy problem. These problems severely obstruct Winograd's minimal filtering algorithm's application to video analysis. In this paper, we propose a novel Decomposable Winograd Method (DWM) for the N-D convolution acceleration, which breaks through the limitation of original Winograd's minimal filtering algorithm to more general convolutions. DWM decomposes kernels with large size or stride>1 to several small kernels with stride as 1 for further applying Winograd algorithm, so that DWMcan reduce the number of multiplications while keeping the numerical accuracy. It enables the fast exploration of larger kernel size, larger stride value, and higher dimensions in CNNs for high performance and accuracy and even the potential for new CNNs. Comparing against the original Winograd algorithm, the proposed DWM is able to support all kinds of N-D convolutions with a speedup of 1.44x-3.38x, without affecting the numerical accuracy.	[Huang, Di; Zhang, Rui; Zhang, Xishan; Wang, Xianzhuo; Jin, Pengwei; Chen, Yunji] Chinese Acad Sci, Inst Comp Technol, SKL Comp Architecture, Beijing, Peoples R China; [Wu, Fan; Li, Ling] Chinese Acad Sci, Inst Software, Beijing, Peoples R China; [Huang, Di; Wu, Fan; Wang, Xianzhuo; Jin, Pengwei; Chen, Yunji] Univ Chinese Acad Sci, Beijing, Peoples R China; [Huang, Di; Zhang, Rui; Zhang, Xishan; Wu, Fan; Wang, Xianzhuo; Jin, Pengwei; Liu, Shaoli] Cambricon Tech Ltd, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Zhang, R (corresponding author), Chinese Acad Sci, Inst Comp Technol, SKL Comp Architecture, Beijing, Peoples R China.; Zhang, R (corresponding author), Cambricon Tech Ltd, Beijing, Peoples R China.	huangdi20b@ict.ac.cn; zhangrui@ict.ac.cn; zhangxishan@ict.ac.cn; wufan2020@iscas.ac.cn; wangxianzhuo19g@ict.ac.cn; jinpengwei20z@ict.ac.cn; liushaoli@cambricon.com; liling@iscas.ac.cn; cyj@ict.ac.cn		Chen, Yunji/0000-0003-3925-5185	Beijing Natural Science Foundation [JQ18013]; NSF of China [61925208, 61906179, 62002338, 61732007, 61732002, U19B2019, U20A20227]; Strategic Priority Research Program of Chinese Academy of Science [XDB 32050200, XDC05010300]; Beijing Academy of Artificial Intelligence (BAAI); Beijing Nova Program of Science and Technology [Z191100001119093]; Youth Innovation Promotion Association CAS	Beijing Natural Science Foundation(Beijing Natural Science Foundation); NSF of China(National Natural Science Foundation of China (NSFC)); Strategic Priority Research Program of Chinese Academy of Science; Beijing Academy of Artificial Intelligence (BAAI); Beijing Nova Program of Science and Technology; Youth Innovation Promotion Association CAS	This work is partially supported by the Beijing Natural Science Foundation (JQ18013), the NSF of China (under Grants 61925208, 61906179, 62002338, 61732007, 61732002, U19B2019, U20A20227), Strategic Priority Research Program of Chinese Academy of Science (XDB 32050200, XDC05010300), Beijing Academy of Artificial Intelligence (BAAI) and Beijing Nova Program of Science and Technology (Z191100001119093), Youth Innovation Promotion Association CAS and Xplore Prize.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; [Anonymous], 2016, P ECCV; Barabasz B, 2019, ARXIV190505233; Budden D, 2017, PR MACH LEARN RES, V70; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Cai H., 2018, ARXIV PREPRINT ARXIV; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chang YL, 2019, IEEE I CONF COMP VIS, P9065, DOI 10.1109/ICCV.2019.00916; Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353; Chetlur S., 2014, ARXIV; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Courbariaux M., 2014, ARXIV PREPRINT ARXIV; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Guo JY, 2020, AAAI CONF ARTIF INTE, V34, P10885; Guo JY, 2020, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR42600.2020.00158; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Han S., 2016, P 4 INT C LEARN REPR, P1; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2; Huang D, 2020, AAAI CONF ARTIF INTE, V34, P4174; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jia LC, 2020, IEEE T COMPUT, V69, P986, DOI 10.1109/TC.2020.2973144; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Lan Q, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/8348671; Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu JH, 2021, IEEE T IMAGE PROCESS, V30, P15, DOI 10.1109/TIP.2020.3028288; Liu Xingyu, 2018, INT C LEARN REPR; Lu LQ, 2017, ANN IEEE SYM FIELD P, P101, DOI 10.1109/FCCM.2017.64; Maji P, 2019, 2019 2ND WORKSHOP ON ENERGY EFFICIENT MACHINE LEARNING AND COGNITIVE COMPUTING FOR EMBEDDED APPLICATIONS (EMC2 2019), P1, DOI 10.1109/EMC249363.2019.00008; Meng Lingchuan, 2019, ARXIV190101965; Paszke A, 2017, NEURAL INFORM PROCES; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Shi L., 2020, ARXIV200403259; Simonyan K, 2014, ADV NEUR IN, V27; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Tran D, 2016, IEEE COMPUT SOC CONF, P402, DOI 10.1109/CVPRW.2016.57; Tran Du, 2017, ARXIV170805038; Vincent K., 2017, ICLR; Wang C, 2019, AAAI CONF ARTIF INTE, P5232; Wang K, 2019, PROC CVPR IEEE, P8604, DOI 10.1109/CVPR.2019.00881; Wang ZL, 2017, LECT NOTES COMPUT SC, V10614, P609, DOI 10.1007/978-3-319-68612-7_69; Wei H., 2020, CGI; Winograd Shmuel, 1980, ARITHMETIC COMPLEXIT, V33, P1; Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xue J, 2013, INTERSPEECH, P2364; Xue T., 2018, INT J COMPUT VISION, P1; Yan D, 2020, PROCEEDINGS OF THE 25TH ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING (PPOPP '20), P32, DOI 10.1145/3332466.3374520; Zhang Shiwen, 2020, ARXIV200207442; Zhang XS, 2020, PROC CVPR IEEE, P2327, DOI 10.1109/CVPR42600.2020.00240; Zhang YX, 2019, IEEE IMAGE PROC, P4270, DOI 10.1109/ICIP.2019.8803541; Zhou A, 2017, INCREMENTAL NETWORK; Zhou S., 2016, ARXIV160606160; Zhu F, 2020, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR42600.2020.00204; Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43; Zoph B., 2017, P1	62	0	0	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2806	2826		10.1007/s11263-021-01500-9	http://dx.doi.org/10.1007/s11263-021-01500-9		AUG 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO					2022-12-18	WOS:000681171700002
J	Li, S; Song, WF; Fang, Z; Shi, JY; Hao, AM; Zhao, QP; Qin, H				Li, Shuai; Song, Wenfeng; Fang, Zheng; Shi, Jiaying; Hao, Aimin; Zhao, Qinping; Qin, Hong			Long-Short Temporal-Spatial Clues Excited Network for Robust Person Re-identification (vol 128, pg 2936, 2020)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Li, Shuai; Song, Wenfeng; Fang, Zheng; Shi, Jiaying; Hao, Aimin; Zhao, Qinping] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China; [Li, Shuai; Hao, Aimin; Zhao, Qinping] Peng Cheng Lab, Shenzhen 518055, Peoples R China; [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA	Beihang University; Peng Cheng Laboratory; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Song, WF (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.; Qin, H (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.	lishuai@buaa.edu.cn; songwenfenga@163.com; qin@cs.stonybrook.edu						Li S, 2020, INT J COMPUT VISION, V128, P2936, DOI 10.1007/s11263-020-01349-4	1	0	0	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2730	2730		10.1007/s11263-021-01497-1	http://dx.doi.org/10.1007/s11263-021-01497-1		JUL 2021	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN		Bronze			2022-12-18	WOS:000670164300001
J	Rozumnyi, D; Kotera, J; Sroubek, F; Matas, J				Rozumnyi, Denys; Kotera, Jan; Sroubek, Filip; Matas, Jiri			Tracking by Deblatting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Fast moving objects; Visual object tracking; Deblatting; Deblurring; Trajectory estimation; Energy minimization		Objects moving at high speed along complex trajectories often appear in videos, especially videos of sports. Such objects travel a considerable distance during exposure time of a single frame, and therefore, their position in the frame is not well defined. They appear as semi-transparent streaks due to the motion blur and cannot be reliably tracked by general trackers. We propose a novel approach called Tracking by Deblatting based on the observation that motion blur is directly related to the intra-frame trajectory of an object. Blur is estimated by solving two intertwined inverse problems, blind deblurring and image matting, which we call deblatting. By postprocessing, non-causal Tracking by Deblatting estimates continuous, complete, and accurate object trajectories for the whole sequence. Tracked objects are precisely localized with higher temporal resolution than by conventional trackers. Energy minimization by dynamic programming is used to detect abrupt changes of motion, called bounces. High-order polynomials are then fitted to smooth trajectory segments between bounces. The output is a continuous trajectory function that assigns location for every real-valued time stamp from zero to the number of frames. The proposed algorithm was evaluated on a newly created dataset of videos from a high-speed camera using a novel Trajectory-IoU metric that generalizes the traditional Intersection over Union and measures the accuracy of the intra-frame trajectory. The proposed method outperforms the baselines both in recall and trajectory accuracy. Additionally, we show that from the trajectory function precise physical calculations are possible, such as radius, gravity, and sub-frame object velocity. Velocity estimation is compared to the high-speed camera measurements and radars. Results show high performance of the proposed method in terms of Trajectory-IoU, recall, and velocity estimation.	[Rozumnyi, Denys; Matas, Jiri] Czech Tech Univ, Fac Elect Engn, Dept Cybernet, Visual Recognit Grp, Prague, Czech Republic; [Rozumnyi, Denys] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Kotera, Jan; Sroubek, Filip] Czech Acad Sci, Inst Informat Theory & Automat, Prague, Czech Republic	Czech Technical University Prague; Swiss Federal Institutes of Technology Domain; ETH Zurich; Czech Academy of Sciences; Institute of Information Theory & Automation of the Czech Academy of Sciences	Rozumnyi, D (corresponding author), Czech Tech Univ, Fac Elect Engn, Dept Cybernet, Visual Recognit Grp, Prague, Czech Republic.; Rozumnyi, D (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	rozumden@cmp.felk.cvut.cz; kotera@utia.cas.cz; sroubekf@utia.cas.cz; matas@cmp.felk.cvut.cz	Rozumnyi, Denys/AAA-9091-2022	Rozumnyi, Denys/0000-0001-9874-1349	Czech Science Foundation [GA18-05360S]; Czech Technical University [SGS17/185/OHK3/3T/13]; Praemium Academiae - Czech Academy of Sciences; Google Focused Research Award	Czech Science Foundation(Grant Agency of the Czech Republic); Czech Technical University; Praemium Academiae - Czech Academy of Sciences; Google Focused Research Award(Google Incorporated)	This work was supported by the Czech Science Foundation grant GA18-05360S, the Czech Technical University student grant SGS17/185/OHK3/3T/13, and by the Praemium Academiae awarded by the Czech Academy of Sciences. D. Rozumnyi was also supported by Google Focused Research Award.	Boyle J. P., 1986, ADV ORDER RESTRICTED, P28, DOI DOI 10.1007/978-1-4613-9940-7_3; Danelljan Martin, 2014, BRIT MACH VIS C NOTT; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Hornakova A, 2020, PR MACH LEARN RES, V119; Hrabalik, 2017, THESIS CZECH TU PRAG; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kotera J, 2019, IEEE INT CONF COMP V, P2300, DOI 10.1109/ICCVW.2019.00283; Kotera J, 2018, IEEE IMAGE PROC, P2860, DOI 10.1109/ICIP.2018.8451661; Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1; Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14; Kroeger T, 2014, LECT NOTES COMPUT SC, V8753, P653, DOI 10.1007/978-3-319-11752-2_54; Lukezic A, 2019, LECT NOTES COMPUT SC, V11362, P595, DOI 10.1007/978-3-030-20890-5_38; Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515; Ma B, 2016, IEEE T IMAGE PROCESS, V25, P5867, DOI 10.1109/TIP.2016.2615812; Moudgil, 2017, ARXIV171201358; Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27; Rozumnyi, 2019, THESIS CZECH TU PRAG; Rozumnyi D, 2020, PROC CVPR IEEE, P6777, DOI 10.1109/CVPR42600.2020.00681; Rozumnyi D, 2019, LECT NOTES COMPUT SC, V11824, P122, DOI 10.1007/978-3-030-33676-9_9; Rozumnyi D, 2017, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2017.514; Runia TFH, 2020, P IEEE CVF C COMP VI, DOI 10.1109/cvpr42600.2020.010519; Seibold C, 2017, COMPUT VIS IMAGE UND, V160, P45, DOI 10.1016/j.cviu.2017.03.005; Tang M, 2018, PROC CVPR IEEE, P4874, DOI 10.1109/CVPR.2018.00512; Tao R., 2017, ARXIV PREPRINT ARXIV; Tibshirani RJ., 2017, P 31 INT C NEUR INF, P517; Vojuir T., 2013, ROBUST SCALE ADAPTIV, DOI 10.1007/978-3-642-38886-6; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Wu Y, 2011, IEEE I CONF COMP VIS, P1100, DOI 10.1109/ICCV.2011.6126357	30	0	0	2	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2583	2604		10.1007/s11263-021-01480-w	http://dx.doi.org/10.1007/s11263-021-01480-w		JUN 2021	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN		Green Published, hybrid			2022-12-18	WOS:000664019500001
J	Knobelreiter, P; Pock, T				Knobelreiter, Patrick; Pock, Thomas			Learned Collaborative Stereo Refinement	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo; Refinement; Deep learning; Optimization; Interpretable AI		In this work, we propose a learning-based method to denoise and refine disparity maps. The proposed variational network arises naturally from unrolling the iterates of a proximal gradient method applied to a variational energy defined in a joint disparity, color, and confidence image space. Our method allows to learn a robust collaborative regularizer leveraging the joint statistics of the color image, the confidence map and the disparity map. Due to the variational structure of our method, the individual steps can be easily visualized, thus enabling interpretability of the method. We can therefore provide interesting insights into how our method refines and denoises disparity maps. To this end, we can visualize and interpret the learned filters and activation functions and prove the increased reliability of the predicted pixel-wise confidence maps. Furthermore, the optimization based structure of our refinement module allows us to compute eigen disparity maps, which reveal structural properties of our refinement module. The efficiency of our method is demonstrated on the publicly available stereo benchmarks Middlebury 2014 and Kitti 2015.	[Knobelreiter, Patrick; Pock, Thomas] Graz Univ Technol, Graz, Austria	Graz University of Technology	Knobelreiter, P (corresponding author), Graz Univ Technol, Graz, Austria.	knoebelreiter@icg.tugraz.at; pock@icg.tugraz.at		Knobelreiter, Patrick/0000-0002-2371-014X	Graz University of Technology	Graz University of Technology	Open access funding provided by Graz University of Technology.	Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen YJ, 2015, PROC CVPR IEEE, P5261, DOI 10.1109/CVPR.2015.7299163; Effland A, 2020, J MATH IMAGING VIS, V62, P396, DOI 10.1007/s10851-019-00926-8; Gidaris S, 2017, PROC CVPR IEEE, P7187, DOI 10.1109/CVPR.2017.760; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Khamis S, 2018, LECT NOTES COMPUT SC, V11219, P596, DOI 10.1007/978-3-030-01267-0_35; Kingma D.P, P 3 INT C LEARNING R; Knobelreiter P, 2019, LECT NOTES COMPUT SC, V11824, P3, DOI 10.1007/978-3-030-33676-9_1; Knobelreiter P, 2017, PROC CVPR IEEE, P1456, DOI 10.1109/CVPR.2017.159; Kuschk G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P700, DOI 10.1109/ICCVW.2013.96; Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Maurer D., 2017, BRIT MACH VIS C; Menze Moritz, 2015, CVPR; Nesterov Y., 1988, MAT METODY RESHENIYA, V24, P509; Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108; Parikh N., 2014, PROXIMAL ALGORITHMS, P127; Ranftl R, 2014, LECT NOTES COMPUT SC, V8689, P439, DOI 10.1007/978-3-319-10590-1_29; Ranftl R, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P401, DOI 10.1109/IVS.2012.6232171; Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720; Riegler G, 2016, LECT NOTES COMPUT SC, V9907, P268, DOI 10.1007/978-3-319-46487-9_17; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3; Shekhovtsov A., 2016, COMP VIS WINT WORKSH; Tulyakov S, 2018, ADV NEUR IN, V31; Vogel C., 2018, AS C COMP VIS ACCV, P340; Vogel C, 2017, LECT NOTES COMPUT SC, V10496, P189, DOI 10.1007/978-3-319-66709-6_16; Wang S., 2016, ADV NEURAL INFORM PR, V29, P865; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zbontar J, 2016, J MACH LEARN RES, V17; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420	40	0	0	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2565	2582		10.1007/s11263-021-01485-5	http://dx.doi.org/10.1007/s11263-021-01485-5		JUN 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN		hybrid, Green Submitted			2022-12-18	WOS:000664313700001
J	Madadi, M; Bertiche, H; Escalera, S				Madadi, Meysam; Bertiche, Hugo; Escalera, Sergio			Deep Unsupervised 3D Human Body Reconstruction from a Sparse set of Landmarks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human body reconstruction; Mocap data; Unsupervised deep learning; Attention model; Cascading	HUMAN POSE ESTIMATION; DEFORMATION	In this paper we propose the first deep unsupervised approach in human body reconstruction to estimate body surface from a sparse set of landmarks, so called DeepMurf. We apply a denoising autoencoder to estimate missing landmarks. Then we apply an attention model to estimate body joints from landmarks. Finally, a cascading network is applied to regress parameters of a statistical generative model that reconstructs body. Our set of proposed loss functions allows us to train the network in an unsupervised way. Results on four public datasets show that our approach accurately reconstructs the human body from real world mocap data.	[Madadi, Meysam; Bertiche, Hugo; Escalera, Sergio] Comp Vis Ctr, Barcelona, Spain; [Madadi, Meysam; Bertiche, Hugo; Escalera, Sergio] Univ Barcelona, Barcelona, Spain	Centre de Visio per Computador (CVC); University of Barcelona	Madadi, M (corresponding author), Comp Vis Ctr, Barcelona, Spain.; Madadi, M (corresponding author), Univ Barcelona, Barcelona, Spain.	mmadadi@cvc.uab.es	Escalera, Sergio/L-2998-2015	Escalera, Sergio/0000-0003-0617-8873; madadi, meysam/0000-0002-7384-5712	ICREA; MINECO/FEDER, UE [PID2019-105093GB-I00]; CERCA Programme/Generalitat de Catalunya; Amazon Research Awards ARA	ICREA(ICREA); MINECO/FEDER, UE(Spanish Government); CERCA Programme/Generalitat de Catalunya; Amazon Research Awards ARA	This work is partially supported by ICREA under the ICREA Academia programme, and by the Spanish project PID2019-105093GB-I00 (MINECO/FEDER, UE) and CERCA Programme/Generalitat de Catalunya, and by Amazon Research Awards ARA.	Achilles Felix, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9900, P491, DOI 10.1007/978-3-319-46720-7_57; Bhatnagar Bharat Lal, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P311, DOI 10.1007/978-3-030-58536-5_19; Bhatnagar Bharat Lal, 2020, NEURAL INFORM PROCES, V3; Bogo F, 2015, IEEE I CONF COMP VIS, P2300, DOI 10.1109/ICCV.2015.265; Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248; Chen CH, 2019, PROC CVPR IEEE, P5707, DOI 10.1109/CVPR.2019.00586; Groueix T, 2018, LECT NOTES COMPUT SC, V11206, P235, DOI 10.1007/978-3-030-01216-8_15; Hoyet L., 2012, PROC ACM SIGGRAPH S, P79; Huang YH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275108; Insafutdinov E, 2018, ADV NEUR IN, V31; Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868; Kudo Y., 2018, ARXIV180308244; Lab C. G., 2000, CMU GRAPHICS LAB MOT; Liu H, 2011, S INTERACT D, P133, DOI [10.1145/1944745.1944768, DOI 10.1145/1944745.1944768]; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Loper M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661273; Madadi Meysam, 2018, ARXIV181210766; Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554; Mehrizi R, 2018, IEEE INT CONF AUTOMA, P485, DOI 10.1109/FG.2018.00078; Mehta D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392410; Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46; Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062; Park SI, 2006, ACM T GRAPHIC, V25, P881, DOI 10.1145/1141911.1141970; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Prokudin S, 2019, IEEE I CONF COMP VIS, P4331, DOI [10.1109/ICCV.2019.00443, 10.1109/ICCVW.2019.00370]; Rhodin H, 2016, LECT NOTES COMPUT SC, V9909, P509, DOI 10.1007/978-3-319-46454-1_31; Schwarz LA, 2009, LECT NOTES COMPUT SC, V5903, P159, DOI 10.1007/978-3-642-10470-1_14; Slyper R., 2008, P 2008 ACM SIGGRAPHE, P193, DOI DOI 10.2312/SCA/SCA08/193-199; Tautges J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966397; Trumble M., 2017, 2017 BRIT MACH VIS C; Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Vincent P, 2010, J MACH LEARN RES, V11, P3371; von Marcard T, 2017, COMPUT GRAPH FORUM, V36, P349, DOI 10.1111/cgf.13131; von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37; von Marcard T, 2016, IEEE T PATTERN ANAL, V38, P1533, DOI 10.1109/TPAMI.2016.2522398; Zhao W., 2012, P ACM SIGGRAPH EUR S, P33, DOI [10.2312/SCA/SCA12/033-042, DOI 10.2312/SCA/SCA12/033-042]	38	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2021	129	8					2499	2512		10.1007/s11263-021-01488-2	http://dx.doi.org/10.1007/s11263-021-01488-2		JUN 2021	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TR3NV		Green Submitted			2022-12-18	WOS:000661789300001
J	Du, DP; Wang, LM; Li, ZY; Wu, GS				Du, Dapeng; Wang, Limin; Li, Zhaoyang; Wu, Gangshan			Cross-Modal Pyramid Translation for RGB-D Scene Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						RGB-D scene recognition; Cross-modal transfer; Multi-modal recognition		The existing RGB-D scene recognition approaches typically employ two separate and modality-specific networks to learn effective RGB and Depth representations respectively. This independent training scheme fails to capture the correlation of two modalities, and thus may be suboptimal for RGB-D scene recognition. To address this issue, this paper proposes a general and flexible framework to enhance RGB-D representation learning with a customized cross-modal pyramid translation branch, coined as TRecgNet. This framework unifies the tasks of cross-modal translation and modality-specific recognition with a shared feature encoder, and aims at leveraging the correspondence between two modalities to regularize the representation learning of each modality. Specifically, we present a cross-modal pyramid translation strategy to perform multi-scale image generation with a carefully designed layer-wise perceptual supervision. To improve the complementarity of cross-modal translation to modality specific scene recognition, we devise a feature selection module to adaptively enhance the discriminative information during the translation procedure. In addition, we train multiple auxiliary classifiers to further regularize the behavior of generated data to be consistent with its paired data on label prediction. Meanwhile, our translation branch enables us to generate cross-modal data for training data augmentation and further improve single modality scene recognition. Extensive experiments on benchmarks of SUN RGB-D and NYU Depth V2 demonstrate the superiority of the proposed method to the state-of-the-art RGB-D scene recognition methods. We also generalize the TRecgNet to the single modality scene recognition benchmark of MIT Indoor, and automatically synthesize a depth view to boost the final recognition accuracy.	[Du, Dapeng; Wang, Limin; Li, Zhaoyang; Wu, Gangshan] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China	Nanjing University	Wang, LM (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.	dudp.nju@gmail.com; lmwang@nju.edu.cn; zhaoyangli@smail.nju.edu.cn; gswu@nju.edu.cn		Wang, Limin/0000-0002-3674-7718	National Science Foundation of China [62076119, 61921006]; Program for Innovative Talents and Entrepreneur in Jiangsu Province; Collaborative Innovation Center of Novel Software Technology and Industrialization	National Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Innovative Talents and Entrepreneur in Jiangsu Province; Collaborative Innovation Center of Novel Software Technology and Industrialization	This work is supported by the National Science Foundation of China (No. 62076119, No. 61921006), Program for Innovative Talents and Entrepreneur in Jiangsu Province, and Collaborative Innovation Center of Novel Software Technology and Industrialization.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2013, COMPUT SCI; Aytar Y, 2018, IEEE T PATTERN ANAL, V40, P2303, DOI 10.1109/TPAMI.2017.2753232; Banica D, 2015, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2015.7298974; Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097; Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986; Cheng XJ, 2018, PATTERN RECOGN, V74, P474, DOI 10.1016/j.patcog.2017.09.025; Christoudias CM, 2010, LECT NOTES COMPUT SC, V6311, P677, DOI 10.1007/978-3-642-15549-9_49; Cimpoi M, 2016, INT J COMPUT VISION, V118, P65, DOI 10.1007/s11263-015-0872-3; Du DP, 2019, PROC CVPR IEEE, P11828, DOI 10.1109/CVPR.2019.01211; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Gupta S, 2015, INT J COMPUT VISION, V112, P133, DOI 10.1007/s11263-014-0777-6; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hensel M, 2017, ADV NEUR IN, V30; Howard A.G., 2017, MOBILENETS EFFICIENT; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Janoch A., 2013, CONSUMER DEPTH CAMER, P141, DOI DOI 10.1007/978-1-4471-4640-7_8; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kalogeiton V, 2017, IEEE I CONF COMP VIS, P2001, DOI 10.1109/ICCV.2017.219; Kapidis G, 2019, IEEE INT CONF COMP V, P4396, DOI 10.1109/ICCVW.2019.00540; Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781; King DB, 2015, ACS SYM SER, V1214, P1; Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Li Tianhao, 2020, ARXIV PREPRINT ARXIV; Li Y, 2018, AAAI CONF ARTIF INTE, P338; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu YN, 2016, SIGNAL PROCESS, V120, P761, DOI 10.1016/j.sigpro.2015.01.001; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; McCormac J, 2017, IEEE I CONF COMP VIS, P2697, DOI 10.1109/ICCV.2017.292; Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Omeiza Daniel, 2019, ARXIV190801224; Paszke A, 2019, ADV NEUR IN, V32; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Ren ZZ, 2018, PROC CVPR IEEE, P762, DOI [10.1109/CVPR.2018.00086, 10.1109/CVPR.2018.00104]; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shmelkov K, 2018, LECT NOTES COMPUT SC, V11206, P218, DOI 10.1007/978-3-030-01216-8_14; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Song XH, 2020, IEEE T IMAGE PROCESS, V29, P525, DOI 10.1109/TIP.2019.2933728; Song XH, 2017, AAAI CONF ARTIF INTE, P4271; Srivastava Nitish, 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang A, 2016, PROC CVPR IEEE, P5995, DOI 10.1109/CVPR.2016.645; Wang HF, 2020, IEEE COMPUT SOC CONF, P111, DOI 10.1109/CVPRW50498.2020.00020; Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155; Wang LM, 2018, INT J COMPUT VISION, V126, P390, DOI 10.1007/s11263-017-1043-5; Wang LM, 2017, IEEE T IMAGE PROCESS, V26, P2055, DOI 10.1109/TIP.2017.2675339; Wang PC, 2017, PROC CVPR IEEE, P416, DOI 10.1109/CVPR.2017.52; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Wang Z, 2017, IEEE T IMAGE PROCESS, V26, P2028, DOI 10.1109/TIP.2017.2666739; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Xiong ZT, 2020, NEUROCOMPUTING, V373, P81, DOI 10.1016/j.neucom.2019.09.066; Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451; Xu XY, 2017, PATTERN RECOGN, V72, P300, DOI 10.1016/j.patcog.2017.07.026; Yuan Y, 2019, AAAI CONF ARTIF INTE, P9176; Zhang J, 2016, PATTERN RECOGN, V60, P86, DOI 10.1016/j.patcog.2016.05.019; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhu HY, 2016, PROC CVPR IEEE, P2969, DOI 10.1109/CVPR.2016.324	73	0	0	2	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2021	129	8					2309	2327		10.1007/s11263-021-01475-7	http://dx.doi.org/10.1007/s11263-021-01475-7		MAY 2021	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TR3NV					2022-12-18	WOS:000651793000001
J	Dai, DX; Tan, RT; Patel, V; Matas, J; Schiele, B; Van Gool, L				Dai, Dengxin; Tan, Robby T.; Patel, Vishal; Matas, Jiri; Schiele, Bernt; Van Gool, Luc			Guest Editorial: Special Issue on "Computer Vision for All Seasons: Adverse Weather and Lighting Conditions"	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Dai, Dengxin; Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland; [Tan, Robby T.] Natl Univ Singapore, Singapore, Singapore; [Patel, Vishal] Johns Hopkins Univ, Baltimore, MD USA; [Matas, Jiri] Czech Tech Univ, Prague, Czech Republic; [Schiele, Bernt] MPI Informat, Saarbrucken, Germany; [Van Gool, Luc] Katholieke Univ Leuven, Leuven, Belgium	Swiss Federal Institutes of Technology Domain; ETH Zurich; National University of Singapore; Johns Hopkins University; Czech Technical University Prague; Max Planck Society; KU Leuven	Dai, DX (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	dai@vision.ee.ethz.ch; robby.tan@nus.edu.sg; vpatel36@jhu.edu; matas@cmp.felk.cvut.cz; schiele@mpi-inf.mpg.de; vangool@vision.ee.ethz.ch	, Matas/AAW-3282-2020					[Anonymous], 2019, P IEEE CVF INT C COM; Bitterwolf J., 2020, EUR C COMP VIS ECCV; Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347; Cheong T., 2020, P IEEE CVF C COMP VI; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai DX, 2020, INT J COMPUT VISION, V128, P1182, DOI 10.1007/s11263-019-01182-4; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Hendrycks D., 2019, INT C LEARN REPR; Kolter J. Z, 2021, INT C LEARN REPR; Lakshminarayanan B., 2020, INT C LEARN REPR; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498; Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723; P Wenzel, 2020, GERM C PATT REC GCPR; Rother C, 2020, P IEEE CVF C COMP VI; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sakaridis C, 2022, IEEE T PATTERN ANAL, V44, P3139, DOI 10.1109/TPAMI.2020.3045882; Sakaridis C, 2019, IEEE I CONF COMP VIS, P7373, DOI 10.1109/ICCV.2019.00747; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Stenborg E, 2019, IEEE C COMP VIS PATT; Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643; Weiss, ECCV; Wulfmeier M, 2018, IEEE INT CONF ROBOT, P4489; Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337; Zheng Z., 2020, IEEE EUR C COMP VIS; Zou Q., 2018, EUR C COMP VIS ECCV	28	0	0	2	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2031	2033		10.1007/s11263-021-01464-w	http://dx.doi.org/10.1007/s11263-021-01464-w		MAY 2021	3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW		Bronze			2022-12-18	WOS:000647988800001
J	Scharstein, D; Dai, A; Kondermann, D; Sattler, T; Schindler, K				Scharstein, Daniel; Dai, Angela; Kondermann, Daniel; Sattler, Torsten; Schindler, Konrad			Guest Editorial: Special Issue on Performance Evaluation in Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Scharstein, Daniel] Middlebury Coll, Dept Comp Sci, Middlebury, VT 05753 USA; [Dai, Angela] Tech Univ Munich, Dept Informat, Boltzmannstr 3, D-85748 Garching, Germany; [Kondermann, Daniel] Qual Match GmbH, Langer Anger 7-9, D-69115 Heidelberg, Germany; [Sattler, Torsten] Czech Tech Univ, Czech Inst Informat Robot & Cybernet, Jugoslavskych Partyzanu 1580-3, Prague 16000 6, Czech Republic; [Schindler, Konrad] Swiss Fed Inst Technol, Photogrammetry & Remote Sensing, Stefano Franscini Pl 5, CH-8093 Zurich, Switzerland	Technical University of Munich; Czech Technical University Prague; Swiss Federal Institutes of Technology Domain; ETH Zurich	Scharstein, D (corresponding author), Middlebury Coll, Dept Comp Sci, Middlebury, VT 05753 USA.	schar@middlebury.edu; angela.dai@tum.de; dk@quality-match.com; torsten.sattler@cvut.cz; schindler@ethz.ch	Sattler, Torsten/AAM-3155-2021					Benny Y, 2021, INT J COMPUT VISION, V129, P1712, DOI 10.1007/s11263-020-01424-w; Bergmann P, 2021, INT J COMPUT VISION, V129, P1038, DOI 10.1007/s11263-020-01400-4; Black S, 2021, INT J COMPUT VISION, V129, P977, DOI 10.1007/s11263-020-01403-1; Ding KY, 2021, INT J COMPUT VISION, V129, P1258, DOI 10.1007/s11263-020-01419-7; Fan H, 2021, INT J COMPUT VISION, V129, P439, DOI 10.1007/s11263-020-01387-y; Georgopoulos M, 2021, INT J COMPUT VISION, V129, P2288, DOI 10.1007/s11263-021-01448-w; Jensen SHN, 2021, INT J COMPUT VISION, V129, P882, DOI 10.1007/s11263-020-01406-y; Jin YH, 2021, INT J COMPUT VISION, V129, P517, DOI 10.1007/s11263-020-01385-0; Kamann C, 2021, INT J COMPUT VISION, V129, P462, DOI 10.1007/s11263-020-01383-2; Li SY, 2021, INT J COMPUT VISION, V129, P1301, DOI 10.1007/s11263-020-01416-w; Ramakrishnan SK, 2021, INT J COMPUT VISION, V129, P1616, DOI 10.1007/s11263-021-01437-z; Shekar AK, 2021, INT J COMPUT VISION, V129, P1185, DOI 10.1007/s11263-020-01423-x; Weinzaepfel P, 2021, INT J COMPUT VISION, V129, P1675, DOI 10.1007/s11263-021-01446-y; Yan JB, 2021, INT J COMPUT VISION, V129, P1768, DOI 10.1007/s11263-021-01450-2; Zaffar M, 2021, INT J COMPUT VISION, V129, P2136, DOI 10.1007/s11263-021-01469-5; Zhang ZC, 2021, INT J COMPUT VISION, V129, P821, DOI 10.1007/s11263-020-01399-8	18	0	0	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2029	2030		10.1007/s11263-021-01455-x	http://dx.doi.org/10.1007/s11263-021-01455-x		APR 2021	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW		Bronze			2022-12-18	WOS:000645194900001
J	Micusik, B; Evangelidis, G				Micusik, Branislav; Evangelidis, Georgios			Renormalization for Initialization of Rolling Shutter Visual-Inertial Odometry	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual-inertial odometry initialization; Renormalization; Rolling-shutter camera	VISION; OPTIMIZATION; CALIBRATION; SURFACES	In this paper we deal with the initialization problem of a visual-inertial odometry system with rolling shutter cameras. Initialization is a prerequisite for using inertial signals and fusing them with visual data. We propose a novel statistical solution to the initialization problem on visual and inertial data simultaneously, by casting it into the renormalization scheme of Kanatani. The renormalization is an optimization scheme which intends to reduce the inherent statistical bias of common linear systems. We derive and present the necessary steps and methodology specific to the initialization problem. Extensive evaluations on ground truth exhibit superior performance and a gain in accuracy of up to 20% over the originally proposed Least Squares solution. The renormalization performs similarly to the optimal Maximum Likelihood estimate, despite arriving at the solution by different means. With this paper we are adding to the set of Computer Vision problems which can be cast into the renormalization scheme.	[Micusik, Branislav; Evangelidis, Georgios] Snap Inc, Fleischmarkt 3-5, A-1010 Vienna, Austria		Micusik, B (corresponding author), Snap Inc, Fleischmarkt 3-5, A-1010 Vienna, Austria.	brano@snap.com; georgios@snap.com						Albl C, 2016, PROC CVPR IEEE, P3355, DOI 10.1109/CVPR.2016.365; Albl C, 2015, PROC CVPR IEEE, P2292, DOI 10.1109/CVPR.2015.7298842; [Anonymous], 2019, UNR ENG; Apple, 2015, ARKIT; Bapat A, 2018, PROC CVPR IEEE, P4824, DOI 10.1109/CVPR.2018.00507; Campos C, 2019, IEEE INT CONF ROBOT, P1288, DOI 10.1109/ICRA.2019.8793718; Chojnacki W, 2001, J MATH IMAGING VIS, V14, P21, DOI 10.1023/A:1008355213497; Chojnacki W, 2000, IEEE T PATTERN ANAL, V22, P1294, DOI 10.1109/34.888714; Cremers, 2018, P EUR C COMP VIS ECC; Dai YC, 2016, PROC CVPR IEEE, P4132, DOI 10.1109/CVPR.2016.448; Dong-Si TC, 2012, IEEE INT C INT ROBOT, P1064, DOI 10.1109/IROS.2012.6386235; Evangelidis G, 2021, IEEE ROBOT AUTOM LET, V6, P1415, DOI 10.1109/LRA.2021.3057564; Evangelidis GD, 2008, IEEE T PATTERN ANAL, V30, P1858, DOI 10.1109/TPAMI.2008.113; Forstner W, 2016, GEOM COMPUT, V11; Forster C, 2017, IEEE T ROBOT, V33, P1, DOI 10.1109/TRO.2016.2597321; Golub G. H., 2013, MATRIX COMPUTATIONS; Google, 2018, ARCORE; Gupta R, 1997, IEEE T PATTERN ANAL, V19, P963, DOI 10.1109/34.615446; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hedborg J, 2012, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2012.6247831; Huang WB, 2020, IEEE T ROBOT, V36, P1153, DOI 10.1109/TRO.2019.2959161; Kaiser J, 2017, IEEE ROBOT AUTOM LET, V2, P18, DOI 10.1109/LRA.2016.2521413; Kanatani K, 2016, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-48493-8; Kanatani K., 1996, STAT OPTIMIZATION GE; Kanatani K, 2008, INT J COMPUT VISION, V80, P167, DOI 10.1007/s11263-007-0098-0; Kanatani K, 2014, INT C PATT RECOG, P1, DOI 10.1109/ICPR.2014.11; Kneip L, 2011, IEEE INT C INT ROBOT; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; Li M, 2013, IEEE INT CONF ROBOT, P4712, DOI 10.1109/ICRA.2013.6631248; Lourakis MIA, 2005, IEEE I CONF COMP VIS, P1526; Martinelli A, 2014, INT J COMPUT VISION, V106, P138, DOI 10.1007/s11263-013-0647-7; Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024; Mur-Artal R, 2017, IEEE ROBOT AUTOM LET, V2, P796, DOI 10.1109/LRA.2017.2653359; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Okatani T, 2009, PROC CVPR IEEE, P959, DOI 10.1109/CVPRW.2009.5206722; Patron-Perez A, 2015, INT J COMPUT VISION, V113, P208, DOI 10.1007/s11263-015-0811-3; Qin T, 2017, IEEE INT C INT ROBOT, P4225; Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275; Sastry, 2005, COMPUTING RES REPOSI; Schubert D, 2019, IEEE INT C INT ROBOT, P2462, DOI 10.1109/IROS40897.2019.8968539; TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273; Zhang, 2018, P EUR C COMP VIS ECC	42	0	0	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					2011	2027		10.1007/s11263-021-01462-y	http://dx.doi.org/10.1007/s11263-021-01462-y		APR 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU		Green Submitted			2022-12-18	WOS:000641238700001
J	Xu, QQ; Xiong, JC; Cao, XC; Huang, QM; Yao, Y				Xu, Qianqian; Xiong, Jiechao; Cao, Xiaochun; Huang, Qingming; Yao, Yuan			Evaluating Visual Properties via Robust HodgeRank	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual properties; Hodge decomposition; Linearized Bregman iteration; Paired comparison; Robust ranking	VARIABLE SELECTION; RANDOM GRAPHS; THRESHOLDS; REGRESSION; SHRINKAGE; RECOVERY; LASSO	Nowadays, how to effectively evaluate visual properties has become a popular topic for fine-grained visual comprehension. In this paper we study the problem of how to estimate such visual properties from a ranking perspective with the help of the annotators from online crowdsourcing platforms. The main challenges of our task are two-fold. On one hand, the annotations often contain contaminated information, where a small fraction of label flips might ruin the global ranking of the whole dataset. On the other hand, considering the large data capacity, the annotations are often far from being complete. What is worse, there might even exist imbalanced annotations where a small subset of samples are frequently annotated. Facing such challenges, we propose a robust ranking framework based on the principle of Hodge decomposition of imbalanced and incomplete ranking data. According to the HodgeRank theory, we find that the major source of the contamination comes from the cyclic ranking component of the Hodge decomposition. This leads us to an outlier detection formulation as sparse approximations of the cyclic ranking projection. Taking a step further, it facilitates a novel outlier detection model as Huber's LASSO in robust statistics. Moreover, simple yet scalable algorithms are developed based on Linearized Bregman Iteration to achieve an even less biased estimator. Statistical consistency of outlier detection is established in both cases under nearly the same conditions. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a promising tool for robust ranking with large scale crowdsourcing data arising from computer vision.	[Xu, Qianqian; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China; [Xiong, Jiechao] Tencent AI Lab, Shenzhen, Peoples R China; [Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing, Peoples R China; [Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing, Peoples R China; [Cao, Xiaochun; Huang, Qingming] Peng Cheng Lab, Shenzhen, Peoples R China; [Yao, Yuan] Hong Kong Univ Sci & Technol, Dept Math, Hong Kong, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Tencent; Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Peng Cheng Laboratory; Hong Kong University of Science & Technology	Xu, QQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.; Yao, Y (corresponding author), Hong Kong Univ Sci & Technol, Dept Math, Hong Kong, Peoples R China.	xuqianqian@ict.ac.cn; jchxiong@gmail.com; caoxiaochun@iie.ac.cn; qmhuang@ucas.ac.cn; yuany@ust.hk			National Key R&D Program of China [2018AAA0102003]; National Natural Science Foundation of China [61861166002, U1736219, 61976202, U1803264, 61620106009, 61931008, 61836002]; Youth Innovation Promotion Association CAS; Strategic Priority Research Program of Chinese Academy of Sciences [XDB28000000]; Hong Kong Research Grant Council (HKRGC) [16303817, ITF UIM/390]; Tencent AI Lab; Si Family Foundation; Microsoft Research-Asia	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Youth Innovation Promotion Association CAS; Strategic Priority Research Program of Chinese Academy of Sciences(Chinese Academy of Sciences); Hong Kong Research Grant Council (HKRGC)(Hong Kong Research Grants Council); Tencent AI Lab; Si Family Foundation; Microsoft Research-Asia(Microsoft)	This work was supported in part by the National Key R&D Program of China under Grant No. 2018AAA0102003, in part by National Natural Science Foundation of China: 61861166002, U1736219, 61976202, U1803264, 61620106009, 61931008 and 61836002, in part by Youth Innovation Promotion Association CAS, and in part by the Strategic Priority Research Program of Chinese Academy of Sciences, Grant No. XDB28000000. The research of Yuan Yao was supported in part by Hong Kong Research Grant Council (HKRGC) Grant 16303817, ITF UIM/390, as well as awards from Tencent AI Lab, Si Family Foundation, and Microsoft Research-Asia.	Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Chen KT, 2009, MATH COMPUT SCI ENG, P491, DOI 10.1145/1631272.1631339; Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785; Cortes Corinna, 2007, P 24 INT C MACH LEAR, P169; Dereich S, 2019, NUMER MATH, V142, P279, DOI 10.1007/s00211-019-01024-y; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Eichhorn A, 2010, NOSSDAV 2010: PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P63; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Fu Y., 2019, PARSIMONIOUS DEEP LE; Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441; Grandvalet Y., 2018, SPARSITY WORST CASE; He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751; Huang, 2018, INT C ART INT STAT, P2047; Huber P., 1981, ROBUST STAT; HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503; Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x; Kahle M, 2014, ANN MATH, V179, P1085, DOI 10.4007/annals.2014.179.3.5; Kahle M, 2009, DISCRETE MATH, V309, P1658, DOI 10.1016/j.disc.2008.02.037; Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40; Kovashka A, 2015, INT J COMPUT VISION, V114, P56, DOI 10.1007/s11263-014-0798-1; Le Callet P., 2005, SUBJECTIVE QUALITY A; Li GL, 2016, IEEE T KNOWL DATA EN, V28, P2296, DOI 10.1109/TKDE.2016.2535242; Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514; Liang J, 2018, EUR SIGNAL PR CONF, P732, DOI 10.23919/EUSIPCO.2018.8553411; Miao Y, 2018, J APPL PROBAB, V55, P559, DOI 10.1017/jpr.2018.35; Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412; Osher S, 2016, APPL COMPUT HARMON A, V41, P436, DOI 10.1016/j.acha.2016.01.002; Osting B, 2013, INVERSE PROBL IMAG, V7, P907, DOI 10.3934/ipi.2013.7.907; Pan HY, 2018, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR.2018.00554; Parthasarathy S, 2017, INT CONF ACOUST SPEE, P4995, DOI 10.1109/ICASSP.2017.7953107; She YY, 2011, J AM STAT ASSOC, V106, P626, DOI 10.1198/jasa.2011.tm10390; Sheikh H.R., 2008, LIVE IMAGE VIDEO QUA; Shen FM, 2019, IEEE T IMAGE PROCESS, V28, P3662, DOI 10.1109/TIP.2019.2899987; Tao SZ, 2016, SIAM J OPTIMIZ, V26, P313, DOI 10.1137/151004549; Tay J.K., 2018, PRINCIPAL COMPONENT; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2020, J COMPUT GRAPH STAT, V29, P215, DOI 10.1080/10618600.2019.1648271; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Wu CC, 2013, IEEE T MULTIMEDIA, V15, P1121, DOI 10.1109/TMM.2013.2241043; Xu, 2011, P 19 ACM INT C MULT, P393, DOI DOI 10.1145/2072298.2072350; Xu, 2016, INT C MACH LEARN, P1282; Xu, 2013, P 21 ACM INT C MULT, P43, DOI DOI 10.1145/2502081.2502083; Xu Q., 2012, P 20 ACM INT C MULTI, P359; Xu QQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1540, DOI 10.1145/3123266.3123267; Xu QQ, 2014, IEEE T MULTIMEDIA, V16, P373, DOI 10.1109/TMM.2013.2292568; Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924; Yin WT, 2008, SIAM J IMAGING SCI, V1, P143, DOI 10.1137/070703983; Yu SX, 2012, IEEE T PATTERN ANAL, V34, P158, DOI 10.1109/TPAMI.2011.107; Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26; Zhao, 2018, INT C MACH LEARN, P5907; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	55	0	0	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1732	1753		10.1007/s11263-021-01438-y	http://dx.doi.org/10.1007/s11263-021-01438-y		MAR 2021	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000625841400001
J	Jiang, HZ; Li, YX; Zhao, HJ; Li, XD; Xu, Y				Jiang, Hongzhi; Li, Yuxi; Zhao, Huijie; Li, Xudong; Xu, Yang			Parallel Single-Pixel Imaging: A General Method for Direct-Global Separation and 3D Shape Reconstruction Under Strong Global Illumination (Jan, 10.1007/s11263-020-01413-z, 2021)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Jiang, Hongzhi; Li, Yuxi; Zhao, Huijie; Li, Xudong; Xu, Yang] Beihang Univ BUAA, Sch Instrumentat & Optoelect Engn, Minist Educ, Key Lab Precis Optomechatron Technol, 37 Xueyuan Rd, Beijing 100191, Peoples R China	Beihang University	Zhao, HJ (corresponding author), Beihang Univ BUAA, Sch Instrumentat & Optoelect Engn, Minist Educ, Key Lab Precis Optomechatron Technol, 37 Xueyuan Rd, Beijing 100191, Peoples R China.	jhz1862@buaa.edu.cn; yuxili@buaa.edu.cn; buaa_zhaohj@163.com; xdli@buaa.edu.cn; xuyang17@buaa.edu.cn		Yuxi, Li/0000-0002-1922-5811				Jiang HZ, 2021, INT J COMPUT VISION, V129, P1060, DOI 10.1007/s11263-020-01413-z	1	0	0	4	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1787	1787		10.1007/s11263-021-01441-3	http://dx.doi.org/10.1007/s11263-021-01441-3		FEB 2021	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Bronze			2022-12-18	WOS:000622275300001
J	Li, ZH; Xi, T; Zhang, G; Liu, JT; He, R				Li, Zhihang; Xi, Teng; Zhang, Gang; Liu, Jingtuo; He, Ran			AutoDet: Pyramid Network Architecture Search for Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Neural architecture search; Feature pyramids		Feature pyramids have delivered significant improvement in object detection. However, building effective feature pyramids heavily relies on expert knowledge, and also requires strenuous efforts to balance effectiveness and efficiency. Automatic search methods, such as NAS-FPN, automates the design of feature pyramids, but the low search efficiency makes it difficult to apply in a large search space. In this paper, we propose a novel search framework for a feature pyramid network, called AutoDet, which enables to automatic discovery of informative connections between multi-scale features and configure detection architectures with both high efficiency and state-of-the-art performance. In AutoDet, a new search space is specifically designed for feature pyramids in object detectors, which is more general than NAS-FPN. Furthermore, the architecture search process is formulated as a combinatorial optimization problem and solved by a Simulated Annealing-based Network Architecture Search method (SA-NAS). Compared with existing NAS methods, AutoDet ensures a dramatic reduction in search times. For example, our SA-NAS can be up to 30x faster than reinforcement learning-based approaches. Furthermore, AutoDet is compatible with both one-stage and two-stage structures with all kinds of backbone networks. We demonstrate the effectiveness of AutoDet with outperforming single-model results on the COCO dataset. Without pre-training on OpenImages, AutoDet with the ResNet-101 backbone achieves an AP of 39.7 and 47.3 for one-stage and two-stage architectures, respectively, which surpass current state-of-the-art methods.	[Li, Zhihang; He, Ran] Chinese Acad Sci, NLPR, CRIPAC, CEBSIT, Beijing, Peoples R China; [Li, Zhihang; He, Ran] Univ Chinese Acad Sci, Sch Artif Intelligence, Beijing, Peoples R China; [Xi, Teng; Zhang, Gang; Liu, Jingtuo] Baidu Inc, Dept Comp Vis Technol VIS, Beijing, Peoples R China; [Xi, Teng] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China	Chinese Academy of Sciences; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Baidu; Tsinghua University	He, R (corresponding author), Chinese Acad Sci, NLPR, CRIPAC, CEBSIT, Beijing, Peoples R China.; He, R (corresponding author), Univ Chinese Acad Sci, Sch Artif Intelligence, Beijing, Peoples R China.	rhe@nlpr.ia.ac.cn			Beijing Natural Science Foundation [JQ18017]; National Natural Science Foundation of China [U20A20223]; Youth Innovation Promotion Association CAS [Y201929]	Beijing Natural Science Foundation(Beijing Natural Science Foundation); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Youth Innovation Promotion Association CAS	This work is partially funded by Beijing Natural Science Foundation (Grant No. JQ18017), National Natural Science Foundation of China (Grant No. U20A20223), and Youth Innovation Promotion Association CAS (Grant No. Y201929).	Adelson E.H., 1984, RCA ENG, V29, P33; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, FSSD FEATURE FUSION; [Anonymous], 2016, ARXIV161105594; [Anonymous], 2017, ARXIV170406904; Baker Bowen, 2017, ICLR; Bell S., 2016, CVPR; Bender Gabriel, 2018, ICML; Bharat Singh, 2017, Arxiv, DOI arXiv:1704.04503; Brock A., 2018, ICLR; Cai H, 2018, PR MACH LEARN RES, V80; Cai Z., 2016, ECCV; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Chen Y., 2019, DETNAS NEURAL ARCHIT, P6638; Chopard B., 2018, INTRO METAHEURISTICS, P59; Dai JF, 2016, NIPS; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479; Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186; Elsken T, 2019, J MACH LEARN RES, V20; Elsken Thomas, 2018, ICLR; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fu C. -Y., 2017, ARXIV17010665; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Howard A., 2019, ICCV; Hu Y., 2020, ECCV; Huang J., 2017, CVPR; Jenatton R, 2017, PR MACH LEARN RES, V70; Jiang N, 2017, PR MACH LEARN RES, V70; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kong T., 2017, P IEEE C COMP VIS PA, P5936; Kong T., 2016, CVPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Law H., 2019, IJCV; Li S, 2019, IEEE I CONF COMP VIS, P6608, DOI 10.1109/ICCV.2019.00671; Li Zhang, 2020, CVPR; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Lin Tsung-Yi, 2018, TPAMI; Liu C., 2019, ARXIV190102985; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu H., 2018, ARXIV180609055; Liu H., 2017, ARXIV171100436; Liu L., 2019, IJCV; Liu S., 2018, CVPR; Liu SY, 2018, IEEE ANN INT CONF CY, P385; Liu W., 2016, ECCV; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo Renqian, 2018, ADV NEURAL INFORM PR, P7816; Newell A., 2016, ECCV; Peng C, 2018, PROC CVPR IEEE, P6181, DOI 10.1109/CVPR.2018.00647; Pham H, 2018, 35 INT C MACH LEARN; Real E., 2018, ARXIV180201548; Redmon J., 2018, COMPUTER VISION PATT; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Ren S., 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.169; Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]; Shao L, 2019, P IEEE C COMP VIS PA, P7336; Shen Z., 2017, ICCV; Shrivastava Abhinav, 2016, ARXIV161206851; Singh B., 2018, ARXIV180509300, P9333, DOI 10.5555/3327546.3327604; Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377; Suganuma M., 2018, P ICML, P4778; Szegedy C, 2017, 31 AAAI C ART INT, P1, DOI DOI 10.5555/3298023.3298188; Tan M, 2019, PR MACH LEARN RES, P6105; Tan Mingxing, 2018, ARXIV180711626; Tan Mingxing, 2020, CVPR; Xie L., 2017, P IEEE INT C COMP VI, P1379, DOI DOI 10.1109/ICCV.2017.154; Xie S., 2018, SNAS STOCHASTIC NEUR; Zhang Chris, 2019, ICLR; Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442; Zhang ZS, 2018, PROC CVPR IEEE, P5813, DOI 10.1109/CVPR.2018.00609; Zhao Q, 2019, AIP ADV, V9, DOI 10.1063/1.5090480; Zheng X., 2019, ICCV; Zhu YS, 2017, IEEE I CONF COMP VIS, P4146, DOI 10.1109/ICCV.2017.444; Zoph B, ARXIV161101578; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	87	0	0	6	33	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1087	1105		10.1007/s11263-020-01415-x	http://dx.doi.org/10.1007/s11263-020-01415-x		JAN 2021	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000605541100007
J	Chai, DF				Chai, Dengfeng			Rooted Spanning Superpixels (vol 128, pg 2692, 2020)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction								The author regrets the omission of the following additional references to theInternational Journal of Computer Visionarticle, "Rooted Spanning Superpixels".	[Chai, Dengfeng] Zhejiang Univ, Sch Earth Sci, Key Lab Geosci Big Data & Deep Resource Zhejiang, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China	Zhejiang University	Chai, DF (corresponding author), Zhejiang Univ, Sch Earth Sci, Key Lab Geosci Big Data & Deep Resource Zhejiang, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China.	chaidf@zju.edu.cn						Alexandre EB, 2015, SIBGRAPI, P337, DOI 10.1109/SIBGRAPI.2015.20; Chai DF, 2020, INT J COMPUT VISION, V128, P2962, DOI 10.1007/s11263-020-01352-9; Vargas-Munoz JE, 2019, IEEE T IMAGE PROCESS, V28, P3477, DOI 10.1109/TIP.2019.2897941	3	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					803	803		10.1007/s11263-020-01391-2	http://dx.doi.org/10.1007/s11263-020-01391-2		OCT 2020	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC		Bronze			2022-12-18	WOS:000578395300001
J	Chen, HL; Zhuo, LA; Zhang, BC; Zheng, XW; Liu, JZ; Ji, RR; Doermann, D; Guo, GD				Chen, Hanlin; Zhuo, Li'an; Zhang, Baochang; Zheng, Xiawu; Liu, Jianzhuang; Ji, Rongrong; Doermann, David; Guo, Guodong			Binarized Neural Architecture Search for Efficient Object Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Neural architecture search (NAS); Binarized network; Object recognition; Edge computing	MULTIARMED BANDIT	Traditional neural architecture search (NAS) has a significant impact in computer vision by automatically designing network architectures for various tasks. In this paper, binarized neural architecture search (BNAS), with a search space of binarized convolutions, is introduced to produce extremely compressed models to reduce huge computational cost on embedded devices for edge computing. The BNAS calculation is more challenging than NAS due to the learning inefficiency caused by optimization requirements and the huge architecture space, and the performance loss when handling the wild data in various computing applications. To address these issues, we introduce operation space reduction and channel sampling into BNAS to significantly reduce the cost of searching. This is accomplished through a performance-based strategy that is robust to wild data, which is further used to abandon less potential operations. Furthermore, we introduce the upper confidence bound to solve 1-bit BNAS. Two optimization methods for binarized neural networks are used to validate the effectiveness of our BNAS. Extensive experiments demonstrate that the proposed BNAS achieves a comparable performance to NAS on both CIFAR and ImageNet databases. An accuracy of 96.53% vs. 97.22% is achieved on the CIFAR-10 dataset, but with a significantly compressed model, and a 40% faster search than the state-of-the-art PC-DARTS. On the wild face recognition task, our binarized models achieve a performance similar to their corresponding full-precision models.	[Chen, Hanlin; Zhuo, Li'an; Zhang, Baochang] Beihang Univ, Beijing, Peoples R China; [Zhang, Baochang] Shenzhen Acad Aerosp Technol, Shenzhen 100083, Peoples R China; [Zheng, Xiawu; Ji, Rongrong] Xiamen Univ, Xiamen, Fujian, Peoples R China; [Liu, Jianzhuang] Shenzhen Inst Adv Technol, Shenzhen, Peoples R China; [Doermann, David] SUNY Buffalo, Buffalo, NY USA; [Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing, Peoples R China; [Guo, Guodong] Natl Engn Lab Deep Learning Technol & Applicat, Beijing, Peoples R China	Beihang University; Xiamen University; Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; Baidu	Zhang, BC (corresponding author), Beihang Univ, Beijing, Peoples R China.; Zhang, BC (corresponding author), Shenzhen Acad Aerosp Technol, Shenzhen 100083, Peoples R China.	hlchen@buaa.edu.cn; lianzhuo@buaa.edu.cn; bczhang@buaa.edu.cn; zhengxiawu@buaa.edu.cn; jz.liu@siat.ac.cn; rrji@buaa.edu.cn; doermann@buffalo.edu; guoguodong01@baidu.com			National Natural Science Foundation of China [62076016, 61672079]; Shenzhen Science and Technology Program [KQTD2016112515134654]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Science and Technology Program	The work was supported in part by National Natural Science Foundation of China under Grants 62076016 and 61672079. This work is supported by Shenzhen Science and Technology Program KQTD2016112515134654. Baochang Zhang is also with Shenzhen Academy of Aerospace Technology, Shenzhen 100083, China. Hanlin Chen and Li'an Zhuo have the same contributions to the paper.	Alizadeh Milad, 2019, INT C LEARNING REPRE; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Brock Andrew, 2017, ARXIV170805344; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cai H., 2018, ARXIV PREPRINT ARXIV; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cai Han, 2018, ARXIV180602639; CHEN H, 2020, P AAAI; Chen JS, 2019, P IEEE, V107, P1655, DOI 10.1109/JPROC.2019.2921977; Chen X, 2019, IEEE I CONF COMP VIS, P1294, DOI 10.1109/ICCV.2019.00138; Courbariaux M, 2015, ADV NEUR IN, V28; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding R, 2019, P CVPR; Dong Y, 2014, COMPUTER SCI; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gu JX, 2019, IEEE I CONF COMP VIS, P4908, DOI 10.1109/ICCV.2019.00501; Gu Jiaxin, 2019, P AAAI; Ha David, 2016, ARXIV160909106; Han S, 2015, ADV NEUR IN, V28; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu J, 2018, P CVPR; Huang G.B., 2008, WORKSH FAC REAL LIF; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jamieson K, 2016, JMLR WORKSH CONF PRO, V51, P240; Juefei-Xu F, 2017, PROC CVPR IEEE, P4284, DOI 10.1109/CVPR.2017.456; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A., 2010, THE CIFAR 10 DATASET; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Leng C, 2018, AAAI CONF ARTIF INTE, P3466; Li E, 2020, IEEE T WIREL COMMUN, V19, P447, DOI 10.1109/TWC.2019.2946140; Li F., 2016, 1 INT WORKSH EFF MET; Li ZF, 2017, IEEE I CONF COMP VIS, P2603, DOI 10.1109/ICCV.2017.282; Lin X, 2017, P NIPS; Liu C., 2018, P EUR C COMP VIS ECC, P19, DOI DOI 10.1007/978-3-030-01246-5_2; Liu CL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P854; Liu H, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM - EDUCATIONAL RESEARCH AND EDUCATIONAL TECHNOLOGY, 2019, P3; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Madry Aleksander, 2017, ARXIV; Mao Y., 2017, ARXIV170101090; Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250; Neu G, 2015, ADV NEUR IN, V28; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pham H, 2018, PR MACH LEARN RES, V80; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sengupta S, 2016, IEEE WINT CONF APPL; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Simonyan K, 2013, P BMVC; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tang W, 2017, AAAI CONF ARTIF INTE, P2625; Tokic M, 2011, ANN C ART INT; WU SH, 2018, P ICLR; Xie Sirui, 2018, ARXIV181209926, P2; Xu Y, 2019, ICC 2019 2019 IEEE I, V5, P58, DOI 10.1080/23754931.2019.1619192; Ying C, 2019, PR MACH LEARN RES, V97; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zheng X, 2019, ARXIV190513543; Zhou S., 2016, ARXIV160606160; Zhu Chenzhuo, 2017, ICLR; Zhuang BH, 2018, PROC CVPR IEEE, P7920, DOI 10.1109/CVPR.2018.00826; Zoph B., 2016, ARXIV161101578; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	75	0	0	2	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					501	516		10.1007/s11263-020-01379-y	http://dx.doi.org/10.1007/s11263-020-01379-y		OCT 2020	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000574381500001
J	Abbasnejad, ME; Shi, J; van den Hengel, A; Liu, LQ				Abbasnejad, M. Ehsan; Shi, Javen; van den Hengel, Anton; Liu, Lingqiao			GADE: A Generative Adversarial Approach to Density Estimation and its Applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generative models; GANs; Flow-based generative models; Deep learning	INFERENCE; VARIABLES	Density estimation is a challenging unsupervised learning problem. Current maximum likelihood approaches for density estimation are either restrictive or incapable of producing high-quality samples. On the other hand, likelihood-free models such as generative adversarial networks, produce sharp samples without a density model. The lack of a density estimate limits the applications to which the sampled data can be put, however. We propose agenerative adversarial density estimator(GADE), a density estimation approach that bridges the gap between the two. Allowing for a prior on the parameters of the model, we extend our density estimator to a Bayesian model where we can leverage the predictive variance to measure our confidence in the likelihood. Our experiments on challenging applications such as visual dialog or autonomous driving where the density and the confidence in predictions are crucial shows the effectiveness of our approach.	[Abbasnejad, M. Ehsan; Shi, Javen; van den Hengel, Anton; Liu, Lingqiao] Univ Adelaide, Australian Inst Machine Learning, Adelaide, SA, Australia	University of Adelaide	Abbasnejad, ME (corresponding author), Univ Adelaide, Australian Inst Machine Learning, Adelaide, SA, Australia.	ehsan.abbasnejad@adelaide.edu.au; javen.shi@adelaide.edu.au; anton.vandenhengel@adelaide.edu.au; lingqiao.liu@adelaide.edu.au		van den Hengel, Anton/0000-0003-3027-8364; liu, lingqiao/0000-0003-3584-795X				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; [Anonymous], 2017, HIGH RESOLUTION IMAG; Arjovsky M., 2017, ARXIV170107875; Ben-Israel A, 1999, SIAM J MATRIX ANAL A, V21, P300, DOI 10.1137/S0895479895296896; Boutsidis C, 2017, LINEAR ALGEBRA APPL, V533, P95, DOI 10.1016/j.laa.2017.07.004; Burda Yuri, 2015, ARXIV150900519; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dinh Laurent, 2014, ARXIV14108516; Dumoulin Vincent, 2017, ICLR; Englert P, 2015, P INT S ROB RES; Espie E., 2000, TORCS OPEN RACING CA; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2014, INT C MACH LEARN ICM; Grover A, 2018, AAAI CONF ARTIF INTE, P3069; Gulrajani I, 2017, P NIPS 2017; Gutmann MU, 2018, STAT COMPUT, V28, P411, DOI 10.1007/s11222-017-9738-6; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Jang E., 2016, ARXIV; Karras T, 2017, ARXIV171010196; Kingma D.P, P 3 INT C LEARNING R; Kolesnikov A, 2017, PR MACH LEARN RES, V70; Konda VR, 2004, ANN APPL PROBAB, V14, P796, DOI 10.1214/105051604000000116; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Levine Sergey, 2013, ICML; Li Yuxi, 2017, ARXIV170107274; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lu Jiasen, 2017, ADV NEURAL INFORM PR, P314; Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289; Maddison Chris J., 2016, CORR; Martens J., 2010, P 27 INT C MACH LEAR, P735; Metz L., 2016, 161102163 ARXIV; Nowozin S, 2016, ADV NEUR IN, V29; Pfau David, 2016, ARXIV161001945; Price B, 2003, P 18 INT JOINT C ART, P712; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Saatchi Y., 2017, ADV NEURAL INFORM PR; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Salimans Tim, 2017, ARXIV170105517; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Shetty R, 2017, IEEE I CONF COMP VIS, P4155, DOI 10.1109/ICCV.2017.445; Stadie BC, 2017, INT C LEARN REPR; Theis Lucas, 2015, ARXIV151101844; van den Oord Aaron, 2016, ARXIV160605328; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Zhang H., 2017, ICCV; Zheng G., 2017, LIKELIHOOD ALMOST FR; Zheng G., 2018, ICML WORKSH THEOR FD; Ziebart B. D., 2008, AAAI, V8, P1433	59	0	0	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2731	2743		10.1007/s11263-020-01360-9	http://dx.doi.org/10.1007/s11263-020-01360-9		AUG 2020	13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY					2022-12-18	WOS:000560985300002
J	Li, HL; Wan, RJ; Wang, SQ; Kot, AC				Li, Haoliang; Wan, Renjie; Wang, Shiqi; Kot, Alex C.			Unsupervised Domain Adaptation in the Wild via Disentangling Representation Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						In the wild; Cross-domain; Recognition; Segmentation		Most recently proposed unsupervised domain adaptation algorithms attempt to learn domain invariant features by confusing a domain classifier through adversarial training. In this paper, we argue that this may not be an optimal solution in the real-world setting (a.k.a. in the wild) as the difference in terms of label information between domains has been largely ignored. As labeled instances are not available in the target domain in unsupervised domain adaptation tasks, it is difficult to explicitly capture the label difference between domains. To address this issue, we propose to learn a disentangled latent representation based on implicit autoencoders. In particular, a latent representation is disentangled into a global code and a local code. The global code is capturing category information via an encoder with a prior, and the local code is transferable across domains, which captures the "style" related information via an implicit decoder. Experimental results on digit recognition, object recognition and semantic segmentation demonstrate the effectiveness of our proposed method.	[Li, Haoliang; Wan, Renjie; Kot, Alex C.] Nanyang Technol Univ, Rapid Rich Object Search Lab, Singapore, Singapore; [Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; City University of Hong Kong	Li, HL (corresponding author), Nanyang Technol Univ, Rapid Rich Object Search Lab, Singapore, Singapore.	lihaoliang@ntu.edu.sg; rjwan@ntu.edu.sg; shiqwang@cityu.edu.hk; eackot@ntu.edu.sg	Wan, Patrick/AAL-2841-2021	Li, Haoliang/0000-0002-8723-8112	Wallenberg-NTU Presidential Postdoctoral Fellowship; NTU-PKU Joint Research Institute; Science and Technology Foundation of Guangzhou Huangpu Development District [201902010028]; Ng Teng Fong Charitable Foundation	Wallenberg-NTU Presidential Postdoctoral Fellowship; NTU-PKU Joint Research Institute; Science and Technology Foundation of Guangzhou Huangpu Development District; Ng Teng Fong Charitable Foundation	The research work was done at the Rapid-Rich Object Search (ROSE) Lab, Nanyang Technological University. This research is supported in part by the Wallenberg-NTU Presidential Postdoctoral Fellowship, the NTU-PKU Joint Research Institute, a collaboration between the Nanyang Technological University and Peking University that is sponsored by a donation from the Ng Teng Fong Charitable Foundation, and the Science and Technology Foundation of Guangzhou Huangpu Development District under Grant 201902010028.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Benford Frank, 1938, P AM PHILOS SOC, V78, P4, DOI DOI 10.2307/984802; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; French Geoffrey, 2017, ARXIV170605208; Ganin Y., 2016, JMLR, V17, P2096; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Haeusser Philip, 2017, CVPR; Hendrycks D., 2019, ICLR, P1; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615; Huang Jiayuan, 2006, NEURIPS; Huang X., 2018, ARXIV180404732; Hull J. J., 1994, PAMI; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Levinkov E., 2013, CVPR; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu Ming-Yu, 2017, NIPS; Liu Y, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278091; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2016, ADV NEUR IN, V29; Makhzani A., 2017, NEURIPS; Makhzani A., 2018, ARXIV180509804; Makhzani A., 2015, ARXIV151105644; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Pan Sinno Jialin, 2011, TNN; Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352; Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845; SAITO K, 2018, CVPR; Sankaranarayanan S, 2017, ARXIV171106969; Scholkopf Bernhard, 2012, ICML; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Sinno J.P., 2009, IEEE T KNOWL DATA EN, V22, P1345, DOI [10.1109/TKDE.2009.191, DOI 10.1109/TKDE.2009.191]; Taigman Y., 2016, ARXIV161102200; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Yeh Y.-R., 2015, ICCV; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang Yonghui, 2017, ICCV; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhe Cao, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5587, DOI 10.1109/CVPR.2019.00574; Zhu Jun-Yan, 2017, ICCV	60	0	0	2	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					267	283		10.1007/s11263-020-01364-5	http://dx.doi.org/10.1007/s11263-020-01364-5		AUG 2020	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF					2022-12-18	WOS:000558602400001
J	Yu, A; Grauman, K				Yu, Aron; Grauman, Kristen			Densifying Supervision for Fine-Grained Visual Comparisons	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Fine-grained; Ranking; Image generation; Relative attributes		Detecting subtle differences in visual attributes requires inferring which of two images exhibits a property more, e.g., which face is smiling slightly more, or which shoe is slightly more sporty. While valuable for applications ranging from biometrics to online shopping, fine-grained attributes are challenging to learn. Unlike traditional recognition tasks, the supervision is inherently comparative. Thus, the space of all possible training comparisons is vast, and learning algorithms face asparsity of supervisionproblem: it is difficult to curate adequate subtly different image pairs for each attribute of interest. We propose to overcome this problem bydensifyingthe space of training images with attribute-conditioned image generation. The main idea is to create synthetic but realistic training images exhibiting slight modifications of attribute(s), obtain their comparative labels from human annotators, and use the labeled image pairs to augment real image pairs when training ranking functions for the attributes. We introduce two variants of our idea. The first passively synthesizes training images by "jittering" individual attributes in real training images. Building on this idea, our second model actively synthesizes training image pairs that would confuse the current attribute model, training both the attribute ranking functions and a generation controller simultaneously in an adversarial manner. For both models, we employ a conditional Variational Autoencoder (CVAE) to perform image synthesis. We demonstrate the effectiveness of bootstrapping imperfect image generators to counteract supervision sparsity in learning-to-rank models. Our approach yields state-of-the-art performance for challenging datasets from two distinct domains.	[Yu, Aron; Grauman, Kristen] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Yu, A (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	aron.yu@utexas.edu; grauman@cs.utexas.edu		Yu, Aron/0000-0001-5920-7610	ONR PECASE [N00014-15-1-2291]; NSF [IIS-1514118]	ONR PECASE(Office of Naval Research); NSF(National Science Foundation (NSF))	We thank Bo Xiong, Qiang Liu, and Xinchen Yan for helpful discussions. We also thank the anonymous reviewers for their valuable comments. The University of Texas at Austin is supported in part by ONR PECASE N00014-15-1-2291 and NSF IIS-1514118.	Alabdulmohsin Ibrahim M, 2015, AAAI; Altwaijry H., 2012, WINT C APPL COMP VIS; Angluin D., 1988, MACHINE LEARNING; [Anonymous], 2017, P IEEE INT C COMP VI; [Anonymous], 2018, CVPR; Baum Eric, 1992, IJCNN; Biswas A., 2013, P IEEE C COMP VIS PA; BRANSON S., 2010, P EUR C COMP VIS ECC; Burges C., 2015, P INT C MACH LEARN I; Cai J., 2015, IEEE T IMAGE PROCESS; Cao C, 2014, IEEE INT CON MULTI; Changpinyo S., 2017, P IEEE INT C COMP VI; Chaudhuri S., 2013, ACM S US INT SOFTW T; Chen K., 2013, P IEEE C COMP VIS PA; Chen L., 2014, P IEEE C COMP VIS PA; Choi Y., 2018, P IEEE C COMP VIS PA; Datta A., 2011, FACE GESTURE; Deng J., 2009, 2009 IEEE C COMP VIS, P248, DOI [DOI 10.1109/CVPR.2009.5206848, 10.1109/CVPR.2009.5206848]; Dixit M.N., 2017, P IEEE C COMP VIS PA; Dosovitskiy A., 2015, P IEEE C COMP VIS PA; Dosovitskiy A., 2014, ADV NEURAL INFORM PR; Fan Q., 2013, P IEEE INT C COMP VI; Farhadi A., 2009, P IEEE C COMP VIS PA; Farrell R., 2011, INT C COMP VIS ICCV; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Freedman D., 2010, DISCOVER; Freytag A., 2014, ECCV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2015, P INT C MACH LEARN I; Hariharan S, 2017, PROCEEDINGS OF 2017 IEEE INTERNATIONAL CONFERENCE ON CIRCUIT ,POWER AND COMPUTING TECHNOLOGIES (ICCPCT); Hauberg S., 2017, INT C ART INT STAT; He K., 2017, P IEEE INT C COMP VI, P2961; Huang Gary B., 2007, 0749 U MASS; Huang Xun, 2018, ECCV; Huijser M. W., 2017, P IEEE INT C COMP VI; Isola P., 2017, CVPR; Jaderberg M., 2014, NIPS 14 DEEP LEARN W; Jaderberg Max, 2015, ADV NEURAL INFORM PR; Joachims T., 2002, KNOWLEDGE DISCOVERY; Kalayeh M. M., 2017, P IEEE C COMP VIS PA; Kemelmacher-Shlizerman I., 2014, P IEEE C COMP VIS PA; Khoreva A., 2017, TECHNICAL REPORT; Khosla A., 2013, INT C COMP VIS ICCV; Kingma D., P INT C LEARN REPR I, DOI DOI 10.1145/1830483.1830503; Kovashka A., 2013, INT C COMP VIS ICCV; Kovashka A., 2012, P IEEE C COMP VIS PA; Kovashka A, 2015, INT J COMPUT VISION, V115, P185, DOI 10.1007/s11263-015-0814-0; Kulkarni T. D., 2015, ADV NEURAL INFORM PR; Kumar Neeraj, 2008, 10 EUR C COMP VIS EC; Kwitt R., 2016, P IEEE C COMP VIS PA; Laffont P.Y., 2014, SIGGRAPH; Lampert C. H., 2009, C COMP VIS PATT REC; Lample G., 2017, ADV NEURAL INFORM PR, P5963; Lee H, 2018, IEEE INT CONF COMM; Li M., 2016, TECHNICAL REPORT; Li S., 2012, AS C COMP VIS ACCV; Liang L., 2014, P IEEE C COMP VIS PA; Lu Y., 2018, P EUR C COMP VIS ECC; Maji S., 2012, 2 INT WORKSH PARTS A; Maji S., 2013, TECHNICAL REPORT, P6; Matthews T., 2013, P IEEE C COMP VIS PA; Meng Zihang, 2018, P EUR C COMP VIS ECC; Miller E., 2000, P IEEE C COMP VIS PA; Moosavi-Dezfooli S., 2016, P IEEE C COMP VIS PA; Nguyen A., 2015, P IEEE C COMP VIS PA; ODonovan P., 2014, SIGGRAPH; Pandey G., 2016, TECHNICAL REPORT; Parikh D., 2011, P IEEE C COMP VIS PA; Park D., 2015, CHALEARN WORKSH CVPR; Paulin M., 2014, P IEEE C COMP VIS PA; Peng X., 2015, P IEEE INT C COMP VI; Pishchulin L, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.5; Qian B., 2013, IJCAI INT JOINT C AR; Radford A., 2016, ICLR; Reid D., 2014, IEEE T PATTERN ANAL, V36; Reid D., 2013, ICB; Sadovnik A., 2013, INT C COMP VIS ICCV; Sandeep R., 2014, C COMP VIS PATT REC; Settles B., 2010, TECHNICAL REPORT; Shakhnarovich G., 2003, P IEEE INT C COMP VI; Shotton J., 2011, P IEEE C COMP VIS PA; Shrivastava A., 2017, P IEEE C COMP VIS PA; Shrivastava A., 2016, CVPR; Shrivastava A., 2012, P EUR C COMP VIS ECC; Siddiquie Behjat, 2011, CVPR; Simard P. Y., 2003, ICDAR; Singh K., 2016, P EUR C COMP VIS ECC; Souri Y., 2016, AS C COMP VIS ACCV; Su J.-C., 2017, P IEEE INT C COMP VI; Tong S., 1998, ICML; Upchurch Paul, 2017, CVPR; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Varol G., 2017, CVPR; Verma Vinay Kumar, 2018, P IEEE C COMP VIS PA; Vijayanarasimhan S., 2009, P IEEE C COMP VIS PA; Vijayanarasimhan S, 2014, INT J COMPUT VISION, V108, P97, DOI 10.1007/s11263-014-0721-9; Vincent P., 2008, P INT C MACH LEARN I; Xian Y., 2018, P IEEE C COMP VIS PA; Xiao F., 2015, P IEEE INT C COMP VI; Yan X., 2016, TECHNICAL REPORT; Yan X., 2016, P EUR C COMP VIS ECC; Yang D., 2017, TECHNICAL REPORT; Yang L., 2015, P IEEE C COMP VIS PA; Yang X., 2016, IEEE T MULTIMEDIA, V18; Yao T., 2017, P IEEE INT C COMP VI; Yu A., 2014, P IEEE C COMP VIS PA; Yu A., 2017, P IEEE INT C COMP VI; Yu A., 2019, P IEEE C COMP VIS PA; Yumer ME, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766908; Zhang Gang, 2018, ECCV; Zhang H., 2017, ICCV; Zhang Y., 2017, CVPR; Zhao L., 2011, HCOMP; Zhu J.-J., 2017, TECHNICAL REPORT; Zhu J.-Y., 2017, P IEEE C COMP VIS IC	115	0	0	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2704	2730		10.1007/s11263-020-01344-9	http://dx.doi.org/10.1007/s11263-020-01344-9		AUG 2020	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY					2022-12-18	WOS:000558423900001
J	Martins, P; Henriques, JF; Batista, J				Martins, Pedro; Henriques, Joao F.; Batista, Jorge			Gradient Shape Model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Facial landmark localization; Face alignment; Constrained local model; CLM	ACTIVE APPEARANCE MODELS	For years, the so-called Constrained Local Model (CLM) and its variants have been the gold standard in face alignment tasks. The CLM combines an ensemble of local feature detectors whose locations are regularized by a shape model. Fitting such a model typically consists of an exhaustive local search using the detectors and a global optimization that finds the CLM's parameters that jointly maximize all the responses. However, one major drawback of CLMs is the inefficiency of the local search, which relies on a large amount of expensive convolutions. This paper introduces the Gradient Shape Model (GSM), a novel approach that addresses this limitation. We are able to align a similar CLM model without the need for any convolutions at all. We also use true analytical gradient and Hessian matrices, which are easy to compute, instead of their approximations. Our formulation is very general, allowing an optional 3D shape term to be seamlessly included. Additionally, we expand the GSM formulation through a cascade regression framework. This revised technique allows a substantially reduction in the complexity/dimensionality of the data term, making it possible to compute a denser, more accurate, regression step per cascade level. Experiments in several standard datasets show that our proposed models perform faster than state-of-the-art CLMs and better than recent cascade regression approaches.	[Martins, Pedro; Batista, Jorge] Univ Coimbra, Inst Syst & Robot, Coimbra, Portugal; [Henriques, Joao F.] Univ Oxford, Visual Geometry Grp, Oxford, England; [Batista, Jorge] Univ Coimbra, Dept Elect & Comp Engn, Coimbra, Portugal	Universidade de Coimbra; University of Oxford; Universidade de Coimbra	Martins, P (corresponding author), Univ Coimbra, Inst Syst & Robot, Coimbra, Portugal.	pedromartins@isr.uc.pt; joao@robots.ox.ac.uk; batista@isr.uc.pt	Martins, Pedro/GWC-7702-2022; Batista, Jorge/A-4196-2011	Batista, Jorge/0000-0003-2387-5961; Martins, Pedro/0000-0001-8984-4506	Portuguese Science Foundation (FCT) [SFRH/BPD/90200/2012]	Portuguese Science Foundation (FCT)(Portuguese Foundation for Science and Technology)	This work was supported by the Portuguese Science Foundation (FCT) through the Grant SFRH/BPD/90200/2012 (P. Martins).	Akhter Ijaz, 2008, NEURAL INFORM PROCES; Alabort-i-Medina J, 2017, INT J COMPUT VISION, V121, P26, DOI 10.1007/s11263-016-0916-3; Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; Baker S, 2001, PROC CVPR IEEE, P1090; Baltruaitis T., 2013, IEEE INT C COMP VIS; Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602; Boddeti V. N., 2013, IEEE C COMP VIS PATT; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015; Cheng LX, 2013, IEEE INT C BIOINFORM; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Cootes T. F., 2004, TECHNICAL REPORT; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cootes TF, 2012, LECT NOTES COMPUT SC, V7578, P278, DOI 10.1007/978-3-642-33786-4_21; Cristinacce D., 2006, P BRIT MACH VIS C, V3, P929; Cristinacce D., 2007, P BMVC, P880; Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024; Cu L, 2008, LECT NOTES COMPUT SC, V5302, P413, DOI 10.1007/978-3-540-88682-2_32; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dantone M, 2012, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2012.6247976; Face, 2018, MEGV FAC API; Fan HQ, 2016, IMAGE VISION COMPUT, V47, P27, DOI 10.1016/j.imavis.2015.11.004; Fanelli G, 2013, IEEE INT CONF AUTOMA; Galoogahi HK, 2013, IEEE I CONF COMP VIS, P3072, DOI 10.1109/ICCV.2013.381; Henriques JF, 2013, IEEE I CONF COMP VIS, P2760, DOI 10.1109/ICCV.2013.343; Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129; HUANG Z, 2015, ARXIV151104901; Jacobs H. O, 2014, ARXIV14103493V3; Jourabloo A, 2015, IEEE I CONF COMP VIS, P3694, DOI 10.1109/ICCV.2015.421; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Lee D.Y, 2015, P IEEE C COMP VIS PA; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Martins P, 2016, IEEE T PATTERN ANAL, V38, P704, DOI 10.1109/TPAMI.2015.2462343; Martins P, 2014, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR.2014.232; Martins P, 2012, LECT NOTES COMPUT SC, V7574, P57, DOI 10.1007/978-3-642-33712-3_5; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Messer K., 1999, 2 INT C AUDIO VIDEO, P965; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Nordstrom M., 2004, TECHNICAL REPORT; Paquet U, 2009, PROC CVPR IEEE, P1193, DOI 10.1109/CVPRW.2009.5206751; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Sanchez-Lozano E, 2018, IEEE T PATTERN ANAL, V40, P2037, DOI 10.1109/TPAMI.2017.2745568; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Saragih JM, 2009, IEEE I CONF COMP VIS, P1034, DOI 10.1109/ICCV.2009.5459377; Silverman B. W, 1986, DENSITY ESTIMATION S, DOI 10.1201/9781315140919; Songsri-in K., 2018, IEEE C AUT FAC GEST; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453; Tzimiropoulos G., 2012, P AS C COMP VIS; Tzimiropoulos G, 2015, IEEE C COMP VIS PATT; Tzimiropoulos G, 2017, INT J COMPUT VISION, V122, P17, DOI 10.1007/s11263-016-0950-1; Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239; Valstar M, 2010, PROC CVPR IEEE, P2729, DOI 10.1109/CVPR.2010.5539996; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang Y, 2008, PROC CVPR IEEE, P3621; Xiao J, 2004, LECT NOTES COMPUT SC, V2034, P573; XIAO J, 2004, IEEE C COMP VIS PATT; Xiong X., 2014, ARXIV14050601; Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Zadeh A., 2017, IEEE COMP VIS PATT R; Zhang J., 2014, EUR C COMP VIS; Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	74	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2828	2848		10.1007/s11263-020-01341-y	http://dx.doi.org/10.1007/s11263-020-01341-y		JUN 2020	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ					2022-12-18	WOS:000539187100001
J	Hague, SM; Govindu, VM				Hague, Sk Mohammadul; Govindu, Venu Madhav			A Face Fairness Framework for 3D Meshes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face fairness; 3D meshes; Mesh-denoising; Mesh-normal fusion	MULTIVIEW; STEREO	In this paper, we present a face fairness framework for 3D meshes that preserves the regular shape of faces and is applicable to a variety of 3D mesh restoration tasks. Specifically, we present a number of desirable properties for any mesh restoration method and show that our framework satisfies them. We then apply our framework to two different tasks-mesh-denoising and mesh-refinement, and present comparative results for these two tasks showing improvement over other relevant methods in the literature.	[Hague, Sk Mohammadul] Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India; [Govindu, Venu Madhav] Indian Inst Sci, Bengaluru, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kharagpur; Indian Institute of Science (IISC) - Bangalore	Hague, SM (corresponding author), Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.	mohammadul@gmail.com; venug@iisc.ac.in						BABUSKA I, 1976, SIAM J NUMER ANAL, V13, P214, DOI 10.1137/0713021; Chatterjee A, 2015, PROC CVPR IEEE, P933, DOI 10.1109/CVPR.2015.7298695; Cheng X, 2014, COMPUT GRAPH-UK, V38, P150, DOI 10.1016/j.cag.2013.10.025; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576; Du D-Z, 1995, COMPUTING EUCLIDEAN; El Ouafdi AF, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P11, DOI 10.1109/SMI.2008.4547940; Fan HQ, 2010, IEEE T VIS COMPUT GR, V16, P312, DOI 10.1109/TVCG.2009.70; FIELD DA, 1988, COMMUN APPL NUMER M, V4, P709, DOI 10.1002/cnm.1630040603; Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368; Fried I, 1960, COMMUNICATIONS PURE, V13, P217, DOI [10.1002/cpa.3160130205, DOI 10.1002/CPA.3160130205]; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Haque S. M., 2015, 3 INT C 3D VIS; Haque S. M., 2017, IEEE INT C COMP VIS; Haque SM, 2014, PROC CVPR IEEE, P2283, DOI 10.1109/CVPR.2014.292; He L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461965; Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119; Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22; Ji ZP, 2005, INT C COMP AID DES C, P269; Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367; Kim K, 2016, LECT NOTES COMPUT SC, V9907, P750, DOI 10.1007/978-3-319-46487-9_46; Li L, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-00363-4; Liu LG, 2007, COMPUT AIDED DESIGN, V39, P772, DOI 10.1016/j.cad.2007.03.004; Lu XQ, 2017, OPT LASER ENG, V90, P186, DOI 10.1016/j.optlaseng.2016.09.003; Lu XQ, 2016, IEEE T VIS COMPUT GR, V22, P1181, DOI 10.1109/TVCG.2015.2500222; Max N., 1999, Journal of Graphics Tools, V4, P1, DOI 10.1080/10867651.1999.10487501; Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI [DOI 10.1007/978-3-662-05105-4_2, 10.1007/978-3-662-05105-4_2]; Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494; Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226; Ohtake Y, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P203; Or El R., 2015, IEEE C COMP VIS PATT; Sun XF, 2008, COMPUT AIDED GEOM D, V25, P437, DOI 10.1016/j.cagd.2007.12.008; Sun XF, 2007, IEEE T VIS COMPUT GR, V13, P925, DOI 10.1109/TVCG.2007.1065; Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473; TAUBIN G, 2001, RC2213 IBM; Thurmer G., 1998, Journal of Graphics Tools, V3, P43, DOI 10.1080/10867651.1998.10487487; Wang PS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980232; Wang RM, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2557449; Wei MQ, 2015, IEEE T VIS COMPUT GR, V21, P43, DOI 10.1109/TVCG.2014.2326872; Wu CL, 2011, PROC CVPR IEEE, P969, DOI 10.1109/CVPR.2011.5995388; Wu CL, 2011, IEEE T VIS COMPUT GR, V17, P1082, DOI [10.1109/TVCG.2010.224, 10.1109/TPDS.2010.224]; Zhang HY, 2015, IEEE T VIS COMPUT GR, V21, P873, DOI 10.1109/TVCG.2015.2398432; Zhang WY, 2015, COMPUT GRAPH FORUM, V34, P23, DOI 10.1111/cgf.12742; Zheng YY, 2011, IEEE T VIS COMPUT GR, V17, P1521, DOI 10.1109/TVCG.2010.264	44	0	0	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1565	1579		10.1007/s11263-019-01268-z	http://dx.doi.org/10.1007/s11263-019-01268-z			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN		Green Submitted			2022-12-18	WOS:000534910600001
J	Ramasinghe, S; Khan, S; Barnes, N; Gould, S				Ramasinghe, Sameera; Khan, Salman; Barnes, Nick; Gould, Stephen			Representation Learning on Unit Ball with 3D Roto-translational Equivariance	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Convolution neural networks; 3D moments; Volumetric convolution; Zernike polynomials; Deep learning	INVARIANT; DESCRIPTORS; RECOGNITION; HISTOGRAMS; ROTATION	Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces-such as a sphere (S2\or a unit ball (B3-entails unique challenges. In this work, we propose a novel 'volumetric convolution' operation that can effectively model and convolve arbitrary functions in B3 We develop a theoretical framework for volumetric convolution based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in B3 around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.	[Ramasinghe, Sameera; Khan, Salman; Barnes, Nick; Gould, Stephen] Australian Natl Univ, Canberra, ACT, Australia	Australian National University	Ramasinghe, S (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.	sameera.ramasinghe@anu.edu.au	Barnes, Nick/Y-2744-2018; Khan, Salman Hameed/M-4834-2016	Barnes, Nick/0000-0002-9343-9535; Khan, Salman Hameed/0000-0002-9502-1749; Ramasinghe, Sameera/0000-0002-3200-9291				Agathos A., 2009, P EUR WORKSH 3D OBJ, P29; Ankerst M, 1999, LECT NOTES COMPUT SC, V1651, P207; [Anonymous], 2015, DEEP CONVOLUTIONAL N; ARBTER K, 1990, IEEE T PATTERN ANAL, V12, P640, DOI 10.1109/34.56206; Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543; Boomsma W., 2017, NIPS; Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; Canterakis N, 1999, 11 SCAND C IM AN; Canterakis Nikolaos, 1996, MUST 18 DAGM S INF A, P339; Carriere M, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12692; Charles R., 2017, ADV NEURAL INFORM PR; Cohen Taco, 2018, ICLR; Cohen Taco, 2018, ARXIV181102017; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; El Mallahi M, 2018, INT J AUTOM COMPUT, V15, P277, DOI 10.1007/s11633-017-1071-1; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Esteves C., 2018, EUR C COMP VIS ECCV; Flusser J, 2003, IEEE T PATTERN ANAL, V25, P234, DOI 10.1109/TPAMI.2003.1177154; Fotenos AF, 2005, NEUROLOGY, V64, P1032, DOI 10.1212/01.WNL.0000154530.72969.11; Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224; FURUYA T, 2016, BMVC; Garcia-Garcia A, 2016, IEEE IJCNN, P1578, DOI 10.1109/IJCNN.2016.7727386; Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; Ilse M., 2018, INT C MACH LEARN, P2127; Janssen MHJ, 2018, J MATH IMAGING VIS, V60, P1427, DOI 10.1007/s10851-018-0806-0; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kanezaki Asako, 2016, ARXIV160306208; Khalil MI, 2001, IEEE T PATTERN ANAL, V23, P1152, DOI 10.1109/34.954605; Khan SH, 2018, IEEE WINT CONF APPL, P1312, DOI 10.1109/WACV.2018.00148; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Kondor R., 2018, ARXIV180609231 ARXIV180609231; Kondor R., 2018, ARXIV PREPRINT ARXIV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurtek S, 2010, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2010.5539778; Lavoue G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x; Li HB, 2011, APPL MATH COMPUT, V218, P260, DOI 10.1016/j.amc.2011.05.036; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Li Y., 2016, ADV NEURAL INFORM PR, P307; LIN CC, 1987, IEEE T PATTERN ANAL, V9, P686, DOI 10.1109/TPAMI.1987.4767963; Liu W., 2017, PROC 31 INT C NEURAL, P3950; Maron Haggai, 2018, ARXIV181209902; Masci J., 2015, P IEEE INT C COMP VI, P37; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Monti F., 2017, PROC CVPR IEEE, P5115, DOI DOI 10.1109/CVPR.2017.576; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Papadakis P., 2008, EUR WORKSH 3D OBJ RE, P8; Qi C. R., 2017, IEEE P COMPUT VIS PA, V1, P4, DOI DOI 10.1109/CVPR.2017.16; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Ramasinghe S., 2019, ARXIV190810209; Ramasinghe S., 2019, ARXIV190100616; Reininghaus J, 2015, PROC CVPR IEEE, P4741, DOI 10.1109/CVPR.2015.7299106; REISS TH, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL III, P493, DOI 10.1109/ICPR.1992.202032; Ronchi C, 1996, J COMPUT PHYS, V124, P93, DOI 10.1006/jcph.1996.0047; Sedaghat N, 2016, ARXIV PREPRINT ARXIV; Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802; Simonovsky M., 2017, P CVPR; Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Suk T, 1996, PATTERN RECOGN, V29, P361, DOI 10.1016/0031-3203(94)00094-8; Tabia H., 2013, EUR WORKSH 3D OBJ RE; Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533; Tatsuma A, 2009, VISUAL COMPUT, V25, P785, DOI 10.1007/s00371-008-0304-2; Thomas Nathaniel, 2018, ARXIV180208219; Tieng QM, 1995, PATTERN RECOGN LETT, V16, P1287, DOI 10.1016/0167-8655(95)00079-1; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Vranic DV, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P177, DOI 10.1109/ICME.2002.1035747; Wang Y., 2018, ARXIV180107829; Weiler M, 2018, ARXIV180702547; Worrall D. E., 2018, EUR C COMP VIS; Worrall DE, 2017, PROC CVPR IEEE, P7168, DOI 10.1109/CVPR.2017.758; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xie J, 2015, PROC CVPR IEEE, P1275, DOI 10.1109/CVPR.2015.7298732; Xuan Guo, 1993, Computer Analysis of Images and Patterns. 5th International Conference, CAIP '93 Proceedings, P518; Yang B, 2015, PATTERN RECOGN LETT, V54, P18, DOI 10.1016/j.patrec.2014.11.014	81	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1612	1634		10.1007/s11263-019-01278-x	http://dx.doi.org/10.1007/s11263-019-01278-x			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN		Green Submitted			2022-12-18	WOS:000534910600004
J	Martyushev, EV				Martyushev, E. V.			Necessary and Sufficient Polynomial Constraints on Compatible Triplets of Essential Matrices	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multiview geometry; Essential matrix; Compatible triplet; Polynomial constraints	MINIMAL PROBLEMS; RELATIVE POSE; MOTION	The essential matrix incorporates relative rotation and translation parameters of two calibrated cameras. The well-known algebraic characterization of essential matrices, i.e. necessary and sufficient conditions under which an arbitrary matrix (of rank two) becomes essential, consists of a single matrix equation of degree three. Based on this equation, a number of efficient algorithmic solutions to different relative pose estimation problems have been proposed in the last two decades. In three views, a possible way to describe the geometry of three calibrated cameras comes from considering compatible triplets of essential matrices. The compatibility is meant the correspondence of a triplet to a certain configuration of calibrated cameras. The main goal of this paper is to give an algebraic characterization of compatible triplets of essential matrices. Specifically, we propose necessary and sufficient polynomial constraints on a triplet of real rank-two essential matrices that ensure its compatibility. The constraints are given in the form of six cubic matrix equations, one quartic and one sextic scalar equations. An important advantage of the proposed constraints is their sufficiency even in the case of cameras with collinear centers. The applications of the constraints may include relative camera pose estimation in three and more views, averaging of essential matrices for incremental structure from motion, multiview camera auto-calibration, etc.	[Martyushev, E. V.] South Ural State Univ, 76 Lenin Ave, Chelyabinsk 454080, Russia	South Ural State University	Martyushev, EV (corresponding author), South Ural State Univ, 76 Lenin Ave, Chelyabinsk 454080, Russia.	martiushevev@susu.ru	Martyushev, Evgeniy/AAS-3379-2021	Martyushev, Evgeniy/0000-0002-6892-079X	Act 211 Government of the Russian Federation [02, A03.21.0011]	Act 211 Government of the Russian Federation	The work was supported by Act 211 Government of the Russian Federation, Contract No. 02.A03.21.0011.	Aholt C, 2014, MATH COMPUT, V83, P2553, DOI 10.1090/S0025-5718-2014-02842-1; Arie-Nachimson M, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P81, DOI 10.1109/3DIMPVT.2012.46; Cox D., 2007, IDEALS VARIETIES ALG; DEMAZURE M, 1988, 882 INRIA; FAUGERAS OD, 1990, INT J COMPUT VISION, V4, P225, DOI 10.1007/BF00054997; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Grayson D.R., 2002, MACAULAY2 SOFTWARE S; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P1310, DOI 10.1109/34.41368; Kasten Y, 2019, IEEE I CONF COMP VIS, P5894, DOI 10.1109/ICCV.2019.00599; Kasten Y, 2019, PROC CVPR IEEE, P3259, DOI 10.1109/CVPR.2019.00338; Kileel J, 2017, SIAM J APPL ALGEBR G, V1, P575, DOI 10.1137/16M1104482; Kukelova Z, 2007, IEEE I CONF COMP VIS, P2816; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Martyushev EV, 2017, J MATH IMAGING VIS, V58, P321, DOI 10.1007/s10851-017-0712-x; MAYBANK S, 1993, THEORY RECONSTRUCTIO; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; SENGUPTA S, 2017, P IEEE C COMP VIS PA, P4798; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; Stewenius H, 2008, IMAGE VISION COMPUT, V26, P871, DOI 10.1016/j.imavis.2007.10.003; Stewenius H, 2006, ISPRS J PHOTOGRAMM, V60, P284, DOI 10.1016/j.isprsjprs.2006.03.005; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P318, DOI 10.1109/34.120327	23	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2781	2793		10.1007/s11263-020-01330-1	http://dx.doi.org/10.1007/s11263-020-01330-1		APR 2020	13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ		Green Submitted			2022-12-18	WOS:000527484700001
J	Angelova, A; Carneiro, G; Sunderhauf, N; Leitner, J				Angelova, Anelia; Carneiro, Gustavo; Sunderhauf, Niko; Leitner, Jurgen			Special Issue on Deep Learning for Robotic Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Angelova, Anelia] Google, Robot, Mountain View, CA 94043 USA; [Carneiro, Gustavo] Univ Adelaide, Adelaide, SA, Australia; [Sunderhauf, Niko] Queensland Univ Technol, Brisbane, Qld, Australia; [Leitner, Jurgen] LYRO Robot Pty Ltd, Brisbane, Qld, Australia	Google Incorporated; University of Adelaide; Queensland University of Technology (QUT)	Angelova, A (corresponding author), Google, Robot, Mountain View, CA 94043 USA.	anelia@google.com; gustavo.carneiro@adelaide.edu.au; niko.suenderhauf@qut.edu.au; juxi@lyro.io	Leitner, Jürgen/AAV-6230-2021; Sünderhauf, Niko/O-2192-2017	Leitner, Jürgen/0000-0003-1319-5073; Sünderhauf, Niko/0000-0001-5286-3789; Carneiro, Gustavo/0000-0002-5571-6220					0	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1160	1161		10.1007/s11263-020-01324-z	http://dx.doi.org/10.1007/s11263-020-01324-z		MAR 2020	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Green Submitted, Bronze			2022-12-18	WOS:000522012600001
J	Brox, T; Fritz, M				Brox, Thomas; Fritz, Mario			Editor's Note	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Brox, Thomas] Univ Freiburg, Freiburg, Germany; [Fritz, Mario] Max Planck Inst Informat, Saarbrucken, Germany	University of Freiburg; Max Planck Society	Brox, T (corresponding author), Univ Freiburg, Freiburg, Germany.	brox@cs.uni-freiburg.de; mfritz@mpi-inf.mpg.de							0	0	0	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		969	969		10.1007/s11263-020-01318-x	http://dx.doi.org/10.1007/s11263-020-01318-x		MAR 2020	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Bronze			2022-12-18	WOS:000520662000003
J	Liu, HM; Wang, RP; Shan, SG; Chen, XL				Liu, Haomiao; Wang, Ruiping; Shan, Shiguang; Chen, Xilin			Learning Multifunctional Binary Codes for Personalized Image Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image retrieval; Multi-task learning; Hashing	DEEP; REPRESENTATION; NETWORK	Due to the highly complex semantic information of images, even with the same query image, the expected content-based image retrieval results could be very different and personalized in different scenarios. However, most existing hashing methods only preserve one single type of semantic similarity, making them incapable of addressing such realistic retrieval tasks. To deal with this problem, we propose a unified hashing framework to encode multiple types of information into the binary codes by exploiting convolutional networks (CNNs). Specifically, we assume that typical retrieval tasks are generally defined in two aspects, i.e. high-level semantics (e.g. object categories) and visual attributes (e.g. object shape and color). To this end, our Dual Purpose Hashing model is trained to jointly preserve two kinds of similarities characterizing the two aspects respectively. Moreover, since images with both category and attribute labels are scarce, our model is carefully designed to leverage the abundant partially labelled data as training inputs to alleviate the risk of overfitting. With such a framework, the binary codes of new-coming images can be readily obtained by quantizing the outputs of a specific CNN layer, and different retrieval tasks can be achieved by using the binary codes in different ways. Experiments on two large-scale datasets show that our method achieves comparable or even better performance than those state-of-the-art methods specifically designed for each individual retrieval task while being more compact than the compared methods.	[Liu, Haomiao; Wang, Ruiping; Shan, Shiguang; Chen, Xilin] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Liu, Haomiao; Wang, Ruiping; Shan, Shiguang; Chen, Xilin] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Liu, Haomiao] Huawei EI Innovat Lab, Beijing 100085, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Wang, RP (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Wang, RP (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.	haomiao.liu@vipl.ict.ac.cn; wangruiping@ict.ac.cn; sgshan@ict.ac.cn; xlchen@ict.ac.cn	; Chen, Xilin/I-4153-2014	Shan, Shiguang/0000-0002-8348-392X; Chen, Xilin/0000-0003-3024-4404	973 Program [2015CB351802]; Natural Science Foundation of China [61390511, 61772500]; CAS [QYZDJ-SSWJSC009]; Youth Innovation Promotion Association [2015085]	973 Program(National Basic Research Program of China); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CAS(Chinese Academy of Sciences); Youth Innovation Promotion Association	This work was done at the Institute of Computing Technology, Chinese Academy of Sciences, where Haomiao Liu pursued the PhD degree. This work is partially supported by 973 Program under contract No. 2015CB351802, Natural Science Foundation of China under contracts Nos. 61390511, 61772500, CAS Frontier Science Key Research Project No. QYZDJ-SSWJSC009, and Youth Innovation Promotion Association No. 2015085.	Al-Halah Z., 2018, ARXIV181200202; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Cakir F., 2018, P EUROPEAN C COMPUTE, P332; Cao JJ, 2018, PROC CVPR IEEE, P4290, DOI 10.1109/CVPR.2018.00451; Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134; Cao Y, 2018, PROC CVPR IEEE, P1287, DOI 10.1109/CVPR.2018.00140; Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921; Escorcia V, 2015, PROC CVPR IEEE, P1256, DOI 10.1109/CVPR.2015.7298730; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493; HUANG C, 2016, PROC CVPR IEEE, P5175, DOI DOI 10.1109/CVPR.2016.559; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894; Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348; Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579; Kovashka A, 2013, IEEE I CONF COMP VIS, P3432, DOI 10.1109/ICCV.2013.426; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Kumar N, 2008, LECT NOTES COMPUT SC, V5305, P340, DOI 10.1007/978-3-540-88693-8_25; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; Li Y, 2015, IEEE I CONF COMP VIS, P3819, DOI 10.1109/ICCV.2015.435; Liu HM, 2017, PROC CVPR IEEE, P6259, DOI 10.1109/CVPR.2017.663; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu XL, 2015, IEEE I CONF COMP VIS, P1107, DOI 10.1109/ICCV.2015.132; Liu XL, 2014, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2014.275; Liu XL, 2013, PROC CVPR IEEE, P1570, DOI 10.1109/CVPR.2013.206; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Long Y, 2018, AAAI CONF ARTIF INTE, P7210; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Parikh D, 2011, PROC CVPR IEEE, P1681, DOI 10.1109/CVPR.2011.5995451; Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z; Rastegari M, 2013, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2013.425; Rastegari M, 2012, LECT NOTES COMPUT SC, V7577, P876, DOI 10.1007/978-3-642-33783-3_63; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sadovnik A, 2013, IEEE I CONF COMP VIS, P2160, DOI 10.1109/ICCV.2013.268; Scheirer WJ, 2012, PROC CVPR IEEE, P2933, DOI 10.1109/CVPR.2012.6248021; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29; Siddiquie B, 2011, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2011.5995329; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tao R, 2015, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2015.7298613; Turakhia N, 2013, IEEE I CONF COMP VIS, P1225, DOI 10.1109/ICCV.2013.155; Veit A, 2017, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2017.193; Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48; Wang Limin, 2015, ARXIV150702159; Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yang EK, 2017, AAAI CONF ARTIF INTE, P1618; Yang H, 2015, PHOTONICS, V2, P279, DOI 10.3390/photonics2010279; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yu FX, 2012, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2012.6248023; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang X, 2012, IEEE T MULTIMEDIA, V14, P995, DOI 10.1109/TMM.2012.2186121; Zhang ZM, 2016, PROC CVPR IEEE, P1487, DOI 10.1109/CVPR.2016.165; Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763; Zhong Y., 2016, ARXIV160201827; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	67	0	0	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2223	2242		10.1007/s11263-020-01315-0	http://dx.doi.org/10.1007/s11263-020-01315-0		MAR 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG					2022-12-18	WOS:000520662000005
J	Alameda-Pineda, X; Ricci, E; Salah, AA; Sebe, N; Yan, SC				Alameda-Pineda, Xavier; Ricci, Elisa; Salah, Albert Ali; Sebe, Nicu; Yan, Shuicheng			Special Issue on Generating Realistic Visual Data of Human Behavior	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Alameda-Pineda, Xavier] Inria Grenoble Rhone Alpes, F-38330 Montbonnot St Martin, Saint Martin, France; [Ricci, Elisa; Sebe, Nicu] Univ Trento, FBK, Via Sommar 9, I-38123 Trento, Italy; [Ricci, Elisa] Univ Trento, DISI, Via Sommar 9, I-38123 Trento, Italy; [Salah, Albert Ali] Univ Utrecht, Princetonpl 5, NL-3584 CC Utrecht, Netherlands; [Yan, Shuicheng] Natl Univ Singapore, Vis & Machine Learning Lab, Singapore 117583, Singapore	Fondazione Bruno Kessler; University of Trento; University of Trento; Utrecht University; National University of Singapore	Salah, AA (corresponding author), Univ Utrecht, Princetonpl 5, NL-3584 CC Utrecht, Netherlands.	a.a.salah@uu.nl	Yan, Shuicheng/HCI-1431-2022; Salah, Albert Ali/ABH-5561-2020	Salah, Albert Ali/0000-0001-6342-428X; Ricci, Elisa/0000-0002-0228-1147					0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1376	1377		10.1007/s11263-020-01319-w	http://dx.doi.org/10.1007/s11263-020-01319-w		MAR 2020	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL9ON		Bronze			2022-12-18	WOS:000519623300001
J	Wu, BY; Shen, L; Zhang, T; Ghanem, B				Wu, Baoyuan; Shen, Li; Zhang, Tong; Ghanem, Bernard			MAP Inference Via l(2)-Sphere Linear Program Reformulation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						MAP inference; Continuous reformulation; Non-convex optimization	DUAL DECOMPOSITION; OPTIMIZATION; CONVERGENCE	Maximum a posteriori (MAP) inference is an important task for graphical models. Due to complex dependencies among variables in realistic models, finding an exact solution for MAP inference is often intractable. Thus, many approximation methods have been developed, among which the linear programming (LP) relaxation based methods show promising performance. However, one major drawback of LP relaxation is that it is possible to give fractional solutions. Instead of presenting a tighter relaxation, in this work we propose a continuous but equivalent reformulation of the original MAP inference problem, called LS-LP. We add the 2-sphere constraint onto the original LP relaxation, leading to an intersected space with the local marginal polytope that is equivalent to the space of all valid integer label configurations. Thus, LS-LP is equivalent to the original MAP inference problem. We propose a perturbed alternating direction method of multipliers (ADMM) algorithm to optimize the LS-LP problem, by adding a sufficiently small perturbation onto the objective function and constraints. We prove that the perturbed ADMM algorithm globally converges to the -Karush-Kuhn-Tucker ( -KKT) point of the LS-LP problem. The convergence rate will also be analyzed. Experiments on several benchmark datasets from Probabilistic Inference Challenge (PIC 2011) and OpenGM 2 show competitive performance of our proposed method against state-of-the-art MAP inference methods.	[Wu, Baoyuan; Shen, Li] Tencent AI Lab, Shenzhen 518000, Peoples R China; [Zhang, Tong] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China; [Ghanem, Bernard] King Abdullah Univ Sci & Technol, Thuwal 23955, Saudi Arabia	Tencent; Hong Kong University of Science & Technology; King Abdullah University of Science & Technology	Shen, L (corresponding author), Tencent AI Lab, Shenzhen 518000, Peoples R China.	wubaoyuan1987@gmail.com; mathshenli@gmail.com; tongzhang@tongzhang-ml.org; bernard.ghanem@kaust.edu.sa	Zhang, Tong/HGC-1090-2022; Ghanem, Bernard/J-7605-2017	Shen, Li/0000-0001-5659-3464; Ghanem, Bernard/0000-0002-5534-587X; Wu, Baoyuan/0000-0003-2183-5990	Tencent AI Lab; King Abdullah University of Science and Technology (KAUST); King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR); Hong Kong University of Science and Technology (HKUST)	Tencent AI Lab; King Abdullah University of Science and Technology (KAUST)(King Abdullah University of Science & Technology); King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR); Hong Kong University of Science and Technology (HKUST)	Baoyuan Wu was partially supported by Tencent AI Lab and King Abdullah University of Science and Technology (KAUST). Li Shen was supported by Tencent AI Lab. Bernard Ghanem was supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR). Tong Zhang was supported by the Hong Kong University of Science and Technology (HKUST). Li Shen is the corresponding author.	[Anonymous], TRACES EMERGENCE NON; Attouch H, 2009, MATH PROGRAM, V116, P5, DOI 10.1007/s10107-007-0133-5; Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; BESAG J, 1986, J R STAT SOC B, V48, P259; Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641; Bolte J, 2007, SIAM J OPTIMIZ, V18, P556, DOI 10.1137/060670080; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Elidan Gal, 2012, PASCAL 2011 PROBABIL; Fu Q, 2013, CHINESE PHYS B, V22, DOI 10.1088/1674-1056/22/7/077309; Globerson Amir, 2008, ADV NEURAL INFORM PR, P553; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; Johnson J. K., 2007, ARXIV07100013; Jojic V., 2010, P 27 INT C MACH LEAR, P503; Kappes JH, 2012, PROC CVPR IEEE, P1688, DOI 10.1109/CVPR.2012.6247863; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Karush W., 1939, THESIS; KELLEY JE, 1960, J SOC IND APPL MATH, V8, P703, DOI 10.1137/0108053; Koller D., 2009, PROBABILISTIC GRAPHI; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Komodakis N, 2007, IEEE I CONF COMP VIS, P488; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; LAND AH, 1960, ECONOMETRICA, V28, P497, DOI 10.2307/1910129; Laurent M., 2002, SEMIDEFINITE PROGRAM; Li GY, 2015, SIAM J OPTIMIZ, V25, P2434, DOI 10.1137/140998135; Lojasiewicz S., 1963, EQUATIONS DERIVEES P, P87, DOI DOI 10.1006/JDEQ.1997.3393; Martins A. F. T., 2011, ICML; Martins AFT, 2015, J MACH LEARN RES, V16, P495; Meshi O, 2011, LECT NOTES ARTIF INT, V6912, P470, DOI 10.1007/978-3-642-23783-6_30; Meshi Ofer, 2015, ADV NEURAL INFORM PR, P298; Otten L., 2012, NIPS WORKSH DISCML; Otten L, 2012, AI COMMUN, V25, P211, DOI 10.3233/AIC-2012-0531; Savchynskyy B., 2012, ARXIV12104906; Schwing Alex, 2012, NIPS, P2384; Schwing AG, 2014, PR MACH LEARN RES, V32, P487; Sontag D., 2012, UAI; Sontag D. A., 2010, THESIS; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Wang ZY, 2017, FRONT CELL INFECT MI, V7, DOI 10.3389/fcimb.2017.00029; Wu BY, 2019, IEEE T PATTERN ANAL, V41, P1695, DOI 10.1109/TPAMI.2018.2845842; Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795	42	0	0	2	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2020	128	7					1913	1936		10.1007/s11263-020-01313-2	http://dx.doi.org/10.1007/s11263-020-01313-2		MAR 2020	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MC3NG		Green Submitted			2022-12-18	WOS:000518246900001
J	Gehrig, D; Rebecq, H; Gallego, G; Scaramuzza, D				Gehrig, Daniel; Rebecq, Henri; Gallego, Guillermo; Scaramuzza, Davide			EKLT: Asynchronous Photometric Feature Tracking Using Events and Frames (vol 128, pg 601, 2020)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction								The original version of this article was unfortunately omitted to publish the footnote "The best result per row is highlighted in bold" in Table 7. This has been corrected by publishing this erratum. The correct version of Table 7 with the caption has been given below:	[Gehrig, Daniel; Rebecq, Henri; Gallego, Guillermo; Scaramuzza, Davide] Univ Zurich, Dept Informat, Robot & Percept Grp, Zurich, Switzerland; [Gehrig, Daniel; Rebecq, Henri; Gallego, Guillermo; Scaramuzza, Davide] Univ Zurich, Dept Neuroinformat, Zurich, Switzerland; [Gehrig, Daniel; Rebecq, Henri; Gallego, Guillermo; Scaramuzza, Davide] Swiss Fed Inst Technol, Zurich, Switzerland	University of Zurich; University of Zurich; Swiss Federal Institutes of Technology Domain; ETH Zurich	Gehrig, D (corresponding author), Univ Zurich, Dept Informat, Robot & Percept Grp, Zurich, Switzerland.; Gehrig, D (corresponding author), Univ Zurich, Dept Neuroinformat, Zurich, Switzerland.; Gehrig, D (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	daniel.gehrig18@gmail.com	Gallego, Guillermo/I-7131-2012	Gallego, Guillermo/0000-0002-2672-9241; Rebecq, Henri/0000-0002-6577-9735				Gehrig D, 2020, INT J COMPUT VISION, V128, P601, DOI 10.1007/s11263-019-01209-w	1	0	0	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		619	619		10.1007/s11263-019-01233-w	http://dx.doi.org/10.1007/s11263-019-01233-w			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KU1MV		Bronze			2022-12-18	WOS:000519475600005
J	Jawahar, CV; Li, HD; Mori, G; Schindler, K				Jawahar, C. V.; Li, Hongdong; Mori, Greg; Schindler, Konrad			Guest Editorial: Special Issue on ACCV 2018	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Jawahar, C. V.] IIIT Hyderabad, Hyderabad, India; [Li, Hongdong] Australian Natl Univ, Canberra, ACT, Australia; [Mori, Greg] Simon Fraser Univ, Vancouver, BC, Canada; [Schindler, Konrad] Swiss Fed Inst Technol, Zurich, Switzerland	International Institute of Information Technology Hyderabad; Australian National University; Simon Fraser University; Swiss Federal Institutes of Technology Domain; ETH Zurich	Li, HD (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.	jawahar@iiit.ac.in; hongdong.li@anu.edu.au; mori@cs.sfu.ca; schindler@ethz.ch		Jawahar, C. V./0000-0001-6767-7057					0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		909	909		10.1007/s11263-020-01296-0	http://dx.doi.org/10.1007/s11263-020-01296-0		FEB 2020	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Bronze			2022-12-18	WOS:000515898600001
J	Berman, M; Blaschko, MB				Berman, Maxim; Blaschko, Matthew B.			Discriminative Training of Conditional Random Fields with Probably Submodular Constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structured prediction; Submodularity; Graphical models; Segmentation	SEGMENTATION	Problems of segmentation, denoising, registration and 3D reconstruction are often addressed with the graph cut algorithm. However, solving an unconstrained graph cut problem is NP-hard. For tractable optimization, pairwise potentials have to fulfill the submodularity inequality. In our learning paradigm, pairwise potentials are created as the dot product of a learned vector w with positive feature vectors. In order to constrain such a model to remain tractable, previous approaches have enforced the weight vector to be positive for pairwise potentials in which the labels differ, and set pairwise potentials to zero in the case that the label remains the same. Such constraints are sufficient to guarantee that the resulting pairwise potentials satisfy the submodularity inequality. However, we show that such an approach unnecessarily restricts the capacity of the learned models. Guaranteeing submodularity for all possible inputs, no matter how improbable, reduces inference error to effectively zero, but increases model error. In contrast, we relax the requirement of guaranteed submodularity to solutions that are probably approximately submodular. We show that the conceptually simple strategy of enforcing submodularity on the training examples guarantees with low sample complexity that test images will also yield submodular pairwise potentials. Results are presented in the binary and muticlass settings, showing substantial improvement from the resulting increased model capacity.	[Berman, Maxim; Blaschko, Matthew B.] Katholieke Univ Leuven, Dept Elect Engn, Ctr Proc Speech & Images, B-3001 Leuven, Belgium	KU Leuven	Berman, M (corresponding author), Katholieke Univ Leuven, Dept Elect Engn, Ctr Proc Speech & Images, B-3001 Leuven, Belgium.	maxim.berman@esat.kuleuven.be; matthew.blaschko@esat.kuleuven.be		Blaschko, Matthew/0000-0002-2640-181X	Internal Funds KU Leuven; Amazon Research Award; Research Foundation - Flanders (FWO) [G0A2716N]; Flemish Government under the Onderzoeksprogramma Artificiele Intelligentie (AI) Vlaanderen programme	Internal Funds KU Leuven; Amazon Research Award; Research Foundation - Flanders (FWO)(FWO); Flemish Government under the Onderzoeksprogramma Artificiele Intelligentie (AI) Vlaanderen programme	This work is partially funded by Internal Funds KU Leuven and an Amazon Research Award. This work made use of a hardware donation from the Facebook GPU Partnership program, and an NVIDIA GPU donation. We acknowledge support from the Research Foundation - Flanders (FWO) through Project Number G0A2716N. This research received funding from the Flemish Government under the Onderzoeksprogramma Artificiele Intelligentie (AI) Vlaanderen programme. The authors thank Wojciech Zaremba for making this work possible.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Anguelov D, 2005, PROC CVPR IEEE, P169; BAKR GH, 2007, PREDICTING STRUCTURE; BARAHONA F, 1982, J PHYS A-MATH GEN, V15, P3241, DOI 10.1088/0305-4470/15/10/028; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Billingsley P., 1995, WILEY SERIES PROBABI, V3rd; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; CASSER V, 2018, ARXIV181206024 CORR; CHANDRA S, 2016, EUR C COMP VIS; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LC, 2015, PR MACH LEARN RES, V37, P1785; Cheng HC, 2017, IEEE IMAGE PROC, P590; Elisseeff A, 2002, ADV NEUR IN, V14, P681; FELZENSZWALB PF, 2010, PROC CVPR IEEE, P2241, DOI DOI 10.1109/CVPR.2010.5539906; Finley T., 2008, INT C MACHINE LEARNI, P304, DOI DOI 10.1145/1390156.1390195; Gelasca ED, 2008, IEEE IMAGE PROC, P1816, DOI 10.1109/ICIP.2008.4712130; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GJORGJEVIKJ D, 2011, 2011 IEEE INT WORKSH, P1; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; HARPELED S, 2011, ARXIV11115340 CORR; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Leibe B., 2004, EUROPEAN C COMPUTER, P17; Lucchi A, 2015, IEEE T MED IMAGING, V34, P1096, DOI 10.1109/TMI.2014.2376274; Lucchi A, 2012, IEEE T MED IMAGING, V31, P474, DOI 10.1109/TMI.2011.2171705; Magnus J. R., 1995, MATRIX DIFFERENTIAL; Metzen JH, 2017, IEEE I CONF COMP VIS, P2774, DOI 10.1109/ICCV.2017.300; Muller AC, 2014, J MACH LEARN RES, V15, P2055; Nowozin S, 2010, LECT NOTES COMPUT SC, V6316, P98, DOI 10.1007/978-3-642-15567-3_8; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rother C, 2005, PROC CVPR IEEE, P589; Rother C, 2007, PROC CVPR IEEE, P1784; Szummer M, 2008, LECT NOTES COMPUT SC, V5303, P582, DOI 10.1007/978-3-540-88688-4_43; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Zaremba W, 2016, IEEE WINT CONF APPL	40	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1722	1735		10.1007/s11263-019-01277-y	http://dx.doi.org/10.1007/s11263-019-01277-y		JAN 2020	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN		Green Submitted, Green Accepted			2022-12-18	WOS:000508705900001
J	Wu, A; Piergiovanni, AJ; Ryoo, MS				Wu, A.; Piergiovanni, A. J.; Ryoo, M. S.			Model-Based Robot Imitation with Future Image Similarity (vol 241, pg 781, 2019)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Wu, A.; Piergiovanni, A. J.; Ryoo, M. S.] Indiana Univ, Bloomington, IN 47405 USA; [Ryoo, M. S.] SUNY Stony Brook, Stony Brook, NY 11794 USA	Indiana University System; Indiana University Bloomington; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Wu, A (corresponding author), Indiana Univ, Bloomington, IN 47405 USA.	alanwu@iu.edu; ajpiergi@indiana.edu; mryoo@indiana.edu			Institute of Information & Communications Technology Planning & Evaluation (IITP) - Korean government (MSIT) [2018-000205]	Institute of Information & Communications Technology Planning & Evaluation (IITP) - Korean government (MSIT)	This work was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (No. 2018-000205, Development of Core Technology of Robot Task-Intelligence for Improvement of Labor Condition).		1	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1375	1375		10.1007/s11263-019-01272-3	http://dx.doi.org/10.1007/s11263-019-01272-3		DEC 2019	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Bronze			2022-12-18	WOS:000505388800002
J	Sun, R; Lampert, CH				Sun, Remy; Lampert, Christoph H.			KS(conf): A Light-Weight Test if a Multiclass Classifier Operates Outside of Its Specifications (10.1007/s11263-019-01232-x, 2019)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Sun, Remy] ENS Rennes, Bruz, France; [Lampert, Christoph H.] IST Austria, Klosterneuburg, Austria	Ecole Normale Superieure de Rennes (ENS Rennes); Institute of Science & Technology - Austria	Lampert, CH (corresponding author), IST Austria, Klosterneuburg, Austria.	remy.sun@ens-rennes.fr; chl@ist.ac.at						Sun R, 2020, INT J COMPUT VISION, V128, P970, DOI 10.1007/s11263-019-01232-x	1	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		996	996		10.1007/s11263-019-01262-5	http://dx.doi.org/10.1007/s11263-019-01262-5		NOV 2019	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		hybrid			2022-12-18	WOS:000495287900001
J	Jung, J; Lee, JY; Kweon, IS				Jung, Jiyoung; Lee, Joon-Young; Kweon, In So			One-Day Outdoor Photometric Stereo Using Skylight Estimation (vol 127, pg 1126, 2019)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction								The original version of this article unfortunately contained mistakes in the figure captions.	[Jung, Jiyoung] Kyung Hee Univ, Dept Software Convergence, Yongin, South Korea; [Lee, Joon-Young] Adobe Res, San Jose, CA USA; [Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea	Kyung Hee University; Adobe Systems Inc.; Korea Advanced Institute of Science & Technology (KAIST)	Jung, J (corresponding author), Kyung Hee Univ, Dept Software Convergence, Yongin, South Korea.	jiyoung.jung@khu.ac.kr; jolee@adobe.com; iskweon77@kaist.ac.kr	Jung, Jiyoung/AAK-7269-2020	Jung, Jiyoung/0000-0001-9316-9750				Jung J, 2019, INT J COMPUT VISION, V127, P1126, DOI 10.1007/s11263-018-01145-1	1	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					1143	1143		10.1007/s11263-019-01167-3	http://dx.doi.org/10.1007/s11263-019-01167-3			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM		Bronze			2022-12-18	WOS:000474559000010
J	Kuniyoshi, F; Funatomi, T; Kubo, H; Sawada, Y; Kato, YO; Mukaigawa, Y				Kuniyoshi, Fusataka; Funatomi, Takuya; Kubo, Hiroyuki; Sawada, Yoshihide; Kato, Yumiko O.; Mukaigawa, Yasuhiro			Visibility Enhancement by Integrating Refocusing and Direct-Global Separation with Contact Imaging	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computational photography; Lensless imaging; Direct-global separation; Refocusing	WIDE-FIELD; ILLUMINATION; MICROSCOPY; PHASE	Contact imaging is a compact lensless microscopy technique that allows observation of living cells placed directly on a charge-coupled device sensor. However, captured images exhibit low visibility in general. This work is aimed at integrating computational refocusing and high-frequency illumination into contact imaging to enhance the visibility of the captured images. Refocusing contributes to the synthesis of shallow depth-of-field images from the light field captured by moving a point light source. The visibility may be enhanced via high-frequency illumination. Naive integration of both techniques requires several observations. Therefore, we propose an efficient integration from reduced observations by reformulating the computational refocusing applied in contact imaging. Furthermore, we developed a prototype system that demonstrates the effectiveness of the proposed method.	[Kuniyoshi, Fusataka; Funatomi, Takuya; Kubo, Hiroyuki; Mukaigawa, Yasuhiro] Nara Inst Sci & Technol, 8916-5 Takayama Cho, Nara 6300192, Japan; [Sawada, Yoshihide; Kato, Yumiko O.] Panasonic Corp, Adv Res Div, 3-4 Hikaridai,Seika Cho, Kyoto 6190237, Japan	Nara Institute of Science & Technology; Panasonic	Funatomi, T (corresponding author), Nara Inst Sci & Technol, 8916-5 Takayama Cho, Nara 6300192, Japan.	kuniyoshi.fusataka.jv6@is.naist.jp; funatomi@is.naist.jp; hkubo@is.naist.jp; sawada.yoshihide@jp.panasonic.com; kato.yumiko@jp.panasonic.com; mukaigawa@is.naist.jp	Funatomi, Takuya/K-5919-2018; Kubo, Hiroyuki/AAS-1487-2021; Sawada, Yoshihide/AID-1394-2022; Kuniyoshi, Fusataka/AAS-8312-2020	Funatomi, Takuya/0000-0001-5588-5932; Kubo, Hiroyuki/0000-0002-7061-7941; Sawada, Yoshihide/0000-0001-7267-8660; Kuniyoshi, Fusataka/0000-0001-5914-009X	Japan Society for the Promotion of Science (JSPS) KAKENHI [J26700013]; Japan Science and Technology Agency (JST) CREST [JPMJCR1764]	Japan Society for the Promotion of Science (JSPS) KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Japan Science and Technology Agency (JST) CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	This study was partially supported by Japan Society for the Promotion of Science (JSPS) KAKENHI Grant No. J26700013 and Japan Science and Technology Agency (JST) CREST Grant Number JPMJCR1764.	Achar S, 2013, IEEE I CONF COMP VIS, P1481, DOI 10.1109/ICCV.2013.187; Asif MS, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P663, DOI 10.1109/ICCVW.2015.89; Bimber O, 2017, P IEEE, V105, P960, DOI 10.1109/JPROC.2016.2627535; Bishara W, 2011, LAB CHIP, V11, P1276, DOI 10.1039/c0lc00684j; Bishara W, 2010, OPT EXPRESS, V18, P11181, DOI 10.1364/OE.18.011181; Cui XQ, 2008, P NATL ACAD SCI USA, V105, P10670, DOI 10.1073/pnas.0804612105; Daloglu MU, 2018, LIGHT-SCI APPL, V7, DOI 10.1038/lsa.2017.121; Daloglu MU, 2017, BIOL REPROD, V97, P182, DOI 10.1093/biolre/iox086; Fuchs C, 2008, COMPUT GRAPH FORUM, V27, P1245, DOI 10.1111/j.1467-8659.2008.01263.x; Greenbaum A, 2014, SCI TRANSL MED, V6, DOI 10.1126/scitranslmed.3009850; Guo KK, 2015, OPT EXPRESS, V23, P6171, DOI 10.1364/OE.23.006171; Guo KK, 2015, BIOMED OPT EXPRESS, V6, P574, DOI 10.1364/BOE.6.000574; Gupta M, 2012, INT J COMPUT VISION, V98, P146, DOI 10.1007/s11263-011-0500-9; Heng X, 2006, LAB CHIP, V6, P1274, DOI 10.1039/b604676b; Isikman SO, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0045044; Isikman SO, 2011, P NATL ACAD SCI USA, V108, P7296, DOI 10.1073/pnas.1015638108; Jensen H. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P311, DOI 10.1145/280814.280925; Jensen H.W., 2001, REALISTIC IMAGE SYNT, VVolume 364; Kesavan SV, 2014, SCI REP-UK, V4, DOI 10.1038/srep05942; Lee SA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0089712; Lee SB, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0022312; Levoy M, 2004, ACM T GRAPHIC, V23, P825, DOI 10.1145/1015706.1015806; Levoy M, 2006, ACM T GRAPHIC, V25, P924, DOI 10.1145/1141911.1141976; Lin X, 2015, BIOMED OPT EXPRESS, V6, P3179, DOI 10.1364/BOE.6.003179; Liu ZJ, 2014, J BIOMED OPT, V19, DOI 10.1117/1.JBO.19.10.106002; Luo W, 2015, LIGHT-SCI APPL, V4, DOI 10.1038/lsa.2015.34; Minzioni P, 2017, J OPTICS-UK, V19, DOI 10.1088/2040-8986/aa783b; Mudanyali O, 2010, LAB CHIP, V10, P1417, DOI 10.1039/c000453g; Nayar SK, 2006, ACM T GRAPHIC, V25, P935, DOI 10.1145/1141911.1141977; Ou XZ, 2013, OPT LETT, V38, P4845, DOI 10.1364/OL.38.004845; Ozcan A, 2016, ANNU REV BIOMED ENG, V18, P77, DOI 10.1146/annurev-bioeng-092515-010849; Saxena M, 2015, ADV OPT PHOTONICS, V7, P241, DOI 10.1364/AOP.7.000241; Seo S, 2009, LAB CHIP, V9, P777, DOI 10.1039/b813943a; Shimano Mihoko, 2017, Medical Image Computing and Computer-Assisted Intervention, MICCAI 2017. 20th International Conference. Proceedings: LNCS 10434, P12, DOI 10.1007/978-3-319-66185-8_2; Su TW, 2009, BIOTECHNOL BIOENG, V102, P856, DOI 10.1002/bit.22116; Tanaka K, 2013, IEEE INT CONF COMPUT; Tian L, 2014, OPT LETT, V39, P1326, DOI 10.1364/OL.39.001326; Waller L, 2010, OPT EXPRESS, V18, P22817, DOI 10.1364/OE.18.022817; Wang MJ, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13134-4; Wu YC, 2018, METHODS, V136, P4, DOI 10.1016/j.ymeth.2017.08.013; Zheng GA, 2013, NAT PHOTONICS, V7, P739, DOI [10.1038/NPHOTON.2013.187, 10.1038/nphoton.2013.187]; Zheng GA, 2011, OPT LETT, V36, P3987, DOI 10.1364/OL.36.003987; Zheng GA, 2010, LAB CHIP, V10, P3125, DOI [10.1039/c01c00213e, 10.1039/c0lc00213e]	43	0	0	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					1162	1174		10.1007/s11263-019-01173-5	http://dx.doi.org/10.1007/s11263-019-01173-5			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM		hybrid			2022-12-18	WOS:000474559000012
J	Hancock, E; Wilson, R; Smith, WAP; Bors, A; Pears, N				Hancock, Edwin; Wilson, Richard; Smith, William A. P.; Bors, Adrian; Pears, Nick			Editorial: Special Issue on Machine Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Hancock, Edwin; Wilson, Richard; Smith, William A. P.; Bors, Adrian; Pears, Nick] Univ York, York, N Yorkshire, England	University of York - UK	Smith, WAP (corresponding author), Univ York, York, N Yorkshire, England.	erh@cs.york.ac.uk; richard.wilson@york.ac.uk; William.Smith@york.ac.uk; adrian.bors@york.ac.uk; nick.pears@york.ac.uk	Smith, William/AAK-9101-2020; Hancock, Edwin R/C-6071-2008; Hancock, Edwin/N-7548-2019; Bors, Adrian/T-3618-2019	Smith, William/0000-0002-6047-0413; Hancock, Edwin R/0000-0003-4496-2028; Hancock, Edwin/0000-0003-4496-2028; Bors, Adrian/0000-0001-7838-0021					0	0	0	0	24	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2018	126	12			SI		1267	1268		10.1007/s11263-018-1128-9	http://dx.doi.org/10.1007/s11263-018-1128-9			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GZ3KT		Bronze			2022-12-18	WOS:000449286200001
J	Santa, Z; Kato, Z				Santa, Zsolt; Kato, Zoltan			Elastic Alignment of Triangular Surface Meshes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Elastic alignment; Triangular surface mesh; Thin plate spline; Correspondence-less registration	MEDICAL IMAGE REGISTRATION; SHAPE REGISTRATION; 3D; TRANSFORMATION; ALGORITHM; MODEL	A novel region-based approach is proposed to find a thin plate spline map between a pair of deformable 3D objects represented by triangular surface meshes. The proposed method works without landmark extraction and feature correspondences. The aligning transformation is simply found by solving a system of integral equations. Each equation is generated by integrating a non-linear function over the object domains. We derive recursive formulas for the efficient computation of these integrals for open and closed surface meshes. Based on a series of comparative tests on a large synthetic dataset, our triangular mesh-based algorithm outperforms state of the art methods both in terms of computing time and accuracy. The applicability of the proposed approach has been demonstrated on the registration of 3D lung CT volumes, brain surfaces and 3D human faces.	[Santa, Zsolt] Univ Szeged, Inst Informat, POB 652, H-6701 Szeged, Hungary; [Kato, Zoltan] J Selye Univ, Dept Math & Informat, Komarno, Slovakia	Szeged University; J. Selye University	Kato, Z (corresponding author), J Selye Univ, Dept Math & Informat, Komarno, Slovakia.	santazs@inf.u-szeged.hu; kato@inf.u-szeged.hu	Kato, Zoltan/AAD-6406-2019; Santa, Zsolt/A-7603-2015	Santa, Zsolt/0000-0002-1299-6162	NKFI-6 fund [K120366]; Research and Development Operational Programme [ITMS 26210120042]; European Regional Development Fund; Agence Universitaire de la Francophonie (AUF); Romanian Institute for Atomic Physics (IFA), through the AUF- RO project NETASSIST	NKFI-6 fund(National Research, Development & Innovation Office (NRDIO) - Hungary); Research and Development Operational Programme; European Regional Development Fund(European Commission); Agence Universitaire de la Francophonie (AUF); Romanian Institute for Atomic Physics (IFA), through the AUF- RO project NETASSIST	This research was partially supported by the NKFI-6 fund through Project K120366; the Research and Development Operational Programme for the project "Modernization and Improvement of Technical Infrastructure for Research and Development of J. Selye University in the Fields of Nanotechnology and Intelligent Space", ITMS 26210120042, co-funded by the European Regional Development Fund; the Agence Universitaire de la Francophonie (AUF) and the Romanian Institute for Atomic Physics (IFA), through the AUF- RO project NETASSIST. Lung images provided by Mediso Ltd., Budapest, Hungary. The MR brain data sets and their manual segmentations were provided by the Center for Morphometric Analysis at Massachusetts General Hospital and the face scans have been obtained from the Bosphorus Dataset (Savran et al. 2008).	Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606; Alliez P., 2000, CGAL USER REFERENCE; [Anonymous], 1990, SOC IND APPL MATH; [Anonymous], 2001, FUZZY SET THEORY AND, DOI DOI 10.1007/978-94-010-0646-0; Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Audette MA, 2000, MED IMAGE ANAL, V4, P201, DOI 10.1016/S1361-8415(00)00014-1; Blais F, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P422, DOI 10.1109/TDPVT.2004.1335269; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103; Bronstein MM, 2011, IEEE T PATTERN ANAL, V33, P1065, DOI 10.1109/TPAMI.2010.210; Bryant AS, 2006, ANN THORAC SURG, V82, P1016, DOI 10.1016/j.athoracsur.2006.03.095; Cheung W, 2009, IEEE T IMAGE PROCESS, V18, P2012, DOI 10.1109/TIP.2009.2024578; Christensen GE, 1999, LECT NOTES COMPUT SC, V1613, P224; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Creusot C, 2013, INT J COMPUT VISION, V102, P146, DOI 10.1007/s11263-012-0605-9; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Domokos C, 2012, IEEE T PATTERN ANAL, V34, P943, DOI 10.1109/TPAMI.2011.200; Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110; Forsyth D., 2008, LECT NOTES COMPUTER, V5303, DOI [10.1007/978-3-540-88688-4_44, DOI 10.1007/978-3-540-88688-4_44]; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Glocker B, 2011, ANNU REV BIOMED ENG, V13, P219, DOI 10.1146/annurev-bioeng-071910-124649; Glocker B, 2008, MED IMAGE ANAL, V12, P731, DOI 10.1016/j.media.2008.03.006; Guo H, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P205, DOI 10.1007/0-387-28831-7_13; Heike CL, 2009, PLAST RECONSTR SURG, V124, P1261, DOI 10.1097/PRS.0b013e3181b454bd; Holden M, 2008, IEEE T MED IMAGING, V27, P111, DOI 10.1109/TMI.2007.904691; Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223; Jin G, 2006, LECT NOTES COMPUT SC, V4291, P761; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974; Koehl P, 2012, IEEE T PATTERN ANAL, V34, P2158, DOI 10.1109/TPAMI.2012.23; Li S, 2009, MARKOV RANDOM FIELD; Lin H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461969; Lipman Y, 2011, ADV MATH, V227, P1047, DOI 10.1016/j.aim.2011.01.020; Lombaert H, 2014, INT J COMPUT VISION, V107, P254, DOI 10.1007/s11263-013-0681-5; Lourakis M.I., 2004, LEVMAR LEVENBERG MAR; Ma JY, 2013, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2013.279; Pozo JM, 2011, IEEE T PATTERN ANAL, V33, P471, DOI 10.1109/TPAMI.2010.139; Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105; Mitra J, 2012, INT C PATT RECOG, P2622; Mitra J., 2011, Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2011), P31, DOI 10.1109/DICTA.2011.14; Mitra J, 2012, MED IMAGE ANAL, V16, P1259, DOI 10.1016/j.media.2012.04.006; Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46; Nair P, 2009, IEEE T MULTIMEDIA, V11, P611, DOI 10.1109/TMM.2009.2017629; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006; Papazov C, 2011, COMPUT GRAPH FORUM, V30, P1493, DOI 10.1111/j.1467-8659.2011.02023.x; Pennec X, 1999, LECT NOTES COMPUT SC, V1679, P597; Peyre G, 2009, ADV COMPUTATIONAL VI; Postelnicu G, 2007, LECT NOTES COMPUT SC, V4584, P675; Rineau L., 2012, CGAL USER REFERENCE; Rineau L, 2007, COMP GEOM-THEOR APPL, V38, P100, DOI 10.1016/j.comgeo.2006.11.008; Sagawa Ryusuke, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1558, DOI 10.1109/ICCVW.2009.5457428; Santa Z., 2013, P IEEE C COMP VIS PA; Santa Z, 2016, LECT NOTES COMPUT SC, V9914, P521, DOI 10.1007/978-3-319-48881-3_36; Santa Z, 2012, 3RD IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFOCOMMUNICATIONS (COGINFOCOM 2012), P547; Santa Z, 2012, 2012 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING TECHNIQUES AND APPLICATIONS (DICTA); Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Scott DW, 2009, WILEY INTERDISCIP RE, V1, DOI 10.1002/wics.4; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Sharifi A, 2008, INT J ORAL MAX SURG, V37, P1089, DOI 10.1016/j.ijom.2008.06.011; Shewchuk JR, 2002, COMP GEOM-THEOR APPL, V22, P21, DOI 10.1016/S0925-7721(01)00047-5; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Sotiras A, 2013, IEEE T MED IMAGING, V32, P1153, DOI 10.1109/TMI.2013.2265603; Sukno FM, 2015, IEEE T CYBERNETICS, V45, P1717, DOI 10.1109/TCYB.2014.2359056; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310; Tamas L, 2015, LECT NOTES COMPUT SC, V8926, P640, DOI 10.1007/978-3-319-16181-5_49; Tanacs A, 2015, PATTERN RECOGN, V48, P1391, DOI 10.1016/j.patcog.2014.10.006; Thirion J P, 1998, Med Image Anal, V2, P243, DOI 10.1016/S1361-8415(98)80022-4; Vereauteren T, 2007, LECT NOTES COMPUT SC, V4792, P319; Weise T., 2007, IEEE C COMPUTER VISI, P1; Xu WP, 2015, IEEE I CONF COMP VIS, P2183, DOI 10.1109/ICCV.2015.252; Yamazaki S, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P191, DOI 10.1109/3DV.2013.33; Zagorchev L, 2006, IEEE T IMAGE PROCESS, V15, P529, DOI 10.1109/TIP.2005.863114; Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748; Zeng Y, 2013, IEEE I CONF COMP VIS, P3360, DOI 10.1109/ICCV.2013.417; Zeng Y, 2010, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2010.5540189; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	80	0	0	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2018	126	11					1220	1244		10.1007/s11263-018-1084-4	http://dx.doi.org/10.1007/s11263-018-1084-4			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GT3IE					2022-12-18	WOS:000444394200004
J	Leibe, B; Matas, J; Sebe, N; Welling, M				Leibe, Bastian; Matas, Jiri; Sebe, Nicu; Welling, Max			Editor's Note: Special Issue on Novel Representations and Learning Methods in Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Leibe, Bastian] Rhein Westfal TH Aachen, Aachen, Germany; [Matas, Jiri] Czech Tech Univ, Prague, Czech Republic; [Sebe, Nicu] Univ Trento, Trento, Italy; [Welling, Max] Univ Amsterdam, Amsterdam, Netherlands	RWTH Aachen University; Czech Technical University Prague; University of Trento; University of Amsterdam	Leibe, B (corresponding author), Rhein Westfal TH Aachen, Aachen, Germany.	leibe@vision.rwth-aachen.de; matas@cmp.felk.cvut.cz; sebe@disi.unitn.it; m.welling@uva.nl	, Matas/AAW-3282-2020						0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2018	126	10			SI		1061	1061		10.1007/s11263-018-1107-1	http://dx.doi.org/10.1007/s11263-018-1107-1			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HO5LR		Bronze			2022-12-18	WOS:000460968700001
J	Raftopoulos, KA; Kollias, SD; Sourlas, DD; Ferecatu, M				Raftopoulos, Konstantinos A.; Kollias, Stefanos D.; Sourlas, Dionysios D.; Ferecatu, Marin			On the Beneficial Effect of Noise in Vertex Localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Noising; Global vertices; Global curvature; Shape representation; Object recognition; Shape modeling; Incremental noising; Vertex localization	CURVATURE SCALE-SPACE; SHAPE; RECOGNITION; INVARIANTS; IMAGE	A theoretical and experimental analysis related to the effect of noise in the task of vertex identification in unknown shapes is presented. Shapes are seen as real functions of their closed boundary. An alternative global perspective of curvature is examined providing insight into the process of noise-enabled vertex localization. The analysis reveals that noise facilitates in the localization of certain vertices. The concept of noising is thus considered and a relevant global method for localizing Global Vertices is investigated in relation to local methods under the presence of increasing noise. Theoretical analysis reveals that induced noise can indeed help localizing certain vertices if combined with global descriptors. Experiments with noise and a comparison to localized methods validate the theoretical results.	[Raftopoulos, Konstantinos A.; Ferecatu, Marin] CEDRIC CNAM, 292 Rue St Martin, F-75141 Paris 03, France; [Kollias, Stefanos D.] Univ Lincoln, Lincoln LN6 7TS, Lincs, England; [Sourlas, Dionysios D.] Elais Unilever Hellas SA, A Legaki 22, Athens 18233, Greece; [Raftopoulos, Konstantinos A.] IVML NTUA, Iroon Polytexneiou 9, Athens 15780, Greece	heSam Universite; Conservatoire National Arts & Metiers (CNAM); University of Lincoln	Raftopoulos, KA (corresponding author), CEDRIC CNAM, 292 Rue St Martin, F-75141 Paris 03, France.; Raftopoulos, KA (corresponding author), IVML NTUA, Iroon Polytexneiou 9, Athens 15780, Greece.	raftop@cs.ucla.edu; skollias@lincoln.ac.uk; dionyssis.sourlas@unilever.com; marin.ferecatu@cnam.fr	Kollias, Stefanos/ACY-7285-2022; Raftopoulos, Konstantinos/AAU-6746-2020	Raftopoulos, Konstantinos A./0000-0002-9791-601X				Abbasi S, 1999, MULTIMEDIA SYST, V7, P467, DOI 10.1007/s005300050147; Bajaj CL, 2003, ACM T GRAPHIC, V22, P4, DOI 10.1145/588272.588276; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BENNETT JR, 1975, IEEE T COMPUT, V24, P803, DOI 10.1109/T-C.1975.224312; Biasotti S, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1391729.1391731; Coeurjolly David, 2013, Discrete Geometry for Computer Imagery. 17th IAPR International Conference, DGCI 2013. Proceedings, P215, DOI 10.1007/978-3-642-37067-0_19; Flusser J, 2006, IEEE T IMAGE PROCESS, V15, P3784, DOI 10.1109/TIP.2006.884913; Goldfeather J, 2004, ACM T GRAPHIC, V23, P45, DOI 10.1145/966131.966134; He X C, 2004, ICPR, V2, P791; Kerautret B, 2009, PATTERN RECOGN, V42, P2265, DOI 10.1016/j.patcog.2008.11.013; Manay S, 2006, IEEE T PATTERN ANAL, V28, P1602, DOI 10.1109/TPAMI.2006.208; Nguyen TP, 2007, LECT NOTES COMPUT SC, V4673, P474; Nguyen TP, 2011, PATTERN RECOGN, V44, P32, DOI 10.1016/j.patcog.2010.06.022; Porteous IR., 2001, GEOMETRIC DIFFERENTI; Pottmann H, 2009, COMPUT AIDED GEOM D, V26, P37, DOI 10.1016/j.cagd.2008.01.002; Raftopoulos KA, 2014, PROC CVPR IEEE, P4162, DOI 10.1109/CVPR.2014.530; Raftopoulos KA, 2011, COMPUT VIS IMAGE UND, V115, P1170, DOI 10.1016/j.cviu.2011.03.009; Razdan A, 2005, COMPUT AIDED DESIGN, V37, P1481, DOI 10.1016/j.cad.2005.03.003; Rosin P., 2008, HDB APPL ALGORITHMS, P347; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924; Sladoje N, 2009, IEEE T PATTERN ANAL, V31, P357, DOI 10.1109/TPAMI.2008.184; Stojmenovic M, 2008, J MATH IMAGING VIS, V30, P73, DOI 10.1007/s10851-007-0039-0; Tward DJ, 2013, INT J BIOMED IMAGING, V2013, DOI 10.1155/2013/205494; Xu D, 2008, PATTERN RECOGN, V41, P240, DOI 10.1016/j.patcog.2007.05.001; Zhang DS, 2003, J VIS COMMUN IMAGE R, V14, P41, DOI 10.1016/S1047-3203(03)00003-8; Zunic J, 2006, IEEE T IMAGE PROCESS, V15, P3478, DOI 10.1109/TIP.2006.877527; Zunic J, 2010, PATTERN RECOGN, V43, P47, DOI 10.1016/j.patcog.2009.06.017	28	0	0	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2018	126	1					111	139		10.1007/s11263-017-1034-6	http://dx.doi.org/10.1007/s11263-017-1034-6			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FS6MC		Green Submitted, Green Accepted			2022-12-18	WOS:000419910500006
J	Ikeuchi, K; Schnorr, C; Sivic, J; Vidal, R				Ikeuchi, Katsushi; Schnoerr, Christoph; Sivic, Josef; Vidal, Rene			Guest Editorial: Best Papers from ICCV 2015	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Ikeuchi, Katsushi] Microsoft Res Asia, Beijing, Peoples R China; [Schnoerr, Christoph] Heidelberg Univ, Dept Math & Comp Sci, Heidelberg, Germany; [Sivic, Josef] Ctr Rech INRIA Paris, Ecole Normale Super, Dept Informat, Paris, France; [Sivic, Josef] Czech Tech Univ, Czech Inst Informat Robot & Cybernet, Prague, Czech Republic; [Vidal, Rene] Johns Hopkins Univ, Dept Biomed Engn, Ctr Imaging Sci, Baltimore, MD USA	Microsoft; Microsoft Research Asia; Ruprecht Karls University Heidelberg; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Czech Technical University Prague; Johns Hopkins University	Ikeuchi, K (corresponding author), Microsoft Res Asia, Beijing, Peoples R China.	katsuike@microsoft.com; schnoerr@math.uni-heidelberg.de; Josef.Sivic@ens.fr; rvidal@jhu.edu							0	0	0	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2017	125	1-3			SI		1	2		10.1007/s11263-017-1046-2	http://dx.doi.org/10.1007/s11263-017-1046-2			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FL2TO		Bronze			2022-12-18	WOS:000414072800001
J	Mitchell, M; Platt, JC; Saenko, K				Mitchell, Margaret; Platt, John C.; Saenko, Kate			Guest Editorial: Image and Language Understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Mitchell, Margaret; Platt, John C.] Google, 601 N 34th St, Seattle, WA 98103 USA; [Saenko, Kate] Boston Univ, Dept Comp Sci, 111 Cummington Mall, Boston, MA 02215 USA	Google Incorporated; Boston University	Saenko, K (corresponding author), Boston Univ, Dept Comp Sci, 111 Cummington Mall, Boston, MA 02215 USA.	mmitchellai@google.com; platt@google.com; saenko@bu.edu	Platt, John/GOH-2678-2022	Saenko, Kate/0000-0002-7564-7218				Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Devlin J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P100; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337; Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162; Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Malinowski M., 2014, ADV NEURAL INFORM PR, V27, P1682; Malisiewicz T., 2009, ADV NEURAL INF PROCE, V22, P1222; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mitchell Margaret, 2012, EACL; Nowlan S. J., 1995, Advances in Neural Information Processing Systems 7, P901; Parikh D., 2008, P IEEE C COMP VIS PA, P1; Ren M., 2015, P 28 INT C NEUR INF, V2, P2953, DOI [10.5555/2969442.2969570, DOI 10.5555/2969442.2969570]; Roberts Lawrence G, 1963, THESIS, P2; Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Zitnick C. L., 2016, AI MAGAZINE, V37	23	0	0	2	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2017	123	1			SI		1	3		10.1007/s11263-017-0993-y	http://dx.doi.org/10.1007/s11263-017-0993-y			3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ET4SY		Bronze			2022-12-18	WOS:000400276400001
J	Liu, KW; Zhang, JG; Yang, PP; Maybank, S; Huang, KQ				Liu, Kangwei; Zhang, Junge; Yang, Peipei; Maybank, Stephen; Huang, Kaiqi			GRMA: Generalized Range Move Algorithms for the Efficient Optimization of MRFs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Markov random field; Discrete optimization; Energy minimization; Range move algorithms	MARKOV RANDOM-FIELDS; ENERGY MINIMIZATION	Markov random fields (MRF) have become an important tool for many vision applications, and the optimization of MRFs is a problem of fundamental importance. Recently, Veksler and Kumar et al. proposed the range move algorithms, which are some of the most successful optimizers. Instead of considering only two labels as in previous move-making algorithms, they explore a large search space over a range of labels in each iteration, and significantly outperform previous move-making algorithms. However, two problems have greatly limited the applicability of range move algorithms: (1) They are limited in the energy functions they can handle (i.e., only truncated convex functions); (2) They tend to be very slow compared to other move-making algorithms (e.g., -expansion and -swap). In this paper, we propose two generalized range move algorithms (GRMA) for the efficient optimization of MRFs. To address the first problem, we extend the GRMAs to more general energy functions by restricting the chosen labels in each move so that the energy function is submodular on the chosen subset. Furthermore, we provide a feasible sufficient condition for choosing these subsets of labels. To address the second problem, we dynamically obtain the iterative moves by solving set cover problems. This greatly reduces the number of moves during the optimization. We also propose a fast graph construction method for the GRMAs. Experiments show that the GRMAs offer a great speedup over previous range move algorithms, while yielding competitive solutions.	[Liu, Kangwei; Zhang, Junge; Yang, Peipei; Huang, Kaiqi] CASIA, Beijing, Peoples R China; [Maybank, Stephen] Univ London, Dept Comp Sci & Informat Syst, Birkbeck Coll, London, England	Chinese Academy of Sciences; Institute of Automation, CAS; University of London; Birkbeck University London	Huang, KQ (corresponding author), CASIA, Beijing, Peoples R China.	kqhuang@nlpr.ia.ac.cn			National Basic Research Program of China [2012CB316302]; National Natural Science Foundation of China [61322209, 61175007, 61403387]; International Partnership Program of Chinese Academy of Sciences [173211KYSB2016008]	National Basic Research Program of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); International Partnership Program of Chinese Academy of Sciences	This work is funded by the National Basic Research Program of China (Grant No. 2012CB316302), National Natural Science Foundation of China (Grant No. 61322209, Grant No. 61175007 and Grant No. 61403387). The work is supported by the International Partnership Program of Chinese Academy of Sciences, Grant No. 173211KYSB2016008. We thank Olga Veksler for her great help to this work, and we thank Pushmeet Kohli for his valuable comments.	Batra D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1865, DOI 10.1109/CVPR.2011.5995449; BESAG J, 1986, J R STAT SOC B, V48, P259; Boykov Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P26; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2000, LECT NOTES COMPUT SC, V1935, P276; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Chekuri C, 2005, SIAM J DISCRETE MATH, V18, P608, DOI 10.1137/S0895480101396937; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Gould S, 2009, PROC CVPR IEEE, P903, DOI 10.1109/CVPRW.2009.5206650; GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x; Gridchyn I, 2013, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2013.288; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Kappes JH, 2013, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2013.175; Kohli P, 2013, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2013.257; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kumar M. P., 2009, P 25 C UNC ART INT, P313; Kumar MP, 2011, J MACH LEARN RES, V12, P31; Kumar P., 2014, P ANN C NEUR INF PRO, P109; Lempitsky V, 2007, IEEE I CONF COMP VIS, P620; Lempitsky V, 2010, IEEE T PATTERN ANAL, V32, P1392, DOI 10.1109/TPAMI.2009.143; Liu KW, 2015, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2015.7298785; Liu KW, 2014, PROC CVPR IEEE, P2321, DOI 10.1109/CVPR.2014.297; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Nagarajan R, 2003, IEEE T MED IMAGING, V22, P882, DOI 10.1109/TMI.2003.815063; Poggio T., 1989, IMAGE UNDERST, V3, P111; Rother C., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383203; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Schlesinger D., 2006, TRANSFORMING ARBITRA; Slavik P., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P435, DOI 10.1006/jagm.1997.0887; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tappen MF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P900; Torr P., 2009, ADV NEURAL INFORM PR, P889; VEKSLER O, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383249; Veksler O, 2012, INT J COMPUT VISION, V98, P1, DOI 10.1007/s11263-011-0491-6	36	0	0	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2017	121	3					365	390		10.1007/s11263-016-0944-z	http://dx.doi.org/10.1007/s11263-016-0944-z			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK9UY		Green Accepted			2022-12-18	WOS:000394270600003
J	Laptev, I; Ramanan, D; Sivic, J				Laptev, Ivan; Ramanan, Deva; Sivic, Josef			Guest Editorial: Video Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Laptev, Ivan; Sivic, Josef] Ecole Normale Super, Inria Willow Team, Dept Informat, Paris, France; [Ramanan, Deva] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Carnegie Mellon University	Laptev, I (corresponding author), Ecole Normale Super, Inria Willow Team, Dept Informat, Paris, France.	ivan.laptev@inria.fr							0	0	0	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	3			SI		217	218		10.1007/s11263-016-0922-5	http://dx.doi.org/10.1007/s11263-016-0922-5			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FE		Bronze			2022-12-18	WOS:000380270000001
J	Hammoud, RI; Sivic, J; Davis, LS; Pollefeys, M				Hammoud, Riad I.; Sivic, Josef; Davis, Larry S.; Pollefeys, Marc			Guest Editorial: Large Scale Visual Media Geo-Localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Hammoud, Riad I.] BAE Syst, Burlington, MA USA; [Sivic, Josef] INRIA, Paris, France; [Davis, Larry S.] Univ Maryland, College Pk, MD 20742 USA; [Pollefeys, Marc] ETH, Zurich, Switzerland	Bae Systems; Inria; University System of Maryland; University of Maryland College Park; Swiss Federal Institutes of Technology Domain; ETH Zurich	Hammoud, RI (corresponding author), BAE Syst, Burlington, MA USA.	Riad.hammoud@baesystems.com; Josef.sivic@ens.fr; lsd@umiacs.umd.edu; Marc.pollefeys@inf.ethz.ch	Pollefeys, Marc/I-7607-2013						0	0	0	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2016	116	3			SI		211	212		10.1007/s11263-015-0870-5	http://dx.doi.org/10.1007/s11263-015-0870-5			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7SY		Bronze			2022-12-18	WOS:000369421900001
J	Warnell, G; David, P; Chellappa, R				Warnell, Garrett; David, Philip; Chellappa, Rama			Ray Saliency: Bottom-Up Visual Saliency for a Rotating and Zooming Camera	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual saliency; Pan-tilt-zoom camera	BINARY SEARCH TREES; SELF-CALIBRATION; ATTENTION	We extend the classical notion of computational visual saliency to multi-image data collected using a stationary pan-tilt-zoom (PTZ) camera by introducing the concept of consistency: the requirement that the set of generated saliency maps should each assign the same saliency value to unique regions of the environment that appear in more than one image. We show that processing each image independently will often fail to provide a consistent measure of saliency, and that using an image mosaic to quantify saliency suffers from several drawbacks. We then propose ray saliency and an immediate extension, approximate ray saliency: a mosaic-free method for calculating a consistent measure of bottom-up saliency. Experimental results demonstrating the effectiveness of the proposed approach are presented.	[Warnell, Garrett; David, Philip] US Army, Res Lab, Adelphi, MD USA; [Chellappa, Rama] Univ Maryland, College Pk, MD 20742 USA	United States Department of Defense; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL); University System of Maryland; University of Maryland College Park	Warnell, G (corresponding author), US Army, Res Lab, Adelphi, MD USA.	garrett.a.warnell.civ@mail.mil	Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012		U.S. Army Research Office (ARO) MURI Grant [W911NF-09-1-0383]	U.S. Army Research Office (ARO) MURI Grant	Funding for this work was provided by U.S. Army Research Office (ARO) MURI Grant W911NF-09-1-0383.	Achanta R., 2010, SLIC SUPERPIXELS; Achanta R., 2009, P IEEE C COMP VIS PA; Agapito L, 2001, INT J COMPUT VISION, V45, P107, DOI 10.1023/A:1012471930694; Agarwal S., 2014, CERES SOLVER TUTORIA; Aloimonos J., 1987, International Journal of Computer Vision, V1, P333, DOI 10.1007/BF00133571; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Bogdanova I, 2008, IEEE T IMAGE PROCESS, V17, P2000, DOI 10.1109/TIP.2008.2003415; Bogdanova I, 2010, COMPUT VIS IMAGE UND, V114, P100, DOI 10.1016/j.cviu.2009.09.003; Borji A., 2012, P EUR C COMP VIS; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5; Bruce Neil D.B., 2005, NEURAL INFORM PROCES; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Fleming K., 2006, P IEEE RSJ INT C INT; Gao D., 2007, NEURAL INFORM PROCES; Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27; Harel J., 2006, NEURAL INFORM PROCES; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hartley RI, 1997, INT J COMPUT VISION, V22, P5, DOI 10.1023/A:1007957826135; Ip CY, 2011, IEEE T VIS COMPUT GR, V17, P1737, DOI 10.1109/TVCG.2011.231; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; KOCH C, 1985, HUM NEUROBIOL, V4, P219; Kovesi P, 2000, MATLAB AND OCTAVE FU; LEE DT, 1977, ACTA INFORM, V9, P23, DOI 10.1007/BF00263763; Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96; Liu MY, 2014, IEEE T PATTERN ANAL, V36, P99, DOI 10.1109/TPAMI.2013.107; LIU T, 2007, P IEEE C COMP VIS PA; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mahadevan V., 2009, P IEEE C COMP VIS PA; Malik J., 2003, P IEEE INT C COMP VI; Oliva A., 2003, P IEEE INT C IM PROC; Perazzi F., 2012, P IEEE C COMP VIS PA; Ruesch J., 2008, P IEEE INT C ROB AUT; Siagian C, 2007, P IEEE RSJ INT C INT; Szeliski R., 2006, MSRTR200492 MICR; Tomasi C., 1998, 6 INT C COMP VIS, P839	39	0	0	2	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2016	116	2					174	189		10.1007/s11263-015-0842-9	http://dx.doi.org/10.1007/s11263-015-0842-9			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7TJ					2022-12-18	WOS:000369423000004
J	Pham, MT; Woodford, OJ; Perbet, F; Maki, A; Gherardi, R; Stenger, B; Cipolla, R				Minh-Tri Pham; Woodford, Oliver J.; Perbet, Frank; Maki, Atsuto; Gherardi, Riccardo; Stenger, Bjoern; Cipolla, Roberto			Distances and Means of Direct Similarities	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Direct similarity; Distance; Mean; Registration; Object recognition	MANIFOLDS; SHIFT; INTERPOLATION; FRAMEWORK	The non-Euclidean nature of direct isometries in a Euclidean space, i.e. transformations consisting of a rotation and a translation, creates difficulties when computing distances, means and distributions over them, which have been well studied in the literature. Direct similarities, transformations consisting of a direct isometry and a positive uniform scaling, present even more of a challenge-one which we demonstrate and address here. In this article, we investigate divergences (a superset of distances without constraints on symmetry and sub-additivity) for comparing direct similarities, and means induced by them via minimizing a sum of squared divergences. We analyze several standard divergences: the Euclidean distance using the matrix representation of direct similarities, a divergence from Lie group theory, and the family of all left-invariant distances derived from Riemannian geometry. We derive their properties and those of their induced means, highlighting several shortcomings. In addition, we introduce a novel family of left-invariant divergences, called SRT divergences, which resolve several issues associated with the standard divergences. In our evaluation we empirically demonstrate the derived properties of the divergences and means, both qualitatively and quantitatively, on synthetic data. Finally, we compare the divergences in a real-world application: vote-based, scale-invariant object recognition. Our results show that the new divergences presented here, and their means, are both more effective and faster to compute for this task.	[Minh-Tri Pham; Woodford, Oliver J.; Perbet, Frank; Gherardi, Riccardo; Stenger, Bjoern] Toshiba Res Europe Ltd, Cambridge CB4 0GZ, England; [Maki, Atsuto] KTH Royal Inst Technol, Sch Comp Sci & Commun, Comp Vis & Act Percept Lab, S-10044 Stockholm, Sweden; [Cipolla, Roberto] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	Toshiba Corporation; Royal Institute of Technology; University of Cambridge	Pham, MT (corresponding author), Toshiba Res Europe Ltd, 208 Cambridge Sci Pk,Milton Rd, Cambridge CB4 0GZ, England.	mtpham@crl.toshiba.co.uk	Arandjelović, Ognjen/V-5255-2019	Arandjelović, Ognjen/0000-0002-9314-194X; Cipolla, Roberto/0000-0002-8999-2151				Agrawal M, 2006, 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, Vols 1-12, P1891, DOI 10.1109/IROS.2006.282313; Arnaudon M, 2014, ESAIM-PROBAB STAT, V18, P185, DOI 10.1051/ps/2013033; Arnold V. I., 1989, GRAD TEXTS MATH, V60; ARSIGNY V, 2006, 5885 INRIA; Arsigny V, 2006, LECT NOTES COMPUT SC, V4057, P120; Begelfor E., 2006, 2006 IEEE COMPUTER S, V2, P2087, DOI DOI 10.1109/CVPR.2006.50; Beltrami E., 1868, ANN MAT PUR APPL, V2, P232; Bhattacharya R, 2003, ANN STAT, V31, P1; Bossa M. N., 2006, WORKSH MATH METH BIO; Carreira-Perpinan MA, 2007, IEEE T PATTERN ANAL, V29, P767, DOI 10.1109/TPAMI.2007.1057; Cetingul HE, 2009, PROC CVPR IEEE, P1896, DOI 10.1109/CVPRW.2009.5206806; Cheng SH, 2001, SIAM J MATRIX ANAL A, V22, P1112, DOI 10.1137/S0895479899364015; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; Coxeter Harold Scott Macdonald, 1961, INTRO GEOMETRY, P3; DOWNS TD, 1972, BIOMETRIKA, V59, P665, DOI 10.2307/2334817; Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108; Dubbelman G, 2012, LECT NOTES COMPUT SC, V7573, P531, DOI 10.1007/978-3-642-33709-3_38; Eade E., 2011, LIE GROUPS 2D 3D TRA; Frechet M., 1948, ANN I H POINCARE, V10, P215; Gallier J., 2002, INT J ROBOT AUTOM, V17, P10; Hall B. C., 2003, LIE GROUPS LIE ALGEB, V222; Hartley R., 2004, ROBOTICA; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; Khoshelham K., 2007, ISPRS WORKSH LAS SCA, VXXXVI, P206; Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43; Lee John M, 2018, INTRO RIEMANNIAN MAN; Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Moakher M, 2002, SIAM J MATRIX ANAL A, V24, P1, DOI 10.1137/S0895479801383877; Oneill B., 1983, SEMIRIEMANNIAN GEOME, V103; Opelt A., 2008, INT J COMPUTER VISIO, V80; PARK FC, 1995, J MECH DESIGN, V117, P48, DOI 10.1115/1.2826116; Park FC, 1997, ACM T GRAPHIC, V16, P277, DOI 10.1145/256157.256160; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Pelletier B, 2005, STAT PROBABIL LETT, V73, P297, DOI 10.1016/j.spl.2005.04.004; Pennec X, 1998, J MATH IMAGING VIS, V9, P49, DOI 10.1023/A:1008270110193; Pennec X, 1997, INT J COMPUT VISION, V25, P203, DOI 10.1023/A:1007976002485; Pennec X., 1998, RR3371 INRIA; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Pham M. T., 2012, TOSHIBA CAD MODEL PO; Pham M. T., 2011, P INT C COMP VIS; POINCARE H, 1882, THEORIE GROUPES FUCH; RAVANI B, 1983, J MECH TRANSM-T ASME, V105, P460, DOI 10.1115/1.3267382; ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Schramm I, 2003, LECT NOTES COMPUT SC, V2669, P356; Shotton J, 2008, IEEE T PATTERN ANAL, V30, P1270, DOI 10.1109/TPAMI.2007.70772; SIBSON R, 1979, J ROY STAT SOC B MET, V41, P217; Sternberg S, 1999, LECT DIFFERENTIAL GE; Strasdat H., 2010, ROB SCI SYST, DOI [10.15607/RSS.2010.VI.010, DOI 10.15607/RSS.2010.VI.010]; Subbarao R, 2009, INT J COMPUT VISION, V84, P1, DOI 10.1007/s11263-008-0195-8; Tombari F., 2010, 4 PAC RIM S IM VID T, P349, DOI DOI 10.1109/PSIVT.2010.65; Vaccaro C., 2012, QUANTITATIVE FINANCE; Woodford O. J., 2013, INT J COMPUTER VISIO; Zefran M, 1998, COMPUT AIDED DESIGN, V30, P179, DOI 10.1016/S0010-4485(97)00060-2	57	0	0	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2015	112	3					285	306		10.1007/s11263-014-0762-0	http://dx.doi.org/10.1007/s11263-014-0762-0			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CG3EA		Green Submitted			2022-12-18	WOS:000353159600002
J	Cuzzolin, F; Mateus, D; Horaud, R				Cuzzolin, Fabio; Mateus, Diana; Horaud, Radu			Robust Temporally Coherent Laplacian Protrusion Segmentation of 3D Articulated Bodies	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised segmentation; 3D shape analysis; 3D shape tracking; Human motion analysis; Spectral embedding	DIMENSIONALITY REDUCTION; KERNELS; GRAPH; RECOGNITION; COMPONENTS; FRAMEWORK; MODELS; FLOW	In motion analysis and understanding it is important to be able to fit a suitable model or structure to the temporal series of observed data, in order to describe motion patterns in a compact way, and to discriminate between them. In an unsupervised context, i.e., no prior model of the moving object(s) is available, such a structure has to be learned from the data in a bottom-up fashion. In recent times, volumetric approaches in which the motion is captured from a number of cameras and a voxel-set representation of the body is built from the camera views, have gained ground due to attractive features such as inherent view-invariance and robustness to occlusions. Automatic, unsupervised segmentation of moving bodies along entire sequences, in a temporally-coherent and robust way, has the potential to provide a means of constructing a bottom-up model of the moving body, and track motion cues that may be later exploited for motion classification. Spectral methods such as locally linear embedding can be useful in this context, as they preserve "protrusions", i.e., high-curvature regions of the 3D volume, of articulated shapes, while improving their separation in a lower dimensional space, making them in this way easier to cluster. In this paper we therefore propose a spectral approach to unsupervised and temporally-coherent body-protrusion segmentation along time sequences. Volumetric shapes are clustered in an embedding space, clusters are propagated in time to ensure coherence, and merged or split to accommodate changes in the body's topology. Experiments on both synthetic and real sequences of dense voxel-set data are shown. This supports the ability of the proposed method to cluster body-parts consistently over time in a totally unsupervised fashion, its robustness to sampling density and shape quality, and its potential for bottom-up model construction.	[Cuzzolin, Fabio] Oxford Brookes Univ, Dept Comp & Commun Technol, Oxford OX3 0BP, England; [Mateus, Diana] Tech Univ Munich, Inst Informat, Garching, Germany; [Horaud, Radu] INRIA Grenoble Rhone Alpes, Montbonnot St Martin, France	Oxford Brookes University; Technical University of Munich	Cuzzolin, F (corresponding author), Oxford Brookes Univ, Dept Comp & Commun Technol, Oxford OX3 0BP, England.	fabio.cuzzolin@brookes.ac.uk; mateus@in.tum.de; radu.horaud@inria.fr	Horaud, Radu/AAR-5982-2021	Horaud, Radu/0000-0001-5232-024X; Mateus, Diana/0000-0002-2252-8717				Agarwal S, 2005, PROC CVPR IEEE, P838; Ali S., 2007, INT C COMP VIS; Alpert CJ, 1999, DISCRETE APPL MATH, V90, P3, DOI 10.1016/S0166-218X(98)00083-3; Anguelov D., 2004, P UAI; ANGUELOV D, 2004, NEURAL INFORM PROCES; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bengio Y, 2004, NEURAL COMPUT, V16, P2197, DOI 10.1162/0899766041732396; Bengio Y., 2003, TECHNICAL REPORT; Bissacco A, 2007, IEEE T PATTERN ANAL, V29, P1958, DOI 10.1109/TPAMI.2007.1101; Biyikoglu J., 2007, LAPLACIAN EIGENVECTO; Blank M, 2005, IEEE I CONF COMP VIS, P1395; Brand M., 1999, INT C COMP VIS; Bronstein AM, 2009, INT J COMPUT VISION, V81, P281, DOI 10.1007/s11263-008-0172-2; Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103; Brostow G.J., 2004, EUR C COMP VIS; Carter N., 2006, P AVSBS; Chang W., 2008, EUR S GEOM PROC; Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821; Chu CW, 2003, PROC CVPR IEEE, P475; Chung F., 1997, AM MATH SOC, DOI 10.1090/cbms/092; Cuzzolin F, 2004, IEEE IMAGE PROC, P881; Cuzzolin F, 2004, 2004 IEEE 6TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P395; Cuzzolin F., 2008, COMPUTER VISION PATT; Cuzzolin F, 2007, LECT NOTES COMPUT SC, V4814, P196; Cvetkovic M., 1998, SPECTRA GRAPHS THEOR; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; ELGAMMAL A, 2004, COMPUTER VISION PATT; Elliot R., 1995, HIDDEN MARKOV MODELS; Esteban CH, 2004, COMPUT VIS IMAGE UND, V96, P367, DOI 10.1016/j.cviu.2004.03.016; Fouss F, 2007, IEEE T KNOWL DATA EN, V19, P355, DOI 10.1109/TKDE.2007.46; Franco J. S., 2011, COMPUTER VISION PATT; Franco JS, 2009, IEEE T PATTERN ANAL, V31, P414, DOI 10.1109/TPAMI.2008.104; Furukawa Y., 2005, SIGGRAPH; Golovinskiy A., 2009, SIGGRAPH; Grauman K, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P641; Gupta A, 2009, PROC CVPR IEEE, P2012, DOI 10.1109/CVPRW.2009.5206492; HAYASHI C, 1972, ANN I STAT MATH, V24, P251, DOI 10.1007/BF02479755; Heiser WJ, 1997, J MATH PSYCHOL, V41, P189, DOI 10.1006/jmps.1997.1166; Huang Q-X, 2009, EUROGRAPHICS, V28; JAIN V, 2006, SHAPE MODELING INT; Jakobson D, 2001, RUSS MATH SURV+, V56, P1085, DOI 10.1070/RM2001v056n06ABEH000453; Jenkins O. C., 2004, INT C MACH LEARN; Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9; Kim TK, 2009, IEEE T PATTERN ANAL, V31, P1415, DOI 10.1109/TPAMI.2008.167; Knossow D, 2008, INT J COMPUT VISION, V79, P247, DOI 10.1007/s11263-007-0116-2; Kulis B., 2004, P 10 ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118; Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1393, DOI 10.1109/TPAMI.2006.184; Levi B., 2006, SHAPE MODELING INT; Lien J.-M., 2007, P 2007 ACM S SOLID P, P121; Lin R., 2006, EUR C COMP VIS; Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Mateus D., 2008, COMPUTER VISION PATT; Meila M., 2001, ARTIFICIAL INTELLIGE; Ng A, 2002, NEURAL INFORM PROCES; Pons JP, 2007, INT J COMPUT VISION, V72, P179, DOI 10.1007/s11263-006-8671-5; Ralaivola L, 2004, ADV NEUR IN, V16, P129; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Saerens M, 2004, LECT NOTES COMPUT SC, V3201, P371; Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x; Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5; Sharma A., 2009, P AAAI FALL S MAN LE; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shi Q., 2008, COMPUTER VISION PATT; Starck J., 2007, INT C COMP VIS; Sundaresan A, 2006, INT C PATT RECOG, P92; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Varanasi K., 2010, P 3DPTV; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Zaharescu A, 2011, IEEE T PATTERN ANAL, V33, P823, DOI 10.1109/TPAMI.2010.116; Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219	71	0	0	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2015	112	1					43	70		10.1007/s11263-014-0754-0	http://dx.doi.org/10.1007/s11263-014-0754-0			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CC4YC		Green Submitted			2022-12-18	WOS:000350361500003
J	Taguchi, Y				Taguchi, Yuichi			Rainbow Flash Camera: Depth Edge Extraction Using Complementary Colors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-flash camera; Depth edge extraction; Color multiplexing; Complementary color; Hue circle; Bin picking		We present a novel color multiplexing method for extracting depth edges in a scene. It has been shown that casting shadows from different light positions provides a simple yet robust cue for extracting depth edges. Instead of flashing a single light source at a time as in conventional methods, our method flashes all light sources simultaneously to reduce the number of captured images. We use a ring light source around a camera and arrange colors on the ring such that the colors form a hue circle. Since complementary colors are arranged at any position and its antipole on the ring, shadow regions where a half of the hue circle is occluded are colorized according to the orientations of depth edges, while non-shadow regions where all the hues are mixed have a neutral color in the captured image. Thus the colored shadows in the single image directly provide depth edges and their orientations in an ideal situation. We present an algorithm that extracts depth edges from a single image by analyzing the colored shadows. We also present a more robust depth edge extraction algorithm using an additional image captured by rotating the hue circle with 180 degrees to compensate for scene textures and ambient lights. We compare our approach with conventional methods for various scenes using a camera prototype consisting of a standard camera and 8 color LEDs. We also demonstrate a bin-picking system using the camera prototype mounted on a robot arm.	MERL, Cambridge, MA 02139 USA		Taguchi, Y (corresponding author), MERL, Cambridge, MA 02139 USA.	taguchi@merl.com						Agrawal A, 2010, INT J ROBOT RES, V29, P155, DOI 10.1177/0278364909353955; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chen C., 2011, P IEEE INT C COMP VI; Crispell D, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P405; De Decker Bert, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2536, DOI 10.1109/CVPRW.2009.5206752; Feris R, 2004, XVII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P316, DOI 10.1109/SIBGRA.2004.1352976; Feris R, 2006, SIBGRAPI, P3; Feris R, 2008, IEEE T PATTERN ANAL, V30, P147, DOI 10.1109/TPAMI.2007.1136; Fyffe G., 2011, P IEEE INT C COMP PH, P16, DOI [10.1109/ICCPHOT.2011.5753116, DOI 10.1109/ICCPH0T.2011.5753116]; Hernandez C, 2007, IEEE I CONF COMP VIS, P873; Liu M. Y., 2013, P IEEE C COMP VIS PA; Liu MY, 2012, INT J ROBOT RES, V31, P951, DOI 10.1177/0278364911436018; MacEvoy B., 2008, COLOR VISION; Minomo Y., 2006, COMPUTERS ENTERTAINM, V4; Park JI, 2007, IEEE I CONF COMP VIS, P2049; Raskar R, 2004, ACM T GRAPHIC, V23, P679, DOI 10.1145/1015706.1015779; Sa A. M., 2002, P VIS MOD VIS C VMV; Schechner YY, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P808; Shroff Nitesh, 2011, IEEE International Conference on Robotics and Automation, P5963; Taguchi Y, 2012, LECT NOTES COMPUT SC, V7577, P513, DOI 10.1007/978-3-642-33783-3_37; Vaquero D. A., 2008, P IEEE C COMP VIS PA; Vaquero DA, 2009, PROC CVPR IEEE, P2082, DOI 10.1109/CVPRW.2009.5206614; Wan G., 2012, 20122 STANF COMP GRA; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479	24	0	0	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2014	110	2			SI		156	171		10.1007/s11263-014-0726-4	http://dx.doi.org/10.1007/s11263-014-0726-4			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AY5UE		Green Submitted			2022-12-18	WOS:000347636400006
J	[Anonymous]				[Anonymous]			Special Issue: Computational Photography	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2014	110	2			SI		91	91		10.1007/s11263-014-0756-y	http://dx.doi.org/10.1007/s11263-014-0756-y			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AY5UE		Bronze			2022-12-18	WOS:000347636400002
J	Bowden, R; Collomosse, J; Mikolajczyk, K				Bowden, R.; Collomosse, J.; Mikolajczyk, K.			Guest Editorial: Tracking, Detection and Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Bowden, R.; Collomosse, J.; Mikolajczyk, K.] Univ Surrey, CVSSP, Guildford GU2 5XH, Surrey, England	University of Surrey	Mikolajczyk, K (corresponding author), Univ Surrey, CVSSP, Guildford GU2 5XH, Surrey, England.	R.Bowden@surrey.ac.uk; J.Collomosse@surrey.ac.uk; K.Mikolajczyk@surrey.ac.uk	Bowden, Richard/AAF-8283-2019	Bowden, Richard/0000-0003-3285-8020					0	0	2	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2014	110	1			SI		1	1		10.1007/s11263-014-0753-1	http://dx.doi.org/10.1007/s11263-014-0753-1			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AQ0BZ		Bronze			2022-12-18	WOS:000342448000001
J	Xu, D; Chellappa, R; Darrell, T; Daume, H				Xu, Dong; Chellappa, Rama; Darrell, Trevor; Daume, Hal, III			Guest Editor's Introduction to the Special Issue on Domain Adaptation for Vision Applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Xu, Dong] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore; [Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA; [Chellappa, Rama] Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA; [Darrell, Trevor] Univ Calif Berkeley, Coll Engn, Berkeley, CA 94720 USA; [Daume, Hal, III] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park; University of California System; University of California Berkeley; University System of Maryland; University of Maryland College Park	Xu, D (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Blk N4,2A-29 Nanyang Ave, Singapore 639798, Singapore.	dongxu@ntu.edu.sg; rama@umiacs.umd.edu; trevor@eecs.berkeley.edu; hal@umiacs.umd.edu	Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012						0	0	0	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2014	109	1-2			SI		1	2		10.1007/s11263-014-0730-8	http://dx.doi.org/10.1007/s11263-014-0730-8			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AI7QY		Bronze			2022-12-18	WOS:000337091700001
J	Aanaes, H; Dahl, AL; Pedersen, KS				Aanaes, Henrik; Dahl, Anders Lindbjerg; Pedersen, Kim Steenstrup			Interesting Interest Points A Comparative Study of Interest Point Performance on a Unique Data Set (vol 97, pg 18, 2012)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Aanaes, Henrik; Dahl, Anders Lindbjerg] Tech Univ Denmark, DTU Informat, DK-2800 Lyngby, Denmark; [Pedersen, Kim Steenstrup] Univ Copenhagen, Dept Comp Sci, Image Grp, E Sci Ctr, Copenhagen, Denmark	Technical University of Denmark; University of Copenhagen	Dahl, AL (corresponding author), Tech Univ Denmark, DTU Informat, DK-2800 Lyngby, Denmark.	haa@imm.dtu.dk; abd@imm.dtu.dk; kimstp@diku.dk		Aanaes, Henrik/0000-0001-7547-4743				Aanaes H, 2012, INT J COMPUT VISION, V97, P18, DOI 10.1007/s11263-011-0473-8	1	0	0	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2014	108	3					259	259		10.1007/s11263-014-0714-8	http://dx.doi.org/10.1007/s11263-014-0714-8			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AH8NO		Bronze			2022-12-18	WOS:000336394900006
J	Grauman, K; Belongie, S				Grauman, Kristen; Belongie, Serge			Editorial: Special Issue on Active and Interactive Methods in Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Grauman, Kristen] Univ Texas Austin, Austin, TX 78712 USA; [Belongie, Serge] Cornell Tech, New York, NY USA	University of Texas System; University of Texas Austin	Grauman, K (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	grauman@cs.utexas.edu; sjb344@cornell.edu		Belongie, Serge/0000-0002-0388-5217					0	0	0	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2014	108	1-2			SI		1	2		10.1007/s11263-014-0724-6	http://dx.doi.org/10.1007/s11263-014-0724-6			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AG7BT		Bronze			2022-12-18	WOS:000335573700001
J	Yuille, AL; Luo, JB				Yuille, A. L.; Luo, Jiebo			Guest Editorial: Geometry, Lighting, Motion, and Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Yuille, A. L.] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA	University of California System; University of California Los Angeles; University of Rochester	Yuille, AL (corresponding author), Univ Calif Los Angeles, Dept Stat, 8967 Math Sci Bldg, Los Angeles, CA 90095 USA.	alan.l.yuille@gmail.com; jiebo.luo@gmail.com	Luo, Jiebo/AAI-7549-2020	Yuille, Alan L./0000-0001-5207-9249; Luo, Jiebo/0000-0002-4516-9729					0	0	0	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2014	107	2			SI		99	100		10.1007/s11263-014-0706-8	http://dx.doi.org/10.1007/s11263-014-0706-8			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AD3QP		Bronze			2022-12-18	WOS:000333161200001
J	McKenna, SJ; Hoey, J; Trucco, E				McKenna, Stephen J.; Hoey, Jesse; Trucco, Emanuele			Objects, Actions, Places	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[McKenna, Stephen J.; Trucco, Emanuele] Univ Dundee, Sch Comp, Dundee DD1 4HN, Scotland; [Hoey, Jesse] Univ Waterloo, David R Cheriton Sch Comp Sci, Waterloo, ON N2L 3G1, Canada	University of Dundee; University of Waterloo	McKenna, SJ (corresponding author), Univ Dundee, Sch Comp, Dundee DD1 4HN, Scotland.	stephen@computing.dundee.ac.uk; jhoey@cs.uwaterloo.ca; manueltrucco@computing.dundee.ac.uk	McKenna, Stephen/AAL-8335-2020	McKenna, Stephen/0000-0003-0530-2035; Trucco, Emanuele/0000-0002-5055-0794				Bilen H, 2014, INT J COMPUT VISION, V106, P237, DOI 10.1007/s11263-013-0646-8; Haines TSF, 2014, INT J COMPUT VISION, V106, P315, DOI 10.1007/s11263-013-0630-3; Johns E, 2014, INT J COMPUT VISION, V106, P297, DOI 10.1007/s11263-013-0648-6; Lehmann AD, 2014, INT J COMPUT VISION, V106, P252, DOI 10.1007/s11263-013-0670-8; Liu K, 2014, INT J COMPUT VISION, V106, P342, DOI 10.1007/s11263-013-0634-z; Marin-Jimenez MJ, 2014, INT J COMPUT VISION, V106, P282, DOI 10.1007/s11263-013-0655-7; Ren CYH, 2014, INT J COMPUT VISION, V106, P269, DOI 10.1007/s11263-013-0635-y; Woodford OJ, 2014, INT J COMPUT VISION, V106, P332, DOI 10.1007/s11263-013-0623-2	8	0	0	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2014	106	3			SI		235	236		10.1007/s11263-014-0699-3	http://dx.doi.org/10.1007/s11263-014-0699-3			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AA3DC		Bronze			2022-12-18	WOS:000330972100001
J	Mikulik, A; Perdoch, M; Chum, O; Matas, J				Mikulik, Andrej; Perdoch, Michal; Chum, Ondrej; Matas, Jiri			Learning Vocabularies over a Fine Quantization (vol 103, pg 163, 2013)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Mikulik, Andrej; Perdoch, Michal; Chum, Ondrej; Matas, Jiri] Czech Tech Univ, Fac Elect Engn, Dept Cybernet, CMP, CR-16635 Prague, Czech Republic	Czech Technical University Prague	Mikulik, A (corresponding author), Czech Tech Univ, Fac Elect Engn, Dept Cybernet, CMP, CR-16635 Prague, Czech Republic.	mikulik@cmp.felk.cvut.cz; predom1@cmp.felk.cvut.cz; chum@cmp.felk.cvut.cz; matas@cmp.felk.cvut.cz	, Matas/AAW-3282-2020; Chum, Ondrej/F-5262-2015	Chum, Ondrej/0000-0001-7042-1810				Mikulik A, 2013, INT J COMPUT VISION, V103, P163, DOI 10.1007/s11263-012-0600-1	1	0	0	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2014	106	1					113	113		10.1007/s11263-013-0682-4	http://dx.doi.org/10.1007/s11263-013-0682-4			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	288QM		Bronze			2022-12-18	WOS:000329626800006
J	Pennec, X; Joshi, S; Nielsen, M				Pennec, Xavier; Joshi, Sarang; Nielsen, Mads			Mathematical Methods for Medical Imaging	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Pennec, Xavier] INRIA Sophia Antipolis, Sophia Antipolis, France; [Joshi, Sarang] Univ Utah, SCI, Salt Lake City, UT USA; [Nielsen, Mads] Univ Copenhagen, Copenhagen, Denmark	Utah System of Higher Education; University of Utah; University of Copenhagen	Nielsen, M (corresponding author), Univ Copenhagen, Copenhagen, Denmark.	madsn@diku.dk	Pennec, Xavier/L-2537-2013	Pennec, Xavier/0000-0002-6617-7664					0	0	0	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2013	105	2			SI		109	110		10.1007/s11263-013-0650-z	http://dx.doi.org/10.1007/s11263-013-0650-z			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	207YP		Bronze			2022-12-18	WOS:000323641100001
J	Boykov, Y; Kahl, F; Lempitsky, V; Schmidt, FR				Boykov, Yuri; Kahl, Fredrik; Lempitsky, Victor; Schmidt, Frank R.			Guest Editorial: Energy Optimization Methods	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Boykov, Yuri] Univ Western Ontario, London, ON, Canada; [Kahl, Fredrik] Lund Univ, Lund, Sweden; [Lempitsky, Victor] Skolkovo Inst Sci & Technol, Moscow, Russia; [Schmidt, Frank R.] Univ Freiburg, D-79106 Freiburg, Germany	Western University (University of Western Ontario); Lund University; Skolkovo Institute of Science & Technology; University of Freiburg	Boykov, Y (corresponding author), Univ Western Ontario, London, ON, Canada.	yuri@csd.uwo.ca; fredrik@maths.lth.se; lempitsky@skoltech.ru; schmidt@cs.uni-freiburg.de	Boykov, Yuri/C-1718-2015						0	0	1	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2013	104	3			SI		221	222		10.1007/s11263-013-0637-9	http://dx.doi.org/10.1007/s11263-013-0637-9			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	190VT		Bronze			2022-12-18	WOS:000322371100001
J	Daniilidis, K; Maragos, P; Paragios, N				Daniilidis, K.; Maragos, P.; Paragios, N.			Novel Representations, Methods, and Algorithms in Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Daniilidis, K.] Univ Penn, Philadelphia, PA 19104 USA; [Maragos, P.] Natl Tech Univ Athens, Athens, Greece; [Paragios, N.] Ecole Cent Paris, Paris, France	University of Pennsylvania; National Technical University of Athens; UDICE-French Research Universities; Universite Paris Saclay	Daniilidis, K (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	kostas@upenn.edu; maragos@cs.ntua.gr; nikos.paragios@ecp.fr							0	0	0	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2013	103	2			SI		177	177		10.1007/s11263-013-0628-x	http://dx.doi.org/10.1007/s11263-013-0628-x			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	150JC		Bronze			2022-12-18	WOS:000319385400001
J	Godin, G; Goesele, M; Matsushita, Y; Sagawa, R; Yang, RG				Godin, Guy; Goesele, Michael; Matsushita, Yasuyuki; Sagawa, Ryusuke; Yang, Ruigang			Guest Editorial: 3D Imaging, Processing and Modelling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Godin, Guy] NRC, Ottawa, ON, Canada; [Goesele, Michael] Tech Univ Darmstadt, Darmstadt, Germany; [Matsushita, Yasuyuki] MSR Asia, Beijing, Peoples R China; [Sagawa, Ryusuke] AIST, Tsukuba, Ibaraki, Japan; [Yang, Ruigang] Univ Kentucky, Lexington, KY USA	National Research Council Canada; Technical University of Darmstadt; National Institute of Advanced Industrial Science & Technology (AIST); University of Kentucky	Goesele, M (corresponding author), Tech Univ Darmstadt, Darmstadt, Germany.	michael.goesele@gris.informatik.tu-darmstadt.de		Yang, Ruigang/0000-0001-5296-6307; Goesele, Michael/0000-0002-0944-0980; Matsushita, Yasuyui/0000-0002-1935-4752					0	0	0	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					1	2		10.1007/s11263-012-0604-x	http://dx.doi.org/10.1007/s11263-012-0604-x			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO		Bronze			2022-12-18	WOS:000315501800001
J	Malleson, C; Collomosse, J				Malleson, Charles; Collomosse, John			Virtual Volumetric Graphics on Commodity Displays Using 3D Viewer Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Volumetric display; 3D graphics; Tracking; Kalman filter; Camera Calibration	PANEL	Three dimensional (3D) displays typically rely on stereo disparity, requiring specialized hardware to be worn or embedded in the display. We present a novel 3D graphics display system for volumetric scene visualization using only standard 2D display hardware and a pair of calibrated web cameras. Our computer vision-based system requires no worn or other special hardware. Rather than producing the depth illusion through disparity, we deliver a full volumetric 3D visualization-enabling users to interactively explore 3D scenes by varying their viewing position and angle according to the tracked 3D position of their face and eyes. We incorporate a novel wand-based calibration that allows the cameras to be placed at arbitrary positions and orientations relative to the display. The resulting system operates at real-time speeds (similar to 25 fps) with low latency (120-225 ms) delivering a compelling natural user interface and immersive experience for 3D viewing. In addition to objective evaluation of display stability and responsiveness, we report on user trials comparing users' timings on a spatial orientation task.	[Malleson, Charles; Collomosse, John] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 5XH, Surrey, England	University of Surrey	Collomosse, J (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 5XH, Surrey, England.	C.Malleson@surrey.ac.uk; J.Collomosse@surrey.ac.uk						Alnowami M., 2011, P SPIE MED IMAGING; Brar RS, 2010, J DISP TECHNOL, V6, P531, DOI 10.1109/JDT.2010.2044367; Chen CH, 2009, APPL OPTICS, V48, P3446, DOI 10.1364/AO.48.003446; Dang T, 2009, IEEE T IMAGE PROCESS, V18, P1536, DOI 10.1109/TIP.2009.2017824; Dodgson NA, 2004, PROC SPIE, V5291, P36, DOI 10.1117/12.529999; Ellis S. R., 2002, Proceedings of the Human Factors and Ergonomics Society 46th Annual Meeting, P2149; Erden E, 2009, IEEE LEOS ANN MTG, P10, DOI 10.1109/LEOS.2009.5343494; EZRA D, 1995, P SOC PHOTO-OPT INS, V2409, P31, DOI 10.1117/12.205873; Free2C, 2010, TECHNICAL REPORT; Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771; LEE J., 2008, TECHNICAL REPORT; Malleson C., 2011, P ICCV WORKSH HUM CO; Nishimura H, 2007, PROC SPIE, V6486, DOI 10.1117/12.703087; Perlin K, 2001, P SOC PHOTO-OPT INS, V4297, P196, DOI 10.1117/12.430851; Sandin DJ, 2001, P SOC PHOTO-OPT INS, V4297, P204, DOI 10.1117/12.430818; Schwartz A., 1985, Conference Record of the 1985 International Display Research Conference (Cat. No. 85CH2239-2), P141; Sorensen SEB, 2004, US Patent, Patent No. 6687003; Surman Phil, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P161, DOI 10.1109/3DTV.2008.4547833; Surman P, 2008, J SOC INF DISPLAY, V16, P743, DOI 10.1889/1.2953481; Takaki Y, 2006, P IEEE, V94, P654, DOI 10.1109/JPROC.2006.870684; TETSUTANI N, 1994, P SOC PHOTO-OPT INS, V2177, P135, DOI 10.1117/12.173868; Tetsutani N., 1989, JAPAN DISPLAY, p[56, 2]; Thacker N. A., 1992, BMVC92. Proceedings of the British Machine Vision Conference, P528; Tsai R.-Y., 2009, P SPIE INT SOC OPTIC, V7329; Urey H, 2011, P IEEE, V99, P540, DOI 10.1109/JPROC.2010.2098351; Viola P, 2001, P COMP VIS PATT REC; WELCH BL, 1947, BIOMETRIKA, V34, P28, DOI 10.1093/biomet/34.1-2.28; Woodgate GJ, 1997, P SOC PHOTO-OPT INS, V3012, P187, DOI 10.1117/12.274457; Woodgate GJ, 2000, P SOC PHOTO-OPT INS, V3957, P153, DOI 10.1117/12.384438; Woods Andrew, 2009, Information Display, V25, P8; Yamamoto H, 2002, APPL OPTICS, V41, P6907, DOI 10.1364/AO.41.006907	31	0	0	0	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2013	101	3			SI		519	532		10.1007/s11263-012-0533-8	http://dx.doi.org/10.1007/s11263-012-0533-8			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	086WF		Green Submitted			2022-12-18	WOS:000314719000008
J	Labrosse, F; Zwiggelaar, R; Liu, YH; Tiddeman, B				Labrosse, Frederic; Zwiggelaar, Reyer; Liu, Yonghuai; Tiddeman, Bernie			Guest Editorial: Scenes, Images and Objects	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Labrosse, Frederic; Zwiggelaar, Reyer; Liu, Yonghuai; Tiddeman, Bernie] Aberystwyth Univ, Dept Comp Sci, Aberystwyth, Dyfed, Wales	Aberystwyth University	Zwiggelaar, R (corresponding author), Aberystwyth Univ, Dept Comp Sci, Aberystwyth, Dyfed, Wales.	rrz@aber.ac.uk	Zwiggelaar, Reyer/HGA-3089-2022; Liu, Yonghuai/ABF-3794-2020						0	0	0	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2012	100	2					121	121		10.1007/s11263-012-0562-3	http://dx.doi.org/10.1007/s11263-012-0562-3			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	000CQ		Bronze			2022-12-18	WOS:000308364500001
J	Blaschko, MB; Lampert, CH				Blaschko, Matthew B.; Lampert, Christoph H.			Guest Editorial: Special Issue on Structured Prediction and Inference	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Blaschko, Matthew B.] Ecole Cent Paris, F-92295 Chatenay Malabry, France; [Lampert, Christoph H.] IST Austria, A-3400 Klosterneuburg, Austria	UDICE-French Research Universities; Universite Paris Saclay; Institute of Science & Technology - Austria	Blaschko, MB (corresponding author), Ecole Cent Paris, Grande Voie Vignes, F-92295 Chatenay Malabry, France.	matthew.blaschko@inria.fr; chl@ist.ac.at	; Lampert, Christoph H./J-2931-2014	Blaschko, Matthew/0000-0002-2640-181X; Lampert, Christoph H./0000-0001-8622-7887				Andriluka M, 2012, INT J COMPUT VISION, V99, P259, DOI 10.1007/s11263-011-0498-z; BakIr G., 2007, PREDICTING STRUCTURE; Binder A, 2012, INT J COMPUT VISION, V99, P281, DOI 10.1007/s11263-010-0417-8; Miao X, 2012, INT J COMPUT VISION, V99, P302, DOI 10.1007/s11263-011-0423-5; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Pal CJ, 2012, INT J COMPUT VISION, V99, P319, DOI 10.1007/s11263-010-0385-z; Scholkopf B., 2001, LEARNING KERNELS SUP; Taskar B., 2004, ADV NEURAL INFORM PR, P16; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Vapnik V.N, 1998, STAT LEARNING THEORY	11	0	0	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2012	99	3					257	258		10.1007/s11263-012-0530-y	http://dx.doi.org/10.1007/s11263-012-0530-y			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	950NI		Green Accepted, Bronze			2022-12-18	WOS:000304655600001
J	Bartoli, A; Magnor, M; Fisher, B; Theobalt, C				Bartoli, Adrien; Magnor, Marcus; Fisher, Bob; Theobalt, Christian			Editorial for the Special Issue on 3D Data Processing, Visualization and Transmission	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Bartoli, Adrien] Univ Auvergne, ISIT, Clermont Ferrand, France; [Magnor, Marcus] TU Braunschweig, Braunschweig, Germany; [Fisher, Bob] Univ Edinburgh, Edinburgh, Midlothian, Scotland; [Theobalt, Christian] MPI Informat, Saarbrucken, Germany	Universite Clermont Auvergne (UCA); Braunschweig University of Technology; University of Edinburgh; Max Planck Society	Bartoli, A (corresponding author), Univ Auvergne, ISIT, Clermont Ferrand, France.	adrien.bartoli@gmail.com; m.magnor@tu-bs.de; rbf@inf.ed.ac.uk; theobalt@mpii.de		Theobalt, Christian/0000-0001-6104-6625					0	0	0	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2012	97	1					1	1		10.1007/s11263-011-0513-4	http://dx.doi.org/10.1007/s11263-011-0513-4			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	897SD		Bronze			2022-12-18	WOS:000300675300001
J	Triggs, B; Williams, CKI				Triggs, Bill; Williams, Christopher K. I.			Special Issue on Probabilistic Models for Image Understanding, Part II	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Triggs, Bill] Lab Jean Kuntzmann, Grenoble, France; [Williams, Christopher K. I.] Univ Edinburgh, Edinburgh, Midlothian, Scotland	UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Inria; University of Edinburgh	Triggs, B (corresponding author), Lab Jean Kuntzmann, Grenoble, France.	bill.triggs@imag.fr; ckiw@inf.ed.ac.uk						Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Guan L, 2010, INT J COMPUT VISION, V90, P283, DOI 10.1007/s11263-010-0341-y; Kapoor A, 2010, INT J COMPUT VISION, V88, P169, DOI 10.1007/s11263-009-0268-3; Larlus D, 2010, INT J COMPUT VISION, V88, P238, DOI 10.1007/s11263-009-0245-x; Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6; Porway J, 2010, INT J COMPUT VISION, V88, P254, DOI 10.1007/s11263-009-0306-1; Ross DA, 2010, INT J COMPUT VISION, V88, P214, DOI 10.1007/s11263-010-0325-y; Shi QF, 2011, INT J COMPUT VISION, V93, P22, DOI 10.1007/s11263-010-0384-0; Triggs B, 2010, INT J COMPUT VISION, V88, P145, DOI 10.1007/s11263-010-0328-8; Tuytelaars T, 2010, INT J COMPUT VISION, V88, P284, DOI 10.1007/s11263-009-0271-8; Vidal C, 2010, INT J COMPUT VISION, V88, P189, DOI 10.1007/s11263-009-0258-5; Zhu L, 2011, INT J COMPUT VISION, V93, P1, DOI 10.1007/s11263-010-0375-1	12	0	0	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2011	95	3					313	314		10.1007/s11263-011-0455-x	http://dx.doi.org/10.1007/s11263-011-0455-x			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	913AD					2022-12-18	WOS:000301842800005
J	Wang, CW; Hunter, A				Wang, Ching-Wei; Hunter, Andrew			Robust Pose Recognition of the Obscured Human Body (vol 90, pg 313, 2010)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Wang, Ching-Wei] Natl Taiwan Univ Sci & Technol, Grad Inst Biomed Engn, Taipei, Taiwan; [Hunter, Andrew] Univ Lincoln, Lincoln LN6 7TS, England	National Taiwan University of Science & Technology; University of Lincoln	Wang, CW (corresponding author), Natl Taiwan Univ Sci & Technol, Grad Inst Biomed Engn, Taipei, Taiwan.	cweiwang@ieee.org	Wang, Ching-Wei/GXV-5212-2022	Wang, Ching-Wei/0000-0001-9992-6863				GAVRILA D, 1998, IEEE INT C PATT REC; Wang CW, 2010, INT J COMPUT VISION, V90, P313, DOI 10.1007/s11263-010-0365-3	2	0	0	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2011	94	3					375	375		10.1007/s11263-011-0440-4	http://dx.doi.org/10.1007/s11263-011-0440-4			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816AC		Bronze			2022-12-18	WOS:000294570100007
J	Todorovic, S; Chellappa, R				Todorovic, Sinisa; Chellappa, Rama			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Todorovic, Sinisa] Oregon State Univ, Corvallis, OR 97331 USA; [Chellappa, Rama] Univ Maryland, College Pk, MD 20742 USA	Oregon State University; University System of Maryland; University of Maryland College Park	Todorovic, S (corresponding author), Oregon State Univ, Corvallis, OR 97331 USA.	sinisa@eecs.oregonstate.edu	Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAJ-1504-2020						0	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2011	93	2					115	116		10.1007/s11263-011-0420-8	http://dx.doi.org/10.1007/s11263-011-0420-8			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	740PD		Bronze			2022-12-18	WOS:000288806000001
J	Karlsson, SM; Pont, SC; Koenderink, JJ; Zisserman, A				Karlsson, Stefan M.; Pont, Sylvia C.; Koenderink, Jan J.; Zisserman, Andrew			Illuminance Flow Estimation by Regression	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Illuminance flow; Surface 3D texture; Histogram of oriented gradients; Illuminant estimation	DIRECTION; TEXTURE	We investigate the estimation of illuminance flow using Histograms of Oriented Gradient features (HOGs). In a regression setting, we found for both ridge regression and support vector machines, that the optimal solution shows close resemblance to the gradient based structure tensor (also known as the second moment matrix). Theoretical results are presented showing in detail how the structure tensor and the HOGs are connected. This relation will benefit computer vision tasks such as affine invariant texture/object matching using HOGs. Several properties of HOGs are presented, among others, how many bins are required for a directionality measure, and how to estimate HOGs through spatial averaging that requires no binning.	[Karlsson, Stefan M.] Halmstad Univ, IDE, S-30118 Halmstad, Sweden; [Pont, Sylvia C.] Delft Univ Technol, NL-2628 CE Delft, Netherlands; [Koenderink, Jan J.] Delft Univ Technol, NL-2028 CD Delft, Netherlands; [Zisserman, Andrew] Univ Oxford, Oxford OX1 3PJ, England	Halmstad University; Delft University of Technology; Delft University of Technology; University of Oxford	Karlsson, SM (corresponding author), Halmstad Univ, IDE, S-30118 Halmstad, Sweden.	Stefan.Karlsson@hh.se		Pont, Sylvia/0000-0002-9834-9600	EU [MRTN-CT-2004-005439]; Netherlands Organization for Scientific Research (NWO)	EU(European Commission); Netherlands Organization for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	Oscar van Hoof is gratefully acknowledged for providing the images used for the data collection. We thank professors Josef Bigun and Christoph Schnoerr for valuable discussions. This work has been funded by EU project VISIONTRAIN (MRTN-CT-2004-005439). Sylvia C. Pont was supported by the Netherlands Organization for Scientific Research (NWO).	AIZERMAN MA, 1965, AUTOMAT REM CONTR+, V25, P821; Bigun J., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P433; Blinn J.F., 1978, COMPUT GRAPH, DOI [10.1145/800248.507101, DOI 10.1145/965139.507101]; BROOKS MJ, 1985, P INT JOINT C ART IN, P932; Chantler M, 2005, INT J COMPUT VISION, V62, P83, DOI 10.1007/s11263-005-4636-3; Chantler MJ, 1997, IEE P-VIS IMAGE SIGN, V144, P213, DOI 10.1049/ip-vis:19971302; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146; JOACHIMS T, 1999, MAKING LARGE SCALE S; Karlsson S, 2008, J OPT SOC AM A, V25, P282, DOI 10.1364/JOSAA.25.000282; Karlsson SM, 2009, J OPT SOC AM A, V26, P1250, DOI 10.1364/JOSAA.26.001250; KNILL DC, 1990, J OPT SOC AM A, V7, P759, DOI 10.1364/JOSAA.7.000759; Koenderink JJ, 2003, J OPT SOC AM A, V20, P1875, DOI 10.1364/JOSAA.20.001875; LLADO X, 2003, BRIT MACH VIS C; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; MARDIA KV, 2000, WILEY SERIES; MICHEL S, 1996, EUSIPCO, V96, P1693; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; PENTLAND AP, 1982, J OPT SOC AM, V72, P448, DOI 10.1364/JOSA.72.000448; PICARD RW, 1995, MULTIMEDIA SYST, V3, P3, DOI 10.1007/BF01236575; Pont SC, 2005, LECT NOTES COMPUT SC, V3753, P205, DOI 10.1007/11577812_18; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vapnik VN, 2000, ADV NEUR IN, V12, P659; Varma M, 2004, PROC CVPR IEEE, P179; ZHENG Q, 1992, PHYS BASED VISION SH, P39	26	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2010	90	3					304	312		10.1007/s11263-010-0353-7	http://dx.doi.org/10.1007/s11263-010-0353-7			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	662BV		Bronze, Green Published			2022-12-18	WOS:000282782700003
J	Theoharis, T; Pratikakis, I; Spagnuolo, M				Theoharis, Theoharis; Pratikakis, Ioannis; Spagnuolo, Michela			IJCV Special Issue on 3D Object Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Pratikakis, Ioannis] Natl Ctr Sci Res Demokritos, Inst Informat & Telecommun, Athens 15310, Greece; [Theoharis, Theoharis] Univ Athens, Dept Informat & Telecommun, Athens 15784, Greece; [Spagnuolo, Michela] CNR, Ist Matemat Applicate & Tecnol Informat, I-16149 Genoa, Italy	National Centre of Scientific Research "Demokritos"; National & Kapodistrian University of Athens; Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR)	Pratikakis, I (corresponding author), Natl Ctr Sci Res Demokritos, Inst Informat & Telecommun, Athens 15310, Greece.	theotheo@di.uoa.gr; ipratika@iit.demokritos.gr; spagnuolo@ge.imati.cnr.it	PRATIKAKIS, IOANNIS/AAD-3387-2019; Spagnuolo, Michela/ABA-1927-2021; Theoharis, Theoharis/AAN-2555-2020; Spagnuolo, Michela/F-5068-2013	PRATIKAKIS, IOANNIS/0000-0002-4124-3688; Spagnuolo, Michela/0000-0002-5682-6990; Spagnuolo, Michela/0000-0002-5682-6990					0	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2010	89	2-3			SI		129	129		10.1007/s11263-009-0309-y	http://dx.doi.org/10.1007/s11263-009-0309-y			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	594OS		Bronze			2022-12-18	WOS:000277547600001
J	Abhau, J; Scherzer, O				Abhau, Jochen; Scherzer, Otmar			A Combinatorial Method for Topology Adaptations in 3D Deformable Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deformable model; Triangular mesh; Topology adaptation; Segmentation; Homology	SEGMENTATION; IMAGES	In this paper we propose an efficient algorithm for topology adaptation of evolving surface meshes in 3D. This system has two novel features: First, a spatial hashing technique is used to detect self-colliding triangles of the evolving mesh. Secondly, for the topology adaptation itself, we use formulas which are derived from homology. In view of this the advantages of our algorithm are that it does not require global mesh re-parameterizations and the topology adaptation can be performed in a stable way via a rather coarse mesh. We apply our algorithm to segmentation of three-dimensional synthetic and ultrasound data.	[Abhau, Jochen; Scherzer, Otmar] Univ Innsbruck, Dept Math, A-6020 Innsbruck, Austria; [Scherzer, Otmar] Radon Inst Computat & Appl Math, A-4040 Linz, Austria	University of Innsbruck	Abhau, J (corresponding author), Univ Innsbruck, Dept Math, Technikerstr 21A, A-6020 Innsbruck, Austria.	jochen.abhau@uibk.ac.at	Scherzer, Otmar/AAA-4132-2019	Scherzer, Otmar/0000-0001-9378-7452	Austrian Science Fund (FWF) [9203-N12]; Photoacoustic Imaging in Biology and Medicine [S10505-N20]	Austrian Science Fund (FWF)(Austrian Science Fund (FWF)); Photoacoustic Imaging in Biology and Medicine	This work has been supported by the Austrian Science Fund (FWF) within the national research networks Industrial Geometry, project 9203-N12, and Photoacoustic Imaging in Biology and Medicine, project S10505-N20. We thank GE-Medical Systems, Kretztechnik, for providing the voxel image of the cyst and Tobias Riser for his QT-Viewer, with which the example pictures of the cyst and the torus were visualized. The QT-Viewer has been developed within the TWF project Parallelisierte Datenauswertung am HPC, GZ:UNI-0404/460. We also want to thank the referees for their very useful comments.	ABHAU J, 2007, P SPIE, V6513; ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; BISCHOFF S, 2006, VISION MODELING VISU, P293; Bredno J, 2003, IEEE T PATTERN ANAL, V25, P550, DOI 10.1109/TPAMI.2003.1195990; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; CHEN Y, 1995, COMPUT VIS IMAGE UND, V61, P325, DOI 10.1006/cviu.1995.1026; DELINGETTE H, 1994, IEEE WORKSH NONR ART; Goldenberg R, 2002, IEEE T MED IMAGING, V21, P1544, DOI 10.1109/TMI.2002.806594; Hatcher A., 2005, ALGEBRAIC TOPOLOGY; Holtzman-Gazit M, 2006, IEEE T IMAGE PROCESS, V15, P354, DOI [10.1109/TIP.2005.860624, 10.1109/tip.2005.860624]; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Lachaud J O, 1999, Med Image Anal, V3, P187, DOI 10.1016/S1361-8415(99)80012-7; Lachaud JO, 2005, COMPUT VIS IMAGE UND, V99, P453, DOI 10.1016/j.cviu.2005.04.002; Lachaud JO, 2004, INT C PATT RECOG, P237, DOI 10.1109/ICPR.2004.1334140; LACHAUD JO, 2003, P 4 INT C 3D DIG IM; Lang S, 2002, ALGEBRA, VThird; LEITNER F, 1992, SPIE C MOD BAS VIS D, P16; Massey W. S., 1991, GRADUATE TEXTS MATH, V127; McInerney T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P518, DOI 10.1109/ICCV.1993.378169; McInerney T, 2000, MED IMAGE ANAL, V4, P73, DOI 10.1016/S1361-8415(00)00008-6; Moller T., 1997, J GRAPH TOOLS, V2, P25, DOI [DOI 10.1080/10867651.1997.10487472, 10.1080/10867651.1997.10487472]; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; *PARI, 2005, PARI GP VERS 2 1 7 P; Rotman J.J., 1988, INTRO ALGEBRAIC TOPO; SZELISKI R, 1993, P C COMP VIS PATT RE; TAUBIN G, 1985, COMPUTER GRAPHICS, P351; Teschner M, 2005, COMPUT GRAPH FORUM, V24, P61, DOI 10.1111/j.1467-8659.2005.00829.x; Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47; Weickert J, 2003, GEOMETRIC LEVEL SET METHODS IN IMAGING, VISION AND GRAPHICS, P43, DOI 10.1007/0-387-21810-6_3; Yan P, 2006, MED IMAGE ANAL, V10, P317, DOI 10.1016/j.media.2005.12.002	30	0	0	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	87	3					304	315		10.1007/s11263-009-0282-5	http://dx.doi.org/10.1007/s11263-009-0282-5			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	551KA					2022-12-18	WOS:000274205700006
J	Belhumeur, P; Ikeuchi, K; Prados, E; Soatto, S; Sturm, P				Belhumeur, Peter; Ikeuchi, Katsushi; Prados, Emmanuel; Soatto, Stefano; Sturm, Peter			Editorial for the Special Issue on Photometric Analysis for Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Prados, Emmanuel; Sturm, Peter] INRIA Grenoble Rhone Alpes, F-38334 Montbonnot St Martin, Saint Ismier, France; [Belhumeur, Peter] Columbia Univ, New York, NY 10027 USA; [Ikeuchi, Katsushi] Univ Tokyo, Meguro Ku, Tokyo, Japan; [Soatto, Stefano] Univ Calif Los Angeles, Los Angeles, CA USA	Columbia University; University of Tokyo; University of California System; University of California Los Angeles	Prados, E (corresponding author), INRIA Grenoble Rhone Alpes, 655 Ave Europe, F-38334 Montbonnot St Martin, Saint Ismier, France.	Emmanuel.Prados@inrialpes.fr							0	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	2-3			SI		125	126		10.1007/s11263-009-0292-3	http://dx.doi.org/10.1007/s11263-009-0292-3			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	534NA		Green Submitted, Bronze			2022-12-18	WOS:000272903200001
J	Nielsen, M; Niessen, W; Westin, CF				Nielsen, Mads; Niessen, Wiro; Westin, Carl-Fredrik			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Nielsen, Mads] Univ Copenhagen, Dept Comp Sci, DK-2100 Copenhagen, Denmark; [Niessen, Wiro] Univ Med Ctr Rotterdam, Biomed Imaging Grp Rotterdam, Erasmus MC, Dept Med Informat, NL-3000 CA Rotterdam, Netherlands; [Niessen, Wiro] Univ Med Ctr Rotterdam, Biomed Imaging Grp Rotterdam, Erasmus MC, Dept Radiol, NL-3000 CA Rotterdam, Netherlands; [Westin, Carl-Fredrik] Harvard Univ, Sch Med, Dept Radiol, Brigham & Womens Hosp, Boston, MA 02215 USA	University of Copenhagen; Erasmus University Rotterdam; Erasmus MC; Erasmus University Rotterdam; Erasmus MC; Harvard University; Brigham & Women's Hospital; Harvard Medical School	Nielsen, M (corresponding author), Univ Copenhagen, Dept Comp Sci, Univ Parken 1, DK-2100 Copenhagen, Denmark.	madsn@diku.dk; w.niessen@erasmusmc.nl; westin@bwh.harvard.edu							0	0	0	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2009	85	3					209	210		10.1007/s11263-009-0246-9	http://dx.doi.org/10.1007/s11263-009-0246-9			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	502CO					2022-12-18	WOS:000270432200001
J	Rocha, KR; Sundaramoorthi, G; Yezzi, AJ; Prince, JL				Rocha, Kelvin R.; Sundaramoorthi, Ganesh; Yezzi, Anthony J.; Prince, Jerry L.			3D Topology Preserving Flows for Viewpoint-Based Cortical Unfolding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visibility; Visibility maximization; Topology preservation; Cortex; Surface flattening; Surface	LEVEL SET METHOD	We present a variational method for unfolding of the cortex based on a user-chosen point of view as an alternative to more traditional global flattening methods, which incur more distortion around the region of interest. Our approach involves three novel contributions. The first is an energy function and its corresponding gradient flow to measure the average visibility of a region of interest of a surface with respect to a given viewpoint. The second is an additional energy function and flow designed to preserve the 3D topology of the evolving surface. The third is a method that dramatically improves the computational speed of the 3D topology preservation approach by creating a tree structure of the 3D surface and using a recursion technique. Experiments results show that the proposed approach can successfully unfold highly convoluted surfaces such as the cortex while preserving their topology during the evolution.	[Rocha, Kelvin R.; Sundaramoorthi, Ganesh; Yezzi, Anthony J.] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA; [Prince, Jerry L.] Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA	University System of Georgia; Georgia Institute of Technology; Johns Hopkins University	Rocha, KR (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.	krocha@ece.gatech.edu; ganeshs@ece.gatech.edu; ayezzi@ece.gatech.edu; prince@jhu.edu	Prince, Jerry L/A-3281-2010; Yezzi, Anthony/AAB-4235-2020	Prince, Jerry L/0000-0002-6553-0876; 	NSF [CCR-0133736]; NIH/NINDS [R01-NS-037747]; NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE [R01NS037747] Funding Source: NIH RePORTER	NSF(National Science Foundation (NSF)); NIH/NINDS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS)); NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS))	This work was supported by the grants NSF CCR-0133736 and NIH/NINDS R01-NS-037747.	Abrams A, 2003, TOPOLOGY, V42, P381, DOI 10.1016/S0040-9383(02)00016-2; Alexandrov O, 2005, J COMPUT PHYS, V204, P121, DOI 10.1016/j.jcp.2004.10.005; Angenent S, 1999, IEEE T MED IMAGING, V18, P700, DOI 10.1109/42.796283; Cantarella J., 2004, P 20 ANN ACM S COMP, P134; CARMAN GJ, 1995, CEREB CORTEX, V5, P506, DOI 10.1093/cercor/5.6.506; DURANT F, 1999, THESIS U J FOURIER G; Fischl B, 1999, NEUROIMAGE, V9, P195, DOI 10.1006/nimg.1998.0396; Han X, 2003, IEEE T PATTERN ANAL, V25, P755, DOI 10.1109/TPAMI.2003.1201824; Han X, 2001, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P213, DOI 10.1109/MMBIA.2001.991736; HERMOSILLO G, 1999, 3663 INRIA; Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074; Iben H. N., 2006, Proceedings of the Twenty-Second Annual Symposium on Computational Geometry (SCG'06), P71, DOI 10.1145/1137856.1137869; LEGUYADER C, 2007, SELF REPELLING SNAKE; OHARA J, 1991, TOPOLOGY, V30, P241, DOI 10.1016/0040-9383(91)90010-2; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; PONS JP, 2004, P INT C MED IM COMP, P376; ROCHA K, 2007, P INT C MED IM COMP, P499; SAPIRO G, 2005, PATTERN ANAL MACH IN, V17, P67; Segonne F, 2008, INT J COMPUT VISION, V79, P107, DOI 10.1007/s11263-007-0102-8; Sethian JA, 1997, IEEE T SEMICONDUCT M, V10, P167, DOI 10.1109/66.554505; Shi YG, 2004, 2004 2ND IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: MACRO TO NANO, VOLS 1 AND 2, P1247; SLABAUGH G, 2005, P IEEE COMP SOC C CO; Sundaramoorthi G, 2005, IEEE I CONF COMP VIS, P1276; Unal G, 2005, INT J COMPUT VISION, V62, P199, DOI 10.1007/s11263-005-4880-6; Wandell BA, 2000, J COGNITIVE NEUROSCI, V12, P739, DOI 10.1162/089892900562561; WITKIN A, 1997, SIGGRAPH 97 COURS NO, P1; WOO A, 1990, IEEE COMPUT GRAPH, V10, P13, DOI 10.1109/38.62693; [No title captured]	28	0	0	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2009	85	3					223	236		10.1007/s11263-009-0214-4	http://dx.doi.org/10.1007/s11263-009-0214-4			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	502CO	19960105	Green Accepted, Green Published			2022-12-18	WOS:000270432200003
J	Drouin, MA; Trudeau, M; Roy, S				Drouin, Marc-Antoine; Trudeau, Martin; Roy, Sebastien			Improving Border Localization of Multi-Baseline Stereo Using Border-Cut	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo; Occlusion; Multi-camera; Dynamic programming		This paper presents a novel algorithm that improves the localization of disparity discontinuities of disparity maps obtained by multi-baseline stereo. Rather than associating a disparity label to every pixel of a disparity map, it associates a position to every disparity discontinuity. This formulation allows us to find an approximate solution to a 2D labeling problem with robust smoothing term by minimizing multiple 1D problems, thus making possible the use of dynamic programming. Dynamic programming allows the efficient computation of the visibility of most of the cameras during the minimization. The proposed algorithm is not a stereo matcher on it own since it requires an initial disparity map. Nevertheless, it is a very effective way of improving the border localization of disparity maps obtained from a large class of stereo matchers. Whilst the proposed minimization strategy is particularly suitable for stereo with occlusion, it may be used for other labeling problems.	[Drouin, Marc-Antoine; Trudeau, Martin; Roy, Sebastien] Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Drouin, MA (corresponding author), Natl Res Council Canada, Inst Informat Technol, Ottawa, ON K1A 0R6, Canada.	marc-antoine.drouin@nrc-cnrc.gc.ca			McGill University; National Research Council Canada	McGill University; National Research Council Canada	The authors would like to thank Mike Langer from McGill University and Guy Godin from the National Research Council Canada for their suggestions. The authors would also like to thank the anonymous reviewers for their constructive comments.	AMIR RCJ, 1990, IEEE T PATTERN ANAL, V12, P855; [Anonymous], EUR C COMP VIS; Belhumeur PN, 1996, INT J COMPUT VISION, V19, P237, DOI 10.1007/BF00055146; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Bleyer M., 2004, INT C IM PROC; BOYKOV Y, 1999, ICCV, P377; Cox IJ, 1996, COMPUT VIS IMAGE UND, V63, P542, DOI 10.1006/cviu.1996.0040; Drouin MA, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P540, DOI 10.1109/3DIM.2005.40; Drouin MA, 2005, PROC CVPR IEEE, P351; Egnal G, 2002, IEEE T PATTERN ANAL, V24, P1127, DOI 10.1109/TPAMI.2002.1023808; FAUGERAS O, 1998, EUR C COMP VIS, P379; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; FUSIELLO A, 1997, IEEE C COMP VIS PATT; GOESELE M, 2006, IEEE C COMP VIS PATT, P2402; GONG M, 2003, INT C COMP VIS; ISHIKAWA H, 1998, EUR C COMP VIS, P232; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; Kang S., 2001, IEEE C COMP VIS PATT; Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; LEUNG C, 2004, BRIT MACH VIS C; NAKAMURA Y, 1996, IEEE C COMP VIS PATT; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; Osher S., 2002, APPL MATH SCI, V44, P685; Park JI, 1998, SIGNAL PROCESS-IMAGE, V14, P7, DOI 10.1016/S0923-5965(98)00025-3; Roy S, 1999, INT J COMPUT VISION, V34, P147, DOI 10.1023/A:1008192004934; SANFOURCHE M, 2004, BRIT MACH VIS C; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; SCHARSTEIN D, 2007, MIDDLEBURY STEREO VI; Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526; SEITZ SM, 2006, IEEE C COMP VIS PATT, P519; Strecha C., 2006, COMP VIS PATT REC 20, P2394, DOI [DOI 10.1109/CVPR.2006.78, 10.1109/CVPR.2006.78]; Sun J, 2005, PROC CVPR IEEE, P399; Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509; SZELISKI R, 1999, VISION ALGORITHMS TH, P1; Szeliski R., 2006, EUR C COMP VIS; VEKSLER O, 2003, IEEE C COMP VIS PATT; VEKSLER O, 2005, IEEE C COMP VIS PATT; VEKSLER O, 1999, THESIS CORNELL U; Wei YC, 2005, PROC CVPR IEEE, P902; YOON KJ, 2006, IEEE C COMP VIS PATT, V2, P2371	41	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2009	83	3					233	247		10.1007/s11263-009-0223-3	http://dx.doi.org/10.1007/s11263-009-0223-3			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	424EI					2022-12-18	WOS:000264547900002
J	Bischof, H; Leonardis, A				Bischof, Horst; Leonardis, Ales			Editorial Special Issue ECCV 2006	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Leonardis, Ales] Univ Ljubljana, Ljubljana, Slovenia; [Bischof, Horst] Graz Univ Technol, A-8010 Graz, Austria	University of Ljubljana; Graz University of Technology	Leonardis, A (corresponding author), Univ Ljubljana, Ljubljana, Slovenia.	alesl@fri.uni-lj.si		Bischof, Horst/0000-0002-9096-6671					0	0	0	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2009	81	1					1	1		10.1007/s11263-008-0177-x	http://dx.doi.org/10.1007/s11263-008-0177-x			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	387RI					2022-12-18	WOS:000261968900001
J	Arnaud, E; Memin, E				Arnaud, Elise; Memin, Etienne			Partial linear Gaussian models for tracking in image sequences using sequential Monte Carlo methods (vol 74, pg 75, 2007)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Arnaud, Elise] Univ Grenoble 1, INRIA Phone Alpes, F-38330 Montbonnot St Martin, France; [Memin, Etienne] Univ Rennes 1, F-35042 Rennes, France	UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Universite Grenoble Alpes (UGA); Universite de Rennes	Arnaud, E (corresponding author), Univ Grenoble 1, INRIA Phone Alpes, F-38330 Montbonnot St Martin, France.	elarnaud@inrialpes.fr; memin@irisa.fr						Arnaud E, 2005, IEEE T IMAGE PROCESS, V14, P63, DOI 10.1109/TIP.2004.838707; Arnaud E, 2007, INT J COMPUT VISION, V74, P75, DOI 10.1007/s11263-006-0003-2	2	0	0	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2008	80	1					166	166		10.1007/s11263-008-0150-8	http://dx.doi.org/10.1007/s11263-008-0150-8			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	344CB		Green Submitted, Bronze			2022-12-18	WOS:000258901900011
J	Fitzgibbon, A; Taylor, CJ; LeCun, Y				Fitzgibbon, Andrew; Taylor, Camillo J.; LeCun, Yann			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Fitzgibbon, Andrew] Microsoft Res Ltd, Cambridge, England; [Taylor, Camillo J.] Univ Penn, Dept Comp & Informat Sci, Grasp Lab, Philadelphia, PA 19104 USA; [LeCun, Yann] NYU, Courant Inst Math Sci, New York, NY 10003 USA	Microsoft; University of Pennsylvania; New York University	Fitzgibbon, A (corresponding author), Microsoft Res Ltd, Cambridge, England.	awf@microsoft.com						HANSON A, 1987, ADV COMPUTER VISION; Torralba A, 2004, PROC CVPR IEEE, P762	2	0	0	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2008	80	1					1	2		10.1007/s11263-008-0162-4	http://dx.doi.org/10.1007/s11263-008-0162-4			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	344CB					2022-12-18	WOS:000258901900001
J	Schnorr, C				Schnorr, Christoph			Editorial: Marr Prize and honorable mentions at ICCV 2005	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material												schnoerr@uni-mannheim.de							0	0	0	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2007	74	1					1	1		10.1007/s11263-007-0040-5	http://dx.doi.org/10.1007/s11263-007-0040-5			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	165PX					2022-12-18	WOS:000246318800001
J	Williams, J; Navab, N; Paragios, N				Williams, James; Navab, Nassir; Paragios, Nikos			Editorial: Special issue on vision and medical imaging activities at siemens corporate research	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Tech Univ Munich, Dept Informat, D-8000 Munich, Germany	Technical University of Munich		jimwilliams@siemens.com; navab@in.tum.de; nikos.paragios@ecp.fr							0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2006	70	2					107	108		10.1007/s11263-006-7939-0	http://dx.doi.org/10.1007/s11263-006-7939-0			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	076SO					2022-12-18	WOS:000239978200001
J	Bobick, A; Chellappa, R; Davis, L				Bobick, Aaron; Chellappa, Rama; Davis, Larry			Special issue: Computer Vision and Pattern Recognition-CVPR 2004	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material													Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAV-8690-2020						0	0	0	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2006	70	1					5	6		10.1007/s11263-006-8887-4	http://dx.doi.org/10.1007/s11263-006-8887-4			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	074QY					2022-12-18	WOS:000239828100001
J	Paragios, N; Faugeras, O				Paragios, Nikos; Faugeras, Olivier			Special issue: Variational geometric and LS methods (VLSM 2003)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									Ecole Cent Paris, Appl Math & Syst Lab, MAS, Paris, France	UDICE-French Research Universities; Universite Paris Saclay	Paragios, N (corresponding author), Ecole Cent Paris, Appl Math & Syst Lab, MAS, Paris, France.	nikos.paragios@ecp.fr; olivier.faugeras@inria.fr						Paragios N., 2005, HDB MATH MODELS COMP	1	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	1					5	6		10.1007/s11263-006-6860-x	http://dx.doi.org/10.1007/s11263-006-6860-x			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	063QH					2022-12-18	WOS:000239034100001
J	Fitzgibbon, AW; Pollefeys, M; Van Gool, L; Zisserman, A				Fitzgibbon, Andrew W.; Pollefeys, Marc; Van Gool, Luc; Zisserman, Andrew			Editorial: IJCV special issue: Vision and modelling of dynamic scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material												awf@microsoft.com; marc@cs.unc.edu; luc.vangool@esat.kuleuven.ac.be; az@robots.ox.ac.uk	Pollefeys, Marc/I-7607-2013						0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2006	68	1					5	6		10.1007/s11263-005-4838-8	http://dx.doi.org/10.1007/s11263-005-4838-8			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	052JB					2022-12-18	WOS:000238228900001
J	Pajdla, T; Matas, J				Pajdla, T; Matas, J			Editorial: Selection of papers for the ECCV 2004 special issue	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material												pajdla@cmp.felk.cvut.cz	Pajdla, Tomas/K-7954-2013; , Matas/AAW-3282-2020	Pajdla, Tomas/0000-0001-6325-0072; 					0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2006	67	2					139	139		10.1007/s11263-005-4297-2	http://dx.doi.org/10.1007/s11263-005-4297-2			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	048ZO					2022-12-18	WOS:000237985300001
J	Penne, R				Penne, R			The characterization of sufficient visibility in the direct reference plane approach for multiple views with missing data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						multi-view geometry; reference plane; matroid theory		Rother and Carlsson introduced the Direct Reference Plane method for obtaining a projective reconstruction from a multi-view system, presuming the availability of a planar reference quadrangle in each view. In this method feature points and camera centres are obtained simultaneously by solving a system of homogeneous linear equations. Each feature-camera visibility pair corresponds to two equations in this system. In this paper we derive a count criterion for characterizing sufficient visibility, yielding a system of linearly independent equations that "determines" feature points and camera centres. Finally we discuss some principles to extend the configuration of feature points and camera centres while maintaining sufficient visibility.	Karel Grote Hogesch, Dept Engn, Antwerp, Belgium		Penne, R (corresponding author), Karel Grote Hogesch, Dept Engn, Antwerp, Belgium.	rudi.penne@kdg.be						CARLSSON S, 1995, IEEE WORKSH REPR VIS; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; IRANI M, 1998, EUR C COMP VIS FREIB, V2, P829; Nash-Williams C.St.J.A., 1961, J LOND MATH SOC, V36, P445, DOI DOI 10.1112/JLMS/S1-36.1.445; Rother C, 2002, INT J COMPUT VISION, V49, P117, DOI 10.1023/A:1020189404787; ROTHER C, 2003, THESIS KTH STOCKHOLM; Tutte W.T., 1961, J LOND MATH SOC, V36, P221, DOI DOI 10.1112/JLMS/S1-36.1.221; Welsh DJA, 1976, MATROID THEORY; WHITE N, 1987, SIAM J ALGEBRA DISCR, V8, P1, DOI 10.1137/0608001; White N., 1986, THEORY MATROIDS; WHITELEY W, 1987, PARALLEL REDRAWING C; Whiteley W., 1988, SIAM J DISCRETE MATH, V1, P237, DOI DOI 10.1137/0401025	12	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2006	67	1					5	20		10.1007/s11263-006-4330-0	http://dx.doi.org/10.1007/s11263-006-4330-0			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	037BS					2022-12-18	WOS:000237122300001
J	Zhang, ZY				Zhang, ZY			Special issue: Best of the Sixth Asian Conference on Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material												zhang@microsoft.com	zhang, zheng/HCH-9684-2022						0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2006	66	2					107	107		10.1007/s11263-005-4000-7	http://dx.doi.org/10.1007/s11263-005-4000-7			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	022CY					2022-12-18	WOS:000236033500001
J	Griffin, L				Griffin, L			Special Issue on Scale Space	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material													Griffin, Lewis/C-2118-2008						0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2005	64	2-3					95	96		10.1007/s11263-005-1836-9	http://dx.doi.org/10.1007/s11263-005-1836-9			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	961YC					2022-12-18	WOS:000231696700001
J	Makram-Ebeid, S; Mory, B				Makram-Ebeid, S; Mory, B			Scale-space image analysis based on hermite polynomials theory	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	4th International Conference on Scale Space Methods in Computer Vision	JUN 10-12, 2003	ISLE SKYE, SCOTLAND	British Machine Vis Assoc, Kings Coll London, IT Univ Copenhagen		hermite transform; anisotropic scale-space; pattern matching; affine registration; Gaussian windowed correlation; local correlation		The Hermite transform allows to locally approximate an image by a linear combination of polynomials. For a given scale or and position xi, the polynomial coefficients are closely related to the differential jet (set of partial derivatives of the blurred image) for the same scale and position. By making use of a classical formula due to Mehler (late 19th century), we establish a linear relationship linking the differential jets at two different scales a and positions involving Hermite polynomials. For multi-dimensional images, anisotropic excursions in scale-space can be handled in this way. Pattern registration and matching applications are suggested. We introduce a Gaussian windowed correlation function K (v) for locally matching two images. When taking the mutual translation parameter v as an independent variable, we express the Hermite coefficients of K (v) in terms of the Hermite coefficients of the two images being matched. This new result bears similarity with the Wiener-Khinchin theorem which links the Fourier transform of the conventional (flat-windowed) correlation function with the Fourier spectra of the images being correlated. Compared to the conventional correlation function, ours is more suited for matching localized image features. Numerical simulations using 2D test images illustrate the potentials of our proposals for signal and image matching in terms of accuracy and algorithmic complexity.	Philips Med Syst Res Paris, F-92156 Suresnes, France	Philips; Philips Healthcare	Makram-Ebeid, S (corresponding author), Philips Med Syst Res Paris, 521 Rue Carnot,BP 301, F-92156 Suresnes, France.	sherif.makram-ebeid@philips.com; benoit.mory@philips.com	Mory, Benoit/AAZ-3859-2020					Arfken G. B., 2012, MATH METHODS PHYS; Bracewell R. N, 1999, FOURIER TRANSFORM IT; Daubechies I., 1992, 10 LECT WAVELETS, DOI [10.1137/1.9781611970104.ch1, DOI 10.1137/1.9781611970104.CH1]; DENBRINKER AC, 1993, IEEE T SIGNAL PROCES, V41, P1980, DOI 10.1109/78.215320; DUITS R, 2002, P 4 IASTED INT C SIG, P12; Dunkl C.F., 2001, ENCYCL MATH, V81; Florack L, 1996, INT J COMPUT VISION, V18, P61, DOI 10.1007/BF00126140; GALLIER J, 2000, GEOMETRIC METHODS AP; Jacovitti G, 2000, IEEE T SIGNAL PROCES, V48, P3242, DOI 10.1109/78.875481; KOENDERINK JJ, 1992, IEEE T PATTERN ANAL, V14, P597, DOI 10.1109/34.141551; Kruger V, 2002, IMAGE VISION COMPUT, V20, P665, DOI 10.1016/S0262-8856(02)00056-2; Lee TS, 1996, IEEE T PATTERN ANAL, V18, P959, DOI 10.1109/34.541406; Mallat S., 1999, WAVELET TOUR SIGNAL; Martens JB, 1997, IEEE T IMAGE PROCESS, V6, P1103, DOI 10.1109/83.605408; MARTENS JB, 1992, P SOC PHOTO-OPT INS, V1666, P276, DOI 10.1117/12.135974; SCHRODER H, 2000, 1 MULTIDIMENSIONAL S; Spain B., 1960, TENSOR CALCULUS; Szego G., 1967, ORTHOGONAL POLYNOMIA, V23; Watson G., 1933, J LOND MATH SOC, V1, P194, DOI 10.1112/jlms/s1-8.3.194	19	0	0	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2005	64	2-3					125	141		10.1007/s11263-005-1839-6	http://dx.doi.org/10.1007/s11263-005-1839-6			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	961YC					2022-12-18	WOS:000231696700004
J	Triggs, B				Triggs, B			Editorial: Selection of Marr Prize papers at ICCV'03	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2005	63	2					111	111		10.1007/s11263-005-6641-y	http://dx.doi.org/10.1007/s11263-005-6641-y			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	925JK					2022-12-18	WOS:000229049200001
J	Faugeras, O				Faugeras, O			In memory of Hugh Christopher Longuet-Higgins	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Biographical-Item																		LONGUETHIGGIN HC, PUBLICATION LIST; MAYBANK SJ, 1987, THESIS BIRBECK COLL	2	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2004	60	1					1	2		10.1023/B:VISI.0000036298.14774.45	http://dx.doi.org/10.1023/B:VISI.0000036298.14774.45			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	820LW					2022-12-18	WOS:000221391600002
J	Ponce, J				Ponce, J			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2004	60	1					V	V		10.1023/B:VISI.0000036297.44375.61	http://dx.doi.org/10.1023/B:VISI.0000036297.44375.61			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	820LW					2022-12-18	WOS:000221391600001
J	Anandan, P; Blake, A				Anandan, P; Blake, A			Computer vision research at Microsoft Corporation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2004	58	2					91	92		10.1023/B:VISI.0000015955.07665.db	http://dx.doi.org/10.1023/B:VISI.0000015955.07665.db			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	829FY					2022-12-18	WOS:000222034000001
J	Yagi, Y; Ikeuchi, K				Yagi, Y; Ikeuchi, K			Research in Japan on omni-directional sensors and their applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Osaka Univ, Suita, Osaka 565, Japan; Univ Tokyo, Tokyo, Japan	Osaka University; University of Tokyo	Yagi, Y (corresponding author), Osaka Univ, Suita, Osaka 565, Japan.							Ishiguro H., 1990, Proceedings. IROS '90. IEEE International Workshop on Intelligent Robots and Systems '90. Towards a New Frontier of Applications (Cat. No.90TH0332-7), P659, DOI 10.1109/IROS.1990.262478; Morita T., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P422, DOI 10.1109/CVPR.1989.37881; Yagi Y., 1990, Proceedings. IROS '90. IEEE International Workshop on Intelligent Robots and Systems '90. Towards a New Frontier of Applications (Cat. No.90TH0332-7), P181, DOI 10.1109/IROS.1990.262385; Zheng J. Y., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P1154, DOI 10.1109/ROBOT.1990.126152; Zheng J. Y., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P161, DOI 10.1109/ICPR.1990.118082	5	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	2004	58	3					171	172		10.1023/B:VISI.0000019713.16046.15	http://dx.doi.org/10.1023/B:VISI.0000019713.16046.15			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	836GD					2022-12-18	WOS:000222542900001
J	Ponce, J				Ponce, J			Guest editorial: Computer vision research at the Beckman Institute for Advanced Science and Technology	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2004	58	1					5	5		10.1023/B:VISI.0000016219.11962.5a	http://dx.doi.org/10.1023/B:VISI.0000016219.11962.5a			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	823OD					2022-12-18	WOS:000221621600001
J	Kerckhove, M				Kerckhove, M			Special issue on scale-space and morphology	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Univ Richmond, Richmond, VA 23173 USA	University of Richmond	Kerckhove, M (corresponding author), Univ Richmond, Richmond, VA 23173 USA.	mkerckho@richmond.edu							0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY-JUN	2003	52	2-3					71	72		10.1023/A:1022938205692	http://dx.doi.org/10.1023/A:1022938205692			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	659EL					2022-12-18	WOS:000181764500001
J	Kutulakos, KN; Shashua, A				Kutulakos, KN; Shashua, A			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Univ Toronto, Dept Comp Sci, Toronto, ON, Canada; Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91905 Jerusalem, Israel	University of Toronto; Hebrew University of Jerusalem	Kutulakos, KN (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.								0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP-OCT	2002	49	2-3					99	100		10.1023/A:1020174103878	http://dx.doi.org/10.1023/A:1020174103878			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	590TA					2022-12-18	WOS:000177837100001
J	Faugeras, O				Faugeras, O			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2002	48	1					5	6		10.1023/A:1014861510176	http://dx.doi.org/10.1023/A:1014861510176			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	534UM					2022-12-18	WOS:000174606100001
J	Horaud, R				Horaud, R			Selection of Marr prize and honourable mentions at ICCV'01	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material													Horaud, Radu/AAR-5982-2021	Horaud, Radu/0000-0001-5232-024X					0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2002	48	1					7	8		10.1023/A:1014807413211	http://dx.doi.org/10.1023/A:1014807413211			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	534UM					2022-12-18	WOS:000174606100002
J	Bradski, G; Boult, TE				Bradski, G; Boult, TE			Guest editorial: Stereo and multi-baseline vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Intel Labs, Santa Clara, CA 95052 USA; Lehigh Univ, Bethlehem, PA 18015 USA	Intel Corporation; Lehigh University	Bradski, G (corresponding author), Intel Labs, Santa Clara, CA 95052 USA.		Boult, Terrance E./AAT-2134-2021	Boult, Terrance E./0000-0001-5007-2529					0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					5	5		10.1023/A:1014590903139	http://dx.doi.org/10.1023/A:1014590903139			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN					2022-12-18	WOS:000174354700001
J	Cheng, YQ; Collins, RT; Wang, XG; Riseman, EM; Hanson, AR				Cheng, YQ; Collins, RT; Wang, XG; Riseman, EM; Hanson, AR			Three-dimensional reconstruction of points and lines with unknown correspondence across images (vol 45, pg 129, 2001)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									Village Networks Inc, Hazlet, NJ 07730 USA; Carnegie Mellon Univ, NSH, Inst Robot, Pittsburgh, PA 15213 USA; Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA	Carnegie Mellon University; University of Massachusetts System; University of Massachusetts Amherst	Cheng, YQ (corresponding author), Village Networks Inc, 100 Village Court, Hazlet, NJ 07730 USA.							Cheng YQ, 2001, INT J COMPUT VISION, V45, P129, DOI 10.1023/A:1012424014764	1	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					289	289		10.1023/A:1014566513133	http://dx.doi.org/10.1023/A:1014566513133			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN		Bronze			2022-12-18	WOS:000174354700022
J	Clark, JJ				Clark, JJ			Untitled - Introduction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									McGill Univ, Montreal, PQ H3A 2T5, Canada	McGill University	Clark, JJ (corresponding author), McGill Univ, Montreal, PQ H3A 2T5, Canada.								0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2001	43	3					139	140		10.1023/A:1011103828798	http://dx.doi.org/10.1023/A:1011103828798			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	457WY					2022-12-18	WOS:000170161400001
J	Clark, JJ; Wang, L				Clark, JJ; Wang, L			Active shape-from-shadows with controlled illuminant trajectories	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						active vision; shape-from-shadows; kalman filter	RECOGNITION; SYSTEMS; VISION	We present an active vision algorithm for computing the orientation and position of a locally planar object, onto which is cast a shadow of the edge of a half-plane at an unknown location. This algorithm utilises active position control of a point light source, and employs a Kalman filter to perform temporal integration of measurements. The light source position is adjusted after each measurement so as to reduce the trace of the expected state estimate error covariance matrix for the next measurement. We demonstrate the active shape-from-shadows algorithm using a real robotic system.	McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2A7, Canada; Washington Univ, Sch Med, Dept Psychiat, St Louis, MO 63110 USA	McGill University; Washington University (WUSTL)	Clark, JJ (corresponding author), McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2A7, Canada.	clark@cim.mcgill.ca	Wang, Lei/I-7552-2013	Wang, Lei/0000-0003-3870-3388; Clark, James/0000-0002-4512-6171				ALOIMONOS J, 1988, INT J COMPUT VISION, V2, P171, DOI 10.1007/BF00133699; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; CHAUMETTE F, 1991, P 11 INT C PATT REC; Clark J. J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P29, DOI 10.1109/CVPR.1992.223231; Clark J.J., 1990, DATA FUSION SENSORY; Clark JJ, 1997, IEEE INT CONF ROBOT, P431, DOI 10.1109/ROBOT.1997.620075; DURRANTWHITE HF, 1988, INTEGRATION COORDINA; Faugeras O. D., 1986, Proceedings 1986 IEEE International Conference on Robotics and Automation (Cat. No.86CH2282-2), P1433; Gelb A., APPL OPTIMAL ESTIMAT; HARRIS C, 1992, ACTIVE VISION; Kender J. R., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P539; KIRLIN RL, 1986, IEEE T ACOUST SPEECH, V34, P252, DOI 10.1109/TASSP.1986.1164827; LANGER MS, 1995, P INT ROB S IROS; MARTINEZ JM, 1993, P 1993 IEEE INT C SY, P517; MATTHIES L, 1989, INT J COMPUT VISION, V3, P209, DOI 10.1007/BF00133032; MEIER L, 1967, IEEE T AUTOMAT CONTR, VAC12, P528, DOI 10.1109/TAC.1967.1098668; OSHIMA M, 1983, IEEE T PATTERN ANAL, V5, P353, DOI 10.1109/TPAMI.1983.4767405; RAVIV D, 1989, IEEE T ROBOTIC AUTOM, V5, P701, DOI 10.1109/70.88087; SHIRAI Y, 1972, PATTERN RECOGN, V4, P243, DOI 10.1016/0031-3203(72)90003-9; Shmuel A., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P48, DOI 10.1109/ICPR.1990.118063; TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P86, DOI 10.1109/70.345940; Terzopoulos D., 1992, ACTIVE VISION; WALTZ D, 1983, PSYCHOL COMPUTER VIS, P19; WANG L, 1995, THESIS HARVARD U; WHAITE P, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P339, DOI 10.1109/CVPR.1994.323849; Wu WR, 1996, IEEE T SIGNAL PROCES, V44, P1454, DOI 10.1109/78.506611	26	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2001	43	3					141	166		10.1023/A:1011131412869	http://dx.doi.org/10.1023/A:1011131412869			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	457WY					2022-12-18	WOS:000170161400002
J	Castrillon, M; Valdes, A				Castrillon, M; Valdes, A			Projective evolution of plane curves	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						evolution operators; heat flows; projective evolution; scale-spaces	HEAT-EQUATION; INVARIANT; FLOWS	We show that projectively invariant evolution operators have unavoidable singularities. In particular, we see that there exists no non-singular projective evolution operator well-defined over straight lines nor conics.	Univ Complutense Madrid, Fac Ciencias Matemat, Dept Geometria & Topol, Madrid 28040, Spain	Complutense University of Madrid	Castrillon, M (corresponding author), Univ Complutense Madrid, Fac Ciencias Matemat, Dept Geometria & Topol, Madrid 28040, Spain.			Valdes, Antonio/0000-0001-5930-8307				ALVAREZ L, 1993, ARCH RATIONAL MECH, V123; Angenent S, 1998, J AM MATH SOC, V11, P601, DOI 10.1090/S0894-0347-98-00262-8; ASTROM K, 1995, IEEE T PATTERN ANAL, V17, P77, DOI 10.1109/34.368148; Calabi E, 1998, INT J COMPUT VISION, V26, P107, DOI 10.1023/A:1007992709392; Calabi E, 1996, ADV MATH, V124, P154, DOI 10.1006/aima.1996.0081; Cartan, 1937, LECONS THEORIE ESPAC; Dibos F, 1998, IEEE T IMAGE PROCESS, V7, P274, DOI 10.1109/83.661177; Epstein C L, 1987, WAVE MOTION THEORY M; FAUGERAS O, 1995, PROCEEDINGS OF EUROPE-CHINA WORKSHOP ON GEOMETRICAL MODELING & INVARIANTS FOR COMPUTER VISION, P17; Faugeras O, 1994, LECT NOTES COMPUTER, V825, P11; FAUGERAS O, 1996, 12 INT C AN OPT SYST; FAUGERAS O, 1995, P IEEE INT C IM PROC, V3, P13; FAUGERAS O, 1995, 9533 CEREMADE; GAGE M, 1986, J DIFFER GEOM, V23, P69; GRAYSON MA, 1989, ANN MATH, V129, P71, DOI 10.2307/1971486; Olver P., 1994, GEOMETRY DRIVEN DIFF, V1, P255; Olver P. J., 1995, EQUIVALENCE INVARIAN, DOI DOI 10.1017/CBO9780511609565; OLVER PJ, 1994, CR ACAD SCI I-MATH, V319, P339; Olver PJ, 1999, ACTA APPL MATH, V59, P45, DOI 10.1023/A:1006295328209; OLVER PJ, 2000, MOVING FRAMES GEOMET; Romeny B.M., 1994, GEOMETRY DRIVEN DIFF; SAPIRO G, 1993, INDIANA U MATH J, V42, P985, DOI 10.1512/iumj.1993.42.42046; SAPIRO G, 1993, INT J COMPUT VISION, V11, P25, DOI 10.1007/BF01420591; SAPIRO G, 1994, J FUNCT ANAL, V119, P79, DOI 10.1006/jfan.1994.1004; Semple J.G., 1998, OXFORD CLASSIC TEXTS; Wilczynski E.J., 1906, PROJECTIVE DIFFERENT; Yezzi A, 1997, IEEE T MED IMAGING, V16, P199, DOI 10.1109/42.563665	27	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		2001	42	3					191	201		10.1023/A:1011143732632	http://dx.doi.org/10.1023/A:1011143732632			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	443FU					2022-12-18	WOS:000169330800004
J	Faugeras, O				Faugeras, O			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		2001	41	1-2					5	5		10.1023/A:1011152830697	http://dx.doi.org/10.1023/A:1011152830697			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	427YL					2022-12-18	WOS:000168434300001
J	Malik, J				Malik, J			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Malik, J (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.								0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		2001	43	1					5	5		10.1023/A:1011122819729	http://dx.doi.org/10.1023/A:1011122819729			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	446HP					2022-12-18	WOS:000169508100001
J	Yuille, AL; Zhu, SC; Mumford, D				Yuille, AL; Zhu, SC; Mumford, D			Statistical and computational theories of vision: Part II - Introduction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		2001	41	1-2					7	7		10.1023/A:1011104914767	http://dx.doi.org/10.1023/A:1011104914767			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	427YL					2022-12-18	WOS:000168434300002
J	Faugheras, O				Faugheras, O			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2000	40	3					185	185		10.1023/A:1008135425523	http://dx.doi.org/10.1023/A:1008135425523			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	399MK					2022-12-18	WOS:000166816800001
J	Zhu, SC; Yuille, A; Mumford, D				Zhu, SC; Yuille, A; Mumford, D			Statistical and computational theories of vision: Modeling, learning, sampling and computing, part i	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Ohio State Univ, Dept Comp & Informat Sci, Columbus, OH 43210 USA; Smith Kettlewell Eye Res Inst, San Francisco, CA 94115 USA; Brown Univ, Providence, RI 02912 USA	University System of Ohio; Ohio State University; The Smith-Kettlewell Eye Research Institute; Brown University	Zhu, SC (corresponding author), Ohio State Univ, Dept Comp & Informat Sci, 2015 Neil Ave, Columbus, OH 43210 USA.			Yuille, Alan L./0000-0001-5207-9249					0	0	0	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2000	40	1					5	6		10.1023/A:1026531017760	http://dx.doi.org/10.1023/A:1026531017760			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	384KV					2022-12-18	WOS:000165942300001
J	Kiryati, N				Kiryati, N			Untitled - Introduction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Kiryati, N (corresponding author), Tel Aviv Univ, Dept Elect Engn Syst, IL-69978 Tel Aviv, Israel.							ALPERT C, 1982, TECHNION STORY ISRAE; NAOT J, 1983, SELECTED PAPERS F OL; OLLENDORFF F, 1974, ISRAEL J TECHNOL, V12, P275; OLLENDORFF F, 1947, SIGHT TECHNICAL PROB, V2, P70; OLLENDORFF F, 1949, TRENDS MOD SCI S WEI; OLLENDORFF F, 1951, SPACE COLOURS; OLLENDORFF F, 1963, P INT C TECHN BLINDN, V2, P223	7	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2000	39	2					79	80		10.1023/A:1008174208672	http://dx.doi.org/10.1023/A:1008174208672			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	359FD					2022-12-18	WOS:000089600100001
J	Kanade, T; Faugeras, O				Kanade, T; Faugeras, O			Untidled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2000	38	3					195	196		10.1023/A:1008161906116	http://dx.doi.org/10.1023/A:1008161906116			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	352VV					2022-12-18	WOS:000089239400001
J	Kanade, T				Kanade, T			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2000	36	2					99	99		10.1023/A:1008176530098	http://dx.doi.org/10.1023/A:1008176530098			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	295FD					2022-12-18	WOS:000085955300001
J	Faugeras, O; Kanade, T				Faugeras, O; Kanade, T			This special issue includes the 1998 Marr Prize winners	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									INRIA, Grenoble, France; Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Inria; Carnegie Mellon University	Faugeras, O (corresponding author), INRIA, Grenoble, France.								0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1999	32	1					5	5		10.1023/A:1008197026296	http://dx.doi.org/10.1023/A:1008197026296			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	237NW					2022-12-18	WOS:000082663800001
J	Viergever, MA				Viergever, MA			Computer vision research in Utrecht	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Univ Utrecht Hosp, Image Sci Inst, NL-3584 CX Utrecht, Netherlands	Utrecht University; Utrecht University Medical Center	Viergever, MA (corresponding author), Univ Utrecht Hosp, Image Sci Inst, Heidelberglaan 100, NL-3584 CX Utrecht, Netherlands.		Viergever, Max A/J-1215-2014					ARNSPANG J, 1991, THESIS U COPENHAGEN; BEEKMAN FJ, 1995, THESIS UTRECHT U; BLOM J, 1992, THESIS UTRECHT U; BOUMA CJ, 1998, THESIS UTRECHT U; De Berg Mark, 1997, COMPUTATIONAL GEOMET; DEGRAAF CN, 1988, INFORMATION PROCESSI; DEVRIES SC, 1993, THESIS UTRECHT U; ERENS RGF, 1993, THESIS UTRECHT U; FASSNACHT CJ, 1997, THESIS UTRECHT U; FLORACK L, 1993, THESIS UTRECHT U; FLORACK LMJ, 1997, IMAGE STRUCTURE COMP, V10; HARING S, 1997, THESIS UTRECHT; HOOGEVEEN RM, 1998, THESIS UTRECHT U; Koenderink J., 1990, SOLID SHAPE; KONING AHJ, 1996, THESIS UTRECHT U; KONINGS MK, 1997, THESIS UTRECHT U; KOSTER ASE, 1995, THESIS UTRECHT U; MAINTZ JBA, 1996, THESIS UTRECHT U; NIESSEN WJ, 1997, THESIS UTRECHT U; OTTENBERG K, 1993, THESIS UTRECHT U; ROMENY BMT, 1997, LECT NOTES COMPUTER, V1252; ROMENY BMT, 1994, GEOMETRY DRIVEN DIFF, V1; ROOS P, 1991, THESIS DELFT U TECHN; SALDEN AH, 1996, THESIS UTRECHT U; SOMMER G, 1997, LECT NOTES COMPUTER, V1315; SPORRING J, 1997, GAUSSIAN SCALE SPACE, V8; STOKKING R, 1998, THESIS UTRECHT U; TODDPOKROPEK A, 1992, MED IMAGES FORMATION; VANDAMME WJM, 1994, THESIS UTRECHT U; VANDENELSEN PA, 1993, THESIS UTRECHT U; VANDIJKE MCA, 1992, THESIS UTRECHT U; Veltkamp R. C., 1994, LECT NOTES COMPUTER, V885; VIERGEVER MA, 1989, P SPIE, V1137; VIERGEVER MA, 1988, MATH COMPUTER SCI ME; VINCKEN KL, 1995, THESIS UTRECHT U; ZUIDERVELD KJ, 1995, THESIS UTRECHT U	36	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1999	31	2-3					107	110		10.1023/A:1008068530060	http://dx.doi.org/10.1023/A:1008068530060			4	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	209NA					2022-12-18	WOS:000081053100001
J	Faugeras, O; Kanade, T				Faugeras, O; Kanade, T			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									CARNEGIE MELLON UNIV,PITTSBURGH,PA 15213	Carnegie Mellon University	Faugeras, O (corresponding author), INRIA,LE CHESNAY,FRANCE.								0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1997	24	2					103	103		10.1023/A:1007969703100	http://dx.doi.org/10.1023/A:1007969703100			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XW658					2022-12-18	WOS:A1997XW65800001
J	Chung, R; Nevatia, R				Chung, R; Nevatia, R			Recovering LSHGCs and SHGCs from stereo	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CONTOURS	We examine the problem of computing shape descriptions from stereo, where by shape descriptions we mean 3-D volumetric descriptions of objects rather than a 2 1/2-D depth map of the scene. We argue that intermediate 2 1/2-D depth measurements may not be always directly available from stereo. especially when there are curved surfaces in the scene, and that 3-D volumetric descriptions ob objects may have to be derived directly from stereo in the scene, and that 3-D volumetric descriptions of objects may have to be derived directly from stereo correspondences. We then present methods to recover volumetric shape from stereo using LSHGCs and SHGCs as the shape models. Our methods are based on some invariant properties of LSHGCs and SHGCs in their monocular and stereo projections. Experimental results on both synthetic and real images of objects with curved surfaces are given. Our technique allows dense surface descriptions to be recovered even for objects without much texture, and it is not restricted to narrow stereo angles or low resolution images. Our technique can also handle objects in close range where perspective distortion in the images can be significant.	UNIV SO CALIF,INST ROBOT & INTELLIGENT SYST,LOS ANGELES,CA 90089	University of Southern California	Chung, R (corresponding author), CHINESE UNIV HONG KONG,DEPT MECH & AUTOMAT ENGN,SHATIN,HONG KONG.		Chung, Chi-Kit Ronald/C-7702-2011					BARROW HG, 1981, ARTIF INTELL, V17, P75, DOI 10.1016/0004-3702(81)90021-7; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Binford T.O., 1971, IEEE C SYST CONTR MI; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CHUNG R, 1995, COMPUT VIS IMAGE UND, V62, P245, DOI 10.1006/cviu.1995.1053; Chung R. C., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P50, DOI 10.1109/CVPR.1991.139660; Chung R. C.-K., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P42, DOI 10.1109/CVPR.1992.223229; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; DHOND UR, 1989, IEEE T SYST MAN CYB, V19, P1489, DOI 10.1109/21.44067; HORAUD R, 1988, ARTIF INTELL, V37, P333, DOI 10.1016/0004-3702(88)90059-8; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; LIM HS, 1988, P DARPA IM UND WORKS, P809; MACKWORTH AK, 1973, ARTIF INTELL, V4, P121, DOI 10.1016/0004-3702(73)90003-9; MOHAN R, 1989, IEEE T PATTERN ANAL, V11, P1121, DOI 10.1109/34.42852; MOHAN R, 1989, JUN P IEEE C COMP VI, P333; PONCE J, 1989, IEEE T PATTERN ANAL, V11, P951, DOI 10.1109/34.35498; RAO K, 1987, INT J COMPUTER VISIO, V2, P33; RAO K, 1988, THESIS U SO CALIFORN; SHAFER SA, 1983, CMUCS083105 MELL U; STEVENS KA, 1981, ARTIF INTELL, V17, P47, DOI 10.1016/0004-3702(81)90020-5; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P413, DOI 10.1109/TPAMI.1986.4767807; ULUPINAR F, 1990, P DARPA IM UND WORKS, P544; ULUPINAR F, 1990, P 10 INT C PATT REC, V1, P147; Xu G., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P692, DOI 10.1109/CVPR.1992.223199; XU G, 1987, 1ST P ICCV LOND, P716; ZERROUG M, 1993, P DARPA IM UND WORKS	28	0	0	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1996	20	1-2					43	58		10.1007/BF00144116	http://dx.doi.org/10.1007/BF00144116			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VQ240					2022-12-18	WOS:A1996VQ24000003
J	Schreiber, I; BenBassat, M				Schreiber, I; BenBassat, M			FEG structures for representation and recognition of 3-D polyhedral objects	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								The paper proposes a fast O (n(2.5)) recognition algorithm for partially occluded 3D polyhedral objects, where n is the number of the polyhedron vertices. Our approach is based on the generate and test mechanism using the alignment approach as its basic recognition tool. The first stage is to align one face of the unknown polyhedron with one face of one library model (generate). The second stage is a recursive test procedure that checks the matching of the remaining faces. A new structure called FEG-Face Edge Graph is introduced. This structure stores information about the 2D coordinates of each face and the identity of its adjacent faces. A very low complexity is achieved by using a divide and conquer strategy. Instead of trying to recognize the whole object at once, we divide it and conquer (recognize) it face by face. This is done by reducing the recognition problem to generalized subgraph matching problem in which two subgraphs are equal not only when they are isomorphic, but also when they represent the same part of the same object. A special mechanism handles false splitting and false merging of adjacent faces as a result of wrong segmentation. The process lends itself to hierarchical parallel processing in that the matching with each library model may be carried out independently, and also for each model-processing at the pixel level may also be done in parallel. We evaluated our approach with several real range data images as well as some synthetic objects. Four of these cases are reported here.	TEL AVIV UNIV,LEON RECANATI GRAD SCH BUSINESS ADM,FAC MANAGEMENT,IL-69978 TEL AVIV,ISRAEL	Tel Aviv University	Schreiber, I (corresponding author), TEL AVIV UNIV,RAYMOND & BEVERLY SACKLER FAC EXACT SCI,MOISE & FRIDA ESKENASY INST COMP SCI,IL-69978 TEL AVIV,ISRAEL.							AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BASRI R, 1986, P 2 INT C COMP VIS T, P482; BASRI R, 1990, THESIS WEIZMANN I SC; Bolles R. C., 1982, INT J ROBOT RES, V1, P57; BOLLES RC, 1983, P 8 INT JOINT C ART, V2, P116; BROWSE RA, 1987, IEEE T PATTERN ANAL, V9, P779, DOI 10.1109/TPAMI.1987.4767984; CALIFANO A, 1991, IEEE C COMPUTER VISI, P28; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; ETTINGER GJ, 1988, IEEE C COMP VIS PATT, P32; FAN TJ, 1989, IEEE T PATTERN ANAL, V11, P1140, DOI 10.1109/34.42853; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FAUGERAS OD, 1983, P 8 INT JOINT C ART, V2, P996; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; HONG J, 1988, P 9 INT C PATT REC R, V1, P72; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; HUTTENLOCHER DP, 1987, P IM UND WORKSH, V1, P370; KALVIN A, 1986, INT J ROBOT RES, V5, P38, DOI 10.1177/027836498600500403; KIM WY, 1991, IEEE T PATTERN ANAL, V13, P224, DOI 10.1109/34.75511; LAMDAN Y, 1990, IEEE T ROBOTIC AUTOM, V6, P578, DOI 10.1109/70.62047; Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995; LAMDAN Y, 1988, JUN P CVPR C ANN ARB, P335; Marr D., 1982, VISION; SCHREIBER I, 1992, P 9 ISR S ART INT CO, P367; SCHREIBER I, 1990, P 10 INT C PATT REC, V1, P852; SCHREIBER I, 1991, 21491 TEL AV U COMP; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; STEIN F, 1991, P 8 ISR S ART INT CO, V1, P125; Stein F., 1990, P 10 INT C PATT REC, V1, P13; ULLMAN S, 1986, 931 MIT AI; YADIDPECHT O, 1992, P SOC PHOTO-OPT INS, V1656, P374, DOI 10.1117/12.135919; 1988, P 2 INT C COMP VIS T, P489	31	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1996	18	3					211	232		10.1007/BF00123142	http://dx.doi.org/10.1007/BF00123142			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VB108					2022-12-18	WOS:A1996VB10800002
J	SWAIN, M				SWAIN, M			SPECIAL ISSUE ON ACTIVE VISION-I	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material											SWAIN, M (corresponding author), UNIV CHICAGO,DEPT COMP SCI,ARTIFICIAL INTELLIGENCE LAB,CHICAGO,IL 60637, USA.								0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1993	11	2					107	107		10.1007/BF01469223	http://dx.doi.org/10.1007/BF01469223			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MB329					2022-12-18	WOS:A1993MB32900001
J	ASADA, M; TSUJI, S				ASADA, M; TSUJI, S			MACHINE VISION RESEARCH AT OSAKA-UNIVERSITY - INTRODUCTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material							MOBILE ROBOT; IMAGES; MOTION; WORLD		OSAKA UNIV,SUITA,OSAKA 565,JAPAN; OSAKA UNIV,DEPT SYST ENGN,TOYONAKA,OSAKA 560,JAPAN	Osaka University; Osaka University								Asada M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P726, DOI 10.1109/CVPR.1992.223189; ASADA M, 1990, IEEE T SYST MAN CYB, V20, P1456, DOI 10.1109/21.61215; ASADA M, 1988, IEEE T PATTERN ANAL, V10, P749, DOI 10.1109/34.6787; ASADA M, 1990, IEEE T SYST MAN CYB, V20, P1326, DOI 10.1109/21.61204; ASADA M, 1983, COMPUT VISION GRAPH, V21, P118, DOI 10.1016/S0734-189X(83)80031-0; ASADA M, 1984, PATTERN RECOGN, V17, P57, DOI 10.1016/0031-3203(84)90035-9; Asada M., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P240; ASADA M, 1985, 9TH P IJCAI, P895; ASADA M, 1987, 1ST P INT C COMP VIS, P412; ASADA M, 1983, 8TH P INT J C ARTF I, P240; ASADA M, 1989, 11TH P IJCAI, P1629; BARTH M, 1991, IEEE T ROBOTIC AUTOM, P2792; DAN S, 1990, P PRICAI 90, P557; ETOH M, 1992, 2ND P ECCV, P24; Fujita T., 1988, Proceedings of IAPR Workshop on Computer Vision: Special Hardware and Industrial Applications, P451; HIRATA S, 1992, P IEEE RSJ INT C INT, P1603; ISHIGURO H, 1992, IEEE T PATTERN ANAL, V14, P257, DOI 10.1109/34.121792; KADONO K, 1991, P IEEE WORKSH DIR AU, P186; KAKUSHO K, 1992, INT J COMP VISION; KIMUAR M, 1992, INT J COMP VISION; KITAHASHI T, P SPIE 92; Kitamura Y., 1990, Advanced Robotics, V4, P29, DOI 10.1163/156855390X00035; LI S, 1991, P IEEE WORKSHOP VISU; Marr D., 1982, VISION; MISHKIN M, 1986, TRENDS NEUROSCI, P414; MITSUMOTO H, IN PRESS IEEE T PATT; NAKAYAMA O, 1992, ICRA NIC FRANC MAY, P1753; Oh W. G., 1988, 9th International Conference on Pattern Recognition (IEEE Cat. No.88CH2614-6), P1043, DOI 10.1109/ICPR.1988.28435; OH WG, 1987, 5TH P SCAND C IM AN, P191; Ozaki Y., 1988, 9th International Conference on Pattern Recognition (IEEE Cat. No.88CH2614-6), P804, DOI 10.1109/ICPR.1988.28365; SATO K, 1985, J ROBOTIC SYST, V2, P27; SATO K, 1987, 1ST P ICCV, P657; SATO Y, 1987, NOV IEEE COMP SOC WO, P252; Tanaka H. T., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P491; TOMITA F, 1982, IEEE T PATTERN ANAL, V4, P183, DOI 10.1109/TPAMI.1982.4767225; TOMITA F, 1977, IEEE T SYST MAN CYB, V7, P107; Tomita F., 1990, COMPUTER ANAL VISUAL; TSUJI S, 1978, IEEE T COMPUT, V27, P777, DOI 10.1109/TC.1978.1675191; TSUJI S, 1980, IEEE T PATTERN ANAL, V2, P516, DOI 10.1109/TPAMI.1980.6447698; Tsuji S., 1986, Eighth International Conference on Pattern Recognition. Proceedings (Cat. No.86CH2342-4), P1103; TSUJI S, 1977, P 5 INT JOINT C ART, P609; TSUJI S, 1985, IEEE T ROBOTIC AUTOM, P850; TSUJI S, 1975, P IJCAI 75, P811; TSUJI S, 1973, COMPUTER GRAPHICS IM, V2, P216; XU G, 1987, IEEE T PATTERN ANAL, V9, P332, DOI 10.1109/TPAMI.1987.4767908; XU G, 1989, 11TH P INT JOINT C A, P1610; YACHIDA M, 1981, IEEE T PATTERN ANAL, V3, P12, DOI 10.1109/TPAMI.1981.4767046; YACHIDA M, 1977, IEEE T COMPUT, V26, P882, DOI 10.1109/TC.1977.1674936; YACHIDA M, 1985, 3RD P INT S ROB RES, P11; Yagi Y., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P160, DOI 10.1109/CVPR.1991.139681; YAGI Y, 1991, P IEE RSJ INT WORKSH, V2, P909; ZHENG JY, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P558; ZHENG JY, 1992, INT J COMP VISION; ZHENG JY, 1989, THESIS OSAKA U	54	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1992	9	1					5	11						7	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JW393					2022-12-18	WOS:A1992JW39300001
J	CLARK, JJ				CLARK, JJ			SPECIAL ISSUE - VLSI FOR COMPUTER VISION - GUEST EDITOR INTRODUCTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									HARVARD UNIV,DIV APPL SCI,CAMBRIDGE,MA 02138	Harvard University									0	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1992	8	3					175	176						2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JU903					2022-12-18	WOS:A1992JU90300001
J	GRIMSON, WEL				GRIMSON, WEL			THE COST OF CHOOSING THE WRONG MODEL IN OBJECT RECOGNITION BY CONSTRAINED SEARCH	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							SATISFACTION PROBLEMS; HOUGH TRANSFORM; ORIENTATION; CONSISTENCY; ALGORITHMS; TREE	Many current recognition systems use variations on constrained tree search to locate objects in cluttered environments. If the system is simply finding instances of an object known to be in the scene, then previous formal analysis has shown that the expected amount of search is quadratic in the number of model and data features when all the data is known to come from a single object, but is exponential when spurious data is included. If one can group the data into subsets likely to have come from a single object, then terminating the search once a "good enough" interpretation is found reduces the expected search to cubic. Without successful grouping, terminated search is still exponential. These results apply to finding instances of a known object in the data. What happens when the object is not present? In this article, we turn to the problem of selecting models from a library, and examine the combinatorial cost of determining that an incorrectly chosen candidate object is not present in the data. We show that the expected search is again exponential, implying that naive approaches to library indexing are likely to carry an expensive overhead, since an exponential amount of work is needed to weed out each incorrect model. The analytic results are shown to be in agreement with empirical data for cluttered object recognition.	MIT, ARTIFICIAL INTELLIGENCE LAB, CAMBRIDGE, MA 02139 USA	Massachusetts Institute of Technology (MIT)			Rohlf, F J/A-8710-2008					[Anonymous], 1985, PERCEPTUAL ORG VISUA; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; Bolles R. C., 1982, INT J ROBOT RES, V1, P57; CASS TA, 1988, IEEE C COMPUT VISION, P879; CYGANSKI D, 1985, IEEE T PATTERN ANAL, V7, P662, DOI 10.1109/TPAMI.1985.4767722; DAVIS LS, 1982, PATTERN RECOGN, V15, P277, DOI 10.1016/0031-3203(82)90030-9; FLYNN PJ, 1991, IEEE T PATTERN ANAL, V13, P1066, DOI 10.1109/34.99239; FREUDER EC, 1982, J ACM, V29, P24, DOI 10.1145/322290.322292; FREUDER EC, 1978, COMMUN ACM, V21, P958, DOI 10.1145/359642.359654; Gaschnig J. G., 1979, THESIS CARNEGIE MELL; GLEASON GJ, 1979, 9TH P INT S IND ROB, P57; Graham R.L., 1989, CONCRETE MATH; Grimson W. E. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P700, DOI 10.1109/CCV.1988.590054; GRIMSON WEL, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P644; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; GRIMSON WEL, 1989, IEEE T PATTERN ANAL, V11, P632, DOI 10.1109/34.24797; GRIMSON WEL, 1991, IEEE T PATTERN ANAL, V13, P920, DOI 10.1109/34.93810; GRIMSON WEL, 1990, ARTIF INTELL, V44, P121, DOI 10.1016/0004-3702(90)90100-E; GRIMSON WEL, 1989, IN PRESS IEEE T PATT, V13, P1201; HARALICK RM, 1980, ARTIF INTELL, V14, P263, DOI 10.1016/0004-3702(80)90051-X; HARALICK RM, 1979, IEEE T PATTERN ANAL, V1, P173, DOI 10.1109/TPAMI.1979.4766903; Hough P.V., 1962, US Patent, Patent No. [US3069654A, 3069654, 3,069,654]; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; ILLINGWORTH J, 1988, COMPUT VISION GRAPH, V44, P87, DOI 10.1016/S0734-189X(88)80033-1; KNAPMAN J, 1987, 1ST P INT C COMP VIS, P547; LAMDAN Y, 1988, JUN P CVPR C ANN ARB, P335; MACKWORTH AK, 1977, ARTIF INTELL, V8, P99, DOI 10.1016/0004-3702(77)90007-8; MACKWORTH AK, 1985, ARTIF INTELL, V25, P65, DOI 10.1016/0004-3702(85)90041-4; MURRAY DW, 1988, INT J COMPUT VISION, V2, P153, DOI 10.1007/BF00133698; NUDEL B, 1983, ARTIF INTELL, V21, P135, DOI 10.1016/S0004-3702(83)80008-3; SILBERBERG TM, 1986, COMPUT VISION GRAPH, V35, P47, DOI 10.1016/0734-189X(86)90125-8; STOCKMAN G, 1982, IEEE T PATTERN ANAL, V4, P229, DOI 10.1109/TPAMI.1982.4767240; TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920; THOMPSON DW, 1987, IEEE T ROBOTIC AUTOM, P28; WANG YF, 1984, IEEE T PATTERN ANAL, V6, P513, DOI 10.1109/TPAMI.1984.4767556; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949; [No title captured]	39	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1992	7	3					195	210						16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HX401					2022-12-18	WOS:A1992HX40100002
J	MURRAY, DW				MURRAY, DW			CORRECTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction, Addition																		MURRAY DW, 1989, INT J COMPUT VISION, V3, P181, DOI 10.1007/BF00133031	1	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1990	4	1					102	102						1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CU085					2022-12-18	WOS:A1990CU08500005
J	FISCHLER, MA				FISCHLER, MA			AN OVERVIEW OF COMPUTER VISION RESEARCH AT SRI-INTERNATIONAL - THEMES AND PROGRESS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									SRI INT,CTR ARTIFICIAL INTELLIGENCE,MENLO PK,CA 94025	SRI International								BAKER HH, 1987, FEB P DARPA IM UND W, P843; BAKER HH, 1989, INT J COM V, V3; BARNARD ST, 1986, 5TH P NAT C ART INT, P676; BARNARD ST, 1986, OCT P SPIE S ADV INT; BARNARD ST, 1989, INT J COM V, V3; BOBICK AF, 1987, AFTR1001 MIT AI LAB; BOBICK AF, 1989, UNPUB IEEE C COMP VI; Bolles R. C., 1982, INT J ROBOT RES, V1, P57; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; BOLLES RC, 1989, UNPUB ROBOTICS AUTOM; BOLLES RC, 1983, 8TH P INT JOINT C AR, P1116; BOLLES RC, 1985, 3RD P INT S ROB RES; Fischler M.A., 1987, INTELLIGENCE EYE BRA; FISCHLER MA, 1987, PATTERN RECOGN, V20, P257, DOI 10.1016/0031-3203(87)90059-8; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FISCHLER MA, 1987, READINGS COMPUTER VI; FISCHLER MA, 1988, MAR P AAAI SPRING S; FUA P, 1987, PATTERN RECOGN LETT, V5, P243, DOI 10.1016/0167-8655(87)90070-5; FUA PV, 1988, APR P DARPA IM UND W; Hannah M., 1985, DEC P DARPA IM UND W, P149; HANNAH MJ, 1988, APR P DARPA IM UND W; HANNAH MJ, 1985, SRI365 ART INT CTR T; HANNAH MJ, 1988, JUL INT SOC PHOT REM; HANSON AJ, 1988, UNPUB COMP VIS GRAPH; HANSON AJ, 1988, APR P DARPA IM UND W; HANSON AJ, 1987, FEB P DARPA IM UND W, P475; HEEGER DJ, 1987, 1ST P INT C COMP VIS, P181; LAWS KI, 1985, APR P SPIE C APPL AR, V548; LAWS KI, 1988, SRI443 ART INT CTR T; LECLERC YG, 1987, IEEE T PATTERN ANAL, V9, P341, DOI 10.1109/TPAMI.1987.4767918; LECLERC YG, 1983, JUN P IEEE C COMP VI, P34; LECLERC YG, 1989, INT J COM V, V3; LECLERC YG, 1987, FEB P DARPA IM UND W; MARIMONT DH, 1986, MAY P IEEE COMP SOC, P7; PENTLAND A, 1987, 1ST INT C COMP VIS, P612; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P661, DOI 10.1109/TPAMI.1984.4767591; PENTLAND AP, 1986, ARTIF INTELL, V28, P293, DOI 10.1016/0004-3702(86)90052-4; PENTLAND AP, 1985, PIXELS PREDICATES; PENTLAND AP, 1984, AUG P AAAI 84, P296; QUAM L, 1984, P IMAGE UNDERSTANDIN, P149; QUAM LH, 1985, DEC P DARPA IM UND W, P327; Smith G. B., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P14; SMITH GB, 1985, JUN P IEEE C COMP VI, P271; SMITH GB, 1988, APR SPIE P DIG OPT S, V938, P377; SMITH GB, 1987, FEB P DARPA IM UND W, P170; SMITH GB, 1986, 5TH P NATN C ART INT, P689; STRAT TM, 1986, IEEE T PATTERN ANAL, V8, P730, DOI 10.1109/TPAMI.1986.4767854; STRAT TM, 1988, APR P DARPA IM UND W; WESLEY LP, 1986, OPT ENG, V25, P363, DOI 10.1117/12.7973833	49	0	0	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	1989	3	1					7	15		10.1007/BF00054835	http://dx.doi.org/10.1007/BF00054835			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AE167					2022-12-18	WOS:A1989AE16700001
