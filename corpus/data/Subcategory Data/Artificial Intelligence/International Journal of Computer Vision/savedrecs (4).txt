PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
J	Kang, D; Dhar, D; Chan, AB				Kang, Di; Dhar, Debarun; Chan, Antoni B.			Incorporating Side Information by Adaptive Convolution	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Convolutional neural network (CNN); Deep learning; Crowd counting		Computer vision tasks often have side information available that is helpful to solve the task. For example, for crowd counting, the camera perspective (e.g., camera angle and height) gives a clue about the appearance and scale of people in the scene. While side information has been shown to be useful for counting systems using traditional hand-crafted features, it has not been fully utilized in deep learning based counting systems. In order to incorporate the available side information, we propose an adaptive convolutional neural network (ACNN), where the convolution filter weights adapt to the current scene context via the side information. In particular, we model the filter weights as a low-dimensional manifold within the high-dimensional space of filter weights. The filter weights are generated using a learned "filter manifold" sub-network, whose input is the side information. With the help of side information and adaptive weights, the ACNN can disentangle the variations related to the side information, and extract discriminative features related to the current context (e.g. camera perspective, noise level, blur kernel parameters). We demonstrate the effectiveness of ACNN incorporating side information on 3 tasks: crowd counting, corrupted digit recognition, and image deblurring. Our experiments show that ACNN improves the performance compared to a plain CNN with a similar number of parameters and achieves similar or better than state-of-the-art performance on crowd counting task. Since existing crowd counting datasets do not contain ground-truth side information, we collect a new dataset with the ground-truth camera angle and height as the side information. We also perform ablation experiments, mainly for crowd counting, to study the helpfulness of the side information, and the effect of the placement of the adaptive convolutional layers in order to get insight about ACNNs.	[Kang, Di; Dhar, Debarun; Chan, Antoni B.] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China; [Kang, Di] Tencent AI Lab, Shenzhen, Peoples R China	City University of Hong Kong; Tencent	Kang, D (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.; Kang, D (corresponding author), Tencent AI Lab, Shenzhen, Peoples R China.	di.kang@outlook.com; ddhar2-c@my.cityu.edu.hk; abchan@cityu.edu.hk		Kang, Di/0000-0002-8996-0897	Research Grants Council of the Hong Kong Special Administrative Region, China [T32-101/15-R, CityU 11212518]	Research Grants Council of the Hong Kong Special Administrative Region, China(Hong Kong Research Grants Council)	The work described in this paper was supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. [T32-101/15-R] and CityU 11212518). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arteta Carlos, 2014, ECCV, P504; Burger H.C., 2012, CVPR; Chan A. B., 2009, ICCV; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Ciresan D., 2012, CVPR; Dai J., 2017, CVPR; Dozat T., 2015, INT C LEARN REPR 201; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Fiaschi L., 2012, ICPR; Gharbi M., 2016, DEEP JOINT DEMOSAICK; Ha David, 2017, ICLR; Idrees H., 2013, CVPR; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jia X., 2016, NIPS; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kang D, 2017, ADV NEUR IN, V30; Kang D, 2019, IEEE T CIRC SYST VID, V29, P1408, DOI 10.1109/TCSVT.2018.2837153; Kang Di, 2018, BMVC; Kingma D.P, P 3 INT C LEARNING R; Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lempitsky V., 2010, NEURAL INF PROCESS S; Li S., 2015, IJCV; Li Y., 2018, ABS180602877 ARXIV; LIU RT, 2008, CVPR; Ma Z., 2015, CVPR; Maas Andrew L, 2013, P ICML CIT, V30, P3, DOI DOI 10.21437/INTERSPEECH.2016-1230; Niu Zhenxing, 2016, CVPR; Onoro- Rubio D., 2016, ECCV; Pech-Pacheco Jose Luis, 2000, ICPR; Ren W, 2017, CVPR; Rodriguez M., 2011, ICCV; Rothe R., 2015, ICCVW; Sam D. B., 2017, CVPR; Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379; Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Xu L., 2014, NIPS; Zhang C., 2015, CVPR; Zhang L., 2018, WACV; Zhang Y., 2016, CVPR; Zhang Z., 2014, ECCV	46	5	6	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2897	2918		10.1007/s11263-020-01345-8	http://dx.doi.org/10.1007/s11263-020-01345-8		JUL 2020	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ					2022-12-18	WOS:000545051500002
J	Rosu, RA; Quenzel, J; Behnke, S				Rosu, Radu Alexandru; Quenzel, Jan; Behnke, Sven			Semi-supervised Semantic Mapping Through Label Propagation with Semantic Texture Meshes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic mapping; Label propagation; Semantic textured mesh	3D RECONSTRUCTION; SEGMENTATION	Scene understanding is an important capability for robots acting in unstructured environments. While most SLAM approaches provide a geometrical representation of the scene, a semantic map is necessary for more complex interactions with the surroundings. Current methods treat the semantic map as part of the geometry which limits scalability and accuracy. We propose to represent the semantic map as a geometrical mesh and a semantic texture coupled at independent resolution. The key idea is that in many environments the geometry can be greatly simplified without loosing fidelity, while semantic information can be stored at a higher resolution, independent of the mesh. We construct a mesh from depth sensors to represent the scene geometry and fuse information into the semantic texture from segmentations of individual RGB views of the scene. Making the semantics persistent in a global mesh enables us to enforce temporal and spatial consistency of the individual view predictions. For this, we propose an efficient method of establishing consensus between individual segmentations by iteratively retraining semantic segmentation with the information stored within the map and using the retrained segmentation to re-fuse the semantics. We demonstrate the accuracy and scalability of our approach by reconstructing semantic maps of scenes from NYUv2 and a scene spanning large buildings.	[Rosu, Radu Alexandru; Quenzel, Jan; Behnke, Sven] Univ Bonn, Autonomous Intelligent Syst Grp, Bonn, Germany	University of Bonn	Quenzel, J (corresponding author), Univ Bonn, Autonomous Intelligent Syst Grp, Bonn, Germany.	rosu@ais.uni-bonn.de; quenzel@ais.uni-bonn.de; behnke@ais.uni-bonn.de	Behnke, Sven/B-5509-2013	Behnke, Sven/0000-0002-5040-7525; Rosu, Radu Alexandru/0000-0001-7349-4126				Acuna D, 2018, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2018.00096; [Anonymous], 2018, P IEEE INT C ROB AUT; [Anonymous], 2017, P IEEE C COMP VIS PA; [Anonymous], P IEEE C COMP VIS PA; [Anonymous], 2017, P IEEE C COMP VIS PA; [Anonymous], 2018, P IEEE C COMP VIS PA; [Anonymous], 2018, ARXIV181009726; [Anonymous], SEMANTIC VISUAL LOCA; [Anonymous], P IEEE C COMP VIS PA; [Anonymous], 2005, P IEEE INT C COMP VI; [Anonymous], P EUR C COMP VIS ECC; Bao SYZ, 2011, PROC CVPR IEEE; Bao SY, 2013, PROC CVPR IEEE, P1264, DOI 10.1109/CVPR.2013.167; Blaha M, 2016, PROC CVPR IEEE, P3176, DOI 10.1109/CVPR.2016.346; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cherabier I, 2016, INT CONF 3D VISION, P601, DOI 10.1109/3DV.2016.68; Civera J, 2011, IEEE INT C INT ROBOT, P1277, DOI 10.1109/IROS.2011.6048293; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Douglas DH, 1973, CARTOGR INT J GEOGR, V10, P112, DOI [10.3138/fm57-6770-u75u-7727, DOI 10.3138/FM57-6770-U75U-7727]; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54; Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312; Hane C, 2014, PROC CVPR IEEE, P652, DOI 10.1109/CVPR.2014.89; Hane C, 2013, PROC CVPR IEEE, P97, DOI 10.1109/CVPR.2013.20; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hermans A, 2014, IEEE INT CONF ROBOT, P2631, DOI 10.1109/ICRA.2014.6907236; Holz D, 2015, ROBOT AUTON SYST, V74, P318, DOI 10.1016/j.robot.2015.07.021; Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0; Jain SD, 2016, PROC CVPR IEEE, P2864, DOI 10.1109/CVPR.2016.313; Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237; Kostavelis I, 2015, ROBOT AUTON SYST, V66, P86, DOI 10.1016/j.robot.2014.12.006; Kundu A, 2014, LECT NOTES COMPUT SC, V8694, P703, DOI 10.1007/978-3-319-10599-4_45; Li X., 2016, ARXIV161104144; Lianos KN, 2018, LECT NOTES COMPUT SC, V11208, P246, DOI 10.1007/978-3-030-01225-0_15; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Ma LN, 2017, IEEE INT C INT ROBOT, P598; Maninchedda F, 2016, LECT NOTES COMPUT SC, V9910, P667, DOI 10.1007/978-3-319-46466-4_40; McCormac John, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4628, DOI 10.1109/ICRA.2017.7989538; Nakajima Y, 2018, IEEE INT C INT ROBOT, P385, DOI 10.1109/IROS.2018.8593993; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Qi CR, 2017, ADV NEUR IN, V30; Quigley M., 2009, ICRA WORKSH OP SOURC, V3, P5; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Savinov N, 2016, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR.2016.589; Sheikh R, 2016, LECT NOTES COMPUT SC, V9914, P3, DOI 10.1007/978-3-319-48881-3_1; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Simonyan Karen, 2015, INT C LEARN REPR; Stuckler J, 2015, J REAL-TIME IMAGE PR, V10, P599, DOI 10.1007/s11554-013-0379-5; Sun L, 2018, IEEE ROBOT AUTOM LET, V3, P3749, DOI 10.1109/LRA.2018.2856268; Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409; Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695; Thurmer G., 1998, Journal of Graphics Tools, V3, P43, DOI 10.1080/10867651.1998.10487487; Valentin JPC, 2013, PROC CVPR IEEE, P2067, DOI 10.1109/CVPR.2013.269; Vezhnevets A, 2012, PROC CVPR IEEE, P3162, DOI 10.1109/CVPR.2012.6248050; Vineet V, 2015, IEEE INT CONF ROBOT, P75, DOI 10.1109/ICRA.2015.7138983; Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang L., 2017, INT C MEDICAL IMAGE, P399; Zaganidis A, 2018, IEEE ROBOT AUTOM LET, V3, P2942, DOI 10.1109/LRA.2018.2848308; Zollhofer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386	62	5	5	2	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1220	1238		10.1007/s11263-019-01187-z	http://dx.doi.org/10.1007/s11263-019-01187-z			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Green Submitted			2022-12-18	WOS:000531431500010
J	Song, T; Kim, Y; Oh, C; Jang, H; Ha, N; Sohn, K				Song, Taeyong; Kim, Youngjung; Oh, Changjae; Jang, Hyunsung; Ha, Namkoo; Sohn, Kwanghoon			Simultaneous Deep Stereo Matching and Dehazing with Feature Attention	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo matching; Dehazing; CNN; Multi-task learning; Knowledge distillation; Stereo confidence; Adverse weather condition		Unveiling the dense correspondence under the haze layer remains a challenging task, since the scattering effects result in less distinctive image features. Contrarily, dehazing is often confused by the airlight-albedo ambiguity which cannot be resolved independently at each pixel. In this paper, we introduce a deep convolutional neural network that simultaneously estimates a disparity and clear image from a hazy stereo image pair. Both tasks are synergistically formulated by fusing depth information from the matching cost and haze transmission. To learn the optimal fusion of depth-related features, we present a novel encoder-decoder architecture that extends the core idea of attention mechanism to the simultaneous stereo matching and dehazing. As a result, our method estimates high-quality disparity for the stereo images in scattering media, and produces appearance images with enhanced visibility. Finally, we further propose an effective strategy for adaptation to camera-captured images by distilling the cross-domain knowledge. Experiments on both synthetic and real-world scenarios including comparisons with state-of-the-art methods demonstrate the effectiveness and flexibility of our approach.	[Song, Taeyong; Sohn, Kwanghoon] Yonsei Univ, Seoul, South Korea; [Kim, Youngjung] Agcy Def Dev, Daejeon, South Korea; [Oh, Changjae] Queen Mary Univ London, London, England; [Jang, Hyunsung; Ha, Namkoo] LIG Nex1 Co Ltd, Yongin, South Korea	Yonsei University; Agency of Defense Development (ADD), Republic of Korea; University of London; Queen Mary University London; LIG Nex1 Co., Ltd.	Sohn, K (corresponding author), Yonsei Univ, Seoul, South Korea.	sty37@yonsei.ac.kr; read12300@add.re.kr; c.oh@qmul.ac.uk; hyunsung.jang@gmail.com; hanamkoo369a@lignex1.com; khsohn@yonsei.ac.kr	Jang, Hyunsung/GQP-0848-2022; Jang, Hyunsung/AGW-5662-2022	Jang, Hyunsung/0000-0002-5797-7264; Song, Taeyong/0000-0001-9296-5026				Abadi Martin, 2016, arXiv; [Anonymous], 2017, P IEEE C COMP VIS PA; [Anonymous], 2013, IEEE INT C COMP VIS; [Anonymous], 2015, IEEE C COMP VIS PATT; [Anonymous], AAAI C ART INT; [Anonymous], IEEE INT C COMP VIS; [Anonymous], 2018, BRIT MACH VIS C BMVC; [Anonymous], BRIT MACH VIS C; [Anonymous], 2010, J MACH LEARN RES P T; [Anonymous], 2018, IEEE C COMP VIS PATT; [Anonymous], 2018, IEEE C COMP VIS PATT; [Anonymous], 2015, IEEE C COMP VIS PATT; [Anonymous], P AS C COMP VIS; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681; Chang WL, 2019, PROC CVPR IEEE, P1900, DOI 10.1109/CVPR.2019.00200; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Duan X, 2017, INT EL DEVICES MEET; Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362; Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; Geiger A., 2012, P IEEE COMP SOC C CO; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Hartley R., 2004, ROBOTICA; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim S, 2017, IEEE T IMAGE PROCESS, V26, P6019, DOI 10.1109/TIP.2017.2750404; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069; Koschmieder H., 1925, THEORIE HORIZONTALEN; Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Mozerov MG, 2015, IEEE T IMAGE PROCESS, V24, P1153, DOI 10.1109/TIP.2015.2395820; Poggi Matteo, 2016, BMVC; Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10; Roser M, 2014, IEEE INT CONF ROBOT, P3840, DOI 10.1109/ICRA.2014.6907416; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643; Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383; Zbontar J, 2016, J MACH LEARN RES, V17; Zhang H, 2017, ARXIV COMPUTER VISIO; Zhang W, 2018, IEEE CONF COMPUT; Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191	52	5	5	3	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		799	817		10.1007/s11263-020-01294-2	http://dx.doi.org/10.1007/s11263-020-01294-2			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Green Submitted			2022-12-18	WOS:000525393600004
J	Li, YA; Wan, J; Miao, QG; Escalera, S; Fang, HJ; Chen, HZ; Qi, XD; Guo, GD				Li, Yunan; Wan, Jun; Miao, Qiguang; Escalera, Sergio; Fang, Huijuan; Chen, Huizhou; Qi, Xiangda; Guo, Guodong			CR-Net: A Deep Classification-Regression Network for Multimodal Apparent Personality Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Personality traits; Multimodal data; Convolutional neural networks; Classification-regression network; Bell Loss function	BIMODAL REGRESSION	First impressions strongly influence social interactions, having a high impact in the personal and professional life. In this paper, we present a deep Classification-Regression Network (CR-Net) for analyzing the Big Five personality problem and further assisting on job interview recommendation in a first impressions setup. The setup is based on the ChaLearn First Impressions dataset, including multimodal data with video, audio, and text converted from the corresponding audio data, where each person is talking in front of a camera. In order to give a comprehensive prediction, we analyze the videos from both the entire scene (including the person's motions and background) and the face of the person. Our CR-Net first performs personality trait classification and applies a regression later, which can obtain accurate predictions for both personality traits and interview recommendation. Furthermore, we present a new loss function called Bell Loss to address inaccurate predictions caused by the regression-to-the-mean problem. Extensive experiments on the First Impressions dataset show the effectiveness of our proposed network, outperforming the state-of-the-art.	[Li, Yunan; Miao, Qiguang; Fang, Huijuan; Chen, Huizhou; Qi, Xiangda] Xidian Univ, Sch Comp Sci & Technol, Xian, Peoples R China; [Li, Yunan; Miao, Qiguang; Fang, Huijuan; Chen, Huizhou; Qi, Xiangda] Xian Key Lab Big Data & Intelligent Vis, Xian, Peoples R China; [Wan, Jun] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Wan, Jun] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China; [Escalera, Sergio] Univ Barcelona, Barcelona, Spain; [Escalera, Sergio] Comp Vis Ctr, Barcelona, Spain; [Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing, Peoples R China; [Guo, Guodong] Natl Engn Lab Deep Learning Technol & Applicat, Beijing, Peoples R China	Xidian University; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; University of Barcelona; Centre de Visio per Computador (CVC); Baidu	Wan, J (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.; Wan, J (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.	yn_li@stu.xidian.edu.cn; jun.wan@ia.ac.cn; qgmiao@mail.xidian.edu.cn; sergio@maia.ub.es; guoguodong01@baidu.com	Escalera, Sergio/L-2998-2015	Escalera, Sergio/0000-0003-0617-8873	National Key RAMP;D Program of China [2018YFC0807500]; National Natural Science Foundations of China [61961160704, 61876179, 61772396, 61772392, 61902296]; Fundamental Research Funds for the Central Universities [JBF180301]; Xi'an Key Laboratory of Big Data and Intelligent Vision [201805053ZD4CG37]; Science and Technology Development Fund of Macau [0008/2018/A1, 0025/2019/A1, 0010/2019/AFJ, 0025/2019/AKP]; MINECO/FEDER, UE [TIN2016-74946-P]; CERCA Programme/Generalitat de Catalunya	National Key RAMP;D Program of China; National Natural Science Foundations of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Xi'an Key Laboratory of Big Data and Intelligent Vision; Science and Technology Development Fund of Macau; MINECO/FEDER, UE(Spanish Government); CERCA Programme/Generalitat de Catalunya	The work was supported by the National Key R&D Program of China under Grant #2018YFC0807500, the National Natural Science Foundations of China #61961160704, #61876179, #61772396, #61772392, #61902296, the Fundamental Research Funds for the Central Universities #JBF180301, Xi'an Key Laboratory of Big Data and Intelligent Vision #201805053ZD4CG37, the Science and Technology Development Fund of Macau (#0008/2018/A1, #0025/2019/A1, #0010/2019/AFJ, #0025/2019/AKP), Spanish project TIN2016-74946-P (MINECO/FEDER, UE) and CERCA Programme/Generalitat de Catalunya.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; BARRICK MR, 1991, PERS PSYCHOL, V44, P1, DOI 10.1111/j.1744-6570.1991.tb00688.x; Basu A, 2018, IEEE T AFFECT COMPUT, V9, P330, DOI 10.1109/TAFFC.2018.2828845; Bekhouche SE, 2017, IEEE COMPUT SOC CONF, P1660, DOI 10.1109/CVPRW.2017.211; BLAND JM, 1994, BRIT MED J, V309, P780, DOI 10.1136/bmj.309.6957.780; BLAND JM, 1994, BRIT MED J, V308, P1499, DOI 10.1136/bmj.308.6942.1499; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chen SX, 2018, IEEE T MULTIMEDIA, V20, P2209, DOI 10.1109/TMM.2017.2786869; Escalante H. J., 2018, ARXIV PREPRINT ARXIV; Eyben F., 2010, PROC ACM INT C MULTI, P1459, DOI [10.1145/1873951.1874246, DOI 10.1145/1873951.1874246]; Gao BB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P712; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Gucluturk Y, 2018, IEEE T AFFECT COMPUT, V9, P316, DOI 10.1109/TAFFC.2017.2751469; Gucluturk Y, 2016, LECT NOTES COMPUT SC, V9915, P349, DOI 10.1007/978-3-319-49409-8_28; Gurpinar F, 2016, INT C PATT RECOG, P43, DOI 10.1109/ICPR.2016.7899605; Gurpinar F, 2016, LECT NOTES COMPUT SC, V9915, P372, DOI 10.1007/978-3-319-49409-8_30; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang GB, 2004, IEEE IJCNN, P985; Huang SY, 2017, PROC CVPR IEEE, P4664, DOI 10.1109/CVPR.2017.496; Escalante HJ, 2016, INT C PATT RECOG, P67, DOI 10.1109/ICPR.2016.7899609; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kaya H, 2017, IEEE COMPUT SOC CONF, P1651, DOI 10.1109/CVPRW.2017.210; King DB, 2015, ACS SYM SER, V1214, P1; Klein DN, 2011, ANNU REV CLIN PSYCHO, V7, P269, DOI 10.1146/annurev-clinpsy-032210-104540; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; LI Y, 2017, IEEE T CIRCUITS SYST; Li YN, 2016, INT C PATT RECOG, P25, DOI 10.1109/ICPR.2016.7899602; Mairesse F., 2007, PROC 45 ANN M ASS CO, P496; Miranda-Correa JA, 2021, IEEE T AFFECT COMPUT, V12, P479, DOI 10.1109/TAFFC.2018.2884461; Mohammadi G, 2015, INT CONF AFFECT, P484, DOI 10.1109/ACII.2015.7344614; Naim Iftekhar, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163127; NIU ZX, 2016, PROC CVPR IEEE, P4920, DOI DOI 10.1109/CVPR.2016.532; NORMAN WT, 1963, J ABNORM PSYCHOL, V66, P574, DOI 10.1037/h0040291; Parkhi Omkar M., 2015, BRIT MACH VIS C; Pennebaker JW, 1999, J PERS SOC PSYCHOL, V77, P1296, DOI 10.1037/0022-3514.77.6.1296; Polzehl T, 2010, IEEE INT C SEMANT CO, P134, DOI 10.1109/ICSC.2010.41; Ponce-Lopez V, 2016, LECT NOTES COMPUT SC, V9915, P400, DOI 10.1007/978-3-319-49409-8_32; Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41; Subramaniam A, 2016, LECT NOTES COMPUT SC, V9915, P337, DOI 10.1007/978-3-319-49409-8_27; Tan ZC, 2018, IEEE T PATTERN ANAL, V40, P2610, DOI 10.1109/TPAMI.2017.2779808; Ventura C, 2017, IEEE COMPUT SOC CONF, P1705, DOI 10.1109/CVPRW.2017.217; Vo NNY, 2018, LECT NOTES ARTIF INT, V10937, P644, DOI 10.1007/978-3-319-93034-3_51; Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070; Wei XS, 2018, IEEE T AFFECT COMPUT, V9, P303, DOI 10.1109/TAFFC.2017.2762299; Xia F, 2017, IEEE SYST J, V11, P2255, DOI 10.1109/JSYST.2014.2342375; Zhang CL, 2016, LECT NOTES COMPUT SC, V9915, P311, DOI 10.1007/978-3-319-49409-8_25; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhao GZ, 2018, IEEE T AFFECT COMPUT, V9, P362, DOI 10.1109/TAFFC.2017.2786207; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11; 2009, CAMBR HDB PERS, P1	53	5	5	2	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2763	2780		10.1007/s11263-020-01309-y	http://dx.doi.org/10.1007/s11263-020-01309-y		MAR 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ					2022-12-18	WOS:000520662000002
J	Feichtenhofer, C; Pinz, A; Wildes, RP; Zisserman, A				Feichtenhofer, Christoph; Pinz, Axel; Wildes, Richard P.; Zisserman, Andrew			Deep Insights into Convolutional Networks for Video Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision; Machine learning; Deep learning; Video recognition; Neural network visualization; Action recognition	COLOR; MOVEMENT; SYSTEM	As the success of deep models has led to their deployment in all areas of computer vision, it is increasingly important to understand how these representations work and what they are capturing. In this paper, we shed light on deep spatiotemporal representations by visualizing the internal representation of models that have been trained to recognize actions in video. We visualize multiple two-stream architectures to show that local detectors for appearance and motion objects arise to form distributed representations for recognizing human actions. Key observations include the following. First, cross-stream fusion enables the learning of true spatiotemporal features rather than simply separate appearance and motion features. Second, the networks can learn local representations that are highly class specific, but also generic representations that can serve a range of classes. Third, throughout the hierarchy of the network, features become more abstract and show increasing invariance to aspects of the data that are unimportant to desired distinctions (e.g. motion patterns across various speeds). Fourth, visualizations can be used not only to shed light on learned representations, but also to reveal idiosyncrasies of training data and to explain failure cases of the system.	[Feichtenhofer, Christoph; Pinz, Axel] Graz Univ Technol, Graz, Austria; [Wildes, Richard P.] York Univ, Toronto, ON, Canada; [Zisserman, Andrew] Univ Oxford, Oxford, England	Graz University of Technology; York University - Canada; University of Oxford	Feichtenhofer, C (corresponding author), Graz Univ Technol, Graz, Austria.	cfeichtenhofer@gmail.com; axel.pinz@tugraz.at; wildes@cse.yorku.ca; az@robots.ox.ac.uk		Feichtenhofer, Christoph/0000-0001-9756-7238				Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2016, NIPS; [Anonymous], 2014, WORKSH INT C LEARN R; [Anonymous], NIPS; [Anonymous], 2015, ICML WORKSH; [Anonymous], 2005, ICCV VS PETS; [Anonymous], P ICLR; [Anonymous], 2018, ARXIV180403308; [Anonymous], 2013, ARXIV13112901 CORR; [Anonymous], 2016, P ECCV; [Anonymous], P CVPR; [Anonymous], 2018, P IEEE C COMP VIS PA; [Anonymous], P CVPR; [Anonymous], 2015, INT C LEARN REPR; [Anonymous], P ECCV; [Anonymous], 2015, ICLR WORKSH; [Anonymous], P ICML; [Anonymous], P CVPR; [Anonymous], 2014, P ICLR; [Anonymous], 2016, NIPS; [Anonymous], 2008, P BMVC; [Anonymous], P ECCV; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Erhan D, 2009, 2018 IEEE INT C MACH; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; GOREA A, 1993, J OPT SOC AM A, V10, P1450, DOI 10.1364/JOSAA.10.001450; GOURAS P, 1974, J PHYSIOL-LONDON, V238, P583, DOI 10.1113/jphysiol.1974.sp010545; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Ioffe S., 2015, P ICML; Kingma Diederik P, 2015, ICLR 2015; Kourtzi Z, 2000, J COGNITIVE NEUROSCI, V12, P48, DOI 10.1162/08989290051137594; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; LIVINGSTONE MS, 1984, J NEUROSCI, V4, P309; Mahendran A, 2016, INT J COMPUT VISION, V120, P233, DOI 10.1007/s11263-016-0911-8; MISHKIN M, 1983, TRENDS NEUROSCI, V6, P414, DOI 10.1016/0166-2236(83)90190-X; Mordvintsev A., 2015, INCEPTIONISM GOING D; REICHARDT W, 1983, BIOL CYBERN, V46, P1, DOI 10.1007/BF00595226; Saleem KS, 2000, J NEUROSCI, V20, P5083; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Simonyan K, 2014, ADV NEUR IN, V27; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Soomro K., 2012, CRCVTR1201; STROMEYER CF, 1984, J OPT SOC AM A, V1, P876, DOI 10.1364/JOSAA.1.000876; Szegedy C., 2015, IEEE C COMPUTER VISI; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zhang J, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901336; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319	60	5	5	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					420	437		10.1007/s11263-019-01225-w	http://dx.doi.org/10.1007/s11263-019-01225-w			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KJ1GX		hybrid, Green Published			2022-12-18	WOS:000511807100006
J	Dong, YP; Ni, RK; Li, JG; Chen, YR; Su, H; Zhu, J				Dong, Yinpeng; Ni, Renkun; Li, Jianguo; Chen, Yurong; Su, Hang; Zhu, Jun			Stochastic Quantization for Learning Accurate Low-Bit Deep Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Compression; Efficient deep neural networks; Low-bit; Stochastic quantization		Low-bit deep neural networks (DNNs) become critical for embedded applications due to their low storage requirement yet high computing efficiency. However, they suffer much from the non-negligible accuracy drop. This paper proposes the stochastic quantization (SQ) algorithm for learning accurate low-bit DNNs. The motivation is due to the following observation. Existing training algorithms approximate the real-valued weights with low-bit representation all together in each iteration. The quantization error may be small for some elements/filters, while is remarkable for others, which leads to inappropriate gradient directions during training, and thus brings notable accuracy drop. Instead, SQ quantizes a portion of elements/filters to low-bit values with a stochastic probability inversely proportional to the quantization error, while keeping the other portion unchanged with full precision. The quantized and full precision portions are updated with their corresponding gradients separately in each iteration. The SQ ratio, which measures the ratio of the quantized weights to all weights, is gradually increased until the whole network is quantized. This procedure can greatly compensate for the quantization error and thus yield better accuracy for low-bit DNNs. Experiments show that SQ can consistently and significantly improve the accuracy for different low-bit DNNs on various datasets and various network structures, no matter whether activation values are quantized or not.	[Dong, Yinpeng; Su, Hang; Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China; [Ni, Renkun] Univ Virginia, Charlottesville, VA USA; [Li, Jianguo; Chen, Yurong] Intel Labs China, Beijing, Peoples R China	Tsinghua University; University of Virginia; Intel Corporation	Li, JG (corresponding author), Intel Labs China, Beijing, Peoples R China.	dyp17@mails.tsinghua.edu.cn; rn9zm@virginia.edu; jianguo.li@intel.com; yurong.chen@intel.com; suhangss@mail.tsinghua.edu.cn; dcszj@mail.tsinghua.edu.cn			National Basic Research Program of China [2013CB329403]; National Natural Science Foundation of China [61620106010, 61621136008]	National Basic Research Program of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was done when Yinpeng Dong and Renkun Ni were interns at Intel Labs supervised by Jianguo Li. Yinpeng Dong, Hang Su and Jun Zhu are also supported by the National Basic Research Program of China (2013CB329403), the National Natural Science Foundation of China (61620106010, 61621136008).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bengio Yoshua, 2013, ESTIMATING PROPAGATI, P4; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen W, 2015, ADV NEURAL INFORM PR; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Dong Y., 2017, BMVC; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2012, COMMUNICATION; Hong S., 2016, ARXIV161108588; Hou L., 2017, INT C LEARN REPR ICL; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Hubara Itay, 2016, QUANTIZED NEURAL NET, P3; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li Fengfu, 2016, NIPS WORKSH EMDNN; Lin DD, 2016, PR MACH LEARN RES, V48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Miyashita Daisuke, 2016, ARXIV160301025; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tang W, 2017, AAAI CONF ARTIF INTE, P2625; Venkatesh G., 2016, ARXIV161000324; Zhou A, 2017, INCREMENTAL NETWORK; Zhou S., 2016, ARXIV160606160; Zhu Chenzhuo, 2017, ICLR	32	5	5	1	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1629	1642		10.1007/s11263-019-01168-2	http://dx.doi.org/10.1007/s11263-019-01168-2			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY					2022-12-18	WOS:000492425300003
J	Wiles, O; Zisserman, A				Wiles, Olivia; Zisserman, Andrew			Learning to Predict 3D Surfaces of Sculptures from Single and Multiple Views	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual hull; Generative model; Silhouette prediction; Depth prediction; Convolutional neural networks; Sculpture dataset	SHAPE	The objective of this work is to reconstruct the 3D surfaces of sculptures from one or more images using a view-dependent representation. To this end, we train a network, SiDeNet, to predict the Silhouette and Depth of the surface given a variable number of images; the silhouette is predicted at a different viewpoint from the inputs (e.g. from the side), while the depth is predicted at the viewpoint of the input images. This has three benefits. First, the network learns a representation of shape beyond that of a single viewpoint, as the silhouette forces it to respect the visual hull, and the depth image forces it to predict concavities (which don't appear on the visual hull). Second, as the network learns about 3D using the proxy tasks of predicting depth and silhouette images, it is not limited by the resolution of the 3D representation. Finally, using a view-dependent representation (e.g. additionally encoding the viewpoint with the input image) improves the network's generalisability to unseen objects. Additionally, the network is able to handle the input views in a flexible manner. First, it can ingest a different number of views during training and testing, and it is shown that the reconstruction performance improves as additional views are added at test-time. Second, the additional views do not need to be photometrically consistent. The network is trained and evaluated on two synthetic datasets-a realistic sculpture dataset (SketchFab), and ShapeNet. The design of the network is validated by comparing to state of the art methods for a set of tasks. It is shown that (i) passing the input viewpoint (i.e. using a view-dependent representation) improves the network's generalisability at test time. (ii) Predicting depth/silhouette images allows for higher quality predictions in 2D, as the network is not limited by the chosen latent 3D representation. (iii) On both datasets the method of combining views in a global manner performs better than a local method. Finally, we show that the trained network generalizes to real images, and probe how the network has encoded the latent 3D shape.	[Wiles, Olivia; Zisserman, Andrew] Univ Oxford, Dept Engn Sci, Oxford, England	University of Oxford	Wiles, O (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	ow@robots.ox.ac.uk			EPSRC studentship; EPSRC Programme Grant [Seebibyte EP/M013774/1]; EPSRC [1798398, EP/M013774/1] Funding Source: UKRI	EPSRC studentship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC Programme Grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Thank you to Andrew Fitzgibbon and the anonymous reviewers for their useful comments. This work was funded by an EPSRC studentship and EPSRC Programme Grant Seebibyte EP/M013774/1.	[Anonymous], 2017, P IEEE C COMP VIS PA; [Anonymous], [No title captured]; [Anonymous], 2001, SPRINGE SER STAT N; [Anonymous], [No title captured]; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; BLAKE A, 1990, ARTIF INTELL, V45, P323, DOI 10.1016/0004-3702(90)90011-N; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blender Online Community, 2017, BLEND A 3D MOD REND; Boyer E., 2003, P IEEE C COMP VIS PA; Cashman TJ, 2013, IEEE T PATTERN ANAL, V35, P232, DOI 10.1109/TPAMI.2012.68; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Choy C., 2016, P EUR C COMP VIS; DeVito Z., 2017, P NIPS 2017 WORKSH A; Fan H., 2016, P IEEE C COMP VIS PA; Fouhey D. F., 2015, P INT C COMP VIS; Fu MY, 2016, NEURAL PLAST, V2016, DOI 10.1155/2016/3512098; Gadelha M., 2016, ARXIV161205872; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Groueix T., 2018, P IEEE C COMP VIS PA; Hartley R., 2004, ROBOTICA; Hedau V., 2009, P INT C COMP VIS; Isola P, 2017, P 2017 IEEE C COMP V; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Kar A., 2017, ADV NEURAL INFORM PR; Kar A., 2015, P IEEE C COMP VIS PA; Kolev K, 2009, INT J COMPUT VISION, V84, P80, DOI 10.1007/s11263-009-0233-1; Koltun V., 2018, CORR; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; Liu ZX, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-12639-2; Mahendran A., 2015, P IEEE C COMP VIS PA; Matusik W., 2000, P ACM SIGGRAPH C COM; Novotny D., 2017, P INT C COMP VIS; Park E., 2017, P IEEE C COMP VIS PA; Prasad M., 2010, P IEEE C COMP VIS PA; Qi C, 2016, IEEE CONF COMPUT; Rezende DJ, 2016, ADV NEUR IN, V29; Rock J., 2015, P IEEE C COMP VIS PA; Ronneberger O., 2015, P INT C MED IM COMP, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28], Patent No. [ArXiv150504597Cs, 150504597]; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Sinha A., 2017, P IEEE C COMP VIS PA; Soltani A. A., 2017, P IEEE C COMP VIS PA; Su H., 2015, P INT C COMP VIS; Tatarchenko M., 2016, P EUR C COMP VIS; Upchurch P., 2017, P IEEE C COMP VIS PA; Vicente S., 2014, P IEEE C COMP VIS PA; Vogiatzis G., 2003, P BRIT MACH VIS C, P711; WITKIN AP, 1981, ARTIF INTELL, V17, P17, DOI 10.1016/0004-3702(81)90019-9; Wu Jiajun, 2017, ADV NEURAL INFORM PR; Xiang Y., 2014, P IEEE WORKSH APPL C; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; Zhu R, 2017, IEEE I CONF COMP VIS, P57, DOI 10.1109/ICCV.2017.16; Zollhofer M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766887	54	5	6	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1780	1800		10.1007/s11263-018-1124-0	http://dx.doi.org/10.1007/s11263-018-1124-0			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY		Green Published, hybrid			2022-12-18	WOS:000492425300013
J	Lasinger, K; Vogel, C; Pock, T; Schindler, K				Lasinger, Katrin; Vogel, Christoph; Pock, Thomas; Schindler, Konrad			3D Fluid Flow Estimation with Integrated Particle Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D PIV/PTV; Variational flow estimation; Joint motion estimation; 3D reconstruction	ALTERNATING LINEARIZED MINIMIZATION; NONCONVEX	The standard approach to densely reconstruct the motion in a volume of fluid is to inject high-contrast tracer particles and record their motion with multiple high-speed cameras. Almost all existing work processes the acquired multi-view video in two separate steps, utilizing either a pure Eulerian or pure Lagrangian approach. Eulerian methods perform a voxel-based reconstruction of particles per time step, followed by 3D motion estimation, with some form of dense matching between the precomputed voxel grids from different time steps. In this sequential procedure, the first step cannot use temporal consistency considerations to support the reconstruction, while the second step has no access to the original, high-resolution image data. Alternatively, Lagrangian methods reconstruct an explicit, sparse set of particles and track the individual particles over time. Physical constraints can only be incorporated in a post-processing step when interpolating the particle tracks to a dense motion field. We show, for the first time, how to jointly reconstruct both the individual tracer particles and a dense 3D fluid motion field from the image data, using an integrated energy minimization. Our hybrid Lagrangian/Eulerian model reconstructs individual particles, and at the same time recovers a dense 3D motion field in the entire domain. Making particles explicit greatly reduces the memory consumption and allows one to use the high-resolution input images for matching. Whereas the dense motion field makes it possible to include physical a-priori constraints and account for the incompressibility and viscosity of the fluid. The method exhibits greatly (approximate to 70%) improved results over our recently published baseline with two separate steps for 3D reconstruction and motion estimation. Our results with only two time steps are comparable to those of state-of-the-art tracking-based methods that require much longer sequences.	[Lasinger, Katrin; Schindler, Konrad] Swiss Fed Inst Technol, Photogrammetry & Remote Sensing, Zurich, Switzerland; [Vogel, Christoph; Pock, Thomas] Graz Univ Technol, Inst Comp Graph & Vis, Graz, Austria	Swiss Federal Institutes of Technology Domain; ETH Zurich; Graz University of Technology	Lasinger, K (corresponding author), Swiss Fed Inst Technol, Photogrammetry & Remote Sensing, Zurich, Switzerland.	katrin.lasinger@geod.baug.ethz.ch; christoph.vogel@icg.tugraz.at; pock@icg.tugraz.at; schindler@geod.baug.ethz.ch		Lasinger, Katrin/0000-0002-9329-2916	ETH [29 14-1]; ERC [640156]	ETH(ETH Zurich); ERC(European Research Council (ERC)European Commission)	This work was supported by ETH Grant 29 14-1. Thomas Pock and Christoph Vogel acknowledge support from the ERC starting Grant 640156, 'HOMOVIS'. We thank Daniel Schanz for kindly sharing their results on the 4th PIV Challenge and for providing experimental data in water.	Adams B., 2007, ACM SIGGRAPH; Adrian RJ., 2011, PARTICLE IMAGE VELOC; Atkinson C, 2009, EXP FLUIDS, V47, P553, DOI 10.1007/s00348-009-0728-0; Barbu I., 2013, 10 INT S PART IM VEL; Basha T., 2010, CVPR; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Boyd S, 2004, CONVEX OPTIMIZATION; Brezzi F., 1991, 15 SPRINGER SERIES C, V15; Champagnat F, 2011, EXP FLUIDS, V50, P1169, DOI 10.1007/s00348-011-1054-x; Cheminet A., 2014, INT S APPL LAS TECHN; Courant R., 1943, B AM MATH SOC, V49, P1, DOI 10.1090/S0002-9904-1943-07818-4; Dalitz R., 2017, SSVM; Discetti S, 2012, EXP FLUIDS, V53, P1437, DOI 10.1007/s00348-012-1370-9; Elsinga GE, 2006, EXP FLUIDS, V41, P933, DOI 10.1007/s00348-006-0212-z; Gesemann S., 2016, 10 INT S APPL LAS TE; Gregson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601147; Hartley R. I., 1997, PROC DARPA IMAGE UND, P649; Huguet F., 2007, ICCV; Kahler CJ, 2016, EXP FLUIDS, V57, DOI 10.1007/s00348-016-2173-1; Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129; Langtangen HP, 2002, ADV WATER RESOUR, V25, P1125, DOI 10.1016/S0309-1708(02)00052-0; Lasinger Katrin, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P315, DOI 10.1007/978-3-030-12939-2_22; Lasinger K., 2017, ICCV; Lasinger K, 2018, MEAS SCI TECHNOL, V29, DOI 10.1088/1361-6501/aab5a0; Li Y, 2008, J TURBUL, V9, P1, DOI 10.1080/14685240802376389; MAAS HG, 1993, EXP FLUIDS, V15, P133, DOI 10.1007/BF00190953; Menze Moritz, 2015, CVPR; Michaelis D., 2006, ISFV12; Michalec FG, 2015, EUR PHYS J E, V38, DOI 10.1140/epje/i2015-15108-2; Monaghan JJ, 2005, REP PROG PHYS, V68, P1703, DOI 10.1088/0034-4885/68/8/R01; Perlman E., 2007, C SUP; Petra S., 2009, IMAGING MEASUREMENT; Pock T, 2016, SIAM J IMAGING SCI, V9, P1756, DOI 10.1137/16M1064064; Rabe C., 2010, ECCV; Raffel M., 2018, PARTICLE IMAGE VELOC, V3rd; Reddy J.N., 1993, INTRO FINITE ELEMENT; Richard A., 2017, 28 BRIT MACH VIS C B; Ruhnau P, 2005, MEAS SCI TECHNOL, V16, P1449, DOI 10.1088/0957-0233/16/7/007; Ruhnau P., 2006, GCPR; Ruhnau P, 2007, EXP FLUIDS, V42, P61, DOI 10.1007/s00348-006-0220-z; Schanz D, 2016, EXP FLUIDS, V57, DOI 10.1007/s00348-016-2157-1; Schanz D, 2013, MEAS SCI TECHNOL, V24, DOI 10.1088/0957-0233/24/2/024009; Schneiders JFG, 2016, EXP FLUIDS, V57, DOI 10.1007/s00348-016-2225-6; Soloff SM, 1997, MEAS SCI TECHNOL, V8, P1441, DOI 10.1088/0957-0233/8/12/008; Taylor C., 1973, Computers & Fluids, V1, P73, DOI 10.1016/0045-7930(73)90027-3; Tompson J., 2016, CORR; Valgaerts Levi, 2010, ECCV; Vogel C., 2011, ICCV; Vogel C., 2013, ICCV; Vogel C, 2015, INT J COMPUT VISION, V115, P1, DOI 10.1007/s11263-015-0806-0; Wedel A, 2011, INT J COMPUT VISION, V95, P29, DOI 10.1007/s11263-010-0404-0; Wieneke B, 2008, EXP FLUIDS, V45, P549, DOI 10.1007/s00348-008-0521-5; Wieneke B, 2013, MEAS SCI TECHNOL, V24, DOI 10.1088/0957-0233/24/2/024008; Wu Z., 2009, ICCV; Xiong JH, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073662; Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298	60	5	5	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		1012	1027		10.1007/s11263-019-01261-6	http://dx.doi.org/10.1007/s11263-019-01261-6		NOV 2019	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Green Accepted, Green Submitted			2022-12-18	WOS:000496254700001
J	Gygli, M; Ferrari, V				Gygli, Michael; Ferrari, Vittorio			Efficient Object Annotation via Speaking and Pointing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Speech-based annotation; Object annotation; Multimodal interfaces; Large-scale computer vision	EYE-MOVEMENTS; SEARCH	Deep neural networks deliver state-of-the-art visual recognition, but they rely on large datasets, which are time-consuming to annotate. These datasets are typically annotated in two stages: (1) determining the presence of object classes at the image level and (2) marking the spatial extent for all objects of these classes. In this work we use speech, together with mouse inputs, to speed up this process. We first improve stage one, by letting annotators indicate object class presence via speech. We then combine the two stages: annotators draw an object bounding box via the mouse and simultaneously provide its class label via speech. Using speech has distinct advantages over relying on mouse inputs alone. First, it is fast and allows for direct access to the class name, by simply saying it. Second, annotators can simultaneously speak and mark an object location. Finally, speech-based interfaces can be kept extremely simple, hence using them requires less mouse movement compared to existing approaches. Through extensive experiments on the COCO and ILSVRC datasets we show that our approach yields high-quality annotations at significant speed gains. Stage one takes 2.3x - 14.9x less annotation time than existing methods based on a hierarchical organization of the classes to be annotated. Moreover, when combining the two stages, we find that object class labels come for free: annotating them at the same time as bounding boxes has zero additional cost. On COCO, this makes the overall process 1.9x faster than the two-stage approach.	[Gygli, Michael; Ferrari, Vittorio] Google Res, Zurich, Switzerland	Google Incorporated	Gygli, M (corresponding author), Google Res, Zurich, Switzerland.	gyglim@google.com; vittoferrari@google.com						Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Bolt R.A., 1980, P 7 ANN C COMP GRAPH, P262, DOI 10.1145/800250.807503; Clarkson E., 2005, CHI; Dai D, 2016, THESIS; Damen D., 2018, P EUR C COMP VIS; Deng J., 2014, CHI; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ehinger KA, 2009, VIS COGN, V17, P945, DOI 10.1080/13506280902834720; Gygli M., 2019, P IEEE C COMP VIS PA; Harwath David, 2018, P EUR C COMP VIS ECC, P2; Hauptmann A. G., 1989, SIGCHI Bulletin, P241, DOI 10.1145/67450.67496; Kahneman D., 1973, ATTENTION EFFORT; Karat CM., 1999, ACM SIGCHI; Krishna RA., 2016, CHI; Kuznetsova Alina, 2018, ARXIV181100982; Laradji IH., 2018, ARXIV180709856; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lleras A, 2005, PSYCHOL SCI, V16, P684, DOI 10.1111/j.1467-9280.2005.01596.x; Manen S, 2017, IEEE I CONF COMP VIS, P290, DOI 10.1109/ICCV.2017.40; Mettes P, 2016, LECT NOTES COMPUT SC, V9909, P437, DOI 10.1007/978-3-319-46454-1_27; Mikolov T., 2013, ARXIV; NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4; Oviatt S, 1996, ACM SIGCHI; OVIATT S, 2003, HUM FAC ER, P286; Oviatt S., 1997, CHI; Papadopoulos DP, 2017, IEEE I CONF COMP VIS, pCP38, DOI 10.1109/ICCV.2017.528; Papadopoulos Dim P, 2017, CVPR; Pausch R., 1991, J AM VOICE INPUT OUT; Pont-Tuset J., 2019, ARXIV190601542; Rayner K, 2009, Q J EXP PSYCHOL, V62, P1457, DOI 10.1080/17470210902816461; Russakovsky O, 2015, PROC CVPR IEEE, P2121, DOI 10.1109/CVPR.2015.7298824; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Su Hao, 2012, AAAI WORKSH; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Vaidyanathan P., 2018, P ASS COMP LING; Vasudevan AB., 2017, C COMP VIS PATT REC; Watson DG, 2007, PSYCHON B REV, V14, P852, DOI 10.3758/BF03194111	37	5	5	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1061	1075		10.1007/s11263-019-01255-4	http://dx.doi.org/10.1007/s11263-019-01255-4		NOV 2019	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Green Submitted			2022-12-18	WOS:000495710500001
J	Bas, A; Smith, WAP				Bas, Anil; Smith, William A. P.			What Does 2D Geometric Information Really Tell Us About 3D Face Shape?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D morphable face model; Shape ambiguity; Perspective projection; Landmarks	NONRIGID SHAPE; RECOGNITION; RECONSTRUCTION; MODELS	A face image contains geometric cues in the form of configurational information and contours that can be used to estimate 3D face shape. While it is clear that 3D reconstruction from 2D points is highly ambiguous if no further constraints are enforced, one might expect that the face-space constraint solves this problem. We show that this is not the case and that geometric information is an ambiguous cue. There are two sources for this ambiguity. The first is that, within the space of 3D face shapes, there are flexibility modes that remain when some parts of the face are fixed. The second occurs only under perspective projection and is a result of perspective transformation as camera distance varies. Two different faces, when viewed at different distances, can give rise to the same 2D geometry. To demonstrate these ambiguities, we develop new algorithms for fitting a 3D morphable model to 2D landmarks or contours under either orthographic or perspective projection and show how to compute flexibility modes for both cases. We show that both fitting problems can be posed as a separable nonlinear least squares problem and solved efficiently. We demonstrate both quantitatively and qualitatively that the ambiguity is present in reconstructions from geometric information alone but also in reconstructions from a state-of-the-art CNN-based method.	[Bas, Anil; Smith, William A. P.] Univ York, Dept Comp Sci, York, N Yorkshire, England; [Bas, Anil] Marmara Univ, Dept Comp Engn, Istanbul, Turkey	University of York - UK; Marmara University	Bas, A (corresponding author), Univ York, Dept Comp Sci, York, N Yorkshire, England.; Bas, A (corresponding author), Marmara Univ, Dept Comp Engn, Istanbul, Turkey.	ab1792@york.ac.uk; william.smith@york.ac.uk	Smith, William/AAK-9101-2020; Bas, Anil/X-1836-2019	Smith, William/0000-0002-6047-0413; Bas, Anil/0000-0002-3833-6023				Albrecht T., 2008, P WORKSH MATH FDN CO; Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206; Amberg B., 2007, P ICCV; [Anonymous], 2016, AS C COMP VIS WORKSH, P377; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Blanz V, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P293, DOI 10.1109/TDPVT.2004.1335212; Booth J, 2018, IEEE T PATTERN ANAL, V40, P2638, DOI 10.1109/TPAMI.2018.2832138; Bryan R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0045301; Burgos-Artizzu XP, 2014, LECT NOTES COMPUT SC, V8689, P313, DOI 10.1007/978-3-319-10590-1_21; Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012; Cootes T. F., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P484, DOI 10.1007/BFb0054760; Ecker A, 2008, LECT NOTES COMPUT SC, V5302, P127, DOI 10.1007/978-3-540-88682-2_11; Farkas LG, 1994, ANTHROPOMETRY HEAD F; Feng ZH, 2018, IEEE INT CONF AUTOMA, P780, DOI 10.1109/FG.2018.00123; Flores A, 2013, LECT NOTES COMPUT SC, V8034, P513, DOI 10.1007/978-3-642-41939-3_50; Fried O, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925933; Gallego G, 2015, J MATH IMAGING VIS, V51, P378, DOI 10.1007/s10851-014-0528-x; Genova K., 2018, P CVPR; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Golub G, 2003, INVERSE PROBL, V19, pR1, DOI 10.1088/0266-5611/19/2/201; Guler R. A., 2017, P CVPR; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hartley R, 2008, LECT NOTES COMPUT SC, V5302, P276, DOI 10.1007/978-3-540-88682-2_22; HILL H, 1994, PERCEPTION, V23, P1335, DOI 10.1068/p231335; Keller M, 2007, LECT NOTES COMPUT SC, V4418, P261; Kleinberg KF, 2007, J FORENSIC SCI, V52, P779, DOI 10.1111/j.1556-4029.2007.00458.x; Knothe R, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P637; Kuhtreiber WM, 2018, NPJ VACCINES, V3, DOI 10.1038/s41541-018-0062-8; Latto R, 2007, LEONARDO, V40, P243, DOI 10.1162/leon.2007.40.3.243; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Liu CH, 2003, VISION RES, V43, P2393, DOI 10.1016/S0042-6989(03)00429-2; Liu CH, 2006, PERCEPTION, V35, P1637, DOI 10.1068/p5545; Luthi M, 2009, LECT NOTES COMPUT SC, V5654, P251, DOI 10.1007/978-3-642-03596-8_14; Moreno-Noguer F, 2013, IEEE T PATTERN ANAL, V35, P463, DOI 10.1109/TPAMI.2012.102; Patel A, 2016, PATTERN RECOGN, V52, P206, DOI 10.1016/j.patcog.2015.10.003; Patel A, 2009, PROC CVPR IEEE, P1327, DOI 10.1109/CVPRW.2009.5206522; Paysan Pascal, 2009, P AVSS; Perona P, 2007, J VISION, V7, P992, DOI [10.1167/7.9.992, DOI 10.1167/7.9.992]; PIOTRASCHKE M, 2016, PROC CVPR IEEE, P3418, DOI DOI 10.1109/CVPR.2016.372; Porter G, 2000, FORENSIC SCI INT, V114, P97, DOI 10.1016/S0379-0738(00)00290-5; Prados E, 2009, LECT NOTES COMPUT SC, V5567, P696, DOI 10.1007/978-3-642-02256-2_58; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Saito S, 2016, LECT NOTES COMPUT SC, V9912, P244, DOI 10.1007/978-3-319-46484-8_15; Salzmann M., 2007, COMP VIS 2007 ICCV 2, P1; Sanyal Soubhik, 2019, P CVPR; Schonborn S, 2017, INT J COMPUT VISION, V123, P160, DOI 10.1007/s11263-016-0967-5; Schumacher M, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2325722.2325724; Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175; Smith WAP, 2006, IEEE T PATTERN ANAL, V28, P1914, DOI 10.1109/TPAMI.2006.251; Smith WAP, 2016, MATH VIS, P299, DOI 10.1007/978-3-319-24726-7_14; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tewari A, 2017, IEEE I CONF COMP VIS, P3735, DOI 10.1109/ICCV.2017.401; Tewari Ayush, 2018, P CVPR; Tran AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163; Valente Joachim, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P9, DOI 10.1109/CVPRW.2015.7301314; Wu CL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980233; Wu Y., 2018, ARXIV180306542; Yu R., 2017, P CVPR, P4723; Zhou XW, 2015, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2015.7299074; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679	64	5	5	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1455	1473		10.1007/s11263-019-01197-x	http://dx.doi.org/10.1007/s11263-019-01197-x			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL		Green Submitted, hybrid, Green Accepted			2022-12-18	WOS:000485320300005
J	Buades, A; Duran, J; Navarro, J				Buades, A.; Duran, J.; Navarro, J.			Motion-Compensated Spatio-Temporal Filtering for Multi-Image and Multimodal Super-Resolution	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video super-resolution; Non-linear 3D filter; Nonlocal regularization; Light-field super-resolution; Multimodal super-resolution	NONLOCAL REGULARIZATION; GENERAL FRAMEWORK; VARIATIONAL MODEL; TV-REGULARIZATION; SUPER RESOLUTION; OPTIMIZATION; ALGORITHMS; IMAGES; ROBUST; SHAPE	The classical multi-image super-resolution model assumes that the super-resolved image is related to the low-resolution frames by warping, convolution and downsampling. State-of-the-art algorithms either use explicit registration to fuse the information for each pixel in its trajectory or exploit spatial and temporal similarities. We propose to combine both ideas, making use of inter-frame motion and exploiting spatio-temporal redundancy with patch-based techniques. We introduce a non-linear filtering approach that combines patches from several frames not necessarily belonging to the same pixel trajectory. The selection of candidate patches depends on a motion-compensated 3D distance, which is robust to noise and aliasing. The selected 3D volumes are then sliced per frame, providing a collection of 2D patches which are finally averaged depending on their similarity to the reference one. This makes the upsampling strategy robust to flow inaccuracies and occlusions. Total variation and nonlocal regularization are used in the deconvolution stage. The experimental results demonstrate the state-of-the-art performance of the proposed method for the super-resolution of videos and light-field images. We also adapt our approach to multimodal sequences when some additional data at the desired resolution is available.	[Buades, A.; Duran, J.; Navarro, J.] Univ Illes Balears, DMI IAC3, Cra Valldemossa Km 7-5, Palma De Mallorca 07122, Spain	Universitat de les Illes Balears	Duran, J (corresponding author), Univ Illes Balears, DMI IAC3, Cra Valldemossa Km 7-5, Palma De Mallorca 07122, Spain.	toni.buades@uib.es; joan.duran@uib.es; julia.navarro@uib.es	Duran, Joan/N-4203-2019; buades, antoni/K-6110-2014; Navarro, Julia/I-9059-2017	Duran, Joan/0000-0003-0043-1663; buades, antoni/0000-0001-9832-3358; Navarro, Julia/0000-0003-3667-7008	 [TIN2014-53772-R];  [TIN2017-85572-P]	; 	The authors were supported by Grants TIN2014-53772-R and TIN2017-85572-P (MINECO/AEI/FEDER, UE).	Al Ismaeil K, 2016, COMPUT VIS IMAGE UND, V147, P38, DOI 10.1016/j.cviu.2016.04.006; Alain M, 2018, IEEE IMAGE PROC, P2501, DOI 10.1109/ICIP.2018.8451162; [Anonymous], 2006, DIGITAL LIGHT FIELD, P1; Arias P, 2011, INT J COMPUT VISION, V93, P319, DOI 10.1007/s11263-010-0418-7; Bodduna K, 2017, LECT NOTES COMPUT SC, V10302, P590, DOI 10.1007/978-3-319-58771-4_47; Boominathan V., 2014, ICCP, P1; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024; Buades A., 2017, P BRIT MACH VIS C BM; Buades A, 2016, IEEE T IMAGE PROCESS, V25, P2573, DOI 10.1109/TIP.2016.2551639; Burger M, 2018, SIAM J IMAGING SCI, V11, P94, DOI 10.1137/16M1084183; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304; Chambolle A, 2004, J MATH IMAGING VIS, V20, P89; Chambolle A, 2016, ACTA NUMER, V25, P161, DOI 10.1017/S096249291600009X; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; CHAN D, 2008, P ECCV WORKSH MULT M; Cui Y, 2013, IEEE T PATTERN ANAL, V35, P1039, DOI 10.1109/TPAMI.2012.190; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Danielyan A., 2008, P INT WORK LOCAL NON, P8; Diebel J., 2006, ADV NEURAL INFORM PR, P291; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Duchi J., 2008, PROC 25 INT C MACH L, P272; Duran J, 2016, SIAM J IMAGING SCI, V9, P116, DOI 10.1137/15M102873X; Duran J, 2014, SIAM J IMAGING SCI, V7, P761, DOI 10.1137/130928625; Duran J, 2016, IMAGE PROCESS ON LIN, V6, P27, DOI 10.5201/ipol.2016.141; Duran J, 2015, LECT NOTES COMPUT SC, V8932, P141, DOI 10.1007/978-3-319-14612-6_11; Ebrahimi Mehran, 2008, Proceedings of the 2008 International Conference on Image Processing, Computer Vision & Pattern Recognition. IPCV 2008, P455; Elad M, 1999, IEEE T IMAGE PROCESS, V8, P387, DOI 10.1109/83.748893; Esser E, 2010, SIAM J IMAGING SCI, V3, P1015, DOI 10.1137/09076934X; Farrugia RA, 2017, IEEE J-STSP, V11, P1058, DOI 10.1109/JSTSP.2017.2747127; Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669; Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Garcia F., 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P42, DOI 10.1109/AVSS.2011.6027291; Garcia F, 2015, IMAGE VISION COMPUT, V41, P26, DOI 10.1016/j.imavis.2015.06.008; Gilboa G, 2007, MULTISCALE MODEL SIM, V6, P595, DOI 10.1137/060669358; Ham B, 2015, PROC CVPR IEEE, P4823, DOI 10.1109/CVPR.2015.7299115; Huang Y., 2015, P ADV NEURAL INFORM, V28, P235; Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936; Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22; Ilg E., 2017, P IEEE C COMP VIS PA, V2, P6; Jung MY, 2011, IEEE T IMAGE PROCESS, V20, P1583, DOI 10.1109/TIP.2010.2092433; Junyi Liu, 2013, Advances in Multimedia Information Processing - PCM 2013. 14th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8294, P408, DOI 10.1007/978-3-319-03731-8_38; Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323; Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926; Kindermann S, 2005, MULTISCALE MODEL SIM, V4, P1091, DOI 10.1137/050622249; Kolb A, 2010, COMPUT GRAPH FORUM, V29, P141, DOI 10.1111/j.1467-8659.2009.01583.x; Kondermann D, 2016, IEEE COMPUT SOC CONF, P19, DOI 10.1109/CVPRW.2016.10; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Levin A, 2008, LECT NOTES COMPUT SC, V5305, P88, DOI 10.1007/978-3-540-88693-8_7; Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68; Liu C, 2011, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2011.5995614; Ma ZY, 2015, PROC CVPR IEEE, P5224, DOI 10.1109/CVPR.2015.7299159; Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8; Mateos J, 2009, IEEE IMAGE PROC, P129, DOI 10.1109/ICIP.2009.5414169; Mignotte M, 2008, PATTERN RECOGN LETT, V29, P2206, DOI 10.1016/j.patrec.2008.08.004; Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164; Mitra K., 2012, IEEE C COMP VIS PATT, P22; Mitzel D, 2009, LECT NOTES COMPUT SC, V5748, P432, DOI 10.1007/978-3-642-03798-6_44; Nasrollahi K, 2014, MACH VISION APPL, V25, P1423, DOI 10.1007/s00138-014-0623-4; Navarro J., 2018, P IEEE INT C IM PROC; Navarro J, 2017, IEEE T IMAGE PROCESS, V26, P1873, DOI 10.1109/TIP.2017.2666041; Navarro J, 2016, IEEE IMAGE PROC, P1449, DOI 10.1109/ICIP.2016.7532598; Ng R., 2005, 200502 CSTR STANF U; Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423; Perwass C, 2010, CISC VIS NETW IND GL; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Peyre G, 2008, LECT NOTES COMPUT SC, V5304, P57, DOI 10.1007/978-3-540-88690-7_5; Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067; Rossi M., 2017, INT DES ENG TECHN C, P1; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Schuon S, 2009, PROC CVPR IEEE, P343, DOI 10.1109/CVPRW.2009.5206804; Seifi M, 2014, IEEE IMAGE PROC, P5482, DOI 10.1109/ICIP.2014.7026109; Smolic a, 2017, MMSP IEEE, P1; Spies H., 2000, P ECCV, V1843, P785; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Tao X., 2017, P IEEE INT C COMP VI, P2380; Unger M, 2010, LECT NOTES COMPUT SC, V6376, P313; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZY, 2015, IEEE T IMAGE PROCESS, V24, P4359, DOI 10.1109/TIP.2015.2462113; Wanner S., 2013, VISION MODELING VISU, P225, DOI DOI 10.2312/PE.VMV.VMV13.225-226; Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147; Wanner S, 2012, LECT NOTES COMPUT SC, V7576, P608, DOI 10.1007/978-3-642-33715-4_44; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211; Wu JD, 2015, 2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP), DOI 10.1109/VCIP.2015.7457904; YAMAMOTO M, 1993, IEEE T PATTERN ANAL, V15, P82, DOI 10.1109/34.184776; Yoon Y, 2017, IEEE SIGNAL PROC LET, V24, P848, DOI 10.1109/LSP.2017.2669333; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zhang XQ, 2010, SIAM J IMAGING SCI, V3, P253, DOI 10.1137/090746379; Zheng HT, 2017, IEEE INT CONF COMP V, P2481, DOI 10.1109/ICCVW.2017.292	95	5	5	1	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1474	1500		10.1007/s11263-019-01200-5	http://dx.doi.org/10.1007/s11263-019-01200-5			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL					2022-12-18	WOS:000485320300006
J	Tommasi, T; Mallya, A; Plummer, B; Lazebnik, S; Berg, AC; Berg, TL				Tommasi, Tatiana; Mallya, Arun; Plummer, Bryan; Lazebnik, Svetlana; Berg, Alexander C.; Berg, Tamara L.			Combining Multiple Cues for Visual Madlibs Question Answering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual question answering; Cue integration; Region phrase correspondence; Computer vision; Language		This paper presents an approach for answering fill-in-the-blank multiple choice questions from the Visual Madlibs dataset. Instead of generic and commonly used representations trained on the ImageNet classification task, our approach employs a combination of networks trained for specialized tasks such as scene recognition, person activity classification, and attribute prediction. We also present a method for localizing phrases from candidate answers in order to provide spatial support for feature extraction. We map each of these features, together with candidate answers, to a joint embedding space through normalized canonical correlation analysis (nCCA). Finally, we solve an optimization problem to learn to combine scores from nCCA models trained on multiple cues to select the best answer. Extensive experimental results show a significant improvement over the previous state of the art and confirm that answering questions from a wide range of types benefits from examining a variety of image cues and carefully choosing the spatial support for feature extraction.	[Tommasi, Tatiana] Italian Inst Technol, Milan, Italy; [Mallya, Arun; Plummer, Bryan; Lazebnik, Svetlana] Univ Illinois, Urbana, IL USA; [Berg, Alexander C.; Berg, Tamara L.] Univ N Carolina, Chapel Hill, NC 27515 USA	Istituto Italiano di Tecnologia - IIT; University of Illinois System; University of Illinois Urbana-Champaign; University of North Carolina; University of North Carolina Chapel Hill	Tommasi, T (corresponding author), Italian Inst Technol, Milan, Italy.	tatiana.tommasi@iit.it		Tommasi, Tatiana/0000-0001-8229-7159	National Science Foundation [1302438, 1563727, 1405822, 1444234, 1562098, 1633295, 1452851]; Microsoft Research Faculty Fellowship; Sloan Foundation Fellowship; ERC [637076 - RoboExNovo]; XeroxUAC	National Science Foundation(National Science Foundation (NSF)); Microsoft Research Faculty Fellowship(Microsoft); Sloan Foundation Fellowship(Alfred P. Sloan Foundation); ERC(European Research Council (ERC)European Commission); XeroxUAC	This material is based upon work supported by the National Science Foundation under Grants 1302438, 1563727, 1405822, 1444234, 1562098, 1633295, 1452851, XeroxUAC, Microsoft Research Faculty Fellowship, and the Sloan Foundation Fellowship. T.T. was partially supported by the ERC Grant 637076 - RoboExNovo.	Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Auer S, 2007, INT SEM WEB C AS SEM; Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413; Chao YW, 2015, IEEE I CONF COMP VIS, P1017, DOI 10.1109/ICCV.2015.122; Duchi J., 2008, PROC 25 INT C MACH L, P272; Fukui Akira, 2016, ARXIV160601847; Gao H, 2015, NEURAL INFORM PROCES; Geman D, 2015, P NATL ACAD SCI USA, V112, P3618, DOI 10.1073/pnas.1422953112; Girshick R., 2015, ICCV; Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Ilievski I., 2016, ARXIV160401485; Lassila O, 1999, TECH REP; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d; Lyu S., 2005, IEEE C COMP VIS PATT; Malinowski M., 2014, ADV NEURAL INFORM PR, V27, P1682; Malinowski M, 2015, NEURAL INFORM PROCES; Mallya A, 2016, LECT NOTES COMPUT SC, V9905, P414, DOI 10.1007/978-3-319-46448-0_25; Mikolov T., 2013, ARXIV; Mokarian A, 2016, BRIT MACH VIS C BMVC; Pishchulin L, 2014, LECT NOTES COMPUT SC, V8753, P678, DOI 10.1007/978-3-319-11752-2_56; Quoc Le, 2014, P 31 INT C MACHINE L, V32, P1188; Ren M, 2015, NEURAL INFORM PROCES; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saito K, 2017, IEEE INT CON MULTI, P829, DOI 10.1109/ICME.2017.8019436; Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499; Socher R., 2013, LONG PAPERS, V1, P455; Sudowe P., 2015, P IEEE INT C COMP VI, P87; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tandon N, 2014, BRIT MACH VIS C BMVC; Wang P, 2017, IEEE T PATTERN ANAL; Wang PS, 2017, PROC CVPR IEEE, P3966, DOI 10.1109/CVPR.2017.422; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500; Wu Qi, 2016, ARXIV160302814; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xu H., 2015, ARXIV151105234; Yang Z, 2016, IEEE CONF COMPUT; Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446; Yu L, 2015, IEEE INT C COMP VIS; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhu YX, 2016, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2016.415; Zhu Yuke, 2015, ARXIV150705670	50	5	5	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2019	127	1					38	60		10.1007/s11263-018-1096-0	http://dx.doi.org/10.1007/s11263-018-1096-0			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HK6AD		Green Submitted			2022-12-18	WOS:000458050000003
J	Wei, X; Yang, QX; Gong, YH				Wei, Xing; Yang, Qingxiong; Gong, Yihong			Joint Contour Filtering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Contour analysis; Edge-preserving filter; Structure-preserving filter; Scale-aware filter	TEXTURE IMAGE; DECOMPOSITION; PHOTOGRAPHY; SCALE; FLASH	Edge/structure-preserving operations for images aim to smooth images without blurring the edges/structures. Many exemplary edge-preserving filtering methods have recently been proposed to reduce the computational complexity and/or separate structures of different scales. They normally adopt a user-selected scale measurement to control the detail smoothing. However, natural photos contain objects of different sizes, which cannot be described by a single scale measurement. On the other hand, contour analysis is closely related to edge-preserving filtering, and significant progress has recently been achieved. Nevertheless, the majority of state-of-the-art filtering techniques have ignored the successes in this area. Inspired by the fact that learning-based edge detectors significantly outperform traditional manually-designed detectors, this paper proposes a learning-based edge-preserving filtering technique. It synergistically combines the differential operations in edge-preserving filters with the effectiveness of the recent edge detectors for scale-aware filtering. Unlike previous filtering methods, the proposed filters can efficiently extract subjectively meaningful structures from natural scenes containing multiple-scale objects.	[Wei, Xing; Gong, Yihong] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China; [Yang, Qingxiong] JingChi Corp, 330 Gibraltar Dr, Sunnyvale, CA 94089 USA	Xi'an Jiaotong University	Yang, QX (corresponding author), JingChi Corp, 330 Gibraltar Dr, Sunnyvale, CA 94089 USA.	xingxjtu@gmail.com; liiton.research@gmail.com; ygong@mail.xjtu.edu.cn	Wei, Xing/W-4902-2019; Yang, Qingxiong/K-1729-2015	Yang, Qingxiong/0000-0002-4378-2335	National Basic Research Program of China [2015CB351705]; State Key Program of National Natural Science Foundation of China [61332018]	National Basic Research Program of China(National Basic Research Program of China); State Key Program of National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We thank all the reviewers for valuable comments. This work was supported by the National Basic Research Program of China (Grant No. 2015CB351705), the State Key Program of National Natural Science Foundation of China (Grant No. 61332018).	Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x; Adams A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531327; An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Arbelaez Pablo, 2014, CVPR; Arnheim R., 1956, ART VISUAL PERCEPTIO; Aujol JF, 2006, INT J COMPUT VISION, V67, P111, DOI 10.1007/s11263-006-4331-z; Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461; Bertasius G., 2015, CVPR; Bertasius Gedas, 2015, ICCV; Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476; Boyadzhiev I, 2012, ACM TOG SIGGRAPH ASI, V31; Buades A, 2016, J MATH IMAGING VIS, V55, P125, DOI 10.1007/s10851-015-0617-5; Canny J., 1986, IEEE TPAMI; Catanzaro B., 2009, ICCV; Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9; Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1239451.1239554, 10.1145/1276377.1276506]; Cho H, 2014, ACM T GRAPHIC, V33, P4; Chui C, 2006, APPL COMPUT HARMON A, V21, P1, DOI 10.1016/j.acha.2006.05.005; Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910; Dollar P., 2015, IEEE TPAMI; Dollar P., 2013, ICCV; Dollar P., 2006, CVPR; Duda R.O., 1973, J ROYAL STAT SOC SER; Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574; Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778; Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171; Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666; Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328; Gastal ESL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185529; Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964; Gilboa G, 2014, SIAM J IMAGING SCI, V7, P1937, DOI 10.1137/130930704; Gupta S, 2015, INT J COMPUT VISION, V112, P133, DOI 10.1007/s11263-014-0777-6; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403; Kivinen J. J., 2014, AISTATS; Kyprianidis J. E., 2008, P EG UK THEOR PRACT, P51, DOI DOI 10.2312/LOCALCHAPTEREVENTS/TPCG/TPCG08/051-058; Kyprianidis JE, 2011, COMPUT GRAPH FORUM, V30, P593, DOI 10.1111/j.1467-8659.2011.01882.x; Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780; Levin A., 2006, CVPR; Lim J. J., 2013, CVPR; Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936; Margolin R., 2014, CVPR; Meyer Y, 2001, AM MATH SOC LEWIS ME, V22; Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600; Paris S, 2008, FOUND TRENDS COMPUT, V4, P1, DOI 10.1561/0600000020; Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963; Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Perreault S, 2007, IEEE T IMAGE PROCESS, V16, P2389, DOI 10.1109/TIP.2007.902329; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Pham T. Q., 2005, ICME; Porikli F., 2008, CVPR; Ren X., 2012, NIPS; Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024; Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Weickert J, 1999, INT J COMPUT VISION, V31, P111, DOI 10.1023/A:1008009714131; Weiss B, 2006, ACM T GRAPHIC, V25, P519, DOI 10.1145/1141911.1141918; Xie S., 2015, PROCEEDINGS OF IEEE; Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464; Xu L, 2013, ACM TOG SIGGRAPH ASI, V32, P197; Xu L, 2012, ACM T GRAPHIC, V31; Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208; Yan Q., 2013, CVPR; Yang J., 2016, CVPR; Yang Q, 2016, CVPR; Yang QX, 2015, INT J COMPUT VISION, V112, P307, DOI 10.1007/s11263-014-0764-y; Yang QX, 2012, LECT NOTES COMPUT SC, V7572, P399, DOI 10.1007/978-3-642-33718-5_29; Yang QX, 2010, PROC CVPR IEEE, P1775, DOI 10.1109/CVPR.2010.5539847; Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542; Yin WT, 2005, LECT NOTES COMPUT SC, V3752, P73; Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70; Zeune L., 2016, ARXIV160406665 CORR; Zhang J., 2015, ICCV; Zhang Q., 2014, ECCV; ZHENG S, 2007, CVPR; Ziou D., 1998, Pattern Recognition and Image Analysis, V8, P537; Zitnick C. L., 2014, ECCV; Zitnick C. L., 2012, CVPR	84	5	7	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2018	126	11					1245	1265		10.1007/s11263-018-1091-5	http://dx.doi.org/10.1007/s11263-018-1091-5			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GT3IE					2022-12-18	WOS:000444394200005
J	Alexander, E; Guo, Q; Koppal, S; Gortler, SJ; Zickler, T				Alexander, Emma; Guo, Qi; Koppal, Sanjeev; Gortler, Steven J.; Zickler, Todd			Focal Flow: Velocity and Depth from Differential Defocus Through Motion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Depth; Optical flow; Defocus; Coded aperture; Ego-motion; Computational sensing	RANGE	We present the focal flow sensor. It is an unactuated, monocular camera that simultaneously exploits defocus and differential motion to measure a depth map and a 3D scene velocity field. It does this using an optical-flow-like, per-pixel linear constraint that relates image derivatives to depth and velocity. We derive this constraint, prove its invariance to scene texture, and prove that it is exactly satisfied only when the sensor's blur kernels are Gaussian. We analyze the inherent sensitivity of the focal flow cue, and we build and test a prototype. Experiments produce useful depth and velocity information for a broader set of aperture configurations, including a simple lens with a pillbox aperture.	[Alexander, Emma; Guo, Qi; Gortler, Steven J.; Zickler, Todd] Harvard Univ, Cambridge, MA 02138 USA; [Koppal, Sanjeev] Univ Florida, Gainesville, FL USA	Harvard University; State University System of Florida; University of Florida	Alexander, E (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	ealexander@fas.harvard.edu		Guo, Qi/0000-0002-8329-7668	National Science Foundation [IIS-1212928, 1514154]; Graduate Research Fellowship [DGE1144152]	National Science Foundation(National Science Foundation (NSF)); Graduate Research Fellowship	We would like to thank J. Zachary Gaslowitz and Ioannis Gkioulekas for helpful discussion. This work was supported by a gift from Texas Instruments Inc. and by the National Science Foundation under awards No. IIS-1212928 and 1514154 and Graduate Research Fellowship No. DGE1144152 to E.A.	Alexander E, 2016, LECT NOTES COMPUT SC, V9907, P667, DOI 10.1007/978-3-319-46487-9_41; Bracewell R. N., 1956, AUSTR J PHYS, V9, P198, DOI 10.1071/PH560198; Chakrabarti A, 2012, LECT NOTES COMPUT SC, V7576, P648, DOI 10.1007/978-3-642-33715-4_47; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; Duhamel PEJ, 2013, IEEE-ASME T MECH, V18, P556, DOI 10.1109/TMECH.2012.2225635; Farid H, 1998, J OPT SOC AM A, V15, P1777, DOI 10.1364/JOSAA.15.001777; Favaro P., 2004, EUR C COMP VIS ECCV; Fisher S.D., 1999, COMPLEX VARIABLES; Floreano D, 2009, FLYING INSECTS AND ROBOTS, P1; GROSSMANN P, 1987, PATTERN RECOGN LETT, V5, P63, DOI 10.1016/0167-8655(87)90026-2; Horn B. K., 2007, INT VEH S IV; Horn B. K., 1981, 1981 TECHN S E INT S; Horn B. K., 2009, INT VEH S IV; Humber J. S., 2007, INTELLIGENT ROBOTS S; Koppal S. J., 2011, COMPUTER VISION PATT; LEE DN, 1976, PERCEPTION, V5, P437, DOI 10.1068/p050437; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Levin A, 2010, LECT NOTES COMPUT SC, V6311, P214, DOI 10.1007/978-3-642-15549-9_16; Lin HY, 2006, OPT ENG, V45, DOI 10.1117/1.2403851; Myles Z, 1998, IEEE T PATTERN ANAL, V20, P652, DOI 10.1109/34.683782; Ng R., 2005, ACM T GRAPHICS TOG; Paramanand C, 2012, IEEE T IMAGE PROCESS, V21, P2798, DOI 10.1109/TIP.2011.2179664; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; Raghavendra C.S., 2006, WIRELESS SENSOR NETW; Rajagopalan A., 1997, COMPUTER VISION PATT; Rajagopalan AN, 2004, IEEE T PATTERN ANAL, V26, P1521, DOI 10.1109/TPAMI.2004.102; Seitz S. M., 2009, INT C COMP VIS ICCV; Sellent A., 2014, GERM C PATT REC GCPR; SUBBARAO M, 1994, INT J COMPUT VISION, V13, P271, DOI 10.1007/BF02028349; Subbarao M., 1988, INT C COMP VIS ICCV; Tai Y. W., 2009, INT C IM PROC ICIP; Tang H., 2017, COMPUTER VISION PATT; Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89; Veeraraghavan A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239520; Watanabe M, 1998, INT J COMPUT VISION, V27, P203, DOI 10.1023/A:1007905828438; Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811; Zhou C., 2009, INT C COMP VIS ICCV; Zhou CY, 2011, INT J COMPUT VISION, V93, P53, DOI 10.1007/s11263-010-0409-8	39	5	5	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2018	126	10			SI		1062	1083		10.1007/s11263-017-1051-5	http://dx.doi.org/10.1007/s11263-017-1051-5			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GR8XD					2022-12-18	WOS:000443018400001
J	Wu, JJ; Xue, TF; Lim, JJ; Tian, YD; Tenenbaum, JB; Torralba, A; Freeman, WT				Wu, Jiajun; Xue, Tianfan; Lim, Joseph J.; Tian, Yuandong; Tenenbaum, Joshua B.; Torralba, Antonio; Freeman, William T.			3D Interpreter Networks for Viewer-Centered Wireframe Modeling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D skeleton; Single image 3D reconstruction; Keypoint estimation; Neural network; Synthetic data		Understanding 3D object structure from a single image is an important but challenging task in computer vision, mostly due to the lack of 3D object annotations to real images. Previous research tackled this problem by either searching for a 3D shape that best explains 2D annotations, or training purely on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Networks (3D-INN), an end-to-end trainable framework that sequentially estimates 2D keypoint heatmaps and 3D object skeletons and poses. Our system learns from both 2D-annotated real images and synthetic 3D data. This is made possible mainly by two technical innovations. First, heatmaps of 2D keypoints serve as an intermediate representation to connect real and synthetic data. 3D-INN is trained on real images to estimate 2D keypoint heatmaps from an input image; it then predicts 3D object structure from heatmaps using knowledge learned from synthetic 3D shapes. By doing so, 3D-INN benefits from the variation and abundance of synthetic 3D objects, without suffering from the domain difference between real and synthesized images, often due to imperfect rendering. Second, we propose a Projection Layer, mapping estimated 3D structure back to 2D. During training, it ensures 3D-INN to predict 3D structure whose projection is consistent with the 2D annotations to real images. Experiments show that the proposed system performs well on both 2D keypoint estimation and 3D structure recovery. We also demonstrate that the recovered 3D information has wide vision applications, such as image retrieval.	[Wu, Jiajun; Tenenbaum, Joshua B.; Torralba, Antonio; Freeman, William T.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Xue, Tianfan] Google Res, Mountain View, CA USA; [Lim, Joseph J.] Univ Southern Calif, Los Angeles, CA USA; [Tian, Yuandong] Facebook Inc, Menlo Pk, CA USA; [Freeman, William T.] Google Res, Cambridge, MA USA	Massachusetts Institute of Technology (MIT); Google Incorporated; University of Southern California; Facebook Inc; Google Incorporated	Wu, JJ (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	jiajunwu@mit.edu; tianfan@google.com; limjj@usc.edu; yuandong@fb.com; jbt@mit.edu; torralba@csail.mit.edu; billf@mit.edu	Xue, Tianfan/AAG-5546-2019; Wu, JiaJun/GQH-7885-2022; Brisuda, Marek/ABF-5197-2021		NSF Robust Intelligence [1212849, 1524817]; NSF Big Data [1447476]; ONR MURI [N00014-16-1-2007]; Shell Research; Toyota Research Institute; Center for Brain, Minds and Machines (NSF STC award) [CCF-1231216]	NSF Robust Intelligence; NSF Big Data; ONR MURI(MURIOffice of Naval Research); Shell Research; Toyota Research Institute; Center for Brain, Minds and Machines (NSF STC award)	This work is supported by NSF Robust Intelligence 1212849 and NSF Big Data 1447476 to W.F., NSF Robust Intelligence 1524817 to A.T., ONR MURI N00014-16-1-2007 to J.B.T., Shell Research, the Toyota Research Institute, and the Center for Brain, Minds and Machines (NSF STC award CCF-1231216). The authors would like to thank Nvidia for GPU donations. Part of this work was done when Jiajun Wu was an intern at Facebook AI Research, and Tianfan Xue was a graduate student at MIT CSAIL.	Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751; [Anonymous], 2012, ADV NEURAL INFORM PR; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642; Barrow H. G., 1978, COMPUTER VISION SYST; Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23; Bever TG, 2010, BIOLINGUISTICS, V4, P174; Bourdev L, 2010, LECT NOTES COMPUT SC, V6316, P168, DOI 10.1007/978-3-642-15567-3_13; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Chen J., 2012, ACM S US INT SOFTW T; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Fidler S., 2012, ADV NEURAL INFORM PR; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Guo SJ, 2015, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2015.7417800; Hejrati M., 2014, IEEE C COMP VIS PATT; Hinton G. F., 1981, INT JOINT C ART INT; Hinton GE, 1997, PHILOS T ROY SOC B, V352, P1177, DOI 10.1098/rstb.1997.0101; Hu WZ, 2015, IEEE T PATTERN ANAL, V37, P1190, DOI 10.1109/TPAMI.2014.2362141; Huang QX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766890; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kar A., 2017, ADV NEURAL INFORM PR; Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulkarni TD, 2015, ADV NEUR IN, V28; Kumar P, 2013, IEEE C ELEC DEVICES; LECLERC YG, 1992, INT J COMPUT VISION, V9, P113, DOI 10.1007/BF00129683; Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071; Lim J. J., 2014, ECCV; Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372; Liu J, 2013, IEEE INT C COMP VIS; Lohit S, 2015, IEEE COMPUT SOC CONF; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; McCormac J, 2017, IEEE I CONF COMP VIS, P2697, DOI 10.1109/ICCV.2017.292; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Pepik B, 2012, PROC CVPR IEEE, P3362, DOI 10.1109/CVPR.2012.6248075; Prasad M, 2010, IEEE C COMP VIS PATT; Ramakrishna Varun, 2012, ECCV, P573, DOI DOI 10.1007/978-3-642-33765-9; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471; Satkin S, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.128; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Shih K. J., 2015, BRIT MACH VIS C; Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Su H, 2015, IEEE INT CON MULTI; Su H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601159; Sun B., 2014, BMVC, V1, P3; Taigman Y, 2015, PROC CVPR IEEE, P2746, DOI 10.1109/CVPR.2015.7298891; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Torresani L, 2004, ADV NEUR IN, V16, P1555; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160; Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vicente S, 2014, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2014.13; Wah C., 2011, TECH REP; Wu J., 2015, ADV NEURAL INFORM PR; Wu Jiajun, 2016, ADV NEURAL INFORM PR; Wu Jiajun, 2017, ADV NEURAL INFORM PR, V3; Xiang Yu, 2014, IEEE WINT C APPL COM; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xue T., 2012, IEEE C COMP VIS PATT; Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741; Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002; Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zia MZ, 2013, IEEE T PATTERN ANAL, V35, P2608, DOI 10.1109/TPAMI.2013.87	73	5	5	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2018	126	9			SI		1009	1026		10.1007/s11263-018-1074-6	http://dx.doi.org/10.1007/s11263-018-1074-6			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GQ3HQ		Green Submitted, Green Published			2022-12-18	WOS:000441553300008
J	Mendez-Rial, R; Martin-Herrero, J				Mendez-Rial, Roi; Martin-Herrero, Julio			Separable Anisotropic Diffusion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image segmentation; Partial differential equations; Anisotropic filtering; Nonlinear diffusion; Separable filters; Fast; High dimensional; Denoising	MULTIVALUED IMAGES; NOISE REMOVAL; SCALE-SPACE; HYPERCUBE; RECONSTRUCTION; COMPRESSION; SCHEMES; FILTER; PDES	Anisotropic diffusion has many applications in image processing, but the high computational cost usually requires accuracy trade-offs in order to grant its applicability in practical problems. This is specially true when dealing with 3D images, where anisotropic diffusion should be able to provide interesting results for many applications, but the usual implementation methods greatly scale in complexity with the additional dimension. Here we propose a separable implementation of the most general anisotropic diffusion formulation, based on Gaussian convolutions, whose favorable computational complexity scales linearly with the number of dimensions, without any assumptions about specific parameterizations. We also present variants that bend the Gaussian kernels for improved results when dealing with highly anisotropic curved or sharp structures. We test the accuracy, speed, stability, and scale-space properties of the proposed methods, and present some results (both synthetic and real) which show their advantages, including up to 60 times faster computation in 3D with respect to the explicit method, improved accuracy and stability, and min-max preservation.	[Mendez-Rial, Roi] AIMEN Technol Ctr, Robot & Control Unit, O Porrino 36410, Spain; [Martin-Herrero, Julio] Univ Vigo, ETSE Telecomunicac, Dept Signal Theory & Commun & AtlanTTic, Vigo 36310, Spain	Universidade de Vigo; atlanTTic	Mendez-Rial, R (corresponding author), AIMEN Technol Ctr, Robot & Control Unit, O Porrino 36410, Spain.	maximino.mendez@aimen.es; julio@uvigo.es		Martin-Herrero, Julio/0000-0003-1109-6338; Mendez-Rial, Roi/0000-0002-9991-0316				Acton ST, 1997, INT J REMOTE SENS, V18, P2877, DOI 10.1080/014311697217404; Aubert G., 2006, MATH PROBLEMS IMAGE, V147; Barash D, 2002, IEEE T PATTERN ANAL, V24, P844, DOI 10.1109/TPAMI.2002.1008390; Barrett R., 1994, TEMPLATES SOLUTION L; Benmansour F, 2011, INT J COMPUT VISION, V92, P192, DOI 10.1007/s11263-010-0331-0; Cabral B., 1993, Computer Graphics Proceedings, P263, DOI 10.1145/166117.166151; Carmona RA, 1998, IEEE T IMAGE PROCESS, V7, P353, DOI 10.1109/83.661185; Despotovic I, 2015, COMPUT MATH METHOD M, V2015, DOI 10.1155/2015/450341; Direkoglu C, 2012, INT J COMPUT VISION, V100, P170, DOI 10.1007/s11263-012-0540-9; Dongarra J., 1996, IML 1 2 ITERATIVE ME; Duarte-Carvajalino JM, 2008, IEEE T GEOSCI REMOTE, V46, P2418, DOI 10.1109/TGRS.2008.916478; Duarte-Carvajalino JM, 2007, IEEE T IMAGE PROCESS, V16, P1303, DOI 10.1109/TIP.2007.894266; Florack L. M. J., 1993, Journal of Mathematical Imaging and Vision, V3, P327, DOI 10.1007/BF01664793; Frangakis AS, 2001, J STRUCT BIOL, V135, P239, DOI 10.1006/jsbi.2001.4406; Galic I, 2008, J MATH IMAGING VIS, V31, P255, DOI 10.1007/s10851-008-0087-0; Gilboa G, 2006, IEEE T IMAGE PROCESS, V15, P2269, DOI 10.1109/TIP.2006.875248; Grewenig S, 2010, LECT NOTES COMPUT SC, V6376, P533; Hajiaboli MR, 2011, INT J COMPUT VISION, V92, P177, DOI 10.1007/s11263-010-0330-1; Hossain Z, 2010, IEEE T VIS COMPUT GR, V16, P1376, DOI 10.1109/TVCG.2010.147; Kopp J, 2008, INT J MOD PHYS C, V19, P845, DOI 10.1142/S0129183108012649; Krissian K, 2009, IEEE T IMAGE PROCESS, V18, P2265, DOI 10.1109/TIP.2009.2025553; Lampert CH, 2006, IEEE T IMAGE PROCESS, V15, P3501, DOI 10.1109/TIP.2006.877501; LINDEBERG T, 1990, IEEE T PATTERN ANAL, V12, P234, DOI 10.1109/34.49051; Lindeberg T., 1994, SCALE SPACE THEORY C; Martin-Herrero J, 2007, CARBON, V45, P1242, DOI 10.1016/j.carbon.2007.01.021; Martin-Herrero J, 2007, IEEE T GEOSCI REMOTE, V45, P1386, DOI 10.1109/TGRS.2007.894569; Mendez-Rial R, 2012, IEEE T IMAGE PROCESS, V21, P2389, DOI 10.1109/TIP.2011.2179059; Mendez-Rial R, 2012, IEEE GEOSCI REMOTE S, V9, P214, DOI 10.1109/LGRS.2011.2164050; Mendez-Rial R, 2010, IEEE GEOSCI REMOTE S, V7, P870, DOI 10.1109/LGRS.2010.2054062; OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Pham T. Q., 2005, IEEE INT C MULT EXP, P1; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Saad Y., 2003, ITERATIVE METHODS SP, Vsecond, DOI DOI 10.1137/1.9780898718003; Sapiro G, 1996, IEEE T IMAGE PROCESS, V5, P1582, DOI 10.1109/83.541429; Sapiro G., 2001, GEOMETRIC PARTIAL DI; Schmaltz C, 2014, INT J COMPUT VISION, V108, P222, DOI 10.1007/s11263-014-0702-z; Schwarz H. R., 1988, NUMER MATH, P43; Stalling D., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P249, DOI 10.1145/218380.218448; Strikwerda JC, 2004, FINITE DIFFERENCE SC, V2nd; Sun QL, 2004, COMPUT MED IMAG GRAP, V28, P461, DOI 10.1016/j.compmedimag.2004.08.001; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Tschumperle D, 2005, IEEE T PATTERN ANAL, V27, P506, DOI 10.1109/TPAMI.2005.87; Tschumperle D, 2006, INT J COMPUT VISION, V68, P65, DOI 10.1007/s11263-006-5631-z; Vogel CR, 1998, IEEE T IMAGE PROCESS, V7, P813, DOI 10.1109/83.679423; Weickert J, 1998, IEEE T IMAGE PROCESS, V7, P398, DOI 10.1109/83.661190; Weickert J, 1999, IMAGE VISION COMPUT, V17, P201, DOI 10.1016/S0262-8856(98)00102-4; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; Weickert J, 2016, INT J COMPUT VISION, V118, P275, DOI 10.1007/s11263-015-0874-1; Winnemoller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018; Yang GZ, 1996, IMAGE VISION COMPUT, V14, P135, DOI 10.1016/0262-8856(95)01047-5	53	5	5	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2018	126	6					651	670		10.1007/s11263-017-1060-4	http://dx.doi.org/10.1007/s11263-017-1060-4			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GC3EO					2022-12-18	WOS:000429667300005
J	Kong, XF; Yang, QX				Kong, Xiangfei; Yang, Qingxiong			No-Reference Image Quality Assessment for Image Auto-Denoising	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image quality assessment; Non-local means; Block matching; KNN searching	STRUCTURAL SIMILARITY	This paper proposes two new non-reference image quality metrics that can be adopted by the state-of-the-art image/video denoising algorithms for auto-denoising. The first metric is proposed based on the assumption that the noise should be independent of the original image. A direct measurement of this dependence is, however, impractical due to the relatively low accuracy of existing denoising method. The proposed metric thus tackles the homogeneous regions and highly-structured regions separately. Nevertheless, this metric is only stable when the noise level is relatively low. Most denoising algorithms reduce noise by (weighted) averaging repeated noisy measurements. As a result, another metric is proposed for high-level noise based on the fact that more noisy measurements will be required when the noise level increases. The number of measurements before converging is thus related to the quality of noisy images. Our patch-matching based metric proposes to iteratively find and add noisy image measurements for averaging until there is no visible difference between two successively averaged images. Both metrics are evaluated on LIVE2 (Sheikh et al. in LIVE image quality assessment database release 2: 2013) and TID2013 (Ponomarenko et al. in Color image database tid2013: Peculiarities and preliminary results: 2005) data sets using standard Spearman and Kendall rank-order correlation coefficients (ROCC), showing that they subjectively outperforms current state-of-the-art no-reference metrics. Quantitative evaluation w.r.t. different level of synthetic noisy images also demonstrates consistently higher performance over state-of-the-art non-reference metrics when used for image denoising.	[Kong, Xiangfei; Yang, Qingxiong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Anhui, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS	Yang, QX (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Anhui, Peoples R China.	liiton.research@gmail.com	Yang, Qingxiong/K-1729-2015	Yang, Qingxiong/0000-0002-4378-2335				BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Buades A, 2011, COMMUN ACM, V54, P109, DOI 10.1145/1941487.1941513; Chen GH, 2006, IEEE IMAGE PROC, P2929, DOI 10.1109/ICIP.2006.313132; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760; Girod Bernd, 1993, P207; He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795; Kong XF, 2013, IEEE I CONF COMP VIS, P2888, DOI 10.1109/ICCV.2013.359; Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050; Narwaria M, 2012, IEEE T SYST MAN CY B, V42, P347, DOI 10.1109/TSMCB.2011.2163391; Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; PONOMARENKO n, 2013, COLOR IMAGE DATABASE, P106; Ramani S, 2008, IEEE T IMAGE PROCESS, V17, P1540, DOI 10.1109/TIP.2008.2001404; Sheikh H., 2005, LIVE IMAGE QUALITY A; Shnayderman A, 2006, IEEE T IMAGE PROCESS, V15, P422, DOI 10.1109/TIP.2005.860605; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330; Tang HX, 2014, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2014.368; Tasdizen T, 2008, IEEE IMAGE PROC, P1728, DOI 10.1109/ICIP.2008.4712108; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang Z, 2002, IEEE IMAGE PROC, P477; Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823; Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435; Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649; Xiao J., 2010, CVPR; Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789; Zhu JY, 2012, IEEE T IMAGE PROCESS, V21, P919, DOI 10.1109/TIP.2011.2169971; Zhu X, 2010, IEEE T IMAGE PROCESS, V19, P3116, DOI 10.1109/TIP.2010.2052820	31	5	5	1	31	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2018	126	5					537	549		10.1007/s11263-017-1054-2	http://dx.doi.org/10.1007/s11263-017-1054-2			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FZ0UP					2022-12-18	WOS:000427289200005
J	Peng, X; Zhang, ST; Yu, Y; Metaxas, DN				Peng, Xi; Zhang, Shaoting; Yu, Yang; Metaxas, Dimitris N.			Toward Personalized Modeling: Incremental and Ensemble Alignment for Sequential Faces in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face alignment; Personalized modeling; Incremental learning; Ensemble learning; Sparse coding	ROBUST VISUAL TRACKING	Fitting facial landmarks on unconstrained videos is a challenging task with broad applications. Both generic and joint alignment methods have been proposed with varying degrees of success. However, many generic methods are heavily sensitive to initializations and usually rely on offline-trained static models, which limit their performance on sequential images with extensive variations. On the other hand, joint methods are restricted to offline applications, since they require all frames to conduct batch alignment. To address these limitations, we propose to exploit incremental learning for personalized ensemble alignment. We sample multiple initial shapes to achieve image congealing within one frame, which enables us to incrementally conduct ensemble alignment by group-sparse regularized rank minimization. At the same time, incremental subspace adaptation is performed to achieve personalized modeling in a unified framework. To alleviate the drifting issue, we leverage a very efficient fitting evaluation network to pick out well-aligned faces for robust incremental learning. Extensive experiments on both controlled and unconstrained datasets have validated our approach in different aspects and demonstrated its superior performance compared with state of the arts in terms of fitting accuracy and efficiency.	[Peng, Xi; Yu, Yang; Metaxas, Dimitris N.] Rutgers State Univ, Piscataway, NJ 08854 USA; [Zhang, Shaoting] Univ N Carolina, Charlotte, NC 28223 USA	Rutgers State University New Brunswick; University of North Carolina; University of North Carolina Charlotte	Peng, X (corresponding author), Rutgers State Univ, Piscataway, NJ 08854 USA.	xpeng.cs@rutgers.edu; szhang16@uncc.edu; yyu@cs.rutgers.edu; dnm@cs.rutgers.edu						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240; Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; Baro X, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301329; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23; BLACK MJ, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P374, DOI 10.1109/ICCV.1995.466915; Brand M, 2006, LINEAR ALGEBRA APPL, V415, P20, DOI 10.1016/j.laa.2005.07.021; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Cheng XL, 2013, 2013 IEEE MILITARY COMMUNICATIONS CONFERENCE (MILCOM 2013), P1, DOI 10.1109/MILCOM.2013.9; Cheng X, 2012, LECT NOTES COMPUT SC, V7583, P133, DOI 10.1007/978-3-642-33863-2_14; Cheng X, 2013, IEEE I CONF COMP VIS, P577, DOI 10.1109/ICCV.2013.77; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; DeCarlo D, 2000, INT J COMPUT VISION, V38, P99, DOI 10.1023/A:1008122917811; Doucet A, 2001, STAT ENG IN, P3; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Escalera S., 2015, P IEEE INT C COMP VI, P1; FGNet, 2004, TALK FAC VID; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kim M, 2008, PROC CVPR IEEE, P1787; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Lin Z., 2010, ARXIV10095055, DOI DOI 10.1016/J.JSB.2012.10.010; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292; Nasrollahi K, 2015, INT CONF IMAG PROC, P67, DOI 10.1109/IPTA.2015.7367098; Parkhi Omkar M., 2015, BRIT MACH VIS C; Patras I, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P97, DOI 10.1109/AFGR.2004.1301515; Peng X, 2016, LECT NOTES COMPUT SC, V9905, P38, DOI 10.1007/978-3-319-46448-0_3; Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151; Peng YG, 2010, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2010.5540138; Perakis P, 2013, IEEE T PATTERN ANAL, V35, P1552, DOI 10.1109/TPAMI.2012.247; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2014, PROC CVPR IEEE, P1789, DOI 10.1109/CVPR.2014.231; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shen J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1003, DOI 10.1109/ICCVW.2015.132; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Sung J, 2009, PATTERN RECOGN LETT, V30, P359, DOI 10.1016/j.patrec.2008.11.006; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tang M, 2012, IEEE T IMAGE PROCESS, V21, P3273, DOI 10.1109/TIP.2012.2189580; Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453; Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989; Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239; Vogler C, 2007, IEEE I CONF COMP VIS, P1456; Wang Z., 2016, P 20 SIGNLL C COMPUT, P31; Wang ZG, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1138; Wang Zhiguo, 2016, P 26 INT C COMP LING, P1340; Wu L., 2016, SIAM J SCI COMPUTING; Wu LF, 2015, SIAM J SCI COMPUT, V37, pS365, DOI 10.1137/140979381; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yan JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P392, DOI 10.1109/ICCVW.2013.126; Yang H., 2015, ARXIV151105049; Zafeiriou L, 2014, LECT NOTES COMPUT SC, V8692, P167, DOI 10.1007/978-3-319-10593-2_12; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhao C, 2011, PROC CVPR IEEE, P561, DOI 10.1109/CVPR.2011.5995381; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu X., 2012, IEEE CONFERENCE ON C	65	5	5	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		184	197		10.1007/s11263-017-0996-8	http://dx.doi.org/10.1007/s11263-017-0996-8			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA					2022-12-18	WOS:000425619100004
J	Roubtsova, N; Guillemaut, JY				Roubtsova, Nadejda; Guillemaut, Jean-Yves			Colour Helmholtz Stereopsis for Reconstruction of Dynamic Scenes with Arbitrary Unknown Reflectance	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D reconstruction; Dynamic scenes; Arbitrary BRDF; Helmholtz Stereopsis	PHOTOMETRIC STEREO; MULTIVIEW STEREO; SHAPE	Helmholtz Stereopsis is a powerful technique for reconstruction of scenes with arbitrary reflectance properties. However, previous formulations have been limited to static objects due to the requirement to sequentially capture reciprocal image pairs (i.e. two images with the camera and light source positions mutually interchanged). In this paper, we propose colour Helmholtz Stereopsis-a novel framework for Helmholtz Stereopsis based on wavelength multiplexing. To address the new set of challenges introduced by multispectral data acquisition, the proposed Colour Helmholtz Stereopsis pipeline uniquely combines a tailored photometric calibration for multiple camera/light source pairs, a novel procedure for spatio-temporal surface chromaticity calibration and a state-of-the-art Bayesian formulation necessary for accurate reconstruction from a minimal number of reciprocal pairs. In this framework, reflectance is spatially unconstrained both in terms of its chromaticity and the directional component dependent on the illumination incidence and viewing angles. The proposed approach for the first time enables modelling of dynamic scenes with arbitrary unknown and spatially varying reflectance using a practical acquisition set-up consisting of a small number of cameras and light sources. Experimental results demonstrate the accuracy and flexibility of the technique on a variety of static and dynamic scenes with arbitrary unknown BRDF and chromaticity ranging from uniform to arbitrary and spatially varying.	[Roubtsova, Nadejda; Guillemaut, Jean-Yves] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England	University of Surrey	Roubtsova, N (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.	n.s.roubtsova@surrey.ac.uk; j.guillemaut@surrey.ac.uk	Guillemaut, Jean-Yves/N-7739-2014	Guillemaut, Jean-Yves/0000-0001-8223-5505	Engineering and Physical Sciences Research Council [EP/M021793/1]; EPSRC [EP/M023281/1, EP/M021793/1] Funding Source: UKRI	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors would like to thank 3D Enterprise for producing high quality static object scans used as ground truth in the quantitative evaluation of the method presented in this paper. Further, let us acknowledge the contribution of our industrial project partners The Foundry and Double Negative for useful discussions. This work was supported by the Engineering and Physical Sciences Research Council (grant number EP/M021793/1).	Ahmed Naveed, 2008, IEEE C COMP VIS PATT, P1; Alldrin N., 2008, IEEE C COMP VIS PATT, P1; Anderson R, 2011, IEEE I CONF COMP VIS, P2182, DOI 10.1109/ICCV.2011.6126495; Ashikhmin M, 2000, COMP GRAPH, P65, DOI 10.1145/344779.344814; Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2; Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Baumgart B. G., 1974, Geometric modeling for computer vision; Brostow GJ, 2011, IEEE T PATTERN ANAL, V33, P2104, DOI 10.1109/TPAMI.2011.37; Brox T., 2009, CVPR; Budd C, 2013, INT J COMPUT VISION, V102, P256, DOI 10.1007/s11263-012-0553-4; Cook R., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293; Delaunoy A., 2010, P AS C COMP VIS, P39; Guillemaut JY, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P352; Guillemaut JY, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P10, DOI 10.1109/TDPVT.2004.1335135; Helmholtz H., 1925, TREATISE PHYSL OPTIC, V1; Hernandez C, 2007, IEEE I CONF COMP VIS, P873; Hertzmann A, 2005, IEEE T PATTERN ANAL, V27, P1254, DOI 10.1109/TPAMI.2005.158; Higo T, 2010, PROC CVPR IEEE, P1157, DOI 10.1109/CVPR.2010.5540084; Holroyd M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409086; Janko Z, 2004, PROC CVPR IEEE, P166; Janko Zsolt, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P55, DOI 10.1007/978-3-642-19309-5_5; Jin HL, 2005, INT J COMPUT VISION, V63, P175, DOI 10.1007/s11263-005-6876-7; Jin HL, 2003, PROC CVPR IEEE, P171; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Klaudiny M, 2012, LECT NOTES COMPUT SC, V7575, P743, DOI 10.1007/978-3-642-33765-9_53; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; Lazebnik S, 2007, INT J COMPUT VISION, V74, P137, DOI 10.1007/s11263-006-0008-x; Liang C, 2010, IMAGE VISION COMPUT, V28, P579, DOI 10.1016/j.imavis.2009.09.012; Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951; Ngan A, 2005, EUR S REND, V2, P117, DOI DOI 10.2312/EGWR/EGSR05/117-126; Oxholm G, 2014, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2014.277; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Roubtsova Nadejda, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P251, DOI 10.1109/3DV.2014.59; Roubtsova N., 2015, SPRINGER LECT NOTES; Roubtsova N, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P335; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Sundaram N., 2010, ECCV; Tan P, 2011, IEEE T PATTERN ANAL, V33, P2506, DOI 10.1109/TPAMI.2011.35; Vlasic D., 2009, SIGGRAPH ASIA; Vogiatzis G, 2012, INT J COMPUT VISION, V97, P91, DOI 10.1007/s11263-011-0482-7; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; Weinmann M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.108; Woodham R. J., 1989, SHAPE SHADING, P513; Wu CL, 2011, IEEE I CONF COMP VIS, P1108, DOI 10.1109/ICCV.2011.6126358; Zhou ZL, 2013, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR.2013.195; Zickler T., 2006, IEEE COMP SOC C COMP, P1801; Zickler TE, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1411; Zickler TE, 2002, INT J COMPUT VISION, V49, P215, DOI 10.1023/A:1020149707513	49	5	5	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2017	124	1					18	48		10.1007/s11263-016-0951-0	http://dx.doi.org/10.1007/s11263-016-0951-0			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FA2JD	32025092	hybrid, Green Published			2022-12-18	WOS:000405265900002
J	Chu, WS; De la Torre, F; Cohn, JF; Messinger, DS				Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey F.; Messinger, Daniel S.			A Branch-and-Bound Framework for Unsupervised Common Event Discovery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Common event discovery; Synchrony discovery; Video indexing; Event detection; Human interaction; Unsupervised learning; Global optimization; Branch and bound; Bag-of-words	FACIAL EXPRESSION; DYNAMICS; IMAGE	Event discovery aims to discover a temporal segment of interest, such as human behavior, actions or activities. Most approaches to event discovery within or between time series use supervised learning. This becomes problematic when relevant event labels are unknown, are difficult to detect, or not all possible combinations of events have been anticipated. To overcome these problems, this paper explores Common Event Discovery (CED), a new problem that aims to discover common events of variable-length segments in an unsupervised manner. A potential solution to CED is searching over all possible pairs of segments, which would incur a prohibitive quartic cost. In this paper, we propose an efficient branch-and-bound (B&B) framework that avoids exhaustive search while guaranteeing a globally optimal solution. To this end, we derive novel bounding functions for various commonality measures and provide extensions to multiple commonality discovery and accelerated search. The B&B framework takes as input any multidimensional signal that can be quantified into histograms. A generalization of the framework can be readily applied to discover events at the same or different times (synchrony and event commonality, respectively). We consider extensions to video search and supervised event detection. The effectiveness of the B&B framework is evaluated in motion capture of deliberate behavior and in video of spontaneous facial behavior in diverse interpersonal contexts: interviews, small groups of young adults, and parent-infant face-to-face interaction.	[Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey F.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey F.] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Messinger, Daniel S.] Univ Miami, Dept Psychol, POB 248185, Coral Gables, FL 33124 USA	Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; University of Miami	Chu, WS (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	wschu@cmu.edu; ftorre@cs.cmu.edu; jeffcohn@pitt.edu; dmessinger@miami.edu	Chu, Wen-Sheng/AAF-6871-2019	Chu, Wen-Sheng/0000-0001-8592-6088	US National Institutes of Health [GM105004, MH096951]; NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCES [R01GM105004] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	US National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of General Medical Sciences (NIGMS)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	This work was supported in part by US National Institutes of Health grants GM105004 and MH096951. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Institutes of Health. The authors would like to thank Feng Zhou and Jiabei Zeng for helping partial experiments.	Amberg B., 2011, ICCV; Chaaraoui AA, 2012, EXPERT SYST APPL, V39, P10873, DOI 10.1016/j.eswa.2012.03.005; [Anonymous], 2003, P ICCV; Balakrishnan V., 1991, International Journal of Robust and Nonlinear Control, V1, P295, DOI 10.1002/rnc.4590010404; Barbic J, 2004, PROC GRAPH INTERF, P185; Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35; Begum N, 2014, PROC VLDB ENDOW, V8, P149, DOI 10.14778/2735471.2735476; Boiman O., 2005, ICCV; Brand M, 1997, CVPR; Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316; Chu W.-S., 2012, ECCV; Chu W. S., 2015, ICCV; Chu W. S., 2010, P ACCV; Chu W. S., 2016, TPAMI; Cooper H., 2009, CVPR; De la Torre F., 2015, AUTOMATIC FACE GESTU; Delaherche E, 2012, IEEE T AFFECT COMPUT, V3, P349, DOI 10.1109/T-AFFC.2012.12; Ding X., 2012, ICCV; Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111; Duchenne O., 2009, INT C COMP VIS; Everingham M., 2006, 2 PASCAL CHALLENGE; Feris R., 2014, ICMR; Gao L, 2015, CVPR; GENDRON B, 1994, OPER RES, V42, P1042, DOI 10.1287/opre.42.6.1042; Girard J. M., 2015, AFGR; Goldberger J., 2003, ICCV; Gusfield D., 1997, ALGORITHMS STRINGS T; Han D., 2009, ICCV; Hoai M., 2011, CVPR; Hongeng S., 2001, ICCV; Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710; Jhuang H., 2007, ICCV; Keogh E, 2005, KNOWL INF SYST, V7, P358, DOI 10.1007/s10115-004-0154-9; Kruger S. E., 2005, INTERSPEECH; Kuipers B., 2011, CVPR; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; Laptev Ivan, 2008, P CVPR; Lehmann A, 2011, INT J COMPUT VISION, V94, P175, DOI 10.1007/s11263-010-0342-x; Littlewort G, 2006, IMAGE VISION COMPUT, V24, P615, DOI 10.1016/j.imavis.2005.09.011; Liu CD, 2010, IEEE T INF TECHNOL B, V14, P1236, DOI 10.1109/TITB.2010.2052061; Liu H.R., 2010, P CVPR; Lucey Patrick, 2010, CVPRW; MAIER D, 1978, J ACM, V25, P322, DOI 10.1145/322063.322075; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Messinger DM, 2010, NEURAL NETWORKS, V23, P1004, DOI 10.1016/j.neunet.2010.08.008; Messinger DS, 2009, INFANCY, V14, P285, DOI 10.1080/15250000902839963; Minnen D., 2007, KDD; Mukherjee L., 2011, P CVPR; NARENDRA P, 1977, IEEE T COMPUT, V26, P917, DOI 10.1109/TC.1977.1674939; Nayak S, 2012, J MACH LEARN RES, V13, P2589; Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684; Paterson M., 1994, Mathematical Foundations of Computer Science 1994. 19th International Symposium, MFCS'94. Proceedings, P127; Platt JC, 2000, ADV NEUR IN, P61; Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sadanand S., 2012, P 2012 IEEE C COMP V; Sangineto E., 2014, P ACMMM; Sayette MA, 2012, PSYCHOL SCI, V23, P869, DOI 10.1177/0956797611435134; Schindler G., 2008, P CVPR; Schmidt RC, 2012, J NONVERBAL BEHAV, V36, P263, DOI 10.1007/s10919-012-0138-5; SCHOLKOPF B, 2001, NIPS; Schuller B., 2006, INTERSPEECH; Si Z., 2011, ICCV; Sun M., 2012, CVPR; Turaga P, 2009, COMPUT VIS IMAGE UND, V113, P353, DOI 10.1016/j.cviu.2008.08.009; Valstar Michel, 2006, CVPRW; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang HX, 2014, WIRES DATA MIN KNOWL, V4, P24, DOI 10.1002/widm.1110; Wang Y., 2009, ICASSP; Wang Y., 2006, P CVPR; YANG Y, 2013, PAMI, V35, P1635, DOI DOI 10.1109/TPAMI.2012.253; Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731; Yu X., 2013, AUTOMATIC FACE GESTU; Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPR.2009.5206671, 10.1109/CVPRW.2009.5206671]; Zheng Y., 2011, ACMMM; Zhou F., 2010, P CVPR; Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	79	5	5	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2017	123	3					372	391		10.1007/s11263-017-0989-7	http://dx.doi.org/10.1007/s11263-017-0989-7			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EX9EO	28943718	Green Accepted			2022-12-18	WOS:000403559600004
J	Bensch, R; Scherf, N; Huisken, J; Brox, T; Ronneberger, O				Bensch, Robert; Scherf, Nico; Huisken, Jan; Brox, Thomas; Ronneberger, Olaf			Spatiotemporal Deformable Prototypes for Motion Anomaly Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Anomaly detection; Motion patterns; Point trajectories; Elastic registration		This paper presents an approach for motion-based anomaly detection, where a prototype pattern is detected and elastically registered against a test sample to detect anomalies in the test sample. The prototype model is learned from multiple sequences to define accepted variations. "Supertrajectories" based on hierarchical clustering of dense point trajectories serve as an efficient and robust representation of motion patterns. An efficient hashing approach provides transformation hypotheses that are refined by a spatiotemporal elastic registration. We propose a new method for elastic registration of 3D+time trajectory patterns that induces spatial elasticity from trajectory affinities. The method is evaluated on a new motion anomaly dataset of juggling patterns and performs well in detecting subtle anomalies. Moreover, we demonstrate the applicability to biological motion patterns.	[Bensch, Robert; Brox, Thomas; Ronneberger, Olaf] Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany; [Bensch, Robert; Brox, Thomas; Ronneberger, Olaf] Univ Freiburg, BIOSS Ctr Biol Signalling Studies, D-79104 Freiburg, Germany; [Scherf, Nico; Huisken, Jan] Max Planck Inst Mol Cell Biol & Genet, D-01307 Dresden, Germany; [Scherf, Nico] Tech Univ Dresden, Inst Med Informat & Biometry, Fac Med Carl Gustav Carus, D-01307 Dresden, Germany	University of Freiburg; University of Freiburg; Max Planck Society; Technische Universitat Dresden	Bensch, R (corresponding author), Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany.; Bensch, R (corresponding author), Univ Freiburg, BIOSS Ctr Biol Signalling Studies, D-79104 Freiburg, Germany.	bensch@cs.uni-freiburg.de; scherf@mpi-cbg.de; huisken@mpi-cbg.de; brox@cs.uni-freiburg.de; ronneber@cs.uni-freiburg.de	Huisken, Jan/B-2472-2009	Huisken, Jan/0000-0001-7250-3756; Bensch, Robert/0000-0001-8618-3167	Excellence Initiative of the German Federal and State Governments (BIOSS Centre for Biological Signalling Studies) [EXC 294]; European Research Council (ERC) under the European Union's Horizon research and innovation program [647885]	Excellence Initiative of the German Federal and State Governments (BIOSS Centre for Biological Signalling Studies); European Research Council (ERC) under the European Union's Horizon research and innovation program(European Research Council (ERC))	We thank J. Koch, A. Kramer, T. Paxian and D. Mai who contributed their juggling expertise and agreed to perform diverse juggling patterns in front of our Kinect camera. This study was supported by the Excellence Initiative of the German Federal and State Governments (BIOSS Centre for Biological Signalling Studies EXC 294 to R.B., T.B. and O.R.). N.S. and J.H. have received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (Grant agreement No. 647885).	Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825; Antic B, 2011, IEEE I CONF COMP VIS, P2415, DOI 10.1109/ICCV.2011.6126525; Benezeth Y., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2458, DOI 10.1109/CVPRW.2009.5206686; Bensch R., 2015, BRIT MACH VIS C BMVC; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Boiman O., 2007, ADV NEURAL INFORM PR, V19, P177; Boiman O, 2007, INT J COMPUT VISION, V74, P17, DOI 10.1007/s11263-006-0009-9; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882; Cong Y, 2013, IEEE T INF FOREN SEC, V8, P1590, DOI 10.1109/TIFS.2013.2272243; Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434; Dee H. M., 2004, BRIT MACH VIS C, P477; Everitt B.S., 2011, CLUSTER ANAL WILEY S; Hu WM, 2006, IEEE T PATTERN ANAL, V28, P1450, DOI 10.1109/TPAMI.2006.176; Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569; Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771; Li C, 2013, NEUROCOMPUTING, V119, P94, DOI 10.1016/j.neucom.2012.03.040; MAHADEVAN V, 2010, PROC CVPR IEEE, P1975, DOI DOI 10.1109/CVPR.2010.5539872; Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641; Nait-Charif H, 2004, INT C PATT RECOG, P323, DOI 10.1109/ICPR.2004.1333768; Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599; Popoola OP, 2012, IEEE T SYST MAN CY C, V42, P865, DOI 10.1109/TSMCC.2011.2178594; Roshtkhari MJ, 2013, COMPUT VIS IMAGE UND, V117, P1436, DOI 10.1016/j.cviu.2013.06.007; Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917; Saligrama V, 2010, IEEE SIGNAL PROC MAG, V27, P18, DOI 10.1109/MSP.2010.937393; Schmid B, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3207; Sillito R. R., 2008, BMVC, P1; Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32; UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573; Winkelbach S, 2006, LECT NOTES COMPUT SC, V4174, P718; Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882	32	5	5	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2017	122	3			SI		502	523		10.1007/s11263-016-0934-1	http://dx.doi.org/10.1007/s11263-016-0934-1			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ES7OH					2022-12-18	WOS:000399739200007
J	Xie, XH; Jones, M; Tam, G				Xie, Xianghua; Jones, Mark; Tam, Gary			Recognition, Tracking, and Optimisation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Xie, Xianghua; Jones, Mark; Tam, Gary] Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales	Swansea University	Xie, XH (corresponding author), Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales.	x.xie@swansea.ac.uk	Jones, Mark W/F-1114-2015; Tam, Gary KL/E-5098-2011	Jones, Mark W/0000-0001-8991-1190; Tam, Gary KL/0000-0001-7387-5180					0	5	5	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2017	122	3			SI		409	410		10.1007/s11263-017-1008-8	http://dx.doi.org/10.1007/s11263-017-1008-8			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ES7OH		Bronze			2022-12-18	WOS:000399739200001
J	Rubinstein, M; Liu, C; Freeman, WT				Rubinstein, Michael; Liu, Ce; Freeman, William T.			Joint Inference in Weakly-Annotated Image Datasets via Dense Correspondence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Inference; Image graph; Semantic segmentation; Image annotation; Object discovery; Co-segmentation	RANDOM-FIELDS; COSEGMENTATION	We present a principled framework for inferring pixel labels in weakly-annotated image datasets. Most previous, example-based approaches to computer vision rely on a large corpus of densely labeled images. However, for large, modern image datasets, such labels are expensive to obtain and are often unavailable. We establish a large-scale graphical model spanning all labeled and unlabeled images, then solve it to infer pixel labels jointly for all images in the dataset while enforcing consistent annotations over similar visual patterns. This model requires significantly less labeled data and assists in resolving ambiguities by propagating inferred annotations from images with stronger local visual evidences to images with weaker local evidences. We apply our proposed framework to two computer vision problems, namely image annotation with semantic segmentation, and object discovery and co-segmentation (segmenting multiple images containing a common object). Extensive numerical evaluations and comparisons show that our method consistently outperforms the state-of-the-art in automatic annotation and semantic labeling, while requiring significantly less labeled data. In contrast to previous co-segmentation techniques, our method manages to discover and segment objects well even in the presence of substantial amounts of noise images (images not containing the common object), as typical for datasets collected from Internet search.	[Rubinstein, Michael; Liu, Ce; Freeman, William T.] Google Res, Cambridge, MA 02139 USA; [Freeman, William T.] MIT CSAIL, Cambridge, MA USA	Google Incorporated; Massachusetts Institute of Technology (MIT)	Rubinstein, M (corresponding author), Google Res, Cambridge, MA 02139 USA.	mrub@google.com; celiu@google.com; wfreeman@google.com			Microsoft Research Ph.D. Fellowship	Microsoft Research Ph.D. Fellowship	We thank Antonio Torralba for his help in collecting human foreground-background segmentations for our object discovery and segmentation Internet datasets. This work was done while Michael Rubinstein was a Ph.D. student at MIT, supported by the Microsoft Research Ph.D. Fellowship.	Bagon S, 2010, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2010.5540233; Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Collins MD, 2012, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2012.6247859; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Delong Andrew, 2011, Energy Minimization Methods Comput Vis Pattern Recognit, V6819, P147, DOI 10.1007/978-3-642-23094-3_11; Faktor A, 2012, LECT NOTES COMPUT SC, V7578, P474, DOI 10.1007/978-3-642-33786-4_35; Felzenszwalb P, 2008, PROC CVPR IEEE, P1984; Feng SL, 2004, PROC CVPR IEEE, P1002; Fergus R, 2003, PROC CVPR IEEE, P264; Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Grubinger M., 2006, INT WORKSHOP ONTOIMA, V2; Heath K, 2010, PROC CVPR IEEE, P3432, DOI 10.1109/CVPR.2010.5539991; Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261; Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121; Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719; Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868; Karsch K, 2012, LECT NOTES COMPUT SC, V7576, P775, DOI 10.1007/978-3-642-33715-4_56; Kim G., 2009, ADV NEURAL INFORM PR, P961; Kim G, 2013, PROC CVPR IEEE, P620, DOI 10.1109/CVPR.2013.86; Kim G, 2012, PROC CVPR IEEE, P837, DOI 10.1109/CVPR.2012.6247756; Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239; Krahenbuhl P., 2012, EFFICIENT INFERENCE; Kuettel D, 2012, LECT NOTES COMPUT SC, V7578, P459, DOI 10.1007/978-3-642-33786-4_34; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131; Liu S, 2012, IEEE T MULTIMEDIA, V14, P361, DOI 10.1109/TMM.2011.2174780; Makadia A, 2010, INT J COMPUT VISION, V90, P88, DOI 10.1007/s11263-010-0338-6; Mukherjee L, 2009, PROC CVPR IEEE, P2028, DOI 10.1109/CVPRW.2009.5206652; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rother C., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91; Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253; Rubinstein M, 2012, LECT NOTES COMPUT SC, V7574, P85, DOI 10.1007/978-3-642-33712-3_7; Russell B. C., 2006, P IEEE C COMP VIS PA, V2, P1605; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tappen MF, 2012, LECT NOTES COMPUT SC, V7578, P236, DOI 10.1007/978-3-642-33786-4_18; Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26; Tompkin J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185564; Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530; Vijayanarasimhan S, 2011, INT J COMPUT VISION, V91, P24, DOI 10.1007/s11263-010-0372-4; Von Ahn Luis, 2004, P SIGCHI C HUM FACT, P319, DOI DOI 10.1145/985692.985733; Wang XJ, 2010, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2010.5540046; Winn J, 2005, IEEE I CONF COMP VIS, P756; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zoran D., 2012, ADV NEURAL INF PROCE, P1736	57	5	5	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2016	119	1					23	45		10.1007/s11263-016-0894-5	http://dx.doi.org/10.1007/s11263-016-0894-5			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DP9AP		Green Published			2022-12-18	WOS:000378789400003
J	Hernandez-Vela, A; Sclaroff, S; Escalera, S				Hernandez-Vela, Antonio; Sclaroff, Stan; Escalera, Sergio			Poselet-Based Contextual Rescoring for Human Pose Estimation via Pictorial Structures	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Contextual rescoring; Poselets; Human pose estimation		In this paper we propose a contextual rescoring method for predicting the position of body parts in a human pose estimation framework. A set of poselets is incorporated in the model, and their detections are used to extract spatial and score-related features relative to other body part hypotheses. A method is proposed for the automatic discovery of a compact subset of poselets that covers the different poses in a set of validation images while maximizing precision. A rescoring mechanism is defined as a set-based boosting classifier that computes a new score for each body joint detection, given its relationship to detections of other body joints and mid-level parts in the image. This new score is incorporated in the pictorial structure model as an additional unary potential, following the recent work of Pishchulin et al. Experiments on two benchmarks show comparable results to Pishchulin et al. while reducing the size of the mid-level representation by an order of magnitude, reducing the execution time by accordingly.	[Hernandez-Vela, Antonio; Escalera, Sergio] Univ Barcelona, Dept Appl Math & Anal, Barcelona, Spain; [Sclaroff, Stan] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA; [Hernandez-Vela, Antonio; Escalera, Sergio] Comp Vis Ctr, Barcelona, Spain	University of Barcelona; Boston University; Centre de Visio per Computador (CVC)	Hernandez-Vela, A (corresponding author), Univ Barcelona, Dept Appl Math & Anal, Barcelona, Spain.; Hernandez-Vela, A (corresponding author), Comp Vis Ctr, Barcelona, Spain.	ahernandez@cvc.uab.cat	Escalera, Sergio/L-2998-2015	Escalera, Sergio/0000-0003-0617-8873	US National Science Foundation [1029430, 0910908]; Spanish Ministry of Economy and Competitiveness [TIN2013-43478-P]; Spanish government; Direct For Computer & Info Scie & Enginr [0910908] Funding Source: National Science Foundation; Div Of Information & Intelligent Systems [1029430] Funding Source: National Science Foundation	US National Science Foundation(National Science Foundation (NSF)); Spanish Ministry of Economy and Competitiveness(Spanish Government); Spanish government(Spanish GovernmentEuropean Commission); Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); Div Of Information & Intelligent Systems(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This work has been partly funded by grants #1029430 and #0910908 from the US National Science Foundation, and research project ref. TIN2013-43478-P from the Spanish Ministry of Economy and Competitiveness. The work of Antonio is supported by an FPU fellowship from the Spanish government.	Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754; [Anonymous], 2014, ADV NEURAL INF PROCE; Bourdev L, 2010, LECT NOTES COMPUT SC, V6316, P168, DOI 10.1007/978-3-642-15567-3_13; Cinbis RG, 2012, LECT NOTES COMPUT SC, V7577, P43, DOI 10.1007/978-3-642-33783-3_4; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Duan K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.116; Eichner M., 2012, COMP VIS ACCV 2012, P138; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Ferrari V, 2009, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2009.5206495; Hernandez-Vela A., 2014, P BRIT MACH IN PRESS; Johnson S., 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12.CITESEER]; Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318; Pishchulin L, 2013, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2013.433; Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82; Puertas E., 2014, ECCV 2014 W IN PRESS; Ramakrishna V, 2014, LECT NOTES COMPUT SC, V8690, P33, DOI 10.1007/978-3-319-10605-2_3; Ramanan D., 2007, ADV NEURAL INFORM PR, V19, P1129; Sapp B, 2010, PROC CVPR IEEE, P422, DOI 10.1109/CVPR.2010.5540182; Sun M, 2012, PROC CVPR IEEE, P1616, DOI 10.1109/CVPR.2012.6247854; Tian TP, 2010, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2010.5540227; Tian YD, 2012, LECT NOTES COMPUT SC, V7576, P256, DOI 10.1007/978-3-642-33715-4_19; Tompson J.J., 2014, ADV NEURAL INF PROCE, P1799, DOI DOI 10.5555/2968826.2969027; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Wang F, 2013, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2013.83; Wang Y, 2011, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR.2011.5995519; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235	27	5	5	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2016	118	1					49	64		10.1007/s11263-015-0869-y	http://dx.doi.org/10.1007/s11263-015-0869-y			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DL6ZT					2022-12-18	WOS:000375789300003
J	Liu, A; Marschner, S; Snavely, N				Liu, Albert; Marschner, Steve; Snavely, Noah			Caliber: Camera Localization and Calibration Using Rigidity Constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Calibration; Complexity; Localization		This article presents a camera calibration system, Caliber, and the underlying pose estimation problem it solves, which we call sensor localization with rigidity (SL-R). SL-R is a constraint-satisfaction-like problem that finds a set of poses satisfying certain constraints. These constraints include not only relative pose constraints such as those found in SLAM and motion estimation problems, but also rigidity constraints: the notion of objects that are rigidly attached to each other so that their relative pose is fixed over time even if that pose is not known a priori. We show that SL-R is NP-hard, but give an inference-based algorithm that works well in practice. SL-R enables Caliber, a tool to calibrate systems of cameras connected by rigid or actuated links, using image observations and information about known motions of the system. The user provides a model of the system in the form of a kinematic tree, and Caliber uses our SL-R algorithm to generate an estimate for the rigidity constraints, then performs nonlinear optimization to produce a solution that is locally least-squares optimal in terms of reprojection error. In this way, Caliber is able to calibrate a variety of setups that would have previously required special-purpose code to calibrate. We demonstrate Caliber in a number of different scenarios using both synthetic and experimental data.	[Liu, Albert; Marschner, Steve; Snavely, Noah] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA	Cornell University	Liu, A (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	ajul@cs.cornell.edu			National Science Foundation [IIS-1011919]; Intel Science and Technology Center for Visual Computing	National Science Foundation(National Science Foundation (NSF)); Intel Science and Technology Center for Visual Computing	We would like to thank Wenzel Jakob, Pramook Khungurn, Bruce Walter, and Rundong Wu for testing out CALIBER on real problems and providing feedback. Funding for this work was provided by National Science Foundation Grant IIS-1011919, by the Intel Science and Technology Center for Visual Computing, and by a gift from Autodesk.	Bouguet J-Y, 2010, CAMERA CALIBRATION T; Chen H. H., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P145, DOI 10.1109/CVPR.1991.139677; Dieudonne Y, 2010, IEEE T ROBOT, V26, P331, DOI 10.1109/TRO.2010.2042753; Esquivel S, 2007, LECT NOTES COMPUT SC, V4713, P82; Estrada C, 2009, IEEE INT CONF ROBOT, P2427; Govindu VM, 2004, PROC CVPR IEEE, P684; Govindu VM, 2001, PROC CVPR IEEE, P218; Hartley R., 2004, ROBOTICA; Heath G., 2008, SOLVE MATRIX EQUATIO; Kummerle R, 2011, IEEE INT C INT ROBOT, P3716, DOI 10.1109/IROS.2011.6048393; Kumar R., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587676; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; Nister D., 2007, INT C COMP VIS, P1, DOI DOI 10.1109/ICCV.2007.4409095; PARK FC, 1994, IEEE T ROBOTIC AUTOM, V10, P717, DOI 10.1109/70.326576; SHIU YC, 1989, IEEE T ROBOTIC AUTOM, V5, P16, DOI 10.1109/70.88014; Strobl KH, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P4647, DOI 10.1109/IROS.2006.282250; Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; TSAI RY, 1989, IEEE T ROBOTIC AUTOM, V5, P345, DOI 10.1109/70.34770; Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801; Zhengyou Zhang, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P666, DOI 10.1109/ICCV.1999.791289	21	5	5	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2016	118	1					1	21		10.1007/s11263-015-0866-1	http://dx.doi.org/10.1007/s11263-015-0866-1			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DL6ZT					2022-12-18	WOS:000375789300001
J	Ozcanli, OC; Dong, Y; Mundy, JL				Ozcanli, Ozge C.; Dong, Yi; Mundy, Joseph L.			Geo-localization using Volumetric Representations of Overhead Imagery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image search; Geo-localization; 3D Modeling; Visibility index; LIDAR; OpenStreetMap		This paper addresses the problem of determining the location of a ground level image by using geo-referenced overhead imagery. The input query image is assumed to be given with no meta-data and the content of the image is to be matched to a priori constructed reference representations. The semantic breakdown of the content of the query image is provided through manual labeling; however, all processing involving the reference imagery and matching are fully automated. In this paper, a volumetric representation is proposed to fuse different modalities of overhead imagery and construct a 3D reference world. Attributes of this reference world such as orientation of the world surfaces, types of land cover, depth order of fronto-parallel surfaces are indexed and matched to the attributes of the surfaces manually marked on the query image. An exhaustive but highly parallelizable matching scheme is proposed and the performance is evaluated on a set of query images located in a coastal region in Eastern United States. The performance is compared to a baseline region reduction algorithm and to a landmark existence matcher that uses a 2D representation of the reference world. The proposed 3D geo-localization framework performs better than the 2D approach for 75 % of the query images.	[Ozcanli, Ozge C.; Dong, Yi; Mundy, Joseph L.] Vis Syst Inc, 3 Davol Sq, Providence, RI 02906 USA		Ozcanli, OC (corresponding author), Vis Syst Inc, 3 Davol Sq, Providence, RI 02906 USA.	ozge@visionsystemsinc.com			Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory (AFRL) [FA8650-12-C-7211]	Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory (AFRL)	Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory (AFRL), Contract FA8650-12-C-7211. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.	Altwaijry H., 2014, P IEEE WINT C APPL C; [Anonymous], P IEEE C COMP VIS PA; Ardeshir S., 2014, P EUR C COMP VIS ECC; Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591009; Baatz G., 2012, P EUR C COMP VIS ECC; Bansal M., 2014, P IEEE C COMP VIS PA; Bansal M., 2012, 1 INT WORKSH VIS AN; Calakli F., 2012, P 3D IM MOD PROC VIS; Crispell D., 2011, IEEE T GEOSCI REMOTE, V49, P489; Efros A.A, 2008, P IEEE C COMP VIS PA; Frohlich B, 2013, ISPRS ANN PHOTO REM, VII-3/W1, P1; Fry JA, 2011, PHOTOGRAMM ENG REM S, V77, P859; Irschara A., 2009, P COMP VIS PATT REC; Kluckner S., 2009, P AS C COMP VIS ACCV; Lee S., 2011, P CAM NETW WORKSH CV; Li A., 2014, P EUR C COMP VIS ECC; Li Y., 2010, P EUR C COMP VIS ECC; Lin T., 2013, P IEEE C COMP VIS PA; Matei B., 2013, P IEEE WINT C APPL C; Miller A., 2011, P 4 WORKSH GEN PURP; Ozcanli O. C., 2014, P COMP VIS PATT REC; Park M., 2013, P 21 ACM SIGSPATIAL, P374; Pollard T., 2009, VOLUMETRIC APPROACH; Pollard Thomas, 2007, P COMP VIS PATT REC; Ramalingam S., 2009, P INT C COMP VIS ICC; Restrepo MI, 2012, IEEE J-STSP, V6, P522, DOI 10.1109/JSTSP.2012.2201693; Sibbing D., 2013, P 3D VIS 3DV; Sobel E., 2012, URGENT PHASE 2 FINAL; Tzeng E, 2013, IEEE COMPUT SOC CONF, P237, DOI 10.1109/CVPRW.2013.42; Ulusoy A. O., 2013, P INT C COMP VIS ICC; Unsalan C, 2011, ADV COMPUT VIS PATT, P1	31	5	5	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2016	116	3			SI		226	246		10.1007/s11263-015-0850-9	http://dx.doi.org/10.1007/s11263-015-0850-9			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7SY					2022-12-18	WOS:000369421900003
J	Anirudh, R; Turaga, P				Anirudh, Rushil; Turaga, Pavan			Geometry-Based Symbolic Approximation for Fast Sequence Matching on Manifolds	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Manifold sequence indexing; Competitive learning; Activity recognition; Differential geometry; Data mining	DIMENSIONALITY REDUCTION; RIEMANNIAN-MANIFOLDS; STATISTICAL-ANALYSIS; VIDEO; RECOGNITION; KERNELS	In this paper, we consider the problem of fast and efficient indexing techniques for sequences evolving in non-Euclidean spaces. This problem has several applications in the areas of human activity analysis, where there is a need to perform fast search, and recognition in very high dimensional spaces. The problem is made more challenging when representations such as landmarks, contours, and human skeletons etc. are naturally studied in a non-Euclidean setting where even simple operations are much more computationally intensive than their Euclidean counterparts. We propose a geometry and data adaptive symbolic framework that is shown to enable the deployment of fast and accurate algorithms for activity recognition, dynamic texture recognition, motif discovery. Toward this end, we present generalizations of key concepts of piece-wise aggregation and symbolic approximation for the case of non-Euclidean manifolds. We show that one can replace expensive geodesic computations with much faster symbolic computations with little loss of accuracy in activity recognition and discovery applications. The framework is general enough to work across both Euclidean and non-Euclidean spaces, depending on appropriate feature representations without compromising on the ultra-low bandwidth, high speed and high accuracy. The proposed methods are ideally suited for real-time systems and low complexity scenarios.	[Anirudh, Rushil; Turaga, Pavan] Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ USA; [Anirudh, Rushil; Turaga, Pavan] Arizona State Univ, Sch Arts Media & Engn, Tempe, AZ USA	Arizona State University; Arizona State University-Tempe; Arizona State University; Arizona State University-Tempe	Anirudh, R (corresponding author), Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ USA.; Anirudh, R (corresponding author), Arizona State Univ, Sch Arts Media & Engn, Tempe, AZ USA.	ranirudh@asu.edu; pturaga@asu.edu	Turaga, Pavan/W-6186-2019; Anirudh, Rushil/R-5994-2019	Anirudh, Rushil/0000-0002-4186-3502; Turaga, Pavan/0000-0002-5263-5943	NSF CCF CIF grant [1320267]; Direct For Computer & Info Scie & Enginr [1320267] Funding Source: National Science Foundation	NSF CCF CIF grant; Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	Rushil Anirudh and PavanTuraga were supported by the NSF CCF CIF grant #1320267.	Absil PA, 2004, ACTA APPL MATH, V80, P199, DOI 10.1023/B:ACAP.0000013855.14971.91; Ali S., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/ICCV.2007.4409046; Allauzen C, 2000, LECT NOTES COMPUT SC, V1848, P364; [Anonymous], 2003, INTRO DIFFERENTIABLE; Cetingul HE, 2009, PROC CVPR IEEE, P1896, DOI 10.1109/CVPRW.2009.5206806; Chakrabarti K, 2002, ACM T DATABASE SYST, V27, P188, DOI 10.1145/568518.568520; Chan AB, 2005, 2005 IEEE INTELLIGENT VEHICLES SYMPOSIUM PROCEEDINGS, P771; Chaudhry R., 2010, EUR C COMP VIS CRET; Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821; Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531; DeSieno D., 1988, IEEE International Conference on Neural Networks (IEEE Cat. No.88CH2632-8), P117, DOI 10.1109/ICNN.1988.23839; DEVROYE L, 1992, SIAM J COMPUT, V21, P48, DOI 10.1137/0221005; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; Gaur U, 2011, IEEE I CONF COMP VIS, P2595, DOI 10.1109/ICCV.2011.6126548; Goodall CR, 1999, J COMPUT GRAPH STAT, V8, P143, DOI 10.2307/1390631; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; GROVE K, 1973, MATH Z, V132, P11, DOI 10.1007/BF01214029; Harandi MT, 2014, LECT NOTES COMPUT SC, V8690, P17, DOI 10.1007/978-3-319-10605-2_2; JORDAN M, 1998, LEARNING GRAPHICAL M; Joshi S. H., 2007, CVPR; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kohonen T., 1995, SELF ORG MAPS; Lafferty J, 2005, J MACH LEARN RES, V6, P129; Lin J., 2010, 2010 IEEE 23rd International Symposium on Computer-Based Medical Systems (CBMS 2010), P13, DOI 10.1109/CBMS.2010.6042675; Lin J., 2003, DMKD, V2003, P2, DOI [DOI 10.1145/882082.882086], DOI 10.1145/882082.882086]; Lu Xia, 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233; Lui YM, 2010, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2010.5540131; Mueen A., 2009, P SIAM INT C DAT MIN, P473; Patel P, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P370, DOI 10.1109/ICDM.2002.1183925; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Rahman IU, 2005, MULTISCALE MODEL SIM, V4, P1201, DOI 10.1137/050622729; Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318; Ripley BD., 1996; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sankaranarayanan AC, 2010, LECT NOTES COMPUT SC, V6311, P129, DOI 10.1007/978-3-642-15549-9_10; Soatto S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P439, DOI 10.1109/ICCV.2001.937658; Spivak M., 1999, COMPREHENSIVE INTRO, VI; Srivastava A., 2007, IEEE C COMP VIS PATT, P1; Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184; Su JY, 2014, ANN APPL STAT, V8, P530, DOI 10.1214/13-AOAS701; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Turaga Pavan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2435, DOI 10.1109/CVPRW.2009.5206710; Turaga P, 2011, IEEE T PATTERN ANAL, V33, P2273, DOI 10.1109/TPAMI.2011.52; Turaga P, 2010, STUD COMPUT INTELL, V287, P115; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Vahdatpour A, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1261; Veeraraghavan A, 2005, IEEE T PATTERN ANAL, V27, P1896, DOI 10.1109/TPAMI.2005.246; Veeraraghavan A, 2006, 2006 IEEE COMPUTER S, V1, P959; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Vishwanathan S. V. N., 2008, CORR; YAO ACC, 1979, SIAM J COMPUT, V8, P368, DOI 10.1137/0208029; Yi S, 2012, IEEE T IMAGE PROCESS, V21, P3416, DOI 10.1109/TIP.2012.2197008; ZADOR PL, 1982, IEEE T INFORM THEORY, V28, P139, DOI 10.1109/TIT.1982.1056490	55	5	5	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2016	116	2					161	173		10.1007/s11263-015-0835-8	http://dx.doi.org/10.1007/s11263-015-0835-8			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7TJ					2022-12-18	WOS:000369423000003
J	Tian, YD; Narasimhan, SG				Tian, Yuandong; Narasimhan, Srinivasa G.			Theory and Practice of Hierarchical Data-driven Descent for Optimal Deformation Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deformation modeling; Globally optimal solutions; Non-rigid deformation; Data-driven approach; Non-linear optimization; Non-convex optimization; Image deformation; High-dimensional regression		Real-world surfaces such as clothing, water and human body deform in complex ways. Estimating deformation parameters accurately and reliably is hard due to its high-dimensional and non-convex nature. Optimization-based approaches require good initialization while regression-based approaches need a large amount of training data. Recently, to achieve globally optimal estimation, data-driven descent (Tian and Narasimhan in Int J Comput Vis , 98:279-302, 2012) applies nearest neighbor estimators trained on a particular distribution of training samples to obtain a globally optimal and dense deformation field between a template and a distorted image. In this work, we develop a hierarchical structure that first applies nearest neighbor estimators on the entire image iteratively to obtain a rough estimation, and then applies estimators with local image support to refine the estimation. Compared to its non-hierarchical version, our approach has the theoretical guarantees with significantly fewer training samples, is faster by several orders, provides a better metric deciding whether a given image requires more (or fewer) samples, and can handle more complex scenes that include a mixture of global motion and local deformation. We demonstrate in both simulation and real experiments that the proposed algorithm successfully tracks a broad range of non-rigid scenes including water, clothing, and medical images, and compares favorably against several other deformation estimation and tracking approaches that do not provide optimality guarantees.	[Tian, Yuandong; Narasimhan, Srinivasa G.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Tian, YD (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	yuandong.tian@gmail.com; srinivas@cs.cmu.edu			ONR [N00014-11-1-0295]; Microsoft Research PhD fellowship; University Transportation Center T-SET grant	ONR(Office of Naval Research); Microsoft Research PhD fellowship(Microsoft); University Transportation Center T-SET grant	This research was supported in parts by ONR grant N00014-11-1-0295, a Microsoft Research PhD fellowship, a University Transportation Center T-SET grant and a gift from TONBO Imaging.	[Anonymous], 2012, CVPR; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Barnes C, 2010, LECT NOTES COMPUT SC, V6313, P29; Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lazebnik S., 2006, P 2006 IEEE COMP VIS, P2169; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucas B. D., 1981, INT JOINT C ART INT, P674, DOI DOI 10.5555/1623264.1623280; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Moll M., 2012, ECCV; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Salzmann M., 2007, ICCV; Salzmann M., 2008, ECCV; Serre T, 2005, PROC CVPR IEEE, P994; Shi J., 1994, CVPR; Shotton J., 2011, CVPR; Tan D. J., 2014, ECCV; Taylor J., 2010, CVPR; Tian YD, 2012, INT J COMPUT VISION, V98, P279, DOI 10.1007/s11263-011-0509-0; Zhang ST, 2012, LECT NOTES COMPUT SC, V7512, P435, DOI 10.1007/978-3-642-33454-2_54	22	5	5	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2015	115	1					44	67		10.1007/s11263-015-0838-5	http://dx.doi.org/10.1007/s11263-015-0838-5			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CS0MN					2022-12-18	WOS:000361754600003
J	Guo, RQ; Hoiem, D				Guo, Ruiqi; Hoiem, Derek			Labeling Complete Surfaces in Scene Understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene understanding; Image parsing; Geometric layout; RGB-depth		Scene understanding requires reasoning about both what we can see and what is occluded. We offer a simple and general approach to infer labels of occluded background regions. Our approach incorporates estimates of visible surrounding background, detected objects, and shape priors from transferred training regions. We demonstrate the ability to infer the labels of occluded background regions in three datasets: the outdoor StreetScenes dataset, IndoorScene dataset and SUN09 dataset, all using the same approach. Furthermore, the proposed approach is extended to 3D space to find layered support surfaces in RGB-Depth scenes. Our experiments and analysis show that our method outperforms competent baselines.	[Guo, Ruiqi; Hoiem, Derek] Univ Illinois, Dept Comp Sci, Champaign, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Guo, RQ (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61801 USA.	guo29@illinois.edu; dhoiem@uiuc.edu						Bileschi S.M., 2006, THESIS CAMBRIDGE; Brostow G., 2008, P 10 EUR C COMP VIS; Choi M. J., 2010, P IEEE C COMP VIS PA; Everingham M., PASCAL VISUAL OBJECT; Felzenszwalb P., 2009, P IEEE T PATTERN ANA; Geiger A., 2011, P ADV NEUR INF PROC; Gould S., 2009, P ADV NEUR INF PROC; Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x; Guo R., 2012, P 12 EUR C COMP VIS; Guo Ruiqi, 2013, P IEEE INT C COMP VI; Gupta A., 2010, P 11 EUR C COMP VIS; Hedau V., 2009, P IEEE 12 INT COMP V; Hoiem D., 2008, P IEEE C COMP VIS PA; Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y; Isola P., 2013, P IEEE INT C COMP VI; Khosla A., 2014, P INT C COMP VIS CVP; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Lee D. C., 2009, P IEEE C COMP VIS PA; Li C., 2010, P ADV NEURAL INFORM; Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131; Malisiewicz T., 2009, ADV NEURAL INFORM PR; Ross S., 2011, P IEEE C COMP VIS PA; Russell B. C., 2005, TECHNICAL REPORT; Shotton J., 2006, P 9 EUR C COMP VIS E; Silberman N., 2014, P EUR C COMP VIS ECC; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Tighe J., 2010, P EUR C COMP VIS ECC; Zhang H., 2010, P 11 EUR C COMP VIS	29	5	6	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2015	112	2			SI		172	187		10.1007/s11263-014-0776-7	http://dx.doi.org/10.1007/s11263-014-0776-7			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CE0TD					2022-12-18	WOS:000351518500004
J	Kowdle, A; Chang, YJ; Gallagher, A; Batra, D; Chen, T				Kowdle, Adarsh; Chang, Yao-Jen; Gallagher, Andrew; Batra, Dhruv; Chen, Tsuhan			Putting the User in the Loop for Image-Based Modeling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image-based modeling; Interactive 3D reconstruction; Active-learning; Energy minimization	SILHOUETTE	We refer to the task of recovering the 3D structure of an object or a scene using 2D images as image-based modeling. In this paper, we formulate the task of recovering the 3D structure as a discrete optimization problem solved via energy minimization. In this standard framework of a Markov random field (MRF) defined over the image we present algorithms that allow the user to intuitively interact with the algorithm. We introduce an algorithm where the user guides the process of image-based modeling to find and model the object of interest by manually interacting with the nodes of the graph. We develop end user applications using this algorithm that allow object of interest 3D modeling on a mobile device and 3D printing of the object of interest. We also propose an alternate active learning algorithm that guides the user input. An initial attempt is made at reconstructing the scene without supervision. Given the reconstruction, an active learning algorithm uses intuitive cues to quantify the uncertainty of the algorithm and suggest regions, querying the user to provide support for the uncertain regions via simple scribbles. These constraints are used to update the unary and the pairwise energies that, when solved, lead to better reconstructions. We show through machine experiments and a user study that the proposed approach intelligently queries the users for constraints, and users achieve better reconstructions of the scene faster, especially for scenes with textureless surfaces lacking strong textural or structural cues that algorithms typically require.	[Kowdle, Adarsh; Gallagher, Andrew; Chen, Tsuhan] Cornell Univ, Ithaca, NY 14853 USA; [Chang, Yao-Jen] Siemens Corp, Corp Technol, Princeton, NJ USA; [Batra, Dhruv] Virginia Tech, Blacksburg, VA USA	Cornell University; Siemens AG; Virginia Polytechnic Institute & State University	Kowdle, A (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	apk64@cornell.edu; yao-jen.chang@siemens.com; acg226@cornell.edu; dbatra@vt.edu; tsuhan@ece.cornell.edu		Chen, Tsuhan/0000-0003-3951-7931				Bartoli A, 2007, COMPUT VIS IMAGE UND, V105, P42, DOI 10.1016/j.cviu.2006.07.011; Batra D, 2011, INT J COMPUT VISION, V93, P273, DOI 10.1007/s11263-010-0415-x; Baumgart B.G., 1974, THESIS STANFORD U; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Campbell N., 2007, BMVC; Campbell Neill D. F., 2008, ECCV; Chen Z., 2008, ICPR; Collins Brendan, 2008, ECCV; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Criminisi A., 1999, ICCV; Debevec P.E., 1996, SIGGRAPH; Fang YH, 2003, PATTERN RECOGN LETT, V24, P1279, DOI 10.1016/S0167-8655(02)00370-7; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Forbes K, 2006, LECT NOTES COMPUT SC, V3952, P165; Furukawa Y., 2010, CVPR; Furukawa Y., 2009, ICCV; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Gallup D., 2010, CVPR; Goesele Michael, 2007, ICCV; Gosselin PH, 2008, IEEE T IMAGE PROCESS, V17, P1200, DOI 10.1109/TIP.2008.924286; HOIEM D, 2007, IJCV, V75; Hoiem D., 2005, ACM T GRAPHICS TOG; Jain P, 2009, PROC CVPR IEEE, P762, DOI 10.1109/CVPRW.2009.5206651; Kapoor A., 2007, ICCV, P2; Kohli P., 2012, IJCV; Kohli P, 2008, COMPUT VIS IMAGE UND, V112, P30, DOI 10.1016/j.cviu.2008.07.002; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kowdle A., 2012, DEM SESS CVPR; Kowdle A., 2011, ICIP; Kowdle A., 2011, CVPR; KOWDLE A, 2010, ECCV RMLE WORKSH; Kowdle Adarsh, 2012, ECCV; LAFARGE F, 2010, CVPR; Lee W., 2007, ACCV; McGuinness K., 2012, COMPUTER VISION IMAG, V115, P868; Micusik B, 2010, INT J COMPUT VISION, V89, P106, DOI 10.1007/s11263-010-0327-9; Pollefeys M, 2008, INT J COMPUT VISION, V78, P143, DOI 10.1007/s11263-007-0086-4; Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Seitz Steven M, 2006, CVPR; Sinha S., 2008, SIGGRAPHASIA; Sinha Sudipta N., 2009, ICCV; Sketchup, 2000, GOOGL SKETCH; Snavely N., 2006, SIGGRAPH; Srivastava S., 2009, VISION MODELING VISU; Sturm P. F., 1999, BMVC; SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029; Tang K., 2009, ISCRIBBLE; van den Hengel A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276485; Vicente S., 2011, CVPR; Vijayanarasimhan S., 2010, CVPR; Yan R., 2003, ICCV; Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3	55	5	5	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2014	108	1-2			SI		30	48		10.1007/s11263-014-0704-x	http://dx.doi.org/10.1007/s11263-014-0704-x			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AG7BT		Green Submitted			2022-12-18	WOS:000335573700003
J	El-Zehiry, NY; Grady, L				El-Zehiry, Noha Youssry; Grady, Leo			Combinatorial Optimization of the Discretized Multiphase Mumford-Shah Functional	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multiphase Mumford-Shah; Discrete optimization; Image segmentation; Image denoising	LEVEL SET; GLOBAL MINIMIZATION; IMAGE SEGMENTATION; ROOF DUALITY; GRAPH CUTS; ALGORITHMS	The Mumford-Shah model has been one of the most influential models in image segmentation and denoising. The optimization of the multiphase Mumford-Shah energy functional has been performed using level sets methods that optimize the Mumford-Shah energy by evolving the level sets via the gradient descent. These methods are very slow and prone to getting stuck in local optima due to the use of gradient descent. After the reformulation of the 2-phase Mumford-Shah functional on a graph, several groups investigated the hierarchical extension of the graph representation to multi class. The discrete hierarchical approaches are more effective than hierarchical (or direct) multiphase formulation using level sets. However, they provide approximate solutions and can diverge away from the optimal solution. In this paper, we present a discrete alternating optimization for the discretized Vese-Chan approximation of the piecewise constant multiphase Mumford-Shah functional that directly minimizes the multiphase functional without recursive bisection on the labels. Our approach handles the nonsubmodularity of the multiphase energy function and provides a global optimum if the image estimation data term is known apriori.	[El-Zehiry, Noha Youssry] Siemens Corp, Imaging & Comp Vis, Corp Technol, Princeton, NJ USA; [Grady, Leo] HeartFlow Inc, Redwood City, CA USA	Siemens AG	El-Zehiry, NY (corresponding author), Siemens Corp, Imaging & Comp Vis, Corp Technol, Princeton, NJ USA.	noha.el-zehiry@siemens.com; leograd@yahoo.com						Badshah N, 2009, IEEE T IMAGE PROCESS, V18, P1097, DOI 10.1109/TIP.2009.2014260; Bae E, 2011, INT J COMPUT VISION, V92, P112, DOI 10.1007/s11263-010-0406-y; Bae E, 2009, LECT NOTES COMPUT SC, V5681, P28, DOI 10.1007/978-3-642-03641-5_3; Bae E, 2009, LECT NOTES COMPUT SC, V5567, P1, DOI 10.1007/978-3-642-02256-2_1; Boros E., 2006, 102006 RRR RUTCOR; Boykov Y., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P377, DOI 10.1109/ICCV.1999.791245; Bresson X, 2007, J MATH IMAGING VIS, V28, P151, DOI 10.1007/s10851-007-0002-0; Brown E., 2010, 0966 CAM UCLA; Bruckstein A., 1997, APPL ANAL, V79, P137; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chung G, 2005, LECT NOTES COMPUT SC, V3757, P439, DOI 10.1007/11585978_29; Collins DL, 1998, IEEE T MED IMAGING, V17, P463, DOI 10.1109/42.712135; Darbon J, 2005, LECT NOTES COMPUT SC, V3522, P351; Delong A., 2009, INT C COMP VIS, V1, P26; El-Zehiry N., 2007, P IM VIS COMP, P321; El-Zehiry N., 2008, INT C PATT REC, P1; El-Zehiry N. Y., 2009, THESIS LOUISVILLE; El-Zehiry N, 2007, PROCEEDINGS OF THE SEVENTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P182; El-Zehiry N, 2011, IMAGE VISION COMPUT, V29, P365, DOI 10.1016/j.imavis.2010.09.002; Grady L.J., 2010, DISCRETE CALCULUS AP; Grady L, 2009, IEEE T IMAGE PROCESS, V18, P2547, DOI 10.1109/TIP.2009.2028258; HAMMER PL, 1984, MATH PROGRAM, V28, P121, DOI 10.1007/BF02612354; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Jeon M, 2005, PATTERN RECOGN LETT, V26, P1461, DOI 10.1016/j.patrec.2004.11.023; Kahl F, 2011, IEEE I CONF COMP VIS, P255, DOI 10.1109/ICCV.2011.6126250; Kohli P, 2009, IEEE T PATTERN ANAL, V31, P1645, DOI 10.1109/TPAMI.2008.217; Kolmogorov V, 2005, IEEE I CONF COMP VIS, P564; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V., 2003, THESIS CORNELL U; Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031; Komodakis N, 2007, IEEE T PATTERN ANAL, V29, P1436, DOI 10.1109/TPAMI.2007.1061; Kwan RKS, 1999, IEEE T MED IMAGING, V18, P1085, DOI 10.1109/42.816072; Lellmann J, 2009, IEEE I CONF COMP VIS, P646, DOI 10.1109/ICCV.2009.5459176; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Ni K, 2009, COMPUT VIS IMAGE UND, V113, P502, DOI 10.1016/j.cviu.2008.12.006; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Pock T, 2008, LECT NOTES COMPUT SC, V5304, P792, DOI 10.1007/978-3-540-88690-7_59; Ramalingam S., 2008, P IEEE C COMP VIS PA, P1; Rother C., 2007, IEEE C COMP VIS PATT, V5302, P248; Simon H., 2001, SIAM J SCI COMPUT, V18, P1436; Vazquez-Reina A, 2009, PROC CVPR IEEE, P2020, DOI 10.1109/CVPRW.2009.5206524; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Yuan J., 2011, P 3 INT C SCAL SPAC, P279	45	5	7	1	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2013	104	3			SI		270	285		10.1007/s11263-013-0617-0	http://dx.doi.org/10.1007/s11263-013-0617-0			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	190VT					2022-12-18	WOS:000322371100004
J	Leung, C; Appleton, B; Buckley, M; Sun, CM				Leung, Carlos; Appleton, Ben; Buckley, Mitchell; Sun, Changming			Embedded Voxel Colouring with Adaptive Threshold Selection Using Globally Minimal Surfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Volumetric 3D reconstruction; Embedded voxel colouring; Globally minimal surfaces; Monotonic carving order; Causality of carving; Water-tightness	SCENE RECONSTRUCTION; STEREO	Image-based 3D reconstruction remains a competitive field of research as state-of-the-art algorithms continue to improve. This paper presents a voxel-based algorithm that adapts the earliest space-carving methods and utilises a minimal surface technique to obtain a cleaner result. Embedded Voxel Colouring is built in two stages: (a) progressive voxel carving is used to build a volume of embedded surfaces and (b) the volume is processed to obtain a surface that maximises photo-consistency data in the volume. This algorithm combines the strengths of classical carving techniques with those of minimal surface approaches. We require only a single pass through the voxel volume, this significantly reduces computation time and is the key to the speed of our approach. We also specify three requirements for volumetric reconstruction: monotonic carving order, causality of carving and water-tightness. Experimental results are presented that demonstrate the strengths of this approach.	[Buckley, Mitchell; Sun, Changming] CSIRO Math Informat & Stat, N Ryde, NSW 1670, Australia; [Appleton, Ben] Google Inc, Sydney, NSW, Australia; [Leung, Carlos] Univ Queensland, ITEE, Intelligent Real Time Imaging & Sensing Grp, Brisbane, Qld 4072, Australia	Commonwealth Scientific & Industrial Research Organisation (CSIRO); Google Incorporated; University of Queensland	Sun, CM (corresponding author), CSIRO Math Informat & Stat, Locked Bag 17, N Ryde, NSW 1670, Australia.	carlos.leung@gmail.com; appleton@google.com; mitchell.buckley@mq.edu.au; changming.sun@csiro.au	Sun, Changming/A-3276-2008	Sun, Changming/0000-0001-5943-1989; Buckley, Mitchell/0000-0003-0665-9749	CSIRO	CSIRO(Commonwealth Scientific & Industrial Research Organisation (CSIRO))	We are grateful to the anonymous reviewers for their very constructive comments. We would like to thank Jochen Wingbermuhle from the University of Hanover for the Dinosaur dataset and Jiang Guang from the Chinese University of Hong Kong for providing the camera calibration parameters. We would also like to thank Greg Slabaugh and Bruce Culbertson for the images and camera calibrations for the Toycar and Ghirardelli datasets. The Temple and Dino datasets are from the well-known Middlebury website. Thanks also to Ron Li for his helpful comments. This research was partially supported by the CSIRO's Transformational Biology Platform.	[Anonymous], 2010, MESHLAB; Appleton B, 2006, IEEE T PATTERN ANAL, V28, P106, DOI 10.1109/TPAMI.2006.12; Appleton B., 2003, DIGITAL IMAGE COMPUT, P987; APPLETON B, 2004, THESIS U QUEENSLAND; Beardsley P., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P683; Bhotika R, 2002, LECT NOTES COMPUT SC, V2352, P112; Broadhurst A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P388, DOI 10.1109/ICCV.2001.937544; Broadhurst A., 2001, THESIS U CAMBRIDGE; Campbell Neill D. F., 2008, ECCV; Carceroni RL, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P60; Caselles V, 1997, IEEE T PATTERN ANAL, V19, P394, DOI 10.1109/34.588023; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; Colosimo A, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P181, DOI 10.1109/ICIP.2001.958454; Culbertson WB., 1999, LECT NOTES COMPUTER, V1883, P100; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Dyer CR, 2001, SPRING INT SER ENG C, V628, P469; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; Freund J. E., 1992, MATH STAT; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; Fua P, 1997, INT J COMPUT VISION, V24, P19, DOI 10.1023/A:1007918123901; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Goldluecke B, 2004, PROC CVPR IEEE, P350; Graham R. L., 1972, Information Processing Letters, V1, P132, DOI 10.1016/0020-0190(72)90045-2; Hiep VH, 2009, PROC CVPR IEEE, P1430, DOI 10.1109/CVPRW.2009.5206617; Kimura M., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P135, DOI 10.1109/ICIP.1999.817086; KOCH R, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P109, DOI 10.1109/ICCV.1995.466799; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; LEUNG C, 2006, THESIS U QUEENSLAND; Leung C., 2003, DIGITAL IMAGE COMPUT, P623; Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44; MARTIN WN, 1983, IEEE T PATTERN ANAL, V5, P150, DOI 10.1109/TPAMI.1983.4767367; Moezzi S, 1997, IEEE MULTIMEDIA, V4, P18, DOI 10.1109/93.580392; Moezzi S, 1996, IEEE COMPUT GRAPH, V16, P58, DOI 10.1109/38.544073; NIEM W, 1994, P SOC PHOTO-OPT INS, V2182, P388, DOI 10.1117/12.171088; OROURKE J, 1998, COMPUTATIONAL GEOMET, pCH3; Pollefeys M., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P14, DOI 10.1109/IM.1999.805330; Pons JP, 2005, PROC CVPR IEEE, P822; Prock A., 1998, DARPA IM UND WORKSH, P315; Roy S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P492, DOI 10.1109/ICCV.1998.710763; Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462; Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882; Slabaugh G. G., 2001, P INT WORKSH VOL GRA, P81; Slabaugh GG, 2004, INT J COMPUT VISION, V57, P179, DOI 10.1023/B:VISI.0000013093.45070.3b; Slabaugh GG, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P545; Snow D, 2000, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2000.855839; Stevens MR, 2002, INT C PATT RECOG, P118, DOI 10.1109/ICPR.2002.1047413; Sun CM, 2002, INT J COMPUT VISION, V47, P99, DOI 10.1023/A:1014585622703; Vedula S, 2000, PROC CVPR IEEE, P592, DOI 10.1109/CVPR.2000.854926; WEGHORST H, 1984, ACM T GRAPHIC, V3, P52, DOI 10.1145/357332.357335; Yezzi A, 2003, INT J COMPUT VISION, V53, P31, DOI 10.1023/A:1023079624234; Yezzi A., 2002, INT S 3D PROC VIS TR, P612; Zeng G, 2007, IEEE T PATTERN ANAL, V29, P141, DOI 10.1109/TPAMI.2007.250605	53	5	5	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2012	99	2					215	231		10.1007/s11263-012-0525-8	http://dx.doi.org/10.1007/s11263-012-0525-8			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	943RU					2022-12-18	WOS:000304143700005
J	Courchay, J; Dalalyan, AS; Keriven, R; Sturm, P				Courchay, Jerome; Dalalyan, Arnak S.; Keriven, Renaud; Sturm, Peter			On Camera Calibration with Linear Programming and Loop Constraint Linearization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Camera calibration; Sequential linear programming	MOTION	A technique for calibrating a network of perspective cameras based on their graph of trifocal tensors is presented. After estimating a set of reliable epipolar geometries, a parameterization of the graph of trifocal tensors is proposed in which each trifocal tensor is linearly encoded by a 4-vector. The strength of this parameterization is that the homographies relating two adjacent trifocal tensors, as well as the projection matrices depend linearly on the parameters. Two methods for estimating these parameters in a global way taking into account loops in the graph are developed. Both methods are based on sequential linear programming: the first relies on a locally linear approximation of the polynomials involved in the loop constraints whereas the second uses alternating minimization. Both methods have the advantage of being non-incremental and of uniformly distributing the error across all the cameras. Experiments carried out on several real data sets demonstrate the accuracy of the proposed approach and its efficiency in distributing errors over the whole set of cameras.	[Courchay, Jerome; Dalalyan, Arnak S.; Keriven, Renaud] Univ Paris Est, IMAGINE, LIGM, Paris, France; [Sturm, Peter] INRIA Grenoble, Lab Jean Kuntzmann, Rhone Alpes, France	Ecole des Ponts ParisTech; UDICE-French Research Universities; Universite Paris Cite; Universite Gustave-Eiffel; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Inria	Courchay, J (corresponding author), Univ Paris Est, IMAGINE, LIGM, Paris, France.	courchay@imagine.enpc.fr; dalalyan@imagine.enpc.fr; keriven@imagine.enpc.fr; Peter.Sturm@inrialpes.fr	Dalalyan, Arnak S/G-7853-2011	Dalalyan, Arnak S/0000-0003-4337-9500	Agence Nationale de la Recherche (ANR) [ANR-09-CORD-003]	Agence Nationale de la Recherche (ANR)(French National Research Agency (ANR))	The authors acknowledge support by the Agence Nationale de la Recherche (ANR) under grant Callisto (ANR-09-CORD-003).	Agarwal S., 2009, ICCV; Avidan S, 2001, IEEE T PATTERN ANAL, V23, P73, DOI 10.1109/34.899947; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bujnak M., 2009, ICCV, P351; Chum O, 2005, PROC CVPR IEEE, P772; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Cornelis N., 2006, CVPR; Courchay J, 2010, LECT NOTES COMPUT SC, V6312, P85, DOI 10.1007/978-3-642-15552-9_7; Estrada C, 2005, IEEE T ROBOT, V21, P588, DOI 10.1109/TRO.2005.844673; Fitzgibbon A. W., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P311, DOI 10.1007/BFb0055675; Furukawa Y, 2009, INT J COMPUT VISION, V84, P257, DOI 10.1007/s11263-009-0232-2; Gherardi R, 2010, PROC CVPR IEEE, P1594, DOI 10.1109/CVPR.2010.5539782; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Govindu VM, 2006, LECT NOTES COMPUT SC, V3852, P457; Hartley R., 2003, MULTIPLE VIEW GEOMET; Havlena M., 2009, CVPR; Jacobs D, 1997, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.1997.609321; Klopschitz M., 2008, 3DPVT; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Martinec D, 2005, PROC CVPR IEEE, P198; Martinec D., 2007, CVPR; Maybank SJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P703, DOI 10.1109/ICCV.1998.710794; Mouragnon E, 2009, IMAGE VISION COMPUT, V27, P1178, DOI 10.1016/j.imavis.2008.11.006; Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a; Ponce J, 2005, PROC CVPR IEEE, P780; QUAN L, 1995, IEEE T PATTERN ANAL, V17, P34; Sattler Torsten, 2009, ICCV; Scaramuzza D, 2010, ROBOT AUTON SYST, V58, P820, DOI 10.1016/j.robot.2010.02.013; Schaffalitzky F, 2000, LECT NOTES COMPUT SC, V1842, P632; Sinha S.N., 2004, CVPR; Snavely N., 2008, CVPR, P1; Snavely N., 2006, PHOTO TOURISM EXPLOR; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Strecha C., 2008, CVPR; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; Tardif JP, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P2531, DOI 10.1109/IROS.2008.4651205; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Toni A., 2009, OMNIVIS; Torr PHS, 1997, PROC CVPR IEEE, P47, DOI 10.1109/CVPR.1997.609296; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; VU H, 2009, CVPR; Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801	43	5	6	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2012	97	1					71	90		10.1007/s11263-011-0483-6	http://dx.doi.org/10.1007/s11263-011-0483-6			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	897SD					2022-12-18	WOS:000300675300006
J	Vidal, C; Jedynak, B				Vidal, Camille; Jedynak, Bruno			Learning to Match: Deriving Optimal Template-Matching Algorithms from Probabilistic Image Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Statistical learning; Deformable template; Image registration; Anatomical landmark detection	ANATOMICAL POINT LANDMARKS; MAXIMUM-LIKELIHOOD; REGISTRATION; SEGMENTATION; LOCALIZATION; MAXIMIZATION	Finding correspondences between images by template matching is a common problem in image understanding. Although a variety of solutions have been proposed, most of them rely on the arbitrary choice of a template and a matching function. Often, different cost functions lead to different results, and the choice of a good cost for a specific application remains an art. Statistical models on the other hand, allow us to derive optimal learning and matching algorithms from modeling assumptions using likelihood maximization principles. The key contribution of this paper is the development of a statistical framework for learning what function to optimize from training examples. We present a family of statistical models for grayscale images, which allow us to derive optimal template-matching algorithms. The intensity at each pixel is described by a random variable whose distribution is encoded by a deformable template. Firstly, we assume the intensity distribution to be Gaussian and derive an intensity-matching algorithm, which is a generalization of the classical sum-of-squared differences. Then, we introduce a hidden segmentation variable in the probabilistic model and derive a segmentation-matching algorithm that can handle photometric variations. Both models are exemplified on the automatic detection of anatomical landmarks in brain Magnetic Resonance Images.	[Vidal, Camille; Jedynak, Bruno] Johns Hopkins Univ, Baltimore, MD 21218 USA	Johns Hopkins University	Vidal, C (corresponding author), Johns Hopkins Univ, 3400 N Charles St, Baltimore, MD 21218 USA.	camille.vidal@jhu.edu	jedynak, bruno m/A-8198-2009		Universite des Sciences et Technologies de Lille (Lille, France)	Universite des Sciences et Technologies de Lille (Lille, France)	This work has been founded by the Graduate Fellowship of the Universite des Sciences et Technologies de Lille (Lille, France), as well as general funds of the Center for Imaging Science and the Department of Biomedical Engineering of the Johns Hopkins University (Baltimore MD, USA). The authors are particularly grateful to Dr. Craig Stark for providing the annotated images on which the proposed method has been demonstrated, to Profs. Michael Miller and ElliotMcVeigh for supporting this work, as well as to Profs. Rene Vidal and Jean-Louis Bon for many fruitful conversations.	Allassonniere S, 2007, J R STAT SOC B, V69, P3; ALLASSONNIERE S, 2006, AC SPEECH SIGN PROC; ARAD N, 1994, CVGIP-GRAPH MODEL IM, V56, P161, DOI 10.1006/cgip.1994.1015; Ashburner J, 2005, NEUROIMAGE, V26, P839, DOI 10.1016/j.neuroimage.2005.02.018; Ashburner J, 1999, HUM BRAIN MAPP, V7, P254, DOI 10.1002/(SICI)1097-0193(1999)7:4<254::AID-HBM4>3.0.CO;2-G; BAJCSY R, 1989, COMPUT VISION GRAPH, V46, P1, DOI 10.1016/S0734-189X(89)80014-3; BARNEA DI, 1972, IEEE T COMPUT, VC 21, P179, DOI 10.1109/TC.1972.5008923; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Bookstein FL., 1992, MORPHOMETRIC TOOLS L; BroNielsen M, 1996, LECT NOTES COMPUT SC, V1131, P267; COLLIGNON A, 1995, COMP IMAG VIS, V3, P263; Cox RW, 1996, COMPUT BIOMED RES, V29, P162, DOI 10.1006/cbmr.1996.0014; Dalal N., 2005, P IEEE COMP SOC C CO, VVolume 1, P886, DOI [DOI 10.1109/CVPR.2005.177, 10.1109/CVPR.2005.177]; DAVATZIKOS C, 1997, COMPUTER VISION IMAG, V2, P207; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fischl B, 2004, NEUROIMAGE, V23, pS69, DOI 10.1016/j.neuroimage.2004.07.016; Frantz S, 2000, LECT NOTES COMPUT SC, V1935, P492; Friston KJ, 1995, HUM BRAIN MAPP, V3, P165, DOI 10.1002/hbm.460030303; Glasbey CA, 2001, J R STAT SOC B, V63, P465, DOI 10.1111/1467-9868.00295; Goshtasby A, 2003, COMPUT VIS IMAGE UND, V89, P109, DOI 10.1016/S1077-3142(03)00016-X; Grenander U, 1998, Q APPL MATH, V56, P617, DOI 10.1090/qam/1668732; HARTKENS T, 1999, P SPIE INT S, V5032, P32; Izard C, 2006, LECT NOTES COMPUT SC, V4190, P849; Joshi SC, 2000, IEEE T IMAGE PROCESS, V9, P1357, DOI 10.1109/83.855431; LEEMPUT KV, 2001, LECT NOTES COMPUTER, V2208, P204; Lester H, 1999, LECT NOTES COMPUT SC, V1613, P238; Levin A, 2006, LECT NOTES COMPUT SC, V3954, P581; LI H, 1995, IEEE T IMAGE PROCESS, V4, P320, DOI 10.1109/83.366480; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664; Pohl KA, 2006, NEUROIMAGE, V31, P228, DOI 10.1016/j.neuroimage.2005.11.044; Pohl KM, 2002, LECT NOTES COMPUT SC, V2488, P564, DOI 10.1007/3-540-45786-0_70; PRATT WK, 1974, IEEE T AERO ELEC SYS, VAE10, P353, DOI 10.1109/TAES.1974.307828; Qiu A, 2007, NEUROIMAGE, V37, P821, DOI 10.1016/j.neuroimage.2007.05.007; Roche A, 2000, INT J IMAG SYST TECH, V11, P71, DOI 10.1002/(SICI)1098-1098(2000)11:1<71::AID-IMA8>3.0.CO;2-5; Rohr K, 2001, IEEE T MED IMAGING, V20, P526, DOI 10.1109/42.929618; ROHR K, 2001, LANDMARK BASED IMAGE; Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446; STUDHOLME C, 1995, COMP IMAG VIS, V3, P287; Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009; Talairach J., 1988, COPLANAR STEREOTAXIC; Thirion JP, 1996, INT J COMPUT VISION, V18, P121, DOI 10.1007/BF00054999; TWINING C, 2002, MEASURING GEODESIC D; VIOLA P, 1995, THESIS MIT; Wahba G., 1990, SPLINE MODELS OBSERV; Wang F, 2006, ACAD RADIOL, V13, P1104, DOI 10.1016/j.acra.2006.05.017; Wells WM, 1996, IEEE T MED IMAGING, V15, P429, DOI 10.1109/42.511747; Worz S, 2006, MED IMAGE ANAL, V10, P41, DOI 10.1016/j.media.2005.02.003; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	49	5	5	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN 10	2010	88	2			SI		189	213		10.1007/s11263-009-0258-5	http://dx.doi.org/10.1007/s11263-009-0258-5			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	573YX					2022-12-18	WOS:000275955400004
J	Smith, WAP; Hancock, ER				Smith, William A. P.; Hancock, Edwin R.			Estimating Facial Reflectance Properties Using Shape-from-Shading	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Workshop on Photometric Analysis for Computer Vision held in Conjunction with the 11th International Conference on Computer Vision Conference	OCT 04, 2007	Rio de Janeiro, BRAZIL			Shape-from-shading; BRDF estimation; Reflectance modelling; Face recognition; Colour constancy	FACE RECOGNITION; ILLUMINATION; RECONSTRUCTION; IMAGES; COLOR; MODEL	In this paper we show how to estimate facial surface reflectance properties (a slice of the BRDF and the albedo) in conjunction with the facial shape from a single image. The key idea underpinning our approach is to iteratively interleave the two processes of estimating reflectance properties based on the current shape estimate and updating the shape estimate based on the current estimate of the reflectance function. For frontally illuminated faces, the reflectance properties can be described by a function of one variable which we estimate by fitting a curve to the scattered and noisy reflectance samples provided by the input image and estimated shape. For non-frontal illumination, we fit a smooth surface to the scattered 2D reflectance samples. We make use of a novel statistical face shape constraint which we term 'model-based integrability' which we use to regularise the shape estimation. We show that the method is capable of recovering accurate shape and reflectance information from single grayscale or colour images using both synthetic and real world imagery. We use the estimated reflectance measurements to render synthetic images of the face in varying poses. To synthesise images under novel illumination, we show how to fit a parametric model of reflectance to the estimated reflectance function.	[Smith, William A. P.; Hancock, Edwin R.] Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England	University of York - UK	Smith, WAP (corresponding author), Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England.	wsmith@cs.york.ac.uk	Smith, William/AAK-9101-2020; Hancock, Edwin R/C-6071-2008; Hancock, Edwin/N-7548-2019	Smith, William/0000-0002-6047-0413; Hancock, Edwin R/0000-0003-4496-2028; Hancock, Edwin/0000-0003-4496-2028				Ahmed A.H., 2006, P IEEE C COMP VIS PA, V2, P1817; Atick JJ, 1996, NEURAL COMPUT, V8, P1321, DOI 10.1162/neco.1996.8.6.1321; Atkinson GA, 2008, COMPUT VIS IMAGE UND, V111, P126, DOI 10.1016/j.cviu.2007.09.005; Baranoski G., 2004, RITA, V11, P33, DOI DOI 10.22456/2175-2745.5961; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Claridge E, 2003, MED IMAGE ANAL, V7, P489, DOI 10.1016/S1361-8415(03)00033-1; COOTES TF, 1998, P EUR C COMP VIS, V2, P484; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; Debevec P., 2000, P SIGGRAPH; DErrico J., 2005, SURFACE FITTING USIN; Donner C., 2006, RENDERING TECHN, V2006, P409, DOI DOI 10.2312/EGWR/EGSR06/409-417; DOVGARD R, 2004, P ECCV, V2, P99; Durou JD, 2008, COMPUT VIS IMAGE UND, V109, P22, DOI 10.1016/j.cviu.2007.09.003; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; Fuchs M, 2005, IEEE T VIS COMPUT GR, V11, P296, DOI 10.1109/TVCG.2005.47; Georghiades A. S., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P230; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Hertzmann A, 2005, IEEE T PATTERN ANAL, V27, P1254, DOI 10.1109/TPAMI.2005.158; HORN BKP, 1979, APPL OPTICS, V18, P1770, DOI 10.1364/AO.18.001770; KEMELMACHER I, 2006, P ECCV, P277; Koenderink J, 2003, MACH VISION APPL, V14, P260, DOI 10.1007/s00138-002-0089-7; OLIENSIS J, 1991, INT J COMPUT VISION, V6, P75, DOI 10.1007/BF00128151; OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Prados E, 2005, INT J COMPUT VISION, V65, P97, DOI 10.1007/s11263-005-3844-1; Ragheb H, 2003, PATTERN RECOGN, V36, P407, DOI 10.1016/S0031-3203(02)00070-5; Robles-Kelly A, 2005, GRAPH MODELS, V67, P518, DOI 10.1016/j.gmod.2004.12.003; Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154; SMITH WAP, 2007, P ACCV, P869; Smith WAP, 2008, INT J COMPUT VISION, V76, P71, DOI 10.1007/s11263-007-0074-8; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; *U S FLOR, 1998, USF HUMANID 3D FAC D; VANGEMERT MJC, 1989, IEEE T BIO-MED ENG, V36, P1146, DOI 10.1109/10.42108; Weyrich T, 2006, ACM T GRAPHIC, V25, P1013, DOI 10.1145/1141911.1141987; Zhang L, 2006, IEEE T PATTERN ANAL, V28, P351, DOI 10.1109/TPAMI.2006.53; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; Zhao WY, 2001, INT J COMPUT VISION, V45, P55, DOI 10.1023/A:1012369907247; Zickler T, 2008, INT J COMPUT VISION, V79, P13, DOI 10.1007/s11I263-007-0087-3; [No title captured]	40	5	5	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	2-3			SI		152	170		10.1007/s11263-008-0175-z	http://dx.doi.org/10.1007/s11263-008-0175-z			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	534NA					2022-12-18	WOS:000272903200004
J	Monaco, JP; Bovik, AC; Cormack, LK				Monaco, James P.; Bovik, Alan C.; Cormack, Lawrence K.			Active, Foveated, Uncalibrated Stereovision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Foveation; Binocular active vision; Stereovision; Registration; Nonuniform sampling; Space-variant sensing; Epipolar geometry; Disparity; Uncalibrated stereo	DEPTH; DISCRETIZATION; COMPUTATION; CALIBRATION; STEREOPSIS; ORIGINS; RANGE; CAT	Biological vision systems have inspired and will continue to inspire the development of computer vision systems. One biological tendency that has never been exploited is the symbiotic relationship between foveation and uncalibrated active, binocular vision systems. The primary goal of any binocular vision system is the correspondence of the two retinal images. For calibrated binocular rigs the search for corresponding points can be restricted to epipolar lines. In an uncalibrated system the precise geometry is unknown. However, the set of possible geometries can be restricted to some reasonable range; and consequently, the search for matching points can be confined to regions delineated by the union of all possible epipolar lines over all possible geometries. We call these regions epipolar spaces. The accuracy and complexity of any correspondence algorithm is directly proportional to the size of these epipolar spaces. Consequently, the introduction of a spatially variant foveation strategy that reduces the average area per epipolar space is highly desirable. This paper provides a set of sampling theorems that offer a path for designing foveation strategies that are optimal with respect to average epipolar area.	[Monaco, James P.; Bovik, Alan C.] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA; [Cormack, Lawrence K.] Univ Texas Austin, Dept Psychol, Austin, TX 78712 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin	Monaco, JP (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	monaco@ece.rutgers.com; bovik@ece.rutgers.com; cormack@psy.utexas.edu	Bovik, Alan/B-6717-2012	Bovik, Alan/0000-0001-6067-710X				Aloimonos J., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P35; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BASU A, 1992, PATTERN RECOGN LETT, V13, P813, DOI 10.1016/0167-8655(92)90132-J; BLAKEMORE C, 1970, J PHYSIOL-LONDON, V211, P599, DOI 10.1113/jphysiol.1970.sp009296; BOUGHEW, 1970, NATURE, V225, P42; Bovik A., 2000, HDB IMAGE VIDEO PROC; CHEN QS, 1994, P SOC PHOTO-OPT INS, V2233, P46, DOI 10.1117/12.179045; CROWELL JA, 1993, PERCEPT PSYCHOPHYS, V53, P325, DOI 10.3758/BF03205187; Davis FA., 1929, T AM OPHTHAL SOC, V27, P401; DHOND UR, 1989, IEEE T SYST MAN CYB, V19, P1489, DOI 10.1109/21.44067; DRAGER UC, 1980, J COMP NEUROL, V191, P383, DOI 10.1002/cne.901910306; EASTER SS, 1972, VISION RES, V12, P673, DOI 10.1016/0042-6989(72)90161-7; Elnagar A, 1998, PATTERN RECOGN LETT, V19, P879, DOI 10.1016/S0167-8655(98)00062-2; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; FITE KV, 1975, BRAIN BEHAV EVOLUT, V12, P97, DOI 10.1159/000124142; FOX R, 1977, SCIENCE, V197, P79, DOI 10.1126/science.867054; Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120; Hespanha JP, 1999, INT J COMPUT VISION, V35, P65, DOI 10.1023/A:1008111128520; Horn B., 1986, ROBOT VISION, P1; Howard I.P., 1995, BINOCULAR VISION STE; JENKIN MRM, 1991, CVGIP-IMAG UNDERSTAN, V53, P14, DOI 10.1016/1049-9660(91)90002-7; KAISER MK, 1995, PERCEPT PSYCHOPHYS, V57, P817, DOI 10.3758/BF03206797; Klarquist WN, 1998, IEEE T ROBOTIC AUTOM, V14, P755, DOI 10.1109/70.720351; Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092; LI F, 1999, SHANGHAI JIAOTONG DA, V33, P516; Manzotti R, 2001, COMPUT VIS IMAGE UND, V83, P97, DOI 10.1006/cviu.2001.0924; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; MONACO J, 2007, IEEE INT C IM PROC, V6, P545; Monaco JP, 2008, IEEE T IMAGE PROCESS, V17, P1672, DOI 10.1109/TIP.2008.2001405; NIELSEN KRK, 1984, VISION RES, V24, P1133, DOI 10.1016/0042-6989(84)90167-6; OLVER P, 2009, FUNDAMENTALS A UNPUB; PTITO M, 1991, NEUROPSYCHOLOGIA, V29, P443, DOI 10.1016/0028-3932(91)90004-R; RAPAPORT DH, 1984, NEUROSCIENCE, V11, P289, DOI 10.1016/0306-4522(84)90024-1; Ross C, 1996, AM J PRIMATOL, V40, P205, DOI 10.1002/(SICI)1098-2345(1996)40:3<205::AID-AJP1>3.0.CO;2-1; SANDINI G, 1990, IEEE T PATTERN ANAL, V12, P13, DOI 10.1109/34.41380; SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114; Schindler K, 2004, INT C PATT RECOG, P40, DOI 10.1109/ICPR.2004.1333700; Schreiber K, 2001, NATURE, V410, P819, DOI 10.1038/35071081; Schreiber Kai M, 2003, Strabismus, V11, P9, DOI 10.1076/stra.11.1.9.14093; SCHWARTZ E, 1985, SCIENCE, V227, P1065, DOI 10.1126/science.3975604; SCHWARTZ EL, 1980, BIOL CYBERN, V37, P63, DOI 10.1007/BF00364246; SHAH S, 1994, IEEE IMAGE PROC, P740, DOI 10.1109/ICIP.1994.413669; Shih SW, 1998, IEEE T SYST MAN CY A, V28, P426, DOI 10.1109/3468.686704; SNYDER AW, 1986, SCIENCE, V231, P499, DOI 10.1126/science.3941914; Stevenson SB, 1997, VISION RES, V37, P2717, DOI 10.1016/S0042-6989(97)00097-7; Strang G., 1988, LINEAR ALGEBRA APPL, V3rd; THOMPSON I, 1991, VISION VISUAL DYSFUN, V2, P136; TIAO YC, 1976, J COMP NEUROL, V168, P439; Tistarelli M., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P226, DOI 10.1109/WVM.1991.212803; TOOTELL RBH, 1982, SCIENCE, V218, P902, DOI 10.1126/science.7134981; van der Willigen RF, 1998, NEUROREPORT, V9, P1233, DOI 10.1097/00001756-199804200-00050; Virsu V, 1996, VISION RES, V36, P2971, DOI 10.1016/0042-6989(95)00344-4; Wang Z, 2001, IEEE T IMAGE PROCESS, V10, P1397, DOI 10.1109/83.951527; WATHEY JC, 1989, BRAIN BEHAV EVOLUT, V33, P279, DOI 10.1159/000115936; WEI J, 1998, IEEE INT C INT ROB S, V2, P866; WEIMAN CFR, 1995, P SOC PHOTO-OPT INS, V2488, P309, DOI 10.1117/12.211983; Yeshurun Y, 1999, BIOL CYBERN, V80, P117, DOI 10.1007/s004220050510; ZEEVI YY, 1993, IEEE T SIGNAL PROCES, V41, P1223, DOI 10.1109/78.205725	59	5	6	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2009	85	2					192	207		10.1007/s11263-009-0230-4	http://dx.doi.org/10.1007/s11263-009-0230-4			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	488JD					2022-12-18	WOS:000269344500005
J	Gui, L; Thiran, JP; Paragios, N				Gui, Laura; Thiran, Jean-Philippe; Paragios, Nikos			Cooperative Object Segmentation and Behavior Inference in Image Sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image segmentation; Behavior inference; Gesture recognition	GEODESIC ACTIVE REGIONS; LEVEL SET METHODS; SHAPE PRIORS; CONTOURS	In this paper, we propose a general framework for fusing bottom-up segmentation with top-down object behavior inference over an image sequence. This approach is beneficial for both tasks, since it enables them to cooperate so that knowledge relevant to each can aid in the resolution of the other, thus enhancing the final result. In particular, the behavior inference process offers dynamic probabilistic priors to guide segmentation. At the same time, segmentation supplies its results to the inference process, ensuring that they are consistent both with prior knowledge and with new image information. The prior models are learned from training data and they adapt dynamically, based on newly analyzed images. We demonstrate the effectiveness of our framework via particular implementations that we have employed in the resolution of two hand gesture recognition applications. Our experimental results illustrate the robustness of our joint approach to segmentation and behavior inference in challenging conditions involving complex backgrounds and occlusions of the target object.	[Gui, Laura; Thiran, Jean-Philippe] Ecole Polytech Fed Lausanne, Signal Proc Inst, Lausanne, Switzerland; [Paragios, Nikos] Ecole Cent Paris, Lab MAS, Chatenay Malabry, France	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; UDICE-French Research Universities; Universite Paris Saclay	Gui, L (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Inst, Lausanne, Switzerland.	laura.gui@epfl.ch; jp.thiran@epfl.ch; nikos.paragios@ecp.fr	Gui, Laura/AAB-6072-2020	Thiran, Jean-Philippe/0000-0003-2938-9657				ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; Bresson X, 2006, INT J COMPUT VISION, V68, P145, DOI 10.1007/s11263-006-6658-x; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985; Cootes TF, 1999, LECT NOTES COMPUT SC, V1613, P322; Cremers D, 2006, INT J COMPUT VISION, V66, P67, DOI 10.1007/s11263-005-3676-z; Cremers D., 2003, IEEE 2 INT WORKSH VA, P169; CREMERS D, 2006, IEEE C COMP VIS PATT, V2, P1777; Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5; Cremers D, 2006, IEEE T PATTERN ANAL, V28, P1262, DOI 10.1109/TPAMI.2006.161; Davis L, 1991, HDB GENETIC ALGORITH, DOI DOI 10.1.1.87.3586; Ferrari V., 2004, ECCV; GUI L, 2007, P IEEE C COMP VIS PA; Gui L, 2007, LECT NOTES COMPUT SC, V4485, P652; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; Kokkinos I, 2005, IEEE I CONF COMP VIS, P617; LEIBE B, 2004, ECCV WORKSH SLCV; Leventon ME, 2000, PROC CVPR IEEE, P316, DOI 10.1109/CVPR.2000.855835; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Padden C, 2003, SIGN LANGUAGE STUDIE, V4, P1, DOI DOI 10.1353/SLS.2003.0026; Paragios N, 2005, COMPUT VIS IMAGE UND, V97, P259, DOI 10.1016/j.cviu.2003.04.001; Paragios N, 2002, INT J COMPUT VISION, V46, P223, DOI 10.1023/A:1014080923068; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rathi Y, 2007, IEEE T PATTERN ANAL, V29, P1470, DOI 10.1109/TPAMI.2007.1081; ROUSSON M, 2002, ECCV, V2, P78; Terzopoulos D., 1992, TRACKING KALMAN SNAK, P3; Tu ZW, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P18, DOI 10.1109/ICCV.2003.1238309; Unser M, 1999, IEEE SIGNAL PROC MAG, V16, P22, DOI 10.1109/79.799930; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Zhao HK, 1996, J COMPUT PHYS, V127, P179, DOI 10.1006/jcph.1996.0167	34	5	5	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2009	84	2					146	162		10.1007/s11263-008-0146-4	http://dx.doi.org/10.1007/s11263-008-0146-4			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	451NO		Green Submitted			2022-12-18	WOS:000266477100003
J	Han, X; Xu, CY; Prince, JL				Han, Xiao; Xu, Chenyang; Prince, Jerry L.			A Moving Grid Framework for Geometric Deformable Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Adaptive grid method; Geometric deformable model; Deformation moving grid; Topology preservation; Level set method	LEVEL SET METHOD; ACTIVE CONTOURS; TOPOLOGY; SEGMENTATION; FLOWS; SHAPE; SNAKES; MESH	Geometric deformable models based on the level set method have become very popular in the last decade. To overcome an inherent limitation in accuracy while maintaining computational efficiency, adaptive grid techniques using local grid refinement have been developed for use with these models. This strategy, however, requires a very complex data structure, yields large numbers of contour points, and is inconsistent with the implementation of topology-preserving geometric deformable models (TGDMs). In this paper, we investigate the use of an alternative adaptive grid technique called the moving grid method with geometric deformable models. In addition to the development of a consistent moving grid geometric deformable model framework, our main contributions include the introduction of a new grid nondegeneracy constraint, the design of a new grid adaptation criterion, and the development of novel numerical methods and an efficient implementation scheme. The overall method is simpler to implement than using grid refinement, requiring no large, complex, hierarchical data structures. It also offers an extra benefit of automatically reducing the number of contour vertices in the final results. After presenting the algorithm, we demonstrate its performance using both simulated and real images.	[Prince, Jerry L.] Johns Hopkins Univ, Dept Elect & Comp Engn, Ctr Imaging Sci, Baltimore, MD 21218 USA; [Han, Xiao] Elekta Inc, CMS Software, St Louis, MO 63043 USA; [Xu, Chenyang] Siemens Corp Res, Princeton, NJ 08540 USA	Johns Hopkins University; Elekta; Siemens AG	Prince, JL (corresponding author), Johns Hopkins Univ, Dept Elect & Comp Engn, Ctr Imaging Sci, Baltimore, MD 21218 USA.	Xiao.Han@cmsrtp.com; Chenyang.Xu@siemens.com; prince@jhu.edu	Prince, Jerry L/A-3281-2010	Prince, Jerry L/0000-0002-6553-0876	NSF/ERC [CISST 9731748]; NIH/NINDS [R01NS37747]; NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE [R01NS037747] Funding Source: NIH RePORTER	NSF/ERC(European Research Council (ERC)National Science Foundation (NSF)); NIH/NINDS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS)); NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS))	This work was supported in part by NSF/ERC Grant CISST# 9731748 and by NIH/NINDS Grant R01NS37747.	ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; Bochev P., 1996, NUMER METH PART D E, V12, P489; Cao WM, 2002, SIAM J SCI COMPUT, V24, P118, DOI 10.1137/S1064827501384925; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; DROSKE M, 2001, LECT NOTES COMPUT SC, V2082, P416; Han X, 2004, NEUROIMAGE, V23, P997, DOI 10.1016/j.neuroimage.2004.06.043; Han X, 2003, IEEE T PATTERN ANAL, V25, P755, DOI 10.1109/TPAMI.2003.1201824; HAN X, 2003, GEOMETRIC LEVEL SET; HAN X, 2003, P CVPR CVPR 03 MAD W, pR1; HAN X, 2006, MOVING GRID FRAMEWOR; IVANENKO SA, 1999, HDB GRID GENERATION, P81; KAO CY, 2002, UCLACAM0266 IPAM; Karacali B, 2006, IEEE T MED IMAGING, V25, P649, DOI 10.1109/TMI.2006.873221; KASS M, 1988, INT J COMPUT VISION, V1, P312; Knupp P., 1994, FUNDAMENTALS GRID GE; KONG TY, 1989, COMPUT VISION GRAPH, V48, P357, DOI 10.1016/0734-189X(89)90147-3; Le Guyader C, 2008, IEEE T IMAGE PROCESS, V17, P767, DOI 10.1109/TIP.2008.919951; Liao G., 1999, P 8 INT MESH ROUNDT, P155; LIAO G, 2006, J COMPUTATIONAL APPL, P83; LIAO G, 1994, NUMER METH PART D E, V10, P21; Liao GJ, 2000, J COMPUT PHYS, V159, P103, DOI 10.1006/jcph.2000.6432; Lorenscn W., 1987, ACM SIGGRAPH COMPUTE, V2, DOI DOI 10.1145/37402.37422; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; MILNE B, 1995, THESIS UC BERKELEY; OISHI T, 2008, P IEEE COMP VIS PATT, P1; Peng DP, 1999, J COMPUT PHYS, V155, P410, DOI 10.1006/jcph.1999.6345; Press WH, 1995, NUMERICAL RECIPES C; Segonne F, 2008, INT J COMPUT VISION, V79, P107, DOI 10.1007/s11263-007-0102-8; Sethian J. A., 1999, LEVEL SET METHODS FA; Sethian JA, 2001, P NATL ACAD SCI USA, V98, P11069, DOI 10.1073/pnas.201222998; SHU CW, 1989, J COMPUT PHYS, V83, P32, DOI 10.1016/0021-9991(89)90222-2; Siddiqi K, 1998, IEEE T IMAGE PROCESS, V7, P433, DOI 10.1109/83.661193; Strang G., 1986, INTRO APPL MATH; Sundaramoorthi G, 2007, IEEE T IMAGE PROCESS, V16, P803, DOI 10.1109/TIP.2007.891071; Sussman M, 1999, J COMPUT PHYS, V148, P81, DOI 10.1006/jcph.1998.6106; Terzopoulos D., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P70, DOI 10.1109/CVPR.1991.139663; TSAI YR, 2001, UCLACAM0127 IPAM; USHAKOVA OV, 2001, SIAM J SCI COMPUTATI, P1274; Vasilescu M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P829, DOI 10.1109/CVPR.1992.223247; Wang SY, 2007, J COMPUT PHYS, V221, P395, DOI 10.1016/j.jcp.2006.06.029; Xie X., 2007, P 18 BRIT MACH VIS C, P1040; XU C, 2001, 34 AS C SIGN SYST CO, P483; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; Xu MH, 2004, IEEE T MED IMAGING, V23, P191, DOI 10.1109/TMI.2003.822823; Yezzi A, 1997, IEEE T MED IMAGING, V16, P199, DOI 10.1109/42.563665	47	5	6	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2009	84	1					63	79		10.1007/s11263-009-0231-3	http://dx.doi.org/10.1007/s11263-009-0231-3			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	445GH	19946381	Green Accepted			2022-12-18	WOS:000266037900004
J	Chen, P; Suter, D				Chen, P; Suter, D			An analysis of linear subspace approaches for computer vision and pattern recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						singular value decomposition; linear subspaces; principal component analysis; structure from motion; factorization method; homography; face recognition; matrix perturbation; first-order perturbation; multiple eigenvalue/singular value	FACTORIZATION METHOD; THEORETICAL-ANALYSIS; FACE RECOGNITION; IMAGE STREAMS; ILLUMINATION; MOTION; SHAPE; OBJECT; CONSTRAINTS; EIGENFACES	Linear subspace analysis (LSA) has become rather ubiquitous in a wide range of problems arising in pattern recognition and computer vision. The essence of these approaches is that certain structures are intrinsically (or approximately) low dimensional: for example, the factorization approach to the problem of structure from motion (SFM) and principal component analysis (PCA) based approach to face recognition. In LSA, the singular value decomposition (SVD) is usually the basic mathematical tool. However, analysis of the performance, in the presence of noise. has been lacking. We present such an analysis here. First, the "denoising capacity" of the SVD is analysed. Specifically, given a rank-r matrix, corrupted by noise-how much noise remains in the rank-r projected version of that corrupted matrix? Second, we study the "learning capacity" of the LSA-based recognition system in a noise-corrupted environment. Specifically, LSA systems that attempt to capture a data class as belonging to a rank-r column space will be affected by noise in both the training samples (measurement noise will mean the learning samples will not produce the "true subspace") and the test sample (which will also have measurement noise on top of the ideal clean sample belonging to the "true subspace"). These results should help one to predict aspects of performance and to design more optimal systems in computer vision, particularly in tasks, such as SFM and face recognition. Our analysis agrees with certain observed phenomenon, and these observations, together with our simulations, verify the correctness of our theory.	Monash Univ, Dept Elect & Comp Syst Engn, ARC Ctr Percept & Intelligent Machines Complex En, Clayton, Vic 3168, Australia	Monash University	Chen, P (corresponding author), Monash Univ, Dept Elect & Comp Syst Engn, ARC Ctr Percept & Intelligent Machines Complex En, Clayton, Vic 3168, Australia.	pei.chen@eng.monash.edu.au; d.suter@eng.monash.edu.au		Suter, David/0000-0001-6306-3023				Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; BASRI R, 1999, P INT C COMP VIS; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Belhumeur PN, 1998, INT J COMPUT VISION, V28, P245, DOI 10.1023/A:1008005721484; Chen P, 2005, INT J PATTERN RECOGN, V19, P479, DOI 10.1142/S0218001405004174; Chen P, 2004, IEEE T PATTERN ANAL, V26, P1051, DOI 10.1109/TPAMI.2004.52; CHEN P, 2004, SIAN C COMP VIS; EIPSTEIN R, 1995, P IEEE WORKSH PHYS B; GEORGHIADES A, 1998, P C COMP VIS PATT RE; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Golub Gene H., 2013, MATRIX COMPUTATION, V3; HALLINAN P, 1994, P C COMP VIS PATT RE; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Irani M, 2002, INT J COMPUT VISION, V48, P173, DOI 10.1023/A:1016372015744; IRANI M, 1999, P INT C COMP VIS; Kahl F, 1999, INT J COMPUT VISION, V33, P163, DOI 10.1023/A:1008192713051; KANATANI K, 2001, P INT C COMP VIS; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; LEEDAN Y, 1999, P INT C COMP VIS; LOURAKIS AND, 2000, P EUR C COMP VIS; Ma Y, 2004, INT J COMPUT VISION, V59, P115, DOI 10.1023/B:VISI.0000022286.53224.3d; Mathai A. M., 1997, JACOBIANS MATRIX TRA; Morita T, 1997, IEEE T PATTERN ANAL, V19, P858, DOI 10.1109/34.608289; MOSES Y, 1994, P EUR C COMP VIS; MURASE H, 1994, IEEE T PATTERN ANAL, V16, P1219, DOI 10.1109/34.387485; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; NENE SA, 1994, ARPA IM UND WORKSH; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448; Ramamoorthi R, 2002, IEEE T PATTERN ANAL, V24, P1322, DOI 10.1109/TPAMI.2002.1039204; SHASHUA V, 1996, RANK 4 CONSTRAINT MU; Stewart G., 1990, MATRIX PERTURBATION; Thomas JI, 1999, COMPUT VIS IMAGE UND, V76, P109, DOI 10.1006/cviu.1999.0779; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Wilkinson JH., 1965, ALGEBRAIC EIGENVALUE; Yuille AL, 1999, INT J COMPUT VISION, V35, P203, DOI 10.1023/A:1008180726317; Zelnik-Manor L, 2002, IEEE T PATTERN ANAL, V24, P214, DOI 10.1109/34.982901; ZELNIKMANOR L, 1999, P INT C COMP VIS; Zhao L, 1999, PATTERN RECOGN, V32, P547, DOI 10.1016/S0031-3203(98)00119-8	42	5	6	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2006	68	1					83	106		10.1007/s11263-006-6659-9	http://dx.doi.org/10.1007/s11263-006-6659-9			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	052JB					2022-12-18	WOS:000238228900007
J	Nayak, A; Trucco, E; Thacker, NA				Nayak, Arvind; Trucco, Emanuele; Thacker, Neil A.			When are simple LS estimators enough? An empirical study of LS, TLS, and GTLS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						least squares; total least squares; generalized total least squares; 2-D homography; correlated noise	MEASUREMENT ERROR MODEL; COMPUTER VISION; REGRESSION; ALGORITHM; SUBJECT	A variety of least-squares estimators of significantly different complexity and generality are available to solve over-constrained linear systems. The most theoretically general may not necessarily be the best choice in practice; problem conditions may be such that simpler and faster algorithms, if theoretically inferior, would yield acceptable errors. We investigate when this may happen using homography estimation as the reference problem. We study the errors of LS, TLS, equilibrated TLS and GTLS algorithms with different noise types and varying intensity and correlation levels. To allow direct comparisons with algorithms from the applied mathematics and computer vision communities, we consider both inhomogeneous and homogeneous systems. We add noise to image co-ordinates and system matrix entries in separate experiments, to take into account the effect on noise properties (heteroscedasticity) of pre-processing data transformations. We find that the theoretically most general algorithms may not always be worth their higher complexity; comparable results are obtained with moderate levels of noise intensity and correlation. We identify such levels quantitatively for the reference problem, thus suggesting when simpler algorithms can be applied with limited errors in spite of their restrictive assumptions.	Heriot Watt Univ, EPS, EECE, Vis Image & Signal Proc Grp, Edinburgh EH14 4AS, Midlothian, Scotland; Univ Manchester, Sch Med, Div Imaging Sci & Biomed Engn, Manchester M13 9PT, Lancs, England	Heriot Watt University; University of Manchester	Nayak, A (corresponding author), Heriot Watt Univ, EPS, EECE, Vis Image & Signal Proc Grp, Edinburgh EH14 4AS, Midlothian, Scotland.	amn1@hw.ac.uk; e.trucco@hw.ac.uk; neil.thacker@manchester.ac.uk	Nayak, Arvind/B-7746-2009	Trucco, Emanuele/0000-0002-5055-0794				Chan NH, 1998, ENVIRONMETRICS, V9, P235, DOI 10.1002/(SICI)1099-095X(199805/06)9:3<235::AID-ENV295>3.3.CO;2-H; CHAUDHURI S, 1991, IEEE T ROBOTIC AUTOM, V7, P707, DOI 10.1109/70.97884; Chojnacki W, 2004, IEEE T PATTERN ANAL, V26, P264, DOI 10.1109/TPAMI.2004.1262197; Chojnacki W, 2000, IEEE T PATTERN ANAL, V22, P1294, DOI 10.1109/34.888714; de Groen P. P. N., 1996, NIEUW ARCH WISK, V14, P237; GALLO PP, 1982, COMMUN STAT A-THEOR, V11, P973, DOI 10.1080/03610928208828287; GHAOUI LE, 1997, SIAM J MATRIX ANAL A; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Hartley R., 2002, MULTIPLE VIEW GEOMET; Hartley R. I., 1995, Proceedings. Fifth International Conference on Computer Vision (Cat. No.95CB35744), P1064, DOI 10.1109/ICCV.1995.466816; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; HOWELL D, 1989, FUNDAMENTAL STAT BEH; Kanatani K, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P2, DOI 10.1109/3DIM.2005.49; Kanatani K, 2000, IEICE T INF SYST, VE83D, P1369; Kanatani K., 1996, STAT OPTIMIZATION GE; Kukush A, 2002, COMPUT STAT DATA AN, V41, P3, DOI 10.1016/S0167-9473(02)00068-3; Kukush A, 2004, COMPUT STAT DATA AN, V47, P123, DOI 10.1016/j.csda.2003.10.022; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; MATEI B, 2000, IEEE COMP VIS PATT R, V2, P18; MEER P, 1991, INT J COMPUT VISION, V6, P59, DOI 10.1007/BF00127126; Muhlich M, 2001, PATTERN RECOGN LETT, V22, P1181, DOI 10.1016/S0167-8655(01)00051-4; MUHLICH M, 1998, P EUR C COMP VI; MUHLICH M, 1999, XPTRC21 JW GOETHE U; Paige CC, 2002, NUMER MATH, V91, P117, DOI 10.1007/s002110100314; SAMPSON PD, 1982, COMPUT VISION GRAPH, V18, P97, DOI 10.1016/0146-664X(82)90101-0; SPRINGER MD, 1979, ALBEBRA RANDOM VARIA; Strang G., 1988, LINEAR ALGEBRA APPL, V3rd; VANHUFFEL S, 1989, SIAM J MATRIX ANAL A, V10, P294	28	5	6	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2006	68	2					203	216		10.1007/s11263-006-6486-z	http://dx.doi.org/10.1007/s11263-006-6486-z			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	055CK					2022-12-18	WOS:000238427400006
J	Yacoob, Y; Davis, L				Yacoob, Y; Davis, L			Segmentation of planar objects and their shadows in motion sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion analysis; shadow; planar objects; projective geometry; homology		We investigate segmentation of planar objects and their cast shadows in image sequences. Given two moving image regions in an image sequence we present an algorithm for determining if the two moving regions can be interpreted as a planar object and its cast shadow. Projective geometry and motion properties are employed to directly recover a homology that constrains point correspondences of the outlines of the image regions and determine if they obey an object/shadow relationship. This homology is derived directly from the motions of the regions and therefore is easier to accomplish than determining point-to-point correspondences between candidate object-shadow pairs. Several experiments under approximate point light source illumination are presented.	Univ Maryland, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Yacoob, Y (corresponding author), Univ Maryland, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA.							ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277; CUCCHIARA R, 2001, P 11 INT C IM AN PRO; HORPRASERT T, 1999, P ICCV FRAME RATE WO; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Mikic I., 2000, ICPR, P1; Noronha S, 2001, IEEE T PATTERN ANAL, V23, P501, DOI 10.1109/34.922708; SONODA Y, 1998, P INT C SIGN PROC, P1261; SPRINGER CE, 1964, GEOMETRY ANAL PROTEC; Stauder J, 1999, IEEE T MULTIMEDIA, V1, P65, DOI 10.1109/6046.748172; SUBBARAO M, 1986, COMPUT VISION GRAPH, V36, P208, DOI 10.1016/0734-189X(86)90076-9; Van Gool L, 1998, IMAGE VISION COMPUT, V16, P21, DOI 10.1016/S0262-8856(97)00046-2; Wolberg G, 1990, DIGITAL IMAGE WARPIN	14	5	5	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2006	67	1					53	69		10.1007/s11263-006-3859-2	http://dx.doi.org/10.1007/s11263-006-3859-2			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	037BS					2022-12-18	WOS:000237122300003
J	Sanchez, O; Dibos, F				Sanchez, O; Dibos, F			Displacement following of hidden objects in a video sequence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion estimation; optical flow; segmentation; occlusion; variational methods	OPTICAL-FLOW; FIELDS	In a video sequence, computing the motion of an object requires the continuity of the apparent velocity field. This property does not hold when the object is hidden by an occlusion during its motion. The minimization of an energy functional leads to a simple algorithm which allows the recovery of the most likely trajectory of the occluded object from optical flow data at the border of the occlusion. Optical flow used for developing our method is an improvement on any variational technique of computing it. This improvement is based on a multichannel segmentation.	Univ Paris 09, CEREMADE, F-75775 Paris 16, France	UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine	Sanchez, O (corresponding author), Univ Paris 09, CEREMADE, Pl Marechal de Lattre de Tassigny, F-75775 Paris 16, France.	sanchez@ceremade.dauphine.fr; fd@ceremade.dauphine.fr						Alvarez L, 2000, INT J COMPUT VISION, V39, P41, DOI 10.1023/A:1008170101536; Alvarez L., 1999, P 16 C EC DIF APL LA, P1349; Aubert G, 1999, SIAM J MATH ANAL, V30, P1282, DOI 10.1137/S003614109834123X; Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Cohen I, 1999, INT J COMPUT VISION, V33, P29, DOI 10.1023/A:1008161130332; Cohen I., 1993, Proceedings of the 8th Scandinavian Conference on Image Analysis, P523; DERICHE R, 1995, 2697 INRIA TR; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; KOEPFLER G, 1993, 14 C GRETSI JUAN PIN, P707; Kumar A, 1996, IEEE T IMAGE PROCESS, V5, P598, DOI 10.1109/83.491336; Masnou S, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P259, DOI 10.1109/ICIP.1998.999016; MOREL JM, 1994, VARIATIONAL METHODS; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; NESI P, 1993, IMAGE VISION COMPUT, V11, P419, DOI 10.1016/0262-8856(93)90046-J; ORKISZ M, 1996, TRAITEMENT SIGNAL, V13; SANCHEZ O, 2002, THESIS U PARIS 9 DAU; Weickert J, 2001, J MATH IMAGING VIS, V14, P245, DOI 10.1023/A:1011286029287	19	5	5	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2004	57	2					91	105		10.1023/B:VISI.0000013084.09631.d8	http://dx.doi.org/10.1023/B:VISI.0000013084.09631.d8			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	766DQ					2022-12-18	WOS:000188330500001
J	Vemuri, BC; Guo, YL; Wang, ZZ				Vemuri, BC; Guo, YL; Wang, ZZ			Deformable pedal curves and surfaces: Hybrid geometric active models for shape recovery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						snakes; pedal curves and surfaces; curve and surface evolution; level-set method	CONTOURS; SNAKES	In this paper, we propose significant extensions to the "snake pedal" model, a powerful geometric shape modeling scheme introduced in (Vemuri and Guo, 1998). The extension allows the model to automatically cope with topological changes and for the first time, introduces the concept of a compact global shape into geometric active models. The ability to characterize global shape of an object using very few parameters facilitates shape learning and recognition. In this new modeling scheme, object shapes are represented using a parameterized function-called the generator-which accounts for the global shape of an object and the pedal curve (surface) of this global shape with respect to a geometric snake to represent any local detail. Traditionally, pedal curves (surfaces) are defined as the loci of the feet of perpendiculars to the tangents of the generator from a fixed point called the pedal point. Local shape control is achieved by introducing a set of pedal points-lying on a snake-for each point on the generator. The model dubbed as a "snake pedal" allows for interactive manipulation via forces applied to the snake. In this work, we replace the snake by a geometric snake and derive all the necessary mathematics for evolving the geometric snake when the snake pedal is assumed to evolve as a function of its curvature. Automatic topological changes of the model may be achieved by implementing the geometric snake in a level-set framework. We demonstrate the applicability of this modeling scheme via examples of shape recovery from a variety of 2D and 3D image data.	Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA; Sarnoff Corp, Princeton, NJ 08530 USA	State University System of Florida; University of Florida; Sarnoff Corporation	Vemuri, BC (corresponding author), Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.	vemuri@cise.ufl.edu; yguo@sarnoff.com; zwang@cise.ufl.edu						CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; Gray A, 1993, MODERN DIFFERENTIAL; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; KIMMEL R, 1995, IEEE T PATTERN ANAL, V17, P635, DOI 10.1109/34.387512; Leventon M. E., 2000, P IEEE C COMP VIS PA; Lorigo LM, 1999, LECT NOTES COMPUT SC, V1613, P126; Ma TY, 1999, IEEE T IMAGE PROCESS, V8, P1549, DOI 10.1109/83.799883; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; MALLADI R, 1993, P SOC PHOTO-OPT INS, V2031, P246, DOI 10.1117/12.146630; MARC P, 1990, IU WORKSH, P720; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; OSHER S, 1991, SIAM J NUMER ANAL, V28, P907, DOI 10.1137/0728049; Paragios NK, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1139, DOI 10.1109/ICCV.1998.710859; Romeny B.M., 1994, GEOMETRY DRIVEN DIFF; Shah J., 1996, IEEE C COMP VIS PATT; Siddiqi K, 1998, IEEE T IMAGE PROCESS, V7, P433, DOI 10.1109/83.661193; TEK H, 1995, 5 INT C COMP VIS BOS; VEMURI BC, 1994, ACM T GRAPHIC, V13, P177, DOI 10.1145/176579.176583; Vemuri BC, 2000, IEEE T PATTERN ANAL, V22, P445, DOI 10.1109/34.857002; Vemuri BC, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P427, DOI 10.1109/ICCV.1998.710754; WICKERT J, 1996, P ICAOS 96 IM WAV PD, V219, P111; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; Yezzi A.  Jr., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P898, DOI 10.1109/ICCV.1999.790317	25	5	5	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2001	44	2					137	155		10.1023/A:1011897628647	http://dx.doi.org/10.1023/A:1011897628647			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	492BX					2022-12-18	WOS:000172148300003
J	Arbel, T; Ferrie, FP				Arbel, T; Ferrie, FP			On the sequential accumulation of evidence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						sequential recognition; belief distributions; appearance flows	OBJECT RECOGNITION; HUMAN FACES; EXPLORATION; UNCERTAINTY; VIEWPOINT; MODELS	In this paper, we introduce a method for sequentially accumulating evidence as it pertains to an active observer seeking to identify an object in a known environment. We develop a probabilistic framework, based on a generalized inverse theory, where assertions are represented by conditional probability density functions. This leads to a sequential recognition strategy in which evidence is accumulated over successive viewpoints using Bayesian chaining until a definitive assertion can be made. To illustrate the theory we show how the characteristics of belief distributions can be exploited in a model-based recognition problem, where the task is to identify an unknown model from a database of known objects on the basis of parameter estimates. We illustrate the robustness of the algorithm through recognition experiments in two very different contexts: (1) a highly structured recognition context where 3-D parametric models can be estimated directly from range data, (2) a complex environment, where the relationship between the data and the model is learned through an appearance-based strategy. Specifically, the flow fields computed through the object's motion are used as structural signatures for recognition.	McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2A7, Canada	McGill University	Arbel, T (corresponding author), McGill Univ, Ctr Intelligent Machines, 3480 Univ St, Montreal, PQ H3A 2A7, Canada.	taly@cim.mcgill.edu; ferrie@cim.mcgill.ca						ALOIMONOS J, 1987, P 1 INT C COMP VIS L; Arbel T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P248, DOI 10.1109/ICCV.1999.791227; ARBEL T, 1994, INT C PATT RECOG, P470, DOI 10.1109/ICPR.1994.576328; Arbel T, 1996, PATTERN RECOGN LETT, V17, P491, DOI 10.1016/0167-8655(96)00008-6; ARBEL T, 1999, P 2 IEEE WORKSH PERC, P87; ARBEL T, 1996, 4 EUR C COMP VIS CAM, V1064, P469; ARBEL T, 1995, THESIS MCGILL U MONT; ARMAN F, 1993, COMPUT SURV, V25, P5, DOI 10.1145/151254.151255; AVIV G, 1989, IEEE T PATTERN ANAL, V2, P477; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; BAJCSY R, 1987, P 1 INT C COMP VIS L; Barr A. H., 1981, IEEE COMPUT GRAPH, V1, P1, DOI [DOI 10.1109/MCG.1981.1673788, 10.1109/MCG.1981.1673788]; Benoit S. M., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P864, DOI 10.1109/ICPR.1996.546147; Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277; BLACK MJ, 1998, INT C AUT FAC GEST R; BLACK MJ, 1998, ECCV98 FREIB GERM JU, V1, P909; BOBICK AF, 1996, UNP ICPR 96; BURGARD W, 1997, P 15 INT JOINT C ART; Callari F. G., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P925, DOI 10.1109/ICPR.1996.546159; CHIN RT, 1986, COMPUT SURV, V18, P67, DOI 10.1145/6462.6464; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Darrell T., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P112, DOI 10.1109/ICCV.1990.139506; Dempster A.P., 1976, MATH THEORY EVIDENCE; Dickinson SJ, 1997, COMPUT VIS IMAGE UND, V67, P239, DOI 10.1006/cviu.1997.0532; DURRANTWHYTE HF, 1988, INT J ROBOT RES, V7, P97, DOI 10.1177/027836498800700608; FERRIE F, 1989, P 5 INT C IM AN POS; FERRIE FP, 1993, IEEE T PATTERN ANAL, V15, P771, DOI 10.1109/34.236252; Herbin S, 1996, PROC CVPR IEEE, P35, DOI 10.1109/CVPR.1996.517050; HUMMEL RA, 1988, IEEE T PATTERN ANAL, V10, P235, DOI 10.1109/34.3885; HUTCHINSON SA, 1989, IEEE T ROBOTIC AUTOM, V5, P765, DOI 10.1109/70.88098; Isard M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P343, DOI 10.1007/BFb0015549; JEBARA T, 1988, P 6 INT C COMP VIS B; KEREN D, 1992, 102 LEMS BROWN U LAB; KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390; Kittler J., 1989, International Journal of Pattern Recognition and Artificial Intelligence, V3, P29, DOI 10.1142/S021800148900005X; Lejeune A., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P800, DOI 10.1109/CVPR.1993.341183; LI N, 1997, MBR97, pCH15; MOGHADDAM WWB, 1998, 3 IEEE INT C AUT FAC; MOHR SPR, 1997, CAIP 97; NAYAR SK, 1996, PARAMETRIC APPEARANC, pCH6; ONISHI MIM, 1998, 2 INT WORKSH STAT TE; Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P715, DOI 10.1109/34.85660; RAJA NS, 1992, IMAGE VISION COMPUT, V10, P179, DOI 10.1016/0262-8856(92)90069-F; RAO BS, 1993, IEEE T SYST MAN CYB, V23, P1683, DOI 10.1109/21.257763; SCHIELE B, 1996, P 13 INT C PATT REC; SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519; SUBRAHMONIA J, 1992, 107 LEMS BROWN U LAB; Tarantola A., 1987, INVERSE PROBLEM THEO; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; WATANABE NTM, 1996, P 13 INT C PATT REC, V10, P528; Whaite P, 1997, IEEE T PATTERN ANAL, V19, P193, DOI 10.1109/34.584097; Whaite P, 1997, IEEE T PATTERN ANAL, V19, P899, DOI 10.1109/34.608292; WHAITE P, 1991, IEEE T PATTERN ANAL, V13, P1038, DOI 10.1109/34.99237; Wilkes D., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P136, DOI 10.1109/CVPR.1992.223215	56	5	5	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2001	43	3					205	230		10.1023/A:1011187530616	http://dx.doi.org/10.1023/A:1011187530616			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	457WY					2022-12-18	WOS:000170161400005
J	Berengolts, A; Lindenbaum, M				Berengolts, A; Lindenbaum, M			On the performance of connected components grouping	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						perceptual organization; grouping performance analysis; quantitative predictions; connected components algorithm	PERCEPTUAL ORGANIZATION; COMPUTER VISION; MODELS; IMAGES	Grouping processes may benefit computationally when simple algorithms are used as part of the grouping process. In this paper we consider a common and extremely fast grouping process based on the connected components algorithm. Relying on a probabilistic model, we focus on analyzing the algorithm's performance. In particular, we derive the expected number of addition errors and the group fragmentation rate. We show that these performance figures depend on a few inherent and intuitive parameters. Furthermore, we show that it is possible to control the grouping process so that the performance may be chosen within the bounds of a given tradeoff. The analytic results are supported by implementing the algorithm and testing it on synthetic and real images.	Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Berengolts, A (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.							Amir A, 1998, IEEE T PATTERN ANAL, V20, P168, DOI 10.1109/34.659934; [Anonymous], 1985, PERCEPTUAL ORG VISUA; Bollobas B., 2001, RANDOM GRAPHS, V2nd edn; Boyer KL, 1999, COMPUT VIS IMAGE UND, V76, P1, DOI 10.1006/cviu.1999.0797; Broadbent S., 1957, P CAMBRIDGE PHIL SOC, V53, P629, DOI DOI 10.1017/S0305004100032680; CLEMENS DT, 1991, IEEE T PATTERN ANAL, V13, P1007, DOI 10.1109/34.99235; CORMEN TH, 1989, MIT ELECT ENG COMP S; DUBUC B, 1994, ICPR 94 JER, V1, P216; DUDA RO, 1972, COMMUN ACM, V15, P1; ERDOS P, 1960, MAGYAR TUD AKAD MAT, V5, P17; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gradshtein I.S., 1980, TABLE INTEGRALS SERI; Grimmett G., 1999, GRUNDLEHREN MATH WIS, Vsecond edition; GRIMSON E, 1990, OBJECT RECOGNITION C; HERAULT L, 1993, IEEE T PATTERN ANAL, V15, P899, DOI 10.1109/34.232076; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; JACOBS D, 1988, THESIS MIT; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; MOHAN R, 1992, IEEE T PATTERN ANAL, V14, P616, DOI 10.1109/34.141553; Ramesh V, 1997, MACH VISION APPL, V9, P229, DOI 10.1007/s001380050044; SAINTMARC P, 1993, IEEE T PATTERN ANAL, V15, P1191, DOI 10.1109/34.244680; SARKAR S, 1993, IEEE T SYST MAN CYB, V23, P382, DOI 10.1109/21.229452; SARKAR S, 2000, PERCEPTUAL ORG ARTIF; Sha'asua A., 1988, INT C COMP VIS, P321; SHAPIRO LG, 1979, IEEE T PATTERN ANAL, V1, P10, DOI 10.1109/TPAMI.1979.4766871; SHI J, 1997, CVPR, P731; Stauffer D., 2018, INTRO PERCOLATION TH; Tenenbaum Jay M, 1983, HUMAN MACHINE VISION, P481	28	5	5	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		2001	41	3					195	216		10.1023/A:1011108121495	http://dx.doi.org/10.1023/A:1011108121495			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	428LE					2022-12-18	WOS:000168462500004
J	Florack, L				Florack, L			Non-linear scale-spaces isomorphic to the linear case with applications to scalar, vector and multispectral images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						linear/nonlinear/morphological scale-space theory; scalar/vector/multispectral imagery	EDGE-DETECTION	A basic requirement of scale-space representations in general is that of scale causality, which states that local extrema in the image should not be enhanced when resolution is diminished. We consider a special class of nonlinear scale-spaces consistent with this constraint, which can be linearised by a suitable isomorphism in the grey-scale domain so as to reproduce the familiar Gaussian scale-space. We consider instances in which nonlinear representations may be the preferred choice, as well as instances in which they enter by necessity. We also establish their relation to morphological scale-space representations based on a quadratic structuring function.	Univ Utrecht, Dept Math, NL-3584 CD Utrecht, Netherlands	Utrecht University	Florack, L (corresponding author), Univ Utrecht, Dept Math, Budapestlaan 6, NL-3584 CD Utrecht, Netherlands.							BLOCH I, 1995, PATTERN RECOGN, V28, P1341, DOI 10.1016/0031-3203(94)00312-A; DORST L, 1994, SIGNAL PROCESS, V38, P79, DOI 10.1016/0165-1684(94)90058-2; Florack L, 1999, INT J COMPUT VISION, V31, P247, DOI 10.1023/A:1008026217765; HENDEE WR, 1993, PERCEPTION VISUAL IN; Iijima T., 1962, BULL ELECT LAB, V26, P368; Kimmel R, 1999, LECT NOTES COMPUT SC, V1682, P294; KIMMEL R, 1997, LECT NOTES COMPUTER, V1252; Koenderink JJ, 1999, INT J COMPUT VISION, V31, P159, DOI 10.1023/A:1008065931878; Lax P. D., 1973, HYPERBOLIC SYSTEMS C; Lindeberg T., 1994, SCALE SPACE THEORY C; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Nielsen M, 1997, J MATH IMAGING VIS, V7, P291, DOI 10.1023/A:1008282127190; Olver P.J., 1986, GRADUATE TEXTS MATH, V107; OTSU N, 1981, THESIS ELECTROTECHNI; ROMENY BMT, 1997, LECT NOTES COMPUTER, V1252; Smoller J., 2012, SHOCK WAVES REACTION, DOI [10.1007/978-1-4612-0873-0, DOI 10.1007/978-1-4612-0873-0]; SPORRING J, 1997, COMPUTATIONAL IMAGIN, V8; Thorne K.S., 2017, GRAVITATION; van den Boomgaard R, 1992, NIEUW ARCH WISKD, V10, P219; VANDENBOOMGAARD R, 1994, IEEE T PATTERN ANAL, V16, P1101, DOI 10.1109/34.334389; vandenBoomgaard R, 1997, COMP IMAG VIS, V8, P203; VANDENBOOMGAARD R, 1992, THESIS U AMSTERDAM; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; [No title captured]	25	5	5	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		2001	42	1-2					39	53		10.1023/A:1011185417206	http://dx.doi.org/10.1023/A:1011185417206			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	437VX					2022-12-18	WOS:000169015200004
J	Genc, Y; Ponce, J				Genc, Y; Ponce, J			Image-based rendering using parameterized image varieties	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image-based rendering; parameterized image varieties; weak perspective and paraperspective projections; motion analysis; multi-view geometry	SHAPE; ACQUISITION; MOTION	This paper addresses the problem of characterizing the set of all images of a rigid set of m points and n lines observed by a weak perspective or paraperspective camera. By taking explicitly into account the Euclidean constraints associated with calibrated cameras, we show that the corresponding image space can be represented by a six-dimensional variety embedded in R2(m+n) and parameterized by the image positions of three reference points. The coefficients defining this parameterized image variety (or PIV for short) can be estimated from a sample of images of a scene via linear and non-linear least squares. The PIV provides an integrated framework for using both point and line features to synthesize new images from a set of pre-recorded pictures (image-based rendering). The proposed technique does not perform any explicit three-dimensional scene reconstruction but it supports hidden-surface elimination, texture mapping and interactive image synthesis at frame rate on ordinary PCs. It has been implemented and extensively tested on real data sets.	Siemens Corp Res Inc, Imaging & Visualizat Dept, Princeton, NJ 08540 USA; Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	Siemens AG; University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Genc, Y (corresponding author), Siemens Corp Res Inc, Imaging & Visualizat Dept, Princeton, NJ 08540 USA.	ygenc@scr.siemens.com; ponce@cs.uiuc.edu	Genc, Yakup/AAG-4668-2019	Genc, Yakup/0000-0002-6952-6735				Adelson E., 1991, PLENOPTIC FUNCTION E; ALOIMONOS JY, 1990, IMAGE VISION COMPUT, V8, P179, DOI 10.1016/0262-8856(90)90064-C; Avidan S, 1997, PROC CVPR IEEE, P1034, DOI 10.1109/CVPR.1997.609457; AYACHE N, 1997, INT J COMPUT VISION, V1, P101; BARRETT EB, 1992, ARTIF INT, P277; BAUMGART B, 1974, AIM249 STANF U DEP C; Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191; Farin G, 1990, CURVES SURFACES COMP; FAUGERAS O, 1995, J OPT SOC AM A, V12, P465, DOI 10.1364/JOSAA.12.000465; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FITZGIBBON AW, 1998, ECCV, P311; Genc Y., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P67, DOI 10.1109/CVPR.1999.784610; Genc Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P11, DOI 10.1109/ICCV.1998.710695; GENC Y, 1999, THESIS U ILLINOIS UR; GENC Y, 1999, VIS ALG WORKSH KER G; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; GRIMSON WEL, 1981, IMAGES SURFACES; Harris C, 1988, P 4 ALV VIS C, P147, DOI DOI 10.5244/C.2.23; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; Havaldar P, 1996, PROC GRAPH INTERF, P61; HECKBERT PS, 1986, IEEE COMPUT GRAPH, V6, P56, DOI 10.1109/MCG.1986.276672; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; KANADE T, 1995, IEEE WORKSH REPR VIS; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; KUTULAKOS K, 1996, LECT NOTES COMPUTER, V1144, P381; LAVEAU S, 1994, 2205 INRIA SOPH ANT; Leedan Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P733, DOI 10.1109/ICCV.1998.710799; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Lippman A., 1980, Computer Graphics, V14, P32, DOI 10.1145/965105.807465; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; Macaulay F.S., 1916, ALGEBRAIC THEORY MOD; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; MCLAUCHLAN P, 1998, P INT C COMP VIS BOS, P314; Morris DD, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P696, DOI 10.1109/ICCV.1998.710793; Narayanan PJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P3, DOI 10.1109/ICCV.1998.710694; NISHIHARA HK, 1984, PRISM PRACTICAL REAL; OHTA Y, 1981, P INT JOINT C ART IN, P746; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P170, DOI 10.1109/TPAMI.1984.4767501; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; Pollefeys M, 1998, IEEE INT CONF ROBOT, P2771, DOI 10.1109/ROBOT.1998.680450; PONCE J, 2000, P SMILE 2000 WORKSH; POPE AR, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P768, DOI 10.1109/CVPR.1994.323895; Pritchett P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P754, DOI 10.1109/ICCV.1998.710802; Seitz S., 1995, WORKSH REPR VIS SCEN; Seitz S. M., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P28, DOI 10.1109/CVPR.1999.784604; SEITZ SM, 1996, SIGGRAPH, P21; SHASHUA A, 1994, LECT NOTES COMPUTER, V800, P479; SHEWCHUK JR, 1996, APPL COMPUTATION MAY, P124; Sullivan S, 1998, IEEE T PATTERN ANAL, V20, P1091, DOI 10.1109/34.722621; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; WEINSHALL D, 1995, IEEE T PATTERN ANAL, V17, P512, DOI 10.1109/34.391392; WHITAKER JW, 1980, PHYSIOLOGIST, V23, P6	55	5	5	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		2001	41	3					143	170		10.1023/A:1011114903748	http://dx.doi.org/10.1023/A:1011114903748			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	428LE					2022-12-18	WOS:000168462500001
J	Pittore, M; Campani, M; Verri, A				Pittore, M; Campani, M; Verri, A			Learning to recognize visual dynamic events from examples	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						dynamic events; pattern recognition; Support Vector Machines; computer vision systems		This paper describes a trainable and flexible system able to recognize visual dynamic events, e.g. movements performed by different people, from a stream of images taken by a fixed camera. Each event is represented by a feature vector built from the spatio-temporal changes detected in the observed image sequence. The system neither attempts to recover the 3D structure nor assumes a prior model of the observed dynamic events. During training a supervisor identifies and labels the events of interest among those automatically detected by the system. At run time, previously unseen events are detected and classified on the basis of the available examples. Several experiments on real images are reported and the benefits of using Support Vector Machines for performing effective classification from a relatively small number of labeled examples and for building noise tolerant representations are discussed. Preliminary results indicate that the proposed system can also be applied with equally good results to the case in which the dynamic events are gestures performed by different people.	Univ Genoa, INFM, DISI, Genoa, Italy; Univ Genoa, INFM, DIFI, Genoa, Italy; MIT, Ctr Biol & Computat Learning, Cambridge, MA 02139 USA	Consiglio Nazionale delle Ricerche (CNR); Istituto Nazionale per la Fisica della Materia (INFM-CNR); University of Genoa; Consiglio Nazionale delle Ricerche (CNR); Istituto Nazionale per la Fisica della Materia (INFM-CNR); University of Genoa; Massachusetts Institute of Technology (MIT)	Pittore, M (corresponding author), Univ Genoa, INFM, DISI, Genoa, Italy.	pittore@disi.unige.it; campani@fisica.unige.it; verri@ai.mit.edu	Pittore, Massimiliano/F-5543-2013	Pittore, Massimiliano/0000-0003-4940-3444; Campani, Marco/0000-0002-3519-7750				ANDRE E, 1988, P EUR C ART INT MUN; Bar-Shalom Y., 1988, TRACKING DATA ASS; Bobick AF, 1999, PRESENCE-VIRTUAL AUG, V8, P369, DOI 10.1162/105474699566297; BREGLER C, 1997, P IEEE C COMP VIS PA; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; EVGENIOU T, 1999, 171 CBCL MIT; HARITAOGLU I, 1998, P EUR C COMP VIS FRI; Intille S.S., 1997, P IEEE C COMP VIS PA; JOACHIMS T, 1998, P EUR C MACH LEARN C; MATIC N, 1993, P 2 INT C PATT REC D; OLSON T, 1997, P DARPA IM UND WORKS; Osuna E., 1997, P IEEE C COMP VIS PA; Pontil M, 1998, IEEE T PATTERN ANAL, V20, P637, DOI 10.1109/34.683777; ROOBAERT D, 1999, P SVM WORKSH INT JOI; SMERALDI F, 1999, P 2 INT C AUD VID BA; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vapnik V.N, 1998, STAT LEARNING THEORY; Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236	18	5	6	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	38	1					35	44		10.1023/A:1008114700759	http://dx.doi.org/10.1023/A:1008114700759			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	342GH					2022-12-18	WOS:000088636500004
J	Yacoob, Y; Davis, LS				Yacoob, Y; Davis, LS			Temporal multi-scale models for flow and acceleration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion estimation; optical flow; non-rigid motion	OPTICAL-FLOW; MOTION; COMPUTATION	A model for computing image flow in image sequences containing a very wide range of instantaneous flows is proposed. This model integrates the spatio-temporal image derivatives from multiple temporal scales to provide both reliable and accurate instantaneous flow estimates. The integration employs robust regression and automatic scale weighting in a generalized brightness constancy framework. In addition to instantaneous flow estimation the model supports recovery of dense estimates of image acceleration and can be readily combined with parameterized flow and acceleration models. A demonstration of performance on image sequences of typical human actions taken with a high frame-rate camera is given.	Univ Maryland, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Yacoob, Y (corresponding author), Univ Maryland, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA.	yaser@umiacs.umd.edu; lsd@umiacs.umd.edu						ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; BATTITI R, 1991, INT J COMPUT VISION, V6, P133, DOI 10.1007/BF00128153; Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141; BERGEN JR, 1992, LECT NOTES COMPUT SC, V588, P237; Black M. J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P231, DOI 10.1109/ICCV.1993.378214; Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; GEMAN S, 1987, B INT STAT I, V4, P5; HEEGER DJ, 1987, INT J COMPUT VISION, V1, P279, DOI 10.1007/BF00133568; Lindeberg T., 1994, SCALE SPACE THEORY C; LINDEBERG T, 1996, 196 CVAP STOCKH U; SINGH A, 1991, IEEE WORKSH VIS MOT, P36; WEBER J, 1995, INT J COMPUT VISION, V14, P67, DOI 10.1007/BF01421489	16	5	5	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1999	32	2					147	163		10.1023/A:1008109516258	http://dx.doi.org/10.1023/A:1008109516258			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	229HC					2022-12-18	WOS:000082186600003
J	Donati, L; Stolfi, N				Donati, L; Stolfi, N			Singularities of illuminated surfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							PROJECTIONS	We study local singularities that appear in pictures of illuminated objects: we suppose there is only one light source and we look at the shadow it casts on the surface. We focus on the frontier between shadow and light. In the visual plane, these curves interact with the apparent contour of the surface. The search of generic situations translates into a classification problem in singularity theory. We are thus able to give the list of the six stable and generic views of illuminated surfaces.			Donati, L (corresponding author), UNIV NICE, URA CNRS 168, LAB JEAN ALEXANDRE DIEUDONNE, NICE, FRANCE.							[Anonymous], 1975, PSYCHOL COMPUTER VIS; ARNOLD VI, 1983, RUSS MATH SURV+, V38, P87, DOI 10.1070/RM1983v038n02ABEH003471; BRUCE JW, 1990, P LOND MATH SOC, V60, P392; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; DONATI L, 1997, MATH ANN, V482; DUFOUR JP, 1992, J GEOM PHYS, V9, P173; Golubitsky 19 M, 1973, STABLE MAPPINGS THEI; GORJUNOV VV, 1990, ADV SOVIET MATH; HENRY JP, 1993, PROG MATH, V109, P105; HUFFMAN DA, 1977, MACH INTELL, V8, P493; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1982, PERCEPTION, V11, P129, DOI 10.1068/p110129; KOENDERINK JJ, 1976, BIOL CYBERN, V24, P51, DOI 10.1007/BF00365595; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; MARTINET J, 1982, LONDON MATH SOC LECT, V58; MATHER JN, 1973, ANN MATH, V98, P226, DOI 10.2307/1970783; PLATONOVA OA, 1981, SINGULARITIES SYSTEM, P647; RIEGER JH, 1987, J LOND MATH SOC, V36, P351, DOI 10.1112/jlms/s2-36.2.351; SHCHERBAK OP, 1982, T TBILISI I MAT, V13, P232; SHCHERBAK OP, 1982, T TBILISI I MAT, V13, P280; WHITNEY H, 1955, ANN MATH, V62, P374, DOI 10.2307/1970070	21	5	5	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN-JUL	1997	23	3					207	216		10.1023/A:1007915015508	http://dx.doi.org/10.1023/A:1007915015508			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XN801					2022-12-18	WOS:A1997XN80100001
J	Sugimoto, A				Sugimoto, A			Object recognition by combining paraperspective images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								This paper provides a study on object recognition under paraperspective projection. Discussed is the problem of determining whether or not a given image was obtained from a 3-D object to be recognized. First it is clarified that paraperspective projection is the first-order approximation of perspective projection. Then it is shown that, if we represent an object as a set of its feature points and the object undergoes a rigid transformation or an affine transformation, any paraperspective image can be expressed as a linear combination of several appropriate paraperspective images: we need at least three images for rigid transformations; whereas we need at least two images for affine transformations. Particularly in the case of a rigid transformation, the coefficients of the combination have to satisfy two conditions: orthogonality and norm equality. A simple algorithm to solve the above problem based on these properties is presented: a linear, single-shot algorithm. Some experimental results with synthetic images and real images are also given.	ATR,HUMAN INFROMAT PROC RES LABS,KYOTO 61902,JAPAN									AGIN GJ, 1976, IEEE T COMPUT, V25, P439, DOI 10.1109/TC.1976.1674626; ALOIMONOS J, 1988, BIOL CYBERN, V58, P345, DOI 10.1007/BF00363944; Aloimonos J., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P518; BESL PJ, 1985, CM COMPUTING SURVEYS, V1, P75; BINFORD TO, 1971, P IEEE C SYST CONTR; BURNS JB, 1993, IEEE T PATTERN ANAL, V15, P51, DOI 10.1109/34.184774; Duda R.O., 1973, J ROYAL STAT SOC SER; MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020; MOSES Y, 1992, P EUR C COMP VIS, P820; Mundy J., 1992, GEOMETRIC INVARIANCE; OHTA Y, 1981, P INT JOINT C ART IN, P746; POELMAN C, 1994, P 3 EUR C COMP VIS S, V2, P97; POELMAN CJ, 1992, CMUCS92208 CARN  MEL; POGGIO T, 1990, NATURE, V343, P263, DOI 10.1038/343263a0; Poggio T., 1990, 900503 IRST; ROTHWELL CA, 1993, P IEEE INT C COMPUTE, P573; SOROKA BI, 1976, P 3 IJCPR, P734; SUGIMOTO A, 1993, TRH034 ATR; SUGIMOTO A, 1994, P 12 INT C PATT REC, V1, P190; SUGIMOTO A, 1995, SIGCV93 INF PROC SOC, P19; SUGIMOTO A, 1993, P 8 SCAND C IM AN, V2, P1161; SUGIMOTO A, 1993, P SPIE C, P183; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; WEISS I, 1993, INT J COMPUT VISION, V10, P207, DOI 10.1007/BF01539536	24	5	6	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1996	19	2					181	201		10.1007/BF00055804	http://dx.doi.org/10.1007/BF00055804			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VE939					2022-12-18	WOS:A1996VE93900005
J	WU, L; KANATANI, K				WU, L; KANATANI, K			INTERPRETATION OF CONIC MOTION AND ITS APPLICATIONS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							RIGID PLANAR PATCH; IMAGE FLOW; PARAMETERS; SURFACE; DEFORMATION; CURVES; POSE	The indeterminacy of conic motion is analyzed in terms of Lie group theory. It is shown that an image motion of a conic is associated with a group of invisible motions that do not cause a visible change of the conic. All such groups are isomorphic to the group associated with a special conic called the standard circle, for which the group of invisible motions is the (three-dimensional) Lorentz group. Similar results are obtained for invisible optical flows. Finally, our analysis is extended to conic stereo: the 3-D position and orientation of a conic in the scene are computed from two projections. This algorithm also works with one camera if a circular pattern is projected from a light source.	GUNMA UNIV,DEPT COMP SCI,KIRYU,GUNMA 376,JAPAN	Gunma University								BERGHOLM F, 1991, CVGIP-IMAG UNDERSTAN, V53, P171, DOI 10.1016/1049-9660(91)90025-K; BERGHOLM F, 1989, IJNT J COMPUT VIS, V4, P395; BOOKSTEIN FL, 1979, COMPUT VISION GRAPH, V9, P56, DOI 10.1016/0146-664X(79)90082-0; DAVIS LS, 1983, COMPUT VISION GRAPH, V23, P313, DOI 10.1016/0734-189X(83)90029-4; ELLIS T, 1992, IMAGE VISION COMPUT, V10, P271, DOI 10.1016/0262-8856(92)90041-Z; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; Griewank A., 1991, AUTOMATIC DIFFERENTI; KANATANI K, 1991, IEICE TRANS COMMUN, V74, P3369; KANATANI K, 1985, COMPUT VISION GRAPH, V29, P1, DOI 10.1016/S0734-189X(85)90146-X; KANATANI K, 1985, COMPUT VISION GRAPH, V29, P13, DOI 10.1016/S0734-189X(85)90147-1; KANATANI K, 1991, CVGIP-IMAG UNDERSTAN, V54, P333, DOI 10.1016/1049-9660(91)90034-M; KANATANI K, 1993, IN PRESS IEEE T PATT, V15; Kanatani K., 1993, GEOMETRIC COMPUTATIO; KANATANI K, IN PRESS COMPUT VIS; KANATANI K, 1992, IN PRESS IEEE T ROBO, V8; Kanatani Kenichi, 1990, GROUP THEORETICAL ME, P4; LONGUETHIGGINS HC, 1986, PROC R SOC SER B-BIO, V227, P399, DOI 10.1098/rspb.1986.0030; MA SD, 1992, 11TH P INT C PATT RE, V1, P1; PORRILL J, 1990, IMAGE VISION COMPUT, V8, P37, DOI 10.1016/0262-8856(90)90054-9; Press WH, 1988, NUMERICAL RECIPES C; ROTHWELL CA, 1992, IMAGE VISION COMPUT, V10, P250, DOI 10.1016/0262-8856(92)90056-9; SAFAEERAD R, 1991, CVGIP-IMAG UNDERSTAN, V54, P259, DOI 10.1016/1049-9660(91)90067-Y; SAFAEERAD R, 1992, IEEE T ROBOTIC AUTOM, V8, P624, DOI 10.1109/70.163786; SAFAEERED R, 1992, IMAGE VISION COMPUT, V19, P532; SAMPSON PD, 1982, COMPUT VISION GRAPH, V18, P97, DOI 10.1016/0146-664X(82)90101-0; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; TSAI RY, 1982, IEEE T ACOUST SPEECH, V30, P525, DOI 10.1109/TASSP.1982.1163931; TSAI RY, 1981, IEEE T ACOUST SPEECH, V29, P1147, DOI 10.1109/TASSP.1981.1163710; TSAI RY, 1984, IEEE T ACOUST SPEECH, V32, P213, DOI 10.1109/TASSP.1984.1164313; WAXMAN AM, 1985, INT J ROBOT RES, V4, P95, DOI 10.1177/027836498500400307; WOHN K, 1990, COMPUT VISION GRAPH, V49, P127, DOI 10.1016/0734-189X(90)90134-H; WOHN KY, 1991, IEEE T PATTERN ANAL, V13, P746, DOI 10.1109/34.85666; Yoshida T., 1989, Transactions of the Information Processing Society of Japan, V30, P799	33	5	5	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1993	10	1					67	84		10.1007/BF01440848	http://dx.doi.org/10.1007/BF01440848			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LP384					2022-12-18	WOS:A1993LP38400005
J	LIU, SC; HARRIS, J				LIU, SC; HARRIS, J			DYNAMIC WIRES - AN ANALOG VLSI MODEL FOR OBJECT-BASED PROCESSING	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CURVES	The dynamic-wire methodology provides dedicated lines of communication among groups of pixels of an image which share common properties. In simple applications, object regions can be grouped together to compute the area or the center of mass of each object. Alternatively, object boundaries may be used to compute curvature or contour length. These measurements are useful for higher-level tasks such as object recognition or structural saliency. The dynamic-wire methodology is efficiently implemented in fast, low-power analog hardware. Switches create a true electrical connection among selected pixels, dynamically configuring wires or resistive networks on the fly. Dynamic wires provide a model for object-based processing. This approach is different from present early vision chips which are limited to pixel-based or image-based operations. Using this methodology, we have successfully designed and demonstrated a custom analog VLSI chip which computes contour length.	ROCKWELL SCI CTR,THOUSAND OAKS,CA 91360; MIT,ARTIFICIAL INTELLIGENCE LAB,CAMBRIDGE,MA 02139	Rockwell Scientific Company; Massachusetts Institute of Technology (MIT)								ASADA H, 1986, IEEE T PATTERN ANAL, V8, P2, DOI 10.1109/TPAMI.1986.4767747; BAIR W, 1991, NEURAL INFORMATION P; BRUSS AR, 1983, COMPUT VISION GRAPH, V21, P3, DOI 10.1016/S0734-189X(83)80026-7; DEWEERTH SP, 1988, 1988 MIT C VER LARG, P259; HAKKARAINEN JM, 1991, APR SPIE TECHN S OPT; HARRIS J, 1989, ANALOG VLSI IMPLEMEN; HARRIS JG, 1990, SCIENCE, V248, P1209, DOI 10.1126/science.2349479; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1986, IEEE T PATTERN ANAL, V8, P665, DOI 10.1109/TPAMI.1986.4767839; KNIGHT T, 1983, THESIS MIT CAMBRIDGE; KOBAYASHI H, 1991, IEEE J SOLID-ST CIRC, V26, P738, DOI 10.1109/4.78244; Lazzaro J., 1988, ADV NEURAL INFORMATI, V1, P703; LOWE D, 1988, P C COMPUT VIS PATT; LUO J, 1989, EXPT SUBTHRESHOLD AN; MAHOWALD M, 1988, P INTELL NEURAL NETW; MCQUIRK IS, 1991, THESIS MIT CAMBR; Mead, 1989, ANALOG VLSI NEURAL S; MEAD C, 1991, ANALOG VLSI SIGNAL P; MEAD CA, 1988, NEURAL NETWORKS, V1, P91, DOI 10.1016/0893-6080(88)90024-X; MOKHTARIAN F, 1986, IEEE T PATTERN ANAL, V8, P34, DOI 10.1109/TPAMI.1986.4767750; SAGE JP, 1986, Q TECH REPT; SHAASHUA A, 1988, P COMPUT VIS PATT RE; SHU DB, 1991, RECONFIGURABLE MASSI, P64; STANDLEY D, 1991, THESIS MIT CAMBR; TANNER JE, 1986, VLSI SIGNAL PROCESSI, V2, P59; YANG W, 1990, P INT S VLSI TECH, P266	26	5	5	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1992	8	3					231	239		10.1007/BF00055154	http://dx.doi.org/10.1007/BF00055154			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JU903					2022-12-18	WOS:A1992JU90300006
J	FLECK, MM				FLECK, MM			A TOPOLOGICAL STEREO MATCHER	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Review							DISPARITY GRADIENT LIMIT; VERGENCE EYE-MOVEMENTS; EDGE-DETECTION; STRUCTURAL-ANALYSIS; INTENSITY CHANGES; NATURAL TEXTURES; VISION; ALGORITHM; MOTION; COMPUTATION	Presented here is a new stereo algorithm that produces dense, high-quality, subpixel disparity maps. It offers two improvements over previous algorithms. First, it does not blur disparity values across sharp changes in depth. Second, it can reconstruct the correct correspondence between two images even when there is substantial vertical displacement between them: this algorithm has been tested with rotations up to 10 degrees and vertical translations up to 16 pixels. Although such image pairs require extra processing time, this ability is vital when exact calibration cannot be maintained. The new algorithm depends on two new ideas. First, it exploits the fact that the correct vertical disparity field is due to camera misalignment and, thus, has only a few (significant) degrees of freedom. The algorithm passes camera alignment parameters, not raw disparity fields, between scales. Disparities at individual locations can diverge only slightly from this global model, greatly reducing the algorithm's search space. Second, the new algorithm uses a pre-match filter that prevents two patches of image from matching if they do not have the same (local) topological structure. This constraint subsumes previous "figural continuity" proposals and can be checked by simple, local operations. The filter seems to improve the algorithm's ability to select the correct match from many alternatives and it suppresses intermediate values near sharp changes in disparity. This technique can be extended to other matching tasks, such as motion tracking, analyzing texture periodicity, and evaluating the performance of edge finders.	DEPT ENGN SCI, OXFORD OX1 3PJ, ENGLAND									AYACHE N, 1987, INT J COMPUT VISION, V1, P107, DOI 10.1007/BF00123161; Bajcsy R.K., 1973, COMPUTER GRAPHICS IM, V2, P118; BAKER HH, 1981, 7TH P INT JOINT C AR, P631; BARNARD ST, 1980, IEEE T PATTERN ANAL, V2, P333, DOI 10.1109/TPAMI.1980.4767032; BARNARD ST, 1989, INT J COMPUT VISION, V3, P17, DOI 10.1007/BF00054836; BARNARD ST, 1982, ACM COMPUT SURV, V14, P553, DOI DOI 10.1145/356893.356896; Boie R. A., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P100; Boie R. A., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P450; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; Bovik A. C., 1987, Proceedings of the IEEE Computer Society Workshop on Computer Vision (Cat. No.87TH0210-5), P201; BOYER KL, 1988, IEEE T PATTERN ANAL, V10, P144, DOI 10.1109/34.3880; BULTHOFF HH, 1988, J OPT SOC AM A, V5, P1749, DOI 10.1364/JOSAA.5.001749; BURT P, 1980, SCIENCE, V208, P615, DOI 10.1126/science.7367885; BURT P, 1980, PERCEPTION, V9, P671, DOI 10.1068/p090671; Buxton B. F., 1984, Image and Vision Computing, V2, P59, DOI 10.1016/0262-8856(84)90001-5; Callahan J., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P240; CALLAHAN J, 1974, AM MATH MON, V81, P211, DOI 10.2307/2319521; CALLAHAN J, 1977, AM MATH MON, V84, P765, DOI 10.2307/2322062; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CANNY JF, 1983, MIT720 ART INT LAB T; CAVANAGH P, 1987, COMPUT VISION GRAPH, V37, P171, DOI 10.1016/S0734-189X(87)80001-4; CHEN L, 1985, PERCEPTION, V14, P197, DOI 10.1068/p140197; CLARK JJ, 1989, IEEE T PATTERN ANAL, V11, P43, DOI 10.1109/34.23112; CLARK JJ, 1988, IEEE T PATTERN ANAL, V10, P720, DOI 10.1109/34.6782; DAY T, 1989, IMAGE VISION COMPUT, V7, P95, DOI 10.1016/0262-8856(89)90002-4; DRUMHELLER M, 1986, IEEE J ROBOTIC AUTOM, P1439; DUWAER AL, 1981, VISION RES, V21, P1727, DOI 10.1016/0042-6989(81)90205-4; FLECK MM, 1988, IMAGE VISION COMPUT, V6, P75, DOI 10.1016/0262-8856(88)90002-9; FLECK MM, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P58; FLECK MM, 1988, MIT1065 ART INT LAB; FLECK MM, 1990, OUEL186090 OXF U DEP; FLECK MM, 1989, P ALVEY VISION C, P127; Fleet D. J., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P379, DOI 10.1109/CVPR.1989.37875; FRAM JR, 1975, IEEE T COMPUT, VC 24, P616, DOI 10.1109/T-C.1975.224274; Gennert M. A., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P139, DOI 10.1109/CCV.1988.589984; GENNERT MA, 1987, THESIS MIT CAMBRIDGE; Gennery D. B., 1977, P 5 INT JOINT C ART, V2, P576; GILLETT WE, 1988, THESIS MIT CAMBRIDGE; Gr?nbaum B., 1987, TILINGS PATTERNS; GRIMSON WEL, 1985, IEEE T PATTERN ANAL, V7, P17, DOI 10.1109/TPAMI.1985.4767615; GRIMSON WEL, 1985, COMPUT VISION GRAPH, V30, P316, DOI 10.1016/0734-189X(85)90163-X; GRIMSON WEL, 1981, PHILOS T ROY SOC B, V292, P217, DOI 10.1098/rstb.1981.0031; GRIMSON WEL, 1981, IMAGES SURFACES; HANNAH MJ, 1980, P IM UND WORKSH COLL, P201; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; HEEGER DJ, 1987, INT J COMPUT VISION, V1, P279, DOI 10.1007/BF00133568; Hildreth E., 1984, MEASUREMENT VISUAL M; HILDRETH EC, 1983, COMPUT VISION GRAPH, V22, P1, DOI 10.1016/0734-189X(83)90093-2; Hoaglin D.C., 1983, UNDERSTANDING ROBUST; HOFF W, 1989, IEEE T PATTERN ANAL, V11, P121, DOI 10.1109/34.16709; HUERTAS A, 1986, IEEE T PATTERN ANAL, V8, P651, DOI 10.1109/TPAMI.1986.4767838; Jepson A. D., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P398, DOI 10.1109/CVPR.1989.37877; KASS M, 1987, COMPUT VISION GRAPH, V37, P362, DOI 10.1016/0734-189X(87)90043-0; KASS M, 1987, INT J COMPUT VISION, V1, P347; KASS M, 1983, P DARPA IMAGE UNDERS, P54; KASS M, 1985, 9TH P INT JOINT C AR, P944; KOENDERINK JJ, 1976, BIOL CYBERN, V24, P51, DOI 10.1007/BF00365595; KROL JD, 1980, PERCEPTION, V9, P651, DOI 10.1068/p090651; KROTKOV EP, 1986, COMPUT VISION GRAPH, V33, P99, DOI 10.1016/0734-189X(86)90222-7; LAWTON DT, 1983, COMPUT VISION GRAPH, V22, P116, DOI 10.1016/0734-189X(83)90098-1; LITTLE J, 1987, P IMAGE UNDERSTANDIN, P915; LITTLE J, 1990, 1ST ECCV ANT, P336; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1978, BIOL CYBERN, V28, P223, DOI 10.1007/BF00344269; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; MATSUYAMA T, 1983, COMPUT VISION GRAPH, V24, P347, DOI 10.1016/0734-189X(83)90060-9; MAYHEW JEW, 1981, ARTIF INTELL, V17, P349, DOI 10.1016/0004-3702(81)90029-1; MAYHEW JEW, 1980, PERCEPTION, V9, P69, DOI 10.1068/p090069; MEDIONI G, 1985, COMPUT VISION GRAPH, V31, P2, DOI 10.1016/S0734-189X(85)80073-6; MOHAN R, 1989, IEEE T PATTERN ANAL, V11, P113, DOI 10.1109/34.16708; MORAVEC HP, 1981, 7TH P INT JOINT C AR, P785; Morevec H.P., 1977, INT JOINT C ART INT, V2, P584; Mori K., 1973, COMPUT GRAPHICS IMAG, V2, P393; MOWFORTH P, 1981, PERCEPTION, V10, P299, DOI 10.1068/p100299; NALWA VS, 1987, IEEE T PATTERN ANAL, V9, P446, DOI 10.1109/TPAMI.1987.4767926; NALWA VS, 1986, IEEE T PATTERN ANAL, V8, P699, DOI 10.1109/TPAMI.1986.4767852; NEVATIA R, 1976, COMPUT GRAPH IMAGE P, V5, P203; NIELSEN KRK, 1983, MIT743 ART INT LAB M; NISHIHARA HK, 1984, OPT ENG, V23, P536, DOI 10.1117/12.7973334; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; OTTO GP, 1989, IMAGE VISION COMPUT, V7, P83, DOI 10.1016/0262-8856(89)90001-2; PEARSON DE, 1985, P IEEE, V73, P795, DOI 10.1109/PROC.1985.13202; POGGIO GF, 1984, ANNU REV NEUROSCI, V7, P379, DOI 10.1146/annurev.ne.07.030184.002115; POLLARD SB, 1985, PERCEPTION, V14, P449, DOI 10.1068/p140449; Pratt W. K., 1978, DIGITAL IMAGE PROCES; PRAZDNY K, 1985, BIOL CYBERN, V52, P93, DOI 10.1007/BF00363999; QUAM L, 1984, P IMAGE UNDERSTANDIN, P149; ROSENFELD A, 1979, AM MATH MON, V86, P621, DOI 10.2307/2321290; Rourke CP., 1982, INTRO PIECEWISE LINE; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; SCHUNCK BG, 1989, IEEE T PATTERN ANAL, V11, P1010, DOI 10.1109/34.42834; SCOTT GL, 1988, LOCAL GLOBAL INTERPR; Serra J, 1982, IMAGE ANAL MATH MORP; Shahraray B., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P641, DOI 10.1109/CCV.1988.590045; Sher D., 1987, Proceedings of the IEEE Computer Society Workshop on Computer Vision (Cat. No.87TH0210-5), P35; SHER DB, 1987, 232 U ROCH DEP COMP; SPACEK LA, 1986, IMAGE VISION COMPUT, V4, P43, DOI 10.1016/0262-8856(86)90007-7; Stewart C. V., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P134, DOI 10.1109/CCV.1988.589983; THOMAS GA, 1987, BBC RD198711 BBC RES; TRIVEDI HP, 1985, PERCEPTION, V14, P685, DOI 10.1068/p140685; VILNROTTER FM, 1986, IEEE T PATTERN ANAL, V8, P76, DOI 10.1109/TPAMI.1986.4767754; VOORHEES H, 1987, 1ST P INT C COMP VIS, P250; WATT RJ, 1983, VISION RES, V23, P97, DOI 10.1016/0042-6989(83)90046-9; WILLIAMS DR, 1988, VISION RES, V28, P433, DOI 10.1016/0042-6989(88)90185-X; WITKIN A, 1987, INT J COMPUT VISION, V1, P133, DOI 10.1007/BF00123162; Yagi, 1973, COMPUT VISION GRAPH, V2, P131; YESHURUN Y, 1989, IEEE T PATTERN ANAL, V11, P750; YOUNG RA, 1986, P SOC PHOTO-OPT INS, V728, P2; ZUCKER SW, 1985, COMPUT VISION GRAPH, V32, P74, DOI 10.1016/0734-189X(85)90003-9	110	5	7	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1991	6	3					197	226		10.1007/BF00115696	http://dx.doi.org/10.1007/BF00115696			30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GG465					2022-12-18	WOS:A1991GG46500002
J	YUILLE, A; GEIGER, D				YUILLE, A; GEIGER, D			STEREO AND CONTROLLED MOVEMENT	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									HARVARD UNIV,DIV APPL SCI,CAMBRIDGE,MA 02138; MIT,ARTIFICIAL INTELLIGENCE LAB,CAMBRIDGE,MA 02139	Harvard University; Massachusetts Institute of Technology (MIT)				Yuille, Alan L./0000-0001-5207-9249				BARNARD T, 1982, COMPUT SURV, P14; CANNY JF, 1983, TM720 MIT ART INT LA; CORNOG K, 1985, THESIS MIT; GEIGER D, 1989, BIOL CYBERNETICS; GRZYWACZ NM, 1986, AI888 MIT ART INT LA; Hildreth E., 1984, MEASUREMENT VISUAL M; KROL JD, 1982, PERCEPTION, V11, P615, DOI 10.1068/p110615; LONGUETHIGGINS HC, 1982, PERCEPTION, V11, P377, DOI 10.1068/p110377; MARR D, 1981, PROC R SOC SER B-BIO, V211, P151, DOI 10.1098/rspb.1981.0001; MILENKOVIC VJ, 1986, P IMAGE UNDERSTANDIN, P163; PRAZDNY K, 1985, BIOL CYBERN, V52, P93, DOI 10.1007/BF00363999; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; Waxman A. M., 1986, Proceedings of the Workshop on Motion: Representation and Analysis (Cat. No.86CH2322-6), P31	13	5	5	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1990	4	2					141	152		10.1007/BF00127814	http://dx.doi.org/10.1007/BF00127814			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CV661					2022-12-18	WOS:A1990CV66100003
J	KASS, M				KASS, M			LINEAR IMAGE FEATURES IN STEREOPSIS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									SCHLUMBERGER PALO ALTO RES, PALO ALTO, CA 94304 USA	Schlumberger								Arnold R. D., 1980, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V238, P281; BAKER HH, 1981, 7TH P INT JOINT C AR, P631; BARNARD ST, 1980, IEEE T PATTERN ANAL, V2, P333, DOI 10.1109/TPAMI.1980.4767032; BURT P, 1980, SCIENCE, V208, P615, DOI 10.1126/science.7367885; BURT P, 1980, PERCEPTION, V9, P671, DOI 10.1068/p090671; BURT P, 1982, PERCEPTION, V11, P621, DOI 10.1068/p110621; GENNERY DB, 1977, P INT JOINT C ART IN, P576; GRIMSON WEL, 1985, IEEE T PATTERN ANAL, V7, P17, DOI 10.1109/TPAMI.1985.4767615; GRIMSON WEL, 1981, PHILOS T ROY SOC B, V292, P217, DOI 10.1098/rstb.1981.0031; GRIMSON WEL, 1981, IMAGES SURFACES COMP; HANNAH MJ, 1974, AIM239 STANF ART INT; KASS M, 1983, 8TH P INT JOINT C AR, P1043; KASS M, 1983, P DARPA IMAGE UNDERS, P54; KASS MH, 1984, THESIS MIT; KOENDERINK JJ, 1976, BIOL CYBERN, V21, P29, DOI 10.1007/BF00326670; KROL JD, 1982, PERCEPTION, V11, P621; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Marsden J. E., 1974, ELEMENTARY CLASSICAL; MAYHEW JEW, 1981, ARTIF INTELL, V17, P349, DOI 10.1016/0004-3702(81)90029-1; MEDIONI GG, 1983, JUN P DARPA IM UND W, P128; Morevec H.P., 1977, INT JOINT C ART INT, V2, P584; Mori K., 1973, COMPUT GRAPHICS IMAG, V2, P393; NISHIHARA HK, 1983, 3RD P INT C ROB VIS, P449; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; POLLARD SB, 1985, PERCEPTION, V14, P449, DOI 10.1068/p140449; QUAM LH, 1984, P IMAGE UNDERSTANDIN; TERZOPOULOS D, 1983, COMPUTER VISION GRAP, V22, P39; TORRE V, 1986, IEEE T PAMI, V8; TYLER CW, 1973, SCIENCE, V181, P276, DOI 10.1126/science.181.4096.276; TYLER CW, 1977, P SPIE, V120; WITKIN A, 1987, INT J COMPUTER VISIO, V2	32	5	5	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		1987	1	4					357	368						12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	M2053					2022-12-18	WOS:A1987M205300005
J	Jin, HB; Liao, SC; Shao, L				Jin, Haibo; Liao, Shengcai; Shao, Ling			Pixel-in-Pixel Net: Towards Efficient Facial Landmark Detection in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Facial landmark detection; Pixel-in-pixel regression; Self-training with curriculum; Unsupervised domain adaptation	REPRESENTATION; NETWORK	Recently, heatmap regression models have become popular due to their superior performance in locating facial landmarks. However, three major problems still exist among these models: (1) they are computationally expensive; (2) they usually lack explicit constraints on global shapes; (3) domain gaps are commonly present. To address these problems, we propose Pixel-in-Pixel Net (PIPNet) for facial landmark detection. The proposed model is equipped with a novel detection head based on heatmap regression, which conducts score and offset predictions simultaneously on low-resolution feature maps. By doing so, repeated upsampling layers are no longer necessary, enabling the inference time to be largely reduced without sacrificing model accuracy. Besides, a simple but effective neighbor regression module is proposed to enforce local constraints by fusing predictions from neighboring landmarks, which enhances the robustness of the new detection head. To further improve the cross-domain generalization capability of PIPNet, we propose self-training with curriculum. This training strategy is able to mine more reliable pseudo-labels from unlabeled data across domains by starting with an easier task, then gradually increasing the difficulty to provide more precise labels. Extensive experiments demonstrate the superiority of PIPNet, which obtains new state-of-the-art results on three out of six popular benchmarks under the supervised setting. The results on two cross-domain test sets are also consistently improved compared to the baselines. Notably, our lightweight version of PIPNet runs at 35.7 FPS and 200 FPS on CPU and GPU, respectively, while still maintaining a competitive accuracy to state-of-the-art methods. The code of PIPNet is available at https://github.com/jhb86253817/PIPNet.	[Jin, Haibo; Liao, Shengcai; Shao, Ling] Incept Inst Artificial Intelligence IIAI, Abu Dhabi, U Arab Emirates; [Shao, Ling] Mohamed Bin Zayed Univ Artificial Intelligence MB, Abu Dhabi, U Arab Emirates	Mohamed Bin Zayed University of Artificial Intelligence	Liao, SC (corresponding author), Incept Inst Artificial Intelligence IIAI, Abu Dhabi, U Arab Emirates.	haibo.jin@inceptioniai.org; scliao@ieee.org; ling.shao@ieee.org		Jin, Haibo/0000-0001-7512-3599				[Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.596; Bansal A., 2016, ARXIV161101484; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Chandran P, 2020, PROC CVPR IEEE, P5860, DOI 10.1109/CVPR42600.2020.00590; Chen LS, 2019, IEEE I CONF COMP VIS, P6991, DOI 10.1109/ICCV.2019.00709; Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352; Dapogny A, 2019, IEEE I CONF COMP VIS, P6892, DOI 10.1109/ICCV.2019.00699; Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525; Deng JK, 2019, INT J COMPUT VISION, V127, P599, DOI 10.1007/s11263-018-1134-y; Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110; Dong XY, 2019, IEEE I CONF COMP VIS, P783, DOI 10.1109/ICCV.2019.00087; Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047; Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31; Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238; Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Yaroslav, 2015, ICML; Ghiasi G, 2014, PROC CVPR IEEE, P1899, DOI 10.1109/CVPR.2014.306; He ZL, 2017, IEEE COMPUT SOC CONF, P2044, DOI 10.1109/CVPRW.2017.255; Honari S, 2016, PROC CVPR IEEE, P5743, DOI 10.1109/CVPR.2016.619; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Islam Md Amirul, 2020, ARXIV200108248, P14; Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503; Khan MH, 2017, IEEE I CONF COMP VIS, P3811, DOI 10.1109/ICCV.2017.409; Kingma D.P, P 3 INT C LEARNING R; Kostinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Kumar Abhinav, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8233, DOI 10.1109/CVPR42600.2020.00826; Liu H, 2018, IEEE T PATTERN ANAL, V40, P2546, DOI 10.1109/TPAMI.2017.2734779; Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212; Liu ZW, 2019, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2019.00358; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Long MS, 2015, PR MACH LEARN RES, V37, P97; Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393; Merget D, 2018, PROC CVPR IEEE, P781, DOI 10.1109/CVPR.2018.00088; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17; Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395; Pei JF, 2013, IEEE INT C BIOINFORM; Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146; Qian SJ, 2019, IEEE I CONF COMP VIS, P10152, DOI 10.1109/ICCV.2019.01025; Ren SQ, 2016, IEEE T IMAGE PROCESS, V25, P1233, DOI 10.1109/TIP.2016.2518867; Robinson JP, 2019, IEEE I CONF COMP VIS, P10102, DOI 10.1109/ICCV.2019.01020; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shen J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1003, DOI 10.1109/ICCVW.2015.132; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Tai Y, 2019, AAAI CONF ARTIF INTE, P8893; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21; Thies Justus, 2016, CVPR, DOI DOI 10.1109/CVPR.2016.262; Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453; Valle R, 2018, LECT NOTES COMPUT SC, V11218, P609, DOI 10.1007/978-3-030-01264-9_36; Valle R, 2019, COMPUT VIS IMAGE UND, V189, DOI 10.1016/j.cviu.2019.102846; Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265; Wang XY, 2019, IEEE I CONF COMP VIS, P6970, DOI 10.1109/ICCV.2019.00707; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wu WY, 2017, IEEE COMPUT SOC CONF, P2096, DOI 10.1109/CVPRW.2017.261; Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253; Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225; Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113; Zafeiriou S, 2017, IEEE COMPUT SOC CONF, P2116, DOI 10.1109/CVPRW.2017.263; Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286; Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11; Zhu ML, 2019, PROC CVPR IEEE, P3481, DOI 10.1109/CVPR.2019.00360; Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953; Zou X, 2019, IEEE I CONF COMP VIS, P141, DOI 10.1109/ICCV.2019.00023	75	4	4	5	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3174	3194		10.1007/s11263-021-01521-4	http://dx.doi.org/10.1007/s11263-021-01521-4		SEP 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN		Green Submitted			2022-12-18	WOS:000696435800001
J	Liu, K; Liu, D; Li, L; Yan, N; Li, HQ				Liu, Kang; Liu, Dong; Li, Li; Yan, Ning; Li, Houqiang			Semantics-to-Signal Scalable Image Compression with Learned Revertible Representations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Image compression; Lifting structure; Machine vision; Scalable coding	DECOMPOSITION SCHEMES; LIFTING SCHEME; MODEL	Image/video compression and communication need to serve both human vision and machine vision. To address this need, we propose a scalable image compression solution. We assume that machine vision needs less information that is related to semantics, whereas human vision needs more information that is to reconstruct signal. We then propose semantics-to-signal scalable compression, where partial bitstream is decodeable for machine vision and the entire bitstream is decodeable for human vision. Our method is inspired by the scalable image coding standard, JPEG2000, and similarly adopts subband-wise representations. We first design a trainable and revertible transform based on the lifting structure, which converts an image into a pyramid of multiple subbands; the transform is trained to make the partial representations useful for multiple machine vision tasks. We then design an end-to-end optimized encoding/decoding network for compressing the multiple subbands, to jointly optimize compression ratio, semantic analysis accuracy, and signal reconstruction quality. We experiment with two datasets: CUB200-2011 and FGVC-Aircraft, taking coarse-to-fine image classification tasks as an example. Experimental results demonstrate that our proposed method achieves semantics-to-signal scalable compression, and outperforms JPEG2000 in compression efficiency. The proposed method sheds light on a generic approach for image/video coding for human and machines.	[Liu, Kang; Liu, Dong; Li, Li; Yan, Ning; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS	Liu, D; Li, HQ (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.	kangliu@mail.ustc.edu.cn; dongeliu@ustc.edu.cn; lil1@ustc.edu.cn; nyan@mail.ustc.edu.cn; lihq@ustc.edu.cn	liu, dong/GRJ-9115-2022	Liu, Dong/0000-0001-9100-2906	National Key Research and Development Program of China [2018YFA0701603]; Natural Science Foundation of China [61772483]; Fundamental Research Funds for the Central Universities [WK3490000005]; GPU cluster built by MCC Lab of the School of Information Science and Technology of USTC	National Key Research and Development Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); GPU cluster built by MCC Lab of the School of Information Science and Technology of USTC	This work was supported by the National Key Research and Development Program of China under Grant 2018YFA0701603, by the Natural Science Foundation of China under Grant 61772483, and by the Fundamental Research Funds for the Central Universities under Contract WK3490000005. We acknowledge the support of the GPU cluster built by MCC Lab of the School of Information Science and Technology of USTC.	AKANSU AN, 1991, OPT ENG, V30, P912, DOI 10.1117/12.55886; Akansu AN., 2001, MULTIRESOLUTION SIGN; [Anonymous], 2011, TECH REP CNS T 2011; Balle Johannes, 2016, ARXIV161101704; Balle Johannes, 2018, INT C LEARN REPR ICL; Rodriguez MXB, 2020, IEEE WINT CONF APPL, P3100, DOI 10.1109/WACV45572.2020.9093580; Baxter J, 1997, MACH LEARN, V28, P7, DOI 10.1023/A:1007327622663; Chen T., 2019, ARXIV191006244; Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468; Dejean-Servieres M., 2017, STUDY IMPACT STANDAR; Dodge S, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX); Duan LY, 2020, IEEE T IMAGE PROCESS, V29, P8680, DOI 10.1109/TIP.2020.3016485; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Goutsias J, 2000, IEEE T IMAGE PROCESS, V9, P1862, DOI 10.1109/83.877209; He C, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11222648; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heijmans HJAM, 2000, IEEE T IMAGE PROCESS, V9, P1897, DOI 10.1109/83.877211; Hu Y, 2020, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM42002.2020.9322414; Huang GL, 2017, IEEE ICC; Jacobsen J.H., 2018, ICLR; Johnston P., 2018, IJCNN, P1; Kwanicka H., 2018, BRIDGING SEMANTIC GA, DOI [10.1007/978-3-319-73891-8, DOI 10.1007/978-3-319-73891-8]; Latif A, 2019, MATH PROBL ENG, V2019, DOI 10.1155/2019/9658350; Lee Jooyoung, 2018, ARXIV180910452; Li M., 2020, ARXIV200504661; Lo SCB, 2003, IEEE T MED IMAGING, V22, P1141, DOI 10.1109/TMI.2003.816953; Ma HC, 2022, IEEE T PATTERN ANAL, V44, P1247, DOI 10.1109/TPAMI.2020.3026003; Ma HC, 2020, IEEE T MULTIMEDIA, V22, P1667, DOI 10.1109/TMM.2019.2957990; Ma SW, 2019, IEEE T CIRC SYST VID, V29, P3095, DOI 10.1109/TCSVT.2018.2873102; Maji S., 2013, TECH REP; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173; Minnen D, 2018, ADV NEUR IN, V31; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Poyser M., 2020, ARXIV200714314; Ruder Sebastian, 2017, ARXIV170605098; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Simonyan K., 2015, ICLR; Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051; Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830; Tishby N., 1999, P 37 ANN AL C COMM C; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Toderici G., 2015, INT C LEARN REPR; Torfason R., 2018, ARXIV180306131; Wang SR, 2019, IEEE IMAGE PROC, P2691, DOI 10.1109/ICIP.2019.8803255; Xia SF, 2020, IEEE INT CON MULTI; Yan N, 2020, IEEE IMAGE PROC, P3114, DOI 10.1109/ICIP40778.2020.9191184; Zhang X, 2017, IEEE T IMAGE PROCESS, V26, P633, DOI 10.1109/TIP.2016.2629447; Zhao JJ, 2020, NEUROCOMPUTING, V395, P150, DOI 10.1016/j.neucom.2018.02.109; Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865; Zhou LT, 2019, INT WORK CONTENT MUL	52	4	4	4	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2605	2621		10.1007/s11263-021-01491-7	http://dx.doi.org/10.1007/s11263-021-01491-7		JUN 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN					2022-12-18	WOS:000664019500002
J	Georgopoulos, M; Oldfield, J; Nicolaou, MA; Panagakis, Y; Pantic, M				Georgopoulos, Markos; Oldfield, James; Nicolaou, Mihalis A.; Panagakis, Yannis; Pantic, Maja			Mitigating Demographic Bias in Facial Datasets with Style-Based Multi-attribute Transfer	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Data augmentation; Style transfer; Dataset bias; Demographic bias; Algorithmic fairness; Diversity; Age progression		Deep learning has catalysed progress in tasks such as face recognition and analysis, leading to a quick integration of technological solutions in multiple layers of our society. While such systems have proven to be accurate by standard evaluation metrics and benchmarks, a surge of work has recently exposed the demographic bias that such algorithms exhibit-highlighting that accuracy does not entail fairness. Clearly, deploying biased systems under real-world settings can have grave consequences for affected populations. Indeed, learning methods are prone to inheriting, or even amplifying the bias present in a training set, manifested by uneven representation across demographic groups. In facial datasets, this particularly relates to attributes such as skin tone, gender, and age. In this work, we address the problem of mitigating bias in facial datasets by data augmentation. We propose a multi-attribute framework that can successfully transfer complex, multi-scale facial patterns even if these belong to underrepresented groups in the training set. This is achieved by relaxing the rigid dependence on a single attribute label, and further introducing a tensor-based mixing structure that captures multiplicative interactions between attributes in a multilinear fashion. We evaluate our method with an extensive set of qualitative and quantitative experiments on several datasets, with rigorous comparisons to state-of-the-art methods. We find that the proposed framework can successfully mitigate dataset bias, as evinced by extensive evaluations on established diversity metrics, while significantly improving fairness metrics such as equality of opportunity.	[Georgopoulos, Markos; Pantic, Maja] Imperial Coll London, Dept Comp, London, England; [Oldfield, James; Nicolaou, Mihalis A.] Cyprus Inst, Computat Based Sci & Technol Res Ctr, Nicosia, Cyprus; [Panagakis, Yannis] Univ Athens, Dept Informat & Telecommun, Athens, Greece	Imperial College London; National & Kapodistrian University of Athens	Georgopoulos, M (corresponding author), Imperial Coll London, Dept Comp, London, England.	m.georgopoulos@imperial.ac.uk; j.oldfield@cyi.ac.cy; m.nicolaou@cyi.ac.cy; yannisp@di.uoa.gr; m.pantic@imperial.ac.uk		Panagakis, Ioannis/0000-0003-0153-5210				Alvi M, 2019, LECT NOTES COMPUT SC, V11129, P556, DOI 10.1007/978-3-030-11009-3_34; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arora S., 2017, ARXIV170608224; BOTHWELL RK, 1989, PERS SOC PSYCHOL B, V15, P19, DOI 10.1177/0146167289151002; Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Dantcheva A, 2016, IEEE T INF FOREN SEC, V11, P441, DOI 10.1109/TIFS.2015.2480381; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Duong CN, 2019, PROC CVPR IEEE, P10005, DOI 10.1109/CVPR.2019.01025; Edwards Harrison, 2016, INT C LEARN REPR ICL, P3; Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Georgopoulos M., 2020 IEEE CVF C COMP, P66; Georgopoulos M, 2020, PR MACH LEARN RES, V119; Georgopoulos M, 2020, IMAGE VISION COMPUT, V102, DOI 10.1016/j.imavis.2020.103954; Georgopoulos M, 2018, IMAGE VISION COMPUT, V80, P58, DOI 10.1016/j.imavis.2018.05.003; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover Aditya, 2019, ARXIV PREPRINT ARXIV; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Harshman R. A., 1970, MULTIMODAL FACTOR AN; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751; He Zhenliang, 2017, ARXIV171110678; Hendricks LA, 2018, LECT NOTES COMPUT SC, V11207, P793, DOI 10.1007/978-3-030-01219-9_47; Holstein K, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300830; Huang G.B., 2008, WORKSH FAC REAL LIF; Huang X., 2018, ARXIV180404732; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Inoue H., 2018, ARXIV PREPRINT ARXIV; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jackson P. T., 2019, P IEEE C COMP VIS PA, P83; Jayakumar S. M., 2020, INT C LEARN REPR; Karras T, 2017, ARXIV171010196; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kim B, 2019, PROC CVPR IEEE, P9004, DOI 10.1109/CVPR.2019.00922; Kim Junho, 2020, ICLR; Kingma D.P, P 3 INT C LEARNING R; Kolda T.G., 2006, OFFICE SCI TECHNICAL, DOI [10.2172/923081, DOI 10.2172/923081]; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kuhlman C, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3593, DOI 10.1145/3394486.3411074; Lanitis A., 2002, FG NET AGING DATABAS; Li M., 2016, DEEP IDENTITY AWARE; Li SM, 2020, J MATER SCI, V55, P6551, DOI 10.1007/s10853-020-04464-2; Lim Jae Hyun, 2017, ABS170502894 ARXIV; Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065; Liu Y., 2019, A3GAN ATTRIBUTE AWAR; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma L., 2018, EXEMPLAR GUIDED UNSU; Madras D, 2018, PR MACH LEARN RES, V80; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Masi I, 2018, SIBGRAPI, P471, DOI 10.1109/SIBGRAPI.2018.00067; Mehrabi N, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3457607; Merler M., 2019, DIVERSITY FACES; Mescheder L, 2018, PR MACH LEARN RES, V80; Nagpal S., 2019, ARXIV190401219; Ng C.B., 2012, P PAC RIM INT C ART, P335; Odena A, 2017, PR MACH LEARN RES, V70; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Perarnau G, 2016, ARXIV161106355; Quadrianto N, 2019, PROC CVPR IEEE, P8219, DOI 10.1109/CVPR.2019.00842; Quadrianto Novi, 2018, DISCOVERING FAIR REP, P3; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Raji ID, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P429, DOI 10.1145/3306618.3314244; Ramanathan N., 2009, J VISUAL LANG COMPUT, V15, P3349; Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341; Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3; Salimans T, 2016, ADV NEUR IN, V29; Sandfort V, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-52737-x; Sattigeri Prasanna, 2018, FAIRNESS GAN; Schaich A, 2016, FRONT AGING NEUROSCI, V8, DOI 10.3389/fnagi.2016.00264; Serna I., 2019, ARXIV191201842; Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0; Tang H., 2019, ATTENTIONGAN UNPAIRE; Verma S, 2018, 2018 IEEE/ACM INTERNATIONAL WORKSHOP ON SOFTWARE FAIRNESS (FAIRWARE 2018), P1, DOI 10.1145/3194770.3194776; Wang J, 2018, ENVIRON TECHNOL, V39, P3055, DOI 10.1080/09593330.2017.1371797; Wang M, 2019, IEEE I CONF COMP VIS, P692, DOI 10.1109/ICCV.2019.00078; Wang W, 2016, PROC CVPR IEEE, P2378, DOI 10.1109/CVPR.2016.261; Wang ZW, 2018, PROC CVPR IEEE, P7939, DOI 10.1109/CVPR.2018.00828; Yang HY, 2018, PROC CVPR IEEE, P31, DOI 10.1109/CVPR.2018.00011; Yang HY, 2016, IEEE T IMAGE PROCESS, V25, P2493, DOI 10.1109/TIP.2016.2547587; Yucer S., 2020, P IEEECVF C COMPUTER, P18; Zhang BH, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P335, DOI 10.1145/3278721.3278779; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31; Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463; Zhao Jieyu, 2017, P 2017 C EMP METH NA, P2941, DOI [10.18653/v1/D17-1323, DOI 10.18653/V1/D17-1323]; Zhao SJ, 2018, ADV NEUR IN, V31; Zheng X, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P107, DOI 10.5220/0007353401070114; ZhenWang Guosheng Hu, 2020, C COMP VIS PATT REC; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	93	4	4	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2288	2307		10.1007/s11263-021-01448-w	http://dx.doi.org/10.1007/s11263-021-01448-w		MAY 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW		hybrid			2022-12-18	WOS:000650834200001
J	Mettes, P; Thong, W; Snoek, CGM				Mettes, Pascal; Thong, William; Snoek, Cees G. M.			Object Priors for Classifying and Localizing Unseen Actions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								This work strives for the classification and localization of human actions in videos, without the need for any labeled video training examples. Where existing work relies on transferring global attribute or object information from seen to unseen action videos, we seek to classify and spatio-temporally localize unseen actions in videos from image-based object information only. We propose three spatial object priors, which encode local person and object detectors along with their spatial relations. On top we introduce three semantic object priors, which extend semantic matching through word embeddings with three simple functions that tackle semantic ambiguity, object discrimination, and object naming. A video embedding combines the spatial and semantic object priors. It enables us to introduce a new video retrieval task that retrieves action tubes in video collections based on user-specified objects, spatial relations, and object size. Experimental evaluation on five action datasets shows the importance of spatial and semantic object priors for unseen actions. We find that persons and objects have preferred spatial relations that benefit unseen action localization, while using multiple languages and simple object filtering directly improves semantic matching, leading to state-of-the-art results for both unseen action classification and localization.	[Mettes, Pascal; Thong, William; Snoek, Cees G. M.] Univ Amsterdam, Amsterdam, Netherlands	University of Amsterdam	Mettes, P (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	P.S.M.Mettes@uva.nl; W.E.Thong@uva.nl; cgmsnoek@uva.nl		Mettes, Pascal/0000-0001-9275-5942				Afouras T., 2020, ECCV; al, 2018, IN; Alexiou I, 2016, IEEE IMAGE PROC, P4190, DOI 10.1109/ICIP.2016.7533149; An RQ, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.2.023007; Asano Y. M., 2020, NEURIPS; Bishay Mina, 2019, BMVC; Bond F., 2013, ANN M ASS COMP LING; Brattoli Biagio, 2020, CVPR; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chakraborty B, 2012, COMPUT VIS IMAGE UND, V116, P396, DOI 10.1016/j.cviu.2011.09.010; Chang Xiaojun, 2016, AAAI; Cheron Guilhem, 2018, NEURIPS; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalton J., 2013, ICIKM; de Boer M, 2016, MULTIMED TOOLS APPL, V75, P9025, DOI 10.1007/s11042-015-2757-4; Escorcia V, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P508, DOI 10.1109/ICCVW.2013.72; Fauw J., 2020, NEURIPS; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Gan C, 2016, AAAI CONF ARTIF INTE, P3487; Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; Grave E., 2018, LREC; Gupta A., 2007, CVPR; Habibian A, 2017, IEEE T PATTERN ANAL, V39, P2089, DOI 10.1109/TPAMI.2016.2627563; Han T., 2020, ECCV; Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620; Inoue N., 2016, MM; Jain M., 2020, CVPR; Jain M, 2017, INT J COMPUT VISION, V124, P287, DOI 10.1007/s11263-017-1023-9; Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Jenni S., 2020, ARXIV PREPRINT ARXIV; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Jing LL, 2022, INT J OCCUP SAF ERGO, V28, P842, DOI 10.1080/10803548.2020.1835234; JOLICOEUR P, 1984, COGNITIVE PSYCHOL, V16, P243, DOI 10.1016/0010-0285(84)90009-4; Junior V. L. E., 2019, CORR; Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472; Klaser A., 2010, ECCV; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Kuehne H, 2014, PROC CVPR IEEE, P780, DOI 10.1109/CVPR.2014.105; Kuipers B., 2011, CVPR; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lan T, 2011, IEEE I CONF COMP VIS, P2003, DOI 10.1109/ICCV.2011.6126472; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Li Y., 2016, ICIP; Li Z., 2019, PATTERN RECOGN, V3, P91; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu J.G., 2008, CVPR; MALT BC, 1995, COGNITIVE PSYCHOL, V29, P85, DOI 10.1006/cogp.1995.1013; Mandal D., 2019, CVPR; Mettes P., 2020, TOMM; Mettes P, 2019, INT J COMPUT VISION, V127, P263, DOI 10.1007/s11263-018-1120-4; Mettes P, 2017, IEEE I CONF COMP VIS, P4453, DOI 10.1109/ICCV.2017.476; Mikolov T., 2013, NEURIPS; Mishra A., 2020, NEUROCOMPUTING, V2, P13; Mishra A, 2018, IEEE WINT CONF APPL, P372, DOI 10.1109/WACV.2018.00047; MOORE D, 1999, ICCV; Murphy G., 2004, BIG BOOK CONCEPTS; Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39; Patrick M., 2020, ARXIV; Pennington J., 2014, EMLNP; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; ROSCH E, 1976, COGNITIVE PSYCHOL, V8, P382, DOI 10.1016/0010-0285(76)90013-X; Rosch E., 1988, READINGS COGNITIVE S, P312; Sener F., 2018, CVPR; Soomro K., 2012, COMPUT SCI; Soomro K, 2017, IEEE I CONF COMP VIS, P696, DOI 10.1109/ICCV.2017.82; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Tian Y., 2018, ICSP; Tschannen M., 2020, CVPR; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang J., 2019, ARXIV190403385; Wang Q., 2017, ECML; Wu JX, 2007, IEEE I CONF COMP VIS, P290, DOI 10.1109/ICCV.2007.4408865; Wu S, 2014, PROC CVPR IEEE, P2665, DOI 10.1109/CVPR.2014.341; Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339; Xu D., 2019, CVPR; Xu X, 2017, INT J COMPUT VISION, V123, P309, DOI 10.1007/s11263-016-0983-5; Xu X, 2016, LECT NOTES COMPUT SC, V9906, P343, DOI 10.1007/978-3-319-46475-6_22; Yao B., 2011, ICML; Zhang L., 2020, CVPR; Zhang Z, 2015, PATTERN ANAL APPL, V18, P157, DOI 10.1007/s10044-013-0349-3; Zhao J., 2019, CVPR; Zhao Yue, 2018, NEURIPS; Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7; Zhu Y, 2018, PROC CVPR IEEE, P9436, DOI 10.1109/CVPR.2018.00983; Zisserman Andrew, 2014, NEURIPS	95	4	4	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1954	1971		10.1007/s11263-021-01454-y	http://dx.doi.org/10.1007/s11263-021-01454-y		APR 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU		hybrid, Green Published, Green Submitted			2022-12-18	WOS:000641238700002
J	Yan, H; Mao, XY; Yang, X; Xia, YQ; Wang, CB; Wang, JJ; Xia, R; Xu, XJ; Wang, ZQ; Li, ZY; Zhao, X; Li, Y; Liu, GY; He, L; Wang, ZY; Wang, ZQ; Li, ZQ; Cai, WD; Shen, H; Chang, H				Yan, Hong; Mao, Xuanyu; Yang, Xu; Xia, Yongquan; Wang, Chengbin; Wang, Junjun; Xia, Rui; Xu, Xuejing; Wang, Zhiqiang; Li, Zhiyang; Zhao, Xie; Li, Yan; Liu, Guoye; He, Li; Wang, Zhongyu; Wang, Zhiqiong; Li, Zhiqiang; Cai, Weidong; Shen, Han; Chang, Hang			Development and Validation of an Unsupervised Feature Learning System for Leukocyte Characterization and Classification: A Multi-Hospital Study	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised feature learning; Leukocyte; Classification; Multi-hospital clinical validation		The characterization and classification of white blood cells (WBC) are critical for the diagnosis of anemia, leukemia, and many other hematologic diseases. We developed WBC-Profiler, an unsupervised feature learning system for quantitative analysis of leukocytes. We demonstrate, through independent validation, that WBC-Profiler enables automatic extraction of complex and robust signatures from microscopic images without human-intervention and, thereafter, effective construction of interpretable leukocyte profiles, which decouples large scale complex leukocyte characterization from limitations in both human-based feature engineering/optimization and the end-to-end solutions provided by many modern deep neural networks. Further evaluation in a real-world clinical setting confirms that, compared with 23 clinicians from 8 hospitals (class-average-sensitivity, 0.798; class-average-specificity, 0.963; cell-average-timecost: 3.158 s), WBC-Profiler performs with significantly improved accuracy and speed (class-average-sensitivity, 0.890; class-average-specificity, 0.980; cell-average-timecost: 0.375 s). Our findings suggest that WBC-Profiler has the potential clinical implications.	[Yan, Hong] Nanjing Med Univ, Affiliated Hosp 2, Lab Med Ctr, Nanjing 210011, Peoples R China; [Yan, Hong; Xia, Rui; Wang, Zhiqiang; Li, Yan; Liu, Guoye] Nanjing Med Univ, Affiliated Brain Hosp, Dept Lab Med, Nanjing 210029, Peoples R China; [Mao, Xuanyu; Chang, Hang] Lawrence Berkeley Natl Lab, Berkeley Biomed Data Sci Ctr, Berkeley, CA 94720 USA; [Mao, Xuanyu; Chang, Hang] Lawrence Berkeley Natl Lab, Biol Syst & Engn Div, Berkeley, CA 94720 USA; [Yang, Xu] Nanjing Med Univ, Sch Publ Hlth, State Key Lab Reprod Med, Nanjing 211166, Peoples R China; [Yang, Xu] Nanjing Med Univ, Sch Publ Hlth, Key Lab Modern Toxicol, Minist Educ, Nanjing 211166, Peoples R China; [Xia, Yongquan; Xu, Xuejing; Li, Zhiyang; Zhao, Xie; Shen, Han] Nanjing Univ, Dept Clin Lab, Med Sch, Affiliated Drum Tower Hosp, Nanjing 210008, Peoples R China; [Wang, Chengbin] 301 Hosp, Dept Lab Med, Beijing 100085, Peoples R China; [Wang, Junjun] Nanjing Univ, Sch Med, Jinling Hosp, Dept Clin Lab, Nanjing 210002, Peoples R China; [He, Li] Wuhan Univ, Zhongnan Hosp, Dept Hematol, Wuhan 430071, Peoples R China; [Wang, Zhongyu] Hubei Univ Med, Clin Sch 1, Shiyan 442000, Peoples R China; [Wang, Zhiqiong] Huazhong Univ Sci & Technol, Tongji Hosp, Dept Hematol, Tongji Med Coll, Wuhan 430030, Peoples R China; [Li, Zhiqiang] Wuhan Univ, Zhongnan Hosp, Dept Neurosurg, Wuhan 430071, Peoples R China; [Cai, Weidong] Univ Sydney, Sch Comp Sci, Sydney, NSW 2006, Australia	Nanjing Medical University; Nanjing Medical University; United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; Nanjing Medical University; Nanjing Medical University; Nanjing University; Nanjing University; Wuhan University; Hubei University of Medicine; Huazhong University of Science & Technology; Wuhan University; University of Sydney	Chang, H (corresponding author), Lawrence Berkeley Natl Lab, Berkeley Biomed Data Sci Ctr, Berkeley, CA 94720 USA.; Chang, H (corresponding author), Lawrence Berkeley Natl Lab, Biol Syst & Engn Div, Berkeley, CA 94720 USA.; Shen, H (corresponding author), Nanjing Univ, Dept Clin Lab, Med Sch, Affiliated Drum Tower Hosp, Nanjing 210008, Peoples R China.	shenhan10366@sina.com; hchang@lbl.gov		Li, Zhiyang/0000-0002-4970-8494	Medical Key Science and Technology Development Projects of Nanjing [ZKX18016]; Medical Science and Technology Development Projects of Nanjing [YKK18167]	Medical Key Science and Technology Development Projects of Nanjing; Medical Science and Technology Development Projects of Nanjing	The authors would like to thank all the participating clinicians and collaborating hospitals involved in this multi-hospital study. This work was supported by the Medical Key Science and Technology Development Projects of Nanjing (ZKX18016), the Medical Science and Technology Development Projects of Nanjing (YKK18167).	Ahn E, 2019, MED IMAGE ANAL, V56, P140, DOI 10.1016/j.media.2019.06.005; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Chang H, 2015, INT J COMPUT VISION, V113, P3, DOI 10.1007/s11263-014-0790-9; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Elsalamony HA, 2016, MEAS SCI TECHNOL, V27, DOI 10.1088/0957-0233/27/8/085401; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Huang Fu Jie, 2006, 2006 IEEE COMPUTER S, V1, P284, DOI 10.1109/CVPR.2006.164; Kavukcuoglu K., 2010, FAST INFERENCE SPARS; Kavukcuoglu K., 2008, CBLLTR20081201 NYU; Kavukcuoglu Koray, 2010, ADV NEURAL INFORM PR, V23, P1090; Kwolek B, 2005, LECT NOTES COMPUT SC, V3696, P551, DOI 10.1007/11550822_86; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee H., 2007, ADV NEURAL INF PROCE, P801; Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434; Lee TS, 1998, VISION RES, V38, P2429, DOI 10.1016/S0042-6989(97)00464-1; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Liu Y, 2019, IEEE ENG MED BIO, P7027, DOI 10.1109/EMBC.2019.8856700; Lv L, 2017, COGN COMPUT, V9, P115, DOI 10.1007/s12559-016-9438-0; Manik S, 2016, PROCEEDINGS OF THE FIRST IEEE INTERNATIONAL CONFERENCE ON POWER ELECTRONICS, INTELLIGENT CONTROL AND ENERGY SYSTEMS (ICPEICES 2016); MCKENNA SJ, 1993, IEE CONF PUBL, V372, P105; Osadchy M., 2005, ADV NEURAL INFORM PR, P1017; Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100; PRESTON K, 1987, APPL OPTICS, V26, P3258, DOI 10.1364/AO.26.003258; Qian L, 2014, INT CONF BIG DATA, P223, DOI 10.1109/BIGCOMP.2014.6741440; Qin FW, 2018, COMPUT METH PROG BIO, V162, P243, DOI 10.1016/j.cmpb.2018.05.024; Ranzato M.A., 2006, ADV NEURAL INFORM PR, V19, P1137, DOI DOI 10.7551/MITPRESS/7503.003.0147; Rezatofighi SH, 2011, COMPUT MED IMAG GRAP, V35, P333, DOI 10.1016/j.compmedimag.2011.01.003; SAHLOL A, 2020, COMPUT MED IMAG GRAP, V10, P2536, DOI DOI 10.1038/s41598-020-59215-9; Shahin AI, 2019, COMPUT METH PROG BIO, V168, P69, DOI 10.1016/j.cmpb.2017.11.015; Simard PY, 2003, PROC INT CONF DOC, P958; Sobrevilla P, 1999, 18TH INTERNATIONAL CONFERENCE OF THE NORTH AMERICAN FUZZY INFORMATION PROCESSING SOCIETY - NAFIPS, P403, DOI 10.1109/NAFIPS.1999.781723; Sukittanon S, 2004, 8 INT C SPOK LANG PR; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Templeton AJ, 2014, JNCI-J NATL CANCER I, V106, DOI 10.1093/jnci/dju124; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Ushizima DM, 2005, HIS 2005: 5TH INTERNATIONAL CONFERENCE ON HYBRID INTELLIGENT SYSTEMS, PROCEEDINGS, P379; Wang JB, 2014, J TRANSL MED, V12, DOI 10.1186/1479-5876-12-7; Wang ST, 2006, IEEE T INF TECHNOL B, V10, P5, DOI 10.1109/TITB.2005.855545; Wang XD, 2014, ATHEROSCLEROSIS, V234, P206, DOI 10.1016/j.atherosclerosis.2014.03.003; Xue P, 2014, CANCER MED-US, V3, P406, DOI 10.1002/cam4.204; Yu Kai, 2009, ADV NEURAL INFORM PR, P2223; Yu W, 2017, INT CONF ASIC, P1041; Zhang JK, 2016, INT SYM COMPUT INTEL, P363, DOI [10.1109/ISCID.2016.89, 10.1109/ISCID.2016.1090]	47	4	4	1	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1837	1856		10.1007/s11263-021-01449-9	http://dx.doi.org/10.1007/s11263-021-01449-9		MAR 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU					2022-12-18	WOS:000633735800001
J	Benny, Y; Galanti, T; Benaim, S; Wolf, L				Benny, Yaniv; Galanti, Tomer; Benaim, Sagie; Wolf, Lior			Evaluation Metrics for Conditional Image Generation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image generation; Conditional generation; Evaluation metrics; Inception Score; Fr&#233; chet Inception Distance		We present two new metrics for evaluating generative models in the class-conditional image generation setting. These metrics are obtained by generalizing the two most popular unconditional metrics: the Inception Score (IS) and the Frechet Inception Distance (FID). A theoretical analysis shows the motivation behind each proposed metric and links the novel metrics to their unconditional counterparts. The link takes the form of a product in the case of IS or an upper bound in the FID case. We provide an extensive empirical evaluation, comparing the metrics to their unconditional variants and to other metrics, and utilize them to analyze existing generative models, thus providing additional insights about their performance, from unlearned classes to mode collapse.	[Benny, Yaniv; Galanti, Tomer; Benaim, Sagie; Wolf, Lior] Tel Aviv Univ, Tel Aviv, Israel; [Wolf, Lior] Facebook AI Res, Tel Aviv, Israel	Tel Aviv University; Facebook Inc	Benny, Y (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.	yanivbenny@mail.tau.ac.il; tomergalanti@mail.tau.ac.il; sagiebenaim@mail.tau.ac.il; wolf@cs.tau.ac.il			European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme [ERC CoG 725974]	European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme(European Research Council (ERC))	This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC CoG 725974).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arjovsky M, 2017, PR MACH LEARN RES, V70; Brock A., 2019, INT C LEARNING REPRE; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Gulrajani I, 2017, P NIPS 2017; Hensel M, 2017, ADV NEUR IN, V30; Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048; Karras Tero, 2019, ABS191204958 CORR; Krizhevsky A, 2010, CIFAR 10 CANADIAN I; LeCun Y., 2010, MNIST HANDWRITTEN DI; Mirza M., 2014, ARXIV; Odena A., 2016, SEMISUPERVISED LEARN; Odena A, 2017, PR MACH LEARN RES, V70; Ravuri S, 2019, ADV NEUR IN, V32; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Singh KK, 2019, PROC CVPR IEEE, P6483, DOI 10.1109/CVPR.2019.00665; Sutherland Danica J, 2018, ICLR; Zhu Jun-Yan, 2017, ICCV	22	4	4	9	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1712	1731		10.1007/s11263-020-01424-w	http://dx.doi.org/10.1007/s11263-020-01424-w		MAR 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		hybrid, Green Submitted			2022-12-18	WOS:000624439800001
J	Liu, RX; Shen, J; Wang, H; Chen, C; Cheung, SC; Asari, VK				Liu, Ruixu; Shen, Ju; Wang, He; Chen, Chen; Cheung, Sen-ching; Asari, Vijayan K.			Enhanced 3D Human Pose Estimation from Videos by Using Attention-Based Neural Network with Dilated Convolutions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D human pose; Motion reconstruction; Monocular capture; Performance-driven retargeting; Attention; Multi-scale dilation		The attention mechanism provides a sequential prediction framework for learning spatial models with enhanced implicit temporal consistency. In this work, we show a systematic design (from 2D to 3D) for how conventional networks and other forms of constraints can be incorporated into the attention framework for learning long-range dependencies for the task of pose estimation. The contribution of this paper is to provide a systematic approach for designing and training of attention-based models for the end-to-end pose estimation, with the flexibility and scalability of arbitrary video sequences as input. We achieve this by adapting temporal receptive field via a multi-scale structure of dilated convolutions. Besides, the proposed architecture can be easily adapted to a causal model enabling real-time performance. Any off-the-shelf 2D pose estimation systems, e.g. Our method achieves the state-of-the-art performance and outperforms existing methods by reducing the mean per joint position error to 33.4mm on Human 3.6M dataset. Our code is available at https://github.com/lrxjason/Attention3DHumanPose	[Liu, Ruixu; Shen, Ju; Wang, He; Asari, Vijayan K.] Univ Dayton, Dayton, OH 45469 USA; [Chen, Chen] Univ North Carolina Charlotte, Charlotte, NC 28223 USA; [Cheung, Sen-ching] Univ Kentucky, Lexington, KY 40506 USA	University of Dayton; University of North Carolina; University of North Carolina Charlotte; University of Kentucky	Liu, RX (corresponding author), Univ Dayton, Dayton, OH 45469 USA.	liur05@udayton.edu			National Endowment for the Humanities [AKA-260488-18]; National Science Foundation (NSF) [1910844]	National Endowment for the Humanities; National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea)	This work is partially supported by the National Endowment for the Humanities under Grant No. AKA-260488-18 and National Science Foundation (NSF) under Grant No. 1910844.	Amin S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.45; Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Bahdanau D, 2016, ICLR; Bai S., 2018, ARXIV PREPRINT ARXIV; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Chen T., 2020, ARXIV200210322; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Chen Y., 2019, IEEE T PATTERN ANAL; Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601; Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41; Dauphin YN, 2017, PR MACH LEARN RES, V70; Fang HS, 2018, AAAI CONF ARTIF INTE, P6821; Ferrari V, 2009, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2009.5206495; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Kalchbrenner Nal, 2016, ARXIV161010099; Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8; Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001; Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu J., 2020, ARXIV200314179; Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511; LiyuanLiu HaomingJiang, 2019, INT C LEARN REPR; Mandery C, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P329, DOI 10.1109/ICAR.2015.7251476; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Neverova N, 2015, LECT NOTES COMPUT SC, V8925, P474, DOI 10.1007/978-3-319-16178-5_33; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Oord A.V.D., 2016, SSW; Palmero C, 2016, INT J COMPUT VISION, V118, P217, DOI 10.1007/s11263-016-0901-x; Park S, 2016, LECT NOTES COMPUT SC, V9915, P156, DOI 10.1007/978-3-319-49409-8_15; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794; Rhodin H, 2018, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2018.00880; Ruck D.W., 1990, J NEURAL NETWORK COM, V2, P40; Sarafianos N, 2016, COMPUT VIS IMAGE UND, V152, P1, DOI 10.1016/j.cviu.2016.09.002; Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072; Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551; Yang XW, 2015, PROCEEDINGS OF THE 22ND INTERNATIONAL CONGRESS ON SOUND AND VIBRATION; Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741; Yao Xuchen, 2013, C EMPIRICAL METHODS, P590; Yin W., 2016, T ASS COMPUT LINGUIS, P259, DOI DOI 10.1162/TACL_A_00097; Yoo JC, 2009, CIRC SYST SIGNAL PR, V28, P819, DOI [10.1007/s00034-009-9130-7, 10.1007/S00034-009-9130-7]; Zhang Michael, 2019, NEURIPS; Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17	58	4	4	2	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1596	1615		10.1007/s11263-021-01436-0	http://dx.doi.org/10.1007/s11263-021-01436-0		FEB 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000622275300003
J	Wu, XX; Wang, RQ; Hou, JY; Lin, HX; Luo, JB				Wu, Xinxiao; Wang, Ruiqi; Hou, Jingyi; Lin, Hanxi; Luo, Jiebo			Spatial-Temporal Relation Reasoning for Action Prediction in Videos	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action prediction; Spatial&#8211; temporal relation reasoning; Long short-term graph network; Improved gated graph neural network		Action prediction in videos refers to inferring the action category label by an early observation of a video. Existing studies mainly focus on exploiting multiple visual cues to enhance the discriminative power of feature representation while neglecting important structure information in videos including interactions and correlations between different object entities. In this paper, we focus on reasoning about the spatial-temporal relations between persons and contextual objects to interpret the observed video part for predicting action categories. With this in mind, we propose a novel spatial-temporal relation reasoning approach that extracts the spatial relations between persons and objects in still frames and explores how these spatial relations change over time. Specifically, for spatial relation reasoning, we propose an improved gated graph neural network to perform spatial relation reasoning between the visual objects in video frames. For temporal relation reasoning, we propose a long short-term graph network to model both the short-term and long-term varying dynamics of the spatial relations with multi-scale receptive fields. By this means, our approach can accurately recognize the video content in terms of fine-grained object relations in both spatial and temporal domains to make prediction decisions. Moreover, in order to learn the latent correlations between spatial-temporal object relations and action categories in videos, a visual semantic relation loss is proposed to model the triple constraints between objects in semantic domain via VTransE. Extensive experiments on five public video datasets (i.e., 20BN-something-something, CAD120, UCF101, BIT-Interaction and HMDB51) demonstrate the effectiveness of the proposed spatial-temporal relation reasoning on action prediction.	[Wu, Xinxiao; Wang, Ruiqi; Hou, Jingyi; Lin, Hanxi] Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 10081, Peoples R China; [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA	Beijing Institute of Technology; University of Rochester	Wu, XX (corresponding author), Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 10081, Peoples R China.	wuxinxiao@bit.edu.cn; wang_ruiqi@bit.edu.cn; houjingyi@bit.edu.cn; hxlin@bit.edu.cn; jluo@cs.rochester.edu		Luo, Jiebo/0000-0002-4516-9729	Natural Science Foundation of China (NSFC) [61673062, 62072041]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported in part by the Natural Science Foundation of China (NSFC) under Grant Nos. 61673062 and 62072041.	Aditya S, 2018, AAAI CONF ARTIF INTE, P629; Aliakbarian MS, 2017, IEEE I CONF COMP VIS, P280, DOI 10.1109/ICCV.2017.39; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bengio Y., 2014, ARXIV14061078; Bhoi, 2019, ARXIV190109403; Cai YJ, 2019, AAAI CONF ARTIF INTE, P8118; Cao Y, 2013, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2013.343; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; EVANS JSBT, 1993, COGNITION, V49, P165, DOI 10.1016/0010-0277(93)90039-X; Gilmer J, 2017, PR MACH LEARN RES, V70; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Herzig R, 2019, IEEE INT CONF COMP V, P2347, DOI 10.1109/ICCVW.2019.00288; Hu JF, 2019, IEEE T PATTERN ANAL, V41, P2568, DOI 10.1109/TPAMI.2018.2863279; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Liu Jiaomin, 2009, Proceedings of the 2009 Second International Conference on Intelligent Networks and Intelligent Systems (ICINIS 2009), P15, DOI [10.1109/CVPRW.2009.5206744, 10.1109/ICINIS.2009.13]; Kay W., 2017, ARXIV PREPRINT ARXIV; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kong Y, 2018, AAAI CONF ARTIF INTE, P7000; Kong Y, 2020, IEEE T PATTERN ANAL, V42, P539, DOI 10.1109/TPAMI.2018.2882805; Kong Y, 2017, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2017.390; Kong Y, 2016, IEEE T PATTERN ANAL, V38, P1844, DOI 10.1109/TPAMI.2015.2491928; Kong Y, 2014, LECT NOTES COMPUT SC, V8693, P596, DOI 10.1007/978-3-319-10602-1_39; Kong Y, 2014, IEEE T PATTERN ANAL, V36, P1775, DOI 10.1109/TPAMI.2014.2303090; Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Lai SF, 2018, IEEE T IMAGE PROCESS, V27, P2272, DOI 10.1109/TIP.2017.2751145; Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45; Li K, 2014, IEEE T PATTERN ANAL, V36, P1644, DOI 10.1109/TPAMI.2013.2297321; Liang KM, 2018, AAAI CONF ARTIF INTE, P7098; Liao WT, 2019, IEEE COMPUT SOC CONF, P444, DOI 10.1109/CVPRW.2019.00058; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Newell A, 2017, ADV NEUR IN, V30; Nicolicioiu A, 2019, ADV NEUR IN, V32; Qi MS, 2019, PROC CVPR IEEE, P3952, DOI 10.1109/CVPR.2019.00408; Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Shang XD, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1300, DOI 10.1145/3123266.3123380; Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7; Soomro K., 2012, ARXIV; Sui YL, 2013, INT SYM CODE GENER, P1; Sun C, 2019, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2019.00036; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tsai YHH, 2019, PROC CVPR IEEE, P10416, DOI 10.1109/CVPR.2019.01067; Velickovic P., 2018, ICLR; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wang XH, 2019, PROC CVPR IEEE, P3551, DOI 10.1109/CVPR.2019.00367; Woo S, 2018, ADV NEUR IN, V31; Xu H, 2019, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2019.00952; Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331; Zhang J, 2019, AAAI CONF ARTIF INTE, P9185; Zhao H, 2019, IEEE I CONF COMP VIS, P7002, DOI 10.1109/ICCV.2019.00710; Zheng W. S., 2019, P 28 INT JOINT C ART; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49	62	4	4	4	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1484	1505		10.1007/s11263-020-01409-9	http://dx.doi.org/10.1007/s11263-020-01409-9		FEB 2021	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC					2022-12-18	WOS:000617408800001
J	Pang, JM; Chen, K; Li, Q; Xu, ZH; Feng, HJ; Shi, JP; Ouyang, WL; Lin, DH				Pang, Jiangmiao; Chen, Kai; Li, Qi; Xu, Zhihai; Feng, Huajun; Shi, Jianping; Ouyang, Wanli; Lin, Dahua			Towards Balanced Learning for Instance Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Instance recognition; Object detection; Balanced learning; Deep learning; Convolutional neural networks		Instance recognition is rapidly advanced along with the developments of deep convolutional neural networks. Compared to the model architectures the training process, which is also crucial to the success of detectors, has received relatively less attention. In this work, we carefully revisit the standard training practice of detectors, and find that the detection performance is often limited by the imbalance during the training process, which generally consists in three levels-sample level, feature level, and objective level. To mitigate the adverse effects caused thereby, we propose Libra R-CNN, a simple yet effective framework towards balanced learning for instance recognition. It integrates IoU-balanced sampling, balanced feature pyramid, and objective re-weighting, respectively for reducing the imbalance at sample, feature, and objective level. Extensive experiments conducted on MS COCO, LVIS and Pascal VOC datasets prove the effectiveness of the overall balanced design.	[Pang, Jiangmiao; Li, Qi; Xu, Zhihai; Feng, Huajun] Zhejiang Univ, Hangzhou, Peoples R China; [Chen, Kai; Lin, Dahua] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Shi, Jianping] SenseTime Res, Beijing, Peoples R China; [Ouyang, Wanli] Univ Sydney, Sydney, NSW, Australia	Zhejiang University; Chinese University of Hong Kong; University of Sydney	Li, Q (corresponding author), Zhejiang Univ, Hangzhou, Peoples R China.	pjm@zju.edu.cn; ck015@ie.cuhk.edu.hk; liqi@zju.edu.cn; xuzh@zju.edu.cn; fenghj@zju.edu.cn; shijianping@sensetime.com; wanli.ouyang@sydney.edu.au; dhlin@ie.cuhk.hk			National Natural Science Foundation of China [61975175]; Civilian Fundamental Research [D040301]; Collaborative Research grant from SenseTime Group (CUHK) [TS1610626, TS1712093]; General Research Fund (GRF) of Hong Kong [14236516, 14203518]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Civilian Fundamental Research; Collaborative Research grant from SenseTime Group (CUHK); General Research Fund (GRF) of Hong Kong	This work is partially supported by National Natural Science Foundation of China (No. 61975175) the Civilian Fundamental Research (No. D040301), the Collaborative Research grant from SenseTime Group (CUHK Agreement No. TS1610626 & No. TS1712093), and the General Research Fund (GRF) of Hong Kong (No. 14236516 & No. 14203518).	Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22; Cao KD, 2019, ADV NEUR IN, V32; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen K, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV; Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236; Chen Xiaozhi, 2017, P IEEE C COMP VIS PA, DOI 10.1109/cvpr.2017.691.2017; Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; He K, 2017, P IEEE INT C COMPUTE, DOI DOI 10.1109/ICCV.2017.322; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428; Hoiem Derek, 2012, ECCV, P340, DOI [10.1007/978-3-642-33712-3_25, DOI 10.1007/978-3-642-33712-3_25]; Hosang J, 2017, PROC CVPR IEEE, P6469, DOI 10.1109/CVPR.2017.685; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168; Kang Bingyi, 2019, ARXIV191009217, DOI 10.48550/arXiv.1910.09217; Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95; Khan Salman H, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3573, DOI 10.1109/TNNLS.2017.2732482; Kim SW, 2018, LECT NOTES COMPUT SC, V11209, P239, DOI 10.1007/978-3-030-01228-1_15; Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Law H, 2019, CORNERNET DETECTING; Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45; Li BY, 2019, AAAI CONF ARTIF INTE, P8577; Li HY, 2019, INT J COMPUT VISION, V127, P225, DOI 10.1007/s11263-018-1101-7; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Ouyang WL, 2017, IEEE I CONF COMP VIS, P1956, DOI 10.1109/ICCV.2017.214; Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren S., 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.169; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shen C., 2017, ARXIV170703124; Shen Li, 2016, ECCV; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Singh B, 2018, ADV NEUR IN, V31; Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Tianheng Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P660, DOI 10.1007/978-3-030-58568-6_39; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308; Wang YC, 2017, ADV NEUR IN, V30; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI 10.1007/s11263-019-01198-w; Xing J., 2014, ARXIV PREPRINT ARXIV; Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975; Yu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10988, DOI 10.1109/CVPR42600.2020.01100; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeng XY, 2018, IEEE T PATTERN ANAL, V40, P2109, DOI 10.1109/TPAMI.2017.2745563; Zhang S., 2020, IEEE C COMP VIS PATT; Zhang SF, 2019, INT J COMPUT VISION, V127, P537, DOI 10.1007/s11263-019-01159-3; Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442; Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259; Zhou X., 2019, ARXIV; Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953; Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52; Zou Z., 2019, ARXIVABS190505055	70	4	4	4	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1376	1393		10.1007/s11263-021-01434-2	http://dx.doi.org/10.1007/s11263-021-01434-2		FEB 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000615139200001
J	Huang, HB; Yu, AJ; Chai, ZH; He, R; Tan, TI				Huang, Huaibo; Yu, Aijing; Chai, Zhenhua; He, Ran; Tan, Tieniu			Selective Wavelet Attention Learning for Single Image Deraining	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deraining; Wavelet transform; Selective attention; Wavelet pooling and unpooling	REPRESENTATION; NETWORK; MODEL	Single image deraining refers to the process of restoring the clean background scene from a rainy image. Current approaches have resorted to deep learning techniques to remove rain from a single image by leveraging some prior information. However, due to the various appearances of rain streaks and accumulation, it is difficult to separate rain and background information in the embedding space, which results in inaccurate deraining. To address this issue, this paper proposes a selective wavelet attention learning method by learning a series of wavelet attention maps to guide the separation of rain and background information in both spatial and frequency domains. The key aspect of our method is utilizing wavelet transform to learn the content and structure of rainy features because the high-frequency features are more sensitive to rain degradations, whereas the low-frequency features preserve more of the background content. To begin with, we develop a selective wavelet attention encoder-decoder network to learn wavelet attention maps guiding the separation of rainy and background features at multiple scales. Meanwhile, we introduce wavelet pooling and unpooling to the encoder-decoder network, which shows superiority in learning increasingly abstract representations while preserving the background details. In addition, we propose latent alignment learning to supervise the background features as well as augment the training data to further improve the accuracy of deraining. Finally, we employ a hierarchical discriminator network based on selective wavelet attention to adversarially improve the visual fidelity of the generated results both globally and locally. Extensive experiments on synthetic and real datasets demonstrate that the proposed approach achieves more appealing results both quantitatively and qualitatively than the recent state-of-the-art methods.	[Huang, Huaibo; Yu, Aijing; He, Ran; Tan, Tieniu] CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China; [He, Ran; Tan, Tieniu] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China; [Huang, Huaibo; Yu, Aijing; He, Ran; Tan, Tieniu] CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; [Huang, Huaibo; Yu, Aijing; He, Ran; Tan, Tieniu] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China; [Chai, Zhenhua] Meituandianping Grp, AI Platform, Vis Intelligence Ctr, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	He, R (corresponding author), CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China.; He, R (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.; He, R (corresponding author), CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.; He, R (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.	huaibo.huang@cripac.ia.ac.cn; aijing.yu@cripac.ia.ac.cn; chaizhenhua@meituan.com; rhe@nlpr.ia.ac.cn; tnt@nlpr.ia.ac.cn		Huang, Huaibo/0000-0001-5866-2283	National Natural Science Foundation of China [62006228, 61721004, U20A20223]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is partially funded by National Natural Science Foundation of China (Grant No. 62006228, 61721004, U20A20223).	[Anonymous], 2019, ARXIV191207150; Billings SA, 2005, IEEE T NEURAL NETWOR, V16, P862, DOI 10.1109/TNN.2005.849842; Chen XY, 2018, LECT NOTES COMPUT SC, V11206, P167, DOI 10.1007/978-3-030-01216-8_11; De Silva DDN, 2020, PROC SPIE, V11433, DOI 10.1117/12.2556535; Deco G, 2001, J COMPUT NEUROSCI, V10, P231, DOI 10.1023/A:1011233530729; Deng X, 2019, IEEE I CONF COMP VIS, P3076, DOI 10.1109/ICCV.2019.00317; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Fan ZW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1751, DOI 10.1145/3240508.3240694; Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481; Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186; Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu X., 2019, IEEE CVF C COMP VIS, P8022; Huang HB, 2019, INT J COMPUT VISION, V127, P763, DOI 10.1007/s11263-019-01154-8; Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jiang Y, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149440; Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057; Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399; Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636; Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173; Li Ruoteng, 2017, ARXIV E PRINTS, P2; Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16; Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121; Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388; Mallat S, 1996, P IEEE, V84, P604, DOI 10.1109/5.488702; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mirza M., 2014, ARXIV; Pan JS, 2018, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2018.00324; Provenzi E, 2014, INT J COMPUT VISION, V106, P153, DOI 10.1007/s11263-013-0651-y; Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263; Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406; Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667; SZU HH, 1992, OPT ENG, V31, P1907, DOI 10.1117/12.59918; TREISMAN A, 1982, J EXP PSYCHOL HUMAN, V8, P194, DOI 10.1037/0096-1523.8.2.194; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang GQ, 2019, IEEE I CONF COMP VIS, P5643, DOI 10.1109/ICCV.2019.00574; Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255; Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143; Yang WH, 2019, IEEE T IMAGE PROCESS, V28, P2948, DOI 10.1109/TIP.2019.2892685; Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183; Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860; Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913; You S, 2016, IEEE T PATTERN ANAL, V38, P1721, DOI 10.1109/TPAMI.2015.2491937; Yuan B., 2019, ARXIV190205625; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang Han, 2018, ARXIV180508318; Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407; Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079; ZHANG QG, 1992, IEEE T NEURAL NETWOR, V3, P889, DOI 10.1109/72.165591; Zhang R, 2019, PR MACH LEARN RES, V97; Zhong ZS, 2018, ADV NEUR IN, V31; Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276	58	4	4	2	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1282	1300		10.1007/s11263-020-01421-z	http://dx.doi.org/10.1007/s11263-020-01421-z		JAN 2021	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000610036300001
J	Li, Y; Huang, HB; Cao, J; He, R; Tan, TN				Li, Yi; Huang, Huaibo; Cao, Jie; He, Ran; Tan, Tieniu			Disentangled Representation Learning of Makeup Portraits in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face verification; Makeup transfer; Disentangled feature; Correspondence field	FACE; NETWORK	Makeup studies have recently caught much attention in computer version. Two of the typical tasks are makeup-invariant face verification and makeup transfer. Although having experienced remarkable progress, both tasks remain challenging, especially encountering data in the wild. In this paper, we propose a disentangled feature learning strategy to fulfil both tasks in a single generative network. Overall, a makeup portrait can be decomposed into three components: makeup, identity and geometry (including expression, pose etc.). We assume that the extracted image representation can be decomposed into a makeup code that captures the makeup style and an identity code to preserve the source identity. As for other variation factors, we consider them as native structures from the source image that should be reserved. Thus a dense correspondence field is integrated in the network to preserve the geometry on a face. To encourage delightful visual results after makeup transfer, we propose a cosmetic loss to learn makeup styles in a delicate way. Finally, a new Cross-Makeup Face (CMF) benchmark dataset () with in-the-wild makeup portraits is built up to push the frontiers of related research. Both visual and quantitative experimental results on four makeup datasets demonstrate the superiority of the proposed method.	[Li, Yi; Huang, Huaibo; Cao, Jie; He, Ran; Tan, Tieniu] CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; [Li, Yi; Huang, Huaibo; Cao, Jie; He, Ran; Tan, Tieniu] CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Li, Yi; Huang, Huaibo; Cao, Jie; He, Ran; Tan, Tieniu] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China; [Li, Yi; Huang, Huaibo; Cao, Jie; He, Ran; Tan, Tieniu] Univ Chinese Acad Sci, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	He, R (corresponding author), CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.; He, R (corresponding author), CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China.; He, R (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.; He, R (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.	rhe@nlpr.ia.ac.cn	Li, Yi/AAC-9201-2019	Li, Yi/0000-0002-2856-7290				Alashkar T, 2017, AAAI CONF ARTIF INTE, P941; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Booth J, 2014, IEEE IMAGE PROC, P4672, DOI 10.1109/ICIP.2014.7025947; Cao J, 2018, ADV NEUR IN, V31; Chen C, 2015, IEEE INT WORKS INFOR, DOI 10.5194/isprsarchives-XL-7-W4-1-2015; Chen Y. C., 2017, IEEE INT C COMP VIS, V2; Choi Y., 2018, IEEE INT C COMP VIS; Dantcheva Antitza, 2012, 2012 IEEE 5 INT C BI, P391, DOI DOI 10.1109/BTAS.2012.6374605; Ding M, 2019, BASIC CLIN PHARMACOL, V125, P6; Gonzalez-Garcia A, 2018, ADV NEUR IN, V31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; GULER RA, 2017, IEEE C COMP VIS PATT, V2, P5; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Guo GD, 2014, IEEE T CIRC SYST VID, V24, P814, DOI 10.1109/TCSVT.2013.2280076; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He R, 2017, AAAI CONF ARTIF INTE, P2000; Hu JL, 2013, INT CONF ACOUST SPEE, P2342, DOI 10.1109/ICASSP.2013.6638073; Hu YB, 2018, PROC CVPR IEEE, P8398, DOI 10.1109/CVPR.2018.00876; Huang HB, 2018, ADV NEUR IN, V31; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jin ZP, 2016, IEEE IJCNN, P2568, DOI 10.1109/IJCNN.2016.7727520; Jing XY, 2016, PATTERN RECOGN, V59, P14, DOI 10.1016/j.patcog.2016.01.023; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618; Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060; Li Y, 2018, AAAI CONF ARTIF INTE, P3579; Li Y, 2019, PATTERN RECOGN, V90, P99, DOI 10.1016/j.patcog.2019.01.013; Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28; Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683; Lu ZH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1083, DOI 10.1145/3240508.3240647; Nguyen Hieu V, 2010, P AS C COMP VIS, P709, DOI DOI 10.1007/978-3-642-19309-5_55; Odena A, 2017, PR MACH LEARN RES, V70; Oord A., 2016, 33 INT C MACH LEARN; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roth J, 2016, PROC CVPR IEEE, P4197, DOI 10.1109/CVPR.2016.455; Roth J, 2015, PROC CVPR IEEE, P2606, DOI 10.1109/CVPR.2015.7298876; Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612; Sun Y, 2017, PATTERN RECOGN, V66, P153, DOI 10.1016/j.patcog.2017.01.011; Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tong WS, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P211, DOI 10.1109/PG.2007.31; Tran L, 2019, INT J COMPUT VISION, V127, P824, DOI 10.1007/s11263-019-01155-7; Tu Xiaoguang, 2019, ARXIV PREPRINT ARXIV; Wang S., 2016, 13 AAAI C ART INT; Wei Z, 2017, PROC CVPR IEEE, P3947, DOI 10.1109/CVPR.2017.420; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; Yu JC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1018; Zhang H, 2016, IEEE INT SYMP SIGNAL, P1, DOI 10.1109/ISSPIT.2016.7885999; Zhao J, 2019, AAAI CONF ARTIF INTE, P9251; Zhao J, 2017, ADV NEUR IN, V30; Zhou X, 2018, IEEE CONF COMPUT; Zhu Jun-Yan, 2017, ICCV; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	67	4	4	2	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2166	2184		10.1007/s11263-019-01267-0	http://dx.doi.org/10.1007/s11263-019-01267-0			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG					2022-12-18	WOS:000559365500010
J	Piasco, N; Sidibe, D; Gouet-Brunet, V; Demonceaux, C				Piasco, Nathan; Sidibe, Desire; Gouet-Brunet, Valerie; Demonceaux, Cedric			Improving Image Description with Auxiliary Modality for Visual Localization in Challenging Conditions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Localization; Image retrieval; Side modality learning; Depth from monocular; Global image descriptor	REPRESENTATIONS; RECOGNITION	Image indexing for lifelong localization is a key component for a large panel of applications, including robot navigation, autonomous driving or cultural heritage valorization. The principal difficulty in long-term localization arises from the dynamic changes that affect outdoor environments. In this work, we propose a new approach for outdoor large scale image-based localization that can deal with challenging scenarios like cross-season, cross-weather and day/night localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We show through extensive evaluation that our method can improve localization performances, especially in challenging scenarios when the visual appearance of the scene has changed. Our method is able to leverage both visual and geometric clues from monocular images to create discriminative descriptors for cross-season localization and effective matching of images acquired at different time periods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images. Finally we extended our method to reflectance modality and we compare multi-modal descriptors respectively based on geometry, material reflectance and a combination of both.	[Piasco, Nathan; Demonceaux, Cedric] Univ Bourgogne Franche Comte, ImViA, VIBOT ERL CNRS 6000, Dijon, France; [Piasco, Nathan; Gouet-Brunet, Valerie] Univ Paris Est, ENSG, IGN, LaSTIG, F-94160 St Mande, France; [Sidibe, Desire] Univ Evry, Univ Paris Saclay, IBISC, F-91020 Evry, France	Universite de Bourgogne; Universite Gustave-Eiffel; UDICE-French Research Universities; Universite Paris Saclay	Piasco, N (corresponding author), Univ Bourgogne Franche Comte, ImViA, VIBOT ERL CNRS 6000, Dijon, France.; Piasco, N (corresponding author), Univ Paris Est, ENSG, IGN, LaSTIG, F-94160 St Mande, France.	nathan.piasco@gmail.com	SIDIBE, DESIRE/AFQ-8070-2022; Gouet-Brunet, Valérie/GYD-8759-2022; Demonceaux, Cedric/S-5643-2017	SIDIBE, DESIRE/0000-0002-5843-7139; Demonceaux, Cedric/0000-0001-6916-1273	French ANR project pLaTINUM [ANR-15-CE23-0010]; NVIDIA Corporation	French ANR project pLaTINUM(French National Research Agency (ANR)); NVIDIA Corporation	We would like to acknowledge the French ANR project pLaTINUM (ANR-15-CE23-0010) for its financial support and Marco Bevilacqua for kindly sharing the code of his inpainting algorithm used in this research. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.	Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI 10.1109/ICRA.2019.8794387; Arandjelovi R., 2014, AS C COMP VIS ACCV; Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Ardeshir S, 2014, LECT NOTES COMPUT SC, V8694, P602, DOI 10.1007/978-3-319-10599-4_39; Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591009; Azzi C., 2016, BRIT MACH VIS C BMVC, V2, P1; Balntas Vassileios, 2016, BMVC, V2, DOI DOI 10.5244/C.30.119; Bansal A, 2014, IEEE INT VEH SYM, P800, DOI 10.1109/IVS.2014.6856605; Bevilacqua M, 2017, ISPRS J PHOTOGRAMM, V125, P16, DOI 10.1016/j.isprsjprs.2017.01.005; Bhowmik N, 2017, JOINT URB REMOTE SEN; Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489; Cao Y, 2016, AAAI CONF ARTIF INTE, P3457; Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Chevalier M, 2018, PATTERN RECOGN LETT, V116, P29, DOI 10.1016/j.patrec.2018.09.007; Christie G., 2016, ARXIV160904794; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601; Croissant JG, 2016, FRONT MOL BIOSCI, V3, DOI 10.3389/fmolb.2016.00001; Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921; Eigen David, 2014, NEURIPS; Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446; Garg S, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Garg S, 2018, IEEE INT CONF ROBOT, P3645; Germain H., 2018, ARXIV181203707; Germain H, 2019, INT CONF 3D VISION, P513, DOI 10.1109/3DV.2019.00063; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Hays James, 2008, CVPR, DOI DOI 10.1109/CVPR.2008.4587784; Hinton G., 2015, ARXIV150302531; Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96; Iscen A., 2018, MINING MANIFOLDS MET; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jegou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609; Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572; Kim HJ, 2017, PROC CVPR IEEE, P3251, DOI 10.1109/CVPR.2017.346; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; Li W, 2018, IEEE T PATTERN ANAL, V40, P2030, DOI 10.1109/TPAMI.2017.2734890; Liu L, 2019, IEEE I CONF COMP VIS, P2570, DOI 10.1109/ICCV.2019.00266; Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685; Loo SY, 2019, IEEE INT CONF ROBOT, P5218, DOI 10.1109/ICRA.2019.8794425; Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823; Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498; Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594; Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623; Morago B, 2016, IEEE T IMAGE PROCESS, V7149, P12; Mordan T, 2018, ADV NEUR IN, V31; Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331; Naseer Tayyab, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2614, DOI 10.1109/ICRA.2017.7989305; Naseer T, 2018, IEEE T ROBOT, V34, P289, DOI 10.1109/TRO.2017.2788045; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Paulin M, 2017, INT J COMPUT VISION, V121, P149, DOI 10.1007/s11263-016-0924-3; Piasco N, 2019, IEEE IMAGE PROC, P2561, DOI 10.1109/ICIP.2019.8803014; Piasco N, 2019, IEEE INT CONF ROBOT, P9094, DOI 10.1109/ICRA.2019.8794221; Piasco N, 2018, PATTERN RECOGN, V74, P90, DOI 10.1016/j.patcog.2017.09.013; Piasco Nathan, 2019, BRIT MACH VIS C BMVC; Porav H, 2019, IEEE INT CONF ROBOT, P7087, DOI 10.1109/ICRA.2019.8793486; Porav H, 2018, IEEE INT CONF ROBOT, P1011; Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566; Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1; Russell BC, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Sarlin P.-E., 2018, TECH REP; Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300; Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897; Sattler T, 2016, PROC CVPR IEEE, P1582, DOI 10.1109/CVPR.2016.175; Schonberger JL, 2018, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR.2018.00721; Seymour M, 2019, COMMUN BIOL, V2, DOI 10.1038/s42003-019-0330-9; Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; Sizikova E., 2016, EUR C COMP VIS WORKS, P1; Stenborg E, 2018, IEEE INT CONF ROBOT, P6484; Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695; Toft C, 2018, LECT NOTES COMPUT SC, V11206, P391, DOI 10.1007/978-3-030-01216-8_24; Torii A, 2015, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2015.7298790; Torii A, 2015, IEEE T PATTERN ANAL, V37, P2346, DOI 10.1109/TPAMI.2015.2409868; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470; Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042; Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451; Zamir AR, 2014, IEEE T PATTERN ANAL, V36, P1546, DOI 10.1109/TPAMI.2014.2299799; Zamir AR, 2010, LECT NOTES COMPUT SC, V6314, P255, DOI 10.1007/978-3-642-15561-1_19; Zwald L., 2012, ARXIV12076868	87	4	4	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2021	129	1								10.1007/s11263-020-01363-6	http://dx.doi.org/10.1007/s11263-020-01363-6		AUG 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PU0KV		Green Submitted			2022-12-18	WOS:000565043900001
J	Chrysos, GG; Kossaifi, J; Zafeiriou, S				Chrysos, Grigorios G.; Kossaifi, Jean; Zafeiriou, Stefanos			RoCGAN: Robust Conditional GAN	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Conditional GAN; Unsupervised learning; Autoencoder; Robust regression; Super-resolution; Adversarial attacks; Cross-noise experiments		Conditional image generation lies at the heart of computer vision and conditional generative adversarial networks (cGAN) have recently become the method of choice for this task, owing to their superior performance. The focus so far has largely been on performance improvement, with little effort in making cGANs more robust to noise. However, the regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGANs unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, calledRoCGAN, which leverages structure in the target space of the model to address the issue. Specifically, we augment the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold, even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and establish with both synthetic and real data the merits of our model. We perform a thorough experimental validation on large scale datasets for natural scenes and faces and observe that our model outperforms existing cGAN architectures by a large margin. We also empirically demonstrate the performance of our approach in the face of two types of noise (adversarial and Bernoulli).	[Chrysos, Grigorios G.; Kossaifi, Jean; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England; [Kossaifi, Jean] NVIDIA, Santa Clara, CA USA	Imperial College London; Nvidia Corporation	Chrysos, GG (corresponding author), Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England.	g.chrysos@imperial.ac.uk; jean.kossaifi@gmail.com; s.zafeiriou@imperial.ac.uk	Kossaifi, Jean/AAW-8519-2021; Chrysos, Grigorios/ABE-2026-2021	Kossaifi, Jean/0000-0002-4445-3429; Chrysos, Grigorios/0000-0002-0650-1856	Imperial College DTA; EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans [EP/S010203/1]; Google Faculty Award; EPSRC [EP/S010203/1] Funding Source: UKRI	Imperial College DTA; EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google Faculty Award(Google Incorporated); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We would like to thank Markos Georgopoulos for our fruitful conversations during the preparation of this work. GG Chrysos would like to thank Amazon web services for the cloud credits. The work of Grigorios Chrysos was partially funded by an Imperial College DTA. The work of Stefanos Zafeiriou was partially funded by the EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans (EP/S010203/1) and a Google Faculty Award.	Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150; Arnab A, 2018, PROC CVPR IEEE, P888, DOI 10.1109/CVPR.2018.00099; Batra, 2016, ICLR, P1; Bora A., 2018, PATTERN ANAL APPL, P1; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Chrysos G., 2020, IEEE P INT C COMP VI; Chrysos G. G., 2019, INT C LEARN REPR ICL; Chrysos GG, 2019, INT J COMPUT VISION, V127, P801, DOI 10.1007/s11263-018-1138-7; Chrysos GG, 2018, IEEE T PATTERN ANAL, V40, P2555, DOI 10.1109/TPAMI.2017.2769654; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dou Z., 2018, ARXIV181106492; Georgopoulos M, 2018, IMAGE VISION COMPUT, V80, P58, DOI 10.1016/j.imavis.2018.05.003; Giryes R, 2017, ARXIV171204741; Gondim-Ribeiro G., 2018, ARXIV180604646; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kaneko T., 2019, ARXIV190502185; Kaneko T, 2019, PROC CVPR IEEE, P2462, DOI 10.1109/CVPR.2019.00257; Kingma D.P, P 3 INT C LEARNING R; Kos J, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P36, DOI 10.1109/SPW.2018.00014; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Kurakin A, 2018, SPRING SER CHALLENGE, P195, DOI 10.1007/978-3-319-94042-7_11; Lamb A., 2018, ARXIV180402485; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lehtinen J, 2018, PR MACH LEARN RES, V80; Li S, 2019, TRANSPORT RES B-METH, V129, P193, DOI 10.1016/j.trb.2019.09.008; Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74; LIU MY, 2017, P ASM 36 INT C OC; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Madry Aleksander, 2017, ARXIV; Makhzani A., 2015, ARXIV151105644; Mirza M., 2014, ARXIV; Murdock C, 2018, LECT NOTES COMPUT SC, V11219, P851, DOI 10.1007/978-3-030-01267-0_50; Pajot Arthur, 2019, INT C LEARN REPR; Panagakis Y, 2016, IEEE T PATTERN ANAL, V38, P1665, DOI 10.1109/TPAMI.2015.2497700; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Salimans T, 2016, ADV NEUR IN, V29; Samangouei Pouya, 2018, ARXIV180506605; Shen J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1003, DOI 10.1109/ICCVW.2015.132; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thekumparampil KK, 2018, ADV NEUR IN, V31; Tokui Seiya, 2015, P WORKSH MACH LEARN, V5, P1; Tran L, 2019, INT J COMPUT VISION, V127, P824, DOI 10.1007/s11263-019-01155-7; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Valpola H., 2015, ADV INDEPENDENT COMP, P143, DOI [10.1016/B978-0-12-802806-3.00008-7, DOI 10.1016/B978-0-12-802806-3.00008-7]; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZY, 2017, ADV NEUR IN, V30; Wu X, 2017, TSINGHUA SCI TECHNOL, V22, P660, DOI 10.23919/TST.2017.8195348; Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434; Yuan X., 2017, IEEE T NEURAL NETWOR; Zhang YT, 2016, PR MACH LEARN RES, V48	67	4	4	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2665	2683		10.1007/s11263-020-01348-5	http://dx.doi.org/10.1007/s11263-020-01348-5		JUL 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		hybrid, Green Published			2022-12-18	WOS:000548483300001
J	Chang, YK; Jung, CK; Sun, J; Wang, FQ				Chang, Yakun; Jung, Cheolkon; Sun, Jun; Wang, Fengqiao			Siamese Dense Network for Reflection Removal with Flash and No-Flash Image Pairs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Reflection removal; Image restoration; Flash; no-flash; Image fusion; Layer separation; Depth filling	SEPARATION; PHOTOGRAPHY; COMPLETION	This work addresses the reflection removal with flash and no-flash image pairs to separate reflection from transmission. When objects are covered by glass, the no-flash image usually contains reflection, and thus flash is used to enhance transmission details. However, the flash image suffers from the specular highlight on the glass surface caused by flash. In this paper, we propose a siamese dense network (SDN) for reflection removal with flash and no-flash image pairs. SDN extracts shareable and complementary features via concatenated siamese dense blocks. We utilize an image fusion block for the SDN to fuse the intermediate output of two branches. Since severe information loss occurs in the specular highlight, we detect the specular highlight in the flash image based on gradient of the maximum chromaticity. Through observations, flash causes various artifacts such as tone distortion and inhomogeneous brightness. Thus, with synthetic datasets we collect 758 pairs of real flash and no-flash image pairs (including their ground truth) by different cameras to gain generalization. Various experiments show that the proposed method successfully removes reflections using flash and no-flash image pairs and outperforms state-of-the-art ones in terms of visual quality and quantitative measurements. Besides, we apply the SDN to color/depth image pairs and achieve both color reflection removal and depth filling.	[Chang, Yakun; Jung, Cheolkon; Sun, Jun; Wang, Fengqiao] Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China	Xidian University	Jung, CK (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China.	zhengzk@xidian.edu.cn						Agrawal A, 2005, ACM T GRAPHIC, V24, P828, DOI 10.1145/1073204.1073269; Aksoy Y, 2018, LECT NOTES COMPUT SC, V11213, P644, DOI 10.1007/978-3-030-01240-3_39; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], ARXIV180605376; [Anonymous], 2017, P IEEE C COMP VIS PA; [Anonymous], P IEEE C COMP VIS PA; [Anonymous], 2005, P IEEE C COMP VIS PA; [Anonymous], 2008, P IEEE C COMP VIS PA; [Anonymous], IEEE C COMP VIS PATT; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Camplani M, 2012, PROC SPIE, V8290, DOI 10.1117/12.911909; Chang YK, 2019, IEEE T IMAGE PROCESS, V28, P1954, DOI 10.1109/TIP.2018.2880088; Chang Y, 2018, IEEE ACCESS, V6, P11782, DOI 10.1109/ACCESS.2018.2797872; Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778; Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351; Farid H., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P262, DOI 10.1109/CVPR.1999.786949; Guo XJ, 2014, PROC CVPR IEEE, P2195, DOI 10.1109/CVPR.2014.281; Han BJ, 2018, IEEE T IMAGE PROCESS, V27, P4873, DOI 10.1109/TIP.2018.2849880; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He SF, 2014, LECT NOTES COMPUT SC, V8691, P110, DOI 10.1007/978-3-319-10578-9_8; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kim H, 2013, PROC CVPR IEEE, P1460, DOI 10.1109/CVPR.2013.192; Kong N, 2012, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2012.6247652; Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780; Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106; Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299; Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346; Li Y, 2013, IEEE I CONF COMP VIS, P2432, DOI 10.1109/ICCV.2013.302; Liu Y, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1070; Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001; Lu C, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P198, DOI 10.1109/MMSP.2006.285296; Matsui Sosuke, 2011, Information and Media Technologies, V6, P202; Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x; Nayar SK, 1997, INT J COMPUT VISION, V21, P163, DOI 10.1023/A:1007937815113; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Saxena A, 2007, IEEE I CONF COMP VIS, P1; Schechner YY, 2000, J OPT SOC AM A, V17, P276, DOI 10.1364/JOSAA.17.000276; Schechner YY, 2000, INT J COMPUT VISION, V39, P25, DOI 10.1023/A:1008166017466; Seo HJ, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-3; Shen J, 2013, PROC CVPR IEEE, P1187, DOI 10.1109/CVPR.2013.157; Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939; Shirai K., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3437, DOI 10.1109/ICIP.2011.6116451; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Simon C, 2015, PROC CVPR IEEE, P4231, DOI 10.1109/CVPR.2015.7299051; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Sun J, 2006, ACM T GRAPHIC, V25, P772, DOI 10.1145/1141911.1141954; Sun J, 2019, IEEE SIGNAL PROC LET, V26, P1011, DOI 10.1109/LSP.2019.2915560; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szeliski R, 2000, PROC CVPR IEEE, P246, DOI 10.1109/CVPR.2000.855826; Wan RJ, 2018, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2018.00502; Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837; Yang JL, 2016, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2016.157; Yang Y, 2019, PROC CVPR IEEE, P8133, DOI 10.1109/CVPR.2019.00833; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zhang H, 2019, LECT NOTES COMPUT SC, V11132, P349, DOI 10.1007/978-3-030-11018-5_32; Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730; Zhou QY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P654	58	4	4	1	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1673	1698		10.1007/s11263-019-01276-z	http://dx.doi.org/10.1007/s11263-019-01276-z			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN					2022-12-18	WOS:000534910600007
J	Li, K; Peng, SC; Zhang, TH; Malik, J				Li, Ke; Peng, Shichong; Zhang, Tianhao; Malik, Jitendra			Multimodal Image Synthesis with Conditional Implicit Maximum Likelihood Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Conditional image synthesis; Multimodal image synthesis; Deep generative models; Implicit maximum likelihood estimation		Many tasks in computer vision and graphics fall within the framework of conditional image synthesis. In recent years, generative adversarial nets have delivered impressive advances in quality of synthesized images. However, it remains a challenge to generate both diverse and plausible images for the same input, due to the problem of mode collapse. In this paper, we develop a new generic multimodal conditional image synthesis method based on implicit maximum likelihood estimation and demonstrate improved multimodal image synthesis performance on two tasks, single image super-resolution and image synthesis from scene layouts. We make our implementation publicly available.	[Li, Ke; Malik, Jitendra] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Peng, Shichong] Univ Toronto, Toronto, ON, Canada; [Zhang, Tianhao] Nanjing Univ, Nanjing, Peoples R China	University of California System; University of California Berkeley; University of Toronto; Nanjing University	Li, K (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	ke.li@eecs.berkeley.edu; shichong.peng@mail.utoronto.ca; bryanzhang@smail.nju.edu.cn; malik@eecs.berkeley.edu			Natural Sciences andEngineeringResearch Council of Canada (NSERC) [ONRMURIN0001414-1-0671]	Natural Sciences andEngineeringResearch Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported by ONRMURIN0001414-1-0671. Ke Li thanks the Natural Sciences andEngineeringResearch Council of Canada (NSERC) for fellowship support.	Almahairi A, 2018, PR MACH LEARN RES, V80; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Arora S., 2017, ARXIV170608224; Bruna J., 2015, ARXIV151105666; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Charpiat G, 2008, LECT NOTES COMPUT SC, V5304, P126, DOI 10.1007/978-3-540-88690-7_10; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dahl R, 2017, IEEE I CONF COMP VIS, P5449, DOI 10.1109/ICCV.2017.581; Denton Emily L, 2015, NEURIPS, V2, P4; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Gauthier J., 2014, CLASS PROJECT STANFO, V2014, P5; Ghosh A., 2017, ARXIV170402906; Goodfellow I. J., 2014, ARXIV14126515; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gutmann Michael U, 2014, ARXIV14074981; Guzm<prime>an-rivera Abner, 2012, ADV NEURAL INFORM PR; Huang X., 2018, ARXIV180404732; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kaneko T, 2017, PROC CVPR IEEE, P7006, DOI 10.1109/CVPR.2017.741; Karacan L., 2016, ARXIV161200215; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Lee S., 2019, INT C LEARN REPR; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Li K., 2017, INT C MACH LEARN, P2081; Li K, 2016, PR MACH LEARN RES, V48; Li Ke, 2018, ARXIV180909087; Liqian a, 2018, ICLR; Mathieu Michael, 2015, ARXIV151105440; Mirza M., 2014, ARXIV; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Yang Dingdong, 2019, ARXIV190109024; Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31; Yu Fisher, 2018, BDD100K DIVERSE DRIV, P6; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36; Zhu Jun-Yan, 2017, ICCV	54	4	4	3	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2607	2628		10.1007/s11263-020-01325-y	http://dx.doi.org/10.1007/s11263-020-01325-y		MAY 2020	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Green Submitted			2022-12-18	WOS:000536430800001
J	Saito, M; Saito, S; Koyama, M; Kobayashi, S				Saito, Masaki; Saito, Shunta; Koyama, Masanori; Kobayashi, Sosuke			Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generative adversarial network; Video generation; Subsampling layer		Training of generative adversarial network (GAN) on a video dataset is a challenge because of the sheer size of the dataset and the complexity of each observation. In general, the computational cost of training GAN scales exponentially with the resolution. In this study, we present a novel memory efficient method of unsupervised learning of high-resolution video dataset whose computational cost scales only linearly with the resolution. We achieve this by designing the generator model as a stack of small sub-generators and training the model in a specific way. We train each sub-generator with its own specific discriminator. At the time of the training, we introduce between each pair of consecutive sub-generators an auxiliary subsampling layer that reduces the frame-rate by a certain ratio. This procedure can allow each sub-generator to learn the distribution of the video at different levels of resolution. We also need only a few GPUs to train a highly complex generator that far outperforms the predecessor in terms of inception scores.	[Saito, Masaki; Saito, Shunta; Koyama, Masanori; Kobayashi, Sosuke] Preferred Networks Inc, Tokyo, Japan		Saito, M (corresponding author), Preferred Networks Inc, Tokyo, Japan.	msaito@preferred.jp; shunta@preferred.jp; masomatics@preferred.jp; sosk@preferred.jp		Saito, Masaki/0000-0003-4200-6585				Acharya Dinesh, 2018, ARXIV181002419; Akiba T., 2017, ARXIV PREPRINT ARXIV; Babaeizadeh Mohammad, 2018, ICLR; Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8; Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009; Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877; Byeon W, 2018, LECT NOTES COMPUT SC, V11220, P781, DOI 10.1007/978-3-030-01270-0_46; Cai HY, 2018, LECT NOTES COMPUT SC, V11206, P374, DOI 10.1007/978-3-030-01216-8_; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chun MM, 2000, TRENDS COGN SCI, V4, P170, DOI 10.1016/S1364-6613(00)01476-5; Denton E, 2018, PR MACH LEARN RES, V80; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Ebert Frederik, 2017, ARXIV171005268; Glorot X., 2010, PROC MACH LEARN RES, P249; Goncalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hao ZK, 2018, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2018.00819; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hensel M, 2017, ADV NEUR IN, V30; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Gulrajani I, 2017, ADV NEUR IN, V30; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kalchbrenner N, 2016, ARXIV161000527; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Lee Alex X, 2018, ARXIV180401523; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Liang XD, 2017, PROC CVPR IEEE, P2175, DOI 10.1109/CVPR.2017.234; Liu MY, 2017, ADV NEUR IN, V30; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Mathieu Michael, 2016, ICLR; Mescheder L, 2018, PR MACH LEARN RES, V80; Ohnishi K, 2018, AAAI CONF ARTIF INTE, P2387; Oliphant TE, 2015, GUIDE NUMPY, P2; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ranzato MarcAurelio, 2014, ARXIV14126604; Rossler Andreas, 2018, FACEFORENSICS LARGE, V2, P19; Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308; Salimans T, 2016, ADV NEUR IN, V29; Shi XJ, 2015, ADV NEUR IN, V28; Soomro K., 2012, ARXIV; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Tokui Seiya, 2015, P WORKSH MACH LEARN, V5, P1; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Unterthiner Thomas, 2018, ARXIV181201717; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang Ting-Chun, 2018, ARXIV180806601; Yang CH, 2018, PROCEEDINGS OF 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION SYSTEMS (ICCCS), P201; Zhang H., 2017, ARXIV PREPRINT ARXIV; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang ZZ, 2018, PROC CVPR IEEE, P9242, DOI 10.1109/CVPR.2018.00963; Zhao L, 2018, LECT NOTES COMPUT SC, V11219, P403, DOI 10.1007/978-3-030-01267-0_24; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	59	4	4	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2586	2606		10.1007/s11263-020-01333-y	http://dx.doi.org/10.1007/s11263-020-01333-y		MAY 2020	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Green Submitted			2022-12-18	WOS:000559841600001
J	Azadi, S; Pathak, D; Ebrahimi, S; Darrell, T				Azadi, Samaneh; Pathak, Deepak; Ebrahimi, Sayna; Darrell, Trevor			Compositional GAN: Learning Image-Conditional Binary Composition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Conditional Generative Adversarial Network; Composition; Decomposition		Generative Adversarial Networks can produce images of remarkable complexity and realism but are generally structured to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. Capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. In this work, we propose a novel self-consistent Composition-by-Decomposition network to compose a pair of objects. Given object images from two distinct distributions, our model can generate a realistic composite image from their joint distribution following the texture and shape of the input objects. We evaluate our approach through qualitative experiments and user evaluations. Our results indicate that the learned model captures potential interactions between the two object domains, and generates realistic composed scenes at test time.	[Azadi, Samaneh; Pathak, Deepak; Ebrahimi, Sayna; Darrell, Trevor] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Azadi, S (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	sazadi@eecs.berkeley.edu; pathak@eecs.berkeley.edu; sayna@eecs.berkeley.edu; trevor@eecs.berkeley.edu	Ebrahimi-Nejad, Salman/S-9415-2018	Ebrahimi-Nejad, Salman/0000-0002-1591-023X				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, PROC INT C COMPUTER; [Anonymous], 2017, NIPS; [Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2016, ICML; [Anonymous], 2017, HIGH RESOLUTION IMAG; [Anonymous], 2017, ICCV; Antoniou Antreas, 2017, ARXIV171104340; Azadi S., 2017, ARXIV171200516; Chang A. X., 2015, TECH REP; Chen X, 2016, ADV NEUR IN, V29; Cordts M., 2016, CVPR; Denton E. L., 2015, NIPS; Goodfellow Ian, 2016, NIPS 2016 TUTORIAL G; Gulrajani Ishaan, 2017, NIPS; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Isola P., 2017, CVPR; Johnson J., 2018, CVPR; Karras T, 2017, ARXIV171010196; Lai W., 2017, CVPR; Lin C.H., 2018, ARXIV180301837; Liu M. -Y., 2017, NIPS; Liu M. -Y., 2016, ADV NEURAL INFORM PR, P469; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mirza M., 2014, ARXIV PREPRINT ARXIV; Odena A., 2016, ARXIV161009585; Pathak D., 2016, CVPR; Radford A., 2016, ICLR; Reed S., 2016, NEURAL INFORM PROCES; Salimans Tim, 2016, ADV NEURAL INFORM PR; Sunkavalli K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778862; Tsai YH, 2017, IEEE INT C ELECTR TA; Xue S, 2012, UNDERSTANDING IMPROV, V31, P10, DOI [10.1145/2185520.2185580, DOI 10.1145/2185520.2185580]; Yang C., 2017, CVPR; Yang J., 2017, ARXIV170301560; Zhang H., 2017, ICCV; Zhang R., 2016, ECCV; Zhou T., 2016, ECCV; Zhu J. -Y., 2015, ICCV	39	4	4	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2570	2585		10.1007/s11263-020-01336-9	http://dx.doi.org/10.1007/s11263-020-01336-9		MAY 2020	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Green Submitted			2022-12-18	WOS:000554776700002
J	Yu, T; Meng, JJ; Fang, C; Jin, HL; Yuan, JS				Yu, Tan; Meng, Jingjing; Fang, Chen; Jin, Hailin; Yuan, Junsong			Product Quantization Network for Fast Visual Search	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Product quantization; Image retrieval; Deep learning; Video retrieval	DEEP; FEATURES	Product quantization has been widely used in fast image retrieval due to its effectiveness of coding high-dimensional visual features. By constructing the approximation function, we extend the hard-assignment quantization to soft-assignment quantization. Thanks to the differentiable property of the soft-assignment quantization, the product quantization operation can be integrated as a layer in a convolutional neural network, constructing the proposed product quantization network (PQN). Meanwhile, by extending the triplet loss to the asymmetric triplet loss, we directly optimize the retrieval accuracy of the learned representation based on asymmetric similarity measurement. Utilizing PQN, we can learn a discriminative and compact image representation in an end-to-end manner, which further enables a fast and accurate image retrieval. By revisiting residual quantization, we further extend the proposed PQN to residual product quantization network (RPQN). Benefited from the residual learning triggered by residual quantization, RPQN achieves a higher accuracy than PQN using the same computation cost. Moreover, we extend PQN to temporal product quantization network (TPQN) by exploiting temporal consistency in videos to speed up the video retrieval. It integrates frame-wise feature learning, frame-wise features aggregation and video-level feature quantization in a single neural network. Comprehensive experiments conducted on multiple public benchmark datasets demonstrate the state-of-the-art performance of the proposed PQN, RPQN and TPQN in fast image and video retrieval.	[Yu, Tan] Baidu Res, Cognit Comp Lab, Seattle, WA 94089 USA; [Meng, Jingjing; Yuan, Junsong] Univ Buffalo State Univ New York, Comp Sci & Engn Dept, New York, NY USA; [Fang, Chen; Jin, Hailin] Adobe Res, San Jose, CA USA	Baidu; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; Adobe Systems Inc.	Yu, T (corresponding author), Baidu Res, Cognit Comp Lab, Seattle, WA 94089 USA.	tyu008@e.ntu.edu.sg; jmeng2@buffalo.edu; cfang@adobe.com; hljin@adobe.com; jsyuan@buffalo.edu			University at Buffalo	University at Buffalo	This work is supported in part by a gift grant from Adobe and startup funds from University at Buffalo.	Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124; Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38; Bai S., 2018, REGULARIZED DIFFUSIO; Bai S., 2017, ENSEMBLE DIFFUSION R; Cakir F., 2017, ICCV; Cao L., 2012, P 20 ACM INT C MULTI, P299; Cao Y, 2016, AAAI CONF ARTIF INTE, P3457; Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965; Chen YJ, 2010, SENSORS-BASEL, V10, P11259, DOI 10.3390/s101211259; Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K, 2018, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2018.00069; Hong W, 2017, CVPR, V11, P18; Hong WX, 2018, AAAI CONF ARTIF INTE, P69; Hong WX, 2018, IEEE T IMAGE PROCESS, V27, P4825, DOI 10.1109/TIP.2018.2846670; Jain H, 2017, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2017.96; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jiang Q. Y., 2018, ASYMMETRIC DEEP SUPE; Klein B., 2017, ARXIV171108589; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; Li Q, 2017, ADV NEUR IN, V30; Li W.J., 2015, P CVPR BOST MA US; Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu X., 2017, ARXIV171208315 CORR; Martinez J, 2016, LECT NOTES COMPUT SC, V9906, P137, DOI 10.1007/978-3-319-46475-6_9; Mohammad Norouzi, 2012, ADV NEURAL INFORM PR, P1061; Ng J.Y.H., 2015, ARXIV150405133; Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388; PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009; Sablayrolles A, 2017, INT CONF ACOUST SPEE, P1732, DOI 10.1109/ICASSP.2017.7952453; Salakhutdinov R., 2007, RBM, V500, P500; Shen F, 2013, IEEE T PAMI, V35, P2916, DOI [10.1109/TPAMI.2013.136, DOI 10.1109/TPAMI.2013.136]; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Soomro K., 2012, ARXIV; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tu Z., PATTERN RECOGNITION; Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang XJ, 2016, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2016.222; Wang XK, 2016, INT SYM WIRELESS COM, P70, DOI 10.1109/ISWCS.2016.7600877; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Wu Gengshen, 2017, IJCAI; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; XIA Y, 2015, PROC CVPR IEEE, P3332; Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150; Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282; Yu J, 2017, PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P2424; Yu T., 2017, ICCV, P833; Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12; Yu T, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4551; Yu T, 2017, AAAI CONF ARTIF INTE, P4320; Zhang QF, 2014, INT CONF MACH LEARN, P807, DOI 10.1109/ICMLC.2014.7009713; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang ZM, 2016, PROC CVPR IEEE, P1487, DOI 10.1109/CVPR.2016.165; Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763; Zhu H, 2016, AAAI CONF ARTIF INTE, P2415	68	4	4	3	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2325	2343		10.1007/s11263-020-01326-x	http://dx.doi.org/10.1007/s11263-020-01326-x		APR 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG					2022-12-18	WOS:000528126300001
J	Hernandez-Juarez, D; Schneider, L; Cebrian, P; Espinosa, A; Vazquez, D; Lopez, AM; Franke, U; Pollefeys, M; Moure, JC				Hernandez-Juarez, Daniel; Schneider, Lukas; Cebrian, Pau; Espinosa, Antonio; Vazquez, David; Lopez, Antonio M.; Franke, Uwe; Pollefeys, Marc; Moure, Juan C.			Slanted Stixels: A Way to Represent Steep Streets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo vision; Stixel world; Self-driving cars; Scene understanding; Automotive vision; Intelligent vehicles	SEMANTIC SEGMENTATION	This work presents and evaluates a novel compact scene representation based on Stixels that infers geometric and semantic information. Our approach overcomes the previous rather restrictive geometric assumptions for Stixels by introducing a novel depth model to account for non-flat roads and slanted objects. Both semantic and depth cues are used jointly to infer the scene representation in a sound global energy minimization formulation. Furthermore, a novel approximation scheme is introduced in order to significantly reduce the computational complexity of the Stixel algorithm, and then achieve real-time computation capabilities. The idea is to first perform an over-segmentation of the image, discarding the unlikely Stixel cuts, and apply the algorithm only on the remaining Stixel cuts. This work presents a novel over-segmentation strategy based on a fully convolutional network, which outperforms an approach based on using local extrema of the disparity map. We evaluate the proposed methods in terms of semantic and geometric accuracy as well as run-time on four publicly available benchmark datasets. Our approach maintains accuracy on flat road scene datasets while improving substantially on a novel non-flat road dataset.	[Hernandez-Juarez, Daniel; Cebrian, Pau; Espinosa, Antonio; Lopez, Antonio M.; Moure, Juan C.] UAB, Barcelona, Spain; [Schneider, Lukas; Franke, Uwe] Daimler AG R&D, Boblingen, Germany; [Vazquez, David] Element AI, Montreal, PQ, Canada; [Lopez, Antonio M.] CVC, Barcelona, Spain; [Pollefeys, Marc] Swiss Fed Inst Technol, Zurich, Switzerland	Autonomous University of Barcelona; Centre de Visio per Computador (CVC); Swiss Federal Institutes of Technology Domain; ETH Zurich	Hernandez-Juarez, D (corresponding author), UAB, Barcelona, Spain.	dhernandez0@gmail.com	López, Antonio M/L-5303-2014; Moure, Juan Carlos/D-1942-2009; Pollefeys, Marc/I-7607-2013; espinosa, antonio/J-9691-2014; Vazquez Bermudez, David/M-1315-2014	López, Antonio M/0000-0002-6979-5783; Moure, Juan Carlos/0000-0001-6697-0331; espinosa, antonio/0000-0002-6460-3789; Vazquez Bermudez, David/0000-0002-2845-8158; Hernandez Juarez, Daniel/0000-0001-5878-1549	MINECO/AEI/FEDER, UE [TIN2017-84553-C2-1-R, TIN2017-88709-R]; Generalitat de Catalunya CERCA Program [2017-SGR-1597, 2017-SGR-313]; SEBAP; ICREA under the ICREA Academia Program	MINECO/AEI/FEDER, UE(Spanish Government); Generalitat de Catalunya CERCA Program; SEBAP; ICREA under the ICREA Academia Program	This work has been partially supported by Spanish TIN2017-84553-C2-1-R (MINECO/AEI/FEDER, UE). We also thank the Generalitat de Catalunya CERCA Program, the 2017-SGR-1597 and 2017-SGR-313 projects, as well as the ACCIO agency. We also acknowledge SEBAP for the internship funding program. Antonio M. Lopez acknowledges the financial support by the Spanish TIN2017-88709-R (MINECO/AEI/FEDER, UE), and by ICREA under the ICREA Academia Program. Finally, we thank Francisco Molero and Marc Garcia at CVC/UAB for the generation of the SYNTHIA-SF dataset.	[Anonymous], P INT C LEARN REPR I; [Anonymous], 2015, P BRIT MACH VIS C; [Anonymous], IEEE INT VEH S 4; [Anonymous], P 13 IAPR INT C MACH; [Anonymous], THESIS; [Anonymous], ICRA; Benenson R., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2010, DOI 10.1109/ICCVW.2011.6130495; Benenson R, 2012, LECT NOTES COMPUT SC, V7585, P11, DOI 10.1007/978-3-642-33885-4_2; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Cordts M, 2017, IMAGE VISION COMPUT, V68, P40, DOI 10.1016/j.imavis.2017.01.009; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Cordts M, 2014, LECT NOTES COMPUT SC, V8753, P172, DOI 10.1007/978-3-319-11752-2_14; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fink E, 2011, J EXP THEOR ARTIF IN, V23, P255, DOI 10.1080/0952813X.2010.505800; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; He H, 2013, IEEE INT C INT ROBOT, P3697, DOI 10.1109/IROS.2013.6696884; Hernandez-Juarez D., 2017, P BRIT MACH VIS C BM; Hernandez-Juarez D, 2017, IEEE WINT CONF APPL, P1054, DOI 10.1109/WACV.2017.122; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; JASCH M, 2018, J COMPUTERS, V13, P393; Kundu A, 2014, LECT NOTES COMPUT SC, V8694, P703, DOI 10.1007/978-3-319-10599-4_45; Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19; Li XF, 2016, IEEE INT VEH SYM, P1028; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Muffert M, 2014, 2014 CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P16, DOI 10.1109/CRV.2014.11; Oana I, 2016, INT C INTELL COMP CO, P217, DOI 10.1109/ICCP.2016.7737150; Pena D, 2016, INT CONF 3D VISION, P66, DOI 10.1109/3DV.2016.80; Pfeiffer D, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.51; Pfeiffer D, 2013, PROC CVPR IEEE, P297, DOI 10.1109/CVPR.2013.45; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Schneider L, 2017, LECT NOTES COMPUT SC, V10269, P98, DOI 10.1007/978-3-319-59126-1_9; Schneider L, 2016, IEEE INT VEH SYM, P110, DOI 10.1109/IVS.2016.7535373; Sengupta S, 2013, IEEE INT CONF ROBOT, P580, DOI 10.1109/ICRA.2013.6630632; Thrun S., 2002, EXPLORING ARTIFICIAL; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Zhang R, 2015, IEEE INT CONF ROBOT, P1850, DOI 10.1109/ICRA.2015.7139439; Ziegler J, 2014, IEEE INT VEH SYM, P450, DOI 10.1109/IVS.2014.6856581; Ziegler J, 2014, IEEE INTEL TRANSP SY, V6, P8, DOI 10.1109/MITS.2014.2306552	39	4	4	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1643	1658		10.1007/s11263-019-01226-9	http://dx.doi.org/10.1007/s11263-019-01226-9			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY		Green Submitted, hybrid, Green Published			2022-12-18	WOS:000492425300004
J	Khan, N; Akram, A; Mahmood, A; Ashraf, S; Murtaza, K				Khan, Nazar; Akram, Arbish; Mahmood, Arif; Ashraf, Sania; Murtaza, Kashif			Masked Linear Regression for Learning Local Receptive Fields for Facial Expression Synthesis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Expression; Face; Mapping; Synthesis; Regression; Masked; Local receptive field; Machine learning; Optimization; Quadratic; Convex; GAN; Adversarial; Generative; Discriminative; Ridge; Linear; Image-to-image translation	STATE	Compared to facial expression recognition, expression synthesis requires a very high-dimensional mapping. This problem exacerbates with increasing image sizes and limits existing expression synthesis approaches to relatively small images. We observe that facial expressions often constitute sparsely distributed and locally correlated changes from one expression to another. By exploiting this observation, the number of parameters in an expression synthesis model can be significantly reduced. Therefore, we propose a constrained version of ridge regression that exploits the local and sparse structure of facial expressions. We consider this model as masked regression for learning local receptive fields. In contrast to the existing approaches, our proposed model can be efficiently trained on larger image sizes. Experiments using three publicly available datasets demonstrate that our model is significantly better than regression, SVD based approaches, and kernelized regression in terms of mean-squared-error, visual quality as well as computational and spatial complexities. The reduction in the number of parameters allows our method to generalize better even after training on smaller datasets. The proposed algorithm is also compared with state-of-the-art GANs including Pix2Pix, CycleGAN, StarGAN and GANimation. These GANs produce photo-realistic results as long as the testing and the training distributions are similar. In contrast, our results demonstrate significant generalization of the proposed algorithm over out-of-dataset human photographs, pencil sketches and even animal faces.	[Khan, Nazar; Akram, Arbish; Ashraf, Sania; Murtaza, Kashif] PUCIT, Lahore, Pakistan; [Mahmood, Arif] Informat Technol Univ, Dept Comp Sci, Lahore, Pakistan		Khan, N (corresponding author), PUCIT, Lahore, Pakistan.	nazarkhan@pucit.edu.pk; phdcsfl8m002@pucit.edu.pk; arif.mahmood@itu.edu.pk; sashraf@pucit.edu.pk; kashifmurtaza@pucit.edu.pk	Khan, Nazar/AAH-8807-2019	Khan, Nazar/0000-0002-9470-2120; Mahmood, Arif/0000-0001-5986-9876				[Anonymous], 2011, HDB FACE RECOGNITION; Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600; Bermano AH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2546276; Bishop C.M, 2006, PATTERN RECOGN; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Coates A., 2011, ADV NEURAL INFORM PR, P2528, DOI DOI 10.1016/J.PSYCHRES.2009.03.008; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Costigan T, 2014, P 7 INT C MOTION GAM, P31; de la Hunty Miles, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3789, DOI 10.1109/ICPR.2010.923; Deng Z., 2008, DATA DRIVEN 3D FACIA, P1, DOI 10.1007/978-1-84628-907-1; Duong CN, 2016, PROC CVPR IEEE, P5772, DOI 10.1109/CVPR.2016.622; Ekman P., 2013, EMOTION HUMAN FACE G; Elaiwat S, 2016, PATTERN RECOGN, V49, P152, DOI 10.1016/j.patcog.2015.07.006; Georgakis C, 2016, IEEE T IMAGE PROCESS, V25, P2021, DOI 10.1109/TIP.2016.2539502; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Havaldar P, 2006, P SIGGRAPH 06 ACM SI, P5; Huang D, 2010, LECT NOTES COMPUT SC, V6312, P364, DOI 10.1007/978-3-642-15552-9_27; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jampour M., 2015, P 20 COMP VIS WINT W; Kim T, 2017, PR MACH LEARN RES, V70; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledig C., 2017, PROC CVPR IEEE, P4681, DOI [10.1109/CVPR.2017.19, DOI 10.1109/CVPR.2017.19]; Lee CS, 2006, INT C PATT RECOG, P497; Lee HS, 2009, IEEE T PATTERN ANAL, V31, P1102, DOI 10.1109/TPAMI.2008.286; Lin JR, 2011, J INF SCI ENG, V27, P337; Liu M. -Y., 2016, ADV NEURAL INFORM PR, P469; Liu MY, 2017, ADV NEUR IN, V30; Liu SY, 2014, ADV MECH ENG, DOI 10.1155/2014/868041; Liu ZC, 2001, COMP GRAPH, P271; Lucey P., 2010, P IEEE COMP SOC C CO, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Lundqvist D., 1998, KAROLINSKA DIRECTED; Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949; Mirza M., 2014, ARXIV PREPRINT ARXIV; Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976; Patel NM., 2010, J COMPUT APPL, V3, P34, DOI [10.5120/719-1011, DOI 10.5120/719-1011]; PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465; Pighin F., 2006, ACM SIGGRAPH 2006 CO, P19; Pumarola A, 2020, INT J COMPUT VISION, V128, P698, DOI 10.1007/s11263-019-01210-3; Rizzo AA, 2001, CYBERPSYCHOL BEHAV, V4, P471, DOI 10.1089/109493101750527033; Saragih J. M., 2011, INT C AUT FAC GEST R, P117, DOI DOI 10.1109/FG.2011.5771383; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Shen W., 2017, P IEEE C COMP VIS PA, P4030; Susskind Joshua M., 2008, Affective Computing. Focus on Emotion Expression, Synthesis and Recognition, P421; Suwajanakorn S, 2015, IEEE I CONF COMP VIS, P3952, DOI 10.1109/ICCV.2015.450; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; THIES J, 2016, PROC CVPR IEEE, P2387, DOI DOI 10.1109/CVPR.2016.262; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Wang HC, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P958, DOI 10.1109/ICCV.2003.1238452; Wei W, 2016, PATTERN RECOGN, V49, P115, DOI 10.1016/j.patcog.2015.08.004; Zeiler M. D., 2011, ADV NEURAL INFORM PR, P1629; Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52; Zhang G., 2018, P EUR C COMP VIS ECC, P417; Zhang QS, 2006, IEEE T VIS COMPUT GR, V12, P48, DOI 10.1109/TVCG.2006.9; Zhang YN, 2012, NEUROCOMPUTING, V89, P21, DOI 10.1016/j.neucom.2012.01.019; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	62	4	4	1	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1433	1454		10.1007/s11263-019-01256-3	http://dx.doi.org/10.1007/s11263-019-01256-3		NOV 2019	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL9ON		Green Submitted			2022-12-18	WOS:000493640700001
J	Croce, F; Rauber, J; Hein, M				Croce, Francesco; Rauber, Jonas; Hein, Matthias			Scaling up the Randomized Gradient-Free Adversarial Attack Reveals Overestimation of Robustness Using Established Attacks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Adversarial attacks; Adversarial robustness; White-box attacks; Gradient-free attacks		Modern neural networks are highly non-robust against adversarial manipulation. A significant amount of work has been invested in techniques to compute lower bounds on robustness through formal guarantees and to build provably robust models. However, it is still difficult to get guarantees for larger networks or robustness against larger perturbations. Thus attack strategies are needed to provide tight upper bounds on the actual robustness. We significantly improve the randomized gradient-free attack for ReLU networks (Croce and Hein in GCPR, 2018), in particular by scaling it up to large networks. We show that our attack achieves similar or significantly smaller robust accuracy than state-of-the-art attacks like PGD or the one of Carlini and Wagner, thus revealing an overestimation of the robustness by these state-of-the-art methods. Our attack is not based on a gradient descent scheme and in this sense gradient-free, which makes it less sensitive to the choice of hyperparameters as no careful selection of the stepsize is required.	[Croce, Francesco; Rauber, Jonas; Hein, Matthias] Univ Tubingen, Dept Comp Sci, Tubingen, Germany	Eberhard Karls University of Tubingen	Croce, F (corresponding author), Univ Tubingen, Dept Comp Sci, Tubingen, Germany.	francesco91.croce@gmail.com			BMBF through the Tubingen AI Center [FKZ: 01IS18039A]; DFG [389792660, TRR 248]; Bosch Research Foundation [T113/30057/17]; International Max Planck Research School for Intelligent Systems (IMPRS-IS); Excellence Cluster "Machine Learning-New Perspectives for Science"	BMBF through the Tubingen AI Center(Federal Ministry of Education & Research (BMBF)); DFG(German Research Foundation (DFG)); Bosch Research Foundation; International Max Planck Research School for Intelligent Systems (IMPRS-IS); Excellence Cluster "Machine Learning-New Perspectives for Science"	F. C. and M. H. acknowledge support from the BMBF through the Tubingen AI Center (FKZ: 01IS18039A) and by the DFG via Grant 389792660 as part of TRR 248 and the Excellence Cluster "Machine Learning-New Perspectives for Science". J. R. acknowledges support from the Bosch Research Foundation (Stifterverband, T113/30057/17) and the International Max Planck Research School for Intelligent Systems (IMPRS-IS).	ARORA R, 2018, ICLR; Athalye A, 2018, PR MACH LEARN RES, V80; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Brendel W., 2018, PROC 6 INT C LEARN R; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; CROCE F, 2019, AISTATS; CROCE F, 2018, GCPR; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hein M, 2017, NIPS 17; HUANG G, 2016, ICLR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Lowd Daniel, 2005, KDD; Madry Aleksander, 2017, ARXIV; McDaniel Patrick, 2017, ARXIV PREPRINT ARXIV; Mirman M, 2018, PR MACH LEARN RES, V80; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Mosbach Marius, 2018, NEURIPS 2018 WORKSH; Nair Vinod, 2014, THE CIFAR 10 DATASET; NARODYTSKA N, 2016, CVPR 2017 WORKSH; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Raghunathan Aditi, 2018, ARXIV180109344; Rauber Jonas, 2017, REL MACH LEARN WILD, P6, DOI DOI 10.21105/JOSS.02607; SCHOTT L, 2018, ICLR; Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016; SZEGEDY C, 2014, ICLR, P2503; TJENG V, 2019, ARXIV171107356V3; Weng TW, 2018, PR MACH LEARN RES, V80; Wong E, 2018, ADV NEUR IN, V31; Wong E, 2018, PR MACH LEARN RES, V80; Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017	38	4	4	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		1028	1046		10.1007/s11263-019-01213-0	http://dx.doi.org/10.1007/s11263-019-01213-0		OCT 2019	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Green Submitted			2022-12-18	WOS:000489440600001
J	Derkach, D; Ruiz, A; Sukno, FM				Derkach, Dmytro; Ruiz, Adria; Sukno, Federico M.			Tensor Decomposition and Non-linear Manifold Modeling for 3D Head Pose Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D head pose; Manifold learning; Tensor decomposition	OBJECT RECOGNITION; SEPARATING STYLE; FACTORIZATION	Head pose estimation is a challenging computer vision problem with important applications in different scenarios such as human-computer interaction or face recognition. In this paper, we present a 3D head pose estimation algorithm based on non-linear manifold learning. A key feature of the proposed approach is that it allows modeling the underlying 3D manifold that results from the combination of rotation angles. To do so, we use tensor decomposition to generate separate subspaces for each variation factor and show that each of them has a clear structure that can be modeled with cosine functions from a unique shared parameter per angle. Such representation provides a deep understanding of data behavior. We show that the proposed framework can be applied to a wide variety of input features and can be used for different purposes. Firstly, we test our system on a publicly available database, which consists of 2D images and we show that the cosine functions can be used to synthesize rotated versions from an object from which we see only a 2D image at a specific angle. Further, we perform 3D head pose estimation experiments using other two types of features: automatic landmarks and histogram-based 3D descriptors. We evaluate our approach on two publicly available databases, and demonstrate that angle estimations can be performed by optimizing the combination of these cosine functions to achieve state-of-the-art performance.	[Derkach, Dmytro; Sukno, Federico M.] Pompeu Fabra Univ, Dept Informat & Commun Technol, Barcelona, Spain; [Ruiz, Adria] Univ Grenoble Alpes, Inst Engn, CNRS, INRIA, F-38000 Grenoble, France	Pompeu Fabra University; Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria	Sukno, FM (corresponding author), Pompeu Fabra Univ, Dept Informat & Commun Technol, Barcelona, Spain.	dmytro.derkach@upf.edu; adria.ruiz-ovejero@inria.fr; federico.sukno@upf.edu	Ruiz, Adria/AAU-2492-2021; Ruiz, Adria/AAA-8783-2021; Sukno, Federico/AAM-4440-2021	Sukno, Federico/0000-0002-2029-1576	Spanish Ministry of Economy and Competitiveness [TIN2017-90124-P]; Ramon y Cajal programme; Maria de Maeztu Units of Excellence Programme [MDM-2015-0502]; ANR [ANR-16-CE23-0006]	Spanish Ministry of Economy and Competitiveness(Spanish Government); Ramon y Cajal programme(Spanish Government); Maria de Maeztu Units of Excellence Programme; ANR(French National Research Agency (ANR))	This work is partly supported by the Spanish Ministry of Economy and Competitiveness under Project Grant TIN2017-90124-P, the Ramon y Cajal programme, and the Maria de Maeztu Units of Excellence Programme (MDM-2015-0502). Adria Ruiz work is partially funded by ANR grant ANR-16-CE23-0006.	Ahn B., 2014, P AS C COMP VIS, P82; Alex M, 2002, LECT NOTES COMPUT SC, V2350, P447; Bakry A, 2014, LECT NOTES COMPUT SC, V8692, P434, DOI 10.1007/978-3-319-10593-2_29; Balasubramanian VN, 2007, PROC CVPR IEEE, P2377; Baltrusaitis T, 2012, PROC CVPR IEEE, P2610, DOI 10.1109/CVPR.2012.6247980; Barros JMD, 2018, IEEE WINT CONF APPL, P2028, DOI 10.1109/WACV.2018.00224; BenAbdelkader C, 2010, LECT NOTES COMPUT SC, V6316, P518, DOI 10.1007/978-3-642-15567-3_38; Bergqvist G, 2010, IEEE SIGNAL PROC MAG, V27, P151, DOI 10.1109/MSP.2010.936030; Bingjie Wang, 2013, 2013 Seventh International Conference on Image and Graphics (ICIG), P650, DOI 10.1109/ICIG.2013.133; Borghi G., 2019, IEEE T PATTERN ANAL; Borghi G, 2017, PROC CVPR IEEE, P5494, DOI 10.1109/CVPR.2017.583; Breitenstein MD, 2008, PROC CVPR IEEE, P3613; BYRD RH, 1994, MATH PROGRAM, V63, P129, DOI 10.1007/BF01582063; Chen JW, 2016, IEEE SW SYMP IMAG, P65, DOI 10.1109/SSIAI.2016.7459176; Comon P, 2014, IEEE SIGNAL PROC MAG, V31, P44, DOI 10.1109/MSP.2014.2298533; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Derkach D, 2018, INT CONF 3D VISION, P505, DOI 10.1109/3DV.2018.00064; Derkach D, 2017, IEEE INT CONF AUTOMA, P820, DOI 10.1109/FG.2017.104; Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11; Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0; Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224; Fu Y, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P3; Ghiass Reza Shoja, 2015, P 2 WORKSH COMP MOD, P25; Gu JW, 2017, PROC CVPR IEEE, P1531, DOI 10.1109/CVPR.2017.167; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Lathuiliere S, 2020, IEEE T PATTERN ANAL, V42, P2065, DOI 10.1109/TPAMI.2019.2910523; Lathuiliere S, 2017, PROC CVPR IEEE, P7149, DOI 10.1109/CVPR.2017.756; Lee D, 2017, IEEE T VEH TECHNOL, P1; Lee D, 2015, IEEE I CONF COMP VIS, P1958, DOI 10.1109/ICCV.2015.227; Li DQ, 2014, PATTERN RECOGN, V47, P525, DOI 10.1016/j.patcog.2013.07.019; Li SN, 2016, IEEE T PATTERN ANAL, V38, P1922, DOI 10.1109/TPAMI.2015.2500221; Liu XB, 2016, IEEE IMAGE PROC, P1289, DOI 10.1109/ICIP.2016.7532566; Liu XY, 2010, IEEE IMAGE PROC, P3277, DOI 10.1109/ICIP.2010.5652540; Lusi I, 2017, IEEE INT CONF AUTOMA, P809, DOI 10.1109/FG.2017.102; Lusi I., 2016, INT WORKSH FAC FAC E, P137; Lusi I, 2016, LECT NOTES COMPUT SC, V9915, P325, DOI 10.1007/978-3-319-49409-8_26; Martin Manuel, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P641, DOI 10.1109/3DV.2014.54; Meyer GP, 2015, IEEE I CONF COMP VIS, P3649, DOI 10.1109/ICCV.2015.416; Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106; Nene, 1996, CUCS00596 COL U DEP; Padeleris P., 2012, 2012 IEEE COMP SOC C, P42, DOI DOI 10.1109/CVPRW.2012.6239236; Papazov C, 2015, PROC CVPR IEEE, P4722, DOI 10.1109/CVPR.2015.7299104; Patacchiola M, 2017, PATTERN RECOGN, V71, P132, DOI 10.1016/j.patcog.2017.06.009; Peng X, 2014, INT C PATT RECOG, P1800, DOI 10.1109/ICPR.2014.316; Raytchev B, 2004, INT C PATT RECOG, P462, DOI 10.1109/ICPR.2004.1333802; Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281; Rusu RB, 2009, IEEE INT CONF ROBOT, P1848; Schmidt M., 2012, MINFUNC UNCONSTRAINE; Seemann E, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P626, DOI 10.1109/AFGR.2004.1301603; Sukno F. M, 2013, P INT C COMP GRAPH T, P7; Sukno FM, 2015, IEEE T CYBERNETICS, V45, P1717, DOI 10.1109/TCYB.2014.2359056; Sukno FM, 2012, LECT NOTES COMPUT SC, V7432, P92, DOI 10.1007/978-3-642-33191-6_10; Sun Y, 2008, INT C PATT RECOG, P104; Sundararajan Kalaivani, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P50, DOI 10.1109/CVPRW.2015.7301354; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Takallou HM, 2014, IET COMPUT VIS, V8, P54, DOI 10.1049/iet-cvi.2012.0217; Tan DJ, 2018, INT J COMPUT VISION, V126, P158, DOI 10.1007/s11263-017-0988-8; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Tenenbaum JB, 1997, ADV NEUR IN, V9, P662; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Tulyakov S, 2014, INT C PATT RECOG, P2263, DOI 10.1109/ICPR.2014.393; Wang C, 2017, HEAD POSE ESTIMATION; Wang C, 2014, NEURAL NETWORKS, V53, P15, DOI 10.1016/j.neunet.2014.01.009; Wang K, 2018, IEEE INT CONF AUTOMA, P540, DOI 10.1109/FG.2018.00087; Wang M., 2017, P IEEE C COMP VIS PA, P4592; Wang YJ, 2019, PATTERN RECOGN, V94, P196, DOI 10.1016/j.patcog.2019.05.026; Xu YY, 2015, INVERSE PROBL IMAG, V9, P601, DOI 10.3934/ipi.2015.9.601; Yu Y, 2017, IEEE INT CONF AUTOMA, P711, DOI 10.1109/FG.2017.90; Zhang HP, 2015, COMPUT VIS IMAGE UND, V139, P89, DOI 10.1016/j.cviu.2015.03.014; Zhao QB, 2015, IEEE T PATTERN ANAL, V37, P1751, DOI 10.1109/TPAMI.2015.2392756; Zhu Y., 2014, INT J SIGNAL PROCESS, V7, P123, DOI [10.14257/ijsip.2014.7.3.11, DOI 10.14257/IJSIP.2014.7.3.11]	72	4	4	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1565	1585		10.1007/s11263-019-01208-x	http://dx.doi.org/10.1007/s11263-019-01208-x			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL		Green Submitted			2022-12-18	WOS:000485320300010
J	Penne, R; Ribbens, B; Roios, P				Penne, Rudi; Ribbens, Bart; Roios, Pedro			An Exact Robust Method to Localize a Known Sphere by Means of One Image	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Extrinsic calibration; Robust localization; Projective geometry	CAMERA CALIBRATION	In this article we provide a very robust algorithm to compute the position of the center of a sphere with known radius from one image by a calibrated camera. To our knowledge it is the first time that an exact sphere localization formula is published that only uses the (pixel) area and the ellipse center of the sphere image. Other authors either derived an approximation formula or followed the less robust and more time consuming procedure of fitting an ellipse through the detected edge pixels. Our method is analytic and deterministic, making use of the unique positive real tool of a cubic equation. We observe that the proposed area method is significantly more accurate and precise than an ellipse fitting method. Furthermore, we investigate in what conditions for sphere images the proposed exact method is preferable to the robust approximation method. These observations are validated by virtual, synthetic and real experiments.	[Penne, Rudi] Univ Antwerp, Fac Appl Engn, Dept Math, B-2020 Antwerp, Belgium; [Ribbens, Bart; Roios, Pedro] Univ Antwerp, Fac Appl Engn, B-2020 Antwerp, Belgium	University of Antwerp; University of Antwerp	Penne, R (corresponding author), Univ Antwerp, Fac Appl Engn, Dept Math, B-2020 Antwerp, Belgium.	rudi.penne@uantwerpen.be		Ribbens, Bart/0000-0001-5882-2531				Agrawal M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P782; [Anonymous], ABS150406375 CORR; ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; Barbeau, 1989, POLYNOMIALS; BEARDSLEY P, 1992, LECT NOTES COMPUT SC, V588, P312; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Dandelin Germinal Pierre, 1822, NOUVEAUX MEMOIRES AC, VII, P171; Daucher N., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P449; Fitzgibbon A., 1996, ICPR, P253; Guan JZ, 2015, SENSORS-BASEL, V15, P18985, DOI 10.3390/s150818985; HO CT, 1995, PATTERN RECOGN, V28, P117, DOI 10.1016/0031-3203(94)00077-Y; Lu Y, 2010, COMPUT VIS IMAGE UND, V114, P8, DOI 10.1016/j.cviu.2009.09.001; Maxon, 2009, CIN 4D; Ouellet JN, 2009, MACH VISION APPL, V21, P59, DOI 10.1007/s00138-008-0141-3; Peng-Yeng Yin, 1994, Journal of Electronic Imaging, V3, P20, DOI 10.1117/12.163973; PENNA MA, 1991, IEEE T PATTERN ANAL, V13, P1240, DOI 10.1109/34.107007; Penne R, 2015, LECT NOTES COMPUT SC, V9386, P501, DOI 10.1007/978-3-319-25903-1_43; Sun JH, 2015, OPT LASER TECHNOL, V65, P83, DOI 10.1016/j.optlastec.2014.07.009; Teramoto H., 2002, Proceedings of the Fifth Asian Conference on Computer Vision, P499; Xie YH, 2002, INT C PATT RECOG, P957, DOI 10.1109/ICPR.2002.1048464; Zhang H, 2007, IEEE T PATTERN ANAL, V29, P499, DOI 10.1109/TPAMI.2007.45; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	23	4	4	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					1012	1024		10.1007/s11263-018-1139-6	http://dx.doi.org/10.1007/s11263-018-1139-6			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM		Green Accepted			2022-12-18	WOS:000474559000003
J	Duong, CN; Quach, KG; Luu, K; Le, THN; Savvides, M; Bui, TD				Duong, Chi Nhan; Quach, Kha Gia; Luu, Khoa; Le, T. Hoang Ngan; Savvides, Marios; Bui, Tien D.			Learning from Longitudinal Face DemonstrationWhere Tractable Deep Modeling Meets Inverse Reinforcement Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face age progression; Deep generative models; Face aging; Subject-dependent deep aging; Tractable graphical probabilistic models	HUMAN AGE ESTIMATION; APPEARANCE; SHAPE	This paper presents a novel subject-dependent deep aging path (SDAP), which inherits the merits of both generative probabilistic modeling and inverse reinforcement learning to model the facial structures and the longitudinal face aging process of a given subject. The proposed SDAP is optimized using tractable log-likelihood objective functions with convolutional neural networks (CNNs) based deep feature extraction. Instead of applying a fixed aging development path for all input faces and subjects, SDAP is able to provide the most appropriate aging development path for individual subject that optimizes the reward aging formulation. Unlike previous methods that can take only one image as the input, SDAP further allows multiple images as inputs, i.e. all information of a subject at either the same or different ages, to produce the optimal aging path for the given subject. Finally, SDAP allows efficiently synthesizing in-the-wild aging faces. The proposed model is experimented in both tasks of face aging synthesis and cross-age face verification. The experimental results consistently show SDAP achieves the state-of-the-art performance on numerous face aging databases, i.e. FG-NET, MORPH, aging faces in the wild (AGFW), and cross-age celebrity dataset (CACD). Furthermore, we also evaluate the performance of SDAP on large-scale Megaface challenge to demonstrate the advantages of the proposed solution.	[Duong, Chi Nhan; Quach, Kha Gia; Bui, Tien D.] Concordia Univ, Dept Comp Sci & Software Engn, Montreal, PQ, Canada; [Luu, Khoa] Univ Arkansas, Comp Sci & Comp Engn Dept, Fayetteville, AR 72701 USA; [Le, T. Hoang Ngan; Savvides, Marios] Carnegie Mellon Univ, CyLab Biometr Ctr, Pittsburgh, PA 15213 USA; [Le, T. Hoang Ngan; Savvides, Marios] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA	Concordia University - Canada; University of Arkansas System; University of Arkansas Fayetteville; Carnegie Mellon University; Carnegie Mellon University	Duong, CN (corresponding author), Concordia Univ, Dept Comp Sci & Software Engn, Montreal, PQ, Canada.	dcnhan@ieee.org; kquach@ieee.org; khoaluu@uark.edu; thihoanl@andrew.cmu.edu; msavvid@ri.cmu.edu; bui@encs.concordia.ca	Quach, Kha Gia/Z-5464-2019; Luu, Khoa/AAQ-8540-2021	Quach, Kha Gia/0000-0001-6952-306X; Luu, Khoa/0000-0003-2104-0901				Agustsson E, 2017, IEEE I CONF COMP VIS, P1652, DOI 10.1109/ICCV.2017.182; Angulu R, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0278-6; [Anonymous], 2004, FGNET AGING DATABASE; Antipov G., 2017, ARXIV170201983; Attia Alexandre, 2018, ARXIV180106503; BURT DM, 1995, P ROY SOC B-BIOL SCI, V259, P137, DOI 10.1098/rspb.1995.0021; Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437; Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49; Chen SX, 2017, PROC CVPR IEEE, P742, DOI 10.1109/CVPR.2017.86; Chen X, 2011, IEEE INT C BIOINFORM, P3, DOI 10.1109/BIBM.2011.12; Duong CN, 2017, IEEE I CONF COMP VIS, P3755, DOI 10.1109/ICCV.2017.403; Duong N, 2015, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2015.7299111; Choi SE, 2011, PATTERN RECOGN, V44, P1262, DOI 10.1016/j.patcog.2010.12.005; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Duan Y, 2016, PR MACH LEARN RES, V48; Duong C.N., 2011, IEEE INT C AC SPEECH, P2032; Duong CN, 2016, PROC CVPR IEEE, P5772, DOI 10.1109/CVPR.2016.622; Finn C, 2016, PR MACH LEARN RES, V48; FU Y, 1976, TPAMI, V32, P1955, DOI DOI 10.1109/TPAMI.2010.36; Geng X., 2008, P 16 ACM INT C MULT, P721; Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733; Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280; Guo GD, 2014, PROC CVPR IEEE, P4257, DOI 10.1109/CVPR.2014.542; Guo GD, 2012, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2012.6247972; Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681; HAN H, 2018, TPAMI, V40, P2597, DOI DOI 10.1109/TPAMI.2017.2738004; Huo Z., 2016, P IEEE C COMP VIS PA, P17; Juefei-Xu F, 2015, IEEE T IMAGE PROCESS, V24, P4780, DOI 10.1109/TIP.2015.2468173; Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527; KEMELMACHERSHLIZER, 2014, PROC CVPR IEEE, P3334, DOI DOI 10.1109/CVPR.2014.426; Kwon YH, 1999, COMPUT VIS IMAGE UND, V74, P1, DOI 10.1006/cviu.1997.0549; Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091; Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553; Le THN, 2015, PATTERN RECOGN, V48, P3843, DOI 10.1016/j.patcog.2015.05.021; Levine S, 2014, ADV NEUR IN, V27; Li K, 2018, PROC CVPR IEEE, P399, DOI 10.1109/CVPR.2018.00049; Liu W., 2017, P IEEE C COMPUTER VI, P212; LOU Z, 2018, TPAMI, V40, P365, DOI DOI 10.1109/TPAMI.2017.2679739; Luu K, 2009, 2009 IEEE 3 INT C BI, P1, DOI DOI 10.1109/BTAS.2009.5339053; Luu K., 2010, 23 CAN C ART INT CAI; NIU ZX, 2016, PROC CVPR IEEE, P4920, DOI DOI 10.1109/CVPR.2016.532; Pan HY, 2018, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR.2018.00554; Patterson E., 2006, INT C VIS IM IM PROC, V171, pC176; Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341; Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3; ROWLAND DA, 1995, IEEE COMPUT GRAPH, V15, P70, DOI 10.1109/38.403830; Shen W, 2018, PROC CVPR IEEE, P2304, DOI 10.1109/CVPR.2018.00245; Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452; SUO J, 2012, PAMI, V34, P2083, DOI DOI 10.1109/TPAMI.2012.22; Suo JL, 2010, IEEE T PATTERN ANAL, V32, P385, DOI 10.1109/TPAMI.2009.39; Taylor Graham, 2009, P 26 ANN INT C MACH, DOI DOI 10.1145/1553374.1553505; Tsai MH, 2014, MULTIMED TOOLS APPL, V72, P801, DOI 10.1007/s11042-013-1399-7; Wang F, 2017, IEEE INT CONF AUTOMA, P173, DOI 10.1109/FG.2017.30; Wang W, 2016, PROC CVPR IEEE, P2378, DOI 10.1109/CVPR.2016.261; Wang ZW, 2018, PROC CVPR IEEE, P7939, DOI 10.1109/CVPR.2018.00828; Xu J., 2011, INT JOINT C BIOM IJC; YAN S, 1971, TIP, V18, P202, DOI DOI 10.1109/TIP.2008.2006400; Yan SC, 2008, INT CONF ACOUST SPEE, P737; Yang HY, 2016, IEEE T IMAGE PROCESS, V25, P2493, DOI 10.1109/TIP.2016.2547587; Yang H, 2018, IEEE CONF COMM NETW; Yang X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P344, DOI 10.1109/ICCVW.2015.53; Yi D, 2015, LECT NOTES COMPUT SC, V9005, P144, DOI 10.1007/978-3-319-16811-1_10; Zhang Y, 2010, PROC CVPR IEEE, P2622, DOI 10.1109/CVPR.2010.5539975; Zhang ZZ, 2017, PROC CVPR IEEE, P3549, DOI 10.1109/CVPR.2017.378	64	4	4	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		957	971		10.1007/s11263-019-01165-5	http://dx.doi.org/10.1007/s11263-019-01165-5			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD		Green Submitted			2022-12-18	WOS:000468525900022
J	Ahmad, S; Cheong, LF				Ahmad, Shahzor; Cheong, Loong-Fah			Robust Detection and Affine Rectification of Planar Homogeneous Texture for Scene Understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Homogeneous texture; Planar rectification; Invariant texture detection; Scene geometric layout; Scene classification; Deep features	SURFACE ORIENTATION; CLASSIFICATION; FEATURES; CATEGORIES; GEOMETRY; DATABASE; MODELS; SCALE; SHAPE	Man-made environments tend to be abundant with planar homogeneous texture, which manifests as regularly repeating scene elements along a plane. In this work, we propose to exploit such structure to facilitate high-level scene understanding. By robustly fitting a texture projection model to optimal dominant frequency estimates in image patches, we arrive at a projective-invariant method to localize such generic, semantically meaningful regions in multi-planar scenes. The recovered projective parameters also allow an affine-ambiguous rectification in real-world images marred with outliers, room clutter, and photometric severities. Comprehensive qualitative and quantitative evaluations are performed that show our method outperforms existing representative work for both rectification and detection. The potential of homogeneous texture for two scene understanding tasks is then explored. Firstly, in environments where vanishing points cannot be reliably detected, or the Manhattan assumption is not satisfied, homogeneous texture detected by the proposed approach is shown to provide alternative cues to obtain a scene geometric layout. Second, low-level feature descriptors extracted upon affine rectification of detected texture are found to be not only class-discriminative but also complementary to features without rectification, improving recognition performance on the 67-category MIT benchmark of indoor scenes. One of our configurations involving deep ConvNet features outperforms most current state-of-the-art work on this dataset, achieving a classification accuracy of 76.90%. The approach is additionally validated on a set of 31 categories (mostly outdoor man-made environments exhibiting regular, repeating structure), being a subset of the large-scale Places2 scene dataset.	[Ahmad, Shahzor] Natl Univ Sci & Technol, Coll Elect & Mech Engn, Islamabad, Pakistan; [Cheong, Loong-Fah] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore	National University of Sciences & Technology - Pakistan; National University of Singapore	Ahmad, S (corresponding author), Natl Univ Sci & Technol, Coll Elect & Mech Engn, Islamabad, Pakistan.	shahzor.ahmad@gmail.com; eleclf@nus.edu.sg		Ahmad, Shahzor/0000-0002-4319-5851				Ahmad S, 2016, LECT NOTES COMPUT SC, V9906, P35, DOI 10.1007/978-3-319-46475-6_3; Aiger D, 2012, COMPUT GRAPH FORUM, V31, P439, DOI 10.1111/j.1467-8659.2012.03023.x; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018; Bappy J. H., 2016, P INT C PATT REC; BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3113, DOI 10.1109/CVPR.2011.5995551; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chum O., 2010, P AS C COMP VIS, P347; Cimpoi M., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299007; Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461; Collins T., 2010, P 3D DAT PROC VIS TR; Coughlan J.M., 1999, P ICCV, V2, P941, DOI DOI 10.1109/ICCV.1999.790349; Criminsi A., 2000, P BRIT MACH VIS C, P8291; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Doersch Carl, 2013, NIPS; Donahue J, 2014, PR MACH LEARN RES, V32; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; HAVLICEK JP, 1992, CONFERENCE RECORD OF THE TWENTY-SIXTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P805, DOI 10.1109/ACSSC.1992.269161; Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411; Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y; Hong W, 2004, INT J COMPUT VISION, V60, P241, DOI 10.1023/B:VISI.0000036837.76476.10; Huang YZ, 2014, IEEE T PATTERN ANAL, V36, P493, DOI 10.1109/TPAMI.2013.113; Jia Y., 2014, P 22 ACM INT C MULT, P675; Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krumm J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P284, DOI 10.1109/CVPR.1992.223262; Kulkarni P, 2016, LECT NOTES COMPUT SC, V9912, P329, DOI 10.1007/978-3-319-46484-8_20; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Leung T., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P546, DOI 10.1007/BFb0015565; Lian XC, 2010, LECT NOTES COMPUT SC, V6314, P157, DOI 10.1007/978-3-642-15561-1_12; Lin D, 2014, PROC CVPR IEEE, P3726, DOI 10.1109/CVPR.2014.476; Liu XQ, 2010, IEEE T PATTERN ANAL, V32, P1182, DOI 10.1109/TPAMI.2009.120; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383; Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z; Petkov N, 1997, BIOL CYBERN, V76, P83, DOI 10.1007/s004220050323; Picard RW, 1996, IBM SYST J, V35, P292, DOI 10.1147/sj.353.0292; Pritts J, 2014, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2014.380; Qi M., 2016, P INT C IM PROC; Quan YH, 2014, PROC CVPR IEEE, P160, DOI 10.1109/CVPR.2014.28; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Renninger LW, 2004, VISION RES, V44, P2301, DOI 10.1016/j.visres.2004.04.006; Ribeiro E, 2000, IMAGE VISION COMPUT, V18, P619, DOI 10.1016/S0262-8856(99)00064-5; Rosenholtz R, 1997, VISION RES, V37, P2283, DOI 10.1016/S0042-6989(96)00121-6; Rother C., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P382; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schaffalitzky F., 1998, P BRIT MACH VIS C, P165; Shaw D., 2006, P EUR C COMP VIS WOR; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; Super B. J., 1991, P SPIE VIS COMM IM P; SUPER BJ, 1995, PATTERN RECOGN, V28, P729, DOI 10.1016/0031-3203(94)00140-H; SUPER BJ, 1995, IEEE T PATTERN ANAL, V17, P333, DOI 10.1109/34.385983; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tuytelaars T, 2003, IEEE T PATTERN ANAL, V25, P418, DOI 10.1109/TPAMI.2003.1190569; Varma M, 2002, LECT NOTES COMPUT SC, V2352, P255; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Wu CC, 2010, LECT NOTES COMPUT SC, V6312, P142; Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224; Wu RB, 2015, IEEE I CONF COMP VIS, P1287, DOI 10.1109/ICCV.2015.152; Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y; Xie LX, 2014, PROC CVPR IEEE, P3734, DOI 10.1109/CVPR.2014.477; Yang JC, 2010, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2010.5539958; Yu SX, 2008, PROC CVPR IEEE, P198; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4; Zhang JG, 2003, PATTERN RECOGN, V36, P657, DOI 10.1016/S0031-3203(02)00099-7; Zhang W, 2003, FIRST IEEE INTERNATIONAL WORKSHOP ON HIGHER-LEVEL KNOWLEDGE IN 3D MODELING AND MOTION ANALYSIS, PROCEEDINGS, P83; Zhang Zhengdong, 2010, P AS C COMP VIS; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561; Zhou B., 2014, P NEUR INF PROC SYST; Zhou Bolei, 2016, PLACES IMAGE DATABAS; Zuo Z, 2014, LECT NOTES COMPUT SC, V8689, P552, DOI 10.1007/978-3-319-10590-1_36	84	4	4	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2018	126	8					822	854		10.1007/s11263-018-1078-2	http://dx.doi.org/10.1007/s11263-018-1078-2			33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GL5ZB					2022-12-18	WOS:000437253500003
J	Ferrante, E; Paragios, N				Ferrante, Enzo; Paragios, Nikos			Graph-Based Slice-to-Volume Deformable Registration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Slice-to-volume registration; Graphical models; Deformable registration; Discrete optimization	MEDICAL IMAGE REGISTRATION; ULTRASOUND; BRAIN; MR; 2D	Deformable image registration is a fundamental problem in computer vision and medical image computing. In this paper we investigate the use of graphical models in the context of a particular type of image registration problem, known as slice-to-volume registration. We introduce a scalable, modular and flexible formulation that can accommodate low-rank and high order terms, that simultaneously selects the plane and estimates the in-plane deformation through a single shot optimization approach. The proposed framework is instantiated into different variants seeking either a compromise between computational efficiency (soft plane selection constraints and approximate definition of the data similarity terms through pair-wise components) or exact definition of the data terms and the constraints on the plane selection. Simulated and real-data in the context of ultrasound and magnetic resonance registration (where both framework instantiations as well as different optimization strategies are considered) demonstrate the potentials of our method.	[Ferrante, Enzo; Paragios, Nikos] Univ Paris Saclay, INRIA, Ctr Visual Comp, Cent Supelec, Paris, France; [Ferrante, Enzo] Imperial Coll London, Dept Comp, Biomed Image Anal BioMedIA Grp, London, England; [Paragios, Nikos] TheraPanacea, Paris, France	Inria; UDICE-French Research Universities; Universite Paris Saclay; Imperial College London	Ferrante, E (corresponding author), Univ Paris Saclay, INRIA, Ctr Visual Comp, Cent Supelec, Paris, France.; Ferrante, E (corresponding author), Imperial Coll London, Dept Comp, Biomed Image Anal BioMedIA Grp, London, England.	eferrante@sinc.unl.edu.ar	Ferrante, Enzo/AAU-6518-2020	Ferrante, Enzo/0000-0002-8500-788X	European Research Council Starting Grant Diocles [ERC-STG-259112]	European Research Council Starting Grant Diocles	This research was partially supported by European Research Council Starting Grant Diocles (ERC-STG-259112). We thank Mihir Sahasrabudhe for proof-reading the paper, and Puneet Kumar Dokania, Vivien Fecamp and Jorg Kappes for helpful discussions.	Andres B, 2012, LECT NOTES COMPUT SC, V7578, P154, DOI 10.1007/978-3-642-33786-4_12; Baudin PY, 2013, LECT NOTES COMPUT SC, V8151, P219, DOI 10.1007/978-3-642-40760-4_28; BESAG J, 1986, J R STAT SOC B, V48, P259; Birkfellner W, 2007, MED PHYS, V34, P246, DOI 10.1118/1.2401661; Chandler AG, 2008, J CARDIOVASC MAGN R, V10, DOI 10.1186/1532-429X-10-13; Eresen A, 2014, IEEE ENG MED BIO, P6418, DOI 10.1109/EMBC.2014.6945097; Estepar RS, 2009, INT J COMPUT ASS RAD, V4, P549, DOI 10.1007/s11548-009-0369-z; Fei Baowei, 2002, Comput Aided Surg, V7, P257, DOI 10.3109/10929080209146034; Ferrante E., 2015, IEEE INT S BIOM IM N; Ferrante E., 2015, INT J COMPUTER ASSIS, V10; Ferrante E, 2017, MED IMAGE ANAL, V39, P101, DOI 10.1016/j.media.2017.04.010; Ferrante E, 2013, LECT NOTES COMPUT SC, V8151, P163, DOI 10.1007/978-3-642-40760-4_21; Gill S., 2008, P 20 INT C SOC MED I, P154; Glocker B., 2010, THESIS; Glocker B, 2011, ANNU REV BIOMED ENG, V13, P219, DOI 10.1146/annurev-bioeng-071910-124649; Glocker B, 2009, LECT NOTES COMPUT SC, V5875, P1101, DOI 10.1007/978-3-642-10331-5_102; Glocker B, 2008, MED IMAGE ANAL, V12, P731, DOI 10.1016/j.media.2008.03.006; Huang XS, 2009, IEEE T MED IMAGING, V28, P1179, DOI 10.1109/TMI.2008.2011557; Jiang SZ, 2009, MAGN RESON MED, V62, P645, DOI 10.1002/mrm.22032; Kappes JH, 2013, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2013.175; Kim B, 1999, MAGN RESON MED, V41, P964, DOI 10.1002/(SICI)1522-2594(199905)41:5<964::AID-MRM16>3.0.CO;2-D; Kohli P., 2012, HIGHER ORDERMODELS C; Komodakis N., 2007, IEEE 11 INT C COMP V, P1; Komodakis N, 2015, IEEE T PATTERN ANAL, V37, P1425, DOI 10.1109/TPAMI.2014.2368990; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Kwon D, 2008, LECT NOTES COMPUT SC, V5302, P373, DOI 10.1007/978-3-540-88682-2_29; Leung KYE, 2008, IEEE T MED IMAGING, V27, P1568, DOI 10.1109/TMI.2008.922685; Liao R, 2013, IEEE T MULTIMEDIA, V15, P983, DOI 10.1109/TMM.2013.2244869; Markelj P, 2012, MED IMAGE ANAL, V16, P642, DOI 10.1016/j.media.2010.03.005; Mercier L, 2012, MED PHYS, V39, P3253, DOI 10.1118/1.4709600; Murphy K.P., 1999, P 15 C UNC ART INT U; NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308; Olesch J., 2011, SPIE MED IMAGING; Osechinskiy S, 2011, ANAT RES INT, V2011, P1, DOI DOI 10.1155/2011/287860; Paragios N, 2016, MED IMAGE ANAL, V33, P102, DOI 10.1016/j.media.2016.06.028; Paragios N, 2014, INT C PATT RECOG, P18, DOI 10.1109/ICPR.2014.13; Porchetto R., 2016, RIGID SLICE VOLUME M; Raman S, 2008, INT J COMPUT GAMES T, V2008, DOI 10.1155/2008/316790; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Seshamani S, 2013, I S BIOMED IMAGING, P796; Shekhovtsov A, 2008, COMPUT VIS IMAGE UND, V112, P91, DOI 10.1016/j.cviu.2008.06.006; Sotiras A, 2013, IEEE T MED IMAGING, V32, P1153, DOI 10.1109/TMI.2013.2265603; Sotiras A, 2010, LECT NOTES COMPUT SC, V6362, P676; Wang CH, 2013, COMPUT VIS IMAGE UND, V117, P1610, DOI 10.1016/j.cviu.2013.07.004; Xu H, 2014, ELIFE, V3, DOI 10.7554/eLife.01489; Zikic D, 2010, MED IMAGE ANAL, V14, P550, DOI 10.1016/j.media.2010.04.003	47	4	4	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2018	126	1					36	58		10.1007/s11263-017-1040-8	http://dx.doi.org/10.1007/s11263-017-1040-8			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FS6MC		Green Submitted			2022-12-18	WOS:000419910500003
J	Baxter, JSH; Rajchl, M; McLeod, AJ; Yuan, J; Peters, TM				Baxter, John S. H.; Rajchl, Martin; McLeod, A. Jonathan; Yuan, Jing; Peters, Terry M.			Directed Acyclic Graph Continuous Max-Flow Image Segmentation for Unconstrained Label Orderings	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Continuous max-flow; Image segmentation; Convex optimization; Variational optimization; ASETS	MINIMIZATION ALGORITHM; ENERGY MINIMIZATION; CO-SEGMENTATION; MR-IMAGES; CUTS; OPTIMIZATION; BRAIN	Label ordering, the specification of subset-superset relationships for segmentation labels, has been of increasing interest in image segmentation as they allow for complex regions to be represented as a collection of simple parts. Recent advances in continuous max-flow segmentation have widely expanded the possible label orderings from binary background/foreground problems to extendable frameworks in which the label ordering can be specified. This article presents Directed Acyclic Graph Max-Flow image segmentation which is flexible enough to incorporate any label ordering without constraints. This framework uses augmented Lagrangian multipliers and primal-dual optimization to develop a highly parallelized solver implemented using GPGPU. This framework is validated on synthetic, natural, and medical images illustrating its general applicability.	[Baxter, John S. H.; McLeod, A. Jonathan; Peters, Terry M.] Western Univ, Robarts Res Inst, London, ON, Canada; [Rajchl, Martin] Imperial Coll, Dept Comp, London, England; [Yuan, Jing] Xidian Univ, Sch Math & Stat, Xian, Peoples R China	Western University (University of Western Ontario); Imperial College London; Xidian University	Baxter, JSH (corresponding author), Western Univ, Robarts Res Inst, London, ON, Canada.	jbaxter@robarts.ca	Baxter, John S.H./ABI-1243-2020; Peters, Terry M/K-6853-2013; Peters, Terry Malcolm/AAD-7797-2022	Baxter, John S.H./0000-0003-3548-4343; Peters, Terry M/0000-0003-1440-7488; Peters, Terry Malcolm/0000-0003-1440-7488	Natural Sciences and Engineering Research Council of Canada	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR)	The authors would like to acknowledge Zahra Hosseini, Maria Drangova, and Ravi Menon's laboratory at the Robarts Research Institute Imaging Laboratories for their assistance in collecting and processing MRI phase data. John S.H. Baxter and A. Jonathan McLeod were funded through the Natural Sciences and Engineering Research Council of Canada.	[Anonymous], 2006, P 2006 IEEE COMP SOC, DOI DOI 10.1109/CVPR.2006.23; Bae E, 2015, LECT NOTES COMPUT SC, V8932, P15, DOI 10.1007/978-3-319-14612-6_2; Bae E, 2014, LECT NOTES COMPUT SC, V8293, P134, DOI 10.1007/978-3-642-54774-4_7; Baxter J. S. H., 2014, ARXIV14040336; Baxter JSH, 2016, PROC SPIE, V9784, DOI 10.1117/12.2216258; Bertsekas D, 1999, NONLINEAR PROGRAMMIN; BILLIONNET A, 1985, DISCRETE APPL MATH, V12, P1, DOI 10.1016/0166-218X(85)90035-6; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chambolle A, 2004, J MATH IMAGING VIS, V20, P89; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chen G, 1993, SIAM J OPTIMIZ, V3, P538, DOI 10.1137/0803026; Cocosco C.A., 1997, NEUROIMAGE; Delong A, 2012, INT J COMPUT VISION, V100, P38, DOI 10.1007/s11263-012-0531-x; Delong A, 2009, IEEE I CONF COMP VIS, P285, DOI 10.1109/ICCV.2009.5459263; Denk C, 2010, J MAGN RESON IMAGING, V31, P185, DOI 10.1002/jmri.21995; Ekeland I, 1999, CONVEX ANAL VARIATIO; Fenster A., 2012, MICCAI GRAND CHALLEN, P82; Giusti E., 1984, MONOGRAPHS MATH; Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073; Guo FM, 2015, MED IMAGE ANAL, V23, P43, DOI 10.1016/j.media.2015.04.001; Haacke EM, 2004, MAGN RESON MED, V52, P612, DOI 10.1002/mrm.20198; Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y; Hong M., 2012, ARXIV12083922; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; IVANESCU PL, 1965, OPER RES, V13, P388, DOI 10.1287/opre.13.3.388; Jang J, 2014, INT CONF UBIQ ROBOT, P80, DOI 10.1109/URAI.2014.7057400; Koch Lisa M, 2015, Inf Process Med Imaging, V24, P221, DOI 10.1007/978-3-319-19992-4_17; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Pock T, 2009, PROC CVPR IEEE, P810, DOI 10.1109/CVPRW.2009.5206604; POTTS RB, 1952, P CAMB PHILOS SOC, V48, P106, DOI 10.1017/S0305004100027419; Rajchl M, 2016, MED IMAGE ANAL, V27, P45, DOI 10.1016/j.media.2015.05.005; Rajchl M, 2014, IEEE T MED IMAGING, V33, P159, DOI 10.1109/TMI.2013.2282932; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; Schlesinger D., 2006, TRANSFORMING ARBITRA; Su XY, 2004, OPT LASER ENG, V42, P245, DOI 10.1016/j.optlaseng.2003.11.002; Thomas B, 2008, NEURORADIOLOGY, V50, P105, DOI 10.1007/s00234-007-0316-z; van der Lijn F, 2008, NEUROIMAGE, V43, P708, DOI 10.1016/j.neuroimage.2008.07.058; Veksler O, 2008, LECT NOTES COMPUT SC, V5304, P454, DOI 10.1007/978-3-540-88690-7_34; Yuan J, 2010, LECT NOTES COMPUT SC, V6316, P379, DOI 10.1007/978-3-642-15567-3_28; Yuan J, 2010, PROC CVPR IEEE, P2217, DOI 10.1109/CVPR.2010.5539903	43	4	6	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2017	123	3					415	434		10.1007/s11263-017-0994-x	http://dx.doi.org/10.1007/s11263-017-0994-x			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EX9EO		Green Submitted			2022-12-18	WOS:000403559600006
J	de Souza, FDM; Sarkar, S; Srivastava, A; Su, JY				de Souza, Fillipe D. M.; Sarkar, Sudeep; Srivastava, Anuj; Su, Jingyong			Spatially Coherent Interpretations of Videos Using Pattern Theory	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Activity detection; Pattern theory; Graphical methods; Compositional approach	RECOGNITION; EVENTS	Activity interpretation in videos results not only in recognition or labeling of dominant activities, but also in semantic descriptions of scenes. Towards this broader goal, we present a combinatorial approach that assumes availability of algorithms for detecting and labeling objects and basic actions in videos, albeit with some errors. Given these uncertain labels and detected objects, we link them into interpretable structures using the domain knowledge, under the framework of Grenander's general pattern theory. Here a semantic description is built using basic units, termed generators, that represent either objects or actions. These generators have multiple out-bonds, each associated with different types of domain semantics, spatial constraints, and image evidence. The generators combine, according to a set of pre-defined combination rules that capture domain semantics, to form larger configurations that represent video interpretations. This framework derives its representational power from flexibility in size and structure of configurations. We impose a probability distribution on the configuration space, with inferences generated using a Markov chain Monte Carlo-based simulated annealing process. The primary advantage of the approach is that it handles known challenges-appearance variabilities, errors in object labels, object clutter, simultaneous events, etc-without the need for exponentially-large (labeled) training data. Experimental results demonstrate its ability to successfully provide interpretations under clutter and the simultaneity of events. They show: (1) a performance increase of more than 30 % over other state-of-the-art approaches using more than 5000 video units from the Breakfast Actions dataset, and (2) an overall recall and precision improvement of more than 50 and 100 %, respectively, on the YouCook data set.	[de Souza, Fillipe D. M.; Sarkar, Sudeep] Univ S Florida, Dept Comp Sci & Engn, Tampa, FL 33620 USA; [Srivastava, Anuj] Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA; [Su, Jingyong] Texas Tech Univ, Dept Math & Stat, Lubbock, TX 79409 USA	State University System of Florida; University of South Florida; State University System of Florida; Florida State University; Texas Tech University System; Texas Tech University	de Souza, FDM (corresponding author), Univ S Florida, Dept Comp Sci & Engn, Tampa, FL 33620 USA.	fillipe@mail.usf.edu	Sarkar, Sudeep/ABD-7629-2021; Srivastava, Anuj/L-4705-2019	Sarkar, Sudeep/0000-0001-7332-4207; Srivastava, Anuj/0000-0001-7406-0338	NSF [1217515, 1217676]; Direct For Computer & Info Scie & Enginr [1513126] Funding Source: National Science Foundation; Div Of Information & Intelligent Systems [1217676] Funding Source: National Science Foundation	NSF(National Science Foundation (NSF)); Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); Div Of Information & Intelligent Systems(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This research was supported in part by NSF Grants 1217515 and 1217676.	Albanese M, 2008, IEEE T MULTIMEDIA, V10, P982, DOI 10.1109/TMM.2008.2001369; Albanese M, 2010, IEEE T PATTERN ANAL, V32, P2246, DOI 10.1109/TPAMI.2010.33; Amer MR, 2013, IEEE I CONF COMP VIS, P1353, DOI 10.1109/ICCV.2013.171; Bhattacharya S, 2014, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2014.287; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chawla NV, 2011, SMOTE SYNTHETIC MINO, V11, P1813; Das P, 2013, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2013.340; de Souza F. D. M., 2014, IEEE INT C PATT REC; Dubba K. S. R., 2012, THESIS; Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872; Ghanem N, 2004, 2004 C COMPUTER VISI, P112, DOI DOI 10.1109/CVPR.2004.430; Grenander U., 2007, PATTERN THEORY REPRE, V1; Grenander U., 1993, GEN PATTERN THEORY M; Hilde K., 2014, IEEE C COMP VIS PATT; Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686; Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2; Joo SW, 2006, IEEE IMAGE PROC, P2897, DOI 10.1109/ICIP.2006.313035; Ke Y., 2007, ICCV; Lan T, 2012, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2012.6247821; Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Morariu Vlad I, 2011, P CVPR, P3289; Narayanaswamy S., 2014, IEEE C COMP VIS PATT; Pei MT, 2011, IEEE I CONF COMP VIS, P487, DOI 10.1109/ICCV.2011.6126279; Romdhane Rim, 2011, Computer Vision Systems. Proceedings 8th International Conference (ICVS 2011), P122, DOI 10.1007/978-3-642-23968-7_13; Ryoo MS, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2850; Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806; Shu TM, 2015, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR.2015.7299088; Si ZZ, 2011, IEEE I CONF COMP VIS, P41, DOI 10.1109/ICCV.2011.6126223; Souza F., 2015, CVPR; Vahdat A., 2013, ICCV; Wang X, 2015, IEEE C COMP VIS PATT; Wei P, 2013, IEEE I CONF COMP VIS, P3136, DOI 10.1109/ICCV.2013.389; Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789	35	4	4	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	1					5	25		10.1007/s11263-016-0913-6	http://dx.doi.org/10.1007/s11263-016-0913-6			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EI3HN					2022-12-18	WOS:000392380900001
J	Del Pero, L; Ricco, S; Sukthankar, R; Ferrari, V				Del Pero, Luca; Ricco, Susanna; Sukthankar, Rahul; Ferrari, Vittorio			Behavior Discovery and Alignment of Articulated Object Classes from Unstructured Video	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Articulated motion; Behavior discovery; Video sequence alignment; Weakly supervised learning from video		We propose an automatic system for organizing the content of a collection of unstructured videos of an articulated object class (e.g., tiger, horse). By exploiting the recurring motion patterns of the class across videos, our system: (1) identifies its characteristic behaviors, and (2) recovers pixel-to-pixel alignments across different instances. Our system can be useful for organizing video collections for indexing and retrieval. Moreover, it can be a platform for learning the appearance or behaviors of object classes from Internet video. Traditional supervised techniques cannot exploit this wealth of data directly, as they require a large amount of time-consuming manual annotations. The behavior discovery stage generates temporal video intervals, each automatically trimmed to one instance of the discovered behavior, clustered by type. It relies on our novel motion representation for articulated motion based on the displacement of ordered pairs of trajectories. The alignment stage aligns hundreds of instances of the class to a great accuracy despite considerable appearance variations (e.g., an adult tiger and a cub). It uses a flexible thin plate spline deformation model that can vary through time. We carefully evaluate each step of our system on a new, fully annotated dataset. On behavior discovery, we outperform the state-of-the-art improved dense trajectory feature descriptor. On spatial alignment, we outperform the popular SIFT Flow algorithm.	[Del Pero, Luca; Ferrari, Vittorio] Univ Edinburgh, Sch Informat, IPAB, Crichton St 10, Edinburgh EH8 9AB, Midlothian, Scotland; [Ricco, Susanna; Sukthankar, Rahul] Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA	University of Edinburgh; Google Incorporated	Del Pero, L (corresponding author), Univ Edinburgh, Sch Informat, IPAB, Crichton St 10, Edinburgh EH8 9AB, Midlothian, Scotland.	ldelper@staffmail.ed.ac.uk; ricco@google.com; sukthankar@google.com; vferrari@staffmail.ed.ac.uk			Google Faculty Research Award; ERC Starting Grant "Visual Culture for Image Understanding"	Google Faculty Research Award(Google Incorporated); ERC Starting Grant "Visual Culture for Image Understanding"	We are very grateful to Anestis Papazoglou for helping with the data collection, and to Shumeet Baluja for his helpful comments. This work was partly funded by a Google Faculty Research Award, and by ERC Starting Grant "Visual Culture for Image Understanding". We also thank the reviewers for their helpful comments.	Azizpour H., 2012, P EUR C COMP VIS; Barnes C., 2010, P EUR C COMP VIS; Bourdev L., 2009, P INT C COMP VIS; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Caspi Y., 2000, P IEEE C COMP VIS PA; Caspi Y, 2006, INT J COMPUT VISION, V68, P53, DOI 10.1007/s11263-005-4842-z; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; Cinbis R., 2013, P INT C COMP VIS; Cootes Timothy F, 1998, P EUR C COMP VIS; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, HISTOGRAMS ORIENTED; Del Pero L., 2015, DATASET ARTICULATED; Del Pero L., 2015, P IEEE C COMP VIS PA; Dexter E., 2009, P BRIT MACH VIS C; Dollar P., 2013, P INT C COMP VIS; Douze M., 2015, ARXIV150602588V1; Evangelidis GD, 2013, IEEE T PATTERN ANAL, V35, P2371, DOI 10.1109/TPAMI.2013.56; Fan QF, 2011, IEEE T IMAGE PROCESS, V20, P2315, DOI 10.1109/TIP.2011.2109727; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Ferrari V, 2006, INT J COMPUT VISION, V67, P159, DOI 10.1007/s11263-005-3964-7; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hospedales T., 2009, P INT C COMP VIS; Hu WM, 2006, IEEE T PATTERN ANAL, V28, P1450, DOI 10.1109/TPAMI.2006.176; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Jain A., 2013, P IEEE C COMP VIS PA; Jegou H., 2008, P EUR C COMP VIS; Jiang Y., 2012, P EUR C COMP VIS; Jiang Y. G., 2014, THUMOS ECCV WORKSH A; JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588; Karpathy A., 2014, P IEEE C COMP VIS PA; KE Y, 2007, P INT C COMP VIS; Kim W. H., 2009, INT S CONS EL; Kuehne H., 2011, P INT C COMP VIS; Kuettel D., 2010, P IEEE C COMP VIS PA; Kuettel D., 2012, P EUR C COMP VIS; Lampert C. H., 2009, P IEEE C COMP VIS PA; Leistner C, 2011, PROC CVPR IEEE; Leordeanu M., 2007, P IEEE C COMP VIS PA; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Liao J., 2014, EUR S REND; Liu C., 2008, P EUR C COMP VIS; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mahadevan V., 2010, P IEEE C COMP VIS PA; Malisiewicz T., 2011, P INT C COMP VIS; MARTIN D, 2001, P INT C COMP VIS; Matikainen P., 2010, P EUR C COMP VIS; Matikainen P, 2009, ICCV WORKSH VID OR O; Messing R., 2009, P INT C COMP VIS; Narayan S., 2014, P IEEE C COMP VIS PA; Papazoglou Anestis, 2013, P INT C COMP VIS; Prest A., 2012, P IEEE C COMP VIS PA; Ramanan D, 2006, IEEE T PATTERN ANAL, V28, P1319, DOI 10.1109/TPAMI.2006.155; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Rao C., 2003, P INT C COMP VIS; Raptis M., 2012, P IEEE C COMP VIS PA; Raptis M., 2010, P EUR C COMP VIS; Ryoo M.S., 2009, P INT C COMP VIS; Santos J. M., 2009, P 19 INT C ART NEUR; Schmid C., 1996, TECHNICAL REPORT; Schuldt C., 2004, P INT C PATT REC; Seitz Steven M., 2006, IEEE C COMP VIS PATT; Smeaton AF, 2006, ACM INT WORKSH MULT; Soomro K., CORR; Tang K., 2013, P IEEE C COMP VIS PA; Tighe J., 2013, P IEEE C COMP VIS PA; Tompkin J., 2012, P ACM SIGGRAPH C COM; TUYTELAARS T, 2004, P IEEE C COMP VIS PA; Ukrainitz Y., 2006, P EUR C COMP VIS; Vezhnevets A., 2014, P IEEE C COMP VIS PA; Viola P. A., 2005, ADV NEURAL INFORM PR; Wahba G., 1990, CBMS NSF REGIONAL C, V59; Wan F., 2009, P 2009 IEEE INT C MU; Wang H., 2013, P INT C COMP VIS; Wang H, 2011, PROC CVPR IEEE; Wang L., 2014, P EUR C COMP VIS; Wang O, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601208; Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87; Wang XY, 2013, IEEE I CONF COMP VIS, P17, DOI 10.1109/ICCV.2013.10; Yang S., 2010, P IEEE C COMP VIS PA; YANG Y, 2013, PAMI, V35, P1635, DOI DOI 10.1109/TPAMI.2012.253; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Yilmaz A., 2005, P IEEE C COMP VIS PA; Yuan J., 2009, P IEEE C COMP VIS PA; Zhao X., 2011, P INT C COMP VIS	90	4	4	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	2					303	325		10.1007/s11263-016-0939-9	http://dx.doi.org/10.1007/s11263-016-0939-9			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK2LI	32336878	Green Published, Green Submitted, Green Accepted, hybrid			2022-12-18	WOS:000393758400006
J	Gibaldi, A; Vanegas, M; Canessa, A; Sabatini, SP				Gibaldi, Agostino; Vanegas, Mauricio; Canessa, Andrea; Sabatini, Silvio P.			A Portable Bio-Inspired Architecture for Efficient Robotic Vergence Control	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Vergence control; Active vision; Stereo vision; Binocular energy models; Neuromorphic architectures	EYE-MOVEMENTS; BINOCULAR DISPARITY; STEREO DISPARITY; ACTIVE VISION; SYSTEM; DEPTH; SEGMENTATION; ATTENTION; MOTION; GAZE	In stereoscopic vision, the ability of perceiving the three-dimensional structure of the surrounding environment is subordinated to a precise and effective motor control for the binocular coordination of the eyes/cameras. If, on the one side, the binocular coordination of camera movements is a complicating factor, on the other side, a proper vergence control, acting on the binocular disparity, facilitates the binocular fusion and the subsequent stereoscopic perception process. In real-world situations, an effective vergence control requires further features other than real time capabilities: real robot systems are indeed characterized by mechanical and geometrical imprecision that affect the binocular vision, and the illumination conditions are changeable and unpredictable. Moreover, in order to allow an effective visual exploration of the peripersonal space, it is necessary to cope with different gaze directions and provide a large working space. The proposed control strategy resorts to a neuromimetic approach that provides a distributed representation of disparity information. The vergence posture is obtained by an open-loop and a closed-loop control, which directly interacts with saccadic control. Before saccade, the open-loop component is computed in correspondence of the saccade target region, to obtain a vergence correction to be applied simultaneously with the saccade. At fixation, the closed-loop component drives the binocular disparity to zero in a foveal region. The obtained vergence servos are able to actively drive both the horizontal and the vertical alignment of the optical axes on the object of interest, thus ensuring a correct vergence posture. Experimental tests were purposely designed to measure the performance of the control in the peripersonal space, and were performed on three different robot platforms. The results demonstrated that the proposed approach yields real-time and effective vergence camera movements on a visual stimulus in a wide working range, regardless of the illumination in the environment and the geometry of the system.	[Gibaldi, Agostino; Vanegas, Mauricio; Canessa, Andrea; Sabatini, Silvio P.] Univ Genoa, Phys Struct Percept & Computat Grp, Dept Informat Bioengn Robot & Syst Engn, Genoa, Italy	University of Genoa	Gibaldi, A (corresponding author), Univ Genoa, Phys Struct Percept & Computat Grp, Dept Informat Bioengn Robot & Syst Engn, Genoa, Italy.	agostino.gibaldi@unige.it	CANESSA, ANDREA/J-5587-2018; Sabatini, Silvio P/A-5500-2012; Gibaldi, Agostino/A-1219-2015	CANESSA, ANDREA/0000-0001-8946-5290; Sabatini, Silvio P/0000-0002-0557-7306; Gibaldi, Agostino/0000-0003-4478-6351	EC [FP7-ICT-217077 EYESHOTS]; EU [FP7-ICT-215866 SEARISE]	EC(European CommissionEuropean Commission Joint Research Centre); EU(European Commission)	This work has been partially supported by the EC Project FP7-ICT-217077 EYESHOTS - Heterogeneous 3D perception across visual fragments (see http://www.eyeshots.it), and by the EU project FP7-ICT-215866 SEARISE (see http://www.searise.eu)	Abbott A. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P532, DOI 10.1109/CCV.1988.590034; Aloimonos J., 1987, International Journal of Computer Vision, V1, P333, DOI 10.1007/BF00133571; Antonelli M, 2014, IEEE T AUTON MENT DE, V6, P259, DOI 10.1109/TAMD.2014.2332875; Bajcsy R., 2016, ARXIV160302729; Bana S., 2007, BIOL MOTIVATED VERGE, P513; Beira R, 2006, IEEE INT CONF ROBOT, P94, DOI 10.1109/ROBOT.2006.1641167; Belhaoua A., 2010, J IMAGE VIDEO PROCES; Bernardino A, 1998, ROBOT AUTON SYST, V25, P137, DOI 10.1016/S0921-8890(98)00043-8; Bernardino A., 1996, INTELLIGENT ROBOTS S; Beuth F, 2015, VISION RES, V116, P241, DOI 10.1016/j.visres.2015.04.004; Bjorkman M, 2000, PROC CVPR IEEE, P506, DOI 10.1109/CVPR.2000.854897; Bradski G., 2008, LEARNING OPENCV COMP; Canessa A, 2014, J VIS COMMUN IMAGE R, V25, P227, DOI 10.1016/j.jvcir.2013.02.011; Capurro C, 1997, INT J COMPUT VISION, V24, P79, DOI 10.1023/A:1007974208880; Chessa M, 2009, LECT NOTES COMPUT SC, V5815, P184, DOI 10.1007/978-3-642-04667-4_19; CHING WS, 1995, COMPUT VIS IMAGE UND, V62, P298, DOI 10.1006/cviu.1995.1056; Choi I, 2003, LECT NOTES COMPUT SC, V2756, P182; Chumerin N, 2010, INT J NEURAL SYST, V20, P267, DOI 10.1142/S0129065710002425; COOMBS D, 1993, INT J COMPUT VISION, V11, P147, DOI 10.1007/BF01469226; Culverhouse P., 2009, P 4 WORKSH HUM SOCC, P60; Cumming BG, 1997, NATURE, V389, P280, DOI 10.1038/38487; Daniilidis K., 1996, AUTONOME MOBILE SYST, P78; Dankers A, 2007, COMPUT VIS IMAGE UND, V108, P74, DOI 10.1016/j.cviu.2006.10.013; DAS S, 1995, IEEE T PATTERN ANAL, V17, P1213, DOI 10.1109/34.476513; DEANGELIS GC, 1993, J NEUROPHYSIOL, V69, P1118, DOI 10.1152/jn.1993.69.4.1118; Enright JT, 1998, J PHYSIOL-LONDON, V512, P235, DOI 10.1111/j.1469-7793.1998.235bf.x; Fleet D.J., 1996, MODELLING BINOCULAR; FLEET DJ, 1993, IEEE T PATTERN ANAL, V15, P1253, DOI 10.1109/34.250844; Franz A., 2007, EMERGENCE DISPARITY, P31; Gibaldi A., 2011, 11 IEEE RAS INT C HU, P1065; Gibaldi A., 2013, 2013 INT JOINT C NEU, P1; Gibaldi A, 2015, I IEEE EMBS C NEUR E, P332, DOI 10.1109/NER.2015.7146627; Gibaldi A, 2015, ROBOT AUTON SYST, V71, P23, DOI 10.1016/j.robot.2015.01.002; Gibaldi A, 2012, P IEEE RAS-EMBS INT, P955, DOI 10.1109/BioRob.2012.6290812; Gibaldi A, 2010, NEUROCOMPUTING, V73, P1065, DOI 10.1016/j.neucom.2009.11.016; Hansard M., 2007, ADV BRAIN VISION ART; Hansard M, 2010, IEEE T SYST MAN CY B, V40, P151, DOI 10.1109/TSMCB.2009.2024211; Hansen M., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P287, DOI 10.1109/ICPR.1996.546035; Hering E., 1868, THEORY BINOCULAR VIS; Howard I., 2002, SEEING DEPTH; HUNG GK, 1986, IEEE T BIO-MED ENG, V33, P1021, DOI 10.1109/TBME.1986.325868; K- Team- Corp, 2010, K TEAM MOB ROB; Kim HJ, 2000, INT C PATT RECOG, P491, DOI 10.1109/ICPR.2000.902964; Knight J, 2006, INT J COMPUT VISION, V68, P219, DOI 10.1007/s11263-005-5032-8; Konolige K., 1998, Robotics Research. Eighth International Symposium, P203; Kyriakoulis N, 2010, J INTELL FUZZY SYST, V21, P385, DOI 10.3233/IFS-2010-0459; Liu Y, 2008, J VISION, V8, DOI 10.1167/8.11.19; Lonini L., 2013, 2013 IEEE 3 JOINT IN, P1, DOI DOI 10.1109/DEVLRN.2013.6652541; Manzotti R, 2001, COMPUT VIS IMAGE UND, V83, P97, DOI 10.1006/cviu.2001.0924; Marefat MM, 1997, PATTERN RECOGN, V30, P1829, DOI 10.1016/S0031-3203(97)00066-6; Marfil R, 2003, INT J IMAG SYST TECH, V13, P224, DOI 10.1002/ima.10058; Masson GS, 1997, NATURE, V389, P283, DOI 10.1038/38496; MINKEN AWH, 1995, VISION RES, V35, P93, DOI 10.1016/0042-6989(94)E0052-M; Mishra A, 2009, IEEE I CONF COMP VIS, P468, DOI 10.1109/ICCV.2009.5459254; Monaco JP, 2009, INT J COMPUT VISION, V85, P192, DOI 10.1007/s11263-009-0230-4; Morgan MJ, 1997, VISION RES, V37, P2737, DOI 10.1016/S0042-6989(97)00074-6; Muhammad W, 2015, ADAPT BEHAV, V23, P265, DOI 10.1177/1059712315607363; Ogale AS, 2005, IEEE INT CONF ROBOT, P819; OLSON TJ, 1991, INT J COMPUT VISION, V7, P67, DOI 10.1007/BF00130490; Pauwels K, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500074; Peng J, 2000, IEEE SYS MAN CYBERN, P1472, DOI 10.1109/ICSMC.2000.886062; Piater J. H., 1999, Proceedings of the 1999 IEEE International Symposium on Intelligent Control Intelligent Systems and Semiotics (Cat. No.99CH37014), P272, DOI 10.1109/ISIC.1999.796667; Point- Grey- Research, 2010, FIR CAM; QIAN N, 1994, NEURAL COMPUT, V6, P390, DOI 10.1162/neco.1994.6.3.390; Qu C, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2266, DOI 10.1109/IJCNN.2011.6033511; Rambold HA, 2008, VISION RES, V48, P2006, DOI 10.1016/j.visres.2008.05.009; Rea F, 2014, IEEE-RAS INT C HUMAN, P779, DOI 10.1109/HUMANOIDS.2014.7041452; Ruesch J, 2008, IEEE INT CONF ROBOT, P962, DOI 10.1109/ROBOT.2008.4543329; Saadi M, 2012, IEEE C ELEC DEVICES; Samarawickrama JG, 2007, FOURTH CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P363, DOI 10.1109/CRV.2007.69; SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114; SCHWARTZ EL, 1977, BIOL CYBERN, V25, P181, DOI 10.1007/BF01885636; Shimonomura K., 2010, 2010 IEEE International Conference on Robotics and Biomimetics (ROBIO), P1774, DOI 10.1109/ROBIO.2010.5723600; Sun W., 2011, 2011 IEEE INT C DEV, P1; Takemura A, 2001, J NEUROPHYSIOL, V85, P2245, DOI 10.1152/jn.2001.85.5.2245; TAYLOR J, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P540, DOI 10.1109/CVPR.1994.323879; THEIMER WM, 1994, CVGIP-IMAG UNDERSTAN, V60, P343, DOI 10.1006/ciun.1994.1061; Tsang EKC, 2008, IEEE INT SYMP CIRC S, P1076, DOI 10.1109/ISCAS.2008.4541608; VANDENBERG AV, 1995, P ROY SOC B-BIOL SCI, V260, P191, DOI 10.1098/rspb.1995.0079; Vanegas M., 2012, TECH BIOINSPIRED ACT; VANRIJN LJ, 1993, VISION RES, V33, P691, DOI 10.1016/0042-6989(93)90189-4; Wang YW, 2011, IEEE T AUTON MENT DE, V3, P247, DOI 10.1109/TAMD.2011.2128318; Wang YW, 2010, NEURAL COMPUT, V22, P730, DOI 10.1162/neco.2009.01-09-950; Yamato J., 1999, IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028), P836, DOI 10.1109/ICSMC.1999.825370; Zhang XJ, 2011, IMAGE VISION COMPUT, V29, P64, DOI 10.1016/j.imavis.2010.08.005; Zhang XJ, 2009, CYBERNET SYST, V40, P549, DOI 10.1080/01969720903068484	86	4	5	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	2					281	302		10.1007/s11263-016-0936-z	http://dx.doi.org/10.1007/s11263-016-0936-z			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK2LI		hybrid, Green Published			2022-12-18	WOS:000393758400005
J	Trager, M; Ponce, J; Hebert, M				Trager, Matthew; Ponce, Jean; Hebert, Martial			Trinocular Geometry Revisited	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Trinocular geometry; Trilinearities; Minimal parameterizations of camera geometry; Camera parameter estimation	POINTS; MOTION	When do the visual rays associated with triplets of point correspondences converge, that is, intersect in a common point? Classical models of trinocular geometry based on the fundamental matrices and trifocal tensor associated with the corresponding cameras only provide partial answers to this fundamental question, in large part because of underlying, but seldom explicit, general configuration assumptions. This paper uses elementary tools from projective line geometry to provide necessary and sufficient geometric and analytical conditions for convergence in terms of transversals to triplets of visual rays, without any such assumptions. In turn, this yields a novel and simple minimal parameterization of trinocular geometry for cameras with non-collinear or collinear pinholes, which can be used to construct a practical and efficient method for trinocular geometry parameter estimation. We present numerical experiments using synthetic and real data.	[Trager, Matthew] ENS CNRS Inria UMR 8548, Willow Team, Inria, Paris, France; [Ponce, Jean] PSL Res Univ, ENS CNRS Inria UMR 8548, Ecole Normale Super, Willow Team, Paris, France; [Hebert, Martial] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Carnegie Mellon University	Trager, M (corresponding author), ENS CNRS Inria UMR 8548, Willow Team, Inria, Paris, France.	matthew.trager@inria.fr; jean.ponce@ens.fr; hebert@ri.cmu.edu			ERC grant VideoWorld; Institut Universitaire de France; Inria - CMU associate team GAYA; ONR MURI [N000141010934]	ERC grant VideoWorld; Institut Universitaire de France; Inria - CMU associate team GAYA; ONR MURI(MURIOffice of Naval Research)	This work was supported in part by the ERC grant VideoWorld, the Institut Universitaire de France, the Inria - CMU associate team GAYA, and ONR MURI N000141010934.	Canterakis N., 2000, ECCV; Carlsson S, 1995, P WORSH REPR VIS SCE; Faugeras O., 1995, 2665 INRIA; Forsyth David A, 2012, COMPUTER VISION MODE; Hartley RI, 1997, INT J COMPUT VISION, V22, P125, DOI 10.1023/A:1007936012022; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Heyden A, 1997, MATH METHOD APPL SCI, V20, P1135, DOI 10.1002/(SICI)1099-1476(19970910)20:13<1135::AID-MMA908>3.0.CO;2-9; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; McGlone C., 2004, MANUAL PHOTOGRAMMETR; Papadopoulo T., 1998, ECCV; Ponce J., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P463; Ponce J., 2014, CVPR; Ponce J., 2005, CVPR; Pottmann Helmut, 2001, MATH VISUAL, V2; QUAN L, 1995, IEEE T PATTERN ANAL, V17, P34; Ressl C., 2002, ISPRS COMM 3 S 3A, VXXXIV; SHASHUA A, 1995, IEEE T PATTERN ANAL, V17, P779, DOI 10.1109/34.400567; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; Thompson M. M., 1996, MANUAL PHOTOGRAMMETR; Torr PHS, 1997, IMAGE VISION COMPUT, V15, P591, DOI 10.1016/S0262-8856(97)00010-3; Trager M, 2015, IEEE I CONF COMP VIS, P909, DOI 10.1109/ICCV.2015.110; Triggs B., 1995, ICCV; Veblen O., 1910, PROJECTIVE GEOMETRY; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P318, DOI 10.1109/34.120327	25	4	4	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2016	120	2					134	152		10.1007/s11263-016-0900-y	http://dx.doi.org/10.1007/s11263-016-0900-y			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DU3EF		Green Submitted			2022-12-18	WOS:000382092900002
J	Efros, A; Torralba, A				Efros, Alyosha; Torralba, Antonio			Guest Editorial: Big Data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Efros, Alyosha] Univ Calif Berkeley, Elect Engn & Comp Sci Dept, Div Comp Sci, Berkeley, CA USA; [Torralba, Antonio] MIT, Dept Elect Engn & Comp Sci, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	University of California System; University of California Berkeley; Massachusetts Institute of Technology (MIT)	Torralba, A (corresponding author), MIT, Dept Elect Engn & Comp Sci, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.	efros@eecs.berkeley.edu; torralba@mit.edu		Efros, Alexei A./0000-0001-5720-8070					0	4	4	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2016	119	1					1	2		10.1007/s11263-016-0914-5	http://dx.doi.org/10.1007/s11263-016-0914-5			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DP9AP		Bronze, Green Submitted			2022-12-18	WOS:000378789400001
J	Tolias, G; Avrithis, Y; Jegou, H				Tolias, Giorgos; Avrithis, Yannis; Jegou, Herve			Image Search with Selective Match Kernels: Aggregation Across Single and Multiple Images (vol 116, pg 247, 2016)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Tolias, Giorgos; Jegou, Herve] INRIA, Rennes, France; [Avrithis, Yannis] NTUA, Athens, Greece	Inria; National Technical University of Athens	Tolias, G (corresponding author), INRIA, Rennes, France.	giorgos.tolias@inria.fr	Tolias, Giorgos/O-9939-2017	Tolias, Giorgos/0000-0002-9570-3870				Tolias G, 2016, INT J COMPUT VISION, V116, P247, DOI 10.1007/s11263-015-0810-4	1	4	4	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2016	116	3			SI		262	262		10.1007/s11263-015-0837-6	http://dx.doi.org/10.1007/s11263-015-0837-6			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7SY		Bronze			2022-12-18	WOS:000369421900005
J	Seo, D; Ho, J; Vemuri, BC				Seo, Dohyung; Ho, Jeffrey; Vemuri, Baba C.			Covariant Image Representation with Applications to Classification Problems in Medical Imaging	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Covariant images; Image graphs; Image matching; Image classification; Medical image applications	NONRIGID REGISTRATION; DIFFUSION; FRAMEWORK; MRI	Images are often considered as functions defined on the image domains, and as functions, their (intensity) values are usually considered to be invariant under the image domain transforms. This functional viewpoint is both influential and prevalent, and it provides the justification for comparing images using functional L-p-norms. However, with the advent of more advanced sensing technologies and data processing methods, the definition and the variety of images has been broadened considerably, and the long-cherished functional paradigm for images is becoming inadequate and insufficient. In this paper, we introduce the formal notion of covariant images and study two types of covariant images that are important in medical image analysis, symmetric positive-definite tensor fields and Gaussian mixture fields, images whose sample values covary i.e., jointly vary with image domain transforms rather than being invariant to them. We propose a novel similarity measure between a pair of covariant images considered as embedded shapes (manifolds) in the ambient space, a Cartesian product of the image and its sample-value domains. The similarity measure is based on matching the two embedded low-dimensional shapes, and both the extrinsic geometry of the ambient space and the intrinsic geometry of the shapes are incorporated in computing the similarity measure. Using this similarity as an affinity measure in a supervised learning framework, we demonstrate its effectiveness on two challenging classification problems: classification of brain MR images based on patients' age and (Alzheimer's) disease status and seizure detection from high angular resolution diffusion magnetic resonance scans of rat brains.	[Seo, Dohyung; Ho, Jeffrey; Vemuri, Baba C.] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL USA	State University System of Florida; University of Florida	Vemuri, BC (corresponding author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL USA.	dhseo@ufl.edu; jho@cise.ufl.edu; vemuri@cise.ufl.edu			NSF [IIS 0916001]; NIH [NS066340]; Div Of Information & Intelligent Systems [1525431] Funding Source: National Science Foundation; NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE [R01NS066340] Funding Source: NIH RePORTER	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Div Of Information & Intelligent Systems(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS))	This research was supported in part by NSF Grant IIS 0916001 to JH and BCV and the NIH grant NS066340 to BCV.	Alexander DC, 2001, IEEE T MED IMAGING, V20, P1131, DOI 10.1109/42.963816; Ashburner J, 1999, HUM BRAIN MAPP, V7, P254, DOI 10.1002/(SICI)1097-0193(1999)7:4<254::AID-HBM4>3.0.CO;2-G; Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2; Barmpoutis A, 2007, IEEE T MED IMAGING, V26, P1537, DOI 10.1109/TMI.2007.903195; BATES DM, 1987, COMMUN STAT SIMULAT, V16, P263, DOI 10.1080/03610918708812590; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Cachier P, 2003, COMPUT VIS IMAGE UND, V89, P272, DOI 10.1016/S1077-3142(03)00002-X; Cachier P, 2000, LECT NOTES COMPUT SC, V1935, P472; Cao Y., 2006, P CVPR, P17; Chen T, 2010, I S BIOMED IMAGING, P1337, DOI 10.1109/ISBI.2010.5490244; Cheng G., 2009, INT C MED IM COMP CO, P190; Christensen GE, 2003, J ELECTRON IMAGING, V12, P106, DOI 10.1117/1.1526494; Christensen GE, 2001, IEEE T MED IMAGING, V20, P568, DOI 10.1109/42.932742; Christensen GE, 1996, IEEE T IMAGE PROCESS, V5, P1435, DOI 10.1109/83.536892; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102; do Carmo M. P., 1992, RIEMANNIAN GEOMETRY; Gur Y, 2009, MATH VIS, P325, DOI 10.1007/978-3-540-88378-4_16; Jian B, 2007, IEEE T MED IMAGING, V26, P1464, DOI 10.1109/TMI.2007.907552; Jian B, 2007, NEUROIMAGE, V37, P164, DOI 10.1016/j.neuroimage.2007.03.074; Johnson HJ, 2002, IEEE T MED IMAGING, V21, P450, DOI 10.1109/TMI.2002.1009381; Joshi S, 2004, NEUROIMAGE, V23, pS151, DOI 10.1016/j.neuroimage.2004.07.068; Kimmel R, 2000, INT J COMPUT VISION, V39, P111, DOI 10.1023/A:1008171026419; Kimmel R., 1999, GEOMETRIC VARIATIONA, P294; Koenderink JJ, 2002, LECT NOTES COMPUT SC, V2350, P158; Kroon DJ, 2009, I S BIOMED IMAGING, P963, DOI 10.1109/ISBI.2009.5193214; Kurtek S., 2011, IEEE T MED IMAGING, V30, P49; Kurtek S, 2010, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2010.5539778; Litke N., 2005, S GEOM PROC, P207; Marcus DS, 2007, J COGNITIVE NEUROSCI, V19, P1498, DOI 10.1162/jocn.2007.19.9.1498; Moakher M, 2005, SIAM J MATRIX ANAL A, V26, P735, DOI 10.1137/S0895479803436937; Seo D, 2009, LECT NOTES COMPUT SC, V5681, P98; Sochen N, 1998, IEEE T IMAGE PROCESS, V7, P310, DOI 10.1109/83.661181; Tagare HD, 2009, J MATH IMAGING VIS, V34, P61, DOI 10.1007/s10851-008-0129-7; Tuch D S, 2002, THESIS MIT; Vereauteren T, 2007, LECT NOTES COMPUT SC, V4792, P319; Xie Y., 2010, INT C MED IM COMP CO, P682; Yanovsky I, 2007, IEEE C COMP VIS PATT, P1; Zhang P., 2013, INT C MED IM COMP CO	39	4	5	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2016	116	2					190	209		10.1007/s11263-015-0841-x	http://dx.doi.org/10.1007/s11263-015-0841-x			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7TJ	27182122	Green Accepted			2022-12-18	WOS:000369423000005
J	Munoz, E; Marquez-Neila, P; Baumela, L				Munoz, Enrique; Marquez-Neila, Pablo; Baumela, Luis			Rationalizing Efficient Compositional Image Alignment	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image registration; Tracking; Efficient Gauss-Newton optimization; Efficient compositional image alignment	ACTIVE APPEARANCE MODELS; TRACKING; CONSTRUCTION	We study the issue of computational efficiency for Gauss-Newton (GN) non-linear least-squares optimization in the context of image alignment. We introduce the Constant Jacobian Gauss-Newton (CJGN) optimization, a GN scheme with constant Jacobian and Hessian matrices, and the equivalence and independence conditions as the necessary requirements that any function of residuals must satisfy to be optimized with this efficient approach. We prove that the Inverse Compositional (IC) image alignment algorithm is an instance of a CJGN scheme and formally derive the compositional and extended brightness constancy assumptions as the necessary requirements that must be satisfied by any image alignment problem so it can be solved with an efficient compositional scheme. Moreover, in contradiction with previous results, we also prove that the forward and inverse compositional algorithms are not equivalent. They are equivalent, however, when the extended brightness constancy assumption is satisfied. To analyze the impact of the satisfaction of these requirements we introduce a new image alignment evaluation framework and the concepts of short- and wide-baseline Jacobian. In wide-baseline Jacobian problems the optimization will diverge if the requirements are not satisfied. However, with a good initialization, a short-baseline Jacobian problem may converge even if the requirements are not satisfied.	[Munoz, Enrique; Marquez-Neila, Pablo; Baumela, Luis] Univ Politecn Madrid, Dept Inteligencia Artificial ETSI Informat, E-28660 Madrid, Spain	Universidad Politecnica de Madrid	Baumela, L (corresponding author), Univ Politecn Madrid, Dept Inteligencia Artificial ETSI Informat, Campus Montegancedo S-N, E-28660 Madrid, Spain.	lbaumela@fi.upm.es	Baumela, Luis/F-8867-2013		Ministerio de Economia y Competitividad of Spain [TIN2013-47630-C2-2-R]	Ministerio de Economia y Competitividad of Spain(Spanish Government)	The authors are grateful to Pascal Fua for interesting discussions about this work. They also thank the anonymous reviewers for their comments. Research funded by the Ministerio de Economia y Competitividad of Spain under contract TIN2013-47630-C2-2-R	Amberg B., 2009, P COMP VIS PATT REC; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Baker S., 2004, CMURITR0464; Baker SA, 2001, P SOC PHOTO-OPT INS, V4308, P1, DOI 10.1117/12.424997; Bartoli A, 2008, IEEE T PATTERN ANAL, V30, P2098, DOI 10.1109/TPAMI.2008.22; Benhimane S, 2007, INT J ROBOT RES, V26, P661, DOI 10.1177/0278364907080252; Benhimane S., 2007, P COMP VIS PATT REC; Brooks R, 2010, INT J COMPUT VISION, V87, P191, DOI 10.1007/s11263-009-0263-8; Buenaposada J. M., 2004, P COMP VIS PATT REC; Buenaposada JM, 2009, IMAGE VISION COMPUT, V27, P560, DOI 10.1016/j.imavis.2008.04.015; Buenaposada M, 2002, INT C PATT RECOG, P697, DOI 10.1109/ICPR.2002.1048397; Cobzas D, 2009, IMAGE VISION COMPUT, V27, P69, DOI 10.1016/j.imavis.2006.10.008; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Dowson N, 2008, IEEE T PATTERN ANAL, V30, P180, DOI 10.1109/TPAMI.2007.70757; Gonzalez-Mora J., 2009, P COMP VIS PATT REC; Gross R, 2006, IMAGE VISION COMPUT, V24, P593, DOI 10.1016/j.imavis.2005.08.001; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Hinterstoisser S, 2011, INT J COMPUT VISION, V91, P107, DOI 10.1007/s11263-010-0379-x; Holzer S., 2012, P EUR C COMP VIS; Holzer S, 2013, IEEE T PATTERN ANAL, V35, P105, DOI 10.1109/TPAMI.2012.86; Jurie F, 2002, IEEE T PATTERN ANAL, V24, P996, DOI 10.1109/TPAMI.2002.1017625; Lucas B.D., 1981, IJCAI 81 P 7 INT JOI, P674, DOI DOI 10.1109/HPDC.2004.1323531; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Matthews I, 2007, INT J COMPUT VISION, V75, P93, DOI 10.1007/s11263-007-0043-2; Megret R, 2010, IEEE T IMAGE PROCESS, V19, P2369, DOI 10.1109/TIP.2010.2048406; Munoz E, 2005, IEEE I CONF COMP VIS, P877; Munoz E., 2009, P IEEE INT C COMP VI, VI; Navarathna R., 2011, P IEEE INT C COMP VI; Nguyen MH, 2010, INT J COMPUT VISION, V88, P69, DOI 10.1007/s11263-009-0299-9; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Romdhani S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P59; Shum HY, 2000, INT J COMPUT VISION, V36, P101, DOI 10.1023/A:1008195814169; Tzimiropoulos G, 2011, IEEE I CONF COMP VIS, P1847, DOI 10.1109/ICCV.2011.6126452; Xu YL, 2008, IEEE T PATTERN ANAL, V30, P1300, DOI 10.1109/TPAMI.2008.81; Zimmermann K, 2009, IEEE T PATTERN ANAL, V31, P677, DOI 10.1109/TPAMI.2008.119	35	4	5	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2015	112	3					354	372		10.1007/s11263-014-0769-6	http://dx.doi.org/10.1007/s11263-014-0769-6			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CG3EA					2022-12-18	WOS:000353159600006
J	Ren, CYH; Prisacariu, V; Reid, I				Ren, Carl Yuheng; Prisacariu, Victor; Reid, Ian			Regressing Local to Global Shape Properties for Online Segmentation and Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Occlusion recovery; Incremental learning; Level-set based tracking; Discrete cosine transform	VISUAL TRACKING; MODELS; FRAMEWORK; OCCLUSION	We propose a novel regression based framework that uses online learned shape information to reconstruct occluded object contours. Our key insight is to regress the global, coarse, properties of shape from its local properties, i.e. its details. We do this by representing shapes using their 2D discrete cosine transforms and by regressing low frequency from high frequency harmonics. We learn this regression model using Locally Weighted Projection Regression which expedites online, incremental learning. After sufficient observation of a set of unoccluded shapes, the learned model can detect occlusion and recover the full shapes from the occluded ones. We demonstrate the ideas using a level-set based tracking system that provides shape and pose, however, the framework could be embedded in any segmentation-based tracking system. Our experiments demonstrate the efficacy of the method on a variety of objects using both real data and artificial data.	[Ren, Carl Yuheng; Prisacariu, Victor; Reid, Ian] Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	University of Oxford	Ren, CYH (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	carl@robots.ox.ac.uk; victor@robos.ox.ac.uk; ian@robots.ox.ac.uk		Reid, Ian/0000-0001-7790-6423	REWIRE Project under the EU 7-Framework Programme [287713]	REWIRE Project under the EU 7-Framework Programme	This work is partially funded by REWIRE Project (Grant No. 287713) under the EU 7-Framework Programme.	Adam A., 2006, IEEE C COMP VIS PATT; Bibby C, 2008, LECT NOTES COMPUT SC, V5303, P831, DOI 10.1007/978-3-540-88688-4_61; Blaschko MB, 2008, LECT NOTES COMPUT SC, V5302, P2, DOI 10.1007/978-3-540-88682-2_2; Chockalingam P, 2009, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2009.5459276; Corduneanu A., 2001, P 8 INT C ART INT ST, P27, DOI DOI 10.1016/J.CSDA.2006.07.020; Cremers D, 2004, LECT NOTES COMPUT SC, V3175, P36; Cremers D, 2007, INT J COMPUT VISION, V72, P195, DOI 10.1007/s11263-006-8711-1; Dambreville S, 2008, IEEE T PATTERN ANAL, V30, P1385, DOI 10.1109/TPAMI.2007.70774; Nguyen-Tuong D, 2011, COGN PROCESS, V12, P319, DOI 10.1007/s10339-011-0404-1; Fritz M, 2005, IEEE I CONF COMP VIS, P1363; Han BY, 2005, IEEE I CONF COMP VIS, P1492; Jepson AD, 2001, PROC CVPR IEEE, P415; Kopicki M., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P5722, DOI 10.1109/ICRA.2011.5980295; KUHL FP, 1982, COMPUT VISION GRAPH, V18, P236, DOI 10.1016/0146-664X(82)90034-X; Kwak S, 2011, IEEE I CONF COMP VIS, P1551, DOI 10.1109/ICCV.2011.6126414; Lawrence N, 2005, J MACH LEARN RES, V6, P1783; LEVENTON ME, 2000, PROC CVPR IEEE, P316, DOI DOI 10.1109/CVPR.2000.855835; Li M, 2013, NEUROCOMPUTING, V101, P338, DOI 10.1016/j.neucom.2012.08.025; Li M, 2010, PROC CVPR IEEE, P1315, DOI 10.1109/CVPR.2010.5539815; Min Li, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P67, DOI 10.1007/978-3-642-19309-5_6; Mirmehdi M., 2009, BRIT MACH VIS C; Nguyen-Tuong D, 2011, NEUROCOMPUTING, V74, P1859, DOI 10.1016/j.neucom.2010.06.033; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Prisacariu V. A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2185, DOI 10.1109/CVPR.2011.5995687; Prisacariu V. A., 2011, INT C COMP VIS; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rathi Y., 2006, P SOC PHOTO-OPT INS, V6064, P425; Ren Y., 2011, BRIT MACH VIS C; Ross D, 2004, LECT NOTES COMPUT SC, V3022, P470; Rousson M, 2002, LECT NOTES COMPUT SC, V2351, P78; RUSU RB, 2007, IEEE INT C ROB AUT I; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Smola A. J., 2004, NEUROCOLT2 TECHNICAL; Tsai A, 2003, IEEE T MED IMAGING, V22, P137, DOI 10.1109/TMI.2002.808355; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Vijayakumar S, 2005, NEURAL COMPUT, V17, P2602, DOI 10.1162/089976605774320557; Watson A. B., 1994, MATH J, V4, P81, DOI DOI 10.1016/0165-1684(90; Yilmaz A, 2004, IEEE T PATTERN ANAL, V26, P1531, DOI 10.1109/TPAMI.2004.96	39	4	5	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2014	106	3			SI		269	281		10.1007/s11263-013-0635-y	http://dx.doi.org/10.1007/s11263-013-0635-y			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AA3DC		Green Published			2022-12-18	WOS:000330972100004
J	Smith, ER; Radke, RJ; Stewart, CV				Smith, Eric R.; Radke, Richard J.; Stewart, Charles V.			Physical Scale Keypoints: Matching and Registration for Combined Intensity/Range Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Range data; Range registration; SIFT; Range/intensity images; Bilateral filter		We present a new framework for detecting, describing, and matching keypoints in combined range-intensity data, resulting in what we call physical scale keypoints. We first produce an image mesh by backprojecting associated 2D intensity images onto the 3D range data. We detect and describe keypoints on the image mesh using an analogue of the SIFT algorithm for images with two key modifications: the process is made insensitive to viewpoint and structural discontinuities using a novel bilinear filter, and a physical scale space is constructed that exploits the reliable range measurements. Keypoints are matched between scans only when their physical scales agree, avoiding many potential false matches. Finally, the matches are rank-ordered using a new quality measure and supplied to a registration algorithm that refines each match into a rigid transformation for the entire scan pair. We report experimental results on keypoint detection and matching and range scan registration and verification in a set of difficult real-world scan pairs, showing that the new physical scale keypoints are demonstrably better than a competing approach based on backprojected SIFT keypoints.			Smith, ER (corresponding author), 110 8th St, Troy, NY 12180 USA.	smithe4@rpi.edu; rjradke@ecse.rpi.edu; stewart@cs.rpi.edu	Radke, Richard J/I-3289-2013	Radke, Richard J/0000-0001-5064-7775	DARPA Computer Science Study Group [HR0011-07-1-0016]	DARPA Computer Science Study Group	This work was supported in part by the DARPA Computer Science Study Group under the award HR0011-07-1-0016.	Akagunduz E., 2009, P IEEE INT C COMP VI; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; BROWN M., 2005, P IEEE C COMP VIS PA; Brown M., 2003, P IEEE INT C COMP VI; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368; Frome A., 2004, P EUR C COMP VIS ECC; Hua J, 2008, IEEE T VIS COMPUT GR, V14, P1643, DOI 10.1109/TVCG.2008.134; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; King B. J., 2005, P INT C 3 D DIG IM M; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MIAN AS, 2004, P EUR C COMP VIS ECC; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Novatnack J., 2007, P IEEE INT C COMP VI; Rusinkiewicz S., 2001, P INT C 3 D DIG IM M; Smith E., 2010, P INT S 3 D DAT PROC; Smith E., 2007, COMPUTER VISION IMAG, V110, P226; Starck J., 2007, P INT C COMP VIS ICC; Wu C., 2008, P IEEE C COMP VIS PA; Xu G, 2004, P GEOM MOD PROC GMP; Zaharescu Andrei, 2009, P IEEE C COMP VIS PA; Zhong Y, 2009, P IEEE INT C COMP VI; Zou GY, 2009, IEEE T VIS COMPUT GR, V15, P1193, DOI 10.1109/TVCG.2009.159	26	4	5	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2012	97	1					2	17		10.1007/s11263-011-0469-4	http://dx.doi.org/10.1007/s11263-011-0469-4			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	897SD					2022-12-18	WOS:000300675300002
J	Caglioti, V; Giusti, A				Caglioti, Vincenzo; Giusti, Alessandro			On the Apparent Transparency of a Motion Blurred Object	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Workshop on Photometric Analysis for Computer Vision held in Conjunction with the 11th International Conference on Computer Vision Conference	OCT 04, 2007	Rio de Janeiro, BRAZIL			Motion blur; Alpha matting; Temporal superresolution; Finite exposure time		An object which moves during the exposure time results in a blurred smear in the image. We consider the smear as if it was the image of a semitransparent object, and we retrieve its alpha matte by means of known techniques. The alpha value at a pixel is meaningfully interpreted as the fraction of the exposure time during which the object projection overlapped that pixel. Basing on this fact, our work highlights interesting qualitative and quantitative properties of the alpha matte, which can be used to derive constraints on the object's apparent contour, and its motion during the exposure time, from a single motion-blurred image; we also show that some of these properties hold on the original image. The theory is validated with experimental results both on synthetic and real images, highlighting strengths and limitations; we point out a range of possible applications, including blurred image interpretation, temporal superresolution of object contours, model-based reconstruction of nontrivial motion, and improvements of alpha matting techniques.	[Caglioti, Vincenzo; Giusti, Alessandro] Politecn Milan, Dipartimento Elettron & Informaz, I-20133 Milan, Italy	Polytechnic University of Milan	Giusti, A (corresponding author), Politecn Milan, Dipartimento Elettron & Informaz, Pza Leonardo da Vinci 32, I-20133 Milan, Italy.	vincenzo.caglioti@polimi.it; alessandro.giusti@polimi.it		Caglioti, Vincenzo/0000-0003-2741-7474; Giusti, Alessandro/0000-0003-1240-0768				APOSTOLOFF NE, 2004, P CVPR 2004; Berman A., 2000, U.S. Patent, Patent No. [6,134,345, 6134345]; BORACCHI G, 2007, P INT C IM AN PROC I; BORACCHI G, 2007, P VISAPP 2007; Chen WG, 1996, IEEE T PATTERN ANAL, V18, P412, DOI 10.1109/34.491622; CHUANG Y, 2001, P CVPR 2001; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; FAVARO P, 2004, P CVPR 2004; FERGUS R, 2006, ACM SIGGRAPH 2006 PA; Furukawa Y, 2006, IEEE T PATTERN ANAL, V28, P302, DOI 10.1109/TPAMI.2006.41; GIUSTI A, 2007, P PACV 2007 WORKSH; GIUSTI A, 2007, P BMVC 2007; JIA J, 2007, P CVPR 2007; KANG S, 1999, P INT C IM PROC ICIP; KLEIN G, 2005, P BMVC 2005; LEVIN A, 2006, P CVPR 2006; Lin HY, 2006, INT C PATT RECOG, P135; Lin HY, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS, P66; LIN HY, 2005, P WORKSH APPL COMP V, P461; Mishima Y., 1993, U.S. Patent, Patent No. [5,355,174, 5355174]; PORTER T, 1984, COMPUTER GRAPHICS; RUZON M, 2000, P CVPR 2000; Shechtman E, 2005, IEEE T PATTERN ANAL, V27, P531, DOI 10.1109/TPAMI.2005.85; SLEPIAN D, 1967, AT&T TECH J, V46, P2353, DOI 10.1002/j.1538-7305.1967.tb02461.x; SMITH AR, 1999, SIGGRAPH 96, P259; Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721; YILMAZ A, 2005, P CVPR 2005	28	4	4	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	2-3			SI		243	255		10.1007/s11263-008-0165-1	http://dx.doi.org/10.1007/s11263-008-0165-1			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	534NA		Green Submitted			2022-12-18	WOS:000272903200009
J	Wilhelmy, J; Kruger, J				Wilhelmy, Jochen; Krueger, Joerg			Shape from Shading Using Probability Functions and Belief Propagation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape-from-shading; Belief propagation; Height reconstruction; Photometric stereo	INTEGRABILITY	Shape-from-shading (SFS) aims to reconstruct the three-dimensional shape of an object from a single shaded image. This article proposes an improved framework based on belief propagation for computing SFS. The implementation of the well-known brightness, integrability and smoothness constraints inside this framework is shown. We implement the constraints as probability density functions. For example, the brightness constraint is a two-dimensional probability density function that relates all possible surface gradients at a pixel to their probability given the pixel intensity. A straightforward extension of the framework to photometric stereo is presented, where multiple images of the same scene taken under different lighting conditions are available. The results are promising, especially since the solution is obtained by iteratively applying simple operations on a regular grid of points. The presented framework therefore can be implemented in parallel and is a reasonably likely biological scheme.	[Krueger, Joerg] Fraunhofer Inst Prod Syst & Design Technol, D-10587 Berlin, Germany	Fraunhofer Gesellschaft		j.wilhelmy@arcor.de						Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; BICHSEL M, 1992, COMPUTER VISION PATT, V92, P459; BROOKS MJ, 1985, P INT JOINT C ART IN, P932; Fan J, 1997, COMPUT VIS IMAGE UND, V65, P347, DOI 10.1006/cviu.1996.0581; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; HESKES T, 2003, UNCERTAINTY ARTIFICI; Horn B.K.P., 1989, SHAPE SHADING; HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3; HORN BKP, 1990, INT J COMPUT VISION, V5, P37, DOI 10.1007/BF00056771; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; Kimmel R, 2001, J MATH IMAGING VIS, V14, P237, DOI 10.1023/A:1011234012449; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; OLIENSIS J, 1991, P SOC PHOTO-OPT INS, V1570, P116, DOI 10.1117/12.48418; ONN R, 1990, INT J COMPUT VISION, V5, P105, DOI 10.1007/BF00056773; Pentland A, 1988, P INT C COMP VIS, P404; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P170, DOI 10.1109/TPAMI.1984.4767501; Petrovic N, 2001, PROC CVPR IEEE, P743; POTETZ B, 2007, P C COMP VIS PATT RE; Ragheb H, 2003, PATTERN RECOGN LETT, V24, P579, DOI 10.1016/S0167-8655(02)00278-7; Sethian JA, 1999, SIAM REV, V41, P199, DOI 10.1137/S0036144598347059; Shon AP, 2005, NEUROCOMPUTING, V65, P393, DOI 10.1016/j.neucom.2004.10.035; STRAT TM, 1979, THESIS MIT CAMBRIDGE; Tankus A, 2005, INT J COMPUT VISION, V63, P21, DOI 10.1007/s11263-005-4945-6; TSAI PS, 1994, IMAGE VISION COMPUT, V12, P487, DOI 10.1016/0262-8856(94)90002-7; Woodham R. J., 1989, SHAPE SHADING, P513; Worthington PL, 1999, IEEE T PATTERN ANAL, V21, P1250, DOI 10.1109/34.817406; Wu TP, 2005, PROC CVPR IEEE, P140; YUILLE A, 2001, P 3 INT WORKSH EN MI; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658	30	4	4	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2009	84	3					269	287		10.1007/s11263-009-0236-y	http://dx.doi.org/10.1007/s11263-009-0236-y			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	458MS					2022-12-18	WOS:000267028600003
J	Damon, J; Giblin, P; Haslinger, G				Damon, James; Giblin, Peter; Haslinger, Gareth			Local Image Features Resulting from 3-Dimensional Geometric Features, Illumination, and Movement: I	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Illuminated surface; Piecewise smooth surface; Geometric features; Shade; Shadow; Viewer movement; Stable Configuration; Generic transition; Singularity theory	CURVED OBJECTS; SINGULARITIES; PROJECTIONS; SURFACES; MAPPINGS	We study images of smooth or piecewise smooth objects illuminated by a single light source, with only background illumination from other sources. The objects may have geometric features (F), namely surface markings, boundary edges, creases and corners; and shade features (S), namely shade curves and cast shadow curves. We determine the local stable interactions between these and apparent contours (C) for the various configurations of F, S, C, and we concisely summarize them using an "alphabet" of local curve configurations. We further determine the generic transitions for the configurations resulting from viewer movement. These classifications are obtained using the methods of singularity theory, which allows us to ensure that our lists are complete, in some cases correcting earlier attempts at similar classifications.	[Damon, James] Univ N Carolina, Dept Math, Chapel Hill, NC 27599 USA; [Giblin, Peter; Haslinger, Gareth] Univ Liverpool, Dept Math Sci, Liverpool L69 7ZL, Merseyside, England	University of North Carolina; University of North Carolina Chapel Hill; University of Liverpool	Damon, J (corresponding author), Univ N Carolina, Dept Math, Chapel Hill, NC 27599 USA.	jndamon@math.unc.edu; pjgiblin@liv.ac.uk			European Commission; National Science Foundation [DMS-0405947, DMS-0706941]	European Commission(European CommissionEuropean Commission Joint Research Centre); National Science Foundation(National Science Foundation (NSF))	Authors were partially supported by Insight 2+ grant from the European Commission. J. Damon was partially supported by the National Science Foundation grants DMS-0405947 and DMS-0706941.	Arnold V.I., 1979, RUSS MATH SURV+, V34, P1; Bruce J.W., 1992, CURVES SINGULARITIES; BRUCE JW, 1990, P LOND MATH SOC, V60, P392; Caselles V, 1996, PROG NONLIN, V25, P35; Caselles V, 1999, INT J COMPUT VISION, V33, P5, DOI 10.1023/A:1008144113494; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; DAMON J, 1983, P SYMP PURE MATH, V40, P233; DAMON J, 1984, MEM AM MATH SOC, P306; DAMON J, 2008, LOCAL IMAGE F UNPUB; DAMON J, 2008, LOCAL IMAGE FEATURES; DAMON J, 1988, MEM AM MATH SOC, P389; DAMON J, 2008, CHARACTERIZING STABL; DEMAZURE M, 1992, ARTIFICIAL BIOL VISI; Donati L., 1995, THESIS U NICE SOPHIA; DUFOUR JP, 1977, ANN SCI ECOLE NORM S, V10, P153; Fitzgerald A., 1999, PROJECTIONS ILLUMINA; GAFFNEY T, 1983, AMS P S PURE MATH 1, V40, P409; Giblin P, 1998, PHILOS T R SOC A, V356, P1087, DOI 10.1098/rsta.1998.0212; GORYUNOV VV, 1990, ADV SOVIET MATH, V1, P157; HENRY JP, 1993, PROG MATH, V109, P105; Horn B., 1986, ROBOT VISION, P1; Horn B. K. P., 1989, SHAPE SHADING; HUFFMAN DA, 1977, MACH INTELL, V8, P493; Koenderink J., 1990, SOLID SHAPE; Koenderink J.J., 2008, INT J COMPUTATIONAL, V1, P45; KOENDERINK JJ, 1976, BIOL CYBERN, V24, P51, DOI 10.1007/BF00365595; Kriegman D., 1990, P 1990 AAAI C BOST M, P1074; MACKWORTH AK, 1973, ARTIF INTELL, V4, P121, DOI 10.1016/0004-3702(73)90003-9; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; MATHER JN, 1973, ANN MATH, V98, P226, DOI 10.2307/1970783; PETITJEAN S, 1992, INT J COMPUT VISION, V9, P231, DOI 10.1007/BF00133703; RIEGER JH, 1987, IMAGE VISION COMPUT, V5, P91, DOI 10.1016/0262-8856(87)90033-3; Sugihara K., 1986, MACHINE INTERPRETATI; TARI F, 1991, J LOND MATH SOC, V44, P155; Tari F., 1990, THESIS U LIVERPOOL; Varley PAC, 2004, GEOMETRIC MODELING AND PROCESSING 2004, PROCEEDINGS, P249, DOI 10.1109/GMAP.2004.1290046; WHITNEY H, 1955, ANN MATH, V62, P374, DOI 10.2307/1970070	37	4	4	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2009	82	1					25	47		10.1007/s11263-008-0182-0	http://dx.doi.org/10.1007/s11263-008-0182-0			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	402AH					2022-12-18	WOS:000262986100002
J	Goshen, L; Shimshoni, I				Goshen, Liran; Shimshoni, Ilan			Guided sampling via weak motion models and outlier sample generation for epipolar geometry estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						epipolar geometry estimation; robust methods; weak motion models	ROBUST ESTIMATOR; SEGMENTATION; RANSAC	The problem of automatic robust estimation of the epipolar geometry in cases where the correspondences are contaminated with a high percentage of outliers is addressed. This situation often occurs when the images have undergone a significant deformation, either due to large rotation or wide baseline of the cameras. An accelerated algorithm for the identification of the false matches between the views is presented. The algorithm generates a set of weak motion models (WMMs). Each WMM roughly approximates the motion of correspondences from one image to the other. The algorithm represents the distribution of the median of the geometric distances of a correspondence to the WMMs as a mixture model of outlier correspondences and inlier correspondences. The algorithm generates a sample of outlier correspondences from the data. This sample is used to estimate the outlier rate and to estimate the outlier pdf. Using these two pdfs the probability that each correspondence is an inlier is estimated. These probabilities enable guided sampling. In the RANSAC process this guided sampling accelerates the search process. The resulting algorithm when tested on real images achieves a speedup of between one or two orders of magnitude.	[Shimshoni, Ilan] Univ Haifa, Dept Management Informat Syst, IL-31905 Haifa, Israel; [Goshen, Liran] Technion Israel Inst Technol, Fac Ind Engn & Management, IL-32000 Haifa, Israel	University of Haifa; Technion Israel Institute of Technology	Shimshoni, I (corresponding author), Univ Haifa, Dept Management Informat Syst, IL-31905 Haifa, Israel.	ishimshoni@mis.haifa.ac.il		Shimshoni, Ilan/0000-0002-5276-0242	Ministry of Science Culture and Sports of Israel [01-99-08430]	Ministry of Science Culture and Sports of Israel	This work was supported partly by grant 01-99-08430 of the Israeli Space Agency through the Ministry of Science Culture and Sports of Israel.	Adam A, 2001, IEEE T PATTERN ANAL, V23, P78, DOI 10.1109/34.899948; Chen HF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P878, DOI 10.1109/ICCV.2003.1238441; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Goshen L, 2005, PROC CVPR IEEE, P1105; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Lee KM, 1998, IEEE T PATTERN ANAL, V20, P200, DOI 10.1109/34.659940; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Matas J, 2004, IMAGE VISION COMPUT, V22, P837, DOI 10.1016/j.imavis.2004.02.009; Moisan L, 2004, INT J COMPUT VISION, V57, P201, DOI 10.1023/B:VISI.0000013094.38752.54; Rozenfeld S, 2005, PROC CVPR IEEE, P1113; STEWART CV, 1995, IEEE T PATTERN ANAL, V17, P925, DOI 10.1109/34.464558; SUBBARAO R, 2007, P IEEE C COMP VIS PA, P1; Tordoff B, 2002, LECT NOTES COMPUT SC, V2350, P82; Torr P.H.S., 1995, THESIS U OXFORD; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Torr PHS, 2003, IEEE T PATTERN ANAL, V25, P354, DOI 10.1109/TPAMI.2003.1182098; Triggs B, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P201, DOI 10.1109/ICCV.2001.937625; TRIGGS B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P338, DOI 10.1109/ICCV.1995.466920; Wand M.P., 1995, KERNEL SMOOTHING; Wang HZ, 2004, IEEE T PATTERN ANAL, V26, P1459, DOI 10.1109/TPAMI.2004.109; Wang HZ, 2004, INT J COMPUT VISION, V59, P139, DOI 10.1023/B:VISI.0000022287.61260.b0; YU XM, 1994, IEEE T PATTERN ANAL, V16, P530, DOI 10.1109/34.291443	25	4	5	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2008	80	2					275	288		10.1007/s11263-008-0126-8	http://dx.doi.org/10.1007/s11263-008-0126-8			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	348DA					2022-12-18	WOS:000259190000007
J	Freeman, W; Perona, P; Scholkopf, B				Freeman, William; Perona, Pietro; Schoelkopf, Bernhard			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Freeman, William] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA; [Perona, Pietro] CALTECH, Dept Elect Engn & Computat & Neural Syst, Pasadena, CA 91125 USA; [Schoelkopf, Bernhard] Max Planck Inst Biol Cybernet, Dept Empirical Interface, D-72076 Tubingen, Germany	Massachusetts Institute of Technology (MIT); California Institute of Technology; Max Planck Society	Freeman, W (corresponding author), MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.	billf@mit.edu	Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925					0	4	4	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2008	77	1-3					1	1		10.1007/s11263-008-0127-7	http://dx.doi.org/10.1007/s11263-008-0127-7			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	267RE		Green Submitted, hybrid			2022-12-18	WOS:000253526100001
J	Leonard, K				Leonard, Kathryn			Efficient shape modeling: epsilon-entropy, adaptive coding, and boundary curves -vs- blum's medial axis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						epsilon-entropy; adaptive coding; shape modeling; medial axis; boundary curve; shape space		We propose efficiency of representation as a criterion for evaluating shape models, then apply this criterion to compare the boundary curve representation with the medial axis. We estimate the epsilon-entropy of two compact classes of curves. We then construct two adaptive encodings for non-compact classes of shapes, one using the boundary curve and the other using the medial axis, and determine precise conditions for when the medial axis is more efficient. Finally, we apply our results to databases of naturally occurring shapes, determining whether the boundary or medial axis is more efficient. Along the way we construct explicit near-optimal boundary-based approximations for compact classes of shapes, construct an explicit compression scheme for non-compact classes of shapes based on the medial axis, and derive some new results about the medial axis.	CALTECH, Dept Appl & Computat Math, Pasadena, CA 91125 USA	California Institute of Technology	Leonard, K (corresponding author), CALTECH, Dept Appl & Computat Math, Pasadena, CA 91125 USA.	kathryn@acm.caltech.edu						BEG MF, 2003, INT J COMPUTER VISIO; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; Damon J, 2005, INT J COMPUT VISION, V63, P45, DOI 10.1007/s11263-005-4946-5; Desolneux A, 2000, INT J COMPUT VISION, V40, P7, DOI 10.1023/A:1026593302236; FOLLAND G, 1984, REAL ANAL, P55; Giblin PJ, 2003, IEEE T PATTERN ANAL, V25, P895, DOI 10.1109/TPAMI.2003.1206518; Katz RA, 2003, INT J COMPUT VISION, V55, P139, DOI 10.1023/A:1026183017197; Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333; Kolmogorov A. N., 1961, AM MATH SOC TRANSL, V17, P277; LANGDON GG, 1981, IEEE T COMMUN, V29, P858, DOI 10.1109/TCOM.1981.1095052; LEONARD K, 2005, MASSIVENESS SPACES P; LEONARD K, 2004, THESIS BROWN U; Li W, 1997, VISION RES, V37, P565, DOI 10.1016/S0042-6989(96)00166-6; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Mokhtarian F., 1996, P BRIT MACH VIS C, P53; Rissanen Jorma, 1989, STOCHASTIC COMPLEXIT; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Sebastian T, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P755, DOI 10.1109/ICCV.2001.937602; SHANKS D, 1993, SOLVED UNSOLVED PROB, P143; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; SHARVITD, 1998, CONTENT BASED ACCES; Siddiqi K, 1996, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.1996.517119; Sloane N. J. A., ONLINE ENCY INTEGER; TAMRAKAR A, 2004, P IEEE WORKSH PERC O; Thalmann D, 1996, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P166, DOI 10.1109/CGI.1996.511798; Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x; YU SX, 2003, IEEE C COMP VIS PATT; YUSHKEVICH P, 2002, P 1 GEN MOD BAS VIS; Zhu SC, 1996, INT J COMPUT VISION, V20, P187; Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1158, DOI 10.1109/34.809109	31	4	4	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2007	74	2					183	199		10.1007/s11263-006-0010-3	http://dx.doi.org/10.1007/s11263-006-0010-3			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	167ZH					2022-12-18	WOS:000246490900005
J	Cooper, MC				Cooper, Martin C.			Constraints between distant lines in the labelling of line drawings of polyhedral scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						line drawing labelling; parallel lines; polyhedral objects; non-trihedral vertices; valued constraint satisfaction problem; soft constraints	CURVED OBJECTS; PROJECTIONS; COMPLEXITY; RECONSTRUCTION; REALIZABILITY; CONSISTENCY	The machine interpretation of line drawings has applications both in vision and geometric modelling. This paper extends the classic technique of assigning semantic labels to lines subject to junction constraints, by introducing new constraints (often between distant lines). These include generic constraints between lines lying on a path in the drawing as well as preference constraints between the labellings of pairs of junctions lying on parallel lines. Such constraints are essential to avoid an exponential number of legal labellings of drawings of objects with non-trihedral vertices The strength of these constraints is demonstrated by their ability to identify the unique correct labelling of many drawings of polyhedral objects with tetrahedral vertices. These new constraints also allowed us to deduce a general polyhedral junction constraint for the case when there is no limit on the number of faces which can meet at a junction.	Univ Toulouse 3, IRIT, F-31062 Toulouse, France	Universite de Toulouse; Universite Toulouse III - Paul Sabatier	Cooper, MC (corresponding author), Univ Toulouse 3, IRIT, 118 Route Narbonne, F-31062 Toulouse, France.	cooper@irit.fr	Cooper, Martin/AAE-8777-2020; Cooper, Martin/AAV-1705-2021	Cooper, Martin/0000-0003-4853-053X; Cooper, Martin/0000-0003-4853-053X				ARLEY PAC, 2002, P 7 ACM S SOL MOD AP, P180; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; Cooper M, 2004, ARTIF INTELL, V154, P199, DOI 10.1016/j.artint.2003.09.002; Cooper MC, 2005, INT J COMPUT VISION, V64, P69, DOI 10.1007/s11263-005-1087-9; Cooper MC, 2004, ARTIF INTELL, V155, P69, DOI 10.1016/j.artint.2003.06.004; Cooper MC, 2003, FUZZY SET SYST, V134, P311, DOI 10.1016/S0165-0114(02)00134-3; Cooper MC, 1998, INT J COMPUT VISION, V30, P27, DOI 10.1023/A:1008013412628; Cooper MC, 2001, INT J COMPUT VISION, V43, P75, DOI 10.1023/A:1011166601983; Cooper MC, 1999, ARTIF INTELL, V108, P31, DOI 10.1016/S0004-3702(98)00118-0; COOPER MC, 1993, IMAGE VISION COMPUT, V11, P82, DOI 10.1016/0262-8856(93)90074-Q; Cooper MC, 1997, IMAGE VISION COMPUT, V15, P263, DOI 10.1016/S0262-8856(96)01135-3; Cooper MC, 2000, ARTIF INTELL, V119, P235, DOI 10.1016/S0004-3702(00)00008-4; DRAPER SW, 1981, ARTIF INTELL, V17, P461, DOI 10.1016/0004-3702(81)90032-1; Grimstead IJ, 1996, COMPUT GRAPH FORUM, V15, P155, DOI 10.1111/1467-8659.1520155; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; HUFFMAN DA, 1977, MACH INTELL, V8, P493; KIROUSIS LM, 1990, IEEE T PATTERN ANAL, V12, P123, DOI 10.1109/34.44400; KIROUSIS LM, 1988, J COMPUT SYST SCI, V37, P14, DOI 10.1016/0022-0000(88)90043-8; Larrosa J., 2003, P 18 INT JOINT C ART, P239; Lipson H, 1996, COMPUT AIDED DESIGN, V28, P651, DOI 10.1016/0010-4485(95)00081-X; Malin S. R. C., 1987, GEOMAGNETISM, V1, P1; Parodi P, 1998, ARTIF INTELL, V105, P47, DOI 10.1016/S0004-3702(98)00077-0; PARODI P, 1994, ARTIF INTELL, V70, P239, DOI 10.1016/0004-3702(94)90107-4; PASTOR JC, 1999, P 11 INT C DES TOOLS, P161; SCHIEX T, 1995, P 14 INT JOINT C ART, P631; SUGIHARA, 1986, MACHINE INTERPRETATI; SUGIHARA K, 1984, IEEE T PATTERN ANAL, V6, P578, DOI 10.1109/TPAMI.1984.4767571; Varley P. A. C., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P13, DOI 10.1109/GMAP.2000.838235; Varley P. A. C., 2001, International Journal of Shape Modeling, V7, P23, DOI 10.1142/S0218654301000035; Varley P. A. C., 2003, International Journal of Shape Modeling, V9, P79, DOI 10.1142/S0218654303000061; Varley PAC, 2004, GEOMETRIC MODELING AND PROCESSING 2004, PROCEEDINGS, P249, DOI 10.1109/GMAP.2004.1290046; VARLEY PAC, 2004, E COMMUNICATION; VICENT AP, 2003, J WSCG, V11; Waltz D., 1975, PSYCHOL COMPUTER VIS, P19	34	4	5	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2007	73	2					195	212		10.1007/s11263-006-9783-7	http://dx.doi.org/10.1007/s11263-006-9783-7			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	146OI					2022-12-18	WOS:000244943500005
J	Pauwels, K; Lappe, M; Van Hulle, MM				Pauwels, Karl; Lappe, Markus; Van Hulle, Marc M.			Fixation as a mechanism for stabilization of short image sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Early Cognitive Vision Workshop	MAY 29-JUN 01, 2004	Isle Skye, SCOTLAND	ECOVISION			OPTICAL-FLOW; MOTION ESTIMATION; ROAD NAVIGATION; VISUAL-MOTION; PERFORMANCE; ALGORITHMS; RECOVERY	A novel method is introduced for the stabilization of short image sequences. Stabilization is achieved by means of fixation of the central image region using a variable window size block matching method. When applied to a sliding temporal window, the stabilization improves the performance of standard optic flow techniques. Due to the unique choice of fixation as the main stabilization mechanism, the proposed method not only increases the flow field density but renders certain global structural properties of the flow fields more predictable as well. This in turn is advantageous for egomotion computation.	Katholieke Univ Leuven, Lab Neuroen Psychofysiol, Louvain, Belgium; Univ Munster, Inst Psychol 2, D-4400 Munster, Germany	KU Leuven; University of Munster	Pauwels, K (corresponding author), Katholieke Univ Leuven, Lab Neuroen Psychofysiol, Louvain, Belgium.		Pauwels, Karl/B-1074-2013	Pauwels, Karl/0000-0003-3731-0582; Van Hulle, Marc/0000-0003-1060-7044				ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; ALOIMONOS Y, 1987, INT J COMPUT VISION, V1, P333; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; Balakirsky SB, 1996, REAL-TIME IMAGING, V2, P297, DOI 10.1006/rtim.1996.0031; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; CALOW D, 2006, IN PRESS NETWORK COM; Daniilidis K, 1997, COMPUT VIS IMAGE UND, V68, P158, DOI 10.1006/cviu.1997.0535; Duric Z, 2003, MACH VISION APPL, V13, P303, DOI 10.1007/s00138-002-0085-y; FERMULLER C, 1993, INT J COMPUT VISION, V11, P165, DOI 10.1007/BF01469227; Gautama T, 2002, IEEE T NEURAL NETWOR, V13, P1127, DOI 10.1109/TNN.2002.1031944; Giachetti A, 1998, IEEE T ROBOTIC AUTOM, V14, P34, DOI 10.1109/70.660838; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Hsu J., 1996, MULTIPLE COMP THEORY; Irani M, 1997, IEEE T PATTERN ANAL, V19, P268, DOI 10.1109/34.584105; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; Lappe M, 1996, NEURAL COMPUT, V8, P1449, DOI 10.1162/neco.1996.8.7.1449; LAPPE M, 1995, BIOL CYBERN, V72, P261, DOI 10.1007/BF00201489; LAPPE M, 2000, NEURONAL PROCESSING, P29; Lewis JP, 1995, VISION INTERFACE, P120; Lin T., 1995, RES COMPUTER ROBOT V, P269, DOI DOI 10.1142/9789812812483_0016; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; Morimoto C, 1996, REAL-TIME IMAGING, V2, P285, DOI 10.1006/rtim.1996.0030; Reddy BS, 1996, IEEE T IMAGE PROCESS, V5, P1266, DOI 10.1109/83.506761; TAALEBINEZHAAD MA, 1992, IEEE T PATTERN ANAL, V14, P847, DOI 10.1109/34.149584; Xiang T, 2003, INT J COMPUT VISION, V51, P111, DOI 10.1023/A:1021627622971; Zhang T, 2002, INT J COMPUT VISION, V46, P51, DOI 10.1023/A:1013248231976	27	4	4	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2007	72	1					67	78		10.1007/s11263-006-8893-6	http://dx.doi.org/10.1007/s11263-006-8893-6			12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	122QR		Green Accepted, Green Submitted			2022-12-18	WOS:000243242000005
J	Haro, G; Bertalmio, M; Caselles, V				Haro, Gloria; Bertalmio, Marcelo; Caselles, Vicent			Visual acuity in day for night	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	2nd IEEE Workshop on Variational, Geometric and Level Set Methods held in Conjunction with the IEEE International Conference on Computer Vision	OCT, 2003	Nice, FRANCE	IEEE, French Natl Inst Res Comp Sci & Control, Siemens Corp Res, Imaging & Visualizat Dept		day for night; visual perception; dark adaptation; non-linear diffusion		In film production, it is sometimes not convenient or directly impossible to shoot some night scenes at night. The film budget, schedule or location may not allow it. In these cases. the scenes are shot at daytime. and the,night look' is achieved by placing a blue filter in front of the lens and under-exposing the film. This technique. that the American film industry has used for many decades. is called 'Day for Night' (or 'American Night' in Europe.) But the images thus obtained don't usually took realistic: they tend to be too bluish, and the objects' brightness seems unnatural for night-light. In this article we introduce a digital Day for Night algorithm that achieves very realistic results. We use a set of very simple equations, based oil real physical data and visual perception experimental data. To simulate the loss of visual acuity we introduce a novel diffusion Partial Differential Equation (PDE) which takes luminance into account and respects contrast, produces no ringing. is stable. very easy to implement and fast. The user only provides the original day image and the desired level of darkness of the result. The whole process from original day image to final night image is implemented in a few seconds, computations being mostly local.	Univ Pompeu Fabra, Dept Tecnol, Barcelona, Spain	Pompeu Fabra University	Haro, G (corresponding author), Univ Pompeu Fabra, Dept Tecnol, Barcelona, Spain.	gloria.haro@upf.edu; marcelo.berialmio@upf.edu; vicent.caselles@upf.edu	Bertalmío, Marcelo/A-4341-2012; Haro, Gloria/D-3394-2014	Haro, Gloria/0000-0002-8194-8092; Bertalmio, Marcelo/0000-0002-1023-8325				ALVAREZ L, 1993, ARCH ROTIONAL MECH, P200; BENILAN P, 1981, INDIANA U MATH J, V30, P161, DOI 10.1512/iumj.1981.30.30014; BERTALMIO M, 2004, P 2 3DPVT; CORNSWEET TN, 1985, J OPT SOC AM A, V2, P1769, DOI 10.1364/JOSAA.2.001769; DURAND F, 2000, P EUR WORKSH REND; Ferwerda JA, 1996, P 23 ANN C COMP GRAP, P249, DOI [10.1145/237170.237262, DOI 10.1145/237170.237262]; FINLAYSON GD, 2002, ECCV; GUICHARD F, 2000, UNPUB IMAGE ITERATIV; HUNT RWG, 1952, J OPT SOC AM, V42, P190, DOI 10.1364/JOSA.42.000190; Lumet Sidney., 1995, MAKING MOVIES; MALONEY LT, 1986, J OPT SOC AM A, V3, P1673, DOI 10.1364/JOSAA.3.001673; MASSEY P, 2000, PUBLICATIONS ASTRONO, V112; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Ramanath R, 2004, COLOR RES APPL, V29, P29, DOI 10.1002/col.10211; SLATER D, 1997, IEEE TPAMI, V19; Stabell B, 2002, J OPT SOC AM A, V19, P1249, DOI 10.1364/JOSAA.19.001249; Thompson LJ, 2002, HELICOBACTER, V7, P1, DOI 10.1046/j.1523-5378.7.s1.1.x; TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554; Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544; WARD G, 1997, IEEE T VISUALIZATION, V3; Wyszecki G., 2000, COLOR SCI CONCEPTS M, V2nd	21	4	5	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	1					109	117		10.1007/s11263-006-6858-4	http://dx.doi.org/10.1007/s11263-006-6858-4			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	063QH					2022-12-18	WOS:000239034100008
J	Mio, W; Srivastava, A; Liu, XW				Mio, Washington; Srivastava, Anuj; Liu, Xiuwen			Contour inferences for image understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	2nd IEEE Workshop on Variational, Geometric and Level Set Methods held in Conjunction with the IEEE International Conference on Computer Vision	OCT, 2003	Nice, FRANCE	IEEE, French Natl Inst Res Comp Sci & Control, Siemens Corp Res, Imaging & Visualizat Dept		curve interpolation; elastica; partial occlusions; shape completion	SHAPE	We present a new approach to the algorithmic study of planar curves, with applications to estimations of contours in images. We construct spaces of curves satisfying constraints suited to specific problems. exploit their geometric structure to quantify properties of contours, and solve optimization and inference problems. Applications include new algorithms for computing planar elasticae, with enhanced performance and speed. and geometric algorithms for the estimation of contours of partially occluded objects in images.	Florida State Univ, Dept Math, Tallahassee, FL 32306 USA; Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA; Florida State Univ, Dept Comp Sci, Tallahassee, FL 32306 USA	State University System of Florida; Florida State University; State University System of Florida; Florida State University; State University System of Florida; Florida State University	Mio, W (corresponding author), Florida State Univ, Dept Math, Tallahassee, FL 32306 USA.	mio@math.fsu.edu; anuj@stat.fsu.edu; liux@cs.fsu.edu	Srivastava, Anuj/L-4705-2019; Srivastava, Anuj/F-7417-2011					BRUCKSTEIN AM, 1990, COMPUT VISION GRAPH, V49, P283, DOI 10.1016/0734-189X(90)90105-5; Chan T. F., 2003, NOT AM MATH SOC, V50, P14; DOCARMO VP, 1976, DIFFERENTIAL GEOMETR; EULER L, 1744, BOUSQUET LAUSAN EA 1, V65, P24; HORN BKP, 1983, ACM T MATH SOFTWARE, V9, P441, DOI 10.1145/356056.356061; JOSHI S, 2004, P ECCV 2004 LNCS PRA, P570; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kimia BB, 2003, INT J COMPUT VISION, V54, P157, DOI 10.1023/A:1023713602895; Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333; Mio W, 2004, Q APPL MATH, V62, P359, DOI 10.1090/qam/2054604; MIO W, 2004, LNCS, P62; Mumford D., 1994, ALGEBRAIC GEOMETRY I, V5681, P491, DOI DOI 10.1007/978-1-4612-2628-4_31; Palais R. S., 1963, TOPOLOGY, V2, P299; Royden HL, 1988, REAL ANAL; Sethian JA, 1996, LEVEL SET METHODS EV; Sharon E, 2000, IEEE T PATTERN ANAL, V22, P1117, DOI 10.1109/34.879792; WEISS I, 1988, COMPUT VISION GRAPH, V41, P80, DOI 10.1016/0734-189X(88)90118-1; Williams LR, 1997, NEURAL COMPUT, V9, P837, DOI 10.1162/neco.1997.9.4.837	19	4	4	2	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	1					137	144		10.1007/s11263-006-6856-6	http://dx.doi.org/10.1007/s11263-006-6856-6			8	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	063QH					2022-12-18	WOS:000239034100011
J	Gong, ML; Yang, YH				Gong, Minglun; Yang, Yee-Hong			Estimate large motions using the reliability-based motion estimation algorithm	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion estimation; optical flow; dynamic programming; reliability-based dynamic programming	OPTICAL-FLOW; STEREO	Detecting and estimating motions of fast moving objects has many important applications. However, most existing motion estimation techniques have difficulties in handling large motions in the scene. In this paper, we extend our recently proposed reliability-based stereo vision technique to solving large motion estimation problem. Compared with our stereo vision approach, the new algorithm removes the constant penalty assumption and explicitly enforces the inter-scanline consistency constraint. The resulting algorithm can handle sequences that contain large motions and can produce optical flows with 100% density over the entire image domain. The experimental results indicate that it can generate more accurate optical flows than existing approaches.	Laurentian Univ, Dept Math & Comp Sci, Sudbury, ON P3E 2C6, Canada; Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2M7, Canada	Laurentian University; University of Alberta	Gong, ML (corresponding author), Laurentian Univ, Dept Math & Comp Sci, Sudbury, ON P3E 2C6, Canada.	mgong@cs.laurentian.ca; yang@cs.ualberta.ca		Gong, Minglun/0000-0001-5820-5381				Alvarez L, 2000, INT J COMPUT VISION, V39, P41, DOI 10.1023/A:1008170101536; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BERGEN JR, 1992, P EUR C COMP VIS, P237; Bobick AF, 1999, INT J COMPUT VISION, V33, P181, DOI 10.1023/A:1008150329890; ELZENSZWALB PF, 2004, P CVPR; Gong M, 2005, PROC CVPR IEEE, P924; GONG M, 2004, P ICIP SING, P24; Gong ML, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P610; Gong ML, 2005, IEEE T PATTERN ANAL, V27, P998, DOI 10.1109/TPAMI.2005.120; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Lucas B.D., 1981, ITERATIVE IMAGE REGI, P674; McCane B, 2001, COMPUT VIS IMAGE UND, V84, P126, DOI 10.1006/cviu.2001.0930; NAGEL HH, 1987, ARTIF INTELL, V33, P299, DOI 10.1016/0004-3702(87)90041-5; ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029; OTTE M, 1994, P 3 EUR C COMP VIS S, P51; QUENOT GM, 1996, WORKSH MACH VIS APPL, P249; ROSENFELD A, 1968, PATTERN RECOGN, V1, P33, DOI 10.1016/0031-3203(68)90013-7; Singh A., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P168, DOI 10.1109/ICCV.1990.139516; Sun CM, 2002, IMAGE VISION COMPUT, V20, P981, DOI 10.1016/S0262-8856(02)00112-9; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895	21	4	4	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2006	68	3					319	330		10.1007/s11263-006-5099-x	http://dx.doi.org/10.1007/s11263-006-5099-x			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	064NU					2022-12-18	WOS:000239097200005
J	Kim, H; Kweon, IS				Kim, H; Kweon, IS			Appearance-cloning: Photo-consistent scene recovery from multi-view images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	6th Asian Conference on Computer Vision	JAN 28-30, 2004	Cheju Isl, SOUTH KOREA			image-based modeling; volumetric reconstruction; voxel coloring; space carving; appearance; photo-consistency; color similarity	STEREO; SHAPE	This paper introduces the novel volumetric methodology "appearance-cloning" as a viable solution for achieving, a more improved photo-consistent scene recovery, including a greatly enhanced geometric recovery performance, from a set of photographs taken at arbitrarily distributed multiple camera viewpoints. We do so while solving many of the problems associated with previous stereo-based and volumetric methodologies. We redesign the photo-consistency decision problem of individual voxel in volumetric space as the photo-consistent shape search problem in image space, by generalizing the concept of the point correspondence search between two images in stereo-based approach, within a volumetric framework. In detail, we introduce a self-constrained greedy-style optimization methodology, which iteratively searches a more photo-consistent shape based on the probabilistic shape photo-consistency measure, by using the probabilistic competition between candidate shapes. Our new measure is designed to bring back the probabilistic photo- consistency of a shape by comparing the appearances captured from multiple cameras with those rendered from that shape using the per-pixel Maxwell model in image space. Through various scene recoveries experiments including specular and dynamic scenes, we demonstrate that if sufficient appearances are given enough to reflect scene characteristics, our appearance-cloning approach can successfully recover both the geometry and photometry information of a scene without any kind of scene-dependent algorithm tuning.	Korea Adv Inst Sci & Technol, Dept Elect Engn & Comp Sci, Taejon 305701, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Kim, H (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn & Comp Sci, 373-1 Guseong Dong, Taejon 305701, South Korea.	imp97@kaist.ac.kr; iskweon@kaist.ac.kr	Kweon, In So/C-2023-2011					BAKER HH, 1981, INT JOINT C ART INT, P631; BHOTIKA R, 2003, THESIS U ROCHESTER C; BHOTIKA R, 2002, P EUR C COMP VIS, V3, P112; Broadhurst A., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P282; Broadhurst A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P388, DOI 10.1109/ICCV.2001.937544; Broadhurst A., 2001, THESIS U CAMBRIDGE; CHEUNG KM, 2003, CMURITR0344; CULBERTSON WB, 1999, SPRINGER VERLAG LECT, V1883, P100; Dinh HQ, 2002, IEEE T PATTERN ANAL, V24, P1358, DOI 10.1109/TPAMI.2002.1039207; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; GARLAND M, 1997, COMPUTER GRAPHICS SI; IKEUCHI K, 2003, IEEE ACM INT S MIX A; Kim JS, 2002, P 5 AS C COMP VIS, P515; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; KUTULAKOS KN, 1999, IEEE INT C COMP VIS, P307; KUTULAKOS KN, 2000, P EUR C COMP VIS, P67; KUTULAKOS KN, 1998, TR680 U ROCH COMP SC; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; LEVOY M, 2000, COMPUTER GRAPHICS SI; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; Mortensen E. N., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P191, DOI 10.1145/218380.218442; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; PROCK A, 1998, IM UND WORKSH, P315; Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462; Slabaugh G, 2001, SPRING EUROGRAP, P81; SLABAUGH G, 2002, THESIS GEORGIA I TEC; Slabaugh GG, 2004, INT J COMPUT VISION, V57, P179, DOI 10.1023/B:VISI.0000013093.45070.3b; Stevens MR, 2002, INT C PATT RECOG, P118, DOI 10.1109/ICPR.2002.1047413; SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029; VEDULA S, 2000, P IEEE COMP SOC C CO, P324; YANG R, 2003, P INT C COMP VIS; Yezzi A, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P618, DOI 10.1109/TDPVT.2002.1024126; Yoon KJ, 2004, ELECTRON LETT, V40, P1260, DOI 10.1049/el:20046194; Zhang L, 2003, PROC CVPR IEEE, P367	36	4	5	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2006	66	2					163	192		10.1007/s11263-005-3956-7	http://dx.doi.org/10.1007/s11263-005-3956-7			30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	022CY					2022-12-18	WOS:000236033500005
J	Chantler, M; Van Gool, L				Chantler, M; Van Gool, L			Special issue on "texture analysis and synthesis"	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material														Chantler, Mike/0000-0002-8381-1751					0	4	4	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR-MAY	2005	62	1-2					5	5		10.1007/s11263-005-4631-8	http://dx.doi.org/10.1007/s11263-005-4631-8			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	866XV					2022-12-18	WOS:000224807600001
J	Majer, P				Majer, P			On the influence of scale selection on feature detection for the case of linelike structures	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						scale selection; ridge detection	EDGE-DETECTION; RIDGE DETECTION	A method to automatically select locally appropriate scales for feature detection, proposed by Lindeberg ( 1993b, 1998), involves choosing a so-called gamma-parameter. The implications of the choice of gamma-parameter are studied and it is demonstrated that different values of. can lead to qualitatively different features being detected. As an example the range of gamma-values is determined such that a second derivative of Gaussian filter kernel detects ridges but not edges. Some results of this relatively simple ridge detector are shown for two-dimensional images.	Bitplane AG, CH-8048 Zurich, Switzerland		Majer, P (corresponding author), Bitplane AG, Badenerstr 682, CH-8048 Zurich, Switzerland.	majer@bitplane.com						EBERLY D, 1996, RIDGES IMAGE DATA AN; Elder JH, 1998, IEEE T PATTERN ANAL, V20, P699, DOI 10.1109/34.689301; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; FRITSCH DS, 1994, PATTERN RECOGN LETT, V15, P445, DOI 10.1016/0167-8655(94)90135-X; IOLLER TM, 1995, THESIS SWISS FEDERAL; KOENDERINK JJ, 1994, PATTERN RECOGN LETT, V15, P439, DOI 10.1016/0167-8655(94)90134-1; KOENDERINK JJ, 1992, IEEE T PATTERN ANAL, V14, P597, DOI 10.1109/34.141551; KOLLER TM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P864, DOI 10.1109/ICCV.1995.466846; KORN AF, 1988, IEEE T PATTERN ANAL, V10, P610, DOI 10.1109/34.6770; LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346; Lindeberg T, 1996, PROC CVPR IEEE, P465, DOI 10.1109/CVPR.1996.517113; Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773; LINDEBERG T, 1993, 8 SCIA, P857; LORENZ C, 1997, LECT NOTES COMPUTER, V1252; MAJER P, 2000, THESIS U GOTTINGEN; Pizer S. M., 1994, Journal of Mathematical Imaging and Vision, V4, P303, DOI 10.1007/BF01254105; Pizer SM, 1998, COMPUT VIS IMAGE UND, V69, P55, DOI 10.1006/cviu.1997.0563; ROMENY BH, 1993, P INF PROC MED IM, P77; STAAL J, 1999, SCALE SPACE THEORIES	19	4	4	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2004	60	3					191	202		10.1023/B:VISI.0000036834.42685.b6	http://dx.doi.org/10.1023/B:VISI.0000036834.42685.b6			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	843OL					2022-12-18	WOS:000223093300001
J	Davis, L				Davis, L			In memory of Azriel Rosenfeld	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Biographical-Item																		ROSENFELD AH, PUBLICATION LIST	1	4	4	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2004	60	1					3	4		10.1023/B:VISI.0000036299.15461.72	http://dx.doi.org/10.1023/B:VISI.0000036299.15461.72			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	820LW					2022-12-18	WOS:000221391600003
J	Ma, Y				Ma, Y			A differential geometric approach to multiple view geometry in spaces of constant curvature	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						multiple view geometry; spaces of constant curvature; gravitational lensing; epipolar constraint; multilinear constraint; algebraic and geometric dependency; trianogulation		Based upon an axiomatic formulation of vision system in a general Riemannian manifold, this paper provides a unified framework for the study of multiple view geometry in three dimensional spaces of Constant curvature. including Euclidean space, spherical space, and hyperbolic space. It is shown that multiple view geometry for Euclidean space can be interpreted as a limit case when (sectional) curvature of a non-Euclidean space approaches to zero. In particular, we show that epipolar constraint in the general case is exactly the same as that known for the Euclidean space but should be interpreted more generally when being applied to triangulation in non-Euclidean spaces. A special triangulation method is hence introduced using trigonometry laws from Absolute Geometry. Based on a common rank condition, we give a complete study of constraints among multiple images as well as relationships among all these constraints. This idealized geometric framework may potentially extend extant multiple view goeometry to the study of astronomical imaging where the effect of space curvature is no longer negligible, e.g., the so-called "gravitational lensing" phenomenon, which is currently active study in astronomical physics and cosmology.	Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA; Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Ma, Y (corresponding author), Univ Illinois, Coordinated Sci Lab, 1406 W Green St, Urbana, IL 61801 USA.	yima@uiuc.edu						FAUGERAS O, 1995, J OPT SOC AM A, V12, P465, DOI 10.1364/JOSAA.12.000465; GEYER C, 2002, P EUR C COMP VIS; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Heyden A, 1997, MATH METHOD APPL SCI, V20, P1135, DOI 10.1002/(SICI)1099-1476(19970910)20:13<1135::AID-MMA908>3.0.CO;2-9; HSIANG WY, 1995, PAM628 U CAL BERK; HUANG K, 2002, P EUR C COMP VIS; Kobayashi, 1996, FDN DIFFERENTIAL GEO, V2; Kobayashi S., 1996, FDN DIFFERENTIAL GEO, V1; Ma Y., 2001, INT J COMPUTER VISIO; MA Y, 2000, P ECCV DUBL IR; MAYBANK S, 1993, THEORY RECONSTRUCTIO; Murray R. M., 1994, MATH INTRO ROBOTIC M; Schneider P., 1992, GRAVITATIONAL LENSES; WEINSTEIN A, 2000, COMMUNICATIONS; WOLF JA, 1984, SPACES CONSTANT CURV	16	4	7	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2004	58	1					37	53		10.1023/B:VISI.0000016146.60243.dc	http://dx.doi.org/10.1023/B:VISI.0000016146.60243.dc			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	823OD					2022-12-18	WOS:000221621600004
J	Lee, CK; Leedham, CG				Lee, CK; Leedham, CG			A new hybrid approach to handwritten address verification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						handwritten postal address recognition; hybrid fusion approach; Singapore postal address sorting; postal automation; verification of postcode recognition	RECOGNITION	The use of optical character recognition (OCR) has achieved considerable success in the sorting of machine-printed mail. The automatic reading of unconstrained handwritten addresses however, is less successful. This is due to the high error rate caused by the wide variability of handwriting styles and writing implements. This paper describes a strategy for automatic handwritten address reading which integrates a postcode recognition system with a hybrid verification stage. The hybrid verification system seeks to reduce the error rate by correlating the postcode against features extracted and words recognised from the remainder of the handwritten address. Novel use of syntactic features extracted from words has resulted in a significant reduction in the error rate while keeping the recognition rate high. Experimental results on a testset of 1,071 typical Singapore addresses showed a significant improvements from 24.0% error rate, 71.2% correct recognition rate, and 4.8% rejection rate using "raw" OCR postcode recognition to 0.4% error rate, 65.1% correct recognition rate, and 34.5% rejection rate using the hybrid verification approach. The performance of the approach compares favourably with the currently installed commercial system at Singapore Post, which achieved 0.7% error rate, 47.8% correct recognition rate, and 51.5% rejection rate for 6-digit postcode using the same test data.	Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Lee, CK (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.	traviseklee@hotmail.com; asgleedham@ntu.edu.sg	Leedham, Graham/AAO-9329-2020					DOWNTON AC, 1989, IEE C CHAR REC APPL, V7, P1; GADER P, 1995, IEEE T FUZZY SYST, V3, P83, DOI 10.1109/91.366562; HENDRAWAN, 1994, THESIS U ESSEX; Ianakiev K, 1999, 18TH INTERNATIONAL CONFERENCE OF THE NORTH AMERICAN FUZZY INFORMATION PROCESSING SOCIETY - NAFIPS, P918, DOI 10.1109/NAFIPS.1999.781828; LEE CK, 2000, P 7 INT WORKSH FRONT, P571; LEE CK, 2000, P 13 INT C IND ENG A, P492; Madhvanath S, 1997, INT J PATTERN RECOGN, V11, P933, DOI 10.1142/S0218001497000421; Mitchell B. T., 1989, Machine Vision and Applications, V2, P231, DOI 10.1007/BF01215877; Nuijt M. R., 1999, Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318), P761, DOI 10.1109/ICDAR.1999.791899; SENI G, 1994, PATTERN RECOGN, V27, P41, DOI 10.1016/0031-3203(94)90016-7; SHRIDHAR M, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, VOLS 1-5, P2341, DOI 10.1109/ICSMC.1995.538131; Srihari SN, 1997, PROC INT CONF DOC, P892, DOI 10.1109/ICDAR.1997.620640; Srihari SN, 1989, INT J RES ENG POSTAL, V1, P37	13	4	4	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2004	57	2					107	120		10.1023/B:VISI.0000013085.47268.e8	http://dx.doi.org/10.1023/B:VISI.0000013085.47268.e8			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	766DQ					2022-12-18	WOS:000188330500002
J	Lee, J; Jeong, SY; Han, KS; Chun, BT; Bae, YJ				Lee, J; Jeong, SY; Han, KS; Chun, BT; Bae, YJ			Image navigation: A massively interactive model for similarity retrieval of images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						content-based image retrieval; similarity retrieval; user-system interaction; subjectivity problem; query description; multi-dimensional feature indexing		The ultimate objective of image retrieval research is to improve the user experiences in dealing with images, which happens to be closely related not only to the retrieval accuracy but also to the way he/she interacts with the retrieval tools. In particular, recognizing the subjectivity inherent in the problem leads to more emphasis on active participation of humans. This paper proposes a new user-system interaction model in the context of similarity retrieval of images. In the proposed model, the interaction is pursued to the extreme so that it tries to help users to browse huge image space with ease and efficiency rather than to find a certain images automatically on behalf of users. The system dynamically reconstructs the view reflecting user commands, while the user continuously modifies his/her commands while seeing the constantly changing view. Here the user's command is called a hint to distinguish it from a query, which is also a representation of user intention, but in a more formalized and complete form. Hints include all the intermediary steps of the user intention description process. By reflecting the intermediary steps to the view immediately, users receive feedback information to modify their description. This gradual and evolutionary process has a huge advantage over traditional approaches, especially when the user intention itself is ambiguous, which is often the case in realistic situations. This paper also describes a simple and efficient multi-dimensional feature indexing algorithm as an enabling technology to ensure immediate response. The algorithm transforms multi-dimensional features to one or more scalar values, which are used for restricting the search space. The algorithm proved to be efficient in realistic situations by being tested on the implementation of the proposed model.	Elect & Telecommun Res Inst, Visual Recognit Res Team, Comp & Software Technol Lab, Taejon 305350, South Korea; Chungbuk Prov Univ Sci & Technol, Dept Elect Commerce, Chungbuk 373807, South Korea	Electronics & Telecommunications Research Institute - Korea (ETRI)	Lee, J (corresponding author), Elect & Telecommun Res Inst, Visual Recognit Res Team, Comp & Software Technol Lab, 161 Gajeong Dong, Taejon 305350, South Korea.	leejy@etri.re.kr; jsy@etri.re.kr; kyuseo@etri.re.kr; chunbt@etri.re.kr; yljb@ctech.ac.kr		Jeong, Seyoon/0000-0002-1675-4814				Ashwin TV, 2001, INT CONF ACOUST SPEE, P1637, DOI 10.1109/ICASSP.2001.941250; BERCHTOLD S, 1998, INT C MAN DAT SCM SI, P142; Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596; Eakins JP, 2002, PATTERN RECOGN, V35, P3, DOI 10.1016/S0031-3203(01)00038-3; Eickeler S, 2001, INT CONF ACOUST SPEE, P1505, DOI 10.1109/ICASSP.2001.941217; FLICKNER M, 1995, IEEE COMPUT, V28, P23, DOI DOI 10.1109/2.410146; FRAKES WB, 1992, INFORMATION RETRIEVA, P28; Gevers T, 2000, IEEE T IMAGE PROCESS, V9, P102, DOI 10.1109/83.817602; Greggains V, 2001, SEED SCI RES, V11, P235; GUDIVADA VN, 1995, COMPUTER, V28, P18, DOI 10.1109/2.410145; Jin JS, 2001, J VIS COMMUN IMAGE R, V12, P123, DOI 10.1006/jvci.2000.0453; JIN Z, 2000, ADV VIS INF SYST 4 I, P521; KLETTE R, 1995, HDB IMAGE PROCESSING; Laaksonen J, 2001, PATTERN ANAL APPL, V4, P140, DOI 10.1007/PL00014575; LEE JY, 1999, IEEE TENCON 99, V1, P577; Ma WY, 1999, MULTIMEDIA SYST, V7, P184, DOI 10.1007/s005300050121; *MPEG 7, 2001, ISOIECJTC1SC29WG11N4; Muller H, 2001, PATTERN RECOGN LETT, V22, P593, DOI 10.1016/S0167-8655(00)00118-5; OGLE VE, 1995, COMPUTER, V28, P40, DOI 10.1109/2.410150; Pecenovic Z, 2000, LECT NOTES COMPUT SC, V1929, P279; PICARD RW, 1995, MULTIMEDIA SYST, V3, P3, DOI 10.1007/BF01236575; Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413; Santini S, 1999, IEEE T PATTERN ANAL, V21, P871, DOI 10.1109/34.790428; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; SRIHARI RK, 1995, COMPUTER, V28, P49, DOI 10.1109/2.410153; STRICKER M, 1995, P SOC PHOTO-OPT INS, V2420, P381; TAMURA H, IEEE T SYSTEMS MAN C, V8, P460; Vendrig J, 2001, MULTIMED TOOLS APPL, V15, P83, DOI 10.1023/A:1011367820253; Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194	29	4	4	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN-FEB	2004	56	1-2			SI		131	145		10.1023/B:VISI.0000004835.55771.0e	http://dx.doi.org/10.1023/B:VISI.0000004835.55771.0e			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	745MJ					2022-12-18	WOS:000186692200009
J	Pizer, SM				Pizer, SM			Guest editorial - Medial & medical: A good match for image analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Univ N Carolina, MIDAG, Chapel Hill, NC 27515 USA	University of North Carolina; University of North Carolina Chapel Hill	Pizer, SM (corresponding author), Univ N Carolina, MIDAG, Chapel Hill, NC 27515 USA.							Aylward SR, 2002, IEEE T MED IMAGING, V21, P61, DOI 10.1109/42.993126; Blum H., 1967, MODELS PERCEPTION SP, P363; CROUCH JR, 2003, LECT NOTES COMPUTER, V2878; DAMON J, 2003, GLOBAL GEOMETRY REGI; DAMON J, 2002, DETERMINING GEOMETRY; FLETCHER PT, 2003, CVPR, V1, P95; FRIDMAN Y, 2003, LECT NOTES COMPUTER, V2879; GERIG G, 2003, LECT NOTES COMPUTER, V2879; KATZ RA, INT J COMP VIS, V55, P139; Lu CL, 2003, LECT NOTES COMPUT SC, V2695, P416; NACKMAN LR, 1985, IEEE T PATTERN ANAL, V7, P187, DOI 10.1109/TPAMI.1985.4767643; Pizer SM, 2003, IEEE T MED IMAGING, V22, P2, DOI 10.1109/TMI.2003.809707; PIZER SM, 2003, IN PRESS INVITED P I, V91; STYNER M, INT J COMP VIS, V55, P107; THALL A, 2003, THESIS UNC COMPUTER; WANG H, 2003, THESIS UNC STAT DEPT; YUSHKEVICH PA, 2003, THESIS UNC COMPUTER	17	4	4	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV-DEC	2003	55	2-3					79	84		10.1023/A:1026198015379	http://dx.doi.org/10.1023/A:1026198015379			6	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	732YN					2022-12-18	WOS:000185973300001
J	Cozzi, A; Worgotter, F				Cozzi, A; Worgotter, F			COMVIS: A communication framework for computer vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						computer vision; data fusion; communication; integration of multiple visual modules	INTEGRATION	We describe a general approach to integrate the information produced by different visual modules with the goal of generating a quantitative 3D reconstruction of the observed scene and to estimate the reconstruction errors. The integration is achieved in two steps. Firstly, several different visual modules analyze the scene in terms of a common data representation: planar patches are used by different visual modules to communicate and represent the 3D structure of the scene. We show how it is possible to use this simple data structure to share and integrate information from different visual modalities, and how it can support the necessities of the great majority of different visual modules known in literature. Secondly, we devise a communication scheme able to merge and improve the description of the scene in terms of planar patches. The applications of state-of-the-art algorithms allows to fuse information affected by an unknown grade of correlation and still guarantee conservative error estimates. Tests on real and synthetic scene show that our system produces a consistent and marked improvement over the results of single visual modules, with error reduction up to a factor of ten and with typical reduction of a factor 2-4.	IBM Corp, Almaden Res Ctr, San Jose, CA 95120 USA; Univ Stirling, Dept Psychol, Stirling FK9 4LA, Scotland	International Business Machines (IBM); University of Stirling	Cozzi, A (corresponding author), IBM Corp, Almaden Res Ctr, B2,650 Harry Rd, San Jose, CA 95120 USA.							ALOIMONOS J, 1989, INTEGRATION VISUAL M; Cozzi A, 1997, MACH VISION APPL, V9, P334, DOI 10.1007/s001380050052; ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FLEET DJ, 1991, CVGIP-IMAG UNDERSTAN, V53, P198, DOI 10.1016/1049-9660(91)90027-M; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; FUA P, 1999, P VIS ALG THEOR PRAC; GAMBLE E, 1987, 970 AI MIT ART INT L; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Haralick R. M., 1996, WORKSH PERF CHAR VIS, P1; IRANI M, 1996, P 4 EUR C COMP VIS C, V1, P17; KANATANI K, 1990, SPRINGER SERIES INFO, V20; LUCAS B, 1984, DARPA 84, P272; Maybeck Peter S., 1979, MATH SCI ENG, V1; Opara R, 1998, NEURAL COMPUT, V10, P1547, DOI 10.1162/089976698300017304; PANKANTI S, 1995, UNIFORM BAYESIAN FRA; POGGIO T, 1988, SCIENCE, V242, P436, DOI 10.1126/science.3175666; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.ne.18.030195.003011; ULHMANN JK, 1997, CULMINATING ADV THEO; Wang DL, 1997, NEURAL COMPUT, V9, P1623	21	4	4	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.		2001	41	3					183	194		10.1023/A:1011156004656	http://dx.doi.org/10.1023/A:1011156004656			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	428LE					2022-12-18	WOS:000168462500003
J	Francisco, A; Bergholm, F				Francisco, A; Bergholm, F			On the importance of being asymmetric in stereopsis - Or why we should use skewed parallel cameras	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						stereo; stereopsis; vergence; horopters; Vieth-Muller circles; corresponding retinal points; image sensor shifts		This paper presents a new clever camera sensor, where relative pose determination is not needed, and the sensor is simultaneously capable of using vergence micromovements. Sweeping depth using vergence micromovements promises subpixel depth precision, measuring zero disparity at each time instant. We show that curves preserving zero disparity are exactly conics, nondegenerate or degenerate. Oddly enough, only circles (Vieth-Muller circles) are routinely considered, either theoretically or in practical work, in vergence stereo. Horopters in human vision, cf. Ogle (1932), closely resemble conics. We introduce translational vergence by suggesting the use of a pair of shift-optics CCD cameras. The nonrigidity causes zero disparity curves to become planes, for each fixation. (They are degenerate conics.) We have parallel optical axes, but slanting left and right primary lines of sight. During vergence movements, the primary lines of sight move over time. This has farreaching consequences: Binocular head-eye systems all involve relative camera rotation, to fixate. But, camera rotation is unnecessary. Hence, for relative depth maps, there is no need for measuring camera rotation (relative camera pose) from mechanical sources. Nor are algorithms needed for calculating epipolar lines. The suggested technique removes the need for camera rotations about the optical centers in a binocular head-eye system.	Royal Inst Technol, Dept Numer Anal & Comp Sci, Computat Vis & Act Percept Lab, S-10044 Stockholm, Sweden	Royal Institute of Technology	Francisco, A (corresponding author), Royal Inst Technol, Dept Numer Anal & Comp Sci, Computat Vis & Act Percept Lab, S-10044 Stockholm, Sweden.							AHUJA N, 1993, IEEE T PATTERN ANAL, V15, P1007, DOI 10.1109/34.254059; ANDERSSON M, 1997, KTHNAP9704 CVAP NADA; [Anonymous], 1950, RES BINOCULAR VISION; Ayres F, 1967, SCHAUMS OUTLINE SERI; Carpenter RHS, 1988, MOVEMENTS EYES; COOMBS D, 1993, INT J COMPUT VISION, V11, P147, DOI 10.1007/BF01469226; *EUR OFF, NEWP CAT PREC LAS OP; Francisco A., 1993, Proceedings of the 8th Scandinavian Conference on Image Analysis, P97; FRANCISCO A, 1993, P 4 ICCV BERL GERM, P481; FRANCISCO A, 1991, MSCIS9137 U PENNS GR; FRY GA, 1984, FDN SENSORY SCI, pCH8; GROSSO E, 1995, IEEE T PATTERN ANAL, V17, P1117; Horn B., 1986, ROBOT VISION, P1; JAIN R, 1987, PAMI, V9, P3; KRISHNAN A, 1993, P SPIE, V2056; LINDSAY PH, 1977, HUMAN INFORMATION PR; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; Ogle KN, 1932, J OPT SOC AM, V22, P665, DOI 10.1364/JOSA.22.000665; RECORDS RE, 1979, PHYSL HUMAN EYE VISU, pCH22; TISTARELLI M, 1990, IMAGE VISION COMPUT, V8, P271, DOI 10.1016/0262-8856(90)80003-C; Zhizhuo W., 1990, PRINCIPLES PHOTOGRAM; ZIELKE T, 1990, LECT NOTES COMPUT SC, V427, P613, DOI 10.1007/BFb0014923	22	4	4	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1998	29	3					181	202		10.1023/A:1008084713069	http://dx.doi.org/10.1023/A:1008084713069			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	138BU					2022-12-18	WOS:000076952200002
J	Parodi, P				Parodi, P			The complexity of understanding line drawings of Origami scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CONSTRAINT SATISFACTION; PERSPECTIVE IMAGES; VANISHING POINTS; PROJECTIONS; OBJECTS	This paper deals with the interpretation of line drawings of Origami scenes (Kanade, 1980), that is scenes obtained by assembling planar panels of negligible thickness, and it addresses the computational complexity of the problem of consistently assigning suitable labels to the segments describing 3D properties as convexity, concavity and occlusion (labeling problem). The main results of the paper are the following: (a) the labeling problem for line drawings of Origami scenes is Np-complete, as for the case of trihedral scenes; (b) the problem remains NP-complete even if the location of the vanishing points in the image plane is given, whereas for trihedral scenes the problem was polynomially solvable; (c) in case the vanishing points are known the labeling problem can be subdivided into two subproblems, the paneling problem and the labeling-a-paneled-line-drawing problem which are both polynomially solvable (there may be, however, exponentially many legal panelings to check against labelability). The approach provides geometrical constraints which help select 'natural' interpretations even in the presence of occlusions and in the absence of apparent 3D-symmetries, and which may help highlight the role of geometrical regularities in the design of biologically plausible algorithms for the interpretation of line drawings.			Parodi, P (corresponding author), UNIV GENOA, DIPARTIMENTO FIS, VIA DODECANESO 33, I-16146 GENOA, ITALY.							BARNARD ST, 1983, ARTIF INTELL, V21, P435, DOI 10.1016/S0004-3702(83)80021-6; BRILLAULTOMAHONY B, 1991, CVGIP-IMAG UNDERSTAN, V54, P289, DOI 10.1016/1049-9660(91)90069-2; Caprile B., 1990, INT J COMPUTER VISIO; CHARTRAND G, 1986, GRAPHS DIAGRAPHS; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; COLLINS R, 1989, P OPT SOC AM WORKSH, V14, P92; COWAN TM, 1977, PERCEPTION, V6, P41, DOI 10.1068/p060041; COWAN TM, 1974, J MATH PSYCHOL, V11, P190, DOI 10.1016/0022-2496(74)90018-2; DENDRIS ND, 1992, LECT NOTES COMPUT SC, V650, P198; DRAPER SW, 1981, ARTIF INTELL, V17, P461, DOI 10.1016/0004-3702(81)90032-1; HORAUD R, 1987, IEEE T PATTERN ANAL, V9, P401, DOI 10.1109/TPAMI.1987.4767922; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; HUFFMAN DA, 1977, MACH INTELL, V8, P493; Huffman David, 1977, MACHINE INTELLIGENCE, V8, P475; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; KANADE T, 1980, ARTIF INTELL, V13, P279, DOI 10.1016/0004-3702(80)90004-1; KANADE T, 1993, ARTIF INTELL, V59, P95, DOI 10.1016/0004-3702(93)90175-B; KANADE T, 1983, P IEEE, V71, P789, DOI 10.1109/PROC.1983.12679; KIROUSIS LM, 1990, IEEE T PATTERN ANAL, V12, P123, DOI 10.1109/34.44400; KIROUSIS LM, 1988, J COMPUT SYST SCI, V37, P14, DOI 10.1016/0022-0000(88)90043-8; KIROUSIS LM, 1993, ARTIF INTELL, V64, P147, DOI 10.1016/0004-3702(93)90063-H; MACKWORTH AK, 1985, ARTIF INTELL, V25, P65, DOI 10.1016/0004-3702(85)90041-4; MACKWORTH AK, 1993, ARTIF INTELL, V59, P57, DOI 10.1016/0004-3702(93)90170-G; MACKWORTH AK, 1984, ARTIF INTELL, P121; MAGEE MJ, 1984, COMPUT VISION GRAPH, V26, P256, DOI 10.1016/0734-189X(84)90188-9; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; Nakatani H., 1981, Transactions of the Institute of Electronics and Communication Engineers of Japan, Section E (English), VE64, P357; PARODI P, 1994, ARTIF INTELL, V70, P239, DOI 10.1016/0004-3702(94)90107-4; PARODI P, 1993, FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION : PROCEEDINGS, P291; PARODI P, 1994, IN PRESS 3D SHAPE RE; QUAN L, 1989, PATTERN RECOGN LETT, V9, P279, DOI 10.1016/0167-8655(89)90006-8; RUTH S, 1984, IEEE T PATTERN ANAL, V6, P789; SHAFER SA, 1982, IEEE WORKSH COMP VIS, P26; SHAPIRA R, 1979, COMMUN ACM, V22, P369; STRAFORINI M, 1993, IMAGE VISION COMPUT, V11, P91, DOI 10.1016/0262-8856(93)90075-R; SUGIHARA K, 1984, ARTIF INTELL, V23, P59, DOI 10.1016/0004-3702(84)90005-5; SUGIHARA K, 1978, COMPUT VISION GRAPH, V8, P382, DOI 10.1016/0146-664X(78)90064-3; SUGIHARA K, 1984, IEEE T PATTERN ANAL, V6, P578, DOI 10.1109/TPAMI.1984.4767571; SUGIHARA K, 1982, IEEE T PATTERN ANAL, V4, P458, DOI 10.1109/TPAMI.1982.4767289; TEROUANNE E, 1980, J MATH PSYCHOL, V22, P24, DOI 10.1016/0022-2496(80)90045-0; TSOTSOS J, 1988, INT J COMPUT VISION, V1, P303; TSOTSOS JK, 1990, BEHAV BRAIN SCI, V13, P423, DOI 10.1017/S0140525X00079577; ULUPINAR F, 1991, CVGIP-IMAG UNDERSTAN, V53, P88, DOI 10.1016/1049-9660(91)90007-C; WALTZ D, 1971, ARTIF INTELL, V2, P79; WHITELEY W, 1979, STRUCTURAL TOPOLOGY, V1, P46	46	4	5	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	1996	18	2					139	170		10.1007/BF00055000	http://dx.doi.org/10.1007/BF00055000			32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	UX449					2022-12-18	WOS:A1996UX44900003
J	GOLDBERG, RR				GOLDBERG, RR			CONSTRAINED POSE REFINEMENT OF PARAMETRIC OBJECTS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							3-D OBJECTS; IMAGE; MODELS; SYSTEM	Pose refinement is an essential task for computer vision systems that require the calibration and verification of model and camera parameters. Typical domains include the real-time tracking of objects and verification in of model and camera parameters. Typical domains include the real-time tracking of objects and verification in model-based recognition systems. A technique is presented for recovering model and camera parameters of 3D objects from a single two-dimensional image. This basic problem is further complicated by the incorporation of simple bounds on the model and camera parameters and linear constraints restricting some subset of object parameters to a specific relationship. It is demonstrated in this paper that this constrained pose refinement formulation is no more difficult than the original problem based on numerical analysis techniques, including active set methods and lagrange multiplier analysis. A number of bounded and linearly constrained parametric models are tested and convergence to proper values occurs from a wide range of initial error, utilizing minimal matching information (relative to the number of parameters and components). The ability to recover model parameters in a constrained search space will thus simplify associated object recognition problems.			GOLDBERG, RR (corresponding author), CUNY QUEENS COLL,DEPT COMP SCI,65-30 KISSENA BLVD,FLUSHING,NY 11367, USA.							BAUMGART B, 1972, STANCS320 STANF U TE; Binford T. O., 1982, INT J ROBOT RES, V1, P18; BOGGS PT, 1982, SIAM J CONTROL OPTIM, V20, P161, DOI 10.1137/0320014; BOLLES RC, 1986, INT J ROBOT RES, V5, P3, DOI 10.1177/027836498600500301; BRAY AJ, 1990, IMAGE VISION COMPUT, V8, P4, DOI 10.1016/0262-8856(90)90049-B; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; CLINE AK, 1976, TRANSFORMATION QUADR; DENNIS JR, 1983, NUMERICAL METHODS UN; DHOME M, 1989, IEEE T PATTERN ANAL, V11, P1265, DOI 10.1109/34.41365; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fletcher R., 1981, PRACTICAL METHODS OP, V1; Gill P., 1977, STATE ART NUMERICAL, P313; Gill P. E., 1981, PRACTICAL OPTIMIZATI; GILL PE, 1974, MATH COMPUT, V28, P505, DOI 10.1090/S0025-5718-1974-0343558-6; GILL PE, 1978, MATH PROGRAM, V14, P349, DOI 10.1007/BF01588976; GILL PE, 1984, ACM T MATH SOFTWARE, V10, P282, DOI 10.1145/1271.1276; GILL PE, 1979, MATH PROGRAM, V17, P32, DOI 10.1007/BF01588224; GILL PE, 1976, NAC71 NAT PHYS LAB; GILL PE, 1976, LECT NOTES MATH, V506, P135; GILL PE, 1986, SOL866R STANF U TECH; GILL PE, 1984, SOL846 STANF U TECH; GILL PE, 1985, NATO ASI SERIES F, V15, P209; GILL PE, 1984, SOL847 STAND U TECH; GILL PE, 1985, ACM SIGNUM NEWSLETTE, V20, P13; GILL PE, 1974, NAC37 NAT PHYS LAB; GOLDBERG R, 1988, P SPIE C SPATIAL REA; GOLDBERG R, 1988, SIAM ANN C P MINNEAP; GOLDBERG R, 1987, P IEEE COMPUTER VISI; GOLDBERG R, 1991, P SPIE MACHINE VISIO; GOLDBERG RR, 1993, IMAGE VISION COMPUT, V11, P49, DOI 10.1016/0262-8856(93)90031-B; GOLDSTEIN A, 1967, NUMER MATH, V10, P360; Grimson W. E. L., 1990, OBJECT RECOGNITION C; GURWITZ CB, 1987, THESIS COURANT I MAT; ISHII M, 1987, INT J ROBOT RES, V6, P45, DOI 10.1177/027836498700600204; KEDEM G, 1987, AUTOMATIC DIFFERENTI, P231; KEENE S, 1987, OBJECT ORIENTED PROG; KEMPER A, 1987, COMPUT SURV, V19, P47, DOI 10.1145/28865.28866; KENDER JR, 1987, P ARPA IMAGE UNDERST, P589; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; KROGH F, 1987, ACM SIGNUM, V22, P7; KUHN HW, 1976, SIAM AMS P, V9, P1; KUMAR R, 1992, THESIS U MASSACHUSET; LINDSTROM P, 1988, S90187 U UM NUM AN R; LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043; MAYER W, 1960, SIAM NUMERICAL ANAL, V2, P1; Press WH, 1988, NUMERICAL RECIPES C; Ramsin H., 1977, BIT (Nordisk Tidskrift for Informationsbehandling), V17, P72, DOI 10.1007/BF01932400; WEDIN P, 1988, UMINF13387 U UM INF; WOLFSON HJ, 1990, LECT NOTES COMPUT SC, V427, P526, DOI 10.1007/BFb0014902; WORALL AD, 1989, IMAGE VISION COMPUT, V7, P17; YAP CK, 1990, J SYMB COMPUT, V10, P349, DOI 10.1016/S0747-7171(08)80069-7; [No title captured]; 1992, FORTRAN LIBRARY REFE	54	4	4	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1994	13	2					181	211		10.1007/BF01427151	http://dx.doi.org/10.1007/BF01427151			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PM568					2022-12-18	WOS:A1994PM56800003
J	VIEVILLE, T; FAUGERAS, O				VIEVILLE, T; FAUGERAS, O			ROBUST AND FAST COMPUTATION OF EDGE CHARACTERISTICS IN IMAGE SEQUENCES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CURVATURE	In this paper we consider a non-parametric analytical model of the intensity for a curved edge, and derive the relations between the image data and some local characteristics of the edge, in the discrete case. In order to identify this model we also study how to develop high order non-biased spatial derivative operators, with subpixel accuracy. In fact, this discrete approach corresponds to the notion of spatio-temporal surfaces in the continuous case, and provides a way to obtain some of the spatio-temporal parameters from an image sequence. An implementation is proposed, and experimental data are provided. Computed characteristics are subpixel localization, normal displacement between two frames, orientation and curvature, but the method is easy to extend to other geometrical or dynamical parameters of the edge. Results derived in this paper are always valid for step-like edges, but computation of orientation and curvature are also valid for edges with more general profiles.			VIEVILLE, T (corresponding author), INRIA,BP 109,F-06561 VALBONNE,FRANCE.							AGGARWAL JK, 1991, ANAL DYNAMIC SCENES, P355; ASADA H, 1986, IEEE T PATTERN ANAL, V8, P2, DOI 10.1109/TPAMI.1986.4767747; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; DANIELSON P, 1989, MACHINE VISION ACQUI; DERICHE R, 1987, INT J COMPUT VISION, V1, P167, DOI 10.1007/BF00123164; DERICHE R, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P66; DERICHE R, 1990, IEEE T PATTERN ANAL, P1; DERICHE R, 1987, INT J COMPUT VISION, P18; DERICHE R, 1990, 1ST P EUR C COMP VIS, P259; Dodson C.T.J., 1979, TENSOR GEOMETRY; Faugeras O. D., 1988, Proceedings of IAPR Workshop on Computer Vision: Special Hardware and Industrial Applications, P35; FAUGERAS OD, 1990, 1ST P ECCV, P107; FAUGERAS OD, 1992, IN PRESS INT J COMPU; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FRANCOIS E, 1990, IMAGE VISION COMPUTI, V8; GUIDUCCI A, 1988, PATTERN RECOGN LETT, V8, P311, DOI 10.1016/0167-8655(88)90080-3; Haralick R., 1984, IEEE T PATTERN ANAL, V6; HILDRETH EC, 1980, AI579 TECH REP; HILDRETH EC, 1984, AI761 TECH REP; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HUECKEL M, 1974, J ASSOC COMPUT MACH, V20, P634; HUERTAS A, 1986, IEEE T PATTERN ANAL, V8, P651, DOI 10.1109/TPAMI.1986.4767838; KOENDERINK JJ, 1988, J OPT SOC AM A, V5, P1136, DOI 10.1364/JOSAA.5.001136; MONGA O, 1991, CVGIP-IMAG UNDERSTAN, V53, P76, DOI 10.1016/1049-9660(91)90006-B; NAGEL HH, 1985, INFORMATIK SPEKTRUM, V8; NALWA VS, 1986, IEEE T PATTERN ANAL, V8, P699, DOI 10.1109/TPAMI.1986.4767852; PERONA P, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P52; Schwartz L., 1950, THEORIE DISTRIBUTION; TSAI RY, 1982, IEEE T ACOUST SPEECH, V30, P525, DOI 10.1109/TASSP.1982.1163931; VERRI A, 1986, AI917 TECH REP; VIEVILLE T, 1992, RR1689 INRIA TECHN R; VIEVILLE T, 1992, 2ND EUR C COMP VIS, P203; VIEVILLE T, 1990, 1ST P ECCV ANT, P281; VIEVILLE T, 1992, RR1669 INRIA TECHN R; WEISS I, 1991, APPLICATION INVARIAN; ZUNIGA OA, 1987, IEEE T SYSTEM MACHIN, V17; [No title captured]	37	4	4	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1994	13	2					153	179		10.1007/BF01427150	http://dx.doi.org/10.1007/BF01427150			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PM568		Green Submitted			2022-12-18	WOS:A1994PM56800002
J	HARRIS, JG				HARRIS, JG			AN ANALOG NETWORK FOR CONTINUOUS-TIME SEGMENTATION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							ILL-POSED PROBLEMS; EARLY VISION; COMPUTATIONAL VISION; DISCONTINUITIES; RESTORATION	A common goal in computer vision is to segment scenes into different objects sharing common properties such as depth, motion, or image intensity. A segmentation algorithm has been developed utilizing an absolute-value smoothness penalty instead of the more common quadratic regularizer. This functional imposes a piece-wide constant constraint on the segmented data. Since the minimized energy is guaranteed to be convex, there are no problems with local minima, and no complex continuation methods are necessary to find the unique global minimum. This is in sharp contrast to previous software and hardware solutions to this problem. The energy minimized can be interpreted as the generalized power (or co-content) of a nonlinear resistive network. The network is called the tiny-tanh network since the I-V characteristic of the nonlinear resistor must be an extremely narrow-width hyperbolic-tangent function. This network has been demonstrated for 1-D step-edges with analog CMOS hardware and for a 2-D stereo algorithm in simulations.	MIT,ARTIFICIAL INTELLIGENCE LAB,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)								BERTERO M, 1988, P IEEE, V76, P869, DOI 10.1109/5.5962; Besag J. E., 1989, J APPL STAT, V16, P395, DOI [10.1080/02664768900000049, DOI 10.1080/02664768900000049]; Black A., 1987, VISUAL RECONSTRUCTIO; Chua LO., 1987, LINEAR NONLINEAR CIR; GEMAN D, 1992, IEEE T PATTERN ANAL, V14, P367, DOI 10.1109/34.120331; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gill P. E., 1981, PRACTICAL OPTIMIZATI; GIROSI F, 1991, AI1287 MIT AI LAB ME; GREEN PJ, 1990, IEEE T MED IMAGING, V9, P84, DOI 10.1109/42.52985; HAMPEL FR, 1986, ROBUST STATISTICS AP; HARRIS J, 1989, ANALOG VLSI IMPLEMEN; HARRIS JG, 1990, SCIENCE, V248, P1209, DOI 10.1126/science.2349479; HARRIS JG, 1991, THESIS CALTECH PASAD; HARRIS JG, 1991, JUL P INT JOINT C NE, P501; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; HORN BKP, 1988, AI1071 MIT ART INT L; Huber P., 1981, ROBUST STATISTICS, DOI [10.1002/0471725250, 10.1002/0471725250.ch1]; KARPLUS WJ, 1958, SOLUTION FIELD PROBL; KOCH C, 1986, P NATL ACAD SCI USA, V83, P4263, DOI 10.1073/pnas.83.12.4263; KOCH C, 1990, NEURAL NETWORKS; Koch C, 1989, NEURAL COMPUT, V1, P184, DOI 10.1162/neco.1989.1.2.184; Lazzaro J, 1989, NEURAL COMPUT, V1, P47, DOI 10.1162/neco.1989.1.1.47; LEVINE M, 1964, METHODS SOLVING ENG; LITTLE JJ, 1988, 2ND IEEE P INT C COM; LUMSDAINE A, 1991, IN PRESS J VLSI SIG; LUO J, 1989, NOV NEUR INF PROC SY; LYON RF, 1988, IEEE T ACOUST SPEECH, V36, P1119, DOI 10.1109/29.1639; MARROQUIN J, 1987, J AM STAT ASSOC, V82, P76, DOI 10.2307/2289127; Mead, 1989, ANALOG VLSI NEURAL S; MEAD CA, 1988, NEURAL NETWORKS, V1, P91, DOI 10.1016/0893-6080(88)90024-X; MILLAR W, 1951, PHILOS MAG, V42, P1150; PLATT J, 1990, CALTECHCSTR8907 DEP; POGGIO T, 1985, PROC R SOC SER B-BIO, V226, P303, DOI 10.1098/rspb.1985.0097; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; POGGIO T, 1984, AI783 MIT ART INT LA; SCHUNCK BG, 1989, P ROBUST METHODS COM; Shulman D., 1989, Proceedings. Workshop on Visual Motion (IEEE Cat. No.89CH2716-9), P81, DOI 10.1109/WVM.1989.47097; SIVILOTTI MA, 1987, 1987 STANF C VER LAR	38	4	4	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1993	10	1					43	51		10.1007/BF01440846	http://dx.doi.org/10.1007/BF01440846			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LP384					2022-12-18	WOS:A1993LP38400003
J	NELSON, RC				NELSON, RC			VISION AS INTELLIGENT BEHAVIOR - AN INTRODUCTION TO MACHINE VISION AT THE UNIVERSITY-OF-ROCHESTER - INTRODUCTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									UNIV ROCHESTER,DEPT COMP SCI,ROCHESTER,NY 14627	University of Rochester								AGRE P, 1987, 6TH P NAT C ART INT, P268; Ballard D. H., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P524, DOI 10.1109/CCV.1988.590033; BALLARD DH, 1989, AUG P INT JOINT C AR, P1635; BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032; BROOKS RA, 1986, TR899 MIT; BROWN C, 1990, BIOL CYBERN, V63, P61, DOI 10.1007/BF00202454; BROWN CM, 1990, IEEE T SYST MAN CYBE, V20; COOMBS DJ, 1990, AUG P AAAI QUAL VIS; Gibson J., 1979, ECOLOGICAL APPROACH; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; Gibson JJ., 1966, SENSES CONSIDERED PE; Marr D., 1982, VISION; MOROVEC HP, 1977, 5TH P INT JOINT C AR; NELSON RC, 1989, IEEE T PATTERN ANAL, V11, P1102, DOI 10.1109/34.42840; NELSON RN, 1989, MAY P DARPA IM UND W, P245; Whitehead SD, 1990, NEURAL COMPUT, V2, P409, DOI 10.1162/neco.1990.2.4.409; WIXSON LE, 1990, S ADV INTELLIGENT RO, V1198, P435; YAMAUCHI B, 1989, P OPT SOC AM IMAGE U; Yarbus A. L., 1967, EYE MOVEMENTS VISION, P171	19	4	4	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1991	7	1					5	9		10.1007/BF00130486	http://dx.doi.org/10.1007/BF00130486			5	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GY893					2022-12-18	WOS:A1991GY89300001
J	KANATANI, K				KANATANI, K			RECONSTRUCTION OF CONSISTENT SHAPE FROM INCONSISTENT DATA - OPTIMIZATION OF 21/2D SKETCHES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									GUNMA UNIV,DEPT COMP SCI,KIRYU,GUNMA 376,JAPAN	Gunma University								ALOIMONOS J, 1988, INT J COMPUT VISION, V2, P171, DOI 10.1007/BF00133699; BARNARD ST, 1985, COMPUT VISION GRAPH, V29, P87, DOI 10.1016/S0734-189X(85)90152-5; BUXTON BF, 1985, COMPUT PHYS COMMUN, V37, P273, DOI 10.1016/0010-4655(85)90162-6; BUXTON BF, 1983, PROC R SOC SER B-BIO, V218, P27, DOI 10.1098/rspb.1983.0024; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; HARALICK RM, 1980, COMPUT VISION GRAPH, V13, P191, DOI 10.1016/0146-664X(80)90046-5; HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; KANATANI K, 1987, COMPUT VISION GRAPH, V38, P122, DOI 10.1016/S0734-189X(87)80133-0; KANATANI K, 1986, COMPUT VISION GRAPH, V35, P181, DOI 10.1016/0734-189X(86)90026-5; KANATANI K, 1988, COMPUT VISION GRAPH, V41, P28, DOI 10.1016/0734-189X(88)90115-6; KANATANI K, 1987, 1ST P IEEE INT C COM, P55; KANATANI KI, 1988, IEEE T PATTERN ANAL, V10, P131, DOI 10.1109/34.3879; KANATANI KI, 1986, IEEE T PATTERN ANAL, V8, P456, DOI 10.1109/TPAMI.1986.4767809; KANATANI KI, 1987, COMPUT VISION GRAPH, V39, P328, DOI 10.1016/S0734-189X(87)80185-8; KOENDERINK JJ, 1975, OPT ACTA, V22, P773, DOI 10.1080/713819112; KOENDERINK JJ, 1976, J OPT SOC AM, V66, P717, DOI 10.1364/JOSA.66.000717; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LONGUETHIGGINS HC, 1984, PROC R SOC SER B-BIO, V223, P165, DOI 10.1098/rspb.1984.0088; MACKWORTH AK, 1976, PERCEPTION, V5, P349, DOI 10.1068/p050349; MAGEE MJ, 1984, COMPUT VISION GRAPH, V26, P256, DOI 10.1016/0734-189X(84)90188-9; Marr D., 1982, VISION COMPUTATIONAL; MULGAONKAR PG, 1986, COMPUT VISION GRAPH, V36, P298, DOI 10.1016/0734-189X(86)90080-0; NEGHANDARIPOUR S, 1987, IEEE T PAMI, V9, P168; POGGIO T, 1985, PROC R SOC SER B-BIO, V226, P303, DOI 10.1098/rspb.1985.0097; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; POLLARD SB, 1987, IMAGE VISION COMPUT, V5, P73, DOI 10.1016/0262-8856(87)90030-8; PORRILL J, 1987, IMAGE VISION COMPUT, V5, P174, DOI 10.1016/0262-8856(87)90046-1; SHAKUNAGA T, 1986, JUN P INT C COMP VIS, P594; SUBBARAO M, 1988, INT J COMPUT VISION, V2, P77, DOI 10.1007/BF00836282; SUBBARAO M, 1986, COMPUT VISION GRAPH, V36, P208, DOI 10.1016/0734-189X(86)90076-9; SUGIHARA K, 1984, ARTIF INTELL, V23, P59, DOI 10.1016/0004-3702(84)90005-5; SUGIHARA K, 1984, IEEE T PATTERN ANAL, V6, P578, DOI 10.1109/TPAMI.1984.4767571; SUGIHARA K, 1982, IEEE T PATTERN ANAL, V4, P458, DOI 10.1109/TPAMI.1982.4767289; SUGIHARA K, 1986, MACHINE INTELLIGENCE; Tikhonov A., 1977, SOLUTIONS ILL POSED; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; WAXMAN AM, 1985, INT J ROBOT RES, V4, P72, DOI 10.1177/027836498500400306; WAXMAN AM, 1985, INT J ROBOT RES, V4, P95, DOI 10.1177/027836498500400307	41	4	4	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1989	3	4					261	292		10.1007/BF00132600	http://dx.doi.org/10.1007/BF00132600			32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CC907					2022-12-18	WOS:A1989CC90700001
J	Ruan, DL; Mo, RY; Yan, Y; Chen, S; Xue, JH; Wang, HZ				Ruan, Delian; Mo, Rongyun; Yan, Yan; Chen, Si; Xue, Jing-Hao; Wang, Hanzi			Adaptive Deep Disturbance-Disentangled Learning for Facial Expression Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Facial expression recognition; Multi-task learning; Adversarial transfer learning; Multi-level attention	JOINT	In this paper, we propose a novel adaptive deep disturbance-disentangled learning (ADDL) method for effective facial expression recognition (FER). ADDL involves a two-stage learning procedure. First, a disturbance feature extraction model is trained to identify multiple disturbing factors on a large-scale face database involving disturbance label information. Second, an adaptive disturbance-disentangled model, which contains a global shared subnetwork and two task-specific subnetworks, is designed and learned to explicitly disentangle disturbing factors from facial expression images. In particular, the expression subnetwork leverages a multi-level attention mechanism to extract expression-specific features, while the disturbance subnetwork embraces a new adaptive disturbance feature learning module to extract disturbance-specific features based on adversarial transfer learning. Moreover, a mutual information neural estimator is adopted to minimize the correlation between expression-specific and disturbance-specific features. Extensive experimental results on both in-the-lab FER databases (including CK+, MMI, and Oulu-CASIA) and in-the-wild FER databases (including RAF-DB, SFEW, Aff-Wild2, and AffectNet) show that our proposed method consistently outperforms several state-of-the-art FER methods. This clearly demonstrates the great potential of disturbance disentanglement for FER. Our code is available at https://github.com/delian11/ADDL.	[Ruan, Delian; Mo, Rongyun; Yan, Yan; Wang, Hanzi] Xiamen Univ, Sch Informat, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China; [Chen, Si] Xiamen Univ Technol, Sch Comp & Informat Engn, Xiamen 361024, Peoples R China; [Xue, Jing-Hao] UCL, Dept Stat Sci, London WC1E 6BT, England	Xiamen University; Xiamen University of Technology; University of London; University College London	Yan, Y (corresponding author), Xiamen Univ, Sch Informat, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China.	delianruan@stu.xmu.edu.cn; morongyun@stu.xmu.edu.cn; yanyan@xmu.edu.cn; chensi@xmut.edu.cn; jinghao.xue@ucl.ac.uk; hanzi.wang@xmu.edu.cn	Wang, Han/GPW-9809-2022		National Natural Science Foundation of China [62071404, U21A20514, 61872307, 2021KG0AB02]; Natural Science Foundation of Fujian Province [2020J01001]; Youth Innovation Foundation of Xiamen City [3502Z20206046]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Fujian Province(Natural Science Foundation of Fujian Province); Youth Innovation Foundation of Xiamen City	This work was partly supported by the National Natural Science Foundation of China under Grants 62071404, U21A20514, and 61872307, by theOpen Research Projects of Zhejiang Lab under Grant 2021KG0AB02, by the Natural Science Foundation of Fujian Province under Grant 2020J01001, and by the Youth Innovation Foundation of Xiamen City under Grant 3502Z20206046.	Acharya D, 2018, IEEE COMPUT SOC CONF, P480, DOI 10.1109/CVPRW.2018.00077; Anas H., 2020, ARXIV PREPRINT ARXIV; Barrett LF, 2019, PSYCHOL SCI PUBL INT, V20, P1, DOI 10.1177/1529100619832930; Belghazi MI, 2018, PR MACH LEARN RES, V80; Chang FJ, 2019, INT J COMPUT VISION, V127, P930, DOI 10.1007/s11263-019-01151-x; Chen JW, 2018, IEEE COMPUT SOC CONF, P1651, DOI 10.1109/CVPRW.2018.00207; Chen YW, 2020, IEEE T SYST MAN CY-S, V50, P5076, DOI 10.1109/TSMC.2018.2856405; Chu Xiao, 2017, PROC CVPR IEEE, P1831, DOI DOI 10.1109/CVPR.2017.601; Dapogny A, 2018, INT J COMPUT VISION, V126, P255, DOI 10.1007/s11263-017-1010-1; Deng DD, 2020, IEEE INT CONF AUTOMA, P592, DOI 10.1109/FG47880.2020.00131; Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23; DONSKER MD, 1983, COMMUN PUR APPL MATH, V36, P183, DOI 10.1002/cpa.3160360204; Dresvyanskiy D., 2020, ARXIV PREPRINT ARXIV; EKMAN P, 1976, ENVIRON PSYCH NONVER, V1, P56, DOI 10.1007/BF01115465; Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245; Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476; Gera Darshan, 2020, ARXIV200914440; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu P., 2017, P 19 ACM INT C MULT, P553; Hung SCY, 2019, ADV NEUR IN, V32; Hung SCY, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P339, DOI 10.1145/3323873.3325053; Jang Y, 2019, COMPUT VIS IMAGE UND, V182, P17, DOI 10.1016/j.cviu.2019.01.006; Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341; King DB, 2015, ACS SYM SER, V1214, P1; Kollias D., 2018, ARXIV180501452; Kollias D, 2020, IEEE INT CONF AUTOMA, P637, DOI 10.1109/FG47880.2020.00126; Kollias D, 2020, INT J COMPUT VISION, V128, P1455, DOI 10.1007/s11263-020-01304-3; Kollias D, 2019, INT J COMPUT VISION, V127, P907, DOI 10.1007/s11263-019-01158-4; Kossaifi J, 2020, J MACH LEARN RES, V21; Kuhnke F, 2020, IEEE INT CONF AUTOMA, P600, DOI 10.1109/FG47880.2020.00056; Li S, 2019, INT J COMPUT VISION, V127, P884, DOI 10.1007/s11263-018-1131-1; Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382; Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277; Li S, 2021, IEEE SYST J, V15, P5126, DOI 10.1109/JSYST.2020.3019939; Li Y, 2019, PROC CVPR IEEE, P10916, DOI 10.1109/CVPR.2019.01118; Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767; Liu Hanyu, 2020, ARXIV200205447; Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197; Liu YY, 2018, IEEE INT CONF AUTOMA, P458, DOI 10.1109/FG.2018.00074; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lucey P., 2010, P IEEE COMP SOC C CO, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Lv F, 2021, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR46437.2021.00258; Meng ZB, 2017, IEEE INT CONF AUTOMA, P558, DOI 10.1109/FG.2017.140; Mollahosseini A, 2016, IEEE WINT CONF APPL; Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923; Motley M. T., 1988, W J SPEECH COMMUNICA, V52, P1, DOI [10.1080/10570318809389622, DOI 10.1080/10570318809389622]; Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976; Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58; Ruan DL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2833, DOI 10.1145/3394171.3413907; Sankaran N, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107127; Savchenko Andrey V., 2021, 2021 IEEE 19th International Symposium on Intelligent Systems and Informatics (SISY), P119, DOI 10.1109/SISY52375.2021.9582508; Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Valstar M, 2010, PROCEEDING 2010 IEEE, P65; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vo TH, 2020, IEEE ACCESS, V8, P131988, DOI 10.1109/ACCESS.2020.3010018; Wang C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P238, DOI 10.1145/3343031.3350872; Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693; Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143; Wang W, 2020, ARXIV PREPRINT ARXIV; Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886; Xie SY, 2021, IEEE T CIRC SYST VID, V31, P2359, DOI 10.1109/TCSVT.2020.3024201; Xie SY, 2019, PATTERN RECOGN, V92, P177, DOI 10.1016/j.patcog.2019.03.019; Xie WC, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106966; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yan Y, 2020, IEEE T MULTIMEDIA, V22, P2792, DOI 10.1109/TMM.2019.2962317; Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231; Yang HY, 2018, IEEE INT CONF AUTOMA, P294, DOI 10.1109/FG.2018.00050; Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435; Zeng JB, 2018, LECT NOTES COMPUT SC, V11217, P227, DOI 10.1007/978-3-030-01261-8_14; Zhang FF, 2020, IEEE T IMAGE PROCESS, V29, P6574, DOI 10.1109/TIP.2020.2991549; Zhang FF, 2020, IEEE T IMAGE PROCESS, V29, P4445, DOI 10.1109/TIP.2020.2972114; Zhang FF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P126, DOI 10.1145/3240508.3240574; Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354; Zhang YH, 2020, IEEE INT CONF AUTOMA, P356, DOI 10.1109/FG47880.2020.00134; Zhang Z., 2018, BMVC, P1; Zhang ZP, 2018, INT J COMPUT VISION, V126, P550, DOI 10.1007/s11263-017-1055-1; Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110; Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002; Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320; Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27	87	3	3	10	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					455	477		10.1007/s11263-021-01556-7	http://dx.doi.org/10.1007/s11263-021-01556-7		JAN 2022	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS					2022-12-18	WOS:000739265400002
J	Li, ZM; Sedlar, J; Carpentier, J; Laptev, I; Mansard, N; Sivic, J				Li, Zongmian; Sedlar, Jiri; Carpentier, Justin; Laptev, Ivan; Mansard, Nicolas; Sivic, Josef			Estimating 3D Motion and Forces of Human-Object Interactions from Internet Videos	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Single-view 3D pose estimation; Force estimation; Person-object interaction; Instructional video; Contact recognition; Motion capture	ALGORITHMS; PEOPLE; POSE	In this paper, we introduce a method to automatically reconstruct the 3D motion of a person interacting with an object from a single RGB video. Our method estimates the 3D poses of the person together with the object pose, the contact positions and the contact forces exerted on the human body. The main contributions of this work are three-fold. First, we introduce an approach to jointly estimate the motion and the actuation forces of the person on the manipulated object by modeling contacts and the dynamics of the interactions. This is cast as a large-scale trajectory optimization problem. Second, we develop a method to automatically recognize from the input video the 2D position and timing of contacts between the person and the object or the ground, thereby significantly simplifying the complexity of the optimization. Third, we validate our approach on a recent video + MoCap dataset capturing typical parkour actions, and demonstrate its performance on a new dataset of Internet videos showing people manipulating a variety of tools in unconstrained environments.	[Li, Zongmian; Carpentier, Justin; Laptev, Ivan] PSL Res Univ, Dept Informat ENS, CNRS, Ecole Normale Super, Paris, France; [Li, Zongmian; Carpentier, Justin; Laptev, Ivan] Inria Paris, Willow Project, Paris, France; [Sedlar, Jiri; Sivic, Josef] Czech Tech Univ, Czech Inst Informat Robot & Cybernet, Prague, Czech Republic; [Mansard, Nicolas] Univ Toulouse, CNRS, LAAS CNRS, Toulouse, France; [Mansard, Nicolas] Artif & Nat Intelligence Toulouse Insitute ANITI, Toulouse, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Czech Technical University Prague; Centre National de la Recherche Scientifique (CNRS); Universite de Toulouse	Li, ZM (corresponding author), PSL Res Univ, Dept Informat ENS, CNRS, Ecole Normale Super, Paris, France.; Li, ZM (corresponding author), Inria Paris, Willow Project, Paris, France.	zongmian.li@gmail.com			ERC grant LEAP [336845]; French government [ANR-19-P3IA-0001, ANR-19-P3IA-0004]; European Regional Development Fund under the project IMPACT [CZ.02.1.01/0.0/0.0/15 003/0000468]	ERC grant LEAP; French government; European Regional Development Fund under the project IMPACT	We thank BrunoWatier (Universite Paul Sabatier and LAAS-CNRS) and GaloMaldonado (ENSAM ParisTech) for making public the Parkour dataset. This work was partly supported by the ERC grant LEAP (No. 336845), the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, references ANR-19-P3IA-0001 (PRAIRIE 3IA Institute) and ANR-19-P3IA-0004 (ANITI 3IA Institute), and the European Regional Development Fund under the project IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15 003/0000468).	Abdulla W., 2017, GITHUB REPOSITORY, DOI DOI 10.1109/CVPR.2017.106; Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751; Alayrac JB, 2016, PROC CVPR IEEE, P4575, DOI 10.1109/CVPR.2016.495; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Biegler LT, 2010, MOS-SIAM SER OPTIMIZ, V10, pXIII, DOI 10.1137/1.9780898719383; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Boulic R., 1990, Visual Computer, V6, P344, DOI 10.1007/BF01901021; Bourdev L., 2011, HUMAN ANNOTATION TOO; Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366; Brubaker M. A., 2007, CVPR; Brubaker MA, 2009, IEEE I CONF COMP VIS, P2389, DOI 10.1109/ICCV.2009.5459407; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carpentier J., 2017, RSS WORKSH CHALL DYN; Carpentier J., 2015, PINOCCHIO FAST FORWA; Carpentier J, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Carpentier J, 2019, IEEE/SICE I S SYS IN, P614, DOI 10.1109/SII.2019.8700380; Carpentier J, 2018, IEEE T ROBOT, V34, P1441, DOI 10.1109/TRO.2018.2862902; Delaitre Vincent, 2011, NIPS; Diehl M, 2006, LECT NOTES CONTR INF, V340, P65; Doumanoglou A, 2016, PROC CVPR IEEE, P3583, DOI 10.1109/CVPR.2016.390; Featherstone Roy, 2008, RIGID BODY DYNAMICS; Fouhey DF, 2014, INT J COMPUT VISION, V110, P259, DOI 10.1007/s11263-014-0710-z; Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1; Gammeter S, 2008, LECT NOTES COMPUT SC, V5303, P816, DOI 10.1007/978-3-540-88688-4_60; GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478; Grabner A, 2018, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2018.00319; Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2017, ARXIV170306870, P2980, DOI [10.1109/ICCV.2017.322, DOI 10.1109/ICCV.2017.322]; Herdt A, 2010, IEEE INT C INT ROBOT; Hinterstoisser S, 2016, LECT NOTES COMPUT SC, V9907, P834, DOI 10.1007/978-3-319-46487-9_51; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jiang Y, 2013, PROC CVPR IEEE, P2993, DOI 10.1109/CVPR.2013.385; Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Kuffner J, 2005, SPR TRA ADV ROBOT, V15, P365; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Li ZM, 2019, PROC CVPR IEEE, P8632, DOI 10.1109/CVPR.2019.00884; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Loing V, 2018, INT J COMPUT VISION, V126, P1045, DOI 10.1007/s11263-018-1102-6; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Loper M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661273; Maldonado, 2018, SOME BIOMECHANICAL R; Maldonado G, 2017, COMPUT METHOD BIOMEC, V20, P123, DOI 10.1080/10255842.2017.1382892; Malmaud Jonathan, 2015, NAACL; Marinoiu E, 2013, IEEE I CONF COMP VIS, P1289, DOI 10.1109/ICCV.2013.163; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Mierle Keir, 2012, CERES SOLVER; Mordatch I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185539; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Newell A, 2017, ADV NEUR IN, V30; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Oberweger M, 2018, LECT NOTES COMPUT SC, V11219, P125, DOI 10.1007/978-3-030-01267-0_8; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Posa M, 2014, INT J ROBOT RES, V33, P69, DOI 10.1177/0278364913506757; Prest A, 2013, IEEE T PATTERN ANAL, V35, P835, DOI 10.1109/TPAMI.2012.175; Rad M, 2018, PROC CVPR IEEE, P4663, DOI 10.1109/CVPR.2018.00490; Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413; Rempe Davis, 2020, P EUR C COMP VIS ECC; Schultz G, 2010, IEEE-ASME T MECH, V15, P783, DOI 10.1109/TMECH.2009.2035112; Shimada S, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417877; Sidenbladh H., 2000, LNCS, V2, P702; Tassa Y, 2012, IEEE INT C INT ROBOT, P4906, DOI 10.1109/IROS.2012.6386025; Taylor CJ, 2000, PROC CVPR IEEE, P677, DOI 10.1109/CVPR.2000.855885; Tejani A, 2014, LECT NOTES COMPUT SC, V8694, P462, DOI 10.1007/978-3-319-10599-4_30; Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Tonneau S, 2018, IEEE T ROBOT, V34, P586, DOI 10.1109/TRO.2018.2819658; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Wei XL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778779; Westervelt ER, 2003, IEEE T AUTOMAT CONTR, V48, P42, DOI 10.1109/TAC.2002.806653; Winkler A.W., 2018, IEEE ROBOT AUTOMAT L, V3, P1560, DOI [10.1109/lra.2018.2798285, DOI 10.1109/LRA.2018.2798285]; Xiang DL, 2019, PROC CVPR IEEE, P10957, DOI 10.1109/CVPR.2019.01122; Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Yao BP, 2012, IEEE T PATTERN ANAL, V34, P1691, DOI 10.1109/TPAMI.2012.67; Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537	80	3	3	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					363	383		10.1007/s11263-021-01540-1	http://dx.doi.org/10.1007/s11263-021-01540-1		JAN 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		Green Submitted			2022-12-18	WOS:000738560100004
J	Fu, H; Jia, RF; Gao, L; Gong, MM; Zhao, BQ; Maybank, S; Tao, DC				Fu, Huan; Jia, Rongfei; Gao, Lin; Gong, Mingming; Zhao, Binqiang; Maybank, Steve; Tao, Dacheng			3D-FUTURE: 3D Furniture Shape with TextURE	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D-FUTURE; Furniture shapes; Textures; Interior designs; Synthetic images	MEANS CLUSTERING-ALGORITHM; OBJECT; RECOGNITION; DATABASE	The 3D CAD shapes in current 3D benchmarks are mostly collected from online model repositories. Thus, they typically have insufficient geometric details and less informative textures, making them less attractive for comprehensive and subtle research in areas such as high-quality 3D mesh and texture recovery. This paper presents 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated and large-scale repository of 3D furniture shapes in the household scenario. At the time of this technical report, 3D-FUTURE contains 9992 modern 3D furniture shapes with high-resolution textures and detailed attributes. To support the studies of 3D modeling from images, we couple the CAD models with 20,240 scene images. The room scenes are designed by professional designers or generated by an industrial scene creating system. Given the well-organized 3D-FUTURE and its characteristics, we provide a package of baseline experiments, such as joint 2D instance segmentation and 3D object pose estimation, image-based 3D shape retrieval, 3D object reconstruction from a single image, texture recovery for 3D shapes, and furniture composition, to facilitate related future researches on our database.	[Fu, Huan; Jia, Rongfei; Zhao, Binqiang] Alibaba Grp, Tao Technol Dept, Hangzhou, Peoples R China; [Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China; [Gong, Mingming] Univ Melbourne, Parkville, Vic, Australia; [Maybank, Steve] Univ London, Birkbeck Coll, Dept Comp Sci & Informat Syst, London, England; [Tao, Dacheng] Univ Sydney, Sch Comp Sci, Camperdown, NSW, Australia	Alibaba Group; Chinese Academy of Sciences; Institute of Computing Technology, CAS; University of Melbourne; University of London; Birkbeck University London; University of Sydney	Tao, DC (corresponding author), Univ Sydney, Sch Comp Sci, Camperdown, NSW, Australia.	fuhuan.fh@alibaba-inc.com; rongfei.jrf@alibaba-inc.com; gaolinorange@gmail.com; mingming.gong@unimelb.edu.au; bingiang.zhao@alibaba-inc.com; sjmaybank@dcs.bbk.ac.uk; dacheng.tao@sydney.edu.au		Fu, Huan/0000-0003-0843-7955	Australian Research Council Project [FL-170100117]; Australian Research Council [DE-210101624]	Australian Research Council Project(Australian Research Council); Australian Research Council(Australian Research Council)	Dacheng Tao is supported by Australian Research Council Project FL-170100117. Mingming Gong is supported by Australian Research Council Project DE-210101624. We would like to thank Alibaba Topping Homestyler for the great help with the data preparation and image rendering. We also appreciate Alibaba Tianchi for managing the dataset so that it can be easily requested and downloaded.	Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Bachman C. W., 1978, US Patent, Patent No. [4,068,300, 4068300]; Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642; Bo LF, 2014, INT J ROBOT RES, V33, P581, DOI 10.1177/0278364913514283; Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35; Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen W, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2662, DOI 10.1145/3292500.3330652; Chen WZ, 2019, ADV NEUR IN, V32; Choi Sungjoon, 2016, ARXIV160202481; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Collet A, 2011, INT J ROBOT RES, V30, P1284, DOI 10.1177/0278364911401765; Cucurull G, 2019, PROC CVPR IEEE, P12609, DOI 10.1109/CVPR.2019.01290; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dhanachandra N, 2015, PROCEDIA COMPUT SCI, V54, P764, DOI 10.1016/j.procs.2015.06.090; Durou JD, 2008, COMPUT VIS IMAGE UND, V109, P22, DOI 10.1016/j.cviu.2007.09.003; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Fathi A., 2017, SEMANTIC INSTANCE SE, P3; Favaro P, 2005, IEEE T PATTERN ANAL, V27, P406, DOI 10.1109/TPAMI.2005.43; Feng YT, 2019, AAAI CONF ARTIF INTE, P8279; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Fu H., 2020, ADV NEURAL INFORM PR, V33, P14675; Gao L., 2020, ARXIV201006217; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Grabner A, 2019, INT CONF 3D VISION, P583, DOI 10.1109/3DV.2019.00070; Grabner A, 2018, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2018.00319; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Hensel M, 2017, ADV NEUR IN, V30; Hinterstoisser Stefan, 2012, P AS C COMP VIS, P2, DOI DOI 10.1007/978-3-642-37331-2_42; Hua BS, 2016, INT CONF 3D VISION, P92, DOI 10.1109/3DV.2016.18; Huang SY, 2018, LECT NOTES COMPUT SC, V11211, P194, DOI 10.1007/978-3-030-01234-2_12; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Kehl W, 2016, LECT NOTES COMPUT SC, V9907, P205, DOI 10.1007/978-3-319-46487-9_13; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee T, 2018, INT CONF 3D VISION, P258, DOI 10.1109/3DV.2018.00038; Li Wenbin, 2018, BRIT MACH VIS C BMVC; Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071; Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780; Liu TQ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766898; Massa F, 2016, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2016.648; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; NAJMAN L, 1994, SIGNAL PROCESS, V38, P99, DOI 10.1016/0165-1684(94)90059-0; Oechsle M, 2019, IEEE I CONF COMP VIS, P4530, DOI 10.1109/ICCV.2019.00463; Park Keunhong, 2020, P IEEE CVF C COMP VI; Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469; Qi CR, 2017, ADV NEUR IN, V30; Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413; Raj Amit, 2019, P IEEE CVF C COMP VI, P32; Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song C, 2020, PROC CVPR IEEE, P428, DOI 10.1109/CVPR42600.2020.00051; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314; Sun Yongbin, 2018, ARXIV180406375; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tasse FP, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980253; Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352; Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038; Tulsiani S., 2017, PROC CVPR IEEE, P2626, DOI DOI 10.1109/CVPR.2017.30; Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167; Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24; Vaswani A, 2017, ADV NEUR IN, V30; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wu JJ, 2017, ADV NEUR IN, V30; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Xiang Y, 2016, LECT NOTES COMPUT SC, V9912, P160, DOI 10.1007/978-3-319-46484-8_10; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu Qiangeng, 2019, ARXIV190510711; Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; Zhang YD, 2017, PROC CVPR IEEE, P5057, DOI 10.1109/CVPR.2017.537; Zheng Jia, 2019, ARXIV190800222, V2, P5; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544	97	3	3	5	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3313	3337		10.1007/s11263-021-01534-z	http://dx.doi.org/10.1007/s11263-021-01534-z		OCT 2021	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN		Green Submitted			2022-12-18	WOS:000707538700002
J	Lu, RY; Chen, B; Liu, GL; Cheng, ZH; Qiao, M; Yuan, X				Lu, Ruiying; Chen, Bo; Liu, Guanliang; Cheng, Ziheng; Qiao, Mu; Yuan, Xin			Dual-view Snapshot Compressive Imaging via Optical Flow Aided Recurrent Neural Network	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Compressive imaging; Compressive sensing; Deep learning; Field of view; Snapshot; Convolutional neural network; Optical flow; Recurrent neural network	VIDEO; SENSOR	Dual-view snapshot compressive imaging (SCI) aims to capture videos from two field-of-views (FoVs) using a 2D sensor (detector) in a single snapshot, achieving joint FoV and temporal compressive sensing, and thus enjoying the advantages of low-bandwidth, low-power and low-cost. However, it is challenging for existing model-based decoding algorithms to reconstruct each individual scene, which usually require exhaustive parameter tuning with extremely long running time for large scale data. In this paper, we propose an optical flow-aided recurrent neural network for dual video SCI systems, which provides high-quality decoding in seconds. Firstly, we develop a diversity amplification method to enlarge the differences between scenes of two FoVs, and design a deep convolutional neural network with dual branches to separate different scenes from the single measurement. Secondly, we integrate the bidirectional optical flow extracted from adjacent frames with the recurrent neural network to jointly reconstruct each video in a sequential manner. Extensive results on both simulation and real data demonstrate the superior performance of our proposed model in short inference time. The code and data are available at .	[Lu, Ruiying; Chen, Bo; Liu, Guanliang; Cheng, Ziheng] Xidian Univ, Natl Lab Radar Signal Proc, Xian, Shaanxi, Peoples R China; [Qiao, Mu] Ningbo Univ, Sch Phys Sci & Technol, Ningbo, Zhejiang, Peoples R China; [Yuan, Xin] Westlake Univ, Hangzhou, Zhejiang, Peoples R China	Xidian University; Ningbo University; Westlake University	Chen, B (corresponding author), Xidian Univ, Natl Lab Radar Signal Proc, Xian, Shaanxi, Peoples R China.; Yuan, X (corresponding author), Westlake Univ, Hangzhou, Zhejiang, Peoples R China.	ruiyinglu_xidian@163.com; bchen@mail.xidian.edu.cn; lgl_xidian@163.com; zhcheng@stu.xidian.edu.cn; mq0829@hotmail.com; xyuan@westlake.edu.cn	Qiao, Mu/AFO-5248-2022; Cheng, Ziheng/GWZ-7504-2022; chen, bo/AAC-7188-2022		National Natural Science Foundation of China [61771361]; 111 Project; Young Thousand Talent by Chinese Central Government	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); 111 Project(Ministry of Education, China - 111 Project); Young Thousand Talent by Chinese Central Government	The funding was provided by National Natural Science Foundation of China (Grand No. 61771361), the 111 Project (Grand No. B18039), Young Thousand Talent by Chinese Central Government.	Angayarkanni V, 2019, INT J MOB COMMUN, V17, P727, DOI 10.1504/IJMC.2019.102723; Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Hitomi Y, 2011, IEEE I CONF COMP VIS, P287, DOI 10.1109/ICCV.2011.6126254; Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Iliadis M, 2018, DIGIT SIGNAL PROCESS, V72, P9, DOI 10.1016/j.dsp.2017.09.010; Jalali S, 2019, IEEE T INFORM THEORY, V65, P8005, DOI 10.1109/TIT.2019.2940666; Kingma D. P., 2015, 3 INT C LEARN REPR I, P1; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P2990, DOI 10.1109/TPAMI.2018.2873587; Llull P, 2013, OPT EXPRESS, V21, P10526, DOI 10.1364/OE.21.010526; Lu SD, 2020, 2020 IEEE/ACM SYMPOSIUM ON EDGE COMPUTING (SEC 2020), P125, DOI 10.1109/SEC50012.2020.00017; Ma JW, 2019, IEEE I CONF COMP VIS, P10222, DOI 10.1109/ICCV.2019.01032; Mait JN, 2018, ADV OPT PHOTONICS, V10, P409, DOI 10.1364/AOP.10.000409; Meng Ziyi, 2020, ARXIV201208364; Miao X, 2019, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2019.00416; Nakamura T, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061329; Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372; Pont-Tuset J., 2017, ARXIV170400675; Qiao M, 2020, OPT LETT, V45, P1659, DOI 10.1364/OL.386238; Qiao M, 2020, APL PHOTONICS, V5, DOI 10.1063/1.5140721; Reddy D, 2011, PROC CVPR IEEE, P329, DOI 10.1109/CVPR.2011.5995542; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Spacek L, 2005, ROBOT AUTON SYST, V51, P3, DOI 10.1016/j.robot.2004.08.009; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun YY, 2017, OPT EXPRESS, V25, P18182, DOI 10.1364/OE.25.018182; Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24; Wagadarikar A, 2008, APPL OPTICS, V47, pB44, DOI 10.1364/AO.47.000B44; Wagadarikar AA, 2009, OPT EXPRESS, V17, P6368, DOI 10.1364/OE.17.006368; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xu K., 2016, ARXIV161205203; Xu R, 2019, PROC CVPR IEEE, P3718, DOI 10.1109/CVPR.2019.00384; Yang JB, 2014, IEEE T IMAGE PROCESS, V23, P4863, DOI 10.1109/TIP.2014.2344294; Yoshida M, 2018, LECT NOTES COMPUT SC, V11214, P649, DOI 10.1007/978-3-030-01249-6_39; Yuan X., 2021, ARXIV210104822; Yuan X., 2020, ARXIV200508028; Yuan X, 2021, IEEE SIGNAL PROC MAG, V38, P65, DOI 10.1109/MSP.2020.3023869; Yuan X, 2020, PROC CVPR IEEE, P1444, DOI 10.1109/CVPR42600.2020.00152; Yuan X, 2017, APPL OPTICS, V56, P2697, DOI 10.1364/AO.56.002697; Yuan X, 2016, IEEE IMAGE PROC, P2539, DOI 10.1109/ICIP.2016.7532817; Yuan X, 2014, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2014.424; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Ziheng Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P258, DOI 10.1007/978-3-030-58586-0_16	47	3	3	5	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3279	3298		10.1007/s11263-021-01532-1	http://dx.doi.org/10.1007/s11263-021-01532-1		OCT 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN		Green Submitted			2022-12-18	WOS:000707538700001
J	Zheng, CAX; Cham, TJ; Cai, JF				Zheng, Chuanxia; Cham, Tat-Jen; Cai, Jianfei			Pluralistic Free-Form Image Completion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image completion; Multi-modal generative models; Image generation; Conditional variational auto-encoders	OBJECT REMOVAL	Image completion involves filling plausible contents to missing regions in images. Current image completion methods produce only one result for a given masked image, although there may be many reasonable possibilities. In this paper, we present an approach for pluralistic image completion-the task of generating multiple and diverse plausible solutions for free-form image completion. A major challenge faced by learning-based approaches is that usually only one ground truth training instance per label for this multi-output problem. To overcome this, we propose a novel and probabilistically principled framework with two parallel paths. One is a reconstructive path that utilizes the only one ground truth to get prior distribution of missing patches and rebuild the original image from this distribution. The other is a generative path for which the conditional prior is coupled to the distribution obtained in the reconstructive path. Both are supported by adversarial learning. We then introduce a new short+long term patch attention layer that exploits distant relations among decoder and encoder features, to improve appearance consistency between the original visible and the generated new regions. Experiments show that our method not only yields better results in various datasets than existing state-of-the-art methods, but also provides multiple and diverse outputs.	[Zheng, Chuanxia; Cham, Tat-Jen] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore; [Cai, Jianfei] Monash Univ, Dept Data Sci & AI, Clayton, Vic, Australia	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Monash University	Zheng, CAX (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.	chuanxia001@e.ntu.edu.sg; astjcham@ntu.edu.sg; Jianfei.Cai@monash.edu	Zheng, Chuanxia/GQI-0645-2022	zheng, chuanxia/0000-0002-3584-9640	RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAFICP) Funding Initiative; Monash FIT Start-up Grant	RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAFICP) Funding Initiative; Monash FIT Start-up Grant	This study is supported under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAFICP) Funding Initiative, as well as cash and in-kind contribution from Singapore Telecommunications Limited (Singtel), through Singtel Cognitive and Artificial Intelligence Lab for Enterprises (SCALE@NTU). This research is also supported by the Monash FIT Start-up Grant.	Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036; Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299; Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388; Chen Z, 2018, ARXIV180405232; Criminisi A., 2003, PROC CVPR IEEE, V2, pII; Deng Y, 2020, IEEE IMAGE PROC, P1088, DOI 10.1109/ICIP40778.2020.9191275; Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hara T, 2020, ARXIV PREPRINT ARXIV; Hensel M, 2017, ADV NEUR IN, V30; Hongyu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P725, DOI 10.1007/978-3-030-58536-5_43; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jaderberg M, 2015, ADV NEUR IN, V28; Jia JY, 2004, IEEE T PATTERN ANAL, V26, P771, DOI 10.1109/TPAMI.2004.10; Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI DOI 10.1109/CVPR42600.2020.00813; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kohler R, 2014, LECT NOTES COMPUT SC, V8753, P523, DOI 10.1007/978-3-319-11752-2_43; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Levin A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P305; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mathieu Michael, 2015, ARXIV151105440; Nazeri Kamyar, 2019, ARXIV190100212; Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Peng, 2021, ARXIV PREPRINT ARXIV; Portenier T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201393; Ren JS., 2015, ADV NEURAL INF PROCE, V1, P901; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Sohn K, 2015, ADV NEUR IN, V28; Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI 10.1007/978-3-030-01216-8_1; Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51; Wang Y, 2018, ADV NEUR IN, V31; Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Yi Zili, 2020, P IEEE CVF C COMP VI, P7508, DOI DOI 10.1109/CVPR42600.2020.00753; Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457; Yu ZL, 2022, CRIT REV FOOD SCI, V62, P905, DOI [10.1007/978-3-030-58529-7_1, 10.1080/10408398.2020.1830262]; Zhang Han, 2018, ARXIV180508318; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153; Zheng CX, 2018, LECT NOTES COMPUT SC, V11211, P798, DOI 10.1007/978-3-030-01234-2_47; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	64	3	3	3	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2786	2805		10.1007/s11263-021-01502-7	http://dx.doi.org/10.1007/s11263-021-01502-7		JUL 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO					2022-12-18	WOS:000679638500001
J	Marin, R; Rampini, A; Castellani, U; Rodola, E; Ovsjanikov, M; Melzi, S				Marin, Riccardo; Rampini, Arianna; Castellani, Umberto; Rodola, Emanuele; Ovsjanikov, Maks; Melzi, Simone			Spectral Shape Recovery and Analysis Via Data-driven Connections	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape from spectrum; Spectral geometry; Shape analysis; Representation learning; Geometry processing	HEAR	We introduce a novel learning-based method to recover shapes from their Laplacian spectra, based on establishing and exploring connections in a learned latent space. The core of our approach consists in a cycle-consistent module that maps between a learned latent space and sequences of eigenvalues. This module provides an efficient and effective link between the shape geometry, encoded in a latent vector, and its Laplacian spectrum. Our proposed data-driven approach replaces the need for ad-hoc regularizers required by prior methods, while providing more accurate results at a fraction of the computational cost. Moreover, these latent space connections enable novel applications for both analyzing and controlling the spectral properties of deformable shapes, especially in the context of a shape collection. Our learning model and the associated analysis apply without modifications across different dimensions (2D and 3D shapes alike), representations (meshes, contours and point clouds), nature of the latent space (generated by an auto-encoder or a parametric model), as well as across different shape classes, and admits arbitrary resolution of the input spectrum without affecting complexity. The increased flexibility allows us to address notoriously difficult tasks in 3D vision and geometry processing within a unified framework, including shape generation from spectrum, latent space exploration and analysis, mesh super-resolution, shape exploration, style transfer, spectrum estimation for point clouds, segmentation transfer and non-rigid shape matching.	[Marin, Riccardo; Rampini, Arianna; Rodola, Emanuele; Melzi, Simone] Sapienza Univ Rome, Rome, Italy; [Castellani, Umberto] Univ Verona, Verona, Italy; [Ovsjanikov, Maks] Ecole Polytech, IP Paris, LIX, Paris, France	Sapienza University Rome; University of Verona	Melzi, S (corresponding author), Sapienza Univ Rome, Rome, Italy.	marin@di.uniroma1.it; melzi@di.uniroma1.it	Melzi, Simone/AFI-2491-2022; Marin, Riccardo/ABF-5990-2022	Melzi, Simone/0000-0003-2790-9591; Marin, Riccardo/0000-0003-2392-4612	KAUST OSR Award [CRG-2017-3426]; ERC [802554, 758800]; ANR AI Chair AIGRETTE; MIUR; Department of Computer Science of Sapienza University; University of Verona	KAUST OSR Award; ERC(European Research Council (ERC)European Commission); ANR AI Chair AIGRETTE(French National Research Agency (ANR)); MIUR(Ministry of Education, Universities and Research (MIUR)); Department of Computer Science of Sapienza University; University of Verona	We gratefully acknowledge Luca Moschella and Silvia Casola for the technical support, Nicholas Sharp for the useful suggestions about pointcloud spectra. Parts of this work were supported by the KAUST OSR Award No. CRG-2017-3426, the ERC Starting Grant No. 758800 (EXPROTEA), the ERC Starting Grant No. 802554 (SPECGEO), the ANR AI Chair AIGRETTE, and the MIUR under grant "Dipartimenti di eccellenza 2018-2022" of the Department of Computer Science of Sapienza University and University of Verona.	Aasen D, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.121301; Achlioptas P, 2018, PR MACH LEARN RES, V80; Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Aumentado-Armstrong T, 2019, IEEE I CONF COMP VIS, P8180, DOI 10.1109/ICCV.2019.00827; BANDO S, 1983, TOHOKU MATH J, V35, P155, DOI 10.2748/tmj/1178229047; Belkin M, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1031; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bharaj G, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818108; Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491; Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844; Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P265, DOI 10.1111/cgf.12558; Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Chavel I., 1984, EIGENVALUES RIEMANNI; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Chu MT, 2005, INVERSE EIGENVALUE P; Ciarlet P.G., 1978, FINITE ELEMENT METHO; Clarenz Ulrich, 2004, P 1 EUR C POINT BAS, P201; Corman E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2999535; Cosmo L, 2019, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2019.00771; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dyke RM, 2020, COMPUT GRAPH-UK, V92, P28, DOI 10.1016/j.cag.2020.08.008; Gao L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356488; GORDON C, 1992, B AM MATH SOC, V27, P134, DOI 10.1090/S0273-0979-1992-00289-6; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Huang Jie, 2018, AAAI, DOI [10.1609/aaai.v32i1.11903, DOI 10.48550/ARXIV.1801.10111]; Huang RQ, 2019, IEEE I CONF COMP VIS, P8587, DOI 10.1109/ICCV.2019.00868; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; KAC M, 1966, AM MATH MON, V73, P1, DOI 10.2307/2313748; Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924; Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974; Kostrikov I, 2018, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR.2018.00269; Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637; Litany O, 2018, PROC CVPR IEEE, P1886, DOI 10.1109/CVPR.2018.00202; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Marin R, 2020, INT CONF 3D VISION, P120, DOI 10.1109/3DV50981.2020.00022; Masci J., 2016, SIGGRAPH ASIA 2016 C; Melzi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356524; Melzi Simone, 2019, EUR WORKSH 3D OBJ RE; Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527; Nogneng D, 2017, COMPUT GRAPH FORUM, V36, P259, DOI 10.1111/cgf.13124; Oztireli AC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866190; Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526; Panine M, 2016, PHYS REV D, V93, DOI 10.1103/PhysRevD.93.084033; Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123; Pinkall U., 1993, EXPT MATH, V2, P15, DOI DOI 10.1080/10586458.1993.10504266; Rampini A, 2019, INT CONF 3D VISION, P37, DOI 10.1109/3DV.2019.00014; Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43; Reuter M., 2005, P ACM S SOL PHYS MOD, P101, DOI [10.1145/1060244.1060256, DOI 10.1145/1060244.1060256]; Reuter M, 2010, INT J COMPUT VISION, V89, P287, DOI 10.1007/s11263-009-0278-1; Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883; Roufosse JM, 2019, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2019.00170; Rustamov RM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461959; Sharp N, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322979; Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Wu ZJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322956; Yi L., 2017, ARXIV PREPRINT ARXIV; Zuffi Silvia, 2017, PROC CVPR IEEE, P6365, DOI DOI 10.1109/CVPR.2017.586	60	3	3	2	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2745	2760		10.1007/s11263-021-01492-6	http://dx.doi.org/10.1007/s11263-021-01492-6		JUL 2021	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO	34720402	Green Published, hybrid			2022-12-18	WOS:000678053000001
J	Yang, Y; Tan, ZC; Tiwari, P; Pandey, HM; Wan, J; Lei, Z; Guo, GD; Li, SZ				Yang, Yang; Tan, Zichang; Tiwari, Prayag; Pandey, Hari Mohan; Wan, Jun; Lei, Zhen; Guo, Guodong; Li, Stan Z.			Cascaded Split-and-Aggregate Learning with Feature Recombination for Pedestrian Attribute Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pedestrian attribute recognition; Attention; Split-and-aggregate learning; Feature recombination		Multi-label pedestrian attribute recognition in surveillance is inherently a challenging task due to poor imaging quality, large pose variations, and so on. In this paper, we improve its performance from the following two aspects: (1) We propose a cascaded Split-and-Aggregate Learning (SAL) to capture both the individuality and commonality for all attributes, with one at the feature map level and the other at the feature vector level. For the former, we split the features of each attribute by using a designed attribute-specific attention module (ASAM). For the later, the split features for each attribute are learned by using constrained losses. In both modules, the split features are aggregated by using several convolutional or fully connected layers. (2) We propose a Feature Recombination (FR) that conducts a random shuffle based on the split features over a batch of samples to synthesize more training samples, which spans the potential samples' variability. To the end, we formulate a unified framework, named CAScaded Split-and-Aggregate Learning with Feature Recombination (CAS-SAL-FR), to learn the above modules jointly and concurrently. Experiments on five popular benchmarks, including RAP, PA-100K, PETA, Market-1501 and Duke attribute datasets, show the proposed CAS-SAL-FR achieves new state-of-the-art performance.	[Yang, Yang; Wan, Jun; Lei, Zhen; Li, Stan Z.] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Ctr Biometr & Secur Res, Beijing, Peoples R China; [Yang, Yang; Wan, Jun; Lei, Zhen] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing, Peoples R China; [Lei, Zhen] Chinese Acad Sci, Hong Kong Inst Sci & Innovat, Ctr Artificial Intelligence & Robot, Hong Kong, Peoples R China; [Tan, Zichang; Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing, Peoples R China; [Tan, Zichang; Guo, Guodong] Natl Engn Lab Deep Learning Technol & Applicat, Beijing, Peoples R China; [Tiwari, Prayag] Aalto Univ, Dept Comp Sci, Espoo, Finland; [Pandey, Hari Mohan] Edge Hill Univ, Dept Comp Sci, Ormskirk, England	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Baidu; Aalto University; Edge Hill University	Wan, J (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Ctr Biometr & Secur Res, Beijing, Peoples R China.; Wan, J (corresponding author), Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing, Peoples R China.	yang.yang@nlpr.ia.ac.cn; tanzichang@baidu.com; prayag.tiwari@aalto.fi; pandeyh@edgehill.ac.uk; jun.wan@nlpr.ia.ac.cn; zlei@nlpr.ia.ac.cn; guoguodong01@baidu.com; szli@nlpr.ia.ac.cn	Pandey, Hari Mohan/M-9658-2015; Tiwari, Prayag/N-6261-2017	Pandey, Hari Mohan/0000-0002-9128-068X; Tiwari, Prayag/0000-0002-2851-4260; yang, yang/0000-0003-0559-5464	National Key Research and Development Program [2020YFC2003901]; Chinese National Natural Science Foundation [61806203, 61961160704, 61876179]; External cooperation key project of Chinese Academy Sciences [173211KYSB20200002]; Key Project of the General Logistics Department [AWS17J001]; Science and Technology Development Fund of Macau [0010/2019/AFJ, 0025/2019/AKP, 0019/2018/ASC]; Spanish project (MINECO/FEDER, UE) [TIN2016-74946-P]; CERCA Programme/Generalitat de Catalunya; Academy of Finland [336033, 315896]; Business Finland [884/31/2018]; EU [101016775]	National Key Research and Development Program; Chinese National Natural Science Foundation(National Natural Science Foundation of China (NSFC)); External cooperation key project of Chinese Academy Sciences; Key Project of the General Logistics Department; Science and Technology Development Fund of Macau; Spanish project (MINECO/FEDER, UE)(Spanish Government); CERCA Programme/Generalitat de Catalunya; Academy of Finland(Academy of Finland); Business Finland; EU(European Commission)	This work was partially supported by the National Key Research and Development Program (No. 2020YFC2003901), the Chinese National Natural Science Foundation Projects #61806203, #61961160704, #61876179, the External cooperation key project of Chinese Academy Sciences #173211KYSB20200002, the Key Project of the General Logistics Department Grant No. AWS17J001, Science and Technology Development Fund of Macau (No. 0010/2019/AFJ, 0025/2019/AKP, 0019/2018/ASC), the Spanish project TIN2016-74946-P (MINECO/FEDER, UE) and CERCA Programme/Generalitat de Catalunya. This work was also supported in part by the Academy of Finland (Grants 336033, 315896), Business Finland (Grant 884/31/2018), and EU H2020 (Grant 101016775).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966; Dixit M, 2017, PROC CVPR IEEE, P3328, DOI 10.1109/CVPR.2017.355; Fu CY, 2019, ADV NEUR IN, V32; Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476; GAO L, 2019, ACM MM, P1340; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082; HAN K, 2019, IJCAI, P2456; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hongyi Z., 2018, INT C LEARN REPR; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jia Y., 2014, P 22 ACM INT C MULT, P675; Jie H., 2017, P IEEE C COMP VIS PA, P99; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li DW, 2018, IEEE INT CON MULTI; Li DW, 2019, IEEE T IMAGE PROCESS, V28, P1575, DOI 10.1109/TIP.2018.2878349; Li DW, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P111, DOI 10.1109/ACPR.2015.7486476; Li QZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P833; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Lim J. J., 2011, NEURIPS; Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006; Liu B, 2018, PROC CVPR IEEE, P9090, DOI 10.1109/CVPR.2018.00947; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Liu Pengze, 2018, BRIT MACH VIS C 2018; Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118; Sarafianos N, 2018, LECT NOTES COMPUT SC, V11215, P708, DOI 10.1007/978-3-030-01252-6_42; Sarfraz M. S., 2017, BMVC; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tan ZC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3548; Tan ZC, 2019, IEEE T IMAGE PROCESS, V28, P6126, DOI 10.1109/TIP.2019.2919199; Tan ZC, 2018, IEEE T PATTERN ANAL, V40, P2610, DOI 10.1109/TPAMI.2017.2779808; Tang CF, 2019, IEEE I CONF COMP VIS, P4996, DOI 10.1109/ICCV.2019.00510; Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI 10.1109/ICCV.2017.283; Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Wu M., 2020, AAAI; Wu SZ, 2019, INT J COMPUT VISION, V127, P560, DOI 10.1007/s11263-019-01157-5; XIANG LY, 2019, INCREMENTAL FEW SHOT, P3912; Xiangyu Z., 2020, IJCV; Yu F., 2016, P ICLR 2016; Zeng HT, 2020, IEEE INT CON MULTI; Zhang SF, 2019, INT J COMPUT VISION, V127, P537, DOI 10.1007/s11263-019-01159-3; Zhao Huang RH Xin, 2019, AAAI; Zhao X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3177; Zhao X, 2019, AAAI CONF ARTIF INTE, P9275; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhu JQ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P331, DOI 10.1109/ICCVW.2013.51; Zhu XY, 2019, INT J COMPUT VISION, V127, P684, DOI 10.1007/s11263-019-01162-8	60	3	3	3	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2731	2744		10.1007/s11263-021-01499-z	http://dx.doi.org/10.1007/s11263-021-01499-z		JUL 2021	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO		Green Submitted			2022-12-18	WOS:000674202900001
J	Zhang, PY; Wang, D; Lu, HCA; Yang, XY				Zhang, Pengyu; Wang, Dong; Lu, Huchuan; Yang, Xiaoyun			Learning Adaptive Attribute-Driven Representation for Real-Time RGB-T Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object tracking; RGB-T tracking; Deep learning	FUSION TRACKING	The development of a real-time and robust RGB-T tracker is an extremely challenging task because the tracked object may suffer from shared and specific challenges in RGB and thermal (T) modalities. In this work, we observe that the implicit attribute information can boost the model discriminability, and propose a novel attribute-driven representation network to improve the RGB-T tracking performance. First, according to appearance change in RGB-T tracking scenarios, we divide the major and special challenges into four typical attributes: extreme illumination, occlusion, motion blur, and thermal crossover. Second, we design an attribute-driven residual branch for each heterogeneous attribute to mine the attribute-specific property and therefore build a powerful residual representation for object modeling. Furthermore, we aggregate these representations in channel and pixel levels by using the proposed attribute ensemble network (AENet) to adaptively fit the attribute-agnostic tracking process. The AENet can effectively make aware of appearance change while suppressing the distractors. Finally, we conduct numerous experiments on three RGB-T tracking benchmarks to compare the proposed trackers with other state-ofthe-art methods. Experimental results show that our tracker achieves very competitive results with a real-time tracking speed. Code will be available at https://github.com/zhang-pengyu/ADRNet.	[Zhang, Pengyu; Wang, Dong; Lu, Huchuan] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China; [Zhang, Pengyu; Wang, Dong; Lu, Huchuan] Dalian Univ Technol, Ningbo Inst, Ningbo 315016, Peoples R China; [Yang, Xiaoyun] Remark Holdings, Las Vegas, NV USA	Dalian University of Technology; Dalian University of Technology	Wang, D (corresponding author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.; Wang, D (corresponding author), Dalian Univ Technol, Ningbo Inst, Ningbo 315016, Peoples R China.	pyzhang@mail.dlut.edu.cn; wdice@dlut.edu.cn; lhchuan@dlut.edu.cn; xyang@remarkholdings.com			National Natural Science Foundation of China [62022021, 61806037, 61872056, 61725202]; Science and Technology Innovation Foundation of Dalian [2020JJ26GX036]; Fundamental Research Funds for the Central Universities [DUT21LAB127]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Innovation Foundation of Dalian; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported in part by the National Natural Science Foundation of China under Grant 62022021, Grant 61806037, Grant 61872056, and Grant 61725202; and in part by the Science and Technology Innovation Foundation of Dalian under Grant 2020JJ26GX036; and in part by the Fundamental Research Funds for the Central Universities under Grant DUT21LAB127.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Camplani M., BRIT MACH VIS C, P1; Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670; Chenglong Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P222, DOI 10.1007/978-3-030-58542-6_14; Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721; Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928; Ding P, 2015, 2015 12TH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (FSKD), P930, DOI 10.1109/FSKD.2015.7382068; Feng Q., 2019, ABS191202048 CORR; Feng Q, 2020, IEEE WINT CONF APPL, P689, DOI 10.1109/WACV45572.2020.9093425; Gao YB, 2021, IEEE T SYST MAN CY-S, V51, P1981, DOI 10.1109/TSMC.2019.2911726; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48; Jung Ilchae, 2018, P EUR C COMP VIS ECC, P83; Kart U., 2018, EUR C COMP VIS WORKS, P1; Kart U, 2019, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2019.00143; Kart U, 2018, INT C PATT RECOG, P2112, DOI 10.1109/ICPR.2018.8546179; Kim DY, 2014, INFORM SCIENCES, V278, P641, DOI 10.1016/j.ins.2014.03.080; Kristan Matej, 2019, P IEEE INT C COMP VI, P1; Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618; Lan XY, 2019, IEEE ACCESS, V7, P67761, DOI 10.1109/ACCESS.2019.2916895; Lan XY, 2020, PATTERN RECOGN LETT, V130, P12, DOI 10.1016/j.patrec.2018.10.002; Lan XY, 2018, AAAI CONF ARTIF INTE, P7008; Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935; Li C, 2016, PROCEEDINGS OF 2016 IEEE 9TH UK-EUROPE-CHINA WORKSHOP ON MILLIMETRE WAVES AND TERAHERTZ TECHNOLOGIES (UCMMT), P54; Li CL, 2019, IEEE INT CONF COMP V, P2262, DOI 10.1109/ICCVW.2019.00279; Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977; Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289; Li CL, 2018, LECT NOTES COMPUT SC, V11217, P831, DOI 10.1007/978-3-030-01261-8_49; Li CL, 2018, NEUROCOMPUTING, V281, P78, DOI 10.1016/j.neucom.2017.11.068; Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135; Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18; Li ZY, 2017, PROC CVPR IEEE, P7350, DOI 10.1109/CVPR.2017.777; Liu HP, 2012, SCI CHINA INFORM SCI, V55, P590, DOI 10.1007/s11432-011-4536-9; Lu Huchuan, 2019, ONLINE VISUAL TRACKI, P1; Luo CW, 2019, INFRARED PHYS TECHN, V99, P265, DOI 10.1016/j.infrared.2019.04.017; Megherbi N, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P135; NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465; Ning JF, 2016, PROC CVPR IEEE, P4266, DOI 10.1109/CVPR.2016.462; Qi YK, 2019, AAAI CONF ARTIF INTE, P8835; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Seunghoon Hong Tackgeun You S.K., 2015, ONLINE TRACKING LEAR, P597; Song X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2414425.2414443; Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661; Wang CQ, 2020, PROC CVPR IEEE, P7062, DOI 10.1109/CVPR42600.2020.00709; Wang D, 2015, IEEE T IMAGE PROCESS, V24, P2646, DOI 10.1109/TIP.2015.2427518; Wang N, 2013, ADV NEURAL INFORM PR, DOI DOI 10.5555/2999611.2999702; Wang W, 2016, IEEE T IMAGE PROCESS, V25, P1465, DOI 10.1109/TIP.2016.2523340; Wang ZQ, 2019, IEEE INT CONF COMP V, P1437, DOI [10.1109/ICCVW.2019.00181, 10.1109/ICCV.2019.00408]; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Wu YF, 2011, PROCEEDINGS OF THE ASME INTERNATIONAL MANUFACTURING SCIENCE AND ENGINEERING CONFERENCE 2011, VOL 1, P1; Xu Y., 2020, AAAI, P12549, DOI [10.1609/aaai.v34i07.6944, DOI 10.1609/AAAI.V34I07.6944]; Yang RN, 2021, IEEE T SYST MAN CY-S, V51, P2737, DOI 10.1109/TSMC.2019.2916417; Yang Z., 2020, IEEE T CIRC SYST VID; Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676; Zhai SL, 2019, NEUROCOMPUTING, V334, P172, DOI 10.1016/j.neucom.2019.01.022; Zhang H., 2020, SENSORS-BASEL, V20; Zhang P., IEEE T IMAGE PROCESS, V30, P3335; Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34; Zhang Xingming, 2018, 2018 11 INT C IM SIG, P1, DOI DOI 10.1109/CISP-BMEI.2018.8633102; Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472; Zhu Y., 2018, ABS181109855 CORR; Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928	68	3	3	6	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2714	2729		10.1007/s11263-021-01495-3	http://dx.doi.org/10.1007/s11263-021-01495-3		JUL 2021	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN					2022-12-18	WOS:000673423500001
J	Chu, WQ; Hung, WC; Tsai, YH; Chang, YT; Li, YJ; Cai, D; Yang, MH				Chu, Wenqing; Hung, Wei-Chih; Tsai, Yi-Hsuan; Chang, Yu-Ting; Li, Yijun; Cai, Deng; Yang, Ming-Hsuan			Learning to Caricature via Semantic Shape Transform	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Caricature generation; Dense shape transformation; Semantic map	FACES	Caricature is an artistic drawing created to abstract or exaggerate facial features of a person. Rendering visually pleasing caricatures is a difficult task that requires professional skills, and thus it is of great interest to design a method to automatically generate such drawings. To deal with large shape changes, we propose an algorithm based on a semantic shape transform to produce diverse and plausible shape exaggerations. Specifically, we predict pixel-wise semantic correspondences and perform image warping on the input photo to achieve dense shape transformation. We show that the proposed framework is able to render visually pleasing shape exaggerations while maintaining their facial structures. In addition, our model allows users to manipulate the shape via the semantic map. We demonstrate the effectiveness of our approach on a large photograph-caricature benchmark dataset with comparisons to the state-of-the-art methods.	[Chu, Wenqing; Cai, Deng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China; [Hung, Wei-Chih; Chang, Yu-Ting; Yang, Ming-Hsuan] Univ Calif Merced, Elect Engn & Comp Sci, Merced, CA USA; [Tsai, Yi-Hsuan] NEC Labs Amer, Santa Clara, CA USA; [Chu, Wenqing] Tencent Youtu Lab, Shanghai, Peoples R China; [Li, Yijun] Adobe Res, San Jose, CA USA; [Yang, Ming-Hsuan] Yonsei Univ, Seoul, South Korea	Zhejiang University; University of California System; University of California Merced; NEC Corporation; Tencent; Adobe Systems Inc.; Yonsei University	Cai, D (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.	wqchu16@gmail.com; whung8@ucmerced.edu; wasidennis@gmail.com; ychang39@ucmerced.edu; yijli@adobe.com; dengcai@cad.zju.edu.cn; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304	National Key Research and Development Program of China [2018AAA0101400]; National Nature Science Foundation of China [62036009, 61936006]; Innovation Capability Support Program of Shaanxi [2021TD-05]; National Science Foundation CAREER Grant [1149783]	National Key Research and Development Program of China; National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); Innovation Capability Support Program of Shaanxi; National Science Foundation CAREER Grant(National Science Foundation (NSF))	This work was supported in part by The National Key Research and Development Program of China (Grant Nos: 2018AAA0101400), in part by The National Nature Science Foundation of China (Grant Nos: 62036009, 61936006), in part by Innovation Capability Support Program of Shaanxi (Program No. 2021TD-05). W.-C. Hung, Y.-T. Chang, Y. Li, and M.-H. Yang were supported in part by National Science Foundation CAREER Grant 1149783.	Aberman K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201332; Akleman E., 2000, P VIS; Akleman E., 1997, ACM SIGGRAPH 97 VIS, P145; Arjovsky M, 2017, PR MACH LEARN RES, V70; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Brennan SE, 2007, LEONARDO, V40, P392, DOI 10.1162/leon.2007.40.4.392; Chang Z, 2018, IEEE WCNC; Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296; Chen KY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3228, DOI 10.1145/3394171.3413643; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Chu WQ, 2019, IEEE IMAGE PROC, P3282, DOI 10.1109/ICIP.2019.8803517; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Ganin Y, 2016, LECT NOTES COMPUT SC, V9906, P311, DOI 10.1007/978-3-319-46475-6_20; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guenin BM, 1998, P IEEE SEMICOND THER, P55, DOI 10.1109/STHERM.1998.660387; Han XG, 2020, IEEE T VIS COMPUT GR, V26, P2349, DOI 10.1109/TVCG.2018.2886007; Hensel M, 2017, ADV NEUR IN, V30; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Huo J., 2018, BMVC; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jaderberg M, 2015, ADV NEUR IN, V28; Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kaidi Cao, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3272127.3275046; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Li WB, 2020, NEURAL NETWORKS, V132, P66, DOI 10.1016/j.neunet.2020.08.011; Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Li YJ, 2017, ADV NEUR IN, V30; Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36; Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683; Liao P. Y., 2004, ACCV; Lin CH, 2018, PROC CVPR IEEE, P9455, DOI 10.1109/CVPR.2018.00985; Liu MY, 2017, ADV NEUR IN, V30; Luo W. C., 2002, P INT COMP S; Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82; Parke FI, 1972, P ACM ANN C, V1, P451; Shi YC, 2019, PROC CVPR IEEE, P10754, DOI 10.1109/CVPR.2019.01102; Shu ZX, 2018, LECT NOTES COMPUT SC, V11214, P664, DOI 10.1007/978-3-030-01249-6_40; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324; Wu QY, 2018, PROC CVPR IEEE, P7336, DOI 10.1109/CVPR.2018.00766; Wu WL, 2017, IEEE I CONF COMP VIS, P3792, DOI 10.1109/ICCV.2017.407; Zheng ZQ, 2019, NEUROCOMPUTING, V355, P71, DOI 10.1016/j.neucom.2019.04.032; Zhou E., 2018, ECCV; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	54	3	3	2	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2663	2679		10.1007/s11263-021-01489-1	http://dx.doi.org/10.1007/s11263-021-01489-1		JUL 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN		hybrid, Green Submitted			2022-12-18	WOS:000671510500001
J	Chen, ZL; Wei, PX; Zhuang, JY; Li, GB; Lin, L				Chen, Ziliang; Wei, Pengxu; Zhuang, Jingyu; Li, Guanbin; Lin, Liang			Deep CockTail Networks A Universal Framework for Visual Multi-source Domain Adaptation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-source domain adaptation; Cross-domain visual recognition; Domain shift; Category shift; Open-set domain adaptation; Diverse transfer scenarios		Transferable deep representations for visual domain adaptation (DA) provides a route to learn from labeled source images to recognize target images without the aid of target-domain supervision. Relevant researches increasingly arouse a great amount of interest due to its potential industrial prospect for non-laborious annotation and remarkable generalization. However, DA presumes source images are identically sampled from a single source while Multi-Source DA (MSDA) is ubiquitous in the real-world. In MSDA, the domain shifts exist not only between source and target domains but also among the sources; especially, the multi-source and target domains may disagree on their semantics (e.g., category shifts). This issue challenges the existing solutions for MSDAs. In this paper, we propose Deep CockTail Network (DCTN), a universal and flexibly-deployed framework to address the problems. DCTN uses a multi-way adversarial learning pipeline to minimize the domain discrepancy between the target and each of the multiple in order to learn domain-invariant features. The derived source-specific perplexity scores measure how similar each target feature appears as a feature from one of source domains. The multi-source category classifiers are integrated with the perplexity scores to categorize target images. We accordingly derive a theoretical analysis towards DCTN, including the interpretation why DCTN can be successful without precisely crafting the source-specific hyper-parameters, and target expected loss upper bounds in terms of domain and category shifts. In our experiments, DCTNs have been evaluated on four benchmarks, whose empirical studies involve vanilla and three challenging category-shift transfer problems in MSDA, i.e., source-shift, target-shift and source-target-shift scenarios. The results thoroughly reveal that DCTN significantly boosts classification accuracies in MSDA and performs extraordinarily to resist negative transfers across different MSDA scenarios.	[Chen, Ziliang; Wei, Pengxu; Zhuang, Jingyu; Li, Guanbin; Lin, Liang] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou, Peoples R China; [Chen, Ziliang] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Sun Yat Sen University; Carnegie Mellon University	Wei, PX (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou, Peoples R China.	c.ziliang@yahoo.com; weipx3@mail.sysu.edu.cn; zhuangjy6@mail2.sysu.edu.cn; liguanbin@mail.sysu.edu.cn; linliang@ieee.org		wei, pengxu/0000-0002-2190-0767	NSFC [62006253, U181146, 61836012, 61976233]; State Key Development Program [2018YFC0830103]; Fundamental Research Funds for the Central Universities [19lgpy228]; Major Project of Guangzhou Science and Technology of Collaborative Innovation and Industry [201605122151511]	NSFC(National Natural Science Foundation of China (NSFC)); State Key Development Program; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Major Project of Guangzhou Science and Technology of Collaborative Innovation and Industry	This work was supported in part by NSFC (Nos. 62006253, U181146, 61836012, 61976233), State Key Development Program (No. 2018YFC0830103), Fundamental Research Funds for the Central Universities (No. 19lgpy228), and Major Project of Guangzhou Science and Technology of Collaborative Innovation and Industry under Grant 201605122151511. We also thank Ruijia Xu for his valuable suggestion to the revision.	[Anonymous], 2007, P 15 ACM INT C MULTI; Baktashmotlagh M, 2016, J MACH LEARN RES, V17; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Blitzer J., 2007, ADV NEURAL INFORM PR, V20, P129; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88; Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9; Cao ZJ, 2018, PROC CVPR IEEE, P2724, DOI 10.1109/CVPR.2018.00288; Cordts M., 2015, CVPR WORKSH FUT DAT, V2, P1; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duan LX, 2012, IEEE T NEUR NET LEAR, V23, P504, DOI 10.1109/TNNLS.2011.2178556; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10; Ganin Yaroslav, 2015, ICML; Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Gong BQ, 2014, INT J COMPUT VISION, V109, P3, DOI 10.1007/s11263-014-0718-4; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Gretton A, 2009, NEURAL INF PROCESS S, P131; Haeusser P, 2017, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2017.301; Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615; Ho HT, 2014, INT J COMPUT VISION, V109, P110, DOI 10.1007/s11263-014-0720-x; Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kan MN, 2014, INT J COMPUT VISION, V109, P94, DOI 10.1007/s11263-013-0693-1; Kim Y, 2020, IEEE SIGNAL PROC LET, V27, P1675, DOI 10.1109/LSP.2020.3025112; Kingma D.P, P 3 INT C LEARNING R; Koniusz P, 2017, PROC CVPR IEEE, P7139, DOI 10.1109/CVPR.2017.755; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liang XD, 2017, IEEE T PATTERN ANAL, V39, P115, DOI 10.1109/TPAMI.2016.2537339; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2017, PR MACH LEARN RES, V70; Long MS, 2016, ADV NEUR IN, V29; Lu H, 2017, IEEE I CONF COMP VIS, P599, DOI 10.1109/ICCV.2017.72; Mancini M, 2018, PROC CVPR IEEE, P3771, DOI 10.1109/CVPR.2018.00397; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Motiian S., 2017, ADV NEURAL INF PROCE, V30, P1; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rostamizadeh A., 2009, ADV NEURAL INFORM PR, P1041; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10; Saito K, 2017, PR MACH LEARN RES, V70; Saminger-Platz S., 2017, P INT C LEARN REPR I; Shao M, 2014, INT J COMPUT VISION, V109, P74, DOI 10.1007/s11263-014-0696-6; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Sun BC, 2016, AAAI CONF ARTIF INTE, P2058; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Van der Maaten L., 2008, J MACH LEARN RES, V9, P2579; Xie JW, 2015, INT J COMPUT VISION, V114, P91, DOI 10.1007/s11263-014-0757-x; Xu JL, 2016, INT J COMPUT VISION, V119, P159, DOI 10.1007/s11263-016-0885-6; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151; Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417; Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107; Yao Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1578, DOI 10.1145/3343031.3350955; You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283; Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547; Zhang S, 2019, IEEE GEOSCI REMOTE S, V16, P864, DOI 10.1109/LGRS.2018.2888887; Zhao H., 2018, ICLR WORKSH	68	3	3	4	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2021	129	8					2328	2351		10.1007/s11263-021-01463-x	http://dx.doi.org/10.1007/s11263-021-01463-x		MAY 2021	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TR3NV					2022-12-18	WOS:000652411900001
J	Casillas-Perez, D; Pizarro, D; Fuentes-Jimenez, D; Mazo, M; Bartoli, A				Casillas-Perez, David; Pizarro, Daniel; Fuentes-Jimenez, David; Mazo, Manuel; Bartoli, Adrien			The Isowarp: The Template-Based Visual Geometry of Isometric Surfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image warp; SfT; Template-based; Non-rigid reconstruction	SHAPE-FROM-TEMPLATE; REGISTRATION; RECOVERY	Registration maps or warps form a key element in Shape-from-Template (SfT). They relate the template with the input image, which contains the projection of the deformed surface. Recently, it was shown that isometric SfT can be solved analytically if the warp and its first-order derivatives are known. In practice, the warp is recovered by interpolating a set of discrete template-to-image point correspondences. This process relies on smoothness priors but ignores the 3D geometry. This may produce errors in the warp and poor reconstructions. In contrast, we propose to create a 3D consistent warp, which technically is a very challenging task, as the 3D shape variables must be eliminated from the isometric SfT equations to find differential constraints for the warp only. Integrating these constraints in warp estimation yields the isowarp, a warp 3D consistent with isometric SfT. Experimental results show that incorporating the isowarp in the SfT pipeline allows the analytic solution to outperform non-convex 3D shape refinement methods and the recent DNN-based SfT methods. The isowarp can be properly initialized with convex methods and its hyperparameters can be automatically obtained with cross-validation. The isowarp is resistant to 3D ambiguities and less computationally expensive than existing 3D shape refinement methods. The isowarp is thus a theoretical and practical breakthrough in SfT.	[Casillas-Perez, David] Univ Rey Juan Carlos, Dept Teoria Senal & Comunicac, Fuenlabrada, Spain; [Casillas-Perez, David; Pizarro, Daniel; Fuentes-Jimenez, David; Mazo, Manuel] Univ Alcala, GEINTRA, Alcala De Henares, Spain; [Pizarro, Daniel; Bartoli, Adrien] Univ Clermont Auvergne, CNRS, Inst Pascal, Clermont Ferrand, France	Universidad Rey Juan Carlos; Universidad de Alcala; Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA)	Casillas-Perez, D (corresponding author), Univ Rey Juan Carlos, Dept Teoria Senal & Comunicac, Fuenlabrada, Spain.; Casillas-Perez, D (corresponding author), Univ Alcala, GEINTRA, Alcala De Henares, Spain.	david.casillas@urjc.es; dani.pizarro@gmail.com; d.fuentes@edu.uah.es; manuel.mazo@uah.es; adrien.bartoli@gmail.com		Casillas-Perez, David/0000-0002-5721-1242	Spanish Ministry of Education and Culture under the scholarship FPU; Spanish Ministry of Economy, Industry and Competitiveness under the project ARTEMISA [TIN2016-80939-R]; EU's FP7 through the ERC [307483]	Spanish Ministry of Education and Culture under the scholarship FPU; Spanish Ministry of Economy, Industry and Competitiveness under the project ARTEMISA; EU's FP7 through the ERC	This research has received funding from the Spanish Ministry of Education and Culture under the scholarship FPU, the Spanish Ministry of Economy, Industry and Competitiveness under the project ARTEMISA (TIN2016-80939-R) and the EU's FP7 through the ERC research grant 307483 FLEXABLE.	Agudo A, 2016, IEEE T PATTERN ANAL, V38, P979, DOI 10.1109/TPAMI.2015.2469293; Agudo A, 2014, PROC CVPR IEEE, P1558, DOI 10.1109/CVPR.2014.202; Bartoli A, 2015, IEEE T PATTERN ANAL, V37, P2099, DOI 10.1109/TPAMI.2015.2392759; Bartoli A, 2010, INT J COMPUT VISION, V88, P85, DOI 10.1007/s11263-009-0303-4; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Blender, 1994, P IEEE C COMP VIS PA; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Brunet F, 2014, COMPUT VIS IMAGE UND, V125, P138, DOI 10.1016/j.cviu.2014.04.003; CASILLASPEREZ D, 2018, J MATH IMAGING VIS; Chhatkuli A, 2017, IEEE T PATTERN ANAL, V39, P833, DOI 10.1109/TPAMI.2016.2562622; Ngo DT, 2016, IEEE T PATTERN ANAL, V38, P172, DOI 10.1109/TPAMI.2015.2435739; do Carmo M. P., 2016, DIFFERENTIAL GEOMETR, V2nd, P136; Dubitzky W., 2007, FUNDAMENTALS DATA MI; Fayad J, 2011, IEEE I CONF COMP VIS, P431, DOI 10.1109/ICCV.2011.6126272; Fuentes-Jimunez D., 2018, CORRABS181107791ARXI; Golyanik V, 2018, LECT NOTES COMPUT SC, V11162, P51, DOI 10.1007/978-3-030-01790-3_4; Haouchine N, 2014, INT SYM MIX AUGMENT, P229, DOI 10.1109/ISMAR.2014.6948432; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Herzet, 2017, P IEEE C COMP VIS PA, P3337; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Malgouyres R, 2009, BRIT MACH VIS C; Malti A, 2015, PROC CVPR IEEE, P1629, DOI 10.1109/CVPR.2015.7298771; Ovsienko V., 2009, NOT AM MATH SOC, V56, P34; Ozgur E, 2017, INT J COMPUT VISION, V123, P184, DOI 10.1007/s11263-016-0968-4; Parashar S, 2020, IEEE T PATTERN ANAL, V42, P3011, DOI 10.1109/TPAMI.2019.2920821; Perriollat M, 2011, INT J COMPUT VISION, V95, P124, DOI 10.1007/s11263-010-0352-8; Pilet J, 2008, INT J COMPUT VISION, V76, P109, DOI 10.1007/s11263-006-0017-9; PIZARRO D, 2013, BMVC; Pizarro D, 2016, INT J COMPUT VISION, V119, P93, DOI 10.1007/s11263-016-0882-9; Pizarro D, 2012, INT J COMPUT VISION, V97, P54, DOI 10.1007/s11263-011-0452-0; Pumarola A, 2018, PROC CVPR IEEE, P4681, DOI 10.1109/CVPR.2018.00492; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Salzmann M, 2011, IEEE T PATTERN ANAL, V33, P931, DOI 10.1109/TPAMI.2010.158; Shimada S., 2019, P IEEE C COMP VIS PA; Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32; Varol A, 2012, PROC CVPR IEEE, P2248, DOI 10.1109/CVPR.2012.6247934; Yu R, 2015, IEEE I CONF COMP VIS, P918, DOI 10.1109/ICCV.2015.111; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22	39	3	3	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2194	2222		10.1007/s11263-021-01472-w	http://dx.doi.org/10.1007/s11263-021-01472-w		MAY 2021	29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW					2022-12-18	WOS:000648223000004
J	Xiao, GB; Wang, HZ; Ma, JY; Suter, D				Xiao, Guobao; Wang, Hanzi; Ma, Jiayi; Suter, David			Segmentation by Continuous Latent Semantic Analysis for Multi-structure Model Fitting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Latent semantic analysis; Preference analysis; Geometric model fitting; Multi-structure data	POINTS	In this paper, we propose a novel continuous latent semantic analysis fitting method, to efficiently and effectively estimate the parameters of model instances in data, based on latent semantic analysis and continuous preference analysis. Specifically, we construct a new latent semantic space (LSS): where inliers of different model instances are mapped into several independent directions, while gross outliers are distributed close to the origin of LSS. After that, we analyze the data distribution to effectively remove gross outliers in LSS, and propose an improved clustering algorithm to segment the remaining data points. On the one hand, the proposed fitting method is able to achieve excellent fitting results; due to the effective continuous preference analysis in LSS. On the other hand, the proposed method can efficiently obtain final fitting results due to the dimensionality reduction in LSS. Experimental results on both synthetic data and real images demonstrate that the proposed method achieves significant superiority over several state-of-the-art model fitting methods on both fitting accuracy and computational speed.	[Xiao, Guobao; Wang, Hanzi] Xiamen Univ, Sch Informat, Fujian Key Lab Sensing & Comp Smart City, Xiamen, Peoples R China; [Xiao, Guobao] Minjiang Univ, Coll Comp & Control Engn, Fujian Prov Key Lab Informat Proc & Intelligent C, Fuzhou, Peoples R China; [Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan, Peoples R China; [Suter, David] Edith Cowan Univ, Sch Sci, Joondalup, Australia	Xiamen University; Minjiang University; Wuhan University; Edith Cowan University	Wang, HZ (corresponding author), Xiamen Univ, Sch Informat, Fujian Key Lab Sensing & Comp Smart City, Xiamen, Peoples R China.	han.wang@xmu.edu.cn	Wang, Han/GPW-9809-2022		National Natural Science Foundation of China [62072223, 61872307]; Natural Science Foundation of Fujian Province [2020J01829]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Fujian Province(Natural Science Foundation of Fujian Province)	This work was supported by the National Natural Science Foundation of China under Grants 62072223, 61872307, and by the Natural Science Foundation of Fujian Province under Grants 2020J01829.	Amayo P, 2018, PROC CVPR IEEE, P8138, DOI 10.1109/CVPR.2018.00849; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Barath D, 2020, PROC CVPR IEEE, P1301, DOI 10.1109/CVPR42600.2020.00138; Barath D, 2019, PROC CVPR IEEE, P10189, DOI 10.1109/CVPR.2019.01044; Chin TJ, 2012, IEEE T PATTERN ANAL, V34, P625, DOI 10.1109/TPAMI.2011.169; Chin TJ, 2009, IEEE I CONF COMP VIS, P413, DOI 10.1109/ICCV.2009.5459150; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Ferraz L, 2007, LECT NOTES COMPUT SC, V4478, P355; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Isack H, 2012, INT J COMPUT VISION, V97, P123, DOI 10.1007/s11263-011-0474-7; Jiang XY, 2020, IEEE T IMAGE PROCESS, V29, P736, DOI 10.1109/TIP.2019.2934572; Kanazawa Y., 2004, P BRIT MACH VIS C, P2701; Le H., 2019, IEEE T PATTERN ANAL, V1, P1; Lee JK, 2019, INT J COMPUT VISION, V127, P1426, DOI 10.1007/s11263-019-01196-y; Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2; Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI 10.1007/s11263-018-1117-z; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Magri L, 2019, PROC CVPR IEEE, P7452, DOI 10.1109/CVPR.2019.00764; Magri L, 2017, IMAGE VISION COMPUT, V67, P1, DOI 10.1016/j.imavis.2017.09.005; Magri L, 2016, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2016.361; Magri L, 2014, PROC CVPR IEEE, P3954, DOI 10.1109/CVPR.2014.505; Meer P, 2020, COMPUTER VISION REFE, P1; Mittal S, 2012, IEEE T PATTERN ANAL, V34, P2351, DOI 10.1109/TPAMI.2012.52; Pham TT, 2014, IEEE T PATTERN ANAL, V36, P1658, DOI 10.1109/TPAMI.2013.2296310; Purkait P, 2017, IEEE T PATTERN ANAL, V39, P1697, DOI 10.1109/TPAMI.2016.2614980; Tran QH, 2014, INT J COMPUT VISION, V106, P93, DOI 10.1007/s11263-013-0643-y; Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072; Tanimoto T., 1958, ELEMENTARY MATH THEO; Tennakoon RB, 2016, IEEE T PATTERN ANAL, V38, P350, DOI 10.1109/TPAMI.2015.2448103; Toldo R, 2008, LECT NOTES COMPUT SC, V5302, P537, DOI 10.1007/978-3-540-88682-2_41; Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang HZ, 2019, IEEE T PATTERN ANAL, V41, P697, DOI 10.1109/TPAMI.2018.2803173; Wang HZ, 2012, IEEE T PATTERN ANAL, V34, P1177, DOI 10.1109/TPAMI.2011.216; Wong HS, 2011, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2011.6126350; Wu YH, 2008, INT J COMPUT VISION, V79, P209, DOI 10.1007/s11263-007-0114-4; Xiao GB, 2019, INT J COMPUT VISION, V127, P323, DOI 10.1007/s11263-018-1100-8; Xiao GB, 2016, PATTERN RECOGN, V60, P748, DOI 10.1016/j.patcog.2016.06.026; XU L, 1990, PATTERN RECOGN LETT, V11, P331, DOI 10.1016/0167-8655(90)90042-Z; Xu X, 2019, IEEE T PATTERN ANAL, V1, P1, DOI [10.1109/TPAMI.2019.2957780, DOI 10.1109/TPAMI.2019.2957780]; Yang X, 2020, IEEE T PATTERN ANAL, V1, P1	43	3	3	2	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2034	2056		10.1007/s11263-021-01468-6	http://dx.doi.org/10.1007/s11263-021-01468-6		APR 2021	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW					2022-12-18	WOS:000643210300001
J	Liu, YL; He, T; Chen, H; Wang, XY; Luo, CJ; Zhang, ST; Shen, CH; Jin, LW				Liu, Yuliang; He, Tong; Chen, Hao; Wang, Xinyu; Luo, Canjie; Zhang, Shuaitao; Shen, Chunhua; Jin, Lianwen			Exploring the Capacity of an Orderless Box Discretization Network for Multi-orientation Scene Text Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene text; Text detection; Orderless box discretization	FASTER	Multi-orientation scene text detection has recently gained significant research attention. Previous methods directly predict words or text lines, typically by using quadrilateral shapes. However, many of these methods neglect the significance of consistent labeling, which is important for maintaining a stable training process, especially when it comprises a large amount of data. Here we solve this problem by proposing a new method, Orderless Box Discretization (OBD), which first discretizes the quadrilateral box into several key edges containing all potential horizontal and vertical positions. To decode accurate vertex positions, a simple yet effective matching procedure is proposed for reconstructing the quadrilateral bounding boxes. Our method solves the ambiguity issue, which has a significant impact on the learning process. Extensive ablation studies are conducted to validate the effectiveness of our proposed method quantitatively. More importantly, based on OBD, we provide a detailed analysis of the impact of a collection of refinements, which may inspire others to build state-of-the-art text detectors. Combining both OBD and these useful refinements, we achieve state-of-the-art performance on various benchmarks, including ICDAR 2015 and MLT. Our method also won the first place in the text detection task at the recent ICDAR2019 Robust Reading Challenge for Reading Chinese Text on Signboards, further demonstrating its superior performance. The code is available at .	[Liu, Yuliang; Luo, Canjie; Zhang, Shuaitao; Jin, Lianwen] South China Univ Technol, Guangzhou, Peoples R China; [Liu, Yuliang; He, Tong; Chen, Hao; Wang, Xinyu; Shen, Chunhua] Univ Adelaide, Adelaide, SA, Australia; [Shen, Chunhua] Monash Univ, Melbourne, Vic, Australia; [Jin, Lianwen] Guangzhou Artificial Intelligence & Digital Econ, Guangzhou, Peoples R China	South China University of Technology; University of Adelaide; Monash University	Jin, LW (corresponding author), South China Univ Technol, Guangzhou, Peoples R China.; Shen, CH (corresponding author), Univ Adelaide, Adelaide, SA, Australia.; Shen, CH (corresponding author), Monash Univ, Melbourne, Vic, Australia.; Jin, LW (corresponding author), Guangzhou Artificial Intelligence & Digital Econ, Guangzhou, Peoples R China.	chunhua.shen@adelaide.edu.au; eelwjin@scut.edu.cn	Wang, Xinyu/AAW-2153-2021		NSFC [61936003]; GD-NSF [2017A030312006]	NSFC(National Natural Science Foundation of China (NSFC)); GD-NSF	This research is supported in part by NSFC (Grant No.: 61936003) and GD-NSF (No. 2017A030312006).	Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959; Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157; Chee Kheng Chng, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1571, DOI 10.1109/ICDAR.2019.00252; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chu, 2018, P JOINT INT C ART IN; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Deng D, 2018, AAAI CONF ARTIF INTE, P6773; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331; He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527; He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588; He WH, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107026; He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87; He ZQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P887, DOI 10.1145/3240508.3240555; Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Huang ZD, 2019, IEEE WINT CONF APPL, P764, DOI 10.1109/WACV.2019.00086; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942; Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221; Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656; Li H, 2019, AAAI CONF ARTIF INTE, P8610; Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615; Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086; Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Liao MH, 2017, AAAI CONF ARTIF INTE, P4161; Liu, 2019, INT J DOC ANAL RECOG, P1; Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595; Liu Y., 2019, IEEE T IMAGE PROCESS; Liu YL, 2019, PROC CVPR IEEE, P9604, DOI 10.1109/CVPR.2019.00984; Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002; Liu YL, 2017, PROC CVPR IEEE, P3454, DOI 10.1109/CVPR.2017.368; Liu ZK, 2017, IEEE IMAGE PROC, P900; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2; Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788; Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5; Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020; Nayef Nibal, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1582, DOI 10.1109/ICDAR.2019.00254; NAYEF N, 2017, PROC INT CONF DOC, V1, P1454, DOI DOI 10.1109/ICDAR.2017.237; Neumann L, 2016, IEEE T PATTERN ANAL, V38, P1872, DOI 10.1109/TPAMI.2015.2496234; Neumann L, 2015, PROC INT CONF DOC, P746, DOI 10.1109/ICDAR.2015.7333861; Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097; Ng, 2019, P INT C DOC AN REC; Qin SY, 2019, IEEE I CONF COMP VIS, P4703, DOI 10.1109/ICCV.2019.00480; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shi BG, 2017, PROC INT CONF DOC, P1429, DOI 10.1109/ICDAR.2017.233; Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020; Tian SX, 2015, IEEE I CONF COMP VIS, P4651, DOI 10.1109/ICCV.2015.528; Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4; Veit A, 2016, ADV NEUR IN, V29; Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308; Wang Peng, 2019, ARXIV PREPRINT ARXIV; Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853; Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956; Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661; Wei F., 2019, P IEEE INT C COMP VI; Wu Y, 2017, IEEE I CONF COMP VIS, P5010, DOI 10.1109/ICCV.2017.535; Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737; Xie LL, 2019, AAAI CONF ARTIF INTE, P9046; Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589; Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989; Xue CH, 2018, LECT NOTES COMPUT SC, V11220, P370, DOI 10.1007/978-3-030-01270-0_22; Yang XH, 2019, J CRYST GROWTH, V528, DOI 10.1016/j.jcrysgro.2019.125286; Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787; Yin XC, 2015, IEEE T PATTERN ANAL, V37, P1930, DOI 10.1109/TPAMI.2014.2388210; Yuanzhi Z., 2020, P AAAI C ART INT; Zhang C, 2019, PROC CVPR IEEE, P5312, DOI 10.1109/CVPR.2019.00546; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871; Zhong Z., 2016, ARXIV PREPRINT ARXIV, DOI 10.1109/ICASSP.2017.7952348; Zhong ZY, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106986; Zhong ZY, 2019, INT J DOC ANAL RECOG, V22, P315, DOI 10.1007/s10032-019-00335-y; Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283; Zhu YX, 2020, IEEE T GEOSCI REMOTE, V58, P7247, DOI 10.1109/TGRS.2020.2981203; Zhu YX, 2018, INT C PATT RECOG, P3735, DOI 10.1109/ICPR.2018.8545067	84	3	3	2	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1972	1992		10.1007/s11263-021-01459-7	http://dx.doi.org/10.1007/s11263-021-01459-7		APR 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU		Green Submitted			2022-12-18	WOS:000641238700005
J	Zhang, SS; Chen, D; Yang, J; Schiele, B				Zhang, Shanshan; Chen, Di; Yang, Jian; Schiele, Bernt			Guided Attention in CNNs for Occluded Pedestrian Detection and Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pedestrian detection; Pedestrian re-identification; CNNs; Attention	PERSON REIDENTIFICATION; PERFORMANCE	Pedestrian detection and re-identification have progressed significantly in the last few years. However, occluded people are notoriously hard to detect and recognize, as their appearance varies substantially depending on a wide range of occlusion patterns. In this paper, we aim to propose a simple and compact method based on CNNs for occlusion handling. We start with interpreting CNN channel features of a pedestrian detector, and we find that different channels activate responses for different body parts respectively. These findings motivate us to employ an attention mechanism across channels to represent various occlusion patterns in one single model, as each occlusion pattern can be formulated as some specific combination of body parts. Therefore, an attention network with self or external guidances is proposed as an add-on to the baseline CNN method. Also, we propose an attention guided self-paced learning method to balance the optimization across different occlusion levels. Our proposed method shows significant improvements over the baseline methods for both pedestrian detection and re-identification tasks. For pedestrian detection, we achieve a considerable improvement of 8pp to the baseline FasterRCNN detector on the heavy occlusion subset of CityPersons and on Caltech we outperform the state-of-the-art method by 5pp. For pedestrian re-identification, our method surpasses the baseline and achieves state-of-the-art performance on multiple re-identification benchmarks.	[Zhang, Shanshan; Chen, Di; Yang, Jian] Nanjing Univ Sci & Technol, Nanjing, Peoples R China; [Schiele, Bernt] Max Planck Inst Informat, Saarbrucken, Germany	Nanjing University of Science & Technology; Max Planck Society	Yang, J (corresponding author), Nanjing Univ Sci & Technol, Nanjing, Peoples R China.	shanshan.zhang@njust.edu.cn; dichen@njust.edu.cn; csjyang@njust.edu.cn; schiele@mpi-inf.mpg.de	C, Deprecated/AHC-8964-2022		Funds for International Cooperation and Exchange of the National Natural Science Foundation of China [61861136011]; Natural Science Foundation of Jiangsu Province, China [BK20181299]; National Science Fund of China [61702262]; Fundamental Research Funds for the Central Universities [30920032201]; National Key Research and Development Program of China [2017YFC0820601]	Funds for International Cooperation and Exchange of the National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Jiangsu Province, China(Natural Science Foundation of Jiangsu Province); National Science Fund of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Key Research and Development Program of China	This work was supported by the Funds for International Cooperation and Exchange of the National Natural Science Foundation of China (Grant No. 61861136011), the Natural Science Foundation of Jiangsu Province, China (Grant No. BK20181299), the National Science Fund of China (Grant No. 61702262), the Fundamental Research Funds for the Central Universities (Grant No. 30920032201), the National Key Research and Development Program of China under Grant 2017YFC0820601.	Ahmed E., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299016; [Anonymous], 2019, IEEE C COMP VIS PATT; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Benenson R, 2015, LECT NOTES COMPUT SC, V8926, P613, DOI 10.1007/978-3-319-16181-5_47; Brazil G, 2019, PROC CVPR IEEE, P7224, DOI 10.1109/CVPR.2019.00740; Brazil G, 2017, IEEE I CONF COMP VIS, P4960, DOI 10.1109/ICCV.2017.530; Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Du X, 2016, FUSED DNN DEEP NEURA; Enzweiler M, 2010, PROC CVPR IEEE, P990, DOI 10.1109/CVPR.2010.5540111; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Feng X, 2017, IEEE INFOCOM SER, DOI 10.1109/JSTARS.2017.2686488; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gonzalez-Garcia A, 2018, INT J COMPUT VISION, V126, P476, DOI 10.1007/s11263-017-1048-0; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739; Hosang J, 2015, PROC CVPR IEEE, P4073, DOI 10.1109/CVPR.2015.7299034; Hu J, 2018, 2018 IEEE CVF C COMP, P7132, DOI DOI 10.1109/CVPR.2018.00745; Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Jaderberg M, 2015, ADV NEUR IN, V28; Jialian Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13427, DOI 10.1109/CVPR42600.2020.01344; Jin Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P88, DOI 10.1007/978-3-030-58520-4_6; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li, 2016, SCALE AWARE FAST R C; Li Guohao, 2020, ACM MM; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Lin CZ, 2020, IEEE T IMAGE PROCESS, V29, P3820, DOI 10.1109/TIP.2020.2966371; Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762; Liu W, 2018, LECT NOTES COMPUT SC, V11218, P643, DOI 10.1007/978-3-030-01264-9_38; Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533; Mathias M, 2013, IEEE I CONF COMP VIS, P1505, DOI 10.1109/ICCV.2013.190; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Noh J, 2018, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2018.00107; Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257; Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062; Paisitkriangkrai S, 2014, LECT NOTES COMPUT SC, V8692, P546, DOI 10.1007/978-3-319-10593-2_36; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051; Shao S, 2018, ARXIV180500123; Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562; Simon M, 2015, LECT NOTES COMPUT SC, V9004, P162, DOI 10.1007/978-3-319-16808-1_12; Song T, 2018, LECT NOTES COMPUT SC, V11211, P554, DOI 10.1007/978-3-030-01234-2_33; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tian YL, 2015, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2015.221; Tian YL, 2015, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2015.7299143; Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9; Wang C, 2018, IEEE CONF COMM NETW; Wang S, 2017, BMVC; Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226; Xuangeng Chu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12211, DOI 10.1109/CVPR42600.2020.01223; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28; Zhang SS, 2018, IEEE T PATTERN ANAL, V40, P973, DOI 10.1109/TPAMI.2017.2700460; Zhang SS, 2016, PROC CVPR IEEE, P1259, DOI 10.1109/CVPR.2016.141; Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39; Zheng L., 2016, ARXIV; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531; Zheng Wei-Shi, 2009, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.23.23; Zhong Zhun, 2017, PROC CVPR IEEE, P1318, DOI DOI 10.1109/CVPR.2017.389; Zhou CL, 2019, IEEE I CONF COMP VIS, P9556, DOI 10.1109/ICCV.2019.00965; Zhou CL, 2017, IEEE I CONF COMP VIS, P3506, DOI 10.1109/ICCV.2017.377	86	3	4	9	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1875	1892		10.1007/s11263-021-01461-z	http://dx.doi.org/10.1007/s11263-021-01461-z		APR 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU					2022-12-18	WOS:000638126400001
J	Zhu, MR; Li, J; Wang, NN; Gao, XB				Zhu, Mingrui; Li, Jie; Wang, Nannan; Gao, Xinbo			Learning Deep Patch representation for Probabilistic Graphical Model-Based Face Sketch Synthesis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face sketch synthesis; Probabilistic graphical model; Deep patch representation; Generative model	PHOTO SYNTHESIS	Face sketch synthesis has a wide range of applications in both digital entertainment and law enforcement. State-of-the-art examplar-based methods typically exploit a Probabilistic Graphical Model (PGM) to represent the joint probability distribution over all of the patches selected from a set of training data. However, these methods suffer from two main shortcomings: (1) most of these methods capture the evidence between patches in pixel-level, which lead to inaccurate parameter estimation under bad environment conditions such as light variations and clutter backgrounds; (2) the assumption that a photo patch and its corresponding sketch patch share similar geometric manifold structure is not rigorous. It has shown that deep convolutional neural network (CNN) has outstanding performance in learning to extract high-level feature representation. Therefore, we extract uniform deep patch representations of test photo patches and training sketch patches from a specially designed CNN model to replace pixel intensity, and directly match between them, which can help select better candidate patches from training data as well as improve parameter learning process. In this way, we investigate a novel face sketch synthesis method called DPGM that combines generative PGM and discriminative deep patch representation, which can jointly model the distribution over the parameters for deep patch representation and the distribution over the parameters for sketch patch reconstruction. Then, we apply an alternating iterative optimization strategy to simultaneously optimize two kinds of parameters. Therefore, both the representation capability of deep patch representation and the reconstruction ability of sketch patches can be boosted. Eventually, high quality reconstructed sketches which is robust against light variations and clutter backgrounds can be obtained. Extensive experiments on several benchmark datasets demonstrate that our method can achieve superior performance than other state-of-the-art methods, especially under the case of bad light conditions or clutter backgrounds.	[Zhu, Mingrui; Wang, Nannan] Xidian Univ, State Key Lab Integrated Serv Networks, Sch Telecommun Engn, Xian 710071, Peoples R China; [Li, Jie; Gao, Xinbo] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China; [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China	Xidian University; Xidian University; Chongqing University of Posts & Telecommunications	Wang, NN (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Sch Telecommun Engn, Xian 710071, Peoples R China.; Gao, XB (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.; Gao, XB (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.	mrzhu@xidian.edu.cn; leejie@mail.xidian.edu.cn; nnwang@xidian.edu.cn; xbgao@mail.xidian.edu.cn	Li, jie/GXG-4583-2022		National Key Research and Development Program of China [2018AAA0103202]; National Natural Science Foundation of China [62036007, 61922066, 61876142, 61772402, 62050175]; Fundamental Research Funds for the Central Universities	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported in part by the National Key Research and Development Program of China under Grant 2018AAA0103202; in part by the National Natural Science Foundation of China under Grant Grants 62036007, 61922066, 61876142, 61772402, and 62050175; in part by the Xidian University Intellifusion Joint Innovation Laboratory of Articial Intelligence; in part by the Fundamental Research Funds for the Central Universities.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Beis JS, 1997, PROC CVPR IEEE, P1000, DOI 10.1109/CVPR.1997.609451; Chang L, 2011, LECT NOTES COMPUT SC, V6761, P555, DOI 10.1007/978-3-642-21602-2_60; Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9; Gao XB, 2012, IEEE T CIRC SYST VID, V22, P1213, DOI 10.1109/TCSVT.2012.2198090; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Isola P., 2016, ARXIV PREPRINT; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jia J., 2012, INT S NONPH AN REND; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272; Liu QS, 2005, PROC CVPR IEEE, P1005; Mao XY, 2001, CAD/GRAPHICS '2001: PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON COMPUTER AIDED DESIGN AND COMPUTER GRAPHICS, VOLS 1 AND 2, P240; Martinez A., 1998, AR FACE DATABASE; MEER P, 1991, INT J COMPUT VISION, V6, P59, DOI 10.1007/BF00127126; Messer K., 1999, AUDIO VIDEO BASED BI, P72; Paszke A., 2017, WEB; Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790; Song YB, 2014, LECT NOTES COMPUT SC, V8694, P800, DOI 10.1007/978-3-319-10599-4_51; Tang X, 2002, IEEE IMAGE PROC, P257; Tang XO, 2004, IEEE T CIRC SYST VID, V14, P50, DOI 10.1109/TCSVT.2003.818353; Tang XO, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P687, DOI 10.1109/ICCV.2003.1238414; Wang N., 2017, ARXIV PREPRINT ARXIV; Wang N., 2017, IEEE T IMAGE PROCESS, P1; Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9; Wang NN, 2013, PATTERN RECOGN LETT, V34, P77, DOI 10.1016/j.patrec.2012.04.005; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8; Zhang JW, 2011, IEEE IMAGE PROC, P1125, DOI 10.1109/ICIP.2011.6115625; Zhang LL, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P627, DOI 10.1145/2671188.2749321; Zhang W, 2011, PROC CVPR IEEE, P513, DOI 10.1109/CVPR.2011.5995324; Zhou H, 2012, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2012.6247788; Zhu MR, 2019, IEEE T NEUR NET LEAR, V30, P3096, DOI 10.1109/TNNLS.2018.2890018; Zhu MR, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3574; Zhu MR, 2016, 8TH INTERNATIONAL CONFERENCE ON INTERNET MULTIMEDIA COMPUTING AND SERVICE (ICIMCS2016), P168, DOI 10.1145/3007669.3007679	37	3	5	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1820	1836		10.1007/s11263-021-01442-2	http://dx.doi.org/10.1007/s11263-021-01442-2		MAR 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU					2022-12-18	WOS:000631740300001
J	Zhu, XP; Zhu, XT; Li, MX; Morerio, P; Murino, V; Gong, SG				Zhu, Xiangping; Zhu, Xiatian; Li, Minxian; Morerio, Pietro; Murino, Vittorio; Gong, Shaogang			Intra-Camera Supervised Person Re-Identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Intra-camera labelling; Cross-camera labelling; Multi-task learning; Multi-label learning		Existing person re-identification (re-id) methods mostly exploit a large set of cross-camera identity labelled training data. This requires a tedious data collection and annotation process, leading to poor scalability in practical re-id applications. On the other hand unsupervised re-id methods do not need identity label information, but they usually suffer from much inferior and insufficient model performance. To overcome these fundamental limitations, we propose a novel person re-identification paradigm based on an idea of independent per-camera identity annotation. This eliminates the most time-consuming and tedious inter-camera identity labelling process, significantly reducing the amount of human annotation efforts. Consequently, it gives rise to a more scalable and more feasible setting, which we call Intra-Camera Supervised (ICS) person re-id, for which we formulate a Multi-tAsk mulTi-labEl (MATE) deep learning method. Specifically, MATE is designed for self-discovering the cross-camera identity correspondence in a per-camera multi-task inference framework. Extensive experiments demonstrate the cost-effectiveness superiority of our method over the alternative approaches on three large person re-id datasets. For example, MATE yields 88.7% rank-1 score on Market-1501 in the proposed ICS person re-id setting, significantly outperforming unsupervised learning models and closely approaching conventional fully supervised learning competitors.	[Zhu, Xiangping] Shenzhen Univ, Sch Comp Sci & Software Engn, Comp Vis Inst, Shenzhen, Peoples R China; [Zhu, Xiangping; Morerio, Pietro; Murino, Vittorio] Ist Italiano Tecnol, Pattern Anal & Comp Vis PAVIS, Genoa, Italy; [Zhu, Xiatian] Vis Semant Ltd, London, England; [Li, Minxian; Gong, Shaogang] Queen Mary Univ London, London, England; [Murino, Vittorio] Huawei Technol Co Ltd, Ireland Res Ctr, Dublin, Ireland; [Murino, Vittorio] Univ Verona, Dept Comp Sci, Verona, Italy	Shenzhen University; Istituto Italiano di Tecnologia - IIT; University of London; Queen Mary University London; Huawei Technologies; University of Verona	Zhu, XT (corresponding author), Vis Semant Ltd, London, England.	xiangping.zhu2010@gmail.com; eddy.zhuxt@gmail.com; m.li@qmul.ac.uk; pietro.morerio@iit.it; vittorio.murino@univr.it; s.gong@qmul.ac.uk		Zhu, Xiatian/0000-0002-9284-2955	Vision Semantics Limited; Alan Turing Institute Fellowship Project on Deep Learning for Large-Scale Video Semantic Search; Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety [98111-571149]	Vision Semantics Limited; Alan Turing Institute Fellowship Project on Deep Learning for Large-Scale Video Semantic Search; Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety(UK Research & Innovation (UKRI)Innovate UK)	This work was partially supported by Vision Semantics Limited, the Alan Turing Institute Fellowship Project on Deep Learning for Large-Scale Video Semantic Search, and the Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety (98111-571149).		0	3	3	3	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1580	1595		10.1007/s11263-021-01440-4	http://dx.doi.org/10.1007/s11263-021-01440-4		FEB 2021	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted, hybrid			2022-12-18	WOS:000622275300004
J	Constantin, MG; Stefan, LD; Ionescu, B; Duong, NQK; Demarty, CH; Sjoberg, M				Constantin, Mihai Gabriel; Stefan, Liviu-Daniel; Ionescu, Bogdan; Duong, Ngoc Q. K.; Demarty, Claire-Helene; Sjoberg, Mats			Visual Interestingness Prediction: A Benchmark Framework and Literature Review	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Review						Visual interestingness prediction; Interestingness10k data set; Benchmarking; Literature survey; Late fusion techniques		In this paper, we report on the creation of a publicly available, common evaluation framework for image and video visual interestingness prediction. We propose a robust data set, the Interestingness10k, with 9831 images and more than 4 h of video, interestigness scores determined based on more than 1M pair-wise annotations of 800 trusted annotators, some pre-computed multi-modal descriptors, and 192 system output results as baselines. The data were validated extensively during the 2016-2017 MediaEval benchmark campaigns. We provide an in-depth analysis of the crucial components of visual interestingness prediction algorithms by reviewing the capabilities and the evolution of the MediaEval benchmark systems, as well as of prominent systems from the literature. We discuss overall trends, influence of the employed features and techniques, generalization capabilities and the reliability of results. We also discuss the possibility of going beyond state-of-the-art performance via an automatic, ad-hoc system fusion, and propose a deep MLP-based architecture that outperforms the current state-of-the-art systems by a large margin. Finally, we provide the most important lessons learned and insights gained.	[Constantin, Mihai Gabriel; Stefan, Liviu-Daniel; Ionescu, Bogdan] Univ Politehn Bucuresti, Bucharest, Romania; [Duong, Ngoc Q. K.; Demarty, Claire-Helene] InterDigital, Paris, France; [Sjoberg, Mats] CSC IT Ctr Sci Ltd, Espoo, Finland	Polytechnic University of Bucharest; Finnish IT center for science	Constantin, MG (corresponding author), Univ Politehn Bucuresti, Bucharest, Romania.	mgconstantin@imag.pub.ro; lstefan@imag.pub.ro; bogdan.ionescu@upb.ro; Ngoc.Duong@interdigital.com; Helene.Demarty@interdigital.com; mats.sjoberg@csc.fi	Ștefan, Liviu-Daniel/AAS-9561-2021; Constantin, Mihai Gabriel/Q-6295-2019	Constantin, Mihai Gabriel/0000-0002-2312-6672; Stefan, Liviu-Daniel/0000-0001-9174-3923	Ministry of Innovation and Research, UEFISCDI, project SPIA-VA [2SOL/2017]; project AI4Media, A European Excellence Centre for Media, Society and Democracy [H2020 ICT-48-2020, 951911]; Operational Programme Human Capital of the Ministry of Europe Funds [51675, 125125]	Ministry of Innovation and Research, UEFISCDI, project SPIA-VA(Consiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii (UEFISCDI)); project AI4Media, A European Excellence Centre for Media, Society and Democracy; Operational Programme Human Capital of the Ministry of Europe Funds	We would like to acknowledge first, Technicolor France for founding and supporting the Interestingness10k data set and the Predicting Media Interestingness task. We acknowledge the work of our fellow task co-organizers (in alphabetical order): Alexey Ozerov, Frederic Lefebvre, Hanli Wang, Michael Gygli, Toan Do, Vincent Demoulin, and Yu-Gang Jiang. We would like to acknowledge also the MediaEval Benchmarking Initiative for Multimedia Evaluation and in particular Martha Larson, for hosting the Predicting Media Interestingness Task, constant support and enlightening discussions. The work of Mihai Gabriel Constantin and Bogdan Ionescu was supported by the Ministry of Innovation and Research, UEFISCDI, project SPIA-VA, via agreement 2SOL/2017, and from project AI4Media, A European Excellence Centre for Media, Society and Democracy, H2020 ICT-48-2020, grant #951911. The work of Liviu-Daniel Stefan was supported by the Operational Programme Human Capital of the Ministry of Europe Funds through the Financial Agreement 51675/ 09.07.2019, SMIS code 125125.	Abdi H., 2007, ENCY MEASUREMENT STA, P508; Almeida, 2016, MEDIAEVAL WORKSH HIL, V1739; Almeida J, 2011, IEEE IMAGE PROC; Almeida J, 2017, LECT NOTES COMPUT SC, V10484, P3, DOI 10.1007/978-3-319-68560-1_1; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Awad G, 2014, ACM T INFORM SYST, V32, DOI 10.1145/2629531; Aytar Y, 2016, P 30 INT C NEUR INF, P892; Bakhshi S, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P575, DOI 10.1145/2858036.2858532; Berlyne DE, 1949, B J PSYCHOL-GEN SECT, V39, P184, DOI 10.1111/j.2044-8295.1949.tb00219.x; BERLYNE DE, 1970, PERCEPT PSYCHOPHYS, V8, P279, DOI 10.3758/BF03212593; Berlyne DE., 1960, CONFLICT AROUSAL CUR, DOI [10.1037/11164-000, DOI 10.1037/11164-000]; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Buckley Chris, 2017, ACM SIGIR Forum, V51, P235, DOI 10.1145/3130348.3130373; Carballal A, 2020, NEURAL COMPUT APPL, V32, P5889, DOI 10.1007/s00521-019-04065-4; Chamaret C, 2016, P SPIE; Chang, 2013, P 21 ACM INT C MULT, P459; Constantin M.G., 2017, 2017 INT S SIGN CIRC, P1; Constantin MG, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3301299; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Danelljan M., 2014, BRIT MACH VIS C NOTT; DATTA R, 2006, ECCV, P288, DOI DOI 10.1007/11744078_23; Demarty, 2017, MEDIAEVAL WORKSH DUB, V1984; Demarty CH, 2017, MULTIMED SYST APPL, P233, DOI 10.1007/978-3-319-57687-9_10; Deselaers T, 2008, PATTERN RECOGN LETT, V29, P1988, DOI 10.1016/j.patrec.2008.03.001; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Duong, 2016, MEDIAEVAL WORKSH HIL, V1739; Duong, 2017, MEDIAEVAL WORKSH DUB, V1984; Erdogan G., 2016, MEDIAEVAL WORKSH HIL, V1739; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Eyben F., 2010, PROC ACM INT C MULTI, P1459, DOI [10.1145/1873951.1874246, DOI 10.1145/1873951.1874246]; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Ghadiyaram D, 2019, PROC CVPR IEEE, P12038, DOI 10.1109/CVPR.2019.01232; Goyal, 2017, ICCV, V1; Grabner Helmut., 2013, P 21 ACM MULT, P1017; Gygli M, 2016, PROC CVPR IEEE, P1001, DOI 10.1109/CVPR.2016.114; Gygli M, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P122, DOI 10.1145/2964284.2967195; Gygli M, 2013, IEEE I CONF COMP VIS, P1633, DOI 10.1109/ICCV.2013.205; Han S, 2016, ADV NEURAL INF PROCE, V29, DOI 10.5555/3157096.3157109; Hayes AF, 2007, COMMUN METHODS MEAS, V1, P77, DOI 10.1080/19312450709336664; Hidi S., 1992, ROLE INTEREST LEARNI, V11, P213, DOI [10.4324/9781315807430-20, DOI 10.4324/9781315807430-20]; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hua X.-S., 2013, P 21 ACM INT C MULT, P243, DOI DOI 10.1145/2502081.2502283; Huet B., 2017, MEDIAEVAL WORKSH DUB, V1984; Ionescu, 2017, MEDIAEVAL WORKSH DUB, V19; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang Y. G., 2013, AAAI C ART INT; Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kalpathy-Cramer J, 2015, COMPUT MED IMAG GRAP, V39, P55, DOI 10.1016/j.compmedimag.2014.03.004; Ke Yan, 2006, 2006 IEEE COMPUTER S, V1, P419, DOI DOI 10.1109/CVPR.2006.303; Khosla A, 2015, IEEE I CONF COMP VIS, P2390, DOI 10.1109/ICCV.2015.275; Kingma D.P., 2015, INT C LEARN REPR, P1; Kittler J., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P897, DOI 10.1109/ICPR.1996.547205; Ko, 2017, MEDIAEVAL WORKSH DUB, V1984; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lam V., 2016, MEDIAEVAL WORKSH HIL, V1739; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lefebvre, 2016, MEDIAEVAL WORKSH HIL, V1739; Li CC, 2009, IEEE J-STSP, V3, P236, DOI 10.1109/JSTSP.2009.2015077; Li J, 2013, PROC SPIE, V8648, DOI 10.1117/12.2002075; Li XR, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P586, DOI 10.1145/2964284.2967289; Liang-Chi Hsieh, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4309, DOI 10.1109/ICASSP.2014.6854415; Liem, 2016, MEDIAEVA WORKSH HILV, V1739; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu F, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P2058; LIU Y, 2018, ICMR 18 P 2018 ACM, P420, DOI DOI 10.1145/3206025.3206071; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MANN HB, 1947, ANN MATH STAT, V18, P50, DOI 10.1214/aoms/1177730491; McCrae RR, 2007, MOTIV EMOTION, V31, P5, DOI 10.1007/s11031-007-9053-1; Mo SS, 2018, NEUROCOMPUTING, V291, P11, DOI 10.1016/j.neucom.2018.02.052; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Opitz M, 2017, IEEE I CONF COMP VIS, P5199, DOI 10.1109/ICCV.2017.555; OVADIA S, 2004, INT J SOC RES METHOD, V0007; Parekh J, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P428, DOI 10.1145/3206025.3206078; Permadi R.A., 2017, MEDIAEVAL WORKSH DUB, V1984; Poignant J, 2017, MULTIMED TOOLS APPL, V76, P22547, DOI 10.1007/s11042-017-4730-x; Randolph Justus J, 2005, JOENS LEARN INSTR S, V2005; Richard S. Zemel, 2014, Arxiv, DOI arXiv:1411.2539; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salesses P, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068400; Sanderson M., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P162, DOI 10.1145/1076034.1076064; Savii, 2017, MEDIAEVAL WORKSH DUB, V1984; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shen YS, 2017, IEEE INT CON MULTI, P1003, DOI 10.1109/ICME.2017.8019300; Silvia PJ, 2009, PSYCHOL AESTHET CREA, V3, P48, DOI 10.1037/a0014632; Silvia PJ, 2005, EMOTION, V5, P89, DOI 10.1037/1528-3542.5.1.89; Sivaraman K, 2016, MOVIESCOPE MOVIE TRA; Smeaton AF, 2010, COMPUT VIS IMAGE UND, V114, P411, DOI 10.1016/j.cviu.2009.03.011; Soleymani, 2016, MEDIAEVAL WORKSH HIL, V1739; Soleymani M, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P919, DOI 10.1145/2733373.2806364; Son J, 2015, IEEE I CONF COMP VIS, P3056, DOI 10.1109/ICCV.2015.350; Springenberg J.T., 2014, ARXIV14126806; Squalli-Houssaini H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2371, DOI 10.1109/icassp.2018.8462292; Sudhakaran S., 2020, P IEEE C COMP VIS PA; Touvron H, 2019, ADV NEURAL INFORM PR, P8250; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Urbano J, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P393; VanGool, 2016, MEDIAEVAL WORKSH HIL, V1739; Vigna S, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1166, DOI 10.1145/2736277.2741088; Wang S, 2018, PROCEEDINGS OF THE JOINT WORKSHOP OF THE 4TH WORKSHOP ON AFFECTIVE SOCIAL MULTIMEDIA COMPUTING AND FIRST MULTI-MODAL AFFECTIVE COMPUTING OF LARGE-SCALE MULTIMEDIA DATA (ASMMC-MMAC'18), P55, DOI 10.1145/3267935.3267952; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu, 2016, MEDIAEVAL WORKSH HIL, V1739; Yalniz I Zeki, 2019, ARXIV190500546, P2; Yang YH, 2011, IEEE T AUDIO SPEECH, V19, P762, DOI 10.1109/TASL.2010.2064164; Yannakakis GN, 2011, LECT NOTES COMPUT SC, V6974, P437, DOI 10.1007/978-3-642-24600-5_47; Zobel J., 1998, P 21 ANN INT ACM SIG	111	3	3	5	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1526	1550		10.1007/s11263-021-01443-1	http://dx.doi.org/10.1007/s11263-021-01443-1		FEB 2021	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000620409500001
J	Husain, SS; Ong, EJ; Bober, M				Husain, Syed Sameed; Ong, Eng-Jon; Bober, Miroslaw			ACTNET: End-to-End Learning of Feature Activations and Multi-stream Aggregation for Effective Instance Image Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image retrieval; Global image descriptor; Activation functions; Convolutional Neural Network; Compact image signature	ARCHITECTURE	We propose a novel CNN architecture called ACTNET for robust instance image retrieval from large-scale datasets. Our key innovation is a learnable activation layer designed to improve the signal-to-noise ratio of deep convolutional feature maps. Further, we introduce a controlled multi-stream aggregation, where complementary deep features from different convolutional layers are optimally transformed and balanced using our novel activation layers, before aggregation into a global descriptor. Importantly, the learnable parameters of our activation blocks are explicitly trained, together with the CNN parameters, in an end-to-end manner minimising triplet loss. This means that our network jointly learns the CNN filters and their optimal activation and aggregation for retrieval tasks. To our knowledge, this is the first time parametric functions have been used to control and learn optimal multi-stream aggregation. We conduct an in-depth experimental study on three non-linear activation functions: Sine-Hyperbolic, Exponential and modified Weibull, showing that while all bring significant gains the Weibull function performs best thanks to its ability to equalise strong activations. The results clearly demonstrate that our ACTNET architecture significantly enhances the discriminative power of deep features, improving significantly over the state-of-the-art retrieval results on all datasets.	[Husain, Syed Sameed; Ong, Eng-Jon; Bober, Miroslaw] Univ Surrey Guildford, Surrey, England	University of Surrey	Husain, SS (corresponding author), Univ Surrey Guildford, Surrey, England.	sameed.husain@surrey.ac.uk; e.ong@surrey.ac.uk; m.bober@surrey.ac.uk			UK Defence Science and Technology Laboratory (Dstl); EPSRC under MURI Grant [EP/R018456/1]	UK Defence Science and Technology Laboratory (Dstl); EPSRC under MURI Grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by the UK Defence Science and Technology Laboratory (Dstl) and EPSRC under MURI Grant EP/R018456/1. The grant is part of the collaboration between US DOD, UK MOD and UK EPSRC under the Multidisciplinary University Research Initiative.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Azizpour H, 2016, IEEE T PATTERN ANAL, V38, P1790, DOI 10.1109/TPAMI.2015.2500224; Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38; Baldi P, 2014, ARXIV14126830; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Glorot X., 2010, PROC MACH LEARN RES, P249; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; Gu Y., 2018, ATTENTION AWARE GEN; Husain SS, 2019, IEEE T IMAGE PROCESS, V28, P5201, DOI 10.1109/TIP.2019.2917234; Husain SS, 2017, IEEE T PATTERN ANAL, V39, P1783, DOI 10.1109/TPAMI.2016.2613873; Iscen A, 2017, PROC CVPR IEEE, P926, DOI 10.1109/CVPR.2017.105; Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Jegou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jimenez Albert, 2017, P 28 BRIT MACH VIS C; Kalantidis Y., 2016, EUR C COMP VIS WORKS; Lodi A., 2019, ABS190109849 CORR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374; Ong Eng-Jon, 2017, ABS170200338 CORR; Pang SM, 2019, IEEE T IMAGE PROCESS, V28, P841, DOI 10.1109/TIP.2018.2874286; Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009; Qian S, 2018, NEUROCOMPUTING, V272, P204, DOI 10.1016/j.neucom.2017.06.070; Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566; Radenovic F, 2018, PROC CVPR IEEE, P5706, DOI 10.1109/CVPR.2018.00598; Revaud J, 2019, IEEE I CONF COMP VIS, P5106, DOI 10.1109/ICCV.2019.00521; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sanjoy D., 2013, INT C MACH LEARN, P1319, DOI DOI 10.5555/3042817.3043084; Seddati O, 2017, IEEE INT CONF COMP V, P1246, DOI 10.1109/ICCVW.2017.150; Simeoni O, 2019, MACH VISION APPL, V30, P243, DOI 10.1007/s00138-019-01005-z; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Teichmann, 2018, DTECT RETRIEVE EFFIC; Tolias Giorgos, 2016, ARXIV151105879, P1; Wu XM, 2018, IEEE IMAGE PROC, P495, DOI 10.1109/ICIP.2018.8451317; Xiao B., 2018, WEAKLY SUPERVISED SO; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu J, 2019, IEEE T IMAGE PROCESS, V28, P601, DOI 10.1109/TIP.2018.2867104; Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150	40	3	4	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1432	1450		10.1007/s11263-021-01444-0	http://dx.doi.org/10.1007/s11263-021-01444-0		FEB 2021	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000616904200001
J	Chen, ZL; Yu, HX; Wu, AC; Zheng, WS				Chen, Zelin; Yu, Hong-Xing; Wu, Ancong; Zheng, Wei-Shi			Letter-Level Online Writer Identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Online writer identification; Online writer identification dataset; Hierarchical Pooling	VERIFICATION	Writer identification (writer-id), an important field in biometrics, aims to identify a writer by their handwriting. Identification in existing writer-id studies requires a complete document or text, limiting the scalability and flexibility of writer-id in realistic applications. To make the application of writer-id more practical (e.g., on mobile devices), we focus on a novel problem, letter-level online writer-id, which requires only a few trajectories of written letters as identification cues. Unlike text-\ document-based writer-id which has rich context for identification, there are much fewer clues to recognize an author from only a few single letters. A main challenge is that a person often writes a letter in different styles from time to time. We refer to this problem as the variance of online writing styles (Var-O-Styles). We address the Var-O-Styles in a capture-normalize-aggregate fashion: Firstly, we extract different features of a letter trajectory by a carefully designed multi-branch encoder, in an attempt to capture different online writing styles. Then we convert all these style features to a reference style feature domain by a novel normalization layer. Finally, we aggregate the normalized features by a hierarchical attention pooling (HAP), which fuses all the input letters with multiple writing styles into a compact feature vector. In addition, we also contribute a large-scale LEtter-level online wRiter IDentification dataset (LERID) for evaluation. Extensive comparative experiments demonstrate the effectiveness of the proposed framework.	[Chen, Zelin; Yu, Hong-Xing; Wu, Ancong; Zheng, Wei-Shi] Sun Yat Set Univ, Sch Data & Comp Sci, Guangzhou, Peoples R China		Zheng, WS (corresponding author), Sun Yat Set Univ, Sch Data & Comp Sci, Guangzhou, Peoples R China.	chenzl9@mail2.sysu.edu.cn; xKoven@gmail.com; wuancong@mail2.sysu.edu.cn; wszheng@ieee.com			National Key Research and Development Program of China [2018YFB1004903]; NSFC [U1911401, U1811461]; Guangdong Province Science and Technology Innovation Leading Talents [2016TX03X157]; Guangdong NSF Project [2018B030312002]; Guangzhou Research Project [201902010037]; Research Projects of Zhejiang Lab [2019KD0AB03]; Key-Area Research and Development Program of Guangzhou [202007030004]	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Guangdong Province Science and Technology Innovation Leading Talents; Guangdong NSF Project; Guangzhou Research Project; Research Projects of Zhejiang Lab; Key-Area Research and Development Program of Guangzhou	This work was supported partially by the National Key Research and Development Program of China (2018YFB1004903), NSFC(U1911401,U1811461), Guangdong Province Science and Technology Innovation Leading Talents (2016TX03X157), Guangdong NSF Project (No. 2018B030312002), Guangzhou Research Project (201902010037), and Research Projects of Zhejiang Lab (No. 2019KD0AB03), and the Key-Area Research and Development Program of Guangzhou (202007030004). The corresponding author and principal investigator for this paper is Wei-Shi Zheng.	Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018; Bangy Li, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P931, DOI 10.1109/ICDAR.2009.26; Bertolini D, 2013, EXPERT SYST APPL, V40, P2069, DOI 10.1016/j.eswa.2012.10.016; Bulacu M, 2007, IEEE T PATTERN ANAL, V29, P701, DOI 10.1109/TPAMI.2007.1009; Carlucci FM, 2017, LECT NOTES COMPUT SC, V10484, P357, DOI 10.1007/978-3-319-68560-1_32; Chaabouni A, 2011, PROC INT CONF DOC, P623, DOI 10.1109/ICDAR.2011.131; Chen K.-T., 1958, T AM MATH SOC, P395, DOI [10.2307/1993193, DOI 10.1090/S0002-9947-1958-0106258-0]; Chen ZL, 2018, IEEE INT CONF AUTOMA, P381, DOI 10.1109/FG.2018.00061; Christlein V, 2018, 2018 13TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS), P169, DOI 10.1109/DAS.2018.9; Dhingra B, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1832, DOI 10.18653/v1/P17-1168; Dwivedi I, 2016, INT CONF FRONT HAND, P572, DOI [10.1109/ICFHR.2016.0110, 10.1109/ICFHR.2016.103]; El Abed Haikal, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1388, DOI 10.1109/ICDAR.2009.284; Feng MW, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P813, DOI 10.1109/ASRU.2015.7404872; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gargouri M, 2013, PROC INT CONF DOC, P428, DOI 10.1109/ICDAR.2013.93; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Nguyen HT, 2019, PATTERN RECOGN LETT, V121, P104, DOI 10.1016/j.patrec.2018.07.022; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Khan FA, 2019, IEEE T INF FOREN SEC, V14, P289, DOI 10.1109/TIFS.2018.2850011; Lai SX, 2020, AAAI CONF ARTIF INTE, V34, P735; Li BY, 2007, LECT NOTES COMPUT SC, V4642, P201; Lin Z., 2017, ARXIV PREPRINT ARXIV; Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713; Moon H, 2001, PERCEPTION, V30, P303, DOI 10.1068/p2896; Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232; Namboodiri A., 2006, INT C FRONT HANDW RE, P566; Nasuno R., 2017, P 5 IIAE INT C INTEL, P94; Ramaiah C, 2013, PROC INT CONF DOC, P917, DOI 10.1109/ICDAR.2013.187; Sae-Bae N, 2014, IEEE T INF FOREN SEC, V9, P933, DOI 10.1109/TIFS.2014.2316472; Schlapbach A, 2008, PATTERN RECOGN, V41, P2381, DOI 10.1016/j.patcog.2008.01.006; Schlapbach A, 2007, PROC INT CONF DOC, P103; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Shivram A, 2013, IET BIOMETRICS, V2, P191, DOI 10.1049/iet-bmt.2013.0017; Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Singh G, 2015, PROC INT CONF DOC, P311, DOI 10.1109/ICDAR.2015.7333774; Song SJ, 2017, AAAI CONF ARTIF INTE, P4263; Songxuan Lai, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1137, DOI 10.1109/ICDAR.2019.00184; Tan GX, 2010, INT J DOC ANAL RECOG, V13, P147, DOI 10.1007/s10032-009-0110-z; Tang YB, 2016, INT CONF FRONT HAND, P566, DOI [10.1109/ICFHR.2016.102, 10.1109/ICFHR.2016.0109]; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tsai MY, 2005, IEEE SYS MAN CYBERN, P1264; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Venugopal V., 2018, IEEE T SYST MAN CY-S, P1; Venugopal V, 2018, IEEE T INF FOREN SEC, V13, P2538, DOI 10.1109/TIFS.2018.2823276; Venugopal V, 2018, PATTERN RECOGN, V78, P318, DOI 10.1016/j.patcog.2018.01.023; Venugopal V, 2017, EXPERT SYST APPL, V72, P196, DOI 10.1016/j.eswa.2016.11.038; Vorugunti C.S., 2019, WORKSH MED FOR CVPR, P88; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359; Xing LJ, 2016, INT CONF FRONT HAND, P584, DOI 10.1109/ICFHR.2016.105; Xing ZJ, 2018, PROC SPIE, V10615, DOI 10.1117/12.2303380; Yang WX, 2016, IEEE INTELL SYST, V31, P45, DOI 10.1109/MIS.2016.22; Zhang XY, 2017, IEEE T HUM-MACH SYST, V47, P285, DOI 10.1109/THMS.2016.2634921	54	3	3	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1394	1409		10.1007/s11263-020-01414-y	http://dx.doi.org/10.1007/s11263-020-01414-y		FEB 2021	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000616147500001
J	Zunino, A; Bargal, SA; Morerio, P; Zhang, JM; Sclaroff, S; Murino, V				Zunino, Andrea; Bargal, Sarah Adel; Morerio, Pietro; Zhang, Jianming; Sclaroff, Stan; Murino, Vittorio			Excitation Dropout: Encouraging Plasticity in Deep Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Data-driven dropout; Deep neural networks; Visual data classification; Grounding; Saliency		We propose a guided dropout regularizer for deep networks based on the evidence of a network prediction defined as the firing of neurons in specific paths. In this work, we utilize the evidence at each neuron to determine the probability of dropout, rather than dropping out neurons uniformly at random as in standard dropout. In essence, we dropout with higher probability those neurons which contribute more to decision making at training time. This approach penalizes high saliency neurons that are most relevant for model prediction, i.e. those having stronger evidence. By dropping such high-saliency neurons, the network is forced to learn alternative paths in order to maintain loss minimization, resulting in a plasticity-like behavior, a characteristic of human brains too. We demonstrate better generalization ability, an increased utilization of network neurons, and a higher resilience to network compression using several metrics over four image/video recognition benchmarks.	[Zunino, Andrea; Morerio, Pietro; Murino, Vittorio] Huawei Technol Co Ltd, Ireland Res Ctr, Dublin, Ireland; [Bargal, Sarah Adel; Sclaroff, Stan] Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA; [Morerio, Pietro; Murino, Vittorio] Ist Italiano Tecnol IIT, Pattern Anal & Comp Vis PAVIS, Genoa, Italy; [Zhang, Jianming] Adobe Res, San Jose, CA USA; [Murino, Vittorio] Univ Verona, Dept Comp Sci, Verona, Italy	Huawei Technologies; Boston University; Istituto Italiano di Tecnologia - IIT; Adobe Systems Inc.; University of Verona	Zunino, A (corresponding author), Huawei Technol Co Ltd, Ireland Res Ctr, Dublin, Ireland.	andrea.zunino@iit.it			Defense Advanced Research Projects Agency (DARPA) Explainable Artificial Intelligence (XAI) program; IBM PhD Fellowship; Hariri Graduate Fellowship	Defense Advanced Research Projects Agency (DARPA) Explainable Artificial Intelligence (XAI) program(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); IBM PhD Fellowship(International Business Machines (IBM)); Hariri Graduate Fellowship	This work was supported in part by the Defense Advanced Research Projects Agency (DARPA) Explainable Artificial Intelligence (XAI) program, an IBM PhD Fellowship, a Hariri Graduate Fellowship, and gifts from Adobe and NVidia. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, DOI/IBC, or the U.S. Government.	Achille A, 2018, IEEE T PATTERN ANAL, V40, P2897, DOI 10.1109/TPAMI.2017.2784440; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba J., 2013, ADV NEURAL INFORM PR, P3084; BARTOLINI RA, 1976, APPL PHYS LETT, V28, P506, DOI 10.1063/1.88834; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ghiasi G, 2018, ADV NEUR IN, V31; Gomez A., 2018, ADV NEURAL INFORM PR, V208; Griffin Gregory, 2007, CALTECH 256 OBJECT C; HEBB D. O., 1949; Hinton G., 2015, ARXIV150302531; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Kang GL, 2018, IEEE T PATTERN ANAL, V40, P1245, DOI 10.1109/TPAMI.2017.2701831; Kingma DP, 2015, ADV NEUR IN, V28; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li Z., 2016, ADV NEURAL INFORM PR; Liang PH., 2013, ADV NEURAL INFORM PR, V26; Ma SG, 2017, PATTERN RECOGN, V68, P334, DOI 10.1016/j.patcog.2017.01.027; Miconi T, 2018, INT C MACH LEARN PML, P3559; Mittal D, 2018, IEEE WINT CONF APPL, P848, DOI 10.1109/WACV.2018.00098; Morerio P, 2017, IEEE I CONF COMP VIS, P3564, DOI 10.1109/ICCV.2017.383; Rong W, 2017, MATER CHARACT, V126, P1, DOI 10.1016/j.matchar.2017.02.010; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829; Soomro K., 2012, ARXIV; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wang SH, 2013, APPL MECH MATER, V422, P118, DOI 10.4028/www.scientific.net/AMM.422.118; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang J, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901336; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319	34	3	5	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1139	1152		10.1007/s11263-020-01422-y	http://dx.doi.org/10.1007/s11263-020-01422-y		JAN 2021	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK		Green Submitted			2022-12-18	WOS:000606383400001
J	Liu, CL; Ding, WR; Hu, Y; Zhang, BC; Liu, JZ; Guo, GD; Doermann, D				Liu, Chunlei; Ding, Wenrui; Hu, Yuan; Zhang, Baochang; Liu, Jianzhuang; Guo, Guodong; Doermann, David			Rectified Binary Convolutional Networks with Generative Adversarial Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Binary convolutional neural network (BNN); Rectified binary convolutional network (RBCN); Generative adversarial network (GAN)	OBJECT TRACKING	Binarized convolutional neural networks (BNNs) are widely used to improve the memory and computational efficiency of deep convolutional neural networks for to be employed on embedded devices. However, existing BNNs fail to explore their corresponding full-precision models' potential, resulting in a significant performance gap. This paper introduces a Rectified Binary Convolutional Network (RBCN) by combining full precision kernels and feature maps to rectify the binarization process in a generative adversarial network (GAN) framework. We further prune our RBCNs using the GAN framework to increase the model efficiency and promote flexibly in practical applications. Extensive experiments validate the superior performance of the proposed RBCN over state-of-the-art BNNs on tasks such as object classification, object tracking, face recognition, and person re-identification.	[Liu, Chunlei; Ding, Wenrui; Hu, Yuan; Zhang, Baochang] Beihang Univ, Beijing, Peoples R China; [Liu, Jianzhuang] Univ Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China; [Guo, Guodong] Inst Deep Learning, Baidu Res & Natl Engn Lab Deep Learning Technol &, Beijing, Peoples R China	Beihang University; Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; University of Chinese Academy of Sciences, CAS	Zhang, BC (corresponding author), Beihang Univ, Beijing, Peoples R China.	liuchunlei@buaa.edu.cn; ding@buaa.edu.cn; huyuan1248@gmail.com; bczhang@buaa.edu.cn; jz.liu@siat.ac.cn; guoguodong01@baidu.com; doermann@buffalo.edu		liu, chunlei/0000-0002-1138-7488	National Natural Science Foundation of China [62076016, 61672079, U20B2042, 62076019]; Science and Technology Innovation 2030-Key Project of "New Generation Artificial Intelligence" [2020AAA0108200]; Shenzhen Science and Technology Program [KQTD2016112515134654]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Innovation 2030-Key Project of "New Generation Artificial Intelligence"; Shenzhen Science and Technology Program	This work is supported by National Natural Science Foundation of China U20B2042, National Natural Science Foundation of China 62076019, and Science and Technology Innovation 2030-Key Project of "New Generation Artificial Intelligence" under Grant 2020AAA0108200. The work was supported in part by National Natural Science Foundation of China under Grants 62076016 and 61672079. This work is also supported by Shenzhen Science and Technology Program KQTD2016112515134654. Chunlei Liu and Wenrui Ding contribute equally. Baochang Zhang is the corresponding author who is also with Shenzhen Academy of Aerospace Technology, Shenzhen, China.	Adam, 2017, MOBILENETS EFFICIENT; Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938; Aila T., 2016, ARXIV PREPRINT ARXIV; Arjovsky M, 2017, PR MACH LEARN RES, V70; Belagiannis V., 2018, P EUR C COMP VIS; Cai H., 2018, ARXIV PREPRINT ARXIV; Changyong S., 2019, ARXIV19040510; Chen HL, 2020, AAAI CONF ARTIF INTE, V34, P10526; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Dentinel Zarembaw, 2014, NEURIPS, P1269; Gao M., 2020, ARXIV200209168, P2020; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu J., 2019, PROJECTION CONVOLUTI; Gulrajani I, 2017, P NIPS 2017; Guo YW, 2016, ADV NEUR IN, V29; Han S, 2015, ADV NEUR IN, V28; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129; Huang H, 2018, BIOCOMPUT-PAC SYM, P304; Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464; Iandola Forrest N., 2016, SQUEEZENET ALEXNET L; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Krizhevsky N, 2009, HINTON CIFAR 10 DATA; Lebedev V., 2015, 3 INT C LEARNING REP; Li YC, 2019, PROC CVPR IEEE, P2795, DOI 10.1109/CVPR.2019.00291; Li ZF, 2017, IEEE I CONF COMP VIS, P2603, DOI 10.1109/ICCV.2017.282; Lin S., 2018, IJCAI, V2, P2425; Lin SH, 2019, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR.2019.00290; Liu C., 2019, INT JOINT C ART INT; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250; Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27; Odena A, 2017, PR MACH LEARN RES, V70; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Rigamonti R, 2013, PROC CVPR IEEE, P2754, DOI 10.1109/CVPR.2013.355; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sengupta S, 2016, IEEE WINT CONF APPL; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tan MX, 2019, PR MACH LEARN RES, V97; Tschannen M., 2018, P NIPS, P5933; Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Yang TJ, 2017, PROC CVPR IEEE, P6071, DOI 10.1109/CVPR.2017.643; Yang ZC, 2015, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2015.173; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zechun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P143, DOI 10.1007/978-3-030-58568-6_9; Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23; Zhang JH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2272, DOI 10.1145/3343031.3350534; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhao Y, 2017, ADVANCES IN ENERGY AND ENVIRONMENT RESEARCH, P345; Zheng C, 2018, PROCEEDINGS OF THE FIRST WORKSHOP ON RADICAL AND EXPERIENTIAL SECURITY (RESEC'18), P61, DOI 10.1145/3203422.3203425; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhou A, 2017, INCREMENTAL NETWORK; Zhou S., 2016, ARXIV160606160	66	3	3	4	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					998	1012		10.1007/s11263-020-01417-9	http://dx.doi.org/10.1007/s11263-020-01417-9		JAN 2021	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000605541100004
J	Peng, YX; Qi, JW; Ye, ZD; Zhuo, YK				Peng, Yuxin; Qi, Jinwei; Ye, Zhaoda; Zhuo, Yunkan			Hierarchical Visual-Textual Knowledge Distillation for Life-Long Correlation Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cross-modal retrieval; Life-long learning; Hierarchical knowledge distillation; Attention transfer; Adaptive network expansion	REPRESENTATION	Correlation learning among different types of multimedia data, such as visual and textual content, faces huge challenges from two important perspectives, namely, cross modal and cross domain. Cross modal means the heterogeneous properties of different types of multimedia data, where the data from different modalities have inconsistent distributions and representations. This situation leads to the first challenge: cross-modal similarity measurement. Cross domain means the multisource property of multimedia data from various domains, in which data from new domains arrive continually, leading to the second challenge: model storage and retraining. Therefore, correlation learning requires a cross-modal continual learning approach, in which only the data from the new domains are used for training, but the previously learned correlation capabilities are preserved. To address the above issues, we introduce the idea of life-long learning into visual-textual cross-modal correlation modeling and propose a visual-textual life-long knowledge distillation (VLKD) approach. In this study, we construct a hierarchical recurrent network that can leverage knowledge from both semantic and attention levels through adaptive network expansion to support cross-modal retrieval in life-long scenarios across various domains. The results of extensive experiments performed on multiple cross-modal datasets with different domains verify the effectiveness of the proposed VLKD approach for life-long cross-modal retrieval.	[Peng, Yuxin; Qi, Jinwei; Ye, Zhaoda; Zhuo, Yunkan] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China	Peking University	Peng, YX (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.	pengyuxin@pku.edu.cn		He, Xiangteng/0000-0001-8502-5685				Akaho Shotaro, 2006, CS0609071 ARXIV; Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753; Andrew Galen, 2013, ICML; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201; Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902; Fukui Akira, 2016, ARXIV160601847; Goodfellow I. J., 2013, ARXIV13126211, DOI DOI 10.1109/ISIE.2007; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645; Huang Yan, 2018, COMPUTER VISION PATT; Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kim Y, 2014, IEEE ASME INT C ADV, P1747, DOI 10.1109/AIM.2014.6878336; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13; Li D., 2003, P 11 ACM INT C MULTI, P604, DOI DOI 10.1145/957013.957143; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin Y., 2017, ARXIV170206700; Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810; Mitchell T, 2018, COMMUN ACM, V61, P103, DOI 10.1145/3191513; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Peng Y., 2016, IJCAI, P3846, DOI DOI 10.5555/3061053.3061157; Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068; Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P5585, DOI 10.1109/TIP.2018.2852503; Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704; Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779; Qi JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P528, DOI 10.1145/3240508.3240558; Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466; Rannen A, 2017, IEEE I CONF COMP VIS, P1329, DOI 10.1109/ICCV.2017.148; Rasiwasia N, 2010, ACM MM, DOI DOI 10.1145/1873951.1873987; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Rusu A. A., 2016, ARXIV160604671; Shin H, 2017, ADV NEUR IN, V30; Song Y., 2019, COMPUTER VISION PATT; Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326; Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311; Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541; Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535; Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966; Yoon Jaehong, 2017, ARXIV170801547; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zenke F, 2017, PR MACH LEARN RES, V70; Zhai X., 2013, P AAAI C ART INT AAA; Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704; Zhang K, 2018, IEEE INT CONF COMMUN, P90, DOI 10.1109/ICCChinaW.2018.8674516; Zhang Xiang, 2015, ADV NEURAL INFORM PR, P649, DOI DOI 10.5555/2969239.2969312	54	3	4	4	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					921	941		10.1007/s11263-020-01392-1	http://dx.doi.org/10.1007/s11263-020-01392-1		JAN 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000605122800001
J	Keller, SM; Samarin, M; Torres, FA; Wieser, M; Roth, V				Keller, Sebastian Mathias; Samarin, Maxim; Torres, Fabricio Arend; Wieser, Mario; Roth, Volker			Learning Extremal Representations with Deep Archetypal Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dimensionality reduction; Archetypal analysis; Deep variational information bottleneck; Generative modeling; Sentiment analysis; Chemical autoencoder	SMALL MOLECULES; OPTIMALITY	Archetypes represent extreme manifestations of a population with respect to specific characteristic traits or features. In linear feature space, archetypes approximate the data convex hull allowing all data points to be expressed as convex mixtures of archetypes. As mixing of archetypes is performed directly on the input data, linear Archetypal Analysis requires additivity of the input, which is a strong assumption unlikely to hold e.g. in case of image data. To address this problem, we propose learning an appropriate latent feature space while simultaneously identifying suitable archetypes. We thus introduce a generative formulation of the linear archetype model, parameterized by neural networks. By introducing the distance-dependent archetype loss, the linear archetype model can be integrated into the latent space of a deep variational information bottleneck and an optimal representation, together with the archetypes, can be learned end-to-end. Moreover, the information bottleneck framework allows for a natural incorporation of arbitrarily complex side information during training. As a consequence, learned archetypes become easily interpretable as they derive their meaning directly from the included side information. Applicability of the proposed method is demonstrated by exploring archetypes of female facial expressions while using multi-rater based emotion scores of these expressions as side information. A second application illustrates the exploration of the chemical space of small organic molecules. By using different kinds of side information we demonstrate how identified archetypes, along with their interpretation, largely depend on the side information provided.	[Keller, Sebastian Mathias; Samarin, Maxim; Torres, Fabricio Arend; Wieser, Mario; Roth, Volker] Univ Basel, Dept Math & Comp Sci, Spiegelgasse 1, CH-4051 Basel, Switzerland	University of Basel	Keller, SM (corresponding author), Univ Basel, Dept Math & Comp Sci, Spiegelgasse 1, CH-4051 Basel, Switzerland.	sebastianmathias.keller@unibas.ch; maxim.samarin@unibas.ch		Keller, Sebastian Mathias/0000-0002-4531-2839; Samarin, Maxim/0000-0002-9242-1827	University of Basel	University of Basel	Open access funding provided by University of Basel	Alemi A.A., 2016, ARXIV161200410 CORR; Anderson E., 1935, B AM IRIS SOC, V59, P2; Atkins P.W., 2014, PHYS CHEM THERMODYNA, V10th; Bauckhage C., 2015, WORKSH NEW CHALL NEU, P8; Bauckhage C, 2014, INT C PATT RECOG, P1544, DOI 10.1109/ICPR.2014.274; Bauckhage C, 2009, LECT NOTES COMPUT SC, V5748, P272, DOI 10.1007/978-3-642-03798-6_28; Cabeza LF, 2015, RENEW SUST ENERG REV, V42, P1106, DOI 10.1016/j.rser.2014.10.096; Canhasi E, 2016, COMPUT SPEECH LANG, V37, P24, DOI 10.1016/j.csl.2015.11.004; Chan BHP, 2003, MON NOT R ASTRON SOC, V338, P790, DOI 10.1046/j.1365-8711.2003.06099.x; Cutler A, 1997, PHYSICA D, V107, P1, DOI 10.1016/S0167-2789(97)84209-1; CUTLER A, 1994, TECHNOMETRICS, V36, P338, DOI 10.2307/1269949; Djawdan M, 1996, PHYSIOL ZOOL, V69, P1176, DOI 10.1086/physzool.69.5.30164252; El Samad H., 2005, P 16 IFAC WORLD C, P16; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Garland T, 2014, CURR BIOL, V24, pR60, DOI 10.1016/j.cub.2013.11.036; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Huggins P, 2007, B MATH BIOL, V69, P2723, DOI 10.1007/s11538-007-9244-7; Jang E., 2017, ICLR; Jmol, 2019, JMOL OPEN SOURCE JAV; Kaufmann D, 2015, LECT NOTES COMPUT SC, V9358, P117, DOI 10.1007/978-3-319-24947-6_10; Keller SM., 2019, GERM C PATT REC, P171; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP, 2014, ADV NEUR IN, V27; Kirkpatrick P, 2004, NATURE, V432, P823, DOI 10.1038/432823a; Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949; Mair S., 2019, ADV NEURAL INFORM PR, V32, P7245; MILLER SL, 1953, SCIENCE, V117, P528, DOI 10.1126/science.117.3046.528; Morup M, 2012, NEUROCOMPUTING, V80, P54, DOI 10.1016/j.neucom.2011.06.033; NORBERG U M, 1987, Philosophical Transactions of the Royal Society of London B Biological Sciences, V316, P335, DOI 10.1098/rstb.1987.0030; Parbhoo S, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22040389; Prabhakaran Sandhya, 2012, Pattern Recognition. Proceedings Joint 34th DAGM and 36th OAGM Symposium, P458, DOI 10.1007/978-3-642-32717-9_46; Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Ruddigkeit L, 2012, J CHEM INF MODEL, V52, P2864, DOI 10.1021/ci300415d; Schuetz R, 2012, SCIENCE, V336, P601, DOI 10.1126/science.1216882; Seth S, 2016, MACH LEARN, V102, P85, DOI 10.1007/s10994-015-5498-8; Shoval O, 2012, SCIENCE, V336, P1157, DOI 10.1126/science.1217405; Steinbeck C, 2003, J CHEM INF COMP SCI, V43, P493, DOI 10.1021/ci025584y; Steuer R., 1986, MULTIPLE CRITERIA OP; Stone E, 1996, PHYSICA D, V96, P110, DOI 10.1016/0167-2789(96)00016-4; Tendler A, 2015, BMC SYST BIOL, V9, DOI 10.1186/s12918-015-0149-z; Tinoco I, 2002, PHYS CHEM PRINCIPLES, P229; Tishby Naftali, 2000, PHYSICS0004057 ARXIV; van Dijk D., 2019, ARXIV190109078; Visini R, 2017, J CHEM INF MODEL, V57, P2707, DOI 10.1021/acs.jcim.7b00457; Wieczorek A., 2018, INT C LEARN REPR ICL; Wieser M., 2020, ARXIV200202782; Wynen D, 2018, ADV NEUR IN, V31	51	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					805	820		10.1007/s11263-020-01390-3	http://dx.doi.org/10.1007/s11263-020-01390-3		DEC 2020	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK	34720403	Green Submitted, Green Published, hybrid			2022-12-18	WOS:000601485200001
J	Wu, X; He, R; Hu, YB; Sun, ZA				Wu, Xiang; He, Ran; Hu, Yibo; Sun, Zhenan			Learning an Evolutionary Embedding via Massive Knowledge Distillation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Knowledge distillation; Open-set problem; Face recognition; Vehicle re-identification; Person re-identification		Knowledge distillation methods aim at transferring knowledge from a large powerful teacher network to a small compact student one. These methods often focus on close-set classification problems and matching features between teacher and student networks from a single sample. However, many real-world classification problems are open-set. This paper proposes an Evolutionary Embedding Learning (EEL) framework to learn a fast and accurate student network for open-set problems via massive knowledge distillation. First, we revisit the formulation of canonical knowledge distillation and make it suitable for the open-set problems with massive classes. Second, by introducing an angular constraint, a novel correlated embedding loss (CEL) is proposed to match embedding spaces between the teacher and student network from a global perspective. Lastly, we propose a simple yet effective paradigm towards a fast and accurate student network development for knowledge distillation. We show the possibility to implement an accelerated student network without sacrificing accuracy, compared with its teacher network. The experimental results are quite encouraging. EEL achieves better performance with other state-of-the-art methods for various large-scale open-set problems, including face recognition, vehicle re-identification and person re-identification.	[Wu, Xiang; He, Ran; Hu, Yibo; Sun, Zhenan] Chinese Acad Sci, Univ Chinese Acad Sci, CASIA Ctr Excellence Brain Sci & Intelligence Tec, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Wu, Xiang; He, Ran; Hu, Yibo; Sun, Zhenan] Chinese Acad Sci, Univ Chinese Acad Sci, CASIA Ctr Excellence Brain Sci & Intelligence Tec, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	He, R (corresponding author), Chinese Acad Sci, Univ Chinese Acad Sci, CASIA Ctr Excellence Brain Sci & Intelligence Tec, Natl Lab Pattern Recognit, Beijing, Peoples R China.; He, R (corresponding author), Chinese Acad Sci, Univ Chinese Acad Sci, CASIA Ctr Excellence Brain Sci & Intelligence Tec, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.	alfredxiangwu@gmail.com; rhe@nlpr.ia.ac.cn; yibo.hu@cripac.ia.ac.cn; znsun@nlpr.ia.ac.cn	Wu, Xiang/AAU-4792-2021	Wu, Xiang/0000-0001-5317-1338				Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Chen J, 2009, PROC CVPR IEEE, P156, DOI 10.1109/CVPRW.2009.5206832; Chen YT, 2018, AAAI CONF ARTIF INTE, P2852; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Czarnecki WM, 2017, ADV NEUR IN, V30; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005; Guo HY, 2018, AAAI CONF ARTIF INTE, P6853; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Han S., 2016, P 4 INT C LEARN REPR, P1; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He R, 2018, IEEE T PATTERN ANAL; He R, 2017, AAAI CONF ARTIF INTE, P2000; Hinton G., 2015, ARXIV150302531; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang C., 2016, ADV NEURAL INFORM PR, P1262; Huang D., 2012, IRIPTR12FR001 BEIJ U; Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129; Huang Z., 2017, ARXIV170701219 CORR; Iandola F.N., 2016, ARXIV; Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527; Kingma D.P, P 3 INT C LEARNING R; LeCun Y., 2010, MNIST HANDWRITTEN DI; Li H., 2017, P INT C LEARN REPR I, P1; Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59; Liao SC, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014); Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Liu X., 2016, ICB; Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966; Liu Y, 2019, PROC CVPR IEEE, P3599, DOI 10.1109/CVPR.2019.00372; Luo H, 2019, 1ST INTERNATIONAL WORKSHOP ON DEEP LEARNING PRACTICE FOR HIGH-DIMENSIONAL SPARSE DATA WITH KDD (DLP-KDD 2019), DOI 10.1145/3326937.3341263; Luo P, 2016, AAAI CONF ARTIF INTE, P3560; Molchanov P., 2017, P INT C LEARN REPR I, P1; Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068; Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409; Parkhi Omkar M., 2015, BRIT MACH VIS C; Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Romero Adriana, 2015, ICLR; Sandler M., 2018, P IEEE C COMP VIS PA; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sohn Kihyuk, 2016, NEURIPS, DOI DOI 10.5555/3157096.3157304; Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129; Song HO, 2017, PROC CVPR IEEE, P2206, DOI 10.1109/CVPR.2017.237; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23; Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI 10.1109/ICCV.2017.283; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wu X, 2018, AAAI CONF ARTIF INTE, P1679; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94; Zagoruyko S., 2017, P INT C LEARN REPR, DOI DOI 10.1109/CVPR.2019.00271; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang X., 2017, ARXIV170701083 CORR; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhang ZY, 2019, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR.2019.00484; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng Z., 2017, CORR ARXIV 1707 0040; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541; Zoph B., 2017, CORR ARXIV 1707 0701	76	3	4	2	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2089	2106		10.1007/s11263-019-01286-x	http://dx.doi.org/10.1007/s11263-019-01286-x			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG					2022-12-18	WOS:000559365500006
J	Lao, YZ; Ait-Aider, O; Bartoli, A				Lao, Yizhen; Ait-Aider, Omar; Bartoli, Adrien			Solving Rolling Shutter 3D Vision Problems using Analogies with Non-rigidity	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Rolling shutter; Absolute pose; Structure-from-motion; Non-rigid; Shape-from-template	SHAPE-FROM-TEMPLATE; MOTION; POSE	We propose an original approach to absolute pose and structure-from-motion (SfM) which handles rolling shutter (RS) effects. Unlike most existing methods which either augment global shutter projection with velocity parameters or impose continuous time and motion through pose interpolation, we use local differential constraints. These are established by drawing analogies with non-rigid 3D vision techniques, namely shape-from-template and non-rigid SfM (NRSfM). The proposed idea is to interpret the images of a rigid surface acquired by a moving RS camera as those of a virtually deformed surface taken by a GS camera. These virtually deformed surfaces are first recovered by relaxing the RS constraint using SfT or NRSfM. Then we upgrade the virtually deformed surface to the actual rigid structure and compute the camera pose and ego-motion by reintroducing the RS constraint. This uses a new 3D-3D registration procedure that minimizes a cost function based on the Euclidean 3D point distance. This is more stable and physically meaningful than the reprojection error or the algebraic distance used in previous work. Experimental results obtained with synthetic and real data show that the proposed methods outperform existing ones in terms of accuracy and stability, even in the known critical configurations.	[Lao, Yizhen] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha, Peoples R China; [Ait-Aider, Omar; Bartoli, Adrien] Univ Clermont Auvergne, CNRS, Inst Pascal, Clermont Ferrand, France	Hunan University; Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA)	Lao, YZ (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha, Peoples R China.	lyz91822@gmail.com; omar.ait-aider@uca.fr; adrien.bartoli@gmail.com			French government research program "Investissements d'Avenir" through the IDEX-ISITE initiative [16-IDEX-0001, CAP 20-25]; IMobS3 Laboratory of Excellence [ANR-10-LABX-16-01]; RobotEx Equipment of Excellence [ANR-10-EQPX-44]; European Union through the Regional Competitiveness and Employment program -2014-2020- (ERDF-AURA region); AURA region	French government research program "Investissements d'Avenir" through the IDEX-ISITE initiative(French National Research Agency (ANR)); IMobS3 Laboratory of Excellence; RobotEx Equipment of Excellence; European Union through the Regional Competitiveness and Employment program -2014-2020- (ERDF-AURA region); AURA region	This work has been sponsored by the French government research program "Investissements d'Avenir" through the IDEX-ISITE initiative 16-IDEX-0001 (CAP 20-25), the IMobS3 Laboratory of Excellence (ANR-10-LABX-16-01) and the RobotEx Equipment of Excellence (ANR-10-EQPX-44). This research was also financed by the European Union through the Regional Competitiveness and Employment program -2014-2020- (ERDF-AURA region) and by the AURA region.	Agudo A, 2016, IEEE T PATTERN ANAL, V38, P979, DOI 10.1109/TPAMI.2015.2469293; Agudo A, 2015, PROC CVPR IEEE, P2179, DOI 10.1109/CVPR.2015.7298830; Ait-Aider O., 2007, CVPR; Ait-Aider O, 2006, LECT NOTES COMPUT SC, V3952, P56; Ait-Aider O, 2009, IEEE I CONF COMP VIS, P1835, DOI 10.1109/ICCV.2009.5459408; Akhter I., 2009, NIPS; Albl C, 2020, IEEE T PATTERN ANAL, V42, P1439, DOI 10.1109/TPAMI.2019.2894395; Albl C, 2016, PROC CVPR IEEE, P3355, DOI 10.1109/CVPR.2016.365; Albl C, 2015, PROC CVPR IEEE, P2292, DOI 10.1109/CVPR.2015.7298842; Albl Cenek, 2016, ECCV; Bartoli A, 2015, IEEE T PATTERN ANAL, V37, P2099, DOI 10.1109/TPAMI.2015.2392759; Bartoli A, 2013, INT J COMPUT VISION, V101, P227, DOI 10.1007/s11263-012-0565-0; Chhatkuli A, 2017, IEEE T PATTERN ANAL, V39, P833, DOI 10.1109/TPAMI.2016.2562622; Collins T., 2015, ISMAR; Dai Yuchao, 2016, CVPR; Dierckx P., 1993, CURVE SURFACE FITTIN; Dryden I.L., 2016, STAT SHAPE ANAL APPL, V995; Duchamp G., 2015, VISAPP; El Gamal A, 2005, IEEE CIRCUITS DEVICE, V21, P6, DOI 10.1109/MCD.2005.1438751; Famouri M, 2018, MACH VISION APPL, V29, P73, DOI 10.1007/s00138-017-0876-9; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Forssen PE, 2010, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2010.5540173; Gallardo M., 2018, THESIS; Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599; Gotardo P. F. U., 2011, ICCV; Haouchine N., 2014, ISMAR; Haralick R. M., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P592, DOI 10.1109/CVPR.1991.139759; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hedborg J., 2011, ICCV WORKSHOPS; Hedborg J, 2012, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2012.6247831; HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127; Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271; Im S, 2019, IEEE T PATTERN ANAL, V41, P775, DOI 10.1109/TPAMI.2018.2819679; Ito E, 2017, PROC CVPR IEEE, P4512, DOI 10.1109/CVPR.2017.480; Kim J. H., 2016, ICRA; Kumar Suryansh, 2019, ARXIV190203791; Lao Y., 2018, ECCV; Lao YZ, 2018, PROC CVPR IEEE, P4795, DOI 10.1109/CVPR.2018.00504; Lee John M, 2018, INTRO RIEMANNIAN MAN; Leng D., 2009, INT WORKSH IM SYST T; Lovegrove S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.93; Magerand L., 2010, INT S 3D DAT PROC VI; Magerand Ludovic, 2012, ECCV; Magnenat S, 2015, IEEE T VIS COMPUT GR, V21, P1201, DOI 10.1109/TVCG.2015.2459871; Malti A, 2017, PROC CVPR IEEE, P143, DOI 10.1109/CVPR.2017.23; Malti A, 2015, PROC CVPR IEEE, P1629, DOI 10.1109/CVPR.2015.7298771; Oth L, 2013, PROC CVPR IEEE, P1360, DOI 10.1109/CVPR.2013.179; Ovren H., 2018, CVPR; Ovren H, 2019, INT J ROBOT RES, V38, P686, DOI 10.1177/0278364919839765; Ovren H, 2013, 2013 IEEE WORKSHOP ON ROBOT VISION (WORV), P68, DOI 10.1109/WORV.2013.6521916; Parashar S, 2018, IEEE T PATTERN ANAL, V40, P2442, DOI 10.1109/TPAMI.2017.2760301; Patron-Perez A, 2015, INT J COMPUT VISION, V113, P208, DOI 10.1007/s11263-015-0811-3; Pizarro D, 2012, INT J COMPUT VISION, V97, P54, DOI 10.1007/s11263-011-0452-0; Purkait P, 2017, IEEE I CONF COMP VIS, P882, DOI 10.1109/ICCV.2017.101; Quan L, 1999, IEEE T PATTERN ANAL, V21, P774, DOI 10.1109/34.784291; Rengarajan Vijay, 2017, CVPR; Rengarajan Vijay, 2016, CVPR; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Russell Chris, 2014, ECCV; Salzmann M, 2011, IEEE T PATTERN ANAL, V33, P931, DOI 10.1109/TPAMI.2010.158; Saurer O., 2016, CVPR; Saurer Olivier, 2015, IROS; Saurer Olivier, 2013, ICCV; Taylor J., 2010, CVPR; Varol A., 2009, ICCV; Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25; Wu YH, 2006, J MATH IMAGING VIS, V24, P131, DOI 10.1007/s10851-005-3617-z; Zhuang Bingbing, 2019, CVPR; Zhuang BH, 2017, IEEE I CONF COMP VIS, P589, DOI 10.1109/ICCV.2017.71	69	3	3	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2021	129	1								10.1007/s11263-020-01368-1	http://dx.doi.org/10.1007/s11263-020-01368-1		AUG 2020	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PU0KV		Green Submitted			2022-12-18	WOS:000560272900001
J	Chai, DF				Chai, Dengfeng			Rooted Spanning Superpixels	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Superpixels; Segmentation; Spanning forest	IMAGE; SEGMENTATION; SHIFT	This paper proposes a new approach for superpixel segmentation. It is formulated as finding a rooted spanning forest of a graph with respect to some roots and a path-cost function. The underlying graph represents an image, the roots serve as seeds for segmentation, each pixel is connected to one seed via a path, the path-cost function measures both the color similarity and spatial closeness between two pixels via a path, and each tree in the spanning forest represents one superpixel. Originating from the evenly distributed seeds, the superpixels are guided by a path-cost function to grow uniformly and adaptively, the pixel-by-pixel growing continues until they cover the whole image. The number of superpixels is controlled by the number of seeds. The connectivity is maintained by region growing. Good performances are assured by connecting each pixel to the similar seed, which are dominated by the path-cost function. It is evaluated by both the superpixel benchmark and supervoxel benchmark. Its performance is ranked as the second among top performing state-of-the-art methods. Moreover, it is much faster than the other superpixel and supervoxel methods.	[Chai, Dengfeng] Zhejiang Univ, Sch Earth Sci, Key Lab Geosci Big Data & Deep Resource Zhejiang, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China	Zhejiang University	Chai, DF (corresponding author), Zhejiang Univ, Sch Earth Sci, Key Lab Geosci Big Data & Deep Resource Zhejiang, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China.	chaidf@zju.edu.cn			National Natural Science Foundation of China [41571335]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China (No. 41571335).	Achanta R, 2017, PROC CVPR IEEE, P4895, DOI 10.1109/CVPR.2017.520; Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; [Anonymous], 1938, SOURCE BOOK GESTALT; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Arnab A, 2016, LECT NOTES COMPUT SC, V9906, P524, DOI 10.1007/978-3-319-46475-6_33; Boix X, 2012, INT J COMPUT VISION, V96, P83, DOI 10.1007/s11263-011-0449-8; Chai DF, 2019, PATTERN RECOGN, V92, P52, DOI 10.1016/j.patcog.2019.03.012; Chang J, 2013, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2013.267; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Corso JJ, 2008, IEEE T MED IMAGING, V27, P629, DOI 10.1109/TMI.2007.912817; Dijkstra EW, 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]; Falcao AX, 2004, IEEE T PATTERN ANAL, V26, P19, DOI 10.1109/TPAMI.2004.1261076; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Gadde R, 2016, LECT NOTES COMPUT SC, V9905, P597, DOI 10.1007/978-3-319-46448-0_36; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893; Guney F, 2015, PROC CVPR IEEE, P4165, DOI 10.1109/CVPR.2015.7299044; He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0; Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908; Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96; Liu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P864; Lucchi A, 2012, IEEE T MED IMAGING, V31, P474, DOI 10.1109/TMI.2011.2171705; Micusik B, 2010, INT J COMPUT VISION, V89, P106, DOI 10.1007/s11263-010-0327-9; Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323; Moore AP, 2008, PROC CVPR IEEE, P998; Moore AP, 2010, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2010.5539890; Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959; Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10; Sharon E, 2000, PROC CVPR IEEE, P70, DOI 10.1109/CVPR.2000.855801; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Stutz D, 2018, COMPUT VIS IMAGE UND, V166, P1, DOI 10.1016/j.cviu.2017.03.007; Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423; Tu WC, 2018, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2018.00066; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Van den Bergh M, 2015, INT J COMPUT VISION, V111, P298, DOI 10.1007/s11263-014-0744-2; Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52; Veksler O, 2010, LECT NOTES COMPUT SC, V6315, P211, DOI 10.1007/978-3-642-15555-0_16; VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344; Wang J, 2012, IEEE T PATTERN ANAL, V34, P1241, DOI 10.1109/TPAMI.2012.47; Wang P, 2013, INT J COMPUT VISION, V103, P1, DOI 10.1007/s11263-012-0588-6; Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385; Xu CL, 2016, INT J COMPUT VISION, V119, P272, DOI 10.1007/s11263-016-0906-5; Xu CL, 2012, PROC CVPR IEEE, P1202, DOI 10.1109/CVPR.2012.6247802; Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101; Yao J, 2015, PROC CVPR IEEE, P2947, DOI 10.1109/CVPR.2015.7298913	52	3	3	3	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2962	2978		10.1007/s11263-020-01352-9	http://dx.doi.org/10.1007/s11263-020-01352-9		JUL 2020	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ		hybrid			2022-12-18	WOS:000550657000001
J	Wang, K; Liu, ZJ; Lin, YJ; Lin, J; Han, S				Wang, Kuan; Liu, Zhijian; Lin, Yujun; Lin, Ji; Han, Song			Hardware-Centric AutoML for Mixed-Precision Quantization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Model quantization; Mixed-precision; Automated ML; Hardware		Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to supportflexible bitwidth(1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off accuracy, latency, energy, and model size, which is both time-consuming and usually sub-optimal. There are plenty of specialized hardware accelerators for neural networks, but little research has been done to design specialized neural networks optimized for a particular hardware accelerator. The latter is demanding given the much longer design cycle of silicon than neural nets. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate the direct feedback signals to the RL agent. Compared with other conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95 x and the energy consumption by 1.9 xwith negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.	[Wang, Kuan; Liu, Zhijian; Lin, Yujun; Lin, Ji; Han, Song] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Han, S (corresponding author), MIT, Cambridge, MA 02139 USA.	kuanwang@mit.edu; zhijian@mit.edu; yujunlin@mit.edu; jilin@mit.edu; songhan@mit.edu	Lin, Yujun/AAR-9588-2020; Han, Song/AAR-9464-2020; Liu, Zhijian/ABF-4061-2020	Lin, Yujun/0000-0001-6314-1722; Han, Song/0000-0002-4186-7618; Liu, Zhijian/0000-0003-3632-9986	NSF [1943349]; MIT-IBM Watson AI Lab; Samsung; AWS; SONY; Xilinx; TI	NSF(National Science Foundation (NSF)); MIT-IBM Watson AI Lab(International Business Machines (IBM)); Samsung(Samsung); AWS; SONY; Xilinx; TI	We thank NSF Career Award #1943349, MIT-IBM Watson AI Lab, Samsung, SONY, Xilinx, TI and AWS for supporting this research.	Adam, 2017, MOBILENETS EFFICIENT; [Anonymous], [No title captured]; Apple, 2018, APPL DESCR 7NM A12 B; Cai H, 2018, PR MACH LEARN RES, V80; Cai Han, 2019, INT C LEARN REPR; Choi Jungwook, 2018, ARXIV180506085; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Couroucli M, 2016, BRIT SCH ATHENS MOD, V1, P1; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Han S., 2017, THESIS; Han S., 2016, P INT C LEARNING REP; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Imagination, 2018, POW VR NEUR NETW ACC; Kingma D.P, P 3 INT C LEARNING R; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Nvidia, 2018, NVID TENS COR; Pham H, 2018, PR MACH LEARN RES, V80; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sharma Hardik, 2018, 2018 ACM IEEE 45 ANN; Umuroglu Y., 2018, 28 INT C FIELD PROGR; Williams S, 2009, COMMUN ACM, V52, P65, DOI 10.1145/1498765.1498785; Xilinx, 2018, ZYNQ 7000 SOC DAT SH; Yang TJ, 2018, LECT NOTES COMPUT SC, V11214, P289, DOI 10.1007/978-3-030-01249-6_18; Yang Tien-Ju, 2016, IEEE C COMP VIS PATT; Zhou AJ, 2018, PROC CVPR IEEE, P9426, DOI 10.1109/CVPR.2018.00982; Zhou Shuchang, 2016, P IEEE C COMP VIS PA; Zhu Chenzhuo, 2017, ICLR; Zoph B., 2017, P1	35	3	3	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2035	2048		10.1007/s11263-020-01339-6	http://dx.doi.org/10.1007/s11263-020-01339-6		JUN 2020	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG		Green Submitted			2022-12-18	WOS:000539947500002
J	Li, AX; Lu, ZW; Guan, JC; Xiang, T; Wang, LW; Wen, JR				Li, Aoxue; Lu, Zhiwu; Guan, Jiechao; Xiang, Tao; Wang, Liwei; Wen, Ji-Rong			Transferrable Feature and Projection Learning with Class Hierarchy for Zero-Shot Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Zero-shot learning; Class hierarchy; Recurrent neural network; Deep feature learning; Projection function learning; Few-shot learning	CLASSIFICATION; DATABASE	Zero-shot learning (ZSL) aims to transfer knowledge from seen classes to unseen ones so that the latter can be recognised without any training samples. This is made possible by learning a projection function between a feature space and a semantic space (e.g. attribute space). Considering the seen and unseen classes as two domains, a big domain gap often exists which challenges ZSL. In this work, we propose a novel inductive ZSL model that leverages superclasses as the bridge between seen and unseen classes to narrow the domain gap. Specifically, we first build a class hierarchy of multiple superclass layers and a single class layer, where the superclasses are automatically generated by data-driven clustering over the semantic representations of all seen and unseen class names. We then exploit the superclasses from the class hierarchy to tackle the domain gap challenge in two aspects: deep feature learning and projection function learning. First, to narrow the domain gap in the feature space, we define a recurrent neural network over superclasses and then plug it into a convolutional neural network for enforcing the superclass hierarchy. Second, to further learn a transferrable projection function for ZSL, a novel projection function learning method is proposed by exploiting the superclasses to align the two domains. Importantly, our transferrable feature and projection learning methods can be easily extended to a closely related task-few-shot learning (FSL). Extensive experiments show that the proposed model outperforms the state-of-the-art alternatives in both ZSL and FSL tasks.	[Li, Aoxue; Wang, Liwei] Peking Univ, Sch Elect Engn & Comp Sci, Key Lab Machine Percept MOE, Beijing 100871, Peoples R China; [Lu, Zhiwu; Guan, Jiechao; Wen, Ji-Rong] Gaoling Sch Artificial Intelligence, Beijing Key Lab Big Data Management & Anal Method, Beijing 100872, Peoples R China; [Xiang, Tao] Univ Surrey, Dept Elect & Elect Engn, Guildford GU2 7XH, Surrey, England	Peking University; University of Surrey	Lu, ZW (corresponding author), Gaoling Sch Artificial Intelligence, Beijing Key Lab Big Data Management & Anal Method, Beijing 100872, Peoples R China.	lax@pku.edu.cn; zhiwu.lu@gmail.com; t.xiang@surrey.ac.uk		Lu, Zhiwu/0000-0003-0280-7724; Lu, Zhiwu/0000-0001-6429-7956	National Key R&D Program of China [2018YFB1402600]; National Natural Science Foundation of China [61976220, 61573026, 61832017]; Beijing Natural Science Foundation [L172037]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work was supported in part by National Key R&D Program of China (2018YFB1402600), National Natural Science Foundation of China (61976220, 61573026, 61832017), and Beijing Natural Science Foundation (L172037).	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Baghshah M. S., 2016, ARXIV160509016; BARTELS RH, 1972, COMMUN ACM, V15, P820, DOI 10.1145/361573.361582; Bo L., P ADV NEUR INF PROC, P2115; Bucher M, 2017, IEEE INT CONF COMP V, P2666, DOI 10.1109/ICCVW.2017.308; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115; Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4; Donahue J, 2014, PR MACH LEARN RES, V32; Finn C, 2017, PR MACH LEARN RES, V70; Frome Andrea, 2013, NEURIPS; Fu YW, 2016, PROC CVPR IEEE, P5337, DOI 10.1109/CVPR.2016.576; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007; FU ZY, 2015, PROC CVPR IEEE, P2635, DOI DOI 10.1109/CVPR.2015.7298879; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hwang Sung Ju, 2014, ADV NEURAL INFORM PR, P271; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kankuekul P, 2012, PROC CVPR IEEE, P3657, DOI 10.1109/CVPR.2012.6248112; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Lake B.M., 2013, ADV NEURAL INFORM PR; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Li AX, 2017, IEEE T GEOSCI REMOTE, V55, P4157, DOI 10.1109/TGRS.2017.2689071; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li X, 2015, IEEE I CONF COMP VIS, P4211, DOI 10.1109/ICCV.2015.479; Liu F, 2017, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2017.443; Long T, 2018, PATTERN RECOGN LETT, V109, P27, DOI 10.1016/j.patrec.2017.09.030; LU Y, 2015, ARXIV150600990; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Mishra A, 2018, IEEE COMPUT SOC CONF, P2269, DOI 10.1109/CVPRW.2018.00294; MISHRA N, 2016, P ICLR; Norouzi Mohammad, 2014, ICLR; Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z; Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755; Radovanovic M, 2010, J MACH LEARN RES, V11, P2487; Ravi S., 2017, OPTIMIZATION MODEL F; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Romera-Paredes Bernardino, 2015, ICML; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schyns PG, 1998, BEHAV BRAIN SCI, V21, P1, DOI 10.1017/S0140525X98000107; Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9; Snell J, 2017, ADV NEUR IN, V30; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Triantafillou E, 2017, ADV NEUR IN, V30; Wah C., 2011, TECH REP; Wang F, 2008, IEEE T KNOWL DATA EN, V20, P55, DOI 10.1109/TKDE.2007.190672; Wang Q, 2017, INT J COMPUT VISION, V124, P356, DOI 10.1007/s11263-017-1027-5; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Xu X, 2017, INT J COMPUT VISION, V123, P309, DOI 10.1007/s11263-016-0983-5; Xu X, 2015, IEEE IMAGE PROC, P63, DOI 10.1109/ICIP.2015.7350760; Ye M, 2017, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR.2017.542; YU Y, 2017, ARXIV170308893; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang ZM, 2016, LECT NOTES COMPUT SC, V9911, P533, DOI 10.1007/978-3-319-46478-7_33; Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474; ZHANG ZM, 2016, PROC CVPR IEEE, P6034, DOI DOI 10.1109/CVPR.2016.649; Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111	73	3	4	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2810	2827		10.1007/s11263-020-01342-x	http://dx.doi.org/10.1007/s11263-020-01342-x		JUN 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ		Green Submitted			2022-12-18	WOS:000537398300001
J	Mendez, O; Hadfield, S; Pugeault, N; Bowden, R				Mendez, Oscar; Hadfield, Simon; Pugeault, Nicolas; Bowden, Richard			SeDAR: Reading Floorplans Like a Human-Using Deep Learning to Enable Human-Inspired Localisation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Robotics; Localisation; Deep Learning; Semantic; MCL; Monte-Carlo; Turtlebot; ROS; Human-level; Segmentation; Indoor	MONTE-CARLO LOCALIZATION	The use of human-level semantic information to aid robotic tasks has recently become an important area for both Computer Vision and Robotics. This has been enabled by advances in Deep Learning that allow consistent and robust semantic understanding. Leveraging this semantic vision of the world has allowed human-level understanding to naturally emerge from many different approaches. Particularly, the use of semantic information to aid in localisation and reconstruction has been at the forefront of both fields. Like robots, humans also require the ability to localise within a structure. To aid this, humans have designed high-level semantic maps of our structures called floorplans. We are extremely good at localising in them, even with limited access to the depth information used by robots. This is because we focus on the distribution of semantic elements, rather than geometric ones. Evidence of this is that humans are normally able to localise in a floorplan that has not been scaled properly. In order to grant this ability to robots, it is necessary to use localisation approaches that leverage the same semantic information humans use. In this paper, we present a novel method for semantically enabled global localisation. Our approach relies on the semantic labels present in the floorplan. Deep Learning is leveraged to extract semantic labels from RGB images, which are compared to the floorplan for localisation. While our approach is able to use range measurements if available, we demonstrate that they are unnecessary as we can achieve results comparable to state-of-the-art without them.	[Mendez, Oscar; Hadfield, Simon; Bowden, Richard] Univ Surrey, Guildford, Surrey, England; [Pugeault, Nicolas] Univ Exeter, Exeter, Devon, England	University of Surrey; University of Exeter	Mendez, O (corresponding author), Univ Surrey, Guildford, Surrey, England.	o.mendez@surrey.ac.uk	Bowden, Richard/AAF-8283-2019	Bowden, Richard/0000-0003-3285-8020; Mendez Maldonado, Oscar Alejandro/0000-0003-4904-4349; Hadfield, Simon/0000-0001-8637-5054				Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; Badrinarayanan Vijay, 2015, SEGNET DEEP CONVOLUT, P1, DOI DOI 10.1109/TPAMI.2016.2644615; Bedkowski JM, 2017, IND ROBOT, V44, P442, DOI 10.1108/IR-11-2016-0309; BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0; Brubaker MA, 2013, PROC CVPR IEEE, P3057, DOI 10.1109/CVPR.2013.393; Caselitz T, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1926, DOI 10.1109/IROS.2016.7759304; Chu H, 2015, IEEE I CONF COMP VIS, P2210, DOI 10.1109/ICCV.2015.255; Dai A., 2017, T GRAPHICS, V36, DOI DOI 10.1145/NNNNNNN.NNNNNNN.HTTP://ARXIV.0RG/ABS/1604.01093; Dellaert F, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1322, DOI 10.1109/ROBOT.1999.772544; DELLAERT F, 1999, P IEEE C COMP VIS PA, V2, P594; DURRANTWHYTE HF, 1988, IEEE J ROBOT AUTOM, V4, P23, DOI 10.1109/56.768; DURRANTWHYTE HF, 1999, P 9 INT S ROB RES SN, P121; Dymczyk M., 2018, IEEE ROBOT AUTOM LET, V3, P1418, DOI [DOI 10.1109/LRA.2018.2800113, 10.1109/LRA.2018.2800113]; Engel J, 2013, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2013.183; Fallon MF, 2012, IEEE INT CONF ROBOT, P1663, DOI 10.1109/ICRA.2012.6224951; Fox D, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P343; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Galliani S., 2015, INT C COMP VIS ICCV; Garage W., TURTLEBOT; Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486; Hilsenbeck S, 2012, INT C INDOOR POSIT; Holder C. J., 2016, ON ROAD TRANSFER LEA, P149, DOI [10.1007/978-3-319-46604-0_11., DOI 10.1007/978-3-319-46604-0_11]; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Kanai S, 2015, INT ARCH PHOTOGRAMM, V44, P61, DOI 10.5194/isprsarchives-XL-4-W5-61-2015; Kendall A., 2017, P BMVC; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Labbe M, 2014, IEEE INT C INT ROBOT, P2661, DOI 10.1109/IROS.2014.6942926; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Liu CX, 2015, PROC CVPR IEEE, P3413, DOI 10.1109/CVPR.2015.7298963; McCormac J, 2018, INT CONF 3D VISION, P32, DOI 10.1109/3DV.2018.00015; Melbouci K, 2016, IEEE IMAGE PROC, P2618, DOI 10.1109/ICIP.2016.7532833; Mendez O., 2016, P BMVC 2016; Mendez O., 2018, INT C ROB AUT ICRA; Mendez O., 2017, INT C COMP VIS ICCV; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Murphy KP, 2000, ADV NEUR IN, V12, P1015; Neubert P., 2017, INT C INT ROB SYST I; Poschmann Johannes, 2017, EUR C MOB ROB ECMR, P403; Sattler T., 2017, C COMP VIS PATT REC; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; SMITH RC, 1986, INT J ROBOT RES, V5, P56, DOI 10.1177/027836498600500404; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Thrun S, 2002, COMMUN ACM, V45, P52, DOI 10.1145/504729.504754; Thrun S, 2001, INT J ROBOT RES, V20, P335, DOI 10.1177/02783640122067435; Thrun S, 2001, ARTIF INTELL, V128, P99, DOI 10.1016/S0004-3702(01)00069-8; Thrun S., 2006, PROBABILISTIC ROBOTI, P238; Von Stumberg L., 2016, ARXIV160907835; Walch Florian, 2017, INT C COMP VIS ICCV; Wang SL, 2015, IEEE I CONF COMP VIS, P2695, DOI 10.1109/ICCV.2015.309; Winterhalter W, 2015, IEEE INT C INT ROBOT, P3138, DOI 10.1109/IROS.2015.7353811; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458	53	3	3	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1286	1310		10.1007/s11263-019-01239-4	http://dx.doi.org/10.1007/s11263-019-01239-4			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		hybrid, Green Accepted			2022-12-18	WOS:000531431500012
J	de Bem, R; Ghosh, A; Ajanthan, T; Miksik, O; Boukhayma, A; Siddharth, N; Torr, P				de Bem, Rodrigo; Ghosh, Arnab; Ajanthan, Thalaiyasingam; Miksik, Ondrej; Boukhayma, Adnane; Siddharth, N.; Torr, Philip			DGPose: Deep Generative Models for Human Body Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep generative models; Semi-supervised learning; Human pose estimation; Variational autoencoders; Generative adversarial networks	PERCEPTION; VISION; PEOPLE; REAL	Deep generative modelling for human body analysis is an emerging problem with many interesting applications. However, the latent space learned by such approaches is typically not interpretable, resulting in less flexibility. In this work, we present deep generative models for human body analysis in which the body pose and the visual appearance are disentangled. Such a disentanglement allows independent manipulation of pose and appearance, and hence enables applications such as pose-transfer without specific training for such a task. Our proposed models, the Conditional-DGPose and the Semi-DGPose, have different characteristics. In the first, body pose labels are taken as conditioners, from a fully-supervised training set. In the second, our structured semi-supervised approach allows for pose estimation to be performed by the model itself and relaxes the need for labelled data. Therefore, the Semi-DGPose aims for the joint understanding and generation of people in images. It is not only capable of mapping images to interpretable latent representations but also able to map these representations back to the image space. We compare our models with relevant baselines, the ClothNet-Body and the Pose Guided Person Generation networks, demonstrating their merits on the Human3.6M, ChictopiaPlus and DeepFashion benchmarks.	[de Bem, Rodrigo; Ghosh, Arnab; Ajanthan, Thalaiyasingam; Miksik, Ondrej; Boukhayma, Adnane; Siddharth, N.; Torr, Philip] Univ Oxford, Dept Engn Sci, Oxford, England; [de Bem, Rodrigo] Fed Univ Rio Grande, Ctr Computat Sci, Rio Grande, Brazil; [Ajanthan, Thalaiyasingam] Australian Natl Univ, Canberra, ACT, Australia	University of Oxford; Universidade Federal do Rio Grande; Australian National University	de Bem, R (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.; de Bem, R (corresponding author), Fed Univ Rio Grande, Ctr Computat Sci, Rio Grande, Brazil.	rodrigo@robots.ox.ac.uk; arnabg@robots.ox.ac.uk; ajanthan@robots.ox.ac.uk; omiksik@robots.ox.ac.uk; adnane@robots.ox.ac.uk; nsid@robots.ox.ac.uk; phst@robots.ox.ac.uk		Narayanaswamy, Siddharth/0000-0003-4911-7333; Andrade de Bem, Rodrigo/0000-0002-4860-0115	ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC [Seebibyte EP/M013774/1]; EPSRC/MURI [EP/N019474/1]; CAPES Foundation [99999.013296/2013-02]; EPSRC [EP/N019474/1] Funding Source: UKRI	ERC(European Research Council (ERC)European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); CAPES Foundation(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by the ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1. We would also like to acknowledge the Royal Academy of Engineering and FiveAI. Rodrigo de Bem is a CAPES Foundation scholarship holder (Process no: 99999.013296/2013-02, Ministry of Education, Brazil).	Achilles Felix, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9900, P491, DOI 10.1007/978-3-319-46720-7_57; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; [Anonymous], 2018, BOEING WILLIAM FETTE; Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491; Borshukov G., 2005, SIGGRAPH; Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chan, 2018, ARXIV180807371; Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601; De Bem R., 2018, P EUR C COMP VIS ECC; de Bem R., 2018, ACML; de Bem R, 2019, IEEE WINT CONF APPL, P1449, DOI 10.1109/WACV.2019.00159; de La Gorce M, 2011, IEEE T PATTERN ANAL, V33, P1793, DOI 10.1109/TPAMI.2011.33; Elgammal A, 2004, PROC CVPR IEEE, P681; Enzweiler M, 2008, PROC CVPR IEEE, P1944; Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923; Ezzat T, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P116, DOI 10.1109/AFGR.1996.557252; Fan SJ, 2018, IEEE T PATTERN ANAL, V40, P2180, DOI 10.1109/TPAMI.2017.2747150; Fei-Fei L, 2005, PROC CVPR IEEE, P524; FETTER WA, 1982, IEEE COMPUT GRAPH, V2, P9; Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174; Fossati A., 2007, CVPR, P1; Franco JS, 2005, IEEE I CONF COMP VIS, P1747; Geisler WS, 2008, ANNU REV PSYCHOL, V59, P167, DOI 10.1146/annurev.psych.58.110405.085632; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hattori H, 2018, INT J COMPUT VISION, V126, P1027, DOI 10.1007/s11263-018-1077-3; Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hilton A, 1999, COMP ANIM CONF PROC, P174, DOI 10.1109/CA.1999.781210; Ichim AE, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766974; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Johnson Sam, 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12]; Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394; Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni TD, 2015, ADV NEUR IN, V28; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98; Lee CS, 2005, LECT NOTES COMPUT SC, V3723, P17; Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Ma LQ, 2017, ADV NEUR IN, V30; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026; Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6; Magnenat-Thalmann N, 2006, HDB VIRTUAL HUMANS; Massiceti D, 2018, PROC CVPR IEEE, P6097, DOI 10.1109/CVPR.2018.00638; Moeslund T. B., 2011, VISUAL ANAL HUMANS; Muller M, 2018, INT J COMPUT VISION, V126, P902, DOI 10.1007/s11263-018-1073-7; NASA, 1995, NASASTD3001, V1; Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Pighin F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P75, DOI 10.1145/280814.280825; Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899; Ranzato M., 2010, P 13 INT C ART INT S, P621; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rhodin H, 2018, LECT NOTES COMPUT SC, V11214, P765, DOI 10.1007/978-3-030-01249-6_46; Rogez G, 2016, ADV NEUR IN, V29; Rogez G, 2018, INT J COMPUT VISION, V126, P993, DOI 10.1007/s11263-018-1071-9; Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rosales R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P378, DOI 10.1109/ICCV.2001.937543; Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870; Seemann E., 2004, FG; Shan Q, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P25, DOI 10.1109/3DV.2013.12; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Starck J., 2005, P ACM SIGGRAPH EUR S, P49, DOI DOI 10.1145/1073368.1073375; Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68; Theis Lucas, 2016, ICLR; Thies Justus, 2016, CVPR, DOI DOI 10.1109/CVPR.2016.262; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Trumble M, 2018, LECT NOTES COMPUT SC, V11214, P800, DOI 10.1007/978-3-030-01249-6_48; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Valenza E, 1996, J EXP PSYCHOL HUMAN, V22, P892, DOI 10.1037/0096-1523.22.4.892; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; von Marcard T., 2017, EUROGRAPHICS; Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361; Wang Ting-Chun, 2018, ARXIV180806601; Wang Y, 2004, COMPUT GRAPH FORUM, V23, P677, DOI 10.1111/j.1467-8659.2004.00800.x; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZY, 2017, ADV NEUR IN, V30; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144; Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002; Zanfir M, 2018, PROC CVPR IEEE, P5391, DOI 10.1109/CVPR.2018.00565; Zhang YT, 2018, PROC CVPR IEEE, P2694, DOI 10.1109/CVPR.2018.00285	102	3	3	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1537	1563		10.1007/s11263-020-01306-1	http://dx.doi.org/10.1007/s11263-020-01306-1		APR 2020	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	LL3BW		hybrid, Green Published, Green Submitted			2022-12-18	WOS:000528435100001
J	Pritts, J; Kukelova, Z; Larsson, V; Lochman, Y; Chum, O				Pritts, James; Kukelova, Zuzana; Larsson, Viktor; Lochman, Yaroslava; Chum, Ondrej			Minimal Solvers for Rectifying from Radially-Distorted Scales and Change of Scales	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Rectification; Radial lens distortion; Minimal solvers; Repeated patterns; Symmetry; Local features		This paper introduces the first minimal solvers that jointly estimate lens distortion and affine rectification from the image of rigidly-transformed coplanar features. The solvers work on scenes without straight lines and, in general, relax strong assumptions about scene content made by the state of the art. The proposed solvers use the affine invariant that coplanar repeats have the same scale in rectified space. The solvers are separated into two groups that differ by how the equal scale invariant of rectified space is used to place constraints on the lens undistortion and rectification parameters. We demonstrate a principled approach for generating stable minimal solvers by the Grobner basis method, which is accomplished by sampling feasible monomial bases to maximize numerical stability. Synthetic and real-image experiments confirm that the proposed solvers demonstrate superior robustness to noise compared to the state of the art. Accurate rectifications on imagery taken with narrow to fisheye field-of-view lenses demonstrate the wide applicability of the proposed method. The method is fully automatic.	[Pritts, James] Czech Tech Univ, CIIRC, Prague, Czech Republic; [Pritts, James] Facebook Real Labs, Pittsburgh, PA 15213 USA; [Kukelova, Zuzana; Chum, Ondrej] Czech Tech Univ, Fac Elect Engn, VRG, Prague, Czech Republic; [Larsson, Viktor] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Lochman, Yaroslava] Ukrainian Catholic Univ, Machine Learning Lab, Lvov, Ukraine	Czech Technical University Prague; Facebook Inc; Czech Technical University Prague; Swiss Federal Institutes of Technology Domain; ETH Zurich; Ukrainian Catholic University	Pritts, J (corresponding author), Czech Tech Univ, CIIRC, Prague, Czech Republic.; Pritts, J (corresponding author), Facebook Real Labs, Pittsburgh, PA 15213 USA.	prittjam@cvut.cz; kukelova@cmp.felk.cvut.cz; viktor.larsson@inf.ethz.ch; lochman@ucu.edu.ua; chum@cmp.felk.cvut.cz	Kukelova, Zuzana/AAM-9096-2020; Kukelova, Zuzana/GXG-1671-2022	Kukelova, Zuzana/0000-0002-1916-8829; Pritts, James/0000-0001-6533-8975	European Regional Development Fund under the project Robotics for Industry 4.0 [CZ.02.1.01/0.0/0.0/15_003/0000470, SGS17/185/OHK3/3T/13]; ESI Fund, OP RDE programme under the project International Mobility of Researchers MSCA-IF at CTU [CZ.02.2.69/0.0/0.0/17_050/0008025]; Grant OP VVV [CZ.02.1.01/0.0/0.0/16_019/0000765]; ETH Zurich Postdoctoral Fellowship program; Marie SklodowskaCurie Actions COFUND program; Robotics for Industry 4.0; ELEKS Ltd.	European Regional Development Fund under the project Robotics for Industry 4.0; ESI Fund, OP RDE programme under the project International Mobility of Researchers MSCA-IF at CTU; Grant OP VVV; ETH Zurich Postdoctoral Fellowship program; Marie SklodowskaCurie Actions COFUND program; Robotics for Industry 4.0; ELEKS Ltd.	James Pritts acknowledges the European Regional Development Fund under the project Robotics for Industry 4.0 (Reg. No. CZ.02.1.01/0.0/0.0/15_003/0000470) and Grant SGS17/185/OHK3/3T/13; Zuzana Kukelova the ESI Fund, OP RDE programme under the project International Mobility of Researchers MSCA-IF at CTU No. CZ.02.2.69/0.0/0.0/17_050/0008025; and Ond.rej Chum Grant OP VVV funded project CZ.02.1.01/0.0/0.0/16_019/0000765 "Research Center for Informatics". Viktor Larsson received funding from the ETH Zurich Postdoctoral Fellowship program and the Marie SklodowskaCurie Actions COFUND program. Yaroslava Lochman was also supported by Robotics for Industry 4.0 as well as ELEKS Ltd.	AHMAD S, 2018, INT J COMPUT VISION, V126, P1; Aiger D, 2012, COMPUT GRAPH FORUM, V31, P439, DOI 10.1111/j.1467-8659.2012.03023.x; ANTUNES M, 2017, CVPR; ARANDJELOVIC R, 2012, PROC CVPR IEEE, P2911, DOI DOI 10.1109/CVPR.2012.6248018; Barath D, 2017, PATTERN RECOGN LETT, V94, P7, DOI 10.1016/j.patrec.2017.04.020; Brown Duane C, 1966, PHOTOGRAMMETRIC ENG, P2, DOI DOI 10.1234/12345678; Bukhari F, 2013, J MATH IMAGING VIS, V45, P31, DOI 10.1007/s10851-012-0342-2; Camposeco F, 2018, PROC CVPR IEEE, P136, DOI 10.1109/CVPR.2018.00022; CHUM O, 2004, ACCV; CHUM O, 2010, ACCV; Conrady AE, 1918, MON NOT R ASTRON SOC, V79, P0384; CRIMINISI A, 2000, BMVC; Devernay F, 2001, MACH VISION APPL, V13, P14, DOI 10.1007/PL00013269; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fitzgibbon AW, 2001, PROC CVPR IEEE, P125; FUNK C, 2017, ICCV WORKSH; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; KOSER K, 2008, CVPR; KOSER K, 2008, ECCV; KUANG Y, 2012, ECCV; KUKELOVA Z, 2008, ECCV; Kukelova Z, 2015, PROC CVPR IEEE, P639, DOI 10.1109/CVPR.2015.7298663; Larsson V, 2018, PROC CVPR IEEE, P3945, DOI 10.1109/CVPR.2018.00415; Larsson V, 2017, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2017.254; Larsson V, 2017, PROC CVPR IEEE, P2383, DOI 10.1109/CVPR.2017.256; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lukac M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073661; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mishkin D, 2018, LECT NOTES COMPUT SC, V11213, P287, DOI 10.1007/978-3-030-01240-3_18; OBDRZALEK S, 2002, BMVC; Ohta T.I., 1981, P INT JOINT C ART IN, P746; PRITTS J, 2016, BMVC; PRITTS J, 2018, CVPR; Pritts J, 2014, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2014.380; STRAND R, 2005, BMVC; VEDALDI A, 2008, P 18 ACM INT C MULT, P1469; Wang AQ, 2009, J MATH IMAGING VIS, V35, P165, DOI 10.1007/s10851-009-0162-1; WILDENAUER H, 2013, BMVC; WU C, 2011, CVPR; Zhang ZD, 2012, INT J COMPUT VISION, V99, P1, DOI 10.1007/s11263-012-0515-x	42	3	3	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		950	968		10.1007/s11263-019-01216-x	http://dx.doi.org/10.1007/s11263-019-01216-x		MAR 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Green Submitted			2022-12-18	WOS:000521783600001
J	Santo, H; Waechter, M; Lin, WY; Sugano, Y; Matsushita, Y				Santo, Hiroaki; Waechter, Michael; Lin, Wen-Yan; Sugano, Yusuke; Matsushita, Yasuyuki			Light Structure from Pin Motion: Geometric Point Light Source Calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Light source calibration; Photometric stereo; Structure from motion	PHOTOMETRIC STEREO; ILLUMINATION; CAMERA; IMAGE; REFLECTANCE; ORIENTATION; POSITION	We present a method for geometric point light source calibration. Unlike prior works that use Lambertian spheres, mirror spheres, or mirror planes, we use a calibration target consisting of a plane and small shadow casters at unknown positions above the plane. We show that shadow observations from a moving calibration target under a fixed light follow the principles of pinhole camera geometry and epipolar geometry, allowing joint recovery of the light position and 3D shadow caster positions, equivalent to how conventional structure from motion jointly recovers camera parameters and 3D feature positions from observed 2D features. Moreover, we devised a unified light model that works with nearby point lights as well as distant light in one common framework. Our evaluation shows that our method yields light estimates that are stable and more accurate than existing techniques while having a much simpler setup and requiring less manual labor.	[Santo, Hiroaki; Waechter, Michael; Lin, Wen-Yan; Matsushita, Yasuyuki] Osaka Univ, Grad Sch Informat Sci & Technol, Osaka, Japan; [Sugano, Yusuke] Univ Tokyo, Inst Ind Sci, Tokyo, Japan	Osaka University; University of Tokyo	Santo, H (corresponding author), Osaka Univ, Grad Sch Informat Sci & Technol, Osaka, Japan.	santo.hiroaki@ist.osaka-u.ac.jp; waechter.michael@ist.osaka-u.ac.jp; lin.daniel@ist.osaka-u.ac.jp; sugano@iis.u-tokyo.ac.jp; yasumat@ist.osaka-u.ac.jp		Santo, Hiroaki/0000-0003-2891-5993; Matsushita, Yasuyui/0000-0002-1935-4752	JSPS KAKENHI [JP19H01123]; JSPS by the Japan Society for the Promotion of Science [JP19J10326, JP17F17350]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JSPS by the Japan Society for the Promotion of Science(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	This work was supported by JSPS KAKENHI Grant Number JP19H01123. Hiroaki Santo is grateful for support through a JSPS research fellowship for young scientists by the Japan Society for the Promotion of Science (JP19J10326). MichaelWaechter is grateful for support through a JSPS postdoctoral fellowship by the Japan Society for the Promotion of Science (JP17F17350).	Alldrin NG, 2007, IEEE I CONF COMP VIS, P417; Aoto T, 2012, INT C PATT RECOG, P3721; Bouguet JY, 1999, INT J COMPUT VISION, V35, P129, DOI 10.1023/A:1008124523456; Bradski G, 2000, DR DOBBS J, V25, P120; Bunteong A., 2016, INT J PHYS SCI, V11, P168; Cao XC, 2005, PROC CVPR IEEE, P918; Chen GY, 2019, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR.2019.00894; Cho D, 2018, IEEE T PATTERN ANAL, V42, P232; Collins T, 2012, LECT NOTES COMPUT SC, V7511, P634, DOI 10.1007/978-3-642-33418-4_78; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; GARDNER MA, 2017, ACM T GRAPHIC, V36, P1; Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005; Goldman D. B., 2009, IEEE T PATTERN ANAL, V32, P1060, DOI DOI 10.1109/TPAMI.2009.102; Hara K, 2005, IEEE T PATTERN ANAL, V27, P493, DOI 10.1109/TPAMI.2005.82; Hartley R., 2004, ROBOTICA; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; HORN B, 1970, AITR232 MIT; Hu B., 2004, CSDTR810 UR U ROCH; Kludt J, 2013, INT INTEG REL WRKSP, P161, DOI 10.1109/IIRW.2013.6804184; Logothetis F., 2017, P IEEE C COMP VIS PA, P941; Ma L, 2019, OPT EXPRESS, V27, P4024, DOI 10.1364/OE.27.004024; NEGAHDARIPOUR S, 1990, J OPT SOC AM A, V7, P279, DOI 10.1364/JOSAA.7.000279; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Park J, 2014, PROC CVPR IEEE, P2267, DOI 10.1109/CVPR.2014.290; Petersen K.B., 2012, MATRIX COOKBOOK; Powell MW, 2001, IEEE T PATTERN ANAL, V23, P1022, DOI 10.1109/34.955114; Ptrucean V, 2016, IEEE T PATTERN ANAL, V39, P788; QUeau Y., 2017, P IEEE C COMP VIS PA; Queau Y, 2018, J MATH IMAGING VIS, V60, P313, DOI 10.1007/s10851-017-0761-1; Sato I, 2001, PROC CVPR IEEE, P400; Sato I, 2003, IEEE T PATTERN ANAL, V25, P290, DOI 10.1109/TPAMI.2003.1182093; Schnieders Dirk, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P96; Schnieders D, 2013, COMPUT VIS IMAGE UND, V117, P1536, DOI 10.1016/j.cviu.2013.06.004; Shen HL, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3533326; Shi BX, 2010, PROC CVPR IEEE, P1118, DOI 10.1109/CVPR.2010.5540091; Silver W.M., 1980, THESIS; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Song Z, 2018, OPT LASER ENG, V111, P172, DOI 10.1016/j.optlaseng.2018.08.014; Szeliski R., 2010, COMPUTER VISION ALGO, DOI DOI 10.1007/978-3-030-34372-9; Takai T, 2009, COMPUT VIS IMAGE UND, V113, P966, DOI 10.1016/j.cviu.2009.03.017; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; Wang Y, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P38, DOI 10.1109/PCCGA.2002.1167837; Weber M., 2001, BMVC, V2001, P471; Wei J, 2003, PATTERN RECOGN LETT, V24, P159, DOI 10.1016/S0167-8655(02)00208-8; Wong KYK, 2008, LECT NOTES COMPUT SC, V5302, P631, DOI 10.1007/978-3-540-88682-2_48; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Zhang YF, 2001, IEEE T PATTERN ANAL, V23, P915, DOI 10.1109/34.946995; Zhou WD, 2002, P ANN INT IEEE EMBS, P206, DOI 10.1109/IEMBS.2002.1134458	49	3	3	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2020	128	7					1889	1912		10.1007/s11263-020-01312-3	http://dx.doi.org/10.1007/s11263-020-01312-3		MAR 2020	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MC3NG		hybrid, Green Published			2022-12-18	WOS:000519831900001
J	Shao, L; Shum, HPH; Hospedales, T				Shao, Ling; Shum, Hubert P. H.; Hospedales, Timothy			Editorial: Special Issue on Machine Vision with Deep Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Shum, Hubert P. H.] Northumbria Univ, Newcastle Upon Tyne, Tyne & Wear, England; [Hospedales, Timothy] Univ Edinburgh, Edinburgh, Midlothian, Scotland	Northumbria University; University of Edinburgh	Shao, L (corresponding author), Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.	ling.shao@ieee.org; hubert.shum@northumbria.ac.uk; t.hospedales@ed.ac.uk	Shum, Hubert P. H./E-8060-2015	Shum, Hubert P. H./0000-0001-5651-6039; Shao, Ling/0000-0002-8264-6117					0	3	3	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		771	772		10.1007/s11263-020-01317-y	http://dx.doi.org/10.1007/s11263-020-01317-y		MAR 2020	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Bronze, Green Accepted			2022-12-18	WOS:000519623300002
J	Zhao, B; Yin, WD; Meng, LL; Sigal, L				Zhao, Bo; Yin, Weidong; Meng, Lili; Sigal, Leonid			Layout2image: Image Generation from Layout	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene image generation; Image translation; Image generation; Generative adversarial networks		Despite significant recent progress on generative models, controlled generation of images depicting multiple and complex object layouts is still a difficult problem. Among the core challenges are the diversity of appearance a given object may possess and, as a result, exponential set of images consistent with a specified layout. To address these challenges, we propose a novel approach for layout-based image generation; we call it Layout2Im. Given the coarse spatial layout (bounding boxes + object categories), our model can generate a set of realistic images which have the correct objects in the desired locations. The representation of each object is disentangled into a specified/certain part (category) and an unspecified/uncertain part (appearance). The category is encoded using a word embedding and the appearance is distilled into a low-dimensional vector sampled from a normal distribution. Individual object representations are composed together using convolutional LSTM, to obtain an encoding of the complete layout, and then decoded to an image. Several loss terms are introduced to encourage accurate and diverse image generation. The proposed Layout2Im model significantly outperforms the previous state-of-the-art, boosting the best reported inception score by 24.66% and 28.57% on the very challenging COCO-Stuff and Visual Genome datasets, respectively. Extensive experiments also demonstrate our model's ability to generate complex and diverse images with many objects.	[Zhao, Bo; Yin, Weidong; Meng, Lili; Sigal, Leonid] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada; [Zhao, Bo] Bank Montreal AI, Toronto, ON, Canada; [Zhao, Bo; Yin, Weidong; Sigal, Leonid] Vector Inst, Toronto, ON, Canada	University of British Columbia; Bank of Montreal	Zhao, B (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.; Zhao, B (corresponding author), Bank Montreal AI, Toronto, ON, Canada.; Zhao, B (corresponding author), Vector Inst, Toronto, ON, Canada.	zhaobo.cs@gmail.com; wdyin@cs.ubc.ca; menglili@cs.ubc.ca; lsigal@cs.ubc.ca	meng, li/GVT-2063-2022		NSERC Discovery grant; NSERC CFI grant; NVIDIA Corporation; NSERC DAS grant	NSERC Discovery grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); NSERC CFI grant; NVIDIA Corporation; NSERC DAS grant	This research was supported, in part, by NSERC Discovery, NSERC DAS and NSERC CFI grants. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan V GPU used for this research.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, NIPS; [Anonymous], 2015, ICLR WORKSH; Ba J. L, 2015, ARXIV151102793; Caesar H., 2016, ARXIV161203716; de Vries H, 2017, ADV NEUR IN, V30; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Karacan L., 2016, ARXIV161200215; Kim Jin-Hwa, 2017, ARXIV171205558; Kingma D.P, P 3 INT C LEARNING R; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304; Lee Hsin-Ying, 2018, ECCV; Liu Ming-Yu, 2017, NIPS; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Mirza M., 2014, ARXIV; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Park T., 2019, ACM SIGGRAPH 2019 RE; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed S, 2017, PR MACH LEARN RES, V70; Salimans T, 2016, ADV NEUR IN, V29; Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723; Sharma Shikhar, 2018, ARXIV180208216; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tan Fuwen, 2018, ARXIV180901110; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Welinder P., 2010, CNSTR2010001 CALTECH; Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434; Zhang H., 2017, ICCV; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang WW, 2008, LECT NOTES COMPUT SC, V5305, P802, DOI 10.1007/978-3-540-88693-8_59; Zhao B, 2018, LECT NOTES COMPUT SC, V11218, P157, DOI 10.1007/978-3-030-01264-9_10; Zhu Jun-Yan, 2017, ICCV	51	3	3	5	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2418	2435		10.1007/s11263-020-01300-7	http://dx.doi.org/10.1007/s11263-020-01300-7		FEB 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY					2022-12-18	WOS:000516487800001
J	Liu, ZH; Lin, GH; Goh, WL				Liu, Zichuan; Lin, Guosheng; Goh, Wang Ling			Bottom-Up Scene Text Detection with Markov Clustering Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Neural networks; Markov clustering; Self-attention; Text detection and segmentation	LOCALIZATION; RECOGNITION	A novel detection framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. Different from the traditional top-down scene text detection approaches that inherit from the classic object detection, MCN detects scene text objects in a bottom-up manner. MCN predicts instance-level bounding boxes by firstly converting an image into a stochastic flow graph where Markov Clustering is performed based on the predicted stochastic flows. The stochastic flows encode the local correlation and semantic information of scene text objects. An object is modeled as strongly connected nodes by flows, which allows flexible and bottom-up detection for scale-varying and rotated text objects without prior knowledge of object size. The flow prediction is supported by the advanced Convolutional Neural Networks architectures and Position-aware spatial attention mechanism, which provides enhanced flow prediction by adaptively fusing spatial representations. The experimental evaluation on public benchmarks shows that our MCN method achieves the state-of-art performance on public benchmarks, especially in retrieving long and oriented texts.	[Liu, Zichuan; Lin, Guosheng; Goh, Wang Ling] Nanyang Technol Univ, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Lin, GH (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	zliu016@e.ntu.edu.sg; gslin@ntu.edu.sg; qzhao@cs.umn.edu	GOH, WANG LING/C-2273-2008; Lin, Guosheng/Q-4024-2017	Lin, Guosheng/0000-0002-0329-7458	National Research Foundation Singapore under its AI Singapore Programme [AISG-RP-2018-003]; MOE Tier-1 research Grants [RG126/17 (S), RG28/18 (S)]	National Research Foundation Singapore under its AI Singapore Programme(National Research Foundation, Singapore); MOE Tier-1 research Grants	This research is supported by the National Research Foundation Singapore under its AI Singapore Programme (Award Number: AISG-RP-2018-003) and the MOE Tier-1 research Grants: RG126/17 (S) and RG28/18 (S).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2008, 2008 IEEE Hot Chips 20 Symposium (HCS), DOI 10.1109/HOTCHIPS.2008.7476516; Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102; Chen DT, 2002, INT C PATT RECOG, P227, DOI 10.1109/ICPR.2002.1047438; Dai Y., 2017, ARXIV170903272; Deng D., 2017, 32 AAAI C ART INT; Deng J., 2012, ILSVRC 2012; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; He DF, 2017, PROC CVPR IEEE, P474, DOI 10.1109/CVPR.2017.58; He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331; He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87; Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529; Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157; Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33; ICDAR, 2017, RROB READ COMP; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Jiang F., 2017, ARXIV170805133; Jiang Y., 2017, ARXIV170609579; Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942; Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li YX, 2017, LECT NOTES COMPUT SC, V10361, P101, DOI 10.1007/978-3-319-63309-1_10; Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Liao MH, 2017, AAAI CONF ARTIF INTE, P4161; Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595; Liu Zichuan, 2018, CVPR; Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2; Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788; Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; Mishra A., 2012, SCENE TEXT RECOGNITI; Neumann L, 2016, IEEE T PATTERN ANAL, V38, P1872, DOI 10.1109/TPAMI.2015.2496234; Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097; Nister D, 2008, LECT NOTES COMPUT SC, V5303, P183, DOI 10.1007/978-3-540-88688-4_14; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Satuluri V., 2010, ACM, P247, DOI DOI 10.1145/1854776.1854812; Satuluri V, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P737; Semeniuta S., 2016, P COLING 2016 26 INT, P1757; Shaw Peter, 2018, P 2018 C N AM CHAPT, P464, DOI DOI 10.18653/V1/N18-2074; Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371; Shi CZ, 2013, PROC CVPR IEEE, P2961, DOI 10.1109/CVPR.2013.381; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Tian SX, 2015, IEEE I CONF COMP VIS, P4651, DOI 10.1109/ICCV.2015.528; Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4; Van Dongen SM., 2001, THESIS; Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43; Wang T, 2012, INT C PATT RECOG, P3304; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xue CH, 2018, LECT NOTES COMPUT SC, V11220, P370, DOI 10.1007/978-3-030-01270-0_22; Yao C., 2016, ARXIV160609002; Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813; Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787; Yuliang L., 2017, ARXIV PREPRINT ARXIV; Zamberletti A, 2015, LECT NOTES COMPUT SC, V9009, P91, DOI 10.1007/978-3-319-16631-5_7; Zhang S, 2018, AAAI CONF ARTIF INTE, P2612; Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871; Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283; Zhu YY, 2016, FRONT COMPUT SCI-CHI, V10, P19, DOI 10.1007/s11704-015-4488-0	64	3	3	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1786	1809		10.1007/s11263-020-01298-y	http://dx.doi.org/10.1007/s11263-020-01298-y		FEB 2020	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN					2022-12-18	WOS:000516006600001
J	Chen, YW; Tsai, YH; Lin, YY; Yang, MH				Chen, Yi-Wen; Tsai, Yi-Hsuan; Lin, Yen-Yu; Yang, Ming-Hsuan			VOSTR: Video Object Segmentation via Transferable Representations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video object segmentation; Transfer learning; Weakly-supervised learning		In order to learn video object segmentation models, conventional methods require a large amount of pixel-wise ground truth annotations. However, collecting such supervised data is time-consuming and labor-intensive. In this paper, we exploit existing annotations in source images and transfer such visual information to segment videos with unseen object categories. Without using any annotations in the target video, we propose a method to jointly mine useful segments and learn feature representations that better adapt to the target frames. The entire process is decomposed into three tasks: (1) refining the responses with fully-connected CRFs, (2) solving a submodular function for selecting object-like segments, and (3) learning a CNN model with a transferable module for adapting seen categories in the source domain to the unseen target video. We present an iterative update scheme between three tasks to self-learn the final solution for object segmentation. Experimental results on numerous benchmark datasets demonstrate that the proposed method performs favorably against the state-of-the-art algorithms.	[Chen, Yi-Wen; Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA USA; [Chen, Yi-Wen; Lin, Yen-Yu] Acad Sinica, Taipei, Taiwan; [Tsai, Yi-Hsuan] NEC Labs Amer, San Jose, CA USA; [Lin, Yen-Yu] Natl Chiao Tung Univ, Hsinchu, Taiwan; [Yang, Ming-Hsuan] Google, Mountain View, CA 94043 USA	University of California System; University of California Merced; Academia Sinica - Taiwan; NEC Corporation; National Yang Ming Chiao Tung University; Google Incorporated	Lin, YY (corresponding author), Acad Sinica, Taipei, Taiwan.; Lin, YY (corresponding author), Natl Chiao Tung Univ, Hsinchu, Taiwan.	ychen319@ucmerced.edu; ytsai@nec-labs.com; lin@cs.nctu.edu.tw; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304; Lin, Yen-Yu/0000-0002-7183-6070	Ministry of Science and Technology [MOST 107-2628-E-001-005-MY3, MOST 108-2634-F-007-009]	Ministry of Science and Technology(Ministry of Science, ICT & Future Planning, Republic of Korea)	Funding was provided by Ministry of Science and Technology (Grant Nos. MOST 107-2628-E-001-005-MY3 and MOST 108-2634-F-007-009).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2015, CVPR; [Anonymous], 2015, CVPR; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; CHEN LC, 2016, ARXIV 1606 00915 CS, V1606, P915, DOI DOI 10.1109/TPAMI.2017.2699184; Chen Y. W., 2018, ACCV; Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130; Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774; Cheng Jingchun, 2017, ICCV; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Gopalan R., 2011, P 2011 INT C COMP VI; Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hoffman Judy, 2014, NIPS; Hong S., 2016, CVPR; Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445; Irani M., 2014, BMVC; Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228; Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43; Khoreva A., 2017, CVPR; Koh YJ, 2017, PROC CVPR IEEE, P7417, DOI 10.1109/CVPR.2017.784; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Lazic N., 2009, ICCV; Lee Y. J., 2011, ICCV; Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273; Li S., 2018, P EUR C COMP VIS ECC, P207; Lim J.J., 2011, ADV NEURAL INFORM PR, P118; Liu Ce, 2009, THESIS, P2; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo Zelun, 2017, NIPS; Marki N., 2016, CVPR; Ochs P., 2011, ICCV; Oh Seoung Wug, 2018, P IEEE C COMP VIS PA; Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223; Patricia N., 2014, P IEEE C COMP VIS PA; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Perazzi F, 2015, IEEE I CONF COMP VIS, P3227, DOI 10.1109/ICCV.2015.369; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Saleh F. Sadat, 2017, ICCV; Shi ZY, 2017, IEEE T PATTERN ANAL, V39, P2525, DOI 10.1109/TPAMI.2016.2645157; Strand R, 2013, COMPUT VIS IMAGE UND, V117, P429, DOI 10.1016/j.cviu.2012.10.011; Tang K., 2013, CVPR; Taylor B., 2015, CVPR; Tokmakov P., 2017, CVPR; Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480; Tommasi T, 2014, IEEE T PATTERN ANAL, V36, P928, DOI 10.1109/TPAMI.2013.197; Tsai Y. H., 2016, P IEEE C COMP VIS PA; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Tsai Yi-Hsuan, 2016, ECCV; YAN Y, 2017, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2017.738; Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680; Zhang D., 2017, CVPR; Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165; Zhong G., 2016, ACCV; Zhu F., 2014, CVPR	59	3	3	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		931	949		10.1007/s11263-019-01224-x	http://dx.doi.org/10.1007/s11263-019-01224-x		FEB 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN					2022-12-18	WOS:000515808300003
J	Li, JN; Wong, YK; Zhao, Q; Kankanhalli, MS				Li, Junnan; Wong, Yongkang; Zhao, Qi; Kankanhalli, Mohan S.			Visual Social Relationship Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Social relationship; Label ambiguity; Context-driven analysis; Attention	PEOPLE	Social relationships form the basis of social structure of humans. Developing computational models to understand social relationships from visual data is essential for building intelligent machines that can better interact with humans in a social environment. In this work, we study the problem of visual social relationship recognition in images. We propose a dual-glance model for social relationship recognition, where the first glance fixates at the person of interest and the second glance deploys attention mechanism to exploit contextual cues. To enable this study, we curated a large scale People in Social Context dataset, which comprises of 23,311 images and 79,244 person pairs with annotated social relationships. Since visually identifying social relationship bears certain degree of uncertainty, we further propose an adaptive focal loss to leverage the ambiguous annotations for more effective learning. We conduct extensive experiments to quantitatively and qualitatively demonstrate the efficacy of our proposed method, which yields state-of-the-art performance on social relationship recognition.	[Li, Junnan] Natl Univ Singapore, Grad Sch Integrat Sci & Engn, Singapore 117456, Singapore; [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore; [Zhao, Qi] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	National University of Singapore; National University of Singapore; University of Minnesota System; University of Minnesota Twin Cities	Li, JN (corresponding author), Natl Univ Singapore, Grad Sch Integrat Sci & Engn, Singapore 117456, Singapore.	lijunnan@u.nus.edu; yongkang.wong@nus.edu.sg; qzhao@cs.umn.edu; mohan@comp.nus.edu.sg			National Research Foundation, Prime Minister's Office, Singapore under its International Research Centre in Singapore Funding Initiative	National Research Foundation, Prime Minister's Office, Singapore under its International Research Centre in Singapore Funding Initiative(National Research Foundation, Singapore)	This research was carried out at the NUS-ZJU SeSaMe Centre. It is supported by the National Research Foundation, Prime Minister's Office, Singapore under its International Research Centre in Singapore Funding Initiative.	Agrawal A., 2018, CVPR, P6904; Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Alameda-Pineda X, 2016, IEEE T PATTERN ANAL, V38, P1707, DOI 10.1109/TPAMI.2015.2496269; Alletto S, 2014, IEEE COMPUT SOC CONF, P594, DOI 10.1109/CVPRW.2014.91; Chen YY, 2012, P 20 ACM INT C MULT, P669, DOI DOI 10.1145/2393347.2393439; Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16; Chu X, 2015, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2015.383; CONTE HR, 1981, J PERS SOC PSYCHOL, V40, P701, DOI 10.1037/0022-3514.40.4.701; Deng ZW, 2016, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2016.516; Dibeklioglu H, 2013, IEEE I CONF COMP VIS, P1497, DOI 10.1109/ICCV.2013.189; Direkoglu C, 2012, LECT NOTES COMPUT SC, V7578, P69, DOI 10.1007/978-3-642-33786-4_6; Fan LF, 2018, PROC CVPR IEEE, P6460, DOI 10.1109/CVPR.2018.00676; Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590; FISKE AP, 1992, PSYCHOL REV, V99, P689, DOI 10.1037/0033-295X.99.4.689; Gallagher AC, 2009, PROC CVPR IEEE, P256, DOI 10.1109/CVPRW.2009.5206828; Gan T., 2013, P 21 ACM INT C MULTI, P937, DOI [DOI 10.1145/2502081.2502096, 10.1145/2502081.2502096]; Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Guo YH, 2014, INT C PATT RECOG, P4287, DOI 10.1109/ICPR.2014.735; Hall E. T., 1959, SILENT LANGUAGE, V948; HASLAM N, 1994, COGNITION, V53, P59, DOI 10.1016/0010-0277(94)90077-9; HASLAM N, 1992, J EXP SOC PSYCHOL, V28, P441, DOI 10.1016/0022-1031(92)90041-H; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hung H., 2007, ACM MULTIMEDIA, P835; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Lan T, 2012, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2012.6247821; Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228; Li JN, 2017, IEEE I CONF COMP VIS, P2669, DOI 10.1109/ICCV.2017.289; Li YB, 2017, INT CON DISTR COMP S, P1261, DOI 10.1109/ICDCS.2017.54; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lv JN, 2018, LECT NOTES COMPUT SC, V10704, P355, DOI 10.1007/978-3-319-73603-7_29; Marin-Jimenez MJ, 2014, INT J COMPUT VISION, V106, P282, DOI 10.1007/s11263-013-0655-7; Maron O, 1998, ADV NEUR IN, V10, P570; Orekondy T., 2017, ICCV; Qin Z, 2016, IEEE T PATTERN ANAL, V38, P2082, DOI 10.1109/TPAMI.2015.2505292; Ramanathan V, 2013, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2013.320; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rienks R, 2006, P INT C MULT INT, P257; Robicquet A, 2016, LECT NOTES COMPUT SC, V9912, P549, DOI 10.1007/978-3-319-46484-8_33; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salamin H, 2009, IEEE T MULTIMEDIA, V11, P1373, DOI 10.1109/TMM.2009.2030740; Shao M., 2014, HUMAN CTR SOCIAL MED, P175, DOI DOI 10.1007/978-3-319-05491-99; Shao M, 2013, IEEE I CONF COMP VIS, P3631, DOI 10.1109/ICCV.2013.451; Sun QR, 2017, PROC CVPR IEEE, P435, DOI 10.1109/CVPR.2017.54; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Vicol P, 2018, PROC CVPR IEEE, P8581, DOI 10.1109/CVPR.2018.00895; Vinciarelli A, 2012, IEEE T AFFECT COMPUT, V3, P69, DOI 10.1109/T-AFFC.2011.27; Wang G, 2010, LECT NOTES COMPUT SC, V6315, P169, DOI 10.1007/978-3-642-15555-0_13; Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436; Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Y, 2012, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR.2012.6248095; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yun Kiwon, 2012, 2012 IEEE COMP SOC C, P28; Zhang N, 2015, PROC CVPR IEEE, P4804, DOI 10.1109/CVPR.2015.7299113; Zhang ZP, 2015, IEEE I CONF COMP VIS, P3631, DOI 10.1109/ICCV.2015.414; Zhou JH, 2014, TRANSL ONCOL-HOBOKEN, P21	62	3	3	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1750	1764		10.1007/s11263-020-01295-1	http://dx.doi.org/10.1007/s11263-020-01295-1		FEB 2020	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	LQ3MN		Green Submitted			2022-12-18	WOS:000515808300002
J	Gallardo, M; Pizarro, D; Collins, T; Bartoli, A				Gallardo, Mathias; Pizarro, Daniel; Collins, Toby; Bartoli, Adrien			Shape-From-Template with Curves	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Curve reconstruction; Shape-from-Template; Isometry; Discrete Hidden Markov Model; Partial differential equations	RECONSTRUCTION; DEFORMATION; MODELS	Shape-from-Template (SfT) is the problem of using a shape template to infer the shape of a deformable object observed in an image. The usual case of SfT is 'Surface' SfT, where the shape is a 2D surface embedded in 3D, and the image is a 2D perspective projection. We introduce 'Curve' SfT, comprising two new cases of SfT where the shape is a 1D curve. The first new case is when the curve is embedded in 2D and the image a 1D perspective projection. The second new case is when the curve is embedded in 3D and the image a 2D perspective projection. We present a thorough theoretical study of these new cases for isometric deformations, which are a good approximation of ropes, cables and wires. Unlike Surface SfT, we show that Curve SfT is only ever solvable up to discrete ambiguities. We present the necessary and sufficient conditions for solvability with critical point analysis. We further show that unlike Surface SfT, Curve SfT cannot be solved locally using exact non-holonomic Partial Differential Equations. Our main technical contributions are two-fold. First, we give a stable, global reconstruction method that models the problem as a discrete Hidden Markov Model. This can generate all candidate solutions. Second, we give a non-convex refinement method using a novel angle-based deformation parameterization. We present quantitative and qualitative results showing that real curve shaped objects such as a necklace can be successfully reconstructed with Curve SfT.	[Gallardo, Mathias; Pizarro, Daniel; Collins, Toby; Bartoli, Adrien] Univ Clermont Auvergne, EnCoV, CNRS, UMR 6602,IP,SIGMA, Clermont Ferrand, France; [Pizarro, Daniel] Univ Alcala, Geintra Res Grp, Alcala De Henares, Spain; [Collins, Toby] IRCAD, Strasbourg, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite Clermont Auvergne (UCA); Universidad de Alcala	Gallardo, M (corresponding author), Univ Clermont Auvergne, EnCoV, CNRS, UMR 6602,IP,SIGMA, Clermont Ferrand, France.	mathias.gallardo@gmail.com; dani.pizarro@gmail.com; toby.collins@ircad.fr; adrien.bartoli@gmail.com		Pizarro, Daniel/0000-0003-0622-4884	EU's FP7 through the ERC research Grant [307483 FLEXABLE]	EU's FP7 through the ERC research Grant	We thank Armine Vardazaryan for her help in creating the simulated dataset 3D cord and the Conseil Departemental du Puy-de-Dome which allows us to acquire the ground-truth of the road dataset. We also thank Bastien Durix for his valuable discussions about the case of closed curvilinear templates. This research has received funding from the EU's FP7 through the ERC research Grant 307483 FLEXABLE.	Agisoft, 2013, AG LENS VERS 0 4 1 B; Agisoft, 2014, AG PHOTOSCAN VERS 1; Bartoli A., 2016, ISMAR; Bartoli A, 2015, IEEE T PATTERN ANAL, V37, P2099, DOI 10.1109/TPAMI.2015.2392759; Berthilsson R, 2001, INT J COMPUT VISION, V41, P171, DOI 10.1023/A:1011104020586; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blender, 2017, BLEND 2 78A; Brunet F, 2014, COMPUT VIS IMAGE UND, V125, P138, DOI 10.1016/j.cviu.2014.04.003; Casillas-Perez D., 2017, ARXIV171004265; Chhatkuli A, 2017, IEEE T PATTERN ANAL, V39, P833, DOI 10.1109/TPAMI.2016.2562622; Collins T., 2016, MICCAI; Collins T., 2015, ISMAR; Collins T., 2014, ECCV; Ngo DT, 2016, IEEE T PATTERN ANAL, V38, P172, DOI 10.1109/TPAMI.2015.2435739; David 3D Scanner, 2014, DAVID 3D SCANNER; Donnelly A, 2015, PEERJ, V3, DOI 10.7717/peerj.726; Eliashberg Y., 2002, NUMBER GRAD STUD MAT; FAUGERAS O, 1993, INT J COMPUT VISION, V10, P125, DOI 10.1007/BF01420734; Gallardo M., 2015, CVPR; Gonzalez IM, 2016, CHEM BIOL TECHNOL AG, V3, DOI 10.1186/s40538-016-0078-0; Haouchine N., 2014, ISMAR; Hartley R., 2003, MULTIPLE VIEW GEOMET; Kahl F., 2003, ICCV; Liu-Yin Q., 2016, BMVC; Lofberg J., 2004, INT S COMP AID CONTR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mai F., 2010, DIGITAL IMAGE COMPUT; Malti A., 2013, CVPR; Malti A., 2015, CVPR; Malti A, 2014, IEEE T BIO-MED ENG, V61, P1684, DOI 10.1109/TBME.2014.2300237; Malti A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Martinsson H., 2007, EMMCVPR; Ngo T. D., 2015, ICCV; Ozgur E, 2017, INT J COMPUT VISION, V123, P184, DOI 10.1007/s11263-016-0968-4; Parashar Shaifali, 2015, ICCV, P1; Perriollat M, 2011, INT J COMPUT VISION, V95, P124, DOI 10.1007/s11263-010-0352-8; Pizarro D, 2012, INT J COMPUT VISION, V97, P54, DOI 10.1007/s11263-011-0452-0; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Robert L., 1991, CVPR; Salzmann M., 2009, CVPR; Salzmann M, 2007, IEEE T PATTERN ANAL, V29, P1481, DOI 10.1109/TPAMI.2007.1080; Salzmann M, 2011, IEEE T PATTERN ANAL, V33, P931, DOI 10.1109/TPAMI.2010.158; Sbert C, 2003, J MATH IMAGING VIS, V18, P211, DOI 10.1023/A:1022821409482; Schmidt M., 2007, UGM MATLAB TOOLBOX P; Sorkine O., 2007, P 5 EUR S GEOM PROC, V4, P109, DOI [DOI 10.2312/SGP/SGP07/109-116, 10.2312/SGP/SGP07/109-116]; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; Vicente S., 2013, 3DV; Wu Changchang, 2011, VISUALSFM VISUAL STR, P1	48	3	3	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2020	128	1					121	165		10.1007/s11263-019-01214-z	http://dx.doi.org/10.1007/s11263-019-01214-z			45	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KI6WA					2022-12-18	WOS:000511490100006
J	Isogawa, M; Mikami, D; Takahashi, K; Iwai, D; Sato, K; Kimata, H				Isogawa, Mariko; Mikami, Dan; Takahashi, Kosuke; Iwai, Daisuke; Sato, Kosuke; Kimata, Hideaki			Which is the Better Inpainted Image?Training Data Generation Without Any Manual Operations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image inpainting; Image quality assessment (IQA); Learning to rank	IMAGE; COMPLETION	This paper proposes a learning-based quality evaluation framework for inpainted results that does not require any subjectively annotated training data. Image inpainting, which removes and restores unwanted regions in images, is widely acknowledged as a task whose results are quite difficult to evaluate objectively. Thus, existing learning-based image quality assessment (IQA) methods for inpainting require subjectively annotated data for training. However, subjective annotation requires huge cost and subjects' judgment occasionally differs from person to person in accordance with the judgment criteria. To overcome these difficulties, the proposed framework generates and uses simulated failure results of inpainted images whose subjective qualities are controlled as the training data. We also propose a masking method for generating training data towards fully automated training data generation. These approaches make it possible to successfully estimate better inpainted images, even though the task is quite subjective. To demonstrate the effectiveness of our approach, we test our algorithm with various datasets and show it outperforms existing IQA methods for inpainting.	[Isogawa, Mariko; Mikami, Dan; Takahashi, Kosuke; Kimata, Hideaki] NTT Media Intelligence Labs, Yokosuka, Kanagawa, Japan; [Isogawa, Mariko; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Grad Sch Engn Sci, Toyonaka, Osaka, Japan; [Mikami, Dan] NTT Commun Sci Labs, Atsugi, Kanagawa, Japan	Osaka University; Nippon Telegraph & Telephone Corporation	Isogawa, M (corresponding author), NTT Media Intelligence Labs, Yokosuka, Kanagawa, Japan.; Isogawa, M (corresponding author), Osaka Univ, Grad Sch Engn Sci, Toyonaka, Osaka, Japan.	mariko.isogawa.kt@hco.ntt.co.jp; dan.mikami.vp@hco.ntt.co.jp						Abe T, 2012, INT C PATT RECOG, P3712; Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718; Ardis P. A., 2009, P SOC PHOTO-OPT INS, V7257; Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261; Chang KY, 2015, IEEE T IMAGE PROCESS, V24, P785, DOI 10.1109/TIP.2014.2387379; Dang TT, 2013, IEEE IMAGE PROC, P398, DOI 10.1109/ICIP.2013.6738082; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Frantc VA, 2014, PROC SPIE, V9120, DOI 10.1117/12.2063664; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2014, IEEE T PATTERN ANAL, V36, P2423, DOI 10.1109/TPAMI.2014.2330611; Herbrich R, 2000, ADV NEUR IN, P115; Herling J, 2014, IEEE T VIS COMPUT GR, V20, P866, DOI 10.1109/TVCG.2014.2298016; Huang JB, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2601097.2601205, 10.1145/2602141]; Isogawa M., 2017, BRITISH MACHINE VISI; Isogawa M, 2017, MULTIMED TOOLS APPL, V76, P9443, DOI 10.1007/s11042-016-3550-8; Isogawa M, 2016, IEEE IMAGE PROC, P3538, DOI 10.1109/ICIP.2016.7533018; Khosla A, 2012, ADV NEURAL INFORM PR, P296; Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780; Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Oncu AI, 2012, LECT NOTES COMPUT SC, V7583, P561, DOI 10.1007/978-3-642-33863-2_58; Pishchulin L, 2012, PROC CVPR IEEE, P3178, DOI 10.1109/CVPR.2012.6248052; ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Venkatesh MV, 2010, IEEE IMAGE PROC, P1109, DOI 10.1109/ICIP.2010.5653640; Voronin V. V., 2015, P SOC PHOTO-OPT INS; Xu ZB, 2010, IEEE T IMAGE PROCESS, V19, P1153, DOI 10.1109/TIP.2010.2042098; Yan JZ, 2014, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2014.382; Yu J., 2018, IEEE CONFERENCE ON C; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009	34	3	3	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1751	1766		10.1007/s11263-018-1132-0	http://dx.doi.org/10.1007/s11263-018-1132-0			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY		hybrid			2022-12-18	WOS:000492425300011
J	Kim, TK; Zafeiriou, S; Glocker, B; Leutenegger, S				Kim, Tae-Kyun; Zafeiriou, Stefanos; Glocker, Ben; Leutenegger, Stefan			Special Issue on Machine Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Kim, Tae-Kyun] Imperial Coll London, Dept EEE, London, England; [Zafeiriou, Stefanos; Glocker, Ben; Leutenegger, Stefan] Imperial Coll London, Dept Comp, London, England	Imperial College London; Imperial College London	Kim, TK (corresponding author), Imperial Coll London, Dept EEE, London, England.	tk.kim@imperial.ac.uk; s.zafeiriou@imperial.ac.uk; b.glocker@imperial.ac.uk; s.leutenegger@imperial.ac.uk							0	3	3	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1611	1613		10.1007/s11263-019-01201-4	http://dx.doi.org/10.1007/s11263-019-01201-4			3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY		Bronze			2022-12-18	WOS:000492425300001
J	Jain, SD; Grauman, K				Jain, Suyog Dutt; Grauman, Kristen			Click Carving: Interactive Object Segmentation in Images and Videos with Point Clicks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Interactive segmentation; Image segmentation; Video segmentation; Point clicks; Hybrid human-computer system; Crowdsourcing		We present a novel form of interactive object segmentation called Click Carving which enables accurate segmentation of objects in images and videos with only a few point clicks. Whereas conventional interactive pipelines take the user's initialization as a starting point, we show the value in the system taking lead even in initialization. In particular, for a given image or a video frame, the system precomputes a ranked list of thousands of possible segmentation hypotheses (also referred to as object region proposals) using appearance and motion cues. Then, the user looks at the top ranked proposals, and clicks on the object boundary to carve away erroneous ones. This process iterates (typically 2-3 times), and each time the system revises the top ranked proposal set, until the user is satisfied with a resulting segmentation mask. In the case of images, this mask is considered as the final object segmentation. However in the case of videos, the object region proposals rely on motion as well, and the resulting segmentation mask in the first frame is further propagated across the video to obtain a complete spatio-temporal object tube. On six challenging image and video datasets, we provide extensive comparisons with both existing work and simpler alternative methods. In all, the proposed Click Carving approach strikes an excellent of accuracy and human effort. It outperforms all similarly fast methods, and is competitive or better than those requiring 2-12 times the effort.	[Jain, Suyog Dutt; Grauman, Kristen] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Jain, SD (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	suyogjain@utexas.edu; grauman@cs.utexas.edu			ONR PECASE [N00014-15-1-2291]; NSF [IIS-1514118]	ONR PECASE(Office of Naval Research); NSF(National Science Foundation (NSF))	This research is supported in part by ONR PECASE N00014-15-1-2291, NSF IIS-1514118, a gift from Qualcomm and a gift from AWS Machine Learning. We would like to thank Shankar Nagaraja for providing the iVideoseg dataset timing data. We also thank all the participants in our user studies.	Acuna D., 2018, EFFICIENT INTERACTIV; Arbelaez Pablo, 2014, CVPR; Badrinarayanan V., 2010, CVPR; Bai X., 2009, SIGGRAPH; Bai X., 2007, 2007 IEEE INT C IM P; Batra D., 2010, CVPR; Bearman A., 2015, ARXIV E PRINTS; Bell S., 2015, COMPUTER VISION PATT; Boykov Y., 2001, CVPR; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Castrejon L., 2017, CVPR; Chen LC., ARXIV 2015ABS1412706; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Cordts M., 2016, P IEEE C COMP VIS PA; Faktor A., 2014, P BRIT MACH VIS C; Fathi A., 2011, BMVC; Fragkiadaki Katerina, 2015, CVPR; Galasso F., 2013, ICCV; Godec M., 2011, ICCV; Grundmann M., 2010, CVPR; Gulshan Varun, 2010, CVPR; Jain S. D., 2016, AAAI C HUM COMP CROW; Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43; Jain Suyog Dutt, 2013, ICCV; Jiang B., 2013, ICCV; Karasev V., 2014, P IEEE C COMP VIS PA; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kohli P, 2012, INT J COMPUT VISION, V100, P261, DOI 10.1007/s11263-012-0537-4; Krahenbuhl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47; Krause A., 2007, NAT C ART INT AAAI N; Lee Y. J., 2011, ICCV; Lempitsky Victor, 2009, ICCV; Levinkov E, 2016, P 24 PAC C COMP GRAP, P33; Li F., 2013, ICCV; Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306; Li Y., 2014, CVPR; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Ma T., 2012, CVPR; Malisiewicz T., 2007, BMVC; Malmberg F., 2011, SCIA; McGuinness K, 2010, PATTERN RECOGN, V43, P434, DOI 10.1016/j.patcog.2009.03.008; Mortensen E. N., 1995, SIGGRAPH; Nickisch H., 2010, ICVGIP, P274; Noh H., 2015, 2015 IEEE INT C COMP; Oneata D., 2014, ECCV; Papadopoulos D. P., 2017, CVPR; Papazoglou A., 2013, ICCV; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Pinheiro P.O., 2015, NIPS; Pont-Tuset J, 2015, INT WORK CONTENT MUL; Ren X., 2007, CVPR; Rother C., 2004, SIGGRAPH; Russakovsky O., 2015, CVPR; ShankarNagaraja N., 2015, ICCV; Sundberg P., 2011, CVPR; Tsai D., 2010, BMVC; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Vijayanarasimhan S., 2012, ECCV; Vondrick Carl, 2011, NIPS; Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233; Wang TH, 2014, COMPUT VIS IMAGE UND, V120, P14, DOI 10.1016/j.cviu.2013.10.013; Weinzaepfel P., 2015, CVPR 2015; Wen L., 2015, CVPR; Wu Z., 2015, CVPR, V1, P2; Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47; Yu G., 2015, CVPR; Zhang D., 2013, CVPR; Zhao R., 2015, CVPR; Zheng Shuai, 2015, CONDITIONAL RANDOM F	71	3	3	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2019	127	9					1321	1344		10.1007/s11263-019-01184-2	http://dx.doi.org/10.1007/s11263-019-01184-2			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IL9YV					2022-12-18	WOS:000477642300008
J	Jung, J; Lee, JY; Kweon, IS				Jung, Jiyoung; Lee, Joon-Young; Kweon, In So			One-Day Outdoor Photometric Stereo Using Skylight Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Photometric stereo; Light modeling; Outdoor illumination; Scene reconstruction; Skylight estimation	ILLUMINATION; DEPTH	We present an outdoor photometric stereo method using images captured in a single day. We simulate a sky hemisphere for each image according to its GPS and timestamp and parameterize the obtained sky hemisphere into quadratic skylight and Gaussian sunlight distributions. Our previous work recovered an outdoor scene on a clear day, whereas the current paper shows that cloudy days can provide better illumination conditions for surface orientation recovery, and hence we propose a modified sky model to represent a well-conditioned skylight distribution for outdoor photometric stereo. The proposed method models natural illumination according to a sky model, providing sufficient constraints for shape reconstruction from 1-day images. We tested the proposed method to recover various sized objects and scenes from real-world outdoor daylight images and verified the method using synthetic and real data experiments.	[Jung, Jiyoung] Kyung Hee Univ, Dept Software Convergence, Yongin, South Korea; [Lee, Joon-Young] Adobe Res, San Jose, CA USA; [Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea	Kyung Hee University; Adobe Systems Inc.; Korea Advanced Institute of Science & Technology (KAIST)	Jung, J (corresponding author), Kyung Hee Univ, Dept Software Convergence, Yongin, South Korea.	jiyoung.jung@khu.ac.kr; jolee@adobe.com; iskweon77@kaist.ac.kr	Jung, Jiyoung/AAK-7269-2020	Jung, Jiyoung/0000-0001-9316-9750	Kyung Hee University [KHU-20170718]; National Research Foundation of Korea [NRF-2017R1C1B5075945]	Kyung Hee University; National Research Foundation of Korea(National Research Foundation of Korea)	This work was supported by a Grant from Kyung Hee University (KHU-20170718) and National Research Foundation of Korea (NRF-2017R1C1B5075945).	Abrams A., 2012, P EUR C COMP VIS ECC; Abrams A., 2013, ARXIV13044112; Abrams A., 2013, P IEEE C COMP VIS PA; Ackermann J., 2012, P IEEE C COMP VIS PA; Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; [Anonymous], 2004, P 3 INT C COMPUTER G; [Anonymous], 2015, GOOGL EARTH; [Anonymous], 2016, THESIS; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; GROSSMANN P, 1987, PATTERN RECOGN LETT, V5, P63, DOI 10.1016/0167-8655(87)90026-2; Han Y., 2013, P INT C COMP VIS ICC; Hoek-Hoek L, 2013, IEEE COMPUT GRAPH, V33, P44, DOI [10.1109/MCG.2013.18, DOI 10.1109/MCG.2013.18]; Hold-Geoffroy Y., 2017, P IEEE C COMP VIS PA; Hold-Geoffroy Y, 2015, IEEE INT CONF COMPUT; Hold-Geoffroy Yannick, 2015, INT C 3D VIS; Horn B. K., 1989, SHAPE SHADING, P123; Huang R., 2011, P INT C IM PROC ICIP; Hung C. H., 2015, P IEEE WINT C APPL C; Inose K., 2013, IPSJ T COMPUTER VISI, V5, P104; Jacobs N., 2007, P IEEE C COMP VIS PA; Johnson MK, 2011, PROC CVPR IEEE; Jung J., 2015, P IEEE C COMP VIS PA; Jung J, 2015, IEEE T PATTERN ANAL, V37, P1501, DOI 10.1109/TPAMI.2014.2363827; Kawakami R, 2013, INT J COMPUT VISION, V105, P187, DOI 10.1007/s11263-013-0632-1; KOENDERINK JJ, 1992, PERCEPT PSYCHOPHYS, V52, P487, DOI 10.3758/BF03206710; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Lalonde J.-F., 2016, LAVAL HDR SKY DATABA; Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8; Lalonde JF, 2010, INT J COMPUT VISION, V88, P24, DOI 10.1007/s11263-009-0291-4; Lee JY, 2013, IEEE T PATTERN ANAL, V35, P144, DOI 10.1109/TPAMI.2012.66; Lombardi S., 2012, P EUR C COMP VIS ECC; Lu F., 2013, P IEEE C COMP VIS PA; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; Oxholm G., 2012, P EUR C COMP VIS ECC; Preetham AJ, 1999, COMP GRAPH, P91, DOI 10.1145/311535.311545; Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448; Saff EB, 1997, MATH INTELL, V19, P5, DOI 10.1007/BF03024331; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Shan Q., 2013, P INT C 3D VIS; Shen FY, 2014, COMPUT GRAPH FORUM, V33, P359, DOI 10.1111/cgf.12504; Shi B., 2010, P IEEE C COMP VIS PA; SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103; SUBBARAO M, 1994, INT J COMPUT VISION, V13, P271, DOI 10.1007/BF02028349; Sunkavalli K., 2008, P IEEE C COMP VIS PA; Sunkavalli K, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239552, 10.1145/1276377.1276504]; Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70; Yu L-F, 2013, P IEEE C COMP VIS PA; Zhang Q., 2012, P IEEE C COMP VIS PA	50	3	3	2	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					1126	1142		10.1007/s11263-018-01145-1	http://dx.doi.org/10.1007/s11263-018-01145-1			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM					2022-12-18	WOS:000474559000009
J	Loy, CC; Liu, XM; Kim, TK; De la Torre, F; Chellappa, R				Loy, Chen Change; Liu, Xiaoming; Kim, Tae-Kyun; De la Torre, Fernando; Chellappa, Rama			Editorial: Special Issue on Deep Learning for Face Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Loy, Chen Change] Nanyang Technol Univ, Singapore, Singapore; [Liu, Xiaoming] Michigan State Univ, E Lansing, MI 48824 USA; [Kim, Tae-Kyun] Imperial Coll London, London, England; [De la Torre, Fernando] Facebook, Menlo Pk, CA USA; [Chellappa, Rama] Univ Maryland, College Pk, MD 20742 USA	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Michigan State University; Imperial College London; Facebook Inc; University System of Maryland; University of Maryland College Park	Loy, CC (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	ccloy@ntu.edu.sg; liuxm@cse.msu.edu; tk.kim@imperial.ac.uk; ftorre@cs.cmu.edu; rama@umiacs.umd.edu	Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/AAV-8690-2020						0	3	3	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		533	536		10.1007/s11263-019-01179-z	http://dx.doi.org/10.1007/s11263-019-01179-z			4	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD		Bronze			2022-12-18	WOS:000468525900001
J	Cai, Q; Wu, YX; Zhang, LL; Zhang, PK				Cai, Qi; Wu, Yuanxin; Zhang, Lilian; Zhang, Peike			Equivalent Constraints for Two-View Geometry: Pose Solution/Pure Rotation Identification and 3D Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Relative pose; Coplanar relationship; Same-side constraint; Intersection constraint; Pure rotation	MATRIX; MOTION	Two-view relative pose estimation and structure reconstruction is a classical problem in computer vision. The typical methods usually employ the singular value decomposition of the essential matrix to get multiple solutions of the relative pose, from which the right solution is picked out by reconstructing the three-dimension (3D) feature points and imposing the constraint of positive depth. This paper revisits the two-view geometry problem and discovers that the two-view imaging geometry is equivalently governed by a Pair of new Pose-Only (PPO) constraints: the same-side constraint and the intersection constraint. From the perspective of solving equation, the complete pose solutions of the essential matrix are explicitly derived and we rigorously prove that the orientation part of the pose can still be recovered in the case of pure rotation. The PPO constraints are simplified and formulated in the form of inequalities to directly identify the right pose solution with no need of 3D reconstruction and the 3D reconstruction can be analytically achieved from the identified right pose. Furthermore, the intersection inequality also enables a robust criterion for pure rotation identification. Experiment results validate the correctness of analyses and the robustness of the derived pose solution/pure rotation identification and analytical 3D reconstruction.	[Cai, Qi; Wu, Yuanxin; Zhang, Peike] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai Key Lab Nav & Locat Based Serv, Shanghai 200240, Peoples R China; [Cai, Qi; Zhang, Peike] Cent S Univ, Sch Aeronaut & Astronaut, Changsha 410083, Hunan, Peoples R China; [Zhang, Lilian] Natl Univ Def Technol, Coll Intelligent Sci & Technol, Changsha 410073, Hunan, Peoples R China	Shanghai Jiao Tong University; Central South University; National University of Defense Technology - China	Wu, YX (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai Key Lab Nav & Locat Based Serv, Shanghai 200240, Peoples R China.	qicaiCN@gmail.com; yuanx_wu@hotmail.com; lilianzhang@nudt.edu.cn; zhangpeike@csu.edu.cn	Wu, Yuanxin/B-3272-2009	Wu, Yuanxin/0000-0001-6941-615X	National Natural Science Foundation of China [61422311, 61673263, 61503403]; Hunan Provincial Natural Science Foundation of China [2015JJ1021]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Hunan Provincial Natural Science Foundation of China(Natural Science Foundation of Hunan Province)	Thanks to anonymous reviewers for their constructive comments and Dr. Danping Zou for group talks. The work is funded by National Natural Science Foundation of China (61422311, 61673263, 61503403) and Hunan Provincial Natural Science Foundation of China (2015JJ1021).	Bazin JC, 2010, COMPUT VIS IMAGE UND, V114, P254, DOI 10.1016/j.cviu.2009.04.006; BEARDSLEY PA, 1995, PROCEEDINGS OF EUROPE-CHINA WORKSHOP ON GEOMETRICAL MODELING & INVARIANTS FOR COMPUTER VISION, P214; FAUGERAS OD, 1990, INT J COMPUT VISION, V4, P225, DOI 10.1007/BF00054997; Ferraz L, 2014, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2014.71; Garro V, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P262, DOI 10.1109/3DIMPVT.2012.40; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hartley R. I., 1992, ECCV 92 P 2 EUR C CO; Hartley R. I, 1995, INVESTIGATION ESSENT; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P1310, DOI 10.1109/34.41368; HUANG TS, 1988, LECT NOTES COMPUT SC, V301, P439; Kneip L, 2012, LECT NOTES COMPUT SC, V7577, P696, DOI 10.1007/978-3-642-33783-3_50; Ling L., 2011, P COMP SEM WEEK 23 I, P1; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Ma Y., 2004, INVITATION 3 D VISIO; MAYBANK S, 1993, THEORY RECONSTRUCTIO; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Stewenius H, 2006, ISPRS J PHOTOGRAMM, V60, P284, DOI 10.1016/j.isprsjprs.2006.03.005; Vieville T., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P207; Wang W, 2000, INT C PATT RECOG, P362, DOI 10.1109/ICPR.2000.905353; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561	23	3	4	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2019	127	2					163	180		10.1007/s11263-018-1136-9	http://dx.doi.org/10.1007/s11263-018-1136-9			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HL5LJ		Green Submitted			2022-12-18	WOS:000458768000003
J	Yu, H; Siskind, JM				Yu, Haonan; Siskind, Jeffrey Mark			Sentence Directed Video Object Codiscovery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video; Object codiscovery; Sentences	GRADIENTS	Video object codiscovery can leverage the weak semantic constraint implied by sentences that describe the video content. Our codiscovery method, like other object codetection techniques, does not employ any pretrained object models or detectors. Unlike most prior work that focuses on codetecting large objects which are usually salient both in size and appearance, our method can discover small or medium sized objects as well as ones that may be occluded for part of the video. More importantly, our method can codiscover multiple object instances of different classes within a single video clip. Although the semantic information employed is usually simple and weak, it can greatly boost performance by constraining the hypothesized object locations. Experiments show promising results on three datasets: an average IoU score of 0.423 on a new dataset with 15 object classes, an average IoU score of 0.373 on a subset of CAD120 with 5 object classes, and an average IoU score of 0.358 on a subset of MPII-Cooking with 7 object classes. Our result on this subset of MPII-Cooking improves upon those of the previous state-of-the-art methods by significant margins.	[Yu, Haonan] Baidu Res Inst Deep Learning, Sunnyvale, CA 94089 USA; [Siskind, Jeffrey Mark] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA	Baidu; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Yu, H (corresponding author), Baidu Res Inst Deep Learning, Sunnyvale, CA 94089 USA.	haonanu@gmail.com; qobi@purdue.edu			Army Research Laboratory [W911NF-10-2-0060]; National Science Foundation [1522954-IIS]; Direct For Computer & Info Scie & Enginr [1522954] Funding Source: National Science Foundation	Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL)); National Science Foundation(National Science Foundation (NSF)); Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This research was sponsored, in part, by the Army Research Laboratory, accomplished under Cooperative Agreement Number W911NF-10-2-0060, and by the National Science Foundation under Grant No. 1522954-IIS. The views, opinions, findings, conclusions, and recommendations contained in this document are those of the authors and should not be interpreted as representing the official policies, either express or implied, of the Army Research Laboratory, the National Science Foundation, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226; Andres B., 2012, 12060111 CORR; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Barbu A., 2012, ARXIV12042742, P102; Berg TL, 2004, PROC CVPR IEEE, P848; Blaschko M., 2010, ADV NEURAL INFORM PR; Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507; Bosch A, 2007, IEEE I CONF COMP VIS, P1863; Bradski Gary R, 1998, COMPUTER VISION FACE, P1; Bylinskii Z., 2012, MIT SALIENCY BENCHMA; Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414; Cinbis RG, 2014, PROC CVPR IEEE, P2409, DOI 10.1109/CVPR.2014.309; Clarke James, 2010, P 14 C COMP NAT LANG, P18; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Das P, 2013, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2013.340; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Farneback G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Graves A., 2014, 14105401 CORR; Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337; Gupta A, 2008, LECT NOTES COMPUT SC, V5302, P16, DOI 10.1007/978-3-540-88682-2_3; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jamieson M, 2010, LECT NOTES COMPUT SC, V6315, P183, DOI 10.1007/978-3-642-15555-0_14; Jamieson M, 2010, IEEE T PATTERN ANAL, V32, P148, DOI 10.1109/TPAMI.2008.283; JIANG M, 2015, PROC CVPR IEEE, P1072, DOI DOI 10.1109/CVPR.2015.7298710; Joulin A, 2014, LECT NOTES COMPUT SC, V8694, P253, DOI 10.1007/978-3-319-10599-4_17; Kong C, 2014, PROC CVPR IEEE, P3558, DOI 10.1109/CVPR.2014.455; Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Kwak S, 2015, IEEE I CONF COMP VIS, P3173, DOI 10.1109/ICCV.2015.363; Lee YJ, 2011, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2011.5995523; Lin DH, 2014, PROC CVPR IEEE, P2657, DOI 10.1109/CVPR.2014.340; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo J., 2009, ADV NEURAL INFORM PR, P1168; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557; Palmer M, 2005, COMPUT LINGUIST, V31, P71, DOI 10.1162/0891201053630264; Pearl J., 1982, AAAI 82 P 2 AAAI C A, P133; Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Ramanathan V, 2014, LECT NOTES COMPUT SC, V8689, P95, DOI 10.1007/978-3-319-10590-1_7; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940; Rohrbach A, 2014, LECT NOTES COMPUT SC, V8753, P184, DOI 10.1007/978-3-319-11752-2_15; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253; Schulter S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.53; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Socher R., 2013, LONG PAPERS, V1, P455; Srikantha A, 2017, COMPUT VIS IMAGE UND, V156, P138, DOI 10.1016/j.cviu.2016.09.006; Srikantha A, 2014, LECT NOTES COMPUT SC, V8694, P415, DOI 10.1007/978-3-319-10599-4_27; Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190; Torabi A., 2015, 150301070 CORR; Tuytelaars T, 2010, INT J COMPUT VISION, V88, P284, DOI 10.1007/s11263-009-0271-8; Venugopalan S., 2015, P C N AM CHAPT ASS C, P1494, DOI 10.3115/v1/N15-1173; Wang L, 2014, LECT NOTES COMPUT SC, V8692, P640, DOI 10.1007/978-3-319-10593-2_42; Wong Yuk Wah, 2007, P 45 ANN M ASS COMP, P960; Xiao FY, 2016, PROC CVPR IEEE, P933, DOI 10.1109/CVPR.2016.107; Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496; Yu H, 2015, J ARTIF INTELL RES, V52, P601, DOI 10.1613/jair.4556; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	65	3	3	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	3					312	334		10.1007/s11263-017-1018-6	http://dx.doi.org/10.1007/s11263-017-1018-6			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FE1ER		hybrid			2022-12-18	WOS:000407961700004
J	Meng, GF; Xiang, SM; Pan, CH; Zheng, NN				Meng, Gaofeng; Xiang, Shiming; Pan, Chunhong; Zheng, Nanning			Active Rectification of Curved Document Images Using Structured Beams	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Document image processing; Document dewarping; Geometric correction; Photometric correction; Structured light	SHADING CORRECTION; PRINTED MATERIALS; RESTORATION; SURFACES; SHAPE; ILLUMINATION	We propose an active method to rectify the geometric and photometric distortions in a document image. This method uses structured beams projected upon the curved document page to recover the spatial curves on it. A developable surface is interpolated to the curves by finding the correspondence between them. To correct the geometric distortion, the estimated developable surface is finally flattened onto a plane by explicitly solving a system of ordinary differential equations. The recovered 3D page shape, together with a Lambertian illumination model as guidance, is further used to rectify the non-uniform illumination in the images. Experimental results based on a variety of synthetic and real-captured document images demonstrate the effectiveness and efficiency of the proposed method.	[Meng, Gaofeng; Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Xi'an Jiaotong University	Meng, GF (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.	gfmeng@nlpr.ia.ac.cn; smxiang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn; nnzheng@mail.xjtu.edu.cn			National Natural Science Foundation of China [61370039, 61272331]; Beijing Nature Science Foundation [4162064]; National 863 projects [2015AA042307]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nature Science Foundation(Beijing Natural Science Foundation); National 863 projects(National High Technology Research and Development Program of China)	Part of this work was done during the visit of Dr. Gaofeng Meng to Delft University of Technology. We are grateful to Dr. Wei Sui for his help on the use of VisualSFM. This work was supported in part by the National Natural Science Foundation of China (Grant No. 61370039, 61272331), the Beijing Nature Science Foundation (Grant No. 4162064) and the National 863 projects (Grant No. 2015AA042307).	Bartoli A, 2010, INT J COMPUT VISION, V88, P85, DOI 10.1007/s11263-009-0303-4; Basu S, 2007, PATTERN RECOGN, V40, P1825, DOI 10.1016/j.patcog.2006.10.002; Blinn James F., 1977, COMPUT GRAPHICS-US, V11, P192, DOI [DOI 10.1145/965141.563893, 10.1145/965141, DOI 10.1145/965141]; Brown MS, 2007, IEEE T PATTERN ANAL, V29, P1904, DOI 10.1109/TPAMI.2007.1118; Brown MS, 2006, IEEE T IMAGE PROCESS, V15, P1544, DOI 10.1109/TIP.2006.871082; Brown MS, 2005, PROC CVPR IEEE, P998; Brown MS, 2004, IEEE T PATTERN ANAL, V26, P1295, DOI 10.1109/TPAMI.2004.87; Brown MS, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P367, DOI 10.1109/ICCV.2001.937649; Bukhari S. S., 2009, P INT C IM PROC, P1993; Cao HG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P228, DOI 10.1109/ICCV.2003.1238346; Cook R., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293; Demartines P, 1997, IEEE T NEURAL NETWOR, V8, P148, DOI 10.1109/72.554199; Dijkstra EW, 1959, NUMER MATH, V1, P269, DOI 10.1007/BF01386390; Doermann D., 2003, P INT C DOC AN REC, V1; Gatos B, 2006, PATTERN RECOGN, V39, P317, DOI 10.1016/j.patcog.2005.09.010; Gumerov N, 2004, LECT NOTES COMPUT SC, V3023, P482; Gumerov NA, 2006, INT J COMPUT VISION, V66, P261, DOI 10.1007/s11263-005-3678-x; He Y, 2013, PROC INT CONF DOC, P403, DOI 10.1109/ICDAR.2013.88; Hsia SC, 2006, IEEE T IMAGE PROCESS, V15, P2719, DOI 10.1109/TIP.2006.877354; Il Koo H, 2010, LECT NOTES COMPUT SC, V6312, P421; Jian Fan, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P561, DOI 10.1109/ICDAR.2009.111; Kim C, 2014, LECT NOTES COMPUT SC, V8357, P101, DOI 10.1007/978-3-319-05167-3_8; Koo HI, 2009, IEEE T IMAGE PROCESS, V18, P1551, DOI 10.1109/TIP.2009.2019301; Lee JS, 2009, IEEE T CIRC SYST VID, V19, P898, DOI 10.1109/TCSVT.2009.2017314; Liang J., 2005, International Journal on Document Analysis and Recognition, V7, P84, DOI 10.1007/s10032-004-0138-z; Liang J, 2005, PROC CVPR IEEE, P338; Liang J, 2008, IEEE T PATTERN ANAL, V30, P591, DOI 10.1109/TPAMI.2007.70724; Lu SJ, 2006, IMAGE VISION COMPUT, V24, P837, DOI 10.1016/j.imavis.2006.02.008; Meng G., 2014, P COMP VIS PATT REC, P1; Meng GF, 2013, IEEE T PATTERN ANAL, V35, P1730, DOI 10.1109/TPAMI.2012.251; Meng GF, 2012, IEEE T PATTERN ANAL, V34, P707, DOI 10.1109/TPAMI.2011.151; Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857; Perriollat M, 2013, COMPUT ANIMAT VIRT W, V24, P457, DOI 10.1002/cav.1478; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Pilu M, 2001, PROC CVPR IEEE, P67; Schneider DC, 2007, PROC INT CONF DOC, P113; Stamatopoulos N, 2011, IEEE T IMAGE PROCESS, V20, P910, DOI 10.1109/TIP.2010.2080280; Sun M, 1996, PROC GRAPH INTERF, P176; Tan CL, 2006, IEEE T PATTERN ANAL, V28, P195, DOI 10.1109/TPAMI.2006.40; Tian YD, 2011, PROC CVPR IEEE, P377, DOI 10.1109/CVPR.2011.5995540; TRIER OD, 1995, IEEE T PATTERN ANAL, V17, P312, DOI 10.1109/34.368197; Tsoi Yau-Chat, 2007, P IEEE C COMP VIS PA, P1; Tsoi YC, 2004, PROC CVPR IEEE, P240; Ulges A, 2005, PROC INT CONF DOC, P1001, DOI 10.1109/ICDAR.2005.90; Ulges A., 2004, P ACM S DOC ENG, P198; Wada T, 1997, INT J COMPUT VISION, V24, P125, DOI 10.1023/A:1007906904009; Watanabe Y., 2012, P AS C COMP VIS, P394; Zhang L, 2008, IEEE T PATTERN ANAL, V30, P728, DOI 10.1109/TPAMI.2007.70831; Zhang L, 2009, PATTERN RECOGN, V42, P2961, DOI 10.1016/j.patcog.2009.03.025; Zhang WF, 2007, PR IEEE COMP DESIGN, P10; Zhang Z, 2004, PROC CVPR IEEE, P10; Zhang Z, 2003, PROC INT CONF DOC, P589; Zhang Z, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P977, DOI 10.1109/ICIP.2002.1039138; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	54	3	4	1	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2017	122	1					34	60		10.1007/s11263-016-0952-z	http://dx.doi.org/10.1007/s11263-016-0952-z			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EL2AF					2022-12-18	WOS:000394421800003
J	Perez-Sala, X; De la Torre, F; Igual, L; Escalera, S; Angulo, C				Perez-Sala, Xavier; De la Torre, Fernando; Igual, Laura; Escalera, Sergio; Angulo, Cecilio			Subspace Procrustes Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Procrustes analysis; Learning 2D shape models; Functional subspace learning	MOTION; MODELS; SHAPE	Procrustes analysis (PA) has been a popular technique to align and build 2-D statistical models of shapes. Given a set of 2-D shapes PA is applied to remove rigid transformations. Later, a non-rigid 2-D model is computed by modeling the residual (e.g., PCA). Although PA has been widely used, it has several limitations for modeling 2-D shapes: occluded landmarks and missing data can result in local minima solutions, and there is no guarantee that the 2-D shapes provide a uniform sampling of the 3-D space of rotations for the object. To address previous issues, this paper proposes subspace PA (SPA). Given several instances of a 3-D object, SPA computes the mean and a 2-D subspace that can model rigid and non-rigid deformations of the 3-D object. We propose a discrete (DSPA) and continuous (CSPA) formulation for SPA, assuming that 3-D samples of an object are provided. DSPA extends the traditional PA, and produces unbiased 2-D models by uniformly sampling different views of the 3-D object. CSPA provides a continuous approach to uniformly sample the space of 3-D rotations, being more efficient in space and time. We illustrate the benefits of SPA in two different applications. First, SPA is used to learn 2-D face and body models from 3-D datasets. Experiments on the FaceWarehouse and CMU motion capture (MoCap) datasets show the benefits of our 2-D models against the state-of-the-art PA approaches and conventional 3-D models. Second, SPA learns an unbiased 2-D model from CMU MoCap dataset and it is used to estimate the human pose on the Leeds Sports dataset.	[Perez-Sala, Xavier] Fundacio Privada St Antoni Abat, Vilanova I La Geltru 08800, Spain; [Perez-Sala, Xavier; De la Torre, Fernando] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Perez-Sala, Xavier; Angulo, Cecilio] Univ Politecn Cataluna, Vilanova I La Geltru 08800, Spain; [Igual, Laura; Escalera, Sergio] Univ Barcelona, Dept Math & Comp Sci, E-08007 Barcelona, Spain; [Perez-Sala, Xavier; Igual, Laura; Escalera, Sergio] Univ Autonoma Barcelona, Comp Vis Ctr, Bellaterra, Spain	Carnegie Mellon University; Universitat Politecnica de Catalunya; University of Barcelona; Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	Perez-Sala, X (corresponding author), Fundacio Privada St Antoni Abat, Vilanova I La Geltru 08800, Spain.; Perez-Sala, X (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.; Perez-Sala, X (corresponding author), Univ Politecn Cataluna, Vilanova I La Geltru 08800, Spain.; Perez-Sala, X (corresponding author), Univ Autonoma Barcelona, Comp Vis Ctr, Bellaterra, Spain.	xavier.perez-sala@upc.edu	Escalera, Sergio/L-2998-2015; Igual, Laura/H-5606-2015; Angulo, Cecilio/A-7953-2010	Escalera, Sergio/0000-0003-0617-8873; Igual, Laura/0000-0002-7225-7441; Angulo, Cecilio/0000-0001-9589-8199	Spanish Ministry of Science and Innovation [TIN2012-38416-C03-01, TIN2015-66951-C2-1-R, TIN2013-43478-P]; SUR, Departament d'Economia i Coneixement [2014 SGR 1219]; Comissionat per a Universitats i Recerca del Departament d'Innovacio, Universitats i Empresa de la Generalitat de Catalunya	Spanish Ministry of Science and Innovation(Ministry of Science and Innovation, Spain (MICINN)Spanish Government); SUR, Departament d'Economia i Coneixement; Comissionat per a Universitats i Recerca del Departament d'Innovacio, Universitats i Empresa de la Generalitat de Catalunya(Generalitat de Catalunya)	This work is partly supported by the Spanish Ministry of Science and Innovation (Projects TIN2012-38416-C03-01, TIN2015-66951-C2-1-R, TIN2013-43478-P), Project 2014 SGR 1219, SUR, Departament d'Economia i Coneixement, and Comissionat per a Universitats i Recerca del Departament d'Innovacio, Universitats i Empresa de la Generalitat de Catalunya.	Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754; Bartoli A, 2013, INT J COMPUT VISION, V101, P227, DOI 10.1007/s11263-012-0565-0; Brand M, 2001, PROC CVPR IEEE, P456; Cao C., 2013, IEEE T VISUALIZATION, V1, P99; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cootes TF., 2004, STAT MODELS APPEARAN; De la Torre F, 2003, COMPUT VIS IMAGE UND, V91, P53, DOI 10.1016/S1077-3142(03)00076-6; De la Torre F, 2012, IEEE T PATTERN ANAL, V34, P1041, DOI 10.1109/TPAMI.2011.184; Dryden I. L., 1998, STAT SHAPE ANAL, V4; Frey BJ, 2003, IEEE T PATTERN ANAL, V25, P1, DOI 10.1109/TPAMI.2003.1159942; GOODALL C, 1991, J ROY STAT SOC B MET, V53, P285, DOI 10.1111/j.2517-6161.1991.tb01825.x; Gower J. C., 2004, PROCRUSTES PROBLEMS, V3; Hartley R., 2003, MULTIPLE VIEW GEOMET; Igual L, 2014, PATTERN RECOGN, V47, P659, DOI 10.1016/j.patcog.2013.08.006; Jiang H, 2007, IEEE T PATTERN ANAL, V29, P959, DOI 10.1109/TPAMI.2007.1048; Johnson S., 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12.CITESEER]; Kokkinos I, 2007, IEEE I CONF COMP VIS, P282; Korte B., 1991, GREEDOIDS 1991 ALGOR; Learned-Miller EG, 2006, IEEE T PATTERN ANAL, V28, P236, DOI 10.1109/TPAMI.2006.34; Li HS, 2013, IEEE T PATTERN ANAL, V35, P411, DOI 10.1109/TPAMI.2012.99; MARIMONT DH, 1992, J OPT SOC AM A, V9, P1905, DOI 10.1364/JOSAA.9.001905; Marques M, 2009, IEEE I CONF COMP VIS, P1288, DOI 10.1109/ICCV.2009.5459318; Matthews I, 2007, INT J COMPUT VISION, V75, P93, DOI 10.1007/s11263-007-0043-2; Minka T., 2000, OLD NEW MATRIX ALGEB; Naimark M. A., 1964, LINEAR NEPRESENTATIV; Park D, 2011, IEEE I CONF COMP VIS, P2627, DOI 10.1109/ICCV.2011.6126552; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Perez-Sala X., 2014, ECCV WORKSH CHALEARN; Pishchulin L, 2013, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2013.433; Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82; Pizarro D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2409, DOI 10.1109/CVPR.2011.5995677; Roig Gemma, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P200, DOI 10.1109/ICCVW.2009.5457698; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9; Yang F., 2012, P GRAPH INT, P93; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Yezzi AJ, 2003, INT J COMPUT VISION, V53, P153, DOI 10.1023/A:1023048024042; Zhou F, 2013, IEEE I CONF COMP VIS, P1025, DOI 10.1109/ICCV.2013.131	38	3	3	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2017	121	3					327	343		10.1007/s11263-016-0938-x	http://dx.doi.org/10.1007/s11263-016-0938-x			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK9UY		Green Submitted, Green Accepted			2022-12-18	WOS:000394270600001
J	Rodner, E; Freytag, A; Bodesheim, P; Frohlich, B; Denzler, J				Rodner, Erik; Freytag, Alexander; Bodesheim, Paul; Froehlich, Bjoern; Denzler, Joachim			Large-Scale Gaussian Process Inference with Generalized Histogram Intersection Kernels for Visual Recognition Tasks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Large-scale learning; Gaussian processes; Hyperparameter optimization; Visual recognition		We present new methods for fast Gaussian process (GP) inference in large-scale scenarios including exact multi-class classification with label regression, hyperparameter optimization, and uncertainty prediction. In contrast to previous approaches, we use a full Gaussian process model without sparse approximation techniques. Our methods are based on exploiting generalized histogram intersection kernels and their fast kernel multiplications. We empirically validate the suitability of our techniques in a wide range of scenarios with tens of thousands of examples. Whereas plain GP models are intractable due to both memory consumption and computation time in these settings, our results show that exact inference can indeed be done efficiently. In consequence, we enable every important piece of the Gaussian process framework-learning, inference, hyperparameter optimization, variance estimation, and online learning-to be used in realistic scenarios with more than a handful of data.	[Rodner, Erik; Freytag, Alexander; Denzler, Joachim] Friedrich Schiller Univ Jena, Jena, Germany; [Rodner, Erik; Freytag, Alexander; Denzler, Joachim] Michael Stifel Ctr Jena, Jena, Germany; [Bodesheim, Paul] Max Planck Inst Biogeochem, Jena, Germany; [Froehlich, Bjoern] Daimler AG Res & Dev, Boblingen, Germany	Friedrich Schiller University of Jena; Max Planck Society	Rodner, E (corresponding author), Friedrich Schiller Univ Jena, Jena, Germany.; Rodner, E (corresponding author), Michael Stifel Ctr Jena, Jena, Germany.	Erik.Rodner@uni-jena.de	Bodesheim, Paul/AAX-3821-2021	Bodesheim, Paul/0000-0002-3564-6528	German Research Foundation (DFG) [DE 735/10-1]	German Research Foundation (DFG)(German Research Foundation (DFG))	This research was partially supported by grant DE 735/10-1 of the German Research Foundation (DFG).	Ablavsky V, 2011, IEEE I CONF COMP VIS, P1473, DOI 10.1109/ICCV.2011.6126404; Bai Z., 1997, ANN NUMERICAL MATH, V4, P29; Barla A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P513; Berg A., 2010, LARGE SCALE VISUAL R; Bo L., 2008, UNCERTAINTY ARTIFICI; Bo L., 2012, ABS12063238 CORR; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Bonilla EV., 2008, ADV NEURAL INF PROCE, V20, P153, DOI DOI 10.5555/2981562.2981582; Bottou L., 2007, LARGE SCALE KERNEL M; Boughorbel S, 2005, IEEE IMAGE PROC, P2629; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6; Donahue J, 2013, P 31 INT C MACH LEAR; Ebert S, 2012, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2012.6248108; Freytag A., 2014, INT C PATT REC ICPR; Freytag A., 2012, AS C COMP VIS ACCV, P511; Freytag A, 2014, LECT NOTES COMPUT SC, V8692, P562, DOI 10.1007/978-3-319-10593-2_37; Freytag A, 2013, LECT NOTES COMPUT SC, V8142, P282, DOI 10.1007/978-3-642-40602-7_31; Freytag A, 2012, INT C PATT RECOG, P3313; Grauman K, 2007, J MACH LEARN RES, V8, P725; He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713; HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044; Kading C, 2015, PROC CVPR IEEE, P4343, DOI 10.1109/CVPR.2015.7299063; Kemmler Michael, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P489, DOI 10.1007/978-3-642-19309-5_38; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Maji S, 2008, PROC CVPR IEEE, P2245; Maji S, 2013, IEEE T PATTERN ANAL, V35, P66, DOI 10.1109/TPAMI.2012.62; NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308; Nickisch H, 2008, J MACH LEARN RES, V9, P2035; Nocedal J, 2006, CONJUGATE GRADIENT M; Perronnin F, 2012, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2012.6248090; Pillonetto G, 2010, IEEE T PATTERN ANAL, V32, P193, DOI 10.1109/TPAMI.2008.297; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rodner E., 2011, LEARNING FEW EXAMPLE; Rodner E., 2010, IM VIS COMP NZ IVCNZ, P1; Rodner E, 2012, LECT NOTES COMPUT SC, V7575, P85, DOI 10.1007/978-3-642-33765-9_7; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Scholkopf B., 2001, LEARNING KERNELS SUP; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Sun J., 2013, IEEE INT C COMP VIS; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Urtasun R, 2008, PROC CVPR IEEE, P149; Vapnik V. N., 1998, STAT LEARNING THEORY, V2; Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163; Vedaldi A, 2010, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2010.5539949; Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183; Wang G, 2012, IEEE T PATTERN ANAL, V34, P2177, DOI 10.1109/TPAMI.2012.29; Williams C., 2000, INT C MACH LEARN; Wu JX, 2012, IEEE T IMAGE PROCESS, V21, P4442, DOI 10.1109/TIP.2012.2207392; Wu JX, 2010, LECT NOTES COMPUT SC, V6312, P552; Yuan Q, 2008, CHINA PET PROCESS PE, P1; Yuster R, 2008, ANN IEEE SYMP FOUND, P137, DOI 10.1109/FOCS.2008.14	56	3	3	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	2					253	280		10.1007/s11263-016-0929-y	http://dx.doi.org/10.1007/s11263-016-0929-y			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK2LI					2022-12-18	WOS:000393758400004
J	Zhao, B; Xing, EP				Zhao, Bin; Xing, Eric P.			Sparse Output Coding for Scalable Visual Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scalable classification; Output coding; Probabilistic decoding; Object recognition; Scene recognition	IMAGE CLASSIFICATION; MULTICLASS	Many vision tasks require a multi-class classifier to discriminate multiple categories, on the order of hundreds or thousands. In this paper, we propose sparse output coding, a principled way for large-scale multi-class classification, by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem. Specifically, sparse output coding is composed of two steps: efficient coding matrix learning with scalability to thousands of classes, and probabilistic decoding. Empirical results on object recognition and scene classification demonstrate the effectiveness of our proposed approach.	[Zhao, Bin; Xing, Eric P.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Zhao, B (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	binzhao@andrew.cmu.edu; epxing@cs.cmu.edu						Allwein EL, 2001, J MACH LEARN RES, V1, P113, DOI 10.1162/15324430152733133; Bakker B, 2004, J MACH LEARN RES, V4, P83, DOI 10.1162/153244304322765658; Bengio Samy, 2010, ADV NEURAL INFORM PR, V1, P163, DOI [10.5555/2997189.2997208, DOI 10.5555/2997189.2997208]; Bergamo A., 2012, COMPUTER VISION PATT; Beygelzimer A., 2009, ALT; Beygelzimer A., 2009, P 25 C UNC ART INT, P51; Binder A, 2011, INT J COMPUT VISION, P1; Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Budanitsky A, 2006, COMPUT LINGUIST, V32, P13, DOI 10.1162/coli.2006.32.1.13; Cai Lijuan, 2004, P 13 ACM INT C INFOR, P78, DOI DOI 10.1145/1031171.1031186; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Dekel O., 2004, ICML, P27; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng Jia, 2011, ADV NEURAL INFORM PR, V1, P567, DOI [10.5555/2986459.2986523, DOI 10.5555/2986459.2986523]; Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Escalera S, 2010, IEEE T PATTERN ANAL, V32, P120, DOI 10.1109/TPAMI.2008.266; Farhadi Ali, 2009, CVPR; Fei-Fei Li, 2004, CVPR WORKSH GEN MOD, P178; Fergus R, 2010, LECT NOTES COMPUT SC, V6311, P762, DOI 10.1007/978-3-642-15549-9_55; Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1; Gao T., 2011, P 28 INT C MACH LEAR; Gao Tianshi, 2011, INT C COMP VIS; Griffin Gregory, 2007, CALTECH 256 OBJECT C; Haussler D., 1999, CONVOLUTION KERNELS; Hsu D., 2009, P 22 INT C NEURAL IN, V22, P772; Jacob L, 2008, ADV NEURAL INFORM PR; Jia D, 2011, PROC CVPR IEEE, P785, DOI 10.1109/CVPR.2011.5995516; Koller Daphne, 1997, ICML; Kosmopoulos A, 2010, ACM SIGIR FORUM, V44, P23; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Le Quoc V., 2012, P ICML; Li L.-J., 2010, NEURAL INFORM PROCES, P1378; Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Nilsson N., 1965, LEARNING MACHINES; Parsana M., 2007, ADV NEURAL INFORM PR; Passerini A, 2004, IEEE T NEURAL NETWOR, V15, P45, DOI 10.1109/TNN.2003.820841; Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998; Poczos Barnabas, 2011, UAI; Pujol O, 2006, IEEE T PATTERN ANAL, V28, P1007, DOI 10.1109/TPAMI.2006.116; Rastegari M, 2012, LECT NOTES COMPUT SC, V7577, P876, DOI 10.1007/978-3-642-33783-3_63; Rifkin R, 2004, J MACH LEARN RES, V5, P101; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Schapire R., 1997, ICML; Schapire RE, 2012, ADAPT COMPUT MACH LE, P1; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56; Wang G., 2009, IEEE 12 INT C COMP V; Weinberger K., 2008, ADV NEURAL INFORM PR; Wen Z., 2012, MATH PROGRAM, P1; Weston Jason, 2011, 22 INT JOINT C ART I; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Zhang X., 2009, 12 INT C COMP VIS IC; Zhang Y., 2012, ICML; Zhao B., 2013, CVPR; Zhou D., 2011, ICML; Zhu X, 2003, INT C MACH LEARN, P912	62	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2016	119	1					60	75		10.1007/s11263-015-0839-4	http://dx.doi.org/10.1007/s11263-015-0839-4			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DP9AP					2022-12-18	WOS:000378789400005
J	Desolneux, A				Desolneux, Agnes			When the a contrario approach becomes generative	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Detection theory; Non-accidentalness principle; Maximum entropy distributions; Clusters of points; Line segments detection; Image reconstruction; Visual information theory	RANDOM-FIELDS; PRINCIPLE; RECOGNITION; FRAMEWORK	The a contrario approach is a statistical, hypothesis testing based approach to detect geometric meaningful events in images. The general methodology consists in computing the probability of an observed geometric event under a noise model (null hypothesis) H-0 and then declare the event meaningful when this probability is small enough. Generally, the noise model is taken to be the independent uniform distribution on the considered elements. Our aim in this paper will be to question the choice of the noise model: What happens if we "enrich" the noise model? How to characterize the noise models such that there are no meaningful events against them? Among them, what is the one that has maximum entropy? What does a sample of it look like? How is this noise model related to probability distributions on the elements that would produce, with high probability, the same detections? All these questions will be formalized and answered in two different frameworks: the detection of clusters in a set of points and the detection of line segments in an image. The general idea is to capture the perceptual information contained in an image, and then generate new images having the same visual content. We believe that such a generative approach can have applications for instance in image compression or for clutter removal.	[Desolneux, Agnes] Ecole Normale Super, CNRS, 61 Ave President Wilson, F-94235 Cachan, France; [Desolneux, Agnes] Ecole Normale Super, CMLA, 61 Ave President Wilson, F-94235 Cachan, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; UDICE-French Research Universities; Universite Paris Saclay	Desolneux, A (corresponding author), Ecole Normale Super, CNRS, 61 Ave President Wilson, F-94235 Cachan, France.; Desolneux, A (corresponding author), Ecole Normale Super, CMLA, 61 Ave President Wilson, F-94235 Cachan, France.	agnes.desolneux@cmla.ens-cachan.fr						Abraham I, 2007, PATTERN RECOGN, V40, P3277, DOI 10.1016/j.patcog.2007.02.015; [Anonymous], 1985, PERCEPTUAL ORG VISUA; [Anonymous], 2010, PATTERN THEORY STOCH; AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423; Cao F., 2004, Computing and Visualization in Science, V7, P3, DOI 10.1007/s00791-004-0123-6; Cao F, 2007, J MATH IMAGING VIS, V27, P91, DOI 10.1007/s10851-006-9176-0; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Delon J, 2007, INVERSE PROBL IMAG, V1, P265; Delon J, 2007, IEEE T IMAGE PROCESS, V16, P253, DOI 10.1109/TIP.2006.884951; Desolneux A, 2003, J PHYSIOL-PARIS, V97, P311, DOI 10.1016/j.jphysparis.2003.09.006; Desolneux A, 2003, ANN STAT, V31, P1822; Desolneux A, 2003, IEEE T PATTERN ANAL, V25, P508, DOI 10.1109/TPAMI.2003.1190576; Desolneux A, 2001, J MATH IMAGING VIS, V14, P271, DOI 10.1023/A:1011290230196; Desolneux A, 2000, INT J COMPUT VISION, V40, P7, DOI 10.1023/A:1026593302236; Desolneux A., 2008, GESTALT THEORY IMAGE, V1st ed.; Grosjean B, 2009, J MATH IMAGING VIS, V33, P313, DOI 10.1007/s10851-008-0111-4; Harremoes P, 2001, IEEE T INFORM THEORY, V47, P2039, DOI 10.1109/18.930936; Igual L, 2007, INVERSE PROBL IMAG, V1, P319; Kaas R, 1980, STAT NEERL, V34, P13, DOI DOI 10.1111/J.1467-9574.1980.TB00681.X; Kato H, 2014, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2014.127; LOWE DG, 1990, AI AND THE EYE, P261; Moisan L, 2004, INT J COMPUT VISION, V57, P201, DOI 10.1023/B:VISI.0000013094.38752.54; Muse P, 2006, INT J COMPUT VISION, V69, P295, DOI 10.1007/s11263-006-7546-0; Myaskouvskey A, 2013, INT J COMPUT VISION, V101, P22, DOI 10.1007/s11263-012-0543-6; PAYTON M, 1989, METRIKA, V36, P347, DOI DOI 10.1007/BF02614111; Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269; Tenenbaum Jay M, 1983, HUMAN MACHINE VISION, P481; Veit T, 2006, INT J COMPUT VISION, V68, P163, DOI 10.1007/s11263-006-6661-2; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; VONGIOI RG, 2012, IMAGE PROCESS ON LIN, V0002, P00035, DOI [10.5201/ipol.2012.gjmr-lsd, DOI 10.5201/IP0L.2012.GJMR-LSD]; WATERHOUSE WC, 1983, AM MATH MON, V90, P378, DOI 10.2307/2975573; Weinzaepfel P, 2011, PROC CVPR IEEE, P337, DOI 10.1109/CVPR.2011.5995616; Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1170, DOI 10.1109/34.809110; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zhu SC, 1997, NEURAL COMPUT, V9, P1627, DOI 10.1162/neco.1997.9.8.1627; [No title captured]; [No title captured]; [No title captured]	38	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2016	116	1					46	65		10.1007/s11263-015-0825-x	http://dx.doi.org/10.1007/s11263-015-0825-x			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7TE					2022-12-18	WOS:000369422500003
J	Aftab, K; Hartley, R; Trumpf, J				Aftab, Khurrum; Hartley, Richard; Trumpf, Jochen			L-q-Closest-Point to Affine Subspaces Using the Generalized Weiszfeld Algorithm	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						L-q Weiszfeld algorithm; Affine subspaces; Triangulation; L-q mean	HERON PROBLEM; CONVERGENCE	This paper presents a method for finding an -closest-point to a set of affine subspaces, that is a point for which the sum of the q-th power of orthogonal distances to all the subspaces is minimized, where . We give a theoretical proof for the convergence of the proposed algorithm to a unique minimum. The proposed method is motivated by the Weiszfeld algorithm, an extremely simple and rapid averaging algorithm, that finds the mean of a set of given points in a Euclidean space. The proposed algorithm is applied to the triangulation problem in computer vision by finding the -closest-point to a set of lines in 3D. Our experimental results for the triangulation problem confirm that the -closest-point method, for , is more robust to outliers than the -closest-point method.	[Aftab, Khurrum; Hartley, Richard] Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT, Australia; [Aftab, Khurrum; Hartley, Richard] Natl ICT Australia, Canberra, ACT, Australia; [Trumpf, Jochen] Australian Natl Univ, Res Sch Engn, Canberra, ACT, Australia	Australian National University; NICTA; Australian National University	Aftab, K (corresponding author), Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT, Australia.	khurrum.aftab@anu.edu.au; richard.hartley@anu.edu.au; jochen.trumpf@anu.edu.au		Hartley, Richard/0000-0002-5005-0191	National ICT Australia; Australian Government	National ICT Australia; Australian Government(Australian GovernmentCGIAR)	This research has been funded by National ICT Australia. National ICT Australia is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.	Aftab K., GEN WEISZFELD UNPUB; Ameri B., 2000, AUTOMATIC 3D BUILDIN; BRIMBERG J, 1993, OPER RES, V41, P1153, DOI 10.1287/opre.41.6.1153; Brimberg J, 1998, COMPUT MATH APPL, V35, P25, DOI 10.1016/S0898-1221(98)00054-6; Brimberg J., 2003, YUGOSL J OPER RES, V13, P199; Chartrand R., 2008, IEEE INT C AC SPEECH; Chi EC, 2014, AM MATH MON, V121, P95, DOI 10.4169/amer.math.monthly.121.02.095; Daubechies I, 2008, 42 ANN C INF SCI SYS; Dick A.R., 2001, IEEE INT C COMP VIS; ECKHARDT U, 1980, MATH PROGRAM, V18, P186, DOI 10.1007/BF01588313; Eldar YC, 2009, IEEE T INFORM THEORY, V55, P5302, DOI 10.1109/TIT.2009.2030471; Fletcher PT, 2009, NEUROIMAGE, V45, pS143, DOI 10.1016/j.neuroimage.2008.10.052; Furukawa Y., 2009, IEEE C COMP VIS PATT; Furukawa Y, 2009, IEEE I CONF COMP VIS, P80, DOI 10.1109/ICCV.2009.5459145; Hartley R., 2011, IEEE C COMP VIS PATT; Hartley R., 2004, ROBOTICA; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Henry P., 2010, 12 INT S EXP ROB ISE; KLEIMAN SL, 1972, AM MATH MON, V79, P1061, DOI 10.2307/2317421; Lourakis MIA, 2009, ACM T MATH SOFTWARE, V36, DOI 10.1145/1486525.1486527; Luenberger D.G., 2003, LINEAR NONLINEAR PRO; Ma R., 2004, THESIS OHIO STATE U; Mordukhovich B, 2011, J OPTIMIZ THEORY APP, V148, P431, DOI 10.1007/s10957-010-9761-7; Mordukhovich BS, 2012, AM MATH MON, V119, P87, DOI 10.4169/amer.math.monthly.119.02.087; Muller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]; Pu S, 2009, ISPRS J PHOTOGRAMM, V64, P575, DOI 10.1016/j.isprsjprs.2009.04.001; Remondino F, 2006, PHOTOGRAMM REC, V21, P269, DOI 10.1111/j.1477-9730.2006.00383.x; Schindler K., 2003, 1 IEEE INT WORKSH HI; Semple J., 1979, ALGEBRAIC PROJECTIVE; TAILLENDIER F., 2005, INT ARCH PHOTOGRAM 3, V36, P105; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; Vanegas C.A., 2010, IEEE C COMP VIS PATT; Weiszfeld E, 1937, TOHOKU MATH J, V43, P2; Werner T., 2003, EUR C COMP VIS; WILCZKOWIAK M, 2003, 9 IEEE INT C COMP VI; Yang L, 2010, LMS J COMPUT MATH, V13, P461, DOI 10.1112/S1461157020090531	37	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2015	114	1					1	15		10.1007/s11263-014-0791-8	http://dx.doi.org/10.1007/s11263-014-0791-8			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CO1UF					2022-12-18	WOS:000358940300001
J	Burghardt, T; Damen, D; Mayol-Cuevas, W; Mirmehdi, M				Burghardt, Tilo; Damen, Dima; Mayol-Cuevas, Walterio; Mirmehdi, Majid			Correspondence, Matching and Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Burghardt, Tilo; Damen, Dima; Mayol-Cuevas, Walterio; Mirmehdi, Majid] Univ Bristol, Dept Comp Sci, MVB, Bristol BS8 1UB, Avon, England	University of Bristol	Burghardt, T (corresponding author), Univ Bristol, Dept Comp Sci, MVB, Woodland Rd, Bristol BS8 1UB, Avon, England.	tb2935@bristol.ac.uk; dima.damen@bristol.ac.uk; wmayol@cs.bris.ac.uk; M.Mirmehdi@cs.bris.ac.uk	Mayol-Cuevas, Walterio/AAD-6590-2019	Mayol-Cuevas, Walterio/0000-0001-8973-1931; Damen, Dima/0000-0001-8804-6238; Mirmehdi, Majid/0000-0002-6478-1403	Engineering and Physical Sciences Research Council [EP/K031910/1] Funding Source: researchfish; EPSRC [EP/K031910/1] Funding Source: UKRI	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))			0	3	4	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2015	113	3			SI		161	162		10.1007/s11263-015-0827-8	http://dx.doi.org/10.1007/s11263-015-0827-8			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CK3BZ		Bronze			2022-12-18	WOS:000356091900001
J	Penne, R; Ribbens, B; Mertens, L				Penne, Rudi; Ribbens, Bart; Mertens, Luc			An Incremental Procedure for the Lateral Calibration of a Time-of-Flight Camera by One Image of a Flat Surface	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Time-of-Flight camera; Lateral calibration; Best line fit		We present a simple and accurate procedure to calibrate the pinhole parameters of a Time-of-Flight camera: the principal point , the focal length and, if needed, the aspect ratio . Only one image of a flat surface is needed. Using the radial distances as provided by the Time-of-Flight principle, we reconstruct the pixel rows (or pixel columns) as collinear points in 3-space. Motivated by theoretical results, we claim that the correct values for , and can be found by an incremental procedure. In case of unknown aspect ratio, some (but few) iterations are needed.	[Penne, Rudi; Ribbens, Bart; Mertens, Luc] Univ Antwerp, Fac Appl Engn, B-2660 Hoboken, Belgium; [Penne, Rudi] Univ Antwerp, Dept Math, B-2020 Antwerp, Belgium; [Ribbens, Bart] VUB, Dept Mech Engn, Brussels, Belgium; [Ribbens, Bart] Univ Antwerp, Lab BioMed Phys, B-2020 Antwerp, Belgium	University of Antwerp; Vrije Universiteit Brussel; University of Antwerp	Penne, R (corresponding author), Univ Antwerp, Fac Appl Engn, Salesianenlaan 90, B-2660 Hoboken, Belgium.	rudi.penne@uantwerpen.be		Ribbens, Bart/0000-0001-5882-2531				Araujo H., CALIBRATING IN PRESS; Beder Christian, 2008, International Journal of Intelligent Systems Technologies and Applications, V5, P285, DOI 10.1504/IJISTA.2008.021291; Gander W., 1997, SOLVING PROBLEMS SCI, V3rd ed., P135; Grossberg MD, 2005, INT J COMPUT VISION, V61, P119, DOI 10.1023/B:VISI.0000043754.56350.10; Hanning T, 2011, INFORM FUSION, V12, P37, DOI 10.1016/j.inffus.2010.01.006; Lindner Marvin, 2008, International Journal of Intelligent Systems Technologies and Applications, V5, P344, DOI 10.1504/IJISTA.2008.021297; Lindner M, 2006, LECT NOTES COMPUT SC, V4292, P524; Mertens L., 2013, ENG TOOLS TECHNIQUES, P353; Penne R, 2008, B BELG MATH SOC-SIM, V15, P127, DOI 10.36045/bbms/1203692451; Penne R, 2013, LECT NOTES COMPUT SC, V8192, P286, DOI 10.1007/978-3-319-02895-8_26; Schiller I., 2008, P 21 ISPRS C; SPATH H, 1986, NUMER MATH, V48, P441, DOI 10.1007/BF01389650; Zhang Z., 1999, P 7 IEEE INT C COMP, V1, P666, DOI DOI 10.1109/ICCV.1999.791289	13	3	3	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2015	113	2					81	91		10.1007/s11263-014-0768-7	http://dx.doi.org/10.1007/s11263-014-0768-7			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CI1FE		Green Accepted			2022-12-18	WOS:000354487300001
J	Dekel, T; Moses, Y; Avidan, S				Dekel (Basha), Tali; Moses, Yael; Avidan, Shai			Photo Sequencing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								A group of people taking pictures of a dynamic event with their mobile phones is a popular sight. The set of still images obtained this way is rich in dynamic content but lacks accurate temporal information. We propose a method for photo-sequencing-temporally ordering a set of still images taken asynchronously by a set of uncalibrated cameras. Photo-sequencing is an essential tool in analyzing (or visualizing) a dynamic scene captured by still images. The first step of the method detects sets of corresponding static and dynamic feature points across images. The static features are used to determine the epipolar geometry between pairs of images, and each dynamic feature votes for the temporal order of the images in which it appears. The partial orders provided by the dynamic features are not necessarily consistent, and we use rank aggregation to combine them into a globally consistent temporal order of images. We demonstrate successful photo-sequencing on several challenging collections of images taken using a number of mobile phones.	[Dekel (Basha), Tali; Avidan, Shai] Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel; [Moses, Yael] Interdisciplinary Ctr, Efi Arazi Sch Comp Sci, IL-46150 Herzliyya, Israel	Tel Aviv University; Reichman University	Dekel, T (corresponding author), Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel.	talib@eng.tau.ac.il			Israel Science Foundation [1556/10, 930/12]; European Community [PIRG05-GA-2009-248527]	Israel Science Foundation(Israel Science Foundation); European Community(European Commission)	This work was supported in part by Israel Science Foundation Grant No. 1556/10 and 930/12, and European Community Grant PIRG05-GA-2009-248527.	Avidan S, 2000, IEEE T PATTERN ANAL, V22, P348, DOI 10.1109/34.845377; Ballan L., 2010, ACM T GRAPHIC, V29, P115; BARTOLI A, 2008, P IEEE C COMP VIS PA; Caspi Y, 2002, IEEE T PATTERN ANAL, V24, P1409, DOI 10.1109/TPAMI.2002.1046148; Dexter E., 2009, P BRIT MACH VIS C BM; Dwork C., 2001, INT C WORLD WID WEB; Elena R. M., 2003, P 2003 ACM S APPL CO; Goshen L., 2006, P EUR C COMP VIS ECC; HaCohen Y., 2011, ACM T GRAPHICS SIGGR; Jegou H, 2010, IEEE T PATTERN ANAL, V32, P2, DOI 10.1109/TPAMI.2008.285; Kaminski JY, 2004, J MATH IMAGING VIS, V21, P27, DOI 10.1023/B:JMIV.0000026555.79056.b8; Lei C, 2006, IEEE T IMAGE PROCESS, V15, P2473, DOI 10.1109/TIP.2006.877438; Llado X., 2005, P BRIT MACH VIS C BM; Meyer B., 2008, P BRIT MACH VIS C BM, P103; Moses Y., 2012, P EUR C COMP VIS ECC; Padua FLC, 2010, IEEE T PATTERN ANAL, V32, P304, DOI 10.1109/TPAMI.2008.301; Park H., 2010, P EUR C COMP VIS ECC; Pedronette D. C. G., 2011, IM PROC ICIP 2011 18; Pundik D., 2010, P EUR C COMP VIS ECC; Sand P, 2004, ACM T GRAPHIC, V23, P592, DOI 10.1145/1015706.1015765; Schalekamp F, 2009, P 11 ALENEX; Schindler G, 2010, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2010.5539803; SHASHUA A, 2000, P EUR C COMP VIS ECC; Shili L., 2010, RANK AGGREGATION MET; Snavely N., 2010, IEEE SPECIAL ISSUE I; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Tresadern P., 2003, P BRIT MACH VIS C NO P BRIT MACH VIS C NO, VVolume 2, P629; Whitehead A., 2005, WMVC; WOLF L, 2002, P WORKSH VIS MOD DYN; Yan J., 2004, ACIVS; YOUNG HP, 1988, AM POLIT SCI REV, V82, P1231, DOI 10.2307/1961757; YOUNG HP, 1978, SIAM J APPL MATH, V35, P285, DOI 10.1137/0135023; Zelnik-Manor L., 2003, P IEEE C COMP VIS PA	33	3	3	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2014	110	3			SI		275	289		10.1007/s11263-014-0712-x	http://dx.doi.org/10.1007/s11263-014-0712-x			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AT2HL					2022-12-18	WOS:000344754500004
J	Jegelka, S; Kapoor, A; Horvitz, E				Jegelka, Stefanie; Kapoor, Ashish; Horvitz, Eric			An Interactive Approach to Solving Correspondence Problems	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human interaction; Active learning; Value of information; Matching; Correspondence problems	ALGORITHMS	Finding correspondences among objects in different images is a critical problem in computer vision. Even good correspondence procedures can fail, however, when faced with deformations, occlusions, and differences in lighting and zoom levels across images. We present a methodology for augmenting correspondence matching algorithms with a means for triaging the focus of attention and effort in assisting the automated matching. For guiding the mix of human and automated initiatives, we introduce a measure of the expected value of resolving correspondence uncertainties. We explore the value of the approach with experiments on benchmark data.	[Jegelka, Stefanie] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Kapoor, Ashish; Horvitz, Eric] Microsoft Res Redmond, Redmond, WA USA	University of California System; University of California Berkeley; Microsoft	Jegelka, S (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	stefje@eecs.berkeley.edu; akapoor@microsoft.com; horvitz@microsoft.com						Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28; CHEGIREDDY CR, 1987, DISCRETE APPL MATH, V18, P155, DOI 10.1016/0166-218X(87)90017-5; Chli M, 2008, LECT NOTES COMPUT SC, V5302, P72, DOI 10.1007/978-3-540-88682-2_7; Cho Y., 2010, EUR C COMP VIS ECCV; Chvatal V., 1979, Mathematics of Operations Research, V4, P233, DOI 10.1287/moor.4.3.233; Cour Timothee, 2006, ADV NEURAL INFORM PR, DOI DOI 10.7551/MITPRESS/7503.003.0044; Dasgupta S., 2004, ADV NEURAL INFORM PR; Debevec P., 1996, COMP GRAPH SIGGRAPH; Duchenne O., 2009, IEEE C COMP VIS PATT; Escolano F, 2011, PROC CVPR IEEE; Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534; Goodrich MT, 1999, IEEE T PATTERN ANAL, V21, P371, DOI 10.1109/34.761267; Handa A., 2010, IEEE C COMP VIS PATT; HECKERMAN DE, 1992, METHOD INFORM MED, V31, P90; HOWARD RA, 1967, IEEE T SYST SCI CYB, VSSC3, P54, DOI 10.1109/TSSC.1967.300108; Joshi Ajay J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2372, DOI 10.1109/CVPRW.2009.5206627; Kamar E., 2013, INT C AUT AG MULT SY; Kamar Ece, 2012, 11 INT C AUTONOMOUS, P467; Kapoor A., 2007, INT JOINT C ART INT; Kapoor A, 2010, INT J COMPUT VISION, V88, P169, DOI 10.1007/s11263-009-0268-3; Kowdle A., 2011, IEEE C COMP VIS PATT; Krause A, 2008, J MACH LEARN RES, V9, P235; Lawrence N., 2002, ADV NEURAL INFORM PR, V15; Lee J, 2011, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2011.5995387; Lordeanu M., 2005, INT C COMP VIS ICCV; Lovasz L, 1993, BOLYAI MATH STUD, V1, P9; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Maji S., 2012, 4 WORKSH HUM COMP AA; Mateus D., 2008, IEEE C COMP VIS PATT; McAuley JJ, 2008, IEEE T PATTERN ANAL, V30, P2047, DOI 10.1109/TPAMI.2008.124; Julian JM, 2012, PATTERN RECOGN, V45, P563, DOI 10.1016/j.patcog.2011.05.008; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350; Starck J., 2007, INT C COMP VIS ICCV; Tong S., 2000, INT C MACH LEARN ICM; Torresani L, 2008, LECT NOTES COMPUT SC, V5303, P596, DOI 10.1007/978-3-540-88688-4_44; UMEYAMA S, 1988, IEEE T PATTERN ANAL, V10, P695, DOI 10.1109/34.6778; Vijayanarasimhan S., 2010, IEEE C COMP VIS PATT; Vijayanarasimhan S., 2011, THESIS UT AUSTIN; von Ahn L., 2004, CHI SIGCHI C HUM FAC; Zass R, 2008, PROC CVPR IEEE, P1221	41	3	3	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2014	108	1-2			SI		49	58		10.1007/s11263-013-0657-5	http://dx.doi.org/10.1007/s11263-013-0657-5			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AG7BT					2022-12-18	WOS:000335573700004
J	Gunther, A; Lamecker, H; Weiser, M				Guenther, Andreas; Lamecker, Hans; Weiser, Martin			Flexible Shape Matching with Finite Element Based LDDMM	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Large deformation; Diffeomorphic registration; Matching; Currents; Adaptive finite elements	FLOWS	The Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework acting on currents is a conceptually powerful tool for matching highly varying shapes. In the classical approach, the numerical treatment is based on currents representing individual particles, and couples the discretization of shape and deformation. This design restricts the capabilities of LDDMM. In this work, we propose to decouple current and deformation discretization by using conforming adaptive finite elements. We show how to efficiently (a) compute the temporal evolution of discrete -current attributes for any , and (b) incorporate multiple scales into the matching process. This effectively leads to more flexibility, which is demonstrated in several numerical experiments on anatomical shapes.	[Guenther, Andreas; Lamecker, Hans; Weiser, Martin] Zuse Inst, D-14195 Berlin, Germany	Zuse Institute Berlin	Weiser, M (corresponding author), Zuse Inst, Takustr 7, D-14195 Berlin, Germany.	weiser@zib.de	Weiser, Martin/AAX-6574-2021	Weiser, Martin/0000-0002-1071-0044	German DFG Research Center MATHEON, Project F2	German DFG Research Center MATHEON, Project F2(German Research Foundation (DFG))	This work was supported by the German DFG Research Center MATHEON, Project F2. We thank Stanley Durrleman from the University of Utah for the fruitful discussion and helpful suggestions at the initial phase of this paper. Furthermore, we thank Malik Kirchner for implementing parts of the required tools.	Adams R. A., 2003, SOBOLEV SPACES, VSecond; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Burger M., 2011, HYPERELASTIC REGULAR; Camion V., 2001, GEODESIC INTERPOLATI; Cao Y, 2005, IEEE T MED IMAGING, V24, P1216, DOI 10.1109/TMI.2005.853923; Ciarlet P.G., 1987, HDB NUMERICAL ANAL; Cotter CJ, 2008, J PHYS A-MATH THEOR, V41, DOI 10.1088/1751-8113/41/34/344003; Deuflhard P., 2012, ADAPTIVE NUMERICAL S; Dupuis P, 1998, Q APPL MATH, V56, P587, DOI 10.1090/qam/1632326; Durrleman S, 2009, MED IMAGE ANAL, V13, P793, DOI 10.1016/j.media.2009.07.007; Durrleman Stanley, 2010, THESIS U NICE SOPHIA; Federer H., 1969, GEOMETRIC MEASURE TH, V153; Glaunes J, 2004, PROC CVPR IEEE, P712; Glaunes J., 2005, THESIS U PARIS PARIS; Glaunes J, 2008, INT J COMPUT VISION, V80, P317, DOI 10.1007/s11263-008-0141-9; Gunther A., 2011, P 3 MICCAI WORKSH MA, P1; Haber E, 2008, SIAM J SCI COMPUT, V30, P3012, DOI 10.1137/070687724; Joshi SC, 2000, IEEE T IMAGE PROCESS, V9, P1357, DOI 10.1109/83.855431; Kirk BS, 2006, ENG COMPUT-GERMANY, V22, P237, DOI 10.1007/s00366-006-0049-3; Marsland S, 2004, IEEE T MED IMAGING, V23, P1006, DOI 10.1109/TMI.2004.831228; Mattheij R. M., 2002, CLASSICS APPL MATH; Morgan F., 2016, GEOMETRIC MEASURE TH; Morita S., 2001, GEOMETRY DIFFERENTIA; Risser L, 2010, LECT NOTES COMPUT SC, V6362, P610; Sommer S, 2011, LECT NOTES COMPUT SC, V6801, P624, DOI 10.1007/978-3-642-22092-0_51; Trouve A., 1995, TECHNICAL REPORT; Vaillant M., 2005, INFORM PROCESSING ME, P1, DOI DOI 10.1007/11505730_; Younes L., 2010, APPL MATH SCI, V171, DOI [10.1007/978-3-642-12055-8, DOI 10.1007/978-3-642-12055-8]	28	3	3	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2013	105	2			SI		128	143		10.1007/s11263-012-0599-3	http://dx.doi.org/10.1007/s11263-012-0599-3			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	207YP					2022-12-18	WOS:000323641100003
J	Sun, M; Bao, SY; Savarese, S				Sun, Min; Bao, Sid Yingze; Savarese, Silvio			Object Detection using Geometrical Context Feedback	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene understanding; Object recognition; Object detection; Focal length estimation; 3D reconstruction; Surface estimation; Viewpoint estimation		We propose a new coherent framework for joint object detection, 3D layout estimation, and object supporting region segmentation from a single image. Our approach is based on the mutual interactions among three novel modules: (i) object detector; (ii) scene 3D layout estimator; (iii) object supporting region segmenter. The interactions between such modules capture the contextual geometrical relationship between objects, the physical space including these objects, and the observer. An important property of our algorithm is that the object detector module is capable of adaptively changing its confidence in establishing whether a certain region of interest contains an object (or not) as new evidence is gathered about the scene layout. This enables an iterative estimation procedure where the detector becomes more and more accurate as additional evidence about a specific scene becomes available. Extensive quantitative and qualitative experiments are conducted on the table-top dataset (Sun et al. in ECCV, 2010b) and two publicly available datasets (Hoiem et al. in CVPR, 2006; Sudderth et al. in IJCV, 2008), and demonstrate competitive object detection, 3D layout estimation, and segmentation results.	[Sun, Min; Bao, Sid Yingze; Savarese, Silvio] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Sun, M (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	sunmin@umich.edu; yingze@umich.edu; silvio@eecs.umich.edu	Bao, Yingze/G-1686-2014		NSF [CNS 0931474]; Gigascale Systems Research Center; Focus Center Research Program (FCRP)	NSF(National Science Foundation (NSF)); Gigascale Systems Research Center; Focus Center Research Program (FCRP)	We acknowledge the support of NSF (Grant CNS 0931474) and the Gigascale Systems Research Center, one of six research centers funded under the Focus Center Research Program (FCRP), a Semiconductor Research Corporation Entity. We thank Gary Bradski for supporting the data collection of the table-top object dataset (Sun et al. 2010b).	[Anonymous], 2007, PASCAL VISUAL OBJECT; Bao S. Y.-Z., 2010, CVPR; Brostow G. J., 2008, ECCV; Cornelis N., 2006, 3DPVT; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; DANCE C, 2004, ECCV WORKSH STAT LEA; FEIFEI L, 2003, ICCV; Felzenszwalb P., 2004, IJCV; Felzenszwalb Pedro F., 2005, IJCV; FERGUS R, 2005, CVPR; Gonfaus J.M., 2010, CVPR; Gould S., 2009, ICCV; Grauman K., 2005, ICCV; Gupta A., 2008, ECCV; Hedau V., 2009, ICCV; Heitz G., 2008, NIPS; Hoiem D., 2008, CVPR; Hoiem D., 2006, CVPR; Hoiem Derek, 2007, IJCV; Hoiem Derek, 2005, CVPR; Ladicky L., 2010, ECCV; Leibe B., 2004, STAT LEARNING COMPUT; Li C., 2010, NIPS; Li L., 2009, CVPR; Li L.-J., 2007, ICCV; Liebelt J., 2010, CVPR; Payet N., 2011, CVPR; Rabinovich A., 2007, ICCV; Russell B. C., 2008, IJCV; Savarese S., 2007, CVPR; Saxena A., 2009, PAMI; Su H., 2009, ICCV; Sudderth Erik B, 2008, IJCV; Sun M., 2010, ECCV; Sun M., 2010, BMVC; Sun M., 2009, CVPR; Thomas A., 2006, P IEEE C COMP VIS PA; Torralba A., 2003, ICCV; VIOLA P, 2002, IJCV	39	3	3	1	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2012	100	2					154	169		10.1007/s11263-012-0547-2	http://dx.doi.org/10.1007/s11263-012-0547-2			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	000CQ					2022-12-18	WOS:000308364500004
J	Keck, M; Davis, JW				Keck, Mark; Davis, James W.			Recovery and Reasoning About Occlusions in 3D Using Few Cameras with Applications to 3D Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Occlusion Recovery; Tracking; Visual hull; Markov random field		In this work we propose algorithms to learn the locations of static occlusions and reason about both static and dynamic occlusion scenarios in multi-camera scenes for 3D surveillance (e. g., reconstruction, tracking). We will show that this leads to a computer system which is able to more effectively track (follow) objects in video when they are obstructed from some of the views. Because of the nature of the application area, our algorithm will be under the constraints of using few cameras (no more than 3) that are configured wide-baseline. Our algorithm consists of a learning phase, where a 3D probabilistic model of occlusions is estimated per-voxel, per-view over time via an iterative framework. In this framework, at each frame the visual hull of each foreground object (person) is computed via a Markov Random Field that integrates the occlusion model. The model is then updated at each frame using this solution, providing an iterative process that can accurately estimate the occlusion model over time and overcome the few-camera constraint. We demonstrate the application of such a model to a number of areas, including visual hull reconstruction, the reconstruction of the occluding structures themselves, and 3D tracking.	[Keck, Mark] BAE Syst Inc, Burlington, MA 01803 USA; [Davis, James W.] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA	Bae Systems; University System of Ohio; Ohio State University	Keck, M (corresponding author), BAE Syst Inc, 6 New England Exec Pk, Burlington, MA 01803 USA.	mark.keck@baesystems.com						Apostoloff N, 2005, PROC CVPR IEEE, P553; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Black J, 2006, IMAGE VISION COMPUT, V24, P1256, DOI 10.1016/j.imavis.2005.06.002; Broadhurst A., 1999, BMVC99. Proceedings of the 10th British Machine Vision Conference, P245; BROADHURST A, 2001, P INT C COMP VIS; Comaniciu D., 2003, IEEE T PATTERN ANAL; Comaniciu D., 2002, IEEE T PATTERN ANAL; Dalal N., 2005, P COMP VIS PATT REC; DAVIS J, 2005, P WKSHP APPL COMP VI; DELLAERT F, 1997, C INT TRANSP SYST; DOCKSTADER S, 2001, P IEEE WKSHP MULT TR; Elgammal A, 2001, PROC CVPR IEEE, P563; Gargallo P, 2005, PROC CVPR IEEE, P885; Gavrila DM, 2000, LNCS, P37, DOI DOI 10.1007/3-540-45053-X; Guan L., 2006, 3 INT S 3D DAT P VIS; Guan L., 2008, P COMP VIS PATT REC; Hartley R., 2004, ROBOTICA; HUANG Y, 2005, P COMP VIS PATT REC; ISARD M, 2001, P INT C COMP VIS; Jarvis R. A., 1973, Information Processing Letters, V2, P18, DOI 10.1016/0020-0190(73)90020-3; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Keck M., 2006, ACM MULT WKSHP VIS S; Keck M., 2008, P COMP VIS PATT REC; KHAN SM, 2006, P EUR C COMP VIS; Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004; KOLMOGOROV V, 2004, IEEE T PATTERN ANAL; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Mittal A, 2002, LECT NOTES COMPUT SC, V2350, P18; Mohan A., 2004, IEEE T PATTERN ANAL; OREN M, 1997, P COMP VIS PATT REC; PAPAGEORGIOU C, 1998, P INT C COMP VIS JAN; Perona Pietro, 1990, IEEE T PATTERN ANAL; Porikli F., 2006, P COMP VIS PATT REC; Rosales R., 1998, CVPR WKSHP INT VIS M; Seitz S. M., 2006, P COMP VIS PATT REC; SENIOR A, 2001, P INT WKSHP PERF EV; SHARMA V, 2007, P INT C COMP VIS; Sheikh Y, 2005, PROC CVPR IEEE, P74; Snow D, 2000, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2000.855839; Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637; Strecha C, 2004, PROC CVPR IEEE, P552; Strecha C., 2008, P COMP VIS PATT REC; Tuzel O., 2007, P COMP VIS PATT REC; TYAGI A, 2007, P WKSHP MOT VID COMP; Viola P., 2003, P INT C COMP VIS; Wu B., 2007, P COMP VIS PATT REC; WU B, 2005, P INT C COMP VIS; ZHANG Z, 2005, P IEEE INT WORK HUM; Zhou QM, 2006, IMAGE VISION COMPUT, V24, P1244, DOI 10.1016/j.imavis.2005.06.008; ZHOU Y, 2003, P INT C COMP VIS	50	3	3	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2011	95	3					240	264		10.1007/s11263-011-0446-y	http://dx.doi.org/10.1007/s11263-011-0446-y			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	913AD		Green Submitted			2022-12-18	WOS:000301842800002
J	Nordberg, K				Nordberg, Klas			The Key to Three-View Geometry	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Three-view geometry; Trifocal tensor; Minimal parameterization; Internal constraints; Trifocal tensor estimation		In this article we describe a set of canonical transformations of the image spaces that make the description of three-view geometry very simple. The transformations depend on the three-view geometry and the canonically transformed trifocal tensor T' takes the form of a sparse array where 17 elements in well-defined positions are zero, it has a linear relation to the camera matrices and to two of the fundamental matrices, a third order relation to the third fundamental matrix, a second order relation to the other two trifocal tensors, and first order relations to the 10 three-view all-point matching constraints. In this canonical form, it is also simple to determine if the corresponding camera configuration is degenerate or co-linear. An important property of the three canonical transformations of the images spaces is that they are in SO(3). The 9 parameters needed to determine these transformations and the 9 parameters that determine the elements of T' together provide a minimal parameterization of the tensor. It does not have problems with multiple maps or multiple solutions that other parameterizations have, and is therefore simple to use. It also provides an implicit representation of the trifocal internal constraints: the sparse canonical representation of the trifocal tensor can be determined if and only if it is consistent with its internal constraints. In the non-ideal case, the canonical transformation can be determined by solving a minimization problem and a simple algorithm for determining the solution is provided. This allows us to extend the standard linear method for estimation of the trifocal tensor to include a constraint enforcement as a final step, similar to the constraint enforcement of the fundamental matrix. Experimental evaluation of this extended linear estimation method shows that it significantly reduces the geometric error of the resulting tensor, but on average the algebraic estimation method is even better. For a small percentage of cases, however, the extended linear method gives a smaller geometric error, implying that it can be used as a complement to the algebraic method for these cases.	Linkoping Univ, Dept Elect Engn, Comp Vis Lab, S-58183 Linkoping, Sweden	Linkoping University	Nordberg, K (corresponding author), Linkoping Univ, Dept Elect Engn, Comp Vis Lab, S-58183 Linkoping, Sweden.	klas@isy.liu.se			EL-LIIT, the Strategic Area for ICT research; Swedish Government; EC [247947]	EL-LIIT, the Strategic Area for ICT research; Swedish Government; EC(European CommissionEuropean Commission Joint Research Centre)	This research has received support from EL-LIIT, the Strategic Area for ICT research, funded by the Swedish Government, and from the EC's 7th Framework Programme (FP7/2007-2013), grant agreement 247947 (GARNICS). The author would also like to thank the reviewers for several suggestions that have improved the final article.	ALZATI A, 2009, TENSORS COMPUTER VIS; Bezdek J.C., 2002, LECT NOTES COMP SCI, V2275, P187, DOI DOI 10.1007/3-540-45631-7_39; Canterakis N, 2000, LECT NOTES COMPUT SC, V1842, P84; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; HARTLEY R, 1992, P 2 EUR C COMP VIS, P579; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Heyden A, 1997, INT J COMPUT VISION, V24, P155, DOI 10.1023/A:1007963021756; Heyden A, 1997, MATH METHOD APPL SCI, V20, P1135, DOI 10.1002/(SICI)1099-1476(19970910)20:13<1135::AID-MMA908>3.0.CO;2-9; Joyeux F, 2008, LECT NOTES COMPUT SC, V5259, P465, DOI 10.1007/978-3-540-88458-3_42; LUONG Q, 1995, IEEE T PATTERN ANAL, V17, P34; Ma Y., 2006, INVITATION 3 D VISIO; NORDBERG K, 2007, P ANN S GERM ASS PAT; NORDBERG K, 2009, COMP SOC C COMP VIS; Papadopoulo T., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P109, DOI 10.1007/BFb0055662; Ressl C., 2003, THESIS TU WIEN; RESSL C, 2002, INT ARCH PHOTOGRAMME, V34, P277; SHASHUA A, 1995, IEEE T PATTERN ANAL, V17, P779, DOI 10.1109/34.400567; Shashua A., 1994, P 3 EUR C COMP VIS, P479; SHASHUA A, 1995, P INT C COMP VIS ICC; Torr PHS, 1997, IMAGE VISION COMPUT, V15, P591, DOI 10.1016/S0262-8856(97)00010-3	21	3	3	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2011	94	3					282	294		10.1007/s11263-011-0428-0	http://dx.doi.org/10.1007/s11263-011-0428-0			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816AC					2022-12-18	WOS:000294570100002
J	Paviotti, A; Forsyth, DA; Cortelazzo, GM				Paviotti, Anna; Forsyth, David A.; Cortelazzo, Guido M.			Lightness Recovery for Pictorial Surfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Lightness problem; Color constancy; Multispectral imaging; Color correction; Cultural heritage	PAINTINGS; SYSTEM	An important technique in cultural heritage preservation is multispectral acquisition, where one recovers a detailed spectral record of a painting using carefully calibrated lighting. This is difficult to do with frescoes, because it is hard to recover the spatial variation in light intensity that results from factors like the imaging setup and the curvature of the fresco. We introduce a new formulation of the lightness problem applied to images of pictorial artworks. The problem is different from the conventional lightness problem, because artists often paint the effects of light, so the albedo field contains a component that mimics an illumination field. Our method distinguishes between physical illumination and painted shading through spatial frequency effects and dynamic range considerations. We evaluate our method using multispectral images of paintings, where the physical illumination field is known. Our method produces estimates of the illumination intensity field that compare very well with the known ground truth, and outperforms other state-of-the art lightness recovery algorithms. For frescoes, ground truth is not available, but we show that our method produces consistent results, in the sense that the illumination functions estimated on the image and on (some of) its subimages are very similar on the overlap. We show our method produces qualitatively good color corrections for images of frescoes found on the web.	[Paviotti, Anna; Cortelazzo, Guido M.] Univ Padua, Dept Informat Engn, I-35131 Padua, Italy; [Forsyth, David A.] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA	University of Padua; University of Illinois System; University of Illinois Urbana-Champaign	Paviotti, A (corresponding author), Nidek Technol Srl, Via Artigianato 6-A, I-35020 Padua, Italy.	anna.paviotti@ieee.org; daf@uiuc.edu; corte@dei.unipd.it	Li, Mengqi/AAG-6804-2021					Antonelli G, 2004, ROMOCO' 04: PROCEEDINGS OF THE FOURTH INTERNATIONAL WORKSHOP ON ROBOT MOTION AND CONTROL, P219; Barni M, 2005, IEEE SIGNAL PROC MAG, V22, P141, DOI 10.1109/MSP.2005.1511835; BARROW HG, 2008, COMPUTER VISION SYST; BLAKE A, 1985, COMPUT VISION GRAPH, V32, P314, DOI 10.1016/0734-189X(85)90054-4; Bousseau A., 2009, SIGGRAPH ASIA; BRAINARD DH, 1986, J OPT SOC AM A, V3, P1651, DOI 10.1364/JOSAA.3.001651; BRELSTAFF G, 1987, PATTERN RECOGN LETT, V5, P129, DOI 10.1016/0167-8655(87)90034-1; Brusco N, 2006, MACH VISION APPL, V17, P373, DOI 10.1007/s00138-006-0026-2; Carcagni P, 2007, OPT LASER ENG, V45, P360, DOI 10.1016/j.optlaseng.2005.02.010; Dacorogna B., DIRECT METHODS CALCU, V78; DZMURA M, 1993, J OPT SOC AM A, V10, P2148, DOI 10.1364/JOSAA.10.002148; Elad M, 2003, J VIS COMMUN IMAGE R, V14, P369, DOI 10.1016/S1047-3203(03)00045-2; FARENZENA M, 2007, P ICIP, V3, P485; Finayson GD, 2001, IEEE T PATTERN ANAL, V23, P1209, DOI 10.1109/34.969113; FORSYTH D, 2009, LIGHT SPACE; Freeman William T, 2006, P C COMP VIS PATT RE; Funt Brian V, 1992, ECCV; FUNT BV, 1992, LECT NOTES COMPUT SC, V588, P124; Gilchrist A., 2006, SEEING BLACK WHITE; GILCHRIST AL, 1977, SCIENCE, V195, P185, DOI 10.1126/science.831266; Grosse Roger, 2009, ICCV; Hordley SD, 2006, COLOR RES APPL, V31, P303, DOI 10.1002/col.20226; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; Kay S. M, 1993, FUNDAMENTALS STAT SI; Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998; Koenderink JJ, 2007, PERCEPTION, V36, P1595, DOI 10.1068/p5672; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; Lenz R, 2002, CGIV'2002: FIRST EUROPEAN CONFERENCE ON COLOUR IN GRAPHICS, IMAGING, AND VISION, CONFERENCE PROCEEDINGS, P249; Martinez K, 2002, P IEEE, V90, P28, DOI 10.1109/5.982403; Mohen J.-P., 2006, M LISA INSIDE PAINTI; Novati G, 2005, INT J DIGIT LIBRARIE, V5, P167, DOI 10.1007/s00799-004-0103-y; Paviotti A, 2009, EURASIP J IMAGE VIDE, DOI 10.1155/2009/793756; PELAGOTTI A, 2007, SPIE C SERIES, V6491; Pelagotti A, 2008, IEEE SIGNAL PROC MAG, V25, P27, DOI [10.1109/MSP.2008.923095, 10.1109/MSP2008-923095]; Romeiro F, 2008, LECT NOTES COMPUT SC, V5305, P859, DOI 10.1007/978-3-540-88693-8_63; Shen L., 2008, CVPR; Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185; Weiss Y., 2001, ICCV; 2006, CASTELLO BUONCONSIGL	39	3	3	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2011	94	1					54	77		10.1007/s11263-011-0424-4	http://dx.doi.org/10.1007/s11263-011-0424-4			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	760JM					2022-12-18	WOS:000290320600005
J	Psarrou, A; Licata, A; Kokla, V; Tselikas, A				Psarrou, Alexandra; Licata, Aaron; Kokla, Vasiliki; Tselikas, Agamemnon			Near-Infrared Ink Differentiation in Medieval Manuscripts	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Document image analysis; Ink type modelling; Co-occurrence matrix analysis		One of the tasks facing historians and preservationists is the authentication or dating of medieval manuscripts. To this end it is important to verify whether writings on the same or different manuscripts are concurrent. We propose a novel approach for the automated image-based differentiation of inks used in medieval manuscripts. We consider the problem of capturing images of manuscript pages in near-infrared (NIR) spectrum and compare the ink appearance and textural features of segmented text. We present feature descriptors that capture the variability of the visual properties of the inks in NIR based on intensity distributions of histograms and co-occurrence matrices. Our approach is novel as it is entirely image based and does not include the spectrum analysis of the inks. The method is validated by using model ink images manufactured based on known recipes and ink segmented from medieval manuscripts dated from the 11th to the 16th century. Model inks are classified by using both supervised and unsupervised clustering. Comparison of inks of unknown composition is achieved through unsupervised multi-dimensional clustering of the feature descriptors and similarity measures of derived probability density functions.	[Psarrou, Alexandra; Licata, Aaron; Kokla, Vasiliki] Univ Westminster, Sch Elect & Comp Sci, London W1R 8AL, England; [Tselikas, Agamemnon] Natl Bank Greece Cultural Fdn, Athens, Greece	University of Westminster	Psarrou, A (corresponding author), Univ Westminster, Sch Elect & Comp Sci, London W1R 8AL, England.	psarroa@westminster.ac.uk; agatselikas@gmail.com	Kokla, Vasiliki/AAS-7362-2021	Kokla, Vasiliki/0000-0002-6039-9178; Psarrou, Alexandra/0000-0003-3167-3454	EU [509145]	EU(European Commission)	We would like to thank the EU for supporting this work under the project EU-Noesis, Non-dEStructive Image-based manuscript analysis System (research contract no. 509145, 6th Framework)	ALEXOPOULOU A, 1999, 6 INT C NOND TEST MI, P2049; Barrow WJ., 1972, MANUSCRIPTS DOCUMENT; BATYEHOUDA Z, 1983, ENCRES NOIRES MOYEN; Bishop, 1995, NEURAL NETWORKS PATT; Brown KL, 2004, J RAMAN SPECTROSC, V35, P181, DOI 10.1002/jrs.1127; Clarke M., 2001, REV CONSERVATION, V2, P3, DOI DOI 10.1179/SIC.2001A6.SUPPLEMENT-1.3; COGGINS JM, 1985, PATTERN RECOGN LETT, V3, P195, DOI 10.1016/0167-8655(85)90053-4; Dasari H, 2007, PROC INT CONF DOC, P486; FARROKHNIA F, 1990, THESIS MICHIGAN STAT; FLIEDER F, 1975, ANAL TANNINS HYDROLY; Franke K, 2002, EIGHTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION: PROCEEDINGS, P268, DOI 10.1109/IWFHR.2002.1030921; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; HUANG Y, 2008, C COMP VIS PATT REC, P1; Janssens K, 2000, X-RAY SPECTROM, V29, P73, DOI 10.1002/(SICI)1097-4539(200001/02)29:1<73::AID-XRS416>3.3.CO;2-D; KOKLA V, 2000, 15 WORLD C NOND TEST, P169; KOKLA V, 2007, P 9 INT C DOC AN REC; Lee AS, 2006, VIB SPECTROSC, V41, P170, DOI 10.1016/j.vibspec.2005.11.006; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; MONIQUE DP, 1975, TRAVAUX EFFECTUES AN; Picard R. W., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P371, DOI 10.1109/CVPR.1991.139718; SAUVOLA J, 2000, PATTERN RECOGNITION, V33; Shi ZX, 2004, INT C PATT RECOG, P473, DOI 10.1109/ICPR.2004.1334167; Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4	23	3	4	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2011	94	1					136	151		10.1007/s11263-011-0419-1	http://dx.doi.org/10.1007/s11263-011-0419-1			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)	Computer Science	760JM					2022-12-18	WOS:000290320600010
J	Ben-Shahar, O; Zucker, S				Ben-Shahar, Ohad; Zucker, Steven			General Geometric Good Continuation: From Taylor to Laplace via Level Sets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Perceptual organization; Good continuation; Grouping; Level set geometry; Singular and regular structure; Harmonic function; Anisotropic diffusion; Scale space; Relaxation labeling	MINIMAL-SURFACES; TEXTURE; INFERENCE; MODEL; COMPUTATION; CONTOURS; COLOR; SHAPE; MAPS	Good continuation is the Gestalt observation that parts often group in particular ways to form coherent wholes. Perceptual integration of edges, for example, involves orientation good continuation, a property which has been exploited computationally very extensively. But more general local-global relationships, such as for shading or color, have been elusive. While Taylor's Theorem suggests certain modeling and smoothness criteria, the consideration of level set geometry indicates a different approach. Using such first principles we derive, for the first time, a generalization of good continuation to all those visual structures that can be abstracted as scalar functions over the image plane. Based on second order differential constraints that reflect good continuation, our analysis leads to a unique class of harmonic models and a cooperative algorithm for structure inference. Among the different applications of good continuation, here we apply these results to the denoising of shading and intensity distributions and demonstrate how our approach eliminates spurious measurements while preserving both singularities and regular structure, a property that facilitates higher level processes which depend so critically on both of these classes of visual structures.	[Ben-Shahar, Ohad] Ben Gurion Univ Negev, Dept Comp Sci, IL-84105 Beer Sheva, Israel; [Zucker, Steven] Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA	Ben Gurion University; Yale University	Ben-Shahar, O (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, POB 653, IL-84105 Beer Sheva, Israel.	ben-shahar@cs.bgu.ac.il	Ben-Shahar, Ohad/F-8918-2015	Ben-Shahar, Ohad/0000-0001-5346-152X	Israel Science Foundation (ISF) [207-07-08, 1245/ 08]; ARO; AFOSR; NGA; ONR; NSF; Frankel fund	Israel Science Foundation (ISF)(Israel Science Foundation); ARO; AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); NGA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); Frankel fund	This work was funded in part by the Psychobiology Young Investigator grant 207-07-08, and the Israel Science Foundation (ISF) grant No. 1245/ 08. Additional support was provided by the ARO, AFOSR, NGA, ONR and NSF. O. B.-S. also thanks the generous support of the Frankel fund, the Paul Ivanier center for Robotics Research and the Zlotowski Center for Neuroscience at Ben-Gurion University.	ALVAREZ L, 1994, SIAM J NUMER ANAL, V31, P590, DOI 10.1137/0731032; Ben-Shahar O, 2004, NEURAL NETWORKS, V17, P753, DOI 10.1016/j.neunet.2004.03.011; Ben-Shahar O, 2004, NEURAL COMPUT, V16, P445, DOI 10.1162/089976604772744866; Ben-Shahar O, 2003, IEEE T PATTERN ANAL, V25, P401, DOI 10.1109/TPAMI.2003.1190568; Ben-Shahar O., 2003, THESIS YALE U; Ben-Shahar O, 2006, P NATL ACAD SCI USA, V103, P15704, DOI 10.1073/pnas.0604410103; BRETON P, 1992, NEURAL NETWORKS VISI, P111; Cao F., 2004, Computing and Visualization in Science, V7, P3, DOI 10.1007/s00791-004-0123-6; CARBAL B, 1993, P SIGGRAPH, P263; Caselles V, 2002, J MATH IMAGING VIS, V16, P89, DOI 10.1023/A:1013943314097; Caselles V, 1997, IEEE T PATTERN ANAL, V19, P394, DOI 10.1109/34.588023; Caselles V, 1997, NUMER MATH, V77, P423, DOI 10.1007/s002110050294; DEVALOIS RL, 1982, VISION RES, V22, P545, DOI 10.1016/0042-6989(82)90113-4; Dierkes U, 1992, MINIMAL SURFACES 1 B; DUNCAN JS, 1992, IEEE T PATTERN ANAL, V14, P502, DOI 10.1109/34.134056; FISCHLER M, 1994, P IM UND WORKSH, V2, P1565; Graustein WC, 1940, T AM MATH SOC, V47, P173, DOI 10.2307/1990055; Guy G, 1997, IEEE T PATTERN ANAL, V19, P1265, DOI 10.1109/34.632985; Guy G, 1996, INT J COMPUT VISION, V20, P113, DOI 10.1007/BF00144119; HAMEL G, 1923, JAAHRESBER DMV, V32, P6; Hoffman D., 1998, VISUAL INTELLIGENCE; HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141; HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085; HUMMEL RA, 1983, IEEE T PATTERN ANAL, V5, P267, DOI 10.1109/TPAMI.1983.4767390; Jain A. K., 1989, FUNDAMENTALS DIGITAL; John F., 1991, PARTIAL DIFFERENTIAL; Keil MS, 2006, VISION RES, V46, P2659, DOI 10.1016/j.visres.2006.01.038; KIMIA B, 1999, P IEEE COMP SOC WORK; Kimmel R, 2000, INT J COMPUT VISION, V39, P111, DOI 10.1023/A:1008171026419; KITTLER J, 1985, IMAGE VISION COMPUT, V3, P206, DOI 10.1016/0262-8856(85)90009-5; Koenderink JJ, 2002, LECT NOTES COMPUT SC, V2350, P158; Koffka K., 1935, PRINCIPLES GESTALT P; MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923; Miller DA, 1999, NEURAL COMPUT, V11, P21, DOI 10.1162/089976699300016782; MUMFORD D., 1993, ALGEBRAIC GEOMETRY I; Nitsche Johannes C. C., 1989, LECT MINIMAL SURFACE, V1; ONeill B., 1966, ELEMENTARY DIFFERENT; PARENT P, 1989, IEEE T PATTERN ANAL, V11, P823, DOI 10.1109/34.31445; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pelillo M, 1997, J MATH IMAGING VIS, V7, P309, DOI 10.1023/A:1008255111261; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; RICHARDS WA, 1987, J OPT SOC AM A, V4, P1168, DOI 10.1364/JOSAA.4.001168; SARA R, 1995, P INT C COMP AN IM P, P416; Savadjiev P, 2007, IEEE I CONF COMP VIS, P2017; Sharon E, 2000, IEEE T PATTERN ANAL, V22, P1117, DOI 10.1109/34.879792; Sochen N, 1998, IEEE T IMAGE PROCESS, V7, P310, DOI 10.1109/83.661181; Tang B, 2000, INT J COMPUT VISION, V36, P149, DOI 10.1023/A:1008152115986; Tenenbaum Jay M, 1983, HUMAN MACHINE VISION, P481; Torsello A, 2000, PATTERN RECOGN, V33, P1897, DOI 10.1016/S0031-3203(99)00174-0; ULLMAN S, 1976, BIOL CYBERN, V25, P1; Ullman S., 1988, P 2 INT C COMP VIS, P321, DOI DOI 10.1109/CCV.1988.590008; Waltz D., 1975, PSYCHOL COMPUTER VIS, P19; WERTHEIMER M, 1955, SOURCE BOOK GESTALT, P71; Williams LR, 1997, NEURAL COMPUT, V9, P837, DOI 10.1162/neco.1997.9.4.837; ZUCKER S, 1975, P INT JOINT C ART IN, P716	55	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	1					48	71		10.1007/s11263-009-0255-8	http://dx.doi.org/10.1007/s11263-009-0255-8			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	534MZ					2022-12-18	WOS:000272903100003
J	Kubota, T				Kubota, Toshiro			A Shape Representation with Elastic Quadratic Polynomials-Preservation of High Curvature Points under Noisy Conditions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Key-point detection; Interpolation; Restoration	PLANAR CURVES; MODELS; SEGMENTATION; RECOGNITION; EXTRACTION; DIFFUSION; METRICS; SPACE	We present a new shape representation technique for a planar shape using overlapping quadratic splines. The technique refines the shape by iteratively updating each spline to reduce the cost associated with C (1) discontinuity. It consists of a series of affine commutative linear operators, producing a smooth bandpass frequency response. The primary purpose of the technique is to remove minute high-curvature points while preserving salient ones. We compare the performance of the technique against those that are based on either linear splines, cubic splines, or wavelets. We consider three criteria: sensitivity of detecting salient high-curvature points, Hausdorff distance between the representation and the original shape, and computation time. The results show that the proposed technique is highly effective in preserving salient high curvature points with a relatively small Hausdorff distance and a computational cost.	Susquehanna Univ, Selinsgrove, PA 17870 USA	Susquehanna University	Kubota, T (corresponding author), Susquehanna Univ, Selinsgrove, PA 17870 USA.	kubota@susqu.edu						ALPERT BK, 1993, SIAM J MATH ANAL, V24, P246, DOI 10.1137/0524016; ASADA H, 1986, IEEE T PATTERN ANAL, V8, P2, DOI 10.1109/TPAMI.1986.4767747; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BENGTSSON A, 1991, IEEE T PATTERN ANAL, V13, P85, DOI 10.1109/34.67634; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Cham TJ, 1999, IEEE T PATTERN ANAL, V21, P49, DOI 10.1109/34.745733; Charpiat G, 2005, FOUND COMPUT MATH, V5, P1, DOI 10.1007/s10208-003-0094-x; Chuang GCH, 1996, IEEE T IMAGE PROCESS, V5, P56, DOI 10.1109/83.481671; Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915; Dambreville S, 2008, IEEE T PATTERN ANAL, V30, P1385, DOI 10.1109/TPAMI.2007.70774; De Boor C., 1978, PRACTICAL GUIDE SPLI, V27; DESSIMOZ JD, 1979, SIGNAL PROCESS, V1, P205, DOI 10.1016/0165-1684(79)90020-3; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Elder JH, 1998, IEEE T PATTERN ANAL, V20, P699, DOI 10.1109/34.689301; ELDER JH, 1996, P 4 EUR C COMP VIS, P399; FUCHS H, 1977, COMMUN ACM, V20, P693, DOI 10.1145/359842.359846; GERONIMO JS, 1994, J APPROX THEORY, V78, P373, DOI 10.1006/jath.1994.1085; Jang BK, 1998, COMPUT VIS IMAGE UND, V70, P121, DOI 10.1006/cviu.1997.0626; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kilian M, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276457, 10.1145/1239451.1239515]; Kimia BB, 1996, COMPUT VIS IMAGE UND, V64, P305, DOI 10.1006/cviu.1996.0062; KUBOTA T, 2003, 4 INT WORKSH EN MIN, P467; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; LU F, 1994, SIGNAL PROCESS, V37, P129, DOI 10.1016/0165-1684(94)90171-6; Marr D., 1982, VISION; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Mio W, 2007, INT J COMPUT VISION, V73, P307, DOI [10.1007/s11263-006-9968-0, 10.1007/s11263-006-996S-0]; MOKHTARIAN F, 1986, IEEE T PATTERN ANAL, V8, P34, DOI 10.1109/TPAMI.1986.4767750; Mokhtarian F., 1996, P INT WORKSH IM DAT, P35; Nonato LG, 2005, ACM T GRAPHIC, V24, P1239, DOI 10.1145/1095878.1095879; PAGLIERONI DW, 1988, COMPUT VISION GRAPH, V42, P87, DOI 10.1016/0734-189X(88)90144-2; Precioso F, 2005, IEEE T IMAGE PROCESS, V14, P910, DOI 10.1109/TIP.2005.849307; RATTARANGSI A, 1992, IEEE T PATTERN ANAL, V14, P430, DOI 10.1109/34.126805; RICHARDSON T, 2006, P INT C MED IM COMP, V1, P17; Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951; Sharon E, 2000, IEEE T PATTERN ANAL, V22, P1117, DOI 10.1109/34.879792; Sundaramoorthi G, 2007, INT J COMPUT VISION, V73, P345, DOI 10.1007/s11263-006-0635-2; TEH CH, 1989, IEEE T PATTERN ANAL, V11, P859, DOI 10.1109/34.31447; Wang S, 2005, IEEE T PATTERN ANAL, V27, P546, DOI 10.1109/TPAMI.2005.84; WANG S, 2004, SHAPE CORRES LANDMAR, pR1; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685	46	3	5	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2009	82	2					133	155		10.1007/s11263-008-0192-y	http://dx.doi.org/10.1007/s11263-008-0192-y			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	408FP					2022-12-18	WOS:000263421200002
J	Boltz, S; Herbulot, A; Debreuve, E; Barlaud, M; Aubert, G				Boltz, Sylvain; Herbulot, Ariane; Debreuve, Eric; Barlaud, Michel; Aubert, Gilles			Motion and appearance nonparametric joint entropy for video segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						spatio-temporal segmentation; nonparametric distribution; joint entropy; active contour	LEVEL SET SEGMENTATION; IMAGE SEGMENTATION; ACTIVE CONTOURS; CURVE EVOLUTION; SHAPE; DISTRIBUTIONS; REGIONS	This paper deals with video segmentation based on motion and spatial information. Classically, the motion term is based on a motion compensation error (MCE) between two consecutive frames. Defining a motion-based energy as the integral of a function of the MCE over the object domain implicitly results in making an assumption on the MCE distribution: Gaussian for the square function and, more generally, parametric distributions for functions used in robust estimation. However, these assumptions are not necessarily appropriate. Instead, we propose to define the energy as a function of (an estimation of) the MCE distribution. This function was chosen to be a continuous version of the Ahmad-Lin entropy approximation, the purpose being to be more robust to outliers inherently present in the MCE. Since a motion-only constraint can fail with homogeneous objects, the motion-based energy is enriched with spatial information using a joint entropy formulation. The resulting energy is minimized iteratively using active contours. This approach provides a general framework which consists in defining a statistical energy as a function of a multivariate distribution, independently of the features associated with the object of interest. The link between the energy and the features observed or computed on the video sequence is then made through a nonparametric, kernel-based distribution estimation. It allows for example to keep the same energy definition while using different features or different assumptions on the features.	[Boltz, Sylvain; Herbulot, Ariane; Debreuve, Eric; Barlaud, Michel] Univ Nice Sophia Antipolis, CNRS, Lab I3S, Nice, France; [Aubert, Gilles] Univ Nice Sophia Antipolis, CNRS, Lab JA Dieudonne, Nice, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Cote d'Azur; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Cote d'Azur	Debreuve, E (corresponding author), Univ Nice Sophia Antipolis, CNRS, Lab I3S, Nice, France.	boltz@i3s.unice.fr; herbulot@i3s.unice.fr; debreuve@i3s.unice.fr; barlaud@i3s.unice.fr; gaubert@math.unice.fr						AHMAD IA, 1976, IEEE T INFORM THEORY, V22, P372, DOI 10.1109/TIT.1976.1055550; ALVAREZ L, 1999, INT C SCAL SPAC THEO; Aubert G, 2003, SIAM J APPL MATH, V63, P2128, DOI 10.1137/S0036139902408928; Awate SP, 2006, IEEE T PATTERN ANAL, V28, P364, DOI 10.1109/TPAMI.2006.64; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; BOLTZ S, 2007, INT C COMP VIS PATT; BOLTZ S, 2006, ECV WORKSH STAT METH; BROX T, 2003, INT C COMP AN IM PAT; BROX T, 2004, EUR C COMP VIS PRAG; BRUHN A, 2002, DAGM S PATT REC ZUR; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Charbonnier P, 1997, IEEE T IMAGE PROCESS, V6, P298, DOI 10.1109/83.551699; COMANICIU D, 2000, INT C COMP VIS PATT; Cremers D, 2005, INT J COMPUT VISION, V62, P249, DOI 10.1007/s11263-005-4882-4; CREMERS D, 2003, INT C COMP VIS NIC F; Cremers D, 2007, INT J COMPUT VISION, V72, P195, DOI 10.1007/s11263-006-8711-1; Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5; Delfour M. C., 2001, SHAPE OPTIMIZATION O, P37, DOI [10.1201/9780203904169.ch3, DOI 10.1201/9780203904169.CH3]; DOREA CC, 2006, INT C IM PROC ATL GA; ELGAMMAL A, 2003, INT C COMP VIS PATT; Freedman D, 2004, IEEE T IMAGE PROCESS, V13, P518, DOI 10.1109/TIP.2003.821445; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Goria MN, 2005, J NONPARAMETR STAT, V17, P277, DOI 10.1080/104852504200026815; HERBULOT A, 2006, INT C IM PROC ATL GA; Hintermuller M, 2003, SIAM J APPL MATH, V64, P442, DOI 10.1137/S0036139902403901; Jehan-Besson S, 2003, INT J COMPUT VISION, V53, P45, DOI 10.1023/A:1023031708305; Kim JM, 2005, IEEE T IMAGE PROCESS, V14, P1486, DOI 10.1109/TIP.2005.854442; LEVENTON M, 2000, INT C COMP VIS PATT; LEVENTON ME, 2000, WORKSH MATH METH BIO; Lucas Bruce D, 1981, P 7 INT JOINT C ART; Mittal A., 2004, INT C COMP VIS PATT; Neemuchwala H., 2005, MULTISENSOR IMAGE FU, P185; ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029; Paragios N, 2002, J VIS COMMUN IMAGE R, V13, P249, DOI 10.1006/jvci.2001.0475; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Samson C, 2000, INT J COMPUT VISION, V40, P187, DOI 10.1023/A:1008183109594; SCHOENEMANN T, 2006, DAGM S PATT REC BERL; Scott D. W., 1992, MULTIVARIATE DENSITY, DOI 10.1002/9780470316849; Silverman B.W., 1986, DENSITY ESTIMATION S, V26; Weickert J, 2001, J MATH IMAGING VIS, V14, P245, DOI 10.1023/A:1011286029287; Wells W M 3rd, 1996, Med Image Anal, V1, P35; Wu S. F., 1993, Journal of Visual Communication and Image Representation, V4, P25, DOI 10.1006/jvci.1993.1003; Yezzi A, 2002, J VIS COMMUN IMAGE R, V13, P195, DOI 10.1006/jvci.2001.0500; Zhu S, 2000, IEEE T IMAGE PROCESS, V9, P287, DOI 10.1109/83.821744	45	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2008	80	2					242	259		10.1007/s11263-007-0124-2	http://dx.doi.org/10.1007/s11263-007-0124-2			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	348DA		Green Submitted			2022-12-18	WOS:000259190000005
J	Lauze, F; Nielsen, M				Lauze, Francois; Nielsen, Mads			From inpainting to active contours	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						segmentation; inpainting; active contours; disocclusion; adjoint methods; variational methods	AUTOMATIC DIFFERENTIATION; IMAGE; SEGMENTATION; MODELS	Background subtraction is an elementary method for detection of foreground objects and their segmentations. Obviously it requires an observation image as well as a background one. In this work we attempt to remove the last requirement by reconstructing the background from the observation image and a guess on the location of the object to be segmented via variational inpainting method. A numerical evaluation of this reconstruction provides a "disocclusion measure" and the correct foreground segmentation region is expected to maximize this measure. This formulation is in fact an optimal control problem, where controls are shapes/regions and states are the corresponding inpaintings. Optimization of the disocclusion measure leads formally to a coupled contour evolution equation, an inpainting equation (the state equation) as well as a linear PDE depending on the inpainting (the adjoint state equation). The contour evolution is implemented in the framework of level sets. Finally, the proposed method is validated on various examples. We focus among others in the segmentation of calcified plaques observed in radiographs from human lumbar aortic regions.	[Lauze, Francois; Nielsen, Mads] Nord Biosci AS, DK-2370 Herlev, Denmark; [Nielsen, Mads] Univ Copenhagen, DIKU, DK-2100 Copenhagen, Denmark	Nordic Bioscience; University of Copenhagen	Lauze, F (corresponding author), Nord Biosci AS, Herlev Hovedgade 207, DK-2370 Herlev, Denmark.	francois@nordicbioscience.com; madsn@diku.dk	Lauze, Francois B/L-8757-2016	Lauze, Francois B/0000-0003-2503-6475				Aubert G, 2003, SIAM J APPL MATH, V63, P2128, DOI 10.1137/S0036139902408928; AUBERT G, 2006, APPL MATH SC, V147; Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036; Bruhn A, 2005, IEEE T IMAGE PROCESS, V14, P608, DOI 10.1109/TIP.2005.846018; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844; COHEN L, 1993, SPIE C GEOM METH COM; COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675; Cremers D., 2007, INT J COMPUTER VISIO, V72; DELFOUR MC, 1989, ANN I H POINCARE A S, V6, P211; DELFOUR MC, 2001, ADV DESIGN CONTROL P; Griewank A, 1996, ACM T MATH SOFTWARE, V22, P131, DOI 10.1145/229473.229474; Gunzburger M, 2000, FLOW TURBUL COMBUST, V65, P249, DOI 10.1023/A:1011455900396; Heiler M, 2005, INT J COMPUT VISION, V63, P5, DOI 10.1007/s11263-005-4944-7; Jehan-Besson S, 2003, INT J COMPUT VISION, V53, P45, DOI 10.1023/A:1023031708305; Kass M., 1987, International Journal of Computer Vision, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; Kim JM, 2005, IEEE T IMAGE PROCESS, V14, P1486, DOI 10.1109/TIP.2005.854442; Lauze F, 2005, LECT NOTES COMPUT SC, V3752, P97; Lauze F, 2007, PROC SPIE, V6512, DOI 10.1117/12.709328; Lions J.L, 1971, OPTIMAL CONTROL SYST; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Ohta N, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P481, DOI 10.1109/ICCV.2001.937664; Osher S., 2003, GEOMETRIC LEVEL SET; PARAGIOS N, 2002, COMPUTER VISION COMP, V13, P249; RONFARD R, 1994, INT J COMPUT VISION, V13, P229, DOI 10.1007/BF01427153; ROSTAING N, 1993, TELLUS A, V45A, P558, DOI 10.1034/j.1600-0870.1993.00016.x; Roy T, 2006, J MATH IMAGING VIS, V24, P259, DOI 10.1007/s10851-005-3627-x; Rybak I.V., 2004, MATH MODEL ANAL, V9, P169; Sethian J.A., 1999, LEVEL SET METHODS FA, V2nd; TAASAN S, 1997, LECT NOTES OPTIMIZAT; Xu CY, 1997, PROC CVPR IEEE, P66, DOI 10.1109/CVPR.1997.609299	33	3	3	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2008	79	1					31	43		10.1007/s11263-007-0088-2	http://dx.doi.org/10.1007/s11263-007-0088-2			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	291JQ					2022-12-18	WOS:000255193500003
J	Zhu, ZG; Kanade, T				Zhu, Zhigang; Kanade, Takeo			Modeling and representations of large-scale 3D scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Zhu, Zhigang] CUNY City Coll, Dept Comp Sci, New York, NY 10031 USA; [Kanade, Takeo] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	City University of New York (CUNY) System; City College of New York (CUNY); Carnegie Mellon University	Zhu, ZG (corresponding author), CUNY City Coll, Dept Comp Sci, Convent Ave & 138th St, New York, NY 10031 USA.	zzhu@ccny.cuny.edu; Tk@cs.cmu.edu							0	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2008	78	2-3					119	120		10.1007/s11263-008-0128-6	http://dx.doi.org/10.1007/s11263-008-0128-6			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	275RK					2022-12-18	WOS:000254089100001
J	Kannan, A; Jojic, N; Frey, BJ				Kannan, Anitha; Jojic, Nebojsa; Frey, Brendan J.			Fast transformation-invariant component analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						TCA; patch; EM algorithm; dimensionality reduction; clustering		Dimensionality reduction techniques such as principal component analysis and factor analysis are used to discover a linear mapping between high-dimensional data samples and points in a lower-dimensional subspace. Previously, Frey and Jojic introduced transformation-invariant component analysis (TCA) to learn a linear mapping, invariant to a set of known form of global transformations. However, parameter estimation in that model using the previously-proposed expectation maximization (EM) algorithm required scalar operations in the order of N-2 where N is the dimensionality of each training example. This is prohibitive for many applications of interest such as modeling mid-to large-size images, where, for instance, N may be as high as 786432 (512 x 512 RGB image). In this paper, we present an efficient algorithm that reduces the computational requirements to order of N log N. With this speedup, we show the effectiveness of transformation-invariant component analysis in various applications including tracking, learning video textures, clustering, object recognition and object detection in images. Software for TCA can be downloaded from http://www.psi.toronto.edu/ fastTCA.htm.	[Kannan, Anitha; Frey, Brendan J.] Univ Toronto, Toronto, ON, Canada; [Jojic, Nebojsa] Microsoft Res, Redmond, WA USA	University of Toronto; Microsoft	Kannan, A (corresponding author), Univ Toronto, Toronto, ON, Canada.	anitha@psi.utoronto.ca						BLACK M, 1998, P IEEE INT C COMP VI; Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436; CHUDOVA D, 2003, P 19 C UNC ART INT; Everitt B., 1984, INTRO LATENT VARIABL; Felzenszwalb PF, 2003, ADV NEURAL INFORM PR; FREY B, 1999, INT C COMP VIS; FREY B, 2003, IEEE C COMP VIS PATT; FREY B, 1998, P IEEE C COMP VIS PA; FREY B, 2002, ADV NEURAL INFORM PR; Frey BJ, 2003, IEEE T PATTERN ANAL, V25, P1, DOI 10.1109/TPAMI.2003.1159942; Ghahramani Zoubin, 1996, CRGTR961 U TOR; GRAY A, 2000, ADV NEURAL INFORM PR; Hinton GE, 1997, IEEE T NEURAL NETWOR, V8, P65, DOI 10.1109/72.554192; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; KANNAN A, 2000, ADV NEURAL INFORM PR; Learned-Miller EG, 2006, IEEE T PATTERN ANAL, V28, P236, DOI 10.1109/TPAMI.2006.34; LI S, 2002, P 2 INT C DEV LEARN; Press W.H., 1992, NUMERICAL RECIPES C, V2; SCHODL A, 2000, P SIGGRAPH; TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586; WOLBERG G, 2000, P IEEE INT C IM PROC; YANG C, 2003, P INT C COMP VIS	23	3	3	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2008	77	1-3					87	101		10.1007/s11263-007-0094-4	http://dx.doi.org/10.1007/s11263-007-0094-4			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	267RE					2022-12-18	WOS:000253526100006
J	Sato, I; Okabe, T; Sato, Y				Sato, Imari; Okabe, Takahiro; Sato, Yoichi			Appearance sampling of real objects for variable illumination	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						physics-based computer vision; image-based modeling and rendering; reflectance analysis	FACE RECOGNITION; IMAGES; MODELS; SHAPE	The appearance of an object greatly changes under different lighting conditions. Even so, previous studies have demonstrated that the appearance of an object under varying illumination conditions can be represented by a linear subspace. A set of basis images spanning such a linear subspace can be obtained by applying the principal component analysis (PCA) for a large number of images taken under different lighting conditions. Since little is known about how to sample the appearance of an object in order to correctly obtain its basis images, it was a common practice to use as many input images as possible. In this study, we present a novel method for analytically obtaining a set of basis images of an object for varying illumination from input images of the object taken properly under a set of light sources, such as point light sources or extended light sources. Our proposed method incorporates the sampling theorem of spherical harmonics for determining a set of lighting directions to efficiently sample the appearance of an object. We further consider the issue of aliasing caused by insufficient sampling of the object's appearance. In particular, we investigate the effectiveness of using extended light sources for modeling the appearance of an object under varying illumination without suffering the aliasing caused by insufficient sampling of its appearance.	Univ Tokyo, Tokyo, Japan	University of Tokyo		imarik@nii.ac.jp; takahiro@iis.u-tokyo.ac.jp; ysato@iis.u-tokyo.ac.jp						Basri R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P383, DOI 10.1109/ICCV.2001.937651; DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008; Epstein R., 1995, Proceedings of the Workshop on Physics-Based Modeling in Computer Vision (Cat. No.95TB8038), P108, DOI 10.1109/PBMCV.1995.514675; Georghiades AS, 1998, PROC CVPR IEEE, P52, DOI 10.1109/CVPR.1998.698587; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; HALLINAN PW, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P995, DOI 10.1109/CVPR.1994.323941; Jin Y, 2004, IEEE C EVOL COMPUTAT, P1; Lee KC, 2001, PROC CVPR IEEE, P519; Malzbender T, 2001, COMP GRAPH, P519, DOI 10.1145/383259.383320; Mohlenkamp MJ, 1999, J FOURIER ANAL APPL, V5, P159, DOI 10.1007/BF01261607; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; NAYAR SK, 1990, IEEE T ROBOTIC AUTOM, V6, P418, DOI 10.1109/70.59367; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; NGAN A, 2005, P EUR S REND, P117; NISHINO K, 2001, P IEEE INT C COMP VI; Paul D., 1998, P SIGGRAPH 98, P189, DOI [10.1145/280814.280864, DOI 10.1145/280814.280864]; Press WH, 1988, NUMERICAL RECIPES C; Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448; Ramamoorthi R, 2005, IEEE T PATTERN ANAL, V27, P288, DOI 10.1109/TPAMI.2005.22; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; Ramantoorthi R, 2002, ACM T GRAPHIC, V21, P517, DOI 10.1145/566570.566611; Sato I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P800; Sato I, 2005, IEEE I CONF COMP VIS, P325; Schechner YY, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P808; Schroder P., 2019, CIRCULAR EC GLOBAL S, P43, DOI DOI 10.1145/218380.218439; Shashua A, 1997, INT J COMPUT VISION, V21, P99, DOI 10.1023/A:1007975506780; SHIM K, 2004, P PICT COD S; SLOAN PP, 2002, P ACM SIGGRAPH, P527; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; WESTIN SH, 1992, COMP GRAPH, V26, P255, DOI 10.1145/142920.134075; Yuille AL, 1999, INT J COMPUT VISION, V35, P203, DOI 10.1023/A:1008180726317; Zhang L, 2003, PROC CVPR IEEE, P19; Zhao L, 1999, PATTERN RECOGN, V32, P547, DOI 10.1016/S0031-3203(98)00119-8	34	3	3	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2007	75	1					29	48		10.1007/s11263-007-0036-1	http://dx.doi.org/10.1007/s11263-007-0036-1			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	197RM					2022-12-18	WOS:000248574200003
J	Goshen, L; Shimshoni, I; Anandan, P; Keren, D				Goshen, L; Shimshoni, I; Anandan, P; Keren, D			Motion recovery by integrating over the joint image manifold	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						fundamental matrix estimation; epipolar geometry estimation; motion recovery; integrated maximum likelihood		Recovery of epipolar geometry is a fundamental problem in computer vision. The introduction of the "joint image manifold" (JIM) allows to treat the recovery of camera motion and epipolar geometry as the problem of fitting a manifold to the data measured in a stereo pair. The manifold has a singularity and boundary, therefore special care must be taken when fitting it. Four fitting methods are discussed-direct, algebraic, geometric, and the integrated maximum likelihood (IML) based method. The first three methods are the exact analogues of three common methods for recovering epipolar geometry. The more recently introduced IML method seeks the manifold which has the highest "support," in the sense that the largest measure of its points are close to the data. While computationally more intensive than the other methods, its results are better in some scenarios. Both simulations and experiments suggest that the advantages of IML manifold fitting carry over to the task of recovering epipolar geometry, especially when the extent of the data and/or the motion are small.	Technion Israel Inst Technol, Fac Ind Engn, IL-32000 Haifa, Israel; Microsoft Res Inst, Bangalore, Karnataka, India; Univ Haifa, Dept Comp Sci, IL-31905 Haifa, Israel	Technion Israel Institute of Technology; Microsoft; University of Haifa	Goshen, L (corresponding author), Technion Israel Inst Technol, Fac Ind Engn, IL-32000 Haifa, Israel.	lirang@techunix.technion.ac.il; ilans@ie.technion.ac.il; anandan@microsoft.com; dkeren@cs.haifa.ac.il		Shimshoni, Ilan/0000-0002-5276-0242				Ahn SJ, 2001, PATTERN RECOGN, V34, P2283, DOI 10.1016/S0031-3203(00)00152-7; Anandan P, 2000, LECT NOTES COMPUT SC, V1842, P907; Berger JO, 1999, STAT SCI, V14, P1; Bruckstein AM, 2000, INT J IMAG SYST TECH, V11, P315, DOI 10.1002/ima.1016; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; FORSYTH D, 1999, INT C COMP VIS ICCV, P660; Goshen L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1321; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; HARTLEY RI, 1992, ECCV, P579; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; KANATANI K, 1994, CVGIP-IMAG UNDERSTAN, V59, P286, DOI 10.1006/ciun.1994.1020; KANATANI K, 1994, IEEE T PATTERN ANAL, V16, P320, DOI 10.1109/34.276132; Kanatani K., 1993, GEOMETRIC COMPUTATIO; Kanatani K., 1996, STAT OPTIMIZATION GE; KEREN D, 1994, IEEE T PATTERN ANAL, V16, P38, DOI 10.1109/34.273718; KEREN D, 2003, THEORETICAL FDN COMP, P72; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Nestares O, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P77; Nestares O, 2000, PROC CVPR IEEE, P523, DOI 10.1109/CVPR.2000.855864; Newsam GN, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P752, DOI 10.1109/ICIP.1997.638605; OHTA N, 2003, 3 IEEE WORKSH STAT C; Okatani T., 2002, Proceedings of the Statistical Methods in Video Processing Workshop, P25; Press WH, 1986, NUMERICAL RECIPES C, V818; SAMPSON PD, 1982, COMPUT VISION GRAPH, V18, P97, DOI 10.1016/0146-664X(82)90101-0; Severini TA., 2001, LIKELIHOOD METHODS S; SULLIVAN S, 1994, IEEE T PATTERN ANAL, V16, P1183, DOI 10.1109/34.387489; TAUBIN G, 1994, IEEE T PATTERN ANAL, V16, P287, DOI 10.1109/34.276128; Torr PHS, 2002, INT J COMPUT VISION, V50, P35, DOI 10.1023/A:1020224303087; TORR PHS, 1998, ECCV98, V1, P511; TRIGGS B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P338, DOI 10.1109/ICCV.1995.466920; WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779; Werman M, 2001, IEEE T PATTERN ANAL, V23, P528, DOI 10.1109/34.922710	35	3	3	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2005	65	3					131	145		10.1007/s11263-005-3673-2	http://dx.doi.org/10.1007/s11263-005-3673-2			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	999ZE		Green Submitted			2022-12-18	WOS:000234429900001
J	Black, MJ; Kimia, BB				Black, MJ; Kimia, BB			Guest editorial: Computational vision at Brown	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material							OBJECT RECOGNITION; STOCHASTIC-MODELS; OPTIC FLOW; MOTION; SHAPE; SEGMENTATION; PERCEPTION; SURFACES; TRACKING; CURVES		Brown Univ, Dept Comp Sci, Providence, RI 02912 USA; Brown Univ, Div Engn, Providence, RI 02912 USA	Brown University; Brown University	Black, MJ (corresponding author), Brown Univ, Dept Comp Sci, Box 1910, Providence, RI 02912 USA.	black@cs.brown.edu; kimia@lems.brown.edu						Andrews S, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P945; ANDREWS S, 2003, IN PRESS ADV NEURAL; Bakircioglu M, 1998, HUM BRAIN MAPP, V6, P329; BALMELLI L, 2002, P EUR 2002 GERM; Barzohar M, 1996, IEEE T PATTERN ANAL, V18, P707, DOI 10.1109/34.506793; BERNARDINI F, 2002, IEEE COMPUTER GR JAN; BIENENSTOCK E, 1994, NETWORK-COMP NEURAL, V5, P241, DOI 10.1088/0954-898X/5/2/008; Bienenstock E, 1997, ADV NEUR IN, V9, P838; Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436; Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277; Black MJ, 2000, COMPUT VIS IMAGE UND, V78, P8, DOI 10.1006/cviu.1999.0825; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; Black MJ, 2000, INT J COMPUT VISION, V38, P231, DOI 10.1023/A:1008195307933; Blais BS, 1998, NEURAL COMPUT, V10, P1797, DOI 10.1162/089976698300017142; Blane MM, 2000, IEEE T PATTERN ANAL, V22, P298, DOI 10.1109/34.841760; Cooper DB, 2002, INT C PATT RECOG, P297, DOI 10.1109/ICPR.2002.1047853; COOPER DB, 2001, P VIRT REAL ARCH CUL; Domini F, 1998, J EXP PSYCHOL HUMAN, V24, P1273, DOI 10.1037/0096-1523.24.4.1273; Domini F, 1997, J EXP PSYCHOL HUMAN, V23, P1111; Domini F, 1999, J EXP PSYCHOL HUMAN, V25, P426; Domini F, 2002, J EXP PSYCHOL HUMAN, V28, P816, DOI 10.1037//0096-1523.28.4.816; Domini F, 1998, PERCEPT PSYCHOPHYS, V60, P1164, DOI 10.3758/BF03206166; Duchon AP, 2002, PSYCHOL SCI, V13, P272, DOI 10.1111/1467-9280.00450; Duchon AP, 1998, ADAPT BEHAV, V6, P473, DOI 10.1177/105971239800600306; Edelman S, 2000, SPATIAL VISION, V13, P255, DOI 10.1163/156856800741072; Festa EK, 1997, VISION RES, V37, P3129, DOI 10.1016/S0042-6989(97)00118-1; Gauthier I, 2002, NEURON, V34, P161, DOI 10.1016/S0896-6273(02)00622-0; GEMAN D, 1990, IEEE T PATTERN ANAL, V12, P609, DOI 10.1109/34.56204; GEMAN S, 1992, CVGIP-GRAPH MODEL IM, V54, P281, DOI 10.1016/1049-9652(92)90075-9; Geman S, 2002, Q APPL MATH, V60, P707, DOI 10.1090/qam/1939008; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; GEMAN S, 1987, B INT STAT I, V52; GEMAN S, 1993, MARKOV RANDOM FIELDS, P93; GIDAS B, 1989, IEEE T PATTERN ANAL, V11, P164, DOI 10.1109/34.16712; Gidas B, 2002, Q APPL MATH, V60, P737, DOI 10.1090/qam/1939009; Gidas B, 2000, INT C PATT RECOG, P175, DOI 10.1109/ICPR.2000.905298; GIDAS B, 1991, SPATIAL STAT IMAGE P, P1; Grenander U, 1998, IEEE T PATTERN ANAL, V20, P790, DOI 10.1109/34.709572; Grenander U, 2001, IEEE T PATTERN ANAL, V23, P424, DOI 10.1109/34.917579; Grenander U, 1998, Q APPL MATH, V56, P617, DOI 10.1090/qam/1668732; Grimm CM, 2002, J BIOMECH ENG-T ASME, V124, P136, DOI 10.1115/1.1431266; HELLER A, 1991, P 3 INT C COMP VIS; Hofmann T, 1998, IEEE T PATTERN ANAL, V20, P803, DOI 10.1109/34.709593; Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950; INTRATOR N, 1997, NETWORK, V8, P283; Kang KB, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P198, DOI 10.1109/ICCV.2001.937518; Kearns MJ, 2002, PERCEPTION, V31, P349, DOI 10.1068/p3311; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; KIMIA BB, 1992, J MATH ANAL APPL, V163, P438, DOI 10.1016/0022-247X(92)90260-K; KIMIA BB, 2003, IN PRESS J PHYSL; Kunsch H, 1995, ANN APPL PROBAB, V5, P577, DOI 10.1214/aoap/1177004696; KUTLIROFF G, 2002, THESIS BROWN U; Laidlaw DH, 1998, IEEE T MED IMAGING, V17, P74, DOI 10.1109/42.668696; LEE AB, IN PRESS INT J COMP; LEE TS, UNPUB J OPTICAL SOC; Li L, 2000, VISION RES, V40, P3873, DOI 10.1016/S0042-6989(00)00196-6; MacEvoy SP, 2001, P NATL ACAD SCI USA, V98, P8827, DOI 10.1073/pnas.161280398; Matthews N, 1997, PERCEPTION, V26, P1431, DOI 10.1068/p261431; MILLER MI, 1993, P NATL ACAD SCI USA, V90, P11944, DOI 10.1073/pnas.90.24.11944; Mumford D, 2001, Q APPL MATH, V59, P85, DOI 10.1090/qam/1811096; MUMFORD D, 2003, IN PRESS NOTICES AM; MUMFORD D, 2002, P ICM 2002 BEIJ, V1; Mundy JL, 1998, PHILOS T R SOC A, V356, P1213, DOI 10.1098/rsta.1998.0218; MUNDY JL, 1995, LECT NOTES COMPUTER, V994; MUNDY JL, 1992, APPL GEOMETRIC INVAR; Paradiso MA, 2002, CURR OPIN NEUROBIOL, V12, P155, DOI 10.1016/S0959-4388(02)00311-2; Puzicha J, 1999, PATTERN RECOGN LETT, V20, P899, DOI 10.1016/S0167-8655(99)00056-2; Rossi AF, 1999, J NEUROSCI, V19, P6145, DOI 10.1523/JNEUROSCI.19-14-06145.1999; ROTHWELL CA, 1993, P 4 INT JOINT C COMP; SEBASTIAN T, 2003, PAMI; Sebastian TB, 2003, MED IMAGE ANAL, V7, P21, DOI 10.1016/S1361-8415(02)00065-8; Sheinberg DL, 2002, PERCEPTUAL LEARNING, P95; Sheinberg DL, 2001, J NEUROSCI, V21, P1340; Shenberg DL, 1997, P NATL ACAD SCI USA, V94, P3408, DOI 10.1073/pnas.94.7.3408; Siddiqi K, 2001, VISION RES, V41, P1153, DOI 10.1016/S0042-6989(00)00274-1; Tarel JP, 2000, IEEE T PATTERN ANAL, V22, P663, DOI 10.1109/34.865183; Tarr MJ, 2003, TRENDS COGN SCI, V7, P23, DOI 10.1016/S1364-6613(02)00010-4; Tarr MJ, 1998, NAT NEUROSCI, V1, P275, DOI 10.1038/1089; Tarr MJ, 2000, NAT NEUROSCI, V3, P764, DOI 10.1038/77666; Tarr MJ, 2002, NAT NEUROSCI, V5, P1089, DOI 10.1038/nn948; Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365; TAUBIN G, 2002, P IEEE VIS 2002 BOST; TAUBIN G, 1998, SIGGRAPH 98 ORL FL; VONDERMALSBURG C, 1987, EUROPHYS LETT, V3, P1243, DOI 10.1209/0295-5075/3/11/015; Warren WH, 2001, NAT NEUROSCI, V4, P213, DOI 10.1038/84054; WARREN WH, IN PRESS VISUAL NEUR; Welch L, 1997, VISION RES, V37, P2725, DOI 10.1016/S0042-6989(97)00084-9	88	3	3	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG-SEP	2003	54	1-2					5	11		10.1023/A:1023788516099	http://dx.doi.org/10.1023/A:1023788516099			7	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	678EA					2022-12-18	WOS:000182851000001
J	Hayman, E; Thorhallsson, T; Murray, D				Hayman, E; Thorhallsson, T; Murray, D			Tracking while zooming using affine transfer and multifocal tensors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						active vision; tracking; affine camera; multifocal tensors	LINE CORRESPONDENCES; MOTION	This paper presents algorithms for tracking unknown objects in the presence of zoom. Since prior models are unavailable, point and line matches in affine views are used to characterize the structure and to transfer a fixation point into new images in a sequence. Because any affine projection matrix is permitted, the intrinsic camera parameters such as focal length may change freely. Also, since the techniques do not require long feature tracks, a further desirable property is insensitivity to partial occlusion caused, for instance, by part of the object falling off the image plane while zooming in. If only point matches are available, a previous method based on factorization is applied. When also incorporating lines, the affine trifocal and quadrifocal tensors are used for tracking in monocular and stereo systems respectively. Methods for computing the tensors, minimizing algebraic error, are developed. In comparison with their projective counterparts, the affine tensors offer significant advantages in terms of computation time and convenience of parameterization, and the relations between the different tensors are shown to be much simpler. Successful tracking is demonstrated on several real image sequences.	Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	University of Oxford	Hayman, E (corresponding author), KTH, Dept Numer Anal & Comp Sci, Computat Vis & Act Percept Lab, SE-10044 Stockholm, Sweden.							Astrom K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P285, DOI 10.1109/ICCV.1999.791232; Baillard C., 1999, INT ARCH PHOTOGRAMME, V32, P69; BLAKE A, 1995, ARTIF INTELL, V78, P179, DOI 10.1016/0004-3702(95)00032-1; Blake A., 1998, ACTIVE CONTOURS, DOI [10.1007/978-1-4471-1555-7, DOI 10.1007/978-1-4471-1555-7]; BRETZNER L, 1998, P 5 EUR C COMP VIS F, P141; CANNY J, 1983, THESIS MIT; Cipolla R., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P616, DOI 10.1109/ICCV.1990.139606; DRUMMOND T, 2000, P 6 EUR C COMP VIS E, P20; Fairley SM, 1998, INT J COMPUT VISION, V29, P47, DOI 10.1023/A:1008038713629; FAUGERAS O, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P951, DOI 10.1109/ICCV.1995.466832; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FITZGIBBON AW, 1998, P EUR C COMP VIS, P311; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; HARRIS C, 1992, ACTIVE VISION; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; HARTLEY R, 1998, P 5 ECCV, V1, P20; HARTLEY R, 2000, MULTIPLES VIEW GEOME; Hartley R. I., 1993, P DARPA IM UND WORKS, P361; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Hartley RI, 1997, INT J COMPUT VISION, V22, P125, DOI 10.1023/A:1007936012022; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1064; Hayman E., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P269, DOI 10.1109/ICCV.1999.791230; HAYMAN E, 1996, P 7 BRIT MACH VIS C, V2, P395; HAYMAN E, 2000, THESIS U OXFORD; HEYDEN A, 1998, INT J COMPUTER VISIO, V30; HEYDEN A, 1998, P 5 EUR C COMP VIS, V1, P3; INOUE H, 1992, 1992 IEEE INTERNATIONAL CONF ON ROBOTICS AND AUTOMATION : PROCEEDINGS, VOLS 1-3, P1621, DOI 10.1109/ROBOT.1992.220020; IRANI M, 1994, INT J COMPUT VISION, V12, P5, DOI 10.1007/BF01420982; Kahl F, 1999, INT J COMPUT VISION, V33, P163, DOI 10.1023/A:1008192713051; Kanade T, 1998, PHILOS T R SOC A, V356, P1153, DOI 10.1098/rsta.1998.0215; Kass M., 1987, International Journal of Computer Vision, V1, P321, DOI 10.1007/BF00133570; Kaucic R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P420, DOI 10.1109/ICCV.2001.937548; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LOWE MR, 1987, ADV EATING DISORDERS, V1, P1; MCLAUCHLAN P, 1999, 499 VSSP U SURR CTR; MENDONCA PRS, 1998, P 9 BRIT MACH VIS C, P125; Morris DD, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P696, DOI 10.1109/ICCV.1998.710793; MURRAY DW, 1995, INT J COMPUT VISION, V16, P205, DOI 10.1007/BF01539627; Pahlavan K, 1996, INT J COMPUT VISION, V17, P113, DOI 10.1007/BF00058748; Quan L, 1997, IEEE T PATTERN ANAL, V19, P834, DOI 10.1109/34.608285; Quan L, 1996, INT J COMPUT VISION, V19, P93, DOI 10.1007/BF00131149; QUAN L, 1998, LECT NOTES COMPUTER, V1506, P32; Reid I. D., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P76, DOI 10.1109/ICCV.1993.378233; Reid ID, 1996, INT J COMPUT VISION, V18, P41, DOI 10.1007/BF00126139; Rother C, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P42, DOI 10.1109/ICCV.2001.937497; Schmid C, 1997, PROC CVPR IEEE, P666, DOI 10.1109/CVPR.1997.609397; SHAPIRO LS, 1995, INT J COMPUT VISION, V16, P147, DOI 10.1007/BF01539553; SHASHUA A, 1995, IEEE T PATTERN ANAL, V17, P779, DOI 10.1109/34.400567; Shashua A, 2000, LECT NOTES COMPUT SC, V1842, P710; SHASHUA A, 1994, P ECCV, V1, P479; SHASHUA A, 2000, P 6 EUR C COMP VIS D, P936; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; Thorballsson T., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P450, DOI 10.1109/CVPR.1999.786977; THORHALLSSON T, 2000, THESIS U OXFORD; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TORDOFF B, 2002, P 7 ECCV, V1, P82; TORR PHS, 1995, THESIS OXFORD U; Triggs B, 1996, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.1996.517170; TRIGGS W, 1995, UNPUB GEOMETRY PROJE, V1; TRIGGS W, 2000, P 6 EUR C COMP VIS D, P522; UHLIN T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P679, DOI 10.1109/ICCV.1995.466873; VIEVILLE T, 1993, 2054 INRIA; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P318, DOI 10.1109/34.120327; YUILLE A, 1992, ACTIVE VISION, P21; ZHANG ZY, 1994, IMAGE VISION COMPUT, V12, P110, DOI 10.1016/0262-8856(94)90020-5; ZISSERMAN A, 1992, NOTES GEOMETRIC INVA	69	3	3	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2003	51	1					37	62		10.1023/A:1020988723254	http://dx.doi.org/10.1023/A:1020988723254			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	610FU					2022-12-18	WOS:000178950700002
J	Mellor, JP				Mellor, JP			Geometry and texture from thousands of images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						APGD; 3D reconstruction; multi-camera stereo; large-scale environments; textured model; architectural facades; urban scenes; approximate camera calibration; non-Lambertian surfaces		This paper presents a novel method for automatically recovering dense surface patches using large sets (1000's) of calibrated images taken from arbitrary positions within the scene. Physical instruments, such as Global Positioning System (GPS), inertial sensors, and inclinometers, are used to estimate the position and orientation of each image. Some of the most important characteristics of our approach are that it: (1) uses and refines noisy calibration estimates; (2) compensates for large variations in illumination; (3) tolerates significant soft occlusion (e.g. tree branches); and (4) associates, at a fundamental level, an estimated normal (eliminating the frontal-planar assumption) and texture with each surface patch.	Rose Hulman Inst Technol, Comp Sci & Software Engn Dept, Terre Haute, IN 47803 USA	Rose Hulman Institute Technology	Mellor, JP (corresponding author), Rose Hulman Inst Technol, Comp Sci & Software Engn Dept, Terre Haute, IN 47803 USA.	j.p.mellor@rose-hulman.edu						BECKER S, 1995, P SOC PHOTO-OPT INS, V2410, P447, DOI 10.1117/12.205979; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; COLLINS R, 1995, INTEGRATING PHOTOGRA, V7617; Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097; Coorg S, 1998, PROC CVPR IEEE, P872, DOI 10.1109/CVPR.1998.698707; COORG S, 1998, THESIS MIT; Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191; DECOUTO D, 1998, THESIS MIT; Gering DT, 1999, IEEE WORKSHOP ON MULTI-VIEW MODELING & ANALYSIS OF VISUAL SCENES (MVIEW'99). PROCEEDINGS, P11, DOI 10.1109/MVIEW.1999.781078; GRUEN AW, 1988, PHOTOGRAMM ENG REM S, V54, P633; HANSON AJ, 1992, 515 SRI INT; HSIEH Y, 1996, CVPR, P499; KUTULAKOS KN, 1998, 692 CS U ROCH; KUTULAKOS KN, 1998, 680 CS U ROCH; LI XP, 1995, IEEE T PATTERN ANAL, V17, P1015, DOI 10.1109/34.464565; MELLOR J, 1997, IM UND WORKSH IUW 97, V2, P893; MELLOR J, 1999, AITR1674 MIT; MELLOR J, 1998, IM UND WORKSH IUW 98, V2, P537; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462; Shum HY, 1998, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.1998.698641; SZELISKI R, 1992, COMP GRAPH, V26, P185, DOI 10.1145/142920.134037; Teller S, 1998, PACIFIC GRAPHICS '98, PROCEEDINGS, P45, DOI 10.1109/PCCGA.1998.731997; TELLER S, 1998, IM UND WORKSH IUW 98, V2, P455; TSAI RY, 1983, IEEE T PATTERN ANAL, V5, P159, DOI 10.1109/TPAMI.1983.4767368; *VEXC, 1997, FOT GAM PROD SERV; Vora P. L., 1997, HPL9753; Vora P. L., 1997, HPL9754; WANG X, 1997, CVPR 97, P301	31	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2003	51	1					5	35		10.1023/A:1020931206416	http://dx.doi.org/10.1023/A:1020931206416			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	610FU					2022-12-18	WOS:000178950700001
J	Taycher, L; Darrell, T				Taycher, L; Darrell, T			Range segmentation using visibility constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						foreground detection; stereo; free space		Visibility constraints can aid the segmentation of foreground objects in a scene observed with multiple range imagers. Points may be labeled as foreground if they can be determined to occlude some space in the scene that we expect to be empty. Visibility constraints from a second range view provide evidence of such occlusions. We present an efficient algorithm to estimate foreground points in each range view using explicit epipolar search. In cases where the background pattern is stationary, we show how visibility constraints from other views can generate virtual background values at points with no valid depth in the primary view. We demonstrate the performance of both algorithms for detecting people in indoor office environments with dynamic illumination variation.	MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Taycher, L (corresponding author), MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA.	lodrion@ai.mit.edu; trevor@ai.mit.edu						BEYMER D, 1999, P INT C COMP VIS ICC; BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025; Brumitt B, 2000, LECT NOTES COMPUT SC, V1927, P12; DARRELL T, 2001, P INT C COMP VIS ICC; DEMIRDJIAN D, 2001, P INT C COMP VIS ICC; Foley J. D., 1993, INTRO COMPUTER GRAPH; GRIMSON WEL, 1998, P CVPR 98; HARVILLE M, 2001, WORKSH DET REC EV VI; KONOLIGE K, 1997, 8 INT S ROB RES JAP; KRUMM J, 2000, IEEE WORKSH VIS SURV; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951; RIDDER C, 1995, P INT C REC ADV MECH, P193; SLABAUGH G, 2001, VG 01; STAUFFER C, 1999, P CVPR 99; TOYAMA K, 1999, P INT C COMP VIS ICC; WREN C, 1995, SPIE, V2615	17	3	3	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					89	98		10.1023/A:1014533505864	http://dx.doi.org/10.1023/A:1014533505864			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN					2022-12-18	WOS:000174354700007
J	Chang, CC; Tsai, WH				Chang, CC; Tsai, WH			Determination of head pose and facial expression from a single perspective view by successive scaled orthographic approximations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						head pose; facial expressions; scaled orthographic projection.; human-computer interaction	IMAGE SEQUENCES; OBJECT POSE; FACE	Human faces are the main organs for expressing human emotion. In this study, anew iterative approach to analyzing the head pose and the facial expression of a human face from a single image is proposed. The proposed approach extends the concept of successive scaled orthographic approximations. which was used to estimate the pose of a rigid object, to develop a method to estimate the parameters for a non-rigid object, namely, a human face. The implementation of the proposed method is simple; furthermore, no initial guess is required. The convergency property of the proposed method is also analyzed theoretically and experimentally. Experimental results show that the proposed method is robust and has a high percentage of convergency, and thus prove the feasibility of the proposed approach.	Natl Chiao Tung Univ, Dept Comp & Informat Sci, Hsinchu 300, Taiwan	National Yang Ming Chiao Tung University	Chang, CC (corresponding author), Natl Chiao Tung Univ, Dept Comp & Informat Sci, Hsinchu 300, Taiwan.	whtsai@cis.nctu.edu.tw						AKIMOTO T, 1993, IEEE COMPUT GRAPH, V13, P16, DOI 10.1109/38.232096; ALOIMONOS JY, 1990, IMAGE VISION COMPUT, V8, P179, DOI 10.1016/0262-8856(90)90064-C; Bascle B, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P323, DOI 10.1109/ICCV.1998.710738; Bazaraa MS., 2013, NONLINEAR PROGRAMMIN; CHOI CS, 1994, IEEE T CIRC SYST VID, V4, P257, DOI 10.1109/76.305871; DEMENTHON DF, 1992, LECT NOTES COMPUT SC, V588, P335, DOI 10.1007/BF01450852; Horaud R, 1997, INT J COMPUT VISION, V22, P173, DOI 10.1023/A:1007940112931; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; Horn R. A., 1986, MATRIX ANAL; Kanatani K., 1993, GEOMETRIC COMPUTATIO; Lei YW, 1996, SIGNAL PROCESS-IMAGE, V8, P353, DOI 10.1016/0923-5965(95)00057-7; LI HB, 1993, IEEE T PATTERN ANAL, V15, P545, DOI 10.1109/34.216724; LI HB, 1994, IEEE T CIRC SYST VID, V4, P276; PARKE FI, 1996, COMPUTER FACIAL ANIM; Stoer J., 1993, INTRO NUMERICAL ANAL; TAO H, 1998, P INT WORKSH CAPTECH, P242; TERZOPOULOS D, 1993, IEEE T PATTERN ANAL, V15, P569, DOI 10.1109/34.216726; Thalmann NM, 1998, P IEEE, V86, P870, DOI 10.1109/5.664277; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; Yurii N., 1994, SIAM STUDIES APPL MA; Zhang L, 1998, IEEE T CIRC SYST VID, V8, P781, DOI 10.1109/76.728423	21	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	2002	46	3					179	199		10.1023/A:1014022522159	http://dx.doi.org/10.1023/A:1014022522159			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	536MN					2022-12-18	WOS:000174706700001
J	Pae, SI; Ponce, J				Pae, SI; Ponce, J			On computing structural changes in evolving surfaces and their appearance	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shape representation; scale space; aspect graphs; differential geometry; algebraic surfaces	EXACT ASPECT GRAPHS; CURVED OBJECTS; SINGULARITIES; PROJECTIONS; SHAPES	As a surface undergoes a one-parameter family of deformations, its shape and its appearance change smoothly except at certain critical parameter values where abrupt structural changes occur. This paper considers the case of surfaces defined as the zero set of smooth density functions undergoing a Gaussian diffusion process and addresses the problem of computing the critical parameter values corresponding to structural changes in the parabolic curves of a surface and in its aspect graph. An algorithm based on homotopy continuation and curve tracing is proposed in the case of polynomial density functions, whose zero set is an algebraic surface. It has been implemented and examples are presented.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Pae, SI (corresponding author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.	pae@uiuc.edu; ponce@cs.uiuc.edu						ARNOLD V, 1969, RUSS MATH SURV, V23, P3; Arnold V.I., 1984, CATASTROPHE THEORY; ASADA H, 1986, IEEE T PATTERN ANAL, V8, P2, DOI 10.1109/TPAMI.1986.4767747; Bruce JW, 1996, INT J COMPUT VISION, V17, P291, DOI 10.1007/BF00128235; Bruce JW, 1996, INT J COMPUT VISION, V18, P195, DOI 10.1007/BF00123141; CASTORE G, 1984, SOLID MODELING COMPU, P277; CHAKRAVARTY I, 1982, IPLTR034 RENSS POL I; EGGERT D, 1989, P IEEE WORKSH INT 3D, P102; EGGERT DW, 1993, IEEE T PATTERN ANAL, V15, P1114, DOI 10.1109/34.244674; GIGUS Z, 1991, IEEE T PATTERN ANAL, V13, P442; Hebert M., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P458; Hilbert D., 1952, GEOMETRY IMAGINATION; IKEUCHI K, 1988, P IEEE, V76, P1016, DOI 10.1109/5.5972; KERGOSIEN YL, 1981, CR ACAD SCI I-MATH, V292, P929; KERGOSIEN YL, 1991, IEEE COMPUT GRAPH, V11, P46, DOI 10.1109/38.90567; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1976, BIOL CYBERN, V24, P51, DOI 10.1007/BF00365595; KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321; KOENDERINK JJ, 1979, BIOL CYBERN, V32, P211, DOI 10.1007/BF00337644; KRIEGMAN DJ, 1990, INT J COMPUT VISION, V5, P119, DOI 10.1007/BF00054918; KRIEGMAN DJ, 1991, CURVES SURFACES, P267; LEYTON M, 1988, ARTIF INTELL, V34, P213, DOI 10.1016/0004-3702(88)90039-2; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; MACKWORTH A, 1988, P IEEE C COMP VIS PA, P318; Marr D., 1982, VISION; MONGA O, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P852, DOI 10.1109/CVPR.1994.323912; Morgan A. P., 1987, SOLVING POLYNOMIAL S; Noble A, 1997, COMPUT VIS IMAGE UND, V66, P179, DOI 10.1006/cviu.1997.0615; PAE S, 1999, P IEEE C COMP VIS PA, V2, P196; PAE S, 2000, THESIS U ILLINOIS UR; PETITJEAN S, 1992, INT J COMPUT VISION, V9, P231, DOI 10.1007/BF00133703; PETITJEAN S, 1995, THESIS U NATL POLYTE; PLANTINGA H, 1990, INT J COMPUT VISION, V5, P137, DOI 10.1007/BF00054919; PLATONOVA OA, 1981, RUSS MATH SURV+, V36, P248, DOI 10.1070/RM1981v036n01ABEH002556; PONCE J, 1987, 3 DIMENSIONAL MACHIN, P195; RIEGER JH, 1987, IMAGE VISION COMPUT, V5, P91, DOI 10.1016/0262-8856(87)90033-3; RIEGER JH, 1992, INT J COMPUT VISION, V7, P171, DOI 10.1007/BF00126392; Sethian JA, 1996, LEVEL SET METHODS EV; Shimshoni I, 1997, IEEE T PATTERN ANAL, V19, P315, DOI 10.1109/34.588001; STAM D, 1992, UIUCBIAIRCV9203; STEWMAN J, 1988, P INT C COMP VIS TAM, P495; THIRION J, 1993, 18811 INRIA; Thom R., 1972, STRUCTURAL STABILITY; WANG R, 1990, INT C PATTERN RECOGN, P8; Weatherburn C.E, 2016, DIFFERENTIAL GEOMETR; WHITNEY H, 1955, ANN MATH, V62, P374, DOI 10.2307/1970070; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; YUILLE AL, 1986, IEEE T PATTERN ANAL, V8, P15, DOI [10.1109/34.41383, 10.1109/TPAMI.1986.4767748]	49	3	4	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2001	43	2					113	131		10.1023/A:1011170702891	http://dx.doi.org/10.1023/A:1011170702891			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	451EH					2022-12-18	WOS:000169787800003
J	Abdellatif, M; Tanaka, Y; Gofuku, A; Nagai, I				Abdellatif, M; Tanaka, Y; Gofuku, A; Nagai, I			Color constancy using the inter-reflection from a reference nose	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						color constancy; nose method; color imaging; robot vision; human color vision	SURFACE SPECTRAL REFLECTANCE; HIGHLIGHTS; IMAGES; ILLUMINATION; COMPONENTS; LIGHTNESS	This paper introduces a novel camera attachment for measuring the illumination color spatially in the scene. The illumination color is then used to transform color appearance in the image into that under white light. The main idea is that the scene inter-reflection through a reference camera-attached surface "Nose" can, under some conditions, represent the illumination color directly. The illumination measurement principle relies on the satisfaction of the gray world assumption in a local scene area or the appearance of highlights, from dielectric surfaces. Scene inter-reflections are strongly blurred due to optical dispersion on the nose surface and defocusing of the nose surface image. Blurring smoothes the intense highlights and it thus becomes possible to measure the nose inter-reflection under conditions in which intensity variation in the main image would exceed the sensor dynamic range. We designed a nose surface to reflect a blurred scene version into a small image section, which is interpreted as a spatial illumination image. The nose image is then mapped to the main image for adjusting every pixel color. Experimental results showed that the nose inter-reflection color is a good measure of illumination color when the model assumptions are satisfied. The nose method performance, operating on real images, is presented and compared with the Retinex and the scene-inserted white patch methods.	Ain Shams Univ, Mechatron Lab, Cairo, Egypt; Okayama Univ, Fac Engn, Okayama 700, Japan	Egyptian Knowledge Bank (EKB); Ain Shams University; Okayama University	Abdellatif, M (corresponding author), Ain Shams Univ, Mechatron Lab, Cairo, Egypt.		NAGAI, Isaku/B-2123-2011; Abdellatif, Mohamed/D-5416-2014; ABDEL-LATIF, Mohamed E./N-3372-2018	Abdellatif, Mohamed/0000-0002-7641-4723; ABDEL-LATIF, Mohamed E./0000-0003-4306-2933				HEALEY G, 1994, J OPT SOC AM A, V11, P3003, DOI 10.1364/JOSAA.11.003003; HEALEY G, 1991, IMAGE VISION COMPUT, V9, P333, DOI 10.1016/0262-8856(91)90038-Q; HO JA, 1990, IEEE T PATTERN ANAL, V12, P966, DOI 10.1109/34.58869; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; HORN BKP, 1985, ROBOT VISION; HURLBERT A, 1986, J OPT SOC AM A, V3, P1684, DOI 10.1364/JOSAA.3.001684; JUDD DB, 1964, J OPT SOC AM, V54, P1031, DOI 10.1364/JOSA.54.001031; KAWATA S, 1987, J OPT SOC AM A, V4, P2101, DOI 10.1364/JOSAA.4.002101; KLINKER GJ, 1990, INT J COMPUT VISION, V4, P7, DOI 10.1007/BF00137441; KLINKER GJ, 1988, INT J COMPUT VISION, V2, P7, DOI 10.1007/BF00836279; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; LAND EH, 1986, P NATL ACAD SCI USA, V83, P3078, DOI 10.1073/pnas.83.10.3078; LEE HC, 1986, J OPT SOC AM A, V3, P1694, DOI 10.1364/JOSAA.3.001694; LEE HC, 1990, IEEE T PATTERN ANAL, V12, P402, DOI 10.1109/34.50626; MALONEY LT, 1986, J OPT SOC AM A, V3, P1673, DOI 10.1364/JOSAA.3.001673; MALONEY LT, 1986, J OPT SOC AM A, V3, P29, DOI 10.1364/JOSAA.3.000029; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; NOVAK CL, 1992, PHYSICS BASED VISION, P284; SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409; SHAFER SA, 1990, SPIE P, V1250, P222; TOMINAGA S, 1989, J OPT SOC AM A, V6, P576, DOI 10.1364/JOSAA.6.000576; TOMINAGA S, 1990, J OPT SOC AM A, V7, P312, DOI 10.1364/JOSAA.7.000312; Tsukada M., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P385, DOI 10.1109/ICCV.1990.139557; WANDELL BA, 1987, IEEE T PATTERN ANAL, V9, P2, DOI 10.1109/TPAMI.1987.4767868	24	3	3	0	3	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2000	39	3					171	194		10.1023/A:1026559628005	http://dx.doi.org/10.1023/A:1026559628005			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	382EJ					2022-12-18	WOS:000165809800001
J	Horaud, R; Chaumette, F				Horaud, R; Chaumette, F			Special issue on image-based servoing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									INRIA Rhone Alpes, F-38330 Montbonnot, France; INRIA Rennes, IRISA, F-35042 Rennes, France		Horaud, R (corresponding author), INRIA Rhone Alpes, 655 Ave Europe, F-38330 Montbonnot, France.		Horaud, Radu/AAR-5982-2021; Francois, Chaumette/AAH-1481-2021	Horaud, Radu/0000-0001-5232-024X; Francois, Chaumette/0000-0002-1238-4385					0	3	3	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	37	1					5	6		10.1023/A:1008188211640	http://dx.doi.org/10.1023/A:1008188211640			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	341FC					2022-12-18	WOS:000088579500001
J	Kinoshita, K; Lindenbaum, M				Kinoshita, K; Lindenbaum, M			Robotic control with partial visual information	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						visual servoing; image-based control; row-action method; POCS; peg-in-hole	SYSTEM; TRACKING; FEEDBACK; VISION	We consider a robotic setting and a class of control tasks that rely on partial visual information. These tasks are difficult in the sense that at every given moment, the available information is insufficient for the control task. This implies that the image Jacobian, which relates the image space and the control space, is no longer of full rank. However, the amount of information collected throughout the control process is still large and thus seems sufficient for carrying out the task. Such situations commonly arise when the object is frequently occluded from one of the camera in a stereo pair or when only one moving camera is available. We propose a generic control for such tasks and characterize the conditions required for the success of the task. The analysis is based on the observation that mathematically the behavior of such systems is related to a class of row-action optimization algorithms which are special cases of POCS (Projection On Convex Sets) algorithms. In the second part of the paper we focus on one particular task from this class: position and orientation control with a single rotating camera. We show that this task can be carried out, in principle, for any camera rotation and suggest efficient control and camera moving strategies. We substantiate our claims by simulations and experiments. Interestingly, it seems that the advisable control law is not consistent with simple intuition.	ATR Human Informat Proc Res Labs, Seika, Kyoto 6190288, Japan; Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Kinoshita, K (corresponding author), ATR Human Informat Proc Res Labs, 2-2-2 Hikaridai, Seika, Kyoto 6190288, Japan.							ALLEN PK, 1993, IEEE T ROBOTIC AUTOM, V9, P152, DOI 10.1109/70.238279; CENSOR Y, 1981, SIAM REV, V23, P444, DOI 10.1137/1023097; COMBETTES PL, 1993, P IEEE, V81, P182, DOI 10.1109/5.214546; CORKE PI, 1993, VISUAL SERVOING; Dickmanns E.D., 1988, MACH VISION APPL, V1, P241; DICKMANNS ED, 1988, MACH VISION APPL, V1, P223; ESPIAU B, 1992, IEEE T ROBOTIC AUTOM, V8, P313, DOI 10.1109/70.143350; Hager GD, 1997, IEEE T ROBOTIC AUTOM, V13, P582, DOI 10.1109/70.611326; HASHIMOTO K, 1993, VISUAL SERVOING; HAUMETTE F, 1998, LECT NOTES CONTROL I, V237, P66; Hutchinson S, 1996, IEEE T ROBOTIC AUTOM, V12, P651, DOI 10.1109/70.538972; KINOSHITA K, 1998, INT C COMP VIS, P883; NELSON BJ, 1994, IEEE C COMP VIS PATT, P829; NICKELS K, 1997, WORKSH NEW TRENDS IM, P53; PAPANIKOLOPOULOS NP, 1993, IEEE T ROBOTIC AUTOM, V9, P14, DOI 10.1109/70.210792; Raik E., 1967, USSR COMP MATH MATH, V7, P1, DOI DOI 10.1016/0041-5553(67)90113-9; SHIRAI Y, 1973, PATTERN RECOGN, V5, P99, DOI 10.1016/0031-3203(73)90015-0; STRANG G, 1976, LINEAR ALGEBRA ITS A; WEISS LE, 1987, IEEE T ROBOTIC AUTOM, V3, P404, DOI 10.1109/JRA.1987.1087115; YOSHIMI BH, 1995, IEEE T ROBOTIC AUTOM, V11, P516, DOI 10.1109/70.406936	20	3	3	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	37	1					65	78		10.1023/A:1008129513457	http://dx.doi.org/10.1023/A:1008129513457			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	341FC					2022-12-18	WOS:000088579500005
J	Poggio, T; Verri, A				Poggio, T; Verri, A			Introduction: Learning and vision at CBCL	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material						machine vision; learning; regularization; graphics	OBJECT CLASSES; IMAGE	This special issue is dedicated to some of the recent work at the Center for Biological and Computational Learning at MIT in applying machine learning techniques to computer vision.	MIT, Artificial Intelligence Lab, Ctr Biol & Computat Learning, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Poggio, T (corresponding author), MIT, Artificial Intelligence Lab, Ctr Biol & Computat Learning, Cambridge, MA 02139 USA.							Beymer D, 1996, SCIENCE, V272, P1905, DOI 10.1126/science.272.5270.1905; BEYMER D, 1993, 1431 AI MIT; BLANZ V, 1999, P SIGGRAPH; BULTHOFF HH, 1992, P NATL ACAD SCI USA, V89, P60, DOI 10.1073/pnas.89.1.60; COOTES TF, 1998, P EUR C COMP VIS, V2, P484; EVGENIOU T, 1999, 171 CBCL MIT; Jones MJ, 1997, CURR BIOL, V7, P991, DOI 10.1016/S0960-9822(06)00419-2; Jones MJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P683, DOI 10.1109/ICCV.1998.710791; LOGOTHETIS NK, 1995, CURR BIOL, V5, P552, DOI 10.1016/S0960-9822(95)00108-4; POGGIO T, 1990, NATURE, V343, P263, DOI 10.1038/343263a0; Poggio T., 1992, 1347 AI MIT; POGGIO T, 1992, 1354 AI MIT; Vapnik V.N, 1998, STAT LEARNING THEORY; Vetter T, 1997, IEEE T PATTERN ANAL, V19, P733, DOI 10.1109/34.598230; VETTER T, 1997, IEEE C COMP VIS PATT, P40	16	3	3	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	38	1					5	7		10.1023/A:1008127215780	http://dx.doi.org/10.1023/A:1008127215780			3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	342GH					2022-12-18	WOS:000088636500001
J	Belhumeur, PN; Duncan, JS; Hager, GD; McDermott, DV; Morse, AS; Zucker, SW				Belhumeur, PN; Duncan, JS; Hager, GD; McDermott, DV; Morse, AS; Zucker, SW			Computational vision at Yale	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							RECOGNITION; SUBSTRATE; OBJECTS; IMAGES; MODELS; SHAPE	We present a brief introduction to the five articles that make up this special issue: Shock Graphs and Shape Matching, The Bas-Relief Ambiguity, Incremental Focus of Attention for Robust Vision Based Tracking, What Tasks can be Performed with an Uncalibrated Stereo Vision System, and Volumetric Deformation Analysis Using Mechanics-Based Data Fusion: Application in Cardiac Motion Recovery. This introduction and accompanying articles provide a by no means exhaustive, but hopefully representative sampling of the computational vision research at Yale University.	Yale Univ, Ctr Computat Vis & Control, Dept Elect Engn, New Haven, CT 06520 USA; Yale Univ, Dept Diagnost Radiol, Imaging Proc & Anal Grp, New Haven, CT 06520 USA; Yale Univ, Dept Elect Engn, Imaging Proc & Anal Grp, New Haven, CT 06520 USA; Yale Univ, Ctr Computat Vis & Control, Dept Comp Sci, New Haven, CT 06520 USA	Yale University; Yale University; Yale University; Yale University	Belhumeur, PN (corresponding author), Yale Univ, Ctr Computat Vis & Control, Dept Elect Engn, New Haven, CT 06520 USA.		Hager, Gregory D/A-3222-2010					AUGUST J, 1999, INT C COMP VIS; AUGUST J, 1999, P IEEE C COMP VIS PA; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Belhumeur PN, 1998, INT J COMPUT VISION, V28, P245, DOI 10.1023/A:1008005721484; Belhumeur PN, 1999, PATTERN ANAL APPL, V2, P82, DOI 10.1007/s100440050017; BELHUMEUR PN, 1997, IEEE P C COMP VIS PA; Chakraborty A, 1999, IEEE T PATTERN ANAL, V21, P12, DOI 10.1109/34.745730; Chui H, 1999, LECT NOTES COMPUT SC, V1613, P168; DOBBINS A, 1987, NATURE, V329, P438, DOI 10.1038/329438a0; Dodds Z, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1607, DOI 10.1109/ROBOT.1999.772589; DODDS Z, 1999, LECT NOTES COMPUTER, V1542, P312; DUBUC B, 1995, 5 INT C COMP VIS; Duncan J, 1998, PROG BIOPHYS MOL BIO, V69, P333, DOI 10.1016/S0079-6107(98)00014-5; Elder JH, 1998, VISION RES, V38, P143, DOI 10.1016/S0042-6989(97)00138-7; GEORGHIADES A, 1998, IEEE C CVPR; GEORGHIADES A, 1999, MVIEW 99 WORKSH IEEE; HAGER G, 1998, P IEEE INT C DEC CON; Hager GD, 1998, COMPUT VIS IMAGE UND, V69, P23, DOI 10.1006/cviu.1997.0586; HAGER GD, 1998, IEEE T PATTERN ANAL; IVERSON LA, 1995, IEEE PAMI; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; KRIEGMAN D, 1998, LECT NOTES CONTROL I, V237; KRIEGMAN DJ, 1998, EUR C COMP VIS; Miller DA, 1999, NEURAL COMPUT, V11, P21, DOI 10.1162/089976699300016782; MILLER DA, 1992, NEURAL COMPUT, V4, P167, DOI 10.1162/neco.1992.4.2.167; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; PAPADEMETRIS X, 1999, INF P MED IM; PELILLO M, 1998, P EUR C COMP VIS; PENTLAND A, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P84, DOI 10.1109/CVPR.1994.323814; Peterson J, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1144, DOI 10.1109/ROBOT.1999.772516; POGGIO T, 1994, P IM UND WORKSH, V2, P843; Rangarajan A, 1997, LECT NOTES COMPUT SC, V1230, P29; RANGARAJAN A, 1999, LNCS, V1654, P237; Rasmussen C, 1998, PROC CVPR IEEE, P16, DOI 10.1109/CVPR.1998.698582; Rasmussen C, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P938; Reid A., 1999, Proceedings of the 1999 International Conference on Software Engineering (IEEE Cat. No.99CB37002), P484, DOI 10.1109/ICSE.1999.841038; SHOKOUFANDEH A, 1999, CVPR 99; Siddiqi K, 1999, IMAGE VISION COMPUT, V17, P365, DOI 10.1016/S0262-8856(98)00130-9; SIDDIQI K, 1999, INT J COMPUTER VISIO; SIDDIQI K, 1999, INT C COMP VIS; SIDDIQI K, 1999, INVESTIGATIVE OPTH V; SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519; Skrinjar O, 1998, LECT NOTES COMPUT SC, V1496, P641, DOI 10.1007/BFb0056250; Tagare HD, 1997, IEEE T MED IMAGING, V16, P108, DOI 10.1109/42.552060; Tagare HD, 1997, LECT NOTES COMPUT SC, V1230, P489; Tagare HD, 1998, IEEE INT CONF ROBOT, P2530, DOI 10.1109/ROBOT.1998.680722; Tagare HD, 1999, IEEE T MED IMAGING, V18, P570, DOI 10.1109/42.790457; Toyama K, 1996, IEEE INT CONF ROBOT, P2636, DOI 10.1109/ROBOT.1996.506560; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; VINCZE M, 1999, ROBUST VISION VISION; Wang YM, 1998, PROC CVPR IEEE, P338, DOI 10.1109/CVPR.1998.698628; ZENG X, 1998, COMPUTER VISION PATT, P708; Zucker SW, 1989, NEURAL COMPUT, V1, P68, DOI 10.1162/neco.1989.1.1.68	55	3	3	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1999	35	1					5	12		10.1023/A:1008181109865	http://dx.doi.org/10.1023/A:1008181109865			8	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	265NY					2022-12-18	WOS:000084251300001
J	Salden, AH; Romeny, BMT; Viergever, MA				Salden, AH; Romeny, BMT; Viergever, MA			Linearised euclidean shortening flow of curve geometry	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						linearised shortening flow; frame field; metric; connection; torsion; curvature; scale-space; similarity jet	SCALE-SPACE; PLANAR CURVES; MEAN-CURVATURE; HEAT-EQUATION; SINGULARITIES; EVOLUTION; SHAPE; MULTISCALE; SURFACES; POLYGONS	The geometry of a space curve is described in terms of a Euclidean invariant frame field, metric, connection, torsion and curvature. Here the torsion and curvature of the connection quantify the curve geometry. In order to retain a stable and reproducible description of that geometry, such that it is slightly affected by non-uniform protrusions of the curve, a linearised Euclidean shortening flow is proposed. (Semi)-discretised versions of the flow subsequently physically realise a concise and exact (semi-)discrete curve geometry. Imposing special ordering relations the torsion and curvature in the curve geometry can be retrieved on a multi-scale basis not only for simply closed planar curves but also for open, branching, intersecting and space curves of non-trivial knot type. In the context of the shortening flows we revisit the maximum principle, the semi-group property and the comparison principle normally required in scale-space theories. We show that our linearised flow satisfies an adapted maximum principle, and that its Green's functions possess a semi-group property. We argue that the comparison principle in the case of knots can obstruct topological changes being in contradiction with the required curve simplification principle. Our linearised flow paradigm is not hampered by this drawback; all non-symmetric knots tend to trivial ones being infinitely small circles in a plane. Finally, the differential and integral geometry of the multi-scale representation of the curve geometry under the flow is quantified by endowing the scale-space of curves with an appropriate connection, and calculating related torsion and curvature aspects. This multi-scale modern geometric analysis forms therewith an alternative for curve description methods based on entropy scale-space theories.	Univ Utrecht Hosp, Imaging Sci Inst, NL-3584 CX Utrecht, Netherlands	Utrecht University; Utrecht University Medical Center	Salden, AH (corresponding author), Univ Utrecht Hosp, Imaging Sci Inst, Room E-01-334,Heidelberglaan 100, NL-3584 CX Utrecht, Netherlands.	Alfons.Salden@gmd.de; B.terHaarRomeny@isi.uu.nl; M.Viergever@isi.uu.nl	Viergever, Max A/J-1215-2014; Romenij, Bart M. ter Haar/A-5323-2013					ABRESCH U, 1986, J DIFFER GEOM, V23, P175; ALTSCHULER SJ, 1992, J DIFFER GEOM, V35, P283; ALVAREZ L, 1992, CR ACAD SCI I-MATH, V315, P135; Alvarez L, 1994, ACTA NUMER, V3, P1; AMBROSIO L, 1994, LEVEL SET APPROACH M; ANGENENT S, 1991, J DIFFER GEOM, V33, P601; ANGENENT S, 1991, ANN MATH, V133, P171, DOI 10.2307/2944327; Angenent S, 1998, J AM MATH SOC, V11, P601, DOI 10.1090/S0894-0347-98-00262-8; ANGENENT S, 1990, ANN MATH, V132, P451, DOI 10.2307/1971426; BARNATAN D, 1995, TOPOLOGY, V34, P423, DOI 10.1016/0040-9383(95)93237-2; BLOM J, 1993, J VISUAL COMMUNICATI, V1, P1; Brakke K., 1978, MOTION SURFACE ITS M, V20; Bruckstein AM, 1997, J MATH IMAGING VIS, V7, P225, DOI 10.1023/A:1008226427785; Bruckstein AM, 1995, INT J PATTERN RECOGN, V9, P991, DOI 10.1142/S0218001495000407; Calugareanu G., 1961, CLASSES ISOTOPIE NOE, P588, DOI 10.21136/CMJ.1961.100486; Cartan, 1937, LECONS THEORIE ESPAC; CARTAN E, 1955, VARIETES CONNEXION A; Cartan E, 1937, THEORIE GROUPES FINI; DARBOUX MG, 1878, B SCI MATH, V2, P298; Eberly D.H., 1994, THESIS U N CAROLINA; EIDELMAN SD, 1962, PARABOLIC SYSTEMS; EVANS LC, 1991, J DIFFER GEOM, V33, P635, DOI 10.4310/jdg/1214446559; Faugeras O., 1994, 3 DIMENSIONAL COMPUT; FAVARD J, 1957, COURS GEOMETRIE DIFF; FREEDMAN MH, 1994, ANN MATH, V139, P1, DOI 10.2307/2946626; GAGE M, 1986, J DIFFER GEOM, V23, P69; GAGE ME, 1984, INVENT MATH, V76, P357, DOI 10.1007/BF01388602; GAGE ME, 1983, DUKE MATH J, V50, P1225, DOI 10.1215/S0012-7094-83-05052-4; GERAETS R, 1995, P COMP SCI NETH SION, P86; GERHARDT C, 1990, J DIFFER GEOM, V32, P299; GERIG G, 1993, DAGM S INF AKT, P289; Greenberg M.J., 1981, ALGEBRAIC TOPOLOGY 1; GRIFFITHS A, 1983, PROGR MATH, V25; HIRSCH W, 1976, DIFFERENTIAL TOPOLOG; HUISKEN G, 1984, J DIFFER GEOM, V20, P237; Jain A. K., 1989, FUNDAMENTALS DIGITAL; KADI A, 1983, LECT NOTES PHYSICS, V174; Kalitzin SN, 1998, J MATH IMAGING VIS, V9, P253, DOI 10.1023/A:1008376504545; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; KIMIA BB, 1992, J MATH ANAL APPL, V163, P438, DOI 10.1016/0022-247X(92)90260-K; KIMIA BB, 1991, P VIS FORM WORKSH, P333; Kleinert H, 1989, GAUGE FIELDS CONDENS, VI; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1986, BIOL CYBERN, V53, P383, DOI 10.1007/BF00318204; KOENDERINK JJ, 1992, LECT NOTES COMPUT SC, V653, P233; KOENDERINK JJ, 1993, SPIE, V2031, P2; KOENDERINK JJ, 1981, 2 INT VIS PSYCH MED; LINDEBERG T, 1990, IEEE T PATTERN ANAL, V12, P234, DOI 10.1109/34.49051; Lindeberg T., 1994, SCALE SPACE THEORY C; MILNOR JW, 1949, ANN MATH, V52, P248; MILNOR T, 1949, SCALE SPACE THEORY C, V52, P248; MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591; Mokhtarian F., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P298, DOI 10.1109/CVPR.1988.196252; Mokhtarian F., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P269, DOI 10.1109/CVPR.1989.37860; MOKHTARIAN F, 1986, IEEE T PATTERN ANAL, V8, P34, DOI 10.1109/TPAMI.1986.4767750; Mokhtarian F., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P660, DOI 10.1109/CVPR.1993.341040; MOKHTARIAN F, 1988, P INT C COMP VIS TAR, P100; MOKHTARIAN F, 1988, P IEEE CVPR, P318; OKUBO T, 1987, DIFFERENTIAL GEOMETR; Olver P.J., 1986, GRADUATE TEXTS MATH, V107; Oreifej O., 2013, DIFFERENTIAL GEOMETR; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; POHL WF, 1968, J MATH MECH, V17, P975; POMMARET J, 1978, SYSTEMS PARTIAL DIFF; Reidemeister K., 1932, KNOTENTHEORIE; Romeny B.M., 1994, GEOMETRY DRIVEN DIFF; ROUGON N, 1993, SPIE P MATH METHODS, V2035, P2; Salden A.H, 1996, THESIS UTRECHT U NET; Salden AH, 1998, J MATH IMAGING VIS, V9, P5, DOI 10.1023/A:1008253609285; Salden AH, 1997, LECT NOTES COMPUT SC, V1252, P248; Salden AH, 1998, J MATH IMAGING VIS, V9, P103, DOI 10.1023/A:1008300826001; SALDEN AH, 1998, UNPUB J MATH IMAGING; SALDEN AH, 1995, IN PRESS P C DIFF GE; SALDEN AH, 1993, IMAGE PROCESSING 93, P141; SALDEN AH, 1998, LECT NOTES COMPUTER, V1352, P65; SALDEN AH, 1999, UNPUB J MATH IMAGING; SALDEN AH, 1997, 9 INT C IM AN PROC, P158; SALDEN AH, 1999, SPIE, V3716, P155; SALDEN AH, 1997, CD ROM, V341; Santal LA., 1953, INTRO INTEGRAL GEOME, V1198; SAPIRO G, 1993, PATTERN RECOGN, V26, P1363, DOI 10.1016/0031-3203(93)90142-J; SAPIRO G, 1995, IEEE T PATTERN ANAL, V17, P67, DOI 10.1109/34.368150; SAPIRO G, 1993, INT J COMPUT VISION, V11, P25, DOI 10.1007/BF01420591; SCHWARTZ R, 1992, INVENT MATH, V110, P627, DOI 10.1007/BF01231347; THURSTON D, 1995, THESIS HARVARD U US; TZE CH, 1988, INT J MOD PHYS A, V3, P1959, DOI 10.1142/S0217751X88000825; Weickert J., 1996, THESIS U KAISERSLAUT; WHITE JH, 1969, AM J MATH, V91, P693, DOI 10.2307/2373348; WOLFF L, 1992, PHYSICS BASED VISION; Yano K., 1955, LIE DERIVATIVES ITS; [No title captured]	93	3	3	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1999	34	1					29	67		10.1023/A:1008172320786	http://dx.doi.org/10.1023/A:1008172320786			39	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	248WN					2022-12-18	WOS:000083299900003
J	Fairley, SM; Reid, ID; Murray, DW				Fairley, SM; Reid, ID; Murray, DW			Transfer of fixation using affine structure: Extending the analysis to stereo	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							MOTION	This paper describes an algorithm for maintaining fixation upon a 3D body-centred point using 3D affine transfer, extending an earlier monocular method to stereo cameras. Transfer is based on corners detected in the image and matched over time and in stereo. The paper presents a method using all available matched data, providing immunity to noise and poor conditioning. The algorithm, implemented at video rates on a multi-processor machine, incorporates controlled degradation in the presence of insufficient data. Results are given from experiments using a four-axis active stereo camera platform, first which show the greater stability of the fixation point over the monocular method, both as it appears in the image and occurs in the scene; and, secondly, which show the recovery and evolution of 3D affine structure during fixation. It is shown that fixation and explicit structure recovery can occur separately, allowing the information required for gaze control to be computed in a fixed time.	Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	University of Oxford	Fairley, SM (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.			Reid, Ian/0000-0001-7790-6423				BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BARBER CG, 1993, GCG53 U MINN GEOM CT; Bradshaw KJ, 1997, IEEE T PATTERN ANAL, V19, P219, DOI 10.1109/34.584099; DEMEY S, 1992, P BRIT MACH VIS C LE, P49; FAUGERAS OD, 1992, P 2 EUR C COMP VIS S, P563; FERMULLER C, 1993, INT J COMPUT VISION, V11, P165, DOI 10.1007/BF01469227; HARRIS C, 1992, ACTIVE VISION; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; Mundy J., 1992, GEOMETRIC INVARIANCE; MURRAY DW, 1995, INT J COMPUT VISION, V16, P205, DOI 10.1007/BF01539627; OLSON TJ, 1991, INT J COMPUT VISION, V7, P67, DOI 10.1007/BF00130490; PAHLAVAN K, 1993, P 4 INT C COMP VIS B, P412; Reid ID, 1996, INT J COMPUT VISION, V18, P41, DOI 10.1007/BF00126139; REID ID, 1993, P 4 INT C COMP VIS B, P76; SHAPIRO LS, 1993, THESIS U OXFORD; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; WANG H, 1991, P BARNAIMAGE 91 BARC; YUILLE A, 1992, ACTIVE VISION, P21	18	3	3	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1998	29	1					47	58		10.1023/A:1008038713629	http://dx.doi.org/10.1023/A:1008038713629			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	125XY					2022-12-18	WOS:000076263700003
J	Kasprzak, W; Niemann, H				Kasprzak, W; Niemann, H			Adaptive road recognition and ego-state tracking in the presence of obstacles	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						adaptive recognition; competitive hypotheses; image sequences; Kalman filter; moving camera; road stripes; vanishing point		An approach to road recognition and ego-state tracking in monocular image sequences of traffic scenes is described. The main contribution of this paper is the adaptive recognition scheme, which deals with competitive road hypotheses, and its application in several processing steps of an image sequence analysis system. No manual initialization of the tracked road is required and the change of the road type is allowed. The road parameters to be recognized are the road width, road lane number and road curvature. For exact estimation of road curvature the translational and rotational velocities of the ego-car are assumed to be available. The estimated ego-state parameters are the camera orientation (which is derived due to vanishing point tracking) and the camera position relative to the road center line.	Bavarian Res Ctr Knowledge Based Syst, Knowledge Proc Res Grp, D-91058 Erlangen, Germany		Kasprzak, W (corresponding author), Warsaw Univ Technol, Inst Control & Computat Engn, Warsaw, Poland.	kasprzak@orwiss.uni-erlangen.de; niemann@forwiss.uni-erlangen.de						BRAMMER K, 1975, KALMAN BUCY FILTER D; CHEN X, 1993, IEEE S INTELLIGENT V, P219; DICKMANNS ED, 1992, IEEE T PATTERN ANAL, V14, P199, DOI 10.1109/34.121789; FUJIMORI T, 1990, VISION NAVIGATION CA, pCH4; GENNERY DB, 1992, INT J COMPUT VISION, V7, P243, DOI 10.1007/BF00126395; GRAEFE V, 1993, P IEEE S INT VEH, P135; JOCHEM TM, 1996, CMURITR9614 CARN MEL; KASPRZAK W, 1994, BRIT MACHINE VISION, V2, P691; KASPRZAK W, 1995, SIGNAL PROCESS, V7, P708; Kasprzak W., 1995, INT ARCH PHOTOGR 5W1, V30, P208; KLUGE K, 1990, VISION NAVIGATION CA, pCH3; KOLLER D, 1993, INT J COMPUT VISION, V10, P257, DOI 10.1007/BF01539538; KOLLER D, 1992, DETEKTION VERFOLGUNG; Masaki I., 1992, VISION BASED VEHICLE; MAURER M, 1996, 13 INT C PATT REC VI; MORGAN AD, 1990, IMAGE VISION COMPUT, V8, P233, DOI 10.1016/0262-8856(90)90070-L; POLK A, 1992, VISION BASED VEHICLE, V1, P284; Pomerleau D. A., 1993, P IEEE INT VEH S, P19; POMERLEAU DA, 1991, ADV NEURAL INFORMATI, V3, P429; POMERLEAU DA, 1995, P IEEE C INTELLIGENT; SCHAASER L, 1992, VISION BASED VEHICLE, P239; Wetzel D., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P65, DOI 10.1109/ACV.1994.341291; WU JJ, 1988, INT J COMPUT VISION, V3, P373	23	3	7	0	3	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1998	28	1					5	26		10.1023/A:1008025713790	http://dx.doi.org/10.1023/A:1008025713790			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZX971					2022-12-18	WOS:000074573800001
J	Wallack, A; Manocha, D				Wallack, A; Manocha, D			Robust algorithms for object localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object registration; object localization; resultants	RECOGNITION; TRACKING; DISTANCE; POSE	Model-based localization, the task of estimating an object's pose from sensed and corresponding model features, is a fundamental task in machine vision. Exact constant time localization algorithms have been developed for the case where the sensed features and the model features are the same type. Still, it is not uncommon for the sensed features and the model features to be of different types, i.e., sensed data points may correspond to model faces or edges. Previous localization approaches have handled different model and sensed features of different types via sampling and synthesizing virtual features to reduce the problem of matching features of dissimilar types to the problem of matching features of similar types. Unfortunately, these approaches may be suboptimal because they introduce artificial errors. Other localization approaches have reformulated object localization as a nonlinear least squares problem where the error is between the sensed data and model features in image coordinates (the Euclidean image error metric). Unfortunately, all of the previous approaches which minimized the Euclidean image error metric relied on gradient descent methods to find the global minima, and gradient descent methods may suffer from problems of local minima. In this paper, we describe an exact, efficient solution to the nonlinear least squares minimization problem based upon resultants, linear algebra, and numerical techniques. On a SPARC 20, our localization algorithm runs in a few microseconds for rectilinear polygonal models, a few milliseconds for generic polygonal models, and one second for generalized polygonal models (models composed of linear edges and circular arcs).	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA; Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA	University of California System; University of California Berkeley; University of North Carolina; University of North Carolina Chapel Hill	Wallack, A (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.			Manocha, Dinesh/0000-0001-7047-9801				ANDERSON E, 1992, LAPACK USERS GUIDE R; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BROCKET R, LINEAR ALGEBRA ITS A; CANNY J, 1990, J SYMB COMPUT, V9, P241, DOI 10.1016/S0747-7171(08)80012-0; DERICHE R, 1990, IMAGE VISION COMPUT, V8, P261, DOI 10.1016/0262-8856(90)80002-B; Dixon A.L., 1908, P LOND MATH SOC, V6, P209; EMIRIS IZ, 1994, P INT S SYMB ALG COM, P114; ERC W, 1990, OBJECT RECOGNITION C; ERIC W, 1984, INT J ROBOT RES, V3, P3; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; Garbow B.S., 1977, LECT NOTES PHYS, V51; Golub G. H., 1996, MATRIX COMPUTATIONS; HORN BKP, 1991, J OPT SOC AM A, V8, P1630, DOI 10.1364/JOSAA.8.001630; HORN BKP, 1989, ROBOT VISION; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; KALVIN A, 1986, INT J ROBOT RES, V5, P38, DOI 10.1177/027836498600500403; KOLLER D, 1993, INT J COMPUT VISION, V10, P257, DOI 10.1007/BF01539538; KRIEGMAN DJ, RECOGNIZING POSITION; MACAULAY FS, 1964, ALGEBRAIC THEORY MOD; MACAULAY FS, 1902, P LOND MATH SOC, P3; MANOCHA D, 1992, THESIS U CALIFORNIA; PAULOS E, 1993, SPIE C SENS FUS BOST, V6; PONCE J, 1992, CVGIP-IMAG UNDERSTAN, V55, P184, DOI 10.1016/1049-9660(92)90016-V; PONCE J, 1992, SYMBOLIC NUMERICAL C, P123; Salmon G., 1885, LESSONS INTRO MODERN; SCHWARTZ JT, 1987, INT J ROBOT RES, V6, P29, DOI 10.1177/027836498700600203; SULLIVAN S, 1994, IEEE T PATTERN ANAL, V16, P1183, DOI 10.1109/34.387489; WALLACE A, 1995, THESIS U CALIFORNIA; WALLACK A, 1993, IEEE INT C ROB AUT, V1, P692; WALLACK A, 1995, IEEE INT C ROB AUT; ZHANG Z, 1990, INT C PATTERN RECOGN, P38; [No title captured]; [No title captured]	34	3	6	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	1998	27	3					243	262		10.1023/A:1007918114326	http://dx.doi.org/10.1023/A:1007918114326			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZL840					2022-12-18	WOS:000073477400003
J	RISEMAN, EM; HANSON, AR				RISEMAN, EM; HANSON, AR			COMPUTER VISION RESEARCH AT THE UNIVERSITY-OF-MASSACHUSETTS - THEMES AND PROGRESS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article																		ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; ADIV G, 1989, IN PRESS T PAMI; ADIV G, 1985, THESIS U MASSACHUSET; ANANDAN P, 1989, INT J COMPUTER V JAN, V2; ANANDAN P, 1987, 1 INT C COMP VIS LON, P219; Anandan P., 1987, THESIS U MASSACHUSET; ARKIN R, 1987, NOV P IEEE COMP SOC, P176; ARKIN R, 1989, IN PRESS ROBOTICA; ARKIN R, 1989, IN PRESS INT J ROBOT; ARKIN RC, 1987, THESIS U MASSACHUSET; BALASUBRAMANYAM P, 1987, NOV P IEEE COMP SOC, P349; BALASUBRAMANYAM P, 1988, APR P DARP IU WORKSH, P907; BEVERIDGE JR, 1989, INT J COMPUTER VISIO, V2; BOLDT M, 1987, TR87104 U MASS DEP C; BROLIO J, 1988, AUG P AAAI WORKSH DA, P15; BURNS J, 1988, APR P IM UND WORKSH, P711; BURNS JB, 1986, IEEE T PATTERN ANAL, V8, P425, DOI 10.1109/TPAMI.1986.4767808; BURNS JB, THESIS U MASSACHUSET; BURNS JB, 1987, 10TH P INT JOINT C A, P763; DOLAN J, UNPUB PERCEPTUAL ORG; DRAPER B, 1989, INT J COMPUTER V JAN, V2; DUTTA R, 1988, APR P DARPA IM UND W, P945; Foster C. C., 1976, CONTENT ADDRESSABLE; GIBLIN P, 1987, JUN P INT C COMP VIS, P136; GLAZER F, 1987, THESIS U MASSACHUSET; GLAZER F, 1987, TR8777 U MASS DEP CO; Hanson A., 1978, COMPUTER VISION SYST, P303; HANSON A, 1987, ADV COMPUTER VISION; HANSON A, 1987, TR8729 1 MASS DEP CO; Hanson A. R., 1978, COMPUTER VISION SYST, P129; KAHN P, 1987, TR8757 U MASS DEP CO; KOHL C, 1987, 10TH P INT JOINT C A, P811; KOHL C, 1988, THESIS U MASSACHUSET; LAWTON DT, 1983, COMPUT VISION GRAPH, V22, P116, DOI 10.1016/0734-189X(83)90098-1; LAWTON DT, 1984, THESIS U MASSACHUSET; LEVITAN S, 1987, PYRAMID MULTI COMPUT; LOWRANCE JD, 1982, THESIS U MASSACHUSET; NAGIN PA, 1982, IEEE T PAMI 3, P263; PAVLIN I, 1985, DEC P DARPA IM UND W, P388; REYNOLDS G, 1987, TR8703 U MASS DEP CO; REYNOLDS G, 1984, APR P WORKSH COMP VI, P238; RISEMAN E, 1987, TR8748 U MASS DEP CO; WEEMS C, 1989, INT J COMPUTER VISIO, V2; WEEMS CC, 1986, EVALUATION MULTICOMP, P161; WEEMS CC, 1984, THESIS U MASSACHUSET; WEEMS CC, 1985, OCT P ICCD NEW YORK, P500; WEISS R, 1986, JUN P IEEE C COMP VI, P489; WEISS R, 1988, AUG P SPIE SAN DIEG; WESLEY L, 1987, THESIS U MASSACHUSET; WESLEY LP, 1986, OPT ENG, V25, P363, DOI 10.1117/12.7973833; WEYMOUTH TE, 1986, THESIS U MASSACHUSET; WILLIAMS LR, 1988, DEC P INT C COMP VIS, P441; WILLIAMS T, 1980, IEEE T PAMI12, V6, P511	53	3	3	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1989	2	3					199	207		10.1007/BF00158164	http://dx.doi.org/10.1007/BF00158164			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC191					2022-12-18	WOS:A1989AC19100001
J	Zhou, KY; Yang, JK; Loy, CC; Liu, ZW				Zhou, Kaiyang; Yang, Jingkang; Loy, Chen Change; Liu, Ziwei			Learning to Prompt for Vision-Language Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming-one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt's context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.	[Zhou, Kaiyang; Yang, Jingkang; Loy, Chen Change; Liu, Ziwei] Nanyang Technol Univ, S Lab, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Zhou, KY (corresponding author), Nanyang Technol Univ, S Lab, Singapore, Singapore.	kaiyang.zhou@ntu.edu.sg; jingkang001@ntu.edu.sg; ccloy@ntu.edu.sg; ziwei.liu@ntu.edu.sg			RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative	RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative	This study is supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).	Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Bommasani R., 2021, ARXIV; Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29; Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, 10.48550/arXiv.2005.14165]; Chen T, 2020, PR MACH LEARN RES, V119; Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Desai K, 2021, PROC CVPR IEEE, P11157, DOI 10.1109/CVPR46437.2021.01101; Elhoseiny M, 2013, IEEE I CONF COMP VIS, P2584, DOI 10.1109/ICCV.2013.321; Frome Andrea, 2013, NEURIPS; Furst A., 2021, ARXIV; Gao P., 2021, ARXIV; Gao T., 2020, ARXIV; Gomez L, 2017, PROC CVPR IEEE, P2017, DOI 10.1109/CVPR.2017.218; He K., 2020, P IEEECVF C COMPUTER, P9729; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HELBER P, 2019, IEEE J-STARS, V12, P2217, DOI [10.1109/IGARSS.2018.8519248, DOI 10.1109/JSTARS.2019.2918242]; Henaff OJ, 2020, PR MACH LEARN RES, V119; Hendrycks D, 2021, PROC CVPR IEEE, P15257, DOI 10.1109/CVPR46437.2021.01501; Hendrycks Dan, 2021, ICCV; Jia C, 2021, PR MACH LEARN RES, V139; Jia M., 2022, ARXIV; Jiang ZB, 2020, T ASSOC COMPUT LING, V8, P423, DOI 10.1162/tacl_a_00324; Joulin A, 2016, LECT NOTES COMPUT SC, V9911, P67, DOI 10.1007/978-3-319-46478-7_5; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Lester Brian, 2021, ARXIV; Li A, 2017, IEEE I CONF COMP VIS, P4193, DOI 10.1109/ICCV.2017.449; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Li Xiang Lisa, 2021, ARXIV; Li Y., 2021, ARXIV; Liu P., ARXIV; Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391; Maji S., 2013, ARXIV; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; PARKHI OM, 2012, PROC CVPR IEEE, P3498, DOI [DOI 10.1007/978-3-030-28954-6_10, 10.1109/CVPR.2012.6248092, DOI 10.1109/CVPR.2012.6248092]; Petroni F, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2463; Radford A, 2021, PR MACH LEARN RES, V139; Recht B, 2019, PR MACH LEARN RES, V97; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Singh A., 2021, ARXIV; Socher Richard, 2013, NEURIPS; Soomro K., 2012, ARXIV; Taori Rohan, 2020, NEURIPS; Tian Y., 2020, ECCV, P776, DOI [10.48550/arXiv.1906.05849, DOI 10.1007/978-3-030-58621-8_45]; Vaswani A, 2017, ADV NEUR IN, V30; Wang D., INT C LEARN REPR; Wang HH, 2019, ADV NEUR IN, V32; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yuan L., 2021, ARXIV; Zhang Y., 2020, SIMPLE BASELINE MULT; Zhong Zexuan, 2021, NAACL; Zhou K., 2022, ARXIV; Zhou K., 2021, ARXIV	55	2	2	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2337	2348		10.1007/s11263-022-01653-1	http://dx.doi.org/10.1007/s11263-022-01653-1		JUL 2022	12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG		Green Submitted			2022-12-18	WOS:000833995900001
J	Li, YF; Yang, MX; Peng, DZ; Li, TH; Huang, JT; Peng, X				Li, Yunfan; Yang, Mouxing; Peng, Dezhong; Li, Taihao; Huang, Jiantao; Peng, Xi			Twin Contrastive Learning for Online Clustering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep clustering; Online clustering; Unsupervised learning; Contrastive learning		This paper proposes to perform online clustering by conducting twin contrastive learning (TCL) at the instance and cluster level. Specifically, we find that when the data is projected into a feature space with a dimensionality of the target cluster number, the rows and columns of its feature matrix correspond to the instance and cluster representation, respectively. Based on the observation, for a given dataset, the proposed TCL first constructs positive and negative pairs through data augmentations. Thereafter, in the row and column space of the feature matrix, instance- and cluster-level contrastive learning are respectively conducted by pulling together positive pairs while pushing apart the negatives. To alleviate the influence of intrinsic false-negative pairs and rectify cluster assignments, we adopt a confidence-based criterion to select pseudo-labels for boosting both the instance- and cluster-level contrastive learning. As a result, the clustering performance is further improved. Besides the elegant idea of twin contrastive learning, another advantage of TCL is that it could independently predict the cluster assignment for each instance, thus effortlessly fitting online scenarios. Extensive experiments on six widely-used image and text benchmarks demonstrate the effectiveness of TCL. The code is released on.	[Li, Yunfan; Yang, Mouxing; Peng, Dezhong; Peng, Xi] Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China; [Li, Taihao; Huang, Jiantao] Zhejiang Lab, Hangzhou, Peoples R China	Sichuan University; Zhejiang Laboratory	Peng, X (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.	yunfanli.gm@gmail.com; yangmouxing@gmail.com; pengdz@scu.edu.cn; lith@zhejianglab.com; jthuang@zhejianglab.edu.cn; pengx.gm@gmail.com	Peng, Xi/B-9002-2012	Peng, Xi/0000-0002-5727-2790	National Key R&D Program of China [2020YFB1406702]; NFSC [62176171, U21B2040, U19A2078]; Open Research Projects of Zhejiang Lab [2021KH0AB02]	National Key R&D Program of China; NFSC(National Natural Science Foundation of China (NSFC)); Open Research Projects of Zhejiang Lab	The authors would like to thank the Associate editor and reviewers for the constructive comments and valuable suggestions that remarkably improve this study. This work was supported in part by the National Key R&D Program of China under Grant 2020YFB1406702; in part by NFSC under Grant 62176171, U21B2040, and U19A2078; and in part by Open Research Projects of Zhejiang Lab under Grant 2021KH0AB02.	Asano Yuki Markus, 2019, ARXIV191105371; Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008; Cai D, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1010; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Caron Mathilde, 2020, NEURIPS; Chang J., 2019, DEEP DISCRIMINATIVE; Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626; Chen GL, 2009, INT J COMPUT VISION, V81, P317, DOI 10.1007/s11263-008-0178-9; Chen T., 2020, ADV NEUR IN, V33; Chen T., ARXIV PREPRINT ARXIV; Chen X., ARXIV PREPRINT ARXIV; Chen X, 2021, AUTOPHAGY, V17, P2054, DOI 10.1080/15548627.2020.1810918; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359; Dang Z., 2021, ARXIV PREPRINT ARXIV; DeVries T., 2017, ARXIV PREPRINT ARXIV; Dizaji KG, 2017, IEEE I CONF COMP VIS, P5747, DOI 10.1109/ICCV.2017.612; Dwibedi D., 2021, ARXIV PREPRINT ARXIV; GOWDA KC, 1978, PATTERN RECOGN, V10, P105; Grill J.B., 2020, ADV NEUR IN; Guo XF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1753; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; Haeusser Philip, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P18, DOI 10.1007/978-3-030-12939-2_2; Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520; He K., 2020, P IEEECVF C COMPUTER, P9729; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu QJ, 2021, PROC CVPR IEEE, P1074, DOI 10.1109/CVPR46437.2021.00113; Hu WH, 2017, PR MACH LEARN RES, V70; Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Ji X, 2019, IEEE I CONF COMP VIS, P9864, DOI 10.1109/ICCV.2019.00996; Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33; Kim Y, 2019, IEEE I CONF COMP VIS, P101, DOI 10.1109/ICCV.2019.00019; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Kiselev VY, 2019, NAT REV GENET, V20, P273, DOI 10.1038/s41576-018-0088-9; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li J., ICLR; Li T, 2006, IEEE DATA MINING, P362; Li XL, 2021, IEEE T NEUR NET LEAR, V32, P443, DOI 10.1109/TNNLS.2020.2978389; Li Y., 2021, CONTRASTIVE CLUSTERI, V35; Liu WW, 2017, ADV NEUR IN, V30; Liu XW, 2016, AAAI CONF ARTIF INTE, P1888; Ma Edward, 2019, NLP AUGMENTATION; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Thanh ND, 2017, IEEE INT CONF FUZZY; Nie FP, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P959, DOI 10.1145/3292500.3330846; Nie FP, 2011, IEEE T NEURAL NETWOR, V22, P1796, DOI 10.1109/TNN.2011.2162000; Niu C, 2021, ARXIV PREPRINT ARXIV; Park S.w., 2020, ARXIV PREPRINT ARXIV; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Pelleg D., 2000, P 17 INT C MACH LEAR, DOI DOI 10.1038/S41598-021-86770-6; Peng X., 2016, IJCAI, P1925; Peng X, 2020, IEEE T NEUR NET LEAR, V31, P4857, DOI 10.1109/TNNLS.2019.2958324; Peng X, 2015, AAAI CONF ARTIF INTE, P3827; Le Q, 2014, PR MACH LEARN RES, V32, P1188; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rakib MRH, 2020, LECT NOTES COMPUT SC, V12089, P105, DOI 10.1007/978-3-030-51310-8_10; Reimers N, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P567; Shen S., 2021, ARXIV PREPRINT ARXIV; Socher Richard, 2011, P C EMP METH NAT LAN, P151; Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735; Sungwon Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P768, DOI 10.1007/978-3-030-58586-0_45; Tang M, 2019, INT J COMPUT VISION, V127, P477, DOI 10.1007/s11263-018-1115-1; van den Oord Aaron, 2018, ARXIV180703748; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Gansbeke Wouter, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P268, DOI 10.1007/978-3-030-58607-2_16; Van Gansbeke W., 2021, ARXIV PREPRINT ARXIV; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang X.G., 2021, ARXIV PREPRINT ARXIV; Wang XD, 2021, PROC CVPR IEEE, P12581, DOI 10.1109/CVPR46437.2021.01240; Wei J, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P6382; Wu JL, 2019, IEEE I CONF COMP VIS, P8149, DOI 10.1109/ICCV.2019.00824; Xie JY, 2016, PR MACH LEARN RES, V48; Xu JM, 2017, NEURAL NETWORKS, V88, P22, DOI 10.1016/j.neunet.2016.12.008; Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556; Yin JH, 2016, PROC INT CONF DATA, P625, DOI 10.1109/ICDE.2016.7498276; Zbontar J, 2021, PR MACH LEARN RES, V139; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957; Zelnik-Manor Lihi, 2005, P ADV NEUR INF PROC, P1601; Zhang D., 2021, ARXIV PREPRINT ARXIV; Zhang D., ARXIV PREPRINT ARXIV; Zhang W, 2012, LECT NOTES COMPUT SC, V7572, P428, DOI 10.1007/978-3-642-33718-5_31; Zhong H., 2020, ARXIV PREPRINT ARXIV	86	2	2	10	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2205	2221		10.1007/s11263-022-01639-z	http://dx.doi.org/10.1007/s11263-022-01639-z		JUL 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG		Green Submitted			2022-12-18	WOS:000822474400002
J	Pan, JS; Sun, DQ; Zhang, JW; Tang, JH; Yang, J; Tai, YW; Yang, MH				Pan, Jinshan; Sun, Deqing; Zhang, Jiawei; Tang, Jinhui; Yang, Jian; Tai, Yu-Wing; Yang, Ming-Hsuan			Dual Convolutional Neural Networks for Low-Level Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Low-level vision; Image restoration; Image filtering; Image enhancement; Dual convolutional neural network	IMAGE; REMOVAL	We propose a general dual convolutional neural network (DualCNN) for low-level vision problems, e.g., super-resolution, edge-preserving filtering, deraining, and dehazing. These problems usually involve estimating two components of the target signals: structures and details. Motivated by this, we design the proposed DualCNN to have two parallel branches, which respectively recovers the structures and details in an end-to-end manner. The recovered structures and details can generate desired signals according to the formation model for each particular application. The DualCNN is a flexible framework for low-level vision tasks and can be easily incorporated into existing CNNs. Experimental results show that the DualCNN can be effectively applied to numerous low-level vision tasks with favorable performance against the state-of-the-art methods that have been specially designed for each individual task.	[Pan, Jinshan; Tang, Jinhui; Yang, Jian] Nanjing Univ Sci & Technol, Nanjing 210094, Peoples R China; [Sun, Deqing] Google, New York, NY USA; [Zhang, Jiawei] SenseTime Res, Shenzhen 518000, Peoples R China; [Tai, Yu-Wing] Kuaishou Technol, Shenzhen 518000, Peoples R China; [Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95343 USA; [Yang, Ming-Hsuan] Yonsei Univ, Seoul, South Korea; [Yang, Ming-Hsuan] Google, Mountain View, CA 94043 USA	Nanjing University of Science & Technology; Google Incorporated; University of California System; University of California Merced; Yonsei University; Google Incorporated	Tang, JH (corresponding author), Nanjing Univ Sci & Technol, Nanjing 210094, Peoples R China.; Yang, MH (corresponding author), Univ Calif Merced, Merced, CA 95343 USA.; Yang, MH (corresponding author), Yonsei Univ, Seoul, South Korea.; Yang, MH (corresponding author), Google, Mountain View, CA 94043 USA.	jspan@njust.edu.cn; deqingsun@google.com; zhjw1988@gmail.com; jinhuitang@njust.edu.cn; csjyang@njust.edu.cn; yuwing@gmail.com; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304	National Key Research and Development Program of China [2018AAA0102001]; National Natural Science Foundation of China [61872421, 61922043, 61925204]; Fundamental Research Funds for the Central Universities [30920041109]; NSF CAREER [1149783]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work is supported in part by the National Key Research and Development Program of China under Grant 2018AAA0102001, the National Natural Science Foundation of China under Grants 61872421, 61922043, and 61925204, the Fundamental Research Funds for the Central Universities under Grant 30920041109, and NSF CAREER under Grant 1149783.	Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185; Bulat A, 2018, LECT NOTES COMPUT SC, V11210, P187, DOI 10.1007/978-3-030-01231-1_12; BURGER HC, 2012, PROC CVPR IEEE, P2392, DOI DOI 10.1109/CVPR.2012.6247952; Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681; Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273; Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Dong JX, 2022, IEEE T PATTERN ANAL, V44, P9960, DOI 10.1109/TPAMI.2021.3138787; Dongdong Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P510, DOI 10.1007/978-3-030-58604-1_31; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Fan QN, 2018, LECT NOTES COMPUT SC, V11217, P455, DOI 10.1007/978-3-030-01261-8_27; Fan QN, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275081; Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186; Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Guo TT, 2019, IEEE COMPUT SOC CONF, P2122, DOI 10.1109/CVPRW.2019.00265; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]; Huang Jia-Bin, 2015, CVPR, DOI DOI 10.1109/CVPR.2015.7299156; Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38; Jain V., 2008, P ADV NEUR INF PROC, V21, P1, DOI DOI 10.5555/2981780.2981876; Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815; Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511; Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856; Li SY, 2019, COMPUT VIS IMAGE UND, V186, P48, DOI 10.1016/j.cviu.2019.05.003; Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299; Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu SF, 2016, LECT NOTES COMPUT SC, V9908, P560, DOI 10.1007/978-3-319-46493-0_34; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82; Pan JS, 2021, IEEE T PATTERN ANAL, V43, P2449, DOI 10.1109/TPAMI.2020.2969348; Pan JS, 2018, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2018.00324; Pan JS, 2018, IEEE T PATTERN ANAL, V40, P2315, DOI 10.1109/TPAMI.2017.2753804; Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263; Ren JS., 2015, ADV NEURAL INF PROCE, V1, P901; Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Singh V, 2020, COMPUT VIS IMAGE UND, V199, DOI 10.1016/j.cviu.2020.103034; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Tarel JP, 2012, IEEE INTEL TRANSP SY, V4, P6, DOI 10.1109/MITS.2012.2189969; Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8; Xie J., 2012, ADV NEURAL INFORM PR, P341, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xu L, 2014, ADV NEUR IN, V27; Xu L, 2015, PR MACH LEARN RES, V37, P1669; Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208; Yang AP, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4627; Yang H., 2017, ARXIV171000279; Zhang D., 2020, ARXIV200703951; Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407; Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337; Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079; Zhang JW, 2017, PROC CVPR IEEE, P6969, DOI 10.1109/CVPR.2017.737; ZHANG K, 2017, PROC CVPR IEEE, P2808, DOI DOI 10.1109/CVPR.2017.300; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362; Zhang Yulun, 2018, P EUROPEAN C COMPUTE, P286; Zhu HY, 2021, IEEE T CYBERNETICS, V51, P829, DOI 10.1109/TCYB.2019.2955092; Zhu HY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1234; Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	78	2	2	6	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1440	1458		10.1007/s11263-022-01583-y	http://dx.doi.org/10.1007/s11263-022-01583-y		APR 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU					2022-12-18	WOS:000779072900001
J	Funatomi, T; Ogawa, T; Tanaka, K; Kubo, H; Caron, G; Mouaddib, E; Matsushita, Y; Mukaigawa, Y				Funatomi, Takuya; Ogawa, Takehiro; Tanaka, Kenichiro; Kubo, Hiroyuki; Caron, Guillaume; Mouaddib, El Mustapha; Matsushita, Yasuyuki; Mukaigawa, Yasuhiro			Eliminating Temporal Illumination Variations in Whisk-broom Hyperspectral Imaging	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computational photography; Hyperspectral imaging; Varying illumination; Low-rank approximation	COLOR-SIGNAL	We propose a method for eliminating the temporal illumination variations in whisk-broom (point-scan) hyperspectral imaging. Whisk-broom scanning is useful for acquiring a spatial measurement using a pixel-based hyperspectral sensor. However, when it is applied to outdoor cultural heritages, temporal illumination variations become an issue due to the lengthy measurement time. As a result, the incoming illumination spectra vary across the measured image locations because different locations are measured at different times. To overcome this problem, in addition to the standard raster scan, we propose an additional perpendicular scan that traverses the raster scan. We show that this additional scan allows us to infer the illumination variations over the raster scan. Furthermore, the sparse structure in the illumination spectrum is exploited to robustly eliminate these variations. We quantitatively show that a hyperspectral image captured under sunlight is indeed affected by temporal illumination variations, that a Naive mitigation method suffers from severe artifacts, and that the proposed method can robustly eliminate the illumination variations. Finally, we demonstrate the usefulness of the proposed method by capturing historic stained-glass windows of a French cathedral.	[Funatomi, Takuya; Ogawa, Takehiro; Mukaigawa, Yasuhiro] Nara Inst Sci & Technol, Grad Sch Sci & Technol, 8916-5 Takayama Cho, Ikoma, Nara 6300192, Japan; [Tanaka, Kenichiro] Ritsumeikan Univ, Coll Informat Sci & Engn, 1-1-1 Noji Higashi, Kusatsu, Shiga 5258577, Japan; [Kubo, Hiroyuki] Chiba Univ, Grad Sch Sci & Engn, Inage Ku, 1-33 Yayoi Cho, Chiba, Chiba 2638522, Japan; [Caron, Guillaume; Mouaddib, El Mustapha] Univ Picardie Jules Verne, MIS Lab, 33 Rue St Leu, F-80039 Amiens 1, France; [Caron, Guillaume] CNRS AIST JRL, IRL, 1-1-1 Umezono, Tsukuba, Ibaraki 3058560, Japan; [Matsushita, Yasuyuki] Osaka Univ, Grad Sch Informat Sci & Technol, 1-5 Yamadaoka, Suita, Osaka 5650871, Japan	Nara Institute of Science & Technology; Ritsumeikan University; Chiba University; Picardie Universites; Universite de Picardie Jules Verne (UPJV); National Institute of Advanced Industrial Science & Technology (AIST); Osaka University	Funatomi, T (corresponding author), Nara Inst Sci & Technol, Grad Sch Sci & Technol, 8916-5 Takayama Cho, Ikoma, Nara 6300192, Japan.	funatomi@is.naist.jp; ken-t@fc.ritsumei.ac.jp; mukaigawa@is.naist.jp		Funatomi, Takuya/0000-0001-5588-5932	JSPS; MAEDI under the Japan-France Integrated Action Program (SAKURA); JSPS KAKENHI [JP20K21816]; Japan Science and Technology Agency CREST [JPMJCR1764]	JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science); MAEDI under the Japan-France Integrated Action Program (SAKURA); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Japan Science and Technology Agency CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	This study is partly supported by JSPS and MAEDI under the Japan-France Integrated Action Program (SAKURA), JSPS KAKENHI JP20K21816, and Japan Science and Technology Agency CREST Grant No. JPMJCR1764.	Babini A., 2020, LOND IMAGING MEET, V2020, P109, DOI [10.2352/issn.2694-118X.2020.LIM-27, DOI 10.2352/ISSN.2694-118X.2020.LIM-27]; Baek SH, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130896; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; CHANG PR, 1995, IEEE T IMAGE PROCESS, V4, P81, DOI 10.1109/83.350812; Chen X., 2017, ELECT IMAGING, V18, P194, DOI [10.2352/ISSN.2470-1173.2017.18.COLOR-060, DOI 10.2352/ISSN.2470-1173.2017.18.COLOR-060]; Choi I, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130810; Cucci C, 2020, DATA HANDL SCI TECHN, V32, P583, DOI 10.1016/B978-0-444-63977-6.00023-7; Cucci C, 2016, ACCOUNTS CHEM RES, V49, P2070, DOI 10.1021/acs.accounts.6b00048; Cucci C, 2013, PROC SPIE, V8790, DOI 10.1117/12.2020286; Cucci C, 2011, PROC SPIE, V8084, DOI 10.1117/12.889460; Drew MS, 2007, J OPT SOC AM A, V24, P294, DOI 10.1364/JOSAA.24.000294; Dun X, 2020, OPTICA, V7, P913, DOI 10.1364/OPTICA.394413; Gat N, 2000, P SOC PHOTO-OPT INS, V4056, P50, DOI 10.1117/12.381686; Gedalin D, 2019, OPT EXPRESS, V27, P35811, DOI 10.1364/OE.27.035811; Gehm ME, 2007, OPT EXPRESS, V15, P14013, DOI 10.1364/OE.15.014013; Hagen N, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.9.090901; Hauser J, 2020, APPL OPTICS, V59, P11196, DOI 10.1364/AO.404524; He W, 2016, IEEE T GEOSCI REMOTE, V54, P176, DOI 10.1109/TGRS.2015.2452812; HO JA, 1990, IEEE T PATTERN ANAL, V12, P966, DOI 10.1109/34.58869; Hu XM, 2019, OPT EXPRESS, V27, P27088, DOI 10.1364/OE.27.027088; Jeon DS, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322946; JUDD DB, 1964, J OPT SOC AM, V54, P1031, DOI 10.1364/JOSA.54.001031; Lin X, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661262; Mansencal T., 2020, COLOUR 0316; Mathys A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0220949; Miao X, 2019, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2019.00416; Monakhova K, 2020, OPTICA, V7, P1298, DOI 10.1364/OPTICA.397214; Morimoto T, 2010, PROC CVPR IEEE, P207, DOI 10.1109/CVPR.2010.5540211; Morimoto T, 2008, INT J AUTOM COMPUT, V5, P226, DOI 10.1007/s11633-008-0226-5; Myers R., 2020, CHROMAXION TRADE; Oiknine Y, 2019, J IMAGING, V5, DOI 10.3390/jimaging5010003; Pillay R, 2019, J AM INST CONSERV, V58, P3, DOI 10.1080/01971360.2018.1549919; Saragadam V, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3345553; Shmilovich S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-60413-8; Su T, 2018, OPT EXPRESS, V26, P26167, DOI 10.1364/OE.26.026167; Wagadarikar A, 2008, APPL OPTICS, V47, pB44, DOI 10.1364/AO.47.000B44; Wang X, 2018, OPT EXPRESS, V26, P25226, DOI 10.1364/OE.26.025226; Wendel A, 2017, ISPRS J PHOTOGRAMM, V129, P162, DOI 10.1016/j.isprsjprs.2017.04.010; Zheng YQ, 2015, PROC CVPR IEEE, P1779, DOI 10.1109/CVPR.2015.7298787; Ziyi Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P187, DOI 10.1007/978-3-030-58592-1_12	40	2	2	2	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1310	1324		10.1007/s11263-022-01587-8	http://dx.doi.org/10.1007/s11263-022-01587-8		MAR 2022	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		hybrid			2022-12-18	WOS:000780350300002
J	Zhu, JP; Zhao, DL; Zhang, B; Zhou, BL				Zhu, Jiapeng; Zhao, Deli; Zhang, Bo; Zhou, Bolei			Disentangled Inference for GANs With Latently Invertible Autoencoder	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						GAN; VAE; Inference; Disentanglement	NONLINEAR DIMENSIONALITY REDUCTION	Generative Adversarial Networks (GANs) can synthesize more and more realistic images. However, one fundamental issue hinders their practical applications: the incapability of encoding real samples in the latent space. Many semantic image editing applications rely on inverting the given image into the latent space and then manipulating inverted code. One possible solution is to learn an encoder for GAN via Variational Auto-Encoder. However, the entanglement of the latent space poses a major challenge for learning the encoder. To tackle the challenge and enable inference in GANs, we propose a novel method named Latently Invertible Autoencoder (LIA). In LIA, an invertible network and its inverse mapping are symmetrically embedded in the latent space of an autoencoder. The decoder of LIA is first trained as a standard GAN with the invertible network, and then the encoder is learned from a disentangled autoencoder by detaching the invertible network from LIA. It thus avoids the entanglement problem caused by the latent space. Extensive experiments on the FFHQ face dataset and three LSUN datasets validate the effectiveness of LIA for the image inversion and its applications. Code and models are available at https://github.com/genforce/lia.	[Zhu, Jiapeng; Zhao, Deli; Zhang, Bo] Xiaomi AI Lab, Beijing, Peoples R China; [Zhu, Jiapeng; Zhou, Bolei] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China	Chinese University of Hong Kong	Zhou, BL (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.	bzhou@ie.cuhk.edu.hk			Research Grants Council (RGC) of Hong Kong under ECS [24206219]; GRF [14204521]; CUHK FoE RSFS	Research Grants Council (RGC) of Hong Kong under ECS; GRF; CUHK FoE RSFS	The project was partially supported through the Research Grants Council (RGC) of Hong Kong under ECS Grant No.24206219, GRF Grant No.14204521, CUHK FoE RSFS Grant.	Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453; Aila T, 2019, ARXIV191204958; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antoniou Antreas, 2017, ARXIV171104340; Arjovsky M, 2017, PR MACH LEARN RES, V70; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877; Cobo, 2017, ARXIV171110433; Dinh L, 2017, 5 INT C LEARN REPR I; Dinh Laurent, 2015, ICLR WORKSH; Doersch Carl, 2016, ARXIV160605908, DOI DOI 10.3389/FPHYS.2016.00108; Donahue J, 2019, ADV NEUR IN, V32; Donahue Jeff, 2017, INT C LEARN REPR ICL; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover A., 2017, ARXIV170508868; Gulrajani I, 2017, P NIPS 2017; Heljakka A., 2018, ARXIV180703026; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lehtinen J, 2018, PR MACH LEARN RES, V80; Lipton Z. C., 2017, INT C LEARN REPR ICL; Lucas J., 2019, P INT C LEARN REPR I; Lucas T., 2019, ARXIV190101091; Luo J., 2017, ARXIV170310094; Makhani, 2018, ARXIV180509804; Makhzani A., 2015, ARXIV151105644; Mescheder L, 2018, PR MACH LEARN RES, V80; Oord A.V.D., 2016, SSW; Pidhorskyi S., 2020, P IEEE C COMP VIS PA; Poole, 2017, INT C LEARN ICLR; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sankaranarayanan S., 2017, P IEEE C COMP VIS PA; Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267; Shen Yujun, 2020, P IEEE CVF C COMP VI; Su, 2018, ARXIV180905861; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Ulyanov Dmitry, 2017, ARXIV170402304; van den Oord Aaron, 2018, ARXIV180703748; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P515, DOI 10.1145/3077136.3080786; Xiao Z., 2019, ARXIV190510485; Yang CY, 2021, INT J COMPUT VISION, V129, P1451, DOI 10.1007/s11263-020-01429-5; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang ZY, 2004, SIAM J SCI COMPUT, V26, P313, DOI 10.1137/S1064827502419154; Zhou D., 2003, P 16 INT C NEUR INF; Zhu B, 2018, NATURE, V555, P487, DOI 10.1038/nature25988; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	62	2	2	2	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1259	1276		10.1007/s11263-022-01598-5	http://dx.doi.org/10.1007/s11263-022-01598-5		MAR 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		Green Submitted			2022-12-18	WOS:000771899300001
J	Pan, JW; Zhu, PF; Zhang, KH; Cao, B; Wang, Y; Zhang, DW; Han, JW; Hu, QH				Pan, Junwen; Zhu, Pengfei; Zhang, Kaihua; Cao, Bing; Wang, Yu; Zhang, Dingwen; Han, Junwei; Hu, Qinghua			Learning Self-supervised Low-Rank Network for Single-Stage Weakly and Semi-supervised Semantic Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Weakly-supervised learning; Semi-supervised Learning; Semantic segmentation		Semantic segmentation with limited annotations, such as weakly supervised semantic segmentation (WSSS) and semi-supervised semantic segmentation (SSSS), is a challenging task that has attracted much attention recently. Most leading WSSS methods employ a sophisticated multi-stage training strategy to estimate pseudo-labels as precise as possible, but they suffer from high model complexity. In contrast, there exists another research line that trains a single network with image-level labels in one training cycle. However, such a single-stage strategy often performs poorly because of the compounding effect caused by inaccurate pseudo-label estimation. To address this issue, this paper presents a Self-supervised Low-Rank Network (SLRNet) for single-stage WSSS and SSSS. The SLRNet uses cross-view self-supervision, that is, it simultaneously predicts several complementary attentive LR representations from different views of an image to learn precise pseudo-labels. Specifically, we reformulate the LR representation learning as a collective matrix factorization problem and optimize it jointly with the network learning in an end-to-end manner. The resulting LR representation deprecates noisy information while capturing stable semantics across different views, making it robust to the input variations, thereby reducing overfitting to self-supervision errors. The SLRNet can provide a unified single-stage framework for various label-efficient semantic segmentation settings: (1) WSSS with image-level labeled data, (2) SSSS with a few pixel-level labeled data, and (3) SSSS with a few pixel-level labeled data and many image-level labeled data. Extensive experiments on the Pascal VOC 2012, COCO, and L2ID datasets demonstrate that our SLRNet outperforms both state-of-the-art WSSS and SSSS methods with a variety of different settings, proving its good generalizability and efficacy.	[Pan, Junwen; Zhu, Pengfei; Cao, Bing; Wang, Yu; Hu, Qinghua] Tianjin Univ, Tianjin, Peoples R China; [Zhang, Kaihua] Nanjing Univ Informat Sci & Technol, Nanjing, Peoples R China; [Zhang, Dingwen; Han, Junwei] Northwestern Polytech Univ, Xian, Peoples R China	Tianjin University; Nanjing University of Information Science & Technology; Northwestern Polytechnical University	Zhu, PF (corresponding author), Tianjin Univ, Tianjin, Peoples R China.	junwenpan@tju.edu.cn; zhupengfei@tju.edu.cn; zhkhua@gmail.com		Pan, Junwen/0000-0002-8781-6090	National Key Research and Development Program of China [2019YFB2101904]; National Natural Science Foundation of China [61732011, 61876127, 61876088, 61925602]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was partially supported by the National Key Research and Development Program of China under Grant 2019YFB2101904, the National Natural Science Foundation of China under Grants 61732011, 61876127, 61876088 and 61925602.	Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231; Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523; Araslanov N, 2020, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR42600.2020.00431; Bearman Amy, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P549, DOI 10.1007/978-3-319-46478-7_34; Cabral R, 2013, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2013.309; Caron Mathilde, 2020, NEURIPS; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen T, 2020, PR MACH LEARN RES, V119; Chen Xingyu, 2021, CVPR; Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762; French Geoff, 2020, BMVC; Geng Z., 2021, ICLR; Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541; Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004; He K., 2020, P IEEECVF C COMPUTER, P9729; Hou Q., 2017, NEURIPS, P547; Hu X, 2013, PROCEEDING ACM INT C, P607, DOI DOI 10.1145/2488388.2488442; Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733; Hung Wei-Chih, 2018, ARXIV180207934; Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216; Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lee Hayeon, 2021, NEURIPS; Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79; Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541; Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960; Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926; Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Ma L, 2012, PROC CVPR IEEE, P2586, DOI 10.1109/CVPR.2012.6247977; O Pinheiro Pedro O, 2020, ADV NEURAL INFORM PR, V33; Ouali Yassine, 2020, P IEEE CVF C COMP VI, DOI DOI 10.1109/CVPR42600.2020.01269; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Paszke A, 2019, ADV NEUR IN, V32; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780; Saleh F, 2016, LECT NOTES COMPUT SC, V9912, P413, DOI 10.1007/978-3-319-46484-8_25; Shimoda W, 2019, IEEE I CONF COMP VIS, P5207, DOI 10.1109/ICCV.2019.00531; Sohn Kihyuk, 2020, ARXIV200107685; Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606; Stretcu O., 2015, BMVC; Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142; Tai C., 2016, ICLR POST; Vaswani A, 2017, ADV NEUR IN, V30; Wang X, 2020, INT J COMPUT VISION, V128, P1736, DOI 10.1007/s11263-020-01293-3; Wang XL, 2021, PROC CVPR IEEE, P3023, DOI 10.1109/CVPR46437.2021.00304; Wei Y., 2020, ARXIV201011724 CORR; Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759; Xie Enze, 2021, ICCV, P8392; Xie Z., 2021, CVPR, P16684; Yu-Ting Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8988, DOI 10.1109/CVPR42600.2020.00901; Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229; Zhang Dong, 2020, ADV NEURAL INFORM PR; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zoph Barret, 2020, NEURIPS; Zou Yuliang, 2021, ICLR	72	2	2	19	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1181	1195		10.1007/s11263-022-01590-z	http://dx.doi.org/10.1007/s11263-022-01590-z		MAR 2022	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		Green Submitted			2022-12-18	WOS:000769341300001
J	Han, K; Wang, YH; Xu, C; Guo, JY; Xu, CJ; Wu, EH; Tian, Q				Han, Kai; Wang, Yunhe; Xu, Chang; Guo, Jianyuan; Xu, Chunjing; Wu, Enhua; Tian, Qi			GhostNets on Heterogeneous Devices via Cheap Operations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Convolutional neural networks; Efficient inference; Visual recognition		Deploying convolutional neural networks (CNNs) on mobile devices is difficult due to the limited memory and computation resources. We aim to design efficient neural networks for heterogeneous devices including CPU and GPU, by exploiting the redundancy in feature maps, which has rarely been investigated in neural architecture design. For CPU-like devices, we propose a novel CPU-efficient Ghost (C-Ghost) module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed C-Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. C-Ghost bottlenecks are designed to stack C-Ghost modules, and then the lightweight C-GhostNet can be easily established. We further consider the efficient networks for GPU devices. Without involving too many GPU-inefficient operations (e.g., depth-wise convolution) in a building stage, we propose to utilize the stage-wise feature redundancy to formulate GPU-efficient Ghost (G-Ghost) stage structure. The features in a stage are split into two parts where the first part is processed using the original block with fewer output channels for generating intrinsic features, and the other are generated using cheap operations by exploiting stage-wise redundancy. Experiments conducted on benchmarks demonstrate the effectiveness of the proposed C-Ghost module and the G-Ghost stage. C-GhostNet and G-GhostNet can achieve the optimal trade-off of accuracy and latency for CPU and GPU, respectively.	[Han, Kai; Wu, Enhua] Univ Chinese Acad Sci, State Key Lab Comp Sci, ISCAS, Beijing, Peoples R China; [Han, Kai; Wang, Yunhe; Guo, Jianyuan; Xu, Chunjing; Tian, Qi] Huawei Noahs Ark Lab, Shenzhen, Peoples R China; [Xu, Chang; Guo, Jianyuan] Univ Sydney, Sydney, NSW, Australia; [Wu, Enhua] Univ Macau, Macau, Peoples R China	Chinese Academy of Sciences; Institute of Software, CAS; University of Chinese Academy of Sciences, CAS; Huawei Technologies; University of Sydney; University of Macau	Wang, YH (corresponding author), Huawei Noahs Ark Lab, Shenzhen, Peoples R China.	hankai@ios.ac.cn; yunhe.wang@huawei.com; weh@ios.ac.cn; tian.qi1@huawei.com		Xu, Chang/0000-0002-4756-0609	NSFC [62072449, 61872241, 61632003]; Macao FDCT Grant [0018/2019/AKP]; Australian Research Council [DP210101859]; University of Sydney SOAR Prize; CANN	NSFC(National Natural Science Foundation of China (NSFC)); Macao FDCT Grant; Australian Research Council(Australian Research Council); University of Sydney SOAR Prize; CANN	This work was supported by NSFC (62072449, 61872241, 61632003), Macao FDCT Grant (0018/2019/AKP). Chang Xu was supported by the Australian Research Council under Project DP210101859 and the University of Sydney SOAR Prize. This project is also partially supported by CANN (https://www.hiascend.com/software/cann).	Abadi M, 2015, P 12 USENIX S OPERAT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Cai Han, 2019, INT C LEARN REPR; Chen HT, 2020, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR42600.2020.00154; Chen HT, 2019, IEEE I CONF COMP VIS, P3513, DOI 10.1109/ICCV.2019.00361; Chen K, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen WJ, 2019, PROC CVPR IEEE, P7234, DOI 10.1109/CVPR.2019.00741; Chen Wuyang, 2020, ICLR; Chin TW, 2020, PROC CVPR IEEE, P1515, DOI 10.1109/CVPR42600.2020.00159; Chollet F., 2017, PROC CVPR IEEE, P1251, DOI DOI 10.1109/CVPR.2017.195; Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Denton E, 2014, ADV NEUR IN, V27; Gholami A, 2018, IEEE COMPUT SOC CONF, P1719, DOI 10.1109/CVPRW.2018.00215; Gui SP, 2019, ADV NEUR IN, V32; Guo JY, 2021, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR46437.2021.00219; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165; Han K, 2020, PR MACH LEARN RES, V119; Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550; Han Kai, 2021, IEEE T PATTERN ANAL; Han S, 2015, ADV NEUR IN, V28; HAN SY, 2016, IEEE ICC; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; He Y, 2020, PROC CVPR IEEE, P2006, DOI 10.1109/CVPR42600.2020.00208; He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hinton G., 2015, ARXIV150302531; Howard A.G., 2017, MOBILENETS EFFICIENT; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang ZH, 2018, LECT NOTES COMPUT SC, V11220, P317, DOI 10.1007/978-3-030-01270-0_19; Hubara I, 2016, ADV NEUR IN, V29; Iandola Forrest, 2017, 2017 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS), DOI 10.1145/3125502.3125606; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jeon Y, 2018, ADV NEUR IN, V31; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li H., 2017, P INT C LEARN REPR I, P1; Liebenwein Lucas, 2020, 8 INT C LEARN REPR I; Lin MB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P673; Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160; Lin SH, 2019, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR.2019.00290; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu CJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3001; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]; Liu Z, 2019, PATTERN ANAL APPL, V22, P1527, DOI 10.1007/s10044-019-00792-5; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152; Paszke A, 2019, ADV NEUR IN, V32; Radosavovic I., 2020, 2020 IEEE CVF C COMP, P10428, DOI 10.1109/cvpr42600.2020.01044; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tan MX, 2019, PR MACH LEARN RES, V97; Wan Alvin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12962, DOI 10.1109/CVPR42600.2020.01298; Wang J, 2016, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'16), P253, DOI 10.1145/2999572.2999589; Wang Y., 2019, NEURIPS, P5138; Wang YH, 2018, ADV NEUR IN, V31; Wen W, 2016, ADV NEUR IN, V29; Williams S, 2009, COMMUN ACM, V52, P65, DOI 10.1145/1498765.1498785; Wilson, 2016, BMVC; Wu BC, 2018, PROC CVPR IEEE, P9127, DOI 10.1109/CVPR.2018.00951; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu Y, 2019, P 32 ANN C NEURAL IN, P2565; Xuefei Ning, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P592, DOI 10.1007/978-3-030-58580-8_35; Yang L, 2021, PROC CVPR IEEE, P3568, DOI 10.1109/CVPR46437.2021.00357; Yang ZH, 2020, PROC CVPR IEEE, P1826, DOI 10.1109/CVPR42600.2020.00190; Yang ZH, 2019, PR MACH LEARN RES, V97; Yang Zhaohui, 2020, NEURIPS; You S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1285, DOI 10.1145/3097983.3098135; Yu Jiahui, 2019, ICLR; Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958; ZHANG HY, 2018, PROC CVPR IEEE; Zhou Daquan, 2020, EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58580-8_40; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	93	2	2	22	34	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					1050	1069		10.1007/s11263-022-01575-y	http://dx.doi.org/10.1007/s11263-022-01575-y		MAR 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ		Green Submitted			2022-12-18	WOS:000763208400001
J	Tian, X; Xu, K; Yang, X; Yin, BC; Lau, RWH				Tian, Xin; Xu, Ke; Yang, Xin; Yin, Baocai; Lau, Rynson W. H.			Learning to Detect Instance-Level Salient Objects Using Complementary Image Labels	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						SID; Weak supervision; Saliency detection; Subitizing		Existing salient instance detection (SID) methods typically learn from pixel-level annotated datasets. In this paper, we present the first weakly-supervised approach to the SID problem. Although weak supervision has been considered in general saliency detection, it is mainly based on using class labels for object localization. However, it is non-trivial to use only class labels to learn instance-aware saliency information, as salient instances with high semantic affinities may not be easily separated by the labels. As the subitizing information provides an instant judgement on the number of salient items, it is naturally related to detecting salient instances and may help separate instances of the same class while grouping different parts of the same instance. Inspired by this observation, we propose to use class and subitizing labels as weak supervision for the SID problem. We propose a novel weakly-supervised network with three branches: a Saliency Detection Branch leveraging class consistency information to locate candidate objects; a Boundary Detection Branch exploiting class discrepancy information to delineate object boundaries; and a Centroid Detection Branch using subitizing information to detect salient instance centroids. This complementary information is then fused to produce a salient instance map. To facilitate the learning process, we further propose a progressive training scheme to reduce label noise and the corresponding noise learned by the model, via reciprocating the model with progressive salient instance prediction and model refreshing. Our extensive evaluations show that the proposed method plays favorably against carefully designed baseline methods adapted from related tasks.	[Tian, Xin; Yang, Xin; Yin, Baocai] Dalian Univ Technol, Dalian, Peoples R China; [Tian, Xin; Xu, Ke; Lau, Rynson W. H.] City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China; [Yin, Baocai] Pengcheng Lab, Shenzhen, Peoples R China	Dalian University of Technology; City University of Hong Kong	Tian, X (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.; Tian, X (corresponding author), City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China.	xtian@mail.dlut.edu.cn		XU, Ke/0000-0001-5855-3810; , Xin/0000-0002-8046-722X; Tian, Xin/0000-0002-4876-5147	National Natural Science Foundation of China [61632006, 61972067]; Innovation Technology Funding of Dalian [2018J11CY010, 2020JJ26GX036]; RGC of Hong Kong [11205620]; City University of Hong Kong [7005674]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Innovation Technology Funding of Dalian; RGC of Hong Kong(Hong Kong Research Grants Council); City University of Hong Kong(City University of Hong Kong)	This work was supported in part by the National Natural Science Foundation of China under Grants 61632006, 61972067, and the Innovation Technology Funding of Dalian (Project No. 2018J11CY010, 2020JJ26GX036); a General Research Fund from RGC of Hong Kong (RGC Ref.: 11205620); and a Strategic Research Grant from City University of Hong Kong (Ref.: 7005674).	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231; Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523; Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chen CC, 2020, AAAI CONF ARTIF INTE, V34, P3414; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Cholakkal H, 2019, PROC CVPR IEEE, P12389, DOI 10.1109/CVPR.2019.01268; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan RC, 2019, PROC CVPR IEEE, P6096, DOI 10.1109/CVPR.2019.00626; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He SF, 2017, IEEE I CONF COMP VIS, P1059, DOI 10.1109/ICCV.2017.120; Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563; Hu MY, 2019, PROC CVPR IEEE, P11509, DOI 10.1109/CVPR.2019.01178; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Laine Samuli, 2016, ARXIV161002242; Laradji Issam H, 2019, ARXIV190701430; Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34; Li X, 2018, LECT NOTES COMPUT SC, V11206, P287, DOI [10.1007/978-3-030-01216-8_18, 10.1007/978-3-030-01267-0_22]; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Liu YX, 2021, IEEE T IMAGE PROCESS, V30, P4423, DOI 10.1109/TIP.2021.3071691; Lu ZW, 2017, IEEE T PATTERN ANAL, V39, P486, DOI 10.1109/TPAMI.2016.2552172; Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847; Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904; Paszke A, 2019, ADV NEUR IN, V32; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Pinheiro Pedro O., 2015, ADV NEURAL INFORM PR, V3, P5; Shang-Hua Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P702, DOI 10.1007/978-3-030-58539-6_42; Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758; Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960; Siris Avishek, 2020, CVPR; Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390; Tarvainen A, 2017, ADV NEUR IN, V30; Tian Xin, 2020, BMVC; Wang B, 2020, AAAI CONF ARTIF INTE, V34, P12128; Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154; Wang WG, 2017, IEEE I CONF COMP VIS, P2205, DOI 10.1109/ICCV.2017.240; Wang W, 2015, PROC CVPR IEEE, P3395, DOI 10.1109/CVPR.2015.7298816; Wei JC, 2020, PROC CVPR IEEE, P4383, DOI 10.1109/CVPR42600.2020.00444; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736; Xu YY, 2019, IEEE I CONF COMP VIS, P3788, DOI 10.1109/ICCV.2019.00389; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Yang Jianwei, 2016, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2016.28; Yang X, 2018, ADV NEUR IN, V31; Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943; Zeng Y, 2019, PROC CVPR IEEE, P6067, DOI 10.1109/CVPR.2019.00623; Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436; Zhang Jason Y., 2020, ECCV; Zhang JF, 2018, PROC CVPR IEEE, P6781, DOI 10.1109/CVPR.2018.00709; Zhang JM, 2016, PROC CVPR IEEE, P5733, DOI 10.1109/CVPR.2016.618; Zhang Jing, 2020, CVPR; Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187; Zhang SD, 2021, IEEE T CYBERNETICS, DOI 10.1109/TCYB.2021.3124231; Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887; Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320; Zhao X., 2020, ECCV, P35, DOI [DOI 10.1007/978-3-030-58536-5_3, 10.1007/978-3-030-58536-5_3]; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou H., 2020, PROC IEEECVF C COMPU, P9141; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zhuge YZ, 2018, IEEE SIGNAL PROC LET, V25, P1800, DOI 10.1109/LSP.2018.2875586	74	2	2	5	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					729	746		10.1007/s11263-021-01553-w	http://dx.doi.org/10.1007/s11263-021-01553-w		JAN 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI		Green Submitted			2022-12-18	WOS:000748453700001
J	Bai, F; Bartoli, A				Bai, Fang; Bartoli, Adrien			Procrustes Analysis with Deformations: A Closed-Form Solution by Eigenvalue Decomposition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Procrustes analysis; Nonlinear deformation models; Shape analysis; Point-cloud registration	LEAST-SQUARES; SHAPE; REGISTRATION; PARAMETERS; ALGORITHM; MANIFOLDS; ROTATION; SPACE	Generalized Procrustes Analysis (GPA) is the problem of bringing multiple shapes into a common reference by estimating transformations. GPA has been extensively studied for the Euclidean and affine transformations. We introduce GPA with deformable transformations, which forms a much wider and difficult problem. We specifically study a class of transformations called the Linear Basis Warps, which contains the affine transformation and most of the usual deformation models, such as the Thin-Plate Spline (TPS). GPA with deformations is a nonconvex underconstrained problem. We resolve the fundamental ambiguities of deformable GPA using two shape constraints requiring the eigenvalues of the shape covariance. These eigenvalues can be computed independently as a prior or posterior. We give a closed-form and optimal solution to deformable GPA based on an eigenvalue decomposition. This solution handles regularization, favoring smooth deformation fields. It requires the transformation model to satisfy a fundamental property of free-translations, which asserts that the model can implement any translation. We show that this property fortunately holds true for most common transformation models, including the affine and TPS models. For the other models, we give another closed-form solution to GPA, which agrees exactly with the first solution for models with free-translation. We give pseudo-code for computing our solution, leading to the proposed DefGPA method, which is fast, globally optimal and widely applicable. We validate our method and compare it to previous work on six diverse 2D and 3D datasets, with special care taken to choose the hyperparameters from cross-validation.	[Bai, Fang] Univ Clermont Auvergne, Inst Pascal, UMR6602 CNRS, TGI,ENCOV, Clermont Ferrand, France; [Bartoli, Adrien] Univ Clermont Auvergne, Inst Pascal, Dept Clin Res & Innovat, CHU Clermont Ferrand,ENCOV,TGI,UMR6602 CNRS, Clermont Ferrand, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite Clermont Auvergne (UCA); Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite Clermont Auvergne (UCA); CHU Clermont Ferrand	Bai, F (corresponding author), Univ Clermont Auvergne, Inst Pascal, UMR6602 CNRS, TGI,ENCOV, Clermont Ferrand, France.	Fang.Bai@yahoo.com; Adrien.Bartoli@gmail.com		Bai, Fang/0000-0001-9606-3943	ANR via the TOPACS project [ANR-19-CE45-0015]	ANR via the TOPACS project(French National Research Agency (ANR))	This work was supported by ANR via the TOPACS project (ANR-19-CE45-0015). We thank the authors of the public datasets which we could use in our experiments. We appreciate the valuable comments of the anonymous reviewers that have helped improving the quality of the manuscript.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965; Bai F, 2021, IEEE T ROBOT, V37, P1381, DOI 10.1109/TRO.2021.3050328; Bartoli A, 2013, INT J COMPUT VISION, V101, P227, DOI 10.1007/s11263-012-0565-0; Bartoli A, 2010, INT J COMPUT VISION, V88, P85, DOI 10.1007/s11263-009-0303-4; Bartoli A, 2006, IEEE INT CONF ROBOT, P3083, DOI 10.1109/ROBOT.2006.1642170; Benjemaa R., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P34, DOI 10.1007/BFb0054732; Bilic P., 2019, LIVER TUMOR SEGMENTA; Birtea P, 2019, OPTIM LETT, V13, P1773, DOI 10.1007/s11590-018-1319-x; Bishop C. M., 2006, PATTERN RECOGN, DOI DOI 10.1117/1.2819119.ARNING; Bookstein F. L., 1991, MORPHOMETRIC TOOLS L; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Bouix S, 2005, NEUROIMAGE, V25, P1077, DOI 10.1016/j.neuroimage.2004.12.051; BROCKETT RW, 1989, LINEAR ALGEBRA APPL, V122, P761, DOI 10.1016/0024-3795(89)90675-7; BroNielsen M, 1996, LECT NOTES COMPUT SC, V1131, P267; Brown BJ, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239472; Christensen GE, 2001, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P37, DOI 10.1109/MMBIA.2001.991697; Davis TA, 2006, FUND ALGORITHMS, V2, P27; Dryden I.L., 2016, STAT SHAPE ANAL APPL, V995; DUCHON J, 1976, REV FR AUTOMAT INFOR, V10, P5; Eggert DW, 1997, MACH VISION APPL, V9, P272, DOI 10.1007/s001380050048; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; Fornefett M, 2001, IMAGE VISION COMPUT, V19, P87, DOI 10.1016/S0262-8856(00)00057-3; Freifeld O, 2012, LECT NOTES COMPUT SC, V7572, P1, DOI 10.1007/978-3-642-33718-5_1; Gallardo M, 2017, IEEE I CONF COMP VIS, P3904, DOI 10.1109/ICCV.2017.419; Golub G, 2003, INVERSE PROBL, V19, pR1, DOI 10.1088/0266-5611/19/2/201; GOODALL C, 1991, J ROY STAT SOC B MET, V53, P285, DOI 10.1111/j.2517-6161.1991.tb01825.x; GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478; GREEN BF, 1952, PSYCHOMETRIKA, V17, P429; Hardy G. H., 1952, INEQUALITIES; HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Iserles A., 2000, Acta Numerica, V9, P215, DOI 10.1017/S0962492900002154; Jermyn IH, 2012, LECT NOTES COMPUT SC, V7576, P804, DOI 10.1007/978-3-642-33715-4_58; Joshi SH, 2007, PROC CVPR IEEE, P1643; KANATANI K, 1994, IEEE T PATTERN ANAL, V16, P543, DOI 10.1109/34.291441; Kendall D. G., 2009, SHAPE SHAPE THEORY, V500; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; KENT JT, 1994, J ROY STAT SOC B MET, V56, P285; Kilian M, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276457, 10.1145/1239451.1239515]; Kim YJ, 2008, PROC WRLD ACAD SCI E, V27, P103; Krishnan S., 2005, S GEOM PROC, P187; Kristof W., 1971, P ANN CONV AM PSYCH; Kurtek S, 2012, IEEE T PATTERN ANAL, V34, P1717, DOI 10.1109/TPAMI.2011.233; Kurtek S, 2010, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2010.5539778; Laga, 2018, SURVEY NONRIGID 3D S; Laga H, 2017, IEEE T PATTERN ANAL, V39, P2451, DOI 10.1109/TPAMI.2016.2647596; Matei B., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P339, DOI 10.1109/CVPR.1999.786961; Meyer CD., 2000, MATRIX ANAL APPL LIN; Ohta N, 1998, IEICE T INF SYST, VE81D, P1247; Osher S., 2003, APPL MATH SCI, V153; ROHLF FJ, 1990, SYST ZOOL, V39, P40, DOI 10.2307/2992207; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Song JW, 2020, IEEE INT CONF ROBOT, P9419, DOI 10.1109/ICRA40945.2020.9196930; Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531; Szeliski R, 1997, INT J COMPUT VISION, V22, P199, DOI 10.1023/A:1007996332012; Ten Berge JMF, 1977, PSYCHOMETRIKA, V42, P267, DOI 10.1007/BF02294053; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573; WALKER MW, 1991, CVGIP-IMAG UNDERSTAN, V54, P358, DOI 10.1016/1049-9660(91)90036-O; Wen GJ, 2006, VISUAL COMPUT, V22, P387, DOI 10.1007/s00371-006-0022-6; Williams J, 2001, COMPUT VIS IMAGE UND, V81, P117, DOI 10.1006/cviu.2000.0884; Younes L, 2008, REND LINCEI-MAT APPL, V19, P25; Younes L, 2012, IMAGE VISION COMPUT, V30, P389, DOI 10.1016/j.imavis.2011.09.009	69	2	2	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					567	593		10.1007/s11263-021-01571-8	http://dx.doi.org/10.1007/s11263-021-01571-8		JAN 2022	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	ZB5ZS		Green Submitted			2022-12-18	WOS:000740610500001
J	Hu, QY; Yang, B; Khalid, S; Xiao, W; Trigoni, N; Markham, A				Hu, Qingyong; Yang, Bo; Khalid, Sheikh; Xiao, Wen; Trigoni, Niki; Markham, Andrew			SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Urban-scale; Photogrammetric point cloud dataset; Semantic segmentation; UAV photogrammetry		With the recent availability and affordability of commercial depth sensors and 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets have been publicized to facilitate research in 3D computer vision. However, existing datasets either cover relatively small areas or have limited semantic annotations. Fine-grained understanding of urban-scale 3D scenes is still in its infancy. In this paper, we introduce SensatUrban, an urban-scale UAV photogrammetry point cloud dataset consisting of nearly three billion points collected from three UK cities, covering 7.6 km(2). Each point in the dataset has been labelled with fine-grained semantic annotations, resulting in a dataset that is three times the size of the previous existing largest photogrammetric point cloud dataset. In addition to the more commonly encountered categories such as road and vegetation, urban-level categories including rail, bridge, and river are also included in our dataset. Based on this dataset, we further build a benchmark to evaluate the performance of state-of-the-art segmentation algorithms. In particular, we provide a comprehensive analysis and identify several key challenges limiting urban-scale point cloud understanding. The dataset is available at http://point-cloud-analysis.cs.ox.ac.uk/.	[Hu, Qingyong; Trigoni, Niki; Markham, Andrew] Univ Oxford, Dept Comp Sci, Oxford, England; [Yang, Bo] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kong, Peoples R China; [Khalid, Sheikh] Sensat Ltd, London, England; [Xiao, Wen] Newcastle Univ, Sch Engn, Newcastle Upon Tyne, Tyne & Wear, England	University of Oxford; Hong Kong Polytechnic University; Newcastle University - UK	Yang, B (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kong, Peoples R China.	Qingyong.Hu@cs.ox.ac.uk; bo.yang@polyu.edu.hk; wen.xiao@newcastle.ac.uk; Niki.Trigoni@cs.ox.ac.uk; Andrew.Markham@cs.ox.ac.uk		YANG, Bo/0000-0002-2419-4140	China Scholarship Council (CSC); Huawei UK AI Fellowship; UKRI Natural Environment Research Council (NERC) Flood-PREPARED project [NE/P017134/1]; HK PolyU [P0034792]; Shenzhen Science and Technology Innovation Commission [JCYJ20210324120603011]	China Scholarship Council (CSC)(China Scholarship Council); Huawei UK AI Fellowship(Huawei Technologies); UKRI Natural Environment Research Council (NERC) Flood-PREPARED project(UK Research & Innovation (UKRI)Natural Environment Research Council (NERC)); HK PolyU; Shenzhen Science and Technology Innovation Commission	This work was supported by a China Scholarship Council (CSC) scholarship, Huawei UK AI Fellowship, and the UKRI Natural Environment Research Council (NERC) Flood-PREPARED project (NE/P017134/1). Bo Yang was partially supported by HK PolyU (P0034792) and Shenzhen Science and Technology Innovation Commission (JCYJ20210324120603011). The authors highly appreciate the Data Study Group (DSG) organised by the Alan Turing Institute and the GPU resources generously provided by the LAVA group led by Professor Yulan Guo in the Sun Yat-sen University, China. The authors would also like to thank the pre-training results provided by Hanchen Wang from the University of Cambridge.	Aksoy EE, 2020, IEEE INT VEH SYM, P926; [Anonymous], 2016, ARXIV PREPRINT ARXIV; Armeni Iro, 2017, ARXIV170201105; Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939; Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464; Boulch, 2019, ARXIV PREPRINT ARXIV; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53; Chang MF, 2019, PROC CVPR IEEE, P8740, DOI 10.1109/CVPR.2019.00895; Chen T, 2020, PR MACH LEARN RES, V119; Chenfeng Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P1, DOI 10.1007/978-3-030-58604-1_1; Cheng R, 2021, PROC CVPR IEEE, P12542, DOI 10.1109/CVPR46437.2021.01236; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Cortinhal Tiago, 2020, SALSANEXT FAST UNCER; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; De Deuge M., 2013, P AUSTR C ROB AUT SY, P1; GAIDON A, 2016, PROC CVPR IEEE, P4340, DOI DOI 10.1109/CVPR.2016.470; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A., 2012, P IEEE COMP SOC C CO; Gerke M, 2011, PHOTOGRAMM ENG REM S, V77, P885, DOI 10.14358/PERS.77.9.885; Geyer Jakob, 2020, A2D2 AUDI AUTONOMOUS; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434; Hackel Timo, 2017, ARXIV170403847, VIV-1-W1, P91, DOI DOI 10.5194/ISPRS-ANNALS-IV-1-W1-91-2017; Han L, 2020, PROC CVPR IEEE, P2937, DOI 10.1109/CVPR42600.2020.00301; HANDA A, 2016, PROC CVPR IEEE, P4077, DOI DOI 10.1109/CVPR.2016.442; Haotian* Tang Zhijian*, 2020, EUR C COMP VIS; Hou J., 2020, ARXIV PREPRINT ARXIV; Hou L, 2014, P IEEE, V102, P204, DOI 10.1109/JPROC.2013.2295327; Hu JH, 2003, IEEE COMPUT GRAPH, V23, P62, DOI 10.1109/MCG.2003.1242383; Hu QY, 2021, PROC CVPR IEEE, P4975, DOI 10.1109/CVPR46437.2021.00494; Hu Qingyong, 2020, CVPR, DOI DOI 10.1109/CVPR42600.2020.01112; Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kolle M., 2021, ARXIV PREPRINT ARXIV; Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479; Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298; Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959; Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410; Li XK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P238, DOI 10.1145/3394171.3413661; Li YY, 2018, ADV NEUR IN, V31; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu ZJ, 2019, ADV NEUR IN, V32; Lyu Yecheng, 2020, P IEEE CVF C COMP VI, P12255, DOI DOI 10.1109/CVPR42600.2020.01227; Meng HY, 2019, IEEE I CONF COMP VIS, P8499, DOI 10.1109/ICCV.2019.00859; Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762; Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100; Munoz D, 2009, PROC CVPR IEEE, P975, DOI 10.1109/CVPRW.2009.5206590; Ozdemir E., 2019, INT ARCH PHOTOGRAMM, V42, P53, DOI DOI 10.5194/ISPRS-ARCHIVES-XLII-1-W2-53-2019; Pan YC, 2020, IEEE INT VEH SYM, P687; Poursaeed Omid, 2020, ECCV; Qi CR, 2017, ADV NEUR IN, V30; Qin NN, 2021, IEEE COMPUT SOC CONF, P1082, DOI 10.1109/CVPRW53098.2021.00119; Rao D, 2010, IEEE INT C INT ROBOT, P2578, DOI 10.1109/IROS.2010.5650493; ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352; Rosu R. A., 2019, P ROB SCI SYST; Rottensteiner F., 2012, ISPRS ANN PHOTOGRAMM, VI- 3, P293, DOI [10.5194/isprsannals-I-3-293-2012, DOI 10.5194/ISPRSANNALS-I-3-293-2012]; Roynard X, 2018, INT J ROBOT RES, V37, P545, DOI 10.1177/0278364918767506; Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34; Sauder J, 2019, ADV NEUR IN, V32; Serna Andres, 2014, 3rd International Conference on Pattern Recognition Applications and Methods (ICPRAM 2014). Proceedings, P819; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252; Tan WK, 2020, IEEE COMPUT SOC CONF, P797, DOI 10.1109/CVPRW50498.2020.00109; Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409; Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Tong GF, 2020, IEEE ACCESS, V8, P87695, DOI 10.1109/ACCESS.2020.2992612; Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167; Valada Abhinav, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4644, DOI 10.1109/ICRA.2017.7989540; Vallet B, 2015, COMPUT GRAPH-UK, V49, P126, DOI 10.1016/j.cag.2015.03.004; Varney N, 2020, IEEE COMPUT SOC CONF, P717, DOI 10.1109/CVPRW50498.2020.00101; Wang HB, 2020, IEEE MULTIMEDIA, V27, P112, DOI 10.1109/MMUL.2020.2999464; Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wei JC, 2020, PROC CVPR IEEE, P4383, DOI 10.1109/CVPR42600.2020.00444; Westoby MJ, 2012, GEOMORPHOLOGY, V179, P300, DOI 10.1016/j.geomorph.2012.08.021; Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI 10.1109/ICRA.2019.8793495; Wu BC, 2018, IEEE INT CONF ROBOT, P1887; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xun Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13703, DOI 10.1109/CVPR42600.2020.01372; Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563; Yang B, 2019, ADV NEUR IN, V32; Yang Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9598, DOI 10.1109/CVPR42600.2020.00962; Ye XQ, 2018, LECT NOTES COMPUT SC, V11211, P415, DOI 10.1007/978-3-030-01234-2_25; Ye Z, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9070450; Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238; Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088; Zhang Z., 2021, ARXIV PREPRINT ARXIV; Zhang ZC, 2018, INT J APPL EARTH OBS, V70, P25, DOI 10.1016/j.jag.2018.04.002; Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169; Zhao H, 2020, P IEEECVF C COMPUTER, P10076, DOI 10.1109/CVPR42600.2020.01009; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zhu X., 2021, P IEEE CVF C COMP VI; Zolanvari SMI, 2019, ARXIV190903613, P44	100	2	2	6	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					316	343		10.1007/s11263-021-01554-9	http://dx.doi.org/10.1007/s11263-021-01554-9		JAN 2022	28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		Green Submitted			2022-12-18	WOS:000738511800001
J	Li, JZZ; Zhang, J; Maybank, SJ; Tao, DC				Li, Jizhizi; Zhang, Jing; Maybank, Stephen J.; Tao, Dacheng			Bridging Composite and Real: Towards End-to-End Deep Image Matting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image matting; Deep learning; Alpha matte; Image composition; Domain gap		Extracting accurate foregrounds from natural images benefits many downstream applications such as film production and augmented reality. However, the furry characteristics and various appearance of the foregrounds, e.g., animal and portrait, challenge existing matting methods, which usually require extra user inputs such as trimap or scribbles. To resolve these problems, we study the distinct roles of semantics and details for image matting and decompose the task into two parallel sub-tasks: high-level semantic segmentation and low-level details matting. Specifically, we propose a novel Glance and Focus Matting network (GFM), which employs a shared encoder and two separate decoders to learn both tasks in a collaborative manner for end-to-end natural image matting. Besides, due to the limitation of available natural images in the matting task, previous methods typically adopt composite images for training and evaluation, which result in limited generalization ability on real-world images. In this paper, we investigate the domain gap issue between composite images and real-world images systematically by conducting comprehensive analyses of various discrepancies between the foreground and background images. We find that a carefully designed composition route RSSN that aims to reduce the discrepancies can lead to a better model with remarkable generalization ability. Furthermore, we provide a benchmark containing 2,000 high-resolution real-world animal images and 10,000 portrait images along with their manually labeled alpha mattes to serve as a test bed for evaluating matting model's generalization ability on real-world images. Comprehensive empirical studies have demonstrated that GFM outperforms state-of-the-art methods and effectively reduces the generalization error. The code and the datasets will be released at hups://github.com/JizhiziLi/GFM.	[Li, Jizhizi; Zhang, Jing; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia; [Maybank, Stephen J.] Univ London, Birkbeck Coll, Dept Comp Sci & Informat Syst, London, England	University of Sydney; University of London; Birkbeck University London	Tao, DC (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia.	jili8515@uni.sydney.edu.au; jing.zhang1@sydney.edu.au; sjmaybank@dcs.bbk.ac.uk; dacheng.tao@sydney.edu.au		Li, Jizhizi/0000-0003-1715-392X; ZHANG, JING/0000-0001-6595-7661	Australian Research Council [FL-170100117, IH-180100002, IC-190100031]	Australian Research Council(Australian Research Council)	This work was supported by Australian Research Council Projects FL-170100117, IH-180100002, IC-190100031.	Aksoy Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201275; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Cai SF, 2019, IEEE I CONF COMP VIS, P8818; Chen BC, 2019, PROC CVPR IEEE, P8407, DOI 10.1109/CVPR.2019.00861; Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18; Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610; Chen Z, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01370-7; Dabov K., 2009, SIGNAL PROCESSING AD; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jinlin Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8560, DOI 10.1109/CVPR42600.2020.00859; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Li XL, 2018, IEEE T NEUR NET LEAR, V29, P3214, DOI 10.1109/TNNLS.2017.2727140; Li YY, 2020, AAAI CONF ARTIF INTE, V34, P11450; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404; Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336; Ning Xu, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P311, DOI 10.1109/CVPR.2017.41; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766; Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruzon MA, 2000, PROC CVPR IEEE, P18, DOI 10.1109/CVPR.2000.855793; Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6; Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721; Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317; Tsai YH, 2017, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2017.299; Wang J, 2005, IEEE I CONF COMP VIS, P936; Wang Jue, 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383006; Wenyan Cong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8391, DOI 10.1109/CVPR42600.2020.00842; Yu Q., 2021, P IEEE C COMP VIS PA; Yu Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13673, DOI 10.1109/CVPR42600.2020.01369; Zhang J, 2021, INT J COMPUT VISION, V129, P2639, DOI 10.1007/s11263-021-01482-8; Zhang J, 2021, IEEE INTERNET THINGS, V8, P7789, DOI 10.1109/JIOT.2020.3039359; Zhang QM, 2019, ADV NEUR IN, V32; Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765; Zheng Y, 2008, 10 INT C UB COMP UBI, P1, DOI 10.1145/1658373.1658374	43	2	2	4	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					246	266		10.1007/s11263-021-01541-0	http://dx.doi.org/10.1007/s11263-021-01541-0		JAN 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		Green Submitted			2022-12-18	WOS:000738560100001
J	Shin, A; Ishii, M; Narihira, T				Shin, Andrew; Ishii, Masato; Narihira, Takuya			Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with Language and Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Language and vision; Transformer; Attention; BERT		Transformer architectures have brought about fundamental changes to computational linguistic field, which had been dominated by recurrent neural networks for many years. Its success also implies drastic changes in cross-modal tasks with language and vision, and many researchers have already tackled the issue. In this paper, we review some of the most critical milestones in the field, as well as overall trends on how transformer architecture has been incorporated into visuolinguistic cross-modal tasks. Furthermore, we discuss its current limitations and speculate upon some of the prospects that we find imminent.	[Shin, Andrew; Ishii, Masato; Narihira, Takuya] Sony Corp, Tokyo, Japan	Sony Corporation	Shin, A (corresponding author), Sony Corp, Tokyo, Japan.	andrew.shin@sony.com; masato.a.ishii@sony.com; takuya.narihira@sony.com		Shin, Andrew/0000-0002-0969-9925				Abu-El-Haija S, 2016, YOUTUBE 8M LARGE SCA; Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13; Akbari H, 2021, ABS210411178 CORR; Alberti C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2131; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ba JL, 2016, LAYER NORMALIZATION; Banerjee S., 2005, P ACL WORKSH INTR EX, P65; Barbu A, 2012, VIDEO SENTENCES OUT; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Bengio Y., 2014, ARXIV14061078; Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, 10.48550/arXiv.2005.14165]; Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13; Chang Wei-Cheng, 2020, INT C LEARN REPR; Chen, PRETRAINED IMAGE PRO; Chen M, 2020, PR MACH LEARN RES, V119; Chen Yen-Chun, 2020, ECCV; Child R., 2019, ARXIV190410509; Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323; Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978; Das P, 2013, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2013.340; Devlin J., 2019, P 2019 C N AM CHAPTE, P4171, DOI [10.18653/v1/n19-1423, DOI 10.18653/V1/N19-1423]; Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174; Dong LH, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5884; Dosovitskiy A, 2020, IMAGE IS WORTH 16X16; Dufter P, 2021, COMPUT LINGUIST, P1; Elliott Desmond, 2013, P 2013 C EMP METH NA, P1292; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2; Fedus W, 2021, SWITCH TRANSFORMERS; Gabeur V., 2020, ECCV, P214; Gella Spandana, 2018, P 2018 C EMP METH NA, P968, DOI DOI 10.18653/V1/D18-1117; Gillick Daniel, 2018, ABS181108008 CORR; Ging S, 2020, COOT COOPERATIVE HIE; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Guo J, 2012, LAMP LABEL AUGMENTED; Han K, 2021, SURVEY VISUAL TRANSF; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698; Hendrycks Dan, 2016, ARXIV160608415; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu R, 2021, ABS210210772 CORR; Huang G, P 1 C AS PAC ASS COM, P470; Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686; Jiao Xiaoqi, 2020, TINYBERT DISTILLING; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; JUN SH, 2017, HADAMARD PRODUCT LOW; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI DOI 10.1109/CVPR42600.2020.00813; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kazemzadeh Sahar, 2014, P 2014 C EMP METH NA, P787, DOI DOI 10.3115/V1/D14-1086; Kervadec C, 2019, WEAK SUPERVISION HEL; Khan S, 2021, TRANSFORMERS VISION; Kim W, 2021, VILT VISION AND LANG; Kingma D.P, P 3 INT C LEARNING R; Kiros R, 2015, SKIP THOUGHT VECTORS; Kitaev Nikita, 2020, ARXIV200104451; Korbar B, 2020, VIDEO UNDERSTANDING; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lei J, 2021, LESS IS MORE CLIPBER; Li, 2005, HERO HIERARCHICAL EN; Li C, 2021, SEMVLP VISION LANGUA; Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336; Li X, 2020, OSCAR OBJECT SEMANTI; Li X, 2021, ABS210411746 CORR; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Lin J, 2021, M6 V0 VISION LANGUAG; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu B, 2020, PIXEL BERT ALIGNING; Liu XD, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4487; Liu Y, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5070; Lu JS, 2019, ADV NEUR IN, V32; Lu JS, 2016, ADV NEUR IN, V29; Luo F, 2020, CAPT CONTRASTIVE PRE; Luo H, 2020, UNIVL UNIFIED VIDEO; Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; NOSOFSKY RM, 1986, J EXP PSYCHOL GEN, V115, P39, DOI 10.1037/0096-3445.115.1.39; Ordonez Vicente, 2011, ADV NEURAL INFORM PR, P1143; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Parmar N, 2018, IMAGE TRANSFORMER; Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Peters Matthew E., 2018, P 2018 C N AM CHAPT, V1, P2227, DOI DOI 10.18653/V1/N18-1202; Qi D, 2020, IMAGEBERT CROSS MODA; Radford A, 2021, PR MACH LEARN RES, V139; Ramesh A, 2021, ZERO SHOT TEXT TO IM; Reed S, 2016, PR MACH LEARN RES, V48; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sanh V, 2020, DISTILBERT DISTILLED; Shao S, 2019, IEEE I CONF COMP VIS, P8429, DOI 10.1109/ICCV.2019.00852; Sharir O, 2020, COST TRAINING NLP MO; Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556; Su Weijie, 2020, INT C LEARN REPR; Suhr A, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6418; Sun C, 2020, LEARNING VIDEO REPRE; Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100; Tan MX, 2019, PR MACH LEARN RES, V97; Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501; Touvron H, 2020, TRAINING DATA EFFICI; Ushiku Y, 2012, ACM MULTIMEDIA; Vaswani A, 2017, ADV NEUR IN, V30; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Venugopalan S, P 2015 C N AM ASS CO, P1494, DOI 10.3115/v1/N15-1173; Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang B, 2021, INT C LEARN REPR; Wang H, 2020, MAX DEEPLAB END TO E; Wang J, 2020, MINIVLM SMALLER FAST; Wang Wenhai, 2021, ARXIV210212122; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Wang YQ, 2020, INT CONF ACOUST SPEE, P6874, DOI 10.1109/ICASSP40776.2020.9054345; Wu L., 2017, STARSPACE EMBED ALL; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang J., 2019, EMBODIED VISUAL RECO; Yang Z., ADV NEURAL INFORM PR, P5753; Yang Z., 2015, ARXIV151102274; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Young P., 2014, P TACL, V2, P67, DOI 10.1162/tacl_a_00166; Yu F., 2020, ERNIE VIL KNOWLEDGE; Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5; Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94; Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688; Zhang B., 2018, CROSS MODAL HIERARCH; Zhang P, 2016, PROC CVPR IEEE, P5014, DOI 10.1109/CVPR.2016.542; Zhang S., 2020, P 28 ACM INT C MULT, DOI 10.1145/3394171.3413518; Zhang Z, 2018, IEEE ICC, DOI 10.1145/3240765.3240860; Zhou L., 2017, ABS170309788 CORR; Zhou L., 2016, WATCH WHAT YOU JUST; Zhou L., 2019, UNIFIED VISION LANGU; Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911; Zhukov D, 2019, PROC CVPR IEEE, P3532, DOI 10.1109/CVPR.2019.00365	148	2	2	5	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					435	454		10.1007/s11263-021-01547-8	http://dx.doi.org/10.1007/s11263-021-01547-8		JAN 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		Green Submitted			2022-12-18	WOS:000738511800003
J	Yang, YX; Cao, Q; Zhang, J; Tao, DC				Yang, Yuxiang; Cao, Qi; Zhang, Jing; Tao, Dacheng			CODON: On Orchestrating Cross-Domain Attentions for Depth Super-Resolution	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Depth super-resolution; Cross-domain orchestration; Attention conciliation module; Recursive multi-scale convolution		The ready accessibility of high-resolution image sensors has stimulated interest in increasing depth resolution by leveraging paired color information as guidance. Nevertheless, how to effectively exploit the depth and color features to achieve a desired depth super-resolution effect remains challenging. In this paper, we propose a novel depth super-resolution method called CODON, which orchestrates cross-domain attentive features to address this problem. Specifically, we devise two essential modules: the recursive multi-scale convolutional module (RMC) and the cross-domain attention conciliation module (CAC). RMC discovers detailed color and depth features by sequentially stacking weight-shared multi-scale convolutional layers, in order to deepen and widen the network at low-complexity. CAC calculates conciliated attention from both domains and uses it as shared guidance to enhance the edges in depth feature while suppressing textures in color feature. Then, the jointly conciliated attentive features are combined and fed into a RMC prediction branch to reconstruct the high-resolution depth image. Extensive experiments on several popular benchmark datasets including Middlebury, New Tsukuba, Sintel, and NYU-V2, demonstrate the superiority of our proposed CODON over representative state-of-the-art methods.	[Yang, Yuxiang; Cao, Qi] Hangzhou Dianzi Univ, Sch Elect & Informat, Hangzhou 310018, Peoples R China; [Yang, Yuxiang; Cao, Qi] Zhejiang Prov Key Lab Equipment Elect, Hangzhou 310018, Peoples R China; [Zhang, Jing] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia; [Tao, Dacheng] JD Explore Acad JDcom, Beijing 101111, Peoples R China	Hangzhou Dianzi University; University of Sydney	Zhang, J (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia.; Tao, DC (corresponding author), JD Explore Acad JDcom, Beijing 101111, Peoples R China.	jing.zhang1@sydney.edu.au; dacheng.tao@gmail.com			National Natural Science Foundation of China [61873077, 61806062]; Zhejiang Provincial Key Lab of Equipment Electronics	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Zhejiang Provincial Key Lab of Equipment Electronics	This work was supported by the National Natural Science Foundation of China (61873077, 61806062). This work was supported by the Zhejiang Provincial Key Lab of Equipment Electronics.	Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Buades A, 2019, INT J COMPUT VISION, V127, P1474, DOI 10.1007/s11263-019-01200-5; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Diebel J., 2006, ADV NEURAL INFORM PR, P291; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127; Ham B, 2015, PROC CVPR IEEE, P4823, DOI 10.1109/CVPR.2015.7299115; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22; Kiechle M, 2013, IEEE I CONF COMP VIS, P1545, DOI 10.1109/ICCV.2013.195; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kwon H, 2015, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2015.7298611; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32; Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10; Lu JJ, 2015, PROC CVPR IEEE, P2245, DOI 10.1109/CVPR.2015.7298837; Lutio R. D., 2019, P IEEE INT C COMP VI, P8829; Martull S., 2012, ICPR WORKSHOP TRAKMA, V111, P117; Mustafa HT, 2019, IMAGE VISION COMPUT, V85, P26, DOI 10.1016/j.imavis.2019.03.001; Nair V, 2010, P 27 INT C MACHINE L, P807; Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Scharstein D, 2003, PROC CVPR IEEE, P195; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3; Shen XY, 2015, IEEE I CONF COMP VIS, P3406, DOI 10.1109/ICCV.2015.389; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Supancic JS, 2018, INT J COMPUT VISION, V126, P1180, DOI 10.1007/s11263-018-1081-7; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wei M, 2018, LECT NOTES COMPUT SC, V10704, P142, DOI 10.1007/978-3-319-73603-7_12; Wen Y, 2019, IEEE T IMAGE PROCESS, V28, P994, DOI 10.1109/TIP.2018.2874285; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Yanjie Li, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P152, DOI 10.1109/ICME.2012.30; Yue KY, 2018, ADV NEUR IN, V31; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang L, 2020, INT J COMPUT VISION, V128, P479, DOI 10.1007/s11263-019-01253-6; Zhang XY, 2020, INT J COMPUT VISION, V128, P1699, DOI 10.1007/s11263-019-01285-y; Zhang Yulun, 2018, P EUROPEAN C COMPUTE, P286; Zhou HZ, 2020, INT J COMPUT VISION, V128, P756, DOI 10.1007/s11263-019-01221-0; Zhou WT, 2017, INT CONF ACOUST SPEE, P1457, DOI 10.1109/ICASSP.2017.7952398; Zhu J, 2017, IEEE IMAGE PROC, P4068; Zou CH, 2019, INT J COMPUT VISION, V127, P143, DOI 10.1007/s11263-018-1133-z	47	2	2	4	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					267	284		10.1007/s11263-021-01545-w	http://dx.doi.org/10.1007/s11263-021-01545-w		JAN 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS					2022-12-18	WOS:000738511800002
J	Song, X; Yang, GR; Zhu, XG; Zhou, H; Ma, YX; Wang, Z; Shi, JP				Song, Xiao; Yang, Guorun; Zhu, Xinge; Zhou, Hui; Ma, Yuexin; Wang, Zhe; Shi, Jianping			AdaStereo: An Efficient Domain-Adaptive Stereo Matching Approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Domain adaptation; Stereo matching; Non-adversarial progressive color transfer; Cost normalization; Self-supervised occlusion-aware reconstruction		Recently, records on stereo matching benchmarks are constantly broken by end-to-end disparity networks. However, the domain adaptation ability of these deep models is quite limited. Addressing such problem, we present a novel domain-adaptive approach called AdaStereo that aims to align multi-level representations for deep stereo matching networks. Compared to previous methods, our AdaStereo realizes a more standard, complete and effective domain adaptation pipeline. Firstly, we propose a non-adversarial progressive color transfer algorithm for input image-level alignment. Secondly, we design an efficient parameter-free cost normalization layer for internal feature-level alignment. Lastly, a highly related auxiliary task, self-supervised occlusion-aware reconstruction is presented to narrow the gaps in output space. We perform intensive ablation studies and break-down comparisons to validate the effectiveness of each proposed module. With no extra inference overhead and only a slight increase in training complexity, our AdaStereo models achieve state-of-the-art cross-domain performance on multiple benchmarks, including KITTI, Middlebury, ETH3D and DrivingStereo, even outperforming some state-of-the-art disparity networks finetuned with target-domain ground-truths. Moreover, based on two additional evaluation metrics, the superiority of our domain-adaptive stereo matching pipeline is further uncovered from more perspectives. Finally, we demonstrate that our method is robust to various domain adaptation settings, and can be easily integrated into quick adaptation application scenarios and real-world deployments.	[Song, Xiao; Zhou, Hui; Wang, Zhe; Shi, Jianping] SenseTime Grp Ltd, Hong Kong, Peoples R China; [Yang, Guorun] Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China; [Zhu, Xinge] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Ma, Yuexin] ShanghaiTech Univ, Shanghai, Peoples R China	Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; Chinese University of Hong Kong; ShanghaiTech University	Song, X (corresponding author), SenseTime Grp Ltd, Hong Kong, Peoples R China.	songxiao@sensetime.com; yangguorun91@gmail.com; zhuxinge123@gmail.com; zhouhui@sensetime.com; mayuezin@shanghaitech.edu.cn; wangzhe@sensetime.com; shijianping@sensetime.com						Abramov Alexey, 2020, ARXIV200512551; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Atapour-Abarghouei Amir, 2018, P IEEE C COMP VIS PA, P2800, DOI DOI 10.1109/CVPR.2018.00296; Black MJ, 1996, INT J COMPUT VISION, V19, P57, DOI 10.1007/BF00131148; Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Chabra R, 2019, PROC CVPR IEEE, P11778, DOI 10.1109/CVPR.2019.01206; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen XZ, 2015, ADV NEUR IN, V28; Chen YH, 2018, PROC CVPR IEEE, P7892, DOI 10.1109/CVPR.2018.00823; Chen ZY, 2015, IEEE I CONF COMP VIS, P972, DOI 10.1109/ICCV.2015.117; Cheng X, 2020, ANAT SCI EDUC, V13, P759, DOI 10.1002/ase.1956; Cheng Xinjing, 2019, IEEE T PATTERN ANAL; Dai Z., 2019, ARXIV PREPRINT ARXIV; Di Stefano L, 2005, PATTERN RECOGN LETT, V26, P2129, DOI 10.1016/j.patrec.2005.03.022; Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448; Engel J, 2015, IEEE INT C INT ROBOT, P1935, DOI 10.1109/IROS.2015.7353631; Fang L., 2018, ACCV; Feihu Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P420, DOI 10.1007/978-3-030-58536-5_25; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Franke U, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P273, DOI 10.1109/IVS.2000.898354; Geiger A., 2012, P IEEE COMP SOC C CO; Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Gidaris Spyros, 2018, ARXIV180307728; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gomez-Ojeda R, 2019, IEEE T ROBOT, V35, P734, DOI 10.1109/TRO.2019.2899783; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258; Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257; Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339; Guo XY, 2018, LECT NOTES COMPUT SC, V11215, P506, DOI 10.1007/978-3-030-01252-6_30; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615; KANG SB, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P88, DOI 10.1109/ICCV.1995.466802; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363; Li PL, 2018, LECT NOTES COMPUT SC, V11206, P664, DOI 10.1007/978-3-030-01216-8_40; Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783; Li Yanghao, 2016, ARXIV160304779; Liang ZF, 2021, IEEE T PATTERN ANAL, V43, P300, DOI 10.1109/TPAMI.2019.2928550; Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297; Liu Rui, 2020, CVPR, P12757; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Long MS, 2018, ADV NEUR IN, V31; Long MS, 2015, PR MACH LEARN RES, V37, P97; Lopez-Rodriguez A., 2020, ARXIV PREPRINT ARXIV; LUO WJ, 2016, PROC CVPR IEEE, P5695, DOI DOI 10.1109/CVPR.2016.614; Mattoccia S, 2008, IEEE T IMAGE PROCESS, V17, P528, DOI 10.1109/TIP.2008.919362; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Menze Moritz, 2015, CVPR; Miclea V. C., 2019, TITS; Nie GY, 2019, PROC CVPR IEEE, P3278, DOI 10.1109/CVPR.2019.00340; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; Pang JH, 2018, PROC CVPR IEEE, P2070, DOI 10.1109/CVPR.2018.00221; Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108; Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629; Roy S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P492, DOI 10.1109/ICCV.1998.710763; Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712; Saminger-Platz S., 2017, P INT C LEARN REPR I; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3; Schonberger JL, 2018, LECT NOTES COMPUT SC, V11217, P758, DOI 10.1007/978-3-030-01261-8_45; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703; Shaked A, 2017, PROC CVPR IEEE, P6901, DOI 10.1109/CVPR.2017.730; Song X, 2020, INT J COMPUT VISION, V128, P910, DOI 10.1007/s11263-019-01287-w; Song X, 2019, LECT NOTES COMPUT SC, V11365, P20, DOI 10.1007/978-3-030-20873-8_2; Song Xiaolin, 2021, CVPR; Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262; Tulyakov S, 2017, IEEE I CONF COMP VIS, P1348, DOI 10.1109/ICCV.2017.150; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Ulyanov D., 2016, ARXIV160708022; Wang R, 2017, IEEE I CONF COMP VIS, P3923, DOI 10.1109/ICCV.2017.421; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wu ZY, 2019, IEEE I CONF COMP VIS, P7483, DOI 10.1109/ICCV.2019.00758; Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203; Xu JL, 2019, IEEE ACCESS, V7, P156694, DOI 10.1109/ACCESS.2019.2949697; Yang G., 2018, ACCV; Yang GS, 2019, PROC CVPR IEEE, P5510, DOI 10.1109/CVPR.2019.00566; Yang GR, 2019, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2019.00099; Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39; Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620; Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913; Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345; Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767; Zhang CX, 2010, LECT NOTES COMPUT SC, V6314, P708, DOI 10.1007/978-3-642-15561-1_51; Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zhang YM, 2020, AAAI CONF ARTIF INTE, V34, P12926; Zhao SS, 2019, PROC CVPR IEEE, P9780, DOI 10.1109/CVPR.2019.01002; Zheng CX, 2018, LECT NOTES COMPUT SC, V11211, P798, DOI 10.1007/978-3-030-01234-2_47; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zhu JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4558; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XG, 2018, LECT NOTES COMPUT SC, V11211, P587, DOI 10.1007/978-3-030-01234-2_35; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953	111	2	2	6	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					226	245		10.1007/s11263-021-01549-6	http://dx.doi.org/10.1007/s11263-021-01549-6		JAN 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		Green Submitted			2022-12-18	WOS:000738511600001
J	Hagemann, A; Knorr, M; Janssen, H; Stiller, C				Hagemann, Annika; Knorr, Moritz; Janssen, Holger; Stiller, Christoph			Inferring Bias and Uncertainty in Camera Calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Camera calibration; Uncertainty; Systematic errors; Bias; Computer vision		Accurate camera calibration is a precondition for many computer vision applications. Calibration errors, such as wrong model assumptions or imprecise parameter estimation, can deteriorate a system's overall performance, making the reliable detection and quantification of these errors critical. In this work, we introduce an evaluation scheme to capture the fundamental error sources in camera calibration: systematic errors (biases) and uncertainty (variance). The proposed bias detection method uncovers smallest systematic errors and thereby reveals imperfections of the calibration setup and provides the basis for camera model selection. A novel resampling-based uncertainty estimator enables uncertainty estimation under non-ideal conditions and thereby extends the classical covariance estimator. Furthermore, we derive a simple uncertainty metric that is independent of the camera model. In combination, the proposed methods can be used to assess the accuracy of individual calibrations, but also to benchmark new calibration algorithms, camera models, or calibration setups. We evaluate the proposed methods with simulations and real cameras.	[Hagemann, Annika; Knorr, Moritz; Janssen, Holger] Robert Bosch GmbH, Comp Vis, Corp Res, Robert Bosch Str 200, D-31139 Hildesheim, Germany; [Hagemann, Annika; Stiller, Christoph] Karlsruhe Inst Technol KIT, Inst Measurement & Control, Karlsruhe, Germany	Bosch; Helmholtz Association; Karlsruhe Institute of Technology	Hagemann, A (corresponding author), Robert Bosch GmbH, Comp Vis, Corp Res, Robert Bosch Str 200, D-31139 Hildesheim, Germany.; Hagemann, A (corresponding author), Karlsruhe Inst Technol KIT, Inst Measurement & Control, Karlsruhe, Germany.	annika.hagemann@de.bosch.com; moritzmichael.knorr@de.bosch.com; holger.janssen@de.bosch.com; stiller@kit.edu		Hagemann, Annika/0000-0002-2429-0773				Abraham S., 1998, MUSTERERKENNUNG 1998, P117, DOI DOI 10.1007/978-3-642-72282-0_11; Beck J, 2018, IEEE INT VEH SYM, P2137, DOI 10.1109/IVS.2018.8500466; Bodenham DA, 2016, STAT COMPUT, V26, P917, DOI 10.1007/s11222-015-9583-4; Cheong LF, 2004, COMPUT VIS IMAGE UND, V93, P221, DOI 10.1016/j.cviu.2003.09.003; Cheong LF, 2011, COMPUT VIS IMAGE UND, V115, P16, DOI 10.1016/j.cviu.2010.08.004; Cramariuc A, 2020, IEEE INT CONF ROBOT, P4997, DOI 10.1109/ICRA40945.2020.9197378; Davison AC, 1997, BOOTSTRAP METHODS TH, V1; Doran, 1989, APPL REGRESSION ANAL, V102; EDWARDS MJ, 2020, 2020 35 INT C IMAGE, P1; Forstner W, 2016, GEOM COMPUT, V11; Furgale P, 2013, IEEE INT C INT ROBOT, P1280, DOI 10.1109/IROS.2013.6696514; Hartley R., 2004, MULTIPLE VIEW GEOMET, DOI [10.1017/cbo9780511811685, DOI 10.1017/CBO9780511811685, 10.1017/CBO9780511811685]; IMHOF JP, 1961, BIOMETRIKA, V48, P419, DOI 10.1093/biomet/48.3-4.419; Kosecka, 2001, MOTION BIAS STRUCTUR, DOI 10.5244/C.15.68; Lavest J.-M., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P158, DOI 10.1007/BFb0055665; Luhmann T., 2013, CLOSE RANGE PHOTOGRA, DOI [10.1515/9783110302783, DOI 10.1515/9783110302783]; MathWorks, MATL COMP VIS TOOLB; Mei C, 2007, IEEE INT CONF ROBOT, P3945, DOI 10.1109/ROBOT.2007.364084; OpenCV, OPENCV FISH CAM MOD; OpenCV, OPENCV TUT CAM CAL; Oth L, 2013, PROC CVPR IEEE, P1360, DOI 10.1109/CVPR.2013.179; Ozog P, 2013, IEEE INT CONF ROBOT, P3777, DOI 10.1109/ICRA.2013.6631108; Peng SY, 2019, IEEE I CONF COMP VIS, P1497, DOI 10.1109/ICCV.2019.00158; Richardson A, 2013, IEEE INT C INT ROBOT, P1814, DOI 10.1109/IROS.2013.6696595; Rojtberg P, 2018, INT SYM MIX AUGMENT, P31, DOI 10.1109/ISMAR.2018.00026; ROS, ROS TUT MONOCULARCAL; ROUSSEEUW PJ, 1993, J AM STAT ASSOC, V88, P1273, DOI 10.2307/2291267; Sattler, 2019, ARXIV PREPRINT ARXIV; Scaramuzza D, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P5695, DOI 10.1109/IROS.2006.282372; Semeniuta O, 2016, PROC CIRP, V41, P765, DOI 10.1016/j.procir.2015.12.108; Stiller C., 2020, IN PRESS; Strau T, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2623, DOI 10.1109/ITSC.2014.6958110; Strauss, 2015, KALIBRIERUNG MULTIKA; Sturm P., 1996, WHAT CAN BE DONE BAD; Sturm P., 1999, P IEEE C COMP VIS PA, P432, DOI DOI 10.1109/CVPR.1999.786974; Sun W, 2006, MACH VISION APPL, V17, P51, DOI 10.1007/s00138-006-0014-6; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Vision, 2021, MANTA G 235; Wiggenhagen M., 2002, 22002 PFG, P117; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	40	2	2	8	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					17	32		10.1007/s11263-021-01528-x	http://dx.doi.org/10.1007/s11263-021-01528-x		OCT 2021	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC		Green Submitted			2022-12-18	WOS:000708332900001
J	Wang, N; Gao, Y; Chen, H; Wang, P; Tian, Z; Shen, CH; Zhang, YN				Wang, Ning; Gao, Yang; Chen, Hao; Wang, Peng; Tian, Zhi; Shen, Chunhua; Zhang, Yanning			NAS-FCOS: Efficient Search for Object Detection Architectures	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Neural architecture search; Object detection; Reinforcement learning; Deep learning		Neural Architecture Search (NAS) has shown great potential in effectively reducing manual effort in network design by automatically discovering optimal architectures. What is noteworthy is that as of now, object detection is less touched by NAS algorithms despite its significant importance in computer vision. To the best of our knowledge, most of the recent NAS studies on object detection tasks fail to satisfactorily strike a balance between performance and efficiency of the resulting models, let alone the excessive amount of computational resources cost by those algorithms. Here we propose an efficient method to obtain better object detectors by searching for the feature pyramid network as well as the prediction head of a simple anchor-free object detector, namely, FCOS (Tian et al. in FCOS: Fully convolutional one-stage object detection, 2019), using a tailored reinforcement learning paradigm. With carefully designed search space, search algorithms, and strategies for evaluating network quality, we are able to find top-performing detection architectures within 4 days using 8 V100 GPUs. The discovered architectures surpass state-of-the-art object detection models (such as Faster R-CNN, RetinaNet and, FCOS) by 1.0 to 5.4% points in AP on the COCO dataset, with comparable computation complexity and memory footprint, demonstrating the efficacy of the proposed NAS method for object detection. Code is available at .	[Wang, Ning; Gao, Yang; Wang, Peng; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian, Peoples R China; [Wang, Ning; Gao, Yang; Wang, Peng; Zhang, Yanning] Natl Engn Lab Integrated AeroSp Ground Ocean Big, Xian, Peoples R China; [Chen, Hao; Tian, Zhi] Univ Adelaide, Adelaide, SA, Australia; [Shen, Chunhua] Monash Univ, Melbourne, Vic, Australia	Northwestern Polytechnical University; University of Adelaide; Monash University	Wang, P (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian, Peoples R China.; Wang, P (corresponding author), Natl Engn Lab Integrated AeroSp Ground Ocean Big, Xian, Peoples R China.	peng.wang@nwpu.edu.cn			National Key R&D Program of China [2020AAA0106900]; National Natural Science Foundation of China [U19B2037, 61876152]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	NW, YG, PW and YZ's participation in this work was supported by the National Key R&D Program of China (No. 2020AAA0106900), and the National Natural Science Foundation of China (Nos. U19B2037, 61876152).	Cai Han, 2019, INT C LEARN REPR; Chen B., 2020, P IEEE CVF C COMP VI, P13607; Chen Y., 2019, DETNAS NEURAL ARCHIT, P6638; Du X., 2020, P IEEE CVF C COMP VI, P11592; Elsken T, 2019, J MACH LEARN RES, V20; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Jiang Chenhan, 2020, P C COMP VIS PATT RE; Jianyuan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11402, DOI 10.1109/CVPR42600.2020.01142; Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656; Kong T., 2019, ARXIV190403797; Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45; Liang F., 2020, P INT C LEARN REPR; Liao, 2020, ARXIV; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Pham H, 2018, PR MACH LEARN RES, V80; Redmon J., 2018, COMPUTER VISION PATT; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Stamoulis D., 2019, ARXIV PREPRINT ARXIV; Tan M., 2020, P IEEE ICVF C COMOP, P10781; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xiong Y., 2020, ARXIV PREPRINT ARXIV; Xu H, 2019, IEEE I CONF COMP VIS, P6648, DOI 10.1109/ICCV.2019.00675; Ya LW, 2020, AAAI CONF ARTIF INTE, V34, P12661; Yang Z., 2020, P EUR C COMP VIS; Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320; Zhong Y., 2020, P EUR C COMP VIS; Zhou HP, 2019, PR MACH LEARN RES, V97; Zhou X., 2019, ARXIV190407850V2; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	51	2	3	10	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3299	3312		10.1007/s11263-021-01523-2	http://dx.doi.org/10.1007/s11263-021-01523-2		OCT 2021	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN		Green Submitted			2022-12-18	WOS:000707538700003
J	Sun, L; Dong, WS; Li, X; Wu, JJ; Li, LD; Shi, GM				Sun, Lu; Dong, Weisheng; Li, Xin; Wu, Jinjian; Li, Leida; Shi, Guangming			Deep Maximum a Posterior Estimator for Video Denoising	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video denoising; Model-guided construction; Deep convolutional neural network; Multiframe fusion	IMAGE; SPARSE; ENHANCEMENT; ALGORITHM	Unlike the maturity of image denoising research, video denoising has remained a challenging problem. A fundamental issue at the core of the video denoising (VD) problem is how to efficiently remove noise by exploiting temporal redundancy in video frames in a principled manner. Based on the maximum a posterior (MAP) estimation framework and recent advances in deep learning, we present a novel deep MAP-based video denoising method named MAP-VDNet with adaptive temporal fusion and deep image prior. The proposed MAP-based VD algorithm allows computationally efficient untangling of motion estimation (frame alignment) and image restoration (denoising). To address the misalignment issue, we also present a robust multi-frame fusion strategy for predicting spatially varying fusion weights by a neural network. To facilitate end-to-end optimization, we unfold the proposed iterative MAP-based VD algorithm into a deep convolutional network named MAP-VDNet. Extensive experimental results on three popular video datasets have shown that the proposed MAP-VDNet significantly outperforms current state-of-the-art VD techniques such as ViDeNN and FastDVDnet. The code is available at .	[Sun, Lu; Dong, Weisheng; Wu, Jinjian; Li, Leida; Shi, Guangming] Xidian Univ, Xian, Peoples R China; [Li, Xin] West Virginia Univ, Morgantown, WV 26506 USA	Xidian University; West Virginia University	Dong, WS (corresponding author), Xidian Univ, Xian, Peoples R China.	sunlu@stu.xidian.edu.cn; wsdong@mail.xidian.edu.cn; xin.li@ieee.org; jinjian.wu@mail.xidian.edu.cn; ldli@xidian.edu.cn; gmshi@xidian.edu.cn	Wu, Jinjian/GQH-0222-2022; Li, Li/AEM-3636-2022		National Key R&D Program of China [2018AAA0101400]; Natural Science Foundation of China [61991451, 61632019, 61621005, 61836008]; NSF [IIS-1951504, OAC-1940855]; DoJ/NIJ [NIJ 2018-75-CX-0032]; WV Higher Education Policy Commission Grant [HEPC.dsr.18.5]	National Key R&D Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); NSF(National Science Foundation (NSF)); DoJ/NIJ; WV Higher Education Policy Commission Grant	This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0101400, and the Natural Science Foundation of China under Grants 61991451, 61632019, 61621005, and 61836008. Xin Li's work is partially supported by the NSF under Grants IIS-1951504 and OAC-1940855, the DoJ/NIJ under Grant NIJ 2018-75-CX-0032, and the WV Higher Education Policy Commission Grant (HEPC.dsr.18.5).	Arias P, 2019, IEEE COMPUT SOC CONF, P1917, DOI 10.1109/CVPRW.2019.00243; Arias P, 2018, J MATH IMAGING VIS, V60, P70, DOI 10.1007/s10851-017-0742-4; Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bertocchi C, 2020, INVERSE PROBL, V36, DOI 10.1088/1361-6420/ab460a; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Buades A, 2016, IEEE T IMAGE PROCESS, V25, P2573, DOI 10.1109/TIP.2016.2551639; Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304; Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1631, DOI 10.1109/83.862646; Claus M, 2019, IEEE COMPUT SOC CONF, P1843, DOI 10.1109/CVPRW.2019.00235; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Davy A, 2019, IEEE IMAGE PROC, P2409, DOI 10.1109/ICIP.2019.8803314; Deng JN, 2020, AAAI CONF ARTIF INTE, V34, P10696; Dong WS, 2018, IEEE J-STSP, V12, P1435, DOI 10.1109/JSTSP.2018.2873047; Dong WS, 2019, IEEE T PATTERN ANAL, V41, P2305, DOI 10.1109/TPAMI.2018.2873610; Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847; Ehret T, 2019, PROC CVPR IEEE, P11361, DOI 10.1109/CVPR.2019.01163; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399; Godard C, 2018, LECT NOTES COMPUT SC, V11219, P560, DOI 10.1007/978-3-030-01267-0_33; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806; Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181; Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402; Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254; Hershey J. R., 2014, ARXIV PREPRINT ARXIV; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Isobe Takashi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8005, DOI 10.1109/CVPR42600.2020.00803; Jaderberg M, 2015, ADV NEUR IN, V28; Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849; Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340; Kingma D.P, P 3 INT C LEARNING R; Kokkinos F, 2019, PROC CVPR IEEE, P5922, DOI 10.1109/CVPR.2019.00608; Kroeger T, 2016, LECT NOTES COMPUT SC, V9908, P471, DOI 10.1007/978-3-319-46493-0_29; Liu C, 2010, LECT NOTES COMPUT SC, V6313, P706; Lu G, 2018, LECT NOTES COMPUT SC, V11218, P591, DOI 10.1007/978-3-030-01264-9_35; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324; Mahmoudi M, 2005, IEEE SIGNAL PROC LET, V12, P839, DOI 10.1109/LSP.2005.859509; Milan A., 2016, MOT16 BENCHMARK MULT; Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265; Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486; Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479; Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143; Tassano M, 2019, IEEE IMAGE PROC, P1805, DOI 10.1109/ICIP.2019.8803136; Tekalp A. M., 2015, DIGITAL VIDEO PROCES, V2nd; Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342; Varghese G, 2010, IEEE T CIRC SYST VID, V20, P1032, DOI 10.1109/TCSVT.2010.2051366; Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247; Wisdom S, 2017, INT CONF ACOUST SPEE, P4346, DOI 10.1109/ICASSP.2017.7952977; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xu XY, 2020, IEEE T IMAGE PROCESS, V29, P7153, DOI 10.1109/TIP.2020.2999209; Xu Xiangyu, 2019, ARXIV190406903; Xu Y, 2019, IEEE I CONF COMP VIS, P7042, DOI 10.1109/ICCV.2019.00714; Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2; Yang R, 2019, IEEE INT CON MULTI, P532, DOI 10.1109/ICME.2019.00098; Yang R, 2019, IEEE T CIRC SYST VID, V29, P2039, DOI 10.1109/TCSVT.2018.2867568; Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697; Yang R, 2017, IEEE INT CON MULTI, P817, DOI 10.1109/ICME.2017.8019299; Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320; Yue HJ, 2020, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR42600.2020.00237; Zhang HK, 2020, PROC CVPR IEEE, P3654, DOI 10.1109/CVPR42600.2020.00371; ZHANG K, 2017, PROC CVPR IEEE, P2808, DOI DOI 10.1109/CVPR.2017.300; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	72	2	2	4	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2827	2845		10.1007/s11263-021-01510-7	http://dx.doi.org/10.1007/s11263-021-01510-7		AUG 2021	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO					2022-12-18	WOS:000681171700001
J	Baslamisli, AS; Das, P; Le, HA; Karaoglu, S; Gevers, T				Baslamisli, Anil S.; Das, Partha; Le, Hoang-An; Karaoglu, Sezer; Gevers, Theo			ShadingNet: Image Intrinsics by Fine-Grained Shading Decomposition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Intrinsic image decomposition; Photometric effects; Shadow; Albedo; Reflectance	SEPARATE; RETINEX	In general, intrinsic image decomposition algorithms interpret shading as one unified component including all photometric effects. As shading transitions are generally smoother than reflectance (albedo) changes, these methods may fail in distinguishing strong photometric effects from reflectance variations. Therefore, in this paper, we propose to decompose the shading component into direct (illumination) and indirect shading (ambient light and shadows) subcomponents. The aim is to distinguish strong photometric effects from reflectance variations. An end-to-end deep convolutional neural network (ShadingNet) is proposed that operates in a fine-to-coarse manner with a specialized fusion and refinement unit exploiting the fine-grained shading model. It is designed to learn specific reflectance cues separated from specific photometric effects to analyze the disentanglement capability. A large-scale dataset of scene-level synthetic images of outdoor natural environments is provided with fine-grained intrinsic image ground-truths. Large scale experiments show that our approach using fine-grained shading decompositions outperforms state-of-the-art algorithms utilizing unified shading on NED, MPI Sintel, GTA V, IIW, MIT Intrinsic Images, 3DRMS and SRD datasets.	[Baslamisli, Anil S.; Das, Partha; Le, Hoang-An; Karaoglu, Sezer; Gevers, Theo] Univ Amsterdam, Sci Pk 904, NL-1098 XH Amsterdam, Netherlands; [Das, Partha; Karaoglu, Sezer; Gevers, Theo] 3D Universum, Sci Pk 400, NL-1098 XH Amsterdam, Netherlands	University of Amsterdam	Baslamisli, AS (corresponding author), Univ Amsterdam, Sci Pk 904, NL-1098 XH Amsterdam, Netherlands.	a.s.baslamisli@uva.nl		Baslamisli, Anil/0000-0002-4592-5379; Le, Hoang-An/0000-0002-7896-5967	EU Horizon 2020 program [688007]	EU Horizon 2020 program	This project was funded by the EU Horizon 2020 program No. 688007 (TrimBot2020). The authors would like to thank the anonymous reviewers for their valuable comment.	Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Barron JT, 2013, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2013.10; Barrow H. G., 1978, COMPUTER VISION SYST; Baslamisli AS, 2018, PROC CVPR IEEE, P6674, DOI 10.1109/CVPR.2018.00698; Baslamisli AS, 2021, COMPUT VIS IMAGE UND, V205, DOI 10.1016/j.cviu.2021.103183; Baslamisli AS, 2018, LECT NOTES COMPUT SC, V11210, P289, DOI 10.1007/978-3-030-01231-1_18; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Carroll R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964938; Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37; Cheng Z, 2019, IEEE I CONF COMP VIS, P2521, DOI 10.1109/ICCV.2019.00261; Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932; Gijsenij Arjan, 2008, CGIV 2008/MCS'08. 4th European Conference on Colour in Graphics, Imaging and Vision. 10th International Symposium on Multispectral Colour Science, P231; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Henderson P, 2020, INT J COMPUT VISION, V128, P835, DOI 10.1007/s11263-019-01219-8; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Innamorati C, 2017, COMPUT GRAPH FORUM, V36, P15, DOI 10.1111/cgf.13220; Isaza C, 2012, SENSORS-BASEL, V12, P13333, DOI 10.3390/s121013333; Janner M, 2017, ADV NEUR IN, V30; Jianbing Shen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3481, DOI 10.1109/CVPR.2011.5995507; Kingma D.P, P 3 INT C LEARNING R; Krahenbuhl P, 2018, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR.2018.00312; Laffont PY, 2013, IEEE T VIS COMPUT GR, V19, P210, DOI 10.1109/TVCG.2012.112; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; Le HA, 2021, IEEE WINT CONF APPL, P1578, DOI 10.1109/WACV48630.2021.00162; Lee KJ, 2012, LECT NOTES COMPUT SC, V7577, P327, DOI 10.1007/978-3-642-33783-3_24; Lettry L, 2018, COMPUT GRAPH FORUM, V37, P409, DOI 10.1111/cgf.13578; Lettry L, 2018, IEEE WINT CONF APPL, P1359, DOI 10.1109/WACV.2018.00153; Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI 10.1007/978-3-030-01240-3_21; Li ZQ, 2018, PROC CVPR IEEE, P9039, DOI 10.1109/CVPR.2018.00942; Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255; Liu YF, 2020, AAAI CONF ARTIF INTE, V34, P11661; Liu YF, 2020, PROC CVPR IEEE, P3245, DOI 10.1109/CVPR42600.2020.00331; Mao XJ, 2016, ADV NEUR IN, V29; Meka A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925907; Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342; Nestmeyer T, 2017, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2017.192; Qilong Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11531, DOI 10.1109/CVPR42600.2020.01155; Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248; Rother C., 2011, ADV NEURAL INFORM PR, P765; Sattler T., 2017, IEEE INT C COMP VIS, P1; Sengupta S, 2019, IEEE I CONF COMP VIS, P8597, DOI 10.1109/ICCV.2019.00869; SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409; Shen L, 2008, PROC CVPR IEEE, P2479; Shen L, 2011, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2011.5995738; Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619; WADA T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P66, DOI 10.1109/ICCV.1995.466805; Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606; Xu C, 2019, TEXT RES J, V89, P3617, DOI 10.1177/0040517518817051; Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060; Yu Y, 2019, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2019.00327; Yuan Y, 2019, LECT NOTES COMPUT SC, V11542, P336, DOI 10.1007/978-3-030-22514-8_28; Zeiler Matthew D, 2012, ARXIV12125701; Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77; Zhou H, 2019, IEEE I CONF COMP VIS, P7819, DOI 10.1109/ICCV.2019.00791	57	2	2	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2021	129	8					2445	2473		10.1007/s11263-021-01477-5	http://dx.doi.org/10.1007/s11263-021-01477-5		MAY 2021	29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TR3NV		Green Published, hybrid, Green Submitted			2022-12-18	WOS:000654859700001
J	Yan, JB; Zhong, Y; Fang, YM; Wang, ZY; Ma, KD				Yan, Jiebin; Zhong, Yu; Fang, Yuming; Wang, Zhangyang; Ma, Kede			Exposing Semantic Segmentation Failures via Maximum Discrepancy Competition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic segmentation; Performance evaluation; Generalization; Maximum discrepancy competition	IMAGE QUALITY ASSESSMENT; SCALE	Semantic segmentation is an extensively studied task in computer vision, with numerous methods proposed every year. Thanks to the advent of deep learning in semantic segmentation, the performance on existing benchmarks is close to saturation. A natural question then arises: Does the superior performance on the closed (and frequently re-used) test sets transfer to the open visual world with unconstrained variations? In this paper, we take steps toward answering the question by exposing failures of existing semantic segmentation methods in the open visual world under the constraint of very limited human labeling effort. Inspired by previous research on model falsification, we start from an arbitrarily large image set, and automatically sample a small image set by maximizing the discrepancy (MAD) between two segmentation methods. The selected images have the greatest potential in falsifying either (or both) of the two methods. We also explicitly enforce several conditions to diversify the exposed failures, corresponding to different underlying root causes. A segmentation method, whose failures are more difficult to be exposed in the MAD competition, is considered better. We conduct a thorough MAD diagnosis of ten PASCAL VOC semantic segmentation algorithms. With detailed analysis of experimental results, we point out strengths and weaknesses of the competing algorithms, as well as potential research directions for further advancement in semantic segmentation. The codes are publicly available at .	[Yan, Jiebin; Zhong, Yu; Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang, Jiangxi, Peoples R China; [Wang, Zhangyang] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA; [Ma, Kede] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China	Jiangxi University of Finance & Economics; University of Texas System; University of Texas Austin; City University of Hong Kong	Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang, Jiangxi, Peoples R China.	jiebinyan@foxmail.com; zhystu@qq.com; fa0001ng@e.ntu.edu.sg; atlaswang@utexas.edu; kede.ma@cityu.edu.hk		Ma, Kede/0000-0001-8608-1128	National Key R&D Program of China [2018AAA0100601]; National Natural Science Foundation of China [62071407, 61822109]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was partially supported by the National Key R&D Program of China under Grant 2018AAA0100601, and the National Natural Science Foundation of China under Grants 62071407 and 61822109.	[Anonymous], 2014, ABS14123474 CORR; [Anonymous], 2004, P WORKSH STAT LEARN; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Arnab A, 2018, PROC CVPR IEEE, P888, DOI 10.1109/CVPR.2018.00099; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Brendel W., 2019, APPROXIMATING CNNS B, DOI 1904.00760.; BURT PJ, 1981, COMPUT VISION GRAPH, V16, P20, DOI 10.1016/0146-664X(81)90092-7; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chen L.C., 2018, ARXIV170605587; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen Liang-Chieh, 2018, P EUROPEAN C COMPUTE, P801; Chen W., 2020, P ICML NOV, P1746; Chen WY, 2019, PROC CVPR IEEE, P8916, DOI 10.1109/CVPR.2019.00913; Chen Wuyang, 2020, ICLR; Cheng B., 2020, P IEEECVF C COMPUTER, P12475; Choi S., 2020, P IEEECVF C COMPUTER, P9373; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Csurka Gabriela, 2008, BMVC, P1; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dietterich T., 2019, ICLR; Dvornik N, 2017, IEEE I CONF COMP VIS, P4174, DOI 10.1109/ICCV.2017.447; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fu J., 2019, IEEE T IMAGE PROCESS; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Geirhos R., 2018, IMAGENET TRAINED CNN, DOI DOI 10.48550/ARXIV.1811.12231; Golan T., 2019, ARXIV PREPRINT ARXIV; Goodfellow I. J., 2014, ARXIV14126572; Grenander U., 2012, LECT PATTERN THEORY, V33; Grenander U., 2012, LECT PATTERN THEORY, V18; Grenander U., 2012, LECT PATTERN THEORY, V24; Guo DZ, 2020, IEEE T IMAGE PROCESS, V29, P782, DOI 10.1109/TIP.2019.2936111; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hendrycks Dan, 2019, ARXIV190707174; Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069; Jacobsen J.H., 2018, ARXIV181100401; Jia XQ, 2019, LECT NOTES COMPUT SC, V11769, P567, DOI 10.1007/978-3-030-32226-7_63; JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0; Julesz B., 1962, IRE T INFORM THEOR, V8, P84, DOI 10.1109/TIT.1962.1057698; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li H., 2018, COMPUT VIS PATTERN R; Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma Kede, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P851, DOI 10.1109/TPAMI.2018.2889948; McKeeman W. M., 1998, DIGIT TECH J, V10, P100; Mehta, 2019, ARXIV190603516; Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941; Meng H, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, VOLS I-V, CONFERENCE PROCEEDINGS, P88, DOI 10.1109/ICMA.2007.4303521; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Mohseni S., 2019, ARXIV191209630; MUMFORD D, 1994, PROG MATH, V119, P187; Nekrasov V., 2018, 2018 BRIT MACH VIS C; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Pei KX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P1, DOI 10.1145/3132747.3132785; Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Recht B, 2019, PR MACH LEARN RES, V97; Ren S., 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.169; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Russell B. C., 2006, P IEEE C COMP VIS PA, V2, P1605; Santurkar S., 2020, BREEDS BENCHMARKS SU; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959; Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Tsukida K., 2011, ANAL PAIRED COMP DAT; Verbeek J., 2008, ADV NEURAL INFORM PR, P1553; Wandell B., 1997, PSYCCRITIQUES, V7, P42; Wang H., 2020, ARXIV200210648; Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang Z., 2006, MODERN IMAGE QUALITY; Wang Z, 2008, J VISION, V8, DOI 10.1167/8.12.8; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yang Y, 2012, IEEE T PATTERN ANAL, V34, P1731, DOI 10.1109/TPAMI.2011.208; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75; Zendel O., 2018, ECCV 2018, P407; Zhang L., 2019, ARXIV190906121; Zhang QS, 2019, PROC CVPR IEEE, P6254, DOI 10.1109/CVPR.2019.00642; Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17; Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	92	2	4	4	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1768	1786		10.1007/s11263-021-01450-2	http://dx.doi.org/10.1007/s11263-021-01450-2		MAR 2021	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000627706900001
J	Sun, P; Wu, JX; Li, SY; Lin, PW; Huang, JZ; Li, X				Sun, Peng; Wu, Jiaxiang; Li, Songyuan; Lin, Peiwen; Huang, Junzhou; Li, Xi			Real-Time Semantic Segmentation via Auto Depth, Downsampling Joint Decision and Feature Aggregation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Real-time semantic segmentation; Neural architecture search; Computer vision; Deep learning		To satisfy the stringent requirements for computational resources in the field of real-time semantic segmentation, most approaches focus on the hand-crafted design of light-weight segmentation networks. To enjoy the ability of model auto-design, Neural Architecture Search (NAS) has been introduced to search for the optimal building blocks of networks automatically. However, the network depth, downsampling strategy, and feature aggregation method are still set in advance and nonadjustable during searching. Moreover, these key properties are highly correlated and essential for a remarkable real-time segmentation model. In this paper, we propose a joint search framework, called AutoRTNet, to automate all the aforementioned key properties in semantic segmentation. Specifically, we propose hyper-cells to jointly decide the network depth and the downsampling strategy via a novel cell-level pruning process. Furthermore, we propose an aggregation cell to achieve automatic multi-scale feature aggregation. Extensive experimental results on Cityscapes and CamVid datasets demonstrate that the proposed AutoRTNet achieves the new state-of-the-art trade-off between accuracy and speed. Notably, our AutoRTNet achieves 73.9% mIoU on Cityscapes and 110.0 FPS on an NVIDIA TitanXP GPU card with input images at a resolution of 768x1536.	[Sun, Peng; Li, Songyuan; Li, Xi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China; [Wu, Jiaxiang] Tencent AI Lab, Shenzhen, Peoples R China; [Lin, Peiwen] Sensetime Res, Beijing, Peoples R China; [Huang, Junzhou] Univ Texas Arlington, Arlington, TX 76019 USA; [Li, Xi] Zhejiang Univ, Shanghai Inst Adv Study, Shanghai, Peoples R China	Zhejiang University; Tencent; University of Texas System; University of Texas Arlington; Zhejiang University	Li, X (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.; Li, X (corresponding author), Zhejiang Univ, Shanghai Inst Adv Study, Shanghai, Peoples R China.	sunpeng1996@zju.edu.cn; jonathanwu@tencent.com; leizungjyun@zju.edu.cn; linpeiwen@sensetime.com; jzhuang@uta.edu; xilizju@zju.edu.cn			National Natural Science Foundation of China [U20A20222]; Zhejiang Provincial Natural Science Foundation of China [LR19F020004]; National Key Research and Development Program of China [2020AAA0107400]; key scientific technological innovation research project by Ministry of Education	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Zhejiang Provincial Natural Science Foundation of China(Natural Science Foundation of Zhejiang Province); National Key Research and Development Program of China; key scientific technological innovation research project by Ministry of Education	This work is supported in part by National Natural Science Foundation of China under Grant U20A20222, Zhejiang Provincial Natural Science Foundation of China under Grant LR19F020004, National Key Research and Development Program of China under Grant 2020AAA0107400, and key scientific technological innovation research project by Ministry of Education.	Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5; Cai H., 2018, ARXIV PREPRINT ARXIV; Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723; Chen Liang-Chieh, 2018, P EUROPEAN C COMPUTE, P801; Chollet F., 2017, PROC CVPR IEEE, P1251, DOI DOI 10.1109/CVPR.2017.195; Cohen R, 2016, ARXIV PREPRINT ARXIV; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Howard A.G., 2017, MOBILENETS EFFICIENT; Jang E., 2016, ARXIV; Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975; Li Hanchao, 2018, ARXIV180510180; Li I., 2019, PROC BRIT MACHVIS C, P1; Li Liam, 2019, ABS190207638 CORR; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo WJ, 2016, ADV NEUR IN, V29; Maddison Chris J, 2016, ARXIV161100712; Mehta Sachin, 2018, P EUR C COMP VIS ECC, P552, DOI [DOI 10.1007/978-3-030-01249-6_34, 10.1007/978-3-030-01249-6_34]; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Orsic M, 2019, PROC CVPR IEEE, P12599, DOI 10.1109/CVPR.2019.01289; Paszke A., 2016, ARXIV PREPRINT ARXIV; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Treml Michael, 2016, MLITS NIPS WORKSH; Vaswani A, 2017, ADV NEUR IN, V30; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu B, 2018, ARXIV PREPRINT ARXIV; Wu Z., 2017, ARXIV171200213; Xie Sirui, 2019, ICLR, V1, P13; Xu H, 2019, IEEE I CONF COMP VIS, P6648, DOI 10.1109/ICCV.2019.00675; Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20; Yu Kaicheng, 2019, ARXIV190208142; Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064; Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17; Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907; Zoph Barret, 2016, ARXIV PREPRINT ARXIV, P30, DOI DOI 10.18653/V1/N16-1004	57	2	2	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1506	1525		10.1007/s11263-021-01433-3	http://dx.doi.org/10.1007/s11263-021-01433-3		FEB 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000619690600001
J	Nie, ZW; Li, C; Liu, HR; Yang, XP				Nie, Ziwei; Li, Chen; Liu, Hairong; Yang, Xiaoping			Deformable Image Registration Based on Functions of Bounded Generalized Deformation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Functions of bounded generalized deformation; Deformable image registration; Functions of bounded deformation; Primal&#8211; dual algorithm	REGULARIZATION; MOTION	Functions of bounded deformation (BD) are widely used in the theory of elastoplasticity to describe the possibly discontinuous displacement fields inside elastoplastic bodies. BD functions have been proved suitable for deformable image registration, the goal of which is to find the displacement field between a moving image and a fixed image. Recently BD functions have been generalized to symmetric tensor fields of bounded generalized variation. In this paper, we focus on the first-order symmetric tensor fields, i.e., vector-valued functions, of bounded generalized variation. We specify these functions as functions of bounded generalized deformation (BGD) since BGD functions are natural generalizations of BD functions. We propose a BGD model for deformable image registration problems by regarding concerned displacement fields as BGD functions. BGD model employs not only the first-order but also higher-order coupling information of components of the displacement field. It turns out that BGD model allows for jump discontinuities of displacements while, in contrast to BD model, at the same time is able to employ higher-order derivatives of displacements in smooth regions. As a result, BGD model tends to capture possible discontinuities of displacements appeared around edges of the target objects while keep the smoothness of displacements inside the target objects as well. This characteristic enables BGD model to obtain better registration results than BD model and other variational models. To our knowledge, it is the first time in literature to use BGD functions for image registration. A first-order adaptive primal-dual algorithm is adopted to solve the proposed BGD model. Numerical experiments on 2D and 3D images show both effectiveness and advantages of BGD model.	[Nie, Ziwei; Li, Chen; Yang, Xiaoping] Nanjing Univ, Dept Math, Nanjing 210093, Peoples R China; [Liu, Hairong] Nanjing Forestry Univ, Sch Sci, Nanjing 210037, Peoples R China	Nanjing University; Nanjing Forestry University	Yang, XP (corresponding author), Nanjing Univ, Dept Math, Nanjing 210093, Peoples R China.	nieziwei@nju.edu.cn; dg1821007@smail.nju.edu.cn; hrliu@njfu.edu.cn; xpyang@nju.edu.cn			National Natural Science Foundation of China [11971229, 12090023]; China's Ministry of Science and Technology [SQ2020YFA070208]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China's Ministry of Science and Technology(Ministry of Science and Technology, China)	This work is supported by National Natural Science Foundation of China (Grant Nos. 11971229, 12090023) and China's Ministry of Science and Technology (Grant No. SQ2020YFA070208).	Aganj I, 2013, IEEE T IMAGE PROCESS, V22, P816, DOI 10.1109/TIP.2012.2224356; Alahyane M, 2018, INVERSE PROBL IMAG, V12, P1055, DOI 10.3934/ipi.2018044; Alam F, 2018, BIOCYBERN BIOMED ENG, V38, P71, DOI 10.1016/j.bbe.2017.10.001; Ambrosio L, 1997, ARCH RATION MECH AN, V139, P201, DOI 10.1007/s002050050051; Balle F, 2019, INVERSE PROBL SCI EN, V27, P540, DOI 10.1080/17415977.2018.1475479; BARROSO A. C., 2000, ANN SCUOLA NORM SUP, V29, P19; Bonettini S, 2012, J MATH IMAGING VIS, V44, P236, DOI 10.1007/s10851-011-0324-9; Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178; Bredies K, 2014, J INVERSE ILL-POSE P, V22, P871, DOI 10.1515/jip-2013-0068; Bredies K, 2013, ANN MAT PUR APPL, V192, P815, DOI 10.1007/s10231-011-0248-4; Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521; Burger M, 2013, SIAM J SCI COMPUT, V35, pB132, DOI 10.1137/110835955; Castillo R, 2013, PHYS MED BIOL, V58, P2861, DOI 10.1088/0031-9155/58/9/2861; Castillo R, 2009, PHYS MED BIOL, V54, P1849, DOI 10.1088/0031-9155/54/7/001; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chumchob N, 2011, MULTISCALE MODEL SIM, V9, P89, DOI 10.1137/100788239; Chumchob N, 2013, IEEE T IMAGE PROCESS, V22, P4551, DOI 10.1109/TIP.2013.2274749; Dal Maso G, 2013, J EUR MATH SOC, V15, P1943, DOI 10.4171/JEMS/410; Du KF, 2012, MED PHYS, V39, P1595, DOI 10.1118/1.3685589; Evans L. C., 1998, GRADUATE STUDIES MAT, V19; Gao YM, 2018, MULTIDIM SYST SIGN P, V29, P1459, DOI 10.1007/s11045-017-0512-x; Goldstein T., 2015, P ADV NEUR INF PROC, V28, P2089; Hajinezhad D., 2016, ADV NEURAL INFORM PR, P3215; Haker S, 2004, INT J COMPUT VISION, V60, P225, DOI 10.1023/B:VISI.0000036836.66311.97; Heinrich MP, 2015, LECT NOTES COMPUT SC, V9350, P338, DOI 10.1007/978-3-319-24571-3_41; Hermann S, 2014, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2014.393; Hermann S, 2014, LECT NOTES COMPUT SC, V8333, P23, DOI 10.1007/978-3-642-53842-1_3; Homke L., 2007, IMAGE PROCESSING BAS, P343; Hossny M, 2008, ELECTRON LETT, V44, P1066, DOI 10.1049/el:20081754; Konig L, 2014, I S BIOMED IMAGING, P580, DOI 10.1109/ISBI.2014.6867937; Lax P. D., 2002, FUNCTIONAL ANAL; Lin FH., 2002, GEOMETRIC MEASURE TH; Lombaert H, 2014, INT J COMPUT VISION, V107, P254, DOI 10.1007/s11263-013-0681-5; Mainardi L, 2008, J DIGIT IMAGING, V21, P27, DOI 10.1007/s10278-007-9021-z; McClelland JR, 2013, MED IMAGE ANAL, V17, P19, DOI 10.1016/j.media.2012.09.005; Modersitzki J, 2009, FUND ALGORITHMS, P1, DOI 10.1137/1.9780898718843; Nie ZW, 2019, IEEE T MED IMAGING, V38, P1488, DOI 10.1109/TMI.2019.2896170; Polzin Thomas, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9902, P28, DOI 10.1007/978-3-319-46726-9_4; Polzin T., 2013, 5 INT WORKSH PULM IM, pSPRI; Ranftl R, 2014, LECT NOTES COMPUT SC, V8689, P439, DOI 10.1007/978-3-319-10590-1_29; Ruhaak J, 2017, IEEE T MED IMAGING, V36, P1746, DOI 10.1109/TMI.2017.2691259; Sotiras A, 2013, IEEE T MED IMAGING, V32, P1153, DOI 10.1109/TMI.2013.2265603; Suetens P., 2009, FUNDAMENTALS MED IMA, DOI [10.1017/cbo9780511596803, DOI 10.1017/CBO9780511596803]; Temam R., 1983, PROBLMES MATH PLASTI; Thevenaz P., 2000, IMAGE INTERPOLATION, P393; Thirion J P, 1998, Med Image Anal, V2, P243, DOI 10.1016/S1361-8415(98)80022-4; Vishnevskiy V, 2017, IEEE T MED IMAGING, V36, P385, DOI 10.1109/TMI.2016.2610583; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Washizu K., 1975, VARIATIONAL METHODS; Yoo JC, 2009, CIRC SYST SIGNAL PR, V28, P819, DOI [10.1007/s00034-009-9130-7, 10.1007/S00034-009-9130-7]; Zhang J, 2018, CHINA PERSPECTIVE, P1, DOI 10.1080/10255842.2018.1484914	51	2	2	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1341	1358		10.1007/s11263-021-01439-x	http://dx.doi.org/10.1007/s11263-021-01439-x		FEB 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC					2022-12-18	WOS:000614668800001
J	Gao, X; Zhu, LJ; Xie, ZX; Liu, HM; Shen, SH				Gao, Xiang; Zhu, Lingjie; Xie, Zexiao; Liu, Hongmin; Shen, Shuhan			Incremental Rotation Averaging	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Rotation averaging; Incremental estimation; Accuracy and robustness		In this paper, we present a simple yet effective rotation averaging pipeline, termed Incremental Rotation Averaging (IRA), which is inspired by the well-developed incremental Structure from Motion (SfM) techniques. Unlike the traditional rotation averaging methods which estimate all the absolute rotations simultaneously and focus on designing either robust loss function or outlier filtering strategy, here the absolute rotations are estimated in an incremental way. Similar to the incremental SfM, our IRA is robust to relative rotation outliers and could achieve accurate rotation averaging results. In addition, we propose several key techniques, such as initial triplet and Next-Best-View selection, Weighted Local/Global Optimization, and Re-Rotation Averaging, to push the rotation averaging results one step further. Ablation studies and comparison experiments on the 1DSfM, Campus, and San Francisco datasets demonstrate the effectiveness of our IRA and its advantages over the state-of-the-art rotation averaging methods in accuracy and robustness.	[Gao, Xiang; Xie, Zexiao] Ocean Univ China, Coll Engn, Qingdao 266100, Peoples R China; [Zhu, Lingjie; Shen, Shuhan] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Zhu, Lingjie; Shen, Shuhan] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China; [Liu, Hongmin] Univ Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 100083, Peoples R China	Ocean University of China; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; University of Science & Technology Beijing	Shen, SH (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Shen, SH (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; Liu, HM (corresponding author), Univ Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 100083, Peoples R China.	hmliu_82@163.com; shshen@nlpr.ia.ac.cn		shen, shu han/0000-0002-8704-7914	National Key Research and Development Program of China [2020YFB1313002]; National Science Foundation of China [62003319, 62076026, 61873265]; Shandong Provincial Natural Science Foundation [ZR2020QF075]; China Postdoctoral Science Foundation [2020M682239]; National Laboratory of Pattern Recognition [202000010]	National Key Research and Development Program of China; National Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shandong Provincial Natural Science Foundation(Natural Science Foundation of Shandong Province); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); National Laboratory of Pattern Recognition	This work was supported by the National Key Research and Development Program of China (2020YFB1313002), the National Science Foundation of China (62003319, 62076026, and 61873265), the Shandong Provincial Natural Science Foundation (ZR2020QF075), the China Postdoctoral Science Foundation (2020M682239), and the Open Projects Program of National Laboratory of Pattern Recognition (202000010). We thank Dr. Zhaopeng Cui for sharing the Campus dataset.	Agarwal S, 2010, LECT NOTES COMPUT SC, V6312, P29, DOI 10.1007/978-3-642-15552-9_3; Bustos AP, 2019, IEEE INT CONF ROBOT, P2385, DOI 10.1109/ICRA.2019.8793749; Chatterjee A, 2018, IEEE T PATTERN ANAL, V40, P958, DOI 10.1109/TPAMI.2017.2693984; Chatterjee A, 2013, IEEE I CONF COMP VIS, P521, DOI 10.1109/ICCV.2013.70; Chng Chee-Kheng, 2020, DIGITAL IMAGE COMPUT; Crandall DJ, 2013, IEEE T PATTERN ANAL, V35, P2841, DOI 10.1109/TPAMI.2012.218; Cui HN, 2019, ISPRS J PHOTOGRAMM, V156, P202, DOI 10.1016/j.isprsjprs.2019.08.005; Cui HN, 2017, IEEE IMAGE PROC, P4517; Cui HN, 2017, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2017.257; Cui ZP, 2015, IEEE I CONF COMP VIS, P864, DOI 10.1109/ICCV.2015.105; Eriksson A, 2018, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2018.00021; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Govindu VM, 2006, LECT NOTES COMPUT SC, V3852, P457; Haner S, 2012, LECT NOTES COMPUT SC, V7573, P545, DOI 10.1007/978-3-642-33709-3_39; Hartley R., 2011, P IEEE C COMP VIS PA, P3041, DOI DOI 10.1109/CVPR.2011.5995745; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Hu Z., 2020, IEEE T CYBERNETICS; Jiang NJ, 2013, IEEE I CONF COMP VIS, P481, DOI 10.1109/ICCV.2013.66; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Moulon P, 2013, IEEE I CONF COMP VIS, P3248, DOI 10.1109/ICCV.2013.403; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Ozyosil O, 2015, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2015.7298883; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Shen TW, 2016, LECT NOTES COMPUT SC, V9907, P139, DOI 10.1007/978-3-319-46487-9_9; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Sweeney C, 2015, IEEE I CONF COMP VIS, P801, DOI 10.1109/ICCV.2015.98; Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5; Wilson K, 2016, LECT NOTES COMPUT SC, V9911, P255, DOI 10.1007/978-3-319-46478-7_16; Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25; Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801; Zhu SY, 2018, PROC CVPR IEEE, P4568, DOI 10.1109/CVPR.2018.00480	31	2	2	1	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1202	1216		10.1007/s11263-020-01427-7	http://dx.doi.org/10.1007/s11263-020-01427-7		JAN 2021	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000608098500001
J	Synakowski, S; Feng, QL; Martinez, A				Synakowski, Stuart; Feng, Qianli; Martinez, Aleix			Adding Knowledge to Unsupervised Algorithms for the Recognition of Intent	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised; Action recognition; Theory of mind; Intent; Commonsense		Computer vision algorithms performance are near or superior to humans in the visual problems including object recognition (especially those of fine-grained categories), segmentation, and 3D object reconstruction from 2D views. Humans are, however, capable of higher-level image analyses. A clear example, involving theory of mind, is our ability to determine whether a perceived behavior or action was performed intentionally or not. In this paper, we derive an algorithm that can infer whether the behavior of an agent in a scene is intentional or unintentional based on its 3D kinematics, using the knowledge of self-propelled motion, Newtonian motion and their relationship. We show how the addition of this basic knowledge leads to a simple, unsupervised algorithm. To test the derived algorithm, we constructed three dedicated datasets from abstract geometric animation to realistic videos of agents performing intentional and non-intentional actions. Experiments on these datasets show that our algorithm can recognize whether an action is intentional or not, even without training data. The performance is comparable to various supervised baselines quantitatively, with sensible intentionality segmentation qualitatively.	[Synakowski, Stuart; Feng, Qianli; Martinez, Aleix] Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Feng, QL (corresponding author), Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA.	synakowski.1@osu.edu; feng.559@osu.edu; martinez.158@osu.edu		Feng, Qianli/0000-0002-7550-2019	National Institutes of Health (NIH) [R01-DC-014498, R01-EY-020834]; Human Frontier Science Program (HFSP) [RGP0036/2016]; Ohio State's Center for Cognitive and Brain Sciences	National Institutes of Health (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Human Frontier Science Program (HFSP)(Human Frontier Science Program); Ohio State's Center for Cognitive and Brain Sciences	This research was supported by the National Institutes of Health (NIH), Grants R01-DC-014498 and R01-EY-020834, the Human Frontier Science Program (HFSP), Grant RGP0036/2016, and a grant from Ohio State's Center for Cognitive and Brain Sciences.	Aditya S., 2015, 2015 AAAI SPRING S S; Aristotle F, 1926, ART RHETORIC; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Chambon V, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-01414-y; Chambon V, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017133; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; del Rincon JM, 2013, PATTERN RECOGN LETT, V34, P1849, DOI 10.1016/j.patrec.2012.10.020; Descartes R, 1960, MEDITATIONS 1 PHILOS; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Epstein D, 2020, PROC CVPR IEEE, P916, DOI 10.1109/CVPR42600.2020.00100; Hastie T., 2009, ELEMENTS STAT LEARNI, P37; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heider F, 1944, AM J PSYCHOL, V57, P243, DOI 10.2307/1416950; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Lopez, 2019, IEEE T INTELLIGENT T; Luo Y, 2005, PSYCHOL SCI, V16, P601, DOI 10.1111/j.1467-9280.2005.01582.x; Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Miller George A, 1998, WORDNET ELECT LEXICA; Ravichandar HC, 2017, IEEE T AUTOM SCI ENG, V14, P855, DOI 10.1109/TASE.2016.2624279; Rudenko A, 2020, INT J ROBOT RES, V39, P895, DOI 10.1177/0278364920917446; Sartori L, 2011, COGNITION, V119, P242, DOI 10.1016/j.cognition.2011.01.014; Shpall S., 2016, STANFORD ENCY PHILOS, V2016; Speer R, 2017, AAAI CONF ARTIF INTE, P4444; Tozeren A., 2000, HUMAN BODY DYNAMICS; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Ullman T., 2009, ADV NEURAL INFORM PR, V22, P1874; Varytimidis D, 2018, 2018 14TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS), P676, DOI 10.1109/SITIS.2018.00109; Vondrick C, 2016, PROC CVPR IEEE, P2997, DOI 10.1109/CVPR.2016.327; Wei P, 2018, PROC CVPR IEEE, P6801, DOI 10.1109/CVPR.2018.00711; Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y; You D, 2011, IEEE T PATTERN ANAL, V33, P631, DOI 10.1109/TPAMI.2010.173; Zellers Rowan, 2018, ARXIV181110830	33	2	2	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					942	959		10.1007/s11263-020-01404-0	http://dx.doi.org/10.1007/s11263-020-01404-0		JAN 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK	34211258	Green Submitted, Green Accepted			2022-12-18	WOS:000605122800004
J	Shen, ZQ; Huang, MY; Shi, JP; Liu, ZC; Maheshwari, H; Zheng, YT; Xue, XY; Savvides, M; Huang, TMS				Shen, Zhiqiang; Huang, Mingyang; Shi, Jianping; Liu, Zechun; Maheshwari, Harsh; Zheng, Yutong; Xue, Xiangyang; Savvides, Marios; Huang, Thomas S.			CDTD: A Large-Scale Cross-Domain Benchmark for Instance-Level Image-to-Image Translation and Domain Adaptive Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cross-domain benchmark; Instance level image-to-image translation; Domain adaptive object detection		Cross-domain visual problems, such as image-to-image translation and domain adaptive object detection, have attracted increasing attentions in the last few years, and also become new rising and challenging directions for the computer vision community. Recently, despite enormous efforts of the field in data collection, there are still few datasets covering the instance-level image-to-image translation and domain adaptive object detection tasks simultaneously. In this work, we introduce a large-scale cross-domain benchmark CDTD (contains 155,529 high-resolution natural images across four different modalities with object bounding box annotations. A summary of the entire dataset is provided in the following sections. Dataset is available at:http://zhiqiangshen.com/projects/INIT/index.html.) .) for the new instance-level translation and object detection tasks. We provide comprehensive baseline results of the benchmark on both of these two tasks. Moreover, we proposed a novel instance-level image-to-image translation approach called INIT and a gradient detach method for the domain adaptive object detection to harvest and exert dataset's function of the instance level annotations across different domains.	[Shen, Zhiqiang; Liu, Zechun; Maheshwari, Harsh; Zheng, Yutong; Savvides, Marios] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Huang, Mingyang; Shi, Jianping] SenseTime Res, Beijing, Peoples R China; [Xue, Xiangyang] Fudan Univ, Shanghai, Peoples R China; [Huang, Thomas S.] Univ Illinois, Champaign, IL USA	Carnegie Mellon University; Fudan University; University of Illinois System; University of Illinois Urbana-Champaign	Shen, ZQ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	zhiqians@andrew.cmu.edu; huangmingyang@sensetime.com; shijianping@sensetime.com; zechunl@andrew.cmu.edu; yutongzh@andrew.cmu.edu; xyxue@fudan.edu.cn; marioss@andrew.cmu.edu; t-huang1@illinois.edu	Zheng, Yutong/AAD-5035-2022	Shen, Zhiqiang/0000-0002-4560-5092				Almahairi A, 2018, PR MACH LEARN RES, V80; [Anonymous], 2015, ICLR WORKSH; Bai YC, 2018, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2018.00010; Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88; Cai Q, 2019, PROC CVPR IEEE, P11449, DOI 10.1109/CVPR.2019.01172; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10; Ganin Yaroslav, 2015, ICML; Geiger A., 2012, P IEEE COMP SOC C CO; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; HANB, 2018, PROC CVPR IEEE, V31; He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He ZW, 2019, IEEE I CONF COMP VIS, P6667, DOI 10.1109/ICCV.2019.00677; Hoffman J, 2018, PR MACH LEARN RES, V80; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Huang Xun, 2018, ECCV; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson-Roberson Matthew, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P746, DOI 10.1109/ICRA.2017.7989092; Karacan L., 2016, ARXIV161200215; Kim T, 2019, PROC CVPR IEEE, P12448, DOI 10.1109/CVPR.2019.01274; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu MY, 2017, ADV NEUR IN, V30; Long MS, 2016, ADV NEUR IN, V29; Ma S, 2018, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2018.00593; Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47; Mirza M., 2014, ARXIV; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149; Peng Xingchao, 2017, VISDA VISUAL DOMAIN; Peng Xingchao, 2018, ARXIV180609755; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Raj, 2015, ARXIV150705578; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Salimans T, 2016, ADV NEUR IN, V29; Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723; Shen ZQ, 2019, AAAI CONF ARTIF INTE, P4886, DOI 10.1609/aaai.v33i01.33014886; Shen ZQ, 2019, PROC CVPR IEEE, P3678, DOI 10.1109/CVPR.2019.00380; Shen ZQ, 2020, IEEE T PATTERN ANAL, V42, P398, DOI 10.1109/TPAMI.2019.2922181; Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng Eric, 2018, ARXIV181200929; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483; Wang XG, 2019, PROC CVPR IEEE, P8868, DOI [10.1109/CVPR.2019.00908, 10.1109/CVPR.2019.00267]; Wei DL, 2018, PROC CVPR IEEE, P8052, DOI 10.1109/CVPR.2018.00840; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu YF, 2019, PR MACH LEARN RES, V97; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang ZZ, 2018, PROC CVPR IEEE, P9242, DOI 10.1109/CVPR.2018.00963; Zhao H, 2019, PR MACH LEARN RES, V97; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu R, 2019, PROC CVPR IEEE, P2263, DOI 10.1109/CVPR.2019.00237	77	2	2	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					761	780		10.1007/s11263-020-01394-z	http://dx.doi.org/10.1007/s11263-020-01394-z		NOV 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC					2022-12-18	WOS:000592014000002
J	Song, LY; Liu, J; Sun, MX; Shang, XQ				Song, Lingyun; Liu, Jun; Sun, Mingxuan; Shang, Xuequn			Weakly Supervised Group Mask Network for Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Weakly supervised; Multiple instance learning	LOCALIZATION; CONTEXT	Learning object detectors from weak image annotations is an important yet challenging problem. Many weakly supervised approaches formulate the task as a multiple instance learning problem, where each image is represented as a bag of instances. For predicting the score for each object that occurs in an image, existing MIL based approaches tend to select the instance that responds more strongly to a specific class, which, however, overlooks the contextual information. Besides, objects often exhibit dramatic variations such as scaling and transformations, which makes them hard to detect. In this paper, we propose the weakly supervised group mask network (WSGMN), which mainly has two distinctive properties: (i) it exploits the relations among regions to generate community instances, which contain context information and are robust to object variations. (ii) It generates a mask for each label group, and utilizes these masks to dynamically select the feature information of the most useful community instances for recognizing specific objects. Extensive experiments on several benchmark datasets demonstrate the effectiveness of WSGMN on the tasks of weakly supervised object detection.	[Song, Lingyun; Shang, Xuequn] Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China; [Song, Lingyun; Shang, Xuequn] Northwestern Polytech Univ, Key Lab Big Data Storage & Management, Minist Ind & Informat Technol, Xian 710129, Peoples R China; [Song, Lingyun; Liu, Jun] Xi An Jiao Tong Univ, Dept Comp Sci & Technol, SPKLSTN Lab, Xian 710049, Peoples R China; [Sun, Mingxuan] Louisiana State Univ, Sch Elect Engn & Comp Sci, Div Comp Sci & Engn, Baton Rouge, LA 70803 USA	Northwestern Polytechnical University; Northwestern Polytechnical University; Xi'an Jiaotong University; Louisiana State University System; Louisiana State University	Song, LY (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.; Song, LY (corresponding author), Northwestern Polytech Univ, Key Lab Big Data Storage & Management, Minist Ind & Informat Technol, Xian 710129, Peoples R China.; Song, LY (corresponding author), Xi An Jiao Tong Univ, Dept Comp Sci & Technol, SPKLSTN Lab, Xian 710049, Peoples R China.	lingyun.a.song@gmail.com; liukeen@mail.xjtu.edu.cn; msun@csc.lsu.edu; shang@nwpu.edu.cn	song, ling/GQZ-5934-2022		National Key Research and Development Program of China [2018YFB1004500]; National Nature Science Foundation of China [61772426, 61672419, 61672418, 61532004, 61502377, 61532015, 61721002]; National Natural Science Foundation of China [U1811262]; Innovation Research Team of Ministry of Education [IRT_17R86]; Fundamental Research Funds for the Central Universities [D5000200146]; China Postdoctoral Science Foundation [2020M673487]	National Key Research and Development Program of China; National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Innovation Research Team of Ministry of Education; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation)	The research was supported in part by National Key Research and Development Program of China under Grant No. 2018YFB1004500, National Nature Science Foundation of China under Grant Nos. 61772426, 61672419, 61672418, 61532004, 61502377, 61532015, 61721002, the Joint Funds of the National Natural Science Foundation of China under Grant No. U1811262, Innovation Research Team of Ministry of Education under Grant No. IRT_17R86, Fundamental Research Funds for the Central Universities under Grant No. D5000200146, China Postdoctoral Science Foundation under Grant No. 2020M673487.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arun A, 2019, PROC CVPR IEEE, P9424, DOI 10.1109/CVPR.2019.00966; Bency AJ, 2016, LECT NOTES COMPUT SC, V9905, P714, DOI 10.1007/978-3-319-46448-0_43; Bilen H, 2014, P BMVC 2014, P1997; Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311; Bosch A, 2006, INT C PATT RECOG, P773; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Deselaers T, 2012, INT J COMPUT VISION, V100, P275, DOI 10.1007/s11263-012-0538-3; Diba A, 2017, PROC CVPR IEEE, P5131, DOI 10.1109/CVPR.2017.545; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532; Durand T, 2016, PROC CVPR IEEE, P4743, DOI 10.1109/CVPR.2016.513; Durand T, 2015, IEEE I CONF COMP VIS, P2713, DOI 10.1109/ICCV.2015.311; Durand Thibaut, 2017, CVPR, DOI DOI 10.1109/CVPR.2017.631; Everingham M., 2012, PASCAL VISUAL OBJECT; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Ge WF, 2018, PROC CVPR IEEE, P1277, DOI 10.1109/CVPR.2018.00139; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Hand EM, 2017, AAAI CONF ARTIF INTE, P4068; He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0; Huang J, 2015, IEEE DATA MINING, P181, DOI 10.1109/ICDM.2015.67; Jiale Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11482, DOI 10.1109/CVPR42600.2020.01150; Jie ZQ, 2017, PROC CVPR IEEE, P4294, DOI 10.1109/CVPR.2017.457; Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Nikulin M. S., 2001, ENCY MATH, V78; Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009; OQUAB M, 2015, PROC CVPR IEEE, P685, DOI DOI 10.1109/CVPR.2015.7298668; Parizi S. N, 2014, ARXIV14126598; Pourian N, 2015, IEEE I CONF COMP VIS, P1359, DOI 10.1109/ICCV.2015.160; Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Tang P, 2018, LECT NOTES COMPUT SC, V11215, P370, DOI 10.1007/978-3-030-01252-6_22; Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304; Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326; Wang WCV, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20133661; Xia B, 2012, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON APAC 2011; Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020; Zhang ML, 2015, IEEE T PATTERN ANAL, V37, P107, DOI 10.1109/TPAMI.2014.2339815; Zhang Xiangyu, 2018, IEEE C COMP VIS PATT; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731; Zhongzheng Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10595, DOI 10.1109/CVPR42600.2020.01061; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhu CC, 2017, ADV COMPUT VIS PATT, P57, DOI 10.1007/978-3-319-61657-5_3; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	54	2	2	7	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					681	702		10.1007/s11263-020-01397-w	http://dx.doi.org/10.1007/s11263-020-01397-w		NOV 2020	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC					2022-12-18	WOS:000587999500001
J	Wegmayr, V; Buhmann, JM				Wegmayr, Viktor; Buhmann, Joachim M.			Entrack: Probabilistic Spherical Regression with Entropy Regularization for Fiber Tractography	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Diffusion MRI; Brain; Tractography; Machine Learning; Maximum-entropy inference; Algorithm validation	WHITE-MATTER; CONNECTOME	White matter tractography, based on diffusion-weighted magnetic resonance images, is currently the only available in vivo method to gather information on the structural brain connectivity. The low resolution of diffusion MRI data suggests to employ probabilistic methods for streamline reconstruction, i.e., for fiber crossings. We propose a general probabilistic model for spherical regression based on the Fisher-von-Mises distribution, which efficiently estimates maximum entropy posteriors of local streamline directions with machine learning methods. The optimal precision of posteriors for streamlines is determined by an information-theoretic technique, the expected log-posterior agreement concept. It relies on the requirement that the posterior distributions of streamlines, inferred on retest measurements of the same subject, should yield stable results within the precision determined by the noise level of the data source.	[Wegmayr, Viktor; Buhmann, Joachim M.] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Wegmayr, V (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	vwegmayr@inf.ethz.ch		Wegmayr, Viktor/0000-0002-8279-139X	Swiss Federal Institute of Technology Zurich	Swiss Federal Institute of Technology Zurich(ETH Zurich)	Open access funding provided by Swiss Federal Institute of Technology Zurich.	Alexander D. C., 2006, VISUALIZATION PROCES; Bargmann CI, 2013, NAT METHODS, V10, P483, DOI 10.1038/NMETH.2451; Basser PJ, 2000, MAGNET RESON MED, V44, P625, DOI 10.1002/1522-2594(200010)44:4<625::AID-MRM17>3.0.CO;2-O; BASSER PJ, 1994, J MAGN RESON SER B, V103, P247, DOI 10.1006/jmrb.1994.1037; Beaulieu C, 2002, NMR BIOMED, V15, P435, DOI 10.1002/nbm.782; Behrens TEJ, 2003, MAGN RESON MED, V50, P1077, DOI 10.1002/mrm.10609; Benou I., 2019, MICCAI; Buhmann JM, 2018, THEOR COMPUT SCI, V745, P1, DOI 10.1016/j.tcs.2018.04.015; Buhmann JM, 2013, ADV COMPUT VIS PATT, P45, DOI 10.1007/978-1-4471-5628-4_3; Buhmann JM, 2010, IEEE INT SYMP INFO, P1398, DOI 10.1109/ISIT.2010.5513616; Chehreghani M.H., 2012, AISTATS; Chilla GS, 2015, QUANT IMAG MED SURG, V5, P407, DOI 10.3978/j.issn.2223-4292.2015.03.01; Cote MA, 2013, MED IMAGE ANAL, V17, P844, DOI 10.1016/j.media.2013.03.009; Filley CM, 2016, J NEUROPHYSIOL, V116, P2093, DOI 10.1152/jn.00221.2016; Fischer B., 2016, GCPR; Frank M, 2011, IEEE INT SYMP INFO, P1036, DOI 10.1109/ISIT.2011.6033687; Friman O, 2006, IEEE T MED IMAGING, V25, P965, DOI 10.1109/TMI.2006.877093; Garyfallidis E, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00175; Glasser MF, 2013, NEUROIMAGE, V80, P105, DOI 10.1016/j.neuroimage.2013.04.127; Gorbach NS, 2018, NEUROIMAGE, V181, P219, DOI 10.1016/j.neuroimage.2018.06.066; Hauberg S., 2015, MICCAI; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Jbabdi S, 2011, BRAIN CONNECT, V1, P169, DOI 10.1089/brain.2011.0033; Jeurissen B, 2019, NMR BIOMED, V32, DOI 10.1002/nbm.3785; Jeurissen B, 2013, HUM BRAIN MAPP, V34, P2747, DOI 10.1002/hbm.22099; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Kingma D.P, P 3 INT C LEARNING R; Kiureghian AD, 2009, STRUCT SAF, V31, P105, DOI 10.1016/j.strusafe.2008.06.020; Kumar S., 2018, ICLR; Le Bihan D, 2015, PLOS BIOL, V13, DOI [10.1371/journal.pbio.1002203, 10.1371/journal.pbio.1002246]; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Maier-Hein KH, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01285-x; Mardia K.V., 2000, DIRECTIONAL STAT; Mardia K.V., 1975, STAT DISTRIBUTIONS S; Neher PF, 2017, NEUROIMAGE, V158, P417, DOI 10.1016/j.neuroimage.2017.07.028; Neher PF, 2014, MAGN RESON MED, V72, P1460, DOI 10.1002/mrm.25045; Nimsky C, 2016, ADV TECH STAND NEURO, V43, P37, DOI 10.1007/978-3-319-21359-0_2; Oishi K, 2011, J ALZHEIMERS DIS, V26, P287, DOI 10.3233/JAD-2011-0007; Poulin P., 2017, MICCAI; Poulin P, 2019, MAGN RESON IMAGING, V64, P37, DOI 10.1016/j.mri.2019.04.013; Poupon C., 2010, ISMRM 18 SCI M EXH S; Prokudin S, 2018, LECT NOTES COMPUT SC, V11213, P542, DOI 10.1007/978-3-030-01240-3_33; Raffelt DA, 2017, NEUROIMAGE, V144, P58, DOI 10.1016/j.neuroimage.2016.09.029; Reisert M, 2011, NEUROIMAGE, V54, P955, DOI 10.1016/j.neuroimage.2010.09.016; Sensoy Murat, 2018, NIPS, P3179; Soares JM, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00031; Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041; Wasserthal J, 2018, NEUROIMAGE, V183, P239, DOI 10.1016/j.neuroimage.2018.07.070; Wegmayr V., 2019, GCPR; Wegmayr V, 2018, I S BIOMED IMAGING, P1030; Williams T. H., 1997, VIRTUAL HOSP; Yamada K, 2009, MAGN RESON MED SCI, V8, P165, DOI 10.2463/mrms.8.165	53	2	2	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					656	680		10.1007/s11263-020-01384-1	http://dx.doi.org/10.1007/s11263-020-01384-1		NOV 2020	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC		hybrid, Green Published			2022-12-18	WOS:000586371000001
J	Xu, JW; Ni, BB; Yang, XK				Xu, Jingwei; Ni, Bingbing; Yang, Xiaokang			Progressive Multi-granularity Analysis for Video Prediction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video prediction; Multiple granularity analysis		Video prediction is challenging as real-world motion dynamics are usually multi-modally distributed. Existing stochastic methods commonly formulate random noise input with simple prior distribution, which is insufficient to model highly complex motion dynamics. This work proposes a progressive multiple granularity analysis framework to tackle the above difficulty. Firstly, to achieve coarse alignment, the input sequence is matched to prototype motion dynamics in the training set, based on self-supervised auto-encoder learning via motion/appearance disentanglement. Secondly, motion dynamics is transferred from the matched prototype sequence to input sequence via adaptively learned kernel, and the predicted frames are further refined through a motion-aware prediction model. Extensive qualitative and quantitative experiments on three widely used video prediction datasets demonstrate that: (1) the proposed framework essentially decomposes the hard task into a series of more approachable sub-tasks where a better solution is easier to be sought and (2) our proposed method performs favorably against state-of-the-art prediction methods.	[Xu, Jingwei; Ni, Bingbing; Yang, Xiaokang] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China; [Yang, Xiaokang] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai, Peoples R China	Shanghai Jiao Tong University; Shanghai Jiao Tong University	Ni, BB (corresponding author), Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.	xjwxjw@sjtu.edu.cn; nibingbing@sjtu.edu.cn			National Science Foundation of China [61976137, U1611461, 61527804, U19B2035]; STCSM [18DZ1112300]; National Key Research and Development Program of China [2016YFB1001003]; Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science Technology [KBDat1604]	National Science Foundation of China(National Natural Science Foundation of China (NSFC)); STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); National Key Research and Development Program of China; Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science Technology	This work was supported by National Science Foundation of China (61976137, U1611461, 61527804, U19B2035), STCSM(18DZ1112300). This work was also supported by National Key Research and Development Program of China (2016YFB1001003). The authors would like to acknowledge the (partial) support from the Open Project Program (No.KBDat1604) of Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science & Technology.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Babaeizadeh Mohammad, 2017, ARXIV171011252; Batra D, 2018, NEURIPS; Bradski G, 2000, DR DOBBS J, V25, P120; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Darrell T, 2020, ARXIV200701738; De Brabandere B, 2016, ADV NEUR IN, V29; Deng J, 2016, IEEE T PATTERN ANAL, V38, P666, DOI 10.1109/TPAMI.2015.2439285; Denton E, 2018, PR MACH LEARN RES, V80; Gavves E, 2015, INT J COMPUT VISION, V111, P191, DOI 10.1007/s11263-014-0741-5; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hariharan B, 2017, IEEE T PATTERN ANAL, V39, P627, DOI 10.1109/TPAMI.2016.2578328; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hsieh JT, 2018, ADV NEUR IN, V31; Huang ZY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P757; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Ioffe S., 2015, INT C MACH LEARN, P448, DOI [10.5555/3045118.3045167, DOI 10.5555/3045118.3045167]; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jang Y, 2018, PR MACH LEARN RES, V80; Jingwei Xu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P178, DOI 10.1007/978-3-030-58621-8_11; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Kingma D.P, P 3 INT C LEARNING R; Kurutach T, 2018, ADV NEUR IN, V31; Lee Alex X, 2018, ARXIV180401523; Li HB, 2015, INT J COMPUT VISION, V113, P128, DOI 10.1007/s11263-014-0785-6; Li M., 2015, EMPIRICAL EVALUATION; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Liang XD, 2017, PROC CVPR IEEE, P2175, DOI 10.1109/CVPR.2017.234; Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400; Luc P, 2017, IEEE I CONF COMP VIS, P648, DOI 10.1109/ICCV.2017.77; Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9; Nair A, 2018, ADV NEUR IN, V31; Ni BB, 2016, INT J COMPUT VISION, V120, P28, DOI 10.1007/s11263-016-0891-8; Pathak D, 2017, IEEE COMPUT SOC CONF, P488, DOI 10.1109/CVPRW.2017.70; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruder M, 2018, INT J COMPUT VISION, V126, P1199, DOI 10.1007/s11263-018-1089-z; Ryoo MS, 2016, INT J COMPUT VISION, V119, P307, DOI 10.1007/s11263-015-0847-4; Salimans T, 2016, ADV NEUR IN, V29; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841; Shi XJ, 2015, ADV NEUR IN, V28; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Tian YH, 2015, INT J COMPUT VISION, V111, P153, DOI 10.1007/s11263-014-0737-1; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Van der Maaten L., 2008, J MACH LEARN RES, V9, P2579; Villegas R, 2017, PR MACH LEARN RES, V70; Villegas Ruben, 2017, DECOMPOSING MOTION C, V2, P7; Wichers N, 2018, PR MACH LEARN RES, V80; Wu XM, 2018, INT J COMPUT VISION, V126, P689, DOI 10.1007/s11263-018-1063-9; Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999; Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226; Xu J, 2018, ADV NEUR IN, V31; Xu Z, 2017, IEEE T IMAGE PROCESS, V26, P135, DOI 10.1109/TIP.2016.2621661; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Yan YC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P199, DOI 10.1145/3123266.3123277; Yang R, 2017, PROC CVPR IEEE, P6383, DOI 10.1109/CVPR.2017.676; Zhao B, 2017, INT J AUTOM COMPUT, V14, P119, DOI 10.1007/s11633-017-1053-3	62	2	2	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					601	618		10.1007/s11263-020-01389-w	http://dx.doi.org/10.1007/s11263-020-01389-w		OCT 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC					2022-12-18	WOS:000581552900001
J	Wan, RJ; Shi, BX; Li, HL; Duan, LY; Kot, AC				Wan, Renjie; Shi, Boxin; Li, Haoliang; Duan, Ling-Yu; Kot, Alex C.			Face Image Reflection Removal	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Reflection removal; Deep learning; Face images; Optical flow	SEPARATION	Face images captured through glass are usually contaminated by reflections. The low-transmitted reflections make the reflection removal more challenging than for general scenes because important facial features would be completely occluded. In this paper, we propose and solve the face image reflection removal problem. We recover the important facial structures by incorporating inpainting ideas into a guided reflection removal framework, which takes two images as the input and considers various face-specific priors. We use a newly collected face reflection image dataset to train our model and compare with state-of-the-art methods. The proposed method shows advantages in estimating reflection-free face images for improving face recognition.	[Wan, Renjie; Li, Haoliang; Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore; [Shi, Boxin; Duan, Ling-Yu] Peking Univ, Dept CS, Natl Engn Lab Video Technol, Beijing, Peoples R China; [Shi, Boxin; Duan, Ling-Yu] Peng Cheng Lab, Shenzhen, Peoples R China; Nanyang Technol Univ, Rapid Rich Object Search ROSE Lab, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Peking University; Peng Cheng Laboratory; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Wan, RJ (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.; Shi, BX (corresponding author), Peking Univ, Dept CS, Natl Engn Lab Video Technol, Beijing, Peoples R China.; Shi, BX (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	rjwan@ntu.edu.sg; shiboxin@pku.edu.cn	Wan, Patrick/AAL-2841-2021	Wan, Renjie/0000-0002-0161-0367; Li, Haoliang/0000-0002-8723-8112	Wallenberg-NTU Presidential Postdoctoral Fellowship; NTU-PKU Joint Research Institute; Ng Teng Fong Charitable Foundation; Science and Technology Foundation of Guangzhou Huangpu Development District [201902010028]; National Natural Science Foundation of China [61872012, U1611461]; Beijing Academy of Artificial Intelligence (BAAI)	Wallenberg-NTU Presidential Postdoctoral Fellowship; NTU-PKU Joint Research Institute; Ng Teng Fong Charitable Foundation; Science and Technology Foundation of Guangzhou Huangpu Development District; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence (BAAI)	The research work was done at the Rapid-Rich Object Search (ROSE) Lab, Nanyang Technological University. The work is supported in part by the Wallenberg-NTU Presidential Postdoctoral Fellowship, the NTU-PKU Joint Research Institute, a collaboration between the Nanyang Technological University and Peking University that is sponsored by a donation from the Ng Teng Fong Charitable Foundation, and the Science and Technology Foundation of Guangzhou Huangpu Development District under Grant 201902010028. This research is in part supported by the National Natural Science Foundation of China under Grants 61872012 and U1611461, and Beijing Academy of Artificial Intelligence (BAAI).	Amos Brandon, 2016, OPENFACE GEN PURPOSE, V6; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arvanitopoulos N, 2017, PROC CVPR IEEE, P1752, DOI 10.1109/CVPR.2017.190; Baker S., 2000, FG; Chang YK, 2020, INT J COMPUT VISION, V128, P1673, DOI 10.1007/s11263-019-01276-z; Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264; Deng J., 2017, ARXIV171204695; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan Q., 2019, ARXIV191203623; Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351; Gai K, 2012, IEEE T PATTERN ANAL, V34, P19, DOI 10.1109/TPAMI.2011.87; Gretton A., 2007, P NEUR INF PROC SYST; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Han BJ, 2018, IEEE T IMAGE PROCESS, V27, P4873, DOI 10.1109/TIP.2018.2849880; Howard A.G., 2017, MOBILENETS EFFICIENT; Jaderberg M, 2015, ADV NEUR IN, V28; Kawulok M., 2016, ADV FACE DETECTION F, P189, DOI DOI 10.1007/978-3-319-25958-1_8; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Lee D., 2018, ARXIV180104102; Li H., 2018, P COMP VIS PATT REC; Li PH, 2017, IEEE I CONF COMP VIS, P2089, DOI 10.1109/ICCV.2017.228; Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6; Li Y., 2017, P COMP VIS PATT REC; Li Y., 2013, P IEEE C COMP VIS PA; Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Lin D., 2007, P COMP VIS PATT REC; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lyu Y., 2019, ADV NEURAL INFORM PR; Ma DQ, 2019, IEEE I CONF COMP VIS, P2444, DOI 10.1109/ICCV.2019.00253; Pan JS, 2014, LECT NOTES COMPUT SC, V8695, P47, DOI 10.1007/978-3-319-10584-0_4; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pitie F., 2005, INT C COMP VIS ICCV; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shen ZY, 2018, PROC CVPR IEEE, P8260, DOI 10.1109/CVPR.2018.00862; Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939; SONG YB, 2017, ARXIV170800223, P4537; Sun C., 2013, P IEEE C COMP VIS PA; TORRALBA A, 2011, PROC CVPR IEEE, P1521, DOI DOI 10.1109/CVPR.2011.5995347; Wan R., 2018, P COMP VIS PATT REC; Wan R., 2019, IEEE T PATTERN ANAL; Wan R., 2017, P INT C MULT EXP; Wan RJ, 2018, IEEE T IMAGE PROCESS, V27, P2927, DOI 10.1109/TIP.2018.2808768; Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423; Wan RJ, 2016, IEEE IMAGE PROC, P21, DOI 10.1109/ICIP.2016.7532311; Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171; Wei K., 2019, P IEEE C COMP VIS PA; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; XUE T, 2017, ARXIV171109078; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Yang Jianwei, 2016, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2016.28; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1; Zhang HY, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3277958; Zhang S, 2018, IEEE T INF FOREN SEC, V13, P637, DOI 10.1109/TIFS.2017.2763119; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360	61	2	2	9	24	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					385	399		10.1007/s11263-020-01372-5	http://dx.doi.org/10.1007/s11263-020-01372-5		SEP 2020	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000569550000001
J	Leroy, V; Franco, JS; Boyer, E				Leroy, Vincent; Franco, Jean-Sebastien; Boyer, Edmond			Volume Sweeping: Learning Photoconsistency for Multi-View Shape Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi view stereo reconstruction; Learned photoconsistency; Performance capture; Volume sweeping	STEREO	We propose a full study and methodology for multi-view stereo reconstruction with performance capture data. Multi-view 3D reconstruction has largely been studied with general, high resolution and high texture content inputs, where classic low-level feature extraction and matching are generally successful. However in performance capture scenarios, texture content is limited by wider angle shots resulting in smaller subject projection areas, and intrinsically low image content of casual clothing. We present a dedicated pipeline, based on a per-camera depth map sweeping strategy, analyzing in particular how recent deep network advances allow to replace classic multi-view photoconsistency functions with one that is learned. We show that learning based on a volumetric receptive field around a 3D depth candidate improves over using per-view 2D windows, giving the photoconsistency inference more visibility over local 3D correlations in viewpoint color aggregation. Despite being trained on a standard dataset of scanned static objects, the proposed method is shown to generalize and significantly outperform existing approaches on performance capture data, while achieving competitive results on recent benchmarks.	[Leroy, Vincent; Franco, Jean-Sebastien; Boyer, Edmond] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France; [Leroy, Vincent; Franco, Jean-Sebastien; Boyer, Edmond] Univ Grenoble Alpes, Inst Engn, Grenoble, France; [Leroy, Vincent] NAVER LABS Europe, 6 Chemin Maupertuis, F-38240 Meylan, France	Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria; Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA)	Leroy, V (corresponding author), Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.; Leroy, V (corresponding author), Univ Grenoble Alpes, Inst Engn, Grenoble, France.; Leroy, V (corresponding author), NAVER LABS Europe, 6 Chemin Maupertuis, F-38240 Meylan, France.	vincent.leroy@naverlabs.com; jean-sebastien.franco@inria.fr; edmond.boyer@inria.fr		Leroy, Vincent/0000-0003-1490-6738	France National Research [ANR-14-CE24-0030 ACHMOV]	France National Research	This work was conducted at the INRIA Grenoble. Funded by France National Research Grant ANR-14-CE24-0030 ACHMOV. Images 1-2-15-17 of Anja Rubik courtesy of Ezra Petronio and Self Service Magazine. Geometric model in Fig. 7 courtesy of 3DScanStore (https://www.3dscanstore.com).	Abadi M, 2015, P 12 USENIX S OPERAT; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14; Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58; Chang Angel X., 2015, ARXIV151203012CSGR P; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945; Collins Robert T, 1996, CVPR; Cremers D, 2011, IEEE T PATTERN ANAL, V33, P1161, DOI 10.1109/TPAMI.2010.174; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969; Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052; Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755; Gallup D., 2007 IEEE C COMP VIS, P1; Gilbert A, 2018, LECT NOTES COMPUT SC, V11215, P591, DOI 10.1007/978-3-030-01252-6_35; Hartmann W, 2017, IEEE I CONF COMP VIS, P1595, DOI 10.1109/ICCV.2017.176; Huang Z, 2018, LECT NOTES COMPUT SC, V11220, P351, DOI 10.1007/978-3-030-01270-0_21; Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22; Izadi Shahram, 2011, UIST, DOI [10.1145/2047196.2047270, DOI 10.1145/2047196.2047270]; Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59; Kar A., 2017, ADV NEURAL INFORM PR; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kingma D.P, P 3 INT C LEARNING R; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Labatut P, 2007, IEEE I CONF COMP VIS, P504; Leroy V, 2018, LECT NOTES COMPUT SC, V11213, P796, DOI 10.1007/978-3-030-01240-3_48; Leroy V, 2017, IEEE I CONF COMP VIS, P3113, DOI 10.1109/ICCV.2017.336; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; LUO WJ, 2016, PROC CVPR IEEE, P5695, DOI DOI 10.1109/CVPR.2016.614; Merrell P, 2007, IEEE I CONF COMP VIS, P1221; Mikolajczyk K, 2003, PROC CVPR IEEE, P257; Mustafa A, 2016, PROC CVPR IEEE, P4660, DOI 10.1109/CVPR.2016.504; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Oswald MR, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P291, DOI 10.1109/ICCVW.2013.46; Pons JP, 2007, INT J COMPUT VISION, V72, P179, DOI 10.1007/s11263-006-8671-5; Pudasaini S, 2017, FRONT MICROBIOL, V8, DOI 10.3389/fmicb.2017.00591; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Seitz S., 2006, 2006 IEEE COMP SOC C, V1, P519, DOI [10.1109/CVPR.2006.19, DOI 10.1109/CVPR.2006.19]; Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68; Strecha C, 2008, PROC CVPR IEEE, P2838; Tola E, 2008, PROC CVPR IEEE, P2578; Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Ulusoy AO, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P10, DOI 10.1109/3DV.2015.9; Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zbontar J, 2016, J MACH LEARN RES, V17	51	2	2	1	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					284	299		10.1007/s11263-020-01377-0	http://dx.doi.org/10.1007/s11263-020-01377-0		SEP 2020	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000568233000001
J	Jiang, L; Xu, M; Wang, ZL; Sigal, L				Jiang, Lai; Xu, Mai; Wang, Zulin; Sigal, Leonid			DeepVS2.0: A Saliency-Structured Deep Learning Method for Predicting Dynamic Visual Attention	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep neural networks; Saliency prediction; Convolutional LSTM; Eye-tracking database; Video; Video database	DETECTION MODEL; GAZE	Deep neural networks (DNNs) have exhibited great success in image saliency prediction. However, few works apply DNNs to predict the saliency of generic videos. In this paper, we propose a novel DNN-based video saliency prediction method, called DeepVS2.0. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which provides sufficient data to train the DNN models for predicting video saliency. Through the statistical analysis of LEDOV, we find that human attention is normally attracted by objects, particularly moving objects or the moving parts of objects. Accordingly, we propose an object-to-motion convolutional neural network (OM-CNN) in DeepVS2.0 to learn spatio-temporal features for predicting the intra-frame saliency via exploring the information of both objectness and object motion. We further find from our database that human attention has a temporal correlation with a smooth saliency transition across video frames. Therefore, a saliency-structured convolutional long short-term memory network (SS-ConvLSTM) is developed in DeepVS2.0 to predict inter-frame saliency, using the extracted features of OM-CNN as the input. Moreover, the center-bias dropout and sparsity-weighted loss are embedded in SS-ConvLSTM, to consider the center-bias and sparsity of human attention maps. Finally, the experimental results show that our DeepVS2.0 method advances the state-of-the-art video saliency prediction.	[Jiang, Lai; Xu, Mai; Wang, Zulin] Beihang Univ, Sch Elect & Informat Engn, Beijing, Peoples R China; [Jiang, Lai; Sigal, Leonid] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada	Beihang University; University of British Columbia	Xu, M (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing, Peoples R China.	jianglai.china@buaa.edu.cn; Maixu@buaa.edu.cn; wzulin@buaa.edu.cn; lsigal@cs.ubc.ca			NSFC [61922009, 61876013, 61573037]	NSFC(National Natural Science Foundation of China (NSFC))	This work was supported by the NSFC Projects 61922009, 61876013 and 61573037.	Alers H., 2012, IS T SPIE ELECT IMAG; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665; Bazzani L., 2017, PROC INT C LEARN REP; Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89; Boulos F, 2009, IEEE IMAGE PROC, P3109, DOI 10.1109/ICIP.2009.5414458; BYLINSKII Z, 2018, IEEE T PATTERN ANAL, V41, P3, DOI DOI 10.1109/TPAMI.2018.2815601; Carmi R, 2006, VISION RES, V46, P4333, DOI 10.1016/j.visres.2006.08.019; Chaabouni S, 2016, IEEE IMAGE PROC, P1604, DOI 10.1109/ICIP.2016.7532629; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Cornia M., 2018, P IEEE CVF INT C COM; Coutrot A, 2013, P 2013 14 INT WORKSH, P1; Coutrot A, 2015, EUR SIGNAL PR CONF, P1531, DOI 10.1109/EUSIPCO.2015.7362640; Dorr M, 2010, J VISION, V10, DOI 10.1167/10.10.28; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549; Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gal Yarin, 2016, ADV NEURAL INFORM PR, P1019, DOI DOI 10.5555/3157096.3157211; Gitman Y, 2014, IEEE IMAGE PROC, P1105, DOI 10.1109/ICIP.2014.7025220; Glorot X., 2010, PROC MACH LEARN RES, P249; Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272; Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969; Hadizadeh H, 2012, IEEE T IMAGE PROCESS, V21, P898, DOI 10.1109/TIP.2011.2165292; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Holmqvist Kenneth, 2015, EYE TRACKING COMPREH; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang CR, 2014, IEEE T CIRC SYST VID, V24, P1336, DOI 10.1109/TCSVT.2014.2308652; Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618; ITTI L, 2009, VISION RES, V49, P1295, DOI DOI 10.1016/J.VISRES.2008.09.007; Jiang L., 2015, P IEEE INT C COMP VI, P54; Jiang L, 2018, LECT NOTES COMPUT SC, V11212, P820, DOI 10.1007/978-3-030-01237-3_49; Jiang Y, 2009, 2009 3RD INTERNATIONAL CONFERENCE ON BIOINFORMATICS AND BIOMEDICAL ENGINEERING, VOLS 1-11, P2944; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; KHATOONABADI SH, 2015, PROC CVPR IEEE, P5501; Kim NW, 2017, ACM T COMPUT-HUM INT, V24, DOI 10.1145/3131275; Kingma D.P, P 3 INT C LEARNING R; Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620; Kummerer Matthias, 2014, ARXIV14111045; Le T. N., 2017, ARXIV170801447; Leboran V, 2017, IEEE T PATTERN ANAL, V39, P893, DOI 10.1109/TPAMI.2016.2567391; Lee SH, 2014, IEEE IMAGE PROC, P1120, DOI 10.1109/ICIP.2014.7025223; Li J, 2018, IEEE T IMAGE PROCESS, V27, P349, DOI 10.1109/TIP.2017.2762594; Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6; Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306; Li ZC, 2011, IMAGE VISION COMPUT, V29, P1, DOI 10.1016/j.imavis.2010.07.001; Liu Tie, 2011, PAMI, V33, P353, DOI DOI 10.1109/TPAMI.2010.70; Liu Y., 2017, CVPR; Manning CD, 1999, FDN STAT NATURAL LAN; Marat Sophie, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P1784; Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557; Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154; MATIN E, 1974, PSYCHOL BULL, V81, P899, DOI 10.1037/h0037368; Mauthner T, 2015, PROC CVPR IEEE, P2494, DOI 10.1109/CVPR.2015.7298864; Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z; Nguyen T. V., 2013, P 21 ACM INT C MULTI, P987; Olsen A, 2012, TOBII 1 VT FIXATION; Palazzi A, 2017, IEEE INT VEH SYM, P920, DOI 10.1109/IVS.2017.7995833; Pan J., 2017, CVPR WORKSH; Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71; Peters R.J., 2007, 2007 IEEE C COMP VIS; Rajashekar U, 2008, IEEE T IMAGE PROCESS, V17, P564, DOI 10.1109/TIP.2008.917218; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren ZX, 2013, IEEE T IMAGE PROCESS, V22, P3120, DOI 10.1109/TIP.2013.2259837; Riche N, 2012, P ACCV NOV, P586; Rodriguez M., 2010, SPATIOTEMPORAL MAXIM; Rudoy D, 2013, PROC CVPR IEEE, P1147, DOI 10.1109/CVPR.2013.152; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tobii I TECHNOLOGY, 2017, TOBII TX300 EYE TRAC; Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50; Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272; Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612; Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941; Wang Y, 2017, WIRELESS PERS COMMUN, V93, P461, DOI 10.1007/s11277-016-3351-4; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Xu M, 2017, IEEE T IMAGE PROCESS, V26, P369, DOI 10.1109/TIP.2016.2628583; Zhang JM, 2016, IEEE T PATTERN ANAL, V38, P889, DOI 10.1109/TPAMI.2015.2473844; Zhong S. -H., 2013, AAAI; Zhou F, 2014, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2014.429	87	2	2	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2021	129	1								10.1007/s11263-020-01371-6	http://dx.doi.org/10.1007/s11263-020-01371-6		AUG 2020	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PU0KV					2022-12-18	WOS:000565043900003
J	Mustafa, A; Volino, M; Kim, H; Guillemaut, JY; Hilton, A				Mustafa, Armin; Volino, Marco; Kim, Hansung; Guillemaut, Jean-Yves; Hilton, Adrian			Temporally Coherent General Dynamic Scene Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dynamic 4D reconstruction; Segmentation; Reconstruction; 3D; Temporal coherence; Dynamic scenes	ENERGY MINIMIZATION; STEREO	Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: an automatic method for initial coarse reconstruction to initialize joint estimation; sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to various applications such as free-view rendering and virtual reality.	[Mustafa, Armin; Volino, Marco; Kim, Hansung; Guillemaut, Jean-Yves; Hilton, Adrian] Univ Surrey, Ctr Vis Speech & Signal Proc CVSSP, Guildford GU27XH, Surrey, England	University of Surrey	Mustafa, A (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc CVSSP, Guildford GU27XH, Surrey, England.	a.mustafa@surrey.ac.uk; m.volino@surrey.ac.uk; h.kim@surrey.ac.uk; j.guillemaut@surrey.ac.uk; a.hilton@surrey.ac.uk	Hilton, Adrian/N-3736-2014; Guillemaut, Jean-Yves/N-7739-2014	Hilton, Adrian/0000-0003-4223-238X; Guillemaut, Jean-Yves/0000-0001-8223-5505; Kim, Hansung/0000-0003-4907-0491; Mustafa, Armin/0000-0002-1779-2775	Royal Academy of Engineering Research Fellowship [RF-201718-17177]; EPSRC Platform Grant on Audio-Visual Media Research [EP/P022529]; EPSRC [EP/M028321/1, EP/P022529/1] Funding Source: UKRI	Royal Academy of Engineering Research Fellowship(Royal Academy of Engineering - UK); EPSRC Platform Grant on Audio-Visual Media Research; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This research was supported by the Royal Academy of Engineering Research Fellowship RF-201718-17177 and the EPSRC Platform Grant on Audio-Visual Media Research EP/P022529.	Atapour-Abarghouei Amir, 2019, CVPR; Bailer C., 2015, ICCV; Ballan L., 2010, ACM T GRAPHIC, P1; Basha T, 2010, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2010.5539791; Bleyer Michael, 2011, BMVC; Bouguet Jean yves, 2000, PYRAMIDAL IMPLEMENTA, P1; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Campbell NDF, 2010, IMAGE VISION COMPUT, V28, P14, DOI 10.1016/j.imavis.2008.09.005; Chen P. Y., 2019, CVPR; Coughlan James M., 2000, NIPS; Das P, 2009, IMAGE VISION COMPUT, V27, P206, DOI 10.1016/j.imavis.2008.02.006; Dimitrov D., 2006, 22 EUR WORKSH COMP G; Djelouah A, 2015, IEEE T PATTERN ANAL, V37, P1890, DOI 10.1109/TPAMI.2014.2385704; Djelouah A, 2013, IEEE I CONF COMP VIS, P2640, DOI 10.1109/ICCV.2013.328; Fortune S., 1997, VORONOI DIAGRAMS DEL, P377; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Goldluecke B, 2004, PROC CVPR IEEE, P350; Grundmann M., 2010, CVPR; Guan L, 2010, INT J COMPUT VISION, V90, P283, DOI 10.1007/s11263-010-0341-y; Guillemaut J.Y., 2010, J GRAPHICS GPU GAME, V93, P73; Guillemaut JY, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P440, DOI 10.1109/3DIMPVT.2012.44; Guillemaut JY, 2011, INT J COMPUT VISION, V93, P73, DOI 10.1007/s11263-010-0413-z; Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073; Hane C, 2013, PROC CVPR IEEE, P97, DOI 10.1109/CVPR.2013.20; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Huang CY, 2018, INT SYMP NEXTGEN, P336; JIANG HQ, 2012, ECCV, V7573, P601; Johnson FC, 2003, INFORM RES, V8; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kheir AEM, 2015, HEALTHC LOW-RESOUR S, V3, DOI 10.4081/hls.2015.4946; Kim H, 2012, IEEE T CIRC SYST VID, V22, P1611, DOI 10.1109/TCSVT.2012.2202185; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1480, DOI 10.1109/TPAMI.2006.193; Kowdle A, 2012, LECT NOTES COMPUT SC, V7576, P789, DOI 10.1007/978-3-642-33715-4_57; Kundu A, 2014, LECT NOTES COMPUT SC, V8694, P703, DOI 10.1007/978-3-319-10599-4_45; Larsen ES, 2007, IEEE I CONF COMP VIS, P1440; Lee W, 2011, IEEE T PATTERN ANAL, V33, P1429, DOI 10.1109/TPAMI.2010.196; Lei C, 2009, IEEE I CONF COMP VIS, P1570, DOI 10.1109/ICCV.2009.5459357; MATTHIES L, 1992, INT J COMPUT VISION, V8, P71, DOI 10.1007/BF00126401; Menze Moritz, 2015, CVPR; Mustafa A., 2016, CVPR; Mustafa A., 2015, ICCV; Mustafa A, 2019, IEEE T IMAGE PROCESS, V28, P1118, DOI 10.1109/TIP.2018.2872906; Mustafa Armin, 2016, ECCV; Narayana M, 2013, IEEE I CONF COMP VIS, P1577, DOI 10.1109/ICCV.2013.199; Oswald Martin Ralf, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8692, P32, DOI 10.1007/978-3-319-10593-2_3; Ozden K., 2007, ICCV, P1; Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223; Qian X, 2017, BIOMED PHYS ENG EXPR, V3, DOI 10.1088/2057-1976/aa50d6; Rusu R. B., 2009, THESIS; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Shin YM, 2013, COMPUT VIS IMAGE UND, V117, P1575, DOI 10.1016/j.cviu.2013.06.008; Slavcheva Miroslava, 2017, CVPR; Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68; Stutz D, 2020, INT J COMPUT VISION, V128, P1162, DOI 10.1007/s11263-018-1126-y; Szeliski R, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P517, DOI 10.1109/ICCV.1998.710766; Taneja A, 2011, LECT NOTES COMPUT SC, V6494, P613, DOI 10.1007/978-3-642-19318-7_48; Ngo TT, 2019, INT J COMPUT VISION, V127, P1707, DOI 10.1007/s11263-019-01149-5; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Tung T, 2009, IEEE I CONF COMP VIS, P1709, DOI 10.1109/ICCV.2009.5459384; Veksler O, 2008, LECT NOTES COMPUT SC, V5304, P454, DOI 10.1007/978-3-540-88690-7_34; Vicente S., 2008, P 2008 IEEE C COMP V, P1, DOI DOI 10.1109/CVPR.2008.4587440; Vo Minh, 2016, CVPR; Wedel A, 2011, INT J COMPUT VISION, V95, P29, DOI 10.1007/s11263-010-0404-0; Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25; Wu Shangzhe, 2018, ECCV; Zach C., 2013, CVPR; ZENG G., 2004, ACCV; Zhang D., 2013, CVPR; Zhang GF, 2011, IEEE T PATTERN ANAL, V33, P603, DOI 10.1109/TPAMI.2010.115	70	2	2	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2021	129	1								10.1007/s11263-020-01367-2	http://dx.doi.org/10.1007/s11263-020-01367-2		AUG 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PU0KV		Green Accepted, Green Published, Green Submitted, hybrid			2022-12-18	WOS:000560273000001
J	Cevikalp, H; Dordinejad, GG				Cevikalp, Hakan; Dordinejad, Golara Ghorban			Video Based Face Recognition by Using Discriminatively Learned Convex Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Discriminative models; Affine hulls; Convex hulls; Face recognition; Image sets	REPRESENTATION	A majority of the image set based face recognition methods use a generatively learned model for each person that is learned independently by ignoring the other persons in the gallery set. In contrast to these methods, this paper introduces a novel method that searches for discriminative convex models that best fit to an individual's face images but at the same time are as far as possible from the images of other persons in the gallery. We learn discriminative convex models for both affine and convex hulls of image sets. During testing, distances from the query set images to these models are computed efficiently by using simple matrix multiplications, and the query set is assigned to the person in the gallery whose image set is closest to the query images. The proposed method significantly outperforms other methods using generatively learned convex models in terms of both accuracy and testing time, and achieves the state-of-the-art results on six of the eight tested datasets. Especially, the accuracy improvement is significant on the challenging PaSC, COX, IJB-C and ESOGU video datasets.	[Cevikalp, Hakan; Dordinejad, Golara Ghorban] Eskisehir Osmagazi Univ, Elect & Elect Engn, Machine Learning & Comp Vis Lab, Eskisehir, Turkey	Eskisehir Osmangazi University	Cevikalp, H (corresponding author), Eskisehir Osmagazi Univ, Elect & Elect Engn, Machine Learning & Comp Vis Lab, Eskisehir, Turkey.	hakan.cevikalp@gmail.com; golaradordinejad@gmail.com	Cevikalp, Hakan/R-1300-2016	Cevikalp, Hakan/0000-0002-1708-8817	Scientific and Technological Research Council of Turkey (TUB.ITAK) [EEEAG-118E294]	Scientific and Technological Research Council of Turkey (TUB.ITAK)(Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK))	This work was supported by the Scientific and Technological Research Council of Turkey (TUB.ITAK) under grant number EEEAG-118E294.	Bennett K.P., 2000, ICML, P57; Beveridge J. R., 2013, P 6 INT C BIOMETRICS, P1; Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Cevikalp H., 2019, IEEE T CIRCUITS SYST; Cevikalp H, 2017, IEEE INT CONF COMP V, P1564, DOI 10.1109/ICCVW.2017.184; Cevikalp H, 2017, PROC CVPR IEEE, P4114, DOI 10.1109/CVPR.2017.438; Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965; Chen JC, 2016, IEEE WINT CONF APPL; Chen SK, 2013, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2013.65; Chen YC, 2013, IEEE INT CONF AUTOMA; Cimen E, 2018, DIGIT SIGNAL PROCESS, V77, P187, DOI 10.1016/j.dsp.2017.11.010; Crosswhite N, 2017, IEEE INT CONF AUTOMA, P1, DOI 10.1109/FG.2017.11; Cui Z, 2014, NEUROCOMPUTING, V135, P306, DOI 10.1016/j.neucom.2013.12.004; Deng J., 2019, 2019 IEEE CVF C COMP; Gasimov RN, 2006, OPTIM METHOD SOFTW, V21, P527, DOI 10.1080/10556780600723252; Hamm J., 2008, P INT C MACH LEARN I, P376, DOI DOI 10.1145/1390156.1390204; Hayat M, 2017, INT J COMPUT VISION, V123, P479, DOI 10.1007/s11263-017-1000-3; Hayat M, 2015, IEEE T PATTERN ANAL, V37, P713, DOI 10.1109/TPAMI.2014.2353635; Hayat M, 2014, PROC CVPR IEEE, P1915, DOI 10.1109/CVPR.2014.246; Hayat M, 2014, LECT NOTES COMPUT SC, V8694, P784, DOI 10.1007/978-3-319-10599-4_50; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hu YQ, 2012, IEEE T PATTERN ANAL, V34, P1992, DOI 10.1109/TPAMI.2011.283; Huang ZW, 2018, IEEE T CIRC SYST VID, V28, P2513, DOI 10.1109/TCSVT.2017.2729660; Huang ZW, 2018, IEEE T PATTERN ANAL, V40, P2827, DOI 10.1109/TPAMI.2017.2776154; Huang ZW, 2015, PR MACH LEARN RES, V37, P720; Huang ZW, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2493448; Kim M, 2008, PROC CVPR IEEE, P1787; Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803; Liu LQ, 2014, IEEE T CIRC SYST VID, V24, P1874, DOI 10.1109/TCSVT.2014.2319671; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033; Mian A, 2013, IEEE T IMAGE PROCESS, V22, P5252, DOI 10.1109/TIP.2013.2282996; Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068; Rao Y., 2017, ICCV; Rao YM, 2017, IEEE I CONF COMP VIS, P3801, DOI 10.1109/ICCV.2017.408; Sankaranarayanan S., 2017, ARXIV160405417; Shi YC, 2019, IEEE I CONF COMP VIS, P6901, DOI 10.1109/ICCV.2019.00700; Wang R., 2009, IEEE C COMP VIS PATT, DOI DOI 10.1109/IWISA.2009.5073030; Wang RP, 2008, PROC CVPR IEEE, P2940; Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965; Wang TS, 2009, PATTERN RECOGN LETT, V30, P1161, DOI 10.1016/j.patrec.2009.06.002; Wang W, 2018, IEEE T IMAGE PROCESS, V27, P151, DOI 10.1109/TIP.2017.2746993; Wang W, 2017, IEEE SIGNAL PROC LET, V24, P1318, DOI 10.1109/LSP.2017.2723084; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wolf L., 2011, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2011.5995566; Wu Y, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.134; Xie W., 2018, BRIT MACH VIS C BMVC; Yalcin M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1078, DOI 10.1109/ICCVW.2015.141; Yamaguchi O, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P318, DOI 10.1109/AFGR.1998.670968; Yang M, 2013, IEEE INT CONF AUTOMA; Yang M, 2017, IMAGE VISION COMPUT, V58, P47, DOI 10.1016/j.imavis.2016.07.008; Zhu PF, 2014, IEEE T INF FOREN SEC, V9, P1120, DOI 10.1109/TIFS.2014.2324277	55	2	2	3	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					3000	3014		10.1007/s11263-020-01356-5	http://dx.doi.org/10.1007/s11263-020-01356-5		JUL 2020	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ					2022-12-18	WOS:000552163800002
J	Li, S; Song, WF; Fang, Z; Shi, JY; Hao, AM; Zhao, QP; Qin, H				Li, Shuai; Song, Wenfeng; Fang, Zheng; Shi, Jiaying; Hao, Aimin; Zhao, Qinping; Qin, Hong			Long-Short Temporal-Spatial Clues Excited Network for Robust Person Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Temporal-spatial clues; Long-short appearance model; Motion-refinement; Low-rank analysis		Directly benefiting from the rapid advancement of deep learning methods, person re-identification (Re-ID) applications have been widespread with remarkable successes in recent years. Nevertheless, cross-scene Re-ID is still hindered by large view variation, since it is challenging to effectively exploit and leverage the temporal clues due to heavy computational burden and the difficulty in flexibly incorporating discriminative features. To alleviate, we articulate a long-short temporal-spatial clues excited network (LSTS-NET) for robust person Re-ID across different scenes. In essence, our LSTS-NET comprises a motion appearance model and a motion-refinement aggregating scheme. Of which, the former abstracts temporal clues based on multi-range low-rank analysis both in consecutive frames and in cross-camera videos, which can augment the person-related features with details while suppressing the clutter background across different scenes. In addition, to aggregate the temporal clues with spatial features, the latter is proposed to automatically activate the person-specific features by incorporating personalized motion-refinement layers and several motion-excitation CNN blocks into deep networks, which expedites the extraction and learning of discriminative features from different temporal clues. As a result, our LSTS-NET can robustly distinguish persons across different scenes. To verify the improvement of our LSTS-NET, we conduct extensive experiments and make comprehensive evaluations on 8 widely-recognized public benchmarks. All the experiments confirm that, our LSTS-NET can significantly boost the Re-ID performance of existing deep learning methods, and outperforms the state-of-the-art methods in terms of robustness and accuracy.	[Li, Shuai; Song, Wenfeng; Fang, Zheng; Shi, Jiaying; Hao, Aimin; Zhao, Qinping] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China; [Li, Shuai; Hao, Aimin; Zhao, Qinping] SUNY Stony Brook, Stony Brook, NY 11794 USA; [Qin, Hong] Peng Cheng Lab, Shenzhen 518055, Peoples R China	Beihang University; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Peng Cheng Laboratory	Song, WF (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.; Qin, H (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.	lishuai@buaa.edu.cn; songwenfenga@163.com; qin@cs.stonybrook.edu			National Key RAMP;D Program of China [2018YFB1700603]; National Natural Science Foundation of China [61672077, 61532002]; Beijing Natural Science Foundation Haidian Primitive Innovation Joint Fund [L182016]; National Science Foundation of USA [IIS-1715985, IIS-1812606]	National Key RAMP;D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation Haidian Primitive Innovation Joint Fund; National Science Foundation of USA(National Science Foundation (NSF))	National Key R&D Program of China (No. 2018YFB1700603), National Natural Science Foundation of China (NO. 61672077 and 61532002), Beijing Natural Science Foundation Haidian Primitive Innovation Joint Fund (L182016), National Science Foundation of USA under Grant IIS-1715985 and IIS-1812606.	[Anonymous], IEEE T PATTERN ANAL; Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083; Bai S, 2017, AAAI CONF ARTIF INTE, P1281; Burr DC, 2001, VISION RES, V41, P1891, DOI 10.1016/S0042-6989(01)00072-4; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128; Chen D, 2018, LECT NOTES COMPUT SC, V11211, P764, DOI 10.1007/978-3-030-01234-2_45; Dai J, 2019, IEEE T IMAGE PROCESS, V28, P1366, DOI 10.1109/TIP.2018.2878505; Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621; Fu Y, 2019, AAAI CONF ARTIF INTE, P8287; Gu XQ, 2019, IEEE I CONF COMP VIS, P9646, DOI 10.1109/ICCV.2019.00974; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9; Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535; Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117; Khan FM, 2016, 2016 13TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P256, DOI 10.1109/AVSS.2016.7738058; Kodirov E, 2016, LECT NOTES COMPUT SC, V9905, P178, DOI 10.1007/978-3-319-46448-0_11; Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782; Li JN, 2019, IEEE I CONF COMP VIS, P3957, DOI 10.1109/ICCV.2019.00406; Li JN, 2020, IEEE T IMAGE PROCESS, V29, P4461, DOI 10.1109/TIP.2020.2972108; Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058; Li MX, 2018, LECT NOTES COMPUT SC, V11208, P772, DOI 10.1007/978-3-030-01225-0_45; Li S, 2018, IEEE T PATTERN ANAL, V40, P2963, DOI 10.1109/TPAMI.2017.2764893; Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046; Li W, 2019, IEEE ACCESS, V7, P22457, DOI [10.1109/ACCESS.2019.2898269, DOI 10.1109/ACCESS.2019.2898269]; Li W, 2020, IEEE T KNOWL DATA EN, V32, P1475, DOI 10.1109/TKDE.2019.2909204; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin J, 2017, PROC CVPR IEEE, P3396, DOI 10.1109/CVPR.2017.362; Liu Hanxiao, 2017, INT C LEARN REPR ICL; Liu H, 2017, IEEE I CONF COMP VIS, P493, DOI 10.1109/ICCV.2017.61; Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431; Liu K, 2015, IEEE I CONF COMP VIS, P3810, DOI 10.1109/ICCV.2015.434; Liu Y, 2017, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2017.499; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Lv JM, 2018, PROC CVPR IEEE, P7948, DOI 10.1109/CVPR.2018.00829; Ma AJ, 2015, IEEE T IMAGE PROCESS, V24, P1599, DOI 10.1109/TIP.2015.2395715; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Ma XL, 2017, PATTERN RECOGN, V65, P197, DOI 10.1016/j.patcog.2016.11.018; McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148; Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063; Peng PX, 2018, IEEE T PATTERN ANAL, V40, P1625, DOI 10.1109/TPAMI.2017.2723882; Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051; Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30; Subramaniam A, 2019, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2019.00065; Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730; Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607; van den Hengel, 2016, ARXIV160601609; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; Wang HX, 2018, INT J COMPUT VISION, V126, P1288, DOI 10.1007/s11263-018-1105-3; Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242; Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418; Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543; Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1; Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226; Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507; Yan YC, 2016, LECT NOTES COMPUT SC, V9910, P701, DOI 10.1007/978-3-319-46466-4_42; Yan Yichao, 2019, CVPR; Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148; Ye M, 2018, LECT NOTES COMPUT SC, V11211, P176, DOI 10.1007/978-3-030-01234-2_11; Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550; Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113; Yu R., 2017, ARXIV170804169; Zeng ZA, 2012, LECT NOTES COMPUT SC, V7576, P325, DOI 10.1007/978-3-642-33715-4_24; Zhang RM, 2019, IEEE T IMAGE PROCESS, V28, P4870, DOI 10.1109/TIP.2019.2911488; Zhang W., 2017, ARXIV170206294; Zhang W, 2018, IEEE T CIRC SYST VID, V28, P2768, DOI 10.1109/TCSVT.2017.2718188; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zhao R, 2017, IEEE T PATTERN ANAL, V39, P356, DOI 10.1109/TPAMI.2016.2544310; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11; Zhong Zhun, 2017, PROC CVPR IEEE, P1318, DOI DOI 10.1109/CVPR.2017.389; Zhou SP, 2019, IEEE I CONF COMP VIS, P8039, DOI 10.1109/ICCV.2019.00813; Zhou T., 2011, INT C MACH LEARN ICM; Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717	98	2	2	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2936	2961		10.1007/s11263-020-01349-4	http://dx.doi.org/10.1007/s11263-020-01349-4		JUL 2020	26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ					2022-12-18	WOS:000548832000001
J	Gadelha, M; Rai, A; Maji, S; Wang, R				Gadelha, Matheus; Rai, Aartika; Maji, Subhransu; Wang, Rui			Inferring 3D Shapes from Image Collections Using Adversarial Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D generative models; Unsupervised learning; Differentiable rendering; Adversarial networks		We investigate the problem of learning a probabilistic distribution over three-dimensional shapes given two-dimensional views of multiple objects taken from unknown viewpoints. Our approach calledprojective generative adversarial network(PrGAN) trains a deep generative model of 3D shapes whose projections (or renderings) matches the distribution of the provided 2D views. The addition of adifferentiable projection moduleallows us to infer the underlying 3D shape distribution without access to any explicit 3D or viewpoint annotation during the learning phase. We show that our approach produces 3D shapes of comparable quality to GANs trained directly on 3D data. Experiments also show that the disentangled representation of 2D shapes into geometry and viewpoint leads to a good generative model of 2D shapes. The key advantage of our model is that it estimates 3D shape, viewpoint, and generates novel views from an input image in a completely unsupervised manner. We further investigate how the generative models can be improved if additional information such as depth, viewpoint or part segmentations is available at training time. To this end, we present new differentiable projection operators that can be used to learn better 3D generative models. Our experiments show thatPrGANcan successfully leverage extra visual cues to create more diverse and accurate shapes.	[Gadelha, Matheus; Rai, Aartika; Maji, Subhransu; Wang, Rui] Univ Massachusetts Amherst, Coll Informat & Comp Sci, 140 Governors Dr, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Gadelha, M (corresponding author), Univ Massachusetts Amherst, Coll Informat & Comp Sci, 140 Governors Dr, Amherst, MA 01003 USA.	mgadelha@cs.umass.edu; aartikarai@cs.umass.edu; smaji@cs.umass.edu; ruiwang@cs.umass.edu			NSF [1617917, 1749833, 1661259, 1908669]; Collaborative RD Fund	NSF(National Science Foundation (NSF)); Collaborative RD Fund	This research was supported in part by the NSF Grants 1617917, 1749833, 1661259 and 1908669. The experiments were performed using equipment obtained under a grant from the Collaborative R&D Fund managed by the Massachusetts Tech Collaborative.	Achlioptas Panos, 2017, ARXIV170702392; Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Barrow Harry, 1978, COMPUTER VISION SYST, V2, P1; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Chang Angel X., 2015, ARXIV151203012CSGR P; Cheng ZZ, 2019, PROC CVPR IEEE, P5438, DOI 10.1109/CVPR.2019.00559; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7; Gadelha M, 2019, IEEE I CONF COMP VIS, P22, DOI 10.1109/ICCV.2019.00011; Gadelha M, 2017, INT CONF 3D VISION, P402, DOI 10.1109/3DV.2017.00053; Gadelha Matheus, 2017, BRIT MACH VIS C; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Groueix Thibault, 2018, COMPUTER VISION PATT; Hartley R., 2003, MULTIPLE VIEW GEOMET; HENDERSON P, 2018, P BRIT MACH VIS C; Hoiem D, 2005, IEEE I CONF COMP VIS, P654; Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23; Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Kulkarni TD, 2015, ADV NEUR IN, V28; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; Li TM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201383; Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071; Lin CH, 2018, AAAI CONF ARTIF INTE, P7114; Liu HTD, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275047; Lun ZL, 2017, INT CONF 3D VISION, P67, DOI 10.1109/3DV.2017.00018; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Nalbach O., 2016, ARXIV160306078; Nguyen-Phuoc Thu, 2018, ADV NEURAL INFORM PR; Odena A, 2016, DISTILL, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003]; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rezende DJ, 2016, ADV NEUR IN, V29; Savarese S, 2007, IEEE I CONF COMP VIS, P1245; Saxena Ashutosh, 2005, ADV NEURAL INFORM PR; Schwing AG, 2012, LECT NOTES COMPUT SC, V7577, P299, DOI 10.1007/978-3-642-33783-3_22; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160; Tulsiani S, 2015, IEEE I CONF COMP VIS, P64, DOI 10.1109/ICCV.2015.16; Tulsiani Shubham, 2018, CVPR, DOI DOI 10.1109/CVPR.2018.00306; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu Jiajun, 2016, ADV NEURAL INFORM PR; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18	49	2	2	4	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2651	2664		10.1007/s11263-020-01335-w	http://dx.doi.org/10.1007/s11263-020-01335-w		JUN 2020	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Green Submitted			2022-12-18	WOS:000543022700001
J	Spampinato, C; Palazzo, S; D'Oro, P; Giordano, D; Shah, M				Spampinato, C.; Palazzo, S.; D'Oro, P.; Giordano, D.; Shah, M.			Adversarial Framework for Unsupervised Learning of Motion Dynamics in Videos	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generative adversarial networks; Video generation; Unsupervised learning; Video object segmentation		Human behavior understanding in videos is a complex, still unsolved problem and requires to accurately model motion at both the local (pixel-wise dense prediction) and global (aggregation of motion cues) levels. Current approaches based on supervised learning require large amounts of annotated data, whose scarce availability is one of the main limiting factors to the development of general solutions. Unsupervised learning can instead leverage the vast amount of videos available on the web and it is a promising solution for overcoming the existing limitations. In this paper, we propose an adversarial GAN-based framework that learns video representations and dynamics through a self-supervision mechanism in order to perform dense and global prediction in videos. Our approach synthesizes videos by (1) factorizing the process into the generation of static visual content and motion, (2) learning a suitable representation of a motion latent space in order to enforce spatio-temporal coherency of object trajectories, and (3) incorporating motion estimation and pixel-wise dense prediction into the training procedure. Self-supervision is enforced by using motion masks produced by the generator, as a co-product of its generation process, to supervise the discriminator network in performing dense prediction. Performance evaluation, carried out on standard benchmarks, shows that our approach is able to learn, in an unsupervised way, both local and global video dynamics. The learned representations, then, support the training of video object segmentation methods with sensibly less (about 50%) annotations, giving performance comparable to the state of the art. Furthermore, the proposed method achieves promising performance in generating realistic videos, outperforming state-of-the-art approaches especially on motion-related metrics.	[Spampinato, C.; Palazzo, S.; D'Oro, P.; Giordano, D.] Univ Catania, PeRCeiVe Lab, Dept Elect Elect & Comp Engn, Catania, Italy; [Spampinato, C.; Shah, M.] Univ Cent Florida, Ctr Res Comp Vis, Orlando, FL 32816 USA	University of Catania; State University System of Florida; University of Central Florida	Palazzo, S (corresponding author), Univ Catania, PeRCeiVe Lab, Dept Elect Elect & Comp Engn, Catania, Italy.	cspampin@dieei.unict.it; palazzosim@dieei.unict.it; dgiordan@dieei.unict.it; shah@crcv.ucf.edu		Palazzo, Simone/0000-0002-2441-0982; Shah, Mubarak/0000-0001-6172-5572				Arjovsky M, 2017, PR MACH LEARN RES, V70; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; Doersch C., 2015, UNSUPERVISED VISUAL; Faktor A., 2014, P BMVC, V2, P8; Farneback G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50; Fragkiadaki K, 2012, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2012.6247883; Giordano D, 2015, PROC CVPR IEEE, P4814, DOI 10.1109/CVPR.2015.7299114; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Haller E, 2017, IEEE I CONF COMP VIS, P5095, DOI 10.1109/ICCV.2017.544; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228; Jang Y, 2018, PR MACH LEARN RES, V80; Keuper Margret, 2015, ICCV; Koh YJ, 2017, PROC CVPR IEEE, P7417, DOI 10.1109/CVPR.2017.784; Lai WS, 2017, ADV NEUR IN, V30; Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318; Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Odena A., 2016, SEMISUPERVISED LEARN; Ohnishi K, 2018, AAAI CONF ARTIF INTE, P2387; Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Radford A., 2016, ICLR 2016 INT C LEAR, DOI DOI 10.1007/S11280-018-0565-2; Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433; Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308; Salimans T, 2016, ADV NEUR IN, V29; Shoemaker K., 1985, Computer Graphics, V19, P245, DOI 10.1145/325165.325242; Soomro K., 2012, ARXIV; Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606; Stretcu O., 2015, BMVC; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tokmakov P, 2017, PROC CVPR IEEE, P531, DOI 10.1109/CVPR.2017.64; Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Vondrick C, 2017, PROC CVPR IEEE, P2992, DOI 10.1109/CVPR.2017.319; Wang Ting-Chun, 2018, ARXIV180806601; Wang W, 2015, PROC CVPR IEEE, P3395, DOI 10.1109/CVPR.2015.7298816; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zhang H., 2017, ICCV; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	59	2	3	2	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1378	1397		10.1007/s11263-019-01246-5	http://dx.doi.org/10.1007/s11263-019-01246-5			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Green Submitted			2022-12-18	WOS:000531431500015
J	Zhao, L; Peng, X; Tian, Y; Kapadia, M; Metaxas, DN				Zhao, Long; Peng, Xi; Tian, Yu; Kapadia, Mubbasir; Metaxas, Dimitris N.			Towards Image-to-Video Translation: A Structure-Aware Approach via Multi-stage Generative Adversarial Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image-to-video translation; Video generation; Multi-stage GANs; Motion prediction; Residual learning		In this paper, we consider the problem of image-to-video translation, where one or a set of input images are translated into an output video which contains motions of a single object. Especially, we focus on predicting motions conditioned by high-level structures, such as facial expression and human pose. Recent approaches are either driven by structural conditions or temporal-based. Condition-driven approaches typically train transformation networks to generate future frames conditioned on the predicted structural sequence. Temporal-based approaches, on the other hand, have shown that short high-quality motions can be generated using 3D convolutional networks with temporal knowledge learned from massive training data. In this work, we combine the benefits of both approaches and propose a two-stage generative framework where videos are forecast from the structural sequence and then refined by temporal signals. To model motions more efficiently in the forecasting stage, we train networks with dense connections to learn residual motions between the current and future frames, which avoids learning motion-irrelevant details. To ensure temporal consistency in the refining stage, we adopt the ranking loss for adversarial training. We conduct extensive experiments on two image-to-video translation tasks: facial expression retargeting and human pose forecasting. Superior results over the state of the art on both tasks demonstrate the effectiveness of our approach.	[Zhao, Long; Tian, Yu; Kapadia, Mubbasir; Metaxas, Dimitris N.] Rutgers State Univ, Piscataway, NJ 08854 USA; [Peng, Xi] Univ Delaware, Newark, DE 19716 USA	Rutgers State University New Brunswick; University of Delaware	Zhao, L (corresponding author), Rutgers State Univ, Piscataway, NJ 08854 USA.	lz311@cs.rutgers.edu; xipeng@udel.edu; yt219@cs.rutgers.edu; mk1353@cs.rutgers.edu; dnm@cs.rutgers.edu		Zhao, Long/0000-0001-8921-8564	NSF [SAS-1723869]; DARPA [SocialSim-W911NF-17-C-0098]	NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work is partly supported by NSF 1763523, 1747778, 1733843 and 1703883 Awards. This work was also funded in part by grant BAAAFOSR-2013-0001 to Dimitris N. Metaxas. Mubbasir Kapadia has been funded in part by NSF IIS-1703883, NSF S&AS-1723869, and DARPA SocialSim-W911NF-17-C-0098.	Aifanti N, 2010, 11 INT WORKSH IM AN, P1; Amos Brandon, 2016, CMUCS16118; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arjovsky M, 2017, PR MACH LEARN RES, V70; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388; Denton Emily L, 2015, NEURIPS, V2, P4; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Farneback G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50; Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494; Gatys LA, 2015, ADV NEUR IN, V28; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Kingma D.P, P 3 INT C LEARNING R; Laine S, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099581; Liang XD, 2018, LECT NOTES COMPUT SC, V11217, P574, DOI 10.1007/978-3-030-01261-8_34; Liang XD, 2017, IEEE I CONF COMP VIS, P3382, DOI 10.1109/ICCV.2017.364; Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Lu JJ, 2017, IEEE I CONF COMP VIS, P446, DOI 10.1109/ICCV.2017.56; Mathieu Michael, 2016, ICLR; Mirza M., 2014, ARXIV; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nieuwoudt P, 2018, INT CONF IMAG VIS; Odena A, 2017, PR MACH LEARN RES, V70; Olszewski K, 2017, IEEE I CONF COMP VIS, P5439, DOI 10.1109/ICCV.2017.580; Pan JT, 2019, PROC CVPR IEEE, P3728, DOI 10.1109/CVPR.2019.00385; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Peng X, 2018, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2018.00237; Peng X, 2016, LECT NOTES COMPUT SC, V9905, P38, DOI 10.1007/978-3-319-46448-0_3; Peng X, 2015, COMPUT VIS IMAGE UND, V136, P92, DOI 10.1016/j.cviu.2015.03.008; Perarnau G, 2016, ARXIV161106355; Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Reed S, 2016, PR MACH LEARN RES, V48; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308; Shen W, 2017, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2017.135; Shi XS, 2015, 2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P802, DOI 10.1109/IAEAC.2015.7428667; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Simonyan K, 2014, ADV NEUR IN, V27; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; TANG Z, 2018, BRIT MACH VIS C BMVC; Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21; Thies Justus, 2016, CVPR, DOI DOI 10.1109/CVPR.2016.262; Tian Y, 2019, ADV NEUR IN, V32; Tian Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P942; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; van den Oord A, 2016, PR MACH LEARN RES, V48; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Villegas R, 2017, PR MACH LEARN RES, V70; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Wang TC, 2018, ADV NEUR IN, V31; Wei K, 2013, IEEE ICC; Xiong W, 2018, PROC CVPR IEEE, P2364, DOI 10.1109/CVPR.2018.00251; Yan SJ, 2019, IEEE I CONF COMP VIS, P4393, DOI 10.1109/ICCV.2019.00449; Yang Y, 2016, ADV SCI, V3, DOI 10.1002/advs.201600097; Yu T, 2019, BIOMED RES INT, V2019, DOI 10.1155/2019/9042542; Zeng X, 2018, 2018 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC); Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297; Zhang H., 2017, ARXIV PREPRINT ARXIV; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407; Zhang Y, 2013, INT J CARDIOL, V163, pS1; Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649; Zhao L, 2019, COMPUT GRAPH-UK, V79, P58, DOI 10.1016/j.cag.2019.01.004; Zhao L, 2018, LECT NOTES COMPUT SC, V11219, P403, DOI 10.1007/978-3-030-01267-0_24; Zhou X, 2017, IEEE ICC; Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152	79	2	2	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2514	2533		10.1007/s11263-020-01328-9	http://dx.doi.org/10.1007/s11263-020-01328-9		APR 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY					2022-12-18	WOS:000529484900001
J	Grard, M; Dellandrea, E; Chen, LM				Grard, Matthieu; Dellandrea, Emmanuel; Chen, Liming			Deep Multicameral Decoding for Localizing Unoccluded Object Instances from a Single RGB Image	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Instance boundary and occlusion detection; Fully convolutional encoder-decoder networks; Synthetic data; Domain adaptation		Occlusion-aware instance-sensitive segmentation is a complex task generally split into region-based segmentations, by approximating instances as their bounding box. We address the showcase scenario of dense homogeneous layouts in which this approximation does not hold. In this scenario, outlining unoccluded instances by decoding a deep encoder becomes difficult, due to the translation invariance of convolutional layers and the lack of complexity in the decoder. We therefore propose a multicameral design composed of subtask-specific lightweight decoder and encoder-decoder units, coupled in cascade to encourage subtask-specific feature reuse and enforce a learning path within the decoding process. Furthermore, the state-of-the-art datasets for occlusion-aware instance segmentation contain real images with few instances and occlusions mostly due to objects occluding the background, unlike dense object layouts. We thus also introduce a synthetic dataset of dense homogeneous object layouts, namely Mikado, which extensibly contains more instances and inter-instance occlusions per image than these public datasets. Our extensive experiments on Mikado and public datasets show that ordinal multiscale units within the decoding process prove more effective than state-of-the-art design patterns for capturing position-sensitive representations. We also show that Mikado is plausible with respect to real-world problems, in the sense that it enables the learning of performance-enhancing representations transferable to real images, while drastically reducing the need of hand-made annotations for finetuning. The proposed dataset will be made publicly available.	[Grard, Matthieu; Dellandrea, Emmanuel; Chen, Liming] Sileane, 17 Rue Descartes, F-42000 St Etienne, France; [Grard, Matthieu] Univ Lyon, CNRS, Ecole Cent Lyon, LIRIS,UMR5205, F-69134 Lyon, France	Centre National de la Recherche Scientifique (CNRS); Ecole Centrale de Lyon; Institut National des Sciences Appliquees de Lyon - INSA Lyon	Grard, M (corresponding author), Sileane, 17 Rue Descartes, F-42000 St Etienne, France.; Grard, M (corresponding author), Univ Lyon, CNRS, Ecole Cent Lyon, LIRIS,UMR5205, F-69134 Lyon, France.	m.grard@sileane.com; emmanuel.dellandrea@ec-lyon.fr; liming.chen@ec-lyon.fr						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antoniou A, 2018, LECT NOTES COMPUT SC, V11141, P594, DOI 10.1007/978-3-030-01424-7_58; Ayvaci A., 2010, ADV NEURAL INFORM PR, P100; Ayvaci A, 2012, INT J COMPUT VISION, V97, P322, DOI 10.1007/s11263-011-0490-7; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305; Batra A, 2019, PROC CVPR IEEE, P10377, DOI 10.1109/CVPR.2019.01063; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Blender Online Community, 2016, BLEND 3D MOD REND PA; Bregier R, 2017, IEEE INT CONF COMP V, P2209, DOI 10.1109/ICCVW.2017.258; Cai Han, 2019, INT C LEARN REPR; Chen L, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P833, DOI 10.1145/3269206.3271759; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35; Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047; Eigen David, 2014, NEURIPS; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fan RC, 2019, PROC CVPR IEEE, P6096, DOI 10.1109/CVPR.2019.00626; Follmann P, 2019, IEEE WINT CONF APPL, P1328, DOI 10.1109/WACV.2019.00146; Follmann P, 2018, LECT NOTES COMPUT SC, V11214, P581, DOI 10.1007/978-3-030-01249-6_35; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Fu H, 2016, PROC CVPR IEEE, P241, DOI 10.1109/CVPR.2016.33; Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470; Gan YK, 2018, LECT NOTES COMPUT SC, V11207, P232, DOI 10.1007/978-3-030-01219-9_14; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; GEIGER D, 1995, INT J COMPUT VISION, V14, P211, DOI 10.1007/BF01679683; Glorot X., 2010, PROC MACH LEARN RES, P249; Grammalidis N, 1998, IEEE T CIRC SYST VID, V8, P328, DOI 10.1109/76.678630; Grard M., 2018, 2017 INT WORKSH HUM, V7, P207; Guan S., 2018, J BIOMEDICAL HLTH IN; Hayder Z, 2017, PROC CVPR IEEE, P587, DOI 10.1109/CVPR.2017.70; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He XM, 2010, LECT NOTES COMPUT SC, V6314, P539; Hollingworth S, 2011, GEOGRAPHIES OF CHILDREN, YOUTH AND FAMILIES: AN INTERNATIONAL PERSPECTIVE, P250; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Humayun A, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995517; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781; Kingma D.P, P 3 INT C LEARNING R; Kirillov A., 2019, ARXIV191208193 CORR; Kirillov A, 2017, PROC CVPR IEEE, P7322, DOI 10.1109/CVPR.2017.774; Korada N, 2018, 2018 IEEE 6TH WORKSHOP ON WIDE BANDGAP POWER DEVICES AND APPLICATIONS (WIPDA), P40, DOI 10.1109/WiPDA.2018.8569189; Lee W, 2019, PROC CVPR IEEE, P4979, DOI 10.1109/CVPR.2019.00512; Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715; Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; Liu G, 2018, PROCEEDINGS OF 2018 TENTH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL INTELLIGENCE (ICACI), P402, DOI 10.1109/ICACI.2018.8377492; Liu R, 2018, ADV NEUR IN, V31; Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849; Luo P, 2017, IEEE I CONF COMP VIS, P2737, DOI 10.1109/ICCV.2017.296; Maninis KK, 2016, LECT NOTES COMPUT SC, V9905, P580, DOI 10.1007/978-3-319-46448-0_35; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; McCormac J, 2017, IEEE I CONF COMP VIS, P2697, DOI 10.1109/ICCV.2017.292; Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433; Pal D., 2010, INT C ART INT STAT, P129; Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481406; Qi Lu, 2019, CVPR, P3014; Ren MY, 2017, PROC CVPR IEEE, P293, DOI 10.1109/CVPR.2017.39; Ren XF, 2006, LECT NOTES COMPUT SC, V3952, P614; Romera-Paredes B, 2016, LECT NOTES COMPUT SC, V9910, P312, DOI 10.1007/978-3-319-46466-4_19; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Stein A., 2006, BRIT MACH VIS C BMVC; Sun DQ, 2014, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2014.144; Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21; Do TT, 2018, IEEE INT CONF ROBOT, P5882; Wang G., 2018, PROC ASIAN C COMPUT, P686; Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163; Wang P, 2016, LECT NOTES COMPUT SC, V9905, P545, DOI 10.1007/978-3-319-46448-0_33; Wang YP, 2017, PROC CVPR IEEE, P1724, DOI 10.1109/CVPR.2017.187; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yang Jianwei, 2016, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2016.28; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Yu, 2019, ICLR, P1; Yu F., 2016, P ICLR 2016; Zhang L., 2019, BRIT MACH VIS C BMVC; Zhu Y, 2017, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2017.320; Zitnick CL, 2000, IEEE T PATTERN ANAL, V22, P675, DOI 10.1109/34.865184	88	2	2	4	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1331	1359		10.1007/s11263-020-01323-0	http://dx.doi.org/10.1007/s11263-020-01323-0		MAR 2020	29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL9ON		Green Submitted			2022-12-18	WOS:000521913600001
J	Bang, D; Kang, SY; Shim, H				Bang, Duhyeon; Kang, Seoungyoon; Shim, Hyunjung			Discriminator Feature-Based Inference by Recycling the Discriminator of GANs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generative adversarial networks; Inference mapping; Conditional image generation; Quality metric for inference mapping; Spatial semantic manipulation		Generative adversarial networks (GANs) successfully generate high quality data by learning a mapping from a latent vector to the data. Various studies assert that the latent space of a GAN is semantically meaningful and can be utilized for advanced data analysis and manipulation. To analyze the real data in the latent space of a GAN, it is necessary to build an inference mapping from the data to the latent vector. This paper proposes an effective algorithm to accurately infer the latent vector by utilizing GAN discriminator features. Our primary goal is to increase inference mapping accuracy with minimal training overhead. Furthermore, using the proposed algorithm, we suggest a conditional image generation algorithm, namely a spatially conditioned GAN. Extensive evaluations confirmed that the proposed inference algorithm achieved more semantically accurate inference mapping than existing methods and can be successfully applied to advanced conditional image generation tasks.	[Bang, Duhyeon; Kang, Seoungyoon; Shim, Hyunjung] Yonsei Univ, Yonsei Inst Convergence Technol, Sch Integrated Technol, Seoul, South Korea	Yonsei University	Shim, H (corresponding author), Yonsei Univ, Yonsei Inst Convergence Technol, Sch Integrated Technol, Seoul, South Korea.	duhyeonbang@yonsei.ac.kr; sy.kang@yonsei.ac.kr; kateshim@yonsei.ac.kr	Bang, Duhyeon/ABB-2668-2021; Shim, Hyunjung/AAS-3610-2021		National Research Foundation of Korea - Korean Government [NRF-2019R1A2C2006123]; MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program [IITP-2019-2016-0-00288]; ICT R&D program of MSIP/IITP [R7124-16-0004]	National Research Foundation of Korea - Korean Government(National Research Foundation of KoreaKorean Government); MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program; ICT R&D program of MSIP/IITP	This research was supported by the Basic Science Research Program through the National Research Foundation of Korea funded by the Korean Government (Grant NRF-2019R1A2C2006123), the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2019-2016-0-00288) supervised by the IITP (Institute for Information & communications Technology Planning & Evaluation), and also by ICT R&D program of MSIP/IITP. [R7124-16-0004, Development of Intelligent Interaction Technology Based on Context Awareness and Human Intention Understanding].	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baldi P., 2012, P ICML WORKSH UNS TR, P37; Bang D, 2018, PR MACH LEARN RES, V80; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Brock Andrew, 2018, ARXIV180911096; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Donahue Jeff, 2017, INT C LEARN REPR ICL; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Dumoulin Vincent, 2017, ICLR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Iandola F.N., 2016, ARXIV; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A., 2014, CORR; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Li Chunyuan, 2017, NIPS; LiraCantu M, 2018, METAL OXIDES, P1; Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Makhzani Alireza, 2016, ICLR WORKSH; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mescheder L, 2018, PR MACH LEARN RES, V80; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Warde-Farley D., 2017, INT C LEARN REPR; Xiao H., 2017, FASHION MNIST NOVEL; Zhang Han, 2018, ARXIV180508318; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang WW, 2008, LECT NOTES COMPUT SC, V5305, P802, DOI 10.1007/978-3-540-88693-8_59; Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36; Zhu Jun-Yan, 2017, ICCV	42	2	2	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2436	2458		10.1007/s11263-020-01311-4	http://dx.doi.org/10.1007/s11263-020-01311-4		MAR 2020	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Green Submitted			2022-12-18	WOS:000518246900003
J	Ma, SZ; Smith, BM; Gupta, M				Ma, Sizhuo; Smith, Brandon M.; Gupta, Mohit			Differential Scene Flow from Light Field Gradients	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	15th European Conference on Computer Vision (ECCV)	SEP 08-14, 2018	Munich, GERMANY			Scene flow; 3D motion estimation; Differential analysis; Differential motion; Light fields; 3D shape and motion estimation; Computational cameras	OPTICAL-FLOW; STEREO	This paper presents novel techniques for recovering 3D dense scene flow, based on differential analysis of 4D light fields. The key enabling result is a per-ray linear equation, called the ray flow equation, that relates 3D scene flow to 4D light field gradients. The ray flow equation is invariant to 3D scene structure and applicable to a general class of scenes, but is under-constrained (3 unknowns per equation). Thus, additional constraints must be imposed to recover motion. We develop two families of scene flow algorithms by leveraging the structural similarity between ray flow and optical flow equations: local 'Lucas-Kanade' ray flow and global 'Horn-Schunck' ray flow, inspired by corresponding optical flow methods. We also develop a combined local-global method by utilizing the correspondence structure in the light fields. We demonstrate high precision 3D scene flow recovery for a wide range of scenarios, including rotation and non-rigid motion. We analyze the theoretical and practical performance limits of the proposed techniques via the light field structure tensor, a 3x3 matrix that encodes the local structure of light fields. We envision that the proposed analysis and algorithms will lead to design of future light-field cameras that are optimized for motion sensing, in addition to depth sensing.	[Ma, Sizhuo; Smith, Brandon M.; Gupta, Mohit] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Gupta, M (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.	sizhuoma@cs.wisc.edu; bmsmith@cs.wisc.edu; mohitg@cs.wisc.edu		Ma, Sizhuo/0000-0003-0092-9744				ADELSON EH, 1992, IEEE T PATTERN ANAL, V14, P99, DOI 10.1109/34.121783; Alexander E, 2016, LECT NOTES COMPUT SC, V9907, P667, DOI 10.1007/978-3-319-46487-9_41; Aujol JF, 2006, INT J COMPUT VISION, V67, P111, DOI 10.1007/s11263-006-4331-z; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; Bok Y, 2017, IEEE T PATTERN ANAL, V39, P287, DOI 10.1109/TPAMI.2016.2541145; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Brox T., 2014, P 8 EUR C COMP VIS P, V3024, P25; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; Chandraker M, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481415; Chandraker M, 2014, PROC CVPR IEEE, P2179, DOI 10.1109/CVPR.2014.279; Chandraker M, 2014, LECT NOTES COMPUT SC, V8695, P202, DOI 10.1007/978-3-319-10584-0_14; Dansereau D. G., 2017, IEEE C COMP VIS PATT; Dansereau DG, 2011, IEEE INT C INT ROBOT, P4455, DOI 10.1109/IROS.2011.6048841; Gottfried Jens-Malte, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P758; Hasinoff SW, 2010, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2010.5540167; Haussecker HW, 2001, IEEE T PATTERN ANAL, V23, P661, DOI 10.1109/34.927465; Heber S, 2014, LECT NOTES COMPUT SC, V8753, P3, DOI 10.1007/978-3-319-11752-2_1; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Hung CH, 2013, INT J COMPUT VISION, V102, P271, DOI 10.1007/s11263-012-0559-y; Jaimez M, 2015, IEEE INT CONF ROBOT, P98, DOI 10.1109/ICRA.2015.7138986; Jo K, 2015, IEEE I CONF COMP VIS, P4319, DOI 10.1109/ICCV.2015.491; Johannsen O, 2015, IEEE I CONF COMP VIS, P720, DOI 10.1109/ICCV.2015.89; Letouzey A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.46; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Li Z., 2017, IEEE C COMP VIS PATT, V1; Lucas B.D., 1981, IJCAI 81 P 7 INT JOI, P674, DOI DOI 10.1109/HPDC.2004.1323531; Ma SZ, 2018, LECT NOTES COMPUT SC, V11212, P681, DOI 10.1007/978-3-030-01237-3_41; Navarro J, 2016, INT CONF SYST SIGNAL, P61; Neumann J, 2004, COMPUT VIS IMAGE UND, V96, P274, DOI 10.1016/j.cviu.2004.03.013; Neumann J, 2003, PROC CVPR IEEE, P294; Ng R., 2005, COMPUT SCI TECH REP, V2, P1; ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Ranjan A., 2018, ARXIV180509806CS; Schmid C., 2017, ARXIV170407804CS; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Smith B. M., 2018, IEEE C COMP VIS PATT; Smith BM, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073607; Srinivasan PP, 2015, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2015.399; Sun D, 2015, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2015.7298653; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89; Vedula S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P722, DOI 10.1109/ICCV.1999.790293; Wang TC, 2016, PROC CVPR IEEE, P5451, DOI 10.1109/CVPR.2016.588; Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147; Wedel A, 2008, LECT NOTES COMPUT SC, V5302, P739, DOI 10.1007/978-3-540-88682-2_56; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Zhang YL, 2017, IEEE INT CONF COMPUT, P67	48	2	2	2	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		679	697		10.1007/s11263-019-01230-z	http://dx.doi.org/10.1007/s11263-019-01230-z			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	KU1MV		Green Submitted			2022-12-18	WOS:000519475600009
J	Wang, C; Niu, WJ; Jiang, YF; Zheng, HY; Yu, ZB; Gu, ZR; Zheng, B				Wang, Chao; Niu, Wenjie; Jiang, Yufeng; Zheng, Haiyong; Yu, Zhibin; Gu, Zhaorui; Zheng, Bing			Discriminative Region Proposal Adversarial Network for High-Quality Image-to-Image Translation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image-to-image translation; GAN; Pix2pix; Pix2pixHD; CycleGAN; DRPAN		Image-to-image translation has been made much progress with embracing Generative Adversarial Networks (GANs). However, it's still very challenging for translation tasks that require high quality, especially at high-resolution and photo-reality. In this work, we present Discriminative Region Proposal Adversarial Network (DRPAN) for high-quality image-to-image translation. We decompose the image-to-image translation procedure into three iterated steps: the first is to generate an image with global structure but some local artifacts (via GAN), the second is to use our Discriminative Region Proposal network (DRPnet) for proposing the most fake region from the generated image, and the third is to implement "image inpainting" on the most fake region for yielding more realistic result through a reviser, so that the system (DRPAN) can be gradually optimized to synthesize images with more attention on the most artifact local part. We explore patch-based GAN to construct DRPnet for proposing the discriminative region to produce masked fake samples, further, we propose a reviser for GANs to distinguish real from masked fake for providing constructive revisions to the generator for producing realistic details, and serve as auxiliaries of the generator to synthesize high-quality results. In addition, we combine pix2pixHD with DRPAN to synthesize high-resolution results with much finer details. Moreover, we improve CycleGAN by DRPAN to address unpaired image-to-image translation with better semantic alignment. Experiments on a variety of paired and unpaired image-to-image translation tasks validate that our method outperforms the state of the art for synthesizing high-quality translation results in terms of both human perceptual studies and automatic quantitative measures. Our code is available at .	[Wang, Chao; Niu, Wenjie; Jiang, Yufeng; Zheng, Haiyong; Yu, Zhibin; Gu, Zhaorui; Zheng, Bing] Ocean Univ China, Dept Elect Engn, Qingdao, Peoples R China; [Zheng, Haiyong] Univ Dundee, Dept Math, Dundee, Scotland	Ocean University of China; University of Dundee	Zheng, HY; Yu, ZB (corresponding author), Ocean Univ China, Dept Elect Engn, Qingdao, Peoples R China.; Zheng, HY (corresponding author), Univ Dundee, Dept Math, Dundee, Scotland.	chaowangplus@gmail.com; winnie_niuwj@163.com; jiangyufeng77@163.com; zhenghaiyong@ouc.edu.cn; yuzhibin@ouc.edu.cn; guzhaoiui@ouc.edu.cn; bingzh@ouc.edu.cn	Yu, Zhibin/Z-1138-2019; Winnie, Niu/GZK-4646-2022; Zheng, Haiyong/I-7771-2014	Yu, Zhibin/0000-0003-4372-1767; Zheng, Haiyong/0000-0002-8027-0734	National Natural Science Foundation of China [61771440, 41776113]; China Scholarship Council [201806335022]; Qingdao Municipal Science and Technology Program [17-1-1-5-jch]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Scholarship Council(China Scholarship Council); Qingdao Municipal Science and Technology Program	The authors would like to thank the pioneer researchers in GAN and image-to-image translation fields. The authors would also like to express their sincere appreciation to the guest editors and anonymous reviewers. This work was supported in part by the National Natural Science Foundation of China under Grants 61771440 and 41776113, in part by the China Scholarship Council under Grant 201806335022, and in part by the Qingdao Municipal Science and Technology Program under Grant 17-1-1-5-jch.	[Anonymous], 2018, ARXIV180304469; Arjovsky M, 2017, PR MACH LEARN RES, V70; Atallah L, 2018, INT CONF WEARAB IMPL, P70; Baroncini V., 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P564; Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deshpande A, 2015, IEEE I CONF COMP VIS, P567, DOI 10.1109/ICCV.2015.72; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dosovitskiy Alexey, 2016, NEURIPS; Durugkar Ishan, 2017, ICLR; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hensel M, 2017, ADV NEUR IN, V30; Hong Y, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3301282; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kheirkhah P, 2017, IEEE IJCNN, P4467, DOI 10.1109/IJCNN.2017.7966422; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kim Taeksoo, 2017, P 34 INT C MACH LEAR, P1857, DOI [10.5555/3305381.3305573, DOI 10.5555/3305381.3305573]; Kodali Naveen, 2017, ARXIV170507215; Kurach K., 2018, ARXIV180704720; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Liu Ming-Yu, 2017, NIPS; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mirza M., 2014, ARXIV; Nair V, 2010, P 27 INT C MACHINE L, P807; Odena A, 2017, PR MACH LEARN RES, V70; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Qi Guo-Jun, 2017, ARXIV170106264; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Salimans T, 2016, ADV NEUR IN, V29; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39; Ulyanov D., 2016, ARXIV160708022; Wang CY, 2018, IEEE T IMAGE PROCESS, V27, P4066, DOI 10.1109/TIP.2018.2836316; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32; Zhang H., 2017, ICCV; Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256; Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	65	2	2	2	30	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2366	2385		10.1007/s11263-019-01273-2	http://dx.doi.org/10.1007/s11263-019-01273-2		DEC 2019	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY					2022-12-18	WOS:000507367900001
J	Mukundan, A; Tolias, G; Bursuc, A; Jegou, H; Chum, O				Mukundan, Arun; Tolias, Giorgos; Bursuc, Andrei; Jegou, Herve; Chum, Ondrej			Understanding and Improving Kernel Local Descriptors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							REPRESENTATION; SCALE; SHAPE	We propose a multiple-kernel local-patch descriptor based on efficient match kernels from pixel gradients. It combines two parametrizations of gradient position and direction, each parametrization provides robustness to a different type of patch mis-registration: polar parametrization for noise in the patch dominant orientation detection, Cartesian for imprecise location of the feature point. Combined with whitening of the descriptor space, that is learned with or without supervision, the performance is significantly improved. We analyze the effect of the whitening on patch similarity and demonstrate its semantic meaning. Our unsupervised variant is the best performing descriptor constructed without the need of labeled data. Despite the simplicity of the proposed descriptor, it competes well with deep learning approaches on a number of different tasks.	[Mukundan, Arun; Tolias, Giorgos; Chum, Ondrej] Czech Tech Univ, FEE, VRG, Prague, Czech Republic; [Bursuc, Andrei] Valeo Ai, Paris, France; [Jegou, Herve] Facebook AI Res, Paris, France	Czech Technical University Prague; Facebook Inc	Mukundan, A (corresponding author), Czech Tech Univ, FEE, VRG, Prague, Czech Republic.	arun.mukundan@cmp.felk.cvut.cz; giorgos.tolias@cmp.felk.cvut.cz; andrei.bursuc@valeo.com; rvj@fb.com; ondra.chum@cmp.felk.cvut.cz			OP VVV Project [CZ.02.1.01/0.0/0.0/16_019/0000765]; MSMT [LL1303 ERC-CZ]; CTU student Grant [SGS17/185/OHK3/3T/13]	OP VVV Project; MSMT(Ministry of Education, Youth & Sports - Czech Republic); CTU student Grant	The authors were supported by the OP VVV funded Project CZ.02.1.01/0.0/0.0/16_019/0000765 "Research Center for Informatics" and MSMT LL1303 ERC-CZ Grant. Arun Mukundan was supported by the CTU student Grant SGS17/185/OHK3/3T/13. We would like to thank Karel Lenc and Vassileios Balntas for their valuable help with the HPatches benchmark, and Dmytro Mishkin for providing the L2Net and HardNet descriptors for the HPatches dataset.	Ahonen T, 2009, LECT NOTES COMPUT SC, V5575, P61, DOI 10.1007/978-3-642-02230-2_7; Alahi Alexandre, 2012, CVPR, V2; Ambai M., 2011, ICCV; Arandjelovic R., 2012, CVPR; Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410; Balntas Vassileios, 2016, ARXIV160105030; Balntas Vassileios, 2016, BMVC, V2, DOI DOI 10.5244/C.30.119; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Bo L., 2011, IROS; Bo L., 2009, NIPS; Brown M, 2005, PROC CVPR IEEE, P510; Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54; Bursuc A., 2015, ICMR; Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56; Chum O, 2015, ICCV; Delhumeau Jonathan, 2013, ACM MM, P653, DOI DOI 10.1145/2502081.2502171; Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145; Forssen PE, 2007, IEEE I CONF COMP VIS, P1530; Fox D., 2010, ADV NEURAL INFORM PR, V23, P244; Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Heikkila M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014; Heinly J., 2015, CVPR; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Ke Y, 2004, PROC CVPR IEEE, P506; Kokkinos I., 2008, CVPR; Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151; Ledoit O, 2004, J PORTFOLIO MANAGE, V30, P110, DOI 10.3905/jpm.2004.110; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mahendran A, 2016, INT J COMPUT VISION, V120, P233, DOI 10.1007/s11263-016-0911-8; Mairal J., 2014, ADV NEURAL INFORM PR, V27, P2627; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mikolajczyk K., 2007, ICCV; Mishchuk Anastasiya, 2017, ADV NEURAL INFORM PR; Mishkin D., 2015, ARXIV150406603; Mukundan A., 2017, BMVC; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Paulin M, 2017, INT J COMPUT VISION, V121, P149, DOI 10.1007/s11263-016-0924-3; Paulin M, 2015, IEEE I CONF COMP VIS, P91, DOI 10.1109/ICCV.2015.19; Philbin J., 2010, EUR C COMP VIS; Radenovi F., 2016, ECCV; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215; Schonberger J. L., 2015, CVPR; Schonberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Scovanner P., 2007, ACM MM, P357; Shechtman E, 2007, PROC CVPR IEEE, P1744; Simo-Serra E, 2015, ICCV; Simonyan K, 2014, IEEE T PATTERN ANAL, V36, P1573, DOI 10.1109/TPAMI.2014.2301163; Taira H., 2016, ICPR; Tian B. F. Y., 2017, CVPR; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Tolias G, 2015, COMPUT VIS IMAGE UND, V140, P9, DOI 10.1016/j.cviu.2015.06.007; Trzcinski T., 2012, NIPS; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; Vedaldi A., 2010, CVPR; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wang P., 2013, CVPR; Winder S., 2007, CVPR; Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Yosinski J., 2015, ICML DEEP LEARN WORK; Yu GS, 2009, INT CONF ACOUST SPEE, P1597, DOI 10.1109/ICASSP.2009.4959904; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou L, 2017, IEEE I CONF COMP VIS, P2381, DOI 10.1109/ICCV.2017.259	72	2	2	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1723	1737		10.1007/s11263-018-1137-8	http://dx.doi.org/10.1007/s11263-018-1137-8			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY		Green Submitted			2022-12-18	WOS:000492425300009
J	Yan, Y; Xu, CL; Cai, DW; Corso, JJ				Yan, Yan; Xu, Chenliang; Cai, Dawen; Corso, Jason J.			A Weakly Supervised Multi-task Ranking Framework for Actor-Action Semantic Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Weakly supervised learning; Actor-action semantic segmentation; Multi-task ranking		Modeling human behaviors and activity patterns has attracted significant research interest in recent years. In order to accurately model human behaviors, we need to perform fine-grained human activity understanding in videos. Fine-grained activity understanding in videos has attracted considerable recent attention with a shift from action classification to detailed actor and action understanding that provides compelling results for perceptual needs of cutting-edge autonomous systems. However, current methods for detailed understanding of actor and action have significant limitations: they require large amounts of finely labeled data, and they fail to capture any internal relationship among actors and actions. To address these issues, in this paper, we propose a novel Schatten p-norm robust multi-task ranking model for weakly-supervised actor-action segmentation where only video-level tags are given for training samples. Our model is able to share useful information among different actors and actions while learning a ranking matrix to select representative supervoxels for actors and actions respectively. Final segmentation results are generated by a conditional random field that considers various ranking scores for video parts. Extensive experimental results on both the actor-action dataset and the Youtube-objects dataset demonstrate that the proposed approach outperforms the state-of-the-art weakly supervised methods and performs as well as the top-performing fully supervised method.	[Yan, Yan] Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA; [Xu, Chenliang] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA; [Cai, Dawen] Univ Michigan, Dept Cell & Dev Biol, Biophys, Ann Arbor, MI 48109 USA; [Corso, Jason J.] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA	Texas State University System; Texas State University San Marcos; University of Rochester; University of Michigan System; University of Michigan; University of Michigan System; University of Michigan	Yan, Y (corresponding author), Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.; Cai, DW (corresponding author), Univ Michigan, Dept Cell & Dev Biol, Biophys, Ann Arbor, MI 48109 USA.	y_y34@txstate.edu; chenliang.xu@rochester.edu; dwcai@umich.edu; jjcorso@umich.edu			University of Michigan MiBrain Grant; DARPA [FA8750-17-2-0112]; National Institute of Standards and Technology [60NANB17D191]; NSF [NeTS-1909185, CSR-1908658, IIS-1741472, IIS-1813709]	University of Michigan MiBrain Grant; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); National Institute of Standards and Technology(National Institute of Standards & Technology (NIST) - USA); NSF(National Science Foundation (NSF))	This research was partially supported by a University of Michigan MiBrain Grant (DC, JC), DARPA FA8750-17-2-0112 (JC), National Institute of Standards and Technology Grant 60NANB17D191 (JC, YY), NSF IIS-1741472 and IIS-1813709 (CX), NSF NeTS-1909185 and CSR-1908658 (YY), and gift donation from Cisco Inc (YY). This article solely reflects the opinions and conclusions of its authors and not the funding agents.	Abu-El-Haija S, 2016, YOUTUBE 8M LARGE SCA; Amini M. R., 2008, SIGIR; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2015, CVPR; Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Bojanowski P, 2014, LECT NOTES COMPUT SC, V8693, P628, DOI 10.1007/978-3-319-10602-1_41; Brendel W., 2009, ICCV; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Chao Y. W., 2015, CVPR; Chen J., 2011, ACM SIGKDD C KNOWL D; Chen W., 2015, ICCV; Chiu W. C., 2013, CVPR; Corso JJ, 2008, IEEE T MED IMAGING, V27, P629, DOI 10.1109/TMI.2007.912817; Courtney PG, 2015, IEEE COMP SEMICON; Dang K., 2018, BMVC; Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z; Deselaers T, 2012, INT J COMPUT VISION, V100, P275, DOI 10.1007/s11263-012-0538-3; Dp B, 1996, CONSTRAINED OPTIMIZA; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Fu Huazhu, 2014, IEEE C COMP VIS PATT; Fulkerson B., 2009, IEEE I CONF COMP VIS, P670, DOI [10.1109/ICCV.2009.5459175, DOI 10.1109/ICCV.2009.5459175]; Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1; Galasso F., 2012, P AS C COMP VIS; Gavrilyuk K, 2018, PROC CVPR IEEE, P5958, DOI 10.1109/CVPR.2018.00624; Geest R. D., 2016, ECCV; Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384; Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893; Guo J., 2013, ICCV; Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83; Hartmann G, 2012, LECT NOTES COMPUT SC, V7583, P198, DOI 10.1007/978-3-642-33863-2_20; Iwashita Y., 2014, IEEE INT C PATT REC; Jacob L, 2008, ARXIV PREPRINT ARXIV; Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100; Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43; Jalali A., 2010, ADV NEURAL INF PROCE, V23, P964; Ji JW, 2018, LECT NOTES COMPUT SC, V11208, P734, DOI 10.1007/978-3-030-01225-0_43; Joachims T, 2006, ACM SIGKDD C KNOWL D; Joulin A, 2014, LECT NOTES COMPUT SC, V8694, P253, DOI 10.1007/978-3-319-10599-4_17; Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Kumar M., 2005, ICCV; Kundu A, 2016, PROC CVPR IEEE, P3168, DOI 10.1109/CVPR.2016.345; Ladicky L, 2014, IEEE T PATTERN ANAL, V36, P1056, DOI 10.1109/TPAMI.2013.165; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Lea Colin, 2016, ECCV; Lezama J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3369, DOI 10.1109/CVPR.2011.6044588; Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348; Liu T-Y., 2009, FOUND TRENDS INF RET, V3, P225, DOI DOI 10.1561/1500000016; Liu X, 2014, PROC CVPR IEEE, P57, DOI 10.1109/CVPR.2014.15; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Lu JW, 2015, PROC CVPR IEEE, P1137, DOI 10.1109/CVPR.2015.7298717; Luo Y, 2013, IEEE T IMAGE PROCESS, V22, P523, DOI 10.1109/TIP.2012.2218825; Mettes Pascal, 2016, ECCV; Mosabbeb E. A., 2014, P AS C COMP VIS; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Paris S, 2008, LECT NOTES COMPUT SC, V5303, P460, DOI 10.1007/978-3-540-88688-4_34; Peng X, 2016, LECT NOTES COMPUT SC, V9905, P38, DOI 10.1007/978-3-319-46448-0_3; Pinto L, 2016, LECT NOTES COMPUT SC, V9906, P3, DOI 10.1007/978-3-319-46475-6_1; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361; Salakhutdinov R, 2011, PROC CVPR IEEE, P1481, DOI 10.1109/CVPR.2011.5995720; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Sculley D, 2010, KDD; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Song Y. C., 2016, INT JOINT C ART INT; Soomro K., 2016, CVPR; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tang K., 2013, CVPR; Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190; Tian Y., 2013, CVPR; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tsai YH, 2016, LECT NOTES COMPUT SC, V9908, P760, DOI 10.1007/978-3-319-46493-0_46; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang L, 2014, LECT NOTES COMPUT SC, V8692, P640, DOI 10.1007/978-3-319-10593-2_42; Xiong C., 2012, ACM INT WORKSH MULT; Xu C., 2012, CVPR; Xu CL, 2016, INT J COMPUT VISION, V119, P272, DOI 10.1007/s11263-016-0906-5; Xu CL, 2015, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2015.7298839; Xu JL, 2016, PROC CVPR IEEE, P5356, DOI 10.1109/CVPR.2016.578; YAN Y, 2017, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2017.738; Yan Y, 2016, IEEE T PATTERN ANAL, V38, P1070, DOI 10.1109/TPAMI.2015.2477843; Yan Y, 2014, IEEE T IMAGE PROCESS, V23, P5599, DOI 10.1109/TIP.2014.2365699; Yan Y, 2013, IEEE I CONF COMP VIS, P1177, DOI 10.1109/ICCV.2013.150; Yang YZ, 2015, AAAI CONF ARTIF INTE, P3686; Yu S., 2007, ICML; Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337; Yunbo Cao, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P186; Zhang D., 2014, ECCV; Zhang D., 2017, CVPR; Zhang Y, 2010, PROCEEDINGS OF THE ASME 29TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2010, VOL 6, P733; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhong G., 2016, ACCV; Zhou J., 2011, MALSAR MULTI TASK LE; Zhou Jiayu, 2011, Adv Neural Inf Process Syst, V2011, P702	99	2	2	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1414	1432		10.1007/s11263-019-01244-7	http://dx.doi.org/10.1007/s11263-019-01244-7		OCT 2019	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL9ON					2022-12-18	WOS:000492718000001
J	Wu, A; Piergiovanni, AJ; Ryoo, MS				Wu, A.; Piergiovanni, A. J.; Ryoo, M. S.			Model-Based Robot Imitation with Future Image Similarity	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Robot action policy learning; Behavioral cloning; Model-based RL	TIME	We present a visual imitation learning framework that enables learning of robot action policies solely based on expert samples without any robot trials. Robot exploration and on-policy trials in a real-world environment could often be expensive/dangerous. We present a new approach to address this problem by learning a future scene prediction model solely from a collection of expert trajectories consisting of unlabeled example videos and actions, and by enabling action selection using future image similarity. In this approach, the robot learns to visually imagine the consequences of taking an action, and obtains the policy by evaluating how similar the predicted future image is to an expert sample. We develop an action-conditioned convolutional autoencoder, and present how we take advantage of future images for zero-online-trial imitation learning. We conduct experiments in simulated and real-life environments using a ground mobility robot with and without obstacles in reaching target objects. We explicitly compare our models to multiple baseline methods requiring only offline samples. The results confirm that our proposed methods perform superior to previous methods, including 1.5 x and 2.5 x higher success rate in two different tasks than behavioral cloning.	[Wu, A.; Piergiovanni, A. J.; Ryoo, M. S.] Indiana Univ, Bloomington, IN 47405 USA; [Ryoo, M. S.] SUNY Stony Brook, Stony Brook, NY 11794 USA	Indiana University System; Indiana University Bloomington; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Wu, A (corresponding author), Indiana Univ, Bloomington, IN 47405 USA.	alanwu@iu.edu; ajpiergi@indiana.edu; mryoo@indiana.edu						Abbeel P., 2004, P 21 INT C MACHINE L, P1; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Babaeizadeh Mohammad, 2017, ARXIV171011252; Baram N, 2017, PR MACH LEARN RES, V70; Bojarski Mariusz, 2016, arXiv; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chao Y. W., 2016, IEEE C COMP VIS PATT; Chiappa S., 2017, RECURRENT ENV SIMULA; Denton E, 2018, PR MACH LEARN RES, V80; Dosovitskiy A, 2017, IEEE T PATTERN ANAL, V39, P692, DOI 10.1109/TPAMI.2016.2567384; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Finn C, 2016, PR MACH LEARN RES, V48; Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024; Ho J, 2016, PR MACH LEARN RES, V48; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kingma D.P, P 3 INT C LEARNING R; LASKEY M, 2017, P C ROBOT LEARNING, P143; Lee Jangwon, 2017, IEEE RSJ INT C INT R; Levine S, 2017, SPR PROC ADV ROBOT, V1, P173, DOI 10.1007/978-3-319-50115-4_16; Liu YX, 2018, IEEE INT CONF ROBOT, P1118; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pathak D, 2018, IEEE COMPUT SOC CONF, P2131, DOI 10.1109/CVPRW.2018.00278; Peng XB, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275014; Piergiovanni AJ, 2018, PROC CVPR IEEE, P5304, DOI 10.1109/CVPR.2018.00556; Pomerleau D.A., 1989, ALVINN AUTONOMOUS LA; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ross St<prime>ephane, 2011, AISTATS; Sadeghi F., 2017, ARXIV171207642; Salvadora S, 2007, INTELL DATA ANAL, V11, P561, DOI 10.3233/IDA-2007-11508; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Torabi F, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4950; Vakanski A, 2012, IEEE T SYST MAN CY B, V42, P1039, DOI 10.1109/TSMCB.2012.2185694; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361; Walker J, 2014, PROC CVPR IEEE, P3302, DOI 10.1109/CVPR.2014.416; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wulfmeier M., 2015, ARXIV150704888; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18; Zhu Y, 2016, 2017 IEEE INT C ROB; Ziebart B. D., 2008, AAAI, V8, P1433	48	2	2	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1360	1374		10.1007/s11263-019-01238-5	http://dx.doi.org/10.1007/s11263-019-01238-5		OCT 2019	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW					2022-12-18	WOS:000505388700001
J	Sun, R; Lampert, CH				Sun, Remy; Lampert, Christoph H.			KS(conf): A Light-Weight Test if a Multiclass Classifier Operates Outside of Its Specifications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-class classification; Specifications; Distribution shift; Deep convolutional networks	BHATTACHARYYA DISTANCE MEASURES; DIVERGENCE	We study the problem of automatically detecting if a given multi-class classifier operates outside of its specifications (out-of-specs), i.e. on input data from a different distribution than what it was trained for. This is an important problem to solve on the road towards creating reliable computer vision systems for real-world applications, because the quality of a classifier's predictions cannot be guaranteed if it operates out-of-specs. Previously proposed methods for out-of-specs detection make decisions on the level of single inputs. This, however, is insufficient to achieve low false positive rate and high false negative rates at the same time. In this work, we describe a new procedure named KS(conf), based on statistical reasoning. Its main component is a classical Kolmogorov-Smirnov test that is applied to the set of predicted confidence values for batches of samples. Working with batches instead of single samples allows increasing the true positive rate without negatively affecting the false positive rate, thereby overcoming a crucial limitation of single sample tests. We show by extensive experiments using a variety of convolutional network architectures and datasets that KS(conf) reliably detects out-of-specs situations even under conditions where other tests fail. It furthermore has a number of properties that make it an excellent candidate for practical deployment: it is easy to implement, adds almost no overhead to the system, works with any classifier that outputs confidence scores, and requires no a priori knowledge about how the data distribution could change.	[Sun, Remy] ENS Rennes, Bruz, France; [Lampert, Christoph H.] IST Austria, Klosterneuburg, Austria	Ecole Normale Superieure de Rennes (ENS Rennes); Institute of Science & Technology - Austria	Lampert, CH (corresponding author), IST Austria, Klosterneuburg, Austria.	remy.sun@ens-rennes.fr; chl@ist.ac.at	Lampert, Christoph H./J-2931-2014	Lampert, Christoph H./0000-0001-8622-7887	Institute of Science and Technology (IST Austria)	Institute of Science and Technology (IST Austria)	Open access funding provided by Institute of Science and Technology (IST Austria). The authors acknowledge the use of GNU parallel (Tange 2018).	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bansal A, 2014, LECT NOTES COMPUT SC, V8694, P366, DOI 10.1007/978-3-319-10599-4_24; Basseville M., 1993, DETECTION ABRUPT CHA; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173; Bendale A, 2015, PROC CVPR IEEE, P1893, DOI 10.1109/CVPR.2015.7298799; Blitzer J., 2006, P 2006 C EMP METH NA, P120, DOI DOI 10.3115/1610075.1610094; Cvach Maria, 2012, Biomed Instrum Technol, V46, P268, DOI 10.2345/0899-8205-46.4.268; Daftry S, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1743, DOI 10.1109/IROS.2016.7759279; DeVries Terrance, 2018, ARXIV180204865; dos Reis D, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1545, DOI 10.1145/2939672.2939836; Dunning T., 2014, COMPUTING EXTREMELY; EDWORTHY J, 1994, APPL ERGON, V25, P202, DOI 10.1016/0003-6870(94)90001-9; Gama J, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2523813; Ganin Y., 2015, INT C MACH LEARN ICM; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Hakkinen M., 2010, THESIS; Harel M, 2014, PR MACH LEARN RES, V32, P1009; Hawkins DM., 1980, IDENTIFICATION OUTLI, DOI DOI 10.1007/978-94-015-3994-4; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hodge VJ, 2004, ARTIF INTELL REV, V22, P85, DOI 10.1023/B:AIRE.0000045502.10941.a9; Howard A.G., 2017, MOBILENETS EFFICIENT; Iandola F.N., 2016, ARXIV; Jain LP, 2014, LECT NOTES COMPUT SC, V8691, P393, DOI 10.1007/978-3-319-10578-9_26; Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Knorr E. M., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, P219; Konstantinov N, 2019, PR MACH LEARN RES, V97; Kuncheva LI, 2014, IEEE T NEUR NET LEAR, V25, P69, DOI 10.1109/TNNLS.2013.2248094; Kuzborskij I, 2013, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2013.431; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lee Kimin, 2018, INT C LEARN REPR; Liang Shiyu, 2018, INT C LEARN REPR ICL, P1; Long MS, 2015, PR MACH LEARN RES, V37, P97; Louizos C, 2017, PR MACH LEARN RES, V70; Marsaglia G., 2003, J STAT SOFTW, V8, P1, DOI [10.18637/jss.v008.i18, DOI 10.18637/JSS.V008.I18]; MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095; Palatucci Mark, 2009, ADV NEURAL INFORM PR, P1410; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Platt JC, 2000, ADV NEUR IN, P61; POLLAK M, 1985, ANN STAT, V13, P206, DOI 10.1214/aos/1176346587; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Rebuffi S. A., 2016, C COMP VIS PATT REC; Royer A, 2015, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2015.7298746; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Scheirer WJ, 2011, IEEE T PATTERN ANAL, V33, P1689, DOI 10.1109/TPAMI.2011.54; Sethi TS, 2016, J INTELL INF SYST, V46, P179, DOI 10.1007/s10844-015-0358-3; Sun R., 2018, GERM C PATT REC GCPR; Tange O, 2018, GNU PARALLEL, DOI [10.5281/zenodo.1146014, DOI 10.5281/ZENODO.1146014]; Tax, 2001, ONE CLASS CLASSIFICA; Tax D. M. J., 1998, Advances in Pattern Recognition. Joint IAPR International Workshops SSPR'98 and SPR'98. Proceedings, P593, DOI 10.1007/BFb0033283; Tommasi T., 2012, AS C COMP VIS ACCV; TOUSSAINT GT, 1972, IEEE T COMMUN, VCO20, P485, DOI 10.1109/TCOM.1972.1091157; Wang H, 2015, IEEE IJCNN; Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Zliobaite Indre, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining Workshops (ICDMW 2010), P843, DOI 10.1109/ICDMW.2010.49; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	67	2	2	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		970	995		10.1007/s11263-019-01232-x	http://dx.doi.org/10.1007/s11263-019-01232-x		OCT 2019	26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN	32313381	Green Published, hybrid			2022-12-18	WOS:000494406800001
J	Mustafa, A; Hilton, A				Mustafa, Armin; Hilton, Adrian			Semantically Coherent 4D Scene Flow of Dynamic Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic 4D flow; Reconstruction; Segmentation	ENERGY MINIMIZATION; FEATURES	Simultaneous semantically coherent object-based long-term 4D scene flow estimation, co-segmentation and reconstruction is proposed exploiting the coherence in semantic class labels both spatially, between views at a single time instant, and temporally, between widely spaced time instants of dynamic objects with similar shape and appearance. In this paper we propose a framework for spatially and temporally coherent semantic 4D scene flow of general dynamic scenes from multiple view videos captured with a network of static or moving cameras. Semantic coherence results in improved 4D scene flow estimation, segmentation and reconstruction for complex dynamic scenes. Semantic tracklets are introduced to robustly initialize the scene flow in the joint estimation and enforce temporal coherence in 4D flow, semantic labelling and reconstruction between widely spaced instances of dynamic objects. Tracklets of dynamic objects enable unsupervised learning of long-term flow, appearance and shape priors that are exploited in semantically coherent 4D scene flow estimation, co-segmentation and reconstruction. Comprehensive performance evaluation against state-of-the-art techniques on challenging indoor and outdoor sequences with hand-held moving cameras shows improved accuracy in 4D scene flow, segmentation, temporally coherent semantic labelling, and reconstruction of dynamic scenes.	[Mustafa, Armin; Hilton, Adrian] Univ Surrey, CVSSP, Guildford, Surrey, England	University of Surrey	Mustafa, A (corresponding author), Univ Surrey, CVSSP, Guildford, Surrey, England.	a.mustafa@surrey.ac.uk; a.hilton@surrey.ac.uk	; Hilton, Adrian/N-3736-2014	Mustafa, Armin/0000-0002-1779-2775; Hilton, Adrian/0000-0003-4223-238X	Royal Academy of Engineering Research Fellowship [RF-201718-17177]; EPSRC Platform Grant on Audio-Visual Media Research [EP/P022529]; Helge Rhodin; EPSRC [EP/M028321/1, EP/P022529/1] Funding Source: UKRI	Royal Academy of Engineering Research Fellowship(Royal Academy of Engineering - UK); EPSRC Platform Grant on Audio-Visual Media Research(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Helge Rhodin; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This research was supported by the Royal Academy of Engineering Research Fellowship RF-201718-17177, and the EPSRC Platform Grant on Audio-Visual Media Research EP/P022529. We would like to thank Helge Rhodin and Abdelaziz Djelouah for providing their data.	[Anonymous], MULT VID REP; Ballan L, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778824; Bao Y., 2013, IEEE INT C COMP VIS; Basha T, 2010, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2010.5539791; Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080; Beeler T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964970; Behl A, 2017, IEEE I CONF COMP VIS, P2593, DOI 10.1109/ICCV.2017.281; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chen L.-C., 2014, ARXIV PREPRINT ARXIV; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen PY, 2019, PROC CVPR IEEE, P2619, DOI 10.1109/CVPR.2019.00273; Chiu W. C., 2013, CVPR; Djelouah A., 2016, INTERNATIONAL CONFER; Djelouah A, 2015, IEEE T PATTERN ANAL, V37, P1890, DOI 10.1109/TPAMI.2014.2385704; Engelmann F, 2016, LECT NOTES COMPUT SC, V9796, P219, DOI 10.1007/978-3-319-45886-1_18; Evangelidis GD, 2008, IEEE T PATTERN ANAL, V30, P1858, DOI 10.1109/TPAMI.2008.113; Everingham M., 2012, PASCAL VISUAL OBJECT; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Floros G, 2012, PROC CVPR IEEE, P2823, DOI 10.1109/CVPR.2012.6248007; Fu Huazhu, 2014, IEEE C COMP VIS PATT; Goldluecke B, 2004, PROC CVPR IEEE, P350; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Hane C., 2013, IEEE C COMP VIS PATT; Hane C, 2017, IEEE T PATTERN ANAL, V39, P1730, DOI 10.1109/TPAMI.2016.2613051; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He K., 2017, ARXIV170306870 CORR; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jiao JB, 2018, LECT NOTES COMPUT SC, V11219, P55, DOI 10.1007/978-3-030-01267-0_4; Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kendall Alex, 2017, MULTITASK LEARNING U, P3; Khoreva A, 2019, INT J COMPUT VISION, V127, P1175, DOI 10.1007/s11263-019-01164-6; Kim H, 2012, IEEE T CIRC SYST VID, V22, P1611, DOI 10.1109/TCSVT.2012.2202185; Kolev K, 2012, IEEE T PATTERN ANAL, V34, P493, DOI 10.1109/TPAMI.2011.150; Kundu A, 2016, PROC CVPR IEEE, P3168, DOI 10.1109/CVPR.2016.345; Kundu A, 2014, LECT NOTES COMPUT SC, V8694, P703, DOI 10.1007/978-3-319-10599-4_45; Langguth F, 2016, LECT NOTES COMPUT SC, V9907, P469, DOI 10.1007/978-3-319-46487-9_29; Larsen ES, 2007, IEEE I CONF COMP VIS, P1440; Li PL, 2018, LECT NOTES COMPUT SC, V11206, P664, DOI 10.1007/978-3-030-01216-8_40; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1187, DOI 10.1145/2733373.2806313; Maninis KK, 2018, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2018.00071; Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959; Mustafa A., 2017, 3DV; Mustafa A., 2016, IEEE C COMP VIS PATT; Mustafa A, 2019, IEEE T IMAGE PROCESS, V28, P1118, DOI 10.1109/TIP.2018.2872906; Mustafa A, 2017, PROC CVPR IEEE, P5583, DOI 10.1109/CVPR.2017.592; Mustafa A, 2016, LECT NOTES COMPUT SC, V9905, P213, DOI 10.1007/978-3-319-46448-0_13; Prada F, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925967; Ranjan A., 2018, IEEE CONFERENCE ON C; Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720; Rhodin H, 2016, LECT NOTES COMPUT SC, V9909, P509, DOI 10.1007/978-3-319-46454-1_31; Rother C., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91; Roussos A., 2012, IEEE INT S MIX AUGM; Rusu R. B., 2009, THESIS; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Semerjian B, 2014, LECT NOTES COMPUT SC, V8694, P719, DOI 10.1007/978-3-319-10599-4_46; Sevilla-Lara L., 2016, ARXIV160303911 CORR; Sevilla-Lara L, 2016, PROC CVPR IEEE, P3889, DOI 10.1109/CVPR.2016.422; Tao M, 2012, COMPUT GRAPH FORUM, V31, P345, DOI 10.1111/j.1467-8659.2012.03013.x; Tokmakov P, 2019, INT J COMPUT VISION, V127, P282, DOI 10.1007/s11263-018-1122-2; Tsai YH, 2016, LECT NOTES COMPUT SC, V9908, P760, DOI 10.1007/978-3-319-46493-0_46; Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423; Tulsiani Shubham, 2018, CVPR, DOI DOI 10.1109/CVPR.2018.00306; Vineet V, 2015, IEEE INT CONF ROBOT, P75, DOI 10.1109/ICRA.2015.7138983; Wedel A, 2011, INT J COMPUT VISION, V95, P29, DOI 10.1007/s11263-010-0404-0; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Xie J., 2016, IEEE C COMP VIS PATT; Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229; Zanfir A, 2015, IEEE I CONF COMP VIS, P4417, DOI 10.1109/ICCV.2015.502; Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441; Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	84	2	2	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					319	335		10.1007/s11263-019-01241-w	http://dx.doi.org/10.1007/s11263-019-01241-w		OCT 2019	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KJ1GX		Green Published, hybrid			2022-12-18	WOS:000489440600002
J	Sharif, N; White, L; Bennamoun, M; Liu, W; Shah, SAA				Sharif, Naeha; White, Lyndon; Bennamoun, Mohammed; Liu, Wei; Shah, Syed Afaq Ali			LCEval: Learned Composite Metric for Caption Evaluation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image captioning; Automatic evaluation metric; Neural networks; Learned metrics; Correlation; Accuracy; Robustness		Automatic evaluation metrics hold a fundamental importance in the development and fine-grained analysis of captioning systems. While current evaluation metrics tend to achieve an acceptable correlation with human judgements at the system level, they fail to do so at the caption level. In this work, we propose a neural network-based learned metric to improve the caption-level caption evaluation. To get a deeper insight into the parameters which impact a learned metric's performance, this paper investigates the relationship between different linguistic features and the caption-level correlation of the learned metrics. We also compare metrics trained with different training examples to measure the variations in their evaluation. Moreover, we perform a robustness analysis, which highlights the sensitivity of learned and handcrafted metrics to various sentence perturbations. Our empirical analysis shows that our proposed metric not only outperforms the existing metrics in terms of caption-level correlation but it also shows a strong system-level correlation against human assessments.	[Sharif, Naeha; Bennamoun, Mohammed; Liu, Wei] Univ Western Australia, Dept Comp Sci, Perth, WA, Australia; [White, Lyndon] Invenia Labs, Cambridge, England; [Shah, Syed Afaq Ali] Murdoch Univ, Discipline Informat Technol Math & Stat, Perth, WA, Australia	University of Western Australia; Murdoch University	Sharif, N (corresponding author), Univ Western Australia, Dept Comp Sci, Perth, WA, Australia.	naeha.sharif@research.uwa.edu.au; lyndon.white@invenialabs.co.uk; mohammed.bennamoun@uwa.edu.au; wei.liu@uwa.edu.au; Afaq.Shah@murdoch.edu.au	Bennamoun, Mohammed/C-2789-2013	Bennamoun, Mohammed/0000-0002-6603-3257; Liu, Wei/0000-0002-7409-0948; Shah, Syed Afaq Ali/0000-0003-2181-8445; Sharif, Naeha/0000-0001-8351-6288	Australian Research Council, ARC [DP150100294]	Australian Research Council, ARC(Australian Research Council)	We are grateful to NVIDIA for providing TitanXp GPU, which was used for the experiments. We also thank Somak Aditya for sharing COMPOSITE dataset and Ramakrishna Vedantam for sharing PASCAL50S and ABSTRACT50S datasets. Thanks to Yin Cui for providing the dataset containing the captions of 12 teams who participated in the 2015 COCO captioning challenge. This work is supported by Australian Research Council, ARC DP150100294.	Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/2951913.2976746, 10.1145/3022670.2976746]; Aditya S, 2018, COMPUT VIS IMAGE UND, V173, P33, DOI 10.1016/j.cviu.2017.12.004; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; [Anonymous], P 52 ANN M ASS COMP; [Anonymous], LECT NOTES COMPUTER; [Anonymous], P 20 AUSTR DOC COMP; Banerjee Satanjeev, 2005, P ACL WORKSH INTR EX, P65; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Bojar O., 2017, P 2 C MACH TRANSL JA, P525, DOI DOI 10.18653/V1/W17-4757; Bojar Ondrej, 2016, P 1 C MACH TRANSL SH, V2, P199, DOI DOI 10.18653/V1/W16-2302; Chen X, 2015, CORR, V1504, P325; Corston-Oliver S, 2001, 39TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P140; Cui Y, 2018, PROC CVPR IEEE, P5804, DOI 10.1109/CVPR.2018.00608; Dancey CP., 2004, STAT MATHS PSYCHOL; Denkowski Michael, 2014, P 9 WORKSH STAT MACH, P376, DOI DOI 10.3115/V1/W14-3348; Erdem A., 2016, ARXIV161207600; Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2; Glorot X., 2010, PROC MACH LEARN RES, P249; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hodosh M., 2016, VL, P19; Karpathy A, 2014, ADV NEUR IN, V27; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Khosrovian K, 2008, LECT NOTES COMPUT SC, V5007, P294, DOI 10.1007/978-3-540-79588-9_26; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulesza A., 2004, P 10 INT C THEORETIC, P75; Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162; Lin C.-Y., 2004, ROUGE PACKAGE AUTOMA; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Ding, 2005, P ACL WORKSH INTR EX, P25; Liu S., 2016, P IEEE INT C COMP VI; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Ma Qingsong, 2018, P 3 C MACH TRANSL SH, P671, DOI DOI 10.18653/V1/W18-6450; Mikolov T., 2013, ARXIV; Mitchell Margaret, 2012, EACL; NG AY, 2004, P 21 INT C MACH LEAR, P78, DOI DOI 10.1145/1015330.1015435; Ordonez V, 2016, INT J COMPUT VISION, V119, P46, DOI 10.1007/s11263-015-0840-y; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303; Ritter S., 2015, 4 JOINT C LEX COMP S; Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61; Sharif Naeha, 2018, P ACL 2018 STUD RES, P14; van Miltenburg E., 2017, ARXIV170404198; Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yao T, 2016, ARXIV161101646; You Q, 2018, ARXIV PREPRINT ARXIV; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Young Peter, 2014, T ASSOC COMPUT LING, V2, P67	52	2	2	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1586	1610		10.1007/s11263-019-01206-z	http://dx.doi.org/10.1007/s11263-019-01206-z			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL		Green Submitted			2022-12-18	WOS:000485320300011
J	Gurari, D; Zhao, YN; Jain, SD; Betke, M; Grauman, K				Gurari, Danna; Zhao, Yinan; Jain, Suyog Dutt; Betke, Margrit; Grauman, Kristen			Predicting How to Distribute Work Between Algorithms and Humans to Segment an Image Batch	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Foreground object segmentation; Interactive segmentation; Hybrid human-computer system; Crowdsourcing		Foreground object segmentation is a critical step for many image analysis tasks. While automated methods can produce high-quality results, their failures disappoint users in need of practical solutions. We propose a resource allocation framework for predicting how best to allocate a fixed budget of human annotation effort in order to collect higher quality segmentations for a given batch of images and automated methods. The framework is based on a prediction module that estimates the quality of given algorithm-drawn segmentations. We demonstrate the value of the framework for two novel tasks related to predicting how to distribute annotation efforts between algorithms and humans. Specifically, we develop two systems that automatically decide, for a batch of images, when to recruit humans versus computers to create (1) coarse segmentations required to initialize segmentation tools and (2) final, fine-grained segmentations. Experiments demonstrate the advantage of relying on a mix of human and computer efforts over relying on either resource alone for segmenting objects in images coming from three diverse modalities (visible, phase contrast microscopy, and fluorescence microscopy).	[Gurari, Danna; Zhao, Yinan; Jain, Suyog Dutt; Grauman, Kristen] Univ Texas Austin, 2317 Speedway,Stop,D9500, Austin, TX 78712 USA; [Grauman, Kristen] Facebook AI Res, Menlo Pk, CA USA; [Jain, Suyog Dutt] CognitiveScale, Austin, TX USA; [Betke, Margrit] Boston Univ, 111 Cummington Mall, Boston, MA 02215 USA	University of Texas System; University of Texas Austin; Facebook Inc; Boston University	Gurari, D (corresponding author), Univ Texas Austin, 2317 Speedway,Stop,D9500, Austin, TX 78712 USA.	danna.gurari@ischool.utexas.edu; yinanzhao@utexas.edu; suyog@utexas.edu; betke@bu.edu; grauman@cs.utexas.edu			National Science Foundation [IIS-1421943, IIS-1755593]; Google Faculty Award; AWS Machine Learning Research Award; IBM Faculty Award; IBM Open Collaborative Research Award	National Science Foundation(National Science Foundation (NSF)); Google Faculty Award(Google Incorporated); AWS Machine Learning Research Award; IBM Faculty Award(International Business Machines (IBM)); IBM Open Collaborative Research Award(International Business Machines (IBM))	The authors thank the anonymous crowd workers for participating in our experiments. This work is supported in part by National Science Foundation funding to DG (IIS-1755593), a gift from Adobe to DG, National Science Foundation funding to MB (IIS-1421943), a Google Faculty Award to MB, AWS Machine Learning Research Award to KG, IBM Faculty Award to KG, IBM Open Collaborative Research Award to KG, and a gift from Qualcomm to KG.	Alpert R. B. S., 2007, P IEEE C COMP VIS PA, V0, P1, DOI [DOI 10.1109/CVPR.2007.383017, 10.1109/CVPR.2007.383017]; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080; Bernard O, 2009, IEEE T IMAGE PROCESS, V18, P1179, DOI 10.1109/TIP.2009.2017343; Biswas A, 2013, PROC CVPR IEEE, P644, DOI 10.1109/CVPR.2013.89; Branson S, 2014, INT J COMPUT VISION, V108, P3, DOI 10.1007/s11263-014-0698-4; Carlos A. F. A, 2014, PRODUCELO ESPACO AGE, P53; Carreira J, 2010, PROC CVPR IEEE, P3241, DOI 10.1109/CVPR.2010.5540063; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Chittajallu DR, 2015, NAT METHODS, V12, P577, DOI 10.1038/nmeth.3363; Cui J., 2008, IEEE C COMP VIS PATT, P1; Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Glenn DR, 2015, NAT METHODS, V12, P736, DOI [10.1038/NMETH.3449, 10.1038/nmeth.3449]; Grady L, 2011, IEEE I CONF COMP VIS, P367, DOI 10.1109/ICCV.2011.6126264; Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073; Gurari D., 2014, C MED IM COMP COMP A, P9; Gurari D., 2015, IEEE WINT C APPL COM, P8; Gurari D, 2018, INT J COMPUT VISION, V126, P714, DOI 10.1007/s11263-018-1065-7; Gurari D, 2016, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2016.48; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jain S. D., 2017, 2017 IEEE C COMP VIS, V1; Jain SD, 2013, IEEE I CONF COMP VIS, P1313, DOI 10.1109/ICCV.2013.166; Kohlberger T, 2012, LECT NOTES COMPUT SC, V7510, P528, DOI 10.1007/978-3-642-33415-3_65; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lankton S, 2008, IEEE T IMAGE PROCESS, V17, P2029, DOI 10.1109/TIP.2008.2004611; Lempitsky V, 2009, IEEE I CONF COMP VIS, P277, DOI 10.1109/ICCV.2009.5459262; Li CM, 2008, IEEE T IMAGE PROCESS, V17, P1940, DOI 10.1109/TIP.2008.2002304; Li HL, 2014, IEEE T IMAGE PROCESS, V23, P3545, DOI 10.1109/TIP.2014.2330759; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Maitra M., 2012, INT J COMPUTER APPL, V53, P18, DOI DOI 10.5120/8505-2274; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Settles B., 2010, TECHNICAL REPORT; Vijayanarasimhan S, 2011, INT J COMPUT VISION, V91, P24, DOI 10.1007/s11263-010-0372-4; Wah C, 2015, IEEE WINT CONF APPL, P502, DOI 10.1109/WACV.2015.73; Wang XW, 2011, PRO INT CONF SCI INF, P848; Wu JJ, 2014, PROC CVPR IEEE, P256, DOI 10.1109/CVPR.2014.40	44	2	2	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2019	127	9					1198	1216		10.1007/s11263-019-01172-6	http://dx.doi.org/10.1007/s11263-019-01172-6			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IL9YV		Green Submitted			2022-12-18	WOS:000477642300002
J	Paudel, DP; Habed, A; Demonceaux, C; Vasseur, P				Paudel, Danda Pani; Habed, Adlane; Demonceaux, Cedric; Vasseur, Pascal			Robust and Optimal Registration of Image Sets and Structured Scenes via Sum-of-Squares Polynomials	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						2D-3D registration; Structure-from-Motion; Polynomial Sum-of-Squares optimization	3D; OPTIMIZATION; ALGORITHM; CONSENSUS; MATRIX; POSE	This paper addresses the problem of registering a known structured 3D scene, typically a 3D scan, and its metric Structure-from-Motion (SfM) counterpart. The proposed registration method relies on a prior plane segmentation of the 3D scan. Alignment is carried out by solving either the point-to-plane assignment problem, should the SfM reconstruction be sparse, or the plane-to-plane one in case of dense SfM. A Polynomial Sum-of-Squares optimization theory framework is employed for identifying point-to-plane and plane-to-plane mismatches, i.e. outliers, with certainty. An inlier set maximization approach within a Branch-and-Bound search scheme is adopted to iteratively build potential inlier sets and converge to the solution satisfied by the largest number of assignments. Plane visibility conditions and vague camera locations may be incorporated for better efficiency without sacrificing optimality. The registration problem is solved in two cases: (i) putative correspondences (with possibly overwhelmingly many outliers) are provided as input and (ii) no initial correspondences are available. Our approach yields outstanding results in terms of robustness and optimality.	[Paudel, Danda Pani] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland; [Habed, Adlane] Univ Strasbourg, CNRS, ICube Lab, Strasbourg, France; [Demonceaux, Cedric] Univ Bourgogne Franche Comte, CNRS, Le2i Lab, Dijon, France; [Vasseur, Pascal] Normandie Univ, LITIS, UNIROUEN, UNIHAVRE,INSA Rouen, F-76000 Rouen, France	Swiss Federal Institutes of Technology Domain; ETH Zurich; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universites de Strasbourg Etablissements Associes; Universite de Strasbourg; Centre National de la Recherche Scientifique (CNRS); Universite de Bourgogne	Paudel, DP (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.	paudel@vision.ee.ethz.ch; adlane.habed@icube.unistra.fr; cedric.demonceaux@u-bourgogne.fr; pascal.vasseur@univ-rouen.fr			International Project NRF-ANR DrAACaR [ANR-11-ISO3-0003]; Regional Council of Bourgogne; European Regional Development Fund	International Project NRF-ANR DrAACaR(French National Research Agency (ANR)); Regional Council of Bourgogne(Region Bourgogne-Franche-Comte); European Regional Development Fund(European Commission)	This research has been funded by the International Project NRF-ANR DrAACaR: ANR-11-ISO3-0003, the Regional Council of Bourgogne and European Regional Development Fund.	[Anonymous], 2015, HOUGH TRANSFORM PLAN; Bazin JC, 2013, IEEE T PATTERN ANAL, V35, P1565, DOI 10.1109/TPAMI.2012.264; Borrmann D, 2011, 3D RES, V32, P13; Boyd S, 2004, CONVEX OPTIMIZATION; Breuel TM, 2003, COMPUT VIS IMAGE UND, V90, P258, DOI 10.1016/S1077-3142(03)00026-2; Castellani U., 2012, 3D IMAGING ANAL APPL, P221; Chandraker M., 2007, P IEEE C COMPUTER VI, P1; Chesi G, 2002, IEEE T PATTERN ANAL, V24, P397, DOI 10.1109/34.990139; Choi M., 1995, P S PURE MATH 2, V2, P103; Christy S, 1999, COMPUT VIS IMAGE UND, V73, P137, DOI 10.1006/cviu.1998.0717; Corsini M, 2013, INT J COMPUT VISION, V102, P91, DOI 10.1007/s11263-012-0552-5; Enqvist O, 2008, LECT NOTES COMPUT SC, V5302, P141, DOI 10.1007/978-3-540-88682-2_12; Enqvist O, 2009, IEEE I CONF COMP VIS, P1295, DOI 10.1109/ICCV.2009.5459319; Ferraz L, 2014, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2014.71; Finsler P., 1936, COMMENT MATH HELV, V9, P188; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FITZGIBBON AW, 2001, BRIT MACH VIS C, P662; Fraundorfer F, 2012, IEEE ROBOT AUTOM MAG, V19, P78, DOI 10.1109/MRA.2012.2182810; Habed A, 2012, LECT NOTES COMPUT SC, V7577, P710, DOI 10.1007/978-3-642-33783-3_51; Hartley R., 2004, ROBOTICA; Hilbert David, 1888, MATH ANN, V32, P342, DOI DOI 10.1007/BF01443605; Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59; Jurie F, 1999, COMPUT VIS IMAGE UND, V73, P357, DOI 10.1006/cviu.1998.0735; Kahl F, 2007, INT J COMPUT VISION, V74, P3, DOI 10.1007/s11263-006-0015-y; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Li HD, 2009, IEEE I CONF COMP VIS, P1074, DOI 10.1109/ICCV.2009.5459398; Liu LY, 2005, PROC CVPR IEEE, P137; Mastin A, 2009, PROC CVPR IEEE, P2631; Moulon Pierre, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P257, DOI 10.1007/978-3-642-37447-0_20; Olsson C., 2006, P IEEE COMP SOC C CO, V1, P1206, DOI DOI 10.1109/CVPR.2006.307; Parrilo P. A., 2000, TECHNICAL REPORT; Paudel DP, 2014, INT C PATT RECOG, P196, DOI 10.1109/ICPR.2014.43; Plotz T, 2015, IEEE I CONF COMP VIS, P2030, DOI 10.1109/ICCV.2015.235; Powers V, 1998, J PURE APPL ALGEBRA, V127, P99, DOI 10.1016/S0022-4049(97)83827-3; PUTINAR M, 1993, INDIANA U MATH J, V42, P969, DOI 10.1512/iumj.1993.42.42045; Ramalingam S, 2013, INT J COMPUT VISION, V102, P73, DOI 10.1007/s11263-012-0576-x; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Schindler G, 2008, PROC CVPR IEEE, P77; Schwartz G, 2008, PUBLIC INVESTMENT AND PUBLIC-PRIVATE PARTNERSHIPS: ADDRESSING INFRASTRUCTURE CHALLENGES AND MANAGING FISCAL RISKS, P1; Segal A., 2009, ROBOTICS SCI SYSTEMS; Shaoyi Du, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P193; Strecha C., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587706; Tamaazousti M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3073, DOI 10.1109/CVPR.2011.5995358; Taneja A, 2013, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2013.22; Verschelde J, 1999, ACM T MATH SOFTWARE, V25, P251, DOI 10.1145/317275.317286; VIOLA P, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P16, DOI 10.1109/ICCV.1995.466930; Wagner S, 2009, THESIS; Yang JL, 2013, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2013.184; Yang JL, 2014, LECT NOTES COMPUT SC, V8689, P111, DOI 10.1007/978-3-319-10590-1_8; Zhang X, 2014, IEEE COMPUT SOC CONF, P746, DOI 10.1109/CVPRW.2014.115	50	2	2	1	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2019	127	5					415	436		10.1007/s11263-018-1114-2	http://dx.doi.org/10.1007/s11263-018-1114-2			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HO7XB		Green Submitted			2022-12-18	WOS:000461162200001
J	Escalera, S; Gonzalez, J; Escalante, HJ; Baro, X; Guyon, I				Escalera, Sergio; Gonzalez, Jordi; Jair Escalante, Hugo; Baro, Xavier; Guyon, Isabelle			Looking at People Special Issue	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Escalera, Sergio] Univ Barcelona, Barcelona, Spain; [Escalera, Sergio; Gonzalez, Jordi; Baro, Xavier] Comp Vis Ctr, Catalonia, Spain; [Escalera, Sergio; Jair Escalante, Hugo] ChaLearn, Berkeley, CA 94708 USA; [Gonzalez, Jordi] Univ Autonoma Barcelona, Barcelona, Spain; [Jair Escalante, Hugo] INAOE, Puebla, Mexico; [Baro, Xavier] Univ Oberta Catalunya, Catalonia, Spain; [Guyon, Isabelle] Univ Paris Saclay, Paris, France	University of Barcelona; Centre de Visio per Computador (CVC); Autonomous University of Barcelona; Instituto Nacional de Astrofisica, Optica y Electronica; UOC Universitat Oberta de Catalunya; UDICE-French Research Universities; Universite Paris Saclay	Escalera, S (corresponding author), Univ Barcelona, Barcelona, Spain.; Escalera, S (corresponding author), Comp Vis Ctr, Catalonia, Spain.; Escalera, S (corresponding author), ChaLearn, Berkeley, CA 94708 USA.	sergio.escalera.guerrero@gmail.com	Baró, Xavier/A-4064-2011; Gonzàlez, Jordi/I-1812-2015; Escalera, Sergio/L-2998-2015	Baró, Xavier/0000-0001-5338-3007; Gonzàlez, Jordi/0000-0001-8033-0306; Escalera, Sergio/0000-0003-0617-8873				Baro X., 2015, P IEEE C COMP VIS PA; Escalera S., 2017, SPRINGER SERIES CHAL, V2	2	2	2	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		141	143		10.1007/s11263-017-1058-y	http://dx.doi.org/10.1007/s11263-017-1058-y			3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA		Bronze			2022-12-18	WOS:000425619100001
J	Martin-Brualla, R; Gallup, D; Seitz, SM				Martin-Brualla, Ricardo; Gallup, David; Seitz, Steven M.			3D Time-Lapse Reconstruction from Internet Photos	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	IEEE International Conference on Computer Vision (ICCV)	DEC 11-18, 2015	Santiago, CHILE	CPS, IEEE Comp Soc, Amazon, Microsoft, SENSETIME, Baidu, Intel, Facebook, Adobe, Panasonic, Google, OMRON, Blippar, iRobot, HISCENE, NVIDIA, Viscovery, AiCUre, M Tec, Inst Elect & Elect Engineers, Comp Vis Fdn		Computational photography; Time-lapse; Internet photos	DEPTH MAPS; RECOVERY	Given an Internet photo collection of a landmark, we compute a 3D time-lapse video sequence where a virtual camera moves continuously in time and space. While previous work assumed a static camera, the addition of camera motion during the time-lapse creates a very compelling impression of parallax. Achieving this goal, however, requires addressing multiple technical challenges, including solving for time-varying depth maps, regularizing 3D point color profiles over time, and reconstructing high quality, hole-free images at every frame from the projected profiles. Our results show photorealistic time-lapses of skylines and natural scenes over many years, with dramatic parallax effects.	[Martin-Brualla, Ricardo; Gallup, David; Seitz, Steven M.] Google Inc, Mountain View, CA USA; [Seitz, Steven M.] Univ Washington, Seattle, WA 98195 USA	Google Incorporated; University of Washington; University of Washington Seattle	Martin-Brualla, R (corresponding author), Google Inc, Mountain View, CA USA.	rmbrualla@google.com			National Science Foundation [IIS-1250793]; Animation Research Labs; Google	National Science Foundation(National Science Foundation (NSF)); Animation Research Labs; Google(Google Incorporated)	The research was supported in part by the National Science Foundation (IIS-1250793), the Animation Research Labs, and Google.	Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; Bennett E. P., 2007, ACM SIGGRAPH 2007 PA; Earth Vision Institute, 2007, EXTR IC SURV; Hauagge D., 2014, P BMVC; Kang SB, 2004, INT J COMPUT VISION, V58, P139, DOI 10.1023/B:VISI.0000015917.35451.df; Kemelmacher-Shlizerman I., 2011, ACM SIGGRAPH 2011 PA; Klose F, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766920; Kopf J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601195; Laffont P. Y., 2012, ACM T GRAPHICS SIGGR, P31; Laforet V., 2013, TIME LAPSE 1; Larsen ES, 2007, IEEE I CONF COMP VIS, P1440; Martin-Brualla R, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766903; Matzen K., 2014, P EUR C COMP VIS; Mierle Keir, 2012, CERES SOLVER; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Rubinstein M, 2011, PROC CVPR IEEE, P313, DOI 10.1109/CVPR.2011.5995374; Schindler G., 2007, IEEE C COMPUTER VISI, P1; Schindler G, 2010, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2010.5539803; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Shan Q, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P25, DOI 10.1109/3DV.2013.12; Simon I., 2009, P GRAPH INT, P1; Simon I, 2007, IEEE I CONF COMP VIS, P274; Snavely N, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360614; Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52; Zhang L, 2003, PROC CVPR IEEE, P367	25	2	2	1	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2017	125	1-3			SI		52	64		10.1007/s11263-017-1003-0	http://dx.doi.org/10.1007/s11263-017-1003-0			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	FL2TO		Green Submitted			2022-12-18	WOS:000414072800005
J	Pertuz, S; Garcia, MA; Puig, D; Arguello, H				Pertuz, Said; Angel Garcia, Miguel; Puig, Domenec; Arguello, Henry			A Closed-Form Focus Profile Model for Conventional Digital Cameras	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Focus measure; Focus profile; Camera model; Camera calibration; Depth of field	FOCAL LENGTH; DEPTH; SHAPE; DECONVOLUTION; CALIBRATION; NOISE; BLUR	According to the thin lens model, the classic depth of field (DOF) is defined as the distance range at which objects in front of a camera are in focus. However, the thin lens poses important practical limitations for modeling the camera focus due to its dependence on internal parameters, such as the focal length, numerical aperture and effective pixel size. In this paper, a new model for describing the focus of conventional digital cameras is proposed. The focus is modeled as the energy of the point-spread-function of the imaging system and describes the joint effect of defocus, diffraction and digitization. Experiments conducted on different acquisition devices show that the proposed model conforms accurately to the behavior of real systems and out-performs the most similar alternatives in the state-of-the-art. In addition, in contrast to the classic DOF model, the proposed approach can be used to predict the changes in the focus of conventional digital cameras when changing focus, zoom, and aperture by means of a simple calibration process.	[Pertuz, Said] Univ Ind Santander, Dept Elect & Elect Engn, Bucaramanga, Colombia; [Angel Garcia, Miguel] Autonomous Univ Madrid, Dept Elect & Commun Technol, Madrid, Spain; [Puig, Domenec] Univ Rovira & Virgili, Dept Comp Sci & Math, Tarragona, Spain; [Arguello, Henry] Univ Ind Santander, Dept Informat Engn, Bucaramanga, Colombia	Universidad Industrial de Santander; Autonomous University of Madrid; Universitat Rovira i Virgili; Universidad Industrial de Santander	Pertuz, S (corresponding author), Univ Ind Santander, Dept Elect & Elect Engn, Bucaramanga, Colombia.	spertuz@uis.edu.co; miguelangel.garcia@uam.es; domenec.puig@urv.cat; henarfu@uis.edu.co	Garcia, Miguel Angel/C-4304-2014; Pertuz, Said/I-5226-2019	Garcia, Miguel Angel/0000-0003-2611-6821; Pertuz, Said/0000-0001-8498-9917				Abramowitz M., 2012, NUMERICAL APERTURE R; Aguet F, 2008, IEEE T IMAGE PROCESS, V17, P1144, DOI 10.1109/TIP.2008.924393; Allen E., 2011, MANUAL PHOTOGRAPHY; Bass M., 2009, HDB OPTICS, V1; Born M., 1999, PRINCIPLES OPTICS, Vseventh, DOI DOI 10.1017/CBO9781139644181; Cao G., 2010, J INF HIDING MULTIME, V1, P20; Chen SY, 2013, IEEE T IND INFORM, V9, P1680, DOI 10.1109/TII.2012.2221471; CODY WJ, 1969, MATH COMPUT, V23, P631, DOI 10.2307/2004390; de Angelis M, 1999, OPT COMMUN, V160, P5, DOI 10.1016/S0030-4018(98)00636-1; Ersoy O. K., 2007, DIFFRACTION FOURIER; Favaro P, 2005, IEEE T PATTERN ANAL, V27, P406, DOI 10.1109/TPAMI.2005.43; Favaro P., 2007, P IEEE INT C COMP VI, P1; FitzGerrell AR, 1997, APPL OPTICS, V36, P5796, DOI 10.1364/AO.36.005796; Goodman J., 1996, OPT ENG, V2nd, DOI DOI 10.1117/1.601121; Hart J. F., 1968, COMPUTER APPROXIMATI; Hasinoff SW, 2011, IEEE T PATTERN ANAL, V33, P2203, DOI 10.1109/TPAMI.2011.62; He J, 2003, IEEE T CONSUM ELECTR, V49, P257, DOI 10.1109/TCE.2003.1209511; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468; HOPKINS HH, 1955, PROC R SOC LON SER-A, V231, P91, DOI 10.1098/rspa.1955.0158; HORN BKP, 1990, ROBOT VISION; Ito A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601207; Jeon J, 2011, IEEE T CONSUM ELECTR, V57, P1, DOI 10.1109/TCE.2011.5735472; Jeon J, 2010, IEEE T CONSUM ELECTR, V56, P1204, DOI 10.1109/TCE.2010.5606247; Joshi N., 2008, P COMP VIS PATT REC; Kehtarnavaz N, 2003, REAL-TIME IMAGING, V9, P197, DOI 10.1016/S1077-2014(03)00037-8; Lai YC, 2011, IEEE INT CONF FUZZY, P1143; LEI F, 1994, APPL OPTICS, V33, P6603, DOI 10.1364/AO.33.006603; Liu C, 2008, IEEE T PATTERN ANAL, V30, P299, DOI [10.1109/TPAMI.2007.1176, 10.1109/TPAMI.20071176]; Mahmood MT, 2010, OPT LETT, V35, P1272, DOI 10.1364/OL.35.001272; Minhas R, 2011, PATTERN RECOGN, V44, P839, DOI 10.1016/j.patcog.2010.10.015; Muhammad MS, 2012, IEEE T PATTERN ANAL, V34, P564, DOI 10.1109/TPAMI.2011.144; Muhammad MS, 2009, PROCEEDINGS OF THE 2009 INTERNATIONAL CONFERENCE OF COMPUTATIONAL SCIENCES AND ITS APPLICATIONS, P191, DOI 10.1109/ICCSA.2009.25; NAYAR SK, 1994, IEEE T PATTERN ANAL, V16, P824, DOI 10.1109/34.308479; Ng KC, 2001, IEEE INT CONF ROBOT, P2791, DOI 10.1109/ROBOT.2001.933045; Oppenheim R. W., 1999, DISCRETE TIME DIGITA; Orieux F, 2010, INT CONF ACOUST SPEE, P1350, DOI 10.1109/ICASSP.2010.5495444; Paramanand C, 2012, IEEE T IMAGE PROCESS, V21, P2798, DOI 10.1109/TIP.2011.2179664; Pertuz S, 2015, INT J COMPUT VISION, V112, P342, DOI 10.1007/s11263-014-0770-0; Pertuz S, 2013, PATTERN RECOGN, V46, P1415, DOI 10.1016/j.patcog.2012.11.011; Pertuz S, 2013, IEEE T IMAGE PROCESS, V22, P1242, DOI 10.1109/TIP.2012.2231087; Pratt WK, 2007, DIGITAL IMAGE PROCES, Vxix; Qin F. Q., 2010, P INT C INF AUT, P1200; Rottenfusser R., 2012, NUMERICAL APERTURE R; STOKSETH PA, 1969, J OPT SOC AM, V59, P1314, DOI 10.1364/JOSA.59.001314; Subbarao M, 1998, IEEE T PATTERN ANAL, V20, P864, DOI 10.1109/34.709612; Sundaram H, 1997, PROC CVPR IEEE, P814, DOI 10.1109/CVPR.1997.609421; Tay CJ, 2005, OPT COMMUN, V248, P339, DOI 10.1016/j.optcom.2004.12.036; Taylor J.R., 1997, INTRO ERROR ANAL STU, P181; Tenenbaum Jay Martin, 1971, THESIS; Tsai DC, 2016, IEEE T IMAGE PROCESS, V25, P818, DOI 10.1109/TIP.2015.2509427; Tsai DC, 2012, IEEE T IMAGE PROCESS, V21, P459, DOI 10.1109/TIP.2011.2164417; Voeltz D. G., 2010, COMPUTATIONAL FOURIE; Yousefi S, 2011, IEEE T CONSUM ELECTR, V57, P1003, DOI 10.1109/TCE.2011.6018848	55	2	2	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	3					273	286		10.1007/s11263-017-1024-8	http://dx.doi.org/10.1007/s11263-017-1024-8			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FE1ER		Green Accepted			2022-12-18	WOS:000407961700002
J	Lee, CM; Cheong, LF				Lee, Choon-Meng; Cheong, Loong-Fah			Minimal Basis Subspace Representation: A Unified Framework for Rigid and Non-rigid Motion Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion segmentation; Subspace segmentation; Non-rigid structure from motion; Minimal basis representation; Shape basis; Joint sparse representation; Factor graph with global cost	STRUCTURE-FROM-MOTION; ARTICULATED STRUCTURE; 3D SHAPE; ALGORITHM; RECOVERY	Motion segmentation and non-rigid structure from motion are two challenging computer vision problems that have attracted numerous research interests. While the previous works handle these two problems separately, we present a general motion segmentation framework in this paper for solving these two seemingly different problems in a unified manner. At the heart of our general motion segmentation framework is a model selection mechanism based on finding the minimal basis subspace representation, by seeking the joint sparse representation of the data matrix. However, such formulation is NP-hard and we solve the convex proxy instead. Unlike other compressive sensing related works, this convex proxy solution is insufficient for our problem. The convex relaxation artefacts and noise yield multiple subspace representations, making identification of the exact number of motion subspaces challenging. We solve for the right number of subspaces by transforming this problem into a Facility Location problem with global cost and solve the factor graph formulation using max product belief propagation message passing.	[Lee, Choon-Meng; Cheong, Loong-Fah] Natl Univ Singapore, Elect & Comp Engn Dept, 4 Engn Dr 3, Singapore 117576, Singapore	National University of Singapore	Lee, CM (corresponding author), Natl Univ Singapore, Elect & Comp Engn Dept, 4 Engn Dr 3, Singapore 117576, Singapore.	leechoonmeng@gmail.com; eleclf@nus.edu.sg			Singapore PSF [1321202075]	Singapore PSF	We like to express our gratitude to Inmar Givoni for her help and guidance, as well as Rui Yu for providing the Messi dataset. The support of the Singapore PSF Grant 1321202075 is gratefully acknowledged.	Akhter I., 2012, TRAJECTORY BASIS NON; Akhter I., 2008, NIPS; Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201; Akhter I, 2009, PROC CVPR IEEE, P1534, DOI 10.1109/CVPRW.2009.5206620; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Chin T., 2009, NIPS; Chin TJ, 2010, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2010.5539931; Chung F., 1997, AM MATH SOC, DOI 10.1090/cbms/092; Dai YC, 2012, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2012.6247905; Del Bue A, 2012, IEEE T PATTERN ANAL, V34, P1496, DOI 10.1109/TPAMI.2011.238; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Fayad J, 2011, IEEE I CONF COMP VIS, P431, DOI 10.1109/ICCV.2011.6126272; Fayad J, 2010, LECT NOTES COMPUT SC, V6314, P297, DOI 10.1007/978-3-642-15561-1_22; Fragkiadaki Katerina, 2014, ADV NEURAL INFORM PR, P55; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Givoni I., 2011, THESIS; Givoni I. E., 2012, HIERARCHICAL AFFINIT; Givoni IE, 2009, NEURAL COMPUT, V21, P1589, DOI 10.1162/neco.2009.05-08-785; Gotardo P. F., 2011, CSF CODE DOWNLOAD; Gotardo PFU, 2011, IEEE T PATTERN ANAL, V33, P2051, DOI 10.1109/TPAMI.2011.50; Hurley N, 2009, IEEE T INFORM THEORY, V55, P4723, DOI 10.1109/TIT.2009.2027527; Ji P, 2014, LECT NOTES COMPUT SC, V8694, P204, DOI 10.1007/978-3-319-10599-4_14; Kanatani K., 2002, Proceedings of the Fifth Asian Conference on Computer Vision, P7; Kundu A, 2011, IEEE I CONF COMP VIS, P2080, DOI 10.1109/ICCV.2011.6126482; Lauer F, 2009, IEEE I CONF COMP VIS, P678, DOI 10.1109/ICCV.2009.5459173; Lazic N., 2010, EXPERT SYST APPL, V9, P429; Lazic N, 2009, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2009.5459302; Lee C. M., 2013, ICCV; Lee M, 2013, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR.2013.169; Li H., 2007, CVPR, P1, DOI DOI 10.1109/CVPR.2007.382975; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Liu G., 2011, LOW RANK REPRESENTAT; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Ma Y, 2008, SIAM REV, V50, P413, DOI 10.1137/060655523; Nadler B, 2007, ADV NEURAL INFORM PR, P1017; Ng AY, 2002, ADV NEUR IN, V14, P849; Nocedal J, 2006, SPRINGER SER OPER RE, P135; Paladini Marco, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2898, DOI 10.1109/CVPRW.2009.5206602; Paladini M, 2010, LECT NOTES COMPUT SC, V6312, P15, DOI 10.1007/978-3-642-15552-9_2; Russell C., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3009, DOI 10.1109/CVPR.2011.5995383; Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38; Sabzevari R, 2014, IEEE INT CONF ROBOT, P23, DOI 10.1109/ICRA.2014.6906585; Schindler K, 2006, LECT NOTES COMPUT SC, V3951, P606; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Szeliski R, 1997, IEEE T PATTERN ANAL, V19, P506, DOI 10.1109/34.589211; Taylor J, 2010, PROC CVPR IEEE, P2761, DOI 10.1109/CVPR.2010.5540002; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L., NONRIGID STRUCTURE M; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Tresadern P, 2005, PROC CVPR IEEE, P1110; Tron R, 2007, PROC CVPR IEEE, P41, DOI 10.1109/cvpr.2007.382974; Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244; Vidal R, 2004, PROC CVPR IEEE, P510; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Xiao J, 2004, LECT NOTES COMPUT SC, V2034, P573; Yan JY, 2008, IEEE T PATTERN ANAL, V30, P865, DOI 10.1109/TPAMI.2007.70739; Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94	58	2	2	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	2					209	233		10.1007/s11263-016-0928-z	http://dx.doi.org/10.1007/s11263-016-0928-z			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK2LI					2022-12-18	WOS:000393758400002
J	Marinoiu, E; Papava, D; Sminchisescu, C				Marinoiu, Elisabeta; Papava, Dragos; Sminchisescu, Cristian			Pictorial Human Spaces: A Computational Study on the Human Perception of 3D Articulated Poses	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human pose estimation; Human perception; Metric learning; Perceptual distance; Human eye movements		Human motion analysis in images and video, with its deeply inter-related 2D and 3D inference components, is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing-as well as the levels of accuracy-involved in the 3D perception of people from images by assessing the human performance. Moreover, we reveal the quantitative and qualitative differences between human and computer performance when presented with the same visual stimuli and showthat metrics incorporating human perception can produce more meaningful results when integrated into automatic pose prediction algorithms. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose; (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their 're-enacted' 3D poses; (3) quantitative analysis revealing the human performance in 3D pose re-enactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses; (4) extensive analysis on the differences between human re-enactments and poses produced by an automatic system when presented with the same visual stimuli; (5) an approach to learning perceptual metrics that, when integrated into visual sensing systems, produces more stable and meaningful results.	[Sminchisescu, Cristian] Lund Univ, Dept Math, Lund, Sweden; [Marinoiu, Elisabeta; Papava, Dragos; Sminchisescu, Cristian] Romanian Acad, Inst Math, Bucharest, Romania	Lund University; Institute of Mathematics of the Romanian Academy; Romanian Academy of Sciences; University of Bucharest	Sminchisescu, C (corresponding author), Lund Univ, Dept Math, Lund, Sweden.; Sminchisescu, C (corresponding author), Romanian Acad, Inst Math, Bucharest, Romania.	elisabeta.marinoiu@imar.ro; dragos.papava@imar.ro; cristian.sminchisescu@math.lth.se			CNCS-UEFISCDI [PCE-2011-3-0438, JRP-RO-FR-2014-16]	CNCS-UEFISCDI(Consiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii (UEFISCDI))	This work was supported in part by CNCS-UEFISCDI under PCE-2011-3-0438, and JRP-RO-FR-2014-16.	Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; Akhter I., 2015, IEEE INT C COMP VIS; Andriluka M., 2010, IEEE INT C COMP VIS; Bar-hillel A., 2003, INT C MACH LEARN; Bo L., 2009, IEEE INT C COMP VIS; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Bourdev L, 2010, LECT NOTES COMPUT SC, V6316, P168, DOI 10.1007/978-3-642-15567-3_13; Chen C, 2009, COMPUT ANIMAT VIRT W, V20, P267, DOI 10.1002/cav.297; Chen X., 2014, P 27 ANN C NEURAL IN, P1736, DOI DOI 10.1109/CVPR.2018.00742; Cortes C., 2005, PROC 22 INT C MACH L, P153, DOI DOI 10.1145/1102351.1102371; Deutscher J., 2000, IEEE INT C COMP VIS; Dickinson S., 1994, INT J COMPUTER VISIO; Ehinger KA, 2009, VIS COGN, V17, P945, DOI 10.1080/13506280902834720; Fan XC, 2015, PROC CVPR IEEE, P1347, DOI 10.1109/CVPR.2015.7298740; Ferrari V., 2009, IEEE INT C COMP VIS; FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602; Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1; Harada T., 2004, INT J HUMANOID ROBOT; Huang CH, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P287, DOI 10.1109/3DV.2013.45; IONESCU C, 2014, IEEE T PATTERN ANAL; Ionescu C, 2014, PROC CVPR IEEE, P1661, DOI 10.1109/CVPR.2014.215; Jain A., 2014, LEARNING HUMAN POSE; Johannson G., 1973, PERCEPTION PSYCHOPHY; Kanaujia A., 2007, IEEE INT C COMP VIS; Koenderink JJ, 1998, PHILOS T R SOC A, V356, P1071, DOI 10.1098/rsta.1998.0211; LEE HJ, 1985, COMPUT VISION GRAPH, V30, P148, DOI 10.1016/0734-189X(85)90094-5; Li F, 2012, IEEE INT C COMP VIS; Li S., 2014, COMP VIS ACCV 2014 2; Lopez-Mendez A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.49; Marinoiu E., 2013, INT C COMP VIS; Mathe S, 2013, ADV NEURAL INFORM PR, P1923; Morris DD, 2003, INT J ROBOT RES, V22, P393, DOI 10.1177/0278364903022006004; Muller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247; Pons-Moll G., 2014, IEEE INT C COMP VIS; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Sapp B, 2010, LECT NOTES COMPUT SC, V6312, P406, DOI 10.1007/978-3-642-15552-9_30; Sekunova A, 2013, PERCEPTION, V42, P176, DOI 10.1068/p7265; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; SIDENBLADH H, 2000, EUR C COMP VIS; Sigal L., 2007, ADV NEURAL INFORM PR; Sigal L., 2010, EUR C COMP VIS; Sigal L., 2010, IEEE INT C COMP VIS; Sigal L, 2006, LECT NOTES COMPUT SC, V4069, P185; Sminchisescu C., 2005, INT J COMPUT VISION, V61, P227; Sminchisescu C., 2004, IEEE INT C COMP VIS, V2; Sminchisescu C., 2003, IEEE INT C COMP VIS; Sminchisescu C., 2006, IEEE INT C COMP VIS; Sun M., 2012, IEEE INT C COMP VIS; Tang JKT, 2008, COMPUT ANIMAT VIRT W, V19, P211, DOI 10.1002/cav.260; Tompson J. J., 2014, ADV NEURAL INFORM PR, P8; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Urtasun R, 2005, IEEE I CONF COMP VIS, P403; Wolpert DM, 2011, NAT REV NEUROSCI, V12, P739, DOI 10.1038/nrn3112; Yang Y., 2011, IEEE INT C COMP VIS	54	2	2	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	2					194	215		10.1007/s11263-016-0888-3	http://dx.doi.org/10.1007/s11263-016-0888-3			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FA					2022-12-18	WOS:000380269600006
J	Wang, P; Shen, CH; van den Hengel, A; Torr, PHS				Wang, Peng; Shen, Chunhua; van den Hengel, Anton; Torr, Philip H. S.			Efficient Semidefinite Branch-and-Cut for MAP-MRF Inference	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						MAP inference; Markov random field; Semidefinite programming; Branch and cut	INTERIOR-POINT METHODS; BELIEF-PROPAGATION; PROGRAMMING APPROACH; ENERGY MINIMIZATION; ALGORITHM; SEARCH; RATES	We propose a branch-and-cut (B&C) method for solving general MAP-MRF inference problems. The core of our method is a very efficient bounding procedure, which combines scalable semidefinite programming (SDP) and a cutting-plane method for seeking violated constraints. In order to further speed up the computation, several strategies have been exploited, including model reduction, warm start and removal of inactive constraints. We analyze the performance of the proposed method under different settings, and demonstrate that our method either outperforms or performs on par with state-of-the-art approaches. Especially when the connectivities are dense or when the relative magnitudes of the unary costs are low, we achieve the best reported results. Experiments show that the proposed algorithm achieves better approximation than the state-of-the-art methods within a variety of time budgets on challenging non-submodular MAP-MRF inference problems.	[Wang, Peng; Shen, Chunhua; van den Hengel, Anton] Univ Adelaide, Adelaide, SA 5005, Australia; [Shen, Chunhua; van den Hengel, Anton] Australian Ctr Robot Vis, Adelaide, SA, Australia; [Torr, Philip H. S.] Univ Oxford, Oxford, England	University of Adelaide; Australian Centre for Robotic Vision; University of Oxford	Shen, CH (corresponding author), Univ Adelaide, Adelaide, SA 5005, Australia.; Shen, CH (corresponding author), Australian Ctr Robot Vis, Adelaide, SA, Australia.	chhshen@gmail.com		van den Hengel, Anton/0000-0003-3027-8364	Engineering and Physical Sciences Research Council [EP/N019474/1] Funding Source: researchfish; EPSRC [EP/N019474/1] Funding Source: UKRI	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))		Aji S. M., 1998, P ISIT; Alahari K., 2008, P IEEE C COMP VIS PA, P1; Alizadeh F, 1998, SIAM J OPTIMIZ, V8, P746, DOI 10.1137/S1052623496304700; Andres B., 2014, OPENGM2; Armbruster M, 2012, MATH PROGRAM COMPUT, V4, P275, DOI 10.1007/s12532-012-0040-5; Arora S, 2005, ANN IEEE SYMP FOUND, P339, DOI 10.1109/SFCS.2005.35; Arora S, 2007, ACM S THEORY COMPUT, P227, DOI 10.1145/1250790.1250823; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; BARAHONA F, 1986, MATH PROGRAM, V36, P157, DOI 10.1007/BF02592023; Batra Dhruv, 2011, AISTATS 11, V15, P146; Bayati M, 2005, 2005 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY (ISIT), VOLS 1 AND 2, P1763; BESAG J, 1986, J R STAT SOC B, V48, P259; Bonato T, 2014, MATH PROGRAM, V146, P351, DOI 10.1007/s10107-013-0688-2; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Burer S, 2008, MATH PROGRAM, V113, P259, DOI 10.1007/s10107-006-0080-6; CHOPRA S, 1993, MATH PROGRAM, V59, P87, DOI 10.1007/BF01581239; Chunhua Shen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2601, DOI 10.1109/CVPR.2011.5995447; DEZA M, 1992, MATH OPER RES, V17, P981, DOI 10.1287/moor.17.4.981; DEZA MM, 1997, ALGORITHMS COMBINATO, V15; Dinh TP, 2010, J GLOBAL OPTIM, V48, P595, DOI 10.1007/s10898-009-9507-y; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; Figueiredo Mario A. T., 2011, P 28 INT C MACH LEAR, P169; Frostig Roy, 2014, P ADV NEUR INF PROC, P3077; Garber D., 2011, P 24 INT C NEURAL IN, P1080; Givry S. D., 2014, C ROADEF 2014 BORD F; Globerson A, 2011, PROBABILISTIC INFERE; Globerson A., 2007, ADV NEUR INF PROC SY; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Gorelick L., 2014, P IEEE C COMP VIS PA; Hazan E, 2008, LECT NOTES COMPUT SC, V4957, P306, DOI 10.1007/978-3-540-78773-0_27; Hazan T, 2010, IEEE T INFORM THEORY, V56, P6294, DOI 10.1109/TIT.2010.2079014; Helmberg C, 1998, MATH PROGRAM, V82, P291, DOI 10.1007/BF01580072; Helmberg C., 1995, Integer Programming and Combinatorial Optimization. 4th International IPOC Conference. Proceedings, P124; Helmberg C, 2000, J COMB OPTIM, V4, P197, DOI 10.1023/A:1009898604624; HELMBERG C, 1994, THESIS GRAZ U TECHNO; Helmberg C., 1996, P 5 INT IPCO C INT P, P175; Helmberg C., 1998, TOPICS SEMIDEFINITE, V18, P197; Hendrix E. M. T., 2010, INTRO NONLINEAR GLOB; Horst R., 2013, GLOBAL OPTIMIZATION; Horst R., 2000, INTRO GLOBAL OPTIMIZ; Huang Q., 2014, INT C MACH LEARN ICM; IBM, 2015, ILOG CPLEX OPT; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Johnson J. K., 2007, ANN ALL C COMM CONTR; Johnson J. K., 2008, THESIS MIT; Jojic V., 2010, P 27 INT C MACH LEAR, P503; Joulin A., 2010, P IEEE C COMP VIS PA; Kappes Jorg Hendrik, 2011, Energy Minimization Methods in Computer Vision and Pattern Recognition. Proceedings 8th International Conference, EMMCVPR 2011, P31, DOI 10.1007/978-3-642-23094-3_3; Kappes J. H., 2013, ARXIV13056387; Kappes JH, 2013, PROC CVPR IEEE, P1752, DOI 10.1109/CVPR.2013.229; Kappes JH, 2012, PROC CVPR IEEE, P1688, DOI 10.1109/CVPR.2012.6247863; Kappes JH, 2010, LECT NOTES COMPUT SC, V6313, P735; Kappes Jorg H., 2015, INT J COMPUTER VISIO; Kernighan B. W., 1970, Bell System Technical Journal, V49, P291; Kim W, 2011, COMPUT VIS IMAGE UND, V115, P1623, DOI 10.1016/j.cviu.2011.05.015; Kohli P., 2008, P 25 INT C MACH LEAR, P480, DOI DOI 10.1145/1390156.1390217; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; KOLMOGOROV V, 2005, P UNC ART INT; Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kolmogorov V, 2006, LECT NOTES COMPUT SC, V3952, P1; Komodakis N, 2008, LECT NOTES COMPUT SC, V5304, P806, DOI 10.1007/978-3-540-88690-7_60; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Kovtun I., 2011, CONTROL SYSTEMS COMP, V2, P71; Kumar M. P., 2006, P IEEE C COMP VIS PA, P1045; Kumar MP, 2009, J MACH LEARN RES, V10, P71; Land A., 1979, Discrete Optimisation, P221; Laue S., 2012, P INT C MACH LEARN, P177; Liu FY, 2015, PATTERN RECOGN, V48, P2983, DOI 10.1016/j.patcog.2015.04.019; Malick J, 2007, J GLOBAL OPTIM, V39, P609, DOI 10.1007/s10898-007-9161-1; Malick J, 2009, SIAM J OPTIMIZ, V20, P336, DOI 10.1137/070704575; Mars S., 2012, TECHNICAL REPORT; Martins A. F., 2011, P INT WORKSH OPT MAC; Meshi O, 2011, LECT NOTES ARTIF INT, V6912, P470, DOI 10.1007/978-3-642-23783-6_30; Mitra G, 1973, MATH PROGRAM, V4, P155; Nesterov YE, 1998, SIAM J OPTIMIZ, V8, P324, DOI 10.1137/S1052623495290209; Olsson C., 2007, P IEEE C COMP VIS PA, p1{8, DOI DOI 10.1109/CVPR.2007.383202; Otten L, 2012, AI COMMUN, V25, P211, DOI 10.3233/AIC-2012-0531; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Peng J., 2012, P 15 INT C ART INT S, V22, P868; RAJ A, 2005, P IEEE INT C COMP VI; Ravikumar P, 2010, J MACH LEARN RES, V11, P1043; Ravikumar Pradeep, 2006, P 23 INT C MACH LEAR, P737, DOI DOI 10.1145/1143844.1143937; Rislock N., 2012, MATH PROGRAMMING; Rockafellar RT., 1973, MATH PROGRAM, V5, P354, DOI [10.1007/BF01580138, DOI 10.1007/BF01580138]; Rother C., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383203; Savchynskyy B., 2012, P UNC ART INT; Schellewald C, 2005, LECT NOTES COMPUT SC, V3757, P171, DOI 10.1007/11585978_12; SCHLESINGER M, 1976, KIBERNETIKA, V4, P113; Shekhovtsov A, 2015, PROC CVPR IEEE, P521, DOI 10.1109/CVPR.2015.7298650; Shekhovtsov A, 2014, PROC CVPR IEEE, P1162, DOI 10.1109/CVPR.2014.152; Sontag D., 2007, P ADV NEUR INF PROC; Sontag D., 2012, P UNC ART INT; Sontag D., 2008, P UNC ART INT; Sun J, 2002, LECT NOTES COMPUT SC, V2351, P510; Sun M., 2012, P INT C ART INT STAT; Swoboda P, 2014, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2014.153; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; TOPKIS DM, 1982, J OPTIMIZ THEORY APP, V36, P1, DOI 10.1007/BF00934337; Torr P. H. S., 2003, P INT C ART INT STAT; Tutuncu RH, 2003, MATH PROGRAM, V95, P189, DOI 10.1007/s10107-002-0347-5; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Wainwright MJ, 2004, ADV NEUR IN, V16, P369; Wang P., 2015, P IEEE C COMP VIS PA; Wang P., 2013, P IEEE C COMP VIS PA; Weiss Y, 2000, NEURAL COMPUT, V12, P1, DOI 10.1162/089976600300015880; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; Wen ZW, 2010, MATH PROGRAM COMPUT, V2, P203, DOI 10.1007/s12532-010-0017-1; Werner T., 2008, P IEEE C COMP VIS PA, P1; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Windheuser T, 2012, LECT NOTES COMPUT SC, V7577, P400, DOI 10.1007/978-3-642-33783-3_29; YE YY, 1994, MATH OPER RES, V19, P53, DOI 10.1287/moor.19.1.53; Zhao XY, 2010, SIAM J OPTIMIZ, V20, P1737, DOI 10.1137/080718206; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236; [No title captured]	120	2	2	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2016	117	3					269	289		10.1007/s11263-015-0865-2	http://dx.doi.org/10.1007/s11263-015-0865-2			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DL1TF		Green Submitted			2022-12-18	WOS:000375414600004
J	Kwak, K; Kim, JS; Huber, DF; Kanade, T				Kwak, Kiho; Kim, Jun-Sik; Huber, Daniel F.; Kanade, Takeo			Online Approximate Model Representation Based on Scale-Normalized and Fronto-Parallel Appearance	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object modeling; Approximate model representation; Sensor fusion; Object detection and tracking	OBJECT DETECTION; VEHICLE TRACKING; RECOGNITION; SEQUENCES; FEATURES; LIDAR	Various object representations have been widely used for many tasks such as object detection, recognition, and tracking. Most of them requires an intensive training process on large database which is collected in advance, and it is hard to add models of a previously unobserved object which is not in the database. In this paper, we investigate how to create a representation of a new and unknown object online, and how to apply it to practical applications like object detection and tracking. To make it viable, we utilize a sensor fusion approach using a camera and a single-line scan LIDAR. The proposed representation consists of an approximated geometry model and a viewpoint-scale invariant appearance model which makes to extremely simple to match the model and the observation. This property makes it possible to model a new object online, and provides a robustness to viewpoint variation and occlusion. The representation has benefits of both an implicit model (referred to as a view-based model) and an explicit model (referred to as a shape-based model). Intensive experiments using synthetic and real data demonstrate the viability of the proposed object representation in both modeling and detecting/tracking objects.	[Kwak, Kiho] Agcy Def Dev, Daejeon, South Korea; [Kim, Jun-Sik] Korea Inst Sci & Technol, Seoul, South Korea; [Huber, Daniel F.; Kanade, Takeo] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Agency of Defense Development (ADD), Republic of Korea; Korea Institute of Science & Technology (KIST); Carnegie Mellon University	Kwak, K (corresponding author), Agcy Def Dev, Daejeon, South Korea.	kkwak.add@gmail.com; junsik.kim@kist.re.kr; dhuber@cs.cmu.edu; tk@cs.cmu.edu						[Anonymous], 2004, STAT PATTERN RECOGNI; Bertalmio M, 2000, IEEE T PATTERN ANAL, V22, P733, DOI 10.1109/34.865191; Bouguet J. Y., 2008, CAMERA CALIBRATION T; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Cannons K., 2008, TECHNICAL REPORT; Collins RT, 2003, PROC CVPR IEEE, P234; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dowson NDH, 2005, PROC CVPR IEEE, P99; Duda R.O., 1973, J ROYAL STAT SOC SER; Ess A, 2010, INT J ROBOT RES, V29, P1707, DOI 10.1177/0278364910365417; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Haag M, 1999, INT J COMPUT VISION, V35, P295, DOI 10.1023/A:1008112528134; Hartley R., 2004, ROBOTICA; Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903; Kasper A, 2012, INT J ROBOT RES, V31, P927, DOI 10.1177/0278364912445831; KOLLER D, 1993, INT J COMPUT VISION, V10, P257, DOI 10.1007/BF01539538; Kwak K, 2014, ELECTRON LETT, V50, P600, DOI 10.1049/el.2014.0355; Kwak K., 2011, P IEEE RSJ INT C INT; Kwak K., 2010, P IEEE INT C ROB AUT; Leibe B, 2008, IEEE T PATTERN ANAL, V30, P1683, DOI 10.1109/TPAMI.2008.170; Lempitsky V. S., 2007, P IEEE COMP SOC C CO; Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001; Li Y, 2011, IEEE T PATTERN ANAL, V33, P1860, DOI 10.1109/TPAMI.2011.40; Lou HG, 2005, IEEE T IMAGE PROCESS, V14, P1561, DOI 10.1109/TIP.2005.854495; Luber M, 2009, AUTON ROBOT, V26, P141, DOI 10.1007/s10514-009-9112-4; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; MacLachlan R., 2005, CMURITR0507; Moreels P, 2007, INT J COMPUT VISION, V73, P263, DOI 10.1007/s11263-006-9967-1; Mundy JL, 2006, LECT NOTES COMPUT SC, V4170, P3; Nguyen V, 2007, AUTON ROBOT, V23, P97, DOI 10.1007/s10514-007-9034-y; Ottlik A, 2008, INT J COMPUT VISION, V80, P211, DOI 10.1007/s11263-007-0112-6; Petrovskaya A, 2009, AUTON ROBOT, V26, P123, DOI 10.1007/s10514-009-9115-1; Premebida C, 2009, J FIELD ROBOT, V26, P696, DOI 10.1002/rob.20312; Rav-Acha A., 2008, ACM SIGGRAPH 2008 C; Rothganger F., 2003, P IEEE COMP SOC C CO, V2; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Sato Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P379, DOI 10.1145/258734.258885; SCHARSTEIN D, 1994, INT C PATT RECOG, P572, DOI 10.1109/ICPR.1994.576363; Schneiderman H, 2000, PROC CVPR IEEE, P746, DOI 10.1109/CVPR.2000.855895; Shafique K, 2005, IEEE T PATTERN ANAL, V27, P51, DOI 10.1109/TPAMI.2005.1; Sinha SN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409112; Szeliski R., 2010, COMPUTER VISION ALGO, DOI DOI 10.1007/978-3-030-34372-9; Terzopoulos D., 1993, ACTIVE VISION; Torralba A, 2004, PROC CVPR IEEE, P762; Veenman CJ, 2001, IEEE T PATTERN ANAL, V23, P54, DOI 10.1109/34.899946; Xiang Y, 2014, LECT NOTES COMPUT SC, V8694, P220, DOI 10.1007/978-3-319-10599-4_15; Xiang Y, 2012, PROC CVPR IEEE, P3410, DOI 10.1109/CVPR.2012.6248081; Yan P., 2007, COMP VIS 2007 ICCV 2, P1; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Yin Z., 2007, 2007 IEEE C COMPUTER, P1, DOI DOI 10.1109/CVPR.2007.383237; Zia MZ, 2013, IEEE T PATTERN ANAL, V35, P2608, DOI 10.1109/TPAMI.2013.87; Zia MZ, 2015, INT J COMPUT VISION, V112, P188, DOI 10.1007/s11263-014-0780-y	55	2	2	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2016	117	1					48	69		10.1007/s11263-015-0848-3	http://dx.doi.org/10.1007/s11263-015-0848-3			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DF2FG					2022-12-18	WOS:000371156500003
J	Zhang, XQ; Hu, WM; Xie, NH; Bao, HJ; Maybank, S				Zhang, Xiaoqin; Hu, Weiming; Xie, Nianhua; Bao, Hujun; Maybank, Stephen			A Robust Tracking System for Low Frame Rate Video (vol 115, pg 279, 2015)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Zhang, Xiaoqin] Wenzhou Univ, Inst Intelligent Syst & Decis, Hangzhou, Zhejiang, Peoples R China; [Hu, Weiming; Xie, Nianhua] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Bao, Hujun] Zhejiang Univ, Dept Comp Sci, Hangzhou, Zhejiang, Peoples R China; [Maybank, Stephen] Birkbeck Coll, Dept Comp Sci & Informat Syst, London, England	Wenzhou University; Chinese Academy of Sciences; Institute of Automation, CAS; Zhejiang University; University of London; Birkbeck University London	Zhang, XQ (corresponding author), Wenzhou Univ, Inst Intelligent Syst & Decis, Hangzhou, Zhejiang, Peoples R China.	zhangxiaoqinnan@gmail.com; wmhu@nlpr.ia.ac.cn; nhxie@nlpr.ia.ac.cn; bao@cad.zju.edu.cn; sjmaybank@dcs.bbk.ac.uk						Zhang XQ, 2015, INT J COMPUT VISION, V115, P279, DOI 10.1007/s11263-015-0819-8	1	2	2	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2015	115	3					305	305		10.1007/s11263-015-0836-7	http://dx.doi.org/10.1007/s11263-015-0836-7			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CW6EH		Bronze			2022-12-18	WOS:000365089800004
J	Thom, M; Rapp, M; Palm, G				Thom, Markus; Rapp, Matthias; Palm, Guenther			Efficient Dictionary Learning with Sparseness-Enforcing Projections	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sparse coding; Sparse representations; Dictionary learning; Explicit sparseness constraints; Sparseness-enforcing projections	INDEPENDENT COMPONENT ANALYSIS; RECEPTIVE-FIELDS; INVARIANT FEATURES; MATRIX; SHRINKAGE; EMERGENCE; ALGORITHM; CODE; SET	Learning dictionaries suitable for sparse coding instead of using engineered bases has proven effective in a variety of image processing tasks. This paper studies the optimization of dictionaries on image data where the representation is enforced to be explicitly sparse with respect to a smooth, normalized sparseness measure. This involves the computation of Euclidean projections onto level sets of the sparseness measure. While previous algorithms for this optimization problem had at least quasi-linear time complexity, here the first algorithm with linear time complexity and constant space complexity is proposed. The key for this is the mathematically rigorous derivation of a characterization of the projection's result based on a soft-shrinkage function. This theory is applied in an original algorithm called Easy Dictionary Learning (EZDL), which learns dictionaries with a simple and fast-to-compute Hebbian-like learning rule. The new algorithm is efficient, expressive and particularly simple to implement. It is demonstrated that despite its simplicity, the proposed learning algorithm is able to generate a rich variety of dictionaries, in particular a topographic organization of atoms or separable atoms. Further, the dictionaries are as expressive as those of benchmark learning algorithms in terms of the reproduction quality on entire images, and result in an equivalent denoising performance. EZDL learns approximately 30 % faster than the already very efficient Online Dictionary Learning algorithm, and is therefore eligible for rapid data set analysis and problems with vast quantities of learning samples.	[Thom, Markus; Rapp, Matthias] Univ Ulm, DriveU Inst Measurement Control & Microtechnol, D-89081 Ulm, Germany; [Palm, Guenther] Univ Ulm, Inst Neural Informat Proc, D-89081 Ulm, Germany	Ulm University; Ulm University	Thom, M (corresponding author), Univ Ulm, DriveU Inst Measurement Control & Microtechnol, D-89081 Ulm, Germany.	markus.thom@uni-ulm.de; matthias.rapp@uni-ulm.de; guenther.palm@uni-ulm.de			Ministry of Science, Research and Arts; Universities of the State of Baden-Wurttemberg, Germany; Daimler AG, Germany	Ministry of Science, Research and Arts; Universities of the State of Baden-Wurttemberg, Germany; Daimler AG, Germany	The authors are grateful to Heiko Neumann, Florian Schule, and Michael Gabb for helpful discussions. We would like to thank Julien Mairal and Karl Skretting for making implementations of their algorithms available. Parts of this work were performed on the computational resource bwUniCluster funded by the Ministry of Science, Research and Arts and the Universities of the State of Baden-Wurttemberg, Germany, within the framework program bwHPC. This work was supported by Daimler AG, Germany.	Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; [Anonymous], 2009, 26 ANN INT C MACH LE, DOI DOI 10.1145/1553374.1553459; Bauer F., 2013, P INT C LEARN REPR; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bishop, 1995, NEURAL NETWORKS PATT; Blackford LS, 2002, ACM T MATH SOFTWARE, V28, P135, DOI 10.1145/567806.567807; Bottou U, 2004, ADV NEUR IN, V16, P217; Bredies K, 2008, J FOURIER ANAL APPL, V14, P813, DOI 10.1007/s00041-008-9041-1; Coates Adam, 2011, P 28 INT C MACH LEAR, P921; Deutsch F., 2001, BEST APPROXIMATION I; Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Duarte-Carvajalino JM, 2009, IEEE T IMAGE PROCESS, V18, P1395, DOI 10.1109/TIP.2009.2022459; Eckart C, 1936, PSYCHOMETRIKA, V1, P211, DOI 10.1007/BF02288367; Elad M, 2006, IEEE T INFORM THEORY, V52, P5559, DOI 10.1109/TIT.2006.885522; Foucart S., 2013, MATH INTRO COMPRESSI, P1, DOI DOI 10.1007/978-0-8176-4948-7; Galassi M, 2009, GNU SCI LIB REFERENC; Gharavi-Alkhansari M., 1998, 1998 P 1998 IEEE INT, V3, P1389; GOLDBERG D, 1991, COMPUT SURV, V23, P5, DOI 10.1145/103162.103163; Hawe S, 2013, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2013.63; Hoggar S. C., 2006, MATH DIGITAL IMAGES; Horev I., 2012, 2012 International Conference on Systems, Signals and Image Processing (IWSSIP), P592; Hoyer PO, 2000, NETWORK-COMP NEURAL, V11, P191, DOI 10.1088/0954-898X/11/3/302; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308; Hurley N, 2009, IEEE T INFORM THEORY, V55, P4723, DOI 10.1109/TIT.2009.2027527; Hyvarinen A, 1999, NEURAL COMPUT, V11, P1739, DOI 10.1162/089976699300016214; Hyvarinen A, 2000, NEURAL COMPUT, V12, P1705, DOI 10.1162/089976600300015312; Hyvarinen A, 2001, NEURAL COMPUT, V13, P1527, DOI 10.1162/089976601750264992; JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233; Kavukcuoglu K, 2009, PROC CVPR IEEE, P1605, DOI 10.1109/CVPRW.2009.5206545; KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325; Kreutz-Delgado K, 2003, NEURAL COMPUT, V15, P349, DOI 10.1162/089976603762552951; Laughlin SB, 2003, SCIENCE, V301, P1870, DOI 10.1126/science.1089662; Lopes M., 2013, P 30 INT C MACH LEAR, V28, P217; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308; NEUDECKE.H, 1969, J AM STAT ASSOC, V64, P953, DOI 10.2307/2283476; Olmos A, 2004, PERCEPTION, V33, P1463, DOI 10.1068/p5321; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Olshausen BA, 2003, IEEE IMAGE PROC, P41; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Potluru V. K., 2013, P INT C LEARN REPR; Rigamonti R, 2013, PROC CVPR IEEE, P2754, DOI 10.1109/CVPR.2013.355; Ringach DL, 2002, J NEUROPHYSIOL, V88, P455, DOI 10.1152/jn.2002.88.1.455; RODGERS JL, 1988, AM STAT, V42, P59, DOI 10.2307/2685263; Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486; Skretting K, 2011, INT CONF ACOUST SPEE, P1517; Skretting K, 2010, IEEE T SIGNAL PROCES, V58, P2121, DOI 10.1109/TSP.2010.2040671; Society of Motion Picture and Television Engineers (SMPTE), 1993, REC PRACT RP 177 193; Theis F. J., 2005, P EUR SIGN PROC C, V3, P1672; Thom M, 2013, J MACH LEARN RES, V14, P1091; Tosic I, 2011, IEEE J-STSP, V5, P941, DOI 10.1109/JSTSP.2011.2158063; Traub J. F., 1964, ITERATIVE METHODS SO; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P2315, DOI 10.1098/rspb.1998.0577; Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649; Watson A. B., 1994, MATH J, V4, P81, DOI DOI 10.1016/0165-1684(90; Willmore B, 2001, NETWORK-COMP NEURAL, V12, P255, DOI 10.1088/0954-898X/12/3/302; Wilson DR, 2003, NEURAL NETWORKS, V16, P1429, DOI 10.1016/S0893-6080(03)00138-2; Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Zelnik-Manor L, 2012, IEEE T SIGNAL PROCES, V60, P2386, DOI 10.1109/TSP.2012.2187642	67	2	2	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2015	114	2-3			SI		168	194		10.1007/s11263-015-0799-8	http://dx.doi.org/10.1007/s11263-015-0799-8			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CP7MJ		Green Submitted			2022-12-18	WOS:000360071900005
J	Lehmann, AD; Gehler, PV; Van Gool, L				Lehmann, Alain D.; Gehler, Peter V.; Van Gool, Luc			Branch&Rank for Efficient Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Branch&rank; Object detection; Non-linear kernel classifier; Sub-linear detection	SCENE	Ranking hypothesis sets is a powerful concept for efficient object detection. In this work, we propose a branch&rank scheme that detects objects with often less than 100 ranking operations. This efficiency enables the use of strong and also costly classifiers like non-linear SVMs with RBF- kernels. We thereby relieve an inherent limitation of branch&bound methods as bounds are often not tight enough to be effective in practice. Our approach features three key components: a ranking function that operates on sets of hypotheses and a grouping of these into different tasks. Detection efficiency results from adaptively sub-dividing the object search space into decreasingly smaller sets. This is inherited from branch&bound, while the ranking function supersedes a tight bound which is often unavailable (except for rather limited function classes). The grouping makes the system effective: it separates image classification from object recognition, yet combines them in a single formulation, phrased as a structured SVM problem. A novel aspect of branch&rank is that a better ranking function is expected to decrease the number of classifier calls during detection. We use the VOC'07 dataset to demonstrate the algorithmic properties of branch&rank.	[Lehmann, Alain D.; Van Gool, Luc] ETH, Comp Vis Lab, Zurich, Switzerland; [Gehler, Peter V.] MPI Intelligent Syst, Tubingen, Germany; [Van Gool, Luc] Katholieke Univ Leuven, ESAT PSI, IBBT, Louvain, Belgium	Swiss Federal Institutes of Technology Domain; ETH Zurich; Max Planck Society; KU Leuven	Lehmann, AD (corresponding author), ETH, Comp Vis Lab, Zurich, Switzerland.	lehmann@vision.ee.ethz.ch; pgehler@tuebingen.mpg.de; vangool@vision.ee.ethz.ch						Alexe B., 2010, IEEE COMP SOC C COMP; Bileschi S., 2005, P BRIT MACH VIS C; Blaschko M., 2010, ADV NEURAL INFORM PR; Blaschko M. B., 2008, EUR C COMP VIS; Blaschko M. B., 2009, P BRIT MACH VIS C; Blaschko M. B., 2011, ENERGY MINIMAZATION; Bottou L., 2008, P ADV NEUR INF PROC; Boureau Y.-L., 2010, ICML, P111, DOI DOI 10.5555/3104322.3104338; Breuel T. M., 2002, EUR C COMP VIS; Burges Chris, 2005, INT C MACH LEARN; Carreira J., 2010, P IEEE C COMP VIS PA; Chapelle O, 2010, INFORM RETRIEVAL, V13, P201, DOI 10.1007/s10791-009-9109-9; Chum O., 2007, IEEE COMP SOC C COMP; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Desai C., 2009, INT C COMP VIS; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; FELZENSZWALB P, 2008, IEEE COMP SOC C COMP; Felzenszwalb P., 2010, IEEE COMP SOC C COMP; Gall J., 2009, IEEE COMP SOC C COMP; Gangaputra S., 2006, IEEE COMP SOC C COMP; Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169; Griffin Gregory, 2007, CALTECH 256 OBJECT C; Harzallah H., 2009, INT C COMP VIS; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Keysers D., 2007, ELECT LETT COMPUTER, V6, P44; Ladicky L., 2011, INT C MACH LEARN; Lampert C. H., 2010, IEEE COMP SOC C COMP; Lampert CH, 2009, MACH LEARN, V77, P249, DOI 10.1007/s10994-009-5111-0; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lehmann A., 2009, INT C COMP VIS; Lehmann A.D., 2011, THESIS EIDGENOSSISCH, DOI [10. 3929/ethz-a-006706798, DOI 10.3929/ETHZ-A-006706798.(DISS.NR.19868]; Lehmann A, 2011, INT J COMPUT VISION, V94, P175, DOI 10.1007/s11263-010-0342-x; Lehmann AD, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.8; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MURPHY K, 2003, ADV NEURAL INFORM PR; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Park D, 2010, LECT NOTES COMPUT SC, V6314, P241, DOI 10.1007/978-3-642-15561-1_18; Pedersoli M., 2010, EUR C COMP VIS; Prisacariu V., 2009, FASTHOG A REAL TIME; Razavi N., 2011, IEEE COMP SOC C COMP; Robertson S. E., 1994, ACM SIGIR C RES DEV; TAO L. T., 2010, P IEEE C COMP VIS PA; Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951; Torralba A, 2010, COMMUN ACM, V53, P107, DOI 10.1145/1666420.1666446; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Weiss D., 2010, ADV NEURAL INFORM PR; Wojek C, 2008, LECT NOTES COMPUT SC, V5096, P71, DOI 10.1007/978-3-540-69321-5_8; Wolf L, 2006, INT J COMPUT VISION, V69, P251, DOI 10.1007/s11263-006-7538-0; Yeh T., 2009, IEEE COMP SOC C COMP; Zhang Z., 2011, IEEE COMP SOC C COMP; Zhang Z., 2011, ADV NEURAL INFORM PR	55	2	2	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2014	106	3			SI		252	268		10.1007/s11263-013-0670-8	http://dx.doi.org/10.1007/s11263-013-0670-8			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AA3DC		Green Published, hybrid			2022-12-18	WOS:000330972100003
J	Marsland, S; McLachlan, RI; Modin, K; Perlmutter, M				Marsland, Stephen; McLachlan, Robert I.; Modin, Klas; Perlmutter, Matthew			Geodesic Warps by Conformal Mappings	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image registration; Conformal mappings; Infinite dimensional manifolds; Geodesic warps; LDDMM	DIFFEOMORPHISMS; FLOWS	In recent years there has been considerable interest in methods for diffeomorphic warping of images, with applications in e.g. medical imaging and evolutionary biology. The original work generally cited is that of the evolutionary biologist D'Arcy Wentworth Thompson, who demonstrated warps to deform images of one species into another. However, unlike the deformations in modern methods, which are drawn from the full set of diffeomorphisms, he deliberately chose lower-dimensional sets of transformations, such as planar conformal mappings. In this paper we study warps composed of such conformal mappings. The approach is to equip the infinite dimensional manifold of conformal embeddings with a Riemannian metric, and then use the corresponding geodesic equation in order to obtain diffeomorphic warps. After deriving the geodesic equation, a numerical discretisation method is developed. Several examples of geodesic warps are then given. We also show that the equation admits totally geodesic solutions corresponding to scaling and translation, but not to affine transformations.	[Marsland, Stephen] Massey Univ, SEAT, Palmerston North, New Zealand; [McLachlan, Robert I.; Modin, Klas; Perlmutter, Matthew] Massey Univ, IFS, Palmerston North, New Zealand	Massey University; Massey University	Modin, K (corresponding author), Massey Univ, IFS, Private Bag 11222, Palmerston North, New Zealand.	s.r.marsland@massey.ac.nz; r.mclachlan@massey.ac.nz; klas.modin@chalmers.se; matthew@mat.ufmg.br		McLachlan, Robert/0000-0003-0392-4957; Modin, Klas/0000-0001-6900-1122	Royal Society of New Zealand Marsden Fund; Massey University Postdoctoral Fellowship Fund	Royal Society of New Zealand Marsden Fund(Royal Society of New ZealandMarsden Fund (NZ)); Massey University Postdoctoral Fellowship Fund	This work was funded by the Royal Society of New Zealand Marsden Fund and the Massey University Postdoctoral Fellowship Fund. The authors would like to thank the reviewers for helpful comments and suggestions.	[Anonymous], 2009, SERIES MODERN SURVEY; Arthur W, 2006, NAT REV GENET, V7, P401, DOI 10.1038/nrg1835; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; BEG MF, 2003, THESIS J HOPKINS U; Dupuis P, 1998, Q APPL MATH, V56, P587, DOI 10.1090/qam/1632326; EBIN DG, 1970, ANN MATH, V92, P102, DOI 10.2307/1970699; Gay-Balmaz F, 2012, J NONLINEAR SCI, V22, P463, DOI 10.1007/s00332-012-9143-4; Joshi SC, 2000, IEEE T IMAGE PROCESS, V9, P1357, DOI 10.1109/83.855431; Khesin B., 1998, APPL MATH SCI, V125; Lang S., 1999, FUNDAMENTALS DIFFERE, V191; Marsden JE, 2001, ACT NUMERIC, V10, P357, DOI 10.1017/S096249290100006X; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Miller MI, 2001, INT J COMPUT VISION, V41, P61, DOI 10.1023/A:1011161132514; Modin K, 2011, J GEOM PHYS, V61, P1446, DOI 10.1016/j.geomphys.2011.03.007; Sharon E, 2006, INT J COMPUT VISION, V70, P55, DOI 10.1007/s11263-006-6121-z; Shkoller S, 1998, J FUNCT ANAL, V160, P337, DOI 10.1006/jfan.1998.3335; Thompson D. A. W, 1917, GROWTH FORM; Trouve A, 1998, INT J COMPUT VISION, V28, P213, DOI 10.1023/A:1008001603737; Trouve A., 1995, TECHNICAL REPORT; Younes L., 2010, SHAPES DIFFEOMORPHIS; [No title captured]; [No title captured]	25	2	2	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2013	105	2			SI		144	154		10.1007/s11263-012-0584-x	http://dx.doi.org/10.1007/s11263-012-0584-x			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	207YP		Green Submitted			2022-12-18	WOS:000323641100004
J	Angst, R; Pollefeys, M				Angst, Roland; Pollefeys, Marc			Multilinear Factorizations for Multi-Camera Rigid Structure from Motion Problems	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision; 3D reconstruction; Structure from motion; Multilinear factorizations; Tensor algebra	NONRIGID SHAPE; CALIBRATION; SUBSPACES	Camera networks have gained increased importance in recent years. Existing approaches mostly use point correspondences between different camera views to calibrate such systems. However, it is often difficult or even impossible to establish such correspondences. But even without feature point correspondences between different camera views, if the cameras are temporally synchronized then the data from the cameras are strongly linked together by the motion correspondence: all the cameras observe the same motion. The present article therefore develops the necessary theory to use this motion correspondence for general rigid as well as planar rigid motions. Given multiple static affine cameras which observe a rigidly moving object and track feature points located on this object, what can be said about the resulting point trajectories? Are there any useful algebraic constraints hidden in the data? Is a 3D reconstruction of the scene possible even if there are no point correspondences between the different cameras? And if so, how many points are sufficient? Is there an algorithm which warrants finding the correct solution to this highly non-convex problem? This article addresses these questions and thereby introduces the concept of low-dimensional motion subspaces. The constraints provided by these motion subspaces enable an algorithm which ensures finding the correct solution to this non-convex reconstruction problem. The algorithm is based on multilinear analysis, matrix and tensor factorizations. Our new approach can handle extreme configurations, e.g. a camera in a camera network tracking only one single point. Results on synthetic as well as on real data sequences act as a proof of concept for the presented insights.	[Angst, Roland; Pollefeys, Marc] ETH, CH-8092 Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Angst, R (corresponding author), ETH, Univ Str 6, CH-8092 Zurich, Switzerland.	rangst@inf.ethz.ch; marc.pollefeys@inf.ethz.ch	Pollefeys, Marc/I-7607-2013		Google Fellowship; 4DVideo ERC Starting [210806]; Packard Foundation Fellowship	Google Fellowship(Google Incorporated); 4DVideo ERC Starting; Packard Foundation Fellowship	Roland Angst is a recipient of the Google Europe Fellowship in Computer Vision, and this research is supported in part by this Google Fellowship. We gratefully acknowledge the support of the 4DVideo ERC Starting Grant Nr. 210806 and a Packard Foundation Fellowship.	Aguiar P., 2008, IEEE C COMP VIS PATT; Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201; Angst R, 2010, LECT NOTES COMPUT SC, V6313, P144; Angst R, 2009, IEEE I CONF COMP VIS, P1203, DOI 10.1109/ICCV.2009.5459337; Brand M, 2005, PROC CVPR IEEE, P122; Brand M, 2001, PROC CVPR IEEE, P456; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; Chen P, 2008, INT J COMPUT VISION, V80, P125, DOI 10.1007/s11263-008-0135-7; Daniilidis K, 1999, INT J ROBOT RES, V18, P286, DOI 10.1177/02783649922066213; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Del Bue A, 2006, INT J COMPUT VISION, V66, P193, DOI 10.1007/s11263-005-3958-5; Guerreiro RFC, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P897, DOI 10.1109/ICIP.2002.1039117; Harshman R. A., 1970, WORKING PAPERS PHONE, V16; Hartley R., 2004, MULTIPLE VIEW GEOMET, DOI DOI 10.1017/CBO9780511811685; Hartley R., 2004, JAP AUSTR WORKSH COM; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Li J, 2005, IEEE WORKSH MOT VID, V2, P154; Magnus J. R, 1999, MATRIX DIFFERENTIAL; Pollefeys M., 2008, IEEE C COMP VIS PATT; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; Svoboda T, 2005, PRESENCE-VIRTUAL AUG, V14, P407, DOI 10.1162/105474605774785325; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L, 2001, PROC CVPR IEEE, P493; Tresadern P, 2005, PROC CVPR IEEE, P1110; Tron Roberto, 2007, IEEE C COMP VIS PATT; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Vidal R, 2002, LECT NOTES COMPUT SC, V2351, P383; Wagner D., 2007, ARTOOLKITPLUS POSE T; Wang G, 2008, PATTERN RECOGN LETT, V29, P72, DOI 10.1016/j.patrec.2007.09.004; Wolf L, 2006, INT J COMPUT VISION, V68, P43, DOI 10.1007/s11263-005-4841-0; Xiao J, 2004, LECT NOTES COMPUT SC, V2034, P573; Yan JY, 2008, IEEE T PATTERN ANAL, V30, P865, DOI 10.1109/TPAMI.2007.70739; Zelnik-Manor L, 2006, INT J COMPUT VISION, V67, P313, DOI 10.1007/s11263-006-5157-4	34	2	3	0	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2013	103	2			SI		240	266		10.1007/s11263-012-0581-0	http://dx.doi.org/10.1007/s11263-012-0581-0			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	150JC		Green Published, Green Submitted			2022-12-18	WOS:000319385400006
J	Taddei, P; Espuny, F; Caglioti, V				Taddei, Pierluigi; Espuny, Ferran; Caglioti, Vincenzo			Planar Motion Estimation and Linear Ground Plane Rectification using an Uncalibrated Generic Camera	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Self-calibration; Plane rectification; Visual odometry; Generic camera; Motion flow; Planar motion	ODOMETRY	We address and solve the self-calibration of a generic camera that performs planar motion while viewing (part of) a ground plane. Concretely, assuming initial sets of correspondences between several images of the ground plane as known, we are interested in determining both the camera motion and the geometry of the ground plane. The latter is obtained through the rectification of the image of the ground plane, which gives a bijective correspondence between pixels and points on the ground plane. We initially propose a method to determine the camera motion by using the motion flow between pairs of images. We perform this step with no need of camera calibration. Our solution requires the fixed ground point of the camera motion to be visible on both images. Once the camera motion is known, either by using our method or by other alternative means (e.g. GPS-based), we show that the rectification of the ground plane can be determined linearly from at least three images up to a scale factor. Experimental results on real images are presented at the end of the paper to validate the proposed methods.	[Taddei, Pierluigi] European Commiss, Joint Res Ctr, Ispra, Italy; [Espuny, Ferran] Univ Barcelona, Dept Algebra & Geometria, Barcelona, Spain; [Caglioti, Vincenzo] Politecn Milan, I-20133 Milan, Italy	European Commission Joint Research Centre; EC JRC ISPRA Site; University of Barcelona; Polytechnic University of Milan	Taddei, P (corresponding author), European Commiss, Joint Res Ctr, Ispra, Italy.	pierluigi.taddei@polimi.it; fespuny@ub.edu; vincenzo.caglioti@polimi.it	Espuny Pujol, Ferran/AAY-9267-2021	Espuny Pujol, Ferran/0000-0001-9085-7400; Caglioti, Vincenzo/0000-0003-2741-7474	Spanish project [MTM2006-14234-C02-01]	Spanish project(Spanish Government)	This paper was written during an internship at Politecnico di Milano of Ferran Espuny, who received the financial support of the Spanish project MTM2006-14234-C02-01.	Agrawal M., 2006, ICPR; Benhimane S., 2006, ICRA; Bonarini A., 2006, IROS; Borenstein J, 1996, IEEE T ROBOTIC AUTOM, V12, P869, DOI 10.1109/70.544770; Bunschoten R., 2003, ICRA; Caglioti V., 2008, OMNIVIS; Caglioti V., 2007, INT WORKSH ROB VIS; Cheng Y, 2006, IEEE ROBOT AUTOM MAG, V13, P54, DOI 10.1109/MRA.2006.1638016; Comport A., 2007, ICRA; Corke P., 2004, IROS; Davison A. J., 2003, ICCV, p[1, 4]; Espuny F., 2007, VISAPP; Gluckman J., 1998, ICCV; GOSHTASBY A, 1988, IMAGE VISION COMPUT, V6, P255, DOI 10.1016/0262-8856(88)90016-9; Grossberg Michael D., 2001, ICCV; Hartley R., 2004, ROBOTICA; Knight J., 2003, CVPR; McCarthy C., 2004, ICRA; Nister D., 2004, CVPR; Prautzsch H., 2002, BEZIER B SPLINE TECH, V6; Ramalingam S., 2005, OMNIVIS; Sturm P, 2004, LECT NOTES COMPUT SC, V3022, P1; TRIGGS B, 1998, ECCV; Wang H., 2005, ICIA	24	2	2	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2012	96	2					162	174		10.1007/s11263-011-0457-8	http://dx.doi.org/10.1007/s11263-011-0457-8			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	876AW					2022-12-18	WOS:000299080200002
J	Dellepiane, M; Venturi, A; Scopigno, R				Dellepiane, Matteo; Venturi, Andrea; Scopigno, Roberto			Image Guided Reconstruction of Un-sampled Data: A Filling Technique for Cultural Heritage Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hole filling; Laser triangulation; Image registration	REPAIR	Cultural Heritage (CH) is one of the major fields of application of 3D scanning technologies. In this context, one of the main limitations perceived by the practitioners is the uncompleteness of the sampling. Whenever we scan a complex artifact, the produced sampling usually presents a large number of unsampled regions. Many algorithmic solutions exist to close those gaps (from specific hole-filling algorithms to the drastic solution of using water-tight reconstruction methods). Unfortunately, adding patches over un-sampled regions is an issue in CH applications: if the 3D model should be used as a master document over the shape (and status) of the artwork, informed CH curators usually do not accept that an algorithm is used to guess portions of a surface. In this paper, we present a low-cost setup and related algorithms to reconstruct un-sampled portions of the 3D models by inferring information about the real shape of the missing region from photographs. Data needed to drive the surface completion process are obtained by coupling a calibrated pattern of laser diodes to a digital camera. Thus, we are proposing a simple active acquisition device (based on consumer components and more flexible than standard 3D scanning devices) to improve selectively the sampling produced by a standard 3D scanning device. After acquiring one or more images with the laser-enhanced camera, an almost completely automatic process analyzes the image/s in order to extract the pattern, to estimate the laser projector intersections over the surface and determining coordinates of those points (using the consolidated triangulation approach). Then, the gathered geometric data are used to steer the hole filling in order to obtain a patch which is coherent with the real shape of the object. A series of tests on real objects proves that our method is able to recover geometrical features that cannot be reconstructed using state-of-the-art methods. Consequently, it can be used to obtain complete 3D models without creating plausible but false data.	[Dellepiane, Matteo; Venturi, Andrea; Scopigno, Roberto] ISTI CNR, Visual Comp Lab, I-56124 Pisa, PI, Italy	Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR)	Dellepiane, M (corresponding author), ISTI CNR, Visual Comp Lab, Via G Moruzzi 1, I-56124 Pisa, PI, Italy.	dellepiane@isti.cnr.it; scopigno@isti.cnr.it	scopigno, roberto/AAH-7645-2020		European Community [231809]; Tuscany Regional Project STArT	European Community(European Commission); Tuscany Regional Project STArT	The research leading to these results has received funding from the European Community's Seventh Framework Programme (/FP7/2007-2013)/, in the context of 3D-COFORM project, under/grant agreement /n<SUP>o</SUP> 231809 and from Tuscany Regional Project STArT. We would like to thank Paolo Cignoni for his supervision, and Federico Ponchio and Massimiliano Corsini for their help.	Akbarzadeh A, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P1; Amenta N, 2004, ACM T GRAPHIC, V23, P264, DOI 10.1145/1015706.1015713; BECKER J, 2009, INT WORKSH 3 D DIG I; BENDELS GH, 2005, 6 INT S VIRT REAL AR, P41; Bischoff S, 2005, ACM T GRAPHIC, V24, P1332, DOI 10.1145/1095878.1095883; Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266; Corsini M, 2009, COMPUT GRAPH FORUM, V28, P1755, DOI 10.1111/j.1467-8659.2009.01552.x; Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098; Franken T, 2005, VISUAL COMPUT, V21, P619, DOI 10.1007/s00371-005-0309-z; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Goesele M., 2007, MULTIVIEW STEREO COM, P1; Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Lensch HPA, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P317, DOI 10.1109/PCCGA.2000.883955; Liao M., 2009, ICCV; Liepa P., 2003, Symposium on Geometry Processing, P200; LIU L, 2006, COMPUTER VISION PATT, V2, P2293, DOI DOI 10.1109/CVPR.2006.204; Lorensen W. E., 1987, COMPUTER GRAPHICS, V21; Lourakis M.I., 2004, LEVMAR LEVENBERG MAR; Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006; Park S, 2005, IEEE I CONF COMP VIS, P1260; PARUS MVJ, 2005, ALGORITMY 2005; PRZYBILLA H, 2006, IEVM06; Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814; Vergauwen M, 2006, MACH VISION APPL, V17, P411, DOI 10.1007/s00138-006-0027-1; Xu S, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P310	26	2	2	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2011	94	1					2	11		10.1007/s11263-010-0382-2	http://dx.doi.org/10.1007/s11263-010-0382-2			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	760JM		Green Submitted			2022-12-18	WOS:000290320600001
J	Dellen, B; Worgotter, F				Dellen, Babette; Woergoetter, Florentin			A Local Algorithm for the Computation of Image Velocity via Constructive Interference of Global Fourier Components	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image velocity; Local methods; Fourier transformation; Constructive interference	OPTIC FLOW COMPUTATION	A novel Fourier-based technique for local motion detection from image sequences is proposed. In this method, the instantaneous velocities of local image points are inferred directly from the global 3D Fourier components of the image sequence. This is done by selecting those velocities for which the superposition of the corresponding Fourier gratings leads to constructive interference at the image point. Hence, image velocities can be assigned locally even though position is computed from the phases and amplitudes of global Fourier components (spanning the whole image sequence) that have been filtered based on the motion constraint equation, reducing certain aperture effects typically arising from windowing in other methods. Regularization is introduced for sequences having smooth flow fields. Aperture effects and their effect on optic-flow regularization are investigated in this context. The algorithm is tested on both synthetic and real image sequences and the results are compared to those of other local methods. Finally, we show that other motion features, i.e. motion direction, can be computed using the same algorithmic framework without requiring an intermediate representation of local velocity, which is an important characteristic of the proposed method.	[Dellen, Babette] CSIC UPC, Inst Robot & Informat Ind, Barcelona 08028, Spain; [Woergoetter, Florentin] Univ Gottingen, Bernstein Ctr Computat Neurosci Gottingen, Phys Inst Biophys 3, D-37077 Gottingen, Germany	Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i Informatica Industrial (IRII); Universitat Politecnica de Catalunya; University of Gottingen	Dellen, B (corresponding author), CSIC UPC, Inst Robot & Informat Ind, Llorens i Artigas 4-6, Barcelona 08028, Spain.	bdellen@iri.upc.edu			German Ministry for Education and Research (BMBF) via the Bernstein Center for Computational Neuroscience (BCCN) Gottingen [01GQ0430]; EU [016276-2]; Spanish Ministry for Science and Innovation via a Ramon y Cajal	German Ministry for Education and Research (BMBF) via the Bernstein Center for Computational Neuroscience (BCCN) Gottingen(Federal Ministry of Education & Research (BMBF)); EU(European Commission); Spanish Ministry for Science and Innovation via a Ramon y Cajal	The work has received support by the German Ministry for Education and Research (BMBF) via the Bernstein Center for Computational Neuroscience (BCCN) Gottingen under Grant No. 01GQ0430 and the EU project Drivsco under contract number 016276-2. B.D. also acknowledges support from the Spanish Ministry for Science and Innovation via a Ramon y Cajal Fellowship.	Aach T, 2006, IEEE T IMAGE PROCESS, V15, P3690, DOI 10.1109/TIP.2006.884921; ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; ANANDAN P, 1987, INT J COMPUT VISION, V2, P283; BAKER S, 2007, ICCV 2007, P2007; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; BRIASSOULI A, 2006, P 18 INT C PATT REC; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; COOKE T, 2008, DICTA 2008, P2008; DELLEN B, 2008, P BRIT MACH VIS C; Dellen BK, 2007, INT J MOD PHYS B, V21, P2493, DOI 10.1142/S021797920704383X; FELZENSZWALB PF, 2006, INT J COMPUTER VISIO, V70; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; GALVIN B, 1998, P BRIT MACH VIS C, P195; Gautama T, 2002, IEEE T NEURAL NETWOR, V13, P1127, DOI 10.1109/TNN.2002.1031944; HEEGER DJ, 1987, J OPT SOC AM A, V4, P1455, DOI 10.1364/JOSAA.4.001455; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; Mota C, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P917, DOI 10.1109/ICIP.2001.958644; NAGEL HH, 1987, CGIP, V21, P85; Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y; SHIZAWA M, 1990, P IEEE ICVPR; SIMONCELLI EP, 1993, THESIS DEP ELECT ENG; Singh A., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P168, DOI 10.1109/ICCV.1990.139516; VERNON D, 1998, LECT NOTES COMPUTER, P69; Weickert J, 2001, J MATH IMAGING VIS, V14, P245, DOI 10.1023/A:1011286029287; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; WILSON R, 1984, IEEE T PATTERN ANAL, P6	28	2	2	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2011	92	1					53	70		10.1007/s11263-010-0402-2	http://dx.doi.org/10.1007/s11263-010-0402-2			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	729GH		Green Published, Green Submitted, Bronze			2022-12-18	WOS:000287936200003
J	Miyazaki, D; Shibata, T; Ikeuchi, K				Miyazaki, Daisuke; Shibata, Takushi; Ikeuchi, Katsushi			Wavelet-Texture Method: Appearance Compression by Polarization, Parametric Reflection Model, and Daubechies Wavelet	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Workshop on Photometric Analysis for Computer Vision held in Conjunction with the 11th International Conference on Computer Vision Conference	OCT 04, 2007	Rio de Janeiro, BRAZIL			BRDF; Daubechies wavelet; Polarization; Torrance-Sparrow model	ILLUMINANT DIRECTION; COLOR; RANGE; SHAPE; SURFACES; OBJECTS	In order to create a photorealistic Virtual Reality model, we have to record the appearance of the object from different directions under different illuminations. In this paper, we propose a method that renders photorealistic images from a small amount of data. First, we separate the images of the object into a diffuse reflection component and a specular reflection component by using linear polarizers. Then, we estimate the parameters of the reflection model for each component. Finally, we compress the difference between the input images and the rendered images by using wavelet transform. At the rendering stage, we first calculate the diffuse and specular reflection images from the reflection parameters, then add the difference decompressed by inverse wavelet transform into the calculated reflection images, and finally obtain the photorealistic image of the object.	[Miyazaki, Daisuke; Shibata, Takushi; Ikeuchi, Katsushi] Univ Tokyo, Inst Ind Sci, Meguro Ku, Tokyo 1538505, Japan	University of Tokyo	Miyazaki, D (corresponding author), Univ Tokyo, Inst Ind Sci, Meguro Ku, 4-6-1 Komaba, Tokyo 1538505, Japan.	miyazaki@hiroshima-cu.ac.jp						Atkinson GA, 2005, IEEE I CONF COMP VIS, P309; ATKINSON GA, 2006, P IEEE COMP SOC C CO, P495; BARIBEAU R, 1992, IEEE T PATTERN ANAL, V14, P263, DOI 10.1109/34.121793; Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395; Cheng L, 2005, LECT NOTES COMPUT SC, V3420, P662; Cula OG, 2005, PROC CVPR IEEE, P1116; Daubechies I, 1992, 10 LECT WAVELETS, P357, DOI DOI 10.1063/1.4823127; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; Fuchs M, 2005, IEEE T VIS COMPUT GR, V11, P296, DOI 10.1109/TVCG.2005.47; Furukawa R., 2002, P 13 EUR WORKSH REND, P257; GEORGHIADES AS, 2003, P EUR WORKSH REND, P230; Goldman DB, 2005, IEEE I CONF COMP VIS, P341; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; Hara K, 2005, IEEE T PATTERN ANAL, V27, P493, DOI 10.1109/TPAMI.2005.82; Hara K, 2005, IEEE I CONF COMP VIS, P1627; Hawkins T., 2001, P C VIRT REAL ARCH C, P333; IKEUCHI K, 1991, IEEE T PATTERN ANAL, V13, P1139, DOI 10.1109/34.103274; JU DY, 2002, SIGGRAPH 2002 SKETCH, P199; KAY G, 1994, CVGIP-IMAG UNDERSTAN, V59, P183, DOI 10.1006/ciun.1994.1012; Kim CY, 1998, J OPT SOC AM A, V15, P2341, DOI 10.1364/JOSAA.15.002341; Kim T, 2005, IEEE I CONF COMP VIS, P266; KLINKER GJ, 1988, INT J COMPUT VISION, V2, P7, DOI 10.1007/BF00836279; Lalonde P, 1999, PROC GRAPH INTERF, P107; Lensch HPA, 2003, ACM T GRAPHIC, V22, P234, DOI 10.1145/636886.636891; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; LI J, 2001, INT J IMAGE GRAPHICS, V1, P45; Lin S, 1997, COMPUT VIS IMAGE UND, V65, P336, DOI 10.1006/cviu.1996.0577; Liu CJ, 2004, IEEE T PATTERN ANAL, V26, P572, DOI 10.1109/TPAMI.2004.1273927; LU JP, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P80; Ma W.-C., 2005, P 2005 S INT 3D GRAP, P187; Machida T, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P170; Magnor M, 2000, IEEE T CIRC SYST VID, V10, P338, DOI 10.1109/76.836278; Magnor M, 2003, IEEE T CIRC SYST VID, V13, P1092, DOI 10.1109/TCSVT.2003.817630; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Mallick SP, 2005, PROC CVPR IEEE, P619; Marschner SR, 2000, APPL OPTICS, V39, P2592, DOI 10.1364/AO.39.002592; MARSCHNER SR, 2000, P 11 EUR WORKSH REND, P231; Masselus Vincent, 2004, RENDERING TECHNIQUES, P287; Miyazaki D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P982; Miyazaki D, 2004, IEEE T PATTERN ANAL, V26, P73, DOI 10.1109/TPAMI.2004.1261080; Miyazaki D, 2007, IEEE T PATTERN ANAL, V29, P2018, DOI 10.1109/TPAMI.2007.1117; Mukaigawa Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P652, DOI 10.1109/ICCV.2001.937688; Nayar SK, 1997, INT J COMPUT VISION, V21, P163, DOI 10.1023/A:1007937815113; NAYAR SK, 1990, IEEE T ROBOTIC AUTOM, V6, P418, DOI 10.1109/70.59367; Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280; Nishino K, 2001, IEEE T PATTERN ANAL, V23, P1257, DOI 10.1109/34.969116; Nishino K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P599, DOI 10.1109/ICCV.2001.937573; Oishi T, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P476, DOI 10.1109/3DIM.2005.41; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684; PETER I, 2001, P EUR WORKSH REND TE, P127; RAMAMOORTHI R, 2001, P SIGGRAPH 2001, P379; Sagawa R, 2005, IEEE T PATTERN ANAL, V27, P392, DOI 10.1109/TPAMI.2005.46; Sato I., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P875, DOI 10.1109/ICCV.1999.790314; Sato I, 2007, INT J COMPUT VISION, V75, P29, DOI 10.1007/s11263-007-0036-1; SATO Y, 1994, J OPT SOC AM A, V11, P2990, DOI 10.1364/JOSAA.11.002990; SATO Y, 1997, P SIGGRAPH 97, P379; Seeling P, 2005, IEEE IC COMP COM NET, P375, DOI 10.1109/ICCCN.2005.1523890; Shashua A, 1997, INT J COMPUT VISION, V21, P99, DOI 10.1023/A:1007975506780; Shen L, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P326; SHIBATA T, 2005, P SOC PHOTO-OPT INS, V5888, P25; Shum HY, 2005, IEEE T MULTIMEDIA, V7, P85, DOI 10.1109/TMM.2004.840591; Shum HY, 2003, IEEE T CIRC SYST VID, V13, P1020, DOI 10.1109/TCSVT.2003.817360; Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804; Sun B, 2007, IEEE T VIS COMPUT GR, V13, P595, DOI 10.1109/TVCG.2007.1013; Tan RT, 2005, IEEE T PATTERN ANAL, V27, P178, DOI 10.1109/TPAMI.2005.36; Tominaga S, 2000, IEEE COMPUT GRAPH, V20, P58, DOI 10.1109/38.865881; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Tsai R.Y., 1986, P IEEE C COMP VIS PA, P364; Umeyama S, 2004, IEEE T PATTERN ANAL, V26, P639, DOI 10.1109/TPAMI.2004.1273960; Vasilescu MAO, 2004, ACM T GRAPHIC, V23, P336, DOI 10.1145/1015706.1015725; Wang HC, 2005, ACM T GRAPHIC, V24, P527, DOI 10.1145/1073204.1073224; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; Wood DN, 2000, COMP GRAPH, P287, DOI 10.1145/344779.344925; Wu J., 2002, THESIS; Zheng B, 2007, LECT NOTES COMPUT SC, V4844, P289; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658	77	2	2	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	2-3			SI		171	191		10.1007/s11263-009-0244-y	http://dx.doi.org/10.1007/s11263-009-0244-y			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	534NA					2022-12-18	WOS:000272903200005
J	Janssen, BJ; Duits, R				Janssen, Bart J.; Duits, Remco			Linear Image Reconstruction by Sobolev Norms on the Bounded Domain	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Reconstruction; Image reconstruction; Scale space; Deep structure; Generalized sampling; Bounded domain; Sobolev space; Interpolation; Singular point	VARIATIONAL APPROACH	The reconstruction problem is usually formulated as a variational problem in which one searches for that image that minimizes a so called prior (image model) while insisting on certain image features to be preserved. When the prior can be described by a norm induced by some inner product on a Hilbert space, the exact solution to the variational problem can be found by orthogonal projection. In previous work we considered the image as compactly supported in L-2(R-2) and we used Sobolev norms on the unbounded domain including a smoothing parameter gamma > 0 to tune the smoothness of the reconstructed image. Due to the assumption of compact support of the original image, components of the reconstructed image near the image boundary are too much penalized. Therefore, in this work we minimize Sobolev norms only on the actual image domain, yielding much better reconstructions (especially for gamma >> 0). As an example we apply our method to the reconstruction of singular points that are present in the scale space representation of an image.	[Janssen, Bart J.; Duits, Remco] Eindhoven Univ Technol, Dept Math, NL-5600 MB Eindhoven, Netherlands; [Janssen, Bart J.; Duits, Remco] Eindhoven Univ Technol, Dept Biomed Engn, NL-5600 MB Eindhoven, Netherlands	Eindhoven University of Technology; Eindhoven University of Technology	Janssen, BJ (corresponding author), Eindhoven Univ Technol, Dept Math, POB 513, NL-5600 MB Eindhoven, Netherlands.	b.j.janssen@tue.nl; r.duits@tue.nl			Netherlands Organisation for Scientific Research (NWO)	Netherlands Organisation for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	The authors would like to thank the anonymous referees whose comments and suggestions led to a significant improvement of the presentation and organization of this paper. The Netherlands Organisation for Scientific Research (NWO) is gratefully acknowledged for financial support. The image in Fig. 4 was provided by Dr. B. Platel, Eindhoven University of Technology, department of Biomedical Engineering.	Averbuch A, 1998, SIAM J SCI COMPUT, V19, P933, DOI 10.1137/S1064827595288589; BOERSMA J, 2002, SIAM 100 DOLLAR 100; Butterworth S., 1930, WIRELESS ENG, V7, P536; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; DAMON J, 1995, J DIFFER EQUATIONS, V115, P368, DOI 10.1006/jdeq.1995.1019; Daubechies I., 1992, SOC IND APPL MATH, DOI [10.1137/1.9781611970104, DOI 10.1137/1.9781611970104]; Duchon J., 1977, SPLINES MINIMIZING R; Duits R, 2004, J MATH IMAGING VIS, V20, P267, DOI 10.1023/B:JMIV.0000024043.96722.aa; Duits R, 2003, LECT NOTES COMPUT SC, V2695, P494; Duits R., 2005, THESIS EINDHOVEN U T; Florack L, 2000, J MATH IMAGING VIS, V12, P65, DOI 10.1023/A:1008304909717; GEORGIEV T, 2005, P MIR 2005 MARCH; Gilmore R., 1993, CATASTROPHE THEORY S; Gradshteyn IS., 2007, TABLES INTEGRALS SER; Janssen B, 2006, INT J COMPUT VISION, V70, P231, DOI 10.1007/s11263-006-6703-9; KANTERS FMW, 2004, SCALESPACEVIZ VISUAL; Kreyszig E., 2011, ADV ENG MATH, V10th ed.; Kybic J, 2002, IEEE T SIGNAL PROCES, V50, P1977, DOI 10.1109/TSP.2002.800386; Kybic J, 2002, IEEE T SIGNAL PROCES, V50, P1965, DOI 10.1109/TSP.2002.800391; Lillholm M, 2003, INT J COMPUT VISION, V52, P73, DOI 10.1023/A:1022995822531; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Mallat S., 1999, WAVELET TOUR SIGNAL; Nielsen M, 2001, LECT NOTES COMPUT SC, V2106, P39; PAPOULIS A, 1977, IEEE T CIRCUITS SYST, V24, P652, DOI 10.1109/TCS.1977.1084284; PLATEL B, 2007, THESIS EINDHOVEN U T; Press WH, 1988, NUMERICAL RECIPES C; SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969; Unser M, 2000, P IEEE, V88, P569, DOI 10.1109/5.843002; WHITTAKER ET, 1946, MODERN ANAL; Yamatani K, 2006, IEEE T IMAGE PROCESS, V15, P3672, DOI 10.1109/TIP.2006.882005; Yosida K, 1980, FUNCTIONAL ANAL, V6th, DOI 10.1007/978-3-662-25762-3	32	2	2	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2009	84	2					205	219		10.1007/s11263-008-0156-2	http://dx.doi.org/10.1007/s11263-008-0156-2			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	451NO		Bronze, Green Submitted			2022-12-18	WOS:000266477100007
J	Kolmogorov, V; Criminisi, A; Blake, A; Cross, G; Rother, C				Kolmogorov, Vladimir; Criminisi, Antonio; Blake, Andrew; Cross, Geoffrey; Rother, Carsten			Probabilistic fusion of stereo with color and contrast for bi-layer segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Conference on Computer Vision and Pattern Recognition	JUN 20-25, 2005	San Diego, CA	IEEE Comp Soc					[Criminisi, Antonio; Blake, Andrew; Cross, Geoffrey; Rother, Carsten] Microsoft Res, Cambridge CB3 0FB, England; [Kolmogorov, Vladimir] UCL, Ipswich IP5 3RE, Suffolk, England	Microsoft; University of London; University College London	Blake, A (corresponding author), Microsoft Res, 7 JJ Thomson Ave, Cambridge CB3 0FB, England.	ablake@microsoft.com							0	2	2	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2008	76	2					107	107		10.1007/s11263-007-0070-z	http://dx.doi.org/10.1007/s11263-007-0070-z			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	255VH					2022-12-18	WOS:000252685400002
J	Hager, G; Hebert, M; Hutchinson, S				Hager, Greg; Hebert, Martial; Hutchinson, Seth			Special issue on Vision and Robotics, Parts I and II	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Johns Hopkins Univ, Baltimore, MD 21218 USA; Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; Univ Illinois, Urbana, IL 61801 USA	Johns Hopkins University; Carnegie Mellon University; University of Illinois System; University of Illinois Urbana-Champaign	Hager, G (corresponding author), Johns Hopkins Univ, Baltimore, MD 21218 USA.	hager@cs.jhu.edu; hebert@ri.cmu.edu; seth@uiuc.edu		hutchinson, seth/0000-0002-3949-6061					0	2	2	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2007	74	3					217	218		10.1007/s11263-007-0065-9	http://dx.doi.org/10.1007/s11263-007-0065-9			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	192YG					2022-12-18	WOS:000248239400001
J	Kimmel, R; Sochen, N; Weickert, J				Kimmel, Ron; Sochen, Nir; Weickert, Joachim			Special issue for the 5th International Conference on Scale-Space and PDE Methods in Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel; Tel Aviv Univ, Sch Math Sci, Dept Appl Math, IL-69978 Tel Aviv, Israel; Univ Saarland, Fac Math & Comp Sci, D-6600 Saarbrucken, Germany	Technion Israel Institute of Technology; Tel Aviv University; Saarland University	Kimmel, R (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.								0	2	2	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2006	70	3					195	195		10.1007/s11263-006-8067-6	http://dx.doi.org/10.1007/s11263-006-8067-6			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	091VL					2022-12-18	WOS:000241056300001
J	Zhu, ZG; Xu, GY; Lin, XY				Zhu, ZG; Xu, GY; Lin, XY			Efficient Fourier-based approach for detecting orientations and occlusions in epipolar plane images for 3D scene modeling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion analysis; 3D reconstruction; epipolar plane image; energy model; occlusion recovery; layered representation; panoramic representation	REPRESENTATION; MOSAICS	This paper presents a Fourier-based approach for automatically constructing a 3D panoramic model of a natural scene from a video sequence. The video sequences could be captured by an unstabilized camera mounted on a moving platform on a common road surface. As the input of the algorithms, "seamless" panoramic view images (PVls) and epipolar plane images (EPIs) are generated after image stabilization if the camera is unstabilized. A novel panoramic EPI analysis method is proposed that combines the advantages of both PVIs and EPIs efficiently in three important steps: locus orientation detection in the Fourier frequency domain, motion boundary localization in the spatio-temporal domain, and occlusion/resolution recovery only at motion boundaries. The Fourier energy-based approaches in literature were usually for low-level local motion analysis and are therefore not accurate for 3D reconstruction and are also computationally expensive. Our panoramic EPI analysis approach is both accurate and efficient for 3D reconstruction. Examples of layered panoramic representations for large-scale 3D scenes from real world video sequences are given.	CUNY City Coll, Dept Comp Sci, New York, NY 10031 USA; Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China	City University of New York (CUNY) System; City College of New York (CUNY); Tsinghua University	Zhu, ZG (corresponding author), CUNY City Coll, Dept Comp Sci, New York, NY 10031 USA.	zhu@cs.ccny.cuny.edu						ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; ALLMEN M, 1991, IEEE C COMP VIS PATT, P303; BAKER HH, 1989, INT J COMPUT VISION, V3, P33, DOI 10.1007/BF00054837; Baker S, 1998, PROC CVPR IEEE, P434, DOI 10.1109/CVPR.1998.698642; BALLARD C, 1999, IEEE C COMP VIS PATT, P559; Black MJ, 1996, IEEE T PATTERN ANAL, V18, P972, DOI 10.1109/34.541407; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; Chang NL, 1997, IEEE T IMAGE PROCESS, V6, P584, DOI 10.1109/83.563323; Chang NL, 2001, INT J COMPUT VISION, V45, P157, DOI 10.1023/A:1012476031602; Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395; Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097; Collins RT, 1998, COMPUT VIS IMAGE UND, V72, P143, DOI 10.1006/cviu.1998.0729; COORG S, 1998, TR729 MIT LCS; Dalmia AK, 1996, COMPUT VIS IMAGE UND, V64, P97, DOI 10.1006/cviu.1996.0047; Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191; Faugeras O, 1998, COMPUT VIS IMAGE UND, V69, P292, DOI 10.1006/cviu.1998.0665; Fleet DJ, 1998, PROC CVPR IEEE, P274, DOI 10.1109/CVPR.1998.698620; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Hansen M., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P54, DOI 10.1109/ACV.1994.341288; Heeger D. J., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P181; Ishiguro H., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P540, DOI 10.1109/ICCV.1990.139591; Jahne B., 1991, DIGITAL IMAGE PROCES; Li Y, 2004, IEEE T PATTERN ANAL, V26, P45, DOI 10.1109/TPAMI.2004.1261078; McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398; Morimoto C, 1997, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.1997.609396; MURRAY DW, 1995, COMPUT VIS IMAGE UND, V61, P285, DOI 10.1006/cviu.1995.1021; Nayar SK, 2000, PROC CVPR IEEE, P388; NIYOGI SA, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1044, DOI 10.1109/ICCV.1995.466819; Peleg S, 1997, PROC CVPR IEEE, P338, DOI 10.1109/CVPR.1997.609346; Peleg S, 2001, IEEE T PATTERN ANAL, V23, P279, DOI 10.1109/34.910880; Peleg S, 2000, IEEE T PATTERN ANAL, V22, P1144, DOI 10.1109/34.879794; Peleg S., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P395, DOI 10.1109/CVPR.1999.786969; Rademacher P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P199, DOI 10.1145/280814.280871; Sawhney HS, 1996, IEEE T PATTERN ANAL, V18, P814, DOI 10.1109/34.531801; Sawhney HS, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P56; Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882; Shum HY, 1998, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.1998.698641; Shum HY, 2000, INT J COMPUT VISION, V36, P101, DOI 10.1023/A:1008195814169; SHUM HY, 1999, P 7 INT C COMP VIS, P22; SHUM HY, 1999, P INT C COMP VIS, P14; SZELISKI R, 1999, IEEE C COMP VIS PATT, P157; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Xiong YL, 1997, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.1997.609326; ZHENG JY, 1992, INT J COMPUT VISION, V9, P55; Zheng JY, 1998, COMPUT VIS IMAGE UND, V72, P237, DOI 10.1006/cviu.1998.0678; ZHU Z, 1999, IEEE C COMP VIS PATT, P531; ZHU Z, 2001, FULL VIEW SPATIOTEMP; ZHU Z, 2001, P IEEE INT C COMP VI, V2, P723; Zhu ZG, 1998, P IEEE VIRT REAL ANN, P105, DOI 10.1109/VRAIS.1998.658453; Zhu ZG, 2004, IEEE T PATTERN ANAL, V26, P226, DOI 10.1109/TPAMI.2004.1262190; Zhu ZG, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P345, DOI 10.1109/ICCV.2001.937539	51	2	4	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	2005	61	3					233	258						26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	896IP					2022-12-18	WOS:000226928100002
J	Chung, R; Wong, HS				Chung, R; Wong, HS			Polyhedral object localization in an image by referencing to a single model view	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						model-based object recognition; polyhedral object; homography; affine projections	RECOGNITION; AFFINE	Identifying a three-dimensional (3D) object in an image is traditionally dealt with by referencing to a 3D model of the object. In the last few years there has been a growing interest of using not a 3D shape but multiple views of the object as the reference. This paper attempts a further step in the direction, using not multiple views but a single clean view as the reference model. The key issue is how to establish correspondences from the model view where the boundary of the object is explicitly available, to the scene view where the object can be surrounded by various distracting entities and its boundary disturbed by noise. We propose a solution to the problem, which is based upon a mechanism of predicting correspondences from just four particular initial point correspondences. The object is required to be polyhedral or near-polyhedral. The correspondence mechanism has a computational complexity linear with respect to the total number of visible corners of the object in the model view. The limitation of the mechanism is also analyzed thoroughly in this paper. Experimental results over real images are presented to illustrate the performance of the proposed solution.	Chinese Univ Hong Kong, Dept Automat & Comp Aided Engn, Shatin, Hong Kong, Peoples R China	Chinese University of Hong Kong	Chung, R (corresponding author), Chinese Univ Hong Kong, Dept Automat & Comp Aided Engn, Shatin, Hong Kong, Peoples R China.	rchung@cuhk.edu.hk	Chung, Chi-Kit Ronald/C-7702-2011	WONG, Hau-San/0000-0002-1530-7529				CHUNG R, 1995, IEE P-VIS IMAGE SIGN, V142, P289, DOI 10.1049/ip-vis:19952196; CHUNG R, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, VOLS 1-5, P2730, DOI 10.1109/ICSMC.1995.538196; CYGANSKI D, 1985, IEEE T PATTERN ANAL, V7, P662, DOI 10.1109/TPAMI.1985.4767722; DELONE BN, 1949, ANAL GEOMETRY, V2; DERICHE R, 1994, P EUR C COMP VIS STO, P567; FAUGERAS O, 1995, J OPT SOC AM A, V12, P465, DOI 10.1364/JOSAA.12.000465; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P536, DOI 10.1109/34.24786; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KIM H, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS - HUMANS, INFORMATION AND TECHNOLOGY, VOLS 1-3, P2402, DOI 10.1109/ICSMC.1994.400226; LAMDAN Y, 1988, P INT C COMP VIS, P218; Mundy J., 1992, GEOMETRIC INVARIANCE; NEVATIA R, 1980, COMPUT VISION GRAPH, V13, P257, DOI 10.1016/0146-664X(80)90049-0; Pritchett P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P754, DOI 10.1109/ICCV.1998.710802; Reid I. D., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P76, DOI 10.1109/ICCV.1993.378233; Rothwell C. A., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P573, DOI 10.1109/ICCV.1993.378159; SHAPIRO LS, 1995, INT J COMPUT VISION, V16, P147, DOI 10.1007/BF01539553; SHASHUA A, 1995, IEEE T PATTERN ANAL, V17, P779, DOI 10.1109/34.400567; Shashua A, 1997, INT J COMPUT VISION, V23, P185, DOI 10.1023/A:1007962930529; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234	21	2	2	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2003	51	2					139	163		10.1023/A:1021679607041	http://dx.doi.org/10.1023/A:1021679607041			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	625GR					2022-12-18	WOS:000179809200003
J	Zaritsky, R; Peterfreund, N; Shimkin, N				Zaritsky, R; Peterfreund, N; Shimkin, N			Velocity-guided tracking of deformable contours in three dimensional space	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						deformable contours; three-dimensional tracking; velocity snakes; optical flow; stereo vision	GEODESIC ACTIVE CONTOURS; MODELS; SNAKES	This paper presents a 3D active contour model for boundary detection and tracking of non-rigid objects, which applies stereo vision and motion analysis to the class of energy-minimizing deformable contour models, known as snakes. The proposed contour evolves in three-dimensional space in reaction to a 3D potential function, which is derived by projecting the contour onto the 2D stereo images. The potential function is augmented by a kinetic term, which is related to the velocity field along the contour. This term is used to guide the inter-image contour displacement. The incorporation of inter-frame velocity estimates in the tracking algorithm is especially important for contours which evolve in 3D space, where the added freedom of motion can easily result in loss of tracking. The proposed scheme incorporates local velocity information seamlessly in the snake model, with little computational overhead, and does not require exogenous computation of the optical flow or related quantities in each image. The resulting algorithm is shown to provide good tracking performance with only one iteration per frame, which provides a considerable advantage for real time operation.	Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel; Oak Ridge Natl Lab, Ctr Engn Sci Adv Res, Oak Ridge, TN 37831 USA; Harmon Inc, Oak Ridge, TN 37831 USA	Technion Israel Institute of Technology; United States Department of Energy (DOE); Oak Ridge National Laboratory	Zaritsky, R (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.	v4p@ornl.gov; shimkin@ee.technion.ac.il						BASCLE B, 1994, INT C PATT RECOG, P426, DOI 10.1109/ICPR.1994.576315; BASCLE B, 1993, SPIE, V2031, P282; BASCLE B, 1993, P 4 INT C COMP VIS B, P421; BERGER MO, 1995, P 9 SCAND C IM AN PU, P1094; Bertalmio M, 2000, IEEE T PATTERN ANAL, V22, P733, DOI 10.1109/34.865191; Blake A., 1992, ACTIVE VISION; Blake A., 1998, ACTIVE CONTOURS, DOI [10.1007/978-1-4471-1555-7, DOI 10.1007/978-1-4471-1555-7]; Caselles V, 1996, SIAM J NUMER ANAL, V33, P2445, DOI 10.1137/S0036142994275044; Caselles V, 1997, IEEE T PATTERN ANAL, V19, P394, DOI 10.1109/34.588023; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Cham TJ, 1997, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR.1997.609466; Deriche R., 1998, 3 AS C COMP VIS HONG; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; Goldenberg R, 2001, IEEE T IMAGE PROCESS, V10, P1467, DOI 10.1109/83.951533; Horn B., 1986, ROBOT VISION, P1; Jain AK, 1998, SIGNAL PROCESS, V71, P109, DOI 10.1016/S0165-1684(98)00139-X; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kichenassamy S, 1996, ARCH RATION MECH AN, V134, P275, DOI 10.1007/BF00379537; McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7; Paragios N., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P688, DOI 10.1109/ICCV.1999.791292; Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758; Peterfreund N, 1999, COMPUT VIS IMAGE UND, V73, P346, DOI 10.1006/cviu.1998.0732; Peterfreund N, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P70, DOI 10.1109/NAMW.1997.609855; Sapiro G., 2001, GEOMETRIC PARTIAL DI; TERZOPOULOS D, 1988, ARTIF INTELL, V36, P91, DOI 10.1016/0004-3702(88)90080-X; Terzopoulos D., 1992, ACTIVE VISION	26	2	3	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	2003	51	3					219	238		10.1023/A:1021853902673	http://dx.doi.org/10.1023/A:1021853902673			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	633TA					2022-12-18	WOS:000180297500004
J	Agapito, L; Hayman, E; Reid, I				Agapito, L; Hayman, E; Reid, I			Self-calibration of rotating and zooming cameras (vol 45, pg 107, 2001)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction								In this paper we describe the theory and practice of self-calibration of cameras which are fixed in location and may freely rotate while changing their internal parameters by zooming. The basis of our approach is to make use of the so-called infinite homography constraint which relates the unknown calibration matrices to the computed inter-image homographies. In order for the calibration to be possible some constraints must be placed on the internal parameters of the camera. We present various self-calibration methods. First an iterative non-linear method is described which is very versatile in terms of the constraints that may be imposed on the camera calibration: each of the camera parameters may be assumed to be known, constant throughout the sequence but unknown, or free to vary. Secondly, we describe a fast linear method which works under the minimal assumption of zero camera skew or the more restrictive conditions of square pixels (zero skew and known aspect ratio) or known principal point. We show experimental results on both synthetic and real image sequences (where ground truth data was available) to assess the accuracy and the stability of the algorithms and to compare the result of applying different constraints on the camera parameters. We also derive an optimal Maximum Likelihood estimator for the calibration and the motion parameters. Prior knowledge about the distribution of the estimated parameters (such as the location of the principal point) may also be incorporated via Maximum a Posteriori estimation. We then identify some near-ambiguities that arise under rotational motions showing that coupled changes of certain parameters are barely observable making them indistinguishable. Finally we study the negative effect of radial distortion in the self-calibration process and point out some possible solutions to it.	Univ Oxford, Dept Engn Sci, Robot Res Grp, Oxford OX1 3PJ, England	University of Oxford	Agapito, L (corresponding author), Univ London Queen Mary Coll, London E1 4NS, England.							Agapito L, 2001, INT J COMPUT VISION, V45, P107, DOI 10.1023/A:1012471930694	1	2	2	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					287	287		10.1023/A:1014514429063	http://dx.doi.org/10.1023/A:1014514429063			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN		Bronze			2022-12-18	WOS:000174354700021
J	Burlina, P; Chellappa, R				Burlina, P; Chellappa, R			Temporal analysis of motion in video sequences through predictive operators	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						time-to-collision; predictive operators; alternate motion/structure representations; motion prediction and estimation; visual guidance	TIME-TO-COLLISION; OPTICAL-FLOW	We study the problem of recovering temporal parameters which act as predictive operators, generalize time-to-collision and have direct interpretation for navigational purposes for piecewise arbitrarily smooth (polynomial) motion. A result stating that, for monocular observers undergoing arbitrary polynomial laws, these parameters are visually observable, is presented in the first part of this paper. This property suggests an alternate temporal representation of visual looming information. The second part of this paper is concerned with algorithmic approaches for environments with maneuvering agents. A method addressing model order determination, collision detection, and temporal parameter estimation is proposed. Experimental results are reported.	Univ Maryland, Comp Vis Lab, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Burlina, P (corresponding author), Univ Maryland, Comp Vis Lab, College Pk, MD 20742 USA.	burlina@cfar.umd.edu; rama@cfar.umd.edu	Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/B-6573-2012					Bar-Shalom Y., 1988, TRACKING DATA ASS; CIPOLLA R, 1996, ACTIVE VISUAL INFERE; DURIC Z, 1994, P ARPA IM UND WORKSH, P1209; FRANCOIS E, 1990, IMAGE VISION COMPUT, V8, P279, DOI 10.1016/0262-8856(90)80004-D; GIBSON JJ, 1979, ECOLOGICLA APPROACH; HERVE JY, 1991, PROCEEDINGS : NINTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P732; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; KOENDERINK JJ, 1986, VISION RES, V26, P161, DOI 10.1016/0042-6989(86)90078-7; LEE DN, 1981, NATURE, V293, P293, DOI 10.1038/293293a0; LEE DN, 1976, PERCEPTION, V5, P436; MAYBANK S, 1987, IMAGE VISION COMPUT, V5, P111, DOI 10.1016/0262-8856(87)90036-9; MEYER FG, 1994, IEEE T ROBOTIC AUTOM, V10, P792, DOI 10.1109/70.338534; NELSON RC, 1989, IEEE T PATTERN ANAL, V11, P1102, DOI 10.1109/34.42840; POGGIO T, 1991, 1289 MIT AI LAB; SUBBARAO M, 1990, COMPUT VISION GRAPH, V50, P329, DOI 10.1016/0734-189X(90)90151-K; TISTARELLI M, 1993, IEEE T PATTERN ANAL, V15, P401, DOI 10.1109/34.206959; Yao Y. S., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P191, DOI 10.1109/ICIP.1995.529578; Zheng Q, 1993, IEEE T IMAGE PROCESS, V2, P311, DOI 10.1109/83.236535	19	2	2	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN-JUL	1998	28	2					175	192		10.1023/A:1008067101494	http://dx.doi.org/10.1023/A:1008067101494			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	103BB					2022-12-18	WOS:000074959800005
J	Young, GS; Herman, M; Hong, TH; Jiang, D; Yang, JCS				Young, GS; Herman, M; Hong, TH; Jiang, D; Yang, JCS			New visual invariants for terrain navigation without 3D reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						autonomous vehicles; mobile robots; obstacle detection; optical flow; image motion; visual navigation; purposive vision	MOTION	For autonomous vehicles to achieve terrain navigation, obstacles must be discriminated from terrain before any path planning and obstacle avoidance activity is undertaken. In this paper, a novel approach to obstacle detection has been developed. The method finds obstacles in the 2D image space, as opposed to 3D reconstructed space, using optical flow. Our method assumes that both nonobstacle terrain regions, as well as regions with obstacles, will be visible in the imagery. Therefore, our goal is to discriminate between terrain regions with obstacles and terrain regions without obstacles. Our method uses new visual linear invariants based on optical flow. Employing the linear invariance property, obstacles can be directly detected by using reference flow lines obtained from measured optical flow. The main features of this approach are: (1) 2D visual information (i.e., optical flow) is directly used to detect obstacles; no range, 3D motion, or 3D scene geometry is recovered; (2) knowledge about the camera-to-ground coordinate transformation is not required; (3) knowledge about vehicle (or camera) motion is not required; (4) the method is valid for the vehicle (or camera) undergoing general six-degree-of-freedom motion; (5) the error sources involved are reduced to a minimum, because the only information required is one component of optical flow. Numerous experiments using both synthetic and real image data are presented. Our methods are demonstrated in both ground and air vehicle scenarios.	Natl Inst Stand & Technol, Gaithersburg, MD 20899 USA; Univ Maryland, Dept Mech Engn, Robot Lab, College Pk, MD 20742 USA	National Institute of Standards & Technology (NIST) - USA; University System of Maryland; University of Maryland College Park	Young, GS (corresponding author), Natl Inst Stand & Technol, Bldg 220,Rm B124, Gaithersburg, MD 20899 USA.							ALBUS JS, 1991, IEEE T SYST MAN CYB, P21; ALBUS JS, 1990, P IEEE INT C ROB AUT; ALOIMONOS Y, 1992, J ROBOTIC SYST, V9, P843, DOI 10.1002/rob.4620090609; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BHANU B, 1990, P IEEE INT C ROB AUT; BURT P, 1994, P ARPA IM UND WORKSH; CAMUS T, 1995, P IEEE C COMP ARCH M; Camus T., 1996, P 13 INT C PATT REC; CAMUS T, IN PRESS REAL TIME I; COOMBS D, IN PRESS IEEE T ROBO; COOMBS D, 1995, P 5 INT C COMP VIS, P226; DAILY MJ, 1987, P DARPA IMAG UND WOR; DUNLAY RT, 1986, SPIE MOBILE ROBOTS, V727; ENKELMANN W, 1990, 1 EUR C COMP VIS; Gibson J.J., 1979, ECOLOGICAL APPROACH, pp. 119; HEBERT M, 1990, ANAL INTERPRETATION; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; HERMAN M, 1997, VISUAL NAVIGATION BI; HERMAN M, 1993, P SPIE 22 APPL IM PA, V2103; HERMAN M, 1991, P NATO DEF RES GROUP; HOFF W, 1990, P IEEE INT C ROB AUT; LAU H, 1992, THESIS U MARYLAND; LENZ RK, 1988, IEEE T PATTERN ANAL, V10; LIU H, 1994, IN PRESS INT J COMPU; LIU H, 1996, P 4 EUR C COMP VIS; LIU H, 1993, NISTIR5333 NIST; LIU H, 1996, P 13 INT C PATT REC; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; MALLOT HA, 1991, BIOL CYBERN, V64, P177, DOI 10.1007/BF00201978; MATTIES L, 1992, P IEEE C COMP VIS PA; NAKAYAMA K, 1985, VISION RES, V25, P625, DOI 10.1016/0042-6989(85)90171-3; NELSON RC, 1989, IEEE T PATTERNS ANAL, V11; OSKARD DN, 1960, IEEE T SYSTEMS MAN C, V20; RAVIV D, 1993, ACTIVE PERCEPTION; RAVIV D, 1992, 4794 NISTIR NIST; SANDINI G, 1993, ACTIVE PERCEPTION; SINGH S, 1991, P IEEE INT C ROB AUT; SOLDER U, 1990, P SPIE, V1388; SRIDHAR B, 1991, P SPIE, V1571; STORJOHANN K, 1990, P IEEE INT C ROB AUT; TISTARELLI M, 1992, COMPUTER VISION GRAP, V56; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; VEATCH PA, 1990, COMPUTER VISION GRAP, V50; YAO YS, 1996, P ARPA IM UND WORKSH; YOUNG GS, 1993, THESIS U MARYLAND	46	2	2	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1998	28	1					45	71		10.1023/A:1008002714698	http://dx.doi.org/10.1023/A:1008002714698			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZX971					2022-12-18	WOS:000074573800003
J	Eklundh, JO				Eklundh, JO			Machine vision research at CVAP: An introduction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material							SPACE PRIMAL SKETCH; HEAD-EYE SYSTEM; SCALE-SPACE; GEOMETRY				Eklundh, JO (corresponding author), ROYAL INST TECHNOL,DEPT NUMER ANAL & COMP SCI,CVAP,KTH,S-10044 STOCKHOLM,SWEDEN.							Aloimonos Y., 1990, P DARPA IMAGE UNDERS, P816; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BINFORD TO, 1981, ARTIF INTELL, V17, P205, DOI 10.1016/0004-3702(81)90025-4; BROOKS RA, 1991, J ARTIFICIAL INTELLI, V47, P137; BRUNNSTROM K, 1990, IMAGE VISION COMPUT, V8, P289, DOI 10.1016/0262-8856(90)80005-E; BRUNNSTROM K, 1992, LECT NOTES COMPUT SC, V588, P701; BURT PJ, 1992, P SOC PHOTO-OPT INS, V1825, P769, DOI 10.1117/12.131580; CARLSSON S, 1993, APPL INVARIANCE COMP, V825, P145; Carpenter RHS, 1988, MOVEMENTS EYES; CROWLEY JL, 1995, VISION PROCESS ESPRI; GARDING J, 1993, IEEE T PATTERN ANAL, V15, P1202, DOI 10.1109/34.244682; GARDING J, 1993, ARTIF INTELL, V64, P243, DOI 10.1016/0004-3702(93)90106-L; Garding J., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P324, DOI 10.1109/ICPR.1990.118124; GARDING J, 1992, J MATH IMAGING VIS, V2, P329; HORN BKP, 1972, AIM285 MIT; HUCKEL M, 1971, JACM, V18, P113; IVERSON L, 1990, CIM906 MCGILL U; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1976, BIOL CYBERN, V21, P29, DOI 10.1007/BF00326670; KOENDERINK JJ, 1987, BIOL CYBERN, V55, P367, DOI 10.1007/BF00318371; KOENDERINK JJ, 1975, OPT ACTA, V22, P775; LINDEBERG T, 1990, IEEE T PATTERN ANAL, V12, P234, DOI 10.1109/34.49051; Lindeberg T., 1992, Journal of Mathematical Imaging and Vision, V1, P65, DOI 10.1007/BF00135225; LINDEBERG T, 1992, IMAGE VISION COMPUT, V10, P3, DOI 10.1016/0262-8856(92)90079-I; LINDEBERG T, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P416; Marr D., 1982, VISION; Mundy J., 1992, GEOMETRIC INVARIANCE; NAEVE A, 1993, KTHNAP9319SE ROY I T; NELSON RC, 1991, INT J COMPUT VISION, V7, P5, DOI 10.1007/BF00130486; OLSON TJ, 1991, INT J COMPUT VISION, V7, P67, DOI 10.1007/BF00130490; PAHLAVAN K, 1992, CVGIP-IMAG UNDERSTAN, V56, P41, DOI 10.1016/1049-9660(92)90084-G; Pahlavan K., 1993, ACTIVE PERCEPTION, P19; ROSENFELD A, 1971, IEEE T COMPUT, V20, P512; TSOTSOS JK, 1992, RBCVTR9242 U TOR	34	2	2	0	3	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1996	17	2					107	112		10.1007/BF00058747	http://dx.doi.org/10.1007/BF00058747			6	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TZ498					2022-12-18	WOS:A1996TZ49800001
J	VONSEELEN, W; BOHRER, S; KOPECZ, J; THEIMER, WM				VONSEELEN, W; BOHRER, S; KOPECZ, J; THEIMER, WM			A NEURAL ARCHITECTURE FOR VISUAL INFORMATION-PROCESSING	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OPTICAL-FLOW; OBSTACLE DETECTION; COMPUTATION; CORTEX; STEREO; MAPS; ORGANIZATION; PERCEPTION; ALGORITHM; FILTERS	We report on the work done at the Institut fur Neuroinformatik in Bochum concerning the development of a neural architecture for the information processing of autonomous visually guided systems acting in a natural environment. Since biological systems like our brain are superior to artificial systems in solving such a task, we use findings from neurophysiology and -anatomy as well as psychophysics for defining processing principles and modules that have been implemented on our mobile platform MARVIN. MARVIN is equipped with an active stereo camera system. Our final objective is to define a neural instruction set for early information processing in the sense of a perception for action approach. From the biological paradigm we use principles like active vision, foveation, two-dimensional cortical layers, mapping, and discrete parametric representations in a task-oriented way to solve problems like obstacle avoidance, path planning, scene recognition, tracking, and 3D perception. This paper has the character of an overview of the work done in this field at our institute. Most of the modules presented here were published either in conference proceedings or in journals which will be referenced for a more thorough discussion of each issue.	CVIS COMP VIS & AUTOMAT GMBH,D-44799 BOCHUM,GERMANY; ZENTRUM NEUROINFORMAT,D-44799 BOCHUM,GERMANY		VONSEELEN, W (corresponding author), RUHR UNIV BOCHUM,INST NEUROINFORMAT,UNIV STR 150 ND,D-44780 BOCHUM,GERMANY.							ALOIMONOS JY, 1987, INT J COMPUT VISION, P333; ANCONA N, 1992, 2ND ECCV GEN, P267; Arbib M. A., 2011, HDB PHYSL NERVOUS SY, P1449, DOI [10.1002/cphy.cp010233, DOI 10.1016/J.JPHYSPARIS.2008.03.001]; Arkin R. C., 1990, Robotics and Autonomous Systems, V6, P105, DOI 10.1016/S0921-8890(05)80031-4; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BLASDEL GG, 1986, NATURE, V321, P579, DOI 10.1038/321579a0; BOHRER S, 1993, THESIS RUHR U BOCHUM; BOHRER S, 1990, INNC 90 PARIS; BOHRER S, 1991, ICANN 91 HELSINKI; BROOKS RA, 1991, SCIENCE, V253, P1227, DOI 10.1126/science.253.5025.1227; BROWN CM, 1988, 257 U ROCH COMP SCI; BULTHOFF H, 1989, NATURE, V337, P549, DOI 10.1038/337549a0; CARLSSON S, 1990, 1ST P EUR C COMP VIS, P297; CHRISTENSEN HI, 1992, SPIE, V1708; Clark J. J., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P514, DOI 10.1109/CCV.1988.590032; Collewijn H, 1990, Rev Oculomot Res, V4, P213; COLLEWIJN H, 1985, ADAPTIVE MECHANISMS; Connolly C., 1990, IEEE INT C ROB AUT C, P2102; CROWLEY JL, 1992, 2 EUR C COMP VIS, P588; DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160; DIAO YC, 1990, EXP BRAIN RES, V79, P271, DOI 10.1007/BF00608236; DONATH M, 1990, ROBOTICS AUTONOMOUS, V6, P145; DOSE M, 1994, AUTONOMOUS MOBILE RO; DOSE M, 1992, EINFUHRUNG SPRACHE V; DUNCAN J, 1989, PSYCHOL REV, V96, P433, DOI 10.1037/0033-295X.96.3.433; DURBIN R, 1990, NATURE, V343, P644, DOI 10.1038/343644a0; ENKELMANN W, 1991, IMAGE VISION COMPUT, V9, P160, DOI 10.1016/0262-8856(91)90010-M; FERMULLER C, 1992, BIOL CYBERN, V67, P259, DOI 10.1007/BF00204399; FLEET DJ, 1991, CVGIP-IMAG UNDERSTAN, V53, P198, DOI 10.1016/1049-9660(91)90027-M; HATA Y, 1988, NATURE, V335, P815, DOI 10.1038/335815a0; HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085; JANSSEN H, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P1203; JANSSEN H, 1990, INFORMATIK FACHBERIC, V254; JANSSEN H, 1991, ARTIFICIAL NEURAL NE, P63; KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288; KOPECZ J, 1993, UNPUB FLEXIBILITY NE; KOPECZ J, 1992, THESIS; KOPECZ J, 1993, INT C NEURAL NETWORK, P138; KRONE G, 1986, PROC R SOC SER B-BIO, V226, P421, DOI 10.1098/rspb.1986.0002; KROTKOV E, 1986, J ROBOTICS AUTOMATIO, V4, P108; MALLOT HA, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P125; MALLOT HA, 1990, NEURAL NETWORKS, V3, P245, DOI 10.1016/0893-6080(90)90069-W; MALLOT HA, 1991, BIOL CYBERN, V64, P177, DOI 10.1007/BF00201978; MALLOT HA, 1992, INVEST OPHTH VIS S S, V33; MALLOT HA, 1989, MODELS BRAIN FUNCTIO, P175; MALLOT HA, 1988, JUN P NEURO 88 PAR, P560; Marr D., 1982, VISION; Nelson R. C., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P188, DOI 10.1109/CCV.1988.589990; PAHLAVAN K, 1992, CVGIP-IMAG UNDERSTAN, V56, P41, DOI 10.1016/1049-9660(92)90084-G; PALM G, 1980, BIOL CYBERN, V36, P19, DOI 10.1007/BF00337019; RIMEY RD, 1990, TR327 U ROCH COMP SC; RIMON E, 1990, MAY P IEEE INT C ROB, P1937; RITTER HJ, 1989, NEURAL NETWORKS, V2, P159, DOI 10.1016/0893-6080(89)90001-4; SANDINI G, 1991, WORKSHOP ROBUST COMP; SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114; Schoner G., 1992, Robotics and Autonomous Systems, V10, P253, DOI 10.1016/0921-8890(92)90004-I; SCHULZE ER, 1990, MUSTERERKENNUNG 1990; Theimer W.M., 1993, NEURAL NETWORKS THEI, P299; THEIMER WM, 1992, INTELLIGENT ROBOTS C, V11, P76; TOLG S, 1992, INTELLIGENT ROBOTS C, V11; TOLG S, 1992, FORTSCHRITT BERIC 10, V197; TREISMAN A, 1985, COMPUT VISION GRAPH, V31, P156, DOI 10.1016/S0734-189X(85)80004-9; Ungerleider LG, 1982, ANAL VISUAL BEHAV, P549; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; VERRI A, 1990, J OPT SOC AM A, V7, P912, DOI 10.1364/JOSAA.7.000912; VIVANI P, 1990, EYE MOVEMENTS THEIR; von der Malsburg C, 1973, Kybernetik, V14, P85; WIKLUND J, 1992, LITHISYI1327 LINK U; YUILLE A, 1990, INT J COMPUT VISION, V4, P141, DOI 10.1007/BF00127814; ZHENG Y, 1990, IMAGE VISION COMPUT, V8, P57, DOI 10.1016/0262-8856(90)90057-C; ZIELKE T, 1993, CVGIP IMAGE UNDERSTA, V58	72	2	2	0	3	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1995	16	3					229	260		10.1007/BF01539628	http://dx.doi.org/10.1007/BF01539628			32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TC768					2022-12-18	WOS:A1995TC76800003
J	ALOIMONOS, Y				ALOIMONOS, Y			QUALITATIVE VISION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									UNIV MARYLAND,INST ADV COMP STUDIES,COLLEGE PK,MD 20742; FORTH,INST COMP SCI,IRAKLION,GREECE	University System of Maryland; University of Maryland College Park; Foundation for Research & Technology - Hellas (FORTH)	ALOIMONOS, Y (corresponding author), UNIV MARYLAND,DEPT COMP SCI,CTR AUTOMAT RES,COMP VIS LAB,COLLEGE PK,MD 20742, USA.		Aloimonos, Yiannis/AAI-2969-2020	Aloimonos, Yiannis/0000-0002-8152-4281				Aloimonos Y, 1993, ACTIVE PERCEPTION; ALOIMONOS Y, 1992, COMPUT VIS GRAPHICS, P56; Aloimonos Y., 1989, INTEGRATION VISUAL M; ALOIMONOS Y, 1988, INT J COMPUT VISION, V7, P333; ATOIMONOS Y, 1990, P IMAGE UNDERSTANDIN, P816; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BROOKS R, 1986, AI899 MIT MEM; Brunswik E., 1956, PERCEPTION REPRESENT; CANNY J, 1993, RAMP932 U CAL TECH R; Farah M.J., 1990, VISUAL AGNOSIA DISOR; Faugeras O., 1992, 3 DIMENSIONAL COMPUT; FERMUELLER C, 1994, P IMAGE UNDERSTANDIN, P645; FERMUELLER C, 1993, THESIS TU VIENNA; FERMUELLER C, EXPLORATORY VISION A; Gibson J., 1979, ECOLOGICAL APPROACH; GORDON L, 1989, THEORIES VISUAL PERC; HELMHOLTZ H, 1924, HELMHOLTZS PHYSL OPT; Horn B., 1986, ROBOT VISION, P1; HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455; Kanizsa Gaetano, 1979, ORG VISION ESSAYS GE; Kant Immanuel, 1990, CRITIQUE PURE REASON; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; Ku┬hler W., 1947, GESTALT PSYCHOL; Marr D., 1982, VISION; Nalwa V. S., 1993, GUIDED TOUR COMPUTER; Sloman A., 1989, Journal of Experimental and Theoretical Artificial Intelligence, V1, P289, DOI 10.1080/09528138908953711; WARRINGTON EK, 1984, BRAIN, V107, P829, DOI 10.1093/brain/107.3.829; ZEKI S, 1992, SCI AM           SEP, P69; Zeki S., 1993, VISION BRAIN	30	2	2	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1995	14	2					115	117		10.1007/BF01418977	http://dx.doi.org/10.1007/BF01418977			3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT789					2022-12-18	WOS:A1995QT78900001
J	HU, XP; AHUJA, N				HU, XP; AHUJA, N			MIRROR UNCERTAINTY AND UNIQUENESS CONDITIONS FOR DETERMINING SHAPE AND MOTION FROM ORTHOGRAPHIC PROJECTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								This paper presents new forms of necessary and sufficient conditions for determining shape and motion to within a mirror uncertainty from monocular orthographic projections of any number of point trajectories over any number of views. The new forms of conditions use image data only and can therefore be employed in any practical algorithms for shape and motion estimation. We prove that the mirror uncertainty for the three view problem also exists for a long sequence: if shape S is a solution, so is its mirror image S' which is symmetric to S about the image plane. The necessary and sufficient conditions for determining the two sets of solutions are associated with the rank of the measurement matrix W. If the rank of W is 3, then the original 3D scene points cannot be coplanar and the shape and motion can be determined to within a mirror uncertainty from the image data if and only if there are three distinct views. This condition is different from Ullman's theorem (which states that three distinct views of four noncoplanar points suffice to determine the shape and motion up to a reflection) in two aspects: (1) it is expressed in terms of image data; (2) it applies to a long image sequence in a homogeneous way. If the rank of W is 2 and the image points in at least one view are not colinear in the image plane, then there are two possibilities: either the motion is around the optical axis, or the 3-D points all lie on the same plane. In the first case, the motion can be determined uniquely but the shape is not determined. In the second case, a necessary and sufficient condition is to be satisfied and at least 3 point trajectories over at least 3 distinct views are needed to determine the shape in each view to within a mirror uncertainty, and the number of motion solutions is equal to the combinatorial number of the possible positions of the plane in different views. The necessary and sufficient condition is associated with the rank of a matrix C: if C has a rank of 1, the plane is undetermined; if C has a rank of 2 (implying there are exactly 3 distinct views), then a necessary and sufficient condition, whose physical meaning is not completely clear, is to be satisfied to determine the plane to within 2 sets; if C has a rank of 3 (implying there are 4 or more distinct views), then the plane can always be determined to within two sets. If the rank of W is 2 or 1 and the image points in each view are colinear in the image plane, then the three dimensional motion problem reduces to a two dimensional motion problem. In this case, the uniqueness condition is associated with the rank of the reduced measurement matrix Psi. If Psi has a rank of 2, then the original 3D points cannot be colinear in the space and the shape and motion can be determined to within two sets if and only if three or more views are distinct. If Psi has a rank of 1, there are two possibilities: if the rows of Psi are identical, then either the original 3D points are not colinear and the motion is zero, or the points are colinear and possibly move between two mirror symmetric positions; if the rows of Psi are not identical, then the motion is not determined. All proofs are constructive and thus define an algorithm for determining the uniqueness of solution as well as for estimating shape and motion from point trajectories.	UNIV ILLINOIS,BECKMAN INST,URBANA,IL 61801	University of Illinois System; University of Illinois Urbana-Champaign	HU, XP (corresponding author), UNIV ILLINOIS,COORDINATED SCI LAB,405 N MATHEWS AVE,URBANA,IL 61801, USA.							ALOIMONOS J, 1986, JUN P IEEE C COMP VI, P510; DEBRUNNER C, 1992, P IMAGE UNDERSTANDIN, P543; DEBRUNNER C, 1992, 2ND P EUR C COMP VIS, P217; DEBRUNNER C, 1990, 10TH P INT C PATT RE, P384; GOLUB GH, 1971, SINGULAR VALUE DECOM, V2; HU X, 1991, P INT C ACOUSTICS SP, V4, P2445; HU XP, 1991, IEEE T ROBOTIC AUTOM, V7, P848, DOI 10.1109/70.105394; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P536, DOI 10.1109/34.24786; KANATANI K, 1986, COMPUT VIS GRAPH IMA, P181; TOMASI C, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P91; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; ULLMAN S, 1977, THESIS MIT; 1992, P DARPA IMAGE UNDERS, P459; 1987, USERS MANUAL FORTRAN	14	2	2	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1994	13	3					295	309		10.1007/BF02028350	http://dx.doi.org/10.1007/BF02028350			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QF170					2022-12-18	WOS:A1994QF17000003
J	Ma, JY; Fan, AX; Jiang, XY; Xiao, GB				Ma, Jiayi; Fan, Aoxiang; Jiang, Xingyu; Xiao, Guobao			Feature Matching via Motion-Consistency Driven Probabilistic Graphical Model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Feature matching; Probabilistic graphical model; Motion-consistency; Robust estimation; Outlier		This paper proposes an effective method, termed as motion-consistency driven matching (MCDM), for mismatch removal from given tentative correspondences between two feature sets. In particular, we regard each correspondence as a hypothetical node, and formulate the matching problem into a probabilistic graphical model to infer the state of each node (e.g., true or false correspondence). By investigating the motion consistency of true correspondences, a general prior is incorporated into our formulation to differentiate false correspondences from the true ones. The final inference is casted into an integer quadratic programming problem, and the solution is obtained by using an efficient optimization technique based on the Frank-Wolfe algorithm. Extensive experiments on general feature matching, as well as fundamental matrix estimation, relative pose estimation and loop-closure detection, demonstrate that our MCDM possesses strong generalization ability as well as high accuracy, which outperforms state-of-the-art methods. Meanwhile, due to the low computational complexity, the proposed method is efficient for practical feature matching tasks.	[Ma, Jiayi; Fan, Aoxiang; Jiang, Xingyu] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China; [Xiao, Guobao] Minjiang Univ, Coll Comp & Control Engn, Fuzhou 350108, Peoples R China	Wuhan University; Minjiang University	Ma, JY (corresponding author), Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.	jyma2010@gmail.com; fanaoxiang@whu.edu.cn; jiangx.y@whu.edu.cn; gbx@mju.edu.cn	Fan, Aoxiang/GRX-2215-2022	Ma, Jiayi/0000-0003-3264-3265	National Natural Science Foundation of China [61773295]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China under Grant No. 61773295.	Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514; Baeza-Yates R., 1999, MODERN INFORM RETRIE; Barath D, 2020, PROC CVPR IEEE, P1301, DOI 10.1109/CVPR42600.2020.00138; Barath D, 2019, PROC CVPR IEEE, P10189, DOI 10.1109/CVPR.2019.01044; Barath D, 2018, PROC CVPR IEEE, P6733, DOI 10.1109/CVPR.2018.00704; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Bian J W, 2019, ARXIV PREPRINT ARXIV; Bian JW, 2020, INT J COMPUT VISION, V128, P1580, DOI 10.1007/s11263-019-01280-3; Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033; Bustos AP, 2018, IEEE T PATTERN ANAL, V40, P2868, DOI 10.1109/TPAMI.2017.2773482; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; Choy C., 2020, P IEEE CVF C COMP VI, P11227; Chum O, 2005, PROC CVPR IEEE, P772; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Goldstein A.A., 1965, J SOC IND APPL MATH, V3, P147, DOI [10.1137/0303013, DOI 10.1137/0303013]; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949; Ivashechkin Maksym, 2021, P IEEE CVF INT C COM, P15243; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599; Koller D., 2009, PROBABILISTIC GRAPHI; Lebeda K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.95; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Leordeanu Marius, 2009, ADV NEURAL INFORM PR; Li XR, 2010, INT J COMPUT VISION, V89, P1, DOI 10.1007/s11263-010-0318-x; Lin WY, 2018, IEEE T PATTERN ANAL, V40, P34, DOI 10.1109/TPAMI.2017.2652468; Liu HR, 2010, PROC CVPR IEEE, P1609, DOI 10.1109/CVPR.2010.5539780; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2; Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI 10.1007/s11263-018-1117-z; Ma JY, 2019, IEEE T NEUR NET LEAR, V30, P3584, DOI 10.1109/TNNLS.2018.2872528; Ma JY, 2015, IEEE T GEOSCI REMOTE, V53, P6469, DOI 10.1109/TGRS.2015.2441954; Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478; Mishkin D, 2015, COMPUT VIS IMAGE UND, V141, P81, DOI 10.1016/j.cviu.2015.08.005; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Speciale P, 2018, PROC CVPR IEEE, P7317, DOI 10.1109/CVPR.2018.00764; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Sun W., 2020, P IEEECVF C COMPUTER, P11286; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Vongkulbhisal J, 2018, PROC CVPR IEEE, P2993, DOI 10.1109/CVPR.2018.00316; Wang C, 2014, LECT NOTES COMPUT SC, V8690, P788, DOI 10.1007/978-3-319-10605-2_51; Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5; Wong HS, 2011, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2011.6126350; Yan JC, 2018, IEEE T CYBERNETICS, V48, P765, DOI 10.1109/TCYB.2017.2655538; Yan JC, 2016, IEEE T PATTERN ANAL, V38, P1228, DOI 10.1109/TPAMI.2015.2477832; Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282; Zass R, 2008, PROC CVPR IEEE, P1221; Zhang JH, 2019, IEEE I CONF COMP VIS, P5844, DOI 10.1109/ICCV.2019.00594; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561; Zhao C, 2019, PROC CVPR IEEE, P215, DOI 10.1109/CVPR.2019.00030; Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1774, DOI 10.1109/TPAMI.2015.2501802; Zhou L, 2017, IEEE I CONF COMP VIS, P2381, DOI 10.1109/ICCV.2017.259	63	1	1	9	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2249	2264		10.1007/s11263-022-01644-2	http://dx.doi.org/10.1007/s11263-022-01644-2		JUL 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG					2022-12-18	WOS:000826836200001
J	Rodrigues, M; Mayo, M; Patros, P				Rodrigues, Mark; Mayo, Michael; Patros, Panos			Surgical Tool Datasets for Machine Learning Research: A Survey	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Surgical tool datasets; Machine learning; Deep learning; Dataset and algorithms survey; Hospi-Tools Dataset	INSTRUMENT SEGMENTATION; ENDOSCOPIC VISION; NEURAL-NETWORKS; TRACKING; RECOGNITION; SIMULATION; SURGERY	This paper is a comprehensive survey of datasets for surgical tool detection and related surgical data science and machine learning techniques and algorithms. The survey offers a high level perspective of current research in this area, analyses the taxonomy of approaches adopted by researchers using surgical tool datasets, and addresses key areas of research, such as the datasets used, evaluation metrics applied and deep learning techniques utilised. Our presentation and taxonomy provides a framework that facilitates greater understanding of current work, and highlights the challenges and opportunities for further innovative and useful research.	[Rodrigues, Mark; Mayo, Michael] Univ Waikato, Dept Comp Sci, Hamilton, New Zealand; [Patros, Panos] Univ Waikato, Dept Software Engn, Hamilton, New Zealand	University of Waikato; University of Waikato	Rodrigues, M (corresponding author), Univ Waikato, Dept Comp Sci, Hamilton, New Zealand.	mark.rodrigues@waikato.ac.nz; michael.mayo@waikato.ac.nz			CAUL	CAUL	Open Access funding enabled and organized by CAUL and its Member Institutions	Abdulbaki Alshirbaji Tamer, 2018, Current Directions in Biomedical Engineering, V4, P407, DOI 10.1515/cdbme-2018-0097; ACS, 2021, WHAT ARE SURG SPECIA; Ahmadi E, 2019, HEALTH SYST, V8, P134, DOI 10.1080/20476965.2018.1496875; Al Hajj H, 2019, MED IMAGE ANAL, V52, P24, DOI 10.1016/j.media.2018.11.008; Ali S, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2021.102002; Allan Max, 2017, ARXIV190206426; Allan Max, 2018, ARXIV200111190; Alshirbaji T. A., 2020, AUTOMED AUTOMATION M; Alshirbaji T. A., 2020, P AUTOMATION MEDICAL, V1, P24; Alshirbaji T.A., 2021, CURR DIR BIOMED ENG, V7, P476, DOI [10.1515/cdbme-2021-2121, DOI 10.1515/CDBME-2021-2121]; Alshirbaji T. A., 2021, AUTOMED 2021; Andersen J. K. H., 2021, 20 INT C ADV ROBOTIC; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Attanasio A, 2020, IEEE ROBOT AUTOM LET, V5, P6528, DOI 10.1109/LRA.2020.3013914; Banerjee N, 2019, L N COMPUT VIS BIOME, V31, P31, DOI 10.1007/978-3-030-04061-1_4; Bar O, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-79173-6; Barbu A, 2019, ADV NEUR IN, V32; Bhatt NR, 2018, WORLD J SURG, V42, P3792, DOI 10.1007/s00268-018-4688-5; Bodenstedt S, 2018, ARXIV180800178; Bouget D, 2017, MED IMAGE ANAL, V35, P633, DOI 10.1016/j.media.2016.09.003; Bouget D, 2015, IEEE T MED IMAGING, V34, P2603, DOI 10.1109/TMI.2015.2450831; Ceron J. C. A., 2021, ANN INT C IEEE ENG M; Chai J., 2021, MACHINE LEARNING APP, DOI 10.1016/j.mlwa.2021.100134; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen H., 2021, ICCDA 2021 2021 5 IN; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen ZR, 2017, CHIN AUTOM CONGR, P2711; Choi B, 2017, IEEE ENG MED BIO, P1756, DOI 10.1109/EMBC.2017.8037183; Choi J, 2021, COMPUT METH PROG BIO, V208, DOI 10.1016/j.cmpb.2021.106251; Ciaparrone G, 2020, IEEE IJCNN; Colleoni E., 2020, P INT C MED IM COMP, P700; Colleoni E, 2019, IEEE ROBOT AUTOM LET, V4, P2714, DOI 10.1109/LRA.2019.2917163; Dergachyova O, 2016, INT J COMPUT ASS RAD, V11, P1081, DOI 10.1007/s11548-016-1371-x; Du XF, 2018, IEEE T MED IMAGING, V37, P1276, DOI 10.1109/TMI.2017.2787672; Egger J., 2020, MED DEEP LEARNING SY, V2010; Fox M, 2020, COMP MED SY, P565, DOI 10.1109/CBMS49503.2020.00112; Gao Y., 2014, MODELING; Garcia-Peraza-Herrera LC, 2021, IEEE T MED IMAGING, V40, P1450, DOI 10.1109/TMI.2021.3057884; Garcia-Peraza-Herrera LC, 2017, IEEE INT C INT ROBOT, P5717; Garrow CR, 2021, ANN SURG, V273, P684, DOI 10.1097/SLA.0000000000004425; Gessert N, 2018, MED IMAGE ANAL, V46, P162, DOI 10.1016/j.media.2018.03.002; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gonzalez C., 2020, MEDICAL IMAGE COMPUT, DOI 10.1007/978-3-030-59716-0_57; Grammatikopoulou M., 2019, ARXIV190611586; Gruijthuijsen C., 2021, ARXIV210702317; Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116; Hasan MK, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2021.101994; Hasan SMK, 2019, IEEE ENG MED BIO, P7205, DOI 10.1109/EMBC.2019.8856791; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Heim E, 2018, J MED IMAGING, V5, DOI 10.1117/1.JMI.5.3.034002; Hiasa Y., 2016, P MEDICAL BIOLOGICAL; Hong W. Y., 2020, ARXIV201212453; Hossain M, 2018, 2018 JOINT 7TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2018 2ND INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR), P470, DOI 10.1109/ICIEV.2018.8641074; HOU Y, 2022, NEURAL COMPUT APPL; Hu XW, 2017, LECT NOTES COMPUT SC, V10553, P186, DOI 10.1007/978-3-319-67558-9_22; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huaulme A, 2021, COMPUT METH PROG BIO, V212, DOI 10.1016/j.cmpb.2021.106452; Iandola Forrest N., 2016, SQUEEZENET ALEXNET L; Iglovikov Vladimir, 2018, ARXIV180105746; Isensee F., 2020, OR UNET OPTIMIZED RO; Islam M., 2019, LEARNING LOOK TRACKI, DOI 10.1007/978-3-030-32254-0_46; Islam M, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101837; Jha D., 2021, MULTIMEDIA MODELING, V2573; Jha D., 2021, ARXIV210702319; Jin A, 2018, IEEE WINT CONF APPL, P691, DOI 10.1109/WACV.2018.00081; Jin YM, 2019, LECT NOTES COMPUT SC, V11768, P440, DOI 10.1007/978-3-030-32254-0_49; Jin YM, 2020, MED IMAGE ANAL, V59, DOI 10.1016/j.media.2019.101572; Jo K, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9142865; Kalavakonda N, 2019, IEEE COMPUT SOC CONF, P514, DOI 10.1109/CVPRW.2019.00076; Kanakatte A, 2020, IEEE ENG MED BIO, P1658, DOI 10.1109/EMBC44109.2020.9176676; Kay W., 2017, ARXIV PREPRINT ARXIV; Kayhan M., 2019, ARXIV191204618; Kletz S, 2019, INT WORK CONTENT MUL; Kletz S, 2019, HEALTHC TECHNOL LETT, V6, P197, DOI 10.1049/htl.2019.0077; Kohli MD, 2017, J DIGIT IMAGING, V30, P392, DOI 10.1007/s10278-017-9976-3; KONG X, 2021, INT J COMPUT ASS RAD; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kugler D, 2020, INT J COMPUT ASS RAD, V15, P1137, DOI 10.1007/s11548-020-02157-4; Kugler D., 2020, 23 INT C MEDICAL IMA; Kurmann Thomas, 2017, Medical Image Computing and Computer-Assisted Intervention, MICCAI 2017. 20th International Conference. Proceedings: LNCS 10434, P505, DOI 10.1007/978-3-319-66185-8_57; Kurmann T, 2021, INT J COMPUT ASS RAD, V16, P1227, DOI 10.1007/s11548-021-02404-2; Laina Iro, 2017, Medical Image Computing and Computer-Assisted Intervention, MICCAI 2017. 20th International Conference. Proceedings: LNCS 10434, P664, DOI 10.1007/978-3-319-66185-8_75; Law H, 2017, MACH LEARN HEALTHC C, P88; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; Lee EJ, 2019, HEALTHC TECHNOL LETT, V6, P231, DOI 10.1049/htl.2019.0083; Lee EJ, 2019, PROC SPIE, V10951, DOI 10.1117/12.2512994; Leibetseder A, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P357, DOI 10.1145/3204949.3208127; Leppanen T, 2018, COMP MED SY, P211, DOI 10.1109/CBMS.2018.00044; Li H., 2018, ARXIV180510180, V1805, P10180; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin X. G., 2019, 2019 INT C ARTIFICIA, P245; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Liu Y, 2020, J VASC ACCESS, V21, P983, DOI 10.1177/1129729820920528; Lu J., 2020, ARXIV200902653; Luengo I., 2021, ARXIV211010965; Maier-Hein L., 2020, ARXIV201102284; Maier-Hein L, 2021, SCI DATA, V8, DOI 10.1038/s41597-021-00882-2; Maier-Hein L, 2014, LECT NOTES COMPUT SC, V8674, P438, DOI 10.1007/978-3-319-10470-6_55; Makinen S., 2021, 2021 IEEE ACM 1 WORK; Mathis A, 2018, NAT NEUROSCI, V21, P1281, DOI 10.1038/s41593-018-0209-y; Matton N, 2022, TRANSL VIS SCI TECHN, V11, DOI 10.1167/tvst.11.4.1; Meeuwsen FC, 2019, SURG ENDOSC, V33, P1426, DOI 10.1007/s00464-018-6417-4; Meireles OR, 2021, SURG ENDOSC, V35, P4918, DOI 10.1007/s00464-021-08578-9; Mhlaba JM., 2015, J HOSP ADM, V4, P82, DOI DOI 10.5430/JHA.V4N6P82; Mishra K, 2017, IEEE COMPUT SOC CONF, P2233, DOI 10.1109/CVPRW.2017.277; Mohammed A, 2019, PROC SPIE, V10951, DOI 10.1117/12.2512518; Mondal S., 2019, ARXIV190508315; Murillo P., 2018, INT S INTELLIGENT CO, P211; Murillo Paula C., 2017, CONT ENG SCI, DOI [10.12988/ces.2017.711157, DOI 10.12988/CES.2017.711157]; Nakawala H, 2019, INT J COMPUT ASS RAD, V14, P685, DOI 10.1007/s11548-018-1882-8; Namazi B., 2019, ARXIV190508983; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Ng A., 2021, CHAT ANDREW MLOPS MO; Ni ZL, 2019, IEEE ENG MED BIO, P5735, DOI 10.1109/EMBC.2019.8856495; NOGUEIRARODRIGU.A, 2020, NEUROCOMPUTING; Nwoye Chinedu Innocent, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12263), P364, DOI 10.1007/978-3-030-59716-0_35; Nwoye C. I., 2021, J MEDICAL IMAGE ANAL; Nwoye C. I., 2021, ARXIV220404746; Nwoye CI, 2019, INT J COMPUT ASS RAD, V14, P1059, DOI 10.1007/s11548-019-01958-6; Orting SN., 2020, HUM COMPUT, V7, P1, DOI [DOI 10.15346/HC.V7I1.1, 10.15346/hc.v7i1.1]; Pakhomov D, 2019, LECT NOTES COMPUT SC, V11861, P566, DOI 10.1007/978-3-030-32692-0_65; Pissas T., 2021, MEDICAL IMAGE COMPUT; Prellberg J, 2018, IEEE IJCNN; Qin FB, 2020, IEEE ROBOT AUTOM LET, V5, P6639, DOI 10.1109/LRA.2020.3009073; Qin FB, 2019, IEEE INT CONF ROBOT, P9821; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Qiu L, 2019, HEALTHC TECHNOL LETT, V6, P159, DOI 10.1049/htl.2019.0068; Raju A., 2016, WORKSH CHALL MOD MON, P1; Ramesh A., 2021, 43 ANN INT C IEEE EN; Ramesh S, 2021, INT J COMPUT ASS RAD, V16, P1111, DOI 10.1007/s11548-021-02388-z; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Reinke A, 2018, LECT NOTES COMPUT SC, V11073, P388, DOI 10.1007/978-3-030-00937-3_45; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rieke N, 2020, NPJ DIGIT MED, V3, DOI 10.1038/s41746-020-00323-1; Rocha CD, 2019, IEEE INT CONF ROBOT, P8720, DOI 10.1109/ICRA.2019.8794334; Rodrigues M., 2022, SMART HLTH, DOI 10.1016/j.smhl.2021.100244; Rodrigues M., 2021, 2021 AUSTRALASIAN JO; Rojas E., 2020, ARXIV200402809; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ross T, 2020, ARXIV200310299; Roychowdhury S., 2017, IDENTIFICATION SURGI; Sahu Manish, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12263), P784, DOI 10.1007/978-3-030-59716-0_75; Sahu M., 2017, ZIB REPORT, V2017, P30; Sahu M, 2021, INT J COMPUT ASS RAD, V16, P849, DOI 10.1007/s11548-021-02383-4; Sahu M, 2017, INT J COMPUT ASS RAD, V12, P1013, DOI 10.1007/s11548-017-1565-x; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sarikaya D, 2017, IEEE T MED IMAGING, V36, P1542, DOI 10.1109/TMI.2017.2665671; Schoeffmann K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P421, DOI 10.1145/3204949.3208137; Shimizu T, 2021, J IMAGING, V7, DOI 10.3390/jimaging7020015; Shvets AA, 2018, 2018 17TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P624, DOI 10.1109/ICMLA.2018.00100; Silva S, 2019, I S BIOMED IMAGING, P270, DOI 10.1109/ISBI.2019.8759317; Stockert EW, 2014, J AM COLL SURGEONS, V219, P646, DOI 10.1016/j.jamcollsurg.2014.06.019; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Sznitman R, 2012, LECT NOTES COMPUT SC, V7511, P568, DOI 10.1007/978-3-642-33418-4_70; TANG E, 2022, BIOMED OPT EXPRESS; Twinanda AP, 2017, IEEE T MED IMAGING, V36, P86, DOI 10.1109/TMI.2016.2593957; van Amsterdam B., 2021, GESTURE RECOGNITION, V68, P2021, DOI [10.1109/TBME.2021.3054828, DOI 10.1109/TBME.2021.3054828]; Vardazaryan A, 2018, LECT NOTES COMPUT SC, V11043, P169, DOI 10.1007/978-3-030-01364-6_19; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Voigtlaender P., 2019, PROC CVPR IEEE; Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349; Wagner M., 2021, ARXIV210914956; Wang S., 2019, INFORM PROCESSING ME, V10, P1; Ward TM, 2021, COMPUT ASSIST SURG, V26, P58, DOI 10.1080/24699322.2021.1937320; Ward TM, 2021, SURGERY, V169, P1253, DOI 10.1016/j.surg.2020.10.039; Wohlin C., 2014, P 18 INT C EV ASS SO, DOI [10.1145/2601248.2601268, DOI 10.1145/2601248.2601268]; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Xue Y, 2022, KNOWL-BASED SYST, V239, DOI 10.1016/j.knosys.2021.107860; Yamazaki Y, 2020, J AM COLL SURGEONS, V230, P725, DOI 10.1016/j.jamcollsurg.2020.01.037; Yang CM, 2020, COMPUT ASSIST SURG, V25, P15, DOI 10.1080/24699322.2020.1801842; Yang HX, 2019, LECT NOTES COMPUT SC, V11768, P263, DOI 10.1007/978-3-030-32254-0_30; Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255; Zadeh SM, 2020, SURG ENDOSC, V34, P5377, DOI 10.1007/s00464-019-07330-8; Zhang C, 2021, KNOWL-BASED SYST, V216, DOI 10.1016/j.knosys.2021.106775; Zhang JY, 2020, INT J COMPUT ASS RAD, V15, P1335, DOI 10.1007/s11548-020-02214-y; Zhang ZK, 2021, IEEE ROBOT AUTOM LET, V6, P6266, DOI 10.1109/LRA.2021.3092302; Zhao ZJ, 2020, IEEE T IND ELECTRON, V67, P4846, DOI 10.1109/TIE.2019.2931230; Zhao ZJ, 2019, HEALTHC TECHNOL LETT, V6, P275, DOI 10.1049/htl.2019.0064; Zhao ZJ, 2019, J ENG-JOE, P467, DOI 10.1049/joe.2018.9401; Zhao ZJ, 2017, COMPUT ASSIST SURG, V22, P26, DOI 10.1080/24699322.2017.1378777; Zia A., 2016, FINE TUNING DEEP ARC; Zisimopoulos O, 2017, HEALTHC TECHNOL LETT, V4, P216, DOI 10.1049/htl.2017.0064; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	189	1	1	5	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2222	2248		10.1007/s11263-022-01640-6	http://dx.doi.org/10.1007/s11263-022-01640-6		JUL 2022	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG		Green Published, hybrid			2022-12-18	WOS:000823328100001
J	Zhang, KH; Ren, WQ; Luo, WH; Lai, WS; Stenger, B; Yang, MH; Li, HD				Zhang, Kaihao; Ren, Wenqi; Luo, Wenhan; Lai, Wei-Sheng; Stenger, Bjorn; Yang, Ming-Hsuan; Li, Hongdong			Deep Image Deblurring: A Survey	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image deblurring; Low-level vision; Image enhancement; Deep learning; Image restoration	QUALITY ASSESSMENT; INFORMATION; SIMILARITY; FRAMEWORK; REMOVAL; SHAKEN	Image deblurring is a classic problem in low-level computer vision with the aim to recover a sharp image from a blurred input image. Advances in deep learning have led to significant progress in solving this problem, and a large number of deblurring networks have been proposed. This paper presents a comprehensive and timely survey of recently published deep-learning based image deblurring approaches, aiming to serve the community as a useful literature review. We start by discussing common causes of image blur, introduce benchmark datasets and performance metrics, and summarize different problem formulations. Next, we present a taxonomy of methods using convolutional neural networks (CNN) based on architecture, loss function, and application, offering a detailed review and comparison. In addition, we discuss some domain-specific deblurring applications including face images, text, and stereo image pairs. We conclude by discussing key challenges and future research directions.	[Zhang, Kaihao; Li, Hongdong] Australian Natl Univ, Canberra, ACT, Australia; [Ren, Wenqi; Luo, Wenhan] Sun Yat Sen Univ, Guangzhou 510275, Peoples R China; [Stenger, Bjorn] Rakuten Grp Inc, Rakuten Inst Technol, Tokyo, Japan; [Lai, Wei-Sheng; Yang, Ming-Hsuan] Univ Calif Merced, Sch Engn, Merced, CA 95343 USA	Australian National University; Sun Yat Sen University; Rakuten Group, Inc; University of California System; University of California Merced	Yang, MH (corresponding author), Univ Calif Merced, Sch Engn, Merced, CA 95343 USA.	kaihao.zhang@anu.edu.au; rwq.renwenqi@gmail.com; whluo.china@gmail.com; wlai24@ucmerced.edu; bjorn@cantab.net; mhyang@ucmerced.edu; hongdong.li@anu.edu.au	Luo, Wenhan/GZL-0535-2022; Zhang, Kaihao/HGC-0368-2022; Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304	NSF CAREER Grant [1149783]; ARC-Discovery grant projects [DP 190102 261, DP220100800]; Ford Alliance URP grant	NSF CAREER Grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); ARC-Discovery grant projects; Ford Alliance URP grant	This research was funded in part by the NSF CAREER Grant #1149783, ARC-Discovery grant projects (DP 190102 261 and DP220100800), and a Ford Alliance URP grant.	Abuolaim Abdullah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P111, DOI 10.1007/978-3-030-58607-2_7; Aittala M, 2018, LECT NOTES COMPUT SC, V11212, P748, DOI 10.1007/978-3-030-01237-3_45; Aljadaany R, 2019, PROC CVPR IEEE, P10227, DOI 10.1109/CVPR.2019.01048; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anwar S., 2017, BRIT MACH VIS C; Bac S, 2007, COMPUT GRAPH FORUM, V26, P571, DOI 10.1111/j.1467-8659.2007.01080.x; Bahat Y, 2017, IEEE I CONF COMP VIS, P3306, DOI 10.1109/ICCV.2017.356; Bigdeli S. A., 2017, PROC INT C NEURAL IN, V30, P763; Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652; Boracchi G, 2012, IEEE T IMAGE PROCESS, V21, P3502, DOI 10.1109/TIP.2012.2192126; Brooks T, 2019, PROC CVPR IEEE, P6833, DOI 10.1109/CVPR.2019.00700; Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14; Chakrabarti A, 2010, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2010.5539954; Chen F, 2009, IEEE T SIGNAL PROCES, V57, P2467, DOI 10.1109/TSP.2009.2018358; Chen HJ, 2018, IEEE INT CONF COMPUT; Chen SJ, 2015, IEEE T IMAGE PROCESS, V24, P4433, DOI 10.1109/TIP.2015.2465162; Chen XG, 2011, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2011.5995568; Cho HJ, 2012, LECT NOTES COMPUT SC, V7576, P524, DOI 10.1007/978-3-642-33715-4_38; Cho S, 2011, IEEE I CONF COMP VIS, P495, DOI 10.1109/ICCV.2011.6126280; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Chrysos GG, 2019, INT J COMPUT VISION, V127, P801, DOI 10.1007/s11263-018-1138-7; Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940; Dong J., 2020, ADV NEUR IN, V33; Eigen D, 2014, ADV NEUR IN, V27; Eris KK, 2018, QUATERN INT, V486, P4, DOI 10.1016/j.quaint.2017.09.027; Eslami SM, 2016, NEURIPS, V1; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Fiori S, 1999, ISCAS '99: PROCEEDINGS OF THE 1999 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, P1, DOI 10.1109/ISCAS.1999.778770; Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397; Gast J, 2016, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2016.204; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Goncalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021; Gong D, 2020, IEEE T NEUR NET LEAR, V31, P5468, DOI 10.1109/TNNLS.2020.2968289; Gong D, 2017, PROC CVPR IEEE, P3806, DOI 10.1109/CVPR.2017.405; Gu CZ, 2021, IEEE T IMAGE PROCESS, V30, P345, DOI 10.1109/TIP.2020.3036745; HaCohen Y, 2013, IEEE I CONF COMP VIS, P2384, DOI 10.1109/ICCV.2013.296; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hirsch M, 2011, IEEE I CONF COMP VIS, P463, DOI 10.1109/ICCV.2011.6126276; Hofeld T., 2016, QUALITY USER EXPERIE, V1, P2, DOI [10.1007/S41233-016-0002-1, DOI 10.1007/S41233-016-0002-1]; Hradi M., 2015, BRIT MACH VIS C; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; HUMMEL RA, 1987, COMPUT VISION GRAPH, V38, P66, DOI 10.1016/S0734-189X(87)80153-6; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jaesung Rim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P184, DOI 10.1007/978-3-030-58595-2_12; Jiang Z, 2020, ARXIV PREPRINT ARXIV; Jin MG, 2018, IEEE COMPUT SOC CONF, P858, DOI 10.1109/CVPRW.2018.00118; Jin MG, 2017, PROC CVPR IEEE, P3834, DOI 10.1109/CVPR.2017.408; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jolicoeur-Martineau Alexia, 2018, ARXIV180700734; Kang SB, 2007, PROC CVPR IEEE, P1867; Kaufman A, 2020, PROC CVPR IEEE, P5810, DOI 10.1109/CVPR42600.2020.00585; Kettunen M., 2019, ARXIV PREPRINT ARXIV; Kheradmand A, 2014, IEEE T IMAGE PROCESS, V23, P5136, DOI 10.1109/TIP.2014.2362059; Kim TH, 2018, LECT NOTES COMPUT SC, V11207, P111, DOI 10.1007/978-3-030-01219-9_7; Kim TH, 2017, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2017.435; Kim TH, 2013, IEEE I CONF COMP VIS, P3160, DOI 10.1109/ICCV.2013.392; Kohler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3; Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402; Kruse J, 2017, IEEE I CONF COMP VIS, P4596, DOI 10.1109/ICCV.2017.491; Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815; Li L, 2020, IEEE T IMAGE PROCESS, V29, P5273, DOI 10.1109/TIP.2020.2980173; Li Peilun, 2018, BMVC; Li Y., 2019, ARXIV PREPRINT ARXIV; Lin S, 2020, EUR C COMP VIS; Liu H, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM - EDUCATIONAL RESEARCH AND EDUCATIONAL TECHNOLOGY, 2019, P3; Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lu BY, 2019, PROC CVPR IEEE, P10217, DOI 10.1109/CVPR.2019.01047; Lu Yiping, 2017, ARXIV PREPRINT ARXIV; Madam NT, 2018, LECT NOTES COMPUT SC, V11214, P358, DOI 10.1007/978-3-030-01249-6_22; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Masia B., 2011, S IB COMP BRAF; Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51; MITSA T, 1993, P IEEE INT C AC SPEE, V5, P301; Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726; Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050; Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325; Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888; Mustaniemi J, 2019, IEEE WINT CONF APPL, P1914, DOI 10.1109/WACV.2019.00208; Nah S., 2020, ARXIV PREPRINT ARXIV; Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251; Nah S, 2019, PROC CVPR IEEE, P8094, DOI 10.1109/CVPR.2019.00829; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Nan YS, 2020, PROC CVPR IEEE, P3623, DOI 10.1109/CVPR42600.2020.00368; Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244; Nimisha TM, 2017, IEEE I CONF COMP VIS, P4762, DOI 10.1109/ICCV.2017.509; Pan JS, 2020, PROC CVPR IEEE, P3040, DOI 10.1109/CVPR42600.2020.00311; Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371; Pan JS, 2014, LECT NOTES COMPUT SC, V8695, P47, DOI 10.1007/978-3-319-10584-0_4; Panci G, 2003, IEEE T IMAGE PROCESS, V12, P1324, DOI 10.1109/TIP.2003.818022; Park I. K., 2019, ARXIV190400352; Park P.D., 2020, EUROPEAN C COMPUTER; Pham H, 2018, PR MACH LEARN RES, V80; Purohit K., 2019, ARXIV PREPRINT ARXIV; Purohit K, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00699; Ren D., 2019, ARXIV PREPRINT ARXIV; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren W., 2018, ADV NEURAL INFORM PR, P295; Ren WQ, 2019, IEEE I CONF COMP VIS, P9387, DOI 10.1109/ICCV.2019.00948; Ren WQ, 2017, IEEE I CONF COMP VIS, P1086, DOI 10.1109/ICCV.2017.123; Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563; Schmidt U, 2013, PROC CVPR IEEE, P604, DOI 10.1109/CVPR.2013.84; Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418; Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142; Sellent A, 2016, LECT NOTES COMPUT SC, V9906, P558, DOI 10.1007/978-3-319-46475-6_35; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378; Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389; Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TNNLS.2020.2995708, 10.1109/TKDE.2020.2970050]; Shen W, 2020, PROC CVPR IEEE, P5113, DOI 10.1109/CVPR42600.2020.00516; Shen ZY, 2019, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2019.00567; Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379; Sim H, 2019, IEEE COMPUT SOC CONF, P2140, DOI 10.1109/CVPRW.2019.00267; Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130; Son CH, 2011, IEEE T CONSUM ELECTR, V57, P1791, DOI 10.1109/TCE.2011.6131155; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677; Sun L., 2012, IEEE INT C COMPUTATI; Sun LB, 2013, IEEE INT CONF COMPUT; Sun TC, 2017, IEEE I CONF COMP VIS, P3268, DOI 10.1109/ICCV.2017.352; Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0; Tang C, 2019, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2019.00281; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; VAIRY M, 1995, PATTERN RECOGN, V28, P965, DOI 10.1016/0031-3203(94)00146-D; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823; Wei Zhang, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1947, DOI 10.1109/ICCVW.2009.5457520; Whyte O, 2014, INT J COMPUT VISION, V110, P185, DOI 10.1007/s11263-014-0727-3; Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7; Wieschollek P, 2017, IEEE I CONF COMP VIS, P231, DOI 10.1109/ICCV.2017.34; Xia FT, 2016, LECT NOTES COMPUT SC, V9909, P648, DOI 10.1007/978-3-319-46454-1_39; Xu L, 2014, ADV NEUR IN, V27; Xu L, 2014, LECT NOTES COMPUT SC, V8693, P33, DOI 10.1007/978-3-319-10602-1_3; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157; Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36; Xu XY, 2018, IEEE T IMAGE PROCESS, V27, P194, DOI 10.1109/TIP.2017.2753658; Yasarla R., 2019, ARXIV PREPRINT ARXIV; Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789; Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613; Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267; Zhang JW, 2017, PROC CVPR IEEE, P6969, DOI 10.1109/CVPR.2017.737; ZHANG K, 2017, PROC CVPR IEEE, P2808, DOI DOI 10.1109/CVPR.2017.300; Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328; Zhang K, 2019, PROC CVPR IEEE, P1671, DOI 10.1109/CVPR.2019.00177; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281; Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3111; Zhao WD, 2019, PROC CVPR IEEE, P8897, DOI 10.1109/CVPR.2019.00911; Zhihang Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P191, DOI 10.1007/978-3-030-58539-6_12; Zhong L, 2013, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2013.85; Zhou SC, 2019, IEEE I CONF COMP VIS, P2482, DOI 10.1109/ICCV.2019.00257; Zhou SC, 2019, PROC CVPR IEEE, P10988, DOI 10.1109/CVPR.2019.01125; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zoph B., 2016, ARXIV161101578; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	169	1	1	30	30	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2103	2130		10.1007/s11263-022-01633-5	http://dx.doi.org/10.1007/s11263-022-01633-5		JUN 2022	28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG		Green Submitted			2022-12-18	WOS:000815554600001
J	Qi, JY; Gao, Y; Hu, Y; Wang, XG; Liu, XY; Bai, X; Belongie, S; Yuille, A; Torr, PHS; Bai, S				Qi, Jiyang; Gao, Yan; Hu, Yao; Wang, Xinggang; Liu, Xiaoyu; Bai, Xiang; Belongie, Serge; Yuille, Alan; Torr, Philip H. S.; Bai, Song			Occluded Video Instance Segmentation: A Benchmark	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video instance segmentation; Occlusion reasoning; Dataset; Video understanding; Benchmark		Can our video understanding systems perceive objects when a heavy occlusion exists in a scene? To answer this question, we collect a large-scale dataset called OVIS for occluded video instance segmentation, that is, to simultaneously detect, segment, and track instances in occluded scenes. OVIS consists of 296k high-quality instance masks from 25 semantic categories, where object occlusions usually occur. While our human vision systems can understand those occluded instances by contextual reasoning and association, our experiments suggest that current video understanding systems cannot. On the OVIS dataset, the highest AP achieved by state-of-the-art algorithms is only 16.3, which reveals that we are still at a nascent stage for understanding objects, instances, and videos in a real-world scenario. We also present a simple plug-and-play module that performs temporal feature calibration to complement missing object cues caused by occlusion. Built upon MaskTrack R-CNN and SipMask, we obtain a remarkable AP improvement on the OVIS dataset. The OVIS dataset and project code are available at http://songbai.site/ovis.	[Qi, Jiyang; Wang, Xinggang; Bai, Xiang] Huazhong Univ Sci & Technol, Wuhan, Peoples R China; [Qi, Jiyang; Gao, Yan; Hu, Yao; Liu, Xiaoyu; Bai, Song] Alibaba Grp, Beijing, Peoples R China; [Belongie, Serge] Univ Copenhagen, Copenhagen, Denmark; [Yuille, Alan] Johns Hopkins Univ, Baltimore, MD USA; [Torr, Philip H. S.; Bai, Song] Univ Oxford, Oxford, England	Huazhong University of Science & Technology; Alibaba Group; University of Copenhagen; Johns Hopkins University; University of Oxford	Bai, S (corresponding author), Alibaba Grp, Beijing, Peoples R China.; Bai, S (corresponding author), Univ Oxford, Oxford, England.	jiyangqi@hust.edu.cn; yangao0119@gmail.com; yaoohu@alibaba-inc.com; xgwang@hust.edu.cn; xiaoyuliu1991xyl@gmail.com; xbai@hust.edu.cn; s.belongie@di.ku.dk; alan.l.yuille@gmail.com; philip.torr@eng.ox.ac.uk; songbai.site@gmail.com			Turing AI Fellowship [EP/W002981/1]	Turing AI Fellowship	This work is supported by Turing AI Fellowship EP/W002981/1.	Abu-El-Haija S., 2006, ARXIV PREPRINT ARXIV; Athar Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P158, DOI 10.1007/978-3-030-58621-8_10; Bertasius Gedas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9736, DOI 10.1109/CVPR42600.2020.00976; Bertasius G, 2018, LECT NOTES COMPUT SC, V11216, P342, DOI 10.1007/978-3-030-01258-8_21; Bolya Daniel, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P558, DOI 10.1007/978-3-030-58580-8_33; Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005; Caelles Sergi, 2019, ARXIV; Carion N., 2020, COMPUTER VISION ECCV, P213, DOI DOI 10.1007/978-3-030-58452-8_13; Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dahun Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9856, DOI 10.1109/CVPR42600.2020.00988; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Devaranjan Jeevan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P715, DOI 10.1007/978-3-030-58520-4_42; DeVries T., 2017, ARXIV PREPRINT ARXIV; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146; Fang Yuxin, 2021, ICCV; Fayyaz M., 2016, ACCV; Geiger A., 2012, P IEEE COMP SOC C CO; Ghiasi G, 2021, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR46437.2021.00294; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; He K., 2022, P IEEE CVF C COMP VI, P16000; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hegde J, 2008, J VISION, V8, DOI 10.1167/8.4.16; Hosang J, 2017, PROC CVPR IEEE, P6469, DOI 10.1109/CVPR.2017.685; Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Hwang S., 2021, ARXIV PREPRINT ARXIV; Jiale Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P1, DOI 10.1007/978-3-030-58568-6_1; Jialian Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13427, DOI 10.1109/CVPR42600.2020.01344; Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916; Kar A, 2019, IEEE I CONF COMP VIS, P4550, DOI 10.1109/ICCV.2019.00465; Ke L, 2021, PROC CVPR IEEE, P4018, DOI 10.1109/CVPR46437.2021.00401; Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963; Kirillov Alexander, 2020, CVPR; Kortylewski Adam, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8937, DOI 10.1109/CVPR42600.2020.00896; Kortylewski A, 2021, INT J COMPUT VISION, V129, P736, DOI 10.1007/s11263-020-01401-3; Kortylewski A, 2020, IEEE WINT CONF APPL, P1322, DOI 10.1109/WACV45572.2020.9093560; Lazarow Justin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10717, DOI 10.1109/CVPR42600.2020.01073; Li Minghan, 2021, CVPR; Li SY, 2018, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2018.00683; Li X, 2018, LECT NOTES COMPUT SC, V11206, P287, DOI [10.1007/978-3-030-01216-8_18, 10.1007/978-3-030-01267-0_22]; LI Y, 2020, NEURIPS, V33; Lin C. C., 2020, CVPR; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969; Liu QK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P530; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Milan A., 2016, MOT16 BENCHMARK MULT; NAKAYAMA K, 1989, PERCEPTION, V18, P55, DOI 10.1068/p180055; Nikolenko S.I., 2019, ARXIV; Nilsson D, 2018, PROC CVPR IEEE, P6819, DOI 10.1109/CVPR.2018.00713; Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/ICCV.2019.00932; Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372; Qi J., 2021, 35 C NEURAL INFORM P; Qizhu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13317, DOI 10.1109/CVPR42600.2020.01333; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shao S, 2018, ARXIV180500123; Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Tokmakov P, 2017, PROC CVPR IEEE, P531, DOI 10.1109/CVPR.2017.64; Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971; Voigtlaender P, 2019, PROC CVPR IEEE, P7934, DOI 10.1109/CVPR.2019.00813; Voigtlaender Paul, 2017, ARXIV170609364; Wang HC, 2021, PROC CVPR IEEE, P1296, DOI 10.1109/CVPR46437.2021.00135; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154; Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11; Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811; Wang Y, 2020, ARCH DERMATOL RES, V312, P581, DOI 10.1007/s00403-020-02044-7; Wen LY, 2020, COMPUT VIS IMAGE UND, V193, DOI 10.1016/j.cviu.2020.102907; Wu J., 2021, CVPR; Wu JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1570, DOI 10.1145/3394171.3413970; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409; Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36; Xu Z., 2020, ECCV; Xuangeng Chu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12211, DOI 10.1109/CVPR42600.2020.01223; Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529; Yang Shusheng, 2021, ICCV, P8043; Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612; Zhan XH, 2020, PROC CVPR IEEE, P6687, DOI 10.1109/CVPR42600.2020.00672; Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39; Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23; Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441	95	1	1	8	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					2022	2039		10.1007/s11263-022-01629-1	http://dx.doi.org/10.1007/s11263-022-01629-1		JUN 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP		hybrid, Green Submitted, Green Published			2022-12-18	WOS:000812583900001
J	Roldao, L; de Charette, R; Verroust-Blondet, A				Roldao, Luis; de Charette, Raoul; Verroust-Blondet, Anne			3D Semantic Scene Completion: A Survey	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic scene completion; 3D Vision; Semantic segmentation; Scene understanding; Scene reconstruction; Point cloud	OF-THE-ART; SEGMENTATION; SHAPE; RECONSTRUCTION; DATABASE; DATASET	Semantic scene completion (SSC) aims to jointly estimate the complete geometry and semantics of a scene, assuming partial sparse input. In the last years following the multiplication of large-scale 3D datasets, SSC has gained significant momentum in the research community because it holds unresolved challenges. Specifically, SSC lies in the ambiguous completion of large unobserved areas and the weak supervision signal of the ground truth. This led to a substantially increasing number of papers on the matter. This survey aims to identify, compare and analyze the techniques providing a critical analysis of the SSC literature on both methods and datasets. Throughout the paper, we provide an in-depth analysis of the existing works covering all choices made by the authors while highlighting the remaining avenues of research. SSC performance of the SoA on the most popular datasets is also evaluated and analyzed.	[Roldao, Luis; de Charette, Raoul; Verroust-Blondet, Anne] INRIA, Paris, France; [Roldao, Luis] AKKA Res, Guyancourt, France	Inria	Roldao, L (corresponding author), INRIA, Paris, France.; Roldao, L (corresponding author), AKKA Res, Guyancourt, France.	luis.roldao@inria.fr; raoul.de-charette@inria.fr; anne.verroust@inria.fr						Abbasi A, 2019, VISUAL COMPUT, V35, P271, DOI 10.1007/s00371-018-1586-7; Ahmed E., 2018, ARXIV PREPRINT ARXIV; Nguyen A, 2013, PROCEEDINGS OF THE 2013 6TH IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P225, DOI 10.1109/RAM.2013.6758588; Armeni Iro, 2017, ARXIV170201105; Avetisyan Armen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P596, DOI 10.1007/978-3-030-58542-6_36; Avetisyan A, 2019, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2019.00272; Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Boulch A., 2017, 3DOR EUROGRAPHICS, V3, P2; Boulch A, 2018, COMPUT GRAPH-UK, V71, P189, DOI 10.1016/j.cag.2017.11.010; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Cai YJ, 2021, PROC CVPR IEEE, P324, DOI 10.1109/CVPR46437.2021.00039; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen X., 2020, ICIP; Chen X., CVPR; Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI 10.1109/ICCV.2019.00987; Cheng R., 2020, CORL; Cherabier I, 2018, LECT NOTES COMPUT SC, V11216, P325, DOI 10.1007/978-3-030-01258-8_20; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Dai A, 2020, PROC CVPR IEEE, P846, DOI 10.1109/CVPR42600.2020.00093; Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28; Dai A, 2018, PROC CVPR IEEE, P4578, DOI 10.1109/CVPR.2018.00481; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098; de Charette R., 2019, ARXIV190801523; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Dourado A, 2020, ARXIV190802893; Dourado Aloisio, 2020, VISIGRAPP; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Firman M, 2016, IEEE COMPUT SOC CONF, P661, DOI 10.1109/CVPRW.2016.88; Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586; Fu Huan, 2020, ARXIV201109127; Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8; GAIDON A, 2016, PROC CVPR IEEE, P4340, DOI DOI 10.1109/CVPR.2016.470; GAO B, 2021, IEEE T INTELL TRANSP; Garbade M, 2019, IEEE COMPUT SOC CONF, P416, DOI 10.1109/CVPRW.2019.00055; Garg S., 2020, ARXIV210100443; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A, 2015, LECT NOTES COMPUT SC, V9358, P183, DOI 10.1007/978-3-319-24947-6_15; Gkioxari G, 2019, IEEE I CONF COMP VIS, P9784, DOI 10.1109/ICCV.2019.00988; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Griffiths D, 2019, ARXIV190704758; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Guedes A. B. S., 2018, ARXIV180204735; Guo R., 2013, ICCV; Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434; Guo YX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P726; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Hackel T., 2017, ISPRS ANN, VIV-1-W1, P9198; Han XG, 2019, PROC CVPR IEEE, P234, DOI 10.1109/CVPR.2019.00032; Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19; Handa A., 2016, ARXIV151107041; Hou J, 2020, PROC CVPR IEEE, P2095, DOI 10.1109/CVPR42600.2020.00217; Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455; Hua BS, 2016, INT CONF 3D VISION, P92, DOI 10.1109/3DV.2016.18; Huang Z, 2020, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR42600.2020.00316; Izadinia H, 2017, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2017.260; Jiao LC, 2019, IEEE ACCESS, V7, P128837, DOI 10.1109/ACCESS.2019.2939201; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kim G, 2020, IEEE INT C INT ROBOT, P10758, DOI 10.1109/IROS45743.2020.9340856; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Kundu A., 2018, CVPR; Kurenkov A, 2018, IEEE WINT CONF APPL, P858, DOI 10.1109/WACV.2018.00099; Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479; Li DP, 2017, IEEE T VIS COMPUT GR, V23, P1809, DOI 10.1109/TVCG.2016.2553102; Li J, 2020, PROC CVPR IEEE, P3348, DOI 10.1109/CVPR42600.2020.00341; Li J, 2019, PROC CVPR IEEE, P7685, DOI 10.1109/CVPR.2019.00788; Li J, 2020, IEEE ROBOT AUTOM LET, V5, P219, DOI 10.1109/LRA.2019.2953639; Li S., 2020, P AAAI C ART INT FEB, P1140211409, DOI DOI 10.1609/AAAI.V34I07.6803; Li YY, 2018, ADV NEUR IN, V31; Li YY, 2015, COMPUT GRAPH FORUM, V34, P435, DOI 10.1111/cgf.12573; Li Y, 2021, IEEE T NEUR NET LEAR, V32, P3412, DOI 10.1109/TNNLS.2020.3015992; Liao YY, 2018, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR.2018.00308; Lin DH, 2013, IEEE I CONF COMP VIS, P1417, DOI 10.1109/ICCV.2013.179; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Liu SJ, 2018, ADV NEUR IN, V31; Liu WP, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194188; Liu Y., 2020, ARXIV200207269; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Lu H., 2020, ARXIV200908920; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; MEAGHER D, 1982, COMPUT VISION GRAPH, V19, P129, DOI 10.1016/0146-664X(82)90104-6; Meng HY, 2019, IEEE I CONF COMP VIS, P8499, DOI 10.1109/ICCV.2019.00859; Mitra NJ, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12010; Muller N, 2021, PROC CVPR IEEE, P6067, DOI 10.1109/CVPR46437.2021.00601; Nair R, 2012, LECT NOTES COMPUT SC, V7584, P1, DOI 10.1007/978-3-642-33868-7_1; Nan LL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366156; Nealen A., 2006, GRAPHITE, V06; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Nie YY, 2020, PROC CVPR IEEE, P52, DOI 10.1109/CVPR42600.2020.00013; Pan YC, 2020, IEEE INT VEH SYM, P687; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Pauly M., 2005, S GEOM PROC, P23; Pauly M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360642; Pintore G, 2020, COMPUT GRAPH FORUM, V39, P667, DOI 10.1111/cgf.14021; Pock T, 2011, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2011.6126441; Pomerleau F., 2015, FDN TRENDS ROBOT, V4, P1, DOI DOI 10.1561/2300000035; Popov S., 2020, ECCV, V12347; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Reed S, 2017, PR MACH LEARN RES, V70; Rezende DJ, 2016, ADV NEUR IN, V29; Riegler G, 2017, INT CONF 3D VISION, P57, DOI 10.1109/3DV.2017.00017; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Rist C., 2020, PROC INTELL VEHICLES, P1086; Rist C, 2022, IEEE T PATTERN ANAL, V44, P7205, DOI 10.1109/TPAMI.2021.3095302; Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863; Roldo L., 2020, ARXIV PREPRINT ARXIV, P111119; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352; Roynard X, 2018, INT J ROBOT RES, V37, P545, DOI 10.1177/0278364918767506; Saputra MRU, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3177853; Shao TJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366155; Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20; Shen CH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366199; Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Sipiran I, 2014, COMPUT GRAPH FORUM, V33, P131, DOI 10.1111/cgf.12481; Smith Edward J., 2017, ABS170709557 CORR; Song SR, 2018, PROC CVPR IEEE, P3847, DOI 10.1109/CVPR.2018.00405; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Sorkine O, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P191, DOI 10.1109/SMI.2004.1314506; Straub Julian, 2019, ARXIV190605797; Stutz D, 2018, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2018.00209; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sung M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818094; Tan WK, 2020, IEEE COMPUT SOC CONF, P797, DOI 10.1109/CVPRW50498.2020.00109; Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047; Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Thrun S, 2005, IEEE I CONF COMP VIS, P1824; Vallet B, 2015, COMPUT GRAPH-UK, V49, P126, DOI 10.1016/j.cag.2015.03.004; Varley J, 2017, IEEE INT C INT ROBOT, P2442; Wang P.-S., 2020, CVPR WORKSHOPS; Wang PS, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275050; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272; Wang X., 2019, GERMAN C PATTERN REC, V11824; Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087; Wang XG, 2020, IEEE INT C INT ROBOT, P10719, DOI 10.1109/IROS45743.2020.9340862; Wang Y., 2020, ECCV, V12348, P7085; Wang YD, 2019, IEEE I CONF COMP VIS, P8607, DOI 10.1109/ICCV.2019.00870; Wang YD, 2018, INT CONF 3D VISION, P426, DOI 10.1109/3DV.2018.00056; Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201; Wu SC, 2020, INT CONF 3D VISION, P801, DOI 10.1109/3DV50981.2020.00090; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Xie H., 2020, ECCV, V12354; Xie YX, 2020, IEEE GEOSC REM SEN M, V8, P38, DOI 10.1109/MGRS.2019.2937630; Yan X, 2021, AAAI CONF ARTIF INTE, V35, P3101; Yang B, 2019, IEEE T PATTERN ANAL, V41, P2820, DOI 10.1109/TPAMI.2018.2868195; Yu F., 2016, P ICLR 2016; Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088; Zhang G., 2021, ICIP; Zhang JH, 2018, LECT NOTES COMPUT SC, V11216, P749, DOI 10.1007/978-3-030-01258-8_45; Zhang JY, 2019, IEEE ACCESS, V7, P179118, DOI 10.1109/ACCESS.2019.2958671; Zhang L, 2018, NEUROCOMPUTING, V318, P182, DOI 10.1016/j.neucom.2018.08.052; Zhang PP, 2019, IEEE I CONF COMP VIS, P7800, DOI 10.1109/ICCV.2019.00789; Zhang W., 2020, ECCV, V12370; Zheng B, 2013, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2013.402; Zhong M, 2020, FRONT ARTIF INTEL AP, V325, P2824, DOI 10.3233/FAIA200424; Zimmermann K, 2017, IEEE I CONF COMP VIS, P1548, DOI 10.1109/ICCV.2017.171; Zollhofer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386	171	1	1	7	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					1978	2005		10.1007/s11263-021-01504-5	http://dx.doi.org/10.1007/s11263-021-01504-5		JUN 2022	28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP		Green Submitted			2022-12-18	WOS:000806653700001
J	Du, B; Ye, J; Zhang, J; Liu, JH; Tao, DC				Du, Bo; Ye, Jian; Zhang, Jing; Liu, Juhua; Tao, Dacheng			I3CL: Intra- and Inter-Instance Collaborative Learning for Arbitrary-Shaped Scene Text Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Text detection; Collaborative learning; Semi-supervised learning; Deep learning; Transformer		Existing methods for arbitrary-shaped text detection in natural scenes face two critical issues, i.e., (1) fracture detections at the gaps in a text instance; and (2) inaccurate detections of arbitrary-shaped text instances with diverse background context. To address these issues, we propose a novel method named Intra- and Inter-Instance Collaborative Learning (I3CL). Specifically, to address the first issue, we design an effective convolutional module with multiple receptive fields, which is able to collaboratively learn better character and gap feature representations at local and long ranges inside a text instance. To address the second issue, we devise an instance-based transformer module to exploit the dependencies between different text instances and a global context module to exploit the semantic context from the shared background, which are able to collaboratively learn more discriminative text feature representation. In this way, I3CL can effectively exploit the intra- and inter-instance dependencies together in a unified end-to-end trainable framework. Besides, to make full use of the unlabeled data, we design an effective semi-supervised learning method to leverage the pseudo labels via an ensemble strategy. Without bells and whistles, experimental results show that the proposed I3CL sets new state-of-the-art results on three challenging public benchmarks, i.e., an F-measure of 77.5% on ArT, 86.9% on Total-Text, and 86.4% on CTW-1500. Notably, our I3CL with the ResNeSt-101 backbone ranked the 1st place on the ArT leaderboard. Code is available at .	[Du, Bo; Ye, Jian] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Inst Artificial Intelligence, Wuhan, Peoples R China; [Du, Bo; Ye, Jian] Wuhan Univ, Hubei Key Lab Multimedia & Network Commun Engn, Wuhan, Peoples R China; [Zhang, Jing; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Sydney, NSW, Australia; [Liu, Juhua] Wuhan Univ, Res Ctr Graph Commun Printing & Packaging, Wuhan, Peoples R China; [Liu, Juhua] Wuhan Univ, Inst Artificial Intelligence, Wuhan, Peoples R China; [Tao, Dacheng] JD Explore Acad, Beijing, Peoples R China	Wuhan University; Wuhan University; University of Sydney; Wuhan University; Wuhan University	Liu, JH (corresponding author), Wuhan Univ, Res Ctr Graph Commun Printing & Packaging, Wuhan, Peoples R China.; Liu, JH (corresponding author), Wuhan Univ, Inst Artificial Intelligence, Wuhan, Peoples R China.	dubo@whu.edu.cn; leaf-yej@whu.edu.cn; jing.zhang1@sydney.edu.au; liujuhua@whu.edu.cn; dacheng.tao@gmail.com		Liu, Juhua/0000-0002-3907-8820	National Natural Science Foundation of China [62076186, 62141112, 41871243]; Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies) [2019AEA170]; ARC [FL-170100117]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies); ARC(Australian Research Council)	This work was supported in part by National Natural Science Foundation of China: Grant No. 62076186, 62141112 and 41871243, in part by Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies): Grant No. 2019AEA170, and in part by ARC FL-170100117. The numerical calculations in this paper have been done on the supercomputing system in the Supercomputing Center of Wuhan University.	Baek Y., 2020, PROC EUR C COMPUT VI, P504; Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959; Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593; Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157; Chee Kheng Chng, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1571, DOI 10.1109/ICDAR.2019.00252; Chen Z, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01370-7; Dai PW, 2021, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR46437.2021.00731; Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978; Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Liao MH, 2017, AAAI CONF ARTIF INTE, P4161; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu JH, 2020, IEEE T IMAGE PROCESS, V29, P5924, DOI 10.1109/TIP.2020.2984082; Liu YL, 2020, IEEE T IMAGE PROCESS, V29, P2918, DOI 10.1109/TIP.2019.2954218; Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Liu ZC, 2019, PROC CVPR IEEE, P7261, DOI 10.1109/CVPR.2019.00744; Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2; Nayef N., 2019, ARXIV190700945; Qiao L, 2020, AAAI CONF ARTIF INTE, V34, P11899; Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070; Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371; Shi-Xue Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9696, DOI 10.1109/CVPR42600.2020.00972; Song, 2018, ARXIV PREPRINT ARXIV; Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020; Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436; Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819; Wang H, 2020, AAAI CONF ARTIF INTE, V34, P12160; Wang J, 2018, PROC CVPR IEEE, P1149, DOI 10.1109/CVPR.2018.00126; Wang PF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1277, DOI 10.1145/3343031.3350988; Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853; Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956; Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu Y., 2021, ADV NEURAL INFORM PR, V34, P28522; Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589; Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989; Yang QP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1071; Ye J, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P516; Yipeng Sun, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1557, DOI 10.1109/ICDAR.2019.00250; Yuliang L., 2017, ARXIV PREPRINT ARXIV; Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983; Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177; Zhang, 2022, ARXIV PREPRINT ARXIV; Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080; Zhang H., ARXIV PREPRINT ARXIV; Zhang H., 2017, ARXIV PREPRINT ARXIV; Zhang J, 2021, INT J COMPUT VISION, V129, P2639, DOI 10.1007/s11263-021-01482-8; Zhang J, 2021, IEEE INTERNET THINGS, V8, P7789, DOI 10.1109/JIOT.2020.3039359; Zhang P, 2021, PROC CVPR IEEE, P12409, DOI 10.1109/CVPR46437.2021.01223; Zhang QM, 2019, ADV NEUR IN, V32; Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283; Zhou Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2571, DOI 10.1145/3394171.3413565; Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314; Zhu YX, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107336; Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608	63	1	1	5	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					1961	1977		10.1007/s11263-022-01616-6	http://dx.doi.org/10.1007/s11263-022-01616-6		MAY 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP		Green Submitted			2022-12-18	WOS:000803816700001
J	Humenberger, M; Cabon, Y; Pion, N; Weinzaepfel, P; Lee, D; Guerin, N; Sattler, T; Csurka, G				Humenberger, Martin; Cabon, Yohann; Pion, Noe; Weinzaepfel, Philippe; Lee, Donghwan; Guerin, Nicolas; Sattler, Torsten; Csurka, Gabriela			Investigating the Role of Image Retrieval for Visual Localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual localization; Image retrieval; Benchmark; Landmark retrieval; Place recognition; Camera pose estimation	QUERY EXPANSION; RECOGNITION; FEATURES; MODEL	Visual localization, i.e., camera pose estimation in a known scene, is a core component of technologies such as autonomous driving and augmented reality. State-of-the-art localization approaches often rely on image retrieval techniques for one of two purposes: (1) provide an approximate pose estimate or (2) determine which parts of the scene are potentially visible in a given query image. It is common practice to use state-of-the-art image retrieval algorithms for both of them. These algorithms are often trained for the goal of retrieving the same landmark under a large range of viewpoint changes which often differs from the requirements of visual localization. In order to investigate the consequences for visual localization, this paper focuses on understanding the role of image retrieval for multiple visual localization paradigms. First, we introduce a novel benchmark setup and compare state-of-the-art retrieval representations on multiple datasets using localization performance as metric. Second, we investigate several definitions of "ground truth" for image retrieval. Using these definitions as upper bounds for the visual localization paradigms, we show that there is still significant room for improvement. Third, using these tools and in-depth analysis, we show that retrieval performance on classical landmark retrieval or place recognition tasks correlates only for some but not all paradigms to localization performance. Finally, we analyze the effects of blur and dynamic scenes in the images. We conclude that there is a need for retrieval approaches specifically designed for localization paradigms. Our benchmark and evaluation protocols are available at https://github.com/naver/kapture-localization.	[Humenberger, Martin; Cabon, Yohann; Pion, Noe; Weinzaepfel, Philippe; Guerin, Nicolas; Csurka, Gabriela] NAVER LABS Europe, Meylan, France; [Lee, Donghwan] NAVER LABS, Seongnam Si, South Korea; [Sattler, Torsten] Czech Tech Univ, Czech Inst Informat Robot & Cybernet, Prague, Czech Republic	Czech Technical University Prague	Humenberger, M (corresponding author), NAVER LABS Europe, Meylan, France.	martin.humenberger@naverlabs.com; yohann.cabon@naverlabs.com; philippe.weinzaepfel@naverlabs.com; donghwan.lee@naverlabs.com; nicolas.guerin@naverlabs.com; torsten.sattler@cvut.cz; gabriela.csurka@naverlabs.com	Sattler, Torsten/AAM-3155-2021	Humenberger, Martin/0000-0003-0600-9164	EU [857306]; European Regional Development Fund under IMPACT [CZ.02.1.01/0.0/0.0/15_003/0000468]	EU(European Commission); European Regional Development Fund under IMPACT	This work received funding through the EU Horizon 2020 research and innovation programme under Grant agreement No. 857306 (RICAIP) and the European Regional Development Fund under IMPACT No. CZ.02.1.01/0.0/0.0/15_003/0000468.	[Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; ARANDJELOVIC R, 2012, PROC CVPR IEEE, P2911, DOI DOI 10.1109/CVPR.2012.6248018; Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Arandjelovic R, 2015, LECT NOTES COMPUT SC, V9006, P188, DOI 10.1007/978-3-319-16817-3_13; Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207; Arth C, 2009, INT SYM MIX AUGMENT, P73, DOI 10.1109/ISMAR.2009.5336494; Avrithis Y., 2010, ACMMM; Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38; Balntas V, 2018, LECT NOTES COMPUT SC, V11218, P782, DOI 10.1007/978-3-030-01264-9_46; Bingyi Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P726, DOI 10.1007/978-3-030-58565-5_43; Brachmann E, 2019, IEEE I CONF COMP VIS, P7524, DOI 10.1109/ICCV.2019.00762; Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489; Brachmann Eric, 2021, ICCV; Brahmbhatt S, 2018, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2018.00277; Brejcha J, 2017, PATTERN ANAL APPL, V20, P613, DOI 10.1007/s10044-017-0611-1; Cao S, 2013, PROC CVPR IEEE, P700, DOI 10.1109/CVPR.2013.96; Castle R, 2008, TWELFTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P15, DOI 10.1109/ISWC.2008.4911577; Cavallari T, 2019, INT CONF 3D VISION, P564, DOI 10.1109/3DV.2019.00068; Cavallari T, 2020, IEEE T PATTERN ANAL, V42, P2465, DOI 10.1109/TPAMI.2019.2915068; Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Crandall David J, 2009, P INT C WORLD WID WE, DOI DOI 10.1145/1526709.1526812; Csurka G., 2004, ECCV WORKSHOPS; Csurka G., 2018, ARXIV180710254; Cui QD, 2017, INT CONF 3D VISION, P165, DOI 10.1109/3DV.2017.00028; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Ding MY, 2019, IEEE I CONF COMP VIS, P2871, DOI 10.1109/ICCV.2019.00296; Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Garcia-Fidalgo E, 2015, ROBOT AUTON SYST, V64, P1, DOI 10.1016/j.robot.2014.11.009; Germain H, 2019, INT CONF 3D VISION, P513, DOI 10.1109/3DV.2019.00063; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; Hausler S, 2021, PROC CVPR IEEE, P14136, DOI 10.1109/CVPR46437.2021.01392; Hays James, 2008, CVPR, DOI DOI 10.1109/CVPR.2008.4587784; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949; Heng L, 2019, IEEE INT CONF ROBOT, P4695, DOI 10.1109/ICRA.2019.8793949; Humenberger Martin, 2020, ARXIV200713867; Irschara Arnold, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2599, DOI 10.1109/CVPRW.2009.5206587; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48; Kalantidis Y, 2011, MULTIMED TOOLS APPL, V51, P555, DOI 10.1007/s11042-010-0651-7; Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Kim HJ, 2017, PROC CVPR IEEE, P3251, DOI 10.1109/CVPR.2017.346; Kneip L, 2011, PROC CVPR IEEE; Knopp J, 2010, LECT NOTES COMPUT SC, V6311, P748, DOI 10.1007/978-3-642-15549-9_54; Kukelova Z, 2013, IEEE I CONF COMP VIS, P2816, DOI 10.1109/ICCV.2013.350; Larsson V, 2017, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2017.254; Laskar Z, 2017, IEEE INT CONF COMP V, P920, DOI 10.1109/ICCVW.2017.113; Lebeda K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.95; Lee D, 2021, PROC CVPR IEEE, P3226, DOI 10.1109/CVPR46437.2021.00324; Li X., 2020, CVPR; Li YP, 2010, LECT NOTES COMPUT SC, V6312, P791; Lim H, 2015, INT J ROBOT RES, V34, P476, DOI 10.1177/0278364914561101; Liu L, 2019, IEEE I CONF COMP VIS, P2570, DOI 10.1109/ICCV.2019.00266; Liu RT, 2008, PROC CVPR IEEE, P954; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823; Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733; Lynen S, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498; Massiceti Daniela, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5118, DOI 10.1109/ICRA.2017.7989598; Middelberg S, 2014, LECT NOTES COMPUT SC, V8690, P268, DOI 10.1007/978-3-319-10605-2_18; Myers JL, 2003, RES DESIGN STAT ANAL; Vo N, 2017, IEEE I CONF COMP VIS, P2640, DOI 10.1109/ICCV.2017.286; Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374; Perronnin F, 2007, PROC CVPR IEEE, P2272; Philbin J, 2008, PROC CVPR IEEE, P2285; Piasco N, 2018, PATTERN RECOGN, V74, P90, DOI 10.1016/j.patcog.2017.09.013; Pion N, 2020, INT CONF 3D VISION, P483, DOI 10.1109/3DV50981.2020.00058; Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566; Radenovic F, 2018, PROC CVPR IEEE, P5706, DOI 10.1109/CVPR.2018.00598; Razavian AS, 2016, ITE T MEDIA TECHNOL, V4, P251, DOI DOI 10.3169/mta.4.251; Revaud J, 2019, IEEE I CONF COMP VIS, P5106, DOI 10.1109/ICCV.2019.00521; Revaud J, 2019, ADV NEUR IN, V32; Sarlin PE, 2021, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR46437.2021.00326; Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300; Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342; Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897; Sattler T, 2017, IEEE T PATTERN ANAL, V39, P1744, DOI 10.1109/TPAMI.2016.2611662; Sattler T, 2016, PROC CVPR IEEE, P1582, DOI 10.1109/CVPR.2016.175; Sattler T, 2015, IEEE I CONF COMP VIS, P2102, DOI 10.1109/ICCV.2015.243; Sattler T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.76; Schindler G, 2007, PROC CVPR IEEE, P1378; Schonberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736; Se S, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P226, DOI 10.1109/IRDS.2002.1041393; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Sun X, 2017, PROC CVPR IEEE, P5641, DOI 10.1109/CVPR.2017.598; Taira H., 2019, IEEE T PATTERN ANAL; Taira H, 2019, IEEE I CONF COMP VIS, P4372, DOI 10.1109/ICCV.2019.00447; Taira H, 2018, PROC CVPR IEEE, P7199, DOI 10.1109/CVPR.2018.00752; Tang ST, 2021, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR46437.2021.00187; Tolias G, 2014, PATTERN RECOGN, V47, P3466, DOI 10.1016/j.patcog.2014.04.007; Tolias Giorgos, 2016, P ICLR; Torii A, 2021, IEEE T PATTERN ANAL, V43, P814, DOI 10.1109/TPAMI.2019.2941876; Torii A, 2018, IEEE T PATTERN ANAL, V40, P257, DOI 10.1109/TPAMI.2017.2667665; Torii A, 2015, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2015.7298790; Torii A, 2015, IEEE T PATTERN ANAL, V37, P2346, DOI 10.1109/TPAMI.2015.2409868; Ventura J, 2014, IEEE T VIS COMPUT GR, V20, P531, DOI 10.1109/TVCG.2014.27; Walch F, 2017, IEEE I CONF COMP VIS, P627, DOI 10.1109/ICCV.2017.75; Weinzaepfel P, 2019, PROC CVPR IEEE, P5617, DOI 10.1109/CVPR.2019.00578; Weyand T, 2020, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR42600.2020.00265; Wijmans E, 2017, PROC CVPR IEEE, P1427, DOI 10.1109/CVPR.2017.156; Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150; Yang LW, 2019, IEEE I CONF COMP VIS, P42, DOI 10.1109/ICCV.2019.00013; Yibin Li, 2009, 2009 IEEE International Conference on Automation and Logistics (ICAL), P1957, DOI 10.1109/ICAL.2009.5262626; Zamir Amir R., 2016, ADV COMPUTER VISION, P1; Zamir AR, 2010, LECT NOTES COMPUT SC, V6314, P255, DOI 10.1007/978-3-642-15561-1_19; Zhang W, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P33, DOI 10.1109/3dpvt.2006.80; Zhang ZC, 2021, INT J COMPUT VISION, V129, P821, DOI 10.1007/s11263-020-01399-8; Zheng EL, 2015, IEEE I CONF COMP VIS, P2075, DOI 10.1109/ICCV.2015.240; Zheng L., 2016, ARXIV PREPRINT ARXIV; Zhou QJ, 2020, IEEE INT CONF ROBOT, P3319, DOI 10.1109/ICRA40945.2020.9196607	118	1	1	9	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1811	1836		10.1007/s11263-022-01615-7	http://dx.doi.org/10.1007/s11263-022-01615-7		MAY 2022	26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH		Green Submitted			2022-12-18	WOS:000802316600003
J	Yang, L; Jiang, H; Song, Q; Guo, J				Yang, Lu; Jiang, He; Song, Qing; Guo, Jun			A Survey on Long-Tailed Visual Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Long-tailed distribution; Visual recognition; Deep learning; Gini coefficient	SMOTE; MIXTURES; EXPERTS	The heavy reliance on data is one of the major reasons that currently limit the development of deep learning. Data quality directly dominates the effect of deep learning models, and the long-tailed distribution is one of the factors affecting data quality. The long-tailed phenomenon is prevalent due to the prevalence of power law in nature. In this case, the performance of deep learning models is often dominated by the head classes while the learning of the tail classes is severely underdeveloped. In order to learn adequately for all classes, many researchers have studied and preliminarily addressed the long-tailed problem. In this survey, we focus on the problems caused by long-tailed data distribution, sort out the representative long-tailed visual recognition datasets and summarize some mainstream long-tailed studies. Specifically, we summarize these studies into ten categories from the perspective of representation learning, and outline the highlights and limitations of each category. Besides, we have studied four quantitative metrics for evaluating the imbalance, and suggest using the Gini coefficient to evaluate the long-tailedness of a dataset. Based on the Gini coefficient, we quantitatively study 20 widely-used and large-scale visual datasets proposed in the last decade, and find that the long-tailed phenomenon is widespread and has not been fully studied. Finally, we provide several future directions for the development of long-tailed learning to provide more ideas for readers.	[Yang, Lu; Jiang, He; Song, Qing; Guo, Jun] Beijing Univ Posts & Telecommun, Beijing, Peoples R China	Beijing University of Posts & Telecommunications	Song, Q (corresponding author), Beijing Univ Posts & Telecommun, Beijing, Peoples R China.	soeaver@bupt.edu.cn; JiangHe@bupt.edu.cn; priv@bupt.edu.cn; guojun@bupt.edu.cn	yanng, lu/GLV-5144-2022		National Key Research and Development Program of China [2021YFF0 500900]	National Key Research and Development Program of China	This work was supported by the National Key Research and Development Program of China (Grant No. 2021YFF0 500900).	Abu-El-Haija S, 2016, YOUTUBE 8M LARGE SCA; Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30; An Xiang, 2020, ARXIV201005222; Anderson Chris, 2006, LONG TAIL WHY FUTURE; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; [Anonymous], 2019, LVIS CHALLENGE; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877; Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, 10.48550/arXiv.2005.14165]; Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011; Byrd J, 2019, PR MACH LEARN RES, V97; Cao KD, 2019, ADV NEUR IN, V32; Castrup H., 2001, P INT DIM WORKSH, P112; Chang N., 2020, ARXIV200807073; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284; Chen T, 2020, PR MACH LEARN RES, V119; Chen XC, 2020, IEEE IJCNN; Cheng Bowen, 2021, ARXIV210706278; Chu Peng, 2020, EUR C COMP VIS ECCV; Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359; Cui Jiequan, 2021, ARXIV210110633; Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949; Dave A., 2021, ARXIV210201066; DAVID HA, 1954, BIOMETRIKA, V41, P482, DOI 10.2307/2332728; Davidson L, 1999, UNCERTAINTY INT MONE, P3037; Delmas R., 2005, STAT ED RES J, V4, P55, DOI [10.52041/serj.v4i1.525, DOI 10.52041/SERJ.V4I1.525]; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Devi D, 2017, PATTERN RECOGN LETT, V93, P3, DOI 10.1016/j.patrec.2016.10.006; Devlin Jacob, 2019, P 2019 C N AM CHAPT, Patent No. [ArXiv181004805Cs, 181004805]; diaeresis>uhl Philipp Krahenb<spacing, 2021, ARXIV210307461; Dong Q, 2019, IEEE T PATTERN ANAL, V41, P1367, DOI 10.1109/TPAMI.2018.2832629; Dong Q, 2017, IEEE I CONF COMP VIS, P1869, DOI 10.1109/ICCV.2017.205; Dosovitskiy A., 2020, ARXIV201011929; Dvir S., 2021, ARXIV210403066; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan Q, 2020, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR42600.2020.00407; Fogarty A, 2000, CHEST, V117, P1656, DOI 10.1378/chest.117.6.1656; Ghosh M, 1996, J AM STAT ASSOC, V91, P1423, DOI 10.2307/2291568; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Gini C., 1912, VARIABILITA MUTABILI; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; Greene DN, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0240783; Gu Xiuye, 2021, ARXIV210413921; Gui S., 2019, ADV NEURAL INFORM PR; Guo HX, 2017, EXPERT SYST APPL, V73, P220, DOI 10.1016/j.eswa.2016.12.035; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; He HB, 2008, IEEE IJCNN, P1322, DOI 10.1109/IJCNN.2008.4633969; He K., 2016, P CVPR, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He Yin-Yin, 2021, ARXIV210315042; Hinton G., 2015, ARXIV150302531; Hu Xinting, 2020, CVPR, P3; Huang C, 2020, IEEE T PATTERN ANAL, V42, P2781, DOI 10.1109/TPAMI.2019.2914680; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Inaturalist, 2018, COMP DAT; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Jamal Muhammad Abdullah, 2020, CVPR, P7610, DOI DOI 10.1109/CVPR42600.2020.00763; Janowczyk Andrew, 2016, J Pathol Inform, V7, P29, DOI 10.4103/2153-3539.186902; Japkowicz N., 2002, Intelligent Data Analysis, V6, P429; Jiang H., 2020, P IEEE CVF C COMP VI, P10267; Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Junran Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9706, DOI 10.1109/CVPR42600.2020.00973; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; KAHN H, 1953, J OPER RES SOC AM, V1, P263, DOI 10.1287/opre.1.5.263; Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975; KAKWANI NC, 1977, ECONOMETRICA, V45, P719, DOI 10.2307/1911684; Kang Bingyi, 2020, INT C LEARN REPR; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kim D.J., 2020, P EUROPEAN C COMPUTE, P718736; Kim J., 2020, P IEEE C COMP VIS PA; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z; Lample Guillaume, 2018, P 2018 C EMP METH NA, P5039, DOI DOI 10.18653/V1/D18-1549; Lan Z., 2020, ARXIV; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Levi G, 2015, IEEE COMPUT SOC CONF; Li B, 2022, ARXIV220102593; Li BY, 2019, AAAI CONF ARTIF INTE, P8577; Li S, 2021, PROC CVPR IEEE, P5208, DOI 10.1109/CVPR46437.2021.00517; Li T., 2021, ARXIV210904075; Li Tianhong, 2021, ARXIV211113998; Li ZQ, 2019, PROC CVPR IEEE, P4516, DOI 10.1109/CVPR.2019.00465; Lin T.Y., 2019, 13 EUR C ZUR SWITZ S; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu B., 2021, P IEEE INT C COMP VI; Liu B., 2021, ARXIV210500127; Liu J., 2020, ARXIV200809809; Liu JL, 2020, PROC CVPR IEEE, P2967, DOI 10.1109/CVPR42600.2020.00304; Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3; Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264; Liuyu Xiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P247, DOI 10.1007/978-3-030-58558-7_15; Madry A., 2018, ARXIV PREPRINT ARXIV; Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12; Mani I., 2003, P WORKSHOP LEARNING, V126; Masoudnia S, 2014, ARTIF INTELL REV, V42, P275, DOI 10.1007/s10462-012-9338-y; Menon Aditya Krishna, 2021, INT C LEARN REPR; Miao JX, 2021, PROC CVPR IEEE, P4131, DOI 10.1109/CVPR46437.2021.00412; Mikolov T., 2013, ARXIV; MMSegmentation Contributors, 2020, MMSEGMENTATION OP SE; Narayanan A., 2018, ARXIV180700864; Oksuz K, 2021, IEEE T PATTERN ANAL, V43, P3388, DOI 10.1109/TPAMI.2020.2981890; Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100; Prabhu V., 2018, ARXIV181103066; Radford A, 2021, PR MACH LEARN RES, V139; Ren Jiawei, 2020, P NEUR INF PROC SYST, V2, P7; Ren MY, 2018, PR MACH LEARN RES, V80; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Riquelme Carlos, 2021, ARXIV210605974; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467; Shao S, 2019, IEEE I CONF COMP VIS, P8429, DOI 10.1109/ICCV.2019.00852; Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Shu J, 2019, ADV NEUR IN, V32; Shu Xiujun, 2021, ARXIV210515076; Sinha Saptarshi, 2020, P AS C COMP VIS; Sohn K, 2016, ADV NEUR IN, V29; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tang Kaihua, 2020, ADV NEURAL INFORM PR, V33, P8; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Tong Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P162, DOI 10.1007/978-3-030-58548-8_10; van den Oord A, 2017, ADV NEUR IN, V30; Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914; Van Horn Grant, 2017, ARXIV170901450; van Steenkiste S., 2019, ARXIV190601035; Wang C., 2021, ARXIV211104901; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang Haoran, 2021, ADV NEURAL INFORM PR; Wang JQ, 2021, PROC CVPR IEEE, P9690, DOI 10.1109/CVPR46437.2021.00957; Wang KJ, 2014, APPL SOFT COMPUT, V20, P15, DOI 10.1016/j.asoc.2013.09.014; Wang P, 2021, PROC CVPR IEEE, P943, DOI 10.1109/CVPR46437.2021.00100; Wang R., 2020, ARXIV200803428; Wang T., 2020, P EUROPEAN C COMPUTE; Wang TC, 2018, ADV NEUR IN, V31; Wang Y., 2021, ARXIV211207225; Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252; Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512; Wang YB, 2017, ADV NEUR IN, V30; Wei C, 2021, PROC CVPR IEEE, P10852, DOI 10.1109/CVPR46437.2021.01071; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Weyand T, 2020, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR42600.2020.00265; Wightman Ross, 2021, ARXIV211000476; WILSON DL, 1972, IEEE T SYST MAN CYB, VSMC2, P408, DOI 10.1109/TSMC.1972.4309137; Wu JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1570, DOI 10.1145/3394171.3413970; Wu T, 2021, PROC CVPR IEEE, P8655, DOI 10.1109/CVPR46437.2021.00855; Wu Y., 2019, DETECTRON2 FACEBOOK; Yang L, 2021, MULTIMED TOOLS APPL, V80, P855, DOI 10.1007/s11042-020-09604-z; Yang Yuzhe, 2020, NEURIPS; Yang ZW, 2021, ANIM BIOTECHNOL, V32, P67, DOI 10.1080/10495398.2019.1653901; Yitzhaki S, 1998, RES EC INEQ, V8, P13; Yu WP, 2021, IEEE WINT CONF APPL, P3257, DOI 10.1109/WACV48630.2021.00330; Zang Y., 2021, ARXIV210212867; Zhang Cheng, 2021, ARXIV210208884, P6; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang P., 2021, PROC CVPR IEEE, DOI DOI 10.1109/CVPR46437.2021.00553; Zhang S., 2021, ARXIV210410510; Zhang SY, 2021, PROC CVPR IEEE, P2361, DOI 10.1109/CVPR46437.2021.00239; Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578; Zhang Y., 2021, P AM ASS ARTIFICIAL; Zhang Yifan, 2021, ARXIV211004596; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509; Zhao Y., 2021, ARXIV210406094; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zhong YY, 2019, IEEE I CONF COMP VIS, P6548, DOI 10.1109/ICCV.2019.00665; Zhong ZS, 2021, PROC CVPR IEEE, P16484, DOI 10.1109/CVPR46437.2021.01622; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	187	1	1	17	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1837	1872		10.1007/s11263-022-01622-8	http://dx.doi.org/10.1007/s11263-022-01622-8		MAY 2022	36	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH		Green Submitted			2022-12-18	WOS:000802316600004
J	Chen, MQ; Quan, YH; Pang, TY; Ji, H				Chen, Mingqin; Quan, Yuhui; Pang, Tongyao; Ji, Hui			Nonblind Image Deconvolution via Leveraging Model Uncertainty in An Untrained Deep Neural Network	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deconvolution; Inverse problems; Unsupervised learning; Untrained neural networks		Nonblind image deconvolution (NID) is about restoring the latent image with sharp details from a noisy blurred one using a known blur kernel. This paper presents a dataset-free deep learning approach for NID using untrained deep neural networks (DNNs), which does not require any external training data with ground-truth images. Based on a spatially-adaptive dropout scheme, the proposed approach learns a DNN with model uncertainty from the input blurred image, and the deconvolution result is obtained by aggregating the multiple predictions from the learned dropout DNN. It is shown that the solution approximates a minimum-mean-squared-error estimator in Bayesian inference. In addition, a self-supervised loss function for training is presented to efficiently handle the noise in blurred images. Extensive experiments show that the proposed approach not only performs noticeably better than existing non-learning-based methods and unsupervised learning-based methods, but also performs competitively against recent supervised learning-based methods.	[Chen, Mingqin; Quan, Yuhui] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China; [Pang, Tongyao; Ji, Hui] Natl Univ Singapore, Dept Math, Singapore 119076, Singapore	South China University of Technology; National University of Singapore	Quan, YH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.	csyhquan@scut.edu.cn	JI, Hui/C-5107-2016	JI, Hui/0000-0002-1674-6056	National Natural Science Foundation of China [61872151]; CCF-Tencent Open Fund 2020; MOE [R146000315114]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CCF-Tencent Open Fund 2020; MOE(Ministry of Higher Education & Scientific Research (MHESR))	This work is supported in part by National Natural Science Foundation of China under Grant 61872151, in part by CCF-Tencent Open Fund 2020, and in part by MOE AcRF Tier 1 Research Grant R146000315114.	Anger J., 2019, INT S IM SIGN PROC A; Anger J, 2019, IMAGE PROCESS ON LIN, V9, P124, DOI 10.5201/ipol.2019.243; Anger J, 2018, IEEE IMAGE PROC, P978, DOI 10.1109/ICIP.2018.8451115; Arridge S, 2019, ACTA NUMER, V28, P1, DOI 10.1017/S0962492919000059; Azzari L, 2016, IEEE SIGNAL PROC LET, V23, P1086, DOI 10.1109/LSP.2016.2580600; Batson J, 2019, PR MACH LEARN RES, V97; Bigdeli SA, 2017, ADV NEUR IN, V30; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Chen GY, 2015, IEEE I CONF COMP VIS, P477, DOI 10.1109/ICCV.2015.62; Cho SJ., 2021, P IEEE CVF INT C COM; Danielyan A, 2012, IEEE T IMAGE PROCESS, V21, P1715, DOI 10.1109/TIP.2011.2176954; Dong J., 2020, ADV NEURAL INFORM PR, V33; Dong JX, 2018, LECT NOTES COMPUT SC, V11215, P777, DOI 10.1007/978-3-030-01252-6_46; Dong WS, 2019, IEEE T PATTERN ANAL, V41, P2305, DOI 10.1109/TPAMI.2018.2873610; Eboli T., 2020, P EUR C COMP VIS; Ehret T, 2019, PROC CVPR IEEE, P11361, DOI 10.1109/CVPR.2019.01163; Gal Y, 2016, PR MACH LEARN RES, V48; Gilton D, 2020, IEEE T COMPUT IMAG, V6, P328, DOI 10.1109/TCI.2019.2948732; Gong D, 2020, IEEE T NEUR NET LEAR, V31, P5468, DOI 10.1109/TNNLS.2020.2968289; Heckel, 2019, ARXIV PREPRINT ARXIV; Heckel R., 2019, P 7 INT C LEARN REPR; Hendriksen AA, 2020, IEEE T COMPUT IMAG, V6, P1320, DOI 10.1109/TCI.2020.3019647; Jin MG, 2017, PROC CVPR IEEE, P3834, DOI 10.1109/CVPR.2017.408; Kaufman A, 2020, PROC CVPR IEEE, P5810, DOI 10.1109/CVPR42600.2020.00585; Kohler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3; Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402; Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223; Kruse J, 2017, IEEE I CONF COMP VIS, P4596, DOI 10.1109/ICCV.2017.491; Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188; Laine S, 2019, ADV NEUR IN, V32; Lehtinen J, 2018, PR MACH LEARN RES, V80; Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308; Li J, 2015, INVERSE PROBL IMAG, V9, P875, DOI 10.3934/ipi.2015.9.875; Nan YS, 2020, PROC CVPR IEEE, P3623, DOI 10.1109/CVPR42600.2020.00368; Nan YS, 2020, PROC CVPR IEEE, P2385, DOI 10.1109/CVPR42600.2020.00246; Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180; Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371; Paszke A., 2017, PYTORCH DOCUMENT MAX; Quan YH, 2020, PROC CVPR IEEE, P1887, DOI 10.1109/CVPR42600.2020.00196; Ren DW, 2020, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR42600.2020.00340; Ren W., 2018, P INT C NEUR INF PRO; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Schmidt U, 2011, PROC CVPR IEEE; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142; Soltanayev S, 2018, ADV NEUR IN, V31; Son H, 2017, IEEE INT CONF COMPUT, P23; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sun LB, 2013, IEEE INT CONF COMPUT; Ulyanov D, 2020, INT J COMPUT VISION, V128, P1867, DOI 10.1007/s11263-020-01303-4; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Vasu S, 2018, PROC CVPR IEEE, P3272, DOI 10.1109/CVPR.2018.00345; Vonesch C, 2008, IEEE T IMAGE PROCESS, V17, P539, DOI 10.1109/TIP.2008.917103; Wang ZX, 2019, IEEE INT CONF COMP V, P980, DOI 10.1109/ICCVW.2019.00127; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xu L, 2014, INT CONF ACOUST SPEE; Yang LG, 2019, PROC CVPR IEEE, P10159, DOI 10.1109/CVPR.2019.01041; Zhang JW, 2017, PROC CVPR IEEE, P6969, DOI 10.1109/CVPR.2017.737; ZHANG K, 2017, PROC CVPR IEEE, P2808, DOI DOI 10.1109/CVPR.2017.300; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278; Zukerman J., 2020, P EUR C COMP VIS WOR	62	1	1	10	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1770	1789		10.1007/s11263-022-01621-9	http://dx.doi.org/10.1007/s11263-022-01621-9		MAY 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH					2022-12-18	WOS:000797258100001
J	Juefei-Xu, F; Wang, R; Huang, YH; Guo, Q; Ma, L; Liu, Y				Juefei-Xu, Felix; Wang, Run; Huang, Yihao; Guo, Qing; Ma, Lei; Liu, Yang			Countering Malicious DeepFakes: Survey, Battleground, and Horizon	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						DeepFake Generation; DeepFake Detection; Face; Misinformation; Disinformation; DeepFakes	MANIPULATION DETECTION; IMAGE MANIPULATION; BLOCKCHAIN	The creation or manipulation of facial appearance through deep generative approaches, known as DeepFake, have achieved significant progress and promoted a wide range of benign and malicious applications, e.g., visual effect assistance in movie and misinformation generation by faking famous persons. The evil side of this new technique poses another popular study, i.e., DeepFake detection aiming to identify the fake faces from the real ones. With the rapid development of the DeepFake-related studies in the community, both sides (i.e., DeepFake generation and detection) have formed the relationship of battleground, pushing the improvements of each other and inspiring new directions, e.g., the evasion of DeepFake detection. Nevertheless, the overview of such battleground and the new direction is unclear and neglected by recent surveys due to the rapid increase of related publications, limiting the in-depth understanding of the tendency and future works. To fill this gap, in this paper, we provide a comprehensive overview and detailed analysis of the research work on the topic of DeepFake generation, DeepFake detection as well as evasion of DeepFake detection, with more than 318 research papers carefully surveyed. We present the taxonomy of various DeepFake generation methods and the categorization of various DeepFake detection methods, and more importantly, we showcase the battleground between the two parties with detailed interactions between the adversaries (DeepFake generation) and the defenders (DeepFake detection). The battleground allows fresh perspective into the latest landscape of the DeepFake research and can provide valuable analysis towards the research challenges and opportunities as well as research trends and future directions. We also elaborately design interactive diagrams (http://www.xujuefei.com/dfsurvey) to allow researchers to explore their own interests on popular DeepFake generators or detectors.	[Juefei-Xu, Felix] Alibaba Grp, Sunnyvale, CA USA; [Wang, Run] Wuhan Univ, Sch Cyber Sci & Engn, Key Lab Aerosp Informat Secur & Trust Comp, Wuhan, Peoples R China; [Huang, Yihao] East China Normal Univ, Shanghai, Peoples R China; [Guo, Qing] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China; [Guo, Qing; Liu, Yang] Nanyang Technol Univ, Singapore, Singapore; [Ma, Lei] Univ Alberta, Alberta Machine Intelligence Inst AMII, Edmonton, AB, Canada; [Liu, Yang] Zhejiang Sci Tech Univ, Hangzhou, Peoples R China	Alibaba Group; Wuhan University; East China Normal University; Tianjin University; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University of Alberta; Zhejiang Sci-Tech University	Wang, R (corresponding author), Wuhan Univ, Sch Cyber Sci & Engn, Key Lab Aerosp Informat Secur & Trust Comp, Wuhan, Peoples R China.; Guo, Q (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.; Guo, Q (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	wangrun@whu.edu.cn; tsingqguo@ieee.org		Guo, Qing/0000-0003-0974-9299; Huang, Yihao/0000-0002-5784-770X	National Key Research and Development Program of China [2021YFB3100700]; Fellowship of China National Postdoctoral Program for Innovative Talents [BX2021229]; Natural Science Foundation of Hubei Province [2021CFB089]; Fundamental Research Funds for the Central Universities [2042021kf1030]; Open Foundation of Henan Key Laboratory of Cyberspace Situation Awareness [HNTS2022004]; National Natural Science Foundation of China (NSFC) [61876134]; National Research Foundation, Singapore under its the AI Singapore Programme [AISG2-RP-2020-019]; National Research Foundation, Prime Ministers Office, Singapore under its National Cybersecurity R D Program [NRF2018NCR-NCR005-0001]; NRF [NRFI06-2020-0001]; National Research Foundation through its National Satellite of Excellence in Trustworthy Software Systems (NSOE-TSS) project under the National Cybersecurity R D (NCR) [NRF2018NCR-NSOE003-0001]; NVIDIA AI Tech Center (NVAITC)	National Key Research and Development Program of China; Fellowship of China National Postdoctoral Program for Innovative Talents; Natural Science Foundation of Hubei Province(Natural Science Foundation of Hubei Province); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Open Foundation of Henan Key Laboratory of Cyberspace Situation Awareness; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); National Research Foundation, Singapore under its the AI Singapore Programme; National Research Foundation, Prime Ministers Office, Singapore under its National Cybersecurity R D Program(National Research Foundation, Singapore); NRF; National Research Foundation through its National Satellite of Excellence in Trustworthy Software Systems (NSOE-TSS) project under the National Cybersecurity R D (NCR); NVIDIA AI Tech Center (NVAITC)	This research was partly supported by the National Key Research and Development Program of China Under Grant No. 2021YFB3100700, the Fellowship of China National Postdoctoral Program for Innovative Talents Under No. BX2021229, the Natural Science Foundation of Hubei Province Under No. 2021CFB089, the Fundamental Research Funds for the Central Universities Under No. 2042021kf1030, the Open Foundation of Henan Key Laboratory of Cyberspace Situation Awareness under No. HNTS2022004, the National Natural Science Foundation of China (NSFC) under No. 61876134. The work was also supported by the National Research Foundation, Singapore under its the AI Singapore Programme (AISG2-RP-2020-019), the National Research Foundation, Prime Ministers Office, Singapore under its National Cybersecurity R &D Program (No. NRF2018NCR-NCR005-0001), NRF Investigatorship NRFI06-2020-0001, the National Research Foundation through its National Satellite of Excellence in Trustworthy Software Systems (NSOE-TSS) project under the National Cybersecurity R &D (NCR) Grant (No. NRF2018NCR-NSOE003-0001). We gratefully acknowledge the support of NVIDIA AI Tech Center (NVAITC) to our research.	115th Congress, 2018, S 3805MALICIOUS DEE; 116th Congress, S 2065DEEPFAKE REPOR; 116th Congress, 2019, H R 3230DEFENDING EA; Abiantun R, 2019, PATTERN RECOGN, V90, P308, DOI 10.1016/j.patcog.2019.01.032; Adobe, AD LIGHTR; Adobe, AD AUD; Adobe, 2021, AD PHOT; Afchar D., 2018, 2018 IEEE INT WORKSH, P17; Afifi M., 2021, CVPR 2021; Agarwal S., 2020, 2020 IEEE INT WORKSH, P16; Agarwal S, 2021, IEEE CVF C COMP VIS; Agarwal S., 2019, CVPR WORKSH, P3845; Agarwal S., 2020, P IEEE CVF C COMP VI; Amerini I., 2019, P IEEE INT C COMPUTE; Aneja S., 2021, ARXIV PREPRINT ARXIV; Antares Audio Technologies, 2021, AUT TUN; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bai Y, 2020, IEEE IMAGE PROC, P1256, DOI 10.1109/ICIP40778.2020.9190892; Barni M., 2020, 2020 IEEE INT WORKSH, P16; Beijing Academy of Artificial Intelligence, 2021, WU DAO 2 0; Bellemare MG, 2017, PR MACH LEARN RES, V70; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Bikowski M, 2018, ARXIV PREPRINT ARXIV; Bilibili, 2010, CHIN VID SHAR WEBS; Bommasani Rishi, 2021, OPPORTUNITIES RISKS; Bondi L, 2020, IEEE INT WORKS INFOR, DOI 10.1109/WIFS49906.2020.9360901; Bonettini N., 2020 25 INT C PATTER; Bonettini N, 2021, INT C PATT RECOG, P5012, DOI 10.1109/ICPR48806.2021.9412711; Bonomi M., 2020, ARXIV PREPRINT ARXIV; Brock A., 2018, ICLR, P1; Brown Tom B, 2017, ARXIV171209665; BuzzFeed, 2018, SPOT DEEPF BAR OB JO; California, 2019, CALIFORNIA ASSEMBLY; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Carlini N., 2020, P IEEE CVF C COMP VI; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chai L., 2020, EUR C COMP VIS; Chandrasegaran K., 2021, CLOSER LOOK FOURIER; Chen C., 2018, P IEEE C COMPUTER VI; Chen HN, 2020, IEEE T INF FOREN SEC, V15, P578, DOI 10.1109/TIFS.2019.2922241; Chen M, 2020, PR MACH LEARN RES, V119; Chen Q., 2017, P IEEE INT C COMPUTE; Chen R. T., 2018, ADV NEURAL INFORM PR; Chen XY, 2019, IEEE T IMAGE PROCESS, V28, P546, DOI 10.1109/TIP.2018.2869695; Chen Y. C., 2019, P IEEE C COMP VIS PA; Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353; Chen Z, 2021, IEEE INT C AC SPEECH; Chen ZP, 2019, IEEE T INF FOREN SEC, V14, P2454, DOI 10.1109/TIFS.2019.2901826; Cheng Y., 2020, ARXIV PREPRINT ARXIV; Cheng Y., ARXIV PREPRINT ARXIV; Cho W., 2019, P IEEE C COMPUTER VI; Choi Y., 2020, P IEEE CVF C COMP VI; Choi Y, 2018, P IEEE C COMPUTER VI; Chollet F, 2017, P IEEE C COMPUTER VI; Chugh K., 2020, P 28 ACM INT C MULT; Ciftci U.A., 2020, ARXIV PREPRINT ARXIV; Ciftci Umur Aybars, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3009287; CNN, 2020, CNN; Cozzolino D., 2020, ARXIV PREPRINT ARXIV; Dai T., 2019, P IEEE C COMPUTER VI; Dang H, 2020, PROC CVPR IEEE, P5780, DOI 10.1109/CVPR42600.2020.00582; Dang LM, 2019, EXPERT SYST APPL, V129, P156, DOI 10.1016/j.eswa.2019.04.005; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding Haibo, 2018, P AAAI C ARTIFICIAL, V32; Ding XY, 2020, EURASIP J INF SECUR, V2020, DOI 10.1186/s13635-020-00109-8; Dogonadze N., 2020, ARXIV PREPRINT ARXIV; Dolhansky B., 2019, ARXIV PREPRINT ARXIV; Dolhansky Brian, 2020, ARXIV200607397; Dong XT, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3023706; Dosovitskiy A., 2020, ARXIV201011929; Du Mengnan, 2019, ARXIV190905999; Dufour Nick, 2019, GOOGLE AI BLOG; Durall R, 2019, ARXIV PREPRINT ARXIV; Durall R., 2020, P IEEE CVF C COMP VI; Ekman P., 1978, FACIAL ACTION CODING; Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268; FaceApp, 2021, FACEAPP; Facebrity, 2021, FAC APPL APP STOR; FaceSwap, 2016, FACESWAP; Feng D., 2020, ICONIP; Fernandes S., 2019, P IEEE INT C COMPUTE; Fernando T., 2019, ARXIV PREPRINT ARXIV; Fraga-Lamas P, 2020, IT PROF, V22, P53, DOI 10.1109/MITP.2020.2977589; Frank J, 2020, PR MACH LEARN RES, V119; Fu L., 2021, P IEEE CVF C COMP VI; Fu L., 2021, ARXIV PREPRINT ARXIV; Gandhi A., 2020, 2020 INT JOINT C NEU, P18; Ganiyusufoglu Ipek, 2020, ARXIV PREPRINT ARXIV; Gao G., 2021, P IEEE CVF C COMP VI; Gao R., ARXIV PREPRINT ARXIV; Gao R., 2020, ARXIV PREPRINT ARXIV; Gao R., 2021, ARXIV PREPRINT ARXIV; Gao Y, 2021, PROC CVPR IEEE, P16110, DOI 10.1109/CVPR46437.2021.01585; George, 2020, DEEPFAKE DETECTION U; Goebel M., 2020, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu S., 2019, P IEEECVF C COMPUTER; Guan'an Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P275, DOI 10.1007/978-3-030-58598-3_17; Guarnera L., 2020, P IEEE CVF C COMP VI; Guarnera L., 2020, 2020 AEIT INT ANN C, P16; Guera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127; Gulrajani I., 2017, INT C NEURAL INF PRO; Guo Q., 2021, P IEEE INT C COMP VI; Guo Q., 2020, ADV NEURAL INFORM PR, P975; Guo Y., 2016, EUROPEAN C COMPUTER, P87102; Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545; Guo Z, 2020, ARXIV PREPRINT ARXIV; Gupta P., 2020, P 2020 INT C MULT IN; Ha S, 2020, AAAI CONF ARTIF INTE, V34, P10893; Haliassos A., 2021, P IEEE CVF C COMP VI; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He PS, 2019, IEEE IMAGE PROC, P2299, DOI 10.1109/ICIP.2019.8803740; He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751; Hernandez-Ortega J., 2020, P 35 AAAI C ART INT; Hsu CC, 2018, INT SYMP COMP CONS, P388, DOI 10.1109/IS3C.2018.00104; Hu S, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2500, DOI 10.1109/ICASSP39728.2021.9414582; Huang Hsin-Yuan, 2018, ARXIV181006683; Huang Y., 2021, ARXIV PREPRINT ARXIV; Huang Y., 2022, IEEE T INF FOREN SEC; Huang Y., 2020, P 28 ACM INT C MULT; Hulzebosch N., 2020, P IEEE CVF C COMP VI; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; IMDb, 2021, AVATAR; Jain A.K., 2007, HDB BIOMETRICS HDB B; Jeon H., 2020, IFIP INT C ICT SYSTE, P416; Jeon H., 2020, IFIP INT C ICT SYST; Jia C, 2021, PR MACH LEARN RES, V139; Jiang L., ARXIV PREPRINT ARXIV; Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492; Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296; Jiang Y., 2021, 35 C NEUR INF PROC S; Jo Y., 2019, P IEEE INT C COMPUTE; Juefei-Xu F., 2015, P IEEE 7 INT C BIOM, P18; Juefei-Xu F., 2016, P IEEE 7 INT C BIOM, P18; Juefei-Xu F, 2015, IEEE T IMAGE PROCESS, V24, P4780, DOI 10.1109/TIP.2015.2468173; Jung S., 2020, ARXIV PREPRINT ARXIV; Karnewar A., 2020, P IEEE CVF C COMP VI; Karras T., 2017, PROGR GROWING GANS I; Karras T., 2020, P IEEE CVF C COMP VI; Karras T., 2019, P IEEE C COMPUTER VI; Kawa P., 2020, ARXIV PREPRINT ARXIV; Kemelmacher-Shlizerman I., 2016, P IEEECVF C COMPUTER; Khalid H., 2020, P IEEE CVF C COMP VI; Khan Salman, 2021, ARXIV210101169; Kim J., 2021, P IEEE CVF C COMP VI; Kim M., 2021, P IEEECVF C COMPUTER; Kingma D. P., 2018, ADV NEURAL INFORM PR; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056; Koopman R. A. M., 2018, 20 IR MACH VIS IM PR, P133; Korshunov P., 2018, ARXIV PREPRINT ARXIV; Kukanov I, 2020, ASIAPAC SIGN INFO PR, P1300; Kumar Prabhat, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2578, DOI 10.1109/WACV45572.2020.9093628; Kwon P., 2021, ARXIV PREPRINT ARXIV; Le T. H., 2021, ARXIV PREPRINT ARXIV; Li HD, 2020, SIGNAL PROCESS, V174, DOI 10.1016/j.sigpro.2020.107616; Li HD, 2018, ASIAPAC SIGN INFO PR, P722, DOI 10.23919/APSIPA.2018.8659461; Li J., 2021, P IEEE CVF C COMP VI; Li JM, 2021, PROC CVPR IEEE, P6454, DOI 10.1109/CVPR46437.2021.00639; Li K, 2019, IEEE I CONF COMP VIS, P4219, DOI 10.1109/ICCV.2019.00432; Li L., P IEEECVF C COMPUTER; Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512; Li T., 2019, P IEEECVF C COMPUTER; Li X., 2020, MM 20; Li Y., 2019, 2017 IEEE C COMP VIS; Li YB, 2018, USNC-URSI RADIO SCI, P17, DOI 10.1109/USNC-URSI.2018.8602856; Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327; Lin CH, 2019, IEEE I CONF COMP VIS, P4511, DOI 10.1109/ICCV.2019.00461; Lin T. Y., 2014, EUROPEAN C COMPUTER; Liu BC, 2021, AAAI CONF ARTIF INTE, V35, P2073; Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083; Liu J., P IEEECVF C COMPUTER; Liu M, 2019, P IEEE C COMPUTER VI; Liu Z, 2015, P IEEE INT C COMPUTE; Liu Ziwei, 2020, CVPR; Trinh L, 2021, IEEE WINT CONF APPL, P1972, DOI 10.1109/WACV48630.2021.00202; Lu S. A, 2018, FACESWAP GAN; Luo YC, 2021, PROC CVPR IEEE, P16312, DOI 10.1109/CVPR46437.2021.01605; Lyu S., 2020, 2020 IEEE INT C MULT, P16; Mansourifar H., 2020, ARXIV PREPRINT ARXIV; Mao X., 2016, ARXIV PREPRINT ARXIV, V5; Marra F., 2018, 2018 IEEE C MULTIMED; Marra F, 2019, IEEE INT WORKS INFOR; Marra F, 2020, IEEE ACCESS, V8, P133488, DOI 10.1109/ACCESS.2020.3009877; Marra F, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P506, DOI 10.1109/MIPR.2019.00103; Masi I., 2020, EUR C COMP VIS; MasMontserrat D., 2020, P IEEE CVF C COMP VI; Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020; Maximov M, 2020, PROC CVPR IEEE, P5446, DOI 10.1109/CVPR42600.2020.00549; McCloskey S., 2019, 2019 IEEE INT C IMAG; Mei Y., 2020, P IEEECVF C COMPUTER; Mirsky Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3425780; Mirza M., 2014, ARXIV; MIT T. R, 2020, DEEPFAKE PUTIN IS HE; Mittal T., 2020, ARXIV PREPRINT ARXIV; Mo H., 2018, P 6 ACM WORKSHOP INF, P4347; Nataraj L., 2019, ELECT IMAGING, V5, P532, DOI DOI 10.2352/ISSN.2470-1173.2019.5.MWSF-532; Natsume R., 2018, ACM SIGGRAPH 2018 PO, P12; Neekhara P, 2021, P IEEECVF C COMPUTER; Neves JC, 2020, IEEE J-STSP, V14, P1038, DOI 10.1109/JSTSP.2020.3007250; Nguyen H. H., 2019, P 10 INT C BIOM THEO; Nguyen H. H., 2019, ICASSP 20192019 IEEE; Nguyen HH., 2019, ARXIV PREPRINT ARXIV; Nguyen TT., 2019, ARXIV PREPRINT ARXIV; Nhu T, 2018, 2018 INT S INF TECHN; Nirkin Y., 2019, P IEEECVF INT C COMP; Nirkin Y., 2020, ARXIV200812262; Noroozi M, 2020, ARXIV PREPRINT ARXIV; NPR, 2020, NPR; Oord A.V.D., 2016, SSW; OpenAI, 2021, DALL E CREAT IM TEXT; Osakabe T, 2021, PROC SPIE, V11766, DOI 10.1117/12.2590977; OValery, 2017, SWAP FAC; Pang T., 2018, P 32 INT C NEUR INF; Park T., 2019, P IEEE C COMPUTER VI; Parkhi Omkar M., 2015, BRIT MACH VIS C; Perarnau G, 2016, ARXIV161106355; Petrov Ivan, 2020, ARXIV200505535; Pinscreen, 2021, PINSCR AI DRIV VIRT; Pinscreen, 2021, AI AV VIRT ASS DEEP; Pishori A., 2020, ARXIV PREPRINT ARXIV; Pu JM, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P981, DOI 10.1145/3442381.3449978; Pu JM, 2020, ANN COMPUT SECURITY, P913, DOI 10.1145/3427228.3427285; Pumarola A, 2018, P EUROPEAN C COMPUTE; Qi H, 2020, P 28 ACM INT C MULTI; Qian Y., 2020, EUR C COMP VIS, P86103; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Radford Alec, 2021, ARXIV210300020; Razavi A, 2019, ADV NEUR IN, V32; Reface, 2021, REFACE; Rezende D., 2015, INT C MACHINE LEARNI; Rossler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009; Rossler A., 2018, ARXIV PREPRINT ARXIV; Rssler A, 2018, ARXIV PREPRINT ARXIV; Sabir E, 2019, INTERFACES GUI, V3, P1; Sambhu N., 2020, ARXIV PREPRINT ARXIV; Schwarcz S, 2021, IEEE COMPUT SOC CONF, P933, DOI 10.1109/CVPRW53098.2021.00104; Shu Z., 2017, P IEEE C COMPUTER VI; Sohrawardi SJ, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P2613, DOI 10.1145/3319535.3363269; Songsri-in K., 2019, ARXIV PREPRINT ARXIV; Sun Ke, 2019, ARXIV190211038; Sun L., 2022, ARXIV PREPRINT ARXIV; Sun QR, 2018, LECT NOTES COMPUT SC, V11205, P570, DOI 10.1007/978-3-030-01246-5_34; Sun X., 2020, ARXIV PREPRINT ARXIV; Sun Y., 2013, P IEEE INT C COMPUTE; Sun Yu, 2020, ICML; Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640; Synthesia, 2021, SYNTH SOFTW; Tan M., 2019, INT C MACHINE LEARNI; Tarasiou M, 2020, IEEE IMAGE PROC, P1821, DOI 10.1109/ICIP40778.2020.9190714; Tariq S., 2018, P 2 INT WORKSHOP MUL, P8187; Tariq Shahroz, 2020, ARXIV PREPRINT ARXIV; Texas, 2019, TEXAS SENATE BILL NO; The Verge, 2019, THE VERGE; The Verge,, THE VERGE; Thies J., 2016, P IEEE C COMPUTER VI; Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035; Tian B., 2021, 2021 IEEE INT C MULT, P16; Tripathy S., 2021, P IEEE CVF WINT C AP; Tripathy S., 2020, P IEEECVF WINTER C A; Tursman E., 2020, P IEEE CVF C COMP VI; Twitter Blog, 2019, HELP US SHAPE OUR AP; Ulyanov D., 2018, P IEEE C COMPUTER VI; VanOord A., 2016, INT C MACHINE LEARNI; Viazovetskyi Y., 2020, EUR C COMP VIS; Wang CR, 2021, PROC CVPR IEEE, P14918, DOI 10.1109/CVPR46437.2021.01468; Wang R, 2020, P 28 ACM INT C MULT; Wang R., INT JOINT C ARTIFICI; Wang R, 2020, J CHEM INF MODEL, V60, P5853, DOI 10.1021/acs.jcim.0c00501; Wang Suchen, 2020, CVPR; Wang TC, 2021, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR46437.2021.00991; Wang X., 2020, ICONIP; Wikipedia, 2021, SANK DIAGR; Wikipedia, 2021, EL RAT SYST; Woods W, 2019, NAT MACH INTELL, V1, P508, DOI 10.1038/s42256-019-0104-6; Wu X., 2020, ICASSP 20202020 IEEE; Wu Y, 2019, P IEEE C COMPUTER VI; Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229; Xuan XS, 2019, LECT NOTES COMPUT SC, V11818, P134, DOI 10.1007/978-3-030-31456-9_15; Yang X., 2019, ICASSP 20192019 IEEE; Yang X, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P113, DOI 10.1145/3335203.3335724; Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yu C. M., ARXIV PREPRINT ARXIV; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Yu J, 2018, P IEEE C COMPUTER VI; Yu N., 2019, P IEEE INT C COMPUTE; Yu N., 2020, ARXIV PREPRINT ARXIV; Yu Youngjoon, 2020, ARXIV PREPRINT ARXIV; Yuan Li, 2021, ARXIV210613112; Zao, 2021, ZAO APPL APP STOR; Zhai L., 2022, ARXIV PREPRINT ARXIV; Zhai L., 2020, ARXIV200909205; Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26; Zhang W., 2021, P IEEE CVF C COMP VI; Zhang XY, 2019, PROCEEDINGS OF 2019 IEEE 3RD INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC 2019), P16, DOI 10.1109/ITNEC.2019.8728993; Zhang Y, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P15; Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222; Zhao T., 2020, ARXIV200606830; Zheng Z., 2018, P 32 INT C NEUR INF; Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416; Zhou P, 2018, P IEEE C COMPUTER VI; Zhou P, 2017, IEEE COMPUT SOC CONF, P1831, DOI 10.1109/CVPRW.2017.229; Zhou T., 2021, P IEEE CVF C COMP VI; Zhu H., 2020, ADV NEURAL INFORM PR, P21699; Zhu J., 2022, ARXIV PREPRINT ARXIV; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu X., 2021, P IEEE CVF C COMP VI; Zi B., 2020, P 28 ACM INT C MULT	317	1	1	24	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1678	1734		10.1007/s11263-022-01606-8	http://dx.doi.org/10.1007/s11263-022-01606-8		MAY 2022	57	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH	35528632	Bronze, Green Submitted, Green Published			2022-12-18	WOS:000790628100002
J	Labussiere, M; Teuliere, C; Bernardin, F; Ait-Aider, O				Labussiere, Mathieu; Teuliere, Celine; Bernardin, Frederic; Ait-Aider, Omar			Leveraging Blur Information for Plenoptic Camera Calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Plenoptic camera; Calibration; Multi-focus; Relative blur; Blur circle		This paper presents a novel calibration algorithm for plenoptic cameras, especially the multi-focus configuration, where several types of micro-lenses are used, using raw images only. Current calibration methods rely on simplified projection models, use features from reconstructed images, or require separated calibrations for each type of micro-lens. In the multi-focus configuration, the same part of a scene will demonstrate different amounts of blur according to the micro-lens focal length. Usually, only micro-images with the smallest amount of blur are used. In order to exploit all available data, we propose to explicitly model the defocus blur in a new camera model with the help of our newly introduced Blur Aware Plenoptic (BAP) feature. First, it is used in a pre-calibration step that retrieves initial camera parameters, and second, to express a new cost function to be minimized in our single optimization process. Third, it is exploited to calibrate the relative blur between micro-images. It links the geometric blur, i.e., the blur circle, to the physical blur, i.e., the point spread function. Finally, we use the resulting blur profile to characterize the camera's depth of field. Quantitative evaluations in controlled environment on real-world data demonstrate the effectiveness of our calibrations.	[Labussiere, Mathieu; Teuliere, Celine; Ait-Aider, Omar] Univ Clermont Auvergne, Inst Pascal, CNRS, Clermont Auvergne INP, F-63000 Clermont Ferrand, France; [Bernardin, Frederic] Cerema, Equipe Projet STI, 10 Rue Bernard Palissy, F-63017 Clermont Ferrand, France	Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA)	Labussiere, M (corresponding author), Univ Clermont Auvergne, Inst Pascal, CNRS, Clermont Auvergne INP, F-63000 Clermont Ferrand, France.	mathieu.labu@gmail.com; celine.teuliere@uca.fr; frederic.bernardin@cerema.fr; omar.ait-aider@uca.fr		Bernardin, Frederic/0000-0002-1248-153X; Labussiere, Mathieu/0000-0001-8105-4139	AURA Region; European Union (FEDER) through the MMII project of CPER 2015-2020 MMaSyF challenge	AURA Region; European Union (FEDER) through the MMII project of CPER 2015-2020 MMaSyF challenge	This work was supported by the AURA Region and the European Union (FEDER) through the MMII project of CPER 2015-2020 MMaSyF challenge.	Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168; Bok Y., 2014, COMPUTER VISIONECCV, P4761; Bok Y., 2017, IEEE T PATTERN ANAL; Brown D., 1966, PHOTOMETRIC ENG, V32; Chen C. H., 2015, 2015 INT P IEEE INT; Conrady A., 1919, MONTHLY NOTICES ROYA, V79; Dansereau D. G., 2013, P IEEE COMPUTER SOC; ENS J, 1993, IEEE T PATTERN ANAL, V15, P97, DOI 10.1109/34.192482; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Georgiev T., 2009, CTUB3 RESOLUTION PLE; Georgiev T., 2006, RENDERING TECHNIQUES, V21; Georgiev T., 2012, DIGITAL PHOTOGRAPHY, P6979; Georgiev T., 2009, EUROGRAPHICS, P58; Grossberg MD, 2005, INT J COMPUT VISION, V61, P119, DOI 10.1023/B:VISI.0000043754.56350.10; Hahne C, 2018, INT J COMPUT VISION, V126, P21, DOI 10.1007/s11263-017-1036-4; Heinze C, 2016, IEEE T INSTRUM MEAS, V65, P1197, DOI 10.1109/TIM.2015.2507412; Johannsen O., 2017, 2017 IEEE COMPUTER S; Johannsen O., 2013, LNCS LECT NOTES COMP; Kneip L, 2011, PROC CVPR IEEE; Labussire M., 2020, IEEE 2020 IEEE CVF C; Levin A, 2008, LECT NOTES COMPUT SC, V5305, P88, DOI 10.1007/978-3-540-88693-8_7; Levoy M., 1996, P 23 ANN C COMP GRAP, P3142; Lippmann Gabriel, 1911, INTEGRAL PHOTOGRAPHY; Lumsdaine Andrew, 2009, IEEE INT C COMPUTATI, P18; Mannan F., 2016, 13 C COMPUTER ROBOT; Mannan F, 2016, 2016 13TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P273, DOI 10.1109/CRV.2016.61; Mignard-Debise L, 2017, IEEE T COMPUT IMAG, V3, P798, DOI 10.1109/TCI.2017.2699427; Ng R., 2005, LIGHT FIELD PHOTOGRA, P111; Noury C. A., 2017, 2017 INT C DIG IM CO, P18; Noury C.-A., 2019, ETALONNAGE CAMRA PLN; Nousias S., 2017, P IEEE INT C COMPUTE; O'Brien SGP, 2018, INT CONF 3D VISION, P286, DOI 10.1109/3DV.2018.00041; Palmieri L., 2017, 2017 IEEE COMPUTER S; Pentland Alex P., 1987, IEEE T PATTERN ANAL, V4; Perwa C., 2012, SINGLE LENS 3D CAMER, V49; Shi S., 2016, FLOW MEASUREMENT INS; Strobl KH, 2016, COMPUT VIS IMAGE UND, V145, P140, DOI 10.1016/j.cviu.2015.12.010; Subbarao M., 1989, IMAGES SIMPLE OBJECT; Subbarao M., 1988, CAMERA PARAMETERS; Subbarao M., 1994, INT J COMPUT VISION, V3; Suliga P., 2018, P 19 INT CARP CONTR, P1922; Thomason C. M., 2014, 52 AER SCI M, P118; Wang Y., 2018, IEEE ACCESS, V6; Zeller N., 2016, IEEE 2016 12 IEEE IN; Zeller N., 2016, REMOTE SENSING SPATI; Zeller N., 2014, 2 3 ISPRS ANN PHOTOG; Zeller N, 2017, IEEE J-STSP, V11, P1004, DOI 10.1109/JSTSP.2017.2737965; Zhang C., 2016, CAMERAS CALIBRATION, P120; Zhang Q., 2018, IEEE T PATTERN ANAL; Zhao Y., 2020, OPT ENG, V1; Zhou P, 2019, OPT LASER ENG, V115, P190, DOI 10.1016/j.optlaseng.2018.11.024	51	1	1	3	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1655	1677		10.1007/s11263-022-01582-z	http://dx.doi.org/10.1007/s11263-022-01582-z		MAY 2022	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH		hybrid, Green Published, Green Submitted			2022-12-18	WOS:000790628100001
J	Wu, XT; Feng, XY; Cao, XC; Xu, X; Hu, DW; Lopez, MB; Liu, L				Wu, Xiaoting; Feng, Xiaoyi; Cao, Xiaochun; Xu, Xin; Hu, Dewen; Lopez, Miguel Bordallo; Liu, Li			Facial Kinship Verification: A Comprehensive Review and Outlook	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Review						Kinship verification; Facial analysis; Metric learning; Deep learning; Feature extraction	KIN RECOGNITION SIGNALS; FACE RECOGNITION; FUNDAMENTAL-FREQUENCY; DISCRIMINANT-ANALYSIS; SIMILARITY; IMAGES; CLASSIFICATION; PATTERNS; FEATURES	The goal of Facial Kinship Verification (FKV) is to automatically determine whether two individuals have a kin relationship or not from their given facial images or videos. It is an emerging and challenging problem that has attracted increasing attention due to its practical applications. Over the past decade, significant progress has been achieved in this new field. Handcrafted features and deep learning techniques have been widely studied in FKV. The goal of this paper is to conduct a comprehensive review of the problem of FKV. We cover different aspects of the research, including problem definition, challenges, applications, benchmark datasets, a taxonomy of existing methods, and state-of-the-art performance. In retrospect of what has been achieved so far, we identify gaps in current research and discuss potential future research directions.	[Wu, Xiaoting; Lopez, Miguel Bordallo; Liu, Li] Univ Oulu, Oulu, Finland; [Xu, Xin; Hu, Dewen; Liu, Li] Natl Univ Def Technol, Changsha, Peoples R China; [Wu, Xiaoting; Feng, Xiaoyi] Northwestern Polytech Univ, Xian, Peoples R China; [Lopez, Miguel Bordallo] VTT Tech Res Ctr Finland, Oulu, Finland; [Cao, Xiaochun] Sun Yat Sen Univ, Guangzhou, Peoples R China	University of Oulu; National University of Defense Technology - China; Northwestern Polytechnical University; VTT Technical Research Center Finland; Sun Yat Sen University	Liu, L (corresponding author), Univ Oulu, Oulu, Finland.; Liu, L (corresponding author), Natl Univ Def Technol, Changsha, Peoples R China.	xiaoting.wu@oulu.fi; fengziao@nwpu.edu.cn; caoxch5@sysu.edu.cn; xinzu@nudt.edu.cn; dwhu@nudt.edu.cn; tniguel.bordallo@oulu.fi; li.liu@oulu.fi		Liu, li/0000-0002-2011-2873	University of Oulu including Oulu University Hospital	University of Oulu including Oulu University Hospital	Open Access funding provided by University of Oulu including Oulu University Hospital.	Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Alirezazadeh P, 2015, IEEE SIGNAL PROC LET, V22, P2459, DOI 10.1109/LSP.2015.2490805; Almuashi M, 2017, MULTIMED TOOLS APPL, V76, P265, DOI 10.1007/s11042-015-3007-5; Alvergne A, 2007, EVOL HUM BEHAV, V28, P135, DOI 10.1016/j.evolhumbehav.2006.08.008; Alvergne A, 2014, BIOL LETTERS, V10, DOI 10.1098/rsbl.2014.0063; Amini A, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P289, DOI 10.1145/3306618.3314243; Arya S, 2015, PROCEDIA COMPUT SCI, V58, P578, DOI 10.1016/j.procs.2015.08.076; Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; Bekhouche SE, 2020, IEEE IMAGE PROC, P2950, DOI 10.1109/ICIP40778.2020.9190787; Bessaoudi M, 2019, NEUROCOMPUTING, V329, P267, DOI 10.1016/j.neucom.2018.09.051; Booth J, 2018, IEEE T PATTERN ANAL, V40, P2638, DOI 10.1109/TPAMI.2018.2832138; Bottinok A, 2015, IEEE INT CONF AUTOMA; Boutellaa E., 2017, ARXIV PREPRINT ARXIV; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Castelvecchi D, 2020, NATURE, V587, P347, DOI 10.1038/d41586-020-03186-4; Caton S., 2020, ARXIV201004053; Chen XJ, 2017, MULTIMED TOOLS APPL, V76, P4105, DOI 10.1007/s11042-015-2930-9; Chen XP, 2021, IEEE GEOSCI REMOTE S, V18, P2011, DOI [10.1109/LGRS.2020.3009259, 10.1109/TKDE.2020.2975777]; Choe G, 2017, INT J COMPUT VISION, V122, P1, DOI 10.1007/s11263-016-0937-y; Clemens AM, 2021, CURR OPIN NEUROBIOL, V68, P116, DOI 10.1016/j.conb.2021.02.007; Crispim Felipe, 2020, Advanced Concepts for Intelligent Vision Systems. 20th International Conference, ACIVS 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12002), P215, DOI 10.1007/978-3-030-40605-9_19; Cui LY, 2017, IEEE INT CON MULTI, P751, DOI 10.1109/ICME.2017.8019326; Dahan E, 2021, IEEE T PATTERN ANAL, V43, P2851, DOI 10.1109/TPAMI.2020.3036993; Dal Martello MF, 2006, J VISION, V6, P1356, DOI 10.1167/6.12.2; Dal Martello MF, 2015, J VISION, V15, DOI 10.1167/15.13.5; Dal Martello MF, 2010, J VISION, V10, DOI 10.1167/10.8.9; Dandekar AR, 2014, 2014 IEEE STUDENTS' CONFERENCE ON ELECTRICAL, ELECTRONICS AND COMPUTER SCIENCE (SCEECS); de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11; DeBruine LM, 2009, VISION RES, V49, P38, DOI 10.1016/j.visres.2008.09.025; Debruyne F, 2002, J VOICE, V16, P466, DOI 10.1016/S0892-1997(02)00121-2; Dehghan A, 2014, PROC CVPR IEEE, P1757, DOI 10.1109/CVPR.2014.227; Dibeklioglu H., 2012, UVA NEMO SMILE DATAB; Dibeklioglu H, 2017, IEEE I CONF COMP VIS, P2478, DOI 10.1109/ICCV.2017.269; Dibeklioglu H, 2013, IEEE I CONF COMP VIS, P1497, DOI 10.1109/ICCV.2013.189; Dibeklioglu H, 2012, LECT NOTES COMPUT SC, V7574, P525, DOI 10.1007/978-3-642-33712-3_38; Drozdowski Pawel, 2020, IEEE Transactions on Technology and Society, V1, P89, DOI 10.1109/TTS.2020.2992344; Duan Q., 2017, P 2017 WORKSH REC FA, P21; Duan XD, 2015, IEEE IMAGE PROC, P1573, DOI 10.1109/ICIP.2015.7351065; Ertugrul IO, 2017, IEEE INT CONF AUTOMA, P33, DOI 10.1109/FG.2017.14; Fang R., 2010, CORNELLKIN DATABASE; Fang R., 2013, FAMILY101 DATABASE; Fang RG, 2013, IEEE IMAGE PROC, P2983, DOI 10.1109/ICIP.2013.6738614; Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590; Fang YM, 2016, IEEE INT CONF MULTI; Fasolt V., 2018, HUMAN ETHOLOGY B, V33, P19, DOI [10.22330/heb/334/019-027, DOI 10.22330/HEB/334/019-027]; Fengyuan Yu, 2021, 2021 IEEE International Conference on Joint Cloud Computing (JCC), P1, DOI 10.1109/JCC53141.2021.00012; Fu Yun, 2011, 22 INT JOINT C ART I, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-422; Gao P., 2019, ARXIV PREPRINT ARXIV; Gao XY, 2014, AAAI CONF ARTIF INTE, P1206; Georgopoulos M, 2018, IMAGE VISION COMPUT, V80, P58, DOI 10.1016/j.imavis.2018.05.003; Ghatas FS, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-020-1949-3; Gokhman D, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-15020-6; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal A, 2021, IEEE T IMAGE PROCESS, V30, P191, DOI 10.1109/TIP.2020.3034027; Goyal A, 2021, PATTERN ANAL APPL, V24, P119, DOI 10.1007/s10044-020-00906-4; Goyal A, 2018, IEEE GLOB COMM CONF; Guo GD, 2012, IEEE T INSTRUM MEAS, V61, P2322, DOI 10.1109/TIM.2012.2187468; Guo YH, 2014, INT C PATT RECOG, P4287, DOI 10.1109/ICPR.2014.735; Haibin Yan, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457930; Hansen F, 2020, J VISION, V20, DOI 10.1167/jov.20.6.18; Hu JL, 2018, IEEE T CIRC SYST VID, V28, P1875, DOI 10.1109/TCSVT.2017.2691801; Hu JL, 2015, LECT NOTES COMPUT SC, V9005, P252, DOI 10.1007/978-3-319-16811-1_17; Huang Y, 2015, BRIT MACH VIS C; Jang W, 2017, PROCEEDINGS OF THE 2017 WORKSHOP ON INTERNET OF THINGS SECURITY AND PRIVACY (IOT S&P'17), P49, DOI 10.1145/3139937.3139941; Jia D, 2011, PROC CVPR IEEE, P785, DOI 10.1109/CVPR.2011.5995516; Jin MG, 2019, PROC CVPR IEEE, P8104, DOI 10.1109/CVPR.2019.00830; Karras T, 2017, ARXIV171010196; Kayser M, 2015, FORENSIC SCI INT-GEN, V18, P33, DOI 10.1016/j.fsigen.2015.02.003; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kohli N., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P245, DOI 10.1109/BTAS.2012.6374584; Kohli N., 2017, WVU DATABASE; Kohli N., 2019, KIVI; Kohli N., 2018, DEEP LEARN BIOMETR, V130, P127; Kohli N., 2019, AUTOMATIC KINSHIP VE; Kohli N, 2019, IEEE T IMAGE PROCESS, V28, P1329, DOI 10.1109/TIP.2018.2840880; Kohli N, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2609811; Konecn J., 2016, ARXIV161005492; Kou L, 2015, MATH PROBL ENG, V2015, DOI 10.1155/2015/472473; Krupp DB, 2008, EVOL HUM BEHAV, V29, P49, DOI 10.1016/j.evolhumbehav.2007.08.002; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Kumar C, 2020, AAAI CONF ARTIF INTE, V34, P11304; Laiadi O, 2021, INT J MACH LEARN CYB, V12, P171, DOI 10.1007/s13042-020-01163-x; Laiadi O, 2020, NEUROCOMPUTING, V377, P286, DOI 10.1016/j.neucom.2019.10.055; Laiadi O, 2019, MULTIMED TOOLS APPL, V78, P16465, DOI 10.1007/s11042-018-7027-9; Lei XH, 2017, C IND ELECT APPL, P1870; Li L, 2016, LECT NOTES COMPUT SC, V9730, P539, DOI 10.1007/978-3-319-41501-7_60; Li W., 2021, P IEEE C COMP VIS PA; Li W., 2020, PROC ICME, P1; Li W., 2021, IEEE T IMAGE PROCESS; Li XS, 2015, 2015 INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS - COMPUTING TECHNOLOGY, INTELLIGENT TECHNOLOGY, INDUSTRIAL INFORMATION INTEGRATION (ICIICII), P1, DOI 10.1109/ICIICII.2015.88; Li YY, 2017, ADV SOC SCI EDUC HUM, V159, P13; Liang JQ, 2019, IEEE T IMAGE PROCESS, V28, P1149, DOI 10.1109/TIP.2018.2875346; Liang JY, 2017, COMM COM INF SC, V771, P563, DOI 10.1007/978-981-10-7299-4_47; Liu HJ, 2017, IEEE IMAGE PROC, P1072; Liu HJ, 2017, IEEE INT CON MULTI, P319, DOI 10.1109/ICME.2017.8019375; Liu Q, 2016, INT C INTEL HUM MACH, P17, DOI 10.1109/IHMSC.2016.93; Liu XC, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P1, DOI 10.1109/CompComm.2015.7387529; Lopez MB, 2018, MACH VISION APPL, V29, P873, DOI 10.1007/s00138-018-0943-x; Lopez MB, 2016, IEEE T PATTERN ANAL, V38, P2342, DOI 10.1109/TPAMI.2016.2522416; Lu J., 2014, KINFACEW DATABASE; Lu JW, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014); Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P4269, DOI 10.1109/TIP.2017.2717505; Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134; M'charek A, 2020, AM ANTHROPOL, V122, P369, DOI 10.1111/aman.13385; Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2; Madry Aleksander, 2017, ARXIV; Maloney LT, 2006, J VISION, V6, P1047, DOI 10.1167/6.10.4; Mehrabi N, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3457607; Monks SA, 2004, AM J HUM GENET, V75, P1094, DOI 10.1086/426461; Moujahid A, 2019, MULTIMED TOOLS APPL, V78, P9335, DOI 10.1007/s11042-018-6517-0; Nagpal S, 2016, IMAGE VISION COMPUT, V55, P9, DOI 10.1016/j.imavis.2016.03.019; Nolan F., 2011, PROC ICPHS, P1506; Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4; Ozkan S, 2018, IEEE IMAGE PROC, P2142, DOI 10.1109/ICIP.2018.8451305; Patel B, 2017, COMPUT VIS IMAGE UND, V160, P24, DOI 10.1016/j.cviu.2017.04.009; Porter R.H., 1991, KIN RECOGNITION, P413; Puthenputhussery A, 2016, IEEE IMAGE PROC, P2921, DOI 10.1109/ICIP.2016.7532894; Qin X., 2015, TSKINFACE DATABASE; Qin XQ, 2020, NEUROCOMPUTING, V377, P213, DOI 10.1016/j.neucom.2019.09.089; Qin XQ, 2016, NEUROCOMPUTING, V214, P350, DOI 10.1016/j.neucom.2016.06.027; Qin XQ, 2015, IEEE T MULTIMEDIA, V17, P1855, DOI 10.1109/TMM.2015.2461462; Rachmadi RF, 2021, 2020 IEEE INTERNATIONAL CONFERENCE ON INTERNET OF THINGS AND INTELLIGENCE SYSTEM (IOTAIS), P123, DOI 10.1109/IoTaIS50849.2021.9359720; Richmond S, 2018, FRONT GENET, V9, DOI 10.3389/fgene.2018.00462; Robinson J., 2019, P IEEE CVF C COMP VI; Robinson J. P., 2016, FIW DATABASE; Robinson JP, 2021, IEEE T PATTERN ANAL, V44, P4432, DOI 10.1109/TPAMI.2021.3063078; Robinson JP, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2096, DOI 10.1145/3240508.3241471; Robinson JP, 2018, IEEE T PATTERN ANAL, V40, P2624, DOI 10.1109/TPAMI.2018.2826549; Robinson Joshua David, 2021, ICLR; Russell S. J, 2002, ADV NEURAL INFORM PR, P12, DOI DOI 10.5555/2968618.2968683; SATALOFF RT, 1995, J VOICE, V9, P16, DOI 10.1016/S0892-1997(05)80218-8; Schneider PM, 2019, DTSCH ARZTEBL INT, V116, P873, DOI 10.3238/arztebl.2019.0873; Shao M., 2011, UBKINFACE DATABASE; Shao M., 2011, PROC CVPR WORKSHOPS, P60, DOI DOI 10.1109/CVPRW.2011.5981801; Shen W, 2020, PROC CVPR IEEE, P5113, DOI 10.1109/CVPR42600.2020.00516; Sinha Raunak, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P297, DOI 10.1007/978-3-030-67070-2_18; Song CH, 2020, IEEE INT CON MULTI; Suh Y, 2019, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2019.00742; Sun Y., 2018, 2018 IEEE C EV COMP, P1, DOI DOI 10.1109/CEC.2018.8477921; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Vakhshiteh F, 2021, IEEE ACCESS, V9, P92735, DOI 10.1109/ACCESS.2021.3092646; Van Gysel W D, 2001, Acta Otorhinolaryngol Belg, V55, P49; Wang J, 2021, ARXIV PREPRINT ARXIV; Wang M, 2015, 2015 IEEE 17 INT WOR, P1, DOI DOI 10.1109/MMSP.2015.7340820; Wang S., 2016, IJCAI, P2125; Wang SW, 2020, PATTERN RECOGN LETT, V138, P38, DOI 10.1016/j.patrec.2020.06.019; Wang SY, 2019, IEEE T PATTERN ANAL, V41, P2783, DOI 10.1109/TPAMI.2018.2861871; Wang SY, 2017, IEEE INT CONF AUTOMA, P216, DOI 10.1109/FG.2017.35; Wang XL, 2014, IEEE IMAGE PROC, P5017, DOI 10.1109/ICIP.2014.7026016; Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252; Wei Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P613, DOI 10.1007/978-3-030-58542-6_37; Wei ZQ, 2019, IEEE ACCESS, V7, P100029, DOI 10.1109/ACCESS.2019.2929939; Weirich M., 2011, PROC ICPHS, P2118; Whiteside SP, 2013, LOGOP PHONIATR VOCO, V38, P173, DOI 10.3109/14015439.2012.742562; Wu X, 2016, INT C PAR DISTRIB SY, P8, DOI [10.1109/ICPADS.2016.9, 10.1109/ICPADS.2016.0011]; Wu XW, 2021, PLANT SOIL, V458, P191, DOI 10.1007/s11104-019-04373-7; Wu Y, 2019, INT J COMPUT VISION, V127, P115, DOI 10.1007/s11263-018-1097-z; Wu Y, 2018, IEEE INT CONF AUTOMA, P143, DOI 10.1109/FG.2018.00030; Xia C, 2018, LECT NOTES ARTIF INT, V11012, P310, DOI 10.1007/978-3-319-97304-3_24; Xia SY, 2012, INT C PATT RECOG, P549; Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436; Xia ZQ, 2020, IEEE T IMAGE PROCESS, V29, P8590, DOI 10.1109/TIP.2020.3018222; Xia ZQ, 2020, IEEE T MULTIMEDIA, V22, P626, DOI 10.1109/TMM.2019.2931351; Xiaojun Wu, 2016, 2016 13th International Conference on Service Systems and Service Management (ICSSSM), P1, DOI 10.1109/ICSSSM.2016.7538581; Xu CF, 2017, NEUROCOMPUTING, V222, P62, DOI 10.1016/j.neucom.2016.10.010; Xu M, 2016, IEEE ACCESS, V4, P10280, DOI 10.1109/ACCESS.2016.2635147; Xu M, 2016, MATH PROBL ENG, V2016, DOI 10.1155/2016/4072323; Yan H., 2018, KFVW; Yan H., 2020, PATTERN RECOGN, V110, P107; Yan HB, 2019, PATTERN RECOGN LETT, V128, P169, DOI 10.1016/j.patrec.2019.08.023; Yan HB, 2019, PATTERN RECOGN LETT, V117, P146, DOI 10.1016/j.patrec.2018.05.027; Yan HB, 2018, PATTERN RECOGN, V75, P15, DOI 10.1016/j.patcog.2017.03.001; Yan HB, 2017, IMAGE VISION COMPUT, V60, P91, DOI 10.1016/j.imavis.2016.08.009; Yan HB, 2015, IEEE T CYBERNETICS, V45, P2535, DOI 10.1109/TCYB.2014.2376934; Yan HB, 2014, IEEE T INF FOREN SEC, V9, P1169, DOI 10.1109/TIFS.2014.2327757; Zhang HM, 2019, IEEE IMAGE PROC, P3856, DOI 10.1109/ICIP.2019.8803647; Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342; Zhang L, 2021, IEEE T CYBERNETICS, V51, P5883, DOI 10.1109/TCYB.2019.2959403; Zhang SS, 2021, INT J COMPUT VISION, V129, P1875, DOI 10.1007/s11263-021-01461-z; Zhang YL, 2015, LECT NOTES COMPUT SC, V9489, P234, DOI 10.1007/978-3-319-26532-2_26; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zhao YG, 2018, INFORM SCIENCES, V430, P247, DOI 10.1016/j.ins.2017.11.048; Zhou X., 2011, P 19 ACM INT C MULT, P953, DOI DOI 10.1145/2072298.2071911; Zhou X, 2012, P 20 ACM INT C MULT, P725; Zhou XZ, 2019, INFORM FUSION, V48, P84, DOI 10.1016/j.inffus.2018.07.011; Zhou XZ, 2016, INFORM FUSION, V32, P40, DOI 10.1016/j.inffus.2015.08.006; Zhou XZ, 2016, NEUROCOMPUTING, V197, P136, DOI 10.1016/j.neucom.2016.02.039; Zhou X, 2016, IEEE IMAGE PROC, P2911, DOI 10.1109/ICIP.2016.7532892	190	1	1	14	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1494	1525		10.1007/s11263-022-01605-9	http://dx.doi.org/10.1007/s11263-022-01605-9		APR 2022	32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU	35465628	Green Published, hybrid			2022-12-18	WOS:000783738500002
J	Cao, X; Waechter, M; Shi, BX; Gao, Y; Zheng, B; Okura, F; Matsushita, Y				Cao, Xu; Waechter, Michael; Shi, Boxin; Gao, Ye; Zheng, Bo; Okura, Fumio; Matsushita, Yasuyuki			Shape and Albedo Recovery by Your Phone using Stereoscopic Flash and No-Flash Photography	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D reconstruction; Stereo camera; Flash photography; Albedo; E-heritage	HIGH-QUALITY SHAPE; PHOTOMETRIC STEREO; IMAGE; DEPTH	Recovering shape and albedo for the immense number of existing cultural heritage artifacts is challenging. Accurate 3D reconstruction systems are typically expensive and thus inaccessible to many and cheaper off-the-shelf 3D sensors often generate results of unsatisfactory quality. This paper presents a high-fidelity shape and albedo recovery method that only requires a stereo camera and a flashlight, a typical camera setup equipped in many off-the-shelf smartphones. The stereo camera allows us to infer rough shape from a pair of no-flash images, and a flash image is further captured for shape refinement based on our flash/no-flash image formation model. We verify the effectiveness of our method on real-world artifacts in indoor and outdoor conditions using smartphones with different camera/flashlight configurations. Comparison results demonstrate that our stereoscopic flash and no-flash photography benefits the high-fidelity shape and albedo recovery on a smartphone. Using our method, people can immediately turn their phones into high-fidelity 3D scanners, facilitating the digitization of cultural heritage artifacts.	[Cao, Xu; Waechter, Michael; Okura, Fumio; Matsushita, Yasuyuki] Osaka Univ, Grad Sch Informat Sci & Technol, Osaka, Japan; [Shi, Boxin] Peking Univ, Dept Comp Sci & Technol, Beijing, Peoples R China; [Shi, Boxin] Peng Cheng Lab, Shenzhen, Peoples R China; [Gao, Ye; Zheng, Bo] Huawei Technol Co Ltd, Shenzhen, Peoples R China	Osaka University; Peking University; Peng Cheng Laboratory; Huawei Technologies	Cao, X (corresponding author), Osaka Univ, Grad Sch Informat Sci & Technol, Osaka, Japan.	cao.xu@ist.osaka-u.ac.jp; shiboxin@pku.edu.cn; okura@ist.osaka-u.ac.jp; yasumat@ist.osaka-u.ac.jp		Cao, Xu/0000-0003-4309-6922	JSPS KAKENHI [JP19H01123]; JSPS postdoctoral fellowship [JP17F17350]; National Natural Science Foundation of China [62136001, 61872012]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JSPS postdoctoral fellowship(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by JSPS KAKENHI Grant Number JP19H01123, JSPS postdoctoral fellowship (JP17F17350), and National Natural Science Foundation of China under Grant Number 62136001 and 61872012.	Aittala M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925917; Aittala M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766967; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Boss M, 2020, PROC CVPR IEEE, P3981, DOI 10.1109/CVPR42600.2020.00404; Cao X., 2021, P COMP VIS PATT REC; Cao X, 2020, PROC CVPR IEEE, P3427, DOI 10.1109/CVPR42600.2020.00349; Choe GM, 2014, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2014.501; Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819; Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378; Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778; Feris R, 2005, IEEE I CONF COMP VIS, P412; Gallardo M, 2017, IEEE I CONF COMP VIS, P3904, DOI 10.1109/ICCV.2017.419; Haefner B, 2020, IEEE T PATTERN ANAL, V42, P2453, DOI 10.1109/TPAMI.2019.2923621; Haefner B, 2018, PROC CVPR IEEE, P164, DOI 10.1109/CVPR.2018.00025; Han Y, 2013, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2013.204; Haque SM, 2014, PROC CVPR IEEE, P2283, DOI 10.1109/CVPR.2014.292; He SF, 2014, LECT NOTES COMPUT SC, V8691, P110, DOI 10.1007/978-3-319-10578-9_8; HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011; IKEUCHI K, 1987, INT J ROBOT RES, V6, P15, DOI 10.1177/027836498700600102; Johnson MK, 2011, PROC CVPR IEEE; Klowsky R, 2012, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2012.6247825; Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5; Li Zi-xin, 2018, Advanced Technology of Electrical Engineering and Energy, V37, P1, DOI 10.12067/ATEEE1712020; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Maier R, 2017, IEEE I CONF COMP VIS, P3133, DOI 10.1109/ICCV.2017.338; Maurer D, 2018, INT J COMPUT VISION, V126, P1342, DOI 10.1007/s11263-018-1079-1; Or-El R, 2015, PROC CVPR IEEE, P5407, DOI 10.1109/CVPR.2015.7299179; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Queau Y., 2017, INT WORKSH EN MIN ME; Queau Y, 2018, J MATH IMAGING VIS, V60, P576, DOI 10.1007/s10851-017-0773-x; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; Saxena A, 2007, IEEE I CONF COMP VIS, P1; Sun J, 2006, ACM T GRAPHIC, V25, P772, DOI 10.1145/1141911.1141954; Wu CL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661232; Wu CL, 2011, IEEE I CONF COMP VIS, P1108, DOI 10.1109/ICCV.2011.6126358; Wu CL, 2011, PROC CVPR IEEE, P969, DOI 10.1109/CVPR.2011.5995388; Yan S, 2018, LECT NOTES COMPUT SC, V11214, P155, DOI 10.1007/978-3-030-01249-6_10; Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186; Zhang Q, 2012, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2012.6247962; Zhou CY, 2012, PROC CVPR IEEE, P342, DOI 10.1109/CVPR.2012.6247694	42	1	1	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1403	1415		10.1007/s11263-022-01597-6	http://dx.doi.org/10.1007/s11263-022-01597-6		APR 2022	13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU					2022-12-18	WOS:000778088800001
J	Xiao, TH; Liu, SF; De Mello, S; Yu, ZD; Kautz, J; Yang, MH				Xiao, Taihong; Liu, Sifei; De Mello, Shalini; Yu, Zhiding; Kautz, Jan; Yang, Ming-Hsuan			Learning Contrastive Representation for Semantic Correspondence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic Correspondence; Image-level Contrastive Learning; Pixel-level Contrastive Learning; Cross-instance Cycle Consistency	FLOW	Dense correspondence across semantically related images has been extensively studied, but still faces two challenges: 1) large variations in appearance, scale and pose exist even for objects from the same category, and 2) labeling pixel-level dense correspondences is labor intensive and infeasible to scale. Most existing methods focus on designing various matching modules using fully-supervised ImageNet pretrained networks. On the other hand, while a variety of self-supervised approaches are proposed to explicitly measure image-level similarities, correspondence matching the pixel level remains under-explored. In this work, we propose a multi-level contrastive learning approach for semantic matching, which does not rely on any ImageNet pretrained model. We show that image-level contrastive learning is a key component to encourage the convolutional features to find correspondence between similar objects, while the performance can be further enhanced by regularizing cross-instance cycle-consistency at intermediate feature levels. Experimental results on the PF-PASCAL, PF-WILLOW, and SPair-71k benchmark datasets demonstrate that our method performs favorably against the state-of-the-art approaches. The source code and trained models will be made available to the public.	[Xiao, Taihong; Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95343 USA; [Liu, Sifei; De Mello, Shalini; Yu, Zhiding; Kautz, Jan] Nvidia, Santa Clara, CA USA; [Yang, Ming-Hsuan] Yonsei Univ, Seoul, South Korea	University of California System; University of California Merced; Nvidia Corporation; Yonsei University	Yang, MH (corresponding author), Univ Calif Merced, Merced, CA 95343 USA.; Yang, MH (corresponding author), Yonsei Univ, Seoul, South Korea.	txiao3@ucmerced.edu; sifeil@nvidia.com; shalinig@nvidia.com; zhidingy@nvidia.com; jkautz@nvidia.com; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304	NSF CAREER grant [1149783]	NSF CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD))	T. Xiao and M.-H. Yang are supported in part by NSF CAREER grant 1149783.	Avd Oord, 2018, ARXIV180703748; Bristow H, 2015, IEEE I CONF COMP VIS, P4024, DOI 10.1109/ICCV.2015.458; Chen T, 2020, PR MACH LEARN RES, V119; Chen Y, 2018, LECT NOTES COMPUT SC, V11256, P347, DOI 10.1007/978-3-030-03398-9_30; Choy C. B., 2016, UNIVERSAL CORRES NET; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dale K, 2009, IEEE I CONF COMP VIS, P2217, DOI 10.1109/ICCV.2009.5459473; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Gidaris Spyros, 2018, ARXIV180307728; Grill J.B., 2020, ADV NEUR IN; Ham B, 2018, IEEE T PATTERN ANAL, V40, P1711, DOI 10.1109/TPAMI.2017.2724510; Ham B, 2016, PROC CVPR IEEE, P3475, DOI 10.1109/CVPR.2016.378; Han K, 2017, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2017.203; He K., 2020, P IEEECVF C COMPUTER, P9729; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hjelm R Devon, 2019, INT C LEARN REPR; Huang SY, 2019, IEEE I CONF COMP VIS, P2010, DOI 10.1109/ICCV.2019.00210; Hur J, 2015, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2015.7298745; Jabri Allan, 2020, NEURIPS; Jeon S, 2018, LECT NOTES COMPUT SC, V11210, P355, DOI 10.1007/978-3-030-01231-1_22; Kanazawa A, 2016, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2016.354; Kang G., 2020, NEURAL INFORM PROCES; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Kim S, 2019, IEEE T PATTERN ANAL, V41, P581, DOI 10.1109/TPAMI.2018.2803169; Kim S, 2017, IEEE I CONF COMP VIS, P4539, DOI 10.1109/ICCV.2017.485; Kim Seungryong, 2018, ADV NEURAL INFORM PR, P6129; Lee J, 2019, PROC CVPR IEEE, P2273, DOI 10.1109/CVPR.2019.00238; Li X., 2019, JOINT TASK SELF SUPE; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu PP, 2019, AAAI CONF ARTIF INTE, P8770; Liu YB, 2020, PROC CVPR IEEE, P4462, DOI 10.1109/CVPR42600.2020.00452; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Meister S, 2018, AAAI CONF ARTIF INTE, P7251; Min J., 2020, EUR C COMP VIS ECCV; Min J., 2019, ARXIV190810543; Min J, 2019, IEEE I CONF COMP VIS, P3394, DOI 10.1109/ICCV.2019.00349; Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Novotny D, 2017, PROC CVPR IEEE, P2867, DOI 10.1109/CVPR.2017.306; Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638; Pinheiro Pedro O, 2020, NEURIPS; Rocco I, 2018, PROC CVPR IEEE, P6917, DOI 10.1109/CVPR.2018.00723; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Rocco Ignacio, 2018, NEIGHBOURHOOD CONSEN, P1; Seo PH, 2018, LECT NOTES COMPUT SC, V11208, P367, DOI 10.1007/978-3-030-01225-0_22; SINKHORN R, 1967, AM MATH MON, V74, P402, DOI 10.2307/2314570; Taniai T, 2016, PROC CVPR IEEE, P4246, DOI 10.1109/CVPR.2016.460; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; van den Oord A, 2016, PR MACH LEARN RES, V48; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Vinyals O., 2016, CONDITIONAL IMAGE GE; Wang XG, 2019, PROC CVPR IEEE, P8868, DOI [10.1109/CVPR.2019.00908, 10.1109/CVPR.2019.00267]; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Wang Xinlong, 2020, DENSE CONTRASTIVE LE, P3; Xiao T., 2018, INT C LEARN REPR WOR; Xiao TH, 2018, LECT NOTES COMPUT SC, V11214, P172, DOI 10.1007/978-3-030-01249-6_11; Xie ZD, 2021, PROC CVPR IEEE, P16679, DOI 10.1109/CVPR46437.2021.01641; Yang HS, 2014, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2014.435; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhou S., 2017, BRIT MACH VIS C BMVC; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhou TH, 2015, PROC CVPR IEEE, P1191, DOI 10.1109/CVPR.2015.7298723; Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	69	1	1	3	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1293	1309		10.1007/s11263-022-01602-y	http://dx.doi.org/10.1007/s11263-022-01602-y		MAR 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		Green Submitted			2022-12-18	WOS:000772716000001
J	Zhang, ZY; Hua, BS; Yeung, SK				Zhang, Zhiyuan; Hua, Binh-Son; Yeung, Sai-Kit			RIConv plus plus : Effective Rotation Invariant Convolutions for 3D Point Clouds Deep Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D point cloud; Convolutional neural networks; Deep learning; Rotation invariance		3D point clouds deep learning is a promising field of research that allows a neural network to learn features of point clouds directly, making it a robust tool for solving 3D scene understanding tasks. While recent works show that point cloud convolutions can be invariant to translation and point permutation, investigations of the rotation invariance property for point cloud convolution has been so far scarce. Some existing methods perform point cloud convolutions with rotation-invariant features, existing methods generally do not perform as well as translation-invariant only counterpart. In this work, we argue that a key reason is that compared to point coordinates, rotation-invariant features consumed by point cloud convolution are not as distinctive. To address this problem, we propose a simple yet effective convolution operator that enhances feature distinction by designing powerful rotation invariant features from the local regions. We consider the relationship between the point of interest and its neighbors as well as the internal relationship of the neighbors to largely improve the feature descriptiveness. Our network architecture can capture both local and global context by simply tuning the neighborhood size in each convolution layer. We conduct several experiments on synthetic and real-world point cloud classifications, part segmentation, and shape retrieval to evaluate our method, which achieves the state-of-the-art accuracy under challenging rotations.	[Zhang, Zhiyuan] Zhejiang Univ, Ningbo Res Inst, Ningbo, Peoples R China; [Zhang, Zhiyuan] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Peoples R China; [Zhang, Zhiyuan] NingboTech Univ, Ningbo, Peoples R China; [Hua, Binh-Son] VinAI, Hanoi, Vietnam; [Yeung, Sai-Kit] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China	Zhejiang University; Zhejiang University; NingboTech University; Hong Kong University of Science & Technology	Zhang, ZY (corresponding author), Zhejiang Univ, Ningbo Res Inst, Ningbo, Peoples R China.; Zhang, ZY (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Peoples R China.; Zhang, ZY (corresponding author), NingboTech Univ, Ningbo, Peoples R China.	cszyzhang@gmail.com; binhson.hua@gmail.com; saikit@ust.hk		Zhang, Zhiyuan/0000-0003-3945-5638	Ningbo Research Institute of Zhejiang University [1149957B20210125]; HKUST [R9429]	Ningbo Research Institute of Zhejiang University; HKUST	We thank the anonymous reviewers for their constructive comments. This research project is supported by the grant from Ningbo Research Institute of Zhejiang University (1149957B20210125), and partially supported by an internal grant from HKUST (R9429).	ARMENI I, 2016, PROC CVPR IEEE, P1534, DOI DOI 10.1109/CVPR.2016.170; Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543; Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109; Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53; Chen C, 2019, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR.2019.00513; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Deng HW, 2018, LECT NOTES COMPUT SC, V11209, P620, DOI 10.1007/978-3-030-01228-1_37; Esteves C, 2019, IEEE I CONF COMP VIS, P1568, DOI 10.1109/ICCV.2019.00165; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; Furuya T., 2016, P BMVC; Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434; Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y; Hua BS, 2016, INT CONF 3D VISION, P92, DOI 10.1109/3DV.2016.18; Kim Jaehyeon, 2020, ARXIV PREPRINT ARXIV; Kim S., 2020, ADV NEURAL INFORM PR, V33, P8174; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Laptev D, 2016, PROC CVPR IEEE, P289, DOI 10.1109/CVPR.2016.38; Li Xianzhi, 2021, IEEE T VIS COMPUT GR; Li YY, 2018, ADV NEUR IN, V31; Li YY, 2016, ADV NEUR IN, V29; Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z; Poulenard A, 2019, INT CONF 3D VISION, P47, DOI 10.1109/3DV.2019.00015; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Rao YM, 2019, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2019.00054; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sanderson M, 2010, NAT LANG ENG, V16, P100, DOI 10.1017/S1351324909005129; Savva M., 2016, P EUR WORKSH 3D OBJ; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Tatsuma A, 2009, VISUAL COMPUT, V25, P785, DOI 10.1007/s00371-008-0304-2; Thomas H, 2020, INT CONF 3D VISION, P504, DOI 10.1109/3DV50981.2020.00060; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Weiler M, 2018, PROC CVPR IEEE, P849, DOI 10.1109/CVPR.2018.00095; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xu Y, 2018, ADV SOC SCI EDUC HUM, V284, P87; Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238; Yongheng Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P1, DOI 10.1007/978-3-030-58452-8_1; Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748; Zhang ZY, 2020, INT CONF 3D VISION, P210, DOI 10.1109/3DV50981.2020.00031; Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169; Zhang ZY, 2019, INT CONF 3D VISION, P204, DOI 10.1109/3DV.2019.00031; Zhou K., 2018, INT C LEARN REPR ICL, P1	51	1	1	10	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1228	1243		10.1007/s11263-022-01601-z	http://dx.doi.org/10.1007/s11263-022-01601-z		MAR 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		Green Submitted			2022-12-18	WOS:000770500300001
J	Dong, JT; Shuai, Q; Sun, JX; Zhang, YQ; Bao, HJ; Zhou, XW				Dong, Junting; Shuai, Qing; Sun, Jingxiang; Zhang, Yuanqing; Bao, Hujun; Zhou, Xiaowei			iMoCap: Motion Capture from Internet Videos	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion capture; Human pose estimation; Internet videos; Multi-view reconstruction	SHAPE; POSE	Recent advances in image-based human pose estimation make it possible to capture 3D human motion from a single RGB video. However, the inherent depth ambiguity and self-occlusion in a single view prohibit the recovery of as high-quality motion as multi-view reconstruction. While multi-view videos are not common, the videos of a person performing a specific action are usually abundant on the Internet. Even if these videos were recorded at different time instances, they would encode the same motion characteristics of the person. Therefore, we propose to capture human motion by jointly analyzing these Internet videos instead of using single videos separately. However, this new task poses many new challenges that cannot be addressed by existing methods, as the videos are unsynchronized, the camera viewpoints are unknown, the background scenes are different, and the human motions are not exactly the same among videos. To address these challenges, we propose a novel optimization-based framework and experimentally demonstrate its ability to recover much more precise and detailed motion from multiple videos, compared against monocular pose estimation methods.	[Dong, Junting; Shuai, Qing; Zhang, Yuanqing; Bao, Hujun; Zhou, Xiaowei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China; [Sun, Jingxiang] Univ Illinois, Champaign, IL USA	Zhejiang University; University of Illinois System; University of Illinois Urbana-Champaign	Zhou, XW (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.	jtdong@zju.edu.cn; s_q@zju.edu.cn; js73@illinois.edu; 3160102539@zju.edu.cn; bao@cad.zju.edu.cn; xwzhou@zju.edu.cn			NSFC [62172364]	NSFC(National Natural Science Foundation of China (NSFC))	The authors would like to acknowledge support from NSFC (No. 62172364).	Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Arnab A, 2019, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2019.00351; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Burenius M, 2013, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2013.464; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Caspi Y, 2002, IEEE T PATTERN ANAL, V24, P1409, DOI 10.1109/TPAMI.2002.1046148; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742; Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081; Dong JT, 2019, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2019.00798; Dwibedi D, 2019, PROC CVPR IEEE, P1801, DOI 10.1109/CVPR.2019.00190; Elhayek A, 2015, PROC CVPR IEEE, P3810, DOI 10.1109/CVPR.2015.7299005; Elhayek A, 2012, PROC CVPR IEEE, P1870, DOI 10.1109/CVPR.2012.6247886; Elhayek A., 2015, OUTDOOR HUMAN MOTION; Feng Y, 2019, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2019.00138; Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33; Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1; Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300; Guler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114; Hasler N, 2009, PROC CVPR IEEE, P224, DOI 10.1109/CVPRW.2009.5206859; HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698; Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184; Huang YH, 2017, INT CONF 3D VISION, P421, DOI 10.1109/3DV.2017.00055; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868; Junting Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P210, DOI 10.1007/978-3-030-58536-5_13; Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Kocabas Muhammed, 2021, ICCV; Lassner Christoph, 2017, CVPR; Lee CS, 2010, INT J COMPUT VISION, V87, P118, DOI 10.1007/s11263-009-0266-5; Li R, 2010, INT J COMPUT VISION, V87, P170, DOI 10.1007/s11263-009-0283-4; Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062; Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123; Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794; Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883; Saini N, 2019, IEEE I CONF COMP VIS, P823, DOI 10.1109/ICCV.2019.00091; Sermanet P, 2018, IEEE INT CONF ROBOT, P1134; Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4; Sigal Leonid, 2008, NEURIPS; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284; Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Tuytelaars T, 2004, PROC CVPR IEEE, P762; Ukrainitz Y, 2006, LECT NOTES COMPUT SC, V3953, P538, DOI 10.1007/11744078_42; Wang J., 2020, ARXIV PREPRINT ARXIV; Wang JY, 2017, IEEE ICC; Wang O, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601208; Wang Y., 2017, TVCG; Wolf L, 2006, INT J COMPUT VISION, V68, P43, DOI 10.1007/s11263-005-4841-0; Xiang DL, 2019, PROC CVPR IEEE, P10957, DOI 10.1109/CVPR.2019.01122; Xu Xiangyu, 2019, ARXIV190406903; Yu C., 2020, ARXIV PREPRINT ARXIV; Yuan X, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533569; Zanfir A, 2018, ADV NEUR IN, V31; Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229; Zheng EL, 2015, IEEE I CONF COMP VIS, P4435, DOI 10.1109/ICCV.2015.504; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51	70	1	1	4	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1165	1180		10.1007/s11263-022-01596-7	http://dx.doi.org/10.1007/s11263-022-01596-7		MAR 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA					2022-12-18	WOS:000767739700001
J	Guo, H; Fan, XC; Wang, S				Guo, Hao; Fan, Xiaochuan; Wang, Song			Visual Attention Consistency for Human Attribute Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Attention maps; Human attribute recognition; Attention consistency; Equivariance	SELECTIVE ATTENTION	The recognition of a human attribute is usually determined by certain regions of the input image, e.g., certain part of the human body, and such attribute-region relevance plays an important role in human attribute recognition. In deep networks, this attribute-region relevance can be derived as an interpretive attention map, where highlighted areas indicate the most relevant regions that contribute to the final recognition. Based on the assumption that more plausible attention maps indicate better networks, in this paper, we propose a new approach for human attribute recognition by exploring and enforcing two kinds of attention consistency in network learning. One kind of consistency enforces the equivariance of the attention map when the input image undergoes certain spatial transforms, such as scaling, rotation and flipping. The other kind of the consistency is enforced between the attention maps derived from two different networks when both of them are trained for recognizing the same attribute from the same image. We formulate these two kinds of consistency as new loss functions and combine them with the traditional classification loss for network training. Experiments on three datasets of human attribute recognition verify the effectiveness of the proposed method by achieving new state-of-the-art performance.	[Guo, Hao; Wang, Song] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29201 USA; [Fan, Xiaochuan] JD Com Amer Technol Corp, Mountain View, CA USA	University of South Carolina System; University of South Carolina Columbia	Wang, S (corresponding author), Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29201 USA.	hguo@email.sc.edu; efan3000@gmail.com; songwang@cec.sc.edu		Wang, Song/0000-0003-4152-5295				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bansal N., 2020, P IEEECVF C COMPUTER, P8673; Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413; Cohen TS, 2016, PR MACH LEARN RES, V48; Connor CE, 2004, CURR BIOL, V14, pR850, DOI 10.1016/j.cub.2004.09.041; Dabkowski P, 2017, ADV NEUR IN, V30; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966; DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev.neuro.18.1.193; ERIKSEN CW, 1972, PERCEPT PSYCHOPHYS, V12, P201, DOI 10.3758/BF03212870; Feris R., 2014, P INT C MULTIMEDIA R, P153; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082; Guo H, 2017, PATTERN RECOGN LETT, V94, P38, DOI 10.1016/j.patrec.2017.05.012; Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hu J., 2017, ARXIV PREPRINT ARXIV, V7; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jaderberg M, 2015, ADV NEUR IN, V28; Kivinen JJ, 2011, LECT NOTES COMPUT SC, V6791, P1, DOI 10.1007/978-3-642-21735-7_1; KOCH C, 1985, HUM NEUROBIOL, V4, P219; Koch K, 2006, CURR BIOL, V16, P1428, DOI 10.1016/j.cub.2006.05.056; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lavie N, 2005, TRENDS COGN SCI, V9, P75, DOI 10.1016/j.tics.2004.12.004; Lenc K, 2016, LECT NOTES COMPUT SC, V9915, P100, DOI 10.1007/978-3-319-49409-8_11; Li DW, 2018, IEEE INT CON MULTI; Li DW, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P111, DOI 10.1109/ACPR.2015.7486476; Li Dangwei, 2016, ARXIV160307054; Li QZ, 2019, AAAI CONF ARTIF INTE, P8634; Li YN, 2016, LECT NOTES COMPUT SC, V9910, P684, DOI 10.1007/978-3-319-46466-4_41; Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006; Liu Peter J., 2018, GENERATING WIKIPEDIA, P2; Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46; Marcos D, 2017, IEEE I CONF COMP VIS, P5058, DOI 10.1109/ICCV.2017.540; MORAN J, 1985, SCIENCE, V229, P782, DOI 10.1126/science.4023713; Muller R, 2019, ADV NEUR IN, V32; Niu X., 2019, ADV NEURAL INFORM PR, P909; Oord A.V.D., 2016, SSW; OQUAB M, 2015, PROC CVPR IEEE, P685, DOI DOI 10.1109/CVPR.2015.7298668; Qiao SY, 2018, LECT NOTES COMPUT SC, V11219, P142, DOI 10.1007/978-3-030-01267-0_9; Ravanbakhsh S, 2017, PR MACH LEARN RES, V70; Ribeiro MT, 2018, AAAI CONF ARTIF INTE, P1527; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Sarafianos N., 2018, ARXIV PREPRINT ARXIV; Sarfraz M. S., 2017, ARXIV171110378; Schmidt U, 2012, PROC CVPR IEEE, P2050, DOI 10.1109/CVPR.2012.6247909; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shrikumar A, 2017, PR MACH LEARN RES, V70; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30; Sudowe P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P329, DOI 10.1109/ICCVW.2015.51; Sundararajan M, 2017, PR MACH LEARN RES, V70; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan ZC, 2020, AAAI CONF ARTIF INTE, V34, P12055; Tan ZC, 2019, IEEE T IMAGE PROCESS, V28, P6126, DOI 10.1109/TIP.2019.2919199; Tang CF, 2019, IEEE I CONF COMP VIS, P4996, DOI 10.1109/ICCV.2019.00510; Tarvainen Antti, 2017, CORR, Vabs/1703; Thewlis J, 2017, IEEE I CONF COMP VIS, P3229, DOI 10.1109/ICCV.2017.348; Thewlis James, 2017, ADV NEURAL INFORM PR, V3, P8; Tian YL, 2015, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2015.7299143; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI 10.1109/ICCV.2017.283; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Wang XM, 2019, ADV NEUR IN, V32; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Worrall D, 2018, LECT NOTES COMPUT SC, V11209, P585, DOI 10.1007/978-3-030-01228-1_35; Worrall DE, 2017, PROC CVPR IEEE, P7168, DOI 10.1109/CVPR.2017.758; Wu MD, 2020, AAAI CONF ARTIF INTE, V34, P12394; Xu C., 2019, ARXIV PREPRINT ARXIV; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212; Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454; Zhao X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3177; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou ZH, 2002, ARTIF INTELL, V137, P239, DOI 10.1016/S0004-3702(02)00190-X; Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219; Zhu JQ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P331, DOI 10.1109/ICCVW.2013.51; Zintgraf Luisa M., 2017, P ICLR	87	1	1	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					1088	1106		10.1007/s11263-022-01591-y	http://dx.doi.org/10.1007/s11263-022-01591-y		MAR 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ					2022-12-18	WOS:000764971600002
J	Stoiber, M; Pfanne, M; Strobl, KH; Triebel, R; Albu-Schaffer, A				Stoiber, Manuel; Pfanne, Martin; Strobl, Klaus H.; Triebel, Rudolph; Albu-Schaffer, Alin			SRT3D: A Sparse Region-Based 3D Object Tracking Approach for the Real World	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Region-based; 3D object tracking; Pose estimation; Sparse; Real-time	TIME VISUAL TRACKING; SEGMENTATION	Region-based methods have become increasingly popular for model-based, monocular 3D tracking of texture-less objects in cluttered scenes. However, while they achieve state-of-the-art results, most methods are computationally expensive, requiring significant resources to run in real-time. In the following, we build on our previous work and develop SRT3D, a sparse region-based approach to 3D object tracking that bridges this gap in efficiency. Our method considers image information sparsely along so-called correspondence lines that model the probability of the object's contour location. We thereby improve on the current state of the art and introduce smoothed step functions that consider a defined global and local uncertainty. For the resulting probabilistic formulation, a thorough analysis is provided. Finally, we use a pre-rendered sparse viewpoint model to create a joint posterior probability for the object pose. The function is maximized using second-order Newton optimization with Tikhonov regularization. During the pose estimation, we differentiate between global and local optimization, using a novel approximation for the first-order derivative employed in the Newton method. In multiple experiments, we demonstrate that the resulting algorithm improves the current state of the art both in terms of runtime and quality, performing particularly well for noisy and cluttered images encountered in the real world.	[Stoiber, Manuel; Pfanne, Martin; Strobl, Klaus H.; Triebel, Rudolph; Albu-Schaffer, Alin] German Aerosp Ctr, D-82234 Wessling, Germany; [Stoiber, Manuel; Triebel, Rudolph; Albu-Schaffer, Alin] Tech Univ Munich, D-80333 Munich, Germany	Helmholtz Association; German Aerospace Centre (DLR); Technical University of Munich	Stoiber, M (corresponding author), German Aerosp Ctr, D-82234 Wessling, Germany.	manuel.stoiber@dlr.de; martin.pfanne@dlr.de; klaus.strobl@dlr.de; rudolph.biebel@dlr.de; alin.albu-schaeffer@dlr.de		Strobl, Klaus H./0000-0001-8123-0606; Triebel, Rudolph/0000-0002-7975-036X; Stoiber, Manuel/0000-0002-0762-9288				BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bibby C, 2008, LECT NOTES COMPUT SC, V5303, P831, DOI 10.1007/978-3-540-88688-4_61; Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366; Brox T, 2010, IEEE T PATTERN ANAL, V32, P402, DOI 10.1109/TPAMI.2009.32; Bugaev B, 2018, LECT NOTES COMPUT SC, V11216, P55, DOI 10.1007/978-3-030-01258-8_4; Comport AI, 2006, IEEE T VIS COMPUT GR, V12, P615, DOI 10.1109/TVCG.2006.78; Crivellaro A, 2014, PROC CVPR IEEE, P3414, DOI 10.1109/CVPR.2014.436; Dambreville S, 2008, LECT NOTES COMPUT SC, V5303, P169, DOI 10.1007/978-3-540-88688-4_13; Deng X., 2021, IEEE T ROBOT; Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620; Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577; Garon M, 2017, IEEE T VIS COMPUT GR, V23, P2410, DOI 10.1109/TVCG.2017.2734599; Harris C., 1990, BMVC90 Proceedings of the British Machine Vision Conference, P73; Hexner J, 2016, INT J COMPUT VISION, V118, P95, DOI 10.1007/s11263-015-0873-2; Hinterstoisser Stefan, 2012, P AS C COMP VIS, P2, DOI DOI 10.1007/978-3-642-37331-2_42; Huang H, 2020, COMPUT GRAPH FORUM, V39, P399, DOI 10.1111/cgf.14154; Kehl W, 2017, PROC CVPR IEEE, P465, DOI 10.1109/CVPR.2017.57; Krainin M, 2011, INT J ROBOT RES, V30, P1311, DOI 10.1177/0278364911403178; Krull A, 2015, LECT NOTES COMPUT SC, V9006, P384, DOI 10.1007/978-3-319-16817-3_25; Lankton S, 2008, IEEE T IMAGE PROCESS, V17, P2029, DOI 10.1109/TIP.2008.2004611; Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001; Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542; Li JC, 2021, J COMPUT SCI TECH-CH, V36, P555, DOI 10.1007/s11390-021-1272-5; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Liu F, 2021, IEEE T IND ELECTRON; Liu Y, 2020, IEEE SENS J, V20, P6727, DOI 10.1109/JSEN.2020.2976202; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Pauwels K, 2013, PROC CVPR IEEE, P2347, DOI 10.1109/CVPR.2013.304; Prisacariu VA, 2012, INT J COMPUT VISION, V98, P335, DOI 10.1007/s11263-011-0514-3; Prisacariu VA, 2015, IEEE T VIS COMPUT GR, V21, P557, DOI 10.1109/TVCG.2014.2355207; Ren CY, 2017, INT J COMPUT VISION, V124, P80, DOI 10.1007/s11263-016-0978-2; Rosenhahn B, 2007, INT J COMPUT VISION, V73, P243, DOI [10.1007/s11263-006-9965-3, 10.1007/S11263-006-9965-3]; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499; Schmaltz C, 2012, MACH VISION APPL, V23, P557, DOI 10.1007/s00138-010-0317-5; Seo BK, 2016, LECT NOTES COMPUT SC, V9915, P551, DOI 10.1007/978-3-319-49409-8_48; Seo BK, 2014, IEEE T VIS COMPUT GR, V20, P99, DOI 10.1109/TVCG.2013.94; Stoiber Manuel, 2020, AS C COMP VIS, P666; Sun X, 2021, IEEE T CIRC SYST VID; Tan DJ, 2017, IEEE T VIS COMPUT GR, V23, P2399, DOI 10.1109/TVCG.2017.2734539; Tjaden H, 2019, IEEE T PATTERN ANAL, V41, P1797, DOI 10.1109/TPAMI.2018.2884990; Vacchetti L, 2004, IEEE T PATTERN ANAL, V26, P1385, DOI 10.1109/TPAMI.2004.92; Wagner D, 2010, IEEE T VIS COMPUT GR, V16, P355, DOI 10.1109/TVCG.2009.99; Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346; Wen BW, 2020, IEEE INT C INT ROBOT, P10367, DOI 10.1109/IROS45743.2020.9341314; Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Wu PC, 2017, ADJUNCT PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P186, DOI 10.1109/ISMAR-Adjunct.2017.62; Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Zhao S, 2014, IEEE IMAGE PROC, P486, DOI 10.1109/ICIP.2014.7025097; Zhong LS, 2020, IEEE ROBOT AUTOM LET, V5, P5159, DOI 10.1109/LRA.2020.3003866; Zhong LS, 2020, IEEE T IMAGE PROCESS, V29, P5065, DOI 10.1109/TIP.2020.2973512; Zhong LS, 2019, INT J COMPUT VISION, V127, P973, DOI 10.1007/s11263-018-1119-x	56	1	1	7	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					1008	1030		10.1007/s11263-022-01579-8	http://dx.doi.org/10.1007/s11263-022-01579-8		FEB 2022	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ		hybrid, Green Submitted			2022-12-18	WOS:000761320900001
J	Bergmann, P; Batzner, K; Fauser, M; Sattlegger, D; Steger, C				Bergmann, Paul; Batzner, Kilian; Fauser, Michael; Sattlegger, David; Steger, Carsten			Beyond Dents and Scratches: Logical Constraints in Unsupervised Anomaly Detection and Localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Anomaly detection; Novelty detection; Datasets; Unsupervised learning; Defect segmentation; Performance metrics		The unsupervised detection and localization of anomalies in natural images is an intriguing and challenging problem. Anomalies manifest themselves in very different ways and an ideal benchmark dataset for this task should contain representative examples for all of them. We find that existing datasets are biased towards local structural anomalies such as scratches, dents, or contaminations. In particular, they lack anomalies in the form of violations of logical constraints, e.g., permissible objects occurring in invalid locations. We contribute a new dataset based on industrial inspection scenarios that evenly covers both types of anomalies. We provide pixel-precise ground truth data for each anomalous region and define a generalized evaluation metric that addresses localization ambiguities that can arise for logical anomalies. Furthermore, we propose a novel algorithm that improves over the state of the art in the joint detection of structural and logical anomalies. It consists of a local and a global network branch. The first one inspects confined regions independent of their spatial locations in the input image and is primarily responsible for the detection of entirely new local structures. The second one learns a globally consistent representation of the training data through a bottleneck that enables the detection of violations of long-range dependencies, a key characteristic of many logical anomalies. We perform extensive evaluations on our new dataset to corroborate our claims.	[Bergmann, Paul; Batzner, Kilian; Fauser, Michael; Sattlegger, David; Steger, Carsten] MVTec Software GmbH, Arnulfstr 205, D-80634 Munich, Germany; [Bergmann, Paul] Tech Univ Munich, Dept Informat, Boltzmannstr 3, D-85748 Garching, Germany	Technical University of Munich	Bergmann, P (corresponding author), MVTec Software GmbH, Arnulfstr 205, D-80634 Munich, Germany.; Bergmann, P (corresponding author), Tech Univ Munich, Dept Informat, Boltzmannstr 3, D-85748 Garching, Germany.	paul.bergmann@mvtec.com; kilian.batzner@mvtec.com; fauser@mvtec.com; sattlegger@mvtec.com; steger@mvtec.com		Steger, Carsten/0000-0003-3426-1703; Sattlegger, David/0000-0002-8336-4672; Bergmann, Paul/0000-0002-4458-3573; Batzner, Kilian/0000-0002-7312-7673				An J., 2015, SPEC LECT, V2, P1, DOI DOI 10.1007/BF00758335; Bailer C., 2017, BRIT MACH VIS C BMVC; Baur C, 2019, LECT NOTES COMPUT SC, V11383, P161, DOI 10.1007/978-3-030-11723-8_16; Bergmann P, 2021, INT J COMPUT VISION, V129, P1038, DOI 10.1007/s11263-020-01400-4; Bergmann P, 2020, PROC CVPR IEEE, P4182, DOI 10.1109/CVPR42600.2020.00424; Bergmann P, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P372, DOI 10.5220/0007364503720380; Bergmann P, 2019, PROC CVPR IEEE, P9584, DOI 10.1109/CVPR.2019.00982; Blum H, 2019, IEEE INT CONF COMP V, P2403, DOI 10.1109/ICCVW.2019.00294; Burlina P, 2019, PROC CVPR IEEE, P11499, DOI 10.1109/CVPR.2019.01177; Carrera D, 2017, IEEE T IND INFORM, V13, P551, DOI 10.1109/TII.2016.2641472; Cohen N., 2020, ARXIV200502357V1; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Ehret T, 2019, J MATH IMAGING VIS, V61, P710, DOI 10.1007/s10851-019-00885-0; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang YB, 2018, IEEE INT CON AUTO SC, P612, DOI 10.1109/COASE.2018.8560423; Gulrajani I, 2017, ADV NEUR IN, V30; Kang Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P360, DOI 10.1007/978-3-030-58565-5_22; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lis K, 2019, IEEE I CONF COMP VIS, P2152, DOI 10.1109/ICCV.2019.00224; Liu W., P IEEE CVF C COMP VI, P8642; Mackowiak R., 2018, TEXTITBRITISH MACH V, P121; Malevich, 1924, BLACK CIRCLE; Napoletano P, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010209; Pang GS, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3439950; Park H, 2020, IEEE INT C ELECTR TA; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schlegl T, 2019, MED IMAGE ANAL, V54, P30, DOI 10.1016/j.media.2019.01.010; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Song KC, 2013, APPL SURF SCI, V285, P858, DOI 10.1016/j.apsusc.2013.09.002; Steger C., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P148; Steger C., 2002, INT ARCH PHOTOGR REM, V34, P345; Steger C., 2018, MACHINE VISION ALGOR; Vasilev A., 2019, MICCAI 2019 INT WORK; Yoo D, 2019, PROC CVPR IEEE, P93, DOI 10.1109/CVPR.2019.00018	37	1	1	9	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					947	969		10.1007/s11263-022-01578-9	http://dx.doi.org/10.1007/s11263-022-01578-9		FEB 2022	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ		hybrid			2022-12-18	WOS:000759289300001
J	Liu, YL; Wang, YR; Wang, MN; Chen, G; Knoll, A; Song, ZJ				Liu, Yinlong; Wang, Yiru; Wang, Manning; Chen, Guang; Knoll, Alois; Song, Zhijian			Globally Optimal Linear Model Fitting with Unit-Norm Constraint	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Robust fitting; Global optimization; Unit-norm constraint	ROBUST; ALGORITHMS; CONSENSUS; OPTIMIZATION; MAXIMIZATION	Robustly fitting a linear model from outlier-contaminated data is an important and basic task in many scientific fields, and it is often tackled by consensus set maximization. There have been several studies on globally optimal methods for consensus set maximization, but most of them are currently confined to problems with small number of input observations and low outlier ratios. In this paper, we develop a globally optimal algorithm aiming at consensus set maximization to solve the robust linear model fitting problems with the unit-norm constraint, which is based on the branch-and-bound optimization framework. The unit-norm constraint is utilized to fix the unknown scale of linear model parameters, and we propose a compact representation of the unit-bounded searching domain to avoid introducing the additional non-linearity in the unit-norm constraint. The compact representation leads to a geometrically derived bound, which accelerates the calculation and enables the method to handle the problems with large number of observations. Experiments on both synthetic and real data show that the proposed algorithm outperforms existing globally optimal methods, especially in low dimensional problems with large number of input observations and high outlier ratios. The implementation of the source code is publicly available https://github.com/YiruWangYuri/Demo-for-GoCR.	[Liu, Yinlong; Knoll, Alois] Tech Univ Munich, Munich, Germany; [Wang, Yiru; Wang, Manning; Song, Zhijian] Fudan Univ, Digital Med Res Ctr, Sch Basic Med Sci, Shanghai, Peoples R China; [Wang, Yiru; Wang, Manning; Song, Zhijian] Shanghai Key Lab Med Image Comp & Comp Assisted I, Shanghai, Peoples R China; [Chen, Guang] Tongji Univ, Shanghai, Peoples R China	Technical University of Munich; Fudan University; Tongji University	Song, ZJ (corresponding author), Fudan Univ, Digital Med Res Ctr, Sch Basic Med Sci, Shanghai, Peoples R China.; Song, ZJ (corresponding author), Shanghai Key Lab Med Image Comp & Comp Assisted I, Shanghai, Peoples R China.; Chen, G (corresponding author), Tongji Univ, Shanghai, Peoples R China.	Yinlong.Liu@tum.de; yiruwang16@fudan.edu.cn; mnwang@fudan.edu.cn; guangchen@tongji.edu.cn; knoll@in.tum.de; zjsong@fudan.edu.cn			National Natural Science Foundation of China [62076070, 82072021]; Shanghai Municipal Science and Technology Major Project [2018SHZDZX01]; Shanghai Rising Star Program [21QC1400900]; European Union's Horizon 2020 Framework Programme for Research and Innovation [945539]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai Municipal Science and Technology Major Project; Shanghai Rising Star Program; European Union's Horizon 2020 Framework Programme for Research and Innovation	This work was supported by National Natural Science Foundation of China under Grant 82072021, by Shanghai Municipal Science and Technology Major Project (No. 2018SHZDZX01), by Shanghai Rising Star Program (No. 21QC1400900), by the European Union's Horizon 2020 Framework Programme for Research and Innovation under the Specic Grant Agreement No. 945539 (Human Brain Project SGA3) and by National Natural Science Foundation of China under Grant 62076070.	Adjiman CS, 1998, COMPUT CHEM ENG, V22, P1159, DOI 10.1016/S0098-1354(98)00218-X; Aftab K, 2015, IEEE T PATTERN ANAL, V37, P728, DOI 10.1109/TPAMI.2014.2353625; Barron JT, 2019, PROC CVPR IEEE, P4326, DOI 10.1109/CVPR.2019.00446; Bazin JC, 2013, IEEE T PATTERN ANAL, V35, P1565, DOI 10.1109/TPAMI.2012.264; Bazin JC, 2012, PROC CVPR IEEE, P638, DOI 10.1109/CVPR.2012.6247731; Benedek C, 2009, IEEE T IMAGE PROCESS, V18, P2303, DOI 10.1109/TIP.2009.2025808; Brachmann E, 2019, IEEE I CONF COMP VIS, P4321, DOI 10.1109/ICCV.2019.00442; Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267; Cai ZP, 2018, LECT NOTES COMPUT SC, V11216, P699, DOI 10.1007/978-3-030-01258-8_42; Cai ZP, 2019, IEEE I CONF COMP VIS, P1637, DOI 10.1109/ICCV.2019.00172; Campbell D, 2020, IEEE T PATTERN ANAL, V42, P328, DOI 10.1109/TPAMI.2018.2848650; Chin, 2017, MAXIMUM CONSENSUS PR; Chin TJ, 2018, LECT NOTES COMPUT SC, V11216, P715, DOI 10.1007/978-3-030-01258-8_43; Chin TJ, 2017, IEEE T PATTERN ANAL, V39, P758, DOI 10.1109/TPAMI.2016.2631531; Chin TJ, 2016, PROC CVPR IEEE, P5858, DOI 10.1109/CVPR.2016.631; Choi S., 1997, P BRIT MACH VIS C, V24, P271; Enqvist O, 2015, INT J COMPUT VISION, V112, P115, DOI 10.1007/s11263-014-0760-2; Enqvist O, 2012, LECT NOTES COMPUT SC, V7572, P738, DOI 10.1007/978-3-642-33718-5_53; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fredriksson J, 2016, IMAGE VISION COMPUT, V52, P114, DOI 10.1016/j.imavis.2016.05.011; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; Heller J, 2016, IEEE T PATTERN ANAL, V38, P1027, DOI 10.1109/TPAMI.2015.2469299; HOLLAND PW, 1977, COMMUN STAT A-THEOR, V6, P813, DOI 10.1080/03610927708827533; Ikami D, 2018, PROC CVPR IEEE, P8147, DOI 10.1109/CVPR.2018.00850; Inkila K., 2005, PHOTOGRAMMETRIC J FI, V19, P34; Joo K, 2019, IEEE T PATTERN ANAL, V41, P682, DOI 10.1109/TPAMI.2018.2799944; Le, 2017, DEV COMPOSITE ENERGY, P1; Le H, 2021, IEEE T PATTERN ANAL, V43, P842, DOI 10.1109/TPAMI.2019.2939307; Le HM, 2019, PROC CVPR IEEE, P124, DOI 10.1109/CVPR.2019.00021; Li HD, 2009, IEEE I CONF COMP VIS, P1074, DOI 10.1109/ICCV.2009.5459398; Liu YL, 2019, IEEE T IMAGE PROCESS, V28, P2599, DOI 10.1109/TIP.2018.2887207; Moore R.E., 2009, INTRO INTERVAL ANAL; Morrison DR, 2016, DISCRETE OPTIM, V19, P79, DOI 10.1016/j.disopt.2016.01.005; Oja E, 1996, NEURAL NETWORKS, V9, P435, DOI 10.1016/0893-6080(95)00071-2; Olsson C, 2008, PROC CVPR IEEE, P3230; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Ruckstuhl, 2014, LECT NOTES; Speciale P, 2017, PROC CVPR IEEE, P5048, DOI 10.1109/CVPR.2017.536; Tennakoon RB, 2016, IEEE T PATTERN ANAL, V38, P350, DOI 10.1109/TPAMI.2015.2448103; Tzoumas V, 2019, IEEE INT C INT ROBOT, P5383, DOI 10.1109/IROS40897.2019.8968174; WANG Y, 2021, PATTERN RECOGN, V115; Yang H, 2019, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2019.00175; Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405; Yinqiang Zheng, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1825, DOI 10.1109/CVPR.2011.5995640; Zach, 2017, BRIT MACH VIS C; Zach C., 2018, P EUR C COMP VIS ECC, P547; Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47	49	1	1	3	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					933	946		10.1007/s11263-022-01574-z	http://dx.doi.org/10.1007/s11263-022-01574-z		FEB 2022	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ					2022-12-18	WOS:000754927600001
J	Fontan, JG; Nayak, A; Briot, S; El Din, MS				Fontan, Jorge Garcia; Nayak, Abhilash; Briot, Sebastien; El Din, Mohab Safey			Singularity Analysis for the Perspective-Four and Five-Line Problems	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose estimation; Visual servoing; Singularity; PnL	POSE ESTIMATION; SERVO CONTROL	This paper deals with image-based visual servoing and pose estimation by observing four and five lines. Our main interest is to determine the relative configurations of the camera and the observed lines that lead to problems in control and stability. Since it is equivalent to finding the singularities of the corresponding Jacobian matrix, we use tools from computational algebraic geometry to seek configurations such that all of its minors vanish simultaneously. By choosing a suitable basis for this matrix, we revisit the problem in the case of three lines to show that one type of the singularities is when the camera lies on the hyperboloid of one sheet uniquely defined by the lines. This result is further exploited to prove that the one-dimensional singularities, if any, in the case of n lines appear when the camera lies on the transversals to the observed lines. Thus, by forcing the transversals to be complex, we can avoid the aforementioned type of singularities in the case of four lines although the algebra shows that there can always be up to 10 inevitable singular locations of the camera for the other type of singularity. For five lines, we find out that there are no singularities in the generic case. The singularities are also characterized for four and five lines with orthogonality and parallelism constraints. Furthermore, a visual servoing library is used to conduct some simulated experiments to substantiate the theoretical results. As expected, we observe problems in control in the vicinity of a singularity as well as increased errors in pose estimation.	[Fontan, Jorge Garcia] Sorbonne Univ, Equipe PolSys UMR 7606, LIP6, Paris, France; [Nayak, Abhilash; Briot, Sebastien] CNRS, Lab Sci Numer Nantes LS2N, UMR CNRS 6004, Nantes, France; [El Din, Mohab Safey] Sorbonne Univ, Equipe PolSys, LIP6, CNRS, Paris, France	UDICE-French Research Universities; Sorbonne Universite; Centre National de la Recherche Scientifique (CNRS); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	Fontan, JG (corresponding author), Sorbonne Univ, Equipe PolSys UMR 7606, LIP6, Paris, France.	Jorge.Garcia-Fontan@lip6.fr; Abhilash.Nayak@ls2n.fr; Sebastien.Briot@ls2n.fr; Mohab.Safey@lip6.fr			project PROMPT - RFI AtlanSTIC2020; French ANR project SESAME [ANR-18-CE33-0011]	project PROMPT - RFI AtlanSTIC2020; French ANR project SESAME(French National Research Agency (ANR))	This work was supported by the project PROMPT funded by the RFI AtlanSTIC2020, and partially funded by the French ANR project SESAME (funding ID: ANR-18-CE33-0011).	Andreff N, 2002, INT J ROBOT RES, V21, P679, DOI 10.1177/027836402761412430; Briot S, 2017, IEEE T ROBOT, V33, P536, DOI 10.1109/TRO.2016.2637912; Briot S, 2017, IEEE ROBOT AUTOM LET, V2, P412, DOI 10.1109/LRA.2016.2633975; Briot S, 2015, IEEE T ROBOT, V31, P1337, DOI 10.1109/TRO.2015.2489499; Briot S, 2013, IEEE INT CONF ROBOT, P4653, DOI 10.1109/ICRA.2013.6631239; Chaumette F., 2008, HDB ROBOTICS; Chaumette F., 1990, THESIS U RENNES; Chaumette F, 2007, IEEE ROBOT AUTOM MAG, V14, P109, DOI 10.1109/MRA.2007.339609; Chaumette F, 2006, IEEE ROBOT AUTOM MAG, V13, P82, DOI 10.1109/MRA.2006.250573; Cox David, 2013, IDEALS VARIETIES ALG; DHOME M, 1989, IEEE T PATTERN ANAL, V11, P1265, DOI 10.1109/34.41365; Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599; Horaud R, 1998, IEEE T ROBOTIC AUTOM, V14, P525, DOI 10.1109/70.704214; Hunt, 1987, KINEMATIC GEOMETRY M; Hutchinson S, 1996, IEEE T ROBOTIC AUTOM, V12, P651, DOI 10.1109/70.538972; Kanaan D, 2009, IEEE T ROBOT, V25, P995, DOI 10.1109/TRO.2009.2017132; Kneip L, 2011, PROC CVPR IEEE; Lazarus, 2014, BASIC ALGEBRAIC GEOM; Lilian Zhang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P217, DOI 10.1007/978-3-642-37431-9_17; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Marchand E, 2005, IEEE ROBOT AUTOM MAG, V12, P40, DOI 10.1109/MRA.2005.1577023; Marchand E, 2016, IEEE T VIS COMPUT GR, V22, P2633, DOI 10.1109/TVCG.2015.2513408; Marchand T, 2002, COMPUT GRAPH FORUM, V21, P289, DOI 10.1111/1467-8659.t01-1-00588; Michel H., 1993, SINGULARITIES DETERM; Odenhal B., 2020, UNIVERSE QUADRICS; Pascual-Escudero B, 2021, INT J COMPUT VISION, V129, P1217, DOI 10.1007/s11263-020-01420-0; Pottmann Helmut, 2001, MATH VISUAL, V2; Rieck MQ, 2014, J MATH IMAGING VIS, V48, P499, DOI 10.1007/s10851-013-0425-8; Rives P., 1987, ESTIMATION RECURSIVE; Rosenzveig V, 2013, IEEE INT C INT ROBOT, P430, DOI 10.1109/IROS.2013.6696387; Wang P, 2020, COMPUT VIS IMAGE UND, V191, DOI 10.1016/j.cviu.2018.08.005; Wang P, 2019, MACH VISION APPL, V30, P603, DOI 10.1007/s00138-019-01012-0; Wu YH, 2006, J MATH IMAGING VIS, V24, P131, DOI 10.1007/s10851-005-3617-z; Xu C, 2017, IEEE T PATTERN ANAL, V39, P1209, DOI 10.1109/TPAMI.2016.2582162; Zhang Cai-Xia, 2006, Acta Automatica Sinica, V32, P504	35	1	1	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					909	932		10.1007/s11263-021-01567-4	http://dx.doi.org/10.1007/s11263-021-01567-4		FEB 2022	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ		Green Submitted			2022-12-18	WOS:000758076300001
J	Shi, ZL; Mettes, P; Maji, S; Snoek, CGM				Shi, Zenglin; Mettes, Pascal; Maji, Subhransu; Snoek, Cees G. M.			On Measuring and Controlling the Spectral Bias of the Deep Image Prior	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Spectral bias; Deep image prior	LINEAR INVERSE PROBLEMS; NEURAL-NETWORKS; RESTORATION; STATISTICS; SPARSITY	The deep image prior showed that a randomly initialized network with a suitable architecture can be trained to solve inverse imaging problems by simply optimizing it's parameters to reconstruct a single degraded image. However, it suffers from two practical limitations. First, it remains unclear how to control the prior beyond the choice of the network architecture. Second, training requires an oracle stopping criterion as during the optimization the performance degrades after reaching an optimum value. To address these challenges we introduce a frequency-band correspondence measure to characterize the spectral bias of the deep image prior, where low-frequency image signals are learned faster and better than high-frequency counterparts. Based on our observations, we propose techniques to prevent the eventual performance degradation and accelerate convergence. We introduce a Lipschitz-controlled convolution layer and a Gaussian-controlled upsampling layer as plug-in replacements for layers used in the deep architectures. The experiments show that with these changes the performance does not degrade during optimization, relieving us from the need for an oracle stopping criterion. We further outline a stopping criterion to avoid superfluous computation. Finally, we show that our approach obtains favorable results compared to current approaches across various denoising, deblocking, inpainting, super-resolution and detail enhancement tasks. Code is available at https://github.com/shizenglin/Measure-and-Control-Spectral-Bias.	[Shi, Zenglin; Mettes, Pascal; Snoek, Cees G. M.] Univ Amsterdam, Amsterdam, Netherlands; [Maji, Subhransu] Univ Massachusetts, Amherst, MA 01003 USA	University of Amsterdam; University of Massachusetts System; University of Massachusetts Amherst	Shi, ZL (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	z.shi@uva.nl; P.S.M.Mettes@uva.nl; smaji@cs.umass.edu; cgmsnoek@uva.nl						Arias P, 2011, INT J COMPUT VISION, V93, P319, DOI 10.1007/s11263-010-0418-7; Arridge S, 2019, ACTA NUMER, V28, P1, DOI 10.1017/S0962492919000059; Asim M., 2019, NEURIPS WORKSH SOLV; Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487; Bertero Mario, 2020, INTRO INVERSE PROBLE, P2; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Chakrabarty P., 2019, NEURIPS WORKSH BAYES; Chen DD, 2020, IEEE T IMAGE PROCESS, V29, P8043, DOI 10.1109/TIP.2020.3009844; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Cheng ZZ, 2019, PROC CVPR IEEE, P5438, DOI 10.1109/CVPR.2019.00559; Crete F, 2007, PROC SPIE, V6492, DOI 10.1117/12.702790; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dai T., 2020, P 28 ACM INT C MULT; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong WS, 2015, INT J COMPUT VISION, V114, P217, DOI 10.1007/s11263-015-0808-y; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Elad M, 2010, P IEEE, V98, P972, DOI 10.1109/JPROC.2009.2037655; Engl H.W., 1996, REGULARIZATION INVER, DOI DOI 10.1007/978-94-009-1740-8; Foi A., 2006, 2006 14 EUR SIGN PRO, P1; Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128; Hahn J, 2011, INT J COMPUT VISION, V92, P308, DOI 10.1007/s11263-010-0371-5; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Heckel R., 2020, INT C LEARN REPR; Heckel Reinhard, 2019, INT C LEARN REPR; Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149; Ho Kary, 2020, ARXIV200104776; Jain V., 2008, NATURAL IMAGE DENOIS; Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099; KATSAGGELOS AK, 1989, OPT ENG, V28, P735, DOI 10.1117/12.7977030; Kattamis A., 2019, NEURIPS WORKSH SOLV; Katznelson Y., 2004, INTRO HARMONIC ANAL; Kindermann S, 2005, MULTISCALE MODEL SIM, V4, P1091, DOI 10.1137/050622249; Kingma D.P, P 3 INT C LEARNING R; Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lefkimmiatis S, 2018, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2018.00338; Li J., 2018, 2018 INT JOINT C NEU, P1; Lin ZC, 2008, INT J COMPUT VISION, V80, P406, DOI 10.1007/s11263-008-0148-2; Liu JM, 2019, INT CONF ACOUST SPEE, P7715, DOI 10.1109/ICASSP.2019.8682856; Lucas A, 2018, IEEE SIGNAL PROC MAG, V35, P20, DOI 10.1109/MSP.2017.2760358; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; Mao XJ, 2016, ADV NEUR IN, V29; Mataev G., 2019, ICCV WORKSH LEARN CO; Morishita K., 1988, US Patent, Patent No. [4,794,531, 4794531]; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Portilla J, 2009, IEEE IMAGE PROC, P3909, DOI 10.1109/ICIP.2009.5413975; Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067; Rahaman N, 2019, PR MACH LEARN RES, V97; Rasti B, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3067802; Ribes A, 2008, IEEE SIGNAL PROC MAG, V25, P84, DOI 10.1109/MSP.2008.923099; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959; Shi ZL, 2021, IEEE T IMAGE PROCESS, V30, P7472, DOI 10.1109/TIP.2021.3106812; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; TITTERINGTON DM, 1985, ASTRON ASTROPHYS, V144, P381; Ulyanov D, 2020, INT J COMPUT VISION, V128, P1867, DOI 10.1007/s11263-020-01303-4; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Vu T, 2021, PHOTOACOUSTICS, V22, DOI 10.1016/j.pacs.2021.100266; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xu ZQJ, 2020, COMMUN COMPUT PHYS, V28, P1746, DOI 10.4208/cicp.OA-2020-0085; Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zukerman J, 2021, EUR SIGNAL PR CONF, P675, DOI 10.23919/Eusipco47968.2020.9287540	73	1	1	6	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2022	130	4					885	908		10.1007/s11263-021-01572-7	http://dx.doi.org/10.1007/s11263-021-01572-7		FEB 2022	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZZ0UJ		Green Submitted, hybrid, Green Published			2022-12-18	WOS:000754128400001
J	Garces, E; Rodriguez-Pardo, C; Casas, D; Lopez-Moreno, J				Garces, Elena; Rodriguez-Pardo, Carlos; Casas, Dan; Lopez-Moreno, Jorge			A Survey on Intrinsic Images: Delving Deep into Lambert and Beyond	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Intrinsic images; Deep learning; Inverse rendering; Inverse material	OF-THE-ART; MODEL; REPRESENTATION; REFLECTION; DECOMPOSITION; RETINEX	Intrinsic imaging or intrinsic image decomposition has traditionally been described as the problem of decomposing an image into two layers: a reflectance, the albedo invariant color of the material; and a shading, produced by the interaction between light and geometry. Deep learning techniques have been broadly applied in recent years to increase the accuracy of those separations. In this survey, we overview those results in context of well-known intrinsic image data sets and relevant metrics used in the literature, discussing their suitability to predict a desirable intrinsic image decomposition. Although the Lambertian assumption is still a foundational basis for many methods, we show that there is increasing awareness on the potential of more sophisticated physically-principled components of the image formation process, that is, optically accurate material models and geometry, and more complete inverse light transport estimations. We classify these methods in terms of the type of decomposition, considering the priors and models used, as well as the learning architecture and methodology driving the decomposition process. We also provide insights about future directions for research, given the recent advances in neural, inverse and differentiable rendering techniques.	[Garces, Elena; Rodriguez-Pardo, Carlos; Casas, Dan; Lopez-Moreno, Jorge] SEDDI, Madrid, Spain; [Garces, Elena; Casas, Dan; Lopez-Moreno, Jorge] Univ Rey Juan Carlos, Madrid, Spain; [Rodriguez-Pardo, Carlos] Univ Carlos III, Madrid, Spain	Universidad Rey Juan Carlos; Universidad Carlos III de Madrid	Garces, E (corresponding author), SEDDI, Madrid, Spain.; Garces, E (corresponding author), Univ Rey Juan Carlos, Madrid, Spain.	elena.garces@urjc.es; carlos.rodriguezpardo.jimenez@gmail.com; dan.casas@urjc.es; jorge@jorg3.com		Rodriguez - Pardo, Carlos/0000-0001-6121-7738; Casas, Dan/0000-0002-3664-089X	Torres Quevedo Fellowship [PTQ2018-009868]; Spanish Ministry of Science [RTI2018-098694-B-I00]	Torres Quevedo Fellowship; Spanish Ministry of Science(Ministry of Science and Innovation, Spain (MICINN)Spanish Government)	Elena Garces was partially supported by a Torres Quevedo Fellowship (PTQ2018-009868). The work was also funded in part by the Spanish Ministry of Science (RTI2018-098694-B-I00 VizLearning).	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Azinovic D, 2019, PROC CVPR IEEE, P2442, DOI 10.1109/CVPR.2019.00255; Balzer J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2537, DOI 10.1109/CVPR.2011.5995346; Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Barrow H., 1978, COMPUT VIS SYST, V2, P2; Baslamisli AS, 2018, PROC CVPR IEEE, P6674, DOI 10.1109/CVPR.2018.00698; Beigpour S, 2011, IEEE I CONF COMP VIS, P327, DOI 10.1109/ICCV.2011.6126259; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Bi S., 2018, COMPUT GRAPH FORUM; Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Bonneel N, 2017, COMPUT GRAPH FORUM, V36, P593, DOI 10.1111/cgf.13149; Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476; Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, 10.48550/arXiv.2005.14165]; Brust C.-A., 2018, ARXIV180909875; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Carroll R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964938; Chaitanya CRA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073601; Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53; Chang J, 2014, LECT NOTES COMPUT SC, V8692, P704, DOI 10.1007/978-3-319-10593-2_46; Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37; Chen T, 2020, PR MACH LEARN RES, V119; Cheng LC, 2018, PROC CVPR IEEE, P656, DOI 10.1109/CVPR.2018.00075; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Deschaintre V, 2020, COMPUT GRAPH FORUM, V39, P91, DOI 10.1111/cgf.14056; Deschaintre V, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13765; Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378; Dong B, 2014, PROC CVPR IEEE, P2299, DOI 10.1109/CVPR.2014.294; Dong B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766979; Dong Y, 2019, VIS INFORM, V3, P59, DOI 10.1016/j.visinf.2019.07.003; Dong Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024180; Dosovitskiy A., 2020, ARXIV201011929; Duchene S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2756549; Erofeev M., 2015, BMVC; Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932; Finlayson GD, 2004, LECT NOTES COMPUT SC, V3023, P582; Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042; Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x; Gastal ESL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185529; Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397; Gidaris Spyros, 2018, ARXIV180307728; GODARD C, 2015, 3DV; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; Guarnera D, 2016, COMPUT GRAPH FORUM, V35, P625, DOI 10.1111/cgf.12867; Guo Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417779; Han X, 2019, IEEE CONF WIREL MOB; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; HORN BKP, 1979, APPL OPTICS, V18, P1770, DOI 10.1364/AO.18.001770; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jakob Wenzel, 2010, MITSUBA RENDERER; Janner M, 2017, ADV NEUR IN, V30; Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319; Jetley Saumya, 2018, INT C LEARN REPR; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902; Kanamori Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275104; Karis B., 2013, P PHYS BAS SHAD THEO, V4, P3, DOI DOI 10.1145/2504459; Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI DOI 10.1109/CVPR42600.2020.00813; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Kovacs B, 2017, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2017.97; Krahenbuhl P, 2018, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR.2018.00312; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laffont PY, 2015, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2015.57; Laffont PY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366221; Lafortune Eric P, 1994, USING MODIFIED PHONG; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lettry L, 2018, IEEE WINT CONF APPL, P1359, DOI 10.1109/WACV.2018.00153; Li TM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275109; Li X, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073641; Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI 10.1007/978-3-030-01240-3_21; Li ZQ, 2018, PROC CVPR IEEE, P9039, DOI 10.1109/CVPR.2018.00942; Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218; Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255; Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5; Li Zi-xin, 2018, Advanced Technology of Electrical Engineering and Energy, V37, P1, DOI 10.12067/ATEEE1712020; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu YF, 2020, PROC CVPR IEEE, P3245, DOI 10.1109/CVPR42600.2020.00331; Loubet G, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356510; Ma WC, 2018, LECT NOTES COMPUT SC, V11218, P211, DOI 10.1007/978-3-030-01264-9_13; Maxwell B. A., 2008, 2008 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2008.4587491; Meka A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323027; Meka A, 2018, PROC CVPR IEEE, P6315, DOI 10.1109/CVPR.2018.00661; Meka A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925907; Merzbach S., 2020, BONN APPEARANCE BENC; Mildenhall Ben, 2020, ECCV, P405, DOI [DOI 10.1007/978-3-030-58452-8_24, DOI 10.1007/978-3-030-58452-824]; Muller C., 2006, SPHERICAL HARMONICS, V17; Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342; Narihira T, 2015, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2015.7298915; Nestmeyer T, 2020, PROC CVPR IEEE, P5123, DOI 10.1109/CVPR42600.2020.00517; Nestmeyer T, 2017, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2017.192; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498; Oh BM, 2001, COMP GRAPH, P433; Omer I, 2004, PROC CVPR IEEE, P946; Patow G, 2003, COMPUT GRAPH FORUM, V22, P663, DOI 10.1111/j.1467-8659.2003.00716.x; Pesce, 2015, ACM SIGGRAPH 2015 CO; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Rematas K, 2016, PROC CVPR IEEE, P4508, DOI 10.1109/CVPR.2016.488; Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503; Ritschel T, 2012, COMPUT GRAPH FORUM, V31, P160, DOI 10.1111/j.1467-8659.2012.02093.x; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rother C., 2011, ADV NEURAL INFORM PR, P765; Sajjadi M. SM, 2020, ARXIV, P7210; Seitz SM, 2005, IEEE I CONF COMP VIS, P1440; Sengupta S, 2019, IEEE I CONF COMP VIS, P8597, DOI 10.1109/ICCV.2019.00869; Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659; Shen L, 2011, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2011.5995738; Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Sun TC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323008; Sunkavalli K, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239552, 10.1145/1276377.1276504]; Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185; Tewari A, 2020, COMPUT GRAPH FORUM, V39, P701, DOI 10.1111/cgf.14022; Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107; Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270; Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; TOMINAGA S, 1994, COLOR RES APPL, V19, P277, DOI 10.1002/col.5080190408; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Vidaurre R, 2019, IEEE WINT CONF APPL, P1347, DOI 10.1109/WACV.2019.00148; Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606; Xu WW, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531341; Yamaguchi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201364; Ye GZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601135; Yu Y, 2019, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2019.00327; Zhang H, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1541; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang YH, 2020, IEEE INT CONF AUTOMA, P356, DOI 10.1109/FG47880.2020.00134; Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77; Zhao Shuang., 2020, ACM SIGGRAPH 2020 CO; Zhou H, 2019, IEEE I CONF COMP VIS, P7819, DOI 10.1109/ICCV.2019.00791; Zhou H, 2019, IEEE I CONF COMP VIS, P7193, DOI 10.1109/ICCV.2019.00729; Zhou TH, 2015, IEEE I CONF COMP VIS, P3469, DOI 10.1109/ICCV.2015.396; Zollhofer M, 2018, COMPUT GRAPH FORUM, V37, P523, DOI 10.1111/cgf.13382; Zoran D, 2015, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2015.52	147	1	1	4	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					836	868		10.1007/s11263-021-01563-8	http://dx.doi.org/10.1007/s11263-021-01563-8		FEB 2022	33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI		Green Submitted			2022-12-18	WOS:000749450600005
J	Fan, JL; Zhang, J; Maybank, SJ; Tao, DC				Fan, Jinlong; Zhang, Jing; Maybank, Stephen J.; Tao, Dacheng			Wide-Angle Image Rectification: A Survey	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							LENS DISTORTION CORRECTION; FISH-EYE; CAMERA CALIBRATION; RADIAL DISTORTION; OMNIDIRECTIONAL CAMERAS; AUTOMATIC CALIBRATION; NONMETRIC CALIBRATION; CATADIOPTRIC CAMERAS; LINEAR-ESTIMATION; STRAIGHT-LINES	Wide field-of-view (FOV) cameras, which capture a larger scene area than narrow FOV cameras, are used in many applications including 3D reconstruction, autonomous driving, and video surveillance. However, wide-angle images contain distortions that violate the assumptions underlying pinhole camera models, resulting in object distortion, difficulties in estimating scene distance, area, and direction, and preventing the use of off-the-shelf deep models trained on undistorted images for downstream computer vision tasks. Image rectification, which aims to correct these distortions, can solve these problems. In this paper, we comprehensively survey progress in wide-angle image rectification from transformation models to rectification methods. Specifically, we first present a detailed description and discussion of the camera models used in different approaches. Then, we summarize several distortion models including radial distortion and projection distortion. Next, we review both traditional geometry-based image rectification methods and deep learning-based methods, where the former formulates distortion parameter estimation as an optimization problem and the latter treats it as a regression problem by leveraging the power of deep neural networks. We evaluate the performance of state-of-the-art methods on public datasets and show that although both kinds of methods can achieve good results, these methods only work well for specific camera models and distortion types. We also provide a strong baseline model and carry out an empirical study of different distortion models on synthetic datasets and real-world wide-angle images. Finally, we discuss several potential research directions that are expected to further advance this area in the future.	[Fan, Jinlong; Zhang, Jing; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia; [Maybank, Stephen J.] Univ London, Dept Comp Sci & Informat Syst, Birkbeck Coll, London, England	University of Sydney; University of London; Birkbeck University London	Tao, DC (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia.	jfan0939@uni.sydney.edu.au; jing.zhang1@sydney.edu.au; sjmaybank@dcs.bbk.ac.uk; dacheng.tao@sydney.edu.au			Australian Research Council [FL-170100117, IH-180100002, IC-190100031]	Australian Research Council(Australian Research Council)	This work was supported by Australian Research Council Projects FL-170100117, IH-180100002, IC-190100031.	Ahmed M, 2005, IEEE T IMAGE PROCESS, V14, P1215, DOI 10.1109/TIP.2005.846025; Ahmed M, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P157, DOI 10.1109/ICIP.2001.958448; Akinlar C, 2011, PATTERN RECOGN LETT, V32, P1633, DOI 10.1016/j.patrec.2011.06.001; Aleman-Flores Miguel, 2013, Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. 18th Iberoamerican Congress, CIARP 2013. Proceedings, LNCS 8258, P415, DOI 10.1007/978-3-642-41822-8_52; Aleman-Flores M, 2014, IMAGE PROCESS ON LIN, V4, P327, DOI 10.5201/ipol.2014.106; Aleman-Flores M, 2014, PATTERN RECOGN LETT, V36, P261, DOI 10.1016/j.patrec.2013.06.020; Antunes M, 2017, PROC CVPR IEEE, P6691, DOI 10.1109/CVPR.2017.708; Baker S, 1999, INT J COMPUT VISION, V35, P175, DOI 10.1023/A:1008128724364; Barreto JP, 2006, COMPUT VIS IMAGE UND, V103, P208, DOI 10.1016/j.cviu.2006.06.003; Barreto JP, 2005, IEEE I CONF COMP VIS, P625; BASU A, 1995, PATTERN RECOGN LETT, V16, P433, DOI 10.1016/0167-8655(94)00115-J; Benligiray B, 2016, EUR SIGNAL PR CONF, P938, DOI 10.1109/EUSIPCO.2016.7760386; Bermudez-Cameo J, 2015, INT J COMPUT VISION, V114, P16, DOI 10.1007/s11263-014-0792-7; Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Bogdan O, 2018, PROCEEDINGS CVMP 2018: THE 15TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION, DOI 10.1145/3278471.3278479; Brauer-Burchardt C, 2001, IEEE IMAGE PROC, P225, DOI 10.1109/ICIP.2001.958994; Brauer-Burchardt C., 2000, MUSTERERKENNUNG 2000, P187; BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855; Bukhari F, 2013, J MATH IMAGING VIS, V45, P31, DOI 10.1007/s10851-012-0342-2; Bukhari F, 2010, LECT NOTES COMPUT SC, V6454, P11, DOI 10.1007/978-3-642-17274-8_2; Byoung-Kwang Kim, 2010, 2010 International Conference on Audio, Language and Image Processing (ICALIP), P1693, DOI 10.1109/ICALIP.2010.5685158; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813; Carroll R, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778864; Carroll R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531349; Caruso D, 2015, IEEE INT C INT ROBOT, P141, DOI 10.1109/IROS.2015.7353366; Chang P, 2000, IEEE WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P127, DOI 10.1109/OMNVIS.2000.853819; Chao CH, 2020, INT CONF ACOUST SPEE, P2248, DOI 10.1109/ICASSP40776.2020.9054191; Chen Z, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01370-7; Cinaroglu I, 2016, SIGNAL IMAGE VIDEO P, V10, P413, DOI 10.1007/s11760-015-0768-2; Clarke TA, 1998, PHOTOGRAMM REC, V16, P51, DOI 10.1111/0031-868X.00113; Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174; Courbon J, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P1689; Cucchiara R, 2003, 12TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P182, DOI 10.1109/ICIAP.2003.1234047; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DEVERNAY F, 1995, P SOC PHOTO-OPT INS, V2567, P62, DOI 10.1117/12.218487; Devernay F, 2001, MACH VISION APPL, V13, P14, DOI 10.1007/PL00013269; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Eichenseer A, 2016, INT CONF ACOUST SPEE, P1541, DOI 10.1109/ICASSP.2016.7471935; Elharrouss O, 2020, NEURAL PROCESS LETT, V51, P2007, DOI 10.1007/s11063-019-10163-0; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; Fenna, 2006, CARTOGRAPHIC SCI COM; Fitzgibbon AW, 2001, PROC CVPR IEEE, P125; Fraser CS, 1997, ISPRS J PHOTOGRAMM, V52, P149, DOI 10.1016/S0924-2716(97)00005-1; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Geiger A., 2012, P IEEE COMP SOC C CO; Gennery D., 1979, P ARPA IUS WORKSHOP, P101; Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hartley RI, 2005, IEEE I CONF COMP VIS, P1834; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468; Huang K, 2018, PROC CVPR IEEE, P626, DOI 10.1109/CVPR.2018.00072; Hugemann W., 2010, CORRECTING LENS DIST; Hughes Ciaran, 2008, IET Irish Signals and Systems Conference. ISSC 2008, P162, DOI 10.1049/cp:20080656; Hughes C, 2010, APPL OPTICS, V49, P3338, DOI 10.1364/AO.49.003338; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jabar F, 2019, IEEE INT CON MULTI, P296, DOI 10.1109/ICME.2019.00059; Jabar F, 2017, IEEE INT SYM MULTIM, P53, DOI 10.1109/ISM.2017.18; Jiang SX, 2015, APPL OPTICS, V54, P4432, DOI 10.1364/AO.54.004432; Kakani V, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030894; Kanamori, 2013, P SPRING C COMP GRAP, P51; Kang SB, 2000, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2000.855820; Kannala J, 2004, INT C PATT RECOG, P10, DOI 10.1109/ICPR.2004.1333993; Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153; Khomutenko B, 2016, IEEE ROBOT AUTOM LET, V1, P137, DOI 10.1109/LRA.2015.2502921; Kim YW, 2017, IEEE I CONF COMP VIS, P4753, DOI 10.1109/ICCV.2017.508; Kingslake R., 1989, HIST PHOTOGRAPHIC LE; Kopf J, 2009, COMPUT GRAPH FORUM, V28, P1083, DOI 10.1111/j.1467-8659.2009.01485.x; Kukelova Z, 2011, IEEE T PATTERN ANAL, V33, P2410, DOI 10.1109/TPAMI.2011.86; Land MF, 2012, OXF ANIMAL BIOL SER, P1; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li H., 2005, 6 WORKSH OMN VIS CAM, V2, P7; Li XY, 2019, PROC CVPR IEEE, P4850, DOI 10.1109/CVPR.2019.00499; Liao K, 2020, IEEE T CIRC SYST VID, V30, P725, DOI 10.1109/TCSVT.2019.2897984; Liao K, 2020, IEEE J-STSP, V14, P222, DOI 10.1109/JSTSP.2019.2955017; Liao K, 2020, IEEE T IMAGE PROCESS, V29, P3707, DOI 10.1109/TIP.2020.2964523; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu XC, 2014, APPL OPTICS, V53, P7355, DOI 10.1364/AO.53.007355; Lopez-Antequera M, 2019, PROC CVPR IEEE, P11809, DOI 10.1109/CVPR.2019.01209; Lorincz SB, 2019, IEEE IJCNN; Mallon J, 2004, INT C PATT RECOG, P18, DOI 10.1109/ICPR.2004.1333995; Markovic I, 2014, IEEE INT CONF ROBOT, P5630, DOI 10.1109/ICRA.2014.6907687; Matsuki H, 2018, IEEE ROBOT AUTOM LET, V3, P3693, DOI 10.1109/LRA.2018.2855443; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; Mei C, 2007, IEEE INT CONF ROBOT, P3945, DOI 10.1109/ROBOT.2007.364084; MIYAMOTO K, 1964, J OPT SOC AM, V54, P1060, DOI 10.1364/JOSA.54.001060; Nayar SK, 1997, PROC CVPR IEEE, P482, DOI 10.1109/CVPR.1997.609369; Neumann J, 2002, THIRD WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P19, DOI 10.1109/OMNVIS.2002.1044486; Paya L, 2017, J SENSORS, V2017, DOI 10.1155/2017/3497650; Philbin J, 2007, PROC CVPR IEEE, P1545; Posada LF, 2010, IEEE INT C INT ROBOT, P804, DOI 10.1109/IROS.2010.5652869; Prescott B, 1997, GRAPH MODEL IM PROC, V59, P39, DOI 10.1006/gmip.1996.0407; Pritts J, 2020, INT J COMPUT VISION, V128, P950, DOI 10.1007/s11263-019-01216-x; Puig L, 2012, COMPUT VIS IMAGE UND, V116, P120, DOI 10.1016/j.cviu.2011.08.003; Ray S. F., 2002, APPL PHOTOGRAPHIC OP, DOI 10.4324/9780080499253; Rituerto Alejandro, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P348, DOI 10.1109/ICPR.2010.94; Rong Jiangpeng, 2016, ACCV; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Royer E, 2007, INT J COMPUT VISION, V74, P237, DOI 10.1007/s11263-006-0023-y; Sacht, 2010, THESIS NATL I PURE A; Sacht L., 2011, P ACM SIGGRAPH; Santana-Cedres D, 2016, IMAGE PROCESS ON LIN, V6, P326, DOI 10.5201/ipol.2016.130; Santana-Cedres D, 2015, SIAM J IMAGING SCI, V8, P1574, DOI 10.1137/151006044; Scaramuzza, 2014, COMPUTER VISION REFE; Scaramuzza Davide, 2006, IEEE INT C COMP VIS; SHAH S, 1994, IEEE INT CONF ROBOT, P3422, DOI 10.1109/ROBOT.1994.351044; Shah S, 1996, PATTERN RECOGN, V29, P1775, DOI 10.1016/0031-3203(96)00038-6; Shih YC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322948; SIDAHMED MA, 1990, IEEE T INSTRUM MEAS, V39, P512, DOI 10.1109/19.106283; Snyder JP, 1997, FLATTENING EARTH 200; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Steele RM, 2006, LECT NOTES COMPUT SC, V3951, P253; Stein GP, 1997, PROC CVPR IEEE, P602, DOI 10.1109/CVPR.1997.609387; Stevenson DE, 1996, THIRD IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV '96, PROCEEDINGS, P214, DOI 10.1109/ACV.1996.572058; Strand, 2005, PROC BRIT MACH VIS C; Sturm P, 2008, LECT NOTES COMPUT SC, V5305, P609, DOI 10.1007/978-3-540-88693-8_45; Sturm P, 2010, FOUND TRENDS COMPUT, V6, P1, DOI 10.1561/0600000023; Swaminathan R, 2000, IEEE T PATTERN ANAL, V22, P1172, DOI 10.1109/34.879797; Swaminathan R, 2006, INT J COMPUT VISION, V66, P211, DOI 10.1007/s11263-005-3220-1; Tang, 2012, SELF CONSISTENCY UNI; Thormahlen T., 2003, MIRAGE 2003, P105; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; Urban S, 2015, ISPRS J PHOTOGRAMM, V108, P72, DOI 10.1016/j.isprsjprs.2015.06.005; Usenko V, 2018, INT CONF 3D VISION, P552, DOI 10.1109/3DV.2018.00069; Vernon, P EUR C COMP VIS, V1843, P445; von Gioi RG, 2012, IMAGE PROCESS ON LIN, V2, P35, DOI 10.5201/ipol.2012.gjmr-lsd; Wang AQ, 2009, J MATH IMAGING VIS, V35, P165, DOI 10.1007/s10851-009-0162-1; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166; Wei J, 2012, IEEE T VIS COMPUT GR, V18, P1771, DOI 10.1109/TVCG.2011.130; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P965, DOI 10.1109/34.159901; Wildenauer H, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.106; Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991; Xue, 2020, ARXIV200311386CS; Xue ZC, 2019, PROC CVPR IEEE, P1643, DOI 10.1109/CVPR.2019.00174; YAGI Y, 1994, IEEE T ROBOTIC AUTOM, V10, P11, DOI 10.1109/70.285581; Yagi Y, 1999, IEICE T INF SYST, VE82D, P568; Yagi Y., 1990, Proceedings. IROS '90. IEEE International Workshop on Intelligent Robots and Systems '90. Towards a New Frontier of Applications (Cat. No.90TH0332-7), P181, DOI 10.1109/IROS.1990.262385; Yan Zhang, 2013, International Journal of Information Technology and Computer Science, V5, P13, DOI 10.5815/ijitcs.2013.03.02; Yang S, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P363, DOI 10.1109/ROBIO.2016.7866349; Yang SR, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102692; Yang WY, 2018, INT C PATT RECOG, P2190, DOI 10.1109/ICPR.2018.8546070; Yang Y., 2020, ARXIV PREPRINT ARXIV; Yin XQ, 2018, LECT NOTES COMPUT SC, V11214, P475, DOI 10.1007/978-3-030-01249-6_29; Ying XG, 2004, LECT NOTES COMPUT SC, V3021, P442; Ying XH, 2015, LECT NOTES COMPUT SC, V9003, P384, DOI 10.1007/978-3-319-16865-4_25; Yuanyuan Shi, 2018, 2018 IEEE Power & Energy Society General Meeting (PESGM), DOI 10.1109/PESGM.2018.8586227; Zhang M, 2015, PROC CVPR IEEE, P4137, DOI 10.1109/CVPR.2015.7299041; Zhang X, 2015, AER ADV ENG RES, V13, P231; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zhengyou Zhang, 1996, Proceedings of the 13th International Conference on Pattern Recognition, P407, DOI 10.1109/ICPR.1996.546059; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zorin D., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P257, DOI 10.1145/218380.218449	162	1	1	11	42	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					747	776		10.1007/s11263-021-01562-9	http://dx.doi.org/10.1007/s11263-021-01562-9		JAN 2022	30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI		Green Submitted			2022-12-18	WOS:000748453700002
J	He, P; Emami, P; Ranka, S; Rangarajan, A				He, Pan; Emami, Patrick; Ranka, Sanjay; Rangarajan, Anand			Learning Scene Dynamics from Point Cloud Sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D deep learning; Scene dynamics; Point cloud processing; Scene flow estimation; Spatiotemporal learning; Self-supervised learning	FLOW ESTIMATION; ALGORITHM; REGISTRATION	Understanding 3D scenes is a critical prerequisite for autonomous agents. Recently, LiDAR and other sensors have made large amounts of data available in the form of temporal sequences of point cloud frames. In this work, we propose a novel problem-sequential scene flow estimation (SSFE)-that aims to predict 3D scene flow for all pairs of point clouds in a given sequence. This is unlike the previously studied problem of scene flow estimation which focuses on two frames. We introduce the SPCM-Net architecture, which solves this problem by computing multi-scale spatiotemporal correlations between neighboring point clouds and then aggregating the correlation across time with an order-invariant recurrent unit. Our experimental evaluation confirms that recurrent processing of point cloud sequences results in significantly better SSFE compared to using only two frames. Additionally, we demonstrate that this approach can be effectively modified for sequential point cloud forecasting (SPF), a related problem that demands forecasting future point cloud frames. Our experimental results are evaluated using a new benchmark for both SSFE and SPF consisting of synthetic and real datasets. Previously, datasets for scene flow estimation have been limited to two frames. We provide non-trivial extensions to these datasets for multi-frame estimation and prediction. Due to the difficulty of obtaining ground truth motion for real-world datasets, we use self-supervised training and evaluation metrics. We believe that this benchmark will be pivotal to future research in this area. All code for benchmark and models will be made accessible at (https://github.com/BestSonny/SPCM).	[He, Pan; Emami, Patrick; Ranka, Sanjay; Rangarajan, Anand] Univ Florida, Dept Comp & Informat Sci & Engn, Modern Artificial Intelligence & Learning Technol, Gainesville, FL 32611 USA	State University System of Florida; University of Florida	He, P; Rangarajan, A (corresponding author), Univ Florida, Dept Comp & Informat Sci & Engn, Modern Artificial Intelligence & Learning Technol, Gainesville, FL 32611 USA.	pan.he@ufl.edu; pemami@ufl.edu; ranka@cise.ufl.edu; anand@cise.ufl.edu	He, Pan/AEQ-3282-2022	He, Pan/0000-0002-6525-6299	NSF [CNS 1922782]; Florida Dept. of Transportation (FDOT); FDOT District 5	NSF(National Science Foundation (NSF)); Florida Dept. of Transportation (FDOT); FDOT District 5	This work is supported by NSF CNS 1922782, by the Florida Dept. of Transportation (FDOT) and FDOT District 5. The opinions, findings and conclusions expressed in this publication are those of the author(s) and not necessarily those of the Florida Department of Transportation or the National Science Foundation.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Achlioptas P, 2018, PR MACH LEARN RES, V80; Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Barrow HG, 1977, P 5 INT JOINT C ART; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bobenko AI, 2007, DISCRETE COMPUT GEOM, V38, P740, DOI 10.1007/s00454-007-9006-1; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53; Chang MF, 2019, PROC CVPR IEEE, P8740, DOI 10.1109/CVPR.2019.00895; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Chui HL, 2000, PROC CVPR IEEE, P44, DOI 10.1109/CVPR.2000.854733; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193; Emami P, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3394659; FAN H, 2019, P 33 C ART INT; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Fan HH, 2021, PROC CVPR IEEE, P14199, DOI 10.1109/CVPR46437.2021.01398; Fan Hehe, 2019, ARXIV191008287; Fan Hehe, 2021, INT C LEARN REPR; Fitzgibbon AW, 2003, IMAGE VISION COMPUT, V21, P1145, DOI 10.1016/j.imavis.2003.09.004; GAIDON A, 2016, PROC CVPR IEEE, P4340, DOI DOI 10.1109/CVPR.2016.470; Geiger A., 2012, P IEEE COMP SOC C CO; Gojcic Z, 2021, PROC CVPR IEEE, P5688, DOI 10.1109/CVPR46437.2021.00564; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Graves A, 2012, STUD COMPUT INTELL, V385, P5; Gu XY, 2019, PROC CVPR IEEE, P3249, DOI 10.1109/CVPR.2019.00337; Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434; Gwak JunYoung, 2020, EUR C COMP VIS; Haotian* Tang Zhijian*, 2020, EUR C COMP VIS; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156; Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455; Huguet F, 2007, IEEE I CONF COMP VIS, P1342, DOI 10.1109/iccv.2007.4409000; Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710; Karpathy A, 2013, IEEE INT CONF ROBOT, P2088, DOI 10.1109/ICRA.2013.6630857; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479; Lee H, 2009, P 26 ANN INT C MACH, V26, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]; Lee MJ, 2019, INT CONF IMAG VIS; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Li RB, 2021, PROC CVPR IEEE, P15572, DOI 10.1109/CVPR46437.2021.01532; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Li YY, 2018, ADV NEUR IN, V31; Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873; Liu XY, 2019, IEEE I CONF COMP VIS, P9245, DOI 10.1109/ICCV.2019.00934; Liu XY, 2019, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2019.00062; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Menze Moritz, 2015, CVPR; Mittal Himangi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11174, DOI 10.1109/CVPR42600.2020.01119; Moenning C., 2003, FAST MARCHING FARTHE; Niemeyer M, 2019, IEEE I CONF COMP VIS, P5378, DOI 10.1109/ICCV.2019.00548; Ouyang B., 2020, ARXIV PREPRINT ARXIV; Pan YC, 2020, IEEE INT VEH SYM, P687; Paszke A, 2019, ADV NEUR IN, V32; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Pons JP, 2007, INT J COMPUT VISION, V72, P179, DOI 10.1007/s11263-006-8671-5; PONTES JK, 2020, ARXIV PREPRINT ARXIV; Prantl L., 2019, ARXIV PREPRINT ARXIV; Puy Gilles, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P527, DOI 10.1007/978-3-030-58604-1_32; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Pham QH, 2019, PROC CVPR IEEE, P8819, DOI 10.1109/CVPR.2019.00903; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Rempe Davis, 2020, ADV NEURAL INFORM PR, V3, P4; Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312; Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054; Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478; Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086; Shi XJ, 2015, ADV NEUR IN, V28; SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Teed Zachary, 2020, ECCV, DOI DOI 10.1007/978-3-030-58536-5_24; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Vedula S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P722, DOI 10.1109/ICCV.1999.790293; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272; Wang XW, 2021, IEEE SYST J, V15, P3881, DOI 10.1109/JSYST.2020.2997050; Wang YC, 2020, PROC CVPR IEEE, P508, DOI 10.1109/CVPR42600.2020.00059; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wang YB, 2018, PR MACH LEARN RES, V80; Wang YB, 2017, ADV NEUR IN, V30; Wang Yunbo, 2019, ICLR; Wang ZR, 2020, IEEE WINT CONF APPL, P91, DOI 10.1109/WACV45572.2020.9093302; Weng X., 2020, C ROB LEARN; Wu BC, 2018, IEEE INT CONF ROBOT, P1887; Wu P.X., 2020, P IEEE CVF C COMP VI, P11385, DOI DOI 10.1109/CVPR42600.2020.01140; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; Wu Wenxuan, 2020, P EUROPEAN C COMPUTE, P88; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Yang B, 2019, ADV NEUR IN, V32; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; YAO S, 2018, ARXIV PREPRINT ARXIV; Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407; Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295; Zhang C, 2019, PROC CVPR IEEE, P5312, DOI 10.1109/CVPR.2019.00546; Zhao L, 2020, AAAI CONF ARTIF INTE, V34, P12951; Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_; Zuanazzi V, 2020, INT CONF 3D VISION, P1049, DOI 10.1109/3DV50981.2020.00115	120	1	1	7	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					669	695		10.1007/s11263-021-01551-y	http://dx.doi.org/10.1007/s11263-021-01551-y		JAN 2022	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI		Green Submitted			2022-12-18	WOS:000745601600001
J	Chen, HH; Wei, ZY; Li, XZ; Xu, YB; Wei, MQ; Wang, J				Chen, Honghua; Wei, Zeyong; Li, Xianzhi; Xu, Yabin; Wei, Mingqiang; Wang, Jun			RePCD-Net: Feature-Aware Recurrent Point Cloud Denoising Network	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Point cloud; 3D deep learning; RNN; Multi-scale feature learning; Geometric feature preservation		The captured 3D point clouds by depth cameras and 3D scanners are often corrupted by noise, so point cloud denoising is typically required for downstream applications. We observe that: (i) the scale of the local neighborhood has a significant effect on the denoising performance against different noise levels, point intensities, as well as various kinds of local details; (ii) non-iteratively evolving a noisy input to its noise-free version is non-trivial; (iii) both traditional geometric methods and learning-based methods often lose geometric features with denoising iterations, and (iv) most objects can be regarded as piece-wise smooth surfaces with a small number of features. Motivated by these observations, we propose a novel and task-specific point cloud denoising network, named RePCD-Net, which consists of four key modules: (i) a recurrent network architecture to effectively remove noise; (ii) an RNN-based multi-scale feature aggregation module to extract adaptive features in different denoising stage; (iii) a recurrent propagation layer to enhance the geometric feature perception across stages; and (iv) a feature-aware CD loss to regularize the predictions towards multi-scale geometric details. Extensive qualitative and quantitative evaluations demonstrate the effectiveness and superiority of our method over state-of-the-arts, in terms of noise removal and feature preservation.	[Chen, Honghua; Wei, Zeyong; Xu, Yabin; Wei, Mingqiang; Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Nanjing, Peoples R China; [Wei, Mingqiang] Shenzhen Res Inst, Shenzhen, Peoples R China; [Li, Xianzhi] Huazhong Univ Sci & Technol, Wuhan, Peoples R China	Nanjing University of Aeronautics & Astronautics; Huazhong University of Science & Technology	Wang, J (corresponding author), Nanjing Univ Aeronaut & Astronaut, Nanjing, Peoples R China.	chenhonghuacn@gmail.com; wjun@nuaa.edu.cn		Honghua, Chen/0000-0001-7473-1146	Hong Kong Centre for Logistics Robotics	Hong Kong Centre for Logistics Robotics	The work was supported by the Hong Kong Centre for Logistics Robotics.	Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093; [Anonymous], 2018, ARXIV180306783; Avron H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857911; Chen HH, 2020, IEEE T VIS COMPUT GR, V26, P3255, DOI 10.1109/TVCG.2019.2920817; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Digne J, 2018, IEEE T VIS COMPUT GR, V24, P2238, DOI 10.1109/TVCG.2017.2719024; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227; Gersho A., 1992, Vector quantization and signal compression; Guo YL, 2015, IEEE T INSTRUM MEAS, V64, P683, DOI 10.1109/TIM.2014.2358131; Hermosilla P, 2019, IEEE I CONF COMP VIS, P52, DOI 10.1109/ICCV.2019.00014; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu W, 2020, IEEE T SIGNAL PROCES, V68, P2841, DOI 10.1109/TSP.2020.2978617; Huang, 2020, ARXIV200306631; Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913; Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522; Kong DM, 2014, IEEE T INSTRUM MEAS, V63, P1200, DOI 10.1109/TIM.2013.2292310; Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730; Li XZ, 2021, IEEE T VIS COMPUT GR, V27, P4060, DOI 10.1109/TVCG.2020.3001681; Lipman Y, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276405, 10.1145/1239451.1239473]; Liu XH, 2019, AAAI CONF ARTIF INTE, P8778; Lu DN, 2020, COMPUT AIDED DESIGN, V125, DOI 10.1016/j.cad.2020.102860; Lu XQ, 2018, IEEE T VIS COMPUT GR, V24, P2315, DOI 10.1109/TVCG.2017.2725948; Luo ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1330, DOI 10.1145/3394171.3413727; Nguyen CV, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P524, DOI 10.1109/3DIMPVT.2012.84; Oztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x; Pistilli Francesca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P103, DOI 10.1007/978-3-030-58565-5_7; Preiner R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601172; Qi CR, 2017, ADV NEUR IN, V30; Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753; Remil O, 2017, COMPUT AIDED DESIGN, V88, P31, DOI 10.1016/j.cad.2017.04.004; Rosman G, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12139; Roveri R, 2018, COMPUT GRAPH FORUM, V37, P87, DOI 10.1111/cgf.13344; Serna Andres, 2014, 3rd International Conference on Pattern Recognition Applications and Methods (ICPRAM 2014). Proceedings, P819; Sun YJ, 2015, COMPUT AIDED GEOM D, V35-36, P2, DOI 10.1016/j.cagd.2015.03.011; Vaswani A, 2017, ADV NEUR IN, V30; Wang PS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980232; Wang Y., 2020, IEEE T INSTRUM MEAS, V70, P1, DOI [10.1109/TIM.2020.3044719, DOI 10.1109/TIM.2020.3044719]; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wei MQ, 2021, IEEE T VIS COMPUT GR, V27, P4469, DOI 10.1109/TVCG.2020.3005424; Xie Q, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3028399; Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295; Yu LQ, 2018, LECT NOTES COMPUT SC, V11211, P398, DOI 10.1007/978-3-030-01234-2_24; Zhang D, 2020, IEEE T VIS COMPUT GR; Zhou H, 2019, IEEE I CONF COMP VIS, P1961, DOI 10.1109/ICCV.2019.00205	46	1	1	13	26	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2022	130	3					615	629		10.1007/s11263-021-01564-7	http://dx.doi.org/10.1007/s11263-021-01564-7		JAN 2022	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZI0UI					2022-12-18	WOS:000744409500001
J	Cui, Y; Guo, DY; Shao, YY; Wang, ZH; Shen, CH; Zhang, LY; Chen, SY				Cui, Ying; Guo, Dongyan; Shao, Yanyan; Wang, Zhenhua; Shen, Chunhua; Zhang, Liyan; Chen, Shengyong			Joint Classification and Regression for Visual Tracking with Fully Convolutional Siamese Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Fully convolutional networks; Anchor-free object detection; Visual tracking	CORRELATION FILTER TRACKER; SCALE	Visual tracking of generic objects is one of the fundamental but challenging problems in computer vision. Here, we propose a novel fully convolutional Siamese network to solve visual tracking by directly predicting the target bounding box in an end-to-end manner. We first reformulate the visual tracking task as two subproblems: a classification problem for pixel category prediction and a regression task for object status estimation at this pixel. With this decomposition, we design a simple yet effective Siamese architecture based classification and regression framework, termed SiamCAR, which consists of two subnetworks: a Siamese subnetwork for feature extraction and a classification-regression subnetwork for direct bounding box prediction. Since the proposed framework is both proposal- and anchor-free, SiamCAR can avoid the tedious hyper-parameter tuning of anchors, considerably simplifying the training. To demonstrate that a much simpler tracking framework can achieve superior tracking results, we conduct extensive experiments and comparisons with state-of-the-art trackers on a few challenging benchmarks. Without bells and whistles, SiamCAR achieves leading performance with a real-time speed. Furthermore, the ablation study validates that the proposed framework is effective with various backbone networks, and can benefit from deeper networks.	[Cui, Ying; Guo, Dongyan; Shao, Yanyan; Wang, Zhenhua] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China; [Shen, Chunhua] Zhejiang Univ, Hangzhou, Peoples R China; [Zhang, Liyan] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China; [Chen, Shengyong] Tianjin Univ Technol, Sch Comp Sci & Engn, Tianjin, Peoples R China	Zhejiang University of Technology; Zhejiang University; Nanjing University of Aeronautics & Astronautics; Tianjin University of Technology	Guo, DY (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.	cuiying@zjut.edu.cn; guodongyan@zjut.edu.cn; 2111912036@zjut.edu.cn; zhhwang@zjut.edu.cn; chunhua@me.com; zhangliyan@nuaa.edu.cn; sy@ieee.org	Chen, S./H-3083-2011	Cui, Ying/0000-0002-8209-2679	National Natural Science Foundation of China [62102364, 62002325, 61802348, 61772268]; Natural Science Foundation of Jiangsu Province [BK20190065]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Jiangsu Province(Natural Science Foundation of Jiangsu Province)	This work is supported in part by the National Natural Science Foundation of China (Grant No. 62102364, 62002325, 61802348, 61772268) and the Natural Science Foundation of Jiangsu Province (Grant No. BK20190065). CS and his employer received no financial support for the research, authorship, and/or publication of this article.	Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928; Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29; Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490; Danelljan Martin, 2014, BRIT MACH VIS C NOTT; Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28; Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667; Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814; Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552; Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI 10.1109/ICCV.2017.129; Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478; Guo DF, 2020, IEEE C EVOL COMPUTAT; Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196; He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Huan L., 2015, ARXIV PREPRINT ARXIV; Huang DF, 2017, INT J COMPUT VISION, V122, P524, DOI 10.1007/s11263-016-0974-6; Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464; Kristan M., 2021, P EUR C COMP VIS, P547; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441; Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935; Li F, 2017, IEEE INT CONF COMP V, P2001, DOI 10.1109/ICCVW.2017.234; Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18; LianghuaHuang K. H., 2020, GLOBALTRACK SIMPLE S; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu T., 2016, IEEE T CIRC SYST VID; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716; Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3; Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI 10.1109/TPAMI.2018.2865311; Ma C, 2018, INT J COMPUT VISION, V126, P771, DOI 10.1007/s11263-018-1076-4; Muller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19; Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27; NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465; Nam H., 2016, ARXIV160807242; Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823; Pu S, 2018, ADV NEUR IN, V31; Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sui Y, 2019, INT J COMPUT VISION, V127, P1084, DOI 10.1007/s11263-019-01156-6; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Tian Z, 2019, PROC CVPR IEEE, P3121, DOI 10.1109/CVPR.2019.00324; Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531; Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376; Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Xingping Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P378, DOI 10.1007/978-3-030-58565-5_23; Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28; Yang TY, 2020, PROC CVPR IEEE, P6717, DOI 10.1109/CVPR42600.2020.00675; Yu Jiahui., 2016, ACM MM, DOI DOI 10.1145/2964284.2967274; Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang TY, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP); Zhou X., 2019, ARXIV; Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710	66	1	1	5	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					550	566		10.1007/s11263-021-01559-4	http://dx.doi.org/10.1007/s11263-021-01559-4		JAN 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		hybrid			2022-12-18	WOS:000739783300001
J	Qiao, XT; Zheng, QL; Cao, Y; Lau, RWH				Qiao, Xiaotian; Zheng, Quanlong; Cao, Ying; Lau, Rynson W. H.			Instance-Aware Scene Layout Forecasting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene layout; Instance interaction; Layout forecasting; Scene understanding		Forecasting scene layout is of vital importance in many vision applications, e.g., enabling autonomous vehicles to plan actions early. It is a challenging problem as it involves understanding of the past scene layouts and the diverse object interactions in the scene, and then forecasting what the scene will look like at a future time. Prior works learn a direct mapping from past pixels to future pixel-wise labels and ignore the underlying object interactions in the scene, resulting in temporally incoherent and averaged predictions. In this paper, we propose a learning framework to forecast semantic scene layouts (represented by instance maps) from an instance-aware perspective. Specifically, our framework explicitly models the dynamics of individual instances and captures their interactions in a scene. Under this formulation, we are able to enforce instance-level constraints to forecast scene layouts by effectively reasoning about their spatial and semantic relations. Experimental results show that our model can predict sharper and more accurate future instance maps than the baselines and prior methods, yielding state-of-the-art performances on short-term, mid-term and long-term scene layout forecasting.	[Qiao, Xiaotian] Xidian Univ, Xian, Peoples R China; [Qiao, Xiaotian; Zheng, Quanlong; Cao, Ying; Lau, Rynson W. H.] City Univ Hong Kong, Hong Kong, Peoples R China	Xidian University; City University of Hong Kong	Qiao, XT (corresponding author), Xidian Univ, Xian, Peoples R China.; Qiao, XT; Lau, RWH (corresponding author), City Univ Hong Kong, Hong Kong, Peoples R China.	qiaoxt1992@gmail.com; xiaolong921001@gmail.com; caoying59@gmail.com; Rynson.Lau@cityu.edu.hk	Qiao, Xiaotian/ABB-7324-2022	Qiao, Xiaotian/0000-0002-5351-8335; Zheng, Quanlong/0000-0001-5059-0078	Research Grants Council of Hong Kong [11205620]	Research Grants Council of Hong Kong(Hong Kong Research Grants Council)	This work was supported by the Research Grants Council of Hong Kong (Grant No. 11205620).	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dosovitskiy Alexey, 2017, INT C LEARN REPR ICL; Ehrig M., 2005, IN PROC K CAP 2005 W, P25; Gammulle H, 2019, IEEE I CONF COMP VIS, P5561, DOI 10.1109/ICCV.2019.00566; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graber C., 2021, ARXIV210403962; Guan JQ, 2020, PROC CVPR IEEE, P170, DOI 10.1109/CVPR42600.2020.00025; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hu Anthony, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P767, DOI 10.1007/978-3-030-58517-4_45; Iannucci S, 2016, IEEE IC COMP COM NET; Jaderberg M, 2015, ADV NEUR IN, V28; Jin XJ, 2017, IEEE I CONF COMP VIS, P5581, DOI 10.1109/ICCV.2017.595; Jin Xiaojie, 2017, NEURIPS, P2; Kim W., 2020, ARXIV201106788; King DB, 2015, ACS SYM SER, V1214, P1; Kwon YH, 2019, PROC CVPR IEEE, P1811, DOI 10.1109/CVPR.2019.00191; Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Luc P, 2017, IEEE I CONF COMP VIS, P648, DOI 10.1109/ICCV.2017.77; Luc Pauline, 2018, ECCV, P584, DOI DOI 10.1007/978-3-030-01240-3_36; Mathieu Michael, 2016, ICLR; Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3; Qi XJ, 2019, PROC CVPR IEEE, P7665, DOI 10.1109/CVPR.2019.00786; Qiao XT, 2019, PROC CVPR IEEE, P2628, DOI 10.1109/CVPR.2019.00274; Ranzato MarcAurelio, 2014, ARXIV14126604; Rochan M., 2018, BMVC; Saric J, 2019, LECT NOTES COMPUT SC, V11824, P189, DOI 10.1007/978-3-030-33676-9_13; Saric Josip, 2020, CVPR, P10648; Shalev-Shwartz S, 2016, ARXIV PREPRINT ARXIV; Vondrick C, 2017, PROC CVPR IEEE, P2992, DOI 10.1109/CVPR.2017.319; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281; Yang LW, 2019, IEEE I CONF COMP VIS, P42, DOI 10.1109/ICCV.2019.00013; Yuen J, 2010, LECT NOTES COMPUT SC, V6312, P707, DOI 10.1007/978-3-642-15552-9_51; Zhou YP, 2016, LECT NOTES COMPUT SC, V9912, P262, DOI 10.1007/978-3-319-46484-8_16	40	1	1	2	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					504	516		10.1007/s11263-021-01560-x	http://dx.doi.org/10.1007/s11263-021-01560-x		JAN 2022	13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS					2022-12-18	WOS:000739265400003
J	Han, CR; Shan, SG; Kan, MN; Wu, SZ; Chen, XL				Han, Chunrui; Shan, Shiguang; Kan, Meina; Wu, Shuzhe; Chen, Xilin			Personalized Convolution for Face Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face recognition; Personalized convolution; Personalized kernel	REPRESENTATION; EIGENFACES; HISTOGRAM; PATTERNS; MODEL	Face recognition has been significantly advanced by deep learning based methods. In all face recognition methods based on convolutional neural network (CNN), the convolutional kernels for feature extraction are fixed regardless of the input face once the training stage is finished. By contrast, we humans are usually impressed by some unique characteristics of different persons, such as one's blue eyes while another one's crooked nose, or even someone's naevus at specific location. Inspired by this observation, we propose a personalized convolution method which aims to extract special distinguishing characteristics of each person for more accurate face recognition. Specifically, given a face, we adaptively generate a set of kernels for him/her, named by us ordinary kernel, which is further analytically decomposed into two orthogonal components, i.e., the commonality component and the specialty component. The former characterizes the commonality among subjects which is optimized on a reference set. The latter is the residual part by filtering out the commonality component from the ordinary kernel, so as to capture those special characteristics, named by us personalized kernel. The CNNs with personalized kernels for convolution can highlight those specialty of a person's distinguishing characteristics while suppress his/her commonality with others, leading to better distinguishing of different faces. Additionally, as a by-product, the reference set also facilitates the adaptation of our method to different scenarios by simply selecting faces of a particular population. Extensive experiments on the challenging LFW, IJB-A and IJB-C datasets validate that our proposed personalized convolution achieves significant improvement over the conventional CNN, and also other existing methods for face recognition.	[Han, Chunrui; Shan, Shiguang; Kan, Meina; Wu, Shuzhe; Chen, Xilin] Chinese Acad Sci, Inst Comp Technol, CAS, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Han, Chunrui; Shan, Shiguang; Kan, Meina; Wu, Shuzhe; Chen, Xilin] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Shan, Shiguang] CAS Ctr Excellence Brain Sci & Intelligence Techn, Shanghai, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Kan, MN (corresponding author), Chinese Acad Sci, Inst Comp Technol, CAS, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Kan, MN (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.	chunrui.han@vipl.ict.ac.cn; sgshan@ict.ac.cn; kanmeina@ict.ac.cn; shuzhe.wu@vipl.ict.ac.cn; xlchen@ict.ac.cn			National KeyResearch andDevelopment Program of China [2017YFA0700800]; Natural Science Foundation of China [61772496]; Beijing Nova Program [Z191100001119123]	National KeyResearch andDevelopment Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission)	This work was partially supported the National KeyResearch andDevelopment Program of China (No. 2017YFA0700800), the Natural Science Foundation of China (No. 61772496), and the Beijing Nova Program (Z191100001119123).	AbdAlmageed W, 2016, IEEE WINT CONF APPL; Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Bertinetto Luca, 2016, NIPS; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; CAO ZM, 2010, PROC CVPR IEEE, P2707, DOI DOI 10.1109/CVPR.2010.5539992; Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41; Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296; Chen JC, 2016, IEEE WINT CONF APPL; De Brabandere B, 2016, ADV NEUR IN, V29; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30; Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042; Duan YQ, 2019, PROC CVPR IEEE, P3410, DOI 10.1109/CVPR.2019.00353; Duan YQ, 2018, IEEE T PATTERN ANAL, V40, P1139, DOI 10.1109/TPAMI.2017.2710183; Gong S., 2020, ECCV, P330; Han CR, 2018, LECT NOTES COMPUT SC, V11213, P120, DOI 10.1007/978-3-030-01240-3_8; He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55; Huang G.B., 2008, WORKSH FAC REAL LIF; Huang Gary B, 2014, 14003 U MASS AMH DEP, V14, P1; Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594; Jian Z., 2019, C ART INT AAAI; Kang BN, 2019, IEEE I CONF COMP VIS, P5471, DOI 10.1109/ICCV.2019.00557; Kang BN, 2018, LECT NOTES COMPUT SC, V11206, P646, DOI 10.1007/978-3-030-01216-8_39; Kang D., 2017, ADV NEURAL INFORM PR; Kim Y, 2020, PROC CVPR IEEE, P5620, DOI 10.1109/CVPR42600.2020.00566; KLARE BF, 2015, PROC CVPR IEEE, P1931, DOI DOI 10.1109/CVPR.2015.7298803; Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073; Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112; Liao S., 2019, INTERPRETABLE GENERA; Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679; Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523; Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35; Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033; Moghaddam B, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P30, DOI 10.1109/AFGR.1998.670921; Sankaranarayanan S., 2016, ARXIV160203418; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Song LX, 2019, IEEE I CONF COMP VIS, P773, DOI 10.1109/ICCV.2019.00086; Sun YP, 2015, ADV DIFFER EQU-NY, P1, DOI 10.1186/s13662-015-0433-7; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645; Tran AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Wang DY, 2017, IEEE T PATTERN ANAL, V39, P1122, DOI 10.1109/TPAMI.2016.2582166; Wang H, 2019, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR.2019.00364; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang M., 2018, DEEP FACE RECOGNITIO; Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87; Wolf L., 2011, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2011.5995566; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Wu WL, 2017, IEEE I CONF COMP VIS, P3792, DOI 10.1109/ICCV.2017.407; Xie SF, 2010, IEEE T IMAGE PROCESS, V19, P1349, DOI 10.1109/TIP.2010.2041397; Yan SC, 2005, PROC CVPR IEEE, P830; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yin XX, 2017, HEALTH INFOR SCI, P1, DOI 10.1007/978-3-319-57027-3_1; Yonghyun Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P536, DOI 10.1007/978-3-030-58545-7_31; Yuge Huang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P138, DOI 10.1007/978-3-030-58577-8_9; Zhang BH, 2007, IEEE T IMAGE PROCESS, V16, P57, DOI 10.1109/TIP.2006.884956; Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277; Zhang WC, 2005, IEEE I CONF COMP VIS, P786; Zhang Y, 2017, INT CONF IMAG VIS; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhao K, 2019, PROC CVPR IEEE, P1136, DOI 10.1109/CVPR.2019.00123; Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17	72	1	1	9	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					344	362		10.1007/s11263-021-01536-x	http://dx.doi.org/10.1007/s11263-021-01536-x		JAN 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS					2022-12-18	WOS:000738560100003
J	Marin-Jimenez, MJ; Romero, J; Li, H; Rogez, G				Marin-Jimenez, Manuel J.; Romero, Javier; Li, Hao; Rogez, Gregory			Preface to the Special Issue on Human Pose, Motion, Activities and Shape in 3D	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Marin-Jimenez, Manuel J.] Univ Cordoba, Cordoba 14071, Spain; [Romero, Javier] Facebook Real Labs, Barcelona 08026, Spain; [Li, Hao] Univ Calif Berkeley, Pinscreen Inc, Berkeley, CA 90025 USA; [Rogez, Gregory] NAVER LABS Europe, F-38240 Meylan, France	Universidad de Cordoba; University of California System; University of California Berkeley	Marin-Jimenez, MJ (corresponding author), Univ Cordoba, Cordoba 14071, Spain.	mjmarin@uco.es; javier.romero@tuebingen.mpg.de; hao@hao-li.com; gregory.rogez@naverlabs.com		Marin-Jimenez, Manuel J./0000-0001-9294-6714					0	1	1	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					199	200		10.1007/s11263-021-01548-7	http://dx.doi.org/10.1007/s11263-021-01548-7		JAN 2022	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS		Bronze			2022-12-18	WOS:000738511800005
J	Zheng, WS; Hong, JC; Jiao, JN; Wu, AC; Zhu, XT; Gong, SG; Qin, JY; Lai, JH				Zheng, Wei-Shi; Hong, Jincheng; Jiao, Jiening; Wu, Ancong; Zhu, Xiatian; Gong, Shaogang; Qin, Jiayin; Lai, Jianhuang			Joint Bilateral-Resolution Identity Modeling for Cross-Resolution Person Re-Identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Low-resolution; Visual surveillance	RECOGNITION; NETWORK	Person images captured by public surveillance cameras often have low resolutions (LRs), along with uncontrolled pose variations, background clutter and occlusion. These issues cause the resolution mismatch problem when matched with high-resolution (HR) gallery images (typically available during collection), harming the person re-identification (re-id) performance. While a number of methods have been introduced based on the joint learning of super-resolution and person re-id, they ignore specific discriminant identity information encoded in LR person images, leading to ineffective model performance. In this work, we propose a novel joint bilateral-resolution identity modeling method that concurrently performs HR-specific identity feature learning with super-resolution, LR-specific identity feature learning, and person re-id optimization. We also introduce an adaptive ensemble algorithm for handling different low resolutions. Extensive evaluations validate the advantages of our method over related state-of-the-art re-id and super-resolution methods on cross-resolution re-id benchmarks. An important discovery is that leveraging LR-specific identity information enables a simple cascade of super-resolution and person re-id learning to achieve state-of-the-art performance, without elaborate model design nor bells and whistles, which has not been investigated before.	[Zheng, Wei-Shi; Jiao, Jiening; Wu, Ancong; Lai, Jianhuang] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China; [Hong, Jincheng; Qin, Jiayin] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Peoples R China; [Zhu, Xiatian] Univ Surrey, Fac Engn & Phys Sci, Ctr Vis Speech & Signal Proc CVSSP, Guildford, Surrey, England; [Gong, Shaogang] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London, England; [Zheng, Wei-Shi] Peng Cheng Lab, Shenzhen 518005, Peoples R China; [Jiao, Jiening] Minist Educ, Key Lab Machine Intelligence & Adv Comp, Beijing, Peoples R China	Sun Yat Sen University; Sun Yat Sen University; University of Surrey; University of London; Queen Mary University London; Peng Cheng Laboratory	Lai, JH (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.	wszheng@ieee.org; hongjch@mail2.sysu.edu.cn; jiaojn@mail2.sysu.edu.cn; wuancong@mail2.sysu.edu.cn; eddy.zhuxt@gmail.com; s.gong@qmul.ac.uk; issqjy@mail.sysu.edu.cn; stsljh@mail.sysu.edu.cn			NSFC [U1911401, U1811461]; Guangdong NSF Project [2020B1515120 085, 2018B030312002]; Guangzhou Research Project [201902010037]; Research Projects of Zhejiang Lab [2019KD0AB03]; Key-Area Research and Development Program of Guangzhou [202007030004]	NSFC(National Natural Science Foundation of China (NSFC)); Guangdong NSF Project; Guangzhou Research Project; Research Projects of Zhejiang Lab; Key-Area Research and Development Program of Guangzhou	This work was supported partially by the NSFC (U1911401, U1811461), Guangdong NSF Project (No. 2020B1515120 085, 2018B030312002), Guangzhou Research Project (201902010037), and Research Projects of Zhejiang Lab (No. 2019KD0AB03), and the Key-Area Research and Development Program of Guangzhou (202007030004). The corresponding author for this paper is Jian-Huang Lai.	Ahmed E., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299016; Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844; Chen YC, 2017, IEEE T CIRC SYST VID, V27, P1661, DOI 10.1109/TCSVT.2016.2515309; Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805; Chen YC, 2019, AAAI CONF ARTIF INTE, P8215; Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68; Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584; Cheng ZY, 2020, PROC CVPR IEEE, P2602, DOI 10.1109/CVPR42600.2020.00268; Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Fan X., 2018, ECCV; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21; Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374; Guo YQ, 2017, IEEE INT C INTELL TR; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He WX, 2016, INT C PATT RECOG, P3410, DOI 10.1109/ICPR.2016.7900161; Hennings-Yeomans PH, 2008, PROC CVPR IEEE, P3637; Huang H, 2011, IEEE T NEURAL NETWOR, V22, P121, DOI 10.1109/TNN.2010.2089470; Jiao JN, 2018, AAAI CONF ARTIF INTE, P6967; Jing XY, 2015, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2015.7298669; Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117; Kim J., 2016, DEEPLY RECURSIVE CON, P1637, DOI 10.1109/CVPR.2016.181; Kim J., 2016, ACCURATE IMAGE SUPER, P1646, DOI [DOI 10.1109/CVPR.2016.182, 10.1109/CVPR.2016.182]; Lai Wei-Sheng, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618; Lazebnik S., 2006, P 2006 IEEE COMP VIS, P2169; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li X, 2015, IEEE I CONF COMP VIS, P3765, DOI 10.1109/ICCV.2015.429; Li YJ, 2019, IEEE I CONF COMP VIS, P8089, DOI 10.1109/ICCV.2019.00818; Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Luo DY, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675384; Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190; MAO SN, 2019, IJCAI, P883; Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152; NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3; Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45; Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171; Wang Z., 2016, IJCAI, P2669; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Wang Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3891; Wu Yonghui, 2016, CORR; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Yongkang Wong, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1200, DOI 10.1109/ICPR.2010.299; Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12; Yukun Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14072, DOI 10.1109/CVPR42600.2020.01409; Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng WS, 2016, IEEE T PATTERN ANAL, V38, P591, DOI 10.1109/TPAMI.2015.2453984; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380; 2014, ADV COMPUT VIS PATT, P1	65	1	1	4	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					136	156		10.1007/s11263-021-01518-z	http://dx.doi.org/10.1007/s11263-021-01518-z		NOV 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC					2022-12-18	WOS:000721452800001
J	Chatzitofis, A; Zarpalas, D; Daras, P; Kollias, S				Chatzitofis, Anargyros; Zarpalas, Dimitrios; Daras, Petros; Kollias, Stefanos			DeMoCap: Low-Cost Marker-Based Motion Capture	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion capture; Low-cost; Marker-based; Depth-based; Pose regression; Multi-view		Optical marker-based motion capture (MoCap) remains the predominant way to acquire high-fidelity articulated body motions. We introduce DeMoCap, the first data-driven approach for end-to-end marker-based MoCap, using only a sparse setup of spatio-temporally aligned, consumer-grade infrared-depth cameras. Trading off some of their typical features, our approach is the sole robust option for far lower-cost marker-based MoCap than high-end solutions. We introduce an end-to-end differentiable markers-to-pose model to solve a set of challenges such as under-constrained position estimates, noisy input data and spatial configuration invariance. We simultaneously handle depth and marker detection noise, label and localize the markers, and estimate the 3D pose by introducing a novel spatial 3D coordinate regression technique under a multi-view rendering and supervision concept. DeMoCap is driven by a special dataset captured with 4 spatio-temporally aligned low-cost Intel RealSense D415 sensors and a 24 MXT40S camera professional MoCap system, used as input and ground truth, respectively.	[Chatzitofis, Anargyros; Kollias, Stefanos] Natl Tech Univ Athens, Zografou Campus 9, Athens 15780, Greece; [Chatzitofis, Anargyros; Zarpalas, Dimitrios; Daras, Petros] Ctr Res & Technol Hellas, 6th Km Charilaou Thermi, Thessaloniki 57001, Greece	National Technical University of Athens; Centre for Research & Technology Hellas	Chatzitofis, A (corresponding author), Natl Tech Univ Athens, Zografou Campus 9, Athens 15780, Greece.; Chatzitofis, A (corresponding author), Ctr Res & Technol Hellas, 6th Km Charilaou Thermi, Thessaloniki 57001, Greece.	tofis3d@gmail.com; zarpalas@iti.gr; daras@iti.gr; stefanos@cs.ntua.gr	Kollias, Stefanos/ACY-7285-2022	Zarpalas, Dimitrios/0000-0002-9649-9306; Chatzitofis, Anargyros/0000-0002-3848-4210; Daras, Petros/0000-0003-3814-6710; Kollias, Stefanos/0000-0003-2899-0598				Alexanderson S, 2017, COMPUT GRAPH-UK, V69, P59, DOI 10.1016/j.cag.2017.10.001; Bascones, 2019, THESIS U PAIS VASCO; Bekhtaoui W., 2020, ARXIV PREPRINT ARXIV; Buhrmester Vanessa, 2019, Pattern Recognition and Image Analysis. 9th Iberian Conference, IbPRIA 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11867), P302, DOI 10.1007/978-3-030-31332-6_27; Burenius M, 2013, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2013.464; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chatzitofis A, 2020, IEEE ACCESS, V8, P176241, DOI 10.1109/ACCESS.2020.3026276; Chatzitofis A, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020282; Cheng Bowen, 2019, ARXIV PREPRINT ARXIV; Doosti B, 2020, PROC CVPR IEEE, P6607, DOI 10.1109/CVPR42600.2020.00664; Elhayek A, 2015, PROC CVPR IEEE, P3810, DOI 10.1109/CVPR.2015.7299005; Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238; Fuglede B, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P31; Gao HY, 2019, PR MACH LEARN RES, V97; Gaschler, 2011, REAL TIME MARKER BAS; Glorot X., 2010, PROC MACH LEARN RES, P249; Guler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114; Han SC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201399; Haque A, 2016, LECT NOTES COMPUT SC, V9905, P160, DOI 10.1007/978-3-319-46448-0_10; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Holden D, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201302; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868; Keselman L, 2017, IEEE COMPUT SOC CONF, P1267, DOI 10.1109/CVPRW.2017.167; King DB, 2015, ACS SYM SER, V1214, P1; Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Loper M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661273; Luvizon DC, 2019, COMPUT GRAPH-UK, V85, P15, DOI 10.1016/j.cag.2019.09.002; Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554; Martinez-Gonzalez A., 2018, 2018 EUR C COMP VIS; Mehta Dushyant, 2019, ARXIV190700837; moai,, 2021, MOAI ACC MOD DAT DRI; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nibali A., 2018, ARXIV PREPRINT ARXIV; Park S, 2017, IEEE COMPUT SOC CONF, P105, DOI 10.1109/CVPRW.2017.19; Paszke A, 2019, ADV NEUR IN, V32; Pavllo D, 2018, ACM SIGGRAPH CONFERENCE ON MOTION, INTERACTION, AND GAMES (MIG 2018), DOI 10.1145/3274247.3274501; Pereira MM, 2019, MED SCI EDUC, V29, P431, DOI 10.1007/s40670-019-00706-4; Qi CR, 2017, ADV NEUR IN, V30; Qiu HB, 2019, IEEE I CONF COMP VIS, P4341, DOI 10.1109/ICCV.2019.00444; Rhodin H, 2018, LECT NOTES COMPUT SC, V11214, P765, DOI 10.1007/978-3-030-01249-6_46; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; RUEGG N, 2020, ARXIV PREPRINT ARXIV; Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4; Sterzentsenko V, 2018, 2018 14TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS), P200, DOI 10.1109/SITIS.2018.00038; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Tensmeyer C, 2019, PROC INT CONF DOC, P1, DOI 10.1109/ICDARW.2019.40072; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Tu H., 2020, EUR C COMP VIS ECCV; VICON, 1984, VICON SYSTEMS LTD; Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; YANG Y, 2011, PROC CVPR IEEE, P1385, DOI [10.1109/CVPR.2011.5995741, DOI 10.1109/CVPR.2011.5995741]; Ying, 2011, SFU MOTION CAPTURE D; Zanfir A, 2018, ADV NEUR IN, V31; Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712; Zhang YX, 2020, PROC CVPR IEEE, P1321, DOI 10.1109/CVPR42600.2020.00140; Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24	64	1	1	1	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3338	3366		10.1007/s11263-021-01526-z	http://dx.doi.org/10.1007/s11263-021-01526-z		OCT 2021	29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN					2022-12-18	WOS:000707538700004
J	Ma, C; Yang, F; Li, Y; Jia, HZ; Xie, XD; Gao, W				Ma, Cong; Yang, Fan; Li, Yuan; Jia, Huizhu; Xie, Xiaodong; Gao, Wen			Deep Trajectory Post-Processing and Position Projection for Single & Multiple Camera Multiple Object Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Trajectory post-processing; Position projection; Multiple object tracking; Multiple-camera multiple object tracking		Multiple Object Tracking (MOT) has attracted increasing interests in recent years, which plays a significant role in video analysis. MOT aims to track the specific targets as whole trajectories and locate the positions of the trajectory at different times. These trajectories are usually applied in Action Recognition, Anomaly Detection, Crowd Analysis and Multiple-Camera Tracking, etc. However, existing methods are still a challenge in complex scene. Generating false (impure, incomplete) tracklets directly affects the performance of subsequent tasks. Therefore, we propose a novel architecture, Siamese Bi-directional GRU, to construct Cleaving Network and Re-connection Network as trajectory post-processing. Cleaving Network is able to split the impure tracklets as several pure sub-tracklets, and Re-connection Network aims to re-connect the tracklets which belong to same person as whole trajectory. In addition, our methods are extended to Multiple-Camera Tracking, however, current methods rarely consider the spatial-temporal constraint, which increases redundant trajectory matching. Therefore, we present Position Projection Network (PPN) to convert trajectory position from local camera-coordinate to global world-coodrinate, which provides adequate and accurate temporal-spatial information for trajectory association. The proposed technique is evaluated over two widely used datasets MOT16 and Duke-MTMCT, and experiments demonstrate its superior effectiveness as compared with the state-of-the-arts.	[Ma, Cong; Yang, Fan; Li, Yuan; Jia, Huizhu; Xie, Xiaodong; Gao, Wen] Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China; [Ma, Cong] Sensetime Res, Beijing, Peoples R China	Peking University	Jia, HZ (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China.	macong@sensetime.com; fyang.eecs@pku.edu.cn; yuanli@pku.edu.cn; hzjia@pku.edu.cn; donxie@pku.edu.cn; wgao@pku.edu.cn						Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Babaee M., 2018, ARXIV181104091; Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159; Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Bredereck M., 2012, 2012 6 INT C DISTR S, P1; Cai YH, 2014, IEEE WINT CONF APPL, P761, DOI 10.1109/WACV.2014.6836026; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Chen JH, 2017, IEEE COMPUT SOC CONF, P2143, DOI 10.1109/CVPRW.2017.266; Chen KW, 2011, IEEE T MULTIMEDIA, V13, P625, DOI 10.1109/TMM.2011.2131639; Cho K., 2014, P 2014 C EMP METH NA, P1724; Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347; Dicle C, 2013, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2013.286; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Gao X, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P201, DOI 10.1145/3240508.3240548; Hartley R., 2003, MULTIPLE VIEW GEOMET; Henschel Roberto, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P770, DOI 10.1109/CVPRW.2019.00105; Henschel R, 2018, IEEE COMPUT SOC CONF, P1509, DOI 10.1109/CVPRW.2018.00192; Henschel Roberto, 2017, IEEE C COMP VIS PATT; Hermans Alexander, 2017, ARXIV170307737; Hou Y, 2020, INT CONF ACOUST SPEE, P4072, DOI 10.1109/ICASSP40776.2020.9053955; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9; Kingma D.P, P 3 INT C LEARNING R; Le N, 2016, LECT NOTES COMPUT SC, V9914, P43, DOI 10.1007/978-3-319-48881-3_4; Leal-Taixe L., 2015, MOTCHALLENGE 2015 BE; Levinkov E, 2017, PROC CVPR IEEE, P1904, DOI 10.1109/CVPR.2017.206; Li YC, 2021, PSYCHOL MED, V51, P1952, DOI 10.1017/S0033291720001555; Liang YM, 2017, LECT NOTES COMPUT SC, V10636, P397, DOI 10.1007/978-3-319-70090-8_41; Liu QK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P530; Liu X., 2021, IJCAI; Liu XB, 2020, IEEE T IMAGE PROCESS, V29, P2638, DOI 10.1109/TIP.2019.2950796; Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190; Ma C, 2018, IEEE INT CON MULTI; Ma C, 2021, INT J COMPUT VISION, V129, P1993, DOI 10.1007/s11263-021-01460-0; Ma C, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P253, DOI 10.1145/3323873.3325010; Maksai Andrii, 2017, INT C COMP VIS; Manen S, 2017, IEEE I CONF COMP VIS, P290, DOI 10.1109/ICCV.2017.40; McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148; Mengxi Guo, 2020, Neural Information Processing. 27th International Conference, ICONIP 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12532), P350, DOI 10.1007/978-3-030-63830-6_30; Milan A., 2016, MOT16 BENCHMARK MULT; Peng JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4615, DOI 10.1145/3394171.3416283; Peng JL, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP); Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41; Sahbani B, 2016, INT CONF SYST ENG, P109, DOI 10.1109/ICSEngT.2016.7849633; Schulter S, 2017, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2017.292; Son J., 2017, P IEEE C COMP VIS PA, P5620; Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394; Tesfaye Y. T., 2017, ARXIV170606196; WANG B., 2016, 2016 IEEE INT C POWE, P1, DOI [10.1109/VTCFall.2015.7390876, DOI 10.1109/POWERCON.2016.7754047]; Wang GA, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P482, DOI 10.1145/3343031.3350853; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Xiang J., 2020, IEEE T CIRC SYST VID; Xiaobin Liu, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P547, DOI 10.1145/3394171.3413904; Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28; Yang M, 2016, COMPUT VIS IMAGE UND, V153, P16, DOI 10.1016/j.cviu.2016.05.003; Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155; Yoon K, 2018, IET IMAGE PROCESS, V12, P1175, DOI 10.1049/iet-ipr.2017.1244; Zhang S, 2015, COMPUT VIS IMAGE UND, V134, P64, DOI 10.1016/j.cviu.2015.01.002; Zhang Y, 2020, IEEE INTERNET THINGS, V7, P7892, DOI 10.1109/JIOT.2020.2996609; Zhang Y, 2020, IEEE T IMAGE PROCESS, V29, P6694, DOI 10.1109/TIP.2020.2993073; Zhang Zhimeng, 2017, ARXIV171209531; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23; Zhuang YQ, 2018, IEEE IMAGE PROC, P3698, DOI 10.1109/ICIP.2018.8451830; Zhuang YQ, 2018, INT C PATT RECOG, P1506, DOI 10.1109/ICPR.2018.8545708	78	1	1	5	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3255	3278		10.1007/s11263-021-01527-y	http://dx.doi.org/10.1007/s11263-021-01527-y		OCT 2021	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN					2022-12-18	WOS:000707538700005
J	Zheng, CX; Dao, DS; Song, GX; Cham, TJ; Cai, JF				Zheng, Chuanxia; Dao, Duy-Son; Song, Guoxian; Cham, Tat-Jen; Cai, Jianfei			Visiting the Invisible: Layer-by-Layer Completed Scene Decomposition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Layered scene decomposition; Scene completion; Amodal instance segmentation; Instance depth order; Scene recomposition		Existing scene understanding systems mainly focus on recognizing the visible parts of a scene, ignoring the intact appearance of physical objects in the real-world. Concurrently, image completion has aimed to create plausible appearance for the invisible regions, but requires a manual mask as input. In this work, we propose a higher-level scene understanding system to tackle both visible and invisible parts of objects and backgrounds in a given scene. Particularly, we built a system to decompose a scene into individual objects, infer their underlying occlusion relationships, and even automatically learn which parts of the objects are occluded that need to be completed. In order to disentangle the occluded relationships of all objects in a complex scene, we use the fact that the front object without being occluded is easy to be identified, detected, and segmented. Our system interleaves the two tasks of instance segmentation and scene completion through multiple iterations, solving for objects layer-by-layer. We first provide a thorough experiment using a new realistically rendered dataset with ground-truths for all invisible regions. To bridge the domain gap to real imagery where ground-truths are unavailable, we then train another model with the pseudo-ground-truths generated from our trained synthesis model. We demonstrate results on a wide variety of datasets and show significant improvement over the state-of-the-art.	[Zheng, Chuanxia; Song, Guoxian; Cham, Tat-Jen] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore; [Dao, Duy-Son; Cai, Jianfei] Monash Univ, Dept Data Sci & AI, Melbourne, Vic, Australia	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Monash University	Zheng, CX (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.	chuanxia001@e.ntu.edu.sg; duy.dao@monash.edu; guoxian001@e.ntu.edu.sg; astjcham@ntu.edu.sg; jianfei.cai@monash.edu	Zheng, Chuanxia/GQI-0645-2022	zheng, chuanxia/0000-0002-3584-9640	Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative; Singapore Telecommunications Limited (Singtel); Monash FIT Start-up Grant	Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative; Singapore Telecommunications Limited (Singtel); Monash FIT Start-up Grant	This study is supported under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from Singapore Telecommunications Limited (Singtel), through Singtel Cognitive and Artificial Intelligence Lab for Enterprises (SCALE@NTU). This research is also supported by the Monash FIT Start-up Grant.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2019, AUTODESK MAYA; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Burgess Christopher P, 2019, ARXIV190111390; Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Dhamo H, 2019, IEEE I CONF COMP VIS, P5368, DOI 10.1109/ICCV.2019.00547; Dinh L, 2017, PR MACH LEARN RES, V70; Dinh Laurent, 2014, ARXIV14108516; Ehsani K, 2018, PROC CVPR IEEE, P6144, DOI 10.1109/CVPR.2018.00643; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Follmann P, 2019, IEEE WINT CONF APPL, P1328, DOI 10.1109/WACV.2019.00146; Gao RX, 2007, LECT NOTES COMPUT SC, V4679, P213; Geiger A., 2012, P IEEE COMP SOC C CO; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; Guo RQ, 2012, LECT NOTES COMPUT SC, V7576, P761, DOI 10.1007/978-3-642-33715-4_55; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hoiem D, 2011, INT J COMPUT VISION, V91, P328, DOI 10.1007/s11263-010-0400-4; Hu YT, 2019, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2019.00322; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Gulrajani I, 2017, ADV NEUR IN, V30; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kar A, 2015, IEEE I CONF COMP VIS, P127, DOI 10.1109/ICCV.2015.23; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D.P, P 3 INT C LEARNING R; Kingma DP, 2018, ADV NEUR IN, V31; Li K, 2016, LECT NOTES COMPUT SC, V9906, P677, DOI 10.1007/978-3-319-46475-6_42; Li Yi, 2017, P IEEE C COMP VIS PA; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Ling H., 2020, ADV NEURAL INFORM PR, V33; Liu C, 2016, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2016.25; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mirza M., 2014, ARXIV; NITZBERG M, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P138; Nitzberg M., 1993, FILTERING SEGMENTATI, V662; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Pinheiro Pedro O., 2015, ADV NEURAL INFORM PR, V3, P5; Qi L, 2019, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR.2019.00313; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Sun D., 2010, ADV NEURAL INFORM PR, V23, P2226; Tighe J, 2014, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2014.479; van den Oord A, 2017, ADV NEUR IN, V30; Winn J., 2006, CVPR; Yan XS, 2019, IEEE I CONF COMP VIS, P7617, DOI 10.1109/ICCV.2019.00771; Yang Y, 2012, IEEE T PATTERN ANAL, V34, P1731, DOI 10.1109/TPAMI.2011.208; Yang Y, 2010, PROC CVPR IEEE, P3113, DOI 10.1109/CVPR.2010.5540070; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zhan XH, 2020, PROC CVPR IEEE, P3783, DOI 10.1109/CVPR42600.2020.00384; Zhang ZY, 2015, IEEE I CONF COMP VIS, P2614, DOI 10.1109/ICCV.2015.300; Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153; Zhu Y, 2017, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2017.320	66	1	1	3	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3195	3215		10.1007/s11263-021-01517-0	http://dx.doi.org/10.1007/s11263-021-01517-0		SEP 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN		Green Submitted			2022-12-18	WOS:000701743600001
J	Chen, D; Zhang, SS; Yang, J; Schiele, B				Chen, Di; Zhang, Shanshan; Yang, Jian; Schiele, Bernt			Norm-Aware Embedding for Efficient Person Search and Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person search; Pedestrian detection; Person re-identification; Multiple object tracking		Person detection and Re-identification are two well-defined support tasks for practically relevant tasks such as Person Search and Multiple Person Tracking. Person Search aims to find and locate all instances with the same identity as the query person in a set of panoramic gallery images. Similarly, Multiple Person Tracking, especially when using the tracking-by-detection pipeline, requires to detect and associate all appeared persons in consecutive video frames. One major challenge shared by the two tasks comes from the contradictory goals of detection and re-identification, i.e, person detection focuses on finding the commonness of all persons while person re-ID handles the differences among multiple identities. Therefore, it is crucial to reconcile the relationship between the two support tasks in a joint model. To this end, we present a novel approach called Norm-Aware Embedding to disentangle the person embedding into norm and angle for detection and re-ID respectively, allowing for both effective and efficient multi-task training. We further extend the proposal-level person embedding to pixel-level, whose discrimination ability is less affected by misalignment. Our Norm-Aware Embedding achieves remarkable performance on both person search and multiple person tracking benchmarks, with the merit of being easy to train and resource-friendly.	[Chen, Di; Zhang, Shanshan; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Minist Educ, PCA Lab,Key Lab Intelligent Percept & Syst High D, Nanjing, Peoples R China; [Chen, Di; Zhang, Shanshan; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing, Peoples R China; [Chen, Di; Schiele, Bernt] Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany	Nanjing University of Science & Technology; Nanjing University of Science & Technology; Max Planck Society	Zhang, SS; Yang, J (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Minist Educ, PCA Lab,Key Lab Intelligent Percept & Syst High D, Nanjing, Peoples R China.; Zhang, SS; Yang, J (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing, Peoples R China.	dichen@njust.edu.cn; shanshan.zhang@njust.edu.cn; csjyang@njust.edu.cn; schiele@mpi-inf.mpg.de	C, Deprecated/AHC-8964-2022		National Science Fund of China [U1713208]; Funds for International Co-operation and Exchange of the National Natural Science Foundation of China [61861136011]; "111" Program [B13022]; Natural Science Foundation of Jiangsu Province, China [BK20181299]; National Key Research and Development Program of China [2017YFC0820601]	National Science Fund of China(National Natural Science Foundation of China (NSFC)); Funds for International Co-operation and Exchange of the National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); "111" Program(Ministry of Education, China - 111 Project); Natural Science Foundation of Jiangsu Province, China(Natural Science Foundation of Jiangsu Province); National Key Research and Development Program of China	This work was partially supported by the National Science Fund of China (Grant No. U1713208), Funds for International Co-operation and Exchange of the National Natural Science Foundation of China (Grant No. 61861136011), "111" Program B13022, Natural Science Foundation of Jiangsu Province, China (Grant No. BK20181299), and National Key Research and Development Program of China (Grant No. 2017YFC0820601).	Ahmed E., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299016; Babaee M., 2018, ARXIV181104091; Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Breitenstein MD, 2009, IEEE I CONF COMP VIS, P1515, DOI 10.1109/ICCV.2009.5459278; Chang XJ, 2018, LECT NOTES COMPUT SC, V11213, P86, DOI 10.1007/978-3-030-01240-3_6; Chen D., 2020, CVPR; Chen DL, 2020, AAAI CONF ARTIF INTE, V34, P3438; Chen D, 2018, LECT NOTES COMPUT SC, V11211, P764, DOI 10.1007/978-3-030-01234-2_45; Chen D, 2020, IEEE T IMAGE PROCESS, V29, P4669, DOI 10.1109/TIP.2020.2973513; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dendorfer P., 2020, MOT20 BENCHMARK MULT, pabs/2003.09003; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005; Dollar P, 2009, BRIT MACHINE VISION, DOI [10.5244/C.23.91, DOI 10.5244/C.23.91]; Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479; Evangelidis GD, 2008, IEEE T PATTERN ANAL, V30, P1858, DOI 10.1109/TPAMI.2008.113; Fan X., 2018, ECCV; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Feng Weitao, 2019, ARXIV190106129; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Guo Y., 2017, ONE SHOT FACE RECOGN; Han CC, 2019, IEEE I CONF COMP VIS, P9813, DOI 10.1109/ICCV.2019.00991; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Henschel Roberto, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P770, DOI 10.1109/CVPRW.2019.00105; Keuper M, 2016, ARXIV160706317; Kim C, 2015, INT CONF ASIC; Kostinger Martin, 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247939; Kuo CH, 2011, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2011.5995384; Lan X, 2018, LECT NOTES COMPUT SC, V11205, P553, DOI 10.1007/978-3-030-01246-5_33; Leal-Taixe L., 2015, ARXIV; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li X, 2015, IEEE I CONF COMP VIS, P3765, DOI 10.1109/ICCV.2015.429; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu H, 2017, IEEE I CONF COMP VIS, P493, DOI 10.1109/ICCV.2017.61; Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762; Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Ma L., 2018, ACCV; Milan A., 2016, MOT16 BENCHMARK MULT; Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103; Munjal B, 2019, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2019.00090; Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257; Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394; TANG SY, 2015, PROC CVPR IEEE, P5033; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9; Wang X, 2007, ADV INTEL SYS RES, DOI 10.2991/iske.2007.208; Wang YT, 2018, LECT NOTES COMPUT SC, V11219, P764, DOI 10.1007/978-3-030-01267-0_45; Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166; Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Xiang J, 2021, IEEE T CIRC SYST VID, V31, P275, DOI 10.1109/TCSVT.2020.2975842; Xiang WM, 2018, P AMER CONTR CONF, P1574; Xiao J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3119; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226; Xu YH, 2020, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR42600.2020.00682; Yan YC, 2021, PROC CVPR IEEE, P7686, DOI 10.1109/CVPR46437.2021.00760; Yan Yichao, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3032542; Yan YC, 2019, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR.2019.00226; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139; Zhang S., 2014, CVPR; Zhang SS, 2018, IEEE T PATTERN ANAL, V40, P973, DOI 10.1109/TPAMI.2017.2700460; ZHANG SS, 2016, PROC CVPR IEEE, P1259, DOI DOI 10.1109/CVPR.2016.141; ZHANG SS, 2015, PROC CVPR IEEE, P1751, DOI DOI 10.1109/CVPR.2015.7298784; Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zhou X., 2019, ARXIV	98	1	1	4	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					3154	3168		10.1007/s11263-021-01512-5	http://dx.doi.org/10.1007/s11263-021-01512-5		SEP 2021	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH8ZM					2022-12-18	WOS:000695432000002
J	Le, TM; Le, V; Venkatesh, S; Tran, T				Le, Thao Minh; Le, Vuong; Venkatesh, Svetha; Tran, Truyen			Hierarchical Conditional Relation Networks for Multimodal Video Question Answering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video QA; Relational networks; Conditional modules; Hierarchy		Video Question Answering (Video QA) challenges modelers in multiple fronts. Modeling video necessitates building not only spatio-temporal models for the dynamic visual channel but also multimodal structures for associated information channels such as subtitles or audio. Video QA adds at least two more layers of complexity - selecting relevant content for each channel in the context of the linguistic query, and composing spatio-temporal concepts and relations hidden in the data in response to the query. To address these requirements, we start with two insights: (a) content selection and relation construction can be jointly encapsulated into a conditional computational structure, and (b) video-length structures can be composed hierarchically. For (a) this paper introduces a general-reusable reusable neural unit dubbed Conditional Relation Network (CRN) taking as input a set of tensorial objects and translating into a new set of objects that encode relations of the inputs. The generic design of CRN helps ease the common complex model building process of Video QA by simple block stacking and rearrangements with flexibility in accommodating diverse input modalities and conditioning features across both visual and linguistic domains. As a result, we realize insight (b) by introducing Hierarchical Conditional Relation Networks (HCRN) for Video QA. The HCRN primarily aims at exploiting intrinsic properties of the visual content of a video as well as its accompanying channels in terms of compositionality, hierarchy, and near-term and far-term relation. HCRN is then applied for Video QA in two forms, short-form where answers are reasoned solely from the visual content of a video, and long-form where an additional associated information channel, such as movie subtitles, presented. Our rigorous evaluations show consistent improvements over state-of-the-art methods on well-studied benchmarks including large-scale real-world datasets such as TGIF-QA and TVQA, demonstrating the strong capabilities of our CRN unit and the HCRN for complex domains such as Video QA. To the best of our knowledge, the HCRN is the very first method attempting to handle long and short-form multimodal Video QA at the same time.	[Le, Thao Minh; Le, Vuong; Venkatesh, Svetha; Tran, Truyen] Deakin Univ, Appl AI Inst, Geelong, Vic, Australia	Deakin University	Le, TM (corresponding author), Deakin Univ, Appl AI Inst, Geelong, Vic, Australia.	lethao@deakin.edu.au		Tran, Truyen/0000-0001-6531-8907; Le, Thao Minh/0000-0002-8089-9962; Le, Vuong/0000-0003-1582-1269				Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339; Chowdhury MIH, 2018, IEEE IMAGE PROC, P599, DOI 10.1109/ICIP.2018.8451103; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Djork-Arn, ICLR 2016; Ezen-Can Aysu, 2020, ARXIV; Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210; Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149; Jin WK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1193, DOI 10.1145/3343031.3351065; Kim JH, 2018, ADV NEUR IN, V31; Kim J, 2019, PROC CVPR IEEE, P8329, DOI 10.1109/CVPR.2019.00853; Kim KM, 2018, LECT NOTES COMPUT SC, V11219, P698, DOI 10.1007/978-3-030-01267-0_41; Kim KM, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2016; Le Thao Minh, 2020, P CVPR, P9972; Lei Jie, 2018, EMNLP, P1369, DOI DOI 10.18653/V1/D18-1167; Lei Jie, 2020, P 58 ANN M ASS COMP, P8211, DOI DOI 10.18653/V1/2020.ACL-MAIN.730; Li Fu, 2017, CORR; Li XP, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1166, DOI 10.1145/3343031.3350971; Li XP, 2019, AAAI CONF ARTIF INTE, P8658; Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642; LIENHART R, 1999, ACM MULTIMEDIA, P37, DOI DOI 10.1145/319878.319888; Little J. J., 2019, BMVC; Mahapatra D, 2008, PROC SPIE, V6806, DOI 10.1117/12.766243; Mao F, 2018, P EUR C COMP VIS ECC, P0; Na S, 2017, IEEE I CONF COMP VIS, P677, DOI 10.1109/ICCV.2017.80; Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590; Sang J., 2010, P 18 ACM INT C MULTI, P855, DOI DOI 10.1145/1873951.1874096; Seo Minjoon, 2017, ABS161101603 ARXIV; Song XM, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P239, DOI 10.1145/3240508.3240563; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tang Yongyi, 2018, P EUR C COMP VIS ECC, P0; Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501; Le TM, 2020, IEEE IJCNN; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Venkatesh, 2018, ARXIV PREPRINT ARXIV; Wang AR, 2020, IEEE T IMAGE PROCESS, V29, P489, DOI 10.1109/TIP.2019.2931534; Wang B, 2018, AAAI CONF ARTIF INTE, P7380; Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427; Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571; Yang TH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1184, DOI 10.1145/3343031.3350969; Yang ZK, 2020, IEEE WINT CONF APPL, P1545, DOI 10.1109/WACV45572.2020.9093596; Ye YN, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P829, DOI 10.1145/3077136.3080655; Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29; Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202; Zeng KH, 2017, AAAI CONF ARTIF INTE, P4334; Zhao Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3690; Zhao Z, 2019, IEEE T IMAGE PROCESS, V28, P5939, DOI 10.1109/TIP.2019.2922062; Zhao Z, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3518; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49; Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7	59	1	1	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					3027	3050		10.1007/s11263-021-01514-3	http://dx.doi.org/10.1007/s11263-021-01514-3		AUG 2021	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH8ZM		Green Submitted			2022-12-18	WOS:000690351900004
J	Thomas, C; Kovashka, A				Thomas, Christopher; Kovashka, Adriana			Predicting Visual Political Bias Using Webly Supervised Data and an Auxiliary Task	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Weak supervision; Noisy data; Unsupervised discovery; Curriculum learning; Privileged information; Image-text alignment; Visual rhetoric	NETWORKS	The news media shape public opinion, and often, the visual bias they contain is evident for careful human observers. This bias can be inferred from how different media sources portray different subjects or topics. In this paper, we model visual political bias in contemporary media sources at scale, using webly supervised data. We collect a dataset of over one million unique images and associated news articles from left- and right-leaning news sources, and develop a method to predict the image's political leaning. This problem is particularly challenging because of the enormous intra-class visual and semantic diversity of our data. We propose two stages of training to tackle this problem. In the first stage, the model is forced to learn relevant visual concepts that, when joined with document embeddings computed from articles paired with the images, enable the model to predict bias. In the second stage, we remove the requirement of the text domain and train a visual classifier from the features of the former model. We show this two-stage approach that relies on an auxiliary task leveraging text, facilitates learning and outperforms several strong baselines. We present extensive quantitative and qualitative results analyzing our dataset. Our results reveal disparities in how different sides of the political spectrum portray individuals, groups, and topics.	[Thomas, Christopher; Kovashka, Adriana] Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Thomas, C (corresponding author), Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA.	chris@cs.pitt.edu; kovashka@cs.pitt.edu			National Science Foundation [1566270, 1718262]; Nvidia hardware grant	National Science Foundation(National Science Foundation (NSF)); Nvidia hardware grant	This material is based upon work supported by the National Science Foundation under Grant Numbers 1566270 and 1718262. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Funding was also provided by a Nvidia hardware grant.	Akyurek Afra Feyza, 2020, P 58 ANN M ASS COMPU, P8614; Alayrac Jean-Baptiste, 2020, NEURAL INFORM PROCES; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Angermeyer MC, 2001, INT J LAW PSYCHIAT, V24, P469, DOI 10.1016/S0160-2527(01)00079-6; Baly Ramy, 2018, P 2018 C EMP METH NA, P3528, DOI [DOI 10.18653/V1/D18-1389, 10.18653/v1/D18-1389]; Baumer Eric, 2015, P 2015 C N AM CHAPT, P1472, DOI DOI 10.3115/V1/N15-1171; Bechavod Y., 2017, ARXIV PREPRINT ARXIV; Boutelier, 2018, BIG POLITICAL DATA; Burns Kaylee, 2018, ECCV; Card D, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P438; Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097; Chen TH, 2017, IEEE I CONF COMP VIS, P521, DOI 10.1109/ICCV.2017.64; Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168; Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231; Cohen Raviv, 2013, P INT AAAI C WEB SOC, V7; Colleoni E, 2014, J COMMUN, V64, P317, DOI 10.1111/jcom.12084; Conover M. D., 2011, 2011 IEEE 3 INT C PR, V2011, P192, DOI DOI 10.1109/PASSAT/SOCIALCOM.2011.34; Cucchiara R., 2018, BRIT MACH VIS C BMVC; Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323; Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597; Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174; Dong Z., 2012, 6 INT AAAI C WEBL SO; Edsall, 2012, STUDIES CONSERVATIVE; ehrek Radim., 2010, P LREC 2010 WORKSH N, P45; Eickhoff C, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P162, DOI 10.1145/3159652.3159654; Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201; Elliott D, 2017, P 8 INT JOINT C NAT, V1, P130; Faghri Fartash, 2018, BMVC, P12; Garbe W, 2019, SYMSPELL; Gilens M, 1996, PUBLIC OPIN QUART, V60, P515, DOI 10.1086/297771; Glorot X., 2010, PROC MACH LEARN RES, P249; Gomez L, 2017, PROC CVPR IEEE, P2017, DOI 10.1109/CVPR.2017.218; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065; Hessel Jack, 2018, NAACL; Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96; Honnibal Matthew, 2017, SPACY 2 NATURAL LANG; Hussain Z, 2017, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.2017.123; Jeewoo Yoon, 2020, Social Informatics. 12th International Conference, SocInfo 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12467), P440, DOI 10.1007/978-3-030-60975-7_32; Jiang L, 2015, AAAI CONF ARTIF INTE, P2694; Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; Joo J, 2015, IEEE I CONF COMP VIS, P3712, DOI 10.1109/ICCV.2015.423; Joo J, 2014, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2014.35; Karimi Hamid, 2019, ARXIV190307389; Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552; King DE, 2009, J MACH LEARN RES, V10, P1755; Kingma D.P, P 3 INT C LEARNING R; Kingsley, 2018, VILIFICATION GEORGE; Kiros R., 2015, TACL; Kosinski M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-79310-1; Kovashka, 2018, P BRIT MACH VIS C BM; Lambert J, 2018, PROC CVPR IEEE, P8886, DOI 10.1109/CVPR.2018.00926; Lee YJ, 2013, IEEE I CONF COMP VIS, P1857, DOI 10.1109/ICCV.2013.233; Li HZ, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P291, DOI 10.1145/3206025.3206039; Li Y, 2017, INT J COMPUT VISION, V121, P344, DOI 10.1007/s11263-016-0945-y; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Liu Siyi, 2019, P 23 C COMPUTATIONAL, P504; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lu J, 2018, PR MACH LEARN RES, V80; Lu JS, 2019, ADV NEUR IN, V32; Lu JS, 2016, ADV NEUR IN, V29; Luo, 2017, INT C SOC INF; Malkov YA, 2020, IEEE T PATTERN ANAL, V42, P824, DOI 10.1109/TPAMI.2018.2889473; Merrill J. B., 2016, NEW YORK TIMES; Messing S, 2016, PUBLIC OPIN QUART, V80, P44, DOI 10.1093/poq/nfv046; Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923; Morin F., 2005, PROC INT WORKSHOP AR, P246; Motiian S, 2016, PROC CVPR IEEE, P1496, DOI 10.1109/CVPR.2016.166; Munoz CL, 2017, J POLITICAL MARKETIN, V16, P290, DOI 10.1080/15377857.2017.1334254; Nguyen D., 2014, P COLING 2014 25 INT, P1950; Noble S.U., 2018, ALGORITHMS OPPRESSIO; OQUAB M, 2015, PROC CVPR IEEE, P685, DOI DOI 10.1109/CVPR.2015.7298668; Orr GB, 1997, ADV NEUR IN, V9, P232; Otterbacher J, 2018, ACM/SIGIR PROCEEDINGS 2018, P933, DOI 10.1145/3209978.3210094; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140; Peng YL, 2018, J COMMUN, V68, P920, DOI 10.1093/joc/jqy041; Pennacchiotti M, 2011, P AAAI INT C WEBL SO, V11, P281; PENTINA A, 2015, PROC CVPR IEEE, P5492, DOI [DOI 10.1109/CVPR.2015.7299188, 10.1109/CVPR.2015.7299188]; Peters ME, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P89; Philo, 2013, J SOCIAL POLITICAL P, V1, P321, DOI [DOI 10.5964/JSPP.V1I1.96, 10.5964/jspp.v1i1.96]; Philo G, 2008, JOURNALISM STUD, V9, P535, DOI 10.1080/14616700802114217; Potthast M, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P231; Le Q, 2014, PR MACH LEARN RES, V32, P1188; Recasens M., 2013, P 51 ANN M ASS COMP, V1, P1650; Richard A, 2017, PROC CVPR IEEE, P1273, DOI 10.1109/CVPR.2017.140; Ryu Hee Jung, 2017, ARXIV PREPRINT ARXIV; Schill Dan., 2012, REV COMMUNICATION, V12, P118, DOI [DOI 10.1080/15358593.2011.653504, 10.1080/15358593.2011.653504]; Schreiber D, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0052970; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sen S, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW'15), P826, DOI 10.1145/2675133.2675285; Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107; Sicre R, 2017, PROC CVPR IEEE, P3116, DOI 10.1109/CVPR.2017.332; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100; Thomas Christopher, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P317, DOI 10.1007/978-3-030-58523-5_19; Thomas Christopher, 2019, ADV NEURAL INFORM PR, P3625; Vapnik V, 2015, J MACH LEARN RES, V16, P2023; Venugopalan S, 2017, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2017.130; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Volkova S, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P186; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Wang Y., 2016, 10 INT AAAI C WEB SO; Wei YC, 2018, LECT NOTES COMPUT SC, V11215, P454, DOI 10.1007/978-3-030-01252-6_27; Wong FMF, 2016, IEEE T KNOWL DATA EN, V28, P2158, DOI 10.1109/TKDE.2016.2553667; Xi Nan, 2020, P INT AAAI C WEB SOC, V14, P726; Xiong C, 2017, 5 INT C LEARN REPR I; Ye KR, 2019, IEEE I CONF COMP VIS, P9685, DOI 10.1109/ICCV.2019.00978; Ye KR, 2021, IEEE T PATTERN ANAL, V43, P1308, DOI 10.1109/TPAMI.2019.2947440; Ye KR, 2018, LECT NOTES COMPUT SC, V11219, P868, DOI 10.1007/978-3-030-01267-0_51; Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7; Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196; Zhang JJ, 2019, PROC CVPR IEEE, P2951, DOI 10.1109/CVPR.2019.00307; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zhao J., 2017, P 2017 C EMP METH NA, P2979; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou F, 2010, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2010.5539966	120	1	1	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					2978	3003		10.1007/s11263-021-01506-3	http://dx.doi.org/10.1007/s11263-021-01506-3		AUG 2021	26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	WH8ZM					2022-12-18	WOS:000690351900002
J	Zhang, Q; Li, HD; Wang, X; Wang, Q				Zhang, Qi; Li, Hongdong; Wang, Xue; Wang, Qing			3D Scene Reconstruction with an Un-calibrated Light Field Camera	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Light field; Self-calibration; 3D reconstruction; Rays of the absolute conic (RAC)	LINE MOTION MATRIX; SELF-CALIBRATION; AUTOCALIBRATION; ALIGNMENT	This paper is concerned with the problem of multi-view 3D reconstruction with an un-calibrated micro-lens array based light field camera. To acquire3DEuclidean reconstruction, existing approaches commonly apply the calibrationwith a checkerboard and motion estimation from static scenes in two steps. Self-calibration is the process of simultaneously estimating intrinsic and extrinsic parameters directly from un-calibrated light fields without the help of a checkerboard. While the self-calibration technique for conventional (pinhole) camera is well understood, how to extend it to light field camera remains a challenging task. This is primarily due to the ultra-small baseline of the light field camera. We propose an effective self-calibration method for a light field camera for automatic metric reconstruction without a laborious pre-calibration process. In contrast to conventional self-calibration, we show how such a self-calibration method can be made numerically stable, by exploiting the regularity and measurement redundancies unique for the light field camera. The proposed method is built upon the derivation of a novel ray-space homography constraint (RSHC) using Plucker parameterization as well as a ray-space infinity homography (RSIH). We also propose a new concept of "rays of the absolute conic (RAC)" defined as a special quadric in 5D projective space P5. A set of new equations are established and solved for self-calibration and 3D metric reconstruction specifically designed for a light field camera. We validate the efficacy of the proposed method on both synthetic and real light fields, and have obtained superior results in both accuracy and robustness.	[Zhang, Qi; Wang, Xue; Wang, Qing] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China; [Li, Hongdong] Australian Natl Univ, ANU, Canberra, ACT 2600, Australia; [Li, Hongdong] Australian Natl Univ, ACRV, Canberra, ACT 2600, Australia	Northwestern Polytechnical University; Australian National University; Australian National University	Wang, X; Wang, Q (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.	nwpuqzhang@gmail.com; hongdong.li@anu.edu.au; xwang@nwpu.edu.cn; qwang@nwpu.edu.cn			NSFC [61531014, 61801396, 62031023]	NSFC(National Natural Science Foundation of China (NSFC))	The work was supported by NSFC under Grant 61531014, 61801396, 62031023.	Bartoli A, 2005, COMPUT VIS IMAGE UND, V100, P416, DOI 10.1016/j.cviu.2005.06.001; Bartoli A, 2004, INT J COMPUT VISION, V57, P159, DOI 10.1023/B:VISI.0000013092.07433.82; Bartoli A, 2001, PROC CVPR IEEE, P287; Birklbauer C, 2014, COMPUT GRAPH FORUM, V33, P43, DOI 10.1111/cgf.12289; Bok Y, 2017, IEEE T PATTERN ANAL, V39, P287, DOI 10.1109/TPAMI.2016.2541145; Bok Y, 2014, LECT NOTES COMPUT SC, V8694, P47, DOI 10.1007/978-3-319-10599-4_4; Chandraker M., 2007, P IEEE C COMPUTER VI, P1; Chandraker M, 2007, IEEE I CONF COMP VIS, P2236; Dansereau DG, 2013, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2013.137; Dansereau DG, 2011, IEEE INT C INT ROBOT, P4455, DOI 10.1109/IROS.2011.6048841; Dong FC, 2013, INT J ROBOT RES, V32, P206, DOI 10.1177/0278364912469420; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gherardi R, 2010, LECT NOTES COMPUT SC, V6311, P790, DOI 10.1007/978-3-642-15549-9_57; Guo XQ, 2016, IEEE T VIS COMPUT GR, V22, P1852, DOI 10.1109/TVCG.2015.2476805; Gurdjos P, 2009, IEEE I CONF COMP VIS, P88, DOI 10.1109/ICCV.2009.5459152; Habed A, 2014, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2014.70; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hartley R. I., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P510, DOI 10.1109/ICCV.1999.791264; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Johannsen O., 2016, P AS C COMP VIS ACCV, P3; Johannsen O, 2015, IEEE I CONF COMP VIS, P720, DOI 10.1109/ICCV.2015.89; Kneip L, 2014, PROC CVPR IEEE, P446, DOI 10.1109/CVPR.2014.64; Larsson V, 2018, PROC CVPR IEEE, P2984, DOI 10.1109/CVPR.2018.00315; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Li HF, 2008, CURR MED RES OPIN, V24, P1, DOI [10.1185/030079908X253933, 10.1088/0256-307X/24/3/072]; Li Y., 2019, OPTOELECTRONIC IMAGI, V11187; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luong QT, 1997, INT J COMPUT VISION, V22, P261, DOI 10.1023/A:1007982716991; Lytro, 2011, LYTR RED PHOT LIGHT; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; Ng, 2005, LIGHT FIELD PHOTOGRA; Ng R, 2006, DIGITAL LIGHT FIELD; Nister D, 2004, INT J COMPUT VISION, V60, P165, DOI 10.1023/B:VISI.0000029667.76852.a1; Nousias S, 2019, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2019.00341; Paudel DP, 2018, LECT NOTES COMPUT SC, V11216, P275, DOI 10.1007/978-3-030-01258-8_17; Pless R, 2003, PROC CVPR IEEE, P587, DOI 10.1109/cvpr.2003.1211520; Pollefeys M, 1999, IEEE T PATTERN ANAL, V21, P707, DOI 10.1109/34.784285; Pottmann H., 2009, COMPUTATIONAL LINE G; Raytrix, 2013, 3D LIGHT FIELD CAMER; Ren Z, 2017, IEEE IMAGE PROC, P1157; Seo Y, 2001, PROC CVPR IEEE, P880; Sturm P, 2005, PROC CVPR IEEE, P206; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388; Vianello A, 2018, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR.2018.00765; Zhang Q., 2020, IEEE T PATTERN ANAL, V1, DOI 10.1109/TPAMI.2020.3025949; Zhang Q., 2018, AS C COMP VIS ACCV, P18; Zhang Q., 2019, OPTOELECTRONIC IMAGI, V11187; Zhang Q, 2019, PROC CVPR IEEE, P10113, DOI 10.1109/CVPR.2019.01036; Zhang Q, 2019, IEEE T PATTERN ANAL, V41, P2539, DOI 10.1109/TPAMI.2018.2864617; Zhang YL, 2017, IEEE I CONF COMP VIS, P4641, DOI 10.1109/ICCV.2017.496; Zhang YL, 2017, IEEE INT CONF COMPUT, P67	54	1	2	4	26	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					3006	3026		10.1007/s11263-021-01516-1	http://dx.doi.org/10.1007/s11263-021-01516-1		AUG 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH8ZM					2022-12-18	WOS:000690351900001
J	Wu, ZX; Li, HD; Zheng, YB; Xiong, CM; Jiang, YG; Davis, LS				Wu, Zuxuan; Li, Hengduo; Zheng, Yingbin; Xiong, Caiming; Jiang, Yu-Gang; Davis, Larry S.			A Coarse-to-Fine Framework for Resource Efficient Video Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Conditional computation; Video classification; Efficient recognition		Deep neural networks have demonstrated remarkable recognition results on video classification, however great improvements in accuracies come at the expense of large amounts of computational resources. In this paper, we introduce LiteEval for resource efficient video recognition. LiteEval is a coarse-to-fine framework that dynamically allocates computation on a per-video basis, and can be deployed in both online and offline settings. Operating by default on low-cost features that are computed with images at a coarse scale, LiteEval adaptively determines on-the-fly when to read in more discriminative yet computationally expensive features. This is achieved by the interactions of a coarse RNN and a fine RNN, together with a conditional gating module that automatically learns when to use more computation conditioned on incoming frames. We conduct extensive experiments on three large-scale video benchmarks, FCVID, ActivityNet and Kinetics, and demonstrate, among other things, that LiteEval offers impressive recognition performance while using significantly less computation for both online and offline settings.	[Wu, Zuxuan; Jiang, Yu-Gang] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China; [Li, Hengduo; Davis, Larry S.] Univ Maryland, College Pk, MD 20742 USA; [Zheng, Yingbin] Videt Lab, Shanghai, Peoples R China; [Xiong, Caiming] Salesforce Res, Palo Alto, CA USA	Fudan University; University System of Maryland; University of Maryland College Park; Salesforce	Wu, ZX (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China.	zxwu@fudan.edu.cn; hdli@cs.umd.edu; zyb@videt.cn; cxiong@salesforce.com; ygj@fudan.edu.cn; lsd@cs.umd.edu	Li, Hengduo/ADM-1901-2022	Zheng, Yingbin/0000-0002-5590-9292	National Natural Science Foundation of China [62032006]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by National Natural Science Foundation of China (#62032006).	Bejnordi Babak Ehteshami, 2020, ICLR; Bolukbasi T, 2017, PR MACH LEARN RES, V70; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Chen YP, 2017, ADV NEUR IN, V30; Chen YP, 2018, LECT NOTES COMPUT SC, V11205, P364, DOI 10.1007/978-3-030-01246-5_22; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Courtney PG, 2015, IEEE COMP SEMICON; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dong XY, 2019, ADV NEUR IN, V32; Fan HH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P705; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Gao MF, 2018, PROC CVPR IEEE, P6926, DOI 10.1109/CVPR.2018.00724; Hazan T., 2012, ICML; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang Gao, 2018, ICLR; Iandola Forrest N., 2016, SQUEEZENET ALEXNET L; Jang E., 2017, ICLR; Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560; Jie H., 2017, P IEEE C COMP VIS PA, P99; Kay W., 2017, ARXIV PREPRINT ARXIV; KOPUKLU O, 2019, FG; Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633; Kuehne H, 2014, PROC CVPR IEEE, P780, DOI 10.1109/CVPR.2014.105; Lei Tao, 2017, ARXIV170902755; Li H., 2017, P INT C LEARN REPR I, P1; Li Z., 2016, P 25 INT JOINT C ART; Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718; Lin J, 2017, ADV NEUR IN, V30; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264; Maddison Chris J, 2017, ICLR; Molchanov Pavlo, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163132; Najibi M, 2019, IEEE I CONF COMP VIS, P9744, DOI 10.1109/ICCV.2019.00984; Qiu Z., 2016, ARXIV161109502; Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Simonyan K, 2014, ADV NEUR IN, V27; Spinoulas L, 2015, IEEE COMPUT SOC CONF; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Uzkent Burak, 2020, CVPR; Veit A, 2018, LECT NOTES COMPUT SC, V11205, P3, DOI 10.1007/978-3-030-01246-5_1; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wu CY, 2018, PROC CVPR IEEE, P6026, DOI 10.1109/CVPR.2018.00631; Wu W., 2019, ICCV; Wu Z., 2019, NEURIPS; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Wu ZX, 2019, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2019.00137; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang L, 2020, PROC CVPR IEEE, P2366, DOI 10.1109/CVPR42600.2020.00244; Yao T., 2012, ACM MULTIMEDIA; Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293; Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49; Zhu C., 2018, ECCV; Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43	71	1	1	3	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					2965	2977		10.1007/s11263-021-01508-1	http://dx.doi.org/10.1007/s11263-021-01508-1		AUG 2021	13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH8ZM					2022-12-18	WOS:000686035400001
J	Jia, W; Li, L; Li, Z; Liu, S				Jia, Wei; Li, Li; Li, Zhu; Liu, Shan			Deep Learning Geometry Compression Artifacts Removal for Video-Based Point Cloud Compression	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Point cloud compression; Denoising; Deep learning; Depth field	MPEG	Point cloud is an essential format for three-dimensional (3-D) object modelling and interaction in Augmented Reality and Virtual Reality applications. In the current state of the art video-based point cloud compression (V-PCC), a dynamic point cloud is projected onto geometry and attribute videos patch by patch, each represented by its texture, depth, and occupancy map for reconstruction. To deal with occlusion, each patch is projected onto near and far depth fields in the geometry video. Once there are artifacts on the compressed two-dimensional (2-D) geometry video, they would be propagated to the 3-D point-cloud frames. In addition, in the lossy compression, there always exists a tradeoff between the rate of bitstream and distortion. Although some geometry-related methods were proposed to attenuate these artifacts and improve the coding efficiency, the interactive correlation between projected near and far depth fields has been ignored. Moreover, the non-linear representation ability of Convolutional Neural Network has not been fully considered. Therefore, we propose a learning-based approach to remove the geometry artifacts and improve the compressing efficiency. We have the following contributions. We devise a two-step method working on the near and far depth fields decomposed from geometry. The first stage is learning-based Pseudo-Motion Compensation. The second stage exploits the potential of the strong correlations between near and far depth fields. Our proposed algorithm is embedded in the V-PCC reference software. To the best of our knowledge, this is the first learning-based solution of the geometry artifacts removal in V-PCC. The extensive experimental results show that the proposed approach achieves significant gains on geometry artifacts removal and quality improvement of 3-D point-cloud reconstruction compared to the state-of-the-art schemes.	[Jia, Wei; Li, Zhu] Univ Missouri, Kansas City, MO 64110 USA; [Li, Li] Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China; [Liu, Shan] Tencent Media Lab, Palo Alto, CA 94306 USA	University of Missouri System; University of Missouri Kansas City; Chinese Academy of Sciences; University of Science & Technology of China, CAS	Li, Z (corresponding author), Univ Missouri, Kansas City, MO 64110 USA.	wj3wr@umsystem.edu; lil1@ustc.edu.cn; zhu.li@ieee.org; shanl@tencent.com						Andrivon P., 2020, JTC1SC29WG11M51501 I; [Anonymous], 2017, JTC1SC29WG11M40059 I; Biswas S., 2020, ADV NEURAL INFORM PR; Bross B., 2019, JVETM1001; Bruder G, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P161, DOI 10.1109/3DUI.2014.6798870; Budagavi M., 2017, JTC1SC29WG11M41808 I; Cai K., 2018, JTC1SC29WG11M42111 I; Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580; Chen X, 2017, IEEE PHOTON CONF; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Committee M., 2020, JTC1SC29WG11W19526 I; Dawar N., 2018, JTC1SC29WG11M43723 I; de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3886, DOI 10.1109/TIP.2017.2707807; Fuchs H, 2014, COMPUTER, V47, P46, DOI 10.1109/MC.2014.185; Gojcic Z, 2020, PROC CVPR IEEE, P1756, DOI 10.1109/CVPR42600.2020.00183; Graziosi D., 2019, JTC1SC29WG11M47496 I; Guo K., 2017, ACM T GRAPHIC, V36, p44a, DOI 10.1145/3072959.3083722; He LY, 2017, ASIA-PAC CONF COMMUN, P345; Huang LL, 2020, PROC CVPR IEEE, P1310, DOI 10.1109/CVPR42600.2020.00139; Huang TX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P890, DOI 10.1145/3343031.3351061; Jang ES, 2019, IEEE SIGNAL PROC MAG, V36, P118, DOI 10.1109/MSP.2019.2900721; Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647; Kingma D.P, P 3 INT C LEARNING R; Lasserre S., 2017, JTC1SC29WG11M41822 I; Li L, 2020, IEEE T IMAGE PROCESS, V29, P289, DOI 10.1109/TIP.2019.2931621; Li Y., 2019, 16WP3 ITUT SG; Mammou K., 2017, JTC1SC29WG11 ISOIEC; Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039; Nakagami O., 2018, JTC1SC29WG11M43501 I; Olivier Y., 2018, JTC1SC29WG11M43723 I; Preda M, 2017, JTC1SC29WG11W17251 I; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Quach M, 2019, IEEE IMAGE PROC, P4320, DOI 10.1109/ICIP.2019.8803413; Rhyu S., 2018, JTC1SC29WG11M43667 I; Schwarz S., 2018, JTC1SC29WG11 ISO IEC; Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981; Sportillo D., 2017, P 9 INT C COMP AUT E, P6, DOI [10.1145/3057039, DOI 10.1145/3057039.3057079]; Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191; Sun XB, 2019, IEEE ROBOT AUTOM LET, V4, P2132, DOI 10.1109/LRA.2019.2900747; Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012; Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506; Tian D, 2017, IEEE IMAGE PROC, P3460; Tu CX, 2019, IEEE INT CONF ROBOT, P3274, DOI 10.1109/ICRA.2019.8794264; Tu CX, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1712, DOI 10.1109/ITSC.2016.7795789; Tulvan C., 2016, USE CASES POINT CLOU; Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349; Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165	47	1	1	2	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					2947	2964		10.1007/s11263-021-01503-6	http://dx.doi.org/10.1007/s11263-021-01503-6		AUG 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH8ZM					2022-12-18	WOS:000685374300001
J	Gong, R; Li, W; Chen, YH; Dai, DX; Van Gool, L				Gong, Rui; Li, Wen; Chen, Yuhua; Dai, Dengxin; Van Gool, Luc			DLOW: Domain Flow and Applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Domain flow; Domain adaptation; Domain generalization; Image translation; Semantic segmentation; Fog generation	ADAPTATION	In this work, we present a domain flow generation (DLOW) model to bridge two different domains by generating a continuous sequence of intermediate domains flowing from one domain to the other. The benefits of our DLOW model are twofold. First, it is able to transfer source images into a domain flow, which consists of images with smoothly changing distributions from the source to the target domain. The domain flow bridges the gap between source and target domains, thus easing the domain adaptation task. Second, when multiple target domains are provided for training, our DLOW model is also able to generate new styles of images that are unseen in the training data. The new images are shown to be able to mimic different artists to produce a natural blend of multiple art styles. Furthermore, for the semantic segmentation in the adverse weather condition, we take advantage of our DLOW model to generate images with gradually changing fog density, which can be readily used for boosting the segmentation performance when combined with a curriculum learning strategy. We demonstrate the effectiveness of our model on benchmark datasets for different applications, including cross-domain semantic segmentation, style generalization, and foggy scene understanding. Our implementation is available at https://github.com/ETHRuiGong/DLOW.	[Gong, Rui; Chen, Yuhua; Dai, Dengxin; Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland; [Li, Wen] Univ Elect Sci & Technol China, Chengdu, Peoples R China; [Van Gool, Luc] Katholieke Univ Leuven, Leuven, Belgium; [Dai, Dengxin] MPI Informat, Saarbrucken, Germany	Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Electronic Science & Technology of China; KU Leuven; Max Planck Society	Li, W (corresponding author), Univ Elect Sci & Technol China, Chengdu, Peoples R China.	liwen@uestc.edu.cn		Li, Wen/0000-0002-5559-8594	Major Project for New Generation of AI [2018AAA0100400]; China Postdoctoral Science Foundation [2019TQ0051]; EU [820434]; Toyota via the research project TRACE-Zurich	Major Project for New Generation of AI; China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); EU(European Commission); Toyota via the research project TRACE-Zurich	This work is partially supported by the Major Project for New Generation of AI under Grant Agreement No. 2018AAA0100400 and China Postdoctoral Science Foundation (NO. 2019TQ0051). This research has also received funding from the EU Horizon 2020 research and innovation programme under grant agreement No. 820434. The authors gratefully acknowledge the support by armasuisse, and the support by Toyota via the research project TRACE-Zurich. We also thank Amazon Web Services (AWS) for providing GPU credits for our research.	Almahairi A, 2018, PR MACH LEARN RES, V80; Arjovsky M, 2017, PR MACH LEARN RES, V70; Baktashmotlagh M, 2014, PROC CVPR IEEE, P2481, DOI 10.1109/CVPR.2014.318; Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100; Chang WL, 2019, PROC CVPR IEEE, P1900, DOI 10.1109/CVPR.2019.00200; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218; Chen Y., 2018, ARXIV181205040; Chen YH, 2018, PROC CVPR IEEE, P7892, DOI 10.1109/CVPR.2018.00823; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Cui Z, 2014, IEEE T CYBERNETICS, V44, P2264, DOI 10.1109/TCYB.2014.2305701; Dai D., 2019, INT J COMPUT VIS, P1; Du L, 2019, IEEE I CONF COMP VIS, P982, DOI 10.1109/ICCV.2019.00107; Dundar A., 2018, ARXIV PREPRINT ARXIV; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; GAIDON A, 2016, PROC CVPR IEEE, P4340, DOI DOI 10.1109/CVPR.2016.470; Ganin Yaroslav, 2015, ICML; Geiger A., 2012, P IEEE COMP SOC C CO; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Hahner M, 2019, IEEE INT C INTELL TR, P3675, DOI 10.1109/ITSC.2019.8917518; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; He Zhenliang, 2017, ARXIV171110678; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615; Hong WX, 2018, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2018.00145; Huang HS, 2018, LECT NOTES COMPUT SC, V11220, P611, DOI 10.1007/978-3-030-01270-0_36; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Huang Xun, 2018, ECCV; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924; Kim Taeksoo, 2017, P 34 INT C MACH LEAR, P1857, DOI [10.5555/3305381.3305573, DOI 10.5555/3305381.3305573]; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Lample Guillaume, 2017, ARXIV170600409; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566; Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin JX, 2018, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2018.00579; Liu MY, 2017, ADV NEUR IN, V30; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu Yiping, 2017, ARXIV PREPRINT ARXIV; Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Muandet Krikamol, 2013, ICML; Murez Z., 2017, ARXIV171200479; Niu L, 2015, IEEE I CONF COMP VIS, P4193, DOI 10.1109/ICCV.2015.477; Niu L, 2015, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2015.7298894; Pan F., 2020, P IEEE CVF C COMP VI; Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29; Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352; Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392; SAKARIDIS C, 2020, IEEE T PATTERN ANAL; Sakaridis C, 2018, LECT NOTES COMPUT SC, V11217, P707, DOI 10.1007/978-3-030-01261-8_42; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Saleh FS, 2018, LECT NOTES COMPUT SC, V11206, P86, DOI 10.1007/978-3-030-01216-8_6; Sankaranarayanan S, 2017, ARXIV171106969; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wei DM, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278040; Wulfmeier M, 2018, IEEE INT CONF ROBOT, P4489; Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43; Yu Fisher, 2018, ARXIV180504687; Zendel O, 2018, LECT NOTES COMPUT SC, V11210, P407, DOI 10.1007/978-3-030-01231-1_25; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400; Zhang Y., 2018, ARXIV180605173; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454; Zheng Z., 2020, IEEE EUR C COMP VIS; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XG, 2018, LECT NOTES COMPUT SC, V11211, P587, DOI 10.1007/978-3-030-01234-2_35; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	89	1	1	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2865	2888		10.1007/s11263-021-01496-2	http://dx.doi.org/10.1007/s11263-021-01496-2		AUG 2021	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO					2022-12-18	WOS:000684774600004
J	Han, L; Wang, PC; Yin, ZZ; Wang, F; Li, H				Han, Liang; Wang, Pichao; Yin, Zhaozheng; Wang, Fan; Li, Hao			Context and Structure Mining Network for Video Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video object detection; Spatial-temporal; Context and structure mining; Cross patch matching		Aggregating temporal features from other frames is verified to be very effective for video object detection to overcome the challenges in still images, such as occlusion, motion blur, and rare pose. Currently, proposal-level feature aggregation dominates this direction. However, there are two main problems for the holistic proposal-level feature aggregation. First, the object proposals generated by the region proposal network ignore the useful context information around the object which is proved to be helpful for object classification. Second, the traditional proposal-level feature aggregation regards the proposal as a whole without considering the important object structure information, which makes the similarity comparison between two proposals less effective when occlusion or pose misalignment occurs on proposal objects. To deal with these problems, we propose the Context and Structure Mining Network to better aggregate features for video object detection. In our method, we first encode the spatial-temporal context information into object features in a global manner, which can benefit the object classification. In addition, the holistic proposal is divided into several patches to capture the structure information of the object, and cross patch matching is conducted to alleviate the pose misalignment between objects in target and support proposals. Moreover, an importance weight is learned for each target proposal patch to indicate how informative this patch is for the final feature aggregation, by which the occluded patches can be neglected. This enables the aggregation module to leverage the most important and informative patches to obtain the final feature aggregation. The proposed framework outperforms all the latest state-of-the-art methods on the ImageNet VID dataset with a large margin. This project is publicly available https://github.com/LiangHann/Context-and-Structure-Mining-Network-for-Video-Object-Detection.	[Han, Liang; Yin, Zhaozheng] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA; [Wang, Pichao] Alibaba Grp, Bellevue, WA USA; [Yin, Zhaozheng] SUNY Stony Brook, Dept Biomed Informat, Stony Brook, NY 11794 USA; [Wang, Fan] Alibaba Grp, Sunnyvale, CA USA; [Li, Hao] Alibaba Grp, Hangzhou, Zhejiang, Peoples R China	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Alibaba Group; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Alibaba Group; Alibaba Group	Yin, ZZ (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.	liahan@cs.stonybrook.edu; pichao.wang@alibaba-inc.com; zyin@cs.stonybrook.edu; fan.w@alibaba-inc.com; lihao.lh@alibaba-inc.com		Han, Liang/0000-0002-6148-1114	NSF [CMMI-1646162, CMMI-1954548]	NSF(National Science Foundation (NSF))	Equal contribution: Liang Han and Pichao Wang. Liang Han and Zhaozheng Yin have been supported by NSF grants CMMI-1646162 and CMMI-1954548.	Bertasius G, 2018, LECT NOTES COMPUT SC, V11216, P342, DOI 10.1007/978-3-030-01258-8_21; Chen K, 2018, PROC CVPR IEEE, P7814, DOI 10.1109/CVPR.2018.00815; Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5; Chun-Han Yao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P160, DOI 10.1007/978-3-030-58568-6_10; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Deng HM, 2019, IEEE I CONF COMP VIS, P6677, DOI 10.1109/ICCV.2019.00678; Deng JJ, 2019, IEEE I CONF COMP VIS, P7022, DOI 10.1109/ICCV.2019.00712; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Fu C. -Y., 2017, ARXIV170106659; Gao ZM, 2019, IEEE T IMAGE PROCESS, V28, P1191, DOI 10.1109/TIP.2018.2872831; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Guo CX, 2019, IEEE I CONF COMP VIS, P3908, DOI 10.1109/ICCV.2019.00401; Han L, 2020, ACM MM; Han M., EUR C COMP VIS, P431; Han Wei, 2016, ARXIV160208465; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Jiang ZK, 2019, AAAI CONF ARTIF INTE, P8529; Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95; Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu MS, 2018, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2018.00596; Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sharif Razavian A, 2015, INT C LEARN REPR; Shvets M, 2019, IEEE I CONF COMP VIS, P9755, DOI 10.1109/ICCV.2019.00985; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Vaswani A, 2017, ADV NEUR IN, V30; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang SY, 2018, LECT NOTES COMPUT SC, V11217, P557, DOI 10.1007/978-3-030-01261-8_33; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wei ZX, 2017, IEEE GLOB COMM CONF; Wu HP, 2019, IEEE I CONF COMP VIS, P9216, DOI 10.1109/ICCV.2019.00931; Zhengkai Jiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P18, DOI 10.1007/978-3-030-58517-4_2; Zhou X., 2019, ARXIV; Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52; Zhujun Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P220, DOI 10.1007/978-3-030-58595-2_14	50	1	1	3	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2927	2946		10.1007/s11263-021-01507-2	http://dx.doi.org/10.1007/s11263-021-01507-2		AUG 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO					2022-12-18	WOS:000684774600002
J	Wang, LZ; Zhang, SP; Huang, H				Wang, Lizhi; Zhang, Shipeng; Huang, Hua			Adaptive Dimension-Discriminative Low-Rank Tensor Recovery for Computational Hyperspectral Imaging	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computational hyperspectral imaging; High-dimensionality structure; Hyperspectral image reconstruction; Low-rank tensor recovery; Adaptive weight estimation strategy	REPRESENTATION; ACQUISITION; ALGORITHMS; RESOLUTION; SPARSITY; DESIGN; VIDEO	Exploiting the prior information is fundamental for image reconstruction in computational hyperspectral imaging (CHI). Existing methods usually unfold the 3D signal as a 1D vector and then handle the prior information among different dimensions in an indiscriminative manner, which inevitably ignores the high-dimensionality nature of the hyperspectral image (HSI) and thus results in poor reconstruction performance. In this paper, we propose a high-order tensor optimization based reconstruction method to boost the quality of CHI. Specifically, we first propose an adaptive dimension-discriminative low-rank tensor recovery (ADLTR) model to exploit the high-dimensionality prior of HSI faithfully. In the ADLTR model, we utilize the 3D tensors as the basic elements to fundamentally preserve the structure information in the spatial and spectral dimensions, introduce a dimension-discriminative low-rankness model to fully characterize the prior in the basic elements, and propose a weight estimation strategy by adaptively exploiting the diversity in each dimension. Then, we develop an optimization framework for the CHI reconstruction by integrating the structure prior in ADLTR with the system imaging principle, which is finally solved via the alternating minimization scheme. Extensive experiments on both synthetic and real data demonstrate that our method outperforms state-of-the-art methods.	[Wang, Lizhi] Beijing Inst Technol, Beijing, Peoples R China; [Zhang, Shipeng] Xi An Jiao Tong Univ, Xian, Peoples R China; [Huang, Hua] Beijing Normal Univ, Beijing, Peoples R China	Beijing Institute of Technology; Xi'an Jiaotong University; Beijing Normal University	Huang, H (corresponding author), Beijing Normal Univ, Beijing, Peoples R China.	wanglizhi@bit.edu.cn; zsp6869123@stu.xjtu.edu.cn; huahuang@bnu.edu.cn		Wang, Lizhi/0000-0002-1953-3339	National Natural Science Foundation of China [62072038, 61922014]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by National Natural Science Foundation of China under Grant 62072038 and Grant 61922014.	[Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.187; Arce GR, 2014, IEEE SIGNAL PROC MAG, V31, P105, DOI 10.1109/MSP.2013.2278763; Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.909718; BASEDOW RW, 1995, P SOC PHOTO-OPT INS, V2480, P258, DOI 10.1117/12.210881; Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319; Candes E.J., 2006, P INT C MATHEMATICIA, P1433, DOI DOI 10.4171/022-3/69; Candes EJ, 2011, APPL COMPUT HARMON A, V31, P59, DOI 10.1016/j.acha.2010.10.002; Cao WF, 2015, NEUROCOMPUTING, V152, P261, DOI 10.1016/j.neucom.2014.10.069; Cao X, 2016, IEEE SIGNAL PROC MAG, V33, P95, DOI 10.1109/MSP.2016.2582378; Cao X, 2011, IEEE T PATTERN ANAL, V33, P2423, DOI 10.1109/TPAMI.2011.80; CHAKRABARTI A, 2011, PROC CVPR IEEE, P193, DOI DOI 10.1109/CVPR.2011.5995660; Chang Y, 2017, PROC CVPR IEEE, P5901, DOI 10.1109/CVPR.2017.625; Choi I, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130810; DESCOUR M, 1995, APPL OPTICS, V34, P4817, DOI 10.1364/AO.34.004817; Dian RW, 2017, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2017.411; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281; Fu Y, 2017, INT J COMPUT VISION, V122, P228, DOI 10.1007/s11263-016-0921-6; Fu Y, 2016, PROC CVPR IEEE, P3727, DOI 10.1109/CVPR.2016.405; Gao L, 2016, PHYS REP, V616, P1, DOI 10.1016/j.physrep.2015.12.004; Gevers T, 2003, INT J COMPUT VISION, V53, P135, DOI 10.1023/A:1023095923133; Giryes R, 2014, IEEE T IMAGE PROCESS, V23, P5057, DOI 10.1109/TIP.2014.2362057; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; He KS, 2011, DIVERS DISTRIB, V17, P381, DOI 10.1111/j.1472-4642.2011.00761.x; He R, 2020, IEEE T PATTERN ANAL, V42, P1025, DOI 10.1109/TPAMI.2019.2961900; HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044; Huang C, 2014, IEEE T IMAGE PROCESS, V23, P5284, DOI 10.1109/TIP.2014.2363734; Kittle D, 2010, APPL OPTICS, V49, P6824, DOI 10.1364/AO.49.006824; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kwon H, 2007, INT J COMPUT VISION, V71, P127, DOI 10.1007/s11263-006-6689-3; Li C, 2015, J OPT SOC AM A, V32, P1604, DOI 10.1364/JOSAA.32.001604; Lin X, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661262; Lin X, 2014, OPT LETT, V39, P2044, DOI 10.1364/OL.39.002044; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P2990, DOI 10.1109/TPAMI.2018.2873587; Llull P, 2013, OPT EXPRESS, V21, P10526, DOI 10.1364/OE.21.010526; Ma CG, 2014, INT J COMPUT VISION, V110, P141, DOI 10.1007/s11263-013-0690-4; Mejia Y, 2017, INT CONF ACOUST SPEE, P3116, DOI 10.1109/ICASSP.2017.7952730; Miao X, 2019, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2019.00416; OKAMOTO T, 1991, OPT LETT, V16, P1277, DOI 10.1364/OL.16.001277; Pan ZH, 2003, IEEE T PATTERN ANAL, V25, P1552, DOI 10.1109/TPAMI.2003.1251148; Peng Y, 2014, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2014.377; Porter WM., 1987, P SOC PHOTO-OPT INS, P22, DOI DOI 10.1117/12.942280; Rajwade A, 2013, IEEE T PATTERN ANAL, V35, P849, DOI 10.1109/TPAMI.2012.140; Romera-Paredes B., 2013, ADV NEURAL INFORM PR, V2, P2967; Salzenstein F, 2006, IEEE T PATTERN ANAL, V28, P1753, DOI 10.1109/TPAMI.2006.228; Schechner YY, 2002, IEEE T PATTERN ANAL, V24, P1334, DOI 10.1109/TPAMI.2002.1039205; Shewchuk J. R., 1994, INTRO CONJUGATE GRAD; Sun J., 2016, ADV NEUR IN, P10; Tan J, 2016, IEEE J-STSP, V10, P389, DOI 10.1109/JSTSP.2015.2500190; Tarabalka Y, 2010, IEEE T SYST MAN CY B, V40, P1267, DOI 10.1109/TSMCB.2009.2037132; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Van Nguyen H., 2010, P IEEE COMP VIS PATT, P44; Wagadarikar A, 2008, APPL OPTICS, V47, pB44, DOI 10.1364/AO.47.000B44; Wald Lucien, 2002, DATA FUSION DEFINITI, P6; Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032; Wang LZ, 2019, IEEE T IMAGE PROCESS, V28, P2257, DOI 10.1109/TIP.2018.2884076; Wang LZ, 2017, IEEE T PATTERN ANAL, V39, P2104, DOI 10.1109/TPAMI.2016.2621050; Wang LZ, 2015, PROC CVPR IEEE, P4942, DOI 10.1109/CVPR.2015.7299128; Wang LZ, 2015, APPL OPTICS, V54, P848, DOI 10.1364/AO.54.000848; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wu YH, 2011, OPT LETT, V36, P2692, DOI 10.1364/OL.36.002692; Xie Q, 2018, IEEE T PATTERN ANAL, V40, P1888, DOI 10.1109/TPAMI.2017.2734888; Xie T, 2019, IEEE T CYBERNETICS, V49, P2344, DOI 10.1109/TCYB.2018.2825598; Xiong ZW, 2017, IEEE INT CONF COMP V, P518, DOI 10.1109/ICCVW.2017.68; Xue SK, 2018, INT C PATT RECOG, P2600, DOI 10.1109/ICPR.2018.8546008; Yamaguchi M, 2006, PROC SPIE, V6062, DOI 10.1117/12.649454; Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811; Yokoya Naoto, 2016, SAL20160527 U TOK; Yuan X, 2014, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2014.424; Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196; Zhang MQ, 2019, OPT EXPRESS, V27, P17449, DOI 10.1364/OE.27.017449; Zhang SP, 2019, IEEE T CIRC SYST VID, V29, P3404, DOI 10.1109/TCSVT.2018.2879983; Zhang SP, 2018, LECT NOTES COMPUT SC, V11164, P711, DOI 10.1007/978-3-030-00776-8_65; Zhang ZM, 2014, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR.2014.485; Zhao WZ, 2016, IEEE T GEOSCI REMOTE, V54, P4544, DOI 10.1109/TGRS.2016.2543748; ZONTAK M, 2011, PROC CVPR IEEE, P977, DOI DOI 10.1109/CVPR.2011.5995401	79	1	1	3	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2907	2926		10.1007/s11263-021-01481-9	http://dx.doi.org/10.1007/s11263-021-01481-9		AUG 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO					2022-12-18	WOS:000684774600003
J	Zhang, Q; Wang, SS; Zhang, XF; Ma, SW; Gao, W				Zhang, Qi; Wang, Shanshe; Zhang, Xinfeng; Ma, Siwei; Gao, Wen			Just Recognizable Distortion for Machine Vision Oriented Image and Video Coding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image and video coding; Machine vision; Deep learning; Just noticeable distortion	DESCRIPTORS; MODELS	Machine visual intelligence has exploded in recent years. Large-scale, high-quality image and video datasets significantly empower learning-based machine vision models, especially deep-learning models. However, images and videos are usually compressed before being analyzed in practical situations where transmission or storage is limited, leading to a noticeable performance loss of vision models. In this work, we broadly investigate the impact on the performance of machine vision from image and video coding. Based on the investigation, we propose Just Recognizable Distortion (JRD) to present the maximum distortion caused by data compression that will reduce the machine vision model performance to an unacceptable level. A large-scale JRD-annotated dataset containing over 340,000 images is built for various machine vision tasks, where the factors for different JRDs are studied. Furthermore, an ensemble-learning-based framework is established to predict the JRDs for diverse vision tasks under few- and non-reference conditions, which consists of multiple binary classifiers to improve the prediction accuracy. Experiments prove the effectiveness of the proposed JRD-guided image and video coding to significantly improve compression and machine vision performance. Applying predicted JRD is able to achieve remarkably better machine vision task accuracy and save a large number of bits.	[Zhang, Qi; Wang, Shanshe; Ma, Siwei; Gao, Wen] Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China; [Zhang, Xinfeng] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing, Peoples R China; [Gao, Wen] Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China	Peking University; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Peng Cheng Laboratory	Zhang, Q (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China.	ywwynm@pku.edu.cn; sswang@pku.edu.cn; xfzhang@ucas.ac.cn; swma@pku.edu.cn; wgao@pku.edu.cn		Zhang, Qi/0000-0002-1189-8755	National Natural Science Foundation of China [62072008, 62025101]; PKU-Baidu Fund [2019BD003]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); PKU-Baidu Fund	This work was supported in part by the National Natural Science Foundation of China (62072008, 62025101), PKU-Baidu Fund (2019BD003) and High-performance Computing Platform of Peking University, which are gratefully acknowledged.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aqqa M, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P96, DOI 10.5220/0007401600960104; Chen YZ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, CONTROL, AND COMPUTING TECHNOLOGIES FOR SMART GRIDS (SMARTGRIDCOMM); Chen Z, 2020, IEEE T IMAGE PROCESS, V29, P2230, DOI 10.1109/TIP.2019.2941660; Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dodge S., 2016, 2016 8 INT C QUALITY, P1, DOI DOI 10.1109/QOMEX.2016.7498955; Dodge Samuel, 2017, 2017 26 INT C COMP C, P1, DOI DOI 10.1109/ICCCN.2017.8038465; Duan LY, 2019, IEEE MULTIMEDIA, V26, P44, DOI 10.1109/MMUL.2018.2873844; Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan CG, 2019, PROCEEDINGS OF 2019 FAR EAST NDT NEW TECHNOLOGY & APPLICATION FORUM (FENDT), P1, DOI 10.1109/FENDT47723.2019.8962705; Geirhos R, 2018, ADV NEUR IN, V31; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu T., 2019, SEE BETTER LOOKING C; Huang GL, 2017, IEEE ICC; Huang Q, 2017, IEEE DATA COMPR CONF, P42, DOI 10.1109/DCC.2017.17; JAYANT N, 1993, P IEEE, V81, P1385, DOI 10.1109/5.241504; Jin L., 2016, ELECT IMAGING, V2016, P1, DOI [10.2352/ISSN.2470-1173.2016.13.IQSP-222, DOI 10.2352/ISSN.2470-1173.2016.13.IQSP-222]; Li Y, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM); Lin H., 2020, QUALITY USER EXPERIE, V5, P1; Lin JY, 2015, PROC SPIE, V9599, DOI 10.1117/12.2188389; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu D, 2016, SENS IMAGING, V18, DOI 10.1007/s11220-016-0152-5; Liu HH, 2020, IEEE T IMAGE PROCESS, V29, P641, DOI 10.1109/TIP.2019.2933743; Liu S, 2018, JVET K1001; Lou YH, 2019, IEEE J SEL AREA COMM, V37, P1489, DOI 10.1109/JSAC.2019.2916488; Ma SW, 2019, IEEE T CIRC SYST VID, V29, P3095, DOI 10.1109/TCSVT.2018.2873102; Majaj N.J., 2018, BIORXIV, DOI 10.1101/407007; Redondi A, 2016, IEEE T MOBILE COMPUT, V15, P3000, DOI 10.1109/TMC.2016.2519340; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shi J, 2020, IEEE INT SYMP CIRC S; Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804; Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858; Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191; Sun Siqi, 2020, ARXIV200714527; Tan M, 2019, PR MACH LEARN RES, P6105; Wah C., 2011, TECH REP; Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610; Wang HQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6747; Wang HQ, 2018, PICT COD SYMP, P278; Wang HQ, 2017, J VIS COMMUN IMAGE R, V46, P292, DOI 10.1016/j.jvcir.2017.04.009; Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001; Zhang JP, 2020, PARTICUL SCI TECHNOL, V38, P596, DOI 10.1080/02726351.2019.1571542; Zhang X, 2017, IEEE T IMAGE PROCESS, V26, P633, DOI 10.1109/TIP.2016.2629447; Zhang XF, 2020, IEEE T IMAGE PROCESS, V29, P3777, DOI 10.1109/TIP.2020.2965994; Zhou X., 2019, ARXIV190407850V2	50	1	1	23	49	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2889	2906		10.1007/s11263-021-01505-4	http://dx.doi.org/10.1007/s11263-021-01505-4		AUG 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO					2022-12-18	WOS:000684774600001
J	Bahri, M; O' Sullivan, E; Gong, SW; Liu, F; Liu, XM; Bronstein, MM; Zafeiriou, S				Bahri, Mehdi; O' Sullivan, Eimear; Gong, Shunwang; Liu, Feng; Liu, Xiaoming; Bronstein, Michael M.; Zafeiriou, Stefanos			Shape My Face: Registering 3D Face Scans by Surface-to-Surface Translation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Surface registration; Non linear morphable models; Face modeling; Point cloud; Graph neural network; Generative modeling	DATABASE; MODEL; REGISTRATION	Standard registration algorithms need to be independently applied to each surface to register, following careful pre-processing and hand-tuning. Recently, learning-based approaches have emerged that reduce the registration of new scans to running inference with a previously-trained model. The potential benefits are multifold: inference is typically orders of magnitude faster than solving a new instance of a difficult optimization problem, deep learning models can be made robust to noise and corruption, and the trained model may be re-used for other tasks, e.g. through transfer learning. In this paper, we cast the registration task as a surface-to-surface translation problem, and design a model to reliably capture the latent geometric information directly from raw 3D face scans. We introduce Shape-My-Face (SMF), a powerful encoder-decoder architecture based on an improved point cloud encoder, a novel visual attention mechanism, graph convolutional decoders with skip connections, and a specialized mouth model that we smoothly integrate with the mesh convolutions. Compared to the previous state-of-the-art learning algorithms for non-rigid registration of face scans, SMF only requires the raw data to be rigidly aligned (with scaling) with a pre-defined face template. Additionally, our model provides topologically-sound meshes with minimal supervision, offers faster training time, has orders of magnitude fewer trainable parameters, is more robust to noise, and can generalize to previously unseen datasets. We extensively evaluate the quality of our registrations on diverse data. We demonstrate the robustness and generalizability of our model with in-the-wild face scans across different modalities, sensor types, and resolutions. Finally, we show that, by learning to register scans, SMF produces a hybrid linear and non-linear morphable model. Manipulation of the latent space of SMF allows for shape generation, and morphing applications such as expression transfer in-the-wild. We train SMF on a dataset of human faces comprising 9 large-scale databases on commodity hardware.	[Bahri, Mehdi; O' Sullivan, Eimear; Gong, Shunwang; Bronstein, Michael M.; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, London, England; [Liu, Feng; Liu, Xiaoming] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA	Imperial College London; Michigan State University	Bahri, M (corresponding author), Imperial Coll London, Dept Comp, London, England.	m.bahri@impetial.ac.uk; e.o-sullivan16@imperial.ac.uk; shunwang.gong16@imperialac.uk; isliuf1990@gmail.com; liuxm@cse.msu.edu; m.bronstein@imperiaLac.uk; s.zafeiriou@imperial.ac.uk		Gong, Shunwang/0000-0001-8717-8722; O' Sullivan, Eimear/0000-0003-0525-3341; Bahri, Mehdi/0000-0002-2409-0261; Liu, Feng/0000-0003-2103-4659; liu, xiaoming/0000-0003-3215-8753	Department of Computing scholarship; Qualcomm Innovation Fellowship; EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans [EP/S010203/1]; ERC Consolidator grant [724228]	Department of Computing scholarship; Qualcomm Innovation Fellowship; EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ERC Consolidator grant(European Research Council (ERC))	M. Bahriwas supported by a Department of Computing scholarship and a Qualcomm Innovation Fellowship. E. O' Sullivan was supported by a Department of Computing scholarship. S. Zafeiriou was partially funded by the EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans (EP/S010203/1). S. Gong and M. M. Bronstein were partially funded by ERC Consolidator grant No. 724228 (LEMAN). We thank Amazon for AWS Cloud Credits for Research.	Abrevaya VF, 2018, IEEE WINT CONF APPL, P1, DOI 10.1109/WACV.2018.00007; Amberg B, 2007, IEEE I CONF COMP VIS, P1326; Amberg B, 2008, IEEE INT CONF AUTOMA, P667; Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733; Aytekin C, 2018, IEEE IJCNN; Bagautdinov T, 2018, PROC CVPR IEEE, P3877, DOI 10.1109/CVPR.2018.00408; Bagdanov A. D., 2011, P ACM MULT ASS 3D HU; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Bolkart T, 2015, IEEE I CONF COMP VIS, P3604, DOI 10.1109/ICCV.2015.411; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Booth J, 2018, IEEE T PATTERN ANAL, V40, P2638, DOI 10.1109/TPAMI.2018.2832138; Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7; Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580; Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Bouritsas G, 2019, IEEE I CONF COMP VIS, P7212, DOI 10.1109/ICCV.2019.00731; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J, 2013, PROC INT C LEARN REP; Burt P., 1984, P SPIE, V0575, P173, DOI [10.1117/12.966501, DOI 10.1117/12.966501]; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; CHEN Y, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P2724, DOI 10.1109/ROBOT.1991.132043; Cheng SY, 2017, IMAGE VISION COMPUT, V58, P3, DOI 10.1016/j.imavis.2016.10.007; Crane K., 2020, CGAL USER REFERENCE; Crane K, 2017, COMMUN ACM, V60, P90, DOI 10.1145/3131280; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai H, 2017, IEEE I CONF COMP VIS, P3104, DOI 10.1109/ICCV.2017.335; De Smet M., 2011, LECT NOTES COMPUTER; Defferrard M, 2016, ADV NEUR IN, V29; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Djork-Arn, ICLR 2016; Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208; Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097; Feydy J., 2017, LECT NOTES COMPUTER; Garland M., 1997, P 24 ANN C COMPUTER, P209, DOI [DOI 10.1145/258734.258849, 10.1145/258734.258849]; Gerig T, 2018, IEEE INT CONF AUTOMA, P75, DOI 10.1109/FG.2018.00021; Gilmer J, 2017, PR MACH LEARN RES, V70; Gong S, 2019, IEEE INT C COMP VIS; Gong S., 2020, IEEE CVF C COMP VIS; Gupta SD, 2010, METHODS MOL BIOL, V589, P97, DOI [10.1007/978-1-60327-114-1_10, 10.1109/SSIAI.2010.5483908]; Hamilton WL, 2017, ADV NEUR IN, V30; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868; Kingma DP., 2014, PATTERN RECOGN LETT, V94, P172; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Knoops PGM, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49506-1; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463; Lefebure M, 2001, LECT NOTES COMPUT SC, V2106, P26; Lei H, 2019, PROC CVPR IEEE, P9623, DOI 10.1109/CVPR.2019.00986; Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936; Li J., 2019, ITERATIVE MATCHING P; Li Q., 2018, 2018 IEEEACM INT S N, P1; Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813; Lim I, 2019, LECT NOTES COMPUT SC, V11131, P349, DOI 10.1007/978-3-030-11015-4_26; Liu F, 2019, IEEE I CONF COMP VIS, P9407, DOI 10.1109/ICCV.2019.00950; Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Lu WX, 2019, IEEE I CONF COMP VIS, P12, DOI 10.1109/ICCV.2019.00010; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Luthi M, 2018, IEEE T PATTERN ANAL, V40, P1860, DOI 10.1109/TPAMI.2017.2739743; Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; MUELLER A, 2011, BRIT J ORAL MAX SURG, V49; Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46; Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498; Patel A, 2009, PROC CVPR IEEE, P1327, DOI 10.1109/CVPRW.2009.5206522; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Phillips J.J., 2005, PROC CVPR IEEE, V1, P947, DOI DOI 10.1109/CVPR.2005.268; Ploumpis S, 2021, IEEE T PATTERN ANAL, V43, P4142, DOI 10.1109/TPAMI.2020.2991150; Ploumpis S, 2019, PROC CVPR IEEE, P10926, DOI 10.1109/CVPR.2019.01119; Qi CR, 2017, ADV NEUR IN, V30; Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43; Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883; Salazar A, 2014, MACH VISION APPL, V25, P859, DOI 10.1007/s00138-013-0579-9; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Shimada S, 2019, INT CONF 3D VISION, P27, DOI 10.1109/3DV.2019.00013; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310; Tena JR, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964971; van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x; Velickovic P., 2018, P INT C LEARN REPR; Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275; Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362; Wang Y, 2019, ADV NEUR IN, V32; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI 10.1007/s11263-019-01198-w; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; [尹宝才 Yin Baocai], 2009, [计算机研究与发展, Journal of Computer Research and Development], V46, P1009; Yin LJ, 2008, IEEE INT CONF AUTOMA, P116; Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169; Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679	101	1	1	2	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2680	2713		10.1007/s11263-021-01494-4	http://dx.doi.org/10.1007/s11263-021-01494-4		JUL 2021	34	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN		Green Submitted, hybrid			2022-12-18	WOS:000671637200001
J	Bauer, M; Charon, N; Harms, P; Hsieh, HW				Bauer, Martin; Charon, Nicolas; Harms, Philipp; Hsieh, Hsi-Wei			A Numerical Framework for Elastic Surface Matching, Comparison, and Interpolation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Elastic shape analysis; Surfaces; Square root normal fields; Varifolds	SHAPE SPACE; SOBOLEV METRICS; GEOMETRIES; CURVES; MODEL	Surface comparison and matching is a challenging problem in computer vision. While elastic Riemannian metrics provide meaningful shape distances and point correspondences via the geodesic boundary value problem, solving this problem numerically tends to be difficult. Square root normal fields considerably simplify the computation of certain distances between parametrized surfaces. Yet they leave open the issue of finding optimal reparametrizations, which induce corresponding distances between unparametrized surfaces. This issue has concentrated much effort in recent years and led to the development of several numerical frameworks. In this paper, we take an alternative approach which bypasses the direct estimation of reparametrizations: we relax the geodesic boundary constraint using an auxiliary parametrization-blind varifold fidelity metric. This reformulation has several notable benefits. By avoiding altogether the need for reparametrizations, it provides the flexibility to deal with simplicial meshes of arbitrary topologies and sampling patterns. Moreover, the problem lends itself to a coarse-to-fine multi-resolution implementation, which makes the algorithm scalable to large meshes. Furthermore, this approach extends readily to higher-order feature maps such as square root curvature fields and is also able to include surface textures in the matching problem. We demonstrate these advantages on several examples, synthetic and real.	[Bauer, Martin] Florida State Univ, Dept Math, Tallahassee, FL 32306 USA; [Charon, Nicolas; Hsieh, Hsi-Wei] Johns Hopkins Univ, Dept Appl Math & Stat, Baltimore, MD 21218 USA; [Harms, Philipp] Univ Freiburg, Dept Math Stochast, Freiburg, Germany	State University System of Florida; Florida State University; Johns Hopkins University; University of Freiburg	Charon, N (corresponding author), Johns Hopkins Univ, Dept Appl Math & Stat, Baltimore, MD 21218 USA.	bauer@maths.fsu.edu; charon@cis.jhu.edu; philipp.harms@stochastik.uni-freiburg.de; hhsieh@cis.jhu.edu	Harms, Philipp/H-6981-2019	Harms, Philipp/0000-0003-4792-1424; Bauer, Martin/0000-0001-7771-056X	NSF [1819131, 1945224, 1912037, 1912030]	NSF(National Science Foundation (NSF))	MB is supported by NSF Grant 1912037 (collaborative research in connection with 1912030). NC and HH are supported by NSF Grants 1819131 and 1945224.	ABE K, 1975, MATH ANN, V215, P197, DOI 10.1007/BF01343889; Almgren F. J., 1966, PLATEAUS PROBLEM INV; Arnaudon M, 2013, COMP GEOM-THEOR APPL, V46, P93, DOI 10.1016/j.comgeo.2012.04.007; Bauer M., 2016, RIEMANNIAN COMPUTING, P233, DOI DOI 10.1007/978-3-319-22957-7_11; Bauer M., 2020, CALC VAR PARTIAL DIF, V59, P1, DOI DOI 10.1007/S00526-019-1640-Y; Bauer M, 2019, ESAIM CONTR OPTIM CA, V25, DOI 10.1051/cocv/2018053; Bauer M, 2019, LECT NOTES COMPUT SC, V11712, P13, DOI 10.1007/978-3-030-26980-7_2; Bauer M, 2017, LECT NOTES COMPUT SC, V10551, P152, DOI 10.1007/978-3-319-67675-3_14; Bauer M, 2014, J MATH IMAGING VIS, V50, P60, DOI 10.1007/s10851-013-0490-z; Bauer M, 2012, SIAM J IMAGING SCI, V5, P244, DOI 10.1137/100807983; Bauer M, 2011, J GEOM MECH, V3, P389, DOI 10.3934/jgm..2011.3.389; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Bernal J., 2016, IEEE C COMP VIS PATT, P111; Bhattacharya A, 2012, INST MATH STAT MG, P1; Bobenko A., 2008, DISCRETE DIFFERENTIA; Bruveris M, 2015, J GEOM MECH, V7, P125, DOI 10.3934/jgm.2015.7.125; Bruveris M, 2014, FORUM MATH SIGMA, V2, DOI 10.1017/fms.2014.19; Cao Q, 2015, PHYS MED BIOL, V60, P947, DOI 10.1088/0031-9155/60/3/947; Cervera V., 1991, DIFFER GEOM APPL, V1, P391; Charlier B, 2017, FOUND COMPUT MATH, V17, P287, DOI 10.1007/s10208-015-9288-2; Charlier B., 2020, ARXIV PREPRINT ARXIV; Charon N., 2020, RIEMANNIAN GEOMETRIC, P441; Charon N, 2014, J MATH IMAGING VIS, V48, P413, DOI 10.1007/s10851-012-0413-4; Charon N, 2013, SIAM J IMAGING SCI, V6, P2547, DOI 10.1137/130918885; Cury Claire, 2013, Geometric Science of Information. First International Conference, GSI 2013. Proceedings. LNCS 8085, P103, DOI 10.1007/978-3-642-40020-9_10; Dryden I.L., 1998, STAT SHAPE ANAL WILE; Federer H., 1969, GEOMETRIC MEASURE TH, V153; Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9; Frenkel M, 2003, LECT NOTES COMPUT SC, V2683, P35; Frohlich S, 2011, COMPUT GRAPH FORUM, V30, P2246, DOI 10.1111/j.1467-8659.2011.01974.x; Geirhos R, 2018, ARXIV PREPRINT ARXIV; Glaunes J, 2008, INT J COMPUT VISION, V80, P317, DOI 10.1007/s11263-008-0141-9; Grzegorzek M., 2013, SENS ALG APPL DAGST, V8200; JERMYN I. H., 2017, SYNTH LECT COMPUT VI, V12, P1, DOI [10.1007/978-3-031-01819-0, DOI 10.2200/S00785ED1V01Y201707COV012]; Jermyn IH, 2012, LECT NOTES COMPUT SC, V7576, P804, DOI 10.1007/978-3-642-33715-4_58; Kaltenmark I., 2017, COMPUTER VISION PATT; Kendall DG., 1999, SHAPE SHAPE THEORY, DOI [10.1002/9780470317006, DOI 10.1002/9780470317006]; Kilian M, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276457, 10.1145/1239451.1239515]; Klassen E, 2020, ARCH MATH-BRNO, V56, P107, DOI 10.5817/AM2020-2-107; Kokoszka P., 2017, INTRO FUNCTIONAL DAT, DOI [10.1201/9781315117416, DOI 10.1201/9781315117416]; Kurtek S, 2012, IEEE T PATTERN ANAL, V34, P1717, DOI 10.1109/TPAMI.2011.233; Laga H, 2017, IEEE T PATTERN ANAL, V39, P2451, DOI 10.1109/TPAMI.2016.2647596; Lahiri S., 2015, GEOMETRY IMAGING COM, V2, P133, DOI DOI 10.4310/GIC.2015.V2.N3.A1; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Marin R, 2019, INT CONF 3D VISION, P230, DOI 10.1109/3DV.2019.00034; Mennucci ACG, 2008, INTERFACE FREE BOUND, V10, P423; Michor PW, 2007, APPL COMPUT HARMON A, V23, P74, DOI 10.1016/j.acha.2006.07.004; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Michor PW, 2005, DOC MATH, V10, P217; Michor PW., 2008, TOPICS DIFFERENTIAL; Miller MI, 2015, FRONT BIOENG BIOTECH, V3, DOI 10.3389/fbioe.2015.00054; Minh H. Q., 2016, ALGORITHMIC ADV RIEM; Needham T, 2020, SIAM J IMAGING SCI, V13, P445, DOI [10.1137/19M1265132, 10.1137/19m1265132]; Niethammer M, 2019, PROC CVPR IEEE, P8455, DOI [10.1109/CVPR.2019.00866, 10.1109/cvpr.2019.00866]; Pennec X., 2019, RIEMANNIAN GEOMETRIC; Roussillon P, 2016, SIAM J IMAGING SCI, V9, P1991, DOI 10.1137/16M1070529; Rumpf M., 2015, HDB MATH METHODS IMA, V2, P1819, DOI [10.1007/978-1-4939-0790-8_56, DOI 10.1007/978-1-4939-0790-8_56]; Rumpf M., 2014, GAMM MITTEILUNGEN, V37, P184, DOI [10.1002/gamm.201410009, DOI 10.1002/GAMM.201410009]; Rumpf M, 2015, IMA J NUMER ANAL, V35, P1011, DOI 10.1093/imanum/dru027; Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951; Sheffer A, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000011; Srivastava A, 2016, SPRINGER SER STAT, P1, DOI 10.1007/978-1-4939-4020-2; Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184; Su Z., 2020, IEEE C COMP VIS PATT; Su Z, 2020, J MATH IMAGING VIS, V62, P1087, DOI 10.1007/s10851-020-00959-4; Sullivan J M, 2008, OBERWOLFACH SEMIN, P175; Sundaramoorthi G, 2011, SIAM J IMAGING SCI, V4, P109, DOI 10.1137/090781139; Tumpach AB., 2016, NOT AM MATH SOC, V63, P342, DOI 10.1090/noti1350; Tumpach AB, 2016, IEEE T PATTERN ANAL, V38, P46, DOI 10.1109/TPAMI.2015.2430319; Turaga P.K., 2016, RIEMANNIAN COMPUTING, DOI [10.1007/978-3-319-22957-7, DOI 10.1007/978-3-319-22957-7]; Vaillant M, 2005, LECT NOTES COMPUT SC, V3565, P381; Willmore T. J., 1993, RIEMANNIAN GEOMETRY; Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685; Younes L., 2010, SHAPES DIFFEOMORPHIS, DOI [10.1007/978-3-642-12055-8, DOI 10.1007/978-3-642-12055-8]; Younes L, 2008, REND LINCEI-MAT APPL, V19, P25	76	1	1	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2021	129	8					2425	2444		10.1007/s11263-021-01476-6	http://dx.doi.org/10.1007/s11263-021-01476-6		MAY 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TR3NV		Green Submitted			2022-12-18	WOS:000654081500001
J	Chen, H; Li, YF; Deng, YJ; Lin, GS				Chen, Hao; Li, Youfu; Deng, Yongjian; Lin, Guosheng			CNN-Based RGB-D Salient Object Detection: Learn, Select, and Fuse	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						RGB-D; Salient object detection; Convolutional neural network; Cross-modal distillation		The goal of this work is to present a systematic solution for RGB-D salient object detection, which addresses the following three aspects with a unified framework: modal-specific representation learning, complementary cue selection, and cross-modal complement fusion. To learn discriminative modal-specific features, we propose a hierarchical cross-modal distillation scheme, in which we use the progressive predictions from the well-learned source modality to supervise learning feature hierarchies and inference in the new modality. To better select complementary cues, we formulate a residual function to incorporate complements from the paired modality adaptively. Furthermore, a top-down fusion structure is constructed for sufficient cross-modal cross-level interactions. The experimental results demonstrate the effectiveness of the proposed cross-modal distillation scheme in learning from a new modality, the advantages of the proposed multi-modal fusion pattern in selecting and fusing cross-modal complements, and the generalization of the proposed designs in different tasks.	[Chen, Hao] Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China; [Chen, Hao] Southeast Univ, Minist Educ, Key Lab Comp Network & Informat Integrat, Nanjing 211189, Peoples R China; [Chen, Hao; Li, Youfu; Deng, Yongjian] City Univ Hong Kong, Dept Mech Engn, Kowloon, Hong Kong, Peoples R China; [Chen, Hao; Lin, Guosheng] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore	Southeast University - China; Southeast University - China; City University of Hong Kong; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Li, YF (corresponding author), City Univ Hong Kong, Dept Mech Engn, Kowloon, Hong Kong, Peoples R China.	meyfli@cityu.edu.hk	deng, yj/GZM-1104-2022	Deng, Yongjian/0000-0001-6253-3564	Research Grants Council of Hong Kong [CityU 11203619, 11213420]; Delta-NTU Corporate Lab for Cyber-Physical Systems; Delta Electronics Inc.; National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme; National Research Foundation Singapore under its AI Singapore Program [AISG-RP-2018-003]; MOE Tier-1 Research [RG22/19(S)]	Research Grants Council of Hong Kong(Hong Kong Research Grants Council); Delta-NTU Corporate Lab for Cyber-Physical Systems; Delta Electronics Inc.; National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme(National Research Foundation, Singapore); National Research Foundation Singapore under its AI Singapore Program; MOE Tier-1 Research	This work was supported in part by the Research Grants Council of Hong Kong under Project CityU 11203619, 11213420; in part by the Delta-NTU Corporate Lab for Cyber-Physical Systems with Delta Electronics Inc.; in part by the National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme; in part by the National Research Foundation Singapore under its AI Singapore Program under Award AISG-RP-2018-003; and in part by the MOE Tier-1 Research under Grant RG22/19 (S).	Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130; Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30; Camplani M., 2015, P BRIT MACH VIS C, P145; Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322; Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104; Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI 10.1007/s11263-021-01490-8; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Cheng Y., 2014, P INT C INTERNET MUL, P23; Cheng YH, 2017, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR.2017.161; Christoudias CM, 2010, LECT NOTES COMPUT SC, V6311, P677, DOI 10.1007/978-3-642-15549-9_49; Ciptadi A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.112; Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819; Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347; Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98; Du DP, 2019, PROC CVPR IEEE, P11828, DOI 10.1109/CVPR.2019.01211; Fan, 2018, PROC CVPR IEEE; Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406; Fan XX, 2014, INT CONF DIGIT SIG, P454, DOI 10.1109/ICDSP.2014.6900706; FENG D, 2016, PROC CVPR IEEE, P2343, DOI DOI 10.1109/CVPR.2016.257; Fu HZ, 2017, IEEE T IMAGE PROCESS, V26, P1418, DOI 10.1109/TIP.2017.2651369; FU HZ, 2015, PROC CVPR IEEE, P4428, DOI DOI 10.1109/CVPR.2015; Garcia NC, 2018, LECT NOTES COMPUT SC, V11212, P106, DOI 10.1007/978-3-030-01237-3_7; Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39; Guo J., 2016, PAPER PRESENTED IEEE, P1; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Gupta S, 2015, INT J COMPUT VISION, V112, P133, DOI 10.1007/s11263-014-0777-6; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378; Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14; Hinton G., 2015, ARXIV150302531; Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96; Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455; Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563; Huang Q., 2017, IEEE T CYBERNETICS; Huang Z., 2017, ARXIV PREPRINT ARXIV; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jianqiang Ren, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P25, DOI 10.1109/CVPRW.2015.7301391; Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8; Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689; Li GB, 2019, IEEE T IMAGE PROCESS, V28, P1591, DOI 10.1109/TIP.2018.2878956; Li J, 2019, PROC CVPR IEEE, P7685, DOI 10.1109/CVPR.2019.00788; Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359; Li Quanquan, 2017, CVPR; Lin D, 2017, IEEE I CONF COMP VIS, P1320, DOI 10.1109/ICCV.2017.147; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mahadevan V, 2013, IEEE T PATTERN ANAL, V35, P541, DOI 10.1109/TPAMI.2012.98; Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39; Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708; Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533; Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7; Piao YR, 2020, AAAI CONF ARTIF INTE, V34, P11865; Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735; Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556; Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981; Romero Adriana, 2014, ARXIV14126550; Shao L, 2006, PATTERN RECOGN, V39, P1932, DOI 10.1016/j.patcog.2006.04.010; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Su D., 2018, PATTERN RECOGN; Wang AR, 2015, IEEE I CONF COMP VIS, P1125, DOI 10.1109/ICCV.2015.134; Wang WY, 2018, LECT NOTES COMPUT SC, V11215, P144, DOI 10.1007/978-3-030-01252-6_9; Xu XY, 2017, PATTERN RECOGN, V72, P300, DOI 10.1016/j.patcog.2017.07.026; Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153; Yang JM, 2017, IEEE T PATTERN ANAL, V39, P576, DOI 10.1109/TPAMI.2016.2547384; Zeng J, 2019, PROC CVPR IEEE, P6146, DOI 10.1109/CVPR.2019.00631; Zhang M, 2019, ADV NEUR IN, V32; Zhang M, 2020, IEEE T IMAGE PROCESS, V29, P6276, DOI 10.1109/TIP.2020.2990341; Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492; Zhao X., 2020, ECCV, P35, DOI [DOI 10.1007/978-3-030-58536-5_3, 10.1007/978-3-030-58536-5_3]; Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z; Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355; Zhu HY, 2016, PROC CVPR IEEE, P2969, DOI 10.1109/CVPR.2016.324	81	1	1	5	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2076	2096		10.1007/s11263-021-01452-0	http://dx.doi.org/10.1007/s11263-021-01452-0		MAY 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW		Green Submitted			2022-12-18	WOS:000647351500002
J	Wan, ZY; Chen, DD; Liao, J				Wan, Ziyu; Chen, Dongdong; Liao, Jing			Visual Structure Constraint for Transductive Zero-Shot Learning in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision; Zero-shot learning; Visual structure constraint		To recognize objects of the unseen classes, most existing Zero-Shot Learning(ZSL) methods first learn a compatible projection function between the common semantic space and the visual space based on the data of source seen classes, then directly apply it to the target unseen classes. However, for data in the wild, distributions between the source and target domain might not match well, thus causing the well-known domain shift problem. Based on the observation that visual features of test instances can be separated into different clusters, we propose a new visual structure constraint on class centers for transductive ZSL, to improve the generality of the projection function (i.e.alleviate the above domain shift problem). Specifically, three different strategies (symmetric Chamfer-distance, Bipartite matching distance, and Wasserstein distance) are adopted to align the projected unseen semantic centers and visual cluster centers of test instances. We also propose two new training strategies to handle the data in the wild, where many unrelated images in the test dataset may exist. This realistic setting has never been considered in previous methods. Extensive experiments demonstrate that the proposed visual structure constraint brings substantial performance gain consistently and the new training strategies make it generalize well for data in the wild. The source code is available at https:// github.com/ raywzy/VSC..	[Wan, Ziyu; Liao, Jing] City Univ Hong Kong, Kowloon, Hong Kong, Peoples R China; [Chen, Dongdong] Microsoft Cloud AI, Lexington, KY USA	City University of Hong Kong	Liao, J (corresponding author), City Univ Hong Kong, Kowloon, Hong Kong, Peoples R China.	ziyuwan2-c@my.cityu.edu.hk; cddlyf@gmail.com; jingliao@cityu.edu.hk		LIAO, Jing/0000-0001-7014-5377; Chen, Dongdong/0000-0002-4642-4373	ECS grant from the Research Grants Council of the Hong Kong Special Administrative Region, China [CityU 21209119]	ECS grant from the Research Grants Council of the Hong Kong Special Administrative Region, China	The work described in this paper was fully supported by an ECS grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CityU 21209119).	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; al, 2013, NIPS; al, 2015, ICCV; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793; Arjovsky M., 2017, ARXIV170107875; Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626; Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Elhoseiny M, 2013, IEEE I CONF COMP VIS, P2584, DOI 10.1109/ICCV.2013.321; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2; Fu YW, 2016, PROC CVPR IEEE, P5337, DOI 10.1109/CVPR.2016.576; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Fu YW, 2014, LECT NOTES COMPUT SC, V8690, P488, DOI 10.1007/978-3-319-10605-2_32; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758; Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779; Li YN, 2017, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2017.553; Liu SC, 2018, ADV NEUR IN, V31; Lu Y., 2016, IJCAI; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220; Norouzi Mohammad, 2014, ICLR; Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Radovanovic M, 2010, J MACH LEARN RES, V11, P2487; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Romera-Paredes Bernardino, 2015, ICML; Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9; Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vyas Maunil R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P70, DOI 10.1007/978-3-030-58577-8_5; Wah C., 2011, TECH REP; Wan ZY, 2019, ADV NEUR IN, V32; Wang WL, 2018, AAAI CONF ARTIF INTE, P4211; Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717; Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Ye M, 2017, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR.2017.542; Ye SQ, 2017, IEEE INT WORKS MACH, DOI 10.1109/TPAMI.2017.2762295; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang ZM, 2016, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2016.649; Zhang ZM, 2016, LECT NOTES COMPUT SC, V9911, P533, DOI 10.1007/978-3-319-46478-7_33; Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474; Zhao A, 2018, ADV NEUR IN, V31; Zhu Xiaojin, 2002, TECHNICAL REPORT, P1; Zhu Y., 2019, ARXIV PREPRINT ARXIV; Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111	62	1	1	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1893	1909		10.1007/s11263-021-01451-1	http://dx.doi.org/10.1007/s11263-021-01451-1		APR 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU					2022-12-18	WOS:000641238700007
J	Xu, YC; Wang, YK; Tsogkas, S; Wan, JQ; Bai, X; Dickinson, S; Siddiqi, K				Xu, Yongchao; Wang, Yukang; Tsogkas, Stavros; Wan, Jianqiang; Bai, Xiang; Dickinson, Sven; Siddiqi, Kaleem			DeepFlux for Skeleton Detection in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Skeleton detection; Medial axis; Flux representation; Convolutional neural network; Mid-level representation	EDGE-DETECTION; RECOGNITION; SEGMENTATION; BOUNDARIES; SHAPE	The medial axis, or skeleton, is a fundamental object representation that has been extensively used in shape recognition. Yet, its extension to natural images has been challenging due to the large appearance and scale variations of objects and complex background clutter that appear in this setting. In contrast to recent methods that address skeleton extraction as a binary pixel classification problem, in this article we present an alternative formulation for skeleton detection. We follow the spirit of flux-based algorithms for medial axis recovery by training a convolutional neural network to predict a two-dimensional vector field encoding the flux representation. The skeleton is then recovered from the flux representation, which captures the position of skeletal pixels relative to semantically meaningful entities (e.g., image points in spatial context, and hence the implied object boundaries), resulting in precise skeleton detection. Moreover, since the flux representation is a region-based vector field, it is better able to cope with object parts of large width. We evaluate the proposed method, termed DeepFlux, on six benchmark datasets, consistently achieving superior performance over state-of-the-art methods. Finally, we demonstrate an application of DeepFlux, augmented with a skeleton scale estimation module, to detect objects in aerial images. This combination yields results that are competitive with models trained specifically for object detection, showcasing the versatility and effectiveness of mid-level representations in high-level tasks. An implementation of our method is available at .	[Xu, Yongchao] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China; [Wang, Yukang; Wan, Jianqiang; Bai, Xiang] Huazhong Univ Sci & Technol, Sch EiC, Wuhan, Peoples R China; [Tsogkas, Stavros; Dickinson, Sven] Univ Toronto, Toronto, ON, Canada; [Dickinson, Sven] Vector Inst Artificial Intelligence, Toronto, ON, Canada; [Tsogkas, Stavros; Dickinson, Sven] Samsung Toronto AI Res Ctr, Toronto, ON, Canada; [Siddiqi, Kaleem] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada; [Siddiqi, Kaleem] McGill Univ, Ctr Intelligent Machines, Montreal, PQ, Canada	Wuhan University; Huazhong University of Science & Technology; University of Toronto; McGill University; McGill University	Bai, X (corresponding author), Huazhong Univ Sci & Technol, Sch EiC, Wuhan, Peoples R China.	yongchao.xu@whu.edu.cn; wangyk@hust.edu.cn; tsogkas@cs.toronto.edu; jianqw@hust.edu.cn; xbai@hust.edu.cn; sven@cs.toronto.edu; siddiqi@cim.mcgill.ca			NSFC [61936003, 61703171]; Major Project for New Generation of AI [2018AAA0100400]; Young Elite Scientists Sponsorship Program by CAST; National Program for Support of Top-Notch Young Professionals; Program for HUST Academic Frontier Youth Team; Natural Sciences and Engineering Research Council of Canada (NSERC)	NSFC(National Natural Science Foundation of China (NSFC)); Major Project for New Generation of AI; Young Elite Scientists Sponsorship Program by CAST; National Program for Support of Top-Notch Young Professionals; Program for HUST Academic Frontier Youth Team; Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported in part by NSFC 61936003 and 61703171, and the Major Project for New Generation of AI underGrant No. 2018AAA0100400. YongchaoXuwas supported by the Young Elite Scientists Sponsorship Program by CAST. The work of Xiang Bai was supported by the National Program for Support of Top-Notch Young Professionals and in part by the Program for HUST Academic Frontier Youth Team. Sven Dickinson and Kaleem Siddiqi would like to thank the Natural Sciences and Engineering Research Council of Canada (NSERC) for research funding.	Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231; Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305; Bai X, 2009, IEEE I CONF COMP VIS, P575, DOI 10.1109/ICCV.2009.5459188; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; Borenstein E, 2002, LECT NOTES COMPUT SC, V2351, P109; Chen LC, 2018, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2018.00422; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen X, 2015, CORR, V1504, P325; Ci Hai, 2018, P EUR C COMP VIS ECC, P501; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dickinson S. J., 2009, OBJECT CATEGORIZATIO, DOI [10.1017/cbo9780511635465, DOI 10.1017/CBO9780511635465]; Dimitrov M, 2013, IEEE INT CONF BIG DA; Ding J, 2019, PROC CVPR IEEE, P2844, DOI 10.1109/CVPR.2019.00296; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; Dufresne-Camaro C. O., 2020, P IEEE INT C COMP VI; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb P.F., 2012, THEORY COMPUT, V8, P415, DOI DOI 10.4086/TOC.2012.V008A019; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Girshick R, 2011, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2011.6126270; Jang JH, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P18, DOI 10.1109/ICCV.2001.937586; Jerripothula KR, 2017, PROC CVPR IEEE, P3881, DOI 10.1109/CVPR.2017.413; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang Y., 2017, ARXIV170609579; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Ke W, 2017, PROC CVPR IEEE, P302, DOI 10.1109/CVPR.2017.40; Kinga D., P INT C LEARN REPR, V5; Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225; Lee TSH, 2013, IEEE I CONF COMP VIS, P1753, DOI 10.1109/ICCV.2013.220; Levinshtein A, 2013, INT J COMPUT VISION, V104, P117, DOI 10.1007/s11263-013-0614-3; Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773; Lindeberg T, 2013, J MATH IMAGING VIS, V46, P177, DOI 10.1007/s10851-012-0378-3; Liu C, 2018, LECT NOTES COMPUT SC, V11206, P136, DOI 10.1007/978-3-030-01216-8_9; Liu TL, 1998, INT C PATT RECOG, P994, DOI 10.1109/ICPR.1998.711856; Liu X., 2017, P ICCV WORKSH DET SY, V6, P8; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo WJ, 2016, ADV NEUR IN, V29; Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020; MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Mattyus G., 2017, P IEEE INT C COMP VI; Nedzved A., 2006, P IEEE INT C PATT RE; Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469; Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148; Shen W, 2017, IEEE T IMAGE PROCESS, V26, P5298, DOI 10.1109/TIP.2017.2735182; Shen W, 2016, PROC CVPR IEEE, P222, DOI 10.1109/CVPR.2016.31; Shen W, 2016, PATTERN RECOGN, V52, P306, DOI 10.1016/j.patcog.2015.10.015; Shen W, 2011, PATTERN RECOGN, V44, P196, DOI 10.1016/j.patcog.2010.08.021; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703; Siddiqi K, 2002, INT J COMPUT VISION, V48, P215, DOI 10.1023/A:1016376116653; Simonyan K, 2015, 3 INT C LEARN REPR I; Sironi A, 2014, PROC CVPR IEEE, P2697, DOI 10.1109/CVPR.2014.351; Trinh NH, 2011, INT J COMPUT VISION, V94, P215, DOI 10.1007/s11263-010-0412-0; Tsogkas S, 2017, IEEE I CONF COMP VIS, P2727, DOI 10.1109/ICCV.2017.295; Tsogkas S, 2012, LECT NOTES COMPUT SC, V7578, P41, DOI 10.1007/978-3-642-33786-4_4; Wang YK, 2019, PROC CVPR IEEE, P5282, DOI 10.1109/CVPR.2019.00543; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418; Xia GS, 2017, IEEE T GEOSCI REMOTE, V55, P3965, DOI 10.1109/TGRS.2017.2685945; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Xu W., 2019, BRIT MACH VIS C; Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589; Yang X, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010132; Yu ZY, 2004, PROC CVPR IEEE, P415; Zhang QP, 2007, IEEE T IMAGE PROCESS, V16, P310, DOI 10.1109/TIP.2006.887731; Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871; Zhao K, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1191; Zhu SC, 1996, INT J COMPUT VISION, V20, P187; Zucker SW, 2012, J PHYSIOL-PARIS, V106, P297, DOI 10.1016/j.jphysparis.2012.08.001; 2008, MEDIAL REPRESENTATIO, V37, P1, DOI DOI 10.1007/978-1-4020-8658-8	73	1	1	3	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1323	1339		10.1007/s11263-021-01430-6	http://dx.doi.org/10.1007/s11263-021-01430-6		JAN 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000613054200001
J	Black, S; Keshavarz, S; Souvenir, R				Black, Samuel; Keshavarz, Somayeh; Souvenir, Richard			Evaluation of Inpainting and Augmentation for Censored Image Queries	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image inpainting; Data augmentation; Classification; Retrieval; Image censoring		Images can be censored by masking the region(s) of interest with a solid color or pattern. When a censored image is used for classification or matching, the mask itself may impact the results. Recent work in image inpainting and data augmentation provide two different approaches for dealing with censored images. In this paper, we perform an extensive evaluation of these methods to understand if the impact of censoring can be mitigated for image classification and retrieval. Results indicate that modern learning-based inpainting approaches outperform augmentation strategies and that metrics typically used to evaluate inpainting performance (e.g., reconstruction accuracy) do not necessarily correspond to improved classification or retrieval, especially in the case of person-shaped masked regions.	[Black, Samuel; Keshavarz, Somayeh; Souvenir, Richard] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple University	Black, S (corresponding author), Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA.	sam.black@temple.edu; somayeh.keshavarz@temple.edu; souvenir@temple.edu		Black, Samuel/0000-0002-6478-8736				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2014, INT J COMPUTER SCI I; Arjovsky M., 2017, ARXIV170107875; Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; BERTALMIO M, 2001, PROC CVPR IEEE, P355, DOI DOI 10.1109/CVPR.2001.990497; Black S, 2020, IEEE WINT CONF APPL, P1049, DOI 10.1109/WACV45572.2020.9093362; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Chan T., 2000, UCLA COMPUTATIONAL A; Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Fong RC, 2019, IEEE INT CONF COMP V, P4158, DOI 10.1109/ICCVW.2019.00511; Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004; Gulrajani I, 2017, P NIPS 2017; Hensel M, 2017, ADV NEUR IN, V30; Hong X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2033, DOI 10.1145/3343031.3351002; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Inoue H., 2018, ARXIV PREPRINT ARXIV; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kohler R, 2014, LECT NOTES COMPUT SC, V8753, P523, DOI 10.1007/978-3-319-11752-2_43; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Moreno-Barea FJ, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P728, DOI 10.1109/SSCI.2018.8628917; Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Perez L., 2017, ARXIV; Rane SD, 2003, IEEE T IMAGE PROCESS, V12, P296, DOI 10.1109/TIP.2002.804264; Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591; Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381; Sohn Kihyuk, 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628; Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI 10.1007/978-3-030-01216-8_1; Soudry D., 2019, AUGMENT YOUR BATCH B; Stylianou A, 2019, AAAI CONF ARTIF INTE, P726; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang X, 2020, NEURAL COMPUT APPL, V32, P15503, DOI 10.1007/s00521-020-04748-3; Wang YY, 2018, INT C MANAGE SCI ENG, P331, DOI 10.1109/ICMSE.2018.8745361; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wu X, 2017, TSINGHUA SCI TECHNOL, V22, P660, DOI 10.23919/TST.2017.8195348; Xie J., 2012, ADV NEURAL INFORM PR, P341, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605; Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158; Xu ZB, 2010, IEEE T IMAGE PROCESS, V19, P1153, DOI 10.1109/TIP.2010.2042098; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153; Zhong ZM, 2020, PROC EUR CONF ANTENN; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18	63	1	1	1	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					977	997		10.1007/s11263-020-01403-1	http://dx.doi.org/10.1007/s11263-020-01403-1		JAN 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000605541100001
J	Morgado, P; Li, YS; Pereira, JC; Saberian, M; Vasconcelos, N				Morgado, Pedro; Li, Yunsheng; Costa Pereira, Jose; Saberian, Mohammad; Vasconcelos, Nuno			Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Proxy embeddings; Metric learning; Image retrieval; Hashing; Transfer learning	IMAGE; QUANTIZATION; QUERY	Image hash codes are produced by binarizing the embeddings of convolutional neural networks (CNN) trained for either classification or retrieval. While proxy embeddings achieve good performance on both tasks, they are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use of a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and a procedure to design proxy sets that are nearly optimal for both classification and hashing is introduced. The resultinghash-consistent large margin(HCLM) proxies are shown to encourage saturation of hashing units, thus guaranteeing a small binarization error, while producing highly discriminative hash-codes. A semantic extension (sHCLM), aimed to improve hashing performance in a transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant improvements over state-of-the-art hashing procedures on several small and large datasets, both within and beyond the set of training classes.	[Morgado, Pedro; Li, Yunsheng; Vasconcelos, Nuno] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA; [Costa Pereira, Jose] Noahs Ark Lab, Huawei Technol, London, England; [Saberian, Mohammad] Netflix, Scotts Valley, CA USA	University of California System; University of California San Diego; Huawei Technologies; Netflix, Inc.	Morgado, P (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.	pmaravil@eng.ucsd.edu	Morgado, Pedro/ABE-8309-2021	Morgado, Pedro/0000-0002-0955-6510; Vasconcelos, Nuno/0000-0002-9024-4302	Portuguese Ministry of Sciences and Education [109135/2015]; NSF [IIS-1546305, IIS-1637941, IIS-1924937]; NVIDIA	Portuguese Ministry of Sciences and Education; NSF(National Science Foundation (NSF)); NVIDIA	This work was funded by graduate fellowship 109135/2015 from the Portuguese Ministry of Sciences and Education, NSF Grants IIS-1546305, IIS-1637941, IIS-1924937, and NVIDIA GPU donations.	Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Andoni A, 2006, ANN IEEE SYMP FOUND, P459; [Anonymous], 1930, THESIS; Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38; Bach JR, 1996, P SOC PHOTO-OPT INS, V2670, P76, DOI 10.1117/12.234785; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Barndorff-Nielsen, 2014, STAT THEORY; Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; Cakir F., 2018, P EUROPEAN C COMPUTE, P332; Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55; Cao Y, 2016, AAAI CONF ARTIF INTE, P3457; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Chua T.-S., 2009, P ACM INT C IM VID R, P1, DOI 10.1145/1646396.1646452; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146; Frome Andrea, 2013, NEURIPS; Goldberger J, 2004, ADV NEURAL INF PROCE, V17, P513, DOI DOI 10.5555/2976040.2976105; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; He K, 2018, PROC CVPR IEEE, P4023, DOI 10.1109/CVPR.2018.00423; Huang SS, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P84, DOI 10.1145/3126686.3126773; Jain H., 2017, ICCV; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; jia Li L., 2010, NIPS, DOI [10.1184/R1/6475985.v1, DOI 10.1184/R1/6475985.V1]; Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Li Q, 2017, ADV NEUR IN, V30; Li W., 2016, INT JOINT C ARTIFICI, P1711; Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133; Lin K, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301269; Liong E., 2015, IEEE C COMP VIS PATT; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P2352, DOI 10.1109/TIP.2017.2678163; Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220; Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47; Mu Y, 2010, AAAI C ART INT; NELDER JA, 1972, J R STAT SOC SER A-G, V135, P370, DOI 10.2307/2344614; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Pereira J. C, 2014, COMPUTER VISION IMAG, V124; Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138; Rohrbach M, 2011, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2011.5995627; Sablayrolles A, 2017, INT CONF ACOUST SPEE, P1732, DOI 10.1109/ICASSP.2017.7952453; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; Smith J. R., 1997, ACM INT C MULT; Sohn Kihyuk, 2016, NEURIPS, DOI DOI 10.5555/3157096.3157304; Song H. O., 2016, ARXIV161201213; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; WANG JJ, 2010, PROC CVPR IEEE, P3360, DOI DOI 10.1109/CVPR.2010.5540018; Wang X., 2016, P AS C COMP VIS, P70; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Weiss Y., 2009, NIPS; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wright S., 1999, SPRINGER SCI, V35, P7; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812; Zhang DY, 2018, IEEE T CIRC SYST VID, V28, P2622, DOI 10.1109/TCSVT.2017.2723429; Zhang J., 2016, IEEE T MULTIMEDIA; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang RM, 2019, IEEE T IMAGE PROCESS, V28, P4870, DOI 10.1109/TIP.2019.2911488; Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763; Zhong GQ, 2016, IEEE IJCNN, P2236, DOI 10.1109/IJCNN.2016.7727476; Zhu H, 2016, AAAI CONF ARTIF INTE, P2415	76	1	1	1	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					419	438		10.1007/s11263-020-01362-7	http://dx.doi.org/10.1007/s11263-020-01362-7		SEP 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000572841700001
J	Zhu, JY; Li, HS; Shechtman, E; Liu, MY; Kautz, J; Torralba, A				Zhu, Jun-Yan; Li, Hongsheng; Shechtman, Eli; Liu, Ming-Yu; Kautz, Jan; Torralba, Antonio			Guest Editorial: Generative Adversarial Networks for Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material							MODELS		[Zhu, Jun-Yan; Shechtman, Eli] Adobe Res, San Jose, CA USA; [Li, Hongsheng] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Liu, Ming-Yu; Kautz, Jan] NVIDIA Res, Santa Clara, CA USA; [Torralba, Antonio] MIT CSAIL, Cambridge, MA USA	Adobe Systems Inc.; Chinese University of Hong Kong; Massachusetts Institute of Technology (MIT)	Zhu, JY (corresponding author), Adobe Res, San Jose, CA USA.	junyanz@cs.cmu.edu; hsli@ee.cuhk.edu.hk; elishe@adobe.com; mingyul@nvidia.com; jkautz@nvidia.com; torralba@csail.mit.edu						Bau D. A, 2019, INT C LEARN REPR ICL; Blanz Volker, 1999, ACM SIGGRAPH; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Dinh L., 2016, ARXIV160508803CSSTAT; Dosovitskiy Alexey, 2015, IEEE C COMP VIS PATT; Fei-Fei L., 2005, IEEE C COMP VIS PATT; Fergus R., 2003, Proceedings 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pII; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Karras T., 2020, IEEE C COMP VIS PATT; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Learned-Miller EG, 2006, IEEE T PATTERN ANAL, V28, P236, DOI 10.1109/TPAMI.2006.34; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Revow M, 1996, IEEE T PATTERN ANAL, V18, P592, DOI 10.1109/34.506410; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Tu Zhuowen, 2007, IEEE C COMP VIS PATT; Vinyals O., 2016, ADV NEURAL INFORM PR; Weber M, 2000, PROC CVPR IEEE, P101, DOI 10.1109/CVPR.2000.854754; Zhu Jun-Yan, 2016, EUR C COMP VIS ECCV; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	23	1	1	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2363	2365		10.1007/s11263-020-01380-5	http://dx.doi.org/10.1007/s11263-020-01380-5		SEP 2020	3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Bronze			2022-12-18	WOS:000568976600002
J	Singh, P; Verma, VK; Rai, P; Namboodiri, VP				Singh, Pravendra; Verma, Vinay Kumar; Rai, Piyush; Namboodiri, Vinay P.			HetConv: Beyond Homogeneous Convolution Kernels for Deep CNNs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Efficient convolutional neural networks; Heterogeneous convolution; FLOPs compression; Model compression; Efficient visual recognition		While usage of convolutional neural networks (CNN) is widely prevalent, methods proposed so far always have considered homogeneous kernels for this task. In this paper, we propose a new type of convolution operation using heterogeneous kernels. The proposed Heterogeneous Kernel-Based Convolution (HetConv) reduces the computation (FLOPs) and the number of parameters as compared to standard convolution operation while it maintains representational efficiency. To show the effectiveness of our proposed convolution, we present extensive experimental results on the standard CNN architectures such as VGG, ResNet, Faster-RCNN, MobileNet, and SSD. We observe that after replacing the standard convolutional filters in these architectures with our proposed HetConv filters, we achieve 1.5 x FLOPs based improvement in speed while it maintains (sometimes improves) the accuracy. We also compare our proposed convolution with group/depth wise convolution and show that it achieves more FLOPs reduction with significantly higher accuracy. Moreover, we demonstrate the efficacy of HetConv based CNN by showing that it also generalizes on object detection and is not constrained to image classification tasks. We also empirically show that the proposed HetConv convolution is more robust towards the over-fitting problem as compared to standard convolution.	[Singh, Pravendra; Verma, Vinay Kumar; Rai, Piyush; Namboodiri, Vinay P.] Indian Inst Technol Kanpur, Dept Comp Sci & Engn, Kanpur 208016, Uttar Pradesh, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kanpur	Singh, P (corresponding author), Indian Inst Technol Kanpur, Dept Comp Sci & Engn, Kanpur 208016, Uttar Pradesh, India.	psingh@iitk.ac.in; vkverma@cse.iitk.ac.in; piyush@cse.iitk.ac.in; vinaypn@iitk.ac.in	Singh, Pravendra/ABC-9247-2020; Verma, Vinay/ABA-9275-2020	Singh, Pravendra/0000-0003-1001-2219; Namboodiri, Vinay/0000-0001-5262-9722				Abbasi-Asl R., 2017, ARXIV170507356; Alvarez Jose M, 2016, ADV NEURAL INFORM PR, P2270; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Brock A., 2018, ICLR, P1; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cai Han, 2018, ARXIV180602639; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Dentinel Zarembaw, 2014, NEURIPS, P1269; Ding X., 2018, 32 AAAI C ART INT; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Finn C, 2017, PR MACH LEARN RES, V70; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guan Melody, 2018, ICML, P4095; Han S., 2016, P 4 INT C LEARN REPR, P1; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He Kaiming, 2015, CVPR, DOI [10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173]; He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu H., 2016, ARXIV PREPRINT ARXIV; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Iandola F.N., 2016, ARXIV; Ioannou Y, 2017, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2017.633; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jie H., 2017, P IEEE C COMP VIS PA, P99; Kamath P., 2018, ARXIV180306744; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lebedev V, 2016, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR.2016.280; Li H., 2017, P INT C LEARN REPR I, P1; Li Y, 2019, ACS APPL MATER INTER, V11, P12605, DOI 10.1021/acsami.8b20422; Lin J, 2017, ADV NEUR IN, V30; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C., 2018, P EUR C COMP VIS ECC, P19, DOI DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2017, INT C LEARN REPR ICL; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Louizos C, 2017, ADV NEUR IN, V30; Miao H, 2017, PROC INT CONF DATA, P571, DOI 10.1109/ICDE.2017.112; Molchanov P., 2017, P INT C LEARN REPR I, P1; Neklyudov Kirill, 2017, ADV NEURAL INFORM PR, V30; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Robertson D, 2015, ARXIV; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Singh P, 2019, PROC CVPR IEEE, P4830, DOI 10.1109/CVPR.2019.00497; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Stamoulis D., 2019, ARXIV PREPRINT ARXIV; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Sun Ke, 2018, ARXIV180600178; Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1016/J.PATREC.2014.01.008; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Vanhoucke Vincent, 2014, ICLR INV TALK, V1, P2; Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wen W, 2016, ADV NEUR IN, V29; Wu ZX, 2018, PROC CVPR IEEE, P8817, DOI 10.1109/CVPR.2018.00919; Xie GT, 2018, PROC CVPR IEEE, P8847, DOI 10.1109/CVPR.2018.00922; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958; Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI 10.1109/CVPR.2017.512; Zhang X., 2018, SHUFFLENET EXTREMELY; Zhang XY, 2015, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR.2015.7298809; Zhou H, 2016, LECT NOTES COMPUT SC, V9908, P662, DOI 10.1007/978-3-319-46493-0_40; Zoph B., 2017, P1	73	1	1	2	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2068	2088		10.1007/s11263-019-01264-3	http://dx.doi.org/10.1007/s11263-019-01264-3			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG					2022-12-18	WOS:000559365500005
J	Nie, Q; Liu, YH				Nie, Qiang; Liu, Yunhui			View Transfer on Human Skeleton Pose: Automatically Disentangle the View-Variant and View-Invariant Information for Pose Representation Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Representation learning; Human skeleton pose; View transfer; Unsupervised action recognition	ACTION RECOGNITION	Learning a good pose representation is significant for many applications, such as human pose estimation and action recognition. However, the representations learned by most approaches are not intrinsic and their transferability in different datasets and different tasks is limited. In this paper, we introduce a method to learn a versatile representation, which is capable of recovering unseen corrupted skeletons, being applied to the human action recognition, and transferring pose from one view to another view without knowing the relationships of cameras. To this end, a sequential bidirectional recursive network (SeBiReNet) is proposed for modeling kinematic dependency between skeleton joints. Utilizing the SeBiReNet as the core module, a denoising autoencoder is designed to learn intrinsic pose features through the task of recovering corrupted skeletons. Instead of only extracting the view-invariant feature as many other methods, we disentangle the view-invariant feature from the view-variant feature in the latent space and use them together as a representation of the human pose. For a better feature disentanglement, an adversarial augmentation strategy is proposed and applied to the denoising autoencoder. Disentanglement of view-variant and view-invariant features enables us to realize view transfer on 3D poses. Extensive experiments on different datasets and different tasks verify the effectiveness and versatility of the learned representation.	[Nie, Qiang; Liu, Yunhui] Chinese Univ Hong Kong, Hong Kong, Peoples R China	Chinese University of Hong Kong	Nie, Q (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	qnie@mae.cuhk.edu.hk	Nie, Qiang/AAW-6076-2021	Nie, Qiang/0000-0002-2778-4058	Hong Kong RGC [14202918]; InnoHK programme of the HKSAR government via the Hong Kong Centre for Logistics Robotics; Shenzhen Science and Technology Innovation Commission [KQTD20140630150243062]	Hong Kong RGC(Hong Kong Research Grants Council); InnoHK programme of the HKSAR government via the Hong Kong Centre for Logistics Robotics; Shenzhen Science and Technology Innovation Commission	This work is supported in part by the Hong Kong RGC via project 14202918, the InnoHK programme of the HKSAR government via the Hong Kong Centre for Logistics Robotics, and Shenzhen Science and Technology Innovation Commission via KQTD20140630150243062.	Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Caetano C, 2019, SIBGRAPI, P16, DOI 10.1109/SIBGRAPI.2019.00011; Demisse GG, 2018, IEEE COMPUT SOC CONF, P301, DOI 10.1109/CVPRW.2018.00056; Ding WW, 2018, PATTERN RECOGN, V77, P75, DOI 10.1016/j.patcog.2017.12.004; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Guo F, 2016, ONCOGENE, V35, P816, DOI 10.1038/onc.2015.139; Holden Daniel, 2015, PROCEEDING SA 15 SIG, P1, DOI [10.1145/2820903.2820918, DOI 10.1145/2820903.2820918]; Hussein Mohamed E, 2013, 23 INT JOINT C ART I; Irsoy Ozan, 2014, ADV NEURAL INFORM PR, V27, P2096; Jia XF, 2012, INT C PATT RECOG, P3001; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486; Ke QH, 2017, IEEE SIGNAL PROC LET, V24, P731, DOI 10.1109/LSP.2017.2690339; Kundu JN, 2019, IEEE WINT CONF APPL, P1459, DOI 10.1109/WACV.2019.00160; Li CK, 2017, IEEE SIGNAL PROC LET, V24, P624, DOI 10.1109/LSP.2017.2678539; Li JN, 2018, ADV NEUR IN, V31; Liao Shujian, 2019, ARXIV190808286; Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279; Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873; Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306; Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030; Liu ZW, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P997, DOI 10.1145/3077136.3080700; Luo ZJ, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS, ELECTRONICS AND CONTROL (ICCSEC), P320, DOI 10.1109/ICCSEC.2017.8446815; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Nie Q, 2019, IEEE T IMAGE PROCESS, V28, P3959, DOI 10.1109/TIP.2019.2907048; Papadopoulos K., 2019, ARXIV191209745V1; Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389; Ramakrishna V, 2012, LECT NOTES COMPUT SC, V7575, P573, DOI 10.1007/978-3-642-33765-9_41; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Socher R., 2011, P 28 INT C INT C MAC, P129; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Socher Richard, 2010, P NIPS 2010 DEEP LEA; Sun X, 2017, IEEE INT C COMPUT, P260, DOI 10.1109/CSE-EUC.2017.54; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang H., 2017, C COMP VIS PATT REC; Wang HS, 2018, PATTERN RECOGN, V81, P23, DOI 10.1016/j.patcog.2018.03.030; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198; Wang Ze-Lin, 2017, Non-Coding RNA, V3, P7, DOI 10.3390/ncrna3010007; Wei SH, 2017, IEEE IMAGE PROC, P91; Xia L., 2012, IEEE COMP SOC C COMP, V2012, P20, DOI DOI 10.1109/CVPRW.2012.6239233; Yoshiyasu Y., 2018, ARXIV181211328; Yu TH, 2013, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2013.467; Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631; Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644	46	1	1	2	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2021	129	1								10.1007/s11263-020-01354-7	http://dx.doi.org/10.1007/s11263-020-01354-7		AUG 2020	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PU0KV					2022-12-18	WOS:000554434800001
J	Liu, L; Pietikainen, M; Qin, J; Ouyang, WL; Van Gool, L				Liu, Li; Pietikainen, Matti; Qin, Jie; Ouyang, Wanli; Van Gool, Luc			Efficient Visual Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Liu, Li] Natl Univ Def Technol, Coll Syst Engn, Changsha, Peoples R China; [Liu, Li; Pietikainen, Matti] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland; [Qin, Jie; Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland; [Ouyang, Wanli] Univ Sydney, Sydney, NSW, Australia	National University of Defense Technology - China; University of Oulu; Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Sydney	Liu, L (corresponding author), Natl Univ Def Technol, Coll Syst Engn, Changsha, Peoples R China.; Liu, L (corresponding author), Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.	li.liu@oulu.fi; matti.pietikainen@oulu.fi; qinjiebuaa@gmail.com; wanli.ouyang@sydney.edu.au; vangool@vision.ee.ethz.ch	Ouyang, Wanli/I-7135-2018	Ouyang, Wanli/0000-0002-9163-2761					0	1	1	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		1997	2001		10.1007/s11263-020-01351-w	http://dx.doi.org/10.1007/s11263-020-01351-w		JUL 2020	5	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG		Bronze			2022-12-18	WOS:000546900900001
J	Wang, YX; Herranz, L; van de Weijer, J				Wang, Yaxing; Herranz, Luis; van de Weijer, Joost			Mix and Match Networks: Cross-Modal Alignment for Zero-Pair Image-to-Image Translation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image-to-image translation; Multi-domain; Multi-modal; Feature alignment; Mix and match networks; Zero-pair translation; Semantic segmentation; Depth estimation; Deep learning		This paper addresses the problem of inferring unseen cross-modal image-to-image translations between multiple modalities. We assume that only some of the pairwise translations have been seen (i.e. trained) and infer the remaining unseen translations (where training pairs are not available). We propose mix and match networks, an approach where multiple encoders and decoders are aligned in such a way that the desired translation can be obtained by simply cascading the source encoder and the target decoder, even when they have not interacted during the training stage (i.e. unseen). The main challenge lies in the alignment of the latent representations at the bottlenecks of encoder-decoder pairs. We propose an architecture with several tools to encourage alignment, including autoencoders and robust side information and latent consistency losses. We show the benefits of our approach in terms of effectiveness and scalability compared with other pairwise image-to-image translation approaches. We also propose zero-pair cross-modal image translation, a challenging setting where the objective is inferring semantic segmentation from depth (and vice-versa) without explicit segmentation-depth pairs, and only from two (disjoint) segmentation-RGB and depth-RGB training sets. We observe that a certain part of the shared information between unseen modalities might not be reachable, so we further propose a variant that leverages pseudo-pairs which allows us to exploit this shared information between the unseen modalities.	[Wang, Yaxing; Herranz, Luis; van de Weijer, Joost] Comp Vis Ctr Barcelona, Edifici O,Campus UAB, Bellaterra 08193, Spain	Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	Wang, YX (corresponding author), Comp Vis Ctr Barcelona, Edifici O,Campus UAB, Bellaterra 08193, Spain.	yaxing@cvc.uab.es; lherranz@cvc.uab.es; joost@cvc.uab.es	Herranz, Luis/B-4573-2016	Herranz, Luis/0000-0002-7022-3395; van de Weijer, Joost/0000-0002-9656-9706	Spanish Projects [TIN2016-79717-R, RTI2018-102285-A-I00]; CHISTERA Project M2CR [PCIN-2015-251]; CERCA Programme/Generalitat de Catalunya; European Union's H2020 research under Marie Sklodowska-Curie Grant [665919]; Chinese Scholarship Council (CSC) [201507040048]	Spanish Projects(Spanish Government); CHISTERA Project M2CR; CERCA Programme/Generalitat de Catalunya; European Union's H2020 research under Marie Sklodowska-Curie Grant; Chinese Scholarship Council (CSC)(China Scholarship Council)	The Titan Xp used for this research was donated by the NVIDIA Corporation. We acknowledge the Spanish Projects TIN2016-79717-R and RTI2018-102285-A-I00, the CHISTERA Project M2CR (PCIN-2015-251) and the CERCA Programme/Generalitat de Catalunya. Herranz also acknowledges the European Union's H2020 research under Marie Sklodowska-Curie Grant No. 665919. Yaxing Wang acknowledges the Chinese Scholarship Council (CSC) Grant No. 201507040048.	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Alharbi Y, 2019, PROC CVPR IEEE, P1458, DOI 10.1109/CVPR.2019.00155; Almahairi A, 2018, PR MACH LEARN RES, V80; Amodio M., 2019, P IEEE C COMP VIS PA; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Badrinarayanan V., 2015, P IEEE C COMP VIS PA; Cadena C, 2016, ROBOTICS: SCIENCE AND SYSTEMS XII; Cai SX, 2017, C IND ELECT APPL, P636; Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Chen Y, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1925, DOI 10.18653/v1/P17-1176; Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189; Cheng Y., 2016, P INT JOINT C ART IN; Cho W, 2019, PROC CVPR IEEE, P10631, DOI 10.1109/CVPR.2019.01089; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446; Fergus R, 2010, LECT NOTES COMPUT SC, V6311, P762, DOI 10.1007/978-3-642-15549-9_55; Firat O., 2016, P 2016 C N AM CHAPTE, P866, DOI [10.18653/v1/N16-1101, DOI 10.18653/V1/N16-1101]; Fu Yanwei, 2017, ARXIV171004837; Ganin Yaroslav, 2015, ICML; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Geusebroek JM, 2001, IEEE T PATTERN ANAL, V23, P1338, DOI 10.1109/34.977559; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gonzalez-Garcia A, 2018, ADV NEUR IN, V31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96; Hoffman J, 2016, IEEE INT CONF ROBOT, P5032, DOI 10.1109/ICRA.2016.7487708; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jayaraman D, 2014, ADV NEUR IN, V27; Johnson M., 2017, T ASSOC COMPUT LING, DOI 10.1162/tacl_a_00065; Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781; Kim S, 2016, LECT NOTES COMPUT SC, V9912, P143, DOI 10.1007/978-3-319-46484-8_9; Kim Taeksoo, 2017, P 34 INT C MACH LEAR, P1857, DOI [10.5555/3305381.3305573, DOI 10.5555/3305381.3305573]; Kingma D.P, P 3 INT C LEARNING R; Kuga R., 2017, P INT C COMP VIS; Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238; Lai K, 2011, IEEE INT CONF ROBOT, P1817; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28; Lin JX, 2018, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2018.00579; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; Liu Ming-Yu, 2017, NIPS; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu M, 2017, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2017.270; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; McCormac J., 2017, P INT C COMP VIS; Mejjati YA, 2018, ADV NEUR IN, V31; Mirza M., 2014, ARXIV; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Perarnau G, 2016, ARXIV161106355; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Rohrbach M, 2011, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2011.5995627; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roy A, 2016, PROC CVPR IEEE, P5506, DOI 10.1109/CVPR.2016.594; Saito K., 2017, ASYMMETRIC TRI TRAIN; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Song XH, 2017, AAAI CONF ARTIF INTE, P4271; Taigman Y., 2017, UNSUPERVISED CROSSDO; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Valada A, 2016, INT S EXP ROB, P465; Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang WY, 2018, LECT NOTES COMPUT SC, V11215, P144, DOI 10.1007/978-3-030-01252-6_9; Wang Y., 2019, ARXIV190806881; Wang YX, 2018, PROC CVPR IEEE, P5467, DOI 10.1109/CVPR.2018.00573; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; Wu Zuxuan, 2018, P EUR C COMP VIS; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Yi Z, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND DIGITAL IMAGE PROCESSING (CGDIP 2017), DOI 10.1145/3110224.3110230; Yu F., 2016, P ICLR 2016; Yu L, 2018, MACH VISION APPL, V29, P361, DOI 10.1007/s00138-017-0902-y; Zhang LC, 2019, IEEE T IMAGE PROCESS, V28, P1837, DOI 10.1109/TIP.2018.2879249; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zheng H., 2017, P INT JOINT C ART IN; Zhu Jun-Yan, 2017, ICCV; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	90	1	1	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2849	2872		10.1007/s11263-020-01340-z	http://dx.doi.org/10.1007/s11263-020-01340-z		JUN 2020	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ		Green Submitted			2022-12-18	WOS:000541274700001
J	Malleson, C; Collomosse, J; Hilton, A				Malleson, Charles; Collomosse, John; Hilton, Adrian			Real-Time Multi-person Motion Capture from Multi-view Video and IMUs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose estimation; Motion capture; IMU; Multi-view video; Real-time; Multi-person	SHAPE ESTIMATION	A real-time motion capture system is presented which uses input from multiple standard video cameras and inertial measurement units (IMUs). The system is able to track multiple people simultaneously and requires no optical markers, specialized infra-red cameras or foreground/background segmentation, making it applicable to general indoor and outdoor scenarios with dynamic backgrounds and lighting. To overcome limitations of prior video or IMU-only approaches, we propose to use flexible combinations of multiple-view, calibrated video and IMU input along with a pose prior in an online optimization-based framework, which allows the full 6-DoF motion to be recovered including axial rotation of limbs and drift-free global position. A method for sorting and assigning raw input 2D keypoint detections into corresponding subjects is presented which facilitates multi-person tracking and rejection of any bystanders in the scene. The approach is evaluated on data from several indoor and outdoor capture environments with one or more subjects and the trade-off between input sparsity and tracking performance is discussed. State-of-the-art pose estimation performance is obtained on the Total Capture (mutli-view video and IMU) and Human 3.6M (multi-view video) datasets. Finally, a live demonstrator for the approach is presented showing real-time capture, solving and character animation using a light-weight, commodity hardware setup.	[Malleson, Charles; Collomosse, John; Hilton, Adrian] Univ Surrey, Ctr Vis Speech & Signal Proc CVSSP, Guildford GU2 7XH, Surrey, England	University of Surrey	Malleson, C (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc CVSSP, Guildford GU2 7XH, Surrey, England.	c.malleson@surrey.ac.uk; j.collomosse@surrey.ac.uk; a.hilton@surrey.ac.uk	Hilton, Adrian/N-3736-2014	Hilton, Adrian/0000-0003-4223-238X				Agarwal S., 2017, CERES SOLVER; Andrews Sheldon, 2016, P 13 EUR C VIS MED P; BeruBe KA, 2004, SCI TOTAL ENVIRON, V324, P41, DOI 10.1016/j.scitotenv.2003.11.003; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Captury T, 2017, CAPTURY MARKERLESS M; Elhayek A, 2015, PROC CVPR IEEE, P3810, DOI 10.1109/CVPR.2015.7299005; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Helten T, 2013, IEEE I CONF COMP VIS, P1105, DOI 10.1109/ICCV.2013.141; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang YH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275108; Ichim AE, 2016, ROBOT AUTON SYST, V75, P539, DOI 10.1016/j.robot.2015.09.029; IKinema, 2017, IKINEMA OR; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Joo H., 2018, C COMP VIS PATT REC; Li S., 2017, INT C COMP VIS ICCV; Lin M., 2017, C COMP VIS PATT REC; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Malleson C, 2017, INT CONF 3D VISION, P449, DOI 10.1109/3DV.2017.00058; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; OptiTrack, 2017, OPTITRACK MOT; Rhodin H, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980235; Rhodin H, 2016, LECT NOTES COMPUT SC, V9909, P509, DOI 10.1007/978-3-319-46454-1_31; Roetenberg D., 2013, XSENS TECH, V3, P1; Rosenhahn B, 2008, LECT NOTES COMPUT SC, V5096, P385, DOI 10.1007/978-3-540-69321-5_39; Tekin B., 2016, ARXIV161105708 CORR; Tome D, 2018, INT CONF 3D VISION, P474, DOI 10.1109/3DV.2018.00061; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Trumble M, 2016, P 13 EUR C VIS MED P, DOI 10.1145/2998559.2998565; Trumble M., 2017, 2017 BRIT MACH VIS C; Vicon, 2017, VIC BLAD; von Marcard T., 2017, EUROGRAPHICS; von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37; von Marcard T, 2016, IEEE T PATTERN ANAL, V38, P1533, DOI 10.1109/TPAMI.2016.2522398; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wei XL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366207; Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229; Zhao MM, 2018, PROC CVPR IEEE, P7356, DOI 10.1109/CVPR.2018.00768; Zhengyou Zhang, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P666, DOI 10.1109/ICCV.1999.791289; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537	41	1	1	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1594	1611		10.1007/s11263-019-01270-5	http://dx.doi.org/10.1007/s11263-019-01270-5			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN		hybrid			2022-12-18	WOS:000534910600003
J	Sheng, L; Pan, JT; Guo, JM; Shao, J; Loy, CC				Sheng, Lu; Pan, Junting; Guo, Jiaming; Shao, Jing; Loy, Chen Change			High-Quality Video Generation from Static Structural Annotations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised learning; Conditioned generative model; Image and video synthesis; Motion prediction and estimatiovn		This paper proposes a novel unsupervised video generation that is conditioned on a single structural annotation map, which in contrast to prior conditioned video generation approaches, provides a good balance between motion flexibility and visual quality in the generation process. Different from end-to-end approaches that model the scene appearance and dynamics in a single shot, we try to decompose this difficult task into two easier sub-tasks in a divide-and-conquer fashion, thus achieving remarkable results overall. The first sub-task is an image-to-image (I2I) translation task that synthesizes high-quality starting frame from the input structural annotation map. The second image-to-video (I2V) generation task applies the synthesized starting frame and the associated structural annotation map to animate the scene dynamics for the generation of a photorealistic and temporally coherent video. We employ a cycle-consistent flow-based conditioned variational autoencoder to capture the long-term motion distributions, by which the learned bi-directional flows ensure the physical reliability of the predicted motions and provide explicit occlusion handling in a principled manner. Integrating structural annotations into the flow prediction also improves the structural awareness in the I2V generation process. Quantitative and qualitative evaluations over the autonomous driving and human action datasets demonstrate the effectiveness of the proposed approach over the state-of-the-art methods. The code has been released:.	[Sheng, Lu] Beihang Univ, Coll Software, Beijing, Peoples R China; [Pan, Junting] Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China; [Guo, Jiaming] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China; [Shao, Jing] SenseTime Res, Shenzhen, Guangdong, Peoples R China; [Loy, Chen Change] Nanyang Technol Univ, SenseTime NTU Joint Res Ctr, Singapore, Singapore	Beihang University; Chinese University of Hong Kong; Chinese Academy of Sciences; Institute of Computing Technology, CAS; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Sheng, L (corresponding author), Beihang Univ, Coll Software, Beijing, Peoples R China.	lsheng@buaa.edu.cn; junting.pa@gmail.com; guojiaming18s@ict.ac.cn; shaojing@sensetime.com; ccloy@ntu.edu.sg		Sheng, Lu/0000-0002-8525-9163	National Natural Science Foundation of China [61906012]; Singapore MOE AcRF Tier 1 [2018-T1-002-056]; NTU NAP; NTU SUG	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Singapore MOE AcRF Tier 1(Ministry of Education, Singapore); NTU NAP; NTU SUG(Nanyang Technological University)	This work was supported in part by the National Natural Science Foundation of China under Grant No. 61906012, and in part by Singapore MOE AcRF Tier 1 (2018-T1-002-056), NTU SUG, and NTU NAP.	Arulkumaran K, 2017, IEEE SIGNAL PROC MAG, V34, P26, DOI 10.1109/MSP.2017.2743240; Babaeizadeh Mohammad, 2017, ARXIV171011252; Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen BY, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P358, DOI 10.1145/3126686.3126737; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Denton E, 2018, PR MACH LEARN RES, V80; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Ganin Y, 2016, LECT NOTES COMPUT SC, V9906, P311, DOI 10.1007/978-3-319-46475-6_20; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jacob Walker, 2016, UNCERTAIN FUTURE FOR, P835; Jaderberg M, 2015, ADV NEUR IN, V28; Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Kalchbrenner N, 2016, ARXIV161000527; Kingma D. P., 2013, AUTO ENCODING VARIAT; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Liang XD, 2017, IEEE I CONF COMP VIS, P3382, DOI 10.1109/ICCV.2017.364; Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Luo ZL, 2017, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR.2017.751; Mathieu Michael, 2015, ARXIV151105440; Meister S, 2018, AAAI CONF ARTIF INTE, P7251; Pan Junting, 2019, CVPR; Patraucean Viorica, 2015, ARXIV151106309; Pintea SL, 2014, LECT NOTES COMPUT SC, V8691, P172, DOI 10.1007/978-3-319-10578-9_12; Radford A., 2016, 4 INT C LEARNING REP; Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Sohn K, 2015, ADV NEUR IN, V28; Soomro K., 2012, ARXIV; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Uria B, 2016, J MACH LEARN RES, V17; van den Oord A, 2016, PR MACH LEARN RES, V48; Villegas R, 2017, PR MACH LEARN RES, V70; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Vondrick C, 2017, PROC CVPR IEEE, P2992, DOI 10.1109/CVPR.2017.319; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281; Walker J, 2014, PROC CVPR IEEE, P3302, DOI 10.1109/CVPR.2014.416; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang TZ, 2018, ADV NEUR IN, V31; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; XUE T, 2017, ARXIV171109078; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhao L, 2018, LECT NOTES COMPUT SC, V11219, P403, DOI 10.1007/978-3-030-01267-0_24; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18; Zhu Jun-Yan, 2017, ICCV	63	1	1	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2552	2569		10.1007/s11263-020-01334-x	http://dx.doi.org/10.1007/s11263-020-01334-x		MAY 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY					2022-12-18	WOS:000554776700001
J	Rajeswar, S; Mannan, F; Golemo, F; Parent-Levesque, J; Vazquez, D; Nowrouzezahrai, D; Courville, A				Rajeswar, Sai; Mannan, Fahim; Golemo, Florian; Parent-Levesque, Jerome; Vazquez, David; Nowrouzezahrai, Derek; Courville, Aaron			Pix2Shape: Towards Unsupervised Learning of 3D Scenes from Images Using a View-Based Representation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision; Differentiable rendering; 3D understanding; Adversarial training		We infer and generate three-dimensional (3D) scene information from a single input image and without supervision. This problem is under-explored, with most prior work relying on supervision from, e.g., 3D ground-truth, multiple images of a scene, image silhouettes or key-points. We propose Pix2Shape, an approach to solve this problem with four component: (i) an encoder that infers the latent 3D representation from an image, (ii) a decoder that generates an explicit 2.5D surfel-based reconstruction of a scene-from the latent code-(iii) a differentiable renderer that synthesizes a 2D image from the surfel representation, and (iv) a critic network trained to discriminate between images generated by the decoder-renderer and those from a training distribution. Pix2Shape can generate complex 3D scenes that scale with the view-dependent on-screen resolution, unlike representations that capture world-space resolution, i.e., voxels or meshes. We show that Pix2Shape learns a consistent scene representation in its encoded latent space, and that the decoder can then be applied to this latent representation in order to synthesize the scene from a novel viewpoint. We evaluate Pix2Shape with experiments on the ShapeNet dataset as well as on a novel benchmark we developed - called 3D-IQTT-to evaluate models based on their ability to enable 3d spatial reasoning. Qualitative and quantitative evaluation demonstrate Pix2Shape's ability to solve scene reconstruction, generation and understanding tasks.	[Rajeswar, Sai; Golemo, Florian; Parent-Levesque, Jerome; Courville, Aaron] Univ Montreal, Montreal, PQ, Canada; [Rajeswar, Sai; Vazquez, David] Element AI, Montreal, PQ, Canada; [Mannan, Fahim] Algolux, Montreal, PQ, Canada; [Nowrouzezahrai, Derek] McGill Univ, Montreal, PQ, Canada	Universite de Montreal; McGill University	Rajeswar, S (corresponding author), Univ Montreal, Montreal, PQ, Canada.; Rajeswar, S (corresponding author), Element AI, Montreal, PQ, Canada.	rajsai24@gmail.com						Arjovsky M, 2017, PR MACH LEARN RES, V70; Belghazi MI, 2018, PR MACH LEARN RES, V80; Caesar H., 2019, P IEEE CVF C COMP VI; Chang Angel X., 2015, ARXIV151203012CSGR P; Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930; Chen WZ, 2019, ADV NEUR IN, V32; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Gadelha M., 2016, ABS161205872 CORR; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Hausdorff F., 1949, SSVM; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HENDERSON P, 2018, P BRIT MACH VIS C; Insafutdinov E., 2018, ABS181009381 CORR; Jiang C. M., 2019, ABS190102070 CORR; Kajiya J.T., 1986, SIGGRAPH, P143, DOI [DOI 10.1145/15922.15902, 10.1145/15886.15902, DOI 10.1145/15886.15902]; Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]; Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23; Kar A., 2014, ABS14116069 CORR; Kato H., 2018, ABS181110719 CORR; Kato H, 2017, NEURAL 3D MESH RENDE; Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009; Koch G., 2015, ICML DEEP LEARNING W; Kulkarni TD, 2015, ADV NEUR IN, V28; Li Chunyuan, 2017, NIPS; Li Z., 2019, ABS190411111 CORR; Liu Shichen, 2019, ARXIV190105567; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Mikolov T, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P612; Mirza M., 2014, ARXIV; Nguyen-Phuoc T., 2019, HOLOGAN UNSUPERVISED; Niu C., 2018, COMPUTER VISION PATT; Novotny D., 2019, ARXIV190902533; Novotny D, 2017, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2017.558; Perez E, 2017, ARXIV170907871; Pfister H., 2000, ANN C COMP GRAPH INT; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rezende DJ, 2016, ADV NEUR IN, V29; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; SHEPARD RN, 1971, SCIENCE, V171, P701, DOI 10.1126/science.171.3972.701; Soltani A. A., 2017, COMPUTER VISION PATT; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Taha A. A., 2015, IEEE T PATTERN ANAL; Tulsiani S., 2016, ABS161200404 CORR; Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30; Wiles Olivia, 2017, P BMVC; Woodcock R. W., 2001, W JOHNSON III TESTS; Wu JJ, 2017, ADV NEUR IN, V30; Wu Jiajun, 2016, ADV NEURAL INFORM PR; Wu J, 2016, INT J BIOMED IMAGING, V2016, DOI 10.1155/2016/7468953; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Zhang XM, 2018, ADV NEUR IN, V31; ZHU JY, 2018, ADV NEURAL INFORM PR; Zuo MJ, 2003, HANDBOOK OF RELIABILITY ENGINEERING, P3, DOI 10.1007/1-85233-841-5_1	59	1	3	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2478	2493		10.1007/s11263-020-01322-1	http://dx.doi.org/10.1007/s11263-020-01322-1		MAR 2020	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Green Submitted			2022-12-18	WOS:000520915700001
J	Weiss, Y; Ferrari, V; Sminchisescu, C; Hebert, M				Weiss, Yair; Ferrari, Vittorio; Sminchisescu, Cristian; Hebert, Martial			Special Issue: Advances in Architectures and Theories for Computer Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Weiss, Yair] Hebrew Univ Jerusalem, Jerusalem, Israel; [Ferrari, Vittorio; Sminchisescu, Cristian] Google Res, Zurich, Switzerland; [Hebert, Martial] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Hebrew University of Jerusalem; Google Incorporated; Carnegie Mellon University	Weiss, Y (corresponding author), Hebrew Univ Jerusalem, Jerusalem, Israel.	yweiss@cs.huji.ac.il						Chin TJ, 2020, INT J COMPUT VISION, V128, P575, DOI 10.1007/s11263-019-01207-y; Esteves C, 2020, INT J COMPUT VISION, V128, P588, DOI 10.1007/s11263-019-01220-1; Gehrig D, 2020, INT J COMPUT VISION, V128, P601, DOI 10.1007/s11263-019-01209-w; Harwath D, 2020, INT J COMPUT VISION, V128, P620, DOI 10.1007/s11263-019-01205-0; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; Li Y, 2020, INT J COMPUT VISION, V128, P657, DOI 10.1007/s11263-019-01250-9; Ma SZ, 2020, INT J COMPUT VISION, V128, P679, DOI 10.1007/s11263-019-01230-z; Pumarola A, 2020, INT J COMPUT VISION, V128, P698, DOI 10.1007/s11263-019-01210-3; Sundermeyer M, 2020, INT J COMPUT VISION, V128, P714, DOI 10.1007/s11263-019-01243-8; Veit A, 2020, INT J COMPUT VISION, V128, P730, DOI 10.1007/s11263-019-01190-4; Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI 10.1007/s11263-019-01198-w; Zhou HZ, 2020, INT J COMPUT VISION, V128, P756, DOI 10.1007/s11263-019-01221-0	12	1	1	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		573	574		10.1007/s11263-019-01289-8	http://dx.doi.org/10.1007/s11263-019-01289-8		FEB 2020	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KU1MV		Bronze			2022-12-18	WOS:000516188900001
J	Chen, GY; Han, K; Wong, KYK				Chen, Guanying; Han, Kai; Wong, Kwan-Yee K.			Learning Transparent Object Matting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Transparent object; Image matting; Convolutional neural network		This paper addresses the problem of image matting for transparent objects. Existing approaches often require tedious capturing procedures and long processing time, which limit their practical use. In this paper, we formulate transparent object matting as a refractive flow estimation problem, and propose a deep learning framework, called TOM-Net, for learning the refractive flow. Our framework comprises two parts, namely a multi-scale encoder-decoder network for producing a coarse prediction, and a residual network for refinement. At test time, TOM-Net takes a single image as input, and outputs a matte (consisting of an object mask, an attenuation mask and a refractive flow field) in a fast feed-forward pass. As no off-the-shelf dataset is available for transparent object matting, we create a large-scale synthetic dataset consisting of 178 K images of transparent objects rendered in front of images sampled from the Microsoft COCO dataset. We also capture a real dataset consisting of 876 samples using 14 transparent objects and 60 background images. Besides, we show that our method can be easily extended to handle the cases where a trimap or a background image is available. Promising experimental results have been achieved on both synthetic and real data, which clearly demonstrate the effectiveness of our approach.	[Chen, Guanying; Wong, Kwan-Yee K.] Univ Hong Kong, Hong Kong, Peoples R China; [Han, Kai] Univ Oxford, Oxford, England	University of Hong Kong; University of Oxford	Chen, GY (corresponding author), Univ Hong Kong, Hong Kong, Peoples R China.	gychen@cs.hku.hk; khan@robots.ox.ac.uk	Han, Kai/AAB-7809-2021; Wong, Kenneth Kwan Yee/C-1577-2009	Han, Kai/0000-0002-7995-9999; Wong, Kenneth Kwan Yee/0000-0001-8560-9007	Research Grant Council of the Hong Kong (SAR), China [HKU 718113E]; NVIDIA Corporation	Research Grant Council of the Hong Kong (SAR), China(Hong Kong Research Grants Council); NVIDIA Corporation	This project is supported by a Grant from the Research Grant Council of the Hong Kong (SAR), China, under the Project HKU 718113E. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research.	Chen G., 2018, CVPR; Cho D, 2016, LECT NOTES COMPUT SC, V9906, P626, DOI 10.1007/978-3-319-46475-6_39; Chuang Y. Y., 2000, SIGGRAPH; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duan Q., 2011, COMPUTER GRAPHICS FO; Duan Q., 2011, ICME; Duan Q, 2015, VISUAL COMPUT, V31, P1587, DOI 10.1007/s00371-014-1035-1; Eigen David, 2014, NEURIPS; Fischer Philipp, 2015, ARXIV150406852, P2; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Kim Jiwon, 2016, CVPR; Kingma D.P, P 3 INT C LEARNING R; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Peers P., 2003, EUROGRAPHICS WORKSHO; Qian Y., 2015, ICCV; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6; Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619; Smith A. R., 1996, SIGGRAPH; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wexler Y., 2002, RENDERING TECHNIQUES; Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41; Yeung SK, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899406; Zhu J., 2004, COMPUTER GRAPHICS AP; Zongker D. E., 1999, SIGGRAPH	27	1	1	1	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1527	1544		10.1007/s11263-019-01202-3	http://dx.doi.org/10.1007/s11263-019-01202-3			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL		Green Submitted			2022-12-18	WOS:000485320300008
J	Weiss, P; Escande, P; Bathie, G; Dong, YQ				Weiss, Pierre; Escande, Paul; Bathie, Gabriel; Dong, Yiqiu			Contrast Invariant SNR and Isotonic Regressions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Local contrast change; Topographic map; Isotonic regression; Convex optimization; Illumination invariance; Signal-to-noise-ratio; Image quality measure; Dynamic programming	IMAGE; MAXIMIZATION; ALGORITHMS	We design an image quality measure independent of contrast changes, which are defined as a set of transformations preserving an order between the level lines of an image. This problem can be expressed as an isotonic regression problem. Depending on the definition of a level line, the partial order between adjacent regions can be defined through chains, polytrees or directed acyclic graphs. We provide a few analytic properties of the minimizers and design original optimization procedures together with a full complexity analysis. The methods worst case complexities range from O(n) for chains, to O(nlogn) for polytrees and O() for directed acyclic graphs, where n is the number of pixels and E is a relative precision. The proposed algorithms have potential applications in change detection, stereo-vision, image registration, color image processing or image fusion. A C++ implementation with Matlab headers is available at https://github.com/pierre-weiss/contrast_invariant_snr.	[Weiss, Pierre] Univ Toulouse, CNRS, IMT, ITAV, Toulouse, France; [Escande, Paul] Univ Toulouse, ISAE, DISC Dept, Toulouse, France; [Bathie, Gabriel] Univ Toulouse, INSA Toulouse, Toulouse, France; [Dong, Yiqiu] Tech Univ Denmark, Dept Appl Math & Comp Sci, Lyngby, Denmark	Centre National de la Recherche Scientifique (CNRS); Universite de Toulouse; Universite Toulouse III - Paul Sabatier; Universite de Toulouse; Institut Superieur de l'Aeronautique et de l'Espace (ISAE-SUPAERO); Universite Federale Toulouse Midi-Pyrenees (ComUE); Universite de Toulouse; Institut National des Sciences Appliquees de Toulouse; Technical University of Denmark	Weiss, P (corresponding author), Univ Toulouse, CNRS, IMT, ITAV, Toulouse, France.	pierre.armand.weiss@gmail.com		Dong, Yiqiu/0000-0001-8363-9448; Bathie, Gabriel/0000-0003-2400-4914	PRES of Toulouse University; Midi-Pyrenees region	PRES of Toulouse University; Midi-Pyrenees region(Region Occitanie)	The authors wish to thank the anonymous reviewers for their excellent reviews which helped them improving the paper. They thank Pascal Monasse for encouraging them to explore this question and for providing the source codes of the FLST (Fast Level Set Transform). P. Weiss wishes to thank Jonas Kahn warmly for helping him to check that dynamic programming could be applied to our problem and to find a hard problem. He also thanks his daughter Anouk for lending her toys to generate the pictures. P. Escande was supported by the PRES of Toulouse University and Midi-Pyrenees region.	[Anonymous], 2013, INTRO LECT CONVEX OP; [Anonymous], 1950, USP MAT NAUK; ApS M., 2017, MOSEK OPTIMIZATION T; Ballester C., 2000, ADV MATH METHODS MEA, P41; Ballester C, 2006, INT J COMPUT VISION, V69, P43, DOI 10.1007/s11263-006-6852-x; BARLOW RE, 1972, J AM STAT ASSOC, V67, P140, DOI 10.2307/2284712; BEST MJ, 1990, MATH PROGRAM, V47, P425, DOI 10.1007/BF01580873; Bovik Alan C., 2010, HDB IMAGE VIDEO PROC; Boyer C, 2014, SIAM J IMAGING SCI, V7, P1080, DOI 10.1137/130941560; BRUNK HD, 1955, ANN MATH STAT, V26, P607, DOI 10.1214/aoms/1177728420; Caselles, 2009, GEOMETRIC DESCRIPTIO; Caselles V, 2002, J MATH IMAGING VIS, V16, P89, DOI 10.1023/A:1013943314097; Caselles V, 1999, INT J COMPUT VISION, V33, P5, DOI 10.1023/A:1008144113494; Chambolle A, 2016, ACTA NUMER, V25, P161, DOI 10.1017/S096249291600009X; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Combettes PL, 2011, SPRINGER SER OPTIM A, V49, P185, DOI 10.1007/978-1-4419-9569-8_10; Deledalle CA, 2017, SIAM J IMAGING SCI, V10, P243, DOI 10.1137/16M1080318; Delon J, 2004, J MATH IMAGING VIS, V21, P119, DOI 10.1023/B:JMIV.0000035178.72139.2d; Droske M, 2003, SIAM J APPL MATH, V64, P668, DOI 10.1137/S0036139902419528; DYKSTRA RL, 1982, ANN STAT, V10, P708, DOI 10.1214/aos/1176345866; Ehrhardt MJ, 2014, IEEE T IMAGE PROCESS, V23, P9, DOI 10.1109/TIP.2013.2277775; Felzenszwalb PF, 2011, IEEE T PATTERN ANAL, V33, P721, DOI 10.1109/TPAMI.2010.135; Geraud Thierry, 2013, Mathematical Morphology and Its Applications to Signal and Image Processing. 11th International Symposium, ISMM 2013. Proceedings, P98, DOI 10.1007/978-3-642-38294-9_9; HOROWITZ E, 2006, FUNDAMENTALS DATA ST; Kolmogorov V, 2016, SIAM J IMAGING SCI, V9, P605, DOI 10.1137/15M1010257; Kyng R., 2015, ADV NEURAL INFORM PR, P2719; Luss R., 2010, P NEUR INF PROC SYST, P1513; Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664; Matheron G., 1975, RANDOM SETS INTEGRAL; Moisan L, 2012, LECT NOTES ENS CACHA; Monasse P, 2000, IEEE T IMAGE PROCESS, V9, P860, DOI 10.1109/83.841532; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y., 1994, STUD APPL MATH, V13; Pardalos PM, 1999, ALGORITHMICA, V23, P211, DOI 10.1007/PL00009258; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Serra J, 1982, IMAGE ANAL MATH MORP; Spielman D., 2003, C P ANN ACM S THEOR, DOI 10.1145/1007352.1007372; STOUT QF, 2014, FASTEST ISOTONIC REG; Stout QF, 2013, ALGORITHMICA, V66, P93, DOI 10.1007/s00453-012-9628-4; VANEEDEN C, 1957, INDAG MATH, V19, P128; VIOLA P, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P16, DOI 10.1109/ICCV.1995.466930; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Weiss P, 2011, SIAM J IMAGING SCI, V4, P448, DOI 10.1137/09077744X; Weiss P, 2009, SIAM J SCI COMPUT, V31, P2047, DOI 10.1137/070696143	44	1	1	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					1144	1161		10.1007/s11263-019-01161-9	http://dx.doi.org/10.1007/s11263-019-01161-9			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM		Green Submitted			2022-12-18	WOS:000474559000011
J	Ackland, S; Chiclana, F; Istance, H; Coupland, S				Ackland, Stephen; Chiclana, Francisco; Istance, Howell; Coupland, Simon			Real-Time 3D Head Pose Tracking Through 2.5D Constrained Local Models with Local Neural Fields	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Head pose estimation; 2; 5D CLM; LNF; Real-time	FACE; REGISTRATION; CAMERA; MOTION; SHAPE	Tracking the head in a video stream is a common thread seen within computer vision literature, supplying the research community with a large number of challenging and interesting problems. Head pose estimation from monocular cameras is often considered an extended application after the face tracking task has already been performed. This often involves passing the resultant 2D data through a simpler algorithm that best fits the data to a static 3D model to determine the 3D pose estimate. This work describes the 2.5D constrained local model, combining a deformable 3D shape point model with 2D texture information to provide direct estimation of the pose parameters, avoiding the need for additional optimization strategies. It achieves this through an analytical derivation of a Jacobian matrix describing how changes in the parameters of the model create changes in the shape within the image through a full-perspective camera model. In addition, the model has very low computational complexity and can run in real-time on modern mobile devices such as tablets and laptops. The point distribution model of the face is built in a unique way, so as to minimize the effect of changes in facial expressions on the estimated head pose and hence make the solution more robust. Finally, the texture information is trained via local neural fieldsa deep learning approach that utilizes small discriminative patches to exploit spatial relationships between the pixels and provide strong peaks at the optimal locations.	[Ackland, Stephen; Chiclana, Francisco; Coupland, Simon] De Montfort Univ, Leicester, Leics, England; [Istance, Howell] Univ Tampere, Tampere, Finland	De Montfort University	Ackland, S (corresponding author), De Montfort Univ, Leicester, Leics, England.	stephenniackland@gmail.com; chiclana@dmu.ac.uk; howell.istance@staff.uta.fi; simonc@dmu.ac.uk	Chiclana, Francisco/B-9031-2008	Chiclana, Francisco/0000-0002-3952-4210				Ackland S., 2014, P S EYE TRACK RES AP, P203; Ariz M, 2016, COMPUT VIS IMAGE UND, V148, P201, DOI 10.1016/j.cviu.2015.04.009; Asthana A, 2015, IEEE T PATTERN ANAL, V37, P1312, DOI 10.1109/TPAMI.2014.2362142; Baltrusaitis T, 2016, IEEE WINT CONF APPL; Baltrusaitis T, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P354, DOI 10.1109/ICCVW.2013.54; Baltrusaitis T, 2012, PROC CVPR IEEE, P2610, DOI 10.1109/CVPR.2012.6247980; Baltruvsaitis T, 2014, THESIS; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116; Bulat Adrian, 2018, IEEE T PATTERN ANAL; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Cheung YM, 2015, IEEE T HUM-MACH SYST, V45, P419, DOI 10.1109/THMS.2015.2400442; Choi S, 2008, PATTERN RECOGN, V41, P2901, DOI 10.1016/j.patcog.2008.02.002; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cristinacce D., 2006, BRITISH MACHINE VISI, V3, P929; DEMENTHON DF, 1992, LECT NOTES COMPUT SC, V588, P335, DOI 10.1007/BF01450852; Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11; GOODALL C, 1991, J ROY STAT SOC B MET, V53, P285, DOI 10.1111/j.2517-6161.1991.tb01825.x; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390; La Cascia M, 2000, IEEE T PATTERN ANAL, V22, P322, DOI 10.1109/34.845375; Martins P, 2010, INT J COMPUT VISION, V56, P221; Martins P., 2012, COMPUTER VISION IMAG; Merget D, 2018, PROC CVPR IEEE, P781, DOI 10.1109/CVPR.2018.00088; Padeleris P., 2012, 2012 IEEE COMP SOC C, P42, DOI DOI 10.1109/CVPRW.2012.6239236; Paquet U, 2009, PROC CVPR IEEE, P1193, DOI 10.1109/CVPRW.2009.5206751; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Pons-Moll G., 2011, VISUAL ANAL HUMANS, P139, DOI [10.1007/978-0-85729-997-0_9, DOI 10.1007/978-0-85729-997-0_9]; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Tang YQ, 2011, LECT NOTES COMPUT SC, V7098, P66, DOI 10.1007/978-3-642-25449-9_9; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Tulyakov S, 2018, IEEE T PATTERN ANAL, V40, P2250, DOI 10.1109/TPAMI.2017.2750687; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wang YH, 2008, IEEE IMTC P, P1, DOI 10.4018/jcini.2008040101; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P965, DOI 10.1109/34.159901; Xiao J, 2003, INT J IMAG SYST TECH, V13, P85, DOI 10.1002/ima.10048; Xiao J., 2004, IEEE COMP SOC C COMP, V2; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	43	1	1	2	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		579	598		10.1007/s11263-019-01152-w	http://dx.doi.org/10.1007/s11263-019-01152-w			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD					2022-12-18	WOS:000468525900004
J	Liao, ZC; Karsch, K; Zhang, HY; Forsyth, D				Liao, Zicheng; Karsch, Kevin; Zhang, Hongyi; Forsyth, David			An Approximate Shading Model with Detail Decomposition for Object Relighting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image-based modeling; Shading model; Object insertion; Image relighting	HIGHLIGHTS; SHAPE	We present an object relighting system that allows an artist to select an object from an image and insert it into a target scene. Through simple interactions, the system can adjust illumination on the inserted object so that it appears naturally in the scene. To support image-based relighting, we build object model from the image, and propose a perceptually-inspired approximate shading model for the relighting. It decomposes the shading field into (a) a rough shape term that can be reshaded, (b) a parametric shading detail that encodes missing features from the first term, and (c) a geometric detail term that captures fine-scale material properties. With this decomposition, the shading model combines 3D rendering and image-based composition and allows more flexible compositing than image-based methods. Quantitative evaluation and a set of user studies suggest our method is a promising alternative to existing methods of object insertion.	[Liao, Zicheng; Zhang, Hongyi] Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejiang, Peoples R China; [Liao, Zicheng; Karsch, Kevin; Forsyth, David] Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA	Zhejiang University; University of Illinois System; University of Illinois Urbana-Champaign	Liao, ZC (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejiang, Peoples R China.; Liao, ZC (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA.	zliao@zju.edu.cn; karsch1@illinois.edu; zhanghongyi@zju.edu.cn; daf@illinois.edu			Division of Information and Intelligent Systems (US) [IIS 09-16014]; Division of Information and Intelligent Systems [IIS-1421521]; Office of Naval Research [N00014-10-10934]; NSFC [61602406]; ZJNSF [Q15F020006]; Alibaba - Zhejiang University Joint Institute of Frontier Technologies	Division of Information and Intelligent Systems (US); Division of Information and Intelligent Systems; Office of Naval Research(Office of Naval Research); NSFC(National Natural Science Foundation of China (NSFC)); ZJNSF(Natural Science Foundation of Zhejiang Province); Alibaba - Zhejiang University Joint Institute of Frontier Technologies	DAF is supported in part by Division of Information and Intelligent Systems (US) (IIS 09-16014), Division of Information and Intelligent Systems (IIS-1421521) and Office of Naval Research (N00014-10-10934). ZL is supported in part by NSFC Grant No. 61602406, ZJNSF Grant No. Q15F020006 and a special fund from Alibaba - Zhejiang University Joint Institute of Frontier Technologies.	Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718; Barron JT, 2012, LECT NOTES COMPUT SC, V7575, P57, DOI 10.1007/978-3-642-33765-9_5; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; BECK J, 1981, PERCEPT PSYCHOPHYS, V30, P407, DOI 10.3758/BF03206160; Berzhanskaya J, 2005, PERCEPTION, V34, P565, DOI 10.1068/p5401; BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247; Cavanagh P, 2005, NATURE, V434, P301, DOI 10.1038/434301a; Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470; Debevec P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P189, DOI 10.1145/280814.280864; Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307; Durou JD, 2008, COMPUT VIS IMAGE UND, V109, P22, DOI 10.1016/j.cviu.2007.09.003; Furukawa Yasutaka, 2015, FDN TRENDS COMPUTER, P2; Fyffe G, 2017, COMPUT GRAPH FORUM, V36, P295, DOI 10.1111/cgf.13127; Fyffe G, 2013, ACM SIGGRAPH 2013 TA; Ghosh A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024163; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; Hartley R., 2004, ROBOTICA; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Johnston S.F., 2002, P 2 INT S NONPH AN R, P45, DOI DOI 10.1145/508530.508538; Karsch K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602146; Karsch K, 2012, LECT NOTES COMPUT SC, V7576, P775, DOI 10.1007/978-3-642-33715-4_56; Karsch K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024191; Kim S, 2016, LECT NOTES COMPUT SC, V9912, P143, DOI 10.1007/978-3-319-46484-8_9; Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239454, 10.1145/1276377.1276381]; Lettry L., 2016, ARXIV161207899; Liao ZC, 2015, PROC CVPR IEEE, P5307, DOI 10.1109/CVPR.2015.7299168; Liao ZC, 2013, PROC CVPR IEEE, P963, DOI 10.1109/CVPR.2013.129; Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342; Niessner M, 2016, COMPUT GRAPH FORUM, V35, P113, DOI 10.1111/cgf.12714; Ostrovsky Y, 2005, PERCEPTION, V34, P1301, DOI 10.1068/p5418; Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269; Prasad M., 2006, IEEE COMP SOC C COMP, P1345; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269; Tarr MJ, 1998, VISION RES, V38, P2259, DOI 10.1016/S0042-6989(98)00041-8; Trigeorgis G, 2017, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2017.44; Twarog N. R., 2012, P ACM S APPL PERC, P47; Wu TP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409072; Xia T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618461; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284	42	1	1	1	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2019	127	1					22	37		10.1007/s11263-018-1090-6	http://dx.doi.org/10.1007/s11263-018-1090-6			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HK6AD		Green Submitted			2022-12-18	WOS:000458050000002
J	Xu, C; Govindarajan, LN; Zhang, Y; Stewart, J; Bichler, Z; Jesuthasan, S; Claridge-Chang, A; Mathuru, AS; Tang, WL; Zhu, PX; Cheng, L				Xu, Chi; Govindarajan, Lakshmi Narasimhan; Zhang, Yu; Stewart, James; Bichler, Zoe; Jesuthasan, Suresh; Claridge-Chang, Adam; Mathuru, Ajay Sriram; Tang, Wenlong; Zhu, Peixin; Cheng, Li			Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups (vol 123, pg 454, 2017)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction								The original author list did not accurately reflect the contributions of the following colleagues.	[Xu, Chi; Govindarajan, Lakshmi Narasimhan; Zhang, Yu; Cheng, Li] ASTAR, Bioinformat Inst, Singapore, Singapore; [Stewart, James; Jesuthasan, Suresh; Claridge-Chang, Adam; Mathuru, Ajay Sriram] ASTAR, Inst Mol & Cell Biol, Singapore, Singapore; [Bichler, Zoe] Natl Neurosci Inst, Singapore, Singapore; [Jesuthasan, Suresh] Nanyang Technol Univ, Lee Kong Chian Sch Med, Singapore, Singapore; [Claridge-Chang, Adam] Duke NUS Med Sch, Singapore, Singapore; [Claridge-Chang, Adam] Natl Univ Singapore, Dept Physiol, Singapore, Singapore; [Mathuru, Ajay Sriram] Yale NUS Coll, Singapore, Singapore; [Tang, Wenlong; Zhu, Peixin] Novartis Inst BioMed Res, Cambridge, MA USA; [Tang, Wenlong; Zhu, Peixin] Harvard Univ, Dept Stem Cells & Regenerat Biol, Cambridge, MA 02138 USA; [Cheng, Li] Natl Univ Singapore, Sch Comp, Singapore, Singapore	Agency for Science Technology & Research (A*STAR); A*STAR - Bioinformatics Institute (BII); Agency for Science Technology & Research (A*STAR); A*STAR - Institute of Molecular & Cell Biology (IMCB); National Neuroscience Institute (NNI); Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; National University of Singapore; National University of Singapore; Yale NUS College; Novartis; Harvard University; National University of Singapore	Cheng, L (corresponding author), ASTAR, Bioinformat Inst, Singapore, Singapore.; Cheng, L (corresponding author), Natl Univ Singapore, Sch Comp, Singapore, Singapore.	xuchi@bii.a-star.edu.sg; lakshming@bii.a-star.edu.sg; zhangyu@bii.a-star.edu.sg; jcstewart@imcb.a-star.edu.sg; zoe_bichler@nni.com.sg; sureshjj@imcb.a-star.edu.sg; acchang@imcb.a-star.edu.sg; ajaym@imcb.a-star.edu.sg; wenlong.tang@novartis.com; peixin.zhu@novartis.com; chengli@bii.a-star.edu.sg	Jesuthasan, Suresh/B-7870-2016; Cheng, Li/AAU-6734-2020; Mathuru, Ajay S./T-8787-2019	Jesuthasan, Suresh/0000-0002-5733-6555; Cheng, Li/0000-0003-3261-3533; Bichler, Zoe/0000-0003-1942-480X; Mathuru, Ajay S./0000-0003-4591-5274	A*STAR JCO [1431AFG120, 15302FG149]	A*STAR JCO(Agency for Science Technology & Research (A*STAR))	The project is partially supported by A*STAR JCO Grants 1431AFG120 and 15302FG149. Zilong Wang helps with the annotation of mouse data, while Wei Gao and Ashwin Nanjappa help with implementing the mouse baseline method.	Xu C, 2017, INT J COMPUT VISION, V123, P454, DOI 10.1007/s11263-017-0998-6	1	1	1	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2018	126	8					897	897		10.1007/s11263-018-1069-3	http://dx.doi.org/10.1007/s11263-018-1069-3			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GL5ZB		Bronze			2022-12-18	WOS:000437253500006
J	Wu, XM; Hiramatsu, K; Kashino, K				Wu, Xiaomeng; Hiramatsu, Kaoru; Kashino, Kunio			Label Propagation with Ensemble of Pairwise Geometric Relations: Towards Robust Large-Scale Retrieval of Object Instances	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image retrieval; Label propagation; Pairwise geometric relation; Reranking; Spatial verification	IMAGE; FEATURES; AGGREGATION; DESCRIPTORS; SIMILARITY; RANKING; SEARCH; MODEL	Spatial verification methods permit geometrically stable image matching, but still involve a difficult trade-off between robustness as regards incorrect rejection of true correspondences and discriminative power in terms of mismatches. To address this issue, we ask whether an ensemble of weak geometric constraints that correlates with visual similarity only slightly better than a bag-of-visual-words model performs better than a single strong constraint. We consider a family of spatial verification methods and decompose them into fundamental constraints imposed on pairs of feature correspondences. Encompassing such constraints leads us to propose a new method, which takes the best of existing techniques and functions as a unified Ensemble of pAirwise GEometric Relations (EAGER), in terms of both spatial contexts and between-image transformations. We also introduce a novel and robust reranking method, in which the object instances localized by EAGER in high-ranked database images are reissued as new queries. EAGER is extended to develop a smoothness constraint where the similarity between the optimized ranking scores of two instances should be maximally consistent with their geometrically constrained similarity. Reranking is newly formulated as two label propagation problems: one is to assess the confidence of new queries and the other to aggregate new independently executed retrievals. Extensive experiments conducted on four datasets show that EAGER and our reranking method outperform most of their state-of-the-art counterparts, especially when large-scale visual vocabularies are used.	[Wu, Xiaomeng; Hiramatsu, Kaoru; Kashino, Kunio] NTT Commun Sci Labs, 3-1 Morinosato Wakamiya, Atsugi, Kanagawa 2430198, Japan	Nippon Telegraph & Telephone Corporation	Wu, XM (corresponding author), NTT Commun Sci Labs, 3-1 Morinosato Wakamiya, Atsugi, Kanagawa 2430198, Japan.	wu.xiaomeng@lab.ntt.co.jp						Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018; Arandjelovic Relja, 2016, CVPR; Avrithis Y, 2014, INT J COMPUT VISION, V107, P1, DOI 10.1007/s11263-013-0659-3; Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150; Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021; CHUM O, 2004, ACCV; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601; Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531; Deng C, 2013, IEEE I CONF COMP VIS, P2600, DOI 10.1109/ICCV.2013.323; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15; Pedronette DCG, 2013, PATTERN RECOGN, V46, P2350, DOI 10.1016/j.patcog.2013.01.004; Hartley R, 2008, CVPR; Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jegou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609; Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121; Joe Yue-Hei Ng, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P53, DOI 10.1109/CVPRW.2015.7301272; Johns E, 2014, LECT NOTES COMPUT SC, V8690, P504, DOI 10.1007/978-3-319-10605-2_33; Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48; Li XC, 2015, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR.2015.7299151; Liu Z., 2012, P 20 ACM INT C MULT, P199, DOI DOI 10.1145/2393347.2393380; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331; Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529; PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009; Philbin J., 2007, CVPR; Philbin J., 2008, CVPR; Qin DF, 2011, PROC CVPR IEEE, P777, DOI 10.1109/CVPR.2011.5995373; Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Romberg S., 2011, ACM INT C MULT RETR, V25, P1, DOI [10.1145/1991996.1992021., DOI 10.1145/1991996.1992021]; Romberg S, 2013, INT J MULTIMED INF R, V2, P243, DOI 10.1007/s13735-013-0040-x; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sattler T, 2016, PROC CVPR IEEE, P1582, DOI 10.1109/CVPR.2016.175; Sattler T, 2009, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2009.5459459; Schonberger JL, 2015, LECT NOTES COMPUT SC, V9358, P53, DOI 10.1007/978-3-319-24947-6_5; Schonberger JL, 2015, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2015.7298703; Schonberger JL, 2015, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR.2015.7299148; Shen XH, 2014, IEEE T PATTERN ANAL, V36, P1229, DOI 10.1109/TPAMI.2013.237; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111; Tian XM, 2011, IEEE T MULTIMEDIA, V13, P639, DOI 10.1109/TMM.2011.2111363; Tolias G., 2015, ICLR; Tolias G, 2016, INT J COMPUT VISION, V116, P247, DOI 10.1007/s11263-015-0810-4; Tolias G, 2014, PATTERN RECOGN, V47, P3466, DOI 10.1016/j.patcog.2014.04.007; Tolias G, 2014, COMPUT VIS IMAGE UND, V120, P31, DOI 10.1016/j.cviu.2013.12.002; Turpin A., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P11, DOI 10.1145/1148170.1148176; Wu X., 2015, BMVC; Wu XM, 2015, IEEE I CONF COMP VIS, P1877, DOI 10.1109/ICCV.2015.218; Wu XM, 2015, IEEE T CIRC SYST VID, V25, P1395, DOI 10.1109/TCSVT.2014.2382985; Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566; Xu B, 2015, IEEE T KNOWL DATA EN, V27, P102, DOI 10.1109/TKDE.2013.70; Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789; Yang Y, 2011, IEEE I CONF COMP VIS, P1465, DOI 10.1109/ICCV.2011.6126403; Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528; Zhou DY, 2004, ADV NEUR IN, V16, P321	64	1	2	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2018	126	7					689	713		10.1007/s11263-018-1063-9	http://dx.doi.org/10.1007/s11263-018-1063-9			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GH0DS					2022-12-18	WOS:000433072800002
J	Chen, CY; Grauman, K				Chen, Chao-Yeh; Grauman, Kristen			Subjects and Their Objects: Localizing Interactees for a Person-Centric View of Importance	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human-object interaction; Importance; Objectness		Understanding images with people often entails understanding their interactions with other objects or people. As such, given a novel image, a vision system ought to infer which other objects/people play an important role in a given person's activity. However, existing methods are limited to learning action-specific interactions (e.g., how the pose of a tennis player relates to the position of his racquet when serving the ball) for improved recognition, making them unequipped to reason about novel interactions with actions or objects unobserved in the training data. We propose to predict the "interactee" in novel images-that is, to localize the object of a person's action. Given an arbitrary image with a detected person, the goal is to produce a saliency map indicating the most likely positions and scales where that person's interactee would be found. To that end, we explore ways to learn the generic, action-independent connections between (a) representations of a person's pose, gaze, and scene cues and (b) the interactee object's position and scale. We provide results on a newly collected UT Interactee dataset spanning more than 10,000 images from SUN, PASCAL, and COCO. We show that the proposed interaction-informed saliency metric has practical utility for four tasks: contextual object detection, image retargeting, predicting object importance, and data-driven natural language scene description. All four scenarios reveal the value in linking the subject to its object in order to understand the story of an image.	[Chen, Chao-Yeh; Grauman, Kristen] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Chen, CY (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	chaoyeh@cs.utexas.edu; grauman@cs.utexas.edu			ONR PECASE [N00014-15-1-2291]	ONR PECASE(Office of Naval Research)	We thank the anonymous reviewers for their feedback, and Larry Zitnick for sharing the captioning data with us. We also thank Texas Advanced Computing Center (TACC) for providing the computing resource. This research is supported in part by ONR PECASE N00014-15-1-2291.	Alexe B., 2010, C COMP VIS PATT REC; [Anonymous], 2014, EUR C COMP VIS; Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461; Berg A., 2012, C COMP VIS PATT REC; Bishop C. M., 1994, TECH REP; Carreira J., 2010, C COMP VIS PATT REC; Chao Y. W., 2015, INT C COMP VIS; Chen C. Y., 2014, P AS C COMP VIS; Cristani M., 2011, BRIT MACH VIS C; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005 IEEE COMP SOC C; Damen D., 2012, IEEE T PATTERN ANAL; Delaitre V., 2012, EUR C COMP VIS; Desai C., 2010, WORKSH STRUCT MOD CO; Desai C., 2013, CVPR WORKSH SCEN AN; Devlin J., 2015, ARXIV150504467; Donahue J., 2015, C COMP VIS PATT REC; Endres I., 2014, IEEE T PATTERN ANAL; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fang H., 2015, C COMP VIS PATT REC; Farhadi A., 2011, C COMP VIS PATT REC; Farhadi A., 2010, EUR C COMP VIS; Fathi A., 2012, C COMP VIS PATT REC; Fouhey D. F., 2014, INT J COMPUTER VISIO; Guadarrama S., 2013, INT C COMP VIS; Gupta A., 2011, C COMP VIS PATT REC; Gupta A., 2009, PAMI, V31; Gupta S., 2015, ARXIV PREPRINT ARXIV; Haritaoglu I., 2000, IEEE T PATTERN ANAL; Hou X, 2007, C COMP VIS PATT REC; Hwang S. J., 2010, BRIT MACH VIS C; Ikizler-Cinbis N., 2010, EUR C COMP VIS; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jia Y., 2014, P ACM INT C MULT; Karpathy A., 2015, C COMP VIS PATT REC; KJELLSTROM H, 2008, EUR C COMP VIS; Koppula H. S., 2013, ROBOTICS SCI SYSTEMS; Krizhevsky A., 2012, NIPS, V1, P4; Kulkarni G., 2011, C COMP VIS PATT REC; Kum SW, 2011, IEEE IC COMP COM NET; Kuznetsova Polina, 2012, ASS COMPUTATIONAL LI; Le D., 2014, VIS LANG WORKSH COLI; Lin T Y, 2014, EUROPEAN C COMPUTER; Liu T., 2007, C COMP VIS PATT REC; Maji S., 2011, C COMP VIS PATT REC; Marin-Jimenez MJ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.22; Ordonez V., 2011, NEUR INF PROC SYST C; Ordonez Vicente, 2013, INT C COMP VIS; Papineni Kishore, 2002, ASS COMPUTATIONAL LI; Park H., 2015, C COMP VIS PATT REC; Peursum P., 2005, INT C COMP VIS; Pirsiavash H., 2014, WORKSH VIS MEETS COG; Prest A, 2012, IEEE T PATTERN ANAL, V34, P601, DOI 10.1109/TPAMI.2011.158; Recasens A., 2015, NEUR INF PROC SYST C; Ronchi M. R., 2015, BRIT MACH VIS C; Sadovnik A., 2012, C COMP VIS PATT REC; Spain M., 2008, EUR C COMP VIS; Spain M, 2011, INT J COMPUT VISION, V91, P59, DOI 10.1007/s11263-010-0376-0; Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951; Xiao Jianxiong, 2010, C COMP VIS PATT REC; Yang Y., 2012, C COMP VIS PATT REC; Yao B., 2011, INT C COMP VIS; Yao B., 2010, C COMP VIS PATT REC; Yao BZ, 2010, P IEEE, V98, P1485, DOI 10.1109/JPROC.2010.2050411; Yatskar Mark, 2016, C COMP VIS PATT REC; Yosinski J, 2014, ADV NEUR IN, V27; Zhou B., 2014, NEURAL INFORM PROCES; Zhu Y., 2014, EUR C COMP VIS	68	1	1	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		292	313		10.1007/s11263-016-0958-6	http://dx.doi.org/10.1007/s11263-016-0958-6			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA		Green Submitted			2022-12-18	WOS:000425619100009
J	Damon, J; Gasparovic, E				Damon, James; Gasparovic, Ellen			Modeling Multi-object Configurations via Medial/Skeletal Linking Structures	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Blum medial axis; Skeletal structures; Spherical axis; Whitney stratified sets; Medial and skeletal linking structures; Generic linking properties; Model configurations; Radial flow; Linking flow	GEOMETRY; BOUNDARIES; SKELETAL	We introduce a method for modeling a configuration of objects in 2D or 3D images using a mathematical "skeletal linking structure" which will simultaneously capture the individual shape features of the objects and their positional information relative to one another. The objects may either have smooth boundaries and be disjoint from the others or share common portions of their boundaries with other objects in a piecewise smooth manner. These structures include a special class of "Blum medial linking structures", which are intrinsically associated to the configuration and build upon the Blum medial axes of the individual objects. We give a classification of the properties of Blum linking structures for generic configurations. The skeletal linking structures add increased flexibility for modeling configurations of objects by relaxing the Blum conditions and they extend in a minimal way the individual "skeletal structures" which have been previously used for modeling individual objects and capturing their geometric properties. This allows for the mathematical methods introduced for single objects to be significantly extended to the entire configuration of objects. These methods not only capture the internal shape structures of the individual objects but also the external structure of the neighboring regions of the objects. In the subsequent second paper (Damon and Gasparovic in Shape and positional geometry of multi-object configurations) we use these structures to identify specific external regions which capture positional information about neighboring objects, and we develop numerical measures for closeness of portions of objects and their significance for the configuration. This allows us to use the same mathematical structures to simultaneously analyze both the shape properties of the individual objects and positional properties of the configuration. This provides a framework for analyzing the statistical properties of collections of similar configurations such as for applications to medical imaging.	[Damon, James] Univ N Carolina, Dept Math, Chapel Hill, NC 27599 USA; [Gasparovic, Ellen] Union Coll, Dept Math, Schenectady, NY 12308 USA	University of North Carolina; University of North Carolina Chapel Hill; Union College	Damon, J (corresponding author), Univ N Carolina, Dept Math, Chapel Hill, NC 27599 USA.	jndamon@math.unc.edu; gasparoe@union.edu			Simons Foundation [230298]; National Science Foundation [DMS-1105470]; DARPA [HR0011-09-1-0055]	Simons Foundation; National Science Foundation(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	Partially supported by the Simons Foundation Grant 230298, the National Science Foundation Grant DMS-1105470 and DARPA Grant HR0011-09-1-0055. This paper contains work from the second author's Ph.D. dissertation at University of North Carolina.	BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0; CHANEY E, 2004, AM SOC THERAPEUTIC R; Damon J, 2005, INT J COMPUT VISION, V63, P45, DOI 10.1007/s11263-005-4946-5; Damon J, 2004, COMPOS MATH, V140, P1657, DOI 10.1112/S0010437X04000570; Damon J, 2003, ANN I FOURIER, V53, P1941, DOI 10.5802/aif.1997; Damon J., 2017, MEMOIRS AM MATH SOC; Damon J., ARXIV170600150; Damon J, 2007, COMMUN ANAL GEOM, V15, P307; Gasparovic E., 2012, THESIS; Giblin P, 2004, IEEE T PATTERN ANAL, V26, P238, DOI 10.1109/TPAMI.2004.1262192; Gorczowski K, 2010, IEEE T PATTERN ANAL, V32, P652, DOI 10.1109/TPAMI.2009.92; Jeong J., 2008, SPIE; Jeong J., 2006, MICCAI; Jeong J. S., 2008, THESIS; Lu CL, 2007, INT J COMPUT VISION, V75, P387, DOI 10.1007/s11263-007-0045-0; MATHER JN, 1983, P SYMP PURE MATH, V40, P199; Pizer SM, 2003, INT J COMPUT VISION, V55, P155, DOI 10.1023/A:1026135101267; Pizer SM, 2003, INT J COMPUT VISION, V55, P85, DOI 10.1023/A:1026313132218; Siddiqi K, 2008, COMPUT IMAGING VIS, V37, P1, DOI 10.1007/978-1-4020-8658-8; YOMDIN Y, 1981, COMPOS MATH, V43, P225	20	1	1	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	3					255	272		10.1007/s11263-017-1019-5	http://dx.doi.org/10.1007/s11263-017-1019-5			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FE1ER		Green Submitted			2022-12-18	WOS:000407961700001
J	Raviv, D; Bayro-Corrochano, E; Raskar, R				Raviv, Dan; Bayro-Corrochano, Eduardo; Raskar, Ramesh			LRA: Local Rigid Averaging of Stretchable Non-rigid Shapes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Non-rigid shapes; Metric invariants; Affine invariant; Averaging shapes; Shape space; Non-rigid statistics	NONLINEAR DIMENSIONALITY REDUCTION; RIEMANNIAN-MANIFOLDS; INVARIANT GEOMETRY; REPRESENTATIONS; REGISTRATION; RECOGNITION; FRAMEWORK; TRACKING; SPACE	We present a novel algorithm for generating the mean structure of non-rigid stretchable shapes. Following an alignment process, which supports local affine deformations, we translate the search of the mean shape into a diagonalization problem where the structure is hidden within the kernel of a matrix. This is the first step required in many practical applications, where one needs to model bendable and stretchable shapes from multiple observations.	[Raviv, Dan] Tel Aviv Univ, Sch Elect Engn, Fac Engn, Tel Aviv, Israel; [Bayro-Corrochano, Eduardo] CINVESTAV, Dept Elect Engn & Comp Sci, Campus Guadalajara, Guadalajara, Jalisco, Mexico; [Raskar, Ramesh] MIT, Media Lab, Cambridge, MA 02139 USA	Tel Aviv University; CINVESTAV - Centro de Investigacion y de Estudios Avanzados del Instituto Politecnico Nacional; Massachusetts Institute of Technology (MIT)	Raviv, D (corresponding author), Tel Aviv Univ, Sch Elect Engn, Fac Engn, Tel Aviv, Israel.	darav@post.tau.ac.il	Raviv, Dan/AAZ-2851-2020					Aflalo Y, 2013, SIAM J IMAGING SCI, V6, P1579, DOI 10.1137/120888107; Alfakih AY, 2002, LINEAR ALGEBRA APPL, V340, P149, DOI 10.1016/S0024-3795(01)00403-7; Alfakih AY, 2001, LINEAR ALGEBRA APPL, V325, P57, DOI 10.1016/S0024-3795(00)00281-0; Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311; Amar A, 2010, IEEE SIGNAL PROC LET, V17, P473, DOI 10.1109/LSP.2010.2043890; Avinash S., 2012, IMAGE PROCESSING ANA; Bauer M, 2014, J MATH IMAGING VIS, V50, P60, DOI 10.1007/s10851-013-0490-z; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Ben Hamza A, 2006, IEEE T IMAGE PROCESS, V15, P2249, DOI 10.1109/TIP.2006.875250; Ben-Chen M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531340; BERARD P, 1994, GEOM FUNCT ANAL, V4, P373, DOI 10.1007/BF01896401; Berger B, 1999, J ACM, V46, P212, DOI 10.1145/301970.301972; Blaschke W., 1923, VORLESUNGEN DIFFEREN; Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P265, DOI 10.1111/cgf.12558; Bronstein AM, 2006, SIAM J SCI COMPUT, V28, P1812, DOI 10.1137/050639296; Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103; Chazal F, 2009, COMPUT GRAPH FORUM, V28, P1393, DOI 10.1111/j.1467-8659.2009.01516.x; Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102; Cox T.F., 1994, MULTIDIMENSIONAL SCA; Davies RH, 2002, IEEE T MED IMAGING, V21, P525, DOI 10.1109/TMI.2002.1009388; DESILVA V, 2003, ADV NEURAL INFORM PR, V15, P705; Devir Y., 2009, P WORKSH NONR SHAP A; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100; Durrleman S, 2013, INT J COMPUT VISION, V103, P22, DOI 10.1007/s11263-012-0592-x; Elad A, 2001, PROC CVPR IEEE, P168; Fletcher PT, 2003, LECT NOTES COMPUT SC, V2732, P450; Heeren B, 2014, COMPUT GRAPH FORUM, V33, P247, DOI 10.1111/cgf.12450; HENDRICKSON B, 1992, SIAM J COMPUT, V21, P65, DOI 10.1137/0221008; HENDRICKSON B, 1995, SIAM J OPTIMIZ, V5, P835, DOI 10.1137/0805040; Huang H, 2005, LECT NOTES COMPUT SC, V3749, P67; Ji X, 2004, IEEE INFOCOM SER, P2652; Kircher S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1356682.1356685; Kovnatsky A, 2013, COMPUT GRAPH FORUM, V32, P439, DOI 10.1111/cgf.12064; Kovnatsky A, 2013, NUMER MATH-THEORY ME, V6, P199, DOI 10.4208/nmtma.2013.mssvm11; Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x; Li R, 2014, PATTERN RECOGN LETT, V43, P3, DOI 10.1016/j.patrec.2013.09.019; Lipman Y., 2009, P ACM T GRAPH SIGGRA, V28; Memoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y; Miller MI, 2014, TECHNOLOGY, V2, P36, DOI 10.1142/S2339547814500010; Ovsjanikov M, 2010, COMPUT GRAPH FORUM, V29, P1555, DOI 10.1111/j.1467-8659.2010.01764.x; Patel A., 2015, PNAS, V52, P206; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Praun E, 2001, COMP GRAPH, P179, DOI 10.1145/383259.383277; RAVIV D., 2010, P 3DPVT; Raviv D, 2015, SIAM J IMAGING SCI, V8, P403, DOI 10.1137/140987675; Raviv D, 2015, INT J COMPUT VISION, V111, P1, DOI 10.1007/s11263-014-0728-2; Raviv D, 2014, J MATH IMAGING VIS, V50, P144, DOI 10.1007/s10851-013-0467-y; Raviv D, 2010, INT J COMPUT VISION, V89, P18, DOI 10.1007/s11263-010-0320-3; Reuter M, 2010, NEUROIMAGE, V53, P1181, DOI 10.1016/j.neuroimage.2010.07.020; Rosman G, 2010, INT J COMPUT VISION, V89, P56, DOI 10.1007/s11263-010-0322-1; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Rustamov RM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461959; Rustamov Raif M, 2007, P 5 EUR S GEOM PROC, P225, DOI DOI 10.2312/SGP/SGP07/225-233; Sheffer A., 2004, P 3D DAT PROC VIS TR; Shtern A., 2014, P INT C 3D VIS 3DV; Singer A, 2008, P NATL ACAD SCI USA, V105, P9507, DOI 10.1073/pnas.0709842104; Su B., 1983, AFFINE DIFFERENTIAL; Su JY, 2014, ANN APPL STAT, V8, P530, DOI 10.1214/13-AOAS701; Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Vaillant M, 2004, NEUROIMAGE, V23, pS161, DOI 10.1016/j.neuroimage.2004.07.023; Wang Y, 2008, INT J COMPUT VISION, V76, P283, DOI 10.1007/s11263-007-0063-y; Weber O, 2012, COMPUT GRAPH FORUM, V31, P2409, DOI 10.1111/j.1467-8659.2012.03130.x; Winkler T, 2010, COMPUT GRAPH FORUM, V29, P309, DOI 10.1111/j.1467-8659.2009.01600.x	65	1	1	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	2					132	144		10.1007/s11263-017-1002-1	http://dx.doi.org/10.1007/s11263-017-1002-1			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FC3PK					2022-12-18	WOS:000406751100002
J	Kita, Y; Ishikawa, H; Masuda, T				Kita, Yasuyo; Ishikawa, Hiroshi; Masuda, Takeshi			Guest Editorial: Machine Vision Applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Kita, Yasuyo; Masuda, Takeshi] Natl Inst Adv Ind Sci & Technol, AIST Tsukuba Cent 1, 1-1-1 Umezono, Tsukuba, Ibaraki 3058560, Japan; [Ishikawa, Hiroshi] Waseda Univ, Dept Comp Sci & Engn, Okubo 3-4-1, Tokyo 1698555, Japan	National Institute of Advanced Industrial Science & Technology (AIST); Waseda University	Kita, Y (corresponding author), Natl Inst Adv Ind Sci & Technol, AIST Tsukuba Cent 1, 1-1-1 Umezono, Tsukuba, Ibaraki 3058560, Japan.	y.kita@aist.go.jp; hfs@waseda.jp; t.masuda@aist.go.jp	Kita, Yasuyo/M-3875-2016	Kita, Yasuyo/0000-0001-8139-7406					0	1	2	1	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		191	192		10.1007/s11263-017-0990-1	http://dx.doi.org/10.1007/s11263-017-0990-1			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS		hybrid			2022-12-18	WOS:000398162200001
J	Bartoli, A				Bartoli, Adrien			Generalizing the Prediction Sum of Squares Statistic and Formula, Application to Linear Fractional Image Warp and Surface Fitting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						PRESS; Cross-validation; Image registration; Warp estimation; Surface reconstruction	MODEL	The prediction sum of squares statistic uses the principle of leave-one-out cross-validation in linear least squares regression. It is computationally attractive, as it can be computed non-iteratively. However, it has limitations: it does not handle coupled measurements, which should be held out simultaneously, and is specific to the principle of leave-one-out, which is known to overfit when used for selecting a model's complexity. We propose multiple-exclusion PRESS (MEXPRESS), which generalizes PRESS to coupled measurements and other types of cross-validation, while retaining computational efficiency with the non-iterative MEXPRESS formula. Using MEXPRESS, various strategies to resolve overfitting can be efficiently implemented. The core principle is to exclude training data too 'close' or too 'similar' to the validation data. We show that this allows one to select the number of control points automatically in three cases: (i) the estimation of linear fractional warps for dense image registration from point correspondences, (ii) surface reconstruction from a dense depth-map obtained by a depth sensor and (iii) surface reconstruction from a sparse point cloud obtained by shape-from-template.	[Bartoli, Adrien] Clermont Univ, Clermont Ferrand, France		Bartoli, A (corresponding author), Clermont Univ, Clermont Ferrand, France.	adrien.bartoli@gmail.com			EU through the ERC [307483 FLEXABLE]	EU through the ERC	I thank Mathias Gallardo and Ajad Chhatkuli for their help in creating the surface fitting datasets and the authors of Varol et al. (2012) for the paper SfT dataset. This research has received funding from the EU's FP7 through the ERC research Grant 307483 FLEXABLE.	AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; ALLEN DM, 1971, TECHNOMETRICS, V13, P469, DOI 10.2307/1267161; [Anonymous], 2011, INT C COMP VIS PATT; Bartoli A, 2008, J MATH IMAGING VIS, V31, P133, DOI 10.1007/s10851-007-0062-1; Bartoli A, 2015, IEEE T PATTERN ANAL, V37, P2099, DOI 10.1109/TPAMI.2015.2392759; Bartoli A, 2010, INT J COMPUT VISION, V88, P85, DOI 10.1007/s11263-009-0303-4; Bartoli A, 2009, INT J COMPUT VISION, V85, P133, DOI 10.1007/s11263-009-0253-x; Bishop, 1995, NEURAL NETWORKS PATT; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; BREIMAN L, 1993, COMPUT STAT DATA AN, V15, P13, DOI 10.1016/0167-9473(93)90217-H; Collins T., 2013, INT C COMP VIS; DIERCKX P, 1981, IMA J NUMER ANAL, V1, P267, DOI 10.1093/imanum/1.3.267; Dierckx P., 1993, CURVE SURFACE FITTIN; Forsyth David A, 2012, COMPUTER VISION MODE; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hastie T, 2009, ELEMENTS STAT LEARNI; JUPP DLB, 1978, SIAM J NUMER ANAL, V15, P328, DOI 10.1137/0715022; Jurie F, 2005, IEEE I CONF COMP VIS, P604; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Malgouyres R, 2009, BRIT MACH VIS C; Miyata S., 2005, J JAPAN STAT SOC, V35, P303, DOI DOI 10.14490/JJSS.35.303; Molinari N, 2004, COMPUT STAT DATA AN, V45, P159, DOI 10.1016/S0167-9473(02)00343-2; Nguyen N, 2001, IEEE T IMAGE PROCESS, V10, P1299, DOI 10.1109/83.941854; Perriollat M, 2013, COMPUT ANIMAT VIRT W, V24, P457, DOI 10.1002/cav.1478; Perriollat M, 2011, INT J COMPUT VISION, V95, P124, DOI 10.1007/s11263-010-0352-8; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Salzmann M, 2007, IEEE T PATTERN ANAL, V29, P1481, DOI 10.1109/TPAMI.2007.1080; Scholkopf B., 2001, LEARNING KERNELS SUP; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Sevilla-Lara L., 2012, INT C COMP VIS PATT; Steinbrucker F., 2013, INT C COMP VIS; Sussmuth J, 2010, COMPUT GRAPH FORUM, V29, P1854, DOI 10.1111/j.1467-8659.2010.01653.x; Varol A., 2012, INT C COMP VIS PATT; Xu W., 2014, EUR C COMP VIS; Yan X., 2009, LINEAR REGRESSION AN, DOI DOI 10.1142/6986; Zollhofer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601165	37	1	1	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2017	122	1					61	83		10.1007/s11263-016-0954-x	http://dx.doi.org/10.1007/s11263-016-0954-x			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EL2AF					2022-12-18	WOS:000394421800004
J	Xu, JT; Yang, QX; Feng, ZR				Xu, Jintao; Yang, Qingxiong; Feng, Zuren			Occlusion-Aware Stereo Matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo matching; Cost aggregation; Foreground segmentation; Flash photography	FLOW	Stereo vision systems with additional flash/no-flash cues have been demonstrated to be robust to depth discontinuities. The ratio of a flash and no-flash image pair naturally provides additional scene depth information and thus can serve as a strong cue for preserving depth discontinuities. However, existing solution simply uses ratio as the guidance to perform matching cost aggregation and thus is still vulnerable to occlusions. Inevitable misalignment of flash and no-flash images due to camera and/or scene motion remains unsolved as well. This paper investigates into these two problems. An occlusion detection approach is derived based on foreground/background extraction. Matching cost computed in the occluded regions (which is useless and harmful) is thus discarded so that reliable information from non-occluded regions can be easily propagated in. The foreground, occlusion and depth estimation is modeled in a uniform framework base on Expectation-Maximum. The proposed solution is evaluated using both indoor and outdoor data sets, showing clear improvement over the state-of-the-art methods.	[Xu, Jintao; Feng, Zuren] Xi An Jiao Tong Univ, Inst Syst Engn, Xian, Shaanxi, Peoples R China; [Yang, Qingxiong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Anhui, Peoples R China	Xi'an Jiaotong University; Chinese Academy of Sciences; University of Science & Technology of China, CAS	Yang, QX (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Anhui, Peoples R China.	liiton.research@gmail.com	Yang, Qingxiong/K-1729-2015	Yang, Qingxiong/0000-0002-4378-2335				[Anonymous], 2015, SOFTK DEPTH SENS; Bastanlar Y, 2012, IMAGE VISION COMPUT, V30, P557, DOI 10.1016/j.imavis.2012.06.001; Blake A, 2004, LECT NOTES COMPUT SC, V3021, P428; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Chen C.-Y., 2014, CVPR; Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hirschmuller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221; Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156; Izadi Shahram, 2011, UIST, DOI [10.1145/2047196.2047270, DOI 10.1145/2047196.2047270]; Kahler O, 2013, IEEE I CONF COMP VIS, P3064, DOI 10.1109/ICCV.2013.380; Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3; Ma Z., 2013, ICCV; Murray D, 2000, AUTON ROBOT, V8, P161, DOI 10.1023/A:1008987612352; Prisacariu VA, 2012, IMAGE VISION COMPUT, V30, P236, DOI 10.1016/j.imavis.2012.01.003; Ren CY, 2013, IEEE I CONF COMP VIS, P1561, DOI 10.1109/ICCV.2013.197; Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scharstein D., MIDDLEBURY STEREO EV; Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x; Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509; SUN JA, 2007, CVPR; Sun J, 2006, ACM T GRAPHIC, V25, P772, DOI 10.1145/1141911.1141954; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Xiong W, 2009, IEEE T PATTERN ANAL, V31, P428, DOI 10.1109/TPAMI.2008.98; Yang HS, 2014, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2014.435; YANG Q, 2009, CVPR; Yang QX, 2012, LECT NOTES COMPUT SC, V7572, P399, DOI 10.1007/978-3-642-33718-5_29; YANG QX, 2012, CVPR, P1402; Ye J., 2013, ICCV; Ye JW, 2012, PROC CVPR IEEE, P310, DOI 10.1109/CVPR.2012.6247690; Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70; Yu Z., 2013, ICCV; Zabih R., 1994, ECCV; Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24; Zhou CY, 2012, PROC CVPR IEEE, P342, DOI 10.1109/CVPR.2012.6247694	36	1	1	0	31	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2016	120	3					256	271		10.1007/s11263-016-0910-9	http://dx.doi.org/10.1007/s11263-016-0910-9			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DV5OK					2022-12-18	WOS:000382977200002
J	Yuan, JS; Li, WQ; Zhang, ZY; Fleet, D; Shotton, J				Yuan, Junsong; Li, Wanqing; Zhang, Zhengyou; Fleet, David; Shotton, Jamie			Guest Editorial: Human Activity Understanding from 2D and 3D Data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Yuan, Junsong] Nanyang Technol Univ, Singapore 639798, Singapore; [Li, Wanqing] Univ Wollongong, Wollongong, NSW, Australia; [Zhang, Zhengyou] Microsoft Res, Redmond, WA USA; [Fleet, David] Univ Toronto, Toronto, ON, Canada; [Shotton, Jamie] Microsoft Res, Cambridge, England	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University of Wollongong; Microsoft; University of Toronto; Microsoft	Yuan, JS (corresponding author), Nanyang Technol Univ, Singapore 639798, Singapore.	JSYUAN@ntu.edu.sg	Yuan, Junsong/R-4352-2019; Yuan, Junsong/A-5171-2011; zhang, zheng/HCH-9684-2022; Li, Wanqing/ABG-2620-2020	/0000-0003-0734-7114; Li, Wanqing/0000-0002-4427-2687					0	1	1	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2016	118	2			SI		113	114		10.1007/s11263-016-0915-4	http://dx.doi.org/10.1007/s11263-016-0915-4			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DO0OE		Bronze			2022-12-18	WOS:000377477400001
J	Diebold, J; Nieuwenhuis, C; Cremers, D				Diebold, Julia; Nieuwenhuis, Claudia; Cremers, Daniel			Midrange Geometric Interactions for Semantic Segmentation Constraints for Continuous Multi-label Optimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Variational; Image segmentation; Convex optimization; Directional relations; Geometric relations; Midlevel range interactions		In this article we introduce the concept of midrange geometric constraints into semantic segmentation. We call these constraints 'midrange' since they are neither global constraints, which take into account all pixels without any spatial limitation, nor are they local constraints, which only regard single pixels or pairwise relations. Instead, the proposed constraints allow to discourage the occurrence of labels in the vicinity of each other, e.g., 'wolf' and 'sheep'. 'Vicinity' encompasses spatial distance as well as specific spatial directions simultaneously, e.g., 'plates' are found directly above 'tables', but do not fly over them. It is up to the user to specifically define the spatial extent of the constraint between each two labels. Such constraints are not only interesting for scene segmentation, but also for part-based articulated or rigid objects. The reason is that object parts such as for example arms, torso and legs usually obey specific spatial rules, which are among the few things that remain valid for articulated objects over many images and which can be expressed in terms of the proposed midrange constraints, i.e. closeness and/or direction. We show, how midrange geometric constraints are formulated within a continuous multi-label optimization framework, and we give a convex relaxation, which allows us to find globally optimal solutions of the relaxed problem independent of the initialization.	[Diebold, Julia; Cremers, Daniel] Tech Univ Munich, D-80290 Munich, Germany; [Nieuwenhuis, Claudia] Univ Calif Berkeley, ICSI, Berkeley, CA USA	Technical University of Munich; University of California System; University of California Berkeley	Diebold, J (corresponding author), Tech Univ Munich, D-80290 Munich, Germany.	julia.diebold@tum.de; cnieuwe@berkeley.edu; cremers@tum.de						Arbelaez P., 2012, IEEE INT C COMP VIS; Batra D., 2010, IEEE INT C COMP VIS; Bergbauer J., 2013, ICCV WORKSH GRAPH MO; Bo YH, 2011, PROC CVPR IEEE; Carreira J., 2012, EUR C COMP VIS ECCV; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Delong A, 2012, INT J COMPUT VISION, V100, P38, DOI 10.1007/s11263-012-0531-x; Delong A, 2009, IEEE I CONF COMP VIS, P285, DOI 10.1109/ICCV.2009.5459263; DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409; Felzenszwalb P., 2010, IEEE INT C COMP VIS; Frohlich B., 2012, AS C COMP VIS ACCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x; Kohli P., 2007, IEEE INT C COMP VIS; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Komodakis N., 2009, IEEE INT C COMP VIS; Kontschieder P., 2013, IEEE INT C COMP VIS; Korc F., 2009, TRIGGP200901; Ladicky L., 2010, EUR C COMP VIS ECCV; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; Liu XQ, 2010, IEEE T PATTERN ANAL, V32, P1182, DOI 10.1109/TPAMI.2009.120; Lucchi A, 2011, IEEE I CONF COMP VIS, P9, DOI 10.1109/ICCV.2011.6126219; Luo P., 2013, IEEE INT C COMP VIS; Malisiewicz T., 2007, P BRIT MACH VIS C UK, DOI 10.5244/C.21.55; MICHELOT C, 1986, J OPTIMIZ THEORY APP, V50, P195, DOI 10.1007/BF00938486; Mollenhoff T., 2013, C EN MIN METH COMP V; Nieuwenhuis C, 2013, IEEE I CONF COMP VIS, P2328, DOI 10.1109/ICCV.2013.289; Nieuwenhuis C, 2013, INT J COMPUT VISION, V104, P223, DOI 10.1007/s11263-013-0619-y; Nieuwenhuis C, 2013, IEEE T PATTERN ANAL, V35, P1234, DOI 10.1109/TPAMI.2012.183; Nosrati M., 2013, IEEE INT C COMP VIS; Pock T, 2009, IEEE INT C COMP VIS; Pock T., 2009, IEEE C COMP VIS PATT; Pock T, 2011, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2011.6126441; Ramanan D., 2006, NIPS, P1129; Savarese S., 2006, IEEE INT C COMP VIS; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X; Shotton J., 2006, EUR C COMP VIS ECCV; Soille P., 2013, MORPHOLOGICAL IMAGE; Souiai M., 2013, ICCV WORKSH GRAPH MO; Souiai M., 2013, C EN MIN METH COMP V; Strekalovskiy E., 2011, IEEE INT C COMP VIS; Strekalovskiy E., 2012, EUR C COMP VIS ECCV; Toeppe E., 2013, IEEE C COMP VIS PATT; Toeppe E., 2010, AS C COMP VIS ACCV; Vezhnevets A, 2011, IEEE I CONF COMP VIS, P643, DOI 10.1109/ICCV.2011.6126299; Wang L., 2007, AS C COMP VIS ACCV; Yao J., 2012, IEEE INT C COMP VIS; Zach C., 2008, P VIS MOD VIS WORKSH	49	1	1	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2016	117	3					199	225		10.1007/s11263-015-0828-7	http://dx.doi.org/10.1007/s11263-015-0828-7			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DL1TF					2022-12-18	WOS:000375414600001
J	Baltaxe, M; Meer, P; Lindenbaum, M				Baltaxe, Michael; Meer, Peter; Lindenbaum, Michael			Local Variation as a Statistical Hypothesis Test	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image segmentation; Image oversegmentation; Superpixels; Grouping	SEGMENTATION; TEXTURE; LAYOUT	The goal of image oversegmentation is to divide an image into several pieces, each of which should ideally be part of an object. One of the simplest and yet most effective oversegmentation algorithms is known as local variation (LV) Felzenszwalb and Huttenlocher in Efficient graph-based image segmentation. IJCV 59(2):167-181 (2004). In this work, we study this algorithm and show that algorithms similar to LV can be devised by applying different statistical models and decisions, thus providing further theoretical justification and a well-founded explanation for the unexpected high performance of the LV approach. Some of these algorithms are based on statistics of natural images and on a hypothesis testing decision; we denote these algorithms probabilistic local variation (pLV). The best pLV algorithm, which relies on censored estimation, presents state-of-the-art results while keeping the same computational complexity of the LV algorithm.	[Baltaxe, Michael] Orbotech Ltd, IL-8110101 Yavne, Israel; [Meer, Peter] Rutgers State Univ, Dept Elect & Comp Engn, Piscataway, NJ 08854 USA; [Lindenbaum, Michael] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Rutgers State University New Brunswick; Technion Israel Institute of Technology	Baltaxe, M (corresponding author), Orbotech Ltd, IL-8110101 Yavne, Israel.	michael.baltaxe@orbotech.com; meer@cronos.rutgers.edu; mic@cs.technion.ac.il						Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Baltaxe M., 2014, THESIS TECHNION; Cabral R, 2014, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2014.546; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; De Bonet J. S., 1997, P 1997 C ADV NEUR IN, P773; Delaitre V, 2012, LECT NOTES COMPUT SC, V7577, P284, DOI 10.1007/978-3-642-33783-3_21; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; EPSTEIN B, 1953, J AM STAT ASSOC, V48, P486, DOI 10.2307/2281004; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Glazer A., 2012, NIPS, P737; Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x; Grenander U, 2001, IEEE T PATTERN ANAL, V23, P424, DOI 10.1109/34.917579; Heiler M, 2005, INT J COMPUT VISION, V63, P5, DOI 10.1007/s11263-005-4944-7; Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y; Jinggang Huang, 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P541, DOI 10.1109/CVPR.1999.786990; Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124; Kim Sungwoong, 2011, ADV NEURAL INFORM PR; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; Larsen R.J., 2012, INTRO MATH STAT ITS; Lee YJ, 2015, INT J COMPUT VISION, V114, P38, DOI 10.1007/s11263-014-0794-5; Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; MEYER F, 1994, SIGNAL PROCESS, V38, P113, DOI 10.1016/0165-1684(94)90060-4; Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323; Moulin P, 1999, IEEE T INFORM THEORY, V45, P909, DOI 10.1109/18.761332; Pauwels EJ, 2000, LECT NOTES COMPUTER, P85; Peng B, 2011, IEEE T IMAGE PROCESS, V20, P3592, DOI 10.1109/TIP.2011.2157512; Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10; Ren ZL, 2013, PROC CVPR IEEE, P2011, DOI 10.1109/CVPR.2013.262; Rosenfeld A, 2011, IEEE I CONF COMP VIS, P1371, DOI 10.1109/ICCV.2011.6126391; Ross Sheldon M., 2009, PROBABILITY STAT ENG; RUGGLES R, 1947, J AM STAT ASSOC, V42, P72, DOI 10.2307/2280189; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444	40	1	1	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2016	117	2					131	141		10.1007/s11263-015-0855-4	http://dx.doi.org/10.1007/s11263-015-0855-4			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DH6UB		Green Submitted			2022-12-18	WOS:000372926500002
J	Mairal, J; Elad, M; Bach, F				Mairal, J.; Elad, M.; Bach, F.			Guest Editorial: Sparse Coding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Mairal, J.] Univ Grenoble Alpes, Lab Jean Kuntzmann, Inria Lear Team, Grenoble, France; [Elad, M.] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel; [Bach, F.] Ecole Normale Super, Dept Informat, Inria Sierra Team, F-75231 Paris, France	UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Inria; Technion Israel Institute of Technology; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Mairal, J (corresponding author), Univ Grenoble Alpes, Lab Jean Kuntzmann, Inria Lear Team, Grenoble, France.	Julien.mairal@m4x.org	Mairal, Julien/AAL-5611-2021; , Miki/AAH-4640-2019						0	1	2	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2015	114	2-3			SI		89	90		10.1007/s11263-015-0845-6	http://dx.doi.org/10.1007/s11263-015-0845-6			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CP7MJ		Bronze			2022-12-18	WOS:000360071900001
J	Adluru, N; Yang, XW; Latecki, LJ				Adluru, Nagesh; Yang, Xingwei; Latecki, Longin Jan			Sequential Monte Carlo for Maximum Weight Subgraphs with Application to Solving Image Jigsaw Puzzles	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sequential Monte Carlo; Particle filtering; Sampling importance resampling; Maximumweight clique; Jigsaw puzzle problem; Graph search; Graph matching; QAP	TABU SEARCH ALGORITHM; GRAPH; SHAPE	We consider a problem of finding maximum weight subgraphs (MWS) that satisfy hard constraints in a weighted graph. The constraints specify the graph nodes that must belong to the solution as well as mutual exclusions of graph nodes, i.e., pairs of nodes that cannot belong to the same solution. Our main contribution is a novel inference approach for solving this problem in a sequential monte carlo (SMC) sampling framework. Usually in an SMC framework there is a natural ordering of the states of the samples. The order typically depends on observations about the states or on the annealing setup used. In many applications (e.g., image jigsaw puzzle problems), all observations (e.g., puzzle pieces) are given at once and it is hard to define a natural ordering. Therefore, we relax the assumption of having ordered observations about states and propose a novel SMC algorithm for obtaining maximum a posteriori estimate of a high-dimensional posterior distribution. This is achieved by exploring different orders of states and selecting the most informative permutations in each step of the sampling. Our experimental results demonstrate that the proposed inference framework significantly outperforms loopy belief propagation in solving the image jigsaw puzzle problem. In particular, our inference quadruples the accuracy of the puzzle assembly compared to that of loopy belief propagation.	[Adluru, Nagesh] Univ Wisconsin, Madison, WI USA; [Yang, Xingwei] Amazon Com, Machine Learning Sci Grp, Seattle, WA USA; [Latecki, Longin Jan] Temple Univ, Philadelphia, PA 19122 USA	University of Wisconsin System; University of Wisconsin Madison; Amazon.com; Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple University	Latecki, LJ (corresponding author), Temple Univ, Philadelphia, PA 19122 USA.	nagesh.adluru@gmail.com; happyyxw@gmail.com; latecki@temple.edu		Latecki, Longin Jan/0000-0002-5102-8244	Waisman Core Grant [P30 HD003352-45]; NSF [IIS-1302164, OIA-1027897]; EUNICE KENNEDY SHRIVER NATIONAL INSTITUTE OF CHILD HEALTH & HUMAN DEVELOPMENT [P30HD003352] Funding Source: NIH RePORTER	Waisman Core Grant; NSF(National Science Foundation (NSF)); EUNICE KENNEDY SHRIVER NATIONAL INSTITUTE OF CHILD HEALTH & HUMAN DEVELOPMENT(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH Eunice Kennedy Shriver National Institute of Child Health & Human Development (NICHD))	The authors would like to thank Taeg Sang Cho for providing the code for the method in Cho et al. (2010). We would like to acknowledge the Waisman Core Grant P30 HD003352-45, the NSF under Grants IIS-1302164 and OIA-1027897.	ALMOHAMAD HA, 1993, IEEE T PATTERN ANAL, V15, P522, DOI 10.1109/34.211474; ALTMAN DG, 1983, J ROY STAT SOC D-STA, V32, P307, DOI 10.2307/2987937; Alvarez-Miranda E., 2013, FACETS COMBINATORIAL, P245, DOI DOI 10.1007/978-3-642-38189-8_11; Arora S, 1996, AN S FDN CO, P21, DOI 10.1109/SFCS.1996.548460; Asahiro Y, 2002, DISCRETE APPL MATH, V121, P15, DOI 10.1016/S0166-218X(01)00243-8; Bisiani R, 1987, ENCY ARTIFICIAL INTE, P56; BORTZ AB, 1975, J COMPUT PHYS, V17, P10, DOI 10.1016/0021-9991(75)90060-1; Butman M., 2008, CVPR; Caetano TS, 2006, IEEE T PATTERN ANAL, V28, P1646, DOI 10.1109/TPAMI.2006.207; Carpenter J., 1999, BUILDING ROBUST SIMU, V1-27; CHEN Z, 2003, TECHNICAL REPORT; Chlebik M, 2008, THEOR COMPUT SCI, V406, P207, DOI 10.1016/j.tcs.2008.06.046; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; CHO TS, 2010, CVPR; CHRISTOFIDES N, 1989, OPER RES, V37, P760, DOI 10.1287/opre.37.5.760; Cour T., 2007, P ADV NEURAL INFORM, P313; Crisan D, 2002, IEEE T SIGNAL PROCES, V50, P736, DOI 10.1109/78.984773; Cross ADJ, 1998, IEEE T PATTERN ANAL, V20, P1236, DOI 10.1109/34.730557; Demaine ED, 2007, GRAPH COMBINATOR, V23, P195, DOI 10.1007/s00373-007-0713-4; Doucet A., 2001, SEQUENTIAL MONTE CAR; DYER ME, 1985, EUR J OPER RES, V20, P102, DOI 10.1016/0377-2217(85)90288-7; FOX D, 2000, SEQUENTIAL MONTE CAR; FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781; Furcy D, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P125; Georgescu B, 2004, IEEE T PATTERN ANAL, V26, P674, DOI 10.1109/TPAMI.2004.2; Glover F., 1989, ORSA Journal on Computing, V1, P190, DOI [10.1287/ijoc.2.1.4, 10.1287/ijoc.1.3.190]; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; GOLDBERG D, 2002, S COMP GEOM; Hamze F., 2005, ADV NEURAL INFORM PR, V18, P1; Hamze F., 2007, UNCERTAINTY ARTIFICI, V1, P167; HASSIN R, 1994, INFORM PROCESS LETT, V51, P133, DOI 10.1016/0020-0190(94)00086-7; HORAUD R, 1989, IEEE T PATTERN ANAL, V11, P1168, DOI 10.1109/34.42855; Ioffe S, 2001, INT J COMPUT VISION, V43, P45, DOI 10.1023/A:1011179004708; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Isard M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P343, DOI 10.1007/BFb0015549; Jiang H, 2007, IEEE T PATTERN ANAL, V29, P959, DOI 10.1109/TPAMI.2007.1048; Khan Z, 2004, LECT NOTES COMPUT SC, V2034, P279; Kong WX, 2001, PROC CVPR IEEE, P583; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Leordeanu Marius, 2009, ADV NEURAL INFORM PR; Liu H., 2010, ADV NEURAL INFORM PR, P1414; Liu JS, 2001, STAT ENG IN, P225; LIUE J, 2001, MONTE CARLO STRATEGI; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu CE, 2009, IEEE I CONF COMP VIS, P2288, DOI 10.1109/ICCV.2009.5459446; Macambira EM, 2002, ANN OPER RES, V117, P175, DOI 10.1023/A:1021525624027; Maciel J, 2003, IEEE T PATTERN ANAL, V25, P187, DOI 10.1109/TPAMI.2003.1177151; Makridis M, 2006, IEEE IMAGE PROC, P2001, DOI 10.1109/ICIP.2006.312891; Misevicius A, 2005, COMPUT OPTIM APPL, V30, P95, DOI 10.1007/s10589-005-4562-x; Misevicius A, 2012, OR SPECTRUM, V34, P665, DOI 10.1007/s00291-011-0274-z; Montenegro R, 2006, FOUND TRENDS THEOR C, V1, P237, DOI 10.1561/0400000003; Nielsen TR, 2008, PATTERN RECOGN LETT, V29, P1924, DOI 10.1016/j.patrec.2008.05.027; Pavan M, 2007, IEEE T PATTERN ANAL, V29, P167, DOI 10.1109/TPAMI.2007.250608; Pomeranz D, 2011, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2011.5995331; RADACK GM, 1982, COMPUT VISION GRAPH, V19, P1, DOI 10.1016/0146-664X(82)90111-3; Rysz M., 2013, J COMB OPTIM, V43, P1; Singh R., 2007, RES COMPUTATIONAL BI, V4453; Smith K, 2005, PROC CVPR IEEE, P962; Sontag D., 2010, OPTIMIZATION MACHINE; Suh Y, 2012, LECT NOTES COMPUT SC, V7574, P624, DOI 10.1007/978-3-642-33712-3_45; TAILLARD E, 1991, PARALLEL COMPUT, V17, P443, DOI 10.1016/S0167-8191(05)80147-4; Taillard E. D., 1995, Location Science, V3, P87, DOI 10.1016/0966-8349(95)00008-6; Thrun S., 2005, PROBABILISTIC ROBOTI; Thrun S., 2002, P 17 ANN C UNC AI UA; Tillmann C, 2003, COMPUT LINGUIST, V29, P97, DOI 10.1162/089120103321337458; UMEYAMA S, 1988, IEEE T PATTERN ANAL, V10, P695, DOI 10.1109/34.6778; Vassilevska V, 2010, ACM T ALGORITHMS, V6, DOI 10.1145/1798596.1798597; Williams VV, 2013, SIAM J COMPUT, V42, P831, DOI 10.1137/09076619X; Wolfson H., 1988, Annals of Operations Research, V12, P51, DOI 10.1007/BF02186360; Yang X., 2011, P IEEE C COMP VIS PA, P2873; Yang XF, 2010, PART FIBRE TOXICOL, V7, DOI 10.1186/1743-8977-7-1; Yao FH, 2003, PATTERN RECOGN LETT, V24, P1819, DOI 10.1016/S0167-8655(03)00006-0; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zass R., 2008, P 2008 IEEE C COMP V, P1, DOI DOI 10.1109/CVPR.2008.4587500; Zhou F, 2013, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2013.376; Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667	79	1	1	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2015	112	3					319	341		10.1007/s11263-014-0766-9	http://dx.doi.org/10.1007/s11263-014-0766-9			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CG3EA	26052182	Green Accepted			2022-12-18	WOS:000353159600004
J	Kulkarni, K; Evangelidis, G; Cech, J; Horaud, R				Kulkarni, Kaustubh; Evangelidis, Georgios; Cech, Jan; Horaud, Radu			Continuous Action Recognition Based on Sequence Alignment (vol 112, pg 90, 2015)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Kulkarni, Kaustubh; Evangelidis, Georgios; Horaud, Radu] INRIA Grenoble Rhone Alpes, Montbonnot St Martin, France; [Cech, Jan] Czech Tech Univ, Ctr Machine Percept, CR-16635 Prague, Czech Republic	Czech Technical University Prague	Horaud, R (corresponding author), INRIA Grenoble Rhone Alpes, Montbonnot St Martin, France.	Kaustubh.Kulkarni@inria.fr; Georgios.Evangelidis@inria.fr; cechj@cmp.felk.cvut.cz; Radu.Horaud@inria.fr	Horaud, Radu/AAR-5982-2021	Horaud, Radu/0000-0001-5232-024X				Kulkarni K, 2015, INT J COMPUT VISION, V112, P90, DOI 10.1007/s11263-014-0758-9	1	1	1	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2015	112	1					130	130		10.1007/s11263-014-0782-9	http://dx.doi.org/10.1007/s11263-014-0782-9			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CC4YC		Bronze			2022-12-18	WOS:000350361500007
J	Cipolla, R; Colombo, C; Del Bimbo, A				Cipolla, Roberto; Colombo, Carlo; Del Bimbo, Alberto			Special Issue on Large-Scale Computer Vision: Geometry, Inference, and Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Cipolla, Roberto] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England; [Colombo, Carlo] Univ Florence, Computat Vis Grp, Dipartimento Sistemi & Informat, I-50139 Florence, Italy; [Del Bimbo, Alberto] Univ Florence, Dipartimento Sistemi & Informat, Fac Ingn, I-50137 Florence, Italy	University of Cambridge; University of Florence; University of Florence	Colombo, C (corresponding author), Univ Florence, Computat Vis Grp, Dipartimento Sistemi & Informat, Via Santa Marta 3, I-50139 Florence, Italy.	carlo.colombo@unifi.it	Colombo, Carlo/AAC-6675-2019; Arandjelović, Ognjen/V-5255-2019	Arandjelović, Ognjen/0000-0002-9314-194X; Cipolla, Roberto/0000-0002-8999-2151; DEL BIMBO, ALBERTO/0000-0002-1052-8322; COLOMBO, CARLO/0000-0001-9234-537X					0	1	1	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2014	110	3			SI		241	242		10.1007/s11263-014-0772-y	http://dx.doi.org/10.1007/s11263-014-0772-y			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AT2HL		Bronze			2022-12-18	WOS:000344754500001
J	Koenderink, JJ; van Doorn, AJ; Pont, SC				Koenderink, J. J.; van Doorn, A. J.; Pont, S. C.			The "shading twist," a dynamical shape cue	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape from shading; Dynamical shading; Shadow edges; Isophotes twist; Gaussian image; Spherical image		When a light source moves, the isophotes of the illuminance field (observed as "shading" in the case of Lambertian surfaces) move over the surfaces of illuminated objects. The sense of rotation of the isophotes relative to the sense of rotation of the "surface illuminance flow," that is the tangential component of the illumination direction over the surface, depends on the Gaussian curvature of the surface. The temporal change of orientation, the "shading twist," reveals this purely surface related quality. Since the shading twist depends only on the local curvature of the surface it doesn't matter at all how the light source moves or where the light sources are, the "shading twist" is a pure surface property, that is to say, it behaves as being painted upon the surface. The formal relations pertaining to the shading twist are analyzed and a numerical simulation is presented that fully corroborates the conclusions from the formal study. The algorithm is also tested on a real scene.	[Koenderink, J. J.] Univ Leuven KU Leuven, Expt Psychol Lab, B-3000 Louvain, Belgium; [Koenderink, J. J.] Univ Utrecht, Fac Sociale Wetenschappen, NL-3584 CS Utrecht, Netherlands; [Koenderink, J. J.] Flemish Acad Ctr Sci & Arts, Brussels, Belgium; [van Doorn, A. J.; Pont, S. C.] Delft Univ Technol, Dept Ind Design, NL-2628 CE Delft, Netherlands	KU Leuven; Utrecht University; Delft University of Technology	Koenderink, JJ (corresponding author), Univ Leuven KU Leuven, Expt Psychol Lab, Tiensestr 102,Box 3711, B-3000 Louvain, Belgium.	jan.koenderink@telfort.nl		Pont, Sylvia/0000-0002-9834-9600	Flemish Government [METH/08/02]	Flemish Government(European Commission)	Jan Koenderink was supported by the Methusalem Program by the Flemish Government (METH/08/02), awarded to Johan Wagemans (KUL).	Banchoff T., 1982, CUSPS GAUSS MAPPINGS; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Blaschke W., 1975, ELEMENTARE DIFFERENT; Breton P, 1996, PROC CVPR IEEE, P782, DOI 10.1109/CVPR.1996.517161; Casorati F., 1889, REND I MATEM ACCAD L, V2, P335; Fechner G. T., 1860, ELEMENTE PSYCHOPHYSI; Forsyth David A, 2012, COMPUTER VISION MODE; Galilei G., 1610, SIDEREUS NUNCIUS; Hilbert D., 1952, GEOMETRY IMAGINATION; Horn B.K.P., 1989, SHAPE SHADING; Koenderink J., 1990, SOLID SHAPE; Koenderink J. J., 2011, PERCEPTION INFERENCE, P27, DOI DOI 10.1155/IJBI/2006/92329; KOENDERINK JJ, 1980, OPT ACTA, V27, P981, DOI 10.1080/713820338; Koenderink JJ, 2003, J OPT SOC AM A, V20, P1875, DOI 10.1364/JOSAA.20.001875; Kortum G., 1969, SPECTROSCOPY-US, P5; Lambert J.H., 1760, PHOTOMETRIA SIVE MEN; Nicodemus F. E., 1977, MN160 US DEP COMM NA; van Diggelen J., 1951, B ASTRON I NETH, V11, P283; von Helmholtz H., 1860, HDB PHYSL OPTIK; Weber E. H., 1834, PULSU RESORPTIONE AU; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284	22	1	1	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2013	105	1					49	62		10.1007/s11263-013-0626-z	http://dx.doi.org/10.1007/s11263-013-0626-z			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	194HL					2022-12-18	WOS:000322621800003
J	Yang, L; Zheng, NN; Chen, M; Yang, Y; Yang, J				Yang, Lei; Zheng, Nanning; Chen, Mei; Yang, Yang; Yang, Jie			Categorization of Multiple Objects in a Scene Using a Biased Sampling Strategy	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object categorization; Bag-of-features method; Biased sampling strategy; Multi-instance multi-label learning	IMAGE; FEATURES; SEGMENTATION; RECOGNITION; TEXTURE; SPARSE; SCALE	Recently, various bag-of-features (BoF) methods show their good resistance to within-class variations and occlusions in object categorization. In this paper, we present a novel approach for multi-object categorization within the BoF framework. The approach addresses two issues in BoF related methods simultaneously: how to avoid scene modeling and how to predict labels of an image when multiple categories of objects are co-existing. We employ a biased sampling strategy which combines the bottom-up, biologically inspired saliency information and loose, top-down class prior information for object class modeling. Then this biased sampling component is further integrated with a multi-instance multi-label leaning and classification algorithm. With the proposed biased sampling strategy, we can perform multi-object categorization within an image without semantic segmentation. The experimental results on PASCAL VOC2007 and SUN09 show that the proposed method significantly improves the discriminative ability of BoF methods and achieves good performance in multi-object categorization tasks.	[Yang, Lei] Xi An Jiao Tong Univ, China Mobile Res Inst, Beijing 100053, Peoples R China; [Zheng, Nanning; Yang, Yang] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China; [Chen, Mei] Intel Labs Pittsburgh, Pittsburgh, PA 15213 USA; [Yang, Jie] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	China Mobile; Xi'an Jiaotong University; Xi'an Jiaotong University; Intel Corporation; Carnegie Mellon University	Yang, L (corresponding author), Xi An Jiao Tong Univ, China Mobile Res Inst, Beijing 100053, Peoples R China.	ylangei@hotmail.com; nnzheng@mail.xjtu.edu.cn; mei.chen@intel.com; yyang_993@hotmail.com; jie.yang@cs.cmu.edu			State Key Program of National Natural Science of China [60635050]; National Science Foundation of USA	State Key Program of National Natural Science of China(National Natural Science Foundation of China (NSFC)); National Science Foundation of USA(National Science Foundation (NSF))	This research was supported by the State Key Program of National Natural Science of China (Grant No. 60635050). The last Author was partially supported by National Science Foundation of USA.	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108; Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248; Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPRW.2009.5206800, 10.1109/CVPR.2009.5206800]; Chum O., 2007, P IEEE C COMP VIS PA, P1; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Dorko G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P634; Edgar G. A., 1990, UNDERGRADUATE TEXTS; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb P, 2008, PROC CVPR IEEE, P1984; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144; Fulkerson B, 2008, LECT NOTES COMPUT SC, V5302, P179, DOI 10.1007/978-3-540-88682-2_15; Galleguillos C, 2008, LECT NOTES COMPUT SC, V5302, P193, DOI 10.1007/978-3-540-88682-2_16; Gevers T, 2008, P 2008 INT C CONT BA, P141, DOI DOI 10.1145/1386352.1386376; Harzallah H, 2009, IEEE I CONF COMP VIS, P237, DOI 10.1109/ICCV.2009.5459257; Hou X, 2007, 2007 IEEE C COMP VIS, V800, P1, DOI DOI 10.1109/CVPR.2007.383267; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Kang F., 2006, 2006 IEEE COMPUTER S, P1719, DOI DOI 10.1109/CVPR.2006.90; Khan F. S., 2009, INT C COMP VIS ICCV, P1719; KITTLER J, 1986, PATTERN RECOGN, V19, P41, DOI 10.1016/0031-3203(86)90030-0; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Leibe B., 2004, EUROPEAN C COMPUTER, P17; Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847; Li LJ, 2009, PROC CVPR IEEE, P2036, DOI 10.1109/CVPRW.2009.5206718; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mairal J, 2008, LECT NOTES COMPUT SC, V5304, P43, DOI 10.1007/978-3-540-88690-7_4; Marszaek M, 2006, IEEE C COMP VIS PATT, P2118; Marszalek M., 2007, VIS REC CHALL WORKSH; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Moosmann F, 2008, IEEE T PATTERN ANAL, V30, P1632, DOI 10.1109/TPAMI.2007.70822; Niblack W., 1986, ADV COMPUTER GRAPHIC; Nister David, 2006, CVPR, P2161, DOI DOI 10.1109/CVPR.2006.264; Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Pantofaru C., 2006, P CVPR 06 INT WORKSH, P23; Parikh D, 2008, LECT NOTES COMPUT SC, V5303, P446, DOI 10.1007/978-3-540-88688-4_33; Perronnin F, 2010, PROC CVPR IEEE, P2297, DOI 10.1109/CVPR.2010.5539914; Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x; van de Weijer J, 2006, LECT NOTES COMPUT SC, V3952, P334; Walther D., 2004, WORKSH ATT PERF COMP, P96; Yang L., 2009, INT C COMP VIS ICCV; Yang L, 2009, AS C COMP VIS; Zha ZJ, 2008, PROC CVPR IEEE, P333; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4; Zhang M.L, 2007, AAAI, V7, P669; Zhou Z.H., 2007, NIPS 19, P1609, DOI DOI 10.1016/J.PATCOG.2006.12.019	58	1	1	0	31	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2013	105	1					1	18		10.1007/s11263-013-0629-9	http://dx.doi.org/10.1007/s11263-013-0629-9			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	194HL					2022-12-18	WOS:000322621800001
J	Wu, FC; Zhang, M; Hu, ZY				Wu, F. C.; Zhang, M.; Hu, Z. Y.			Self-Calibration Under the Cayley Framework	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cayley transformation; Self-calibration; Affine reconstruction; Metric reconstruction		The Cayley framework here is meant to tackle the vision problems under the infinite Cayley transformation (ICT), its main advantage lies in its numerical stability. In this work, the stratified self-calibration under the Cayley framework is investigated. It is well known that the main difficulty of the stratified self-calibration in multiple view geometry is to upgrade a projective reconstruction to an affine one, in other words, to estimate the unknown 3-vector of the plane at infinity, called the normal vector. To our knowledge, without any prior knowledge about the scene or the camera motion, the only available constraint on a moving camera with constant intrinsic parameters is the well-known Modulus Constraint in the literature. Do other kinds of constraints exist? If yes, what they are? How could they be used? In this work, such questions will be systematically investigated under the Cayley framework. Our key contributions include: 1. The original projective expression of the ICT is simplified and a new projective expression is derived to make the upgrade easier from a projective reconstruction to a metric reconstruction. 2. The constraints on the normal vector are systematically investigated. For two views, two constraints on the normal vector are derived; one of them is the well-known modulus constraint, while the other is a new inequality constraint. There are only these two constraints for two views. For three views, besides the constraints for two views, two groups of new constraints are derived and each of them contains three constraints. In other words, there are 12 constraints in total for three views. 3. Based on our projective expression and these constraints, a stratified Cayley algorithm and a total Cayley algorithm are proposed for the metric reconstruction from images. It is experimentally shown that they both improve significantly the numerical stability of the classical algorithms. Compared with the global optimal algorithm under the infinite homography framework, the Cayley algorithms have comparable calibration accuracy, but substantially reduce the computational load.	[Wu, F. C.; Zhang, M.; Hu, Z. Y.] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS	Wu, FC (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, POB 2728, Beijing 100190, Peoples R China.	fcwu@nlpr.ia.ac.cn; mzhang@nlpr.ia.ac.cn; huzy@nlpr.ia.ac.cn			National Natural Science Foundation of China [60835003, 91120012]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We wish to thank the anonymous reviewers for their inspiring comments and suggestions. Also, we gratefully acknowledge the support from the National Natural Science Foundation of China (60835003, 91120012).	Beardsley P., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P683; Cayley A., 1846, J REINE ANGEW MATH, V32, P119, DOI DOI 10.1515/CRLL.1846.32.119; Chandraker M, 2010, INT J COMPUT VISION, V90, P236, DOI 10.1007/s11263-009-0305-2; Courant R, 1989, METHODS MATH PHYS; Faugeras O. D., 1992, Computer Vision - ECCV '92. Second European Conference on Computer Vision Proceedings, P563; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; Hartley R. I., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P510, DOI 10.1109/ICCV.1999.791264; Hartley R. I., 1994, Applications of Invariance in Computer Vision. Second Joint European - US Workshop Proceedings, P237; Hartley RI, 1997, INT J COMPUT VISION, V22, P5, DOI 10.1023/A:1007957826135; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; Heyden A., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P339, DOI 10.1109/ICPR.1996.546045; Horaud R, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P96, DOI 10.1109/ICCV.1998.710706; Mohr R., 1994, APPL INVARIANCE COMP, P297; Morgan M., 1987, SOLVING POLYNOMIAL S; Oliensis J, 2007, IEEE T PATTERN ANAL, V29, P2217, DOI 10.1109/TPAMI.2007.1132; Pollefeys M, 1999, IEEE T PATTERN ANAL, V21, P707, DOI 10.1109/34.784285; Pollefeys M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P90, DOI 10.1109/ICCV.1998.710705; Ponce J, 2005, PROC CVPR IEEE, P780; Schaffalitzky F., 2000, IND C COMP VIS GRAPH, P314; SHERMAN J, 1950, ANN MATH STAT, V21, P124, DOI 10.1214/aoms/1177729893; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; Torr P., 1995, THESIS DEP ENG SCI; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388; Wu FC, 2009, INT J COMPUT VISION, V82, P156, DOI 10.1007/s11263-008-0193-x	25	1	7	0	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2013	103	3					372	398		10.1007/s11263-013-0610-7	http://dx.doi.org/10.1007/s11263-013-0610-7			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	155WJ					2022-12-18	WOS:000319778800005
J	Zivkovic, Z; Sebe, N; Aghajan, H; Kisacanin, B				Zivkovic, Zoran; Sebe, Nicu; Aghajan, Hamid; Kisacanin, Branislav			Guest Editorial: Human-Computer Interaction: Real-Time Vision Aspects of Natural User Interfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Zivkovic, Zoran] NXP Semicond, Eindhoven, Netherlands; [Sebe, Nicu] Univ Trent, Trento, Italy; [Aghajan, Hamid] Stanford Univ, Stanford, CA 94305 USA; [Aghajan, Hamid] Univ Ghent, B-9000 Ghent, Belgium; [Kisacanin, Branislav] Texas Instruments Inc, Dallas, TX USA	NXP Semiconductors; University of Trento; Stanford University; Ghent University; Texas Instruments	Zivkovic, Z (corresponding author), NXP Semicond, Eindhoven, Netherlands.	zoran.z.zivkovic@googlemail.com	Kisacanin, Branislav/ABE-6346-2020	Kisacanin, Branislav/0000-0001-7532-1106; Sebe, Niculae/0000-0002-6597-7248					0	1	1	0	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2013	101	3			SI		401	402		10.1007/s11263-012-0603-y	http://dx.doi.org/10.1007/s11263-012-0603-y			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	086WF		Bronze			2022-12-18	WOS:000314719000001
J	Perina, A; Jojic, N; Cristani, M; Murino, V				Perina, Alessandro; Jojic, Nebojsa; Cristani, Marco; Murino, Vittorio			Stel Component Analysis: Joint Segmentation, Modeling and Recognition of Objects Classes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object class modeling; Statistical learning; Bag of words and beyond; Segmentation; Recognition	SCENES; EM	Models that captures the common structure of an object class have appeared few years ago in the literature (Jojic and Caspi in Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pp. 212-219, 2004; Winn and Jojic in Proceedings of International Conference on Computer Vision (ICCV), pp. 756-763, 2005); they are often referred as "stel models." Their main characteristic is to segment objects in clear, often semantic, parts as a consequence of the modeling constraint which forces the regions belonging to a single segment to have a tight distribution over local measurements, such as color or texture. This self-similarity within a region in a single image is typical of many meaningful image parts, even when across different images of similar objects, the corresponding parts may not have similar local measurements. Moreover, the segmentation itself is expected to be consistent within a class, although still flexible. These models have been applied mostly to segmentation scenarios. In this paper, we extent those ideas (1) proposing to capture correlations that exist in structural elements of an image class due to global effects, (2) exploiting the segmentations to capture feature co-occurrences and (3) allowing the use of multiple, eventually sparse, observation of different nature. In this way we obtain richer models more suitable to recognition tasks. We accomplish these requirements using a novel approach we dubbed stel component analysis. Experimental results show the flexibility of the model as it can deal successfully with image/video segmentation and object recognition where, in particular, it can be used as an alternative of, or in conjunction with, bag-of-features and related classifiers, where stel inference provides a meaningful spatial partition of features.	[Perina, Alessandro; Jojic, Nebojsa] Microsoft Res, Redmond, WA USA; [Perina, Alessandro; Cristani, Marco; Murino, Vittorio] Univ Verona, I-37100 Verona, Italy; [Cristani, Marco; Murino, Vittorio] Italian Inst Technol, Genoa, Italy	Microsoft; University of Verona; Istituto Italiano di Tecnologia - IIT	Perina, A (corresponding author), Microsoft Res, Redmond, WA USA.	alessandro.perina@gmail.com	Cristani, Marco/I-5275-2012	Murino, Vittorio/0000-0002-8645-2328				Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226; Alexe B, 2010, LECT NOTES COMPUT SC, V6315, P380, DOI 10.1007/978-3-642-15555-0_28; [Anonymous], P ICPR WORKSH LEARN; Bagon S, 2010, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2010.5540233; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598; Borenstein E, 2004, LECT NOTES COMPUT SC, V3023, P315; Bosch A, 2007, IEEE I CONF COMP VIS, P1863; Bosch A, 2006, LECT NOTES COMPUT SC, V3954, P517; Buntine W, 2002, LECT NOTES ARTIF INT, V2430, P23; CHEUNG V, 2007, PSYCHOL MED, P1; Chu XQ, 2010, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2010.5540196; Collins M., 2001, ADV NEURAL INFORM PR, P617; Cristani M., 2008, P 2008 IEEE C COMP V, P1; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Deng Li., 2003, SPEECH PROCESSING DY; Deselaers T, 2010, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2010.5539775; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Fei-Fei L., 2007, COMPUTER VISION IMAG; Frey BJ, 2003, IEEE T PATTERN ANAL, V25, P1, DOI 10.1109/TPAMI.2003.1159942; Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169; Graham D. B., 1997, FACE RECOGNITION THE, V163, P446; Hao S, 2009, IEEE I CONF COMP VIS, P213, DOI 10.1109/ICCV.2009.5459168; Harris C. G., 1988, P 4 ALV VIS C, V15, P10, DOI [10.5244/C.2.23, DOI 10.5244/C.2.23]; Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649; Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950; Jojic N, 2004, PROC CVPR IEEE, P212; Jojic N, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P34; Jojic N., 2006, P IEEE COMP SOC C CO, V1, P117; Jojic N., 2010, ADV NEURAL INFORM PR, P1027; Jojic N., 2004, P C UNC ART INT UAI, P293; Jojic N, 2009, PROC CVPR IEEE, P2044, DOI 10.1109/CVPRW.2009.5206581; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Leibe B., 2004, EUROPEAN C COMPUTER, P17; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Marszaek M, 2006, IEEE C COMP VIS PATT, P2118; Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384; Meng H, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, VOLS I-V, CONFERENCE PROCEEDINGS, P88, DOI 10.1109/ICMA.2007.4303521; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Munder S, 2006, IEEE T PATTERN ANAL, V28, P1863, DOI 10.1109/TPAMI.2006.217; Ni K, 2009, IEEE T PATTERN ANAL, V31, P2158, DOI 10.1109/TPAMI.2009.165; Perina A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1985, DOI 10.1109/CVPR.2011.5995742; Perina Alessandro, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P995, DOI 10.1109/ICPR.2010.249; Perina A., 2008, ECCV 2008 WORKSH 1 I; Perina A, 2010, LECT NOTES COMPUT SC, V6316, P15, DOI 10.1007/978-3-642-15567-3_2; Perina A, 2010, IMAGE VISION COMPUT, V28, P927, DOI 10.1016/j.imavis.2009.11.007; Quattoni A., 2004, PROC ADV NEURAL INF, V17, P1097; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Savarese S, 2007, IEEE I CONF COMP VIS, P1245; Shechtman E, 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383198; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; SIVIC J, 2005, INT C COMP VIS ICCV; Sudderth EB, 2005, IEEE I CONF COMP VIS, P1331; Sun M, 2009, PROC CVPR IEEE, P1247, DOI 10.1109/CVPRW.2009.5206723; Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183; Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1; Wallach H., 2009, ADV NEURAL INFORM PR, V22, P1973, DOI DOI 10.1007/S10708-008-9161-9; Weber M, 2000, LECT NOTES COMPUT SC, V1842, P18; Winn J, 2005, IEEE I CONF COMP VIS, P756; Yang JJ, 2009, IEEE I CONF COMP VIS, P436, DOI 10.1109/ICCV.2009.5459172; Yang Y, 2010, PROC CVPR IEEE, P3113, DOI 10.1109/CVPR.2010.5540070; Yin P., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383008; Zhang H., 2006, IEEE INT C COMP VIS, V2, P2126	67	1	1	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2012	100	3					241	260		10.1007/s11263-012-0536-5	http://dx.doi.org/10.1007/s11263-012-0536-5			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	008KV		Green Published, hybrid			2022-12-18	WOS:000308956700002
J	Kim, TK; Budvytis, I; Cipolla, R				Kim, Tae-Kyun; Budvytis, Ignas; Cipolla, Roberto			Making a Shallow Network Deep: Conversion of a Boosting Classifier into a Decision Tree by Boolean Optimisation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Boosting; Decision tree; Decision regions; Boolean optimisation; Boosting cascade; Face detection; Tracking; Segmentation		This paper presents a novel way to speed up the evaluation time of a boosting classifier. We make a shallow (flat) network deep (hierarchical) by growing a tree from decision regions of a given boosting classifier. The tree provides many short paths for speeding up while preserving the reasonably smooth decision regions of the boosting classifier for good generalisation. For converting a boosting classifier into a decision tree, we formulate a Boolean optimisation problem, which has been previously studied for circuit design but limited to a small number of binary variables. In this work, a novel optimisation method is proposed for, firstly, several tens of variables i.e. weak-learners of a boosting classifier, and then any larger number of weak-learners by using a two-stage cascade. Experiments on the synthetic and face image data sets show that the obtained tree achieves a significant speed up both over a standard boosting classifier and the Fast-exit-a previously described method for speeding-up boosting classification, at the same accuracy. The proposed method as a general meta-algorithm is also useful for a boosting cascade, where it speeds up individual stage classifiers by different gains. The proposed method is further demonstrated for fast-moving object tracking and segmentation problems.	[Kim, Tae-Kyun] Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, London SW7 2AZ, England; [Budvytis, Ignas; Cipolla, Roberto] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	Imperial College London; University of Cambridge	Kim, TK (corresponding author), Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, S Kensington Campus, London SW7 2AZ, England.	tk.kim@imperial.ac.uk	Arandjelović, Ognjen/V-5255-2019	Arandjelović, Ognjen/0000-0002-9314-194X; Cipolla, Roberto/0000-0002-8999-2151				Avidan S., 2006, P ECCV HER CRET GREE; Basak J, 2004, NEURAL COMPUT, V16, P1959, DOI 10.1162/0899766041336396; Brostow G., 2008, P ECCV HER CRET GREE; Chen J., 1994, P ACM S APPL COMP, P303; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Esposito F, 1997, IEEE T PATTERN ANAL, V19, P476, DOI 10.1109/34.589207; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Freund Y., 1999, P ICML; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Grabner H, 2006, IEEE C COMP VIS PATT, P260; Grossmann E., 2004, ORAL HLTH STATUS ORA; Grossmann E., 2004, J COMPUTER VISION PA, P105; Huang C., 2005, P ICCV; Kim TK, 2005, IMAGE VISION COMPUT, V23, P631, DOI 10.1016/j.imavis.2005.02.005; Li SZ, 2004, IEEE T PATTERN ANAL, V26, P1112, DOI 10.1109/TPAMI.2004.68; Mason L, 2000, ADV NEUR IN, V12, P512; Pham M.-T., 2007, P ICCV; Quinlan JR, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P725; Rahimi A., 2008, NIPS, V21; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Schwender H., 2007, ORAL HLTH STATUS ORA; Sochman J., 2005, P CVPR; Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055; Tu Z., 2005, P ICCV; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Viola P., 2001, P WORKSH STAT COMP T, VVolume 266, P56; Wu B., 2007, P ICCV; XIAO R, 2003, P ICCV; Yeh T, 2007, P ICCV; Zhou SK, 2005, LECT NOTES COMPUT SC, V3723, P198	32	1	1	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2012	100	2					203	215		10.1007/s11263-011-0461-z	http://dx.doi.org/10.1007/s11263-011-0461-z			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	000CQ					2022-12-18	WOS:000308364500007
J	Miao, X; Rao, RPN				Miao, Xu; Rao, Rajesh P. N.			Fast Structured Prediction Using Large Margin Sigmoid Belief Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structured prediction; Probabilistic graphical models; Exact and approximate inference	IMAGE ANNOTATION	Images usually contain multiple objects that are semantically related to one another. Mapping from low-level visual features to mutually dependent high-level semantics can be formulated as a structured prediction problem. Current statistical models for structured prediction make simplifying assumptions about the underlying output graph structure, such as assuming a low-order Markov chain, because exact inference becomes intractable as the tree-width of the underlying graph increases. Approximate inference algorithms, on the other hand, force one to trade off representational power with computational efficiency. In this paper, we present large margin sigmoid belief networks (LMSBNs) for structured prediction in images. LMSBNs allow a very fast inference algorithm for arbitrary graph structures that runs in polynomial time with high probability. This probability is data-distribution dependent and is maximized in learning. The new approach overcomes the representation-efficiency trade-off in previous models and allows fast structured prediction with complicated graph structures. We present results from applying a fully connected model to semantic image annotation, image retrieval and optical character recognition (OCR) problems, and demonstrate that the proposed approach can yield significant performance gains over current state-of-the-art methods.	[Miao, Xu; Rao, Rajesh P. N.] Univ Washington, Seattle, WA 98125 USA	University of Washington; University of Washington Seattle	Miao, X (corresponding author), Univ Washington, Seattle, WA 98125 USA.	xm@cs.washington.edu; rao@cs.washington.edu		Rao, Rajesh P. N./0000-0003-0682-8952	ONR [N000140910097]; NSF [0930908]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This research was supported by ONR Cognitive Science program grant No. N000140910097 and NSF grant No. 0930908.	Abdou S, 2004, SPEECH COMMUN, V42, P409, DOI 10.1016/j.specom.2003.11.002; [Anonymous], 2006, IEEE T AUTOM SCI ENG; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bengio Y., 2007, ADV NEURAL INFORM PR, DOI 10.7551/mitpress/7503.003.0024; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009; Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61; Collins M, 2008, J MACH LEARN RES, V9, P1775; Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; Domingos P, 2008, P 24 C UNC ART INT, P383; Doucet A., 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.; Duygulu P., 2006, LECT NOTES COMPUTER, V2353, P349; Fan R, 2007, STUDY THRESHOLD SELE; Feng S., 2004, COMPUTER VISION PATT; Finley T., 2008, INT C MACHINE LEARNI, P304, DOI DOI 10.1145/1390156.1390195; Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266; Guo Y., 2005, ANN C UNC ART INT; Hinton G. E., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P448; Hoefel G., 2008, P 17 ACM C INF KNOWL, P271, DOI DOI 10.1145/1458082.1458120; Hsieh C.-J., 2008, P 25 INT C MACH LEAR, P408, DOI [10.1145/1390156.1390208, DOI 10.1145/1390156.1390208]; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Kassel R. H., 1995, THESIS CAMBRIDGE; Kolmogorov V., 2002, EUR C COMP VIS ECCV, P185; Kulesza A., 2007, ADV NEURAL INFORM PR; Lafferty J., 2001, INT C MACH LEARN; Lavrenko V., 2003, ADV NEURAL INFORM PR; Lazebnik S., 2006, COMPUTER VISION PATT; Liu J, 2009, PATTERN RECOGN, V42, P218, DOI 10.1016/j.patcog.2008.04.012; Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24; McCallum A, 2000, P 17 INT C MACH LEAR, P591; Metzler D, 2004, LECT NOTES COMPUT SC, V3115, P42; Miao X, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1156; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Perez-Cruz F., 2007, CONDITIONAL GRAPHICA; Quattoni A., 2004, PROC ADV NEURAL INF, V17, P1097; Rosenberg D.S., 2007, ANN C UNC ART INT, P318; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Shalev-Shwartz S., 2008, ADV NEURAL INFORM PR; Tadepalli P, 1996, J ARTIF INTELL RES, V4, P445, DOI 10.1613/jair.154; Taskar B., 2004, ADV NEURAL INFORM PR; Tsochantaridis I., 2004, INT C MACH LEARN; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Wainwright M. J., 2003, AISTATS; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091; Yavlinsky A, 2005, LECT NOTES COMPUT SC, V3568, P507; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085; Zhang T, 2004, ANN STAT, V32, P56	50	1	1	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2012	99	3					302	318		10.1007/s11263-011-0423-5	http://dx.doi.org/10.1007/s11263-011-0423-5			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	950NI					2022-12-18	WOS:000304655600004
J	Ronda, JI; Valdes, A				Ronda, Jose I.; Valdes, Antonio			Euclidean Upgrading from Segment Lengths	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Camera calibration; Euclidean upgrading; 3D reconstruction	CAMERA CALIBRATION; AUTOCALIBRATION	We address the problem of the recovery of Euclidean structure of a projectively distorted n-dimensional space from the knowledge of the, possibly diverse, lengths of a set of segments. This problem is relevant, in particular, for Euclidean reconstruction with uncalibrated cameras, extending previously known results in the affine setting. The key concept is the Quadric of Segments (QoS), defined in a higher-dimensional space by the set of segments of a fixed length, from which Euclidean structure can be obtained in closed form. We have intended to make a thorough study of the properties of the QoS, including the determination of the minimum number of segments of arbitrary length that determine it and its relationship with the standard geometric objects associated to the Euclidean structure of space. Explicit formulas are given to obtain the dual absolute quadric and the absolute quadratic complex from the QoS. Experiments with real and synthetic images evaluate the performance of the techniques.	[Valdes, Antonio] Univ Complutense Madrid, Dep Geometria & Topol, E-28040 Madrid, Spain; [Ronda, Jose I.] Univ Politecn Madrid, Grp Tratamiento Imagenes, E-28040 Madrid, Spain	Complutense University of Madrid; Universidad Politecnica de Madrid	Valdes, A (corresponding author), Univ Complutense Madrid, Dep Geometria & Topol, E-28040 Madrid, Spain.	jir@gti.ssr.upm.es; Antonio_Valdes@mat.ucm.es	Ronda, José Ignacio/T-7334-2018	Ronda, José Ignacio/0000-0003-1430-1835; Valdes, Antonio/0000-0001-5930-8307	Spanish Administration agency CDTI [CENIT-VISION 2007-1007]; Ministerio de Ciencia e Innovacion of the Spanish Government [TEC2007-67764]	Spanish Administration agency CDTI(Spanish Government); Ministerio de Ciencia e Innovacion of the Spanish Government(Ministry of Science and Innovation, Spain (MICINN)Spanish Government)	This work has been partially supported by the Spanish Administration agency CDTI under project CENIT-VISION 2007-1007 and by the Ministerio de Ciencia e Innovacion of the Spanish Government under project TEC2007-67764 (SmartVision).	Agrawal M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P782; Bayro-Corrochano E, 2002, PATTERN RECOGN, V35, P169, DOI 10.1016/S0031-3203(00)00182-5; Harris J., 1995, ALGEBRAIC GEOMETRY 1; HARTLEY R, 1992, P 2 EUR C COMP VIS, P579; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hartley R. I., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P510, DOI 10.1109/ICCV.1999.791264; HEYDEN A, 1997, P IEEE C COMP VIS PA; Kahl F, 2000, J MATH IMAGING VIS, V13, P131, DOI 10.1023/A:1026524030731; Liebowitz D, 2003, INT J COMPUT VISION, V51, P171, DOI 10.1023/A:1021897717694; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; Pollefeys M, 1997, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.1997.609357; Ponce J, 2005, PROC CVPR IEEE, P780; PONCE J, 2001, 2 EUR WORKSH 3D STRU, P52; Pribanic T, 2009, IET COMPUT VIS, V3, P124, DOI 10.1049/iet-cvi.2009.0004; ROE ED, 1897, AM MATH MO, V4, P132; RONDA JI, 2005, INT C IM PROC GEN IT, V3, P800; Ronda JI, 2008, J MATH IMAGING VIS, V32, P193, DOI 10.1007/s10851-008-0095-0; Seo Y., 2000, P INT C PATT REC LOS, V1, P1067; Sturm P, 1997, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.1997.609467; Tresadern PA, 2008, IMAGE VISION COMPUT, V26, P851, DOI 10.1016/j.imavis.2007.10.001; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388; Tsai R. Y., 1992, RADIOMETRY, P221; Valdes A, 2006, INT J COMPUT VISION, V66, P283, DOI 10.1007/s11263-005-3677-y; Valdes A, 2005, J MATH IMAGING VIS, V23, P167, DOI 10.1007/s10851-005-6464-z; Wong KYK, 2003, IEEE T PATTERN ANAL, V25, P147, DOI 10.1109/TPAMI.2003.1177148; Zhang H, 2007, IEEE T PATTERN ANAL, V29, P499, DOI 10.1109/TPAMI.2007.45; Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	28	1	1	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2010	90	3					350	368		10.1007/s11263-010-0369-z	http://dx.doi.org/10.1007/s11263-010-0369-z			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	662BV		Green Accepted			2022-12-18	WOS:000282782700006
J	Damerval, C; Meignen, S				Damerval, C.; Meignen, S.			Study of a Robust Feature: The Pointwise Lipschitz Regularity	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Lipschitz regularity; Invariance properties; Multiscale contour point detector; Extraction of characteristic values; Robustness to geometric deformations and transformations applied to the image		The aim of this paper is to highlight the relevance in computer vision of the pointwise Lipschitz regularity alpha aa"e. The regularity alpha gives a measure of the local regularity of the intensity function associated to an image. Known wavelet methods provide an efficient computation of alpha at contour points of the image. From a theoretical point of view, we study the effect of geometric deformations and other specific transformations applied to the image, showing invariance properties. From a practical point of view, we assess the robustness of the regularity alpha when the image undergoes various transformations. The results we obtain show the Lipschitz regularity alpha is a suitable feature for applications in computer vision.	[Damerval, C.] Katholieke Univ Leuven, Dept Comp Sci, B-3001 Heverlee, Belgium; [Meignen, S.] Univ Grenoble, Lab Jean Kuntzmann, F-38400 St Martin Dheres, France	KU Leuven; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Inria	Damerval, C (corresponding author), Katholieke Univ Leuven, Dept Comp Sci, Celestijnenlaan 200A,Postbus 2402, B-3001 Heverlee, Belgium.	christophe.damerval@cs.kuleuven.be; sylvain.meignen@imag.fr		Meignen, sylvain/0000-0002-6713-0593				Arneodo A, 1997, J STAT PHYS, V87, P179, DOI 10.1007/BF02181485; Arneodo A, 1998, J FOURIER ANAL APPL, V4, P159, DOI 10.1007/BF02475987; ARNEODO A, 1999, CRM P LECT NOTES, V18, P315; BACRY E, 1993, J STAT PHYS, V70, P635, DOI 10.1007/BF01053588; Baumberg A, 2000, PROC CVPR IEEE, P774, DOI 10.1109/CVPR.2000.855899; Benassi A, 1998, STOCH PROC APPL, V75, P31, DOI 10.1016/S0304-4149(97)00123-3; Bigot R, 2003, RECENT ADVANCES AND TRENDS IN NONPARAMETRIC STATISTICS, P479; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; CANDES E, 2000, COMMUNICATIONS PURE, V57, P219; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Damerval C, 2007, IEEE SIGNAL PROC LET, V14, P39, DOI 10.1109/LSP.2006.879830; Deguy S., 2000, BRIT MACH VIS C; Florack L. M. J., 1993, Journal of Mathematical Imaging and Vision, V3, P327, DOI 10.1007/BF01664793; Iijima T., 1962, BULL ELECT LAB, V26, P368; Jaffard S., 1996, WAVELET METHODS POIN; KAPLAN LM, 1995, IEEE T PATTERN ANAL, V17, P1043, DOI 10.1109/34.473230; KAPLAN LM, 1995, IEEE ICASSP 95, V4, P575; KOENDERINK JJ, 1987, BIOL CYBERN, V55, P367, DOI 10.1007/BF00318371; Lee J, 2008, PATTERN RECOGN LETT, V29, P1934, DOI 10.1016/j.patrec.2008.06.006; Lehmann E. L., 1997, TESTING STAT HYPOTHE; Lindeberg T, 1997, IMAGE VISION COMPUT, V15, P415, DOI 10.1016/S0262-8856(97)01144-X; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Lindeberg T., 1994, SCALE SPACE THEORY C; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MALLAT S, 1992, IEEE T PATTERN ANAL, V14, P710, DOI 10.1109/34.142909; MALLAT S, 1992, IEEE T INFORM THEORY, V38, P617, DOI 10.1109/18.119727; Mallat S., 1999, WAVELET TOUR SIGNAL; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Mallat S, 2007, NUMER ALGORITHMS, V44, P205, DOI 10.1007/s11075-007-9092-4; Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Tuytelaars T, 2004, INT J COMPUT VISION, V59, P61, DOI 10.1023/B:VISI.0000020671.28016.e8; Witkin A., 1983, PROCESSING IJCAI; Zisserman A, 2004, ECCV, P404	37	1	2	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2010	88	3					363	381		10.1007/s11263-009-0310-5	http://dx.doi.org/10.1007/s11263-009-0310-5			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	580EL					2022-12-18	WOS:000276429900002
J	Triggs, B; Williams, CKI				Triggs, Bill; Williams, Christopher K. I.			Editorial: Special Issue on Probabilistic Models for Image Understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Triggs, Bill] Lab Jean Kuntzman, Grenoble, France; [Williams, Christopher K. I.] Univ Edinburgh, Edinburgh, Midlothian, Scotland	UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Universite Grenoble Alpes (UGA); University of Edinburgh	Triggs, B (corresponding author), Lab Jean Kuntzman, Grenoble, France.	Bill.Triggs@imag.fr; ckiw@inf.ed.ac.uk							0	1	1	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN 10	2010	88	2			SI		145	146		10.1007/s11263-010-0328-8	http://dx.doi.org/10.1007/s11263-010-0328-8			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	573YX		Bronze			2022-12-18	WOS:000275955400001
J	Lin, WY; Tan, GC; Cheong, LF				Lin, Wen-Yan; Tan, Geok-Choo; Cheong, Loong-Fah			When Discrete Meets Differential	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Perturbation analysis	OPTICAL-FLOW; MOTION; ALGORITHMS	We provide a theoretical proof showing that under a proportional noise model, the discrete eight point algorithm behaves similarly to the differential eight point algorithm when the motion is small. This implies that the discrete algorithm can handle arbitrarily small motion for a general scene, as long as the noise decreases proportionally with the amount of image motion and the proportionality constant is small enough. This stability result extends to all normalized variants of the eight point algorithm. Using simulations, we show that given arbitrarily small motions and proportional noise regime, the normalized eight point algorithms outperform their differential counterparts by a large margin. Using real data, we show that in practical small motion problems involving optical flow, these discrete structure from motion (SFM) algorithms also provide better estimates than their differential counterparts, even when the motion magnitudes reach sub-pixel level. The better performance of these normalized discrete variants means that there is much to recommend them as differential SFM algorithms that are linear and normalized.	[Lin, Wen-Yan; Cheong, Loong-Fah] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore; [Tan, Geok-Choo] Nanyang Technol Univ, Div Math Sci, Sch Phys & Math Sci, Singapore 637371, Singapore	National University of Singapore; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Lin, WY (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, 4 Engn Dr 3, Singapore 117576, Singapore.	linwenyan@yahoo.com; gctan@ntu.edu.sg; eleclf@nus.edu.sg			DSO [R263-000-400-592]	DSO	The support of the DSO grant R263-000-400-592 is gratefully acknowledged.	BAKER S, 2007, IEEE 11 INT C COMP V; Baumela L, 2000, INT C PATT RECOG, P840, DOI 10.1109/ICPR.2000.903675; Brooks MJ, 1997, J OPT SOC AM A, V14, P2670, DOI 10.1364/JOSAA.14.002670; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; Chiuso A, 2000, INT J COMPUT VISION, V39, P195, DOI 10.1023/A:1026563712076; Chojnacki W, 2003, IEEE T PATTERN ANAL, V25, P1172, DOI 10.1109/TPAMI.2003.1227992; DANIILIDIS K, 1997, VISUAL NAVIGATION BI, P61; FERMULLER C, 1995, INT J COMPUT VISION, V14, P147, DOI 10.1007/BF01418980; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; HO HT, 2008, IEEE C COMP VIS PATT; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; Kanatani K, 2003, J ELECTRON IMAGING, V12, P478, DOI 10.1117/1.1579018; KANATANI K, 1993, INT J COMPUT VISION, V11, P267, DOI 10.1007/BF01469345; Lempitsky V. S., 2008, IEEE C COMP VIS PATT; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; Ma Y, 2000, INT J COMPUT VISION, V36, P71, DOI 10.1023/A:1008124507881; Ma Y, 2001, INT J COMPUT VISION, V44, P219, DOI 10.1023/A:1012276232049; Ma Y., 2003, INVITATION 3 D VISIO; Mainberger M, 2008, LECT NOTES COMPUT SC, V5112, P630, DOI 10.1007/978-3-540-69812-8_62; Maybank S., 1992, THEORY RECONSTRUCTIO; MUHLICH M, 1998, EUR C COMP VIS; NEGAHDARIPOUR S, 1989, INT J COMPUT VISION, V3, P293, DOI 10.1007/BF00132601; Nir T, 2008, INT J COMPUT VISION, V76, P205, DOI 10.1007/s11263-007-0051-2; NISTER D, 2007, IEEE C COMP VIS PATT; Ohta N, 1996, IEICE T INF SYST, VE79D, P958; REN X, 2008, IEEE C COMP VIS PATT; SAND P, 2006, IEEE C COMP VIS PATT; Timoner SJ, 2001, OPT ENG, V40, P2003, DOI 10.1117/1.1391495; Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552; Triggs B., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P370, DOI 10.1109/ICCV.1999.791244; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; VIEVILLE T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P750, DOI 10.1109/ICCV.1995.466863; WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779; Wilkinson JH., 1965, ALGEBRAIC EIGENVALUE; Xiang T, 2003, INT J COMPUT VISION, V51, P111, DOI 10.1023/A:1021627622971	41	1	1	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	1					87	110		10.1007/s11263-009-0260-y	http://dx.doi.org/10.1007/s11263-009-0260-y			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	534MZ		Green Published			2022-12-18	WOS:000272903100005
J	Xu, DH; Wang, RS				Xu, Dahong; Wang, Runsheng			An Improved FoE Model for Image Deblurring	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image deblurring; Fields of experts; Prior model	RESTORATION	Image restoration from noisy and blurred image is one of the important tasks in image processing and computer vision systems. In this paper, an improved Fields of Experts model for deconvolution of isotropic Gaussian blur is developed, where edges are preserved in deconvolution by introducing local prior information. The edges with different local background in a blur image are retained since local prior information is adaptively estimated. Experiments indicate that the proposed approach is capable of producing highly accurate solutions and preserving more edge and object boundaries than many other algorithms.	[Xu, Dahong; Wang, Runsheng] Natl Univ Def Technol, ATR Lab, Changsha 410073, Peoples R China	National University of Defense Technology - China	Xu, DH (corresponding author), Natl Univ Def Technol, ATR Lab, Changsha 410073, Peoples R China.	dhxu_757@hotmail.com						Bar L, 2006, IEEE T IMAGE PROCESS, V15, P483, DOI 10.1109/TIP.2005.863120; Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258; Chan TF, 1999, SIAM J NUMER ANAL, V36, P354, DOI 10.1137/S0036142997327075; DEMOMENT G, 1989, IEEE T ACOUST SPEECH, V37, P2024, DOI 10.1109/29.45551; Goldfarb D, 2005, SIAM J SCI COMPUT, V27, P622, DOI 10.1137/040608982; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; HUANG J, 1999, P CVPR, P1541; Jalobeanu A, 2004, IEEE T IMAGE PROCESS, V13, P613, DOI 10.1109/TIP.2003.819969; Neelamani R, 1999, INT CONF ACOUST SPEE, P3241, DOI 10.1109/ICASSP.1999.757532; Roth S, 2005, PROC CVPR IEEE, P860; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Takeda H, 2008, IEEE T IMAGE PROCESS, V17, P550, DOI 10.1109/TIP.2007.918028; Vogel CR, 1996, SIAM J SCI COMPUT, V17, P227, DOI 10.1137/0917016; WANG Y., 2007, TR0710 CAAM	14	1	3	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2009	81	2					167	171		10.1007/s11263-008-0155-3	http://dx.doi.org/10.1007/s11263-008-0155-3			5	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	393VI					2022-12-18	WOS:000262401600005
J	Ng, J; Bharath, AA; Kin, PCP				Ng, Jeffrey; Bharath, Anil A.; Kin, Patrick Chow Pak			Extrapolative spatial models for detecting perceptual boundaries in colour images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						edge detection; boundary detection; extrapolative spatial model; phase-invariance; steerable filter	COMPLEX WAVELET CONSTRUCTION; EDGE-DETECTION	We present a novel approach for automatically building image-specific extrapolative spatial models of non-boundary local energy that can be used to perform local statistical tests to detect perceptual boundaries. The non-boundary model consists of statistics of local energy that are spatially extrapolated by a non-boundary confidence map and a scale-adaptive normalised filtering algorithm. We exploit the flexibility of steerable filters to both extract oriented local energy and to provide local statistics of the energy distribution in the orientation-domain to compute the non-boundary confidence map. Finally, we apply our local thresholding technique separately to the three channels of colour images and adopt a max operator to combine the results. We provide a qualitative and quantitative comparison on real images from a hand-segmented natural image database against the best combination of the most widely cited colour edge detectors and automatic global thresholding methods.	Univ London Imperial Coll Sci Technol & Med, Dept Bioengn, London SW7 2AZ, England; Singapore Polytech, Sch Elect & Elect Engn, Singapore, Singapore	Imperial College London; Singapore Polytechnic	Ng, J (corresponding author), Univ London Imperial Coll Sci Technol & Med, Dept Bioengn, London SW7 2AZ, England.	jeffrey.ng@imperial.ac.uk; a.bharath@imperial.ac.uk; pat.chow@sp.edu.sg						Bharath AA, 2005, IEEE T IMAGE PROCESS, V14, P948, DOI 10.1109/TIP.2005.849295; Bouganis CS, 2004, LECT NOTES COMPUT SC, V3203, P394; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CHOI SB, 2004, NEURAL INFORM PROCES, V2, P19; CIOS KJ, 1990, IEEE T BIO-MED ENG, V37, P520, DOI 10.1109/10.55643; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; GRANDLUND G, 1994, SIGNAL PROCESSING CO; Knutsson H., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P515, DOI 10.1109/CVPR.1993.341081; Knutsson H., 1983, 1983 IEEE Computer Society Workshop on Computer Architecture for Pattern Analysis and Image Database Management Proceedings, P206; Kovesi P., 2002, Proceedings of the Fifth Asian Conference on Computer Vision, P822; Kubota T, 2001, LECT NOTES COMPUT SC, V2134, P328; LAI G, 2000, P IEEE INT S CIRC SY, V5, P37; LITTENBERG B, 1993, MED DECIS MAKING, V13, P313, DOI 10.1177/0272989X9301300408; Luthon F, 2004, SIGNAL PROCESS, V84, P1789, DOI 10.1016/j.sigpro.2004.06.008; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Medina-Carnicer R, 2005, PATTERN RECOGN LETT, V26, P1423, DOI 10.1016/j.patrec.2004.11.024; MORRONE MC, 1988, PROC R SOC SER B-BIO, V235, P221, DOI 10.1098/rspb.1988.0073; Ng J, 2004, LECT NOTES COMPUT SC, V3021, P482; Oren M, 1997, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.1997.609319; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Papoulis A., 2002, PROBABILITY RANDOM V; Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003; Pellegrino FA, 2004, IEEE T SYST MAN CY B, V34, P1500, DOI 10.1109/TSMCB.2004.824147; Romberg JK, 2001, IEEE T IMAGE PROCESS, V10, P1056, DOI 10.1109/83.931100; Rosin PL, 2001, PATTERN RECOGN, V34, P2083, DOI 10.1016/S0031-3203(00)00136-9; SCARABOTTOLO N, 1993, EUR WORKSH PAR DISTR, P14; Scharcanski J, 1997, IEEE T CIRC SYST VID, V7, P397, DOI 10.1109/76.564116; Scharr H, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P840; Zhang Ai-hua, 2003, Mini-Micro Systems, V24, P661	29	1	1	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2007	73	2					179	194		10.1007/s11263-006-9782-8	http://dx.doi.org/10.1007/s11263-006-9782-8			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	146OI					2022-12-18	WOS:000244943500004
J	Kruger, N; Worgotter, F; van Hulle, MM				Kruger, Norbert; Worgotter, Florentin; van Hulle, Marc M.			Editorial: ECOVISION: Challenges in early-cognitive vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material												marc@neuro.kuleuven.be	Kruger, Norbert/P-6315-2015	Kruger, Norbert/0000-0002-3931-116X; Van Hulle, Marc/0000-0003-1060-7044				Aloimonos Y., 1989, INTEGRATION VISUAL M; BAYERL P, 2006, INT J COMPUTER VISIO; DUITS R, 2006, INT J COMPUTER VISIO; ELDER JH, 2006, INT J COMPUTER VISIO; ELLIS WD, 1938, GESTALT THEORY SOURC; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; GOODHILL G, 2005, NETWORK COMPUTATION; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HOMEL B, 2001, TRENDS COGN SCI, V8, P494; HORN BKP, 1994, ROBOT VISION; Kandell E.R., 2000, PRINCIPLES NEURAL SC; Kruger N, 2004, ADV IMAG ELECT PHYS, V131, P81, DOI 10.1016/S1076-5670(04)31003-7; Marr D., 1977, VISION COMPUTATIONAL; Mota S, 2004, INT J ROBOT AUTOM, V19, P190, DOI 10.2316/Journal.206.2004.4.206-2713; Norman JF, 2004, PSYCHOL SCI, V15, P565, DOI 10.1111/j.0956-7976.2004.00720.x; OGALE AS, 2006, INT J COMPUTER VISIO; PAUWELS K, 2006, INT J COMPUTER VISIO; SARKAR S, 1994, COMPUTING PERCEPTUAL; Worgotter Florentin, 2004, Natural Computing, V3, P293, DOI 10.1023/B:NACO.0000036817.38320.fe; YOUNG MP, 1992, NATURE, V358, P152, DOI 10.1038/358152a0; [No title captured]	22	1	1	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2007	72	1					5	7		10.1007/s11263-006-8889-2	http://dx.doi.org/10.1007/s11263-006-8889-2			3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	122QR					2022-12-18	WOS:000243242000001
J	Hammoud, RI				Hammoud, Riad Ibrahim			Object tracking and classification beyond the visible spectrum	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	1	1	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2007	71	2					123	124		10.1007/s11263-006-8657-3	http://dx.doi.org/10.1007/s11263-006-8657-3			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	098DN					2022-12-18	WOS:000241501300001
J	Niethammer, M; Vela, PA; Tannenbaum, A				Niethammer, M; Vela, PA; Tannenbaum, A			On the evolution of vector distance functions of closed curves	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						vector distance function; level set methods; dynamic active contours	LEVEL-SET APPROACH; MOTION; MANIFOLDS	Inspired by the work by Gomes et al., we describe and analyze a vector distance function approach for the implicit evolution of closed curves of codimension larger than one. The approach is set up in complete generality, and then applied to the evolution of dynamic geometric active contours in R 4 (codimension three case). In order to carry this out one needs an explicit expression for the zero level set for which we propose a discrete connectivity method. This leads us to make connections with the new theory of cubical homology. We provide some explicit simulation results in order to illustrate the methodology.	Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Niethammer, M (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, 777 Atlantic Dr NW, Atlanta, GA 30332 USA.	marcn@ece.gatech.edu; pvela@ece.gatech.edu; tannenba@ece.gatech.edu		Vela, Patricio/0000-0002-6888-7002	NATIONAL CENTER FOR RESEARCH RESOURCES [P41RR013218] Funding Source: NIH RePORTER; NCRR NIH HHS [P41 RR013218] Funding Source: Medline	NATIONAL CENTER FOR RESEARCH RESOURCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR)); NCRR NIH HHS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR))		ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; Ambrosio L, 1996, J DIFFER GEOM, V43, P693; Bellettini G, 1998, COMMUN PART DIFF EQ, V23, P1475, DOI 10.1080/03605309808821392; Bertalmio M, 1999, IEEE T MED IMAGING, V18, P448, DOI 10.1109/42.774172; Bertrand G, 1996, PATTERN RECOGN LETT, V17, P115, DOI 10.1016/0167-8655(95)00100-X; Burchard P, 2001, J COMPUT PHYS, V170, P720, DOI 10.1006/jcph.2001.6758; Dierckx P., 1993, CURVE SURFACE FITTIN; do Carmo M.P., 1976, DIFFERENTIAL GEOMETR; Dupont TF, 2003, J COMPUT PHYS, V190, P311, DOI 10.1016/S0021-9991(03)00276-6; DUPONT TF, 2004, CDSNS2004399 GEORG I; EVANS LC, 1992, T AM MATH SOC, V330, P321, DOI 10.2307/2154167; EVANS LC, 1995, J GEOM ANAL, V5, P77, DOI 10.1007/BF02926443; EVANS LC, 1991, J DIFFER GEOM, V33, P635, DOI 10.4310/jdg/1214446559; EVANS LC, 1992, J GEOM ANAL, V2, P121, DOI [10.1007/BF02921385, DOI 10.1007/BF02921385]; GAU CJ, 2002, LNCS, V2301, P81; Gomes J, 2001, LECT NOTES COMPUT SC, V2106, P1; GOMES J, 2000, 4012 INRIA; GOMES J, 2000, 4011 INRIA; GRAYSON MA, 1987, J DIFFER GEOM, V26, P285; Kaczynski T, 2004, APPL MATH SCI, V157; KACZYNSKI T, 2001, COMPUTING HOMOLOGY; KALIES WD, 1999, CONLEY INDEX THEORY, P115; Klette G, 2003, LECT NOTES COMPUT SC, V2756, P57; KONG TY, 1997, LECT NOTES COMPUTER, V3, P1347; LEVEQUE R. J, 2002, CAMBRIDGE TEXTS APPL, V31, DOI [10.1017/CBO9780511791253, DOI 10.1017/CBO9780511791253]; Lorigo LM, 1999, LECT NOTES COMPUT SC, V1613, P126; Massey W. S., 1991, GRADUATE TEXTS MATH, V127; Min CH, 2004, J COMPUT PHYS, V200, P368, DOI 10.1016/j.jcp.2004.04.019; NIETHAMMER M, 2004, UNPUB DETECTING SIMP; NIETHAMMER M, 2004, UNPUB IEEE T AUTOMAT; Osher S, 2001, J COMPUT PHYS, V169, P463, DOI 10.1006/jcph.2000.6636; Osher S, 2002, J COMPUT PHYS, V179, P622, DOI 10.1006/jcph.2002.7080; Osher S, 2003, LEVEL SET METHODS DY; Pottmann H, 2003, VISUALIZATION AND MATHEMATICS III, P221; Sethian J. A., 1999, LEVEL SET METHODS FA; Slepcev D, 2003, INTERFACE FREE BOUND, V5, P417; SUSSMAN M, 1994, J COMPUT PHYS, V114, P146, DOI 10.1006/jcph.1994.1155; Thurmer G, 2003, GRAPH MODELS, V65, P43, DOI 10.1016/S1524-0703(03)00007-9	38	1	1	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2005	65	1-2					5	27		10.1007/s11263-005-3849-9	http://dx.doi.org/10.1007/s11263-005-3849-9			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	994NU	23700357	Green Accepted			2022-12-18	WOS:000234039300001
J	Keshet, R; Heijmans, HJAM				Keshet, R; Heijmans, HJAM			Adjunctions in pyramids, curve evolution and scale-spaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	3rd International Conference on Scale-Space and Morphology held in conjunction with the 8th International Conference on Computer Vision	JUL 07-08, 2001	VANCOUVER, CANADA	IEEE Tech Comm Pattern Anal & Machine Intelligence				We have been witnessing lately a convergence among mathematical morphology and other nonlinear fields, such as curve evolution, PDE-based geometrical image processing, and scale-spaces. An obvious benefit of such a convergence is a cross-fertilization of concepts and techniques among these fields. The concept of adjunction however, so fundamental in mathematical morphology, is not yet shared by other disciplines. The aim of this paper is to show that other areas in image processing can possibly benefit from the use of adjunctions. In particular, a strong relationship between pyramids and adjunctions is presented. We show how this relationship may help in analyzing existing pyramids, and construct new pyramids. Moreover, it will be explained that adjunctions based on a curve evolution scheme can provide idempotent shape filters. This idea is illustrated in this paper by means of a simple affine-invariant polygonal flow. Finally, the use of adjunctions in scale-space theory is also addressed.	Hewlett Packard Labs, IL-32000 Haifa, Israel; Ctr Math & Comp Sci, CWI, NL-1098 SJ Amsterdam, Netherlands	Hewlett-Packard	Keshet, R (corresponding author), Hewlett Packard Labs, IL-32000 Haifa, Israel.	renato@hpli.hpl.hp.com; Henk.Heijmans@cwi.nl						Bruckstein AM, 1995, INT J PATTERN RECOGN, V9, P991, DOI 10.1142/S0218001495000407; Goutsias J, 2000, IEEE T IMAGE PROCESS, V9, P1862, DOI 10.1109/83.877209; HEIJMANS HJA, 2000, IN PRESS JVCIR; HEIJMANS HJAM, 1990, COMPUT VISION GRAPH, V50, P245, DOI 10.1016/0734-189X(90)90148-O; Heijmans HJAM, 2002, J VIS COMMUN IMAGE R, V13, P269, DOI 10.1006/jvci.2001.0480; KESHET R, 2000, MATH MORPHOLOGY ITS, V5; KIMMEL R, 1995, THESIS TECHNION ISRA; Kresch R, 1998, COMP IMAG VIS, V12, P35; MALLADI R, 1995, P NATL ACAD SCI USA, V92, P7046, DOI 10.1073/pnas.92.15.7046; Moisan L, 1998, IEEE T IMAGE PROCESS, V7, P411, DOI 10.1109/83.661191; PPAUWELS EJ, 1995, IEEE T PAMI, V17; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; SAPIRO G, 1995, P ICIP 95, V3, P1; Sapiro G., 2001, GEOMETRIC PARTIAL DI; Serra J, 1988, IMAGE ANAL MATH MORP; Steiner A., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P523, DOI 10.1109/ICPR.1996.546081	16	1	1	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY-JUN	2003	52	2-3					139	151						13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	659EL					2022-12-18	WOS:000181764500005
J	Paragios, N				Paragios, N			Guest editorial: Special issue on variational and level set methods in computer vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Siemens Corp Res, Princeton, NJ 08540 USA	Siemens AG	Paragios, N (corresponding author), Siemens Corp Res, Princeton, NJ 08540 USA.								0	1	1	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2002	50	3					235	235		10.1023/A:1020893223097	http://dx.doi.org/10.1023/A:1020893223097			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	607XU					2022-12-18	WOS:000178816700001
J	Yao, JC				Yao, JC			Estimation of 2D displacement field based on affine geometric invariance and scene constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						2D displacement field; affine geometric invariance; robust least squares; point transfer; second order cone programming	OPTICAL-FLOW	In the paper, a novel approach using affine transfer and scene constraint for estimation of 2D displacement field was developed. In this approach, we derived a system of 5 linear equations for computing corresponding point of any image point via the utilisation of affine invariant. Subsequently, the characteristics of such a linear system was thoroughly studied, and the way to obtain a reliable solution of the system via robust least squares (RLS) approach, in conjunction with total least squares technique, was proposed. In addition, through the interpretation of the approach from both geometric point of view and numerical point of view, we gave the limitation of the algorithm. The limitation was then relaxed to a certain extent by using full fundamental matrix. These findings were further verified through the experimental results.	DSO Natl Labs, Signal Proc Lab, Singapore 118230, Singapore		Yao, JC (corresponding author), DSO Natl Labs, Signal Proc Lab, Singapore 118230, Singapore.	yjiancha@dso.org.sg						ANANDAN NP, 1989, INT J COMPUT VISION, V2, P282; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BERGEN JR, 1992, IEEE PAMI, V14; BOYD S, 1997, INTRO CONVEX OPTIMIS; CHIN TM, 1994, IEEE T IMAGE PROCESS, V3, P773, DOI 10.1109/83.336247; DERICHE R, 1994, ECCV94, P567; FAUGERAS O, 1995, J OPT SOC AM A, V12, P465, DOI 10.1364/JOSAA.12.000465; FUH CS, 1991, OPTICAL ENG, V30, P88; GHAOUI LE, 1997, SIAM J MATRIX ANAL A, V18, P1035, DOI DOI 10.1137/S0895479896298130; Hildreth E., 1984, MEASUREMENT VISUAL M; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; LUCAS BD, 1981, P 7 INT C ART INT; LUONG QT, 1994, ECCV 94, P577; Mundy J., 1992, GEOMETRIC INVARIANCE; NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833; Nesterov Y., 1994, STUDIES APPL MATH, V13; NOBEL AJ, 1988, IMAGE VISION COMPUT, V6, P121; Reid ID, 1996, INT J COMPUT VISION, V18, P41, DOI 10.1007/BF00126139; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; SHAPIRO LS, 1994, ECCV 94, P73; SINGH A, 1991, OPTICAL FLOW COMPUTA; TOMASI C, 1991, CMUCS91105; TOMASI C, 1991, CMUCS91172; Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552; Tsui HT, 1997, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.1997.609327; Xu A., 2018, KINETIC THEORY, DOI [10.1007/978-94-015-8668-9, DOI 10.1007/978]; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561	28	1	1	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		2002	46	1					25	50		10.1023/A:1013296015138	http://dx.doi.org/10.1023/A:1013296015138			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	503NT					2022-12-18	WOS:000172805100002
J	Vieville, T; Lingrand, D; Gaspard, F				Vieville, T; Lingrand, D; Gaspard, F			Implementing a multi-model estimation method	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						non-linear estimation; robust estimation; multi-model	MOTION; CALIBRATION	We revisit the problem of parameter estimation in computer vision, reconsidering and implementing what may be called the Kanatani's estimation method, presented here as a simple optimisation problem, so (a) without any direct reference to a probabilistic framework but (b) considering (i) non-linear implicit measurement equations and parameter constraints, plus (ii) robust estimation in the presence of outliers and (iii) multi-model comparisons. Here, (A) a projection algorithm based on generalisations of square-root decompositions allows an efficient and numerically stable local resolution of a set of non-linear equations. On the other hand, (B) a robust estimation module of a hierarchy of non-linear models has been designed and validated. A step ahead, (C) the software architecture of the estimation module is discussed with the goal of being integrated in reactive software environments or within applications with time constraints, while an experimentation considering the parameterisation of retinal displacements between two views is proposed as an illustration of the estimation module.	INRIA, F-06902 Valbonne, France	Inria	Vieville, T (corresponding author), INRIA, BP93, F-06902 Valbonne, France.	thierry.vieville@inria.fr						Akaike H., 1977, J R STAT SOC C-APPL, P27; ARIAS S, 1999, THESIS U NICE; Bard Y., 1974, NONLINEAR PARAMETER; Biernacki C., 1998, ASSESSING MIXTURE MO; BOLLES RC, 1981, INT JOINT C ART INT, P637; CHAUMETTE F, 1989, AFCET, P527; CHOJNACKI W, 1999, STAT METHODS IMAGE P, P61; DANVY O, 1996, LECT NOTES COMPUTER, V1110; Draper N.R., 1981, APPL REGRESSION ANAL, Vsecond, DOI DOI 10.1002/9781118625590; ENCISO R, 1995, LECT NOTES COMPUTER, V974, P307; GASPARD F, 2000, 6 INT C INF SYST AN, V8, P366; GASPARD F, 1996, 3002 RR INRIA; GILL PE, 1993, PRACTICAL OPTIMIZATI; GOELB A, 1974, APPL OPTIMAL ESTIMAT; GRIMM J, 1996, 2794 RR INRIA; HARTLEY R, 1992, P 2 EUR C COMP VIS, P579; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Holland JH., 1975, ADAPT NATUR ARTIFIC; Huber P., 1981, ROBUST STAT; Kanatani K, 1998, PHILOS T R SOC A, V356, P1303, DOI 10.1098/rsta.1998.0223; KANATANI K, 1992, GEOMETRIC COMPUTATIO; KANATANI K, 1996, P 4 EUR C COMP VIS C, P697; Kanatani K., 1996, STAT OPTIMIZATION GE; LEE RC, 1964, OPTIMAL ESTIMATION I; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; LEEDAN Y, 1997, THESIS RUTGERS U; LINGRAND D, 1999, THESIS U NICE SOPHIA; LUONG QT, 1993, 1894 INRIA; MATEI B, 2000, COMP VIS PATT REC P; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; MEER P, 1991, INT J COMPUT VISION, V6, P59, DOI 10.1007/BF00127126; POWELL MJD, 1978, NONLINEAR PROGRAMMIN; Press WH, 1988, NUMERICAL RECIPES C; Rey W.J.J., 1983, INTRO ROBUST QUASIRO; Rousseeuw P. J., 1987, ROBUST REGRESSION OU; SCHWARZ H, 1989, NUMERICAL ANAL; Stewart CV, 1997, IEEE T PATTERN ANAL, V19, P818, DOI 10.1109/34.608280; Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552; Torr PHS, 1998, PHILOS T R SOC A, V356, P1321, DOI 10.1098/rsta.1998.0224; TRIGGS B, 1998, LECT NOTES COMP SCI; Vieville T, 1996, COMPUT VIS IMAGE UND, V64, P128, DOI 10.1006/cviu.1996.0049; Vieville T, 1999, INT J COMPUT VISION, V31, P5, DOI 10.1023/A:1008082308694; Vieville T, 1996, INT J COMPUT VISION, V17, P7, DOI 10.1007/BF00127817; Vieville T, 1996, INT J COMPUT VISION, V20, P213, DOI 10.1007/BF00208720; VIEVILLE T, 1992, RR1669 INRIA; VIEVILLE T, 2000, 4050 RR INRIA; WEI GQ, 1994, IEEE T PATTERN ANAL, V16, P469, DOI 10.1109/34.291450; ZHANG Z, 1994, AI J, V78, P87; ZHANG Z, 1994, 2273 INRIA; Zhang ZY, 1997, IMAGE VISION COMPUT, V15, P59, DOI 10.1016/S0262-8856(96)01112-2	50	1	1	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2001	44	1					41	64		10.1023/A:1011120419133	http://dx.doi.org/10.1023/A:1011120419133			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	464WT					2022-12-18	WOS:000170556900002
J	Okatani, T; Deguchi, K				Okatani, T; Deguchi, K			Closed form solution of local shape from shading at critical points	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shape from shading; critical point; singular point; characteristic equation; uniqueness and existence of solution; local shading analysis	VISCOSITY SOLUTIONS; UNIQUENESS	In the theory of shape from shading, behaviours of the local solution around a critical point of the image play an important role. This paper shows that the second derivatives of the object surface can be locally determined at these image critical points. Closed form expressions of the surface second derivatives in terms of the second derivatives of the image brightness and of the reflectance map are shown. They are derived as follows: By differentiating the image irradiance equation twice at an image critical point, a set of polynomial equations is obtained that contains the second derivatives of the surface, of the image brightness and of the reflectance map. Regarding these equations as simultaneous equations for unknown surface second derivatives, they are algebraically solved and their explicit expressions are derived. Such a derivation is possible only at image critical points and is impossible at any other image point. The applicability of the derived expressions to noisy images is tested using synthetic images.	Tohoku Univ, Grad Sch Informat Sci, Sendai, Miyagi 9808579, Japan	Tohoku University	Okatani, T (corresponding author), Tohoku Univ, Grad Sch Informat Sci, Sendai, Miyagi 9808579, Japan.		Okatani, Takayuki/AAE-3339-2019					BISCHEL M, 1992, P IEEE COMP VIS PATT, P459; BROOKS MJ, 1994, INT C PATT RECOG, P114, DOI 10.1109/ICPR.1994.576240; BRUCKSTEIN AM, 1988, COMPUT VISION GRAPH, V44, P139, DOI 10.1016/S0734-189X(88)80002-1; BRUSS AR, 1982, J MATH PHYS, V23, P890, DOI 10.1063/1.525441; Horn B.K.P., 1989, SHAPE SHADING; HORN BKP, 1990, INT J COMPUT VISION, V5, P37, DOI 10.1007/BF00056771; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P47, DOI 10.1006/cviu.1995.1040; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P360, DOI 10.1006/cviu.1995.1060; KIMMEL R, 1995, INT J COMPUT VISION, V16, P107, DOI 10.1007/BF01539551; LEE CH, 1985, ARTIF INTELL, V26, P125, DOI 10.1016/0004-3702(85)90026-8; OLIENSIS J, 1991, INT J COMPUT VISION, V6, P75, DOI 10.1007/BF00128151; OLIENSIS J, 1991, CVGIP-IMAG UNDERSTAN, V54, P163, DOI 10.1016/1049-9660(91)90061-S; OLIENSIS J, 1993, P INT C COMP VIS, P692; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P170, DOI 10.1109/TPAMI.1984.4767501; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; SARA R, 1994, THESIS J KEPLER U LI; SAXBERG BVH, 1992, INT J ROBOT RES, V11, P202, DOI 10.1177/027836499201100304; Wei GQ, 1997, IEEE T PATTERN ANAL, V19, P353, DOI 10.1109/34.588016; Winston P. H., 1975, PSYCHOL COMPUTER VIS; ZHANG R, 1994, P COMP VIS PATT REC, P377	20	1	2	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2000	40	2					169	178		10.1023/A:1026510403689	http://dx.doi.org/10.1023/A:1026510403689			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	388TA					2022-12-18	WOS:000166197700004
J	Sigelle, M				Sigelle, M			A cumulant expansion technique for simultaneous Markov random Field image restoration and hyperparameter estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Markov random fields; image restoration; boundary process; hyperparameter estimation; stochastic gradient; cumulants	CARLO MAXIMUM-LIKELIHOOD; PARAMETER-ESTIMATION; ALGORITHMS; SYSTEMS	We investigate hyperparameter estimation for incomplete data in Markov random Field image restoration. Assuming linear dependence of energies with respect to hyperparameters framework, we use a cumulant expansion technique widely known in Statistical Physics and Signal Processing. New insight is given on Maximum Likelihood estimation of hyperparameters of the prior, regularization and contour probability distribution functions (pdfs) for an explicit joint boundary-pixel process aimed to preserve discontinuities. In particular the case where the prior regularization potential is an homogeneous function of pixels is fully analyzed. A Generalized Stochastic Gradient (GSG) algorithm with a fast sampling technique is devised aiming to achieve simultaneous hyperparameter estimation and pixel restoration. Image restoration performances of Posterior Mean performed during GSG convergence and of Simulated Annealing performed after GSG convergence are compared experimentally. Results and perspectives are given.			Sigelle, M (corresponding author), Ecole Natl Super Telecommun Bretagne, Dept Traitement Signal & Images, 46 Rue Barrault, F-75634 Paris 13, France.	sigelle@tsi.enst.fr						BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; Bournan C, 1993, IEEE T IMAGE PROCESS, V2, P296, DOI 10.1109/83.236536; CARDOSO JF, 1995, IEEE T SIGNAL PROCES, V43, P214, DOI 10.1109/78.365301; CHARBONNIER P, 1996, IEEE T IMAGE PROCESS, V5, P1; Descombes X, 1999, IEEE T IMAGE PROCESS, V8, P954, DOI 10.1109/83.772239; GEMAN D, 1992, IEEE T PATTERN ANAL, V14, P367, DOI 10.1109/34.120331; GEMAN D, 1990, IEEE T PATTERN ANAL, V12, P609, DOI 10.1109/34.56204; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GEMAN S, 1985, P STAT COMP SECT AM, P13; GEYER CJ, 1992, J R STAT SOC B, V54, P657; Gimelfarb GL, 1996, IEEE T PATTERN ANAL, V18, P1110, DOI 10.1109/34.544081; GIMELFARB GL, 1997, 3202 INRIA; Huang K., 1987, STAT MECH; KHOUMRI M, 1998, P ICIP CHIC US; LAKSHMANAN S, 1989, IEEE T PATTERN ANAL, V11, P799, DOI 10.1109/34.31443; Ma S. K., 1985, STAT MECH; Malyshev V., 1991, GIBBS RANDOM FIELDS; MASSEY PD, 1993, J COMPUT INFORM SYST, V34, P3; METIVIER M, 1987, PROBAB THEORY REL, V74, P403, DOI 10.1007/BF00699098; Nikias C. L., 1993, HIGHER ORDER SPECTRA; NIKOLOVA M, 1999, IN PRESS SIAM J APPL; QUELLE HC, 1996, MACHINE GRAPHICS VIS, V5, P613; Robert C. P., 1999, MONTE CARLO STAT MET; Saquib SS, 1998, IEEE T IMAGE PROCESS, V7, P1029, DOI 10.1109/83.701163; YOUNES L, 1988, ANN I H POINCARE-PR, V24, P269; YOUNES L, 1989, PROBAB THEORY REL, V82, P625, DOI 10.1007/BF00341287; YOUNES L, 1991, SPRINGER LECT NOTES; YOUNES L, 1995, CONVERGENCE MARKOVIA; ZHOU Z, 1997, IEEE T PATTERN ANAL, V6, P799; Zhu SC, 1997, IEEE T PATTERN ANAL, V19, P1236, DOI 10.1109/34.632983	30	1	2	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	37	3					275	293		10.1023/A:1008136208859	http://dx.doi.org/10.1023/A:1008136208859			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	342GF					2022-12-18	WOS:000088636300004
J	Cox, IJ				Cox, IJ			Introduction: Computer vision research at NECI	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material							STOCHASTIC COMPLETION FIELDS; NEURAL SPIKE TRAINS; RECOGNITION; MOTION; IMAGES; SHAPE; WATERMARKING; INFORMATION; TRACKING; MODEL	This special issue of the International Journal of Computer Vision highlights research on computer vision at the NEC Research Institute. The purpose of this preface is to provide some context to how this work reflects our broader goals and themes of computer vision research at NECI.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Cox, IJ (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.							Alter TD, 1998, INT J COMPUT VISION, V27, P127, DOI 10.1023/A:1007989016491; Barlow H. B., 1994, LARGE SCALE NEURONAL, P1; Barlow Horace, 1995, P415; Basri R, 1998, PROC CVPR IEEE, P414, DOI 10.1109/CVPR.1998.698639; Basri R, 1998, VISION RES, V38, P2365, DOI 10.1016/S0042-6989(98)00043-1; Basri R, 1997, INT J COMPUT VISION, V25, P145, DOI 10.1023/A:1007919917506; BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199; Bloom JA, 1999, P IEEE, V87, P1267, DOI 10.1109/5.771077; BULTHOFF HH, 1999, INVEST OPHTHALMOL, V40, P398; Cox I. J., 1992, BMVC92. Proceedings of the British Machine Vision Conference, P337; COX IJ, 1993, INT J COMPUT VISION, V11, P5, DOI 10.1007/BF01420590; Cox IJ, 1998, IEEE J SEL AREA COMM, V16, P587, DOI 10.1109/49.668980; Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120; Cox IJ, 1996, IEEE T PATTERN ANAL, V18, P138, DOI 10.1109/34.481539; Cox IJ, 1997, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P76; Cox IJ, 1997, IEEE T AERO ELEC SYS, V33, P295, DOI 10.1109/7.570789; Cox IJ, 1997, P SOC PHOTO-OPT INS, V3016, P92, DOI 10.1117/12.274502; Cox IJ, 1996, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.1996.517076; COX IJ, 1995, IEEE T AERO ELEC SYS, V31, P486, DOI 10.1109/7.366332; Cox IJ, 1996, PROCEEDINGS OF THE THIRD FORUM ON RESEARCH AND TECHNOLOGY ADVANCES IN DIGITAL LIBRARIES (ADL '96), P66, DOI 10.1109/ADL.1996.502517; COX IJ, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P733, DOI 10.1109/CVPR.1994.323889; Cox IJ, 1999, P IEEE, V87, P1127, DOI 10.1109/5.771068; COX IJ, 1994, COMPUTER VISION IMAG, V63, P733; COX IJ, 1996, P INT C PATT REC, V2, P557; COX IJ, 1996, 13 INT C PATT REC, V3, P361; COX IJ, 1998, IEEE INT C COMP VIS; Dupuis P, 1994, ANN APPL PROBAB, V4, P287, DOI 10.1214/aoap/1177005063; ELDER J, 1996, LECT NOTES COMPUTER, P399; Gear CW, 1998, INT J COMPUT VISION, V29, P133, DOI 10.1023/A:1008026310903; Jacobs D, 1997, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.1997.609321; JACOBS D, 1994, IEEE WORKSH MOT NONR, P96; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; Jacobs DW, 1998, PROC CVPR IEEE, P610, DOI 10.1109/CVPR.1998.698668; Jacobs DW, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P596, DOI 10.1109/ICCV.1998.710778; Langer MS, 1998, PERCEPTION, V27, P47; Langer MS, 1997, PROC CVPR IEEE, P172, DOI 10.1109/CVPR.1997.609316; LANGER MS, 1997, INVESTIGATIVE OPHTHA, V38; LAWRENCE S, 1997, SMC 97 C P, V3, P2016; LIU Z, IN PRESS VISION RES; Liu ZL, 1998, VISION RES, V38, P2507, DOI 10.1016/S0042-6989(98)00063-7; Liu ZL, 1999, VISION RES, V39, P603, DOI 10.1016/S0042-6989(98)00167-9; Liu ZL, 1998, THEORETICAL ASPECTS OF NEURAL COMPUTATION, P145; Liu ZL, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P549, DOI 10.1109/ICCV.1998.710770; Liu ZL, 1996, SPATIAL VISION, V9, P491, DOI 10.1163/156856896X00222; Liu ZL, 1998, COGNITIVE BRAIN RES, V6, P347, DOI 10.1016/S0926-6410(98)00008-1; Miller ML, 1997, IEEE T AERO ELEC SYS, V33, P851, DOI 10.1109/7.599256; OLIENSIS J, 1991, INT J COMPUT VISION, V6, P75, DOI 10.1007/BF00128151; OLIENSIS J, 1991, CVGIP-IMAG UNDERSTAN, V54, P163, DOI 10.1016/1049-9660(91)90061-S; OLIENSIS J, 1992, PHYSICS BASED VISION, P17; PAPATHOMAS TV, 1998, IS T SPIE S EL IM SC; POTTERS M, 1994, J PHYS I, V4, P1755, DOI 10.1051/jp1:1994219; Rieke F., 1997, SPIKES EXPLORING NEU; Rinberg D, 1999, ADV NEUR IN, V11, P146; Roy S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P492, DOI 10.1109/ICCV.1998.710763; ROY S, 1996, P IEEE INT C PATT RE, V1, P728; RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814; Sakamoto S, 1997, PATTERN RECOGN LETT, V18, P923, DOI 10.1016/S0167-8655(97)00066-4; Stewart AJ, 1997, IEEE T PATTERN ANAL, V19, P1020, DOI 10.1109/34.615450; Stone HS, 1999, IEEE T SIGNAL PROCES, V47, P97, DOI 10.1109/78.738243; STONE HS, 1997, INT J DIGITAL LIB, V1, P329; STONE HS, 1999, IN PRESS IEEE T PATT; STONE HS, 1998, RECENT ADV MULTIMEDI, P282; Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197; Thornber KK, 1996, BIOL CYBERN, V75, P141, DOI 10.1007/s004220050282; THORNBER KK, 1998, NEURAL INFORMATION P, V11; TJAN BS, 1999, INVESTIGATIVE OPHT S, V40, P414; TJAN BS, 1998, INVESTIGATIVE OPHT S, V39, P170; vanSteveninck RRD, 1996, NATURE, V379, P642, DOI 10.1038/379642a0; vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805; Williams LR, 1997, NEURAL COMPUT, V9, P859, DOI 10.1162/neco.1997.9.4.859; Williams LR, 1997, NEURAL COMPUT, V9, P837, DOI 10.1162/neco.1997.9.4.837	71	1	1	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1999	34	2-3					75	79		10.1023/A:1008176402209	http://dx.doi.org/10.1023/A:1008176402209			5	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	265NF					2022-12-18	WOS:000084249700001
J	Grimson, WEL				Grimson, WEL			Object recognition research at MIT - Introduction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material																			0	1	1	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1997	21	1-2					5	8		10.1023/A:1007938804963	http://dx.doi.org/10.1023/A:1007938804963			4	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WM797					2022-12-18	WOS:A1997WM79700001
J	Nevatia, R; Medioni, G				Nevatia, R; Medioni, G			Computer vision research at the University of Southern California	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							IMAGES				Nevatia, R (corresponding author), UNIV SO CALIF, INST ROBOT & INTELLIGENT SYST, DEPT COMP SCI, LOS ANGELES, CA 90089 USA.							BEJANIN M, 1994, P 1994 ARPA IM UND W, V1, P287; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BINFORD TO, 1981, ARTIF INTELL, V17, P205, DOI 10.1016/0004-3702(81)90025-4; BINFORD TO, 1971, P IEEE C SYST CONTR; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; CHEN YY, 1994, INT CONF WAFER SCALE, P153, DOI 10.1109/ICWSI.1994.291256; CHUNG R, 1993, P IEEE WORKSH APPL C, P64; CHUNG Y, 1995, IN PRESS INT C COMP; FRANZEN W, 1991, P WORKSH VIS MOT, P14; FRANZEN W, 1992, P DARPA IM UND WORKS, P487; GUY G, 1994, P ARPA IM UND WORKSH; KIM D, 1994, P IEEE WORKSH APPL C, P280; KIM D, 1993, P INT C INT AUT SYST, P268; KIM Y, 1993, P DARPA IM UND WORKS, P611; Kim Y. C., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P836, DOI 10.1109/CVPR.1992.223245; LIAO C, 1995, P COMP VIS PATT REC, P617; LIN C, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P62, DOI 10.1109/CVPR.1994.323811; Marr D., 1981, VISION; NEVATIA R, 1977, ARTIF INTELL, V8, P77, DOI 10.1016/0004-3702(77)90006-6; NEVATIA R, 1994, INT C PATT RECOG, P290, DOI 10.1109/ICPR.1994.577180; Reinhart C. C., 1992, Proceedings. 11th IAPR International Conference on Pattern Recognition. Vol. IV. Conference D: Architectures for Vision and Pattern Recognition, P225, DOI 10.1109/ICPR.1992.202172; WANG CL, 1995, IN PRESS INT COMP AR; Zerroug M., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P96, DOI 10.1109/CVPR.1993.340973; Zerroug M., 1995, Proceedings International Symposium on Computer Vision (Cat. No.95TB100006), P431, DOI 10.1109/ISCV.1995.477040; ZERROUG M, 1994, INT C PATT RECOG, P108	25	1	1	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1996	20	1-2					5	9		10.1007/BF00144114	http://dx.doi.org/10.1007/BF00144114			5	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VQ240					2022-12-18	WOS:A1996VQ24000001
J	Faugeras, O; Kanade, T				Faugeras, O; Kanade, T			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									CARNEGIE MELLON UNIV,DEPT COMP SCI,PITTSBURGH,PA 15213	Carnegie Mellon University	Faugeras, O (corresponding author), INST NATL RECH INFORMAT & AUTOMAT,UNITE RECH SOPHIAS ANTIPOLIS,2004 ROUTE LUCIOLES,F-06565 VALBONNE,FRANCE.								0	1	1	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1996	17	1					5	5						1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TX328					2022-12-18	WOS:A1996TX32800001
J	ROSENFELD, A				ROSENFELD, A			SPECIAL ISSUE - IMAGE UNDERSTANDING RESEARCH AT THE UNIVERSITY-OF-MARYLAND - INTRODUCTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material											ROSENFELD, A (corresponding author), UNIV MARYLAND,CTR AUTOMAT RES,BLDG 094,COLLEGE PK,MD 20742, USA.								0	1	1	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1995	15	1-2					5	5		10.1007/BF01450847	http://dx.doi.org/10.1007/BF01450847			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QY042					2022-12-18	WOS:A1995QY04200001
J	DEVRIENDT, J				DEVRIENDT, J			FAST COMPUTATION OF UNBIASED INTENSITY DERIVATIVES IN IMAGES USING SEPARABLE FILTERS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							2ND DIRECTIONAL-DERIVATIVES; DIGITAL STEP EDGES; ZERO CROSSINGS; ALGORITHMS	In this paper we prove that all high order non-biased spatial intensity derivative operators in images can be computed using linear combinations of separable filters. The separable filters are the same as those used by Haralick (1984), but different linear combinations are taken. A comparison of the number of operations necessary to compute the derivatives using separable and non-separable filters is made. The conclusion of our analysis is that the optimal way to compute the needed derivatives depends on which derivatives we have to compute, on the size of the window and on the order of expansion. Finally, we discuss the performance of an edge detector using these derivatives for unsmoothed and smoothed step edges.			DEVRIENDT, J (corresponding author), STATE UNIV GHENT,COMMUN ENGN LAB,SINT PIETERSNIEUWSTR 41,B-9000 GHENT,BELGIUM.							BECKMANN P, 1973, ORTHOGONAL POLYNOMIA; BERGHOLM F, 1993, 8TH P SCAND C IM AN, P1093; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CLARK JJ, 1989, IEEE T PATTERN ANAL, V11, P43, DOI 10.1109/34.23112; DERICHE R, 1990, IEEE T PATTERN ANAL, V12, P78, DOI 10.1109/34.41386; DEVRIENDT J, 1993, MULTIDIM SYST SIGN P, V4, P227, DOI 10.1007/BF00985890; GRIMSON WEL, 1985, IEEE T PATTERN ANAL, V7, P121, DOI 10.1109/TPAMI.1985.4767628; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; HARALICK RM, 1985, IEEE T PATTERN ANAL, V7, P127, DOI 10.1109/TPAMI.1985.4767629; SARKAR S, 1991, IEEE T PATTERN ANAL, V13, P1154, DOI 10.1109/34.103275; SARKAR S, 1991, CVGIP-IMAG UNDERSTAN, V54, P224, DOI 10.1016/1049-9660(91)90065-W; VIEVILLE T, 1992, LECT NOTES COMPUT SC, V588, P203	12	1	1	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1994	13	3					259	269		10.1007/BF02028348	http://dx.doi.org/10.1007/BF02028348			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QF170					2022-12-18	WOS:A1994QF17000001
J	BOLOTSKI, M; BARMAN, R; LITTLE, JJ; CAMPORESE, D				BOLOTSKI, M; BARMAN, R; LITTLE, JJ; CAMPORESE, D			SILT - A DISTRIBUTED BIT-PARALLEL ARCHITECTURE FOR EARLY VISION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							COMPUTER VISION	A new form of parallelism, distributed bit-parallelism, is introduced. A DBP organization distributes each bit of a data item to a different processor. DBP allows computation that is sublinear with word size for such operations as integer addition, arithmetic shifts, and data moves. The implications of DBP for system architecture are analyzed. An implementation of a DPB architecture based on a mesh with a fast-bypass network is presented, and the performance of DBP algorithms on this architecture is analyzed. The application of the architecture to early vision algorithms is discussed.	UNIV BRITISH COLUMBIA,DEPT COMP SCI,COMPUTAT INTELLIGENCE LAB,VANCOUVER V6T 1Z2,BC,CANADA; UNIV BRITISH COLUMBIA,DEPT ELECT ENGN,VLSI LAB,VANCOUVER V6T 1Z2,BC,CANADA	University of British Columbia; University of British Columbia	BOLOTSKI, M (corresponding author), MIT,ARTIFICIAL INTELLIGENCE LAB,CAMBRIDGE,MA 02139, USA.							BARMAN RA, 1989, CS531 TERM REP; BARMAN RA, 1990, P INT C PATT RECOG; Beadle P., 1985, VLSI: Algorithms and Architectures. Proceedings of the International Workshop on Parallel Computing and VLSI, P153; BLELLOCH GE, 1989, IEEE T COMPUT, V38, P1526, DOI 10.1109/12.42122; BOLOTSKI M, 1990, THESIS U BRIT COLUMB; BRENT RP, 1982, IEEE T COMPUT, V31; CLOUD EL, 1988, FRONTIERS PARALLEL C; FOUNTAIN T, 1987, PROCESSOR ARRAYS ARC; HARRIS J, 1986, THESIS MIT CAMBRIDGE; HILLIS D, 1985, CONNECTION MACHINE D; HILLIS WD, 1986, COMMUN ACM, V29, P1170, DOI 10.1145/7902.7903; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1986, MIT ELECTRICAL ENG C; Hwang K., 1979, COMPUTER ARITHMETIC; KIM D, 1988, HAWAII C SYST SCI; LI HW, 1989, IEEE T PATTERN ANAL, V11, P233, DOI 10.1109/34.21792; LITTLE JJ, 1989, IEEE T PATTERN ANAL, V11, P244, DOI 10.1109/34.21793; LITTLE JJ, 1988, MIT AI929 MEM; MACSORLEY O, 1961, P IRE, V49, P67, DOI 10.1109/JRPROC.1961.287779; Marr D., 1982, VISION; MILLER R, 1988, IEEE T COMPUT, V37, P1605, DOI 10.1109/12.9737; NASSIMI D, 1979, IEEE T COMPUT, V27, P2; POTTER JL, 1985, MIT PRESS SERIES SCI; RUSHTON A, 1989, RES MONOGRAPH PARALL; STOUT QF, 1983, IEEE T COMPUT, V32, P826, DOI 10.1109/TC.1983.1676331	26	1	1	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1993	11	1					63	74		10.1007/BF01420593	http://dx.doi.org/10.1007/BF01420593			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LW796					2022-12-18	WOS:A1993LW79600004
J	FAUGERAS, O				FAUGERAS, O			COMPUTER VISION RESEARCH AT INRIA	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							3D EDGE-DETECTION; POINT MATCHES; 3-D OBJECTS; MOTION; STEREO; REPRESENTATION; MULTIPLICITY; RECOGNITION; ALGORITHMS; TRACKING		INRIA SOPHIA ANTIPOLIS,F-06902 SOPHIA ANTIPOLIS,FRANCE									ABRAMATIC JF, 1982, IEEE T ACOUST SPEECH, V30, P1, DOI 10.1109/TASSP.1982.1163840; [Anonymous], STRUCTURED DOCUMENT; AVNAIM F, INFORMATIQUE THEORIQ, V23, P5; AYACHE N, 1988, INT J ROBOT RES, V7, P45, DOI 10.1177/027836498800700605; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; AYACHE N, 1987, 1ST P INT C COMP VIS, P422; AYACHE N, 1987, INT J COMPUT VIS, V1; AYACHE N, 1991, 1ST EUR C BIOM ENG N; AYACHE N, 1992, ACTIVE VISION, pCH17; BAAZIZ N, 1993, WAVELETS IMAGE COMMU; BASCLE B, 1993, 4TH P INT C COMP VIS; BENNIS C, 1989, COMPUT GRAPH FORUM, V8; BENNIS C, 1991, ACM SIGGRAPH LAS JUL; BERGER MO, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL III, P598, DOI 10.1109/ICPR.1992.202058; BERTHOD M, 1984, 7TH P INT C PATT REC; BERTHOD M, 1988, 9TH P INT C PATT REC; BERTHOD M, 1982, 6TH P INT C PATT REC, P339; BERTHOD M, 1992, MAY P INT C COMP VIS; BOISSONNAT JD, 1992, ALGORITHMICA, V8, P321, DOI 10.1007/BF01758849; BOISSONNAT JD, 1988, COMPUT VISION GRAPH, V44, P1, DOI 10.1016/S0734-189X(88)80028-8; BOISSONNAT JD, 1992, DISCRETE COMPUT GEOM, V8, P51, DOI 10.1007/BF02293035; BOISSONNAT JD, 1993, IN PRESS ALGORITHMIC; BOUTHEMY P, 1989, IEEE T PATTERN ANAL, V11, P499, DOI 10.1109/34.24782; BOUTHEMY P, 1984, IEEE T PATTERN ANAL, V6, P587, DOI 10.1109/TPAMI.1984.4767572; BOUTHEMY P, 1993, IN PRESS OPTICAL ENG; BUFFA M, 1992, IN PRESS DEC P MVA W; BUFFA M, 1992, VISION BASED VEHICLE, P268; CAMILLERAPP J, 1992, NOV P C FRONT HANDWR; CHAUMETTE F, 1992, 11TH P INT C PATT RE; CLEMENT V, 1992, MAY P EUR C COMP VIS; COQUILLART S, 1991, COMP GRAPH, V25, P23, DOI 10.1145/127719.122720; COQUILLART S., 1990, COMPUT GRAPH, V24, P187, DOI [DOI 10.1145/97880.97900, DOI 10.1145/97879.97900]; DERICHE R, 1990, IEEE T PATTERN ANAL, V12, P78, DOI 10.1109/34.41386; DERICHE R, 1987, INT J COMPUT VISION, V1, P167, DOI 10.1007/BF00123164; DERICHE R, 1990, IMAGE VISION COMPUT, V8, P261, DOI 10.1016/0262-8856(90)80002-B; DERICHE R, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P66; DERICHE R, 1992, ARTIFICIAL BIOL VISI, P93; DERICHE R, 1992, MACHINE PERCEPTION A, V2, P71; DERICHE R, 1992, 2 INT C IM PROC, P263; DERICHE R, 1982, MAY P INT C AC SPCH, V3, P2046; DERICHE R, 1988, 9TH P INT C PATT REC, V1, P588; DEVILLERS O, 1992, COMPUTATIONAL GEOMET, V2, P2; DEVILLERS O, 1992, INT J COMPUT GEOM AP, V2; DONIKIAN S, 1992, GRAPHICON 92, P28; ESPIAU B, 1992, IEEE T ROBOTIC AUTOM, V8, P313, DOI 10.1109/70.143350; FAUGERAS O, 1991, CR ACAD SCI II, V312, P1279; FAUGERAS O, 1991, CR ACAD SCI II, V312, P177; FAUGERAS O, 1993, IN PRESS 3 DIMENSION; Faugeras O. D., 1988, International Journal of Pattern Recognition and Artificial Intelligence, V2, P485, DOI 10.1142/S0218001488000285; Faugeras O. D., 1992, International Journal of Pattern Recognition and Artificial Intelligence, V6, P353, DOI 10.1142/S0218001492000229; Faugeras O. D., 1990, International Journal of Imaging Systems and Technology, V2, P356, DOI 10.1002/ima.1850020410; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; FAUGERAS OD, 1990, ARTIF INTELL, V44, P41, DOI 10.1016/0004-3702(90)90098-K; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FAUGERAS OD, 1981, IEEE T PATTERN ANAL, V3, P412, DOI 10.1109/TPAMI.1981.4767127; FAUGERAS OD, 1984, COMPUT VISION GRAPH, V25, P169, DOI 10.1016/0734-189X(84)90101-4; FAUGERAS OD, 1990, INT J COMPUT VISION, V4, P225, DOI 10.1007/BF00054997; FAUGERAS OD, 1990, 1ST P ECCV, P107; FAUGERAS OD, 1986, JUN P IEEE C COMP VI, P15; FAUGERAS OD, 1992, LECTURE NOTES COMPUT, V588, P563; FRANCOIS E, 1990, IMAGE VISION COMPUT, V8, P279, DOI 10.1016/0262-8856(90)80004-D; Gagalowicz A., 1987, Visual Computer, V3, P186, DOI 10.1007/BF01952826; GAGALOWICZ A, 1986, COMPUT GRAPH, V10, P1986; GIAICHECA B, 1992, 7TH IEEE S INT CONTR, P341; GIRAUDON G, 1987, 5TH P SCAN C IM AN S, P547; GIRAUDON G, 1991, JUN P C COMP VIS PAT, P650; GROS P, 1993, IN PRESS MACHINES PE, V5; GUEZIEC A, 1992, 2ND P EUR C COMP VIS; HEGRON G, 1992, EUROGRAPHICS TECHNIC; HEITZ F, 1990, 10TH P IEEE C PATT R, P378; HERLIN I, 1992, JUN P C COMP VIS PAT; HERLIN IL, 1992, IMAGE VISION COMPUT, V10, P673, DOI 10.1016/0262-8856(92)90012-R; HOLAND R, 1992, P IEEE C ROBOT AUTOM, P1539; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P1310, DOI 10.1109/34.41368; JURIE F, 1992, DEC P WORKSH MACH VI; KATO Z, 1992, MAR P INT C AC SPEEC; KATO Z, 1993, MAY P INT C COMP VIS; LABIT C, 1991, IMAGE COMMUNICAT NOV; LANGUENOU E, 1992, EUROGRAPHICS 92  SEP; LEVYVEHEL J, 1991, P C COMPUT VIS PATT; LEVYVEHEL J, 1992, P C COMPUT VIS PATT; LEVYVEHEL J, 1990, 3RD P INT C COMP VIS; LIU YC, 1990, IEEE T PATTERN ANAL, V12, P28, DOI 10.1109/34.41381; LORETTE G, 1990, COMPUT PROCESS HANDW; LUONG T, 1993, IN PRESS CALIBRATION; MA R, 1992, 11TH P INT C PATT RE; MAISEL E, 1992, SEP P EUR WORKSH AN; MARTINET P, 1991, COMPUT ARCHITECT DEC; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; MEYER F, 1992, 2ND P EUR C COMP VIS, P476; MEYGRET A, 1993, INTELLIGENT AUTOMOTI; MEYGRET A, 1991, COMPUTER VISION ADV, P282; MITCHIE A, 1985, COMPUT VIS GRAPH IMA, V32, P384; MOHR R, 1992, GEOMETRIC INVARIANCE, P440; MONGA O, 1991, CVGIP-IMAG UNDERSTAN, V53, P76, DOI 10.1016/1049-9660(91)90006-B; MONGA O, 1991, IMAGE VISION COMPUT, V9, P203, DOI 10.1016/0262-8856(91)90025-K; MONGA O, 1992, P C COMPUT VIS PATT; Monga O., 1987, INT J PATTERN RECOGN, V1, P351, DOI [10.1142/s0218001487000242, DOI 10.1142/S0218001487000242]; MONGA O, 1992, IMAGE VISION COMPUT, V10, P673; MONGA O, 1992, LECTURE NOTES COMPUT, V511; NAVAB N, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P513; NAVAB N, 1993, 4TH P INT C COMP VIS; NICOLAS H, 1992, MAR P INT C AC SPEEC; PEREZ P, 1992, MAR P INT C AC SPEEC, P30; PONCE J, 1987, COMPUT VISION GRAPH, V38, P1, DOI 10.1016/S0734-189X(87)80151-2; PRATT WK, 1978, IEEE T SYST MAN CYB, V8, P796, DOI 10.1109/TSMC.1978.4309867; PRATT WK, 1980, IEEE T PATTERN ANAL, V2, P323; RIVES P, 1990, NATO ASI SERIES F, V57; RIVES P, 1992, 3RD ANN C ART INT SI; ROBERT L, 1991, JUN P COMP VIS PATT, P57; SOSSA H, 1992, JUN P C COMP VIS PAT; TABBONE S, 1992, 11 INT C PATT REC, V3, P655; TAMTAOUI A, 1992, AUG P EUR SIG PROC C; THIRION JP, 1992, JUL IEEE INT S OPT A; THONNAT M, 1989, WORLD GALAXIES, P53; TOMBRE K, 1992, DEC P IAPR WKSHP MAC, P393; TZIRITAS G, 1993, IN PRESS MOTION ESTI; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; VAXIVIERE P, 1992, COMPUTER, V25, P46, DOI 10.1109/2.144439; VIEVILLE T, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P517; VIEVILLE T, 1993, IN PRESS 4TH P INT C; VIEVILLE T, 1989, 5 INT S ROB RES, P57; VIEVILLE T, IN PRESS INTELLIGENT; ZERUBIA J, 1993, IN PRESS IEEE T NEUR; ZERUBIA J, 1991, P ICANN ESPOO; ZERUBIA J, 1991, P IMACS DUBLIN; ZHANG Z, 1992, 3D DYNAMIC SCENE ANA; ZHANG ZY, 1992, IEEE T PATTERN ANAL, V14, P1141, DOI 10.1109/34.177380; ZHANG ZY, 1992, INT J COMPUT VISION, V7, P211, DOI 10.1007/BF00126394; ZHANG ZY, 1992, INT J ROBOT RES, V11, P269, DOI 10.1177/027836499201100401; ZHANG ZY, 1991, IMAGE VISION COMPUT, V9, P10, DOI 10.1016/0262-8856(91)90043-O; ZIOU D, 1991, PATTERN RECOGN, V24, P465, DOI 10.1016/0031-3203(91)90014-V	132	1	1	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1993	10	2					91	99		10.1007/BF01420732	http://dx.doi.org/10.1007/BF01420732			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LP385					2022-12-18	WOS:A1993LP38500001
J	KAKUSHO, K; DAN, S; KITAHASHI, T; ABE, N				KAKUSHO, K; DAN, S; KITAHASHI, T; ABE, N			COMPUTER VISION BASED ON A HYPOTHESIZATION AND VERIFICATION SCHEME BY PARALLEL RELAXATION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OPTIMIZATION	Shape recovery from a monocular image is addressed. It is often said that the information conveyed by an image is insufficient to reconstruct 3D shapes of objects in the image. This implies that shape recovery from an image necessitates the use of additional plausible constraints on typical structures and features of the objects in an ordinary scene. We propose a hypothesization and verification method for 3D shape recovery based on geometrical constraints peculiar to man-made objects. The objective is to increase the robustness of computer vision systems. One difficulty with this method lies in the mutual dependency between proper assignment of constraints to the regions in a given image and recovery of a consistent 3D shape. A concurrent mechanism has been implemented which is based on energy minimization using a parallel network for relaxation. This mechanism is capable of maintaining consistency between constraint assignment and shape recovery.	OSAKA UNIV,INST SCI & IND RES,IBARAKI,OSAKA 567,JAPAN; KYUSHU INST TECHNOL,FAC COMP SCI & SYST ENGN,FUKUOKA 820,JAPAN	Osaka University; Kyushu Institute of Technology								FUNAHASHI K, 1988, IEICHE MBE8852 TECH, P127; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141; HORAUD R, 1987, IEEE T PATTERN ANAL, V9, P401, DOI 10.1109/TPAMI.1987.4767922; Irie B., 1988, IEEE INT C NEURAL NE, V1, P641; ISHIZUKA M, 1988, IEICE PRU87123 TECH, P33; KAWAHARA H, 1989, IEICE MBE8854 TECHN, P47; Kawato M., 1988, Journal of the Institute of Television Engineers of Japan, V42, P918, DOI 10.3169/itej1978.42.918; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; KOCH C, 1986, P NATL ACAD SCI USA, V83, P4263, DOI 10.1073/pnas.83.12.4263; Maehara K., 1989, Transactions of the Institute of Electronics, Information and Communication Engineers D-II, VJ72D-II, P887; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; SAKAUE K, 1989, JOHO SHORI, V30, P1047; Shirai Y, 1987, 3 DIMENSIONAL COMPUT; Sugihara K., 1983, Transactions of the Institute of Electronics and Communication Engineers of Japan, Part D, VJ66D, P541; Tanaka T., 1989, Transactions of the Institute of Electronics, Information and Communication Engineers D-II, VJ72D-II, P517; ULUPINAR F, 1991, CVGIP-IMAG UNDERSTAN, V53, P88, DOI 10.1016/1049-9660(91)90007-C; WATANABE Y, 1989, IEICE IE88127 TECH R, P9	18	1	1	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1992	9	1					13	30						18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JW393					2022-12-18	WOS:A1992JW39300002
J	BERGHOLM, F				BERGHOLM, F			MOTION FROM FLOW ALONG CONTOURS - A NOTE ON ROBUSTNESS AND AMBIGUOUS CASES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									ROYAL INST TECHNOL,NADA,S-10044 STOCKHOLM 70,SWEDEN	Royal Institute of Technology				Bergholm, Fredrik/0000-0002-0969-7196				ADIV G, 1984, 8407 U MASS COINS TE; CANNY J, 1986, IEEE T PAMI, V8; CANNY JF, 1983, TR720 MIT ARTIF INT; CARLSSON S, 1986, APR P ICASSP C TOKY; DAVIS LS, 1983, COMPUT VISION GRAPH, V23, P303; HILDRETH EC, 1984, ARTIF INTELL, V23, P309, DOI 10.1016/0004-3702(84)90018-3; *HOLLAND HC, 1965, SPIRAL EFFECT; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; KANATANI K, 1985, P DARPA IMAGE UNDERS, P107; Kmenta J., 1971, ELEMENTS ECONOMETRIC, V2nd; LENZ R, 1984, MTH P INT C PATT REC, V1, P546; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; REILLO C, 1985, TRITATTT8506 ROY I T; Simmons G. F., 1972, DIFFERENTIAL EQUATIO; SOPHUS L, 1893, VORLESUNGEN CONTINUE; SUBBARAO M, 1985, CSTR1485 U MAR COLL; TSAI RY, 1982, IEEE T ACOUST SPEECH, V30, P525, DOI 10.1109/TASSP.1982.1163931; TSAI RY, 1981, IEEE T ACOUST SPEECH, V29, P1147, DOI 10.1109/TASSP.1981.1163710; WALLACH H, 1956, AM J PSYCHOL, V69, P48, DOI 10.2307/1418114; WAXMAN A, 1983, CARTR24 CSTR1332 CTR; WAXMAN A, 1984, CARTR58 CSTR1394; WAXMAN AM, 1985, INT J ROBOT RES, V4, P72, DOI 10.1177/027836498500400306; WOHN KY, 1983, PATTERN RECOGN, V16, P563, DOI 10.1016/0031-3203(83)90072-9	23	1	1	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1989	2	4					395	415		10.1007/BF00133557	http://dx.doi.org/10.1007/BF00133557			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC192					2022-12-18	WOS:A1989AC19200003
J	Ratnayake, MN; Amarathunga, DC; Zaman, A; Dyer, AG; Dorin, A				Ratnayake, Malika Nisal; Amarathunga, Don Chathurika; Zaman, Asaduz; Dyer, Adrian G.; Dorin, Alan			Spatial Monitoring and Insect Behavioural Analysis Using Computer Vision for Precision Pollination	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Deep learning; Camera trapping; Honeybees; Pollination; Food security; Insect tracking	CROP YIELD; AGRICULTURE; DENSITY	Insects are the most important global pollinator of crops and play a key role in maintaining the sustainability of natural ecosystems. Insect pollination monitoring and management are therefore essential for improving crop production and food security. Computer vision facilitated pollinator monitoring can intensify data collection over what is feasible using manual approaches. The new data it generates may provide a detailed understanding of insect distributions and facilitate fine-grained analysis sufficient to predict their pollination efficacy and underpin precision pollination. Current computer vision facilitated insect tracking in complex outdoor environments is restricted in spatial coverage and often constrained to a single insect species. This limits its relevance to agriculture. Therefore, in this article we introduce a novel system to facilitate markerless data capture for insect counting, insect motion tracking, behaviour analysis and pollination prediction across large agricultural areas. Our system is comprised of edge computing multi-point video recording, offline automated multi-species insect counting, tracking and behavioural analysis. We implement and test our system on a commercial berry farm to demonstrate its capabilities. Our system successfully tracked four insect varieties, at nine monitoring stations within polytunnels, obtaining an F-score above 0.8 for each variety. The system enabled calculation of key metrics to assess the relative pollination impact of each insect variety. With this technological advancement, detailed, ongoing data collection for precision pollination becomes achievable. This is important to inform growers and apiarists managing crop pollination, as it allows data-driven decisions to be made to improve food production and food security.	[Ratnayake, Malika Nisal; Amarathunga, Don Chathurika; Zaman, Asaduz; Dorin, Alan] Monash Univ, Fac Informat Technol, Dept Data Sci & AI, Computat & Collect Intelligence Grp, Wellington Rd, Melbourne, Vic 3800, Australia; [Dyer, Adrian G.] RMIT Univ, Sch Media & Commun, La Trobe St, Melbourne, Vic 3000, Australia; [Dyer, Adrian G.] Monash Univ, Dept Physiol, Wellington Rd, Melbourne, Vic 3800, Australia	Monash University; Royal Melbourne Institute of Technology (RMIT); Monash University	Ratnayake, MN (corresponding author), Monash Univ, Fac Informat Technol, Dept Data Sci & AI, Computat & Collect Intelligence Grp, Wellington Rd, Melbourne, Vic 3800, Australia.	malika.ratnayake@monash.edu; don.amarathunga@monash.edu; asaduzzaman@monash.edu; adrian.dyer@monash.edu; alan.dorin@monash.edu		Dyer, Adrian/0000-0002-2632-9061	Australian Research Council [DP160100161]; AgriFutures grant [PRJ-012993]; ARC Research Hub [IH180100002]; Monash-Bosch AgTech Launchpad primer Grant	Australian Research Council(Australian Research Council); AgriFutures grant; ARC Research Hub(Australian Research Council); Monash-Bosch AgTech Launchpad primer Grant	Authors were supported by the Australian Research Council Discovery Projects grant DP160100161 and Monash-Bosch AgTech Launchpad primer Grant. This study was funded by AgriFutures grant PRJ-012993. Amarathunga is supported by ARC Research Hub IH180100002.	Abadi Martin, 2016, 12 USENIX S OP SYST, P265, DOI DOI 10.5555/3026877.3026899; Abdel-Raziq HM, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-82537-1; Afonso M, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.571299; Aizen MA, 2009, ANN BOT-LONDON, V103, P1579, DOI 10.1093/aob/mcp076; Alexey Bochkovskiy, 2020, Arxiv, DOI arXiv:2004.10934; Aslam Masood, 2021, 2021 Digital Image Computing: Techniques and Applications (DICTA), P01, DOI 10.1109/DICTA52665.2021.9647409; Barreiros MD, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-81997-9; Batsleer F, 2020, METHODS ECOL EVOL, V11, P350, DOI 10.1111/2041-210X.13356; Bjerge K, 2022, REMOTE SENS ECOL CON, V8, P315, DOI 10.1002/rse2.245; Bjerge K, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020343; Branson K, 2009, NAT METHODS, V6, P451, DOI [10.1038/nmeth.1328, 10.1038/NMETH.1328]; Breeze TD, 2021, J APPL ECOL, V58, P44, DOI 10.1111/1365-2664.13755; Campbell J., 2008, VIS OBS ANAL ANIM IN, V8, P1; CHAGNON M, 1989, J ECON ENTOMOL, V82, P1350, DOI 10.1093/jee/82.5.1350; Dennis RLH, 2006, BIOL CONSERV, V128, P486, DOI 10.1016/j.biocon.2005.10.015; FAO, 2018, WHY BEES MATT IMP BE; Fijen TPM, 2018, ECOL LETT, V21, P1704, DOI 10.1111/ele.13150; Food & Agriculture Organization of the United Nation, 2019, GLOB ACT POLL SERV S; Garibaldi LA, 2020, J APPL ECOL, V57, P664, DOI 10.1111/1365-2664.13574; Garibaldi LA, 2017, CURR OPIN INSECT SCI, V21, P105, DOI 10.1016/j.cois.2017.05.016; Garibaldi LA, 2016, SCIENCE, V351, P388, DOI 10.1126/science.aac7287; Goscinski WJ, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00030; Grundy J., 2021, SMART AGR TECHNOL, V1, DOI [10.1016/j.atech.2021.100023, DOI 10.1016/J.ATECH.2021.100023]; Haalck L, 2020, J NEUROSCI METH, V330, DOI 10.1016/j.jneumeth.2019.108455; Hall MA, 2020, J ECON ENTOMOL, V113, P1337, DOI 10.1093/jee/toaa037; Hallmann CA, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0185809; Howard SR, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0251572; Hoye TT, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2002545117; Jolles JW, 2021, METHODS ECOL EVOL, V12, P1562, DOI 10.1111/2041-210X.13652; Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016; KEVAN PG, 1975, SCIENCE, V189, P723, DOI 10.1126/science.189.4204.723; Kirkeby C, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-81005-0; Koirala A, 2019, COMPUT ELECTRON AGR, V162, P219, DOI 10.1016/j.compag.2019.04.017; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Lu H, 2017, J MT SCI-ENGL, V14, P731, DOI 10.1007/s11629-016-3950-2; MacInnis G, 2019, J APPL ECOL, V56, P824, DOI 10.1111/1365-2664.13344; Magnier B, 2019, PROC SPIE, V11059, DOI 10.1117/12.2526120; O'Grady M. J., 2019, Artificial Intelligence in Agriculture, V3, P42, DOI 10.1016/j.aiia.2019.12.001; Odemer R, 2022, ANN APPL BIOL, V180, P73, DOI 10.1111/aab.12727; Outhwaite CL, 2022, NATURE, V605, P97, DOI 10.1038/s41586-022-04644-x; Perez-Escudero A, 2014, NAT METHODS, V11, P743, DOI [10.1038/NMETH.2994, 10.1038/nmeth.2994]; Potts SG, 2016, NATURE, V540, P220, DOI 10.1038/nature20588; Rader R, 2016, P NATL ACAD SCI USA, V113, P146, DOI 10.1073/pnas.1517092112; Ratnayake MN, 2021, IEEE COMPUT SOC CONF, P2915, DOI 10.1109/CVPRW53098.2021.00327; Ratnayake MN, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0239504; Real L, 2012, POLLINATION BIOL; Redmon J., 2017, P IEEE C COMPUTER VI, P7263, DOI DOI 10.1109/CVPR.2017.690; Rollin O, 2019, J APPL ECOL, V56, P1152, DOI 10.1111/1365-2664.13355; Schweiger O, 2010, BIOL REV, V85, P777, DOI 10.1111/j.1469-185X.2010.00125.x; Sekachev Boris, 2019, Zenodo, DOI 10.5281/ZENODO.3497106; Settele J, 2016, NAT PLANTS, V2, DOI [10.1038/NPLANTS.2016.92, 10.1038/nplants.2016.92]; Simons DJ, 1999, PERCEPTION, V28, P1059, DOI 10.1068/p2952; Spaethe J, 2001, P NATL ACAD SCI USA, V98, P3898, DOI 10.1073/pnas.071053098; Spencer EE, 2020, FOOD WEBS, V24, DOI 10.1016/j.fooweb.2020.e00144; Stojnic V, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040653; Su D, 2021, COMPUT ELECTRON AGR, V190, DOI 10.1016/j.compag.2021.106418; van der Kooi CJ, 2019, ANN BOT-LONDON, V124, P343, DOI 10.1093/aob/mcz073; Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914; Vanbergen AJ, 2013, FRONT ECOL ENVIRON, V11, P251, DOI 10.1890/120126; Walter T, 2021, ELIFE, V10, DOI 10.7554/eLife.64000; Wang SJ, 2016, IEEE IJCNN, P4368, DOI 10.1109/IJCNN.2016.7727770; Wood TJ, 2020, APIDOLOGIE, V51, P1100, DOI 10.1007/s13592-020-00788-9; Yang C, 2017, SENS IMAGING, V19, DOI 10.1007/s11220-017-0185-4; Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005	64	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01715-4	http://dx.doi.org/10.1007/s11263-022-01715-4		NOV 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6N2MN		Green Submitted			2022-12-18	WOS:000889393100001
J	Ishikawa, H; Liu, CL; Pajdla, T; Shi, JB				Ishikawa, Hiroshi; Liu, Cheng-Lin; Pajdla, Tomas; Shi, Jianbo			Guest Editorial: Special Issue on Advances in Computer Vision and Applications (ACCV 2020)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material; Early Access									[Ishikawa, Hiroshi] Waseda Univ, Tokyo, Tokyo 1698555, Japan; [Liu, Cheng-Lin] Inst Automat Chinese Acad Sci, Beijing, Peoples R China; [Pajdla, Tomas] Czech Syst Univ Prague, Prague, Czech Republic; [Shi, Jianbo] Univ Penn, Philadelphia, PA 19104 USA	Waseda University; Chinese Academy of Sciences; Institute of Automation, CAS; University of Pennsylvania	Ishikawa, H (corresponding author), Waseda Univ, Tokyo, Tokyo 1698555, Japan.	hfs@waseda.jp							0	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01717-2	http://dx.doi.org/10.1007/s11263-022-01717-2		NOV 2022	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6M3PM		Bronze			2022-12-18	WOS:000888783500002
J	Smith, W				Smith, William			Guest Editorial: Special Issue on Computer Vision from 2D to 3D	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material; Early Access									[Smith, William] Univ York, Dept Comp Sci, York, England	University of York - UK	Smith, W (corresponding author), Univ York, Dept Comp Sci, York, England.	william.smith@york.ac.uk							0	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01724-3	http://dx.doi.org/10.1007/s11263-022-01724-3		NOV 2022	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6M3PM	36467668	Bronze, Green Published			2022-12-18	WOS:000888783500001
J	Broome, S; Feighelstein, M; Zamansky, A; Lencioni, GC; Andersen, PH; Pessanha, F; Mahmoud, M; Kjellstrom, H; Salah, AA				Broome, Sofia; Feighelstein, Marcelo; Zamansky, Anna; Lencioni, Gabriel Carreira; Andersen, Pia Haubro; Pessanha, Francisca; Mahmoud, Marwa; Kjellstrom, Hedvig; Salah, Albert Ali			Going Deeper than Tracking: A Survey of Computer-Vision Based Recognition of Animal Pain and Emotions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Affective computing; Non-human behavior analysis; Pain estimation; Pain recognition; Emotion recognition; Computer vision for animals	CODING SYSTEM; EQUINE PAIN; EXPRESSION; HORSES; COMMUNICATION; ANALGESIA; FRAMEWORK; BEHAVIOR; SCIENCE; POSTURE	Advances in animal motion tracking and pose recognition have been a game changer in the study of animal behavior. Recently, an increasing number of works go 'deeper' than tracking, and address automated recognition of animals' internal states such as emotions and pain with the aim of improving animal welfare, making this a timely moment for a systematization of the field. This paper provides a comprehensive survey of computer vision-based research on recognition of pain and emotional states in animals, addressing both facial and bodily behavior analysis. We summarize the efforts that have been presented so far within this topic-classifying them across different dimensions, highlight challenges and research gaps, and provide best practice recommendations for advancing the field, and some future directions for research.	[Broome, Sofia; Kjellstrom, Hedvig] KTH Royal Inst Technol, Div Robot Percept & Learning, Stockholm, Sweden; [Feighelstein, Marcelo; Zamansky, Anna] Univ Haifa, Informat Syst Dept, Tech4Anim Lab, Haifa, Israel; [Lencioni, Gabriel Carreira] Univ Sao Paulo, Sch Vet Med & Anim Sci, Dept Prevent Vet Med & Anim Hlth, Sao Paulo, SP, Brazil; [Mahmoud, Marwa] Univ Glasgow, Sch Comp Sci, Glasgow, Lanark, Scotland; [Kjellstrom, Hedvig] Silo AI, Stockholm, Sweden; [Andersen, Pia Haubro] Swedish Univ Agr Sci, Dept Clin Sci, Uppsala, Sweden; [Pessanha, Francisca; Salah, Albert Ali] Univ Utrecht, Dept Informat & Comp Sci, Utrecht, Netherlands; [Salah, Albert Ali] Bogazici Univ, Dept Comp Engn, Istanbul, Turkey	Royal Institute of Technology; University of Haifa; Universidade de Sao Paulo; University of Glasgow; Swedish University of Agricultural Sciences; Utrecht University; Bogazici University	Broome, S (corresponding author), KTH Royal Inst Technol, Div Robot Percept & Learning, Stockholm, Sweden.	sbroome@kth.se; feighels@gmail.com; annazam@is.haifa.acil; gabrieLlencioni@gmail.com; pia.haubro.andersen@slu.se; f.pessanha@uu.nl; marwa.mahmoud@glasgow.ac.uk; hedvig@kth.se; a.a.salah@uu.nl			Royal Institute of Technology	Royal Institute of Technology	Open access funding provided by Royal Institute of Technology.	Abadi M, 2015, P 12 USENIX S OPERAT; Al-Eidan RM, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10175984; Amir Roshan Zamir, 2012, Arxiv, DOI arXiv:1212.0402; Amir S., 2017, P AN COMP INT; Anand KJ., 2007, PAIN NEONATES INFANT; Andersen P.H., 2021, ARXIV; Andersen PH, 2021, ANIMALS-BASEL, V11, DOI 10.3390/ani11061643; Anderson DJ, 2014, NEURON, V84, P18, DOI 10.1016/j.neuron.2014.09.005; Anderson DJ, 2014, CELL, V157, P187, DOI 10.1016/j.cell.2014.03.003; Andresen  N, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0228059; Andrew Zisserman, 2017, Arxiv, DOI arXiv:1705.06950; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ask K, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10112155; Auer U, 2021, ANIMALS-BASEL, V11, DOI 10.3390/ani11030850; Barrett LF, 2004, J PERS SOC PSYCHOL, V87, P266, DOI 10.1037/0022-3514.87.2.266; Bartlett MS, 2014, CURR BIOL, V24, P738, DOI 10.1016/j.cub.2014.02.009; Bateson M., 2021, MEASURING BEHAV INTR, DOI [10.1017/9781108776462, DOI 10.1017/9781108776462]; Biggs Benjamin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P195, DOI 10.1007/978-3-030-58621-8_12; Birch J., 2021, REV EVIDENCE SENTIEN; Blumrosen G, 2017, IEEE INT CONF COMP V, P2810, DOI 10.1109/ICCVW.2017.332; Boissy A, 2007, ANIM WELFARE, V16, P37; Boneh-Shitrit T., 2022, DEEP LEARNING MODELS; Bremhorst A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-55714-6; Briefer EF, 2015, ANIM BEHAV, V99, P131, DOI 10.1016/j.anbehav.2014.11.002; Broome S, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0263854; Broome S, 2019, PROC CVPR IEEE, P12859, DOI 10.1109/CVPR.2019.01295; Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011; Caeiro CC, 2017, APPL ANIM BEHAV SCI, V189, P66, DOI 10.1016/j.applanim.2017.01.005; Camras LA, 2010, EMOT REV, V2, P120, DOI 10.1177/1754073909352529; Cao JK, 2019, IEEE I CONF COMP VIS, P9497, DOI 10.1109/ICCV.2019.00959; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Clegg ILK, 2017, BEHAV BRAIN RES, V322, P115, DOI 10.1016/j.bbr.2017.01.026; Correia-Caeiro C, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0245117; Corujo LA, 2021, FUTURE INTERNET, V13, DOI 10.3390/fi13100250; Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949; Dael N, 2012, J NONVERBAL BEHAV, V36, P97, DOI 10.1007/s10919-012-0130-0; Dalla Costa E, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092281; Dawkins MS, 2008, ETHOLOGY, V114, P937, DOI 10.1111/j.1439-0310.2008.01557.x; de Vere AJ, 2016, WIRES COGN SCI, V7, P354, DOI 10.1002/wcs.1399; Descovich KA., 2017, FACIAL EXPRESSION UN; Diogo R, 2008, J ANAT, V213, P391, DOI 10.1111/j.1469-7580.2008.00953.x; Donato G, 1999, IEEE T PATTERN ANAL, V21, P974, DOI 10.1109/34.799905; Duhn Lenora J, 2004, Adv Neonatal Care, V4, P126, DOI 10.1016/j.adnc.2004.04.005; Duncan IJH, 1996, ACTA AGR SCAND A-AN, P29; Dyson S, 2018, J VET BEHAV, V23, P47, DOI 10.1016/j.jveb.2017.10.008; Ede T, 2019, J DAIRY SCI, V102, P10677, DOI 10.3168/jds.2019-16325; EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068; Ekman P., 2002, FACIAL ACTION CODING; Feighelstein M., 2022, AUTOMATED RECO UNPUB; Ferres K, 2022, FUTURE INTERNET, V14, DOI 10.3390/fi14040097; Finka LR, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-46330-5; FITZGERALD M, 1989, ARCH DIS CHILD-FETAL, V64, P441, DOI 10.1136/adc.64.4_Spec_No.441; Foris B, 2019, APPL ANIM BEHAV SCI, V210, P60, DOI 10.1016/j.applanim.2018.10.016; Forkosh O, 2021, PATTERNS, V2, DOI 10.1016/j.patter.2020.100194; Franzoni V, 2019, 2019 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE WORKSHOPS (WI 2019 COMPANION), P91, DOI 10.1145/3358695.3361750; Gleerup KB, 2016, EQUINE VET EDUC, V28, P47, DOI 10.1111/eve.12383; Gleerup KB, 2015, VET ANAESTH ANALG, V42, P103, DOI 10.1111/vaa.12212; Graving JM, 2019, ELIFE, V8, DOI 10.7554/eLife.47994; GRUNAU RVE, 1987, PAIN, V28, P395, DOI 10.1016/0304-3959(87)90073-X; Hale CJ., 1997, PAIN RES MANAG, V2, P217, DOI [10.1155/1997/283582, DOI 10.1155/1997/283582]; Hall C, 2018, APPL ANIM BEHAV SCI, V205, P183, DOI 10.1016/j.applanim.2018.03.006; Hassan T, 2021, IEEE T PATTERN ANAL, V43, P1815, DOI 10.1109/TPAMI.2019.2958341; Higgins I., 2016, INT C LEARNING REPRE; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Huber A., 2018, MEASURING BEHAV; Hummel H. I., 2020, 2020 15 IEEE INT C A, P793; Japkowicz N., 2002, Intelligent Data Analysis, V6, P429; Kim H., 2018, P 35 INT C MACHINE L; Koskimaki H, 2015, 2015 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P301, DOI 10.1109/SSCI.2015.52; Kremer L, 2020, NEUROSCI BIOBEHAV R, V113, P273, DOI 10.1016/j.neubiorev.2020.01.028; Kret Mariska E, 2022, Affect Sci, V3, P182, DOI 10.1007/s42761-021-00099-x; Kulkarni A, 2020, DATA DEMOCRACY: AT THE NEXUS OF ARTIFICIAL INTELLIGENCE, SOFTWARE DEVELOPMENT, AND KNOWLEDGE ENGINEERING, P83, DOI 10.1016/B978-0-12-818366-3.00005-8; Kumar A., 2018, 6 INT C LEARNING REP; Labus JS, 2003, PAIN, V102, P109, DOI 10.1016/S0304-3959(02)00354-8; Lansade L, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-32993-z; Lecorps B, 2016, PHYSIOL BEHAV, V157, P209, DOI 10.1016/j.physbeh.2016.02.014; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lencioni GC, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0258672; Li S, 2022, IEEE T AFFECT COMPUT, V13, P881, DOI 10.1109/TAFFC.2020.2973158; Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; London AJ, 2019, HASTINGS CENT REP, V49, P15, DOI 10.1002/hast.973; Low P., 2012, CAMBRIDGE DECLARATIO; Lu YT, 2017, IEEE INT CONF AUTOMA, P394, DOI 10.1109/FG.2017.56; Lucey P., 2010, P IEEE COMP SOC C CO, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Lundblad J, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0241532; Mahmoud M., 2018, HDB PAIN PALLIATIVE, P145, DOI [10.1007/978-3-319-95369-4_9, DOI 10.1007/978-3-319-95369-4_9]; Maisonpierre IN, 2019, EQUINE VET J, V51, P840, DOI 10.1111/evj.13130; Mathis A, 2018, NAT NEUROSCI, V21, P1281, DOI 10.1038/s41593-018-0209-y; Mayo LM, 2019, NEUROBIOL STRESS, V10, DOI 10.1016/j.ynstr.2019.100166; McLennan K, 2019, ANIMALS-BASEL, V9, DOI 10.3390/ani9040196; McLennan KM, 2016, APPL ANIM BEHAV SCI, V176, P19, DOI 10.1016/j.applanim.2016.01.007; Mendl M, 2020, NEUROSCI BIOBEHAV R, V112, P144, DOI 10.1016/j.neubiorev.2020.01.025; Mendl M, 2010, P ROY SOC B-BIOL SCI, V277, P2895, DOI 10.1098/rspb.2010.0303; Merkies K, 2019, ANIMALS-BASEL, V9, DOI 10.3390/ani9080562; Morozov A, 2021, ENEURO, V8, DOI 10.1523/ENEURO.0117-21.2021; Morris PH, 2008, COGNITION EMOTION, V22, P3, DOI 10.1080/02699930701273716; Mott RO, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-78386-z; Nanni L, 2017, PATTERN RECOGN, V71, P158, DOI 10.1016/j.patcog.2017.05.025; Noroozi F, 2021, IEEE T AFFECT COMPUT, V12, P505, DOI 10.1109/TAFFC.2018.2874986; Oliveira T, 2022, J EQUINE VET SCI, V110, DOI 10.1016/j.jevs.2021.103832; Panksepp J, 2010, HBK BEHAV NEUROSCI, V19, P201, DOI 10.1016/B978-0-12-374593-4.00020-6; Parr LA, 2010, AM J PHYS ANTHROPOL, V143, P625, DOI 10.1002/ajpa.21401; Paul ES, 2018, APPL ANIM BEHAV SCI, V205, P202, DOI 10.1016/j.applanim.2018.01.008; Paul ES, 2005, NEUROSCI BIOBEHAV R, V29, P469, DOI 10.1016/j.neubiorev.2005.01.002; Pennington ZT, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-56408-9; Pereira TD, 2019, NAT METHODS, V16, P117, DOI 10.1038/s41592-018-0234-5; Pessanha F., 2022, IEEE T AFFECT COMPUT, DOI [10.1109/TAFFC.2022.3177639, DOI 10.1109/TAFFC.2022.3177639]; Pessanha F., 2020, 2020 15 IEEE INT C A, P670; Plutchik R., 1979, EMOTION PSYCHOEVOLUT; Podturkin AA, 2022, J APPL ANIM WELF SCI, DOI 10.1080/10888705.2021.2012783; Posner J, 2005, DEV PSYCHOPATHOL, V17, P715, DOI 10.1017/S0954579405050340; Price J, 2002, VET REC, V151, P570, DOI 10.1136/vr.151.19.570; Proctor HS, 2015, PHYSIOL BEHAV, V147, P1, DOI 10.1016/j.physbeh.2015.04.011; Raja SN, 2020, PAIN, V161, P1976, DOI 10.1097/j.pain.0000000000001939; Rashid M., 2022, P IEEECVF WINTER C A, P1646; Reulke R., 2018, 2018 15 IEEE INT C A, P1; Romero-Ferrero F, 2019, NAT METHODS, V16, P179, DOI 10.1038/s41592-018-0295-5; Ruess D, 2019, LECT NOTES COMPUT SC, V11854, P156, DOI 10.1007/978-3-030-34879-3_13; Schnaider MA, 2022, J VET BEHAV, V49, P28, DOI 10.1016/j.jveb.2021.11.011; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Seneque E, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0211852; Seuss D, 2019, INT CONF AFFECT; Sharma G., 2021, ADV DATA SCI METHODO, P35, DOI [10.1007/978-3-030-51870-7_3, DOI 10.1007/978-3-030-51870-7_3]; Shi XJ, 2015, ADV NEUR IN, V28; Amir S, 2021, Arxiv, DOI arXiv:2112.05814; Sneddon LU, 2014, ANIM BEHAV, V97, P201, DOI 10.1016/j.anbehav.2014.09.007; Statham P, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-65954-6; Susskind Joshua M., 2008, Affective Computing. Focus on Emotion Expression, Synthesis and Recognition, P421; Thabtah F, 2020, INFORM SCIENCES, V513, P429, DOI 10.1016/j.ins.2019.11.004; Tuttle AH, 2018, MOL PAIN, V14, DOI 10.1177/1744806918763658; Uccheddu S, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-05669-y; Vabalas A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0224365; Varma S, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-91; Waller B., 2013, DOGFACS DOG FACIAL A; Waller BM, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082686; Walsh J, 2014, PAIN, V155, P2282, DOI 10.1016/j.pain.2014.08.019; Wang NN, 2018, NEUROCOMPUTING, V275, P50, DOI 10.1016/j.neucom.2017.05.013; Waran N, 2010, NEW ZEAL VET J, V58, P274, DOI 10.1080/00480169.2010.69402; Wathan J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0131738; Wu Y, 2019, INT J COMPUT VISION, V127, P115, DOI 10.1007/s11263-018-1097-z; Yinghong Qiu, 2019, 2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC), P1356, DOI 10.1109/IAEAC47372.2019.8997580; Yu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10988, DOI 10.1109/CVPR42600.2020.01100; Zamzmi Ghada, 2018, IEEE Rev Biomed Eng, V11, P77, DOI 10.1109/RBME.2017.2777907; Zhu H., 2022, ARXIV	146	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01716-3	http://dx.doi.org/10.1007/s11263-022-01716-3		NOV 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6M2MQ		hybrid, Green Accepted			2022-12-18	WOS:000888708500001
J	Shi, SS; Jiang, L; Deng, JJ; Wang, Z; Guo, CX; Shi, JP; Wang, XG; Li, HS				Shi, Shaoshuai; Jiang, Li; Deng, Jiajun; Wang, Zhe; Guo, Chaoxu; Shi, Jianping; Wang, Xiaogang; Li, Hongsheng			PV-RCNN plus plus : Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						3D object Detection; Point clouds; LiDAR; Autonomous driving; Sparse convolution	NETWORKS	3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose Point-Voxel Region-based Convolution Neural Networks (PV-RCNNs) for 3D object detection on point clouds. First, we propose a novel 3D detector, PV-RCNN, which boosts the 3D detection performance by deeply integrating the feature learning of both point-based set abstraction and voxel-based sparse convolution through two novel steps, i.e., the voxel-to-keypoint scene encoding and the keypoint-to-grid RoI feature abstraction. Second, we propose an advanced framework, PV-RCNN++, for more efficient and accurate 3D object detection. It consists of two major improvements: sectorized proposal-centric sampling for efficiently producing more representative keypoints, and VectorPool aggregation for better aggregating local point features with much less resource consumption. With these two strategies, our PV-RCNN++ is about 3x faster than PV-RCNN, while also achieving better performance. The experiments demonstrate that our proposed PV-RCNN++ framework achieves state-of-the-art 3D detection performance on the large-scale and highly-competitive Waymo Open Dataset with 10 FPS inference speed on the detection range of 150m x 150m.	[Shi, Shaoshuai; Jiang, Li; Wang, Xiaogang; Li, Hongsheng] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Shi, Shaoshuai; Jiang, Li] Max Planck Inst Informat, Saarbrucken, Germany; [Deng, Jiajun] Univ Sydney, Sydney, NSW, Australia; [Wang, Zhe; Guo, Chaoxu; Shi, Jianping] SenseTime Res, Shanghai, Peoples R China	Chinese University of Hong Kong; Max Planck Society; University of Sydney	Shi, SS (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.; Shi, SS (corresponding author), Max Planck Inst Informat, Saarbrucken, Germany.	sshi@mpi-inf.mpg.de; lijiang@mpi-inf.mpg.de; jiajun.deng@sydney.edu.au; wangzhe@sensetime.com; cxguo@sensetime.com; shijianping@sensetime.com; xgwang@ee.cuhk.com.hk; hsli@ee.cuhk.com.hk			Projekt DEAL	Projekt DEAL	Open Access funding enabled and organized by Projekt DEAL.	Benjamin Caine, 2019, Arxiv, DOI arXiv:1908.11069; Brazil G, 2019, IEEE I CONF COMP VIS, P9286, DOI 10.1109/ICCV.2019.00938; Chabot F, 2017, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2017.198; Chen Q., 2019, OBJECT HOTSPOTS ANCH; Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236; Chen Xiaozhi, 2017, P IEEE C COMP VIS PA, DOI 10.1109/cvpr.2017.691.2017; Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI 10.1109/ICCV.2019.00987; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Du DL, 2021, Arxiv, DOI arXiv:2112.11790; Xie EZ, 2022, Arxiv, DOI arXiv:2204.05088; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Girshick R., 2015, ICCV; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Han Hu, 2020, Arxiv, DOI arXiv:2007.01294; Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278; Jaritz M, 2019, IEEE INT CONF COMP V, P3995, DOI 10.1109/ICCVW.2019.00494; Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053; Jin Hyeok Yoo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P720, DOI 10.1007/978-3-030-58583-9_43; Huang JJ, 2022, Arxiv, DOI arXiv:2203.17054; Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049; Kuang HW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030704; Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298; Li BY, 2019, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2019.00111; Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783; Li Peixuan, 2020, ECCV; Li YY, 2018, ADV NEUR IN, V31; Li Z., 2022, ARXIV; Li ZC, 2021, PROC CVPR IEEE, P7542, DOI 10.1109/CVPR46437.2021.00746; Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752; Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu Y., 2022, ARXIV; Liu ZJ, 2019, ADV NEUR IN, V32; Manhardt F, 2019, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2019.00217; Mao JG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3144, DOI 10.1109/ICCV48922.2021.00315; Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597; Murthy J. Krishna, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P724, DOI 10.1109/ICRA.2017.7989089; Philion Jonah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P194, DOI 10.1007/978-3-030-58568-6_12; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Qian R, 2020, PROC CVPR IEEE, P5880, DOI 10.1109/CVPR42600.2020.00592; Reading C, 2021, PROC CVPR IEEE, P8551, DOI 10.1109/CVPR46437.2021.00845; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054; Sheng HL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2723, DOI 10.1109/ICCV48922.2021.00274; Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086; Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268; Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252; Sun Pei, 2021, CVPR; Sun SY, 2018, ADV NEUR IN, V31; Tengteng Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P35, DOI 10.1007/978-3-030-58555-6_3; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Vora S, 2020, PROC CVPR IEEE, P4603, DOI 10.1109/CVPR42600.2020.00466; Wang Y, 2020, ECCV; Wang Y., 2022, CORL; Wang Y, 2019, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2019.00864; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wang ZX, 2019, IEEE INT C INT ROBOT, P1742, DOI 10.1109/IROS40897.2019.8968513; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337; Yang B., 2018, CORL; Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798; Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204; Yang Zetong, 2021, CVPR; Jiang YQ, 2022, Arxiv, DOI arXiv:2206.15398; Ye MS, 2020, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR42600.2020.00170; Yilun Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12533, DOI 10.1109/CVPR42600.2020.01255; Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161; Liu YF, 2022, Arxiv, DOI arXiv:2206.01256; Li YH, 2022, Arxiv, DOI arXiv:2206.10092; You Y., 2020, ICLR; Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105; Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zhou Yin, 2020, CORL	82	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01710-9	http://dx.doi.org/10.1007/s11263-022-01710-9		NOV 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6M2MR		Green Submitted, hybrid			2022-12-18	WOS:000888708600002
J	Yuan, JK; Ma, X; Chen, DF; Kuang, K; Wu, F; Lin, LF				Yuan, Junkun; Ma, Xu; Chen, Defang; Kuang, Kun; Wu, Fei; Lin, Lanfen			Domain-Specific Bias Filtering for Single Labeled Domain Generalization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Domain generalization; Visual recognition; Single labeled multi-source data; Bias filtering; Semantic feature projection	ADAPTATION	Conventional Domain Generalization (CDG) utilizes multiple labeled source datasets to train a generalizable model for unseen target domains. However, due to expensive annotation costs, the requirements of labeling all the source data are hard to be met in real-world applications. In this paper, we investigate a Single Labeled Domain Generalization (SLDG) task with only one source domain being labeled, which is more practical and challenging than the CDG task. A major obstacle in the SLDG task is the discriminability-generalization bias: the discriminative information in the labeled source dataset may contain domain-specific bias, constraining the generalization of the trained model. To tackle this challenging task, we propose a novel framework called Domain-Specific Bias Filtering (DSBF), which initializes a discriminative model with the labeled source data and then filters out its domain-specific bias with the unlabeled source data for generalization improvement. We divide the filtering process into (1) feature extractor debiasing via k-means clustering-based semantic feature re-extraction and (2) classifier rectification through attention-guided semantic feature projection. DSBF unifies the exploration of the labeled and the unlabeled source data to enhance the discriminability and generalization of the trained model, resulting in a highly generalizable model. We further provide theoretical analysis to verify the proposed domain-specific bias filtering process. Extensive experiments on multiple datasets show the superior performance of DSBF in tackling both the challenging SLDG task and the CDG task.	[Yuan, Junkun; Ma, Xu; Chen, Defang; Kuang, Kun; Wu, Fei; Lin, Lanfen] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China; [Wu, Fei] Zhejiang Univ, Shanghai Inst Adv Study, Shanghai, Peoples R China; [Wu, Fei] Shanghai AI Lab, Shanghai, Peoples R China	Zhejiang University; Zhejiang University	Kuang, K (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.	yuanjk@zju.edu.cn; maxu@zju.edu.cn; defchem@zju.edu.cn; kunkuang@zju.edu.cn; wufei@zju.edu.cn; llf@zju.edu.cn			National Key Research and Development Program of China [2021YFC3340300]; National Natural Science Foundation of China [U20A20387, 62006207, 62037001]; Young Elite Scientists Sponsorship Program by CAST [2021QNRC001]; Shanghai AI Laboratory [P22KS00111]; Zhejiang University Shanghai Institute for Advanced Study [SN-ZJU-SIAS-0010]; Natural Science Foundation of Zhejiang Province [LZ22F020012, LQ21F020020]; Fundamental Research Funds for the Central Universities [226-2022-00142, 226-2022-00051]; National Key Research and Development Project [2022YFC2504605]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Young Elite Scientists Sponsorship Program by CAST; Shanghai AI Laboratory; Zhejiang University Shanghai Institute for Advanced Study; Natural Science Foundation of Zhejiang Province(Natural Science Foundation of Zhejiang Province); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Key Research and Development Project	This work was supported in part by National Key Research and Development Program of China (2021YFC3340300), National Natural Science Foundation of China (U20A20387, Nos. 62006207, 62037001), Young Elite Scientists Sponsorship Program by CAST(2021QNRC001), Project by Shanghai AI Laboratory (P22KS00111), the Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study (SN-ZJU-SIAS-0010), Natural Science Foundation of Zhejiang Province (LZ22F020012, LQ21F020020), Fundamental Research Funds for the Central Universities (226-2022-00142, 226-2022-00051), National Key Research and Development Project (2022YFC2504605).	Balaji Y, 2018, ADV NEUR IN, V31; Bellitto G, 2021, INT J COMPUT VISION, V129, P3216, DOI 10.1007/s11263-021-01519-y; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Blanchard Gilles, 2011, NIPS, V24, P3; Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Chen YH, 2021, INT J COMPUT VISION, V129, P2223, DOI 10.1007/s11263-021-01447-x; Chen ZL, 2019, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2019.00235; D'Innocente Antonio, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P187, DOI 10.1007/978-3-030-12939-2_14; Dai DX, 2020, INT J COMPUT VISION, V128, P1182, DOI 10.1007/s11263-019-01182-4; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Ding ZM, 2018, IEEE T IMAGE PROCESS, V27, P304, DOI 10.1109/TIP.2017.2758199; Dou Q, 2019, ADV NEUR IN, V32; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fengchun Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12553, DOI 10.1109/CVPR42600.2020.01257; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Ganin Y, 2016, J MACH LEARN RES, V17; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Gholami B, 2020, IEEE T IMAGE PROCESS, V29, P3993, DOI 10.1109/TIP.2019.2963389; Gong B., 2013, ADV NEURAL INFORM PR; Gong BQ, 2014, INT J COMPUT VISION, V109, P3, DOI 10.1007/s11263-014-0718-4; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoffman J, 2014, INT J COMPUT VISION, V109, P28, DOI 10.1007/s11263-014-0719-3; Hoffman J, 2012, LECT NOTES COMPUT SC, V7573, P702, DOI 10.1007/978-3-642-33709-3_50; Huang Y, 2021, INT J COMPUT VISION, V129, P2244, DOI 10.1007/s11263-021-01474-8; Huanhuan Yu, 2018, Arxiv, DOI arXiv:1809.00852; Ho HT, 2014, INT J COMPUT VISION, V109, P110, DOI 10.1007/s11263-014-0720-x; Zhou KY, 2021, Arxiv, DOI arXiv:2106.00592; Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33; Kan MN, 2014, INT J COMPUT VISION, V109, P94, DOI 10.1007/s11263-013-0693-1; Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503; Kundu JN, 2020, PROC CVPR IEEE, P4543, DOI 10.1109/CVPR42600.2020.00460; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566; Li Haoliang, 2020, ADV NEURAL INFORM PR; Li H, 2020, LEUKEMIA, V34, P1503, DOI [10.1038/s41375-020-0848-3, 10.1007/s11263-020-01364-5]; Li R, 2020, IEEE T IMAGE PROCESS, V29, P7997, DOI 10.1109/TIP.2020.3009853; Li YD, 2020, IEEE T IMAGE PROCESS, V29, P6110, DOI 10.1109/TIP.2020.2988175; Liang J., 2020, INT C MACH LEARN ICM; Lin S, 2021, IEEE T IMAGE PROCESS, V30, P1596, DOI 10.1109/TIP.2020.3046864; Long MS, 2018, ADV NEUR IN, V31; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2017, PR MACH LEARN RES, V70; Mancini M., 2019, IEEE T PATTERN ANAL; Mancini M, 2019, PROC CVPR IEEE, P6561, DOI 10.1109/CVPR.2019.00673; Matsuura T, 2020, AAAI CONF ARTIF INTE, V34, P11749; Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149; Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI; Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392; Schmidhuber J, 1987, THESIS; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Seonguk Seo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P68, DOI 10.1007/978-3-030-58542-6_5; Shankar S., 2018, INT C LEARN REPR; Shen ZQ, 2021, INT J COMPUT VISION, V129, P761, DOI 10.1007/s11263-020-01394-z; Sindagi V, 2017, INT J COMPUT VISION, V122, P193, DOI 10.1007/s11263-016-0953-y; Sohn Kihyuk, 2020, ADV NEURAL INFORM PR, P4; Tarvainen A, 2017, ADV NEUR IN, V30; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; VAPNIK V, 1992, ADV NEUR IN, V4, P831; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Volpi R, 2018, ADV NEUR IN, V31; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang JH, 2021, IEEE T IMAGE PROCESS, V30, P5505, DOI 10.1109/TIP.2021.3084354; Wang S., 2020, P EUROPEAN C COMPUTE; Wang X, 2021, IEEE T IMAGE PROCESS, V30, P1639, DOI 10.1109/TIP.2020.3044220; Wang YX, 2021, IEEE T IMAGE PROCESS, V30, P892, DOI 10.1109/TIP.2020.3031161; Wu ZX, 2019, IEEE I CONF COMP VIS, P2121, DOI 10.1109/ICCV.2019.00221; Yang XL, 2021, Arxiv, DOI arXiv:2103.00550; Xiong CM, 2014, AAAI CONF ARTIF INTE, P2860; Xu HQ, 2021, IEEE T IMAGE PROCESS, V30, P4516, DOI 10.1109/TIP.2021.3073285; Xu JL, 2016, INT J COMPUT VISION, V119, P159, DOI 10.1007/s11263-016-0885-6; Yamada M, 2014, INT J COMPUT VISION, V109, P126, DOI 10.1007/s11263-013-0689-x; Yasarla R, 2021, IEEE T IMAGE PROCESS, V30, P6570, DOI 10.1109/TIP.2021.3096323; Yuan J., 2021, ARXIV; Yuan J., 2021, ARXIV; Zeyi Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P124, DOI 10.1007/978-3-030-58536-5_8; Zhang C, 2020, ADV NEUR IN, V33; Zhang H, 2019, PR MACH LEARN RES, V97; Zhang K, 2015, AAAI CONF ARTIF INTE, P3150; Zhang YF, 2020, IEEE T IMAGE PROCESS, V29, P7834, DOI 10.1109/TIP.2020.3006377; Zhang YC, 2019, PR MACH LEARN RES, V97; Zhao H, 2018, ADV NEUR IN, V31; Zhao Shengyu, 2020, ADV NEURAL INFORM PR; Zhao SC, 2021, INT J COMPUT VISION, V129, P2399, DOI 10.1007/s11263-021-01479-3; Zheng ZD, 2021, INT J COMPUT VISION, V129, P1106, DOI 10.1007/s11263-020-01395-y; Zhou K., 2021, INT C LEARNING REPRE; Zhou KY, 2021, IEEE T IMAGE PROCESS, V30, P8008, DOI 10.1109/TIP.2021.3112012; Ziwei Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12403, DOI 10.1109/CVPR42600.2020.01242; Zuo YK, 2021, IEEE T IMAGE PROCESS, V30, P3793, DOI 10.1109/TIP.2021.3065254	94	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01712-7	http://dx.doi.org/10.1007/s11263-022-01712-7		NOV 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6M2MR		Green Submitted			2022-12-18	WOS:000888708600001
J	Patel, M; Gu, YW; Carstensen, LC; Hasselmo, ME; Betke, M				Patel, Mahir; Gu, Yiwen; Carstensen, Lucas C.; Hasselmo, Michael E.; Betke, Margrit			Animal Pose Tracking: 3D Multimodal Dataset and Token-based Pose Optimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Animal video dataset; Pose estimation; Tracking; Optimization; Thermal infrared; Multimodal		Accurate tracking of the 3D pose of animals from video recordings is critical for many behavioral studies, yet there is a dearth of publicly available datasets that the computer vision community could use for model development. We here introduce the Rodent3D dataset that records animals exploring their environment and/or interacting with each other with multiple cameras and modalities (RGB, depth, thermal infrared). Rodent3D consists of 200 min of multimodal video recordings from up to three thermal and three RGB-D synchronized cameras (approximately 4 million frames). For the task of optimizing estimates of pose sequences provided by existing pose estimation methods, we provide a baseline model called OptiPose. While deep-learned attention mechanisms have been used for pose estimation in the past, with OptiPose, we propose a different way by representing 3D poses as tokens for which deep-learned context models pay attention to both spatial and temporal keypoint patterns. Our experiments show how OptiPose is highly robust to noise and occlusion and can be used to optimize pose sequences provided by state-of-the-art models for animal pose estimation.	[Patel, Mahir; Gu, Yiwen; Carstensen, Lucas C.; Betke, Margrit] Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA; [Hasselmo, Michael E.; Betke, Margrit] Boston Univ, Ctr Syst Neurosci, Boston, MA 02215 USA	Boston University; Boston University	Patel, M (corresponding author), Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA.	mahirp@bu.edu			ONR MURI grant [N00014-19-1-2571];  [AUSMURIB000001]	ONR MURI grant; 	This work has been partially supported by ONR MURI grant N00014-19-1-2571 associated with AUSMURIB000001.	Alexander AS, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aaz2322; Biggs Benjamin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P195, DOI 10.1007/978-3-030-58621-8_12; Hu B, 2021, Arxiv, DOI arXiv:2106.09251; Breslav M, 2016, IEEE WINT CONF APPL; Carstensen LC, 2021, ISCIENCE, V24, DOI 10.1016/j.isci.2021.103377; Cheng Y, 2020, AAAI CONF ARTIF INTE, V34, P10631; Dannenberg H, 2020, ELIFE, V9, DOI 10.7554/eLife.62500; Dunn TW, 2021, NAT METHODS, V18, P564, DOI 10.1038/s41592-021-01106-6; Gong KH, 2021, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR46437.2021.00847; Gosztolai A., 2021, BIORXIV; Graving JM, 2019, ELIFE, V8, DOI 10.7554/eLife.47994; Gunel S, 2019, ELIFE, V8, DOI 10.7554/eLife.48571; Hoydal OA, 2019, NATURE, V568, P400, DOI 10.1038/s41586-019-1077-7; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Jiteng Mu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12383, DOI 10.1109/CVPR42600.2020.01240; Joska D., 2021, ARXIV; Karashchuk P, 2021, CELL REP, V36, DOI 10.1016/j.celrep.2021.109730; Kearney Sinead, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8333, DOI 10.1109/CVPR42600.2020.00836; Kingma DP, 2017, P INT C LEARN REPR I; Lauer J., 2021, BIORXIV; Li C, 2021, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR46437.2021.00153; Li S., 2020, P IEEECVF C COMPUTER, P13158; Li W., 2022, IEEE T MULTIMEDIA, DOI [10.1109/TMM.2022.3141231, DOI 10.1109/TMM.2022.3141231]; Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199; Liu X, 2020, J GENET GENOMICS, V47, P119, DOI 10.1016/j.jgg.2020.02.001; Marshall J. D., 2021, BIORXIV; Marshall J. D., 2021, RAT 7M, DOI [10.6084/m9.figshare.c.5295370.v3, DOI 10.6084/M9.FIGSHARE.C.5295370.V3]; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Mathis A, 2020, NEURON, V108, P44, DOI 10.1016/j.neuron.2020.09.017; Mathis A, 2018, NAT NEUROSCI, V21, P1281, DOI 10.1038/s41593-018-0209-y; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Monsees A., 2021, BIORXIV; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Nath T., 2018, BIORXIV, DOI [10.1101/476531, DOI 10.1101/476531]; O'Keefe J, 2005, HIPPOCAMPUS, V15, P853, DOI 10.1002/hipo.20115; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Pereira TD, 2019, NAT METHODS, V16, P117, DOI 10.1038/s41592-018-0234-5; Ramdya Pavan P, 2019, HarvardDataverse, V2, DOI 10.7910/DVN/PKKXOE; Raudies F, 2015, BRAIN RES, V1621, P355, DOI 10.1016/j.brainres.2014.10.053; Rempe D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11468, DOI 10.1109/ICCV48922.2021.01129; Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0; Shuai Hui, 2022, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2022.3188716; Theriault DH, 2014, J EXP BIOL, V217, P1843, DOI 10.1242/jeb.100529; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wah C., 2011, TECH REP; Wu Z, 2016, COMPUT VIS IMAGE UND, V143, P25, DOI 10.1016/j.cviu.2015.10.006; Wu Z, 2011, PROC CVPR IEEE, P1185, DOI 10.1109/CVPR.2011.5995515; Yuan Y, 2021, PROC CVPR IEEE, P7155, DOI 10.1109/CVPR46437.2021.00708; Zhang L, 2021, PR MACH LEARN RES, V130; Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145; Zuffi S, 2017, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2017.586	54	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01714-5	http://dx.doi.org/10.1007/s11263-022-01714-5		NOV 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6L0JL		hybrid			2022-12-18	WOS:000887876200001
J	Brissman, E; Johnander, J; Danelljan, M; Felsberg, M				Brissman, Emil; Johnander, Joakim; Danelljan, Martin; Felsberg, Michael			Recurrent Graph Neural Networks for Video Instance Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Detection; Tracking; Segmentation; Video		Video instance segmentation is one of the core problems in computer vision. Formulating a purely learning-based method, which models the generic track management required to solve the video instance segmentation task, is a highly challenging problem. In this work, we propose a novel learning framework where the entire video instance segmentation problem is modeled jointly. To this end, we design a graph neural network that in each frame jointly processes all detections and a memory of previously seen tracks. Past information is considered and processed via a recurrent connection. We demonstrate the effectiveness of the proposed approach in comprehensive experiments. Our approach operates online at over 25 FPS and obtains 16.3 AP on the challenging OVIS benchmark, setting a new state-of-the-art. We further conduct detailed ablative experiments that validate the different aspects of our approach. Code is available at https://github.com/emibr948/RGNNVIS-PlusPlus.	[Brissman, Emil; Johnander, Joakim; Felsberg, Michael] Linkoping Univ, Dept Elect Engn, Comp Vis Lab, Linkoping, Sweden; [Felsberg, Michael] Univ KwaZulu Natal, Sch Engn, Durban, South Africa; [Danelljan, Martin] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland; [Brissman, Emil] Saab, Linkoping, Sweden; [Johnander, Joakim] Zenseact, Gothenburg, Sweden	Linkoping University; University of Kwazulu Natal; Swiss Federal Institutes of Technology Domain; ETH Zurich; Saab Group	Johnander, J (corresponding author), Linkoping Univ, Dept Elect Engn, Comp Vis Lab, Linkoping, Sweden.; Johnander, J (corresponding author), Zenseact, Gothenburg, Sweden.	emil.brissman@liu.se; joakim.johnander@liu.se; martin.danelljan@vision.ee.ethz.ch; michael.felsberg@liu.se			Wallenberg Artificial Intelligence, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation; Excellence Center at Linkoping-Lund in Information Technology (ELLIT)	Wallenberg Artificial Intelligence, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation; Excellence Center at Linkoping-Lund in Information Technology (ELLIT)	y This work was partially supported by the Wallenberg Artificial Intelligence, Autonomous Systems and Software Program (WASP) funded by Knut and Alice Wallenberg Foundation; and the Excellence Center at Linkoping-Lund in Information Technology (ELLIT); and the computations were enabled by the Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the National Supercomputer Centre.	Adam Santoro, 2018, Arxiv, DOI arXiv:1806.01261; Alexander Kirillov, 2020, Arxiv, DOI arXiv:2005.12872; [Anonymous], 2007, CONJUGATE BAYESIAN A; Athar Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P158, DOI 10.1007/978-3-030-58621-8_10; Athar A, 2021, IEEE INT CONF COMP V, P3851, DOI 10.1109/ICCVW54120.2021.00431; Benenson R, 2019, PROC CVPR IEEE, P11692, DOI 10.1109/CVPR.2019.01197; Berg A, 2019, IEEE INT CONF COMP V, P2242, DOI 10.1109/ICCVW.2019.00277; Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464; Bertasius Gedas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9736, DOI 10.1109/CVPR42600.2020.00976; Bertasius G, 2018, LECT NOTES COMPUT SC, V11216, P342, DOI 10.1007/978-3-030-01258-8_21; Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925; Burghardt T, 2006, IEE P-VIS IMAGE SIGN, V153, P305, DOI 10.1049/ip-vis:20050052; Cao J., 2020, P EUROPEAN C COMPUTE; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Fang YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6890, DOI 10.1109/ICCV48922.2021.00683; Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Honghui Shi, 2016, Arxiv, DOI arXiv:1602.08465; Hwang S., 2021, ADV NEURAL INFORM PR, V34, P13352; Izquierdo R, 2019, IEEE INT C INTELL TR, P3114, DOI 10.1109/ITSC.2019.8917433; Johnander J., 2021, DAGM GERMAN C PATTER, P206; Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z; Li Minghan, 2021, CVPR; Li Z., 2021, P IEEE CVF INT C COM, P3854; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Luiten J, 2020, IEEE WINT CONF APPL, P1989, DOI 10.1109/WACV45572.2020.9093285; Luiten J, 2019, IEEE INT CONF COMP V, P709, DOI 10.1109/ICCVW.2019.00088; Meng DP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3631, DOI 10.1109/ICCV48922.2021.00363; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Qi Jiyang, 2021, ARXIV210201558; Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; TJampens R., 2016, 2016 6 INT C IMAGE P, P1; Vaswani A, 2017, ADV NEUR IN, V30; Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971; Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863; Wojke N, 2017, IEEE IMAGE PROC, P3645; Wu J., 2021, P IEEECVF C COMPUTER, P12352; Yang L., 2018, IEEE C COMPUTER VISI; Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529; Yang Shusheng, 2021, ICCV, P8043; Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271; Zhu X., 2020, ARXIV201004159	50	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01703-8	http://dx.doi.org/10.1007/s11263-022-01703-8		NOV 2022	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6H1VP		hybrid			2022-12-18	WOS:000885236800001
J	Zhang, LB; Gao, JY; Xiao, Z; Fan, H				Zhang, Libo; Gao, Junyuan; Xiao, Zhen; Fan, Heng			AnimalTrack: A Benchmark for Multi-Animal Tracking in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Tracking; Multi-object tracking (MOT); Multi-animal tracking (MAT); AnimalTrack; Tracking evaluation		Multi-animal tracking (MAT), a multi-object tracking (MOT) problem, is crucial for animal motion and behavior analysis and has many crucial applications such as biology, ecology and animal conservation. Despite its importance, MAT is largely under-explored compared to other MOT problems such as multi-human tracking due to the scarcity of dedicated benchmarks. To address this problem, we introduce AnimalTrack, a dedicated benchmark for multi-animal tracking in the wild. Specifically, AnimalTrack consists of 58 sequences from a diverse selection of 10 common animal categories. On average, each sequence comprises of 33 target objects for tracking. In order to ensure high quality, every frame in AnimalTrack is manually labeled with careful inspection and refinement. To our best knowledge, AnimalTrack is the first benchmark dedicated to multi-animal tracking. In addition, to understand how existing MOT algorithms perform on AnimalTrack and provide baselines for future comparison, we extensively evaluate 14 state-of-the-art representative trackers. The evaluation results demonstrate that, not surprisingly, most of these trackers become degenerated due to the differences between pedestrians and animals in various aspects (e.g., pose, motion, and appearance), and more efforts are desired to improve multi-animal tracking. We hope that AnimalTrack together with evaluation and analysis will foster further progress on multi-animal tracking. The dataset and evaluation as well as our analysis will be made available upon the acceptance.	[Zhang, Libo; Gao, Junyuan; Xiao, Zhen] Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing, Peoples R China; [Zhang, Libo; Gao, Junyuan] Univ Chinese Acad Sci, Hangzhou Inst Adv Study, Hangzhou, Peoples R China; [Zhang, Libo] Nanjing Inst Software Technol, Nanjing, Peoples R China; [Fan, Heng] Univ North Texas, Dept Comp Sci & Engn, Denton, TX 76203 USA	Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; University of North Texas System; University of North Texas Denton	Fan, H (corresponding author), Univ North Texas, Dept Comp Sci & Engn, Denton, TX 76203 USA.	libo@iscas.ac.cn; 2018091621016@uestc.edu.cn; isrc_exam@iscas.ac.cn; heng.fan@unt.edu			Key Research Program of Frontier Sciences, CAS [ZDBSLY-JSC038]; Youth Innovation Promotion Association, CAS [2020111]	Key Research Program of Frontier Sciences, CAS; Youth Innovation Promotion Association, CAS	Libo Zhang was supported by the Key Research Program of Frontier Sciences, CAS, Grant No. ZDBSLY-JSC038, CAAI-Huawei MindSpore Open Fund and Youth Innovation Promotion Association, CAS (2020111).	Anton Milan, 2016, Arxiv, DOI arXiv:1603.00831; Bai H., 2021, IEEE INT C COMPUTER; Bala PC, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-18441-5; Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Betke M, 2007, PROC CVPR IEEE, P192; Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003; Bochinski E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS); Cao JK, 2019, IEEE I CONF COMP VIS, P9497, DOI 10.1109/ICCV.2019.00959; Chu P, 2019, IEEE WINT CONF APPL, P161, DOI 10.1109/WACV.2019.00023; Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023; Dai P, 2021, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR46437.2021.00247; Dave Achal, 2020, ECCV; Dendorfer P., 2020, ARXIV; Du DW, 2018, LECT NOTES COMPUT SC, V11214, P375, DOI 10.1007/978-3-030-01249-6_23; Emami P, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3394659; Ferryman J., 2009, PETS WORKSHOP; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Guo S, 2021, PROC CVPR IEEE, P8132, DOI 10.1109/CVPR46437.2021.00804; He K., 2016, P CVPR, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Iwashita Y, 2014, INT C PATT RECOG, P4310, DOI 10.1109/ICPR.2014.739; Khan Z, 2004, LECT NOTES COMPUT SC, V2034, P279; Leal-Taixe L., 2015, ARXIV; Li SY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2590, DOI 10.1145/3394171.3413569; Liang C., 2022, ASS ADVANCEMENT ARTI; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Luo WH, 2021, ARTIF INTELL, V293, DOI 10.1016/j.artint.2020.103448; Mathis A, 2021, IEEE WINT CONF APPL, P1858, DOI 10.1109/WACV48630.2021.00190; Meinhardt T., 2022, P IEEECVF C COMPUTER, P8844; Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023; Parham J, 2018, IEEE WINT CONF APPL, P1075, DOI 10.1109/WACV.2018.00123; Sun PZ, 2020, Arxiv, DOI arXiv:2012.15460; Peng J., 2020, EUROPEAN C COMPUTER; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schulter S, 2017, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2017.292; Shuai B, 2021, PROC CVPR IEEE, P12367, DOI 10.1109/CVPR46437.2021.01219; Sun P., 2022, IEEE INT C COMPUTER; Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A., 2017, C NEURAL INFORM PROC; Voigtlaender P., 2019, IEEE INT C COMPUTER; Wen LY, 2020, COMPUT VIS IMAGE UND, V193, DOI 10.1016/j.cviu.2020.102907; Wojke N, 2017, IEEE IMAGE PROC, P3645; Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28; Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409; Xu YH, 2020, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR42600.2020.00682; Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529; Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680; Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271; Yu H., 2021, C WORKSHOP NEURAL IN; Zhang Y., 2021, ARXIV; Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4; Zhu J., 2018, EUROPEAN C COMPUTER; Zhu PF, 2022, IEEE T PATTERN ANAL, V44, P7380, DOI 10.1109/TPAMI.2021.3119563	65	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01711-8	http://dx.doi.org/10.1007/s11263-022-01711-8		NOV 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6H1VP		Green Submitted			2022-12-18	WOS:000885236800002
J	Zhang, HG; Zhang, LM; Dai, YC; Li, HD; Koniusz, P				Zhang, Hongguang; Zhang, Limeng; Dai, Yuchao; Li, Hongdong; Koniusz, Piotr			Event-guided Multi-patch Network with Self-supervision for Non-uniform Motion Deblurring	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Motion deblurring; Deep learning; Multi-patch; Event camera; Self-supervision		Contemporary deep learning multi-scale deblurring models suffer from many issues: (I) They perform poorly on non-uniformly blurred images/videos; (II) Simply increasing the model depth with finer-scale levels cannot improve deblurring; (III) Individual RGB frames contain a limited motion information for deblurring; (IV) Previous models have a limited robustness to spatial transformations and noise. Below, we propose several mechanisms based on the multi-patch network to address the above issues: (I) We present a novel self-supervised event-guided deep hierarchical Multi-patch Network (MPN) to deal with blurry images and videos via fine-to-coarse hierarchical localized representations; (II) We propose a novel stacked pipeline, StackMPN, to improve the deblurring performance under the increased network depth; (III) We propose an event-guided architecture to exploit motion cues contained in videos to tackle complex blur in videos; (IV) We propose a novel self-supervised step to expose the model to random transformations (rotations, scale changes), and make it robust to Gaussian noises. Our MPN achieves the state of the art on the GoPro and VideoDeblur datasets with a 40x faster runtime compared to current multi-scale methods. With 30 ms to process an image at 1280x720 resolution, it is the first real-time deep motion deblurring model for 720p images at 30 fps. For StackMPN, we obtain significant improvements over 1.2 dB on the GoPro dataset by increasing the network depth. Utilizing the event information and self-supervision further boost results to 33.83 dB.	[Zhang, Hongguang] AMS, Syst Engn Inst, Beijing 100141, Peoples R China; [Zhang, Limeng] Shanghai Jiao Tong Univ, Shanghai, Peoples R China; [Dai, Yuchao] Northwestern Polytech Univ, Xian, Peoples R China; [Li, Hongdong; Koniusz, Piotr] Australian Natl Univ, Canberra, ACT, Australia; [Koniusz, Piotr] CSIRO, Data61, Canberra, ACT, Australia	Shanghai Jiao Tong University; Northwestern Polytechnical University; Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO)	Zhang, HG (corresponding author), AMS, Syst Engn Inst, Beijing 100141, Peoples R China.	zhang.hongguang@outlook.com; piotr.koniusz@data61.csiro.au			Natural Science Foundation of China [62106282, 62271410, 61871325]; Beijing Nova Program	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission)	This research is supported by the Natural Science Foundation of China (Grant Nos. 62106282, 62271410, 61871325), and Beijing Nova Program.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102; Bharath Hariharan, 2019, Arxiv, DOI arXiv:1906.07079; Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715; Bursuc A., 2019, ARXIV; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Das SD, 2020, IEEE COMPUT SOC CONF, P1994, DOI 10.1109/CVPRW50498.2020.00249; Delbracio M, 2015, IEEE T COMPUT IMAG, V1, P270, DOI 10.1109/TCI.2015.2501245; Dmytro Mishkin, 2018, Arxiv, DOI arXiv:1711.07064; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Dongwon Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P327, DOI 10.1007/978-3-030-58539-6_20; Dosovitskiy A, 2014, ADV NEUR IN, V27; Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607; Gan C, 2018, PROC CVPR IEEE, P5589, DOI 10.1109/CVPR.2018.00586; Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Jia J., 2014, MATH MODELS PRACTICA, P1; Jia J., 2007, IEEE C COMP VIS PATT, P1; Jiang Z, 2020, PROC CVPR IEEE, P3317, DOI 10.1109/CVPR42600.2020.00338; Kim TH, 2015, PROC CVPR IEEE, P5426, DOI 10.1109/CVPR.2015.7299181; Kingma D.P, P 3 INT C LEARNING R; Koniusz P, 2018, PROC CVPR IEEE, P5774, DOI 10.1109/CVPR.2018.00605; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lazebnik S, 2006, COMPUTER VISION PATT, P2169, DOI [10.1109/CVPR.2006.68, DOI 10.1109/CVPR.2006.68]; Levin A., 2007, ADV NEURAL INFORM PR, P841; Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337; Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119; Miyatani Y, 2016, IEEE WINT CONF APPL; Munda G, 2018, INT J COMPUT VISION, V126, P1381, DOI 10.1007/s11263-018-1106-2; Nah S, 2019, PROC CVPR IEEE, P8094, DOI 10.1109/CVPR.2019.00829; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Nikos Komodakis, 2018, Arxiv, DOI arXiv:1803.07728; Nimisha TM, 2017, IEEE I CONF COMP VIS, P4762, DOI 10.1109/ICCV.2017.509; Pan JS, 2020, PROC CVPR IEEE, P3040, DOI 10.1109/CVPR42600.2020.00311; Pan LY, 2019, PROC CVPR IEEE, P6813, DOI 10.1109/CVPR.2019.00698; Pan LY, 2017, PROC CVPR IEEE, P6987, DOI 10.1109/CVPR.2017.739; Rajagopalan A.N., 2014, MOTION DEBLURRING AL; Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398; Rebecq Henri, 2018, C ROB LEARN, P2; Scheerlinck C, 2019, LECT NOTES COMPUT SC, V11365, P308, DOI 10.1007/978-3-030-20873-8_20; Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418; Sellent A, 2016, LECT NOTES COMPUT SC, V9906, P558, DOI 10.1007/978-3-319-46475-6_35; Sermanet P, 2018, IEEE INT CONF ROBOT, P1134; Song C., 2022, P IEEECVF C COMPUTER, P7803; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366; Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; Tas Y., 2021, BRIT MACHINE VISION; Wang Bishan, 2020, ECCV, P155, DOI DOI 10.1007/978-3-030-58601-0_10; Wang L., 2021, ACM INT C MULTIMEDIA, DOI [10.1145/3474085.3475572, DOI 10.1145/3474085.3475572]; Wang L, 2019, IEEE I CONF COMP VIS, P8697, DOI 10.1109/ICCV.2019.00879; Wang Z., 2021, P IEEECVF INT C COMP, P448; Xiang XG, 2020, IEEE T IMAGE PROCESS, V29, P8976, DOI 10.1109/TIP.2020.3023534; Xu L, 2014, ADV NEUR IN, V27; Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157; Zhang H., 2020, EUROPEAN C COMPUTER; Zhang HG, 2021, PROC CVPR IEEE, P9427, DOI 10.1109/CVPR46437.2021.00931; Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613; Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267; Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281; Zhou SC, 2019, IEEE I CONF COMP VIS, P2482, DOI 10.1109/ICCV.2019.00257	62	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01708-3	http://dx.doi.org/10.1007/s11263-022-01708-3		NOV 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6E2PM					2022-12-18	WOS:000883224000001
J	Dworakowski, D; Fung, A; Nejat, G				Dworakowski, Daniel; Fung, Angus; Nejat, Goldie			Robots Understanding Contextual Information in Human-Centered Environments Using Weakly Supervised Mask Data Distillation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Weakly supervised learning for robots; Environment context identification; Segmentation and labeling; Robot navigation and exploration	ORIENTED TEXT; SEGMENTATION; RECOGNITION; ATTENTION	Contextual information contained within human environments, such as text on signs, symbols and objects provide important information for robots to use for exploration and navigation. To identify and segment contextual information from images obtained in these environments data-driven methods such as Convolutional Neural Networks (CNNs) can be used. However, these methods require significant amounts of human labeled data which is time-consuming to obtain. In this paper, we present the novel Weakly Supervised Mask Data Distillation (WeSuperMaDD) architecture for autonomously generating pseudo segmentation labels (PSLs) using CNNs not specifically trained for the task of text segmentation, e.g., CNNs alternatively trained for: object classification or image captioning. WeSuperMaDD is uniquely able to generate PSLs using learned image features from datasets that are sparse and with limited diversity, which are common in robot navigation tasks in human-centred environments (i.e., malls, stores). Our proposed architecture uses a new mask refinement system which automatically searches for the PSL with the fewest foreground pixels that satisfies cost constraints. This removes the need for handcrafted heuristic rules. Extensive experiments were conducted to validate the performance of WeSuperMaDD in generating PSLs for datasets containing text of various scales, fonts, orientations, curvatures, and perspectives in several indoor/outdoor environments. A detailed comparison study conducted with existing approaches found a significant improvement in PSL quality. Furthermore, an instance segmentation CNN trained using the WeSuperMaDD architecture achieved measurable improvements in accuracy when compared to an instance segmentation CNN trained with Naive PSLs. We also found our method to have comparable performance to existing text detection methods.	[Dworakowski, Daniel; Fung, Angus; Nejat, Goldie] Univ Toronto, Dept Mech & Ind Engn, Autonomous Syst & Biomechatron Lab ASBLab, 5 Kings Coll Rd, Toronto, ON M5S 3G8, Canada	University of Toronto	Dworakowski, D (corresponding author), Univ Toronto, Dept Mech & Ind Engn, Autonomous Syst & Biomechatron Lab ASBLab, 5 Kings Coll Rd, Toronto, ON M5S 3G8, Canada.	daniel.dworakowski@mail.utoronto.ca; angus.fung@mail.utoronto.ca; nejat@mie.utoronto.ca			AGE-WELL Inc.; Natural Sciences and Engineering Research Council of Canada (NSERC); Canada Research Chairs program (CRC); Vector Institute Scholarship in Artificial Intelligence; NVIDIA GPU grant; Longo Brothers Fruit Markets Inc.	AGE-WELL Inc.; Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canada Research Chairs program (CRC)(Canada Research ChairsAustralian GovernmentDepartment of Industry, Innovation and ScienceCooperative Research Centres (CRC) Programme); Vector Institute Scholarship in Artificial Intelligence; NVIDIA GPU grant; Longo Brothers Fruit Markets Inc.	This work was supported by AGE-WELL Inc., the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canada Research Chairs program (CRC), the Vector Institute Scholarship in Artificial Intelligence, the NVIDIA GPU grant, and Longo Brothers Fruit Markets Inc.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, OVERVIEW ICDAR2017 C; Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Baek J, 2019, IEEE I CONF COMP VIS, P4714, DOI 10.1109/ICCV.2019.00481; Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959; Barnes Dan, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P203, DOI 10.1109/ICRA.2017.7989025; Bellocchio E, 2019, IEEE ROBOT AUTOM LET, V4, P2348, DOI 10.1109/LRA.2019.2903260; Benenson R, 2019, PROC CVPR IEEE, P11692, DOI 10.1109/CVPR.2019.01197; Bojarski M, 2018, IEEE INT CONF ROBOT, P4701; Bonechi S, 2019, LECT NOTES COMPUT SC, V11729, P238, DOI 10.1007/978-3-030-30508-6_20; Case Carl, 2011, IEEE International Conference on Robotics and Automation, P3297; Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157; Chapelle Olivier, 2010, SEMISUPERVISED LEARN, V2, P5; Cleveland J, 2017, IEEE T AUTOM SCI ENG, V14, P820, DOI 10.1109/TASE.2016.2631085; De Gregorio D, 2020, IEEE T AUTOM SCI ENG, V17, P611, DOI 10.1109/TASE.2019.2938316; Deng LJ, 2019, IEEE ACCESS, V7, P153400, DOI 10.1109/ACCESS.2019.2948405; Deng LJ, 2019, NEUROCOMPUTING, V334, P134, DOI 10.1016/j.neucom.2019.01.013; Dworakowski D, 2021, ROBOTICS, V10, DOI 10.3390/robotics10040110; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fu C.Y., 2019, ARXIV; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang J.J., 2018, ARXIV; Ibrahim M. S., 2018, IEEECVF C COMPUTER V, P12715; Jaderberg M, 2014, ARXIV; Jain SD, 2013, IEEE I CONF COMP VIS, P1313, DOI 10.1109/ICCV.2013.166; Jing LL, 2020, IEEE T IMAGE PROCESS, V29, P225, DOI 10.1109/TIP.2019.2926748; Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942; Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221; Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181; Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42; Kotsiantis S., 2006, GESTS INT T COMPUTER, V30, P25, DOI DOI 10.1007/978-0-387-09823-4_45; Li GB, 2018, AAAI CONF ARTIF INTE, P7024; Liang HJ, 2019, IEEE T AUTOM SCI ENG, V16, P1619, DOI 10.1109/TASE.2019.2900980; Liao M., ARXIV; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu Jingchao, 2019, ARXIV190311800; Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595; Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788; Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5; Mahendran A, 2016, LECT NOTES COMPUT SC, V9910, P120, DOI 10.1007/978-3-319-46466-4_8; Massiceti D., 2017, INT WORKSHOP ENERGY, P263; Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127; Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237; Niu SL, 2019, IEEE INT CON AUTO SC, P127, DOI 10.1109/COASE.2019.8843204; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Overview-Focused Scene Text-Robust Reading Competition, ROBT READ COMP; Overview-Incidental scene text-robust reading competition, ROB READ COMP; Peng Z, 2018, IEEE T AUTOM SCI ENG, V15, P369, DOI 10.1109/TASE.2017.2761793; Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320; Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Ruder, 2017, ARXIV; Saleh FS, 2018, IEEE T PATTERN ANAL, V40, P1382, DOI 10.1109/TPAMI.2017.2713785; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shariati A, 2020, IEEE ROBOT AUTOM LET, V5, P1223, DOI 10.1109/LRA.2020.2967307; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Simonyan K., 2014, WORKSH INT C LEARN R, P1; Singh A, 2017, IEEE I CONF COMP VIS, P5852, DOI 10.1109/ICCV.2017.623; Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252; Thompson C., 2018, IEEE RSJ P WORKSH RO; Vardazaryan A, 2018, LECT NOTES COMPUT SC, V11043, P169, DOI 10.1007/978-3-030-01364-6_19; Vilar E, 2014, HUM FACTOR ERGON MAN, V24, P601, DOI 10.1002/hfm.20503; Wan F, 2019, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2019.00230; Wan F, 2018, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2018.00141; Wang BH, 2019, IEEE ROBOT AUTOM LET, V4, P2902, DOI 10.1109/LRA.2019.2922582; Wang C, 2021, IEEE T IMAGE PROCESS, V30, P8212, DOI 10.1109/TIP.2021.3113157; Wang HC, 2015, IEEE INT C INT ROBOT, P3701, DOI 10.1109/IROS.2015.7353895; Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404; Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687; Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759; Wellhausen L, 2019, IEEE ROBOT AUTOM LET, V4, P1509, DOI 10.1109/LRA.2019.2895390; Wu W., 2020, ARXIV; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10988, DOI 10.1109/CVPR42600.2020.01100; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zhang B., 2019, ARXIV; Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x; Zhao XY, 2018, PROC CVPR IEEE, P4061, DOI 10.1109/CVPR.2018.00427; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399; Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106	85	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01706-5	http://dx.doi.org/10.1007/s11263-022-01706-5		NOV 2022	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6C3CO		Green Submitted			2022-12-18	WOS:000881897100002
J	Lasaruk, A; Pajdla, T				Lasaruk, Aless; Pajdla, Tomas			An Efficient Model for a Camera Behind a Parallel Refractive Slab	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Camera model; Calibration; Windshield; Automotive; Open source	VIEW	We present a new and efficient solution for the forward world to image projection of a pinhole camera with distortions placed behind a planar refractive slab. Firstly, we introduce a novel way to compute the projection by reducing the problem to finding a real quantity called a slab shift. We characterize a physically meaningful slab shift as the unique solution of a fixed point equation on the one hand and as a specific uniquely defined root of a quartic polynomial in the unknown slab shift on the other. In the latter case we obtain a closed-form formula, which provides the unique physically meaningful projection. Secondly, we develop an approximation of the projection through the slab that reaches single-precision floating point accuracy for practically relevant problem instances with considerably lower computational costs compared to the exact solution. We demonstrate the accuracy and the efficiency of our method with realistic synthetic experiments. We demonstrate with real experiments that our method enables efficient camera calibration behind the windshield for automotive industry applications.	[Lasaruk, Aless] ZF Friedrichshafen, Friedrichshafen, Germany; [Pajdla, Tomas] Czech Tech Univ, Czech Inst Informat Robot & Cybernet, Prague 6, Czech Republic	ZF Friedrichshafen AG; Czech Technical University Prague	Lasaruk, A (corresponding author), ZF Friedrichshafen, Friedrichshafen, Germany.	aless.lasaruk@zf.com; pajdla@cvut.cz			EU RDF IMPACT [CZ.02.1.01/0.0/0.0/15 003/0000468]; EU [856994]	EU RDF IMPACT; EU(European Commission)	This work was supported by projects: EU RDF IMPACT No. CZ.02.1.01/0.0/0.0/15 003/0000468 and EU H2020 ARtwin No. 856994.	Agrawal A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2993, DOI 10.1109/CVPR.2011.5995596; Agrawal A, 2013, PROC CVPR IEEE, P1399, DOI 10.1109/CVPR.2013.184; Agrawal A, 2012, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2012.6248073; Agrawal A, 2010, LECT NOTES COMPUT SC, V6313, P129; Brousseau PA, 2019, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2019.00414; BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855; Chadebecq F, 2017, IEEE I CONF COMP VIS, P5325, DOI 10.1109/ICCV.2017.568; Chari V., 2009, BRIT MACHINE VISION, P1, DOI [10.5244/C.23.56, DOI 10.5244/C.23.56]; Chari V, 2013, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2013.189; Cox D. A., 2015, IDEALS VARIETIES ALG; Gao Y, 2018, IEEE T INTELL TRANSP, V19, P320, DOI 10.1109/TITS.2017.2750087; Glaeser G., 2000, J GEOM GRAPH, V4, P1; Hanel A., 2016, ISPRS INT ARCH PHOTO, VXLI-B5, P461, DOI [10.5194/isprs-archives-XLI-B5-461-2016, DOI 10.5194/ISPRS-ARCHIVES-XLI-B5-461-2016]; Hanel A., 2018, VEHITS; Haner S, 2015, PROC CVPR IEEE, P1428, DOI 10.1109/CVPR.2015.7298749; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hecht E., 2015, OPTICS; Hernandez-Juarez D, 2016, PROCEDIA COMPUT SCI, V80, P143, DOI 10.1016/j.procs.2016.05.305; Jordt-Sedlazeck A, 2013, IEEE I CONF COMP VIS, P57, DOI 10.1109/ICCV.2013.14; Kawahara R, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P819, DOI 10.1109/ICCVW.2013.112; Kukkala VK, 2018, IEEE CONSUM ELECTR M, V7, P18, DOI 10.1109/MCE.2018.2828440; Luczynski T, 2017, OCEAN ENG, V133, P9, DOI 10.1016/j.oceaneng.2017.01.029; Mei C, 2007, IEEE INT CONF ROBOT, P3945, DOI 10.1109/ROBOT.2007.364084; Meng D., 2018, THESIS U BOURGOGNE; Ponce J, 2017, ADV APPL MATH, V88, P62, DOI 10.1016/j.aam.2017.01.001; Ramalingam S, 2017, IEEE T PATTERN ANAL, V39, P1309, DOI 10.1109/TPAMI.2016.2592904; Rees EL., 1922, AM MATH MON, V29, P51, DOI [10.1080/00029890.1922.11986100, DOI 10.1080/00029890.1922.11986100]; Rudin W, 1976, PRINCIPLES MATH ANAL, V3rd; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Sturm P, 2010, FOUND TRENDS COMPUT, V6, P1, DOI 10.1561/0600000023; Taguchi Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866194; Thirthala S, 2005, IEEE I CONF COMP VIS, P1539; Thirthala S, 2005, PROC CVPR IEEE, P321; Thirthala S, 2012, INT J COMPUT VISION, V96, P195, DOI 10.1007/s11263-011-0463-x; Trager M., 2018, THESIS ECOLE NORMALE; Verbiest Frank, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P397, DOI 10.1007/978-3-030-58539-6_24; Yang TL, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8112118; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	38	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01691-9	http://dx.doi.org/10.1007/s11263-022-01691-9		NOV 2022	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	6C3CO					2022-12-18	WOS:000881897100001
J	Gao, ZT; Wang, LM; Wu, GS				Gao, Ziteng; Wang, Limin; Wu, Gangshan			LIP: Local Importance-Based Pooling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Convolutional neural networks; Spatial pooling layer; Image classification; Object detection; Instance segmentation; Semantic segmentation; Pose estimation		Spatial downsampling layers are favored in convolutional neural networks (CNNs) to downscale feature maps for larger receptive fields and less memory consumption. However, for visual recognition tasks, these layers might lose discriminative details due to improper pooling strategies. In this paper, we present a unified framework (LAN) over the common downsampling layers (e.g., average pooling, max pooling, and strided convolution) from a view of local aggregation based on importance. In this LAN framework, we analyze the issues of these widely-used pooling layers and figure out the criteria of designing an effective downsampling layer. Based on this analysis, we propose a simple, general, and effective pooling operation based on local importance modeling, termed as Local Importance-based Pooling (LIP). LIP is able to enhance discriminative features during the downsampling procedure by learning adaptive importance weights based on inputs. To further modulate different pooling windows for more effective pooling, we present the improved version of LIP, termed LIP++, by introducing an explicit margin term and efficient logit modules. Our LIP++ can yield consistent accuracy improvement over the original LIP yet with a smaller computational cost. Extensive experiments show that our presented LIP method consistently yields notable gains with different CNN architectures on the image classification task. In the challenging MS COCO dataset, detectors with our LIP-ResNets as backbones obtain a consistent performance improvement over the vanilla ResNets on both bounding box detection and instance segmentation. Finally, we also verify the effectiveness of LIP on the tasks of pose estimation and semantic segmentation, demonstrating its generalization to the dense prediction task.	[Gao, Ziteng; Wang, Limin; Wu, Gangshan] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China	Nanjing University	Wang, LM (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.	gzt@outlook.com; lmwang@nju.edu.cn; gswu@nju.edu.cn		Wang, Limin/0000-0002-3674-7718	National Natural Science Foundation of China [62076119, 61921006]; Fundamental Research Funds for the Central Universities [020214380091]; Collaborative Innovation Center of Novel Software Technology and Industrialization	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Collaborative Innovation Center of Novel Software Technology and Industrialization	This work is supported by the National Natural Science Foundation of China (No. 62076119, No. 61921006), the Fundamental Research Funds for the Central Universities (No. 020214380091), and Collaborative Innovation Center of Novel Software Technology and Industrialization.	Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13; Boureau Y.-L., 2010, ICML, P111, DOI DOI 10.5555/3104322.3104338; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen Z, 2020, IEEE WINT CONF APPL, P1169, DOI 10.1109/WACV45572.2020.9093418; Cheng BW, 2018, LECT NOTES COMPUT SC, V11219, P473, DOI 10.1007/978-3-030-01267-0_28; Chollet F., 2017, P 2017 IEEE C COMP V, P1800, DOI DOI 10.1109/CVPR.2017.195; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; De Brabandere B, 2016, ADV NEUR IN, V29; Deliege A., 2019, BMVC, P76; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; Gao ZT, 2019, IEEE I CONF COMP VIS, P3354, DOI 10.1109/ICCV.2019.00345; Goyal P., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1706.02677; Graham B., 2014, ARXIV; Gulcehre Caglar, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P530, DOI 10.1007/978-3-662-44848-9_34; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He K, 2016, 2016 IEEE C COMP VIS, DOI [10.1109/cvpr.2016.90, 10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065; Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jie H., 2017, P IEEE C COMP VIS PA, P99; Kobayashi T, 2019, IEEE I CONF COMP VIS, P3364, DOI 10.1109/ICCV.2019.00346; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lazebnik S, 2006, COMPUTER VISION PATT, P2169, DOI [10.1109/CVPR.2006.68, DOI 10.1109/CVPR.2006.68]; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee CY, 2016, JMLR WORKSH CONF PRO, V51, P464; Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801; Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5; Lin M, 2014, INT C LEARN REPR 201; Lin T.Y., 2019, 13 EUR C ZUR SWITZ S; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754; Luo WJ, 2016, ADV NEUR IN, V29; Palacio S, 2018, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2018.00328; Radosavovic Ilija, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10425, DOI 10.1109/CVPR42600.2020.01044; Ramachandran P, 2019, ADV NEUR IN, V32; Saeedan F, 2018, PROC CVPR IEEE, P9108, DOI 10.1109/CVPR.2018.00949; Simonyan K., 2015, ICLR; Simonyan K, 2014, ADV NEUR IN, V27; Singh B, 2018, ADV NEUR IN, V31; Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Stergiou A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10337, DOI 10.1109/ICCV48922.2021.01019; Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Ulyanov D., 2016, ARXIV160708022; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang JQ, 2019, IEEE I CONF COMP VIS, P3007, DOI 10.1109/ICCV.2019.00310; Wang L, 2017, SIGNAL PROCESS, V140, P45, DOI 10.1016/j.sigpro.2017.05.005; Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668; Wang LM, 2017, IEEE T IMAGE PROCESS, V26, P2055, DOI 10.1109/TIP.2017.2675339; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wei Z, 2019, PROC CVPR IEEE, P7108, DOI 10.1109/CVPR.2019.00728; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Xie GS, 2015, IEEE I CONF COMP VIS, P1179, DOI 10.1109/ICCV.2015.140; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu DJ, 2014, LECT NOTES ARTIF INT, V8818, P364, DOI 10.1007/978-3-319-11740-9_34; Zhai SF, 2017, PROC CVPR IEEE, P4003, DOI 10.1109/CVPR.2017.426; Zhang R, 2019, PR MACH LEARN RES, V97; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhao J., 2021, ICLR; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953; Zou Xueyan, 2020, BMVC	81	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01707-4	http://dx.doi.org/10.1007/s11263-022-01707-4		OCT 2022	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5V0IK		Green Submitted			2022-12-18	WOS:000876921900001
J	Wang, RQ; Liu, Z; Zhang, BC; Guo, GD; Doermann, D				Wang, Runqi; Liu, Zhen; Zhang, Baochang; Guo, Guodong; Doermann, David			Few-Shot Learning with Complex-Valued Neural Networks and Dependable Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Few-shot learning; Complex-valued CNNs; Dependable learning	VISUAL-ATTENTION; MODEL	We present a flexible, general framework for few-shot learning where both inter-class differences and intra-class relationships are fully considered to improve recognition performance significantly. We introduce complex-valued convolutional neural networks (CNNs) to describe the subtle difference among inter-class samples and Dependable Learning to capture the intra-class relationship. Conventional CNNs use only real-valued CNNs and fail to extract more detailed information. Complex-valued CNNs, on the other hand, can provide amplitude and phase information to enhance the feature representation ability based on the proposed complex metric module (CMM). Building upon the recent episodic training mechanism, CMMs can improve the representation capacity by extracting robust complex-valued features to facilitate the modeling of subtle relationships among few-shot samples. Furthermore, we use Dependable Learning as a new learning paradigm, to promote a robust model against perturbation based on a new bilinear optimization to enhance the feature extraction capacity for very few available intra-class samples. Experiments on two benchmark datasets show that the proposed methods significantly improve the performance over other approaches and achieve state-of-the-art results.	[Wang, Runqi; Liu, Zhen; Zhang, Baochang] Beihang Univ, Beijing, Peoples R China; [Guo, Guodong] Ant Grp, Beijing, Peoples R China; [Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing, Peoples R China; [Guo, Guodong] Natl Engn Lab Deep Learning Technol & Applicat, Beijing, Peoples R China; [Doermann, David] Univ Buffalo, Buffalo, NY USA	Beihang University; Baidu; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Zhang, BC (corresponding author), Beihang Univ, Beijing, Peoples R China.	runqiwang@buaa.edu.cn; liuzhenbuaa@buaa.edu.cn; bczhang@buaa.edu.cn; guoguodong01@baidu.com; doermann@buffalo.edu			Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China [62076016]; Beijing Natural Science Foundation-Xiaomi Innovation Joint Fund [L223024]	Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation-Xiaomi Innovation Joint Fund	This work was supported by "the Fundamental Research Funds for the Central Universities", and the National Natural Science Foundation of China under Grant 62076016, Beijing Natural Science Foundation-Xiaomi Innovation Joint Fund L223024	Amar D, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-020-20516-2; [Anonymous], 2014, EXPLAINING HARNESSIN; Arjovsky M, 2016, PR MACH LEARN RES, V48; Athalye A, 2018, PR MACH LEARN RES, V80; Bertinetto Luca, 2019, INT C LEARN REPR, P2; Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974; Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen MT, 2020, AAAI CONF ARTIF INTE, V34, P10559; Cubuk E.D., 2017, ICLR, P1; Danihelka I, 2016, PR MACH LEARN RES, V48; Das N., 2017, ARXIV, P1; Dhillon G.S., 2019, ICLR; Dong X., 2020, CVPR, P12895; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Fehervari I., 2019, UNBIASED EVALUATION, P1; Finn C, 2017, PR MACH LEARN RES, V70; Gidaris S, 2019, IEEE I CONF COMP VIS, P8058, DOI 10.1109/ICCV.2019.00815; Gidaris S, 2019, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2019.00011; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Gupta P, 2019, IEEE I CONF COMP VIS, P6707, DOI 10.1109/ICCV.2019.00681; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149; Hirose A, 2012, IEEE T NEUR NET LEAR, V23, P541, DOI 10.1109/TNNLS.2012.2183613; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kai Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13467, DOI 10.1109/CVPR42600.2020.01348; Kim J., 2020, COMPUTER VISIONECCV, P599; Kingma D, 2014, COMPUTER SCI; Koch Gregory, 2015, P ICML DEEP LEARN WO, V2; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Larochelle H., 2010, ADV NEURAL INFORM PR, P1243; Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li'an Zhuo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7956, DOI 10.1109/CVPR42600.2020.00798; Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191; Liu Y, 2016, ADVANCES OF ATOMS AND MOLECULES IN STRONG LASER FIELDS, P1; Liu YH, 2019, INT J PSYCHIAT CLIN, V23, P164, DOI 10.1080/13651501.2019.1569238; Liu Z., 2020, BMVC, P541; Madry Aleksander, 2018, ICLR; Mairal J, 2010, J MACH LEARN RES, V11, P19; Mishra N., 2018, INT C LEARN REPR; Mnih V, 2014, ADV NEUR IN, V27; Monning N., 2018, EVALUATION COMPLEX V, P1; Munkhdalai T., 2018, INT C MACH LEARN, P3664; Mustafa A, 2019, IEEE I CONF COMP VIS, P3384, DOI 10.1109/ICCV.2019.00348; Na T., 2017, ICLR, P1; Nichol A., 2018, ICML, P324; Nitta T, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P1099; OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700; Oreshkin BN, 2018, ADV NEUR IN, V31; Petersen K.B., 2008, TECH U DENMARK, V7, P15; Qiao LM, 2019, IEEE I CONF COMP VIS, P3602, DOI 10.1109/ICCV.2019.00370; Rassadin Alexandr, 2020, Image Analysis and Recognition. 17th International Conference (ICIAR 2020). Proceedings. Lecture Notes in Computer Science (LNCS 12132), P419, DOI 10.1007/978-3-030-50516-5_37; Ravi S., 2017, P INT C LEARN REPR, P1; Ravichandran A, 2019, IEEE I CONF COMP VIS, P331, DOI 10.1109/ICCV.2019.00042; Reichert D.P, 2014, ICLR, P1; Rizve MN, 2021, PROC CVPR IEEE, P10831, DOI 10.1109/CVPR46437.2021.01069; Wang RQ, 2022, Arxiv, DOI arXiv:2208.12967; Rusu Andrei A, 2019, ICLR; Shafahi A, 2019, ADV NEUR IN, V32; Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419; Simonyan K., 2015, ICLR; Snell J, 2017, ADV NEUR IN, V30; Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Szegedy C, 2013, 2 INT C LEARNING REP; Trabalón Carina I., 2018, Polis, V17, P163, DOI 10.32735/s0718-6568/2018-n51-1354; Tramer F., 2017, ARXIV; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang JY, 2019, IEEE I CONF COMP VIS, P6628, DOI 10.1109/ICCV.2019.00673; Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284; Yang LL, 2017, IEEE J-STSP, V11, P1072, DOI 10.1109/JSTSP.2017.2743683; Ye SK, 2019, IEEE I CONF COMP VIS, P111, DOI 10.1109/ICCV.2019.00020; Yiluan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13496, DOI 10.1109/CVPR42600.2020.01351; Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16; Zeiler M. D., 2014, EUR C COMP VIS, P818; Zhang BH, 2007, IEEE T IMAGE PROCESS, V16, P57, DOI 10.1109/TIP.2006.884956; Zhang RX, 2018, ADV NEUR IN, V31; Zhang ZM, 2017, IEEE T GEOSCI REMOTE, V55, P7177, DOI 10.1109/TGRS.2017.2743222	88	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01700-x	http://dx.doi.org/10.1007/s11263-022-01700-x		OCT 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5V0IK					2022-12-18	WOS:000876921900002
J	Guan, BL; Zhao, J; Barath, D; Fraundorfer, F				Guan, Banglei; Zhao, Ji; Barath, Daniel; Fraundorfer, Friedrich			Minimal Solvers for Relative Pose Estimation of Multi-Camera Systems using Affine Correspondences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Relative pose estimation; Multi-camera system; Affine correspondence; Minimal solver	SELF-DRIVING CARS; VISUAL ODOMETRY; MOTION ESTIMATION; ROBUST; VISION	We propose three novel solvers for estimating the relative pose of a multi-camera system from affine correspondences (ACs). A new constraint is derived interpreting the relationship of ACs and the generalized camera model. Using the constraint, we demonstrate efficient solvers for two types of motions. Considering that the cameras undergo planar motion, we propose a minimal solution using a single AC and a solver with two ACs to overcome the degenerate case. Also, we propose a minimal solution using two ACs (a minimal number of one AC and one point correspondence) with known vertical direction, e.g., from an IMU. Since the proposed methods require significantly fewer correspondences than state-of-the-art algorithms, they can be efficiently used within RANSAC for outlier removal and initial motion estimation. The solvers are tested both on synthetic data and on three real-world scenes. It is shown that the accuracy of the estimated poses is superior to the state-of-the-art techniques. Source code is released at .	[Guan, Banglei] Natl Univ Def Technol, Coll Aerosp Sci & Engn, Changsha 410073, Peoples R China; [Barath, Daniel] Swiss Fed Inst Technol, Dept Comp Sci, CH-8092 Zurich, Switzerland; [Fraundorfer, Friedrich] Graz Univ Technol, Inst Comp Graph & Vis, A-8010 Graz, Austria; [Fraundorfer, Friedrich] German Aerosp Ctr, Remote Sensing Technol Inst, D-82234 Wessling, Germany	National University of Defense Technology - China; Swiss Federal Institutes of Technology Domain; ETH Zurich; Graz University of Technology; Helmholtz Association; German Aerospace Centre (DLR)		guanbanglei12@nudt.edu.cn; zhaoji84@gmail.com; dbarath@ethz.ch; fraundorfer@icg.tugraz.at			National Natural Science Foundation of China [11902349, 11727804]; Natural Science Foundation of Hunan Province [2020JJ5645]; ETH Zurich Postdoctoral Fellowship	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Hunan Province(Natural Science Foundation of Hunan Province); ETH Zurich Postdoctoral Fellowship	This work has been partially funded by the National Natural Science Foundation of China (Grant Nos. 11902349 and 11727804) and the Natural Science Foundation of Hunan Province (Grant No. 2020JJ5645). Daniel Barath was supported by the ETH Zurich Postdoctoral Fellowship.	Agarwal S, 2017, INT J COMPUT VISION, V121, P403, DOI 10.1007/s11263-016-0949-7; Alyousefi Khaled, 2020, Image Analysis and Recognition. 17th International Conference, ICIAR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12131), P417, DOI 10.1007/978-3-030-50347-5_36; Barath Daniel, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P723, DOI 10.1007/978-3-030-58621-8_42; Barath D, 2019, IEEE I CONF COMP VIS, P1091, DOI 10.1109/ICCV.2019.00118; Barath D, 2018, PROC CVPR IEEE, P235, DOI 10.1109/CVPR.2018.00032; Barath D, 2018, IEEE T IMAGE PROCESS, V27, P5328, DOI 10.1109/TIP.2018.2849866; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Bentolila J, 2014, COMPUT VIS IMAGE UND, V122, P105, DOI 10.1016/j.cviu.2014.02.004; Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Choi S, 2018, IMAGE VISION COMPUT, V69, P103, DOI 10.1016/j.imavis.2017.08.007; Clipp B, 2008, IEEE WORK APP COMP, P125; Cox D., 2013, IDEALS VARIETIES ALG, DOI [10.1007/978-3-319-16721-3, DOI 10.1007/978-3-319-16721-3]; Eichhardt I., 2020, EUR C COMP VIS, P627; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fragoso V, 2020, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR42600.2020.00228; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Guan B., 2021, IEEE INT C COMPUTER, P6068; Guan BL, 2021, IEEE INT CONF ROBOT, P1305, DOI 10.1109/ICRA48506.2021.9561796; Guan BL, 2022, IEEE T CYBERNETICS, V52, P10111, DOI 10.1109/TCYB.2021.3069806; Guan BL, 2020, PROC CVPR IEEE, P1926, DOI 10.1109/CVPR42600.2020.00200; Guan BL, 2018, IEEE INT CONF ROBOT, P2320; Hajder L, 2020, IEEE INT CONF ROBOT, P8651, DOI 10.1109/ICRA40945.2020.9197438; Hane C, 2017, IMAGE VISION COMPUT, V68, P14, DOI 10.1016/j.imavis.2017.07.003; Hartley R., 2003, MULTIPLE VIEW GEOMET; Heng L, 2019, IEEE INT CONF ROBOT, P4695, DOI 10.1109/ICRA.2019.8793949; Kim JH, 2010, IEEE T PATTERN ANAL, V32, P1044, DOI 10.1109/TPAMI.2009.82; Kneip L., 2016, IEEE WINTER C APPL C, P1; Kneip L, 2014, PROC CVPR IEEE, P446, DOI 10.1109/CVPR.2014.64; Kneip L, 2014, IEEE INT CONF ROBOT, P1, DOI 10.1109/ICRA.2014.6906582; Lee GH, 2014, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2014.76; Lee GH, 2013, PROC CVPR IEEE, P2746, DOI 10.1109/CVPR.2013.354; Li B., 2020, EUR C COMP VIS, P215; Li H., 2008, IEEE C COMP VIS PATT, P1; Lim J, 2010, IEEE T PATTERN ANAL, V32, P1907, DOI 10.1109/TPAMI.2010.113; Liu L, 2018, IEEE T INTELL TRANSP, V19, P2432, DOI 10.1109/TITS.2017.2749409; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Martyushev E, 2020, J MATH IMAGING VIS, V62, P1076, DOI 10.1007/s10851-020-00958-5; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9; Mishkin D, 2018, LECT NOTES COMPUT SC, V11213, P287, DOI 10.1007/978-3-030-01240-3_18; Mishkin D, 2015, COMPUT VIS IMAGE UND, V141, P81, DOI 10.1016/j.cviu.2015.08.005; Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Nutzi G, 2011, J INTELL ROBOT SYST, V61, P287, DOI 10.1007/s10846-010-9490-z; Pless R, 2003, PROC CVPR IEEE, P587, DOI 10.1109/cvpr.2003.1211520; Quan L, 1999, IEEE T PATTERN ANAL, V21, P774, DOI 10.1109/34.784291; Raposo C, 2016, PROC CVPR IEEE, P5470, DOI 10.1109/CVPR.2016.590; Saurer O, 2017, IEEE T PATTERN ANAL, V39, P327, DOI 10.1109/TPAMI.2016.2545663; Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233; Scaramuzza D, 2009, IEEE INT CONF ROBOT, P488; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Stewenius Henrik, 2005, WORKSH OMN VIS; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Sweeney Chris, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P483, DOI 10.1109/3DV.2014.66; Sweeney C, 2015, 2015 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P19, DOI 10.1109/ISMAR.2015.20; Sweeney C, 2015, PROC CVPR IEEE, P3305, DOI 10.1109/CVPR.2015.7298951; Ventura J, 2015, IEEE I CONF COMP VIS, P747, DOI 10.1109/ICCV.2015.92; Zhao J., 2020, P IEEECVF C COMPUTER, P12034	59	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01690-w	http://dx.doi.org/10.1007/s11263-022-01690-w		OCT 2022	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T3VO					2022-12-18	WOS:000875798800002
J	Ye, ZZ; Haefner, B; Queau, Y; Mollenhoff, T; Cremers, D				Ye, Zhenzhang; Haefner, Bjoern; Queau, Yvain; Moellenhoff, Thomas; Cremers, Daniel			A Cutting-Plane Method for Sublabel-Accurate Relaxation of Problems with Product Label Spaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Variational methods; Manifold-valued problems; Convex relaxation; Global optimization	CONVEX; OPTIMIZATION	Many problems in imaging and low-level vision can be formulated as nonconvex variational problems. A promising class of approaches to tackle such problems are convex relaxation methods, which consider a lifting of the energy functional to a higher-dimensional space. However, they come with increased memory requirements due to the lifting. The present paper is an extended version of the earlier conference paper by Ye et al. (in: DAGM German conference on pattern recognition (GCPR), 2021) which combined two recent approaches to make lifting more scalable: product-space relaxation and sublabel-accurate discretization. Furthermore, it is shown that a simple cutting-plane method can be used to solve the resulting semi-infinite optimization problem. This journal version extends the previous conference work with additional experiments, a more detailed outline of the complete algorithm and a user-friendly introduction to functional lifting methods.	[Ye, Zhenzhang; Haefner, Bjoern; Cremers, Daniel] Tech Univ Munich, Dept Informat, Garching, Germany; [Queau, Yvain] Normandie Univ, GREYC, CNRS, ENSICAEN,UNICAEN, Caen, France; [Moellenhoff, Thomas] RIKEN, Ctr AI Project, Tokyo, Japan	Technical University of Munich; Centre National de la Recherche Scientifique (CNRS); Universite de Caen Normandie; RIKEN	Ye, ZZ (corresponding author), Tech Univ Munich, Dept Informat, Garching, Germany.	zhenzhang.ye@tum.de; bjoern.haefner@tum.de; yvain.queau@ensicaen.fr; thomas.moellenhoff@riken.jp; cremers@tum.de			Projekt DEAL	Projekt DEAL	Open Access funding enabled and organized by Projekt DEAL.	Bach F, 2019, MATH PROGRAM, V175, P419, DOI 10.1007/s10107-018-1248-6; Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2; Bauermeister H., 2021, ARXIV; BLANKENSHIP JW, 1976, J OPTIMIZ THEORY APP, V19, P261, DOI 10.1007/BF00934096; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Caillaud C., 2020, PREPRINTS; Carlier G, 2003, J CONVEX ANAL, V10, P517; Chambolle A., 2010, THEORETICAL FDN NUME, P263, DOI [10.1515/9783110226157.263, DOI 10.1515/9783110226157.263]; Chambolle A, 2012, SIAM J IMAGING SCI, V5, P1113, DOI 10.1137/110856733; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Cremers D, 2013, J MATH IMAGING VIS, V47, P258, DOI 10.1007/s10851-012-0396-1; De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fix A, 2014, LECT NOTES COMPUT SC, V8691, P266, DOI 10.1007/978-3-319-10578-9_18; Ghoussoub N, 2021, SIAM J MATH ANAL, V53, P1070, DOI 10.1137/20M1333377; Gorlitz A, 2019, IEEE INT C INT ROBOT, P1758, DOI 10.1109/IROS40897.2019.8968018; Goldluecke B, 2013, SIAM J IMAGING SCI, V6, P1626, DOI 10.1137/120862351; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Kappes JH, 2013, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2013.175; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Laude E, 2016, LECT NOTES COMPUT SC, V9905, P614, DOI 10.1007/978-3-319-46448-0_37; Lellmann J, 2011, SIAM J IMAGING SCI, V4, P1049, DOI 10.1137/100805844; Lellmann J, 2013, IEEE I CONF COMP VIS, P2944, DOI 10.1109/ICCV.2013.366; Lellmann J, 2013, INT J COMPUT VISION, V104, P241, DOI 10.1007/s11263-013-0621-4; Lellmann J, 2009, LECT NOTES COMPUT SC, V5567, P150, DOI 10.1007/978-3-642-02256-2_13; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Mollenhoff T, 2017, IEEE I CONF COMP VIS, P1192, DOI 10.1109/ICCV.2017.134; Mollenhoff T, 2016, PROC CVPR IEEE, P3948, DOI 10.1109/CVPR.2016.428; Mollenhoff T, 2019, PROC CVPR IEEE, P11109, DOI 10.1109/CVPR.2019.01137; Ollivier Y, 2017, J MACH LEARN RES, V18; Peng J., 2011, INT C MACH LEARN ICM; Pock T, 2008, LECT NOTES COMPUT SC, V5304, P792, DOI 10.1007/978-3-540-88690-7_59; Pock T, 2011, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2011.6126441; Pock T, 2010, SIAM J IMAGING SCI, V3, P1122, DOI 10.1137/090757617; Pock T, 2009, IEEE I CONF COMP VIS, P1133, DOI 10.1109/ICCV.2009.5459348; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Schaul T., 2011, THESIS TU MUNCHEN; Steinke F, 2010, SIAM J IMAGING SCI, V3, P527, DOI 10.1137/080744189; Strekalovskiy E, 2014, SIAM J IMAGING SCI, V7, P294, DOI 10.1137/130908348; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Vogt T., 2020, HDB VARIATIONAL METH; Weinmann A, 2014, SIAM J IMAGING SCI, V7, P2226, DOI 10.1137/130951075; Ye Z., 2021, DAGM GERMAN C PATTER; Zach C., 2008, P VIS MOD VIS WORKSH; Zach C., 2013, INT C ART INT STAT A; Zach C, 2012, LECT NOTES COMPUT SC, V7577, P386, DOI 10.1007/978-3-642-33783-3_28	47	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01704-7	http://dx.doi.org/10.1007/s11263-022-01704-7		OCT 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T3VO		hybrid			2022-12-18	WOS:000875798800001
J	Vo, K; Truong, S; Yamazaki, K; Raj, B; Tran, MT; Le, N				Khoa Vo; Truong, Sang; Yamazaki, Kashu; Raj, Bhiksha; Minh-Triet Tran; Ngan Le			AOE-Net: Entities Interactions Modeling with Adaptive Attention Mechanism for Temporal Action Proposals Generation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Temporal action proposal; Temporal action detection; Human perceiving process; Attention mechanism; Human; Objects; Environment; Interaction; Video understanding		Temporal action proposal generation (TAPG) is a challenging task, which requires localizing action intervals in an untrimmed video. Intuitively, we as humans, perceive an action through the interactions between actors, relevant objects, and the surrounding environment. Despite the significant progress of TAPG, a vast majority of existing methods ignore the aforementioned principle of the human perceiving process by applying a backbone network into a given video as a black-box. In this paper, we propose to model these interactions with a multi-modal representation network, namely, Actors-Objects-Environment Interaction Network (AOE-Net). Our AOE-Net consists of two modules, i.e., perception-based multi-modal representation (PMR) and boundary-matching module (BMM). Additionally, we introduce adaptive attention mechanism (AAM) in PMR to focus only on main actors (or relevant objects) and model the relationships among them. PMR module represents each video snippet by a visual-linguistic feature, in which main actors and surrounding environment are represented by visual information, whereas relevant objects are depicted by linguistic features through an image-text model. BMM module processes the sequence of visual-linguistic features as its input and generates action proposals. Comprehensive experiments and extensive ablation studies on ActivityNet-1.3 and THUMOS-14 datasets show that our proposed AOE-Net outperforms previous state-of-the-art methods with remarkable performance and generalization for both TAPG and temporal action detection. To prove the robustness and effectiveness of AOE-Net, we further conduct an ablation study on egocentric videos, i.e. EPIC-KITCHENS 100 dataset. Our source code is publicly available at .	[Khoa Vo; Truong, Sang; Yamazaki, Kashu; Ngan Le] Univ Arkansas, AICV Lab, Fayetteville, AR 72701 USA; [Raj, Bhiksha] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Minh-Triet Tran] Univ Sci, Ho Chi Minh City, Vietnam; [Minh-Triet Tran] Vietnam Natl Univ, Ho Chi Minh City, Vietnam	University of Arkansas System; University of Arkansas Fayetteville; Carnegie Mellon University; Vietnam National University Hochiminh City	Vo, K (corresponding author), Univ Arkansas, AICV Lab, Fayetteville, AR 72701 USA.	khoavoho@uark.edu; sangt@uark.edu; kyamazak@uark.edu; bhiksha@cs.cmu.edu; tmtriet@hcmus.edu.vn; thile@uark.edu		Le, T. Hoang Ngan/0000-0003-2571-0511; Tran, Minh-Triet/0000-0003-3046-3041	National Science Foundation (NSF) [OIA-1946391]; NSF [2223793, 1920920]; Vingroup Innovation Foundation (VINIF) [VINIF.2019]	National Science Foundation (NSF)(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); Vingroup Innovation Foundation (VINIF)	This material is based upon work supported by the National Science Foundation (NSF) under Award No OIA-1946391, NSF 1920920, and NSF 2223793. Minh-Triet Tran was funded by Vingroup and supported by Vingroup Innovation Foundation (VINIF) under project code VINIF.2019.DA19.	Alec Radford, 2021, ARXIV; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Andrew Zisserman, 2017, Arxiv, DOI arXiv:1705.06950; Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593; Bowen Zhang, 2016, Arxiv, DOI arXiv:1608.00797; Buch Shyamal, 2017, BMVC, P2; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124; Chaudhari S, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3465055; Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044; Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610; Damen Dima, 2021, INT J COMPUT VISION, P1; Dosovitskiy A., 2021, CVPR; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Elsayed GF, 2019, ADV NEUR IN, V32; Eun H., 2019, IEEE T CIRC SYST VID, P1; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Galassi A, 2021, IEEE T NEUR NET LEAR, V32, P4291, DOI 10.1109/TNNLS.2020.3019893; Gao JL, 2020, AAAI CONF ARTIF INTE, V34, P10810; Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5; Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688; Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jiang Y.-G., 2014, THUMOS CHALLENGE ACT; Jiyang Gao, 2017, Arxiv, DOI arXiv:1705.01180; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499; Lin T.Y., 2019, 13 EUR C ZUR SWITZ S; Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399; Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu S., 2020, P AS C COMP VIS KYOT; Liu Y, 2019, PROC CVPR IEEE, P3599, DOI 10.1109/CVPR.2019.00372; Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043; Malinowski M, 2018, LECT NOTES COMPUT SC, V11210, P3, DOI 10.1007/978-3-030-01231-1_1; Mei T, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.10; Mengmeng Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10153, DOI 10.1109/CVPR42600.2020.01017; Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479; Patro B, 2018, PROC CVPR IEEE, P7680, DOI 10.1109/CVPR.2018.00801; Peisen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P539, DOI 10.1007/978-3-030-58598-3_32; Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richard A, 2016, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2016.341; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Chaudhari S, 2021, Arxiv, DOI arXiv:1904.02874; Su H., 2020, ACCV; Tan J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13506, DOI 10.1109/ICCV48922.2021.01327; Vaswani A., 2017, ARXIV170603762 CSCL, V30; Vo-Ho VK, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2160, DOI 10.1109/ICASSP39728.2021.9415101; Vo K., 2021, 32 BRIT MACHINE VISI; Vo K, 2021, IEEE ACCESS, V9, P126431, DOI 10.1109/ACCESS.2021.3110973; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Wang X, 2021, PROC CVPR IEEE, P1905, DOI 10.1109/CVPR46437.2021.00194; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yao T., 2017, CVPR WORKSHOPS; Yueran Bai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P121, DOI 10.1007/978-3-030-58604-1_8; Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719; Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317; Zheng JX, 2021, NAT ENERGY, V6, P398, DOI 10.1038/s41560-021-00797-7	67	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01702-9	http://dx.doi.org/10.1007/s11263-022-01702-9		OCT 2022	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T0SX		Green Submitted			2022-12-18	WOS:000875589000001
J	Zhong, ZH; Gao, Y; Zheng, YQ; Zheng, B; Sato, I				Zhong, Zhihang; Gao, Ye; Zheng, Yinqiang; Zheng, Bo; Sato, Imari			Real-World Video Deblurring: A Benchmark Dataset and an Efficient Recurrent Neural Network	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Video deblurring; Network efficiency; RNN; Real-world dataset; Beam-splitter acquisition system		Real-world video deblurring in real time still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. Another issue that needs to be addressed urgently is the lack of a real-world benchmark dataset. Thus, we contribute a novel dataset (BSD) to the community, by collecting paired blurry/sharp video clips using a co-axis beam splitter acquisition system. Experimental results show that the proposed method (ESTRNN) can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods. In addition, cross-validation experiments between datasets illustrate the high generality of BSD over the synthetic datasets. The code and dataset are released at .	[Zhong, Zhihang; Zheng, Yinqiang] Univ Tokyo, Tokyo, Japan; [Gao, Ye; Zheng, Bo] Huawei, Tokyo Res Ctr, Tokyo, Japan; [Sato, Imari] Natl Inst Informat, Tokyo, Japan	University of Tokyo; Huawei Technologies; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan	Zheng, YQ (corresponding author), Univ Tokyo, Tokyo, Japan.	yqzheng@ai.u-tokyo.ac.jp			JSPS KAKENHI [JP22H00529, JP20H05951, JP20H05953]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This was supported in part by JSPS KAKENHI Grant Numbers JP22H00529, JP20H05951 and JP20H05953.	Bar L, 2007, IEEE I CONF COMP VIS, P1410; Cao M., 2022, ARXIV; Cao M., 2022, P IEEECVF C COMPUTER, P17785; Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14; Cho S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185560; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Francesco Visin, 2018, Arxiv, DOI arXiv:1603.07285; Goldstein A, 2012, LECT NOTES COMPUT SC, V7576, P622, DOI 10.1007/978-3-642-33715-4_45; Gongshen Liu, 2018, Arxiv, DOI arXiv:1807.02291; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G., P IEEE C COMP VIS PA, P4700; Hui Dai, 2018, Arxiv, DOI arXiv:1709.02755; Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936; Jaesung Rim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P184, DOI 10.1007/978-3-030-58595-2_12; Jin MG, 2018, PROC CVPR IEEE, P6334, DOI 10.1109/CVPR.2018.00663; Kim TH, 2018, IEEE T PATTERN ANAL, V40, P2374, DOI 10.1109/TPAMI.2017.2761348; Kim TH, 2017, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2017.435; Kim TH, 2014, PROC CVPR IEEE, P2766, DOI 10.1109/CVPR.2014.348; Kingma D.P, P 3 INT C LEARNING R; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Lee HS, 2013, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2013.42; Lee HS, 2011, IEEE I CONF COMP VIS, P1203, DOI 10.1109/ICCV.2011.6126370; Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51; Min Lin, 2014, Arxiv, DOI arXiv:1312.4400; Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251; Nah S, 2019, PROC CVPR IEEE, P8094, DOI 10.1109/CVPR.2019.00829; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Nair V., 2010, ICML, P807; Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37; Nimisha TM, 2017, IEEE I CONF COMP VIS, P4762, DOI 10.1109/ICCV.2017.509; Pan JS, 2020, PROC CVPR IEEE, P3040, DOI 10.1109/CVPR42600.2020.00311; Ren WQ, 2017, IEEE I CONF COMP VIS, P1086, DOI 10.1109/ICCV.2017.123; Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142; Shen W, 2020, PROC CVPR IEEE, P5113, DOI 10.1109/CVPR42600.2020.00516; Son H, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3453720; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677; Sun LB, 2014, LECT NOTES COMPUT SC, V8692, P231; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247; Wieschollek P, 2017, IEEE I CONF COMP VIS, P231, DOI 10.1109/ICCV.2017.34; Wu Y, 2011, IEEE I CONF COMP VIS, P1100, DOI 10.1109/ICCV.2011.6126357; Wulff J, 2014, LECT NOTES COMPUT SC, V8694, P236, DOI 10.1007/978-3-319-10599-4_16; Xu L, 2014, ADV NEUR IN, V27; Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157; Wang YS, 2022, Arxiv, DOI arXiv:2207.13374; Zhang XN, 2019, PROC CVPR IEEE, P3757, DOI 10.1109/CVPR.2019.00388; Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhong ZH, 2022, Arxiv, DOI arXiv:2207.10123; Zhihang Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P191, DOI 10.1007/978-3-030-58539-6_12; Zhong Z., 2021, P IEEE CVF C COMP VI, P9219; Zhou SC, 2019, IEEE I CONF COMP VIS, P2482, DOI 10.1109/ICCV.2019.00257; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	58	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01705-6	http://dx.doi.org/10.1007/s11263-022-01705-6		OCT 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5R4EV		Green Submitted			2022-12-18	WOS:000874466100001
J	Dunnhofer, M; Furnari, A; Farinella, GM; Micheloni, C				Dunnhofer, Matteo; Furnari, Antonino; Farinella, Giovanni Maria; Micheloni, Christian			Visual Object Tracking in First Person Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						First person vision; Egocentric vision; Visual object tracking; Single object tracking	ROBUST	The understanding of human-object interactions is fundamental in First Person Vision (FPV). Visual tracking algorithms which follow the objects manipulated by the camera wearer can provide useful information to effectively model such interactions. In the last years, the computer vision community has significantly improved the performance of tracking algorithms for a large variety of target objects and scenarios. Despite a few previous attempts to exploit trackers in the FPV domain, a methodical analysis of the performance of state-of-the-art trackers is still missing. This research gap raises the question of whether current solutions can be used "off-the-shelf" or more domain-specific investigations should be carried out. This paper aims to provide answers to such questions. We present the first systematic investigation of single object tracking in FPV. Our study extensively analyses the performance of 42 algorithms including generic object trackers and baseline FPV-specific trackers. The analysis is carried out by focusing on different aspects of the FPV setting, introducing new performance measures, and in relation to FPV-specific tasks. The study is made possible through the introduction of TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV poses new challenges to current visual trackers. We highlight the factors causing such behavior and point out possible research directions. Despite their difficulties, we prove that trackers bring benefits to FPV downstream tasks requiring short-term object tracking. We expect that generic object tracking will gain popularity in FPV as new and FPV-specific methodologies are investigated.	[Dunnhofer, Matteo; Micheloni, Christian] Univ Udine, Machine Learning & Percept Lab, Via Sci 206, I-33100 Udine, Italy; [Furnari, Antonino; Farinella, Giovanni Maria] Univ Catania, Image Proc Lab, Viale A Doria 6, I-95125 Catania, Italy	University of Udine; University of Catania	Dunnhofer, M (corresponding author), Univ Udine, Machine Learning & Percept Lab, Via Sci 206, I-33100 Udine, Italy.	matteo.dunnhofer@uniud.it; furnari@dmi.unict.it; gfarinella@dmi.unict.it; christian.micheloni@uniud.it	FARINELLA, Giovanni Maria/L-8555-2015	FARINELLA, Giovanni Maria/0000-0002-6034-0432; FURNARI, Antonino/0000-0001-6911-0302; Dunnhofer, Matteo/0000-0002-1672-667X; Micheloni, Christian/0000-0003-4503-7483	Universita degli Studi di Udine within the CRUI-CARE Agreement	Universita degli Studi di Udine within the CRUI-CARE Agreement	Open access funding provided by Universita degli Studi di Udine within the CRUI-CARE Agreement.	Aghaei M., 2016, ICPR DETECTING SOCIA; Aghaei M, 2016, COMPUT VIS IMAGE UND, V149, P146, DOI 10.1016/j.cviu.2016.02.013; Alletto S, 2015, LECT NOTES COMPUT SC, V9280, P687, DOI 10.1007/978-3-319-23234-8_63; Bertasius G, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII; Bertasius G, 2017, IEEE I CONF COMP VIS, P1974, DOI 10.1109/ICCV.2017.216; Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003; Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13; Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Cai MJ, 2016, ROBOTICS: SCIENCE AND SYSTEMS XII; Cao Z., 2020, ARXIV; Cehovin L, 2013, IEEE T PATTERN ANAL, V35, P941, DOI 10.1109/TPAMI.2012.145; Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803; Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670; Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761; Dai KN, 2020, PROC CVPR IEEE, P6297, DOI 10.1109/CVPR42600.2020.00633; Damen D, 2022, INT J COMPUT VISION, V130, P33, DOI 10.1007/s11263-021-01531-2; Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44; Damen D, 2016, COMPUT VIS IMAGE UND, V149, P98, DOI 10.1016/j.cviu.2016.02.016; Dandan Shan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9866, DOI 10.1109/CVPR42600.2020.00989; Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721; Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928; Dave Achal, 2020, ECCV; Dela Torre F., 2009, WORKSH DEV SHAR HOM; Dunnhofer M., 2020, ACCV; Dunnhofer M, 2021, IEEE ROBOT AUTOM LET, V6, P5016, DOI 10.1109/LRA.2021.3070816; Dunnhofer M, 2019, IEEE INT CONF COMP V, P2290, DOI 10.1109/ICCVW.2019.00282; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fan Haoqi, 2021, ICCV; Fan H, 2021, INT J COMPUT VISION, V129, P439, DOI 10.1007/s11263-020-01387-y; Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552; Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356; Furnari A, 2017, J VIS COMMUN IMAGE R, V49, P401, DOI 10.1016/j.jvcir.2017.10.004; Furnari A, 2019, IEEE I CONF COMP VIS, P6261, DOI 10.1109/ICCV.2019.00635; Furnari A, 2021, IEEE T PATTERN ANAL, V43, P4021, DOI 10.1109/TPAMI.2020.2992889; Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI 10.1109/ICCV.2017.129; Grauman Kristen, 2022, CVPR, P1; Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942; Han SC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392452; Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974; Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037; Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kapidis G., 2019, IEEE SMARTWORLD UB I; Kapidis G, 2019, 2019 IEEE SMARTWORLD, P922; Kristan M., 2019, ICCVW; Kristan M., 2021, ICCVW; Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230; Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982; Kristan Matej, 2020, ECCVW; Li AN, 2016, IEEE T PATTERN ANAL, V38, P335, DOI 10.1109/TPAMI.2015.2417577; Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441; Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515; Li Y, 2018, LECT NOTES COMPUT SC, V11209, P639, DOI 10.1007/978-3-030-01228-1_38; Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu M., 2020, ECCV; Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22; Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716; Lukezic A, 2019, IEEE I CONF COMP VIS, P10012, DOI 10.1109/ICCV.2019.01011; Lukezic A, 2021, IEEE T CYBERNETICS, V51, P6305, DOI 10.1109/TCYB.2020.2980618; Ma MH, 2016, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2016.209; Maggio D. E., 2011, VIDEO TRACKING OR; Maresca ME, 2013, LECT NOTES COMPUT SC, V8157, P419, DOI 10.1007/978-3-642-41184-7_43; Mayer Christoph, 2021, LEARNING TARGET CAND; Mengtian Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P473, DOI 10.1007/978-3-030-58536-5_28; Mueller F, 2017, IEEE I CONF COMP VIS, P1163, DOI 10.1109/ICCV.2017.131; Muller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19; Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27; Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465; Nam H, 2014, LECT NOTES COMPUT SC, V8693, P112, DOI 10.1007/978-3-319-10602-1_8; Nigam J, 2017, IEEE COMPUT SOC CONF, P980, DOI 10.1109/CVPRW.2017.134; Park E, 2018, LECT NOTES COMPUT SC, V11207, P587, DOI 10.1007/978-3-030-01219-9_35; Pirsiavash H, 2012, PROC CVPR IEEE, P2847, DOI 10.1109/CVPR.2012.6248010; Ragusa F, 2020, PATTERN RECOGN LETT, V131, P150, DOI 10.1016/j.patrec.2019.12.016; Rai A., 2021, ARXIV; Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Rodin I, 2021, COMPUT VIS IMAGE UND, V211, DOI 10.1016/j.cviu.2021.103252; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sener Fadime, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P154, DOI 10.1007/978-3-030-58517-4_10; Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230; Socher R., 2009, CVPR09; Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937; Sun L, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2009.5204358; Valmadre J, 2018, LECT NOTES COMPUT SC, V11207, P692, DOI 10.1007/978-3-030-01219-9_41; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Visee RJ, 2020, IEEE T NEUR SYS REH, V28, P748, DOI 10.1109/TNSRE.2020.2968912; Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162; Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509; Wang Q., 2017, DCFNET DISCRIMINANT; Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142; Wang XH, 2020, AAAI CONF ARTIF INTE, V34, P12249; Wojke N, 2017, IEEE IMAGE PROC, P3645; Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36; Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549; Yan B, 2021, PROC CVPR IEEE, P15175, DOI 10.1109/CVPR46437.2021.01493; Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028; Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247; Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148; Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13; Zhang LC, 2019, IEEE I CONF COMP VIS, P4009, DOI 10.1109/ICCV.2019.00411; Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472; Zhao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P455, DOI 10.1007/978-3-030-58610-2_27	117	0	0	4	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01694-6	http://dx.doi.org/10.1007/s11263-022-01694-6		OCT 2022	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5J3SD		Green Submitted, hybrid			2022-12-18	WOS:000868962900001
J	Yao, Y; Bala, P; Mohan, A; Bliss-Moreau, E; Coleman, K; Freeman, SM; Machado, CJ; Raper, J; Zimmermann, J; Hayden, BY; Park, HS				Yao, Yuan; Bala, Praneet; Mohan, Abhiraj; Bliss-Moreau, Eliza; Coleman, Kristine; Freeman, Sienna M.; Machado, Christopher J.; Raper, Jessica; Zimmermann, Jan; Hayden, Benjamin Y.; Park, Hyun Soo			OpenMonkeyChallenge: Dataset and Benchmark Challenges for Pose Estimation of Non-human Primates	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Behavioral tracking; Deep learning; Non-human primates; Dataset and benchmark challenge		The ability to automatically estimate the pose of non-human primates as they move through the world is important for several subfields in biology and biomedicine. Inspired by the recent success of computer vision models enabled by benchmark challenges (e.g., object detection), we propose a new benchmark challenge called OpenMonkeyChallenge that facilitates collective community efforts through an annual competition to build generalizable non-human primate pose estimation models. To host the benchmark challenge, we provide a new public dataset consisting of 111,529 annotated (17 body landmarks) photographs of non-human primates in naturalistic contexts obtained from various sources including the Internet, three National Primate Research Centers, and the Minnesota Zoo. Such annotated datasets will be used for the training and testing datasets to develop generalizable models with standardized evaluation metrics. We demonstrate the effectiveness of our dataset quantitatively by comparing it with existing datasets based on seven state-of-the-art pose estimation models.	[Yao, Yuan; Bala, Praneet; Mohan, Abhiraj; Park, Hyun Soo] Univ Minnesota, Comp Sci & Engn, Minneapolis, MN 55455 USA; [Coleman, Kristine] Oregon Natl Primate Res Ctr, Beaverton, OR USA; [Freeman, Sienna M.; Raper, Jessica] Emory Natl Primate Res Ctr, Atlanta, GA USA; [Bliss-Moreau, Eliza; Machado, Christopher J.] Calif Natl Primate Res Ctr, Davis, CA USA; [Zimmermann, Jan; Hayden, Benjamin Y.] Univ Minnesota, Neurosci, Minneapolis, MN USA	University of Minnesota System; University of Minnesota Twin Cities; Oregon Health & Science University; Oregon National Primate Research Center; University of Minnesota System; University of Minnesota Twin Cities	Bala, P (corresponding author), Univ Minnesota, Comp Sci & Engn, Minneapolis, MN 55455 USA.	yaoxx340@umn.edu; balax007@umn.edu; mohan056@umn.edu; eblissmoreau@ucdavis.edu; colemank@ohsu.edu; sienna.freeman@emory.edu; cjmachado@ucdavis.edu; jraper@emory.edu; janz@umn.edu; benhayden@gmail.com; hspark@umn.edu	Raper, Jessica/J-6355-2019	Raper, Jessica/0000-0002-0964-9944	NSF [IIS 2024581]; NIH [P51 OD011092, P51 OD011132, R01-NS120182, K99-MH083883]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work is partially supported by NSF IIS 2024581 (HSP, JZ, and BYH), NIH P51 OD011092 (ONPRC), NIH P51 OD011132 (YNPRC), NIH R01-NS120182 (JR), and K99-MH083883 (CJM).	Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Bala PC, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-18441-5; Belagiannis V, 2017, IEEE INT CONF AUTOMA, P468, DOI 10.1109/FG.2017.64; Bliss-Moreau E, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0071170; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543; de Bem R., 2018, ASIAN C MACHINE LEAR; Dunn TW, 2021, NAT METHODS, V18, P564, DOI 10.1038/s41592-021-01106-6; Eichner M, 2010, LECT NOTES COMPUT SC, V6311, P228, DOI 10.1007/978-3-642-15549-9_17; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Gunel S, 2019, ELIFE, V8, DOI 10.7554/eLife.48571; Hayden BY, 2022, AM J PRIMATOL, V84, DOI 10.1002/ajp.23348; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Iqbal U, 2017, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2017.495; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Jakab Tomas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8784, DOI 10.1109/CVPR42600.2020.00881; James Melville, 2020, Arxiv, DOI arXiv:1802.03426; Karashchuk Pierre, 2020, BIORXIV, V1; Knaebe B, 2022, ANIMALS-BASEL, V12, DOI 10.3390/ani12131648; Labuguen R, 2021, FRONT BEHAV NEUROSCI, V14, DOI 10.3389/fnbeh.2020.581154; Li SY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2590, DOI 10.1145/3394171.3413569; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Ludwig K, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9456000; Machado CJ, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0026598; Mathis A, 2018, NAT NEUROSCI, V21, P1281, DOI 10.1038/s41593-018-0209-y; Mathis MW, 2020, CURR OPIN NEUROBIOL, V60, P1, DOI 10.1016/j.conb.2019.10.008; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Ng X.L., 2022, COMPUTER VISION PATT; Pereira TD, 2019, NAT METHODS, V16, P117, DOI 10.1038/s41592-018-0234-5; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Ren ZZ, 2018, PROC CVPR IEEE, P762, DOI [10.1109/CVPR.2018.00086, 10.1109/CVPR.2018.00104]; SADE DS, 1973, AM J PHYS ANTHROPOL, V38, P537, DOI 10.1002/ajpa.1330380263; Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471; Sumer O., 2017, INT C COMPUTER VISIO; Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37; Wan CD, 2019, PROC CVPR IEEE, P10845, DOI 10.1109/CVPR.2019.01111; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wiltschko AB, 2015, NEURON, V88, P1121, DOI 10.1016/j.neuron.2015.11.031; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Yang H, 2021, PROC CVPR IEEE, P14345, DOI 10.1109/CVPR46437.2021.01412; Yao Y, 2019, INT C COMPUTER VISIO	49	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01698-2	http://dx.doi.org/10.1007/s11263-022-01698-2		OCT 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5I6MJ					2022-12-18	WOS:000868467800001
J	Zhou, M; Yan, KY; Pan, JS; Ren, WQ; Xie, Q; Cao, XY				Zhou, Man; Yan, Keyu; Pan, Jinshan; Ren, Wenqi; Xie, Qi; Cao, Xiangyong			Memory-Augmented Deep Unfolding Network for Guided Image Super-resolution	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Guided image super-resolution; Deep unfolding network; Persistent memory mechanism; Pan-sharpening; Depth image super-resolution; MR image super-resolution	MULTI-CONTRAST SUPERRESOLUTION; SPARSE REPRESENTATION; FUSION; INTERPOLATION; ENHANCEMENT; RECOVERY; MRI	Guided image super-resolution (GISR) aims to obtain a high-resolution (HR) target image by enhancing the spatial resolution of a low-resolution (LR) target image under the guidance of a HR image. However, previous model-based methods mainly take the entire image as a whole, and assume the prior distribution between the HR target image and the HR guidance image, simply ignoring many non-local common characteristics between them. To alleviate this issue, we firstly propose a maximum a posteriori (MAP) estimation model for GISR with two types of priors on the HR target image, i.e., local implicit prior and global implicit prior. The local implicit prior aims to model the complex relationship between the HR target image and the HR guidance image from a local perspective, and the global implicit prior considers the non-local auto-regression property between the two images from a global perspective. Secondly, we design a novel alternating optimization algorithm to solve this model for GISR. The algorithm is in a concise framework that facilitates to be replicated into commonly used deep network structures. Thirdly, to reduce the information loss across iterative stages, the persistent memory mechanism is introduced to augment the information representation by exploiting the Long short-term memory unit (LSTM) in the image and feature spaces. In this way, a deep network with certain interpretation and high representation ability is built. Extensive experimental results validate the superiority of our method on a variety of GISR tasks, including Pan-sharpening, depth image super-resolution, and MR image super-resolution. Code will be released at https://github.com/manman1995/pansharpening.	[Zhou, Man; Yan, Keyu] Chinese Acad Sci, Hefei Inst Phys Sci, Hefei, Peoples R China; [Zhou, Man; Yan, Keyu] Univ Sci & Technol China, Hefei, Peoples R China; [Pan, Jinshan] Nanjing Univ Sci & Technol, Nanjing, Peoples R China; [Ren, Wenqi] Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China; [Xie, Qi] Xi An Jiao Tong Univ, Sch Math & Stat, Xian, Peoples R China; [Cao, Xiangyong] Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian, Peoples R China; [Cao, Xiangyong] Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian, Peoples R China	Chinese Academy of Sciences; Hefei Institutes of Physical Science, CAS; Chinese Academy of Sciences; University of Science & Technology of China, CAS; Nanjing University of Science & Technology; Chinese Academy of Sciences; Institute of Information Engineering, CAS; Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong University	Cao, XY (corresponding author), Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian, Peoples R China.; Cao, XY (corresponding author), Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian, Peoples R China.	caoxiangyong@mail.xjtu.edu.cn			National Key Research and Development Project of China [2021ZD0110700]; National Natural Science Foundation of China [62272375, 61906151, 62050194, 62037001]; Innovative Research Group of the National Natural Science Foundation of China [61721002]; Innovation Research Team of Ministry of Education [IRT_17R86]; Project of China Knowledge Centre for Engineering Science and Technology; Project of XJTU Undergraduate Teaching Reform [20JX04Y]	National Key Research and Development Project of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Innovative Research Group of the National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Innovation Research Team of Ministry of Education; Project of China Knowledge Centre for Engineering Science and Technology; Project of XJTU Undergraduate Teaching Reform	This work was supported by National Key Research and Development Project of China (2021ZD0110700), National Natural Science Foundation of China (62272375, 61906151, 62050194, 62037001), Innovative Research Group of the National Natural Science Foundation of China(61721002), Innovation Research Team of Ministry of Education (IRT_17R86), Project of China Knowledge Centre for Engineering Science and Technology, and Project of XJTU Undergraduate Teaching Reform (20JX04Y).	Alparone L, 2007, IEEE T GEOSCI REMOTE, V45, P3012, DOI 10.1109/TGRS.2007.904923; [Anonymous], 2010, P INT C MACH LEARN; Bahrampour S, 2016, IEEE T IMAGE PROCESS, V25, P24, DOI 10.1109/TIP.2015.2496275; Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Bruna Joan, 2015, ARXIV; Cai JJ, 2021, IEEE T GEOSCI REMOTE, V59, P5206, DOI 10.1109/TGRS.2020.3015878; Cao XY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3115501; Dai S., 2007, P IEEE C COMP VIS PA, P1, DOI [10.1109/CVPR.2007.383028, DOI 10.1109/CVPR.2007.383028]; Deng X, 2021, IEEE T PATTERN ANAL, V43, P3333, DOI 10.1109/TPAMI.2020.2984244; Deng X, 2020, IEEE T IMAGE PROCESS, V29, P1683, DOI 10.1109/TIP.2019.2944270; Diebel J., 2005, NIPS; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847; Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086; Feng C.M., 2021, ARXIV, DOI [10.48550/arXiv.2105.08949, DOI 10.48550/ARXIV.2105.08949]; Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127; GEMAN D, 1995, IEEE T IMAGE PROCESS, V4, P932, DOI 10.1109/83.392335; GEMAN D, 1992, IEEE T PATTERN ANAL, V14, P367, DOI 10.1109/34.120331; GILLESPIE AR, 1987, REMOTE SENS ENVIRON, V22, P343, DOI 10.1016/0034-4257(87)90088-5; Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029; Ham B, 2018, IEEE T PATTERN ANAL, V40, P192, DOI 10.1109/TPAMI.2017.2669034; Ham B, 2015, PROC CVPR IEEE, P4823, DOI 10.1109/CVPR.2015.7299115; Haydn R., 1982, P NATL ACAD SCI USA, V79, P571; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; He R, 2014, IEEE T PATTERN ANAL, V36, P261, DOI 10.1109/TPAMI.2013.102; Hirschmuller Heiko, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383248; Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22; Jia K, 2013, IEEE T PATTERN ANAL, V35, P367, DOI 10.1109/TPAMI.2012.95; Jing XY, 2015, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2015.7298669; Kim B, 2021, INT J COMPUT VISION, V129, P579, DOI 10.1007/s11263-020-01386-z; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Krishnan D., 2009, ADV NEURAL INFORM PR, V22, P1033; Laben C.A., 2000, US Patent, Patent No. [6011875, 6,011,875, 6011875A]; Li YJ, 2019, IEEE T PATTERN ANAL, V41, P1909, DOI 10.1109/TPAMI.2018.2890623; Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10; Liao W., 2017, WORKSHOP HYPERSPECTR; Liu D, 2016, IEEE T IMAGE PROCESS, V25, P3194, DOI 10.1109/TIP.2016.2564643; Liu JG, 2000, INT J REMOTE SENS, V21, P3461, DOI 10.1080/014311600750037499; Liu X, 2014, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2014.454; Lu S, 2014, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2014.433; Lyu Q, 2020, IEEE T MED IMAGING, V39, P2738, DOI 10.1109/TMI.2020.2974858; Mallat S, 2010, IEEE T IMAGE PROCESS, V19, P2889, DOI 10.1109/TIP.2010.2049927; Mao XJ, 2016, ADV NEUR IN, V29; Marivani I, 2020, IEEE T IMAGE PROCESS, V29, P8443, DOI 10.1109/TIP.2020.3014729; Masi G, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8070594; Ngiam J., 2011, IEEE INT C MACHINE L; Oktay Ozan, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9902, P246, DOI 10.1007/978-3-319-46726-9_29; Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Pham CH, 2017, I S BIOMED IMAGING, P197, DOI 10.1109/ISBI.2017.7950500; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; Sanchez-Beato A, 2008, IEEE T IMAGE PROCESS, V17, P1817, DOI 10.1109/TIP.2008.2002833; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Shen XY, 2015, IEEE T PATTERN ANAL, V37, P2518, DOI 10.1109/TPAMI.2015.2417569; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song J., 2021, ACM MM; Song PF, 2020, IEEE T COMPUT IMAG, V6, P57, DOI 10.1109/TCI.2019.2916502; Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142; Sun BL, 2021, PROC CVPR IEEE, P7788, DOI 10.1109/CVPR46437.2021.00770; Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8; Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241; Tomasi C., 1998, P IEEE INT C COMP VI, DOI 10.1109/iccv.1998.710815; Vivone G, 2015, IEEE T GEOSCI REMOTE, V53, P2565, DOI 10.1109/TGRS.2014.2361734; Wald L, 1997, PHOTOGRAMM ENG REM S, V63, P691; Wang JC, 2020, IEEE WINT CONF APPL, P3616, DOI 10.1109/WACV45572.2020.9093603; Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197; Xu S, 2021, PROC CVPR IEEE, P1366, DOI 10.1109/CVPR46437.2021.00142; Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647; Yang JC, 2013, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2013.141; Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yang JF, 2017, IEEE I CONF COMP VIS, P1753, DOI 10.1109/ICCV.2017.193; Ye XC, 2020, IEEE T IMAGE PROCESS, V29, P7427, DOI 10.1109/TIP.2020.3002664; Yuan QQ, 2018, IEEE J-STARS, V11, P978, DOI 10.1109/JSTARS.2018.2794888; Yuhas R. H., 1992, P SUMM 3 ANN JPL AIR, V1, P14; Zeng K, 2018, COMPUT BIOL MED, V99, P133, DOI 10.1016/j.compbiomed.2018.06.010; Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhang Yulun, 2018, P EUROPEAN C COMPUTE, P286; Zhou M., 2022, PROC IEEE C COMPUT V, P1798; Zhou M., 2022, 36 AAAI C ARTIFICIAL; Zhou M, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3137967; Zhuang Yueting, 2013, 27 AAAI C ART INT, P1070	90	0	0	14	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01699-1	http://dx.doi.org/10.1007/s11263-022-01699-1		OCT 2022	28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5J7DL		Green Submitted			2022-12-18	WOS:000869199700001
J	Zhang, JY; Li, SW; Luo, ZX; Fang, T; Yao, Y				Zhang, Jingyang; Li, Shiwei; Luo, Zixin; Fang, Tian; Yao, Yao			Vis-MVSNet: Visibility-Aware Multi-view Stereo Network	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Multi-view stereo; Visibility; MVSNet	RECONSTRUCTION	Learning-based multi-view stereo (MVS) methods have demonstrated promising results. However, very few existing networks explicitly take the pixel-wise visibility into consideration, resulting in erroneous cost aggregation from occluded pixels. In this paper, we explicitly infer and integrate the pixel-wise occlusion information in the MVS network via the matching uncertainty estimation. The pair-wise uncertainty map is jointly inferred with the pair-wise depth map, which is further used as weighting guidance during the multi-view cost volume fusion. As such, the adverse influence of occluded pixels is suppressed in the cost fusion. The proposed framework Vis-MVSNet significantly improves depth accuracy in reconstruction scenes with severe occlusion. Extensive experiments are performed on DTU, BlendedMVS, Tanks and Temples and ETH3D datasets to justify the effectiveness of the proposed framework.	[Zhang, Jingyang; Luo, Zixin; Yao, Yao] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China; [Li, Shiwei; Fang, Tian] Everest Innovat Technol, Kowloon, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Yao, Y (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.	jzhangbs@cse.ust.hk; sli@altiwre.com; zluoag@cse.ust.hk; fangtian@altizure.com; yyaoag@cse.ust.hk			Hong Kong RGC GRF [16206819, 16203518, T22-603/15N]	Hong Kong RGC GRF(Hong Kong Research Grants Council)	This work is supported by Hong Kong RGC GRF 16206819, 16203518 and T22-603/15N.	Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58; Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162; Cheng S, 2020, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR42600.2020.00260; Diebel J., CVPR, V1, P519; Furukawa Y, 2006, LECT NOTES COMPUT SC, V3951, P564; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106; Grum M, 2014, PATTERN RECOGN, V47, P326, DOI 10.1016/j.patcog.2013.04.020; Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257; Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339; Hartmann W, 2017, IEEE I CONF COMP VIS, P1595, DOI 10.1109/ICCV.2017.176; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298; Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59; Ji MQ, 2021, IEEE T PATTERN ANAL, V43, P4078, DOI 10.1109/TPAMI.2020.2996798; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Jianfeng Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P674, DOI 10.1007/978-3-030-58548-8_39; Jingyang Zhang, 2020, 2020 25th International Conference on Pattern Recognition (ICPR), P1611, DOI 10.1109/ICPR48806.2021.9412342; Kar Abhishek, 2017, LEARNING MULTIVIEW S, P2; Kendall A., 2017, NEURAL INFORM PROCES, V30; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim S, 2019, PROC CVPR IEEE, P205, DOI 10.1109/CVPR.2019.00029; Kim S, 2019, IEEE T IMAGE PROCESS, V28, P1299, DOI 10.1109/TIP.2018.2878325; Kingma D.P, P 3 INT C LEARNING R; Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599; Kuhn A, 2020, INT CONF 3D VISION, P404, DOI 10.1109/3DV50981.2020.00050; Kuhn A, 2019, LECT NOTES COMPUT SC, V11824, P18, DOI 10.1007/978-3-030-33676-9_2; Kuhn A, 2017, INT J COMPUT VISION, V124, P2, DOI 10.1007/s11263-016-0946-x; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44; Li ZX, 2020, IEEE T IMAGE PROCESS, V29, P7176, DOI 10.1109/TIP.2020.2999853; Liao J, 2019, COMPUT GRAPH FORUM, V38, P335, DOI 10.1111/cgf.13841; Merrell P, 2007, IEEE I CONF COMP VIS, P1221; Paschalidou D, 2018, PROC CVPR IEEE, P3897, DOI 10.1109/CVPR.2018.00410; Poggi M., 2016, PROC BRIT MACH VIS C, P4; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Slabaugh GG, 2004, INT J COMPUT VISION, V57, P179, DOI 10.1023/B:VISI.0000013093.45070.3b; Sormann C, 2020, INT CONF 3D VISION, P394, DOI 10.1109/3DV50981.2020.00049; Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8; Tosi F, 2018, LECT NOTES COMPUT SC, V11210, P323, DOI 10.1007/978-3-030-01231-1_20; Wang FJ, 2021, PROC CVPR IEEE, P14189, DOI 10.1109/CVPR46437.2021.01397; Xu QS, 2020, AAAI CONF ARTIF INTE, V34, P12516; Xu QS, 2019, PROC CVPR IEEE, P5478, DOI 10.1109/CVPR.2019.00563; Xu ZY, 2020, PROC CVPR IEEE, P5980, DOI 10.1109/CVPR42600.2020.00602; Xue YZ, 2019, IEEE I CONF COMP VIS, P4311, DOI 10.1109/ICCV.2019.00441; Yang JY, 2020, PROC CVPR IEEE, P4876, DOI 10.1109/CVPR42600.2020.00493; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yao Y, 2020, PROC CVPR IEEE, P1787, DOI 10.1109/CVPR42600.2020.00186; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; Yao Y, 2017, INT CONF 3D VISION, P185, DOI 10.1109/3DV.2017.00030; Zhang J, 2020, BRIT MACHINE VISION; Zhang RZ, 2015, IEEE I CONF COMP VIS, P2084, DOI 10.1109/ICCV.2015.241; Zheng EL, 2014, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2014.196	56	0	0	8	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01697-3	http://dx.doi.org/10.1007/s11263-022-01697-3		OCT 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5Q6QE					2022-12-18	WOS:000873952800001
J	Lin, WY; Liu, SY; Dai, BT; Li, HD				Lin, Wen-Yan; Liu, Siying; Dai, Bing Tian; Li, Hongdong			Distance Based Image Classification: A solution to generative classification's conundrum?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Incremental learning; High dimensions; Statistics; Shell theory; Generative classifiers; Anomaly detection; Nearest neighbor; Distance		Most classifiers rely on discriminative boundaries that separate instances of each class from everything else. We argue that discriminative boundaries are counter-intuitive as they define semantics by what-they-are-not; and should be replaced by generative classifiers which define semantics by what-they-are. Unfortunately, generative classifiers are significantly less accurate. This may be caused by the tendency of generative models to focus on easy to model semantic generative factors and ignore non-semantic factors that are important but difficult to model. We propose a new generative model in which semantic factors are accommodated by shell theory's Wen-Yan et al. (IEEE Trans Pattern Anal Mach Intell, 2021) hierarchical generative process and non-semantic factors by an instance specific noise term. We use the model to develop a classification scheme which suppresses the impact of noise while preserving semantic cues. The result is a surprisingly accurate generative classifier, that takes the form of a modified nearest-neighbor algorithm; we term it distance classification. Unlike discriminative classifiers, a distance classifier: defines semantics by what-they-are; is amenable to incremental updates; and scales well with the number of classes.	[Lin, Wen-Yan; Dai, Bing Tian] Singapore Management Univ, Singapore, Singapore; [Liu, Siying] Inst Infocomm Res, Singapore, Singapore; [Li, Hongdong] Australia Natl Univ, Canberra, ACT, Australia	Singapore Management University; Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R); Australian National University	Lin, WY (corresponding author), Singapore Management Univ, Singapore, Singapore.	daniellin@smu.edu.sg; liusy1@i2r.a-star.edu.sg; btdai@smu.edu.sg; hongdong.li@anu.edu.au			Lee Kong Chian foundation	Lee Kong Chian foundation	We would like to thank Ng Hongwei of Blackmagic Design for many hours of fruitful discussions; and the Lee Kong Chian foundation for supporting our work.	Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101; Aggarwal CC, 2001, LECT NOTES COMPUT SC, V1973, P420; Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217; Beyer L., 2020, ARXIV, DOI DOI 10.48550/ARXIV.2006.07159; Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15; Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946; Chen Z, 2018, SYNTHESIS LECT ARTIF, V12, P1, DOI [10.2200/S00737ED1V01Y201610AIM033, DOI 10.2200/S00737ED1V01Y201610AIM033]; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Elson J, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P366; Han Xiao, 2017, Arxiv, DOI arXiv:1708.07747; Hayes T. L., 2019, EURCONFCOMPUTVIS; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428; Hyvonen V, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P881, DOI 10.1109/BigData.2016.7840682; Jaasaari E, 2019, LECT NOTES ARTIF INT, V11440, P590, DOI 10.1007/978-3-030-16145-3_46; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Kuo YH, 2011, PROC CVPR IEEE, P905, DOI 10.1109/CVPR.2011.5995639; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledent A., 2021, ADV NEURAL INFORM PR, V34, P25540; Lin W. -Y., 2022, P EUROPEAN C COMPUTE; Lin WY, 2022, IEEE T PATTERN ANAL, V44, P6438, DOI 10.1109/TPAMI.2021.3084598; Lin WY, 2018, PROC CVPR IEEE, P5784, DOI 10.1109/CVPR.2018.00606; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Lu ZC, 2021, IEEE T PATTERN ANAL, V43, P2971, DOI 10.1109/TPAMI.2021.3052758; Markopoulos PP, 2017, IEEE T SIGNAL PROCES, V65, P4252, DOI 10.1109/TSP.2017.2708023; Nilsback M-E., 2006, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2006., DOI 10.1109/CVPR.2006.42]; Peterson L., 2009, SCHOLARPEDIA, V4, P1883, DOI [10.4249/scholarpedia.1883, DOI 10.4249/SCHOLARPEDIA.1883]; Platt JC, 2000, ADV NEUR IN, P61; Rao D, 2019, ADV NEUR IN, V32; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Rennie J. D., 2003, P 20 INT C MACH LEAR, P616; Sanderson M, 2010, NAT LANG ENG, V16, P100, DOI 10.1017/S1351324909005129; van de Ven Gido M, 2021, P IEEE CVF C COMP VI, P3611; wikipedia.org, US; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Wu Y, 2019, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2019.00046; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yoon J., 2017, INTCONFLEARNREPRESEN; Zhang H, 2005, INT J PATTERN RECOGN, V19, P183, DOI 10.1142/S0218001405003983; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	46	0	0	5	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01675-9	http://dx.doi.org/10.1007/s11263-022-01675-9		OCT 2022	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5F4MX		Green Accepted, Green Submitted			2022-12-18	WOS:000866292400001
J	Kazemi, E; Kerdreux, T; Wang, LQ				Kazemi, Ehsan; Kerdreux, Thomas; Wang, Liqiang			Minimally Distorted Structured Adversarial Attacks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Adversarial attacks; Blurriness; Group norm; Image classification	WOLFE	White box adversarial perturbations are generated via iterative optimization algorithms most often by minimizing an adversarial loss on a l(p) neighborhood of the original image, the so-called distortion set. Constraining the adversarial search with different norms results in disparately structured adversarial examples. Here we explore several distortion sets with structure-enhancing algorithms. These new structures for adversarial examples might provide challenges for provable and empirical robust mechanisms. Because adversarial robustness is still an empirical field, defense mechanisms should also reasonably be evaluated against differently structured attacks. Besides, these structured adversarial perturbations may allow for larger distortions size than their l(p) counterpart while remaining imperceptible or perceptible as natural distortions of the image. We will demonstrate in this work that the proposed structured adversarial examples can significantly bring down the classification accuracy of adversarially trained classifiers while showing a low l(2) distortion rate. For instance, on ImagNet dataset the structured attacks drop the accuracy of the adversarial model to near zero with only 50% of l(2) distortion generated using white-box attacks like PGD. As a byproduct, our findings on structured adversarial examples can be used for adversarial regularization of models to make models more robust or improve their generalization performance on datasets that are structurally different.	[Kazemi, Ehsan; Wang, Liqiang] Univ Cent Florida, Dept Comp Sci, Orlando, FL 32816 USA; [Kerdreux, Thomas] TU Univ Berlin, Zuse Inst Berlin, Berlin, Germany	State University System of Florida; University of Central Florida; Zuse Institute Berlin	Kazemi, E (corresponding author), Univ Cent Florida, Dept Comp Sci, Orlando, FL 32816 USA.	ehsan_kazemy@knights.ucf.edu; thomaskerdreux@gmail.com; lwang@cs.ucf.edu						Adrian Vladu, 2019, Arxiv, DOI arXiv:1706.06083; Aleksander Madry, 2019, Arxiv, DOI arXiv:1902.06705; Alexandre d'Aspremont, 2018, Arxiv, DOI arXiv:1803.07348; Alexandre d'Aspremont, 2020, Arxiv, DOI arXiv:2004.11053; Alon Gonen, 2011, Arxiv, DOI arXiv:1106.1622; Anish Athalye, 2018, Arxiv, DOI arXiv:1802.00420; Atara Kaplan, 2019, Arxiv, DOI arXiv:1802.05581; Ayon Sen, 2019, Arxiv, DOI arXiv:1906.02439; Bing Yu, 2020, Arxiv, DOI arXiv:2002.03500; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Changshui Zhang, 2019, Arxiv, DOI arXiv:1906.04392; Connelly Barnes, 2017, Arxiv, DOI arXiv:1701.08893; Croce F, 2020, PR MACH LEARN RES, V119; Croce F, 2019, IEEE I CONF COMP VIS, P4723, DOI 10.1109/ICCV.2019.00482; Cui J., 2021, P IEEECVF INT C COMP, P15721; Dan Garber, 2015, Arxiv, DOI arXiv:1301.4666; David Andersen, 2018, Arxiv, DOI arXiv:1807.06732; Demyanov V. F., 1970, MODERN ANAL COMPUTAT; Dheevatsa Mudigere, 2017, Arxiv, DOI arXiv:1609.04836; Dina Katabi, 2019, Arxiv, DOI arXiv:1905.11971; Dongruo Zhou, 2019, Arxiv, DOI arXiv:1811.10828; Dongxian Wu, 2020, Arxiv, DOI arXiv:2004.05884; Dudik M., 2012, ARTIF INTELL, P327; DUNN JC, 1979, SIAM J CONTROL OPTIM, V17, P187, DOI 10.1137/0317015; Edward Cheung, 2017, Arxiv, DOI arXiv:1704.04285; Elan Rosenfeld, 2019, Arxiv, DOI arXiv:1902.02918; Engstrom L., 2017, ARXIV; Eric Wong, 2020, Arxiv, DOI arXiv:1902.07906; Eric Wong, 2020, Arxiv, DOI arXiv:2007.08450; Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Freund RM, 2017, SIAM J OPTIMIZ, V27, P319, DOI 10.1137/15M104726X; Garber D, 2015, PR MACH LEARN RES, V37, P541; Garber D, 2013, ANN IEEE SYMP FOUND, P420, DOI 10.1109/FOCS.2013.52; Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397; Goodfellow I. J., 2015, P ICLR; Gragnaniello D, 2021, PATTERN RECOGN LETT, V147, P142, DOI 10.1016/j.patrec.2021.03.033; Greg Yang, 2020, Arxiv, DOI arXiv:2002.08118; GUELAT J, 1986, MATH PROGRAM, V35, P110, DOI 10.1007/BF01589445; Guo C., 2018, ARXIV; Hameed M. Z., 2021, ARXIV; Harchaoui Z, 2012, PROC CVPR IEEE, P3386, DOI 10.1109/CVPR.2012.6248078; Jaggi M., 2010, ICML; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Lacoste-Julien S., 2013, ARXIV; Lacoste-Julien S, 2015, ADV NEURAL INFORM PR, V28, P496; Levitin Evgeny S, 1966, USSR COMP MATH MATH, V6, P1, DOI DOI 10.1016/0041-5553(66)90114-5; Li Y., 2017, ADV NEURAL INFORM PR, P6191; Liu H. T. D., 2018, INT C LEARN REPR; Lu M, 2017, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2017.270; Luo B, 2018, AAAI CONF ARTIF INTE, P1652; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Langenberg P, 2019, Arxiv, DOI arXiv:1901.10371; Rauber J., 2017, REL MACH LEARN WILD; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Schmidt L, 2018, ADV NEUR IN, V31; Sharif M, 2018, IEEE COMPUT SOC CONF, P1686, DOI 10.1109/CVPRW.2018.00211; Stutz D, 2019, PROC CVPR IEEE, P6969, DOI 10.1109/CVPR.2019.00714; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xu K., 2018, ARXIV; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319	68	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01701-w	http://dx.doi.org/10.1007/s11263-022-01701-w		OCT 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5E5VB					2022-12-18	WOS:000865690900002
J	Xiao, MQ; Zheng, SX; Liu, C; Lin, ZC; Liu, TY				Xiao, Mingqing; Zheng, Shuxin; Liu, Chang; Lin, Zhouchen; Liu, Tie-Yan			Invertible Rescaling Network and Its Extensions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Image degradation and restoration; Invertible neural network; Information loss; Image rescaling; Image decolorization-colorization; Image compression	IMAGE COMPRESSION; SUPERRESOLUTION	Image rescaling is a commonly used bidirectional operation, which first downscales high-resolution images to fit various display screens or to be storage- and bandwidth-friendly, and afterward upscales the corresponding low-resolution images to recover the original resolution or the details in the zoom-in images. However, the non-injective downscaling mapping discards high-frequency contents, leading to the ill-posed problem for the inverse restoration task. This can be abstracted as a general image degradation-restoration problem with information loss. In this work, we propose a novel invertible framework to handle this general problem, which models the bidirectional degradation and restoration from a new perspective, i.e. invertible bijective transformation. The invertibility enables the framework to model the information loss of pre-degradation in the form of distribution, which could mitigate the ill-posed problem during post-restoration. To be specific, we develop invertible models to generate valid degraded images and meanwhile transform the distribution of lost contents to the fixed distribution of a latent variable during the forward degradation. Then restoration is made tractable by applying the inverse transformation on the generated degraded image together with a randomly-drawn latent variable. We start from image rescaling and instantiate the model as Invertible Rescaling Network, which can be easily extended to the similar decolorization-colorization task. We further propose to combine the invertible framework with existing degradation methods such as image compression for wider applications. Experimental results demonstrate the significant improvement of our model over existing methods in terms of both quantitative and qualitative evaluations of upscaling and colorizing reconstruction from downscaled and decolorized images, and rate-distortion of image compression. Code is available at https:// github.com/pkuxmq/Invertible-Image-Rescaling.	[Xiao, Mingqing; Lin, Zhouchen] Peking Univ, Sch Intelligence Sci & Technol, Key Lab Machine Percept MoE, Beijing 100871, Peoples R China; [Zheng, Shuxin; Liu, Chang; Liu, Tie-Yan] Microsoft Res Asia, Machine Learning Grp, Beijing, Peoples R China; [Lin, Zhouchen] Peng Cheng Lab, Shenzhen, Peoples R China	Peking University; Microsoft; Microsoft Research Asia; Peng Cheng Laboratory	Lin, ZC (corresponding author), Peking Univ, Sch Intelligence Sci & Technol, Key Lab Machine Percept MoE, Beijing 100871, Peoples R China.; Liu, C (corresponding author), Microsoft Res Asia, Machine Learning Grp, Beijing, Peoples R China.; Lin, ZC (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	mingqing_xiao@pku.edu.cn.com; Shuxin.Zheng@microsoft.com; Chang.Liu@microsoft.com; zlin@pku.edu.cn; Tie-Yan.Liu@microsoft.com			major key project of PCL [PCL2021A12]; NSF China [62276004]	major key project of PCL; NSF China(National Natural Science Foundation of China (NSFC))	The authors would like to thank Yaolong Wang, Di He, Guolin Ke and Jiang Bian for their help on discussions, experiments and writing in the preliminary version of this paper. The authors would also like to thank the reviewers for their valuable suggestions. Z. Lin was supported by the major key project of PCL (grant no. PCL2021A12) and the NSF China (No. 62276004).	Agustsson E., 2017, P IEEE C COMP VIS PA; Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031; Ardizzone L., 2018, INT C LEARN REPR; Ardizzone L, 2019, ARXIV; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Asim M, 2020, PR MACH LEARN RES, V119; Ba J, 2014, ADV NEURAL INFORM PR; Bala R, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P82; Balle J., 2018, P INT C LEARN REPR; Balle Johannes, 2017, ICLR 2017; Behrmann J, 2019, PR MACH LEARN RES, V97; Bengio Y., 2013, ARXIV; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21; Bruckstein AM, 2003, IEEE T IMAGE PROCESS, V12, P1132, DOI 10.1109/TIP.2003.816023; Chang Liu, 2020, Arxiv, DOI arXiv:2006.11999; Chen RTQ, 2019, ADV NEUR IN, V32; Chen YZ, 2020, IEEE IMAGE PROC, P523, DOI 10.1109/ICIP40778.2020.9190729; Cheng K. L., 2021, P IEEE INT C COMPUTE; Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132; Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307; Dinh L., 2017, P INT C LEARNING REP; Dinh Laurent, 2014, ARXIV14108516; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Franzen Rich, 1999, KODAK LOSSLESS TRUE; Giachetti A, 2011, IEEE T IMAGE PROCESS, V20, P2760, DOI 10.1109/TIP.2011.2136352; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; Goodfellow I., 2014, NIPS; Grathwohl W., 2019, P INT C LEARN REPR; Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545; Hensel M, 2017, ADV NEUR IN, V30; Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156; Huang Y.-C., 2021, P IEEE C COMPUTER VI; Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3; Jacobsen J orn-Henrik, 2018, P ICLR; Xing JB, 2022, Arxiv, DOI arXiv:2201.12576; Jing JP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4713, DOI 10.1109/ICCV48922.2021.00469; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kim H, 2018, LECT NOTES COMPUT SC, V11208, P419, DOI 10.1007/978-3-030-01225-0_25; Kingma D.P., 2015, INT C LEARN REPR ICL; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kobyzev I, 2021, IEEE T PATTERN ANAL, V43, P3964, DOI 10.1109/TPAMI.2020.2992934; Kopf J, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508370; Kumar M., 2020, P INT C LEARNING REP; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780; Li M, 2021, IEEE T PATTERN ANAL, V43, P3446, DOI 10.1109/TPAMI.2020.2983926; Li Y, 2019, IEEE T IMAGE PROCESS, V28, P1092, DOI 10.1109/TIP.2018.2872876; Li Z., 2019, P ACM MULTIMEDIA ASI; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Lin WS, 2006, IEEE T IMAGE PROCESS, V15, P2513, DOI 10.1109/TIP.2006.877415; Liu C., 2021, ADV NEURAL INFORM PR; Liu JJ, 2018, IEEE T IMAGE PROCESS, V27, P1076, DOI 10.1109/TIP.2017.2772838; Liu QG, 2015, IEEE T IMAGE PROCESS, V24, P2889, DOI 10.1109/TIP.2015.2423615; Liu Y, 2021, PROC CVPR IEEE, P13360, DOI 10.1109/CVPR46437.2021.01316; Lu Cheng, 2021, INT C LEARNING REPRE; Lu S.-P., 2021, P IEEE C COMPUTER VI; Lugmayr A, 2020, P EUROPEAN C COMPUTE; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Minnen D, 2018, ADV NEUR IN, V31; Mitchell D. P., 1988, Computer Graphics, V22, P221, DOI 10.1145/378456.378514; Oztireli AC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766891; Ren Simiao, 2020, ADV NEURAL INFORM PR; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rippel O, 2017, PR MACH LEARN RES, V70; Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003; SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969; Shen MM, 2011, IEEE T CIRC SYST VID, V21, P755, DOI 10.1109/TCSVT.2011.2130390; Sneyers J, 2016, IEEE IMAGE PROC, P66, DOI 10.1109/ICIP.2016.7532320; Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191; Sun WJ, 2020, IEEE T IMAGE PROCESS, V29, P4027, DOI 10.1109/TIP.2020.2970248; Teshima T., 2020, ADV NEURAL INFORM PR; Tian Y, 2021, P IEEE INT C COMPUTE; van den Berg R, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P393; van der Ouderaa TFA, 2019, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR.2019.00485; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Weber N, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980239; Wu XL, 2009, IEEE T IMAGE PROCESS, V18, P552, DOI 10.1109/TIP.2008.2010638; Xia MH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275080; Xiao M., 2020, P EUROPEAN C COMPUTE; Xie Y., 2021, P 29 ACM INT C MULT; Xing YZ, 2021, PROC CVPR IEEE, P6283, DOI 10.1109/CVPR46437.2021.00622; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Ye TZ, 2020, IEEE ACCESS, V8, P89670, DOI 10.1109/ACCESS.2020.2994148; Yeo H, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P645; Yeo H, 2017, HOTNETS-XVI: PROCEEDINGS OF THE 16TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS, P57, DOI 10.1145/3152434.3152440; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhang Yulun, 2018, P EUROPEAN C COMPUTE; Zhao R, 2021, IEEE T IMAGE PROCESS, V30, P6081, DOI 10.1109/TIP.2021.3091902; Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796; Zhong ZS, 2018, ADV NEUR IN, V31; Zhu XB, 2019, AAAI CONF ARTIF INTE, P5981, DOI 10.1609/aaai.v33i01.33015981	99	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01688-4	http://dx.doi.org/10.1007/s11263-022-01688-4		OCT 2022	26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5E5VB		Green Submitted			2022-12-18	WOS:000865690900003
J	Yang, FL; Zhao, Y; Wang, XC				Yang, Fengli; Zhao, Yue; Wang, Xuechun			Common Pole-Polar Properties of Central Catadioptric Sphere and Line Images Used for Camera Calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Camera calibration; Sphere and line images; Image models; Pole-polar properties	GEOMETRIC-PROPERTIES; POINT	Central catadioptric cameras with a single effective viewpoint contain both mirrors and pinhole cameras that increase the imaging field of view. In this study, the common pole-polar properties of central catadioptric sphere or line images are investigated and used for camera calibration. From these properties, the pole and polar with respect to the image of absolute conic and the modified image of absolute conic, respectively, can be recovered according to the generalized eigenvalue decomposition. Moreover, these techniques are valid for paracatadioptric sensors with the degenerate conic dual to the circular points being considered. At least three images of spheres or lines are required to completely calibrate any central catadioptric camera. The intrinsic parameters of the camera, the shape of reflective mirror, and the distortion parameters can be linearly estimated using the algebraic and geometric constraints of the sphere or line images obtained by the central catadioptric camera. The obtained experimental results demonstrate the effectiveness and feasibility of the proposed calibration algorithm.	[Yang, Fengli; Zhao, Yue; Wang, Xuechun] Yunnan Univ, Inst Math & Stat, Kunming 650091, Yunnan, Peoples R China	Yunnan University	Zhao, Y (corresponding author), Yunnan Univ, Inst Math & Stat, Kunming 650091, Yunnan, Peoples R China.	739875493@qq.com; zhao6685@yeah.net; 1329269925@qq.com			National Natural Science Foundation of China [61663048, 11861075]; Program for Innovative Research Team in Science and Technology for the Universities of Yunnan Province; Key Joint Project of the Science and Technology Department of Yunnan Province and Yunnan University [2018FY001(-014)]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Innovative Research Team in Science and Technology for the Universities of Yunnan Province; Key Joint Project of the Science and Technology Department of Yunnan Province and Yunnan University	This work was supported in part by: the National Natural Science Foundation of China (grant numbers 61663048 and 11861075); the Program for Innovative Research Team in Science and Technology for the Universities of Yunnan Province; and the Key Joint Project of the Science and Technology Department of Yunnan Province and Yunnan University (grant 2018FY001(-014)).	Andrew Alex M, 2001, MULTIPLE VIEW GEOMET; Baker S, 1999, INT J COMPUT VISION, V35, P175, DOI 10.1023/A:1008128724364; Baker S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P35, DOI 10.1109/ICCV.1998.710698; Barreto J. P. d.A, 2004, THESIS; Barreto JP, 2006, COMPUT VIS IMAGE UND, V101, P151, DOI 10.1016/j.cviu.2005.07.002; Barreto JP, 2005, IEEE T PATTERN ANAL, V27, P1327, DOI 10.1109/TPAMI.2005.163; Daucher N., 1994, CAMERA CALIBRATION S; Deng Xiao-Ming, 2007, Acta Automatica Sinica, V33, P801, DOI 10.1360/aas-007-0801; Duan HX, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P56; Duan HX, 2014, LECT NOTES ARTIF INT, V8818, P229, DOI 10.1007/978-3-319-11740-9_22; Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022; Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658; Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135; Geyer Christopher, 2000, LNCS, P445, DOI DOI 10.1007/3-540-45053-X_29; Jimenez J, 2012, CONCURR COMP-PRACT E, V24, P1551, DOI 10.1002/cpe.1900; Kang SB, 2000, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2000.855820; Liu Z, 2017, OPT EXPRESS, V25, P15268, DOI 10.1364/OE.25.015269; Mei C, 2007, IEEE INT CONF ROBOT, P3945, DOI 10.1109/ROBOT.2007.364084; Puig L, 2011, INT J COMPUT VISION, V93, P101, DOI 10.1007/s11263-010-0411-1; Scaramuzza D, 2008, IEEE T ROBOT, V24, P1015, DOI 10.1109/TRO.2008.2004490; Scaramuzza D, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P5695, DOI 10.1109/IROS.2006.282372; Schonbein M, 2014, IEEE INT CONF ROBOT, P4443, DOI 10.1109/ICRA.2014.6907507; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; Svoboda T, 2002, INT J COMPUT VISION, V49, P23, DOI 10.1023/A:1019869530073; Tahri O, 2012, IEEE INT C INT ROBOT, P1683, DOI 10.1109/IROS.2012.6385668; Wu FC, 2008, PATTERN RECOGN, V41, P3166, DOI 10.1016/j.patcog.2008.03.010; Wu YH, 2004, LECT NOTES COMPUT SC, V3021, P190; Yang FL, 2020, IEEE ACCESS, V8, P28324, DOI 10.1109/ACCESS.2020.2972029; Ying X., 2006, INTERPRETING SPHERE; Ying XG, 2004, LECT NOTES COMPUT SC, V3021, P442; Ying XH, 2004, IEEE T PATTERN ANAL, V26, P1260, DOI 10.1109/TPAMI.2004.79; Ying XH, 2008, INT J COMPUT VISION, V78, P89, DOI 10.1007/s11263-007-0082-8; Ying XH, 2006, IEEE T PATTERN ANAL, V28, P2031, DOI 10.1109/TPAMI.2006.245; Zhang H, 2007, IEEE T PATTERN ANAL, V29, P499, DOI 10.1109/TPAMI.2007.45; Zhang L, 2011, J ZHEJIANG U-SCI C, V12, P239, DOI 10.1631/jzus.C1000043; Zhao Y, 2020, OSA CONTINUUM, V3, P993, DOI 10.1364/OSAC.391088; Zhao Y, 2018, APPL OPTICS, V57, P4345, DOI 10.1364/AO.57.004345	37	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01696-4	http://dx.doi.org/10.1007/s11263-022-01696-4		OCT 2022	13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5E5VB					2022-12-18	WOS:000865690900001
J	Logothetis, F; Mecca, R; Budvytis, I; Cipolla, R				Logothetis, Fotios; Mecca, Roberto; Budvytis, Ignas; Cipolla, Roberto			A CNN Based Approach for the Point-Light Photometric Stereo Problem	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Photometric stereo; Point-light; Convolutional neural network	SHAPE; INTEGRABILITY; MODEL	Reconstructing the 3D shape of an object using several images under different light sources is a very challenging task, especially when realistic assumptions such as light propagation and attenuation, perspective viewing geometry and specular light reflection are considered. Many of works tackling Photometric Stereo (PS) problems often relax most of the aforementioned assumptions. Especially they ignore specular reflection and global illumination effects. In this work, we propose a CNN-based approach capable of handling these realistic assumptions by leveraging recent improvements of deep neural networks for far-field Photometric Stereo and adapt them to the point light setup. We achieve this by employing an iterative procedure of point-light PS for shape estimation which has two main steps. Firstly we train a per-pixel CNN to predict surface normals from reflectance samples. Secondly, we compute the depth by integrating the normal field in order to iteratively estimate light directions and attenuation which is used to compensate the input images to compute reflectance samples for the next iteration. Our approach sigificantly outperforms the state-of-the-art on the DiLiGenT real world dataset. Furthermore, in order to measure the performance of our approach for near-field point-light source PS data, we introduce LUCES the first real-world 'dataset for near-fieLd point light soUrCe photomEtric Stereo' of 14 objects of different materials were the effects of point light sources and perspective viewing are a lot more significant. Our approach also outperforms the competition on this dataset as well. Data and test code are available at the project page.	[Logothetis, Fotios; Mecca, Roberto] Toshiba Europe Ltd, Cambridge, England; [Budvytis, Ignas; Cipolla, Roberto] Univ Cambridge, Cambridge, England	Toshiba Corporation; University of Cambridge	Logothetis, F (corresponding author), Toshiba Europe Ltd, Cambridge, England.	flogothetis@crl.toshiba.co.uk; rmecca@crl.toshiba.co.uk; ib255@cam.ac.uk; rc10001@cam.ac.uk						Aanaes H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9; Aanaes H, 2012, INT J COMPUT VISION, V97, P18, DOI 10.1007/s11263-011-0473-8; Ackermann J, 2013, FOUND TRENDS COMPUT, V9, P149, DOI 10.1561/0600000065; Agrawal A, 2006, LECT NOTES COMPUT SC, V3951, P578; Alldrin N, 2008, PROC CVPR IEEE, P2447; [Anonymous], 2018, BLEND ONL COMM BLEND; Blinn J. F., 1977, SIGGRAPH; Burley Brent, 2012, SIGGRAPH 2012; Chandraker M.K., 2007, P IEEE C COMP VIS PA; Chen GY, 2022, IEEE T PATTERN ANAL, V44, P129, DOI 10.1109/TPAMI.2020.3005397; Chen GY, 2019, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR.2019.00894; Chen GY, 2018, LECT NOTES COMPUT SC, V11213, P3, DOI 10.1007/978-3-030-01240-3_1; Cignoni Paolo, 2008, EUROGRAPHICS ITALIAN, DOI 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136; Clark J. J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P29, DOI 10.1109/CVPR.1992.223231; Collins T, 2012, LECT NOTES COMPUT SC, V7511, P634, DOI 10.1007/978-3-642-33418-4_78; Deguchi K, 1996, PROCEEDINGS OF THE IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, P290, DOI 10.1109/MMBIA.1996.534081; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; Hinton G.E., 2009, SCHOLARPEDIA, V4, P5947, DOI [DOI 10.4249/SCHOLARPEDIA.5947, 10.4249/scholarpedia.5947]; Huang G., 2017, IEEE C COMPUTER VISI; Hui Z, 2017, IEEE T PATTERN ANAL, V39, P2060, DOI 10.1109/TPAMI.2016.2623613; Ikehata S, 2018, LECT NOTES COMPUT SC, V11219, P3, DOI 10.1007/978-3-030-01267-0_1; Ikehata S, 2014, PROC CVPR IEEE, P2187, DOI 10.1109/CVPR.2014.280; Ikehata S, 2012, PROC CVPR IEEE, P318, DOI 10.1109/CVPR.2012.6247691; Iwahori Y., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P83, DOI 10.1109/ICPR.1990.118069; Ju YK, 2018, IEEE ACCESS, V6, P30804, DOI 10.1109/ACCESS.2018.2840138; Konstantinou C, 2021, MATERIALS, V14, DOI 10.3390/ma14164735; LEE S, 1991, IMAGE VISION COMPUT, V9, P39, DOI 10.1016/0262-8856(91)90047-S; Li ZQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275055; Liu C., 2018, ICCV; Logothetis, 2020, P 31 BRIT MACH VIS C; Logothetis F., 2021, ICCV; Logothetis F., 2016, BRIT MACH VIS C BMVC; Logothetis F, 2019, IEEE I CONF COMP VIS, P1052, DOI 10.1109/ICCV.2019.00114; Logothetis F, 2017, PROC CVPR IEEE, P4521, DOI 10.1109/CVPR.2017.481; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; Mecca R, 2015, COMPUT GRAPH-UK, V51, P8, DOI 10.1016/j.cag.2015.05.020; Mecca R., 2021, BRIT MACHINE VISION; Mecca R, 2016, SIAM J IMAGING SCI, V9, P1858, DOI 10.1137/16M1068177; Mecca R, 2014, SIAM J IMAGING SCI, V7, P2732, DOI 10.1137/140968100; Mecca R, 2013, SIAM J IMAGING SCI, V6, P616, DOI 10.1137/110857258; ONN R, 1990, INT J COMPUT VISION, V5, P105, DOI 10.1007/BF00056773; Parot V, 2013, J BIOMED OPT, V18, DOI 10.1117/1.JBO.18.7.076017; Prados E, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P826; Queau<prime> Yvain, 2015, SSVM; Queau Y, 2018, J MATH IMAGING VIS, V60, P313, DOI 10.1007/s10851-017-0761-1; Queau Y, 2016, PROC CVPR IEEE, P4359, DOI 10.1109/CVPR.2016.472; Queau Y, 2016, SIGNAL PROCESS-IMAGE, V40, P65, DOI 10.1016/j.image.2015.11.006; Saiz FA, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22030882; Santo Hiroaki, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P137, DOI 10.1007/978-3-030-58598-3_9; Santo H, 2017, IEEE INT CONF COMP V, P501, DOI 10.1109/ICCVW.2017.66; Shi BX, 2019, IEEE T PATTERN ANAL, V41, P271, DOI 10.1109/TPAMI.2018.2799222; Shi BX, 2016, PROC CVPR IEEE, P3707, DOI 10.1109/CVPR.2016.403; SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103; Smith W, 2016, COMPUT VIS IMAGE UND, V145, P128, DOI 10.1016/j.cviu.2015.11.019; Tang Y., 2012, ICML; Taniai T, 2018, PR MACH LEARN RES, V80; Tankus A, 2005, IEEE I CONF COMP VIS, P611; Vogiatzis G, 2010, STUD COMPUT INTELL, V285, P313; Wang X, 2020, IEEE T IMAGE PROCESS, V29, P6032, DOI 10.1109/TIP.2020.2987176; Wetzler A., 2014, 3DV; Wolff L. B., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P247, DOI 10.1007/BFb0028358; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu CY, 2010, INT J COMPUT VISION, V86, P211, DOI 10.1007/s11263-009-0207-3; Xiong Y, 2015, IEEE T PATTERN ANAL, V37, P67, DOI 10.1109/TPAMI.2014.2343211; Yu Y, 2017, IEEE INT CONF COMP V, P526, DOI 10.1109/ICCVW.2017.69; Yuille AL, 1999, INT J COMPUT VISION, V35, P203, DOI 10.1023/A:1008180726317; Zheng Q, 2019, IEEE I CONF COMP VIS, P8548, DOI 10.1109/ICCV.2019.00864; Zhu D., 2020, EUR C COMP VIS ECCV	68	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01689-3	http://dx.doi.org/10.1007/s11263-022-01689-3		OCT 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5D5IN					2022-12-18	WOS:000864975300002
J	Yang, X; Zhang, HW; Gao, CY; Cai, JF				Yang, Xu; Zhang, Hanwang; Gao, Chongyang; Cai, Jianfei			Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Image captioning; Distinguishable neural modules; Soft module collocations		Humans tend to decompose a sentence into different parts like STH DO STH AT SOMEPLACE and then fill each part with certain content. Inspired by this, we follow the principle of modular design to propose a novel image captioner: learning to Collocate Visual-Linguistic Neural Modules (CVLNM). Unlike the widely used neural module networks in VQA, where the language (i.e., question) is fully observable, the task of collocating visual-linguistic modules is more challenging. This is because the language is only partially observable, for which we need to dynamically collocate the modules during the process of image captioning. To sum up, we make the following technical contributions to design and train our CVLNM: (1) distinguishable module design-four modules in the encoder including one linguistic module for function words and three visual modules for different content words (i.e., noun, adjective, and verb) and another linguistic one in the decoder for commonsense reasoning, (2) a self-attention based module controller for robustifying the visual reasoning, (3) a part-of-speech based syntax loss imposed on the module controller for further regularizing the training of our CVLNM. Extensive experiments on the MS-COCO dataset show that our CVLNM is more effective, e.g., achieving a new state-of-the-art 129.5 CIDEr-D, and more robust, e.g., being less likely to overfit to dataset bias and suffering less when fewer training samples are available. Codes are available at htips://github.com/GCYZSIJCVLMN.	[Yang, Xu] Southeast Univ, Sch Comp Sci & Engn, Nanjing, Peoples R China; [Zhang, Hanwang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore; [Gao, Chongyang] Northwestern Univ, Comp Sci Dept, Evanston, IL USA; [Cai, Jianfei] Monash Univ, Fac Informat Technol, Clayton, Vic, Australia	Southeast University - China; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Northwestern University; Monash University	Zhang, HW (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.	101013120@seu.edu.cn; hanwangzhang@ntu.edu.sg; cygao@u.northwestern.edu; Jianfei.Cai@monash.edu		yang, xu/0000-0002-8276-2679				Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Banerjee Satanjeev, 2005, P ACL WORKSH INTR EX, P65; Bengio S, 2015, ADV NEUR IN, V28; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bing Xu, 2015, Arxiv, DOI arXiv:1505.00853; Chen L, 2019, IEEE I CONF COMP VIS, P4612, DOI 10.1109/ICCV.2019.00471; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059; Daqing Liu, 2019, Arxiv, DOI arXiv:1812.03299; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Devlin J., 2018, P 2019 C N AM CHAPT, DOI [DOI 10.18653/V1/N19-1423, 10.18653/v1/N19-1423]; Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127; Gu JX, 2018, AAAI CONF ARTIF INTE, P6837; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Herdade S, 2019, ADV NEUR IN, V32; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu RH, 2018, LECT NOTES COMPUT SC, V11211, P55, DOI 10.1007/978-3-030-01234-2_4; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470; Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473; Jang Eric, 2017, P 5 INT C LEARN REPR; Jiang W., 2018, P EUR C COMP VIS ECC, P499; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kim Y., 2017, ABS170200887 CORR; Kingma D. P., 2015, 3 INT C LEARN REPR I, P1; Kitaev N, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2676; Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162; Kuznetsova P., 2012, P 50 ANN M ASS COMPU, V1, P359; Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477; Liu DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1416, DOI 10.1145/3240508.3240632; Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d; Locatello F, 2019, PR MACH LEARN RES, V97; Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Luo R., 2017, IMAGE CAPTIONING COD; Luo RT, 2018, PROC CVPR IEEE, P6964, DOI 10.1109/CVPR.2018.00728; Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information; Mascharka D, 2018, PROC CVPR IEEE, P4942, DOI 10.1109/CVPR.2018.00519; Miller A., 2016, P 2016 C EMPIRICAL M; Mitchell Margaret, 2012, EACL; Niu YL, 2019, PROC CVPR IEEE, P6672, DOI 10.1109/CVPR.2019.00684; Papineni K., 2002, P 40 ANN M ASS COMP, P311, DOI [10.3115/1073083.1073135, DOI 10.3115/1073083.1073135]; Qin Y, 2019, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2019.00856; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Rohrbach A., 2018, P 2018 C EMPIRICAL M; Shen Yikang, 2019, 7 INT C LEARNING REP; Shi JX, 2019, PROC CVPR IEEE, P8368, DOI 10.1109/CVPR.2019.00857; Simonyan K., 2015, ICLR; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556; Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678; Toutanova K, 2000, PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA, P63, DOI 10.3115/1117794.1117802; Vaswani A, 2017, ADV NEUR IN, V30; Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Yang X, 2019, IEEE I CONF COMP VIS, P4249, DOI 10.1109/ICCV.2019.00435; Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094; Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271; Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42; Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524; Yi KX, 2018, ADV NEUR IN, V31; Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611; Zha ZJ, 2022, IEEE T PATTERN ANAL, V44, P710, DOI 10.1109/TPAMI.2019.2909864; Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331	86	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01692-8	http://dx.doi.org/10.1007/s11263-022-01692-8		OCT 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5D5IN		Green Submitted			2022-12-18	WOS:000864975300001
J	Guo, XJ; Hu, QM				Guo, Xiaojie; Hu, Qiming			Low-light Image Enhancement via Breaking Down the Darkness	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Low-light image enhancement; Image decomposition; Divide and rule	HISTOGRAM EQUALIZATION; NETWORK; RETINEX	Images captured in low-light environments often suffer from complex degradation. Simply adjusting light would inevitably result in burst of hidden noise and color distortion. To seek results with satisfied lighting, cleanliness, and realism from degraded inputs, this paper presents a novel framework inspired by the divide-and-rule principle, greatly alleviating the degradation entanglement. Assuming that an image can be decomposed into texture (with possible noise) and color components, one can specifically execute noise removal and color correction along with light adjustment. For this purpose, we propose to convert an image from the RGB colorspace into a luminance-chrominance one. An adjustable noise suppression network is designed to eliminate noise in the brightened luminance, having the illumination map estimated to indicate noise amplification levels. The enhanced luminance further serves as guidance for the chrominance mapper to generate realistic colors. Extensive experiments are conducted to reveal the effectiveness of our design, and demonstrate its superiority over state-of-the-art alternatives both quantitatively and qualitatively on several benchmark datasets. Our code has been made publicly available at https://github. com/m ngcv/Bread.	[Guo, Xiaojie; Hu, Qiming] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China	Tianjin University	Guo, XJ (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.	xj.max.guo@gmail.com; huqiming@tju.edu.cn			National Natural Science Foundation of China [62072327]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The authors would like to thank the editors and reviewers for their effort in handling our submission, as well as the comments and suggestions that improve the quality of this paper. This work was supported by the National Natural Science Foundation of China under Grant no. 62072327.	Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734; Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129; Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218; Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513; Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347; Cheng HD, 2004, DIGIT SIGNAL PROCESS, V14, P158, DOI 10.1016/j.dsp.2003.07.002; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Fan Feng, 2017, Arxiv, DOI arXiv:1711.02488; Fan MH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2317, DOI 10.1145/3394171.3413757; Fang YM, 2020, IEEE T IMAGE PROCESS, V29, P1127, DOI 10.1109/TIP.2019.2940678; Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304; Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185; Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450; Ignatov A, 2020, IEEE COMPUT SOC CONF, P2275, DOI 10.1109/CVPRW50498.2020.00276; Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462; Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272; Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356; LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108; Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059; Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010; Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243; Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539; Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361; Lin Shanchuan, 2021, P IEEECVF C COMPUTER, P8762; Liu J., 2018, P BRIT MECH VIS C, P1; Liu ZW, 2018, IEEE T PATTERN ANAL, V40, P1814, DOI 10.1109/TPAMI.2017.2737535; Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008; Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526; Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726; Pisano ED, 1998, J DIGIT IMAGING, V11, P193, DOI 10.1007/BF03178082; Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412; Schwartz E, 2019, IEEE T IMAGE PROCESS, V28, P912, DOI 10.1109/TIP.2018.2872858; Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070; Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396; Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701; Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309; Wang WCV, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20133661; Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118; Wang Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2015, DOI 10.1145/3343031.3350983; Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235; Yang J, 2016, INT C PATT RECOG, P751, DOI 10.1109/ICPR.2016.7899725; Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850; Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1623, DOI 10.1145/3343031.3351069; Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x; Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926; Ge Z, 2021, Arxiv, DOI arXiv:2107.08430; Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106	49	0	0	7	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01667-9	http://dx.doi.org/10.1007/s11263-022-01667-9		OCT 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5A2WO		Green Submitted			2022-12-18	WOS:000862752400001
J	Qin, HT; Zhang, XG; Gong, RH; Ding, YF; Xu, Y; Liu, XL				Qin, Haotong; Zhang, Xiangguo; Gong, Ruihao; Ding, Yifu; Xu, Yi; Liu, Xianglong			Distribution-Sensitive Information Retention for Accurate Binary Neural Network	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Binary neural network; Network quantization; Model compression; Deep learning		Model binarization is an effective method of compressing neural networks and accelerating their inference process, which enables state-of-the-art models to run on resource-limited devices. Recently, advanced binarization methods have been greatly improved by minimizing the quantization error directly in the forward process. However, a significant performance gap still exists between the 1-bit model and the 32-bit one. The empirical study shows that binarization causes a great loss of information in the forward and backward propagation which harms the performance of binary neural networks (BNNs). We present a novel distribution-sensitive information retention network (DIR-Net) that retains the information in the forward and backward propagation by improving internal propagation and introducing external representations. The DIR-Net mainly relies on three technical contributions: (1) Information Maximized Binarization (1MB): minimizing the information loss and the binarization error of weights/activations simultaneously by weight balance and standardization; (2) Distribution-sensitive Tivo-stage Estimator (DTE): retaining the information of gradients by distribution-sensitive soft approximation by jointly considering the updating capability and accurate gradient; (3) Representation-align Binarization-aware Distillation (RBD): retaining the representation information by distilling the representations between full-precision and binarized networks. The DIR-Net investigates both forward and backward processes of BNNs from the unified information perspective, thereby providing new insight into the mechanism of network binarization. The three techniques in our DIR-Net are versatile and effective and can be applied in various structures to improve BNNs. Comprehensive experiments on the image classification and objective detection tasks show that our DIR-Net consistently outperforms the state-of-the-art binarization approaches under mainstream and compact architectures, such as ResNet, VGG, EfficientNet, DARTS, and MobileNet. Additionally, we conduct our DIR-Net on real-world resource-limited devices which achieves 11.1x storage saving and 5.4x speedup.	[Qin, Haotong; Zhang, Xiangguo; Gong, Ruihao; Ding, Yifu; Xu, Yi; Liu, Xianglong] Beihang Univ, State Key Lab Software Dev Environm, Beijing, Peoples R China; [Qin, Haotong; Ding, Yifu] Beihang Univ, Shen Yuan Honors Coll, Beijing, Peoples R China	Beihang University; Beihang University	Liu, XL (corresponding author), Beihang Univ, State Key Lab Software Dev Environm, Beijing, Peoples R China.	xlliu@buaa.edu.cn	Qin, Haotong/AGP-1834-2022	Qin, Haotong/0000-0001-7391-7539	National Key Research and Development Plan of China [2020AAA0103503]; National Natural Science Foundation of China [62022009, 61872021]; Beijing Nova Program of Science and Technology [Z191100001119050]; Academic Excellence Foundation of BUAA	National Key Research and Development Plan of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program of Science and Technology; Academic Excellence Foundation of BUAA	This work was supported by The National Key Research and Development Plan of China (2020AAA0103503), National Natural Science Foundation of China (62022009 and 61872021), Beijing Nova Program of Science and Technology (Z191100001119050), and the Academic Excellence Foundation of BUAA for PhD Students.	Adrian Bulat, 2019, Arxiv, DOI arXiv:1904.05868; Adrian Bulat, 2019, Arxiv, DOI arXiv:1909.13863; Ajanthan T, 2019, IEEE I CONF COMP VIS, P4870, DOI 10.1109/ICCV.2019.00497; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2020, NIHUI BUG1989 HOWAVE; [Anonymous], 2021, AKAMASTER PYTORCH RE; [Anonymous], 2021, KUANGLIU YPWHS FDUCA; Baochang Zhang, 2018, Arxiv, DOI arXiv:1811.12755; Bengio Y., 2013, ARXIV, DOI 10.1007/978-3-642-39593-2_1; Bethge J, 2021, IEEE WINT CONF APPL, P1438, DOI 10.1109/WACV48630.2021.00148; Bin Liu, 2016, Arxiv, DOI arXiv:1605.04711; Cai ZW, 2017, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR.2017.574; Cao SJ, 2019, PROC CVPR IEEE, P11208, DOI 10.1109/CVPR.2019.01147; Chen H, 2020, ICCV; Chen SY, 2019, ADV NEUR IN, V32; Chen YT, 2018, AAAI CONF ARTIF INTE, P2852; Daniel Soudry, 2019, Arxiv, DOI arXiv:1810.05723; Daniel Soudry, 2016, Arxiv, DOI arXiv:1602.02830; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding HS, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.07.002; Ding RZ, 2019, PROC CVPR IEEE, P11400, DOI 10.1109/CVPR.2019.01167; Dong Y., 2017, BMVC; Dong Z, 2019, IEEE I CONF COMP VIS, P293, DOI 10.1109/ICCV.2019.00038; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Ge SM, 2017, IEEE INT CON MULTI, P667, DOI 10.1109/ICME.2017.8019465; Girshick R., 2014, PROC IEEE C COMPUT V; Girshick R., 2015, ICCV; Gong RH, 2019, IEEE I CONF COMP VIS, P4851, DOI 10.1109/ICCV.2019.00495; Gu JX, 2019, IEEE I CONF COMP VIS, P4908, DOI 10.1109/ICCV.2019.00501; Hai Phan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13417, DOI 10.1109/CVPR42600.2020.01343; Han S., 2016, P 4 INT C LEARN REPR, P1; Hanxiao Liu, 2019, Arxiv, DOI arXiv:1806.09055; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He X., 2021, ARXIV; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; He ZZ, 2019, PROC CVPR IEEE, P11430, DOI 10.1109/CVPR.2019.01170; Hinton G., 2015, ARXIV150302531; Hou L., 2017, INT C LEARN REPR; Hu QH, 2018, AAAI CONF ARTIF INTE, P3247; Hu QH, 2018, LECT NOTES COMPUT SC, V11217, P657, DOI 10.1007/978-3-030-01261-8_39; Hubara I, 2016, ADV NEUR IN, V29; Jaderberg M., 2014, P BRIT MACH VIS C; Jung S, 2019, PROC CVPR IEEE, P4345, DOI 10.1109/CVPR.2019.00448; KAMGARPARSI B, 1989, IEEE T PATTERN ANAL, V11, P929, DOI 10.1109/34.35496; Krizhevsky A., 2012, ADV NEURAL INFORM PR; Krizhevsky A., 2010, THE CIFAR 10 DATASET; Kruge CP, 2014, IEEE INTL CONF IND I, P611, DOI 10.1109/INDIN.2014.6945583; Lahoud F., 2019, ARXIV; Lebedev V, 2016, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR.2016.280; Lebedev Vadim, 2015, ICLR; Li RD, 2019, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR.2019.00292; Li ZF, 2017, IEEE I CONF COMP VIS, P2603, DOI 10.1109/ICCV.2017.282; Lin J., 2019, INT C LEARN REPR; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Lin XF, 2017, ADV NEUR IN, V30; Liu CL, 2021, INT J COMPUT VISION, V129, P998, DOI 10.1007/s11263-020-01417-9; Liu CL, 2019, PROC CVPR IEEE, P2686, DOI 10.1109/CVPR.2019.00280; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Liu ZC, 2020, INT J COMPUT VISION, V128, P202, DOI 10.1007/s11263-019-01227-8; Loshchilov I., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1608.03983; Martinez Brais, 2020, INT C LEARN REPR, V2, P8; Matthieu Courbariaux, 2020, Arxiv, DOI arXiv:1812.11800; Mishra Asit K., 2018, ICLR; Morozov S, 2019, IEEE I CONF COMP VIS, P3036, DOI 10.1109/ICCV.2019.00313; Nagel M, 2019, IEEE I CONF COMP VIS, P1325, DOI 10.1109/ICCV.2019.00141; Pang Jiangmiao, 2019, IEEE CVPR; Qin H., 2020, ARXIV; Qin HT, 2020, PROC CVPR IEEE, P2247, DOI 10.1109/CVPR42600.2020.00232; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Song JK, 2020, INT J COMPUT VISION, V128, P2243, DOI 10.1007/s11263-020-01305-2; Szegedy C, 2015, 2015 IEEE C COMP VIS; Tan MX, 2019, PR MACH LEARN RES, V97; Wang K, 2019, PROC CVPR IEEE, P8604, DOI 10.1109/CVPR.2019.00881; Wang P, 2020, ICML; Wang PS, 2020, AAAI CONF ARTIF INTE, V34, P12192; Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512; Wang YH, 2019, IEEE T PATTERN ANAL, V41, P2495, DOI 10.1109/TPAMI.2018.2857824; Wang ZW, 2020, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR42600.2020.00212; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wen J, 2018, PATTERN RECOGN, V81, P326, DOI 10.1016/j.patcog.2018.04.004; WONG PW, 1991, IEEE T PATTERN ANAL, V13, P951, DOI 10.1109/34.93812; Wu Y, 2021, IEEE T MULTIMEDIA, V23, P353, DOI 10.1109/TMM.2020.2978593; Wu YD, 2020, PROC CVPR IEEE, P6865, DOI 10.1109/CVPR42600.2020.00690; Xie B, 2017, PR MACH LEARN RES, V54, P1216; Yang JW, 2019, PROC CVPR IEEE, P7300, DOI 10.1109/CVPR.2019.00748; Yu XY, 2017, PROC CVPR IEEE, P67, DOI 10.1109/CVPR.2017.15; Zagoruyko S., 2017, ICLR; Ze Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P326, DOI 10.1007/978-3-030-58592-1_20; Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23; Zhang JH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2272, DOI 10.1145/3343031.3350534; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhou S., 2016, ARXIV; Zhu Chenzhuo, 2016, ARXIV161201064; Zhu F., 2019, UNIFIED INT8 TRAININ; Zhuang BH, 2019, PROC CVPR IEEE, P413, DOI 10.1109/CVPR.2019.00050	100	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01687-5	http://dx.doi.org/10.1007/s11263-022-01687-5		OCT 2022	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5A2WO		Green Submitted			2022-12-18	WOS:000862752400003
J	Zou, XY; Xiao, FY; Yu, ZD; Li, YH; Lee, YJ				Zou, Xueyan; Xiao, Fanyi; Yu, Zhiding; Li, Yuheng; Lee, Yong Jae			Delving Deeper into Anti-Aliasing in ConvNets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Computer vision; Anti-aliasing; Neural network consistency; Neural network architecture		Aliasing refers to the phenomenon that high frequency signals degenerate into completely different ones after sampling. It arises as a problem in the context of deep learning as downsampling layers are widely adopted in deep architectures to reduce parameters and computation. The standard solution is to apply a low-pass filter (e.g., Gaussian blur) before downsampling (Zhang in: ICML, 2020). However, it can be suboptimal to apply the same filter across the entire content, as the frequency of feature maps can vary across both spatial locations and feature channels. To tackle this, we propose an adaptive content-aware low-pass filtering layer, which predicts separate filter weights for each spatial location and channel group of the input feature maps. We investigate the effectiveness and generalization of the proposed method across multiple tasks, including image classification, semantic segmentation, instance segmentation, video instance segmentation, and image-to-image translation. Both qualitative and quantitative results demonstrate that our approach effectively adapts to the different feature frequencies to avoid aliasing while preserving useful information for recognition. Code is available at https://maureenzou.github.io/ddac/.	[Zou, Xueyan; Li, Yuheng; Lee, Yong Jae] Univ Wisconsin, Madison, WI 53706 USA; [Xiao, Fanyi] Meta AI, Menlo Pk, CA USA; [Yu, Zhiding] NVIDIA, Santa Clara, CA USA	University of Wisconsin System; University of Wisconsin Madison; Nvidia Corporation	Zou, XY (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	xueyan@cs.wisc.edu; fanyix@fb.com; zhidingy@nvidia.com; li2464@wisc.edu; yongjaelee@cs.wisc.edu			ARO YIP [W911NF17-1-0410]; NSF [IIS-2150012, IIS-2204808, CCF-1934568]; GCP research credit program; AWS ML research award	ARO YIP; NSF(National Science Foundation (NSF)); GCP research credit program; AWS ML research award	This work was supported in part by ARO YIP W911NF17-1-0410, NSF CAREER IIS-2150012, NSF IIS-2204808, NSF CCF-1934568, GCP research credit program, and AWS ML research award.	Alexey Kurakin, 2018, Arxiv, DOI arXiv:1803.06373; Arman Cohan, 2020, Arxiv, DOI arXiv:2004.05150; Azulay A., 2018, JMLR; Bietti A., 2017, NEURIPS; Bloem-Reddy B., 2020, JMLR; Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925; Caelli T. M., 1988, PATTERN RECOGN; Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13; Chaman A, 2021, PROC CVPR IEEE, P3772, DOI 10.1109/CVPR46437.2021.00377; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen Liang-Chieh, 2017, RETHINKING ATROUS CO; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; De Brabandere B, 2016, ADV NEUR IN, V29; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy Alexey, 2021, ICLR, DOI DOI 10.48550/ARXIV.2010.11929; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Geirhos Robert, 2019, INT C LEARNING REPRE; Gonzalez R.C., 2006, DIGITAL IMAGE PROCES; Gu Z., 2021, ARXIV; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hu P., 2020, ECCV WORKSHOP; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Karras Tero, 2021, ARXIV; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Lee J, 2019, PR MACH LEARN RES, V97; Lee K., 2017, ARXIV171109325; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; LI YJ, 1992, PATTERN RECOGN, V25, P723, DOI 10.1016/0031-3203(92)90135-6; Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Madry Aleksander, 2017, ARXIV; Mairal J, 2014, ADV NEUR IN, V27; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Muandet K., 2013, INT C MACH LEARN, P10; Paris S, 2008, FOUND TRENDS COMPUT, V4, P1, DOI 10.1561/0600000020; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Proakis J.G., 1996, DIGIT SIGNAL PROCESS; Rajpurkar P., 2016, CORR, P2383, DOI [10.18653/v1/D16-1264, DOI 10.18653/V1/D16-1264]; Richardson Elad, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P2287, DOI 10.1109/CVPR46437.2021.00232; Rosenberg D., 1974, US Patent, Patent No. [3,815,754, 3815754]; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Sclaroff S., 2020, CVPR; SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Songnan Li, 2010, 2010 28th Picture Coding Symposium (PCS 2010), P590, DOI 10.1109/PCS.2010.5702572; Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142; Tan MX, 2019, PR MACH LEARN RES, V97; Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39; VainF, 2020, DEEPLABV3PLUS PYT; Vaswani A., 2017, P 31 INT C NEURAL IN, P6000, DOI DOI 10.5555/3295222.3295349; Wang H., 2019, ICLR; Wang HH, 2019, ADV NEUR IN, V32; Wang JQ, 2019, IEEE I CONF COMP VIS, P3007, DOI 10.1109/ICCV.2019.00310; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; WEBBER CJS, 1994, NETWORK-COMP NEURAL, V5, P471, DOI 10.1088/0954-898X/5/4/004; Wood J, 1996, PATTERN RECOGN, V29, P1, DOI 10.1016/0031-3203(95)00069-0; Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI 10.1007/s11263-019-01198-w; Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059; Xie E., 2021, ARXIV; Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529; Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637; Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612; Zeyi Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P124, DOI 10.1007/978-3-030-58536-5_8; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang R, 2019, PR MACH LEARN RES, V97; Zhang ZY, 2019, INT CONF 3D VISION, P204, DOI 10.1109/3DV.2019.00031; Zou Xueyan, 2020, BMVC	78	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01672-y	http://dx.doi.org/10.1007/s11263-022-01672-y		OCT 2022	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5A2WO		Green Submitted			2022-12-18	WOS:000862752400002
J	Zhai, YP; Peng, PX; Jia, MX; Li, SY; Chen, WQ; Gao, XS; Tian, YH				Zhai, Yunpeng; Peng, Peixi; Jia, Mengxi; Li, Shiyong; Chen, Weiqiang; Gao, Xuesong; Tian, Yonghong			Population-Based Evolutionary Gaming for Unsupervised Person Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Early Access						Evolutionary gaming; Population-based training; Unsupervised learning; Person re-identification	PERFORMANCE; LEVEL	Unsupervised person re-identification has achieved great success through the self-improvement of individual neural networks. However, limited by the lack of diversity of discriminant information, a single network has difficulty learning sufficient discrimination ability by itself under unsupervised conditions. To address this limit, we develop a population-based evolutionary gaming (PEG) framework in which a population of diverse neural networks are trained concurrently through selection, reproduction, mutation, and population mutual learning iteratively. Specifically, the selection of networks to preserve is modeled as a cooperative game and solved by the best-response dynamics, then the reproduction and mutation are implemented by cloning and fluctuating hyper-parameters of networks to learn more diversity, and population mutual learning improves the discrimination of networks by knowledge distillation from each other within the population. In addition, we propose a cross-reference scatter (CRS) to approximately evaluate re-ID models without labeled samples and adopt it as the criterion of network selection in PEG. CRS measures a model's performance by indirectly estimating the accuracy of its predicted pseudo-labels according to the cohesion and separation of the feature space. Extensive experiments demonstrate that (1) CRS approximately measures the performance of models without labeled samples; (2) and PEG produces new state-of-the-art accuracy for person re-identification, indicating the great potential of population-based network cooperative training for unsupervised learning. The code is released on github.com/YunpengZhai/PEG.	[Zhai, Yunpeng; Peng, Peixi; Jia, Mengxi; Tian, Yonghong] Peking Univ, Sch Comp Sci, Natl Engn Res Ctr Visual Technol, Beijing, Peoples R China; [Tian, Yonghong] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen, Peoples R China; [Peng, Peixi; Tian, Yonghong] Peng Cheng Lab, Shenzhen, Peoples R China; [Li, Shiyong] AI Huawei Technol Co Ltd, Applicat Res Ctr, Shenzhen, Peoples R China; [Chen, Weiqiang; Gao, Xuesong] State Key Lab Digital Multimedia Technol, Hisense, Qingdao, Peoples R China	Peking University; Peking University; University Town of Shenzhen; Peng Cheng Laboratory; Hisense	Peng, PX; Tian, YH (corresponding author), Peking Univ, Sch Comp Sci, Natl Engn Res Ctr Visual Technol, Beijing, Peoples R China.; Tian, YH (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen, Peoples R China.; Peng, PX; Tian, YH (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	ypzhai@pku.edu.cn; pxpeng@pku.edu.cn; mxjia@pku.edu.cn; lishiyong@huawei.com; chenweiqiang@iCloud.com; xuesong@outlook.com; yhtian@pku.edu.cn			Key-Area Research and Development Program of Guangdong Province [2019B010153002]; National Natural Science Foundation of China [61825101, 62088102]	Key-Area Research and Development Program of Guangdong Province; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is partially supported by grants from the Key-Area Research and Development Program of Guangdong Province under contact No. 2019B010153002, and grants from the National Natural Science Foundation of China under contract No. 61825101 andNo. 62088102. The computing resources of Pengcheng Cloudbrain are used in this research.	Alexander G. Hauptmann, 2016, Arxiv, DOI arXiv:1610.02984; Ali B, 2020, INTELL DATA ANAL, V24, P1345, DOI 10.3233/IDA-194887; BAKER FB, 1975, J AM STAT ASSOC, V70, P31, DOI 10.2307/2285371; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Chen H., 2021, ARXIV; Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204; Congyan Lang, 2019, Arxiv, DOI arXiv:1905.10529; Cuiling Lan, 2020, Arxiv, DOI arXiv:2006.00752; Dapeng Chen, 2020, Arxiv, DOI arXiv:2006.02713; Dapeng Chen, 2020, Arxiv, DOI arXiv:2001.01526; DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099; Dunn J.C., 1973, J CYBERNETICS, V3, P32, DOI [10.1080/ 01969727308546046, DOI 10.1080/01969727308546046]; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316; Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31; Fu Dengpan, 2021, P IEEE C COMP VIS PA; Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621; Fukui K, 2013, PROC INT C TOOLS ART, P398, DOI 10.1109/ICTAI.2013.66; Gao H., 2017, ARXIV E PRINTS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guangyi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P643, DOI 10.1007/978-3-030-58598-3_38; Halkidi M, 2002, SIGMOD REC, V31, P19, DOI 10.1145/601858.601862; HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871; Ho D, 2019, PR MACH LEARN RES, V97; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; HUBERT LJ, 1976, PSYCHOL BULL, V83, P1072, DOI 10.1037/0033-2909.83.6.1072; Jaderberg M, 2019, SCIENCE, V364, P859, DOI 10.1126/science.aau6249; Ji HXY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3641, DOI 10.1109/ICCV48922.2021.00364; Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29; Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367; Kalintha W, 2019, INTELL DATA ANAL, V23, P1271, DOI 10.3233/IDA-184283; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Kingma D.P, P 3 INT C LEARNING R; Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231; Lakshminarayanan B, 2017, ADV NEUR IN, V30; Li M., 2018, P EUR C COMP VIS ECC, P737; Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058; Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345; Lin YT, 2019, AAAI CONF ARTIF INTE, P8738; Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737; Maulik U, 2002, IEEE T PATTERN ANAL, V24, P1650, DOI 10.1109/TPAMI.2002.1114856; Mengxi Jia, 2020, Arxiv, DOI arXiv:2007.01546; Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29; Peng PX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3037; Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146; Perrone MP., 1992, NETWORKS DISAGREE EN; Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Shen ZQ, 2019, AAAI CONF ARTIF INTE, P4886, DOI 10.1609/aaai.v33i01.33014886; Singh S., 2016, ADV NEURAL INFORM PR, V29; Song L, 2018, ARXIV PREPRINT ARXIV; Spearman C, 1904, AM J PSYCHOL, V15, P72, DOI 10.2307/1412159; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tarvainen Antti, 2017, CORR, Vabs/1703; Vinyals O, 2019, NATURE, V575, P350, DOI 10.1038/s41586-019-1724-z; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242; Wang M., 2020, ARXIV; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wu JL, 2019, IEEE I CONF COMP VIS, P8320, DOI 10.1109/ICCV.2019.00841; Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175; Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522; Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482; Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6; Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550; Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017; Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904; Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831; Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454; Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng Yi, 2021, PROC IEEE INT C COMP, P8371; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11; Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069; Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313; Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P72, DOI 10.1007/978-3-030-58621-8_5; Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380; Dai ZZ, 2021, Arxiv, DOI arXiv:2103.11568	91	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.												10.1007/s11263-022-01693-7	http://dx.doi.org/10.1007/s11263-022-01693-7		OCT 2022	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z9UV					2022-12-18	WOS:000862544900001
J	Liu, WD; Zhang, C; Lin, GS; Liu, FY				Liu, Weide; Zhang, Chi; Lin, Guosheng; Liu, Fayao			CRCNet: Few-Shot Segmentation with Cross-Reference and Region-Global Conditional Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Few shot learning; Semantic segmentation		Few-shot segmentation aims to learn a segmentation model that can be generalized to novel classes with only a few training images. In this paper, we propose a Cross-Reference and Local-Global Conditional Networks (CRCNet) for few-shot segmentation. Unlike previous works that only predict the query image's mask, our proposed model concurrently makes predictions for both the support image and the query image. Our network can better find the co-occurrent objects in the two images with a cross-reference mechanism, thus helping the few-shot segmentation task. To further improve feature comparison, we develop a local-global conditional module to capture both global and local relations. We also develop a mask refinement module to refine the prediction of the foreground regions recurrently. Experiments on the PASCAL VOC 2012, MS COCO, and FSS-1000 datasets show that our network achieves new state-of-the-art performance.	[Liu, Weide; Zhang, Chi; Lin, Guosheng] Nanyang Technol Univ NTU, Sch Comp Sci & Engn, Singapore 639798, Singapore; [Liu, Weide; Liu, Fayao] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)	Lin, GS (corresponding author), Nanyang Technol Univ NTU, Sch Comp Sci & Engn, Singapore 639798, Singapore.	weide001@e.ntu.edu.sg; chi007@e.ntu.edu.sg; gslin@ntu.edu.sg; liu_fayao@i2r.a-star.edu.sg			National Research Foundation, Singapore under its AI Singapore Programme (AISG) [AISG-RP-2018-003]; Ministry of Education, Singapore, under its Academic Research Fund Tier 2 [MOE-T2EP20220-0007]; Ministry of Education, Singapore, under its Academic Research Fund Tier 1 [RG95/20]; Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funds [A20H6b0151]	National Research Foundation, Singapore under its AI Singapore Programme (AISG); Ministry of Education, Singapore, under its Academic Research Fund Tier 2; Ministry of Education, Singapore, under its Academic Research Fund Tier 1(Ministry of Education, Singapore); Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funds(Agency for Science Technology & Research (A*STAR))	This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG-RP-2018-003), the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (MOE-T2EP20220-0007) and Tier 1 (RG95/20). This research is also partly supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funds (Grant No. A20H6b0151).	Azad R, 2021, IEEE WINT CONF APPL, P2673, DOI 10.1109/WACV48630.2021.00272; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376; Cao YZH, 2017, IEEE T IMAGE PROCESS, V26, P836, DOI 10.1109/TIP.2016.2621673; Chen H., 2018, ARXIV; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen WY, 2019, PROC CVPR IEEE, P8916, DOI 10.1109/CVPR.2019.00913; Chen Wuyang, 2019, ARXIV191210917; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong Nanqing, 2018, BMVC; Dong ZH, 2019, P INT COMP SOFTW APP, P42, DOI 10.1109/COMPSAC.2019.10181; Fan Q, 2020, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR42600.2020.00407; Han JW, 2018, IEEE T IMAGE PROCESS, V27, P1639, DOI 10.1109/TIP.2017.2781424; Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43; Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hendryx S.M., 2019, ARXIV; Hou R., 2019, ARXIV; Hu T, 2019, AAAI CONF ARTIF INTE, P8441; Huang ZL, 2020, IEEE T IMAGE PROCESS, V29, P2066, DOI 10.1109/TIP.2019.2941644; Jing LL, 2020, IEEE T IMAGE PROCESS, V29, P225, DOI 10.1109/TIP.2019.2926748; Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719; Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071; Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li Gen, 2021, P IEEE CVF C COMP VI, P8334; Li X, 2020, PROC CVPR IEEE, P2866, DOI 10.1109/CVPR42600.2020.00294; Lin GS, 2020, IEEE T PATTERN ANAL, V42, P1228, DOI 10.1109/TPAMI.2019.2893630; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu BH, 2021, PROC CVPR IEEE, P9742, DOI 10.1109/CVPR46437.2021.00962; Liu W., 2021, ARXIV; Liu WD, 2020, PROC CVPR IEEE, P4164, DOI 10.1109/CVPR42600.2020.00422; Liu Yongfei, 2020, EUROPEAN C COMPUTER, P142, DOI DOI 10.1007/978-3-030-58545-7_9; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Min J., 2021, P IEEECVF INT C COMP, P6941; Mukherjee P., 2018, ARXIV; Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189; Rakelly Kate, 2018, ICLR WORKSH; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rother C., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91; Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253; Shaban A., 2017, ARXIV; Siam M., 2019, ARXIV; Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535; Snell J, 2017, ADV NEUR IN, V30; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929; Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065; Xie GS, 2021, PROC CVPR IEEE, P5471, DOI 10.1109/CVPR46437.2021.00543; Yang B., 2020, ARXIV; Yang L., 2021, ICCV, P8721; Yang L, 2018, IEEE T IMAGE PROCESS, V27, P4025, DOI 10.1109/TIP.2018.2834221; Yosinski J., 2015, ICML DEEP LEARN WORK; Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968; Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536; Zhang X., 2018, ARXIV; Zhou SH, 2020, IEEE T IMAGE PROCESS, V29, P461, DOI 10.1109/TIP.2019.2919937	63	0	0	7	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					3140	3157		10.1007/s11263-022-01677-7	http://dx.doi.org/10.1007/s11263-022-01677-7		SEP 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI		Green Submitted			2022-12-18	WOS:000862212600001
J	Zhang, Q; Chan, AB				Zhang, Qi; Chan, Antoni B.			3D Crowd Counting via Geometric Attention-Guided Multi-view Fusion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Crowd counting; 3D projection; 3D fusion; geometric attention; height estimation; 2D-3D projection	PEOPLE	Recently multi-view crowd counting using deep neural networks has been proposed to enable counting in large and wide scenes using multiple cameras. The current methods project the camera-view features to the average-height plane of the 3D world, and then fuse the projected multi-view features to predict a 2D scene-level density map on the ground (i.e., birds-eye view). Unlike the previous research, we consider the variable height of the people in the 3D world and propose to solve the multi-view crowd counting task through 3D feature fusion with 3D scene-level density maps, instead of the 2D density map on the ground-plane. Compared to 2D fusion, the 3D fusion extracts more information of the people along the z-dimension (height), which helps to address the scale variations across multiple views. The 3D density maps still preserve the 2D density maps property that the sum is the count, while also providing 3D information about the crowd density. Furthermore, instead of using the standard method of copying the features along the view ray in the 2D-to-3D projection, we propose an attention module based on a height estimation network, which forces each 2D pixels to be projected to one 3D voxel along the view ray. We also explore the projection consistency among the 3D prediction and the ground-truth in the 2D views to further enhance the counting performance. The proposed method is tested on the synthetic and real-world multi-view counting datasets and achieves better or comparable counting performance to the state-of-the-art.	[Zhang, Qi] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Guangdong, Peoples R China; [Zhang, Qi; Chan, Antoni B.] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China	Shenzhen University; City University of Hong Kong	Zhang, Q (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Guangdong, Peoples R China.; Zhang, Q (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.	qzhang364-c@my.cityu.edu.hk; abchan@cityu.edu.hk	CHAN, Antoni B./D-7858-2013	CHAN, Antoni B./0000-0002-2886-2513; ZHANG, Qi/0000-0001-6212-9799	Research Grants Council of the Hong Kong Special Administrative Region, China [CityU 11212518, CityU 11215820]; City University of Hong Kong [7005665]	Research Grants Council of the Hong Kong Special Administrative Region, China(Hong Kong Research Grants Council); City University of Hong Kong(City University of Hong Kong)	This work was supported by grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (CityU 11212518, CityU 11215820), and by a Strategic Research Grant from City University of Hong Kong (Project No. 7005665).	Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465; Boominathan L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P640, DOI 10.1145/2964284.2967300; Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Dittrich F., 2017, ARXIV; FERRYMAN J, 2009, IEEE INT WORKSH PERF, P1; Ge WN, 2010, LECT NOTES COMPUT SC, V6315, P324; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298; Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Jaderberg M, 2015, ADV NEUR IN, V28; Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476; Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629; Kang D., 2018, P BMVC, P89; Kang D, 2017, ADV NEUR IN, V30; Kar A, 2017, ADV NEUR IN, V30; Li JW, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P178, DOI 10.1109/AVSS.2012.54; Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120; Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372; Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545; Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524; Liu X., 2020, ARXIV; Ma HD, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2089094.2089107; Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624; Maddalena L, 2014, PATTERN RECOGN LETT, V36, P125, DOI 10.1016/j.patrec.2013.10.006; Onoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38; Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Ryan D, 2014, PATTERN RECOGN LETT, V44, P98, DOI 10.1016/j.patrec.2013.10.002; Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550; Shi ML, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON INDUSTRIAL ARTIFICIAL INTELLIGENCE (IAI 2019); Sindagi V. A., 2020, ARXIV; Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206; Sitzmann V, 2019, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2019.00254; Tang NC, 2015, IEEE T IMAGE PROCESS, V24, P80, DOI 10.1109/TIP.2014.2363445; Wang BN, 2020, AAAI CONF ARTIF INTE, V34, P9146; Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839; Xiong HP, 2019, IEEE I CONF COMP VIS, P8361, DOI 10.1109/ICCV.2019.00845; Yang YF, 2020, PROC CVPR IEEE, P4373, DOI 10.1109/CVPR42600.2020.00443; Zhang, 2021, CVPR 2021 UNPUB; Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684; Zhang Q, 2020, AAAI CONF ARTIF INTE, V34, P12837; Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849; Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70	55	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					3123	3139		10.1007/s11263-022-01685-7	http://dx.doi.org/10.1007/s11263-022-01685-7		SEP 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI					2022-12-18	WOS:000863211200001
J	Chi, C; Hao, TY; Wang, QJ; Guo, P; Yang, X				Chi, Cheng; Hao, Tianyu; Wang, Qingjie; Guo, Peng; Yang, Xin			Subspace-PnP: A Geometric Constraint Loss for Mutual Assistance of Depth and Optical Flow Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo depth estimation; Optical flow prediction; Subspace-clustering; Perspective-n-Point	NETWORK; PREDICTION; ALGORITHM; ROBUST; SLAM	Unsupervised optical flow and stereo depth estimation are two fundamental tasks in computer vision. Current studies (Tosi et al., in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 4654-4665, 2020; Ranjan et al., in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 12240-12249, 2019; Wang et al., in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 8071-8081, 2019; Yin and Shi, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1983-1992, 2018) demonstrate that jointly learning networks for optical flow and stereo depth estimation via the geometric constraints can mutually benefit the two tasks and in turn yield large accuracy improvements. However, most of these methods generate geometric constraints based on estimated camera pose, which are not applicable to scenarios with moving objects that have different motions from the camera. In addition, errors of estimated camera pose would yield inaccurate constraints for the two tasks. In this paper, we propose a novel and universal geometric loss function, named Subspace-PnP, which is based on the Perspective-n-Points (PnP) and union-of-subspaces theory (Ji et al., in: IEEE Winter conference on applications of computer vision, pp 461-468, 2014) to jointly estimate the optical flow and stereo depth. The construction of Subspace-PnP dose not rely on the camera pose, but implicitly contains information of camera pose and motions of all moving objects. Our experiments show that the Subspace-PnP loss can mutually guide the estimation of optical flow and depth, enabling better robustness and greater accuracy even in dynamic scenes. In addition, we propose a motion-occlusion simulation method to handle occlusions caused by moving objects in optical flow estimation, which in turn can yield further performance improvement. Our method achieves the state-of-the-art performance for joint optical flow and stereo depth estimation on the KITTI 2012 and KITTI 2015 benchmarks.	[Chi, Cheng; Hao, Tianyu; Wang, Qingjie; Guo, Peng; Yang, Xin] Huazhong Univ Sci & Technol, Wuhan, Peoples R China	Huazhong University of Science & Technology	Yang, X (corresponding author), Huazhong Univ Sci & Technol, Wuhan, Peoples R China.	chengchi2019@hust.edu.cn; hty@hust.edu.cn; wqj@hust.edu.cn; guopeng@hust.edu.cn; xinyang2014@hust.edu.cn			National Natural Science Foundation of China [62122029, 62061160490, U20B2064]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China under Grants 62122029, 62061160490 and U20B2064.	Andrew Alex M, 2001, MULTIPLE VIEW GEOMET; Cao Yuanzhouhan, 2018, IEEE Trans Image Process, DOI 10.1109/TIP.2018.2877944; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen JY, 2021, IEEE T PATTERN ANAL, V43, P2598, DOI 10.1109/TPAMI.2020.2977021; Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162; DeSouza GN, 2002, IEEE T PATTERN ANAL, V24, P237, DOI 10.1109/34.982903; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Geiger A., 2012, P IEEE COMP SOC C CO; Gissot SF, 2008, SOL PHYS, V252, P397, DOI 10.1007/s11207-008-9270-0; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257; Guan SS, 2019, IEEE INT CON MULTI, P181, DOI 10.1109/ICME.2019.00039; Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339; Herakleous K, 2013, IEEE IMAGE PROC, P3403, DOI 10.1109/ICIP.2013.6738702; Hu P, 2018, IEEE T MULTIMEDIA, V20, P2814, DOI 10.1109/TMM.2018.2815784; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Ince S, 2008, IEEE T IMAGE PROCESS, V17, P1443, DOI 10.1109/TIP.2008.925381; Ji P, 2014, IEEE WINT CONF APPL, P461, DOI 10.1109/WACV.2014.6836065; Jonschkowski Rico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P557, DOI 10.1007/978-3-030-58536-5_33; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Laga H, 2022, IEEE T PATTERN ANAL, V44, P1738, DOI 10.1109/TPAMI.2020.3032602; Lai HY, 2019, PROC CVPR IEEE, P1890, DOI 10.1109/CVPR.2019.00199; Liu L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P876; Liu L, 2020, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR42600.2020.00652; Liu PP, 2020, PROC CVPR IEEE, P6647, DOI 10.1109/CVPR42600.2020.00668; Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470; Liu PP, 2019, AAAI CONF ARTIF INTE, P8770; Luo CX, 2020, IEEE T PATTERN ANAL, V42, P2624, DOI 10.1109/TPAMI.2019.2930258; Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034; Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2; Mayer N, 2018, INT J COMPUT VISION, V126, P942, DOI 10.1007/s11263-018-1082-6; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Meister S, 2018, AAAI CONF ARTIF INTE, P7251; Menze Moritz, 2015, CVPR; Mishiba K, 2020, IEEE T IMAGE PROCESS, V29, P4232, DOI 10.1109/TIP.2020.2970814; Mostafavi M, 2021, INT J COMPUT VISION, V129, P900, DOI 10.1007/s11263-020-01410-2; Ranjan A., 2019, CVPR, P12240, DOI DOI 10.1109/CVPR.2019.01252; Ranjan A, 2020, INT J COMPUT VISION, V128, P873, DOI 10.1007/s11263-019-01279-w; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Ren Z, 2017, AAAI CONF ARTIF INTE, P1495; Song X, 2020, INT J COMPUT VISION, V128, P910, DOI 10.1007/s11263-019-01287-w; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Tang MH, 2019, IEEE T MULTIMEDIA, V21, P957, DOI 10.1109/TMM.2018.2867266; Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028; Tosi F, 2020, PROC CVPR IEEE, P4653, DOI 10.1109/CVPR42600.2020.00471; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513; Wang YJ, 2019, INT CONF ACOUST SPEE, P8063, DOI 10.1109/ICASSP.2019.8682578; Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39; Yang X, 2021, IEEE T MULTIMEDIA, V23, P4208, DOI 10.1109/TMM.2020.3038323; Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121; Yang ZH, 2019, LECT NOTES COMPUT SC, V11133, P691, DOI 10.1007/978-3-030-11021-5_43; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1; Zhai ML, 2020, IEEE T IMAGE PROCESS, V29, P7807, DOI 10.1109/TIP.2020.3007843; Zhang CX, 2017, IEEE T IMAGE PROCESS, V26, P4055, DOI 10.1109/TIP.2017.2712279; Zhong Y., 2017, ARXIV; Zhong YR, 2019, PROC CVPR IEEE, P12087, DOI 10.1109/CVPR.2019.01237; Zhou HZ, 2020, INT J COMPUT VISION, V128, P756, DOI 10.1007/s11263-019-01221-0; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	65	0	0	5	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					3054	3069		10.1007/s11263-022-01652-2	http://dx.doi.org/10.1007/s11263-022-01652-2		SEP 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI					2022-12-18	WOS:000860407600001
J	Navasardyan, S; Ohanyan, M				Navasardyan, Shant; Ohanyan, Marianna			The Family of Onion Convolutions for Image Inpainting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Inpainting; Onion convolution; Patch-match	TEXTURE SYNTHESIS; OBJECT REMOVAL	Recently deep learning methods have achieved great success in image inpainting problem. However, reconstructing continuities of complex structures with non-stationary textures remains a challenging task for computer vision. In this paper the family of onion convolutions is presented, the concept of which arises from a connection between patch-based techniques and attention mechanisms. The onion convolutions are building blocks designed for the iterative completion of the missing region from its boundary to the center. It allows to continuously propagate structures and textures from the known region to the missing one and meet human criteria on high-quality image completions. As qualitative and quantitative comparisons show, our method with onion convolutions outperforms state-of-the-art methods by producing more realistic, visually plausible and semantically coherent results.	[Navasardyan, Shant; Ohanyan, Marianna] Picsart AI Res, Yerevan, Armenia		Navasardyan, S (corresponding author), Picsart AI Res, Yerevan, Armenia.	shant.navasardyan@picsart.com; marianna.ohanyan@picsart.com		Navasardyan, Shant/0000-0002-1999-9999				Abadi M, 2015, P 12 USENIX S OPERAT; Adobe Inc, 2019, AD PHOT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ashikhmin M., 2001, P 2001 S INT 3D GRAP, P217, DOI DOI 10.1145/364338.364405; Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338; Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487; Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen T., 2016, ARXIV; Cordonnier J.B., 2020, INT C LEARNING REPRE; Criminisi A., 2003, PROC CVPR IEEE, V2, pII; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Djork-Arn, ICLR 2016; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Garber D.D., 1981, COMPUTATIONAL MODELS; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Harrison P, 2001, W S C G ' 2001, VOLS I & II, CONFERENCE PROCEEDINGS, P190; Hensel M, 2017, ADV NEUR IN, V30; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang JB, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2601097.2601205, 10.1145/2602141]; Hung Jason C., 2008, Journal of Software, V3, P57; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Karras T., 2017, PROGR GROWING GANS I; Kingma D.P, P 3 INT C LEARNING R; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kohler R, 2014, LECT NOTES COMPUT SC, V8753, P523, DOI 10.1007/978-3-319-11752-2_43; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264; Levin A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P305; Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272; Li JJ, 2020, IEEE COMPUT SOC CONF, P1894, DOI 10.1109/CVPRW50498.2020.00239; Li Y., 2020, ARXIV; Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Luong M., 2015, P 2015 C EMP METH NA, P1412, DOI [10.18653/v1/D15-1166, DOI 10.18653/V1/D15-1166]; Navasardyan S., 2020, P ASIAN C COMPUTER V; Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408; Oh SW, 2019, IEEE I CONF COMP VIS, P4402, DOI 10.1109/ICCV.2019.00450; Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Ramachandran P, 2019, ADV NEUR IN, V32; Rane SD, 2003, IEEE T IMAGE PROCESS, V12, P296, DOI 10.1109/TIP.2002.804264; Ren JS., 2015, ADV NEURAL INF PROCE, V1, P901; Song Y., 2018, ARXIV; Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI 10.1007/978-3-030-01216-8_1; Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366; Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274; Vaswani A, 2017, ADV NEUR IN, V30; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895; Xie J., 2012, ADV NEURAL INFORM PR, P341, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605; Xiong W, 2019, PROC CVPR IEEE, P5833, DOI 10.1109/CVPR.2019.00599; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xu ZB, 2010, IEEE T IMAGE PROCESS, V19, P1153, DOI 10.1109/TIP.2010.2042098; Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1; Yang C., 2016, ARXIV; Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Yi Zili, 2020, P IEEE CVF C COMP VI, P7508, DOI DOI 10.1109/CVPR42600.2020.00753; Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457; Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458; Zhang H, 2019, PR MACH LEARN RES, V97; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009	81	0	0	5	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					3070	3099		10.1007/s11263-022-01679-5	http://dx.doi.org/10.1007/s11263-022-01679-5		SEP 2022	30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI					2022-12-18	WOS:000860407600002
J	Zhang, QJ; Hou, JH; Qian, Y; Chan, AB; Zhang, JY; He, Y				Zhang, Qijian; Hou, Junhui; Qian, Yue; Chan, Antoni B.; Zhang, Juyong; He, Ying			RegGeoNet: Learning Regular Representations for Large-Scale 3D Point Clouds	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Large-scale 3D point clouds; Regular representation; Geometry image; Unsupervised learning		Deep learning has proven an effective tool for 3D point cloud processing. Currently, most deep set architectures are developed for sparse inputs (typically with a few thousand points), which are unable to provide sufficient structural statistics and semantic cues due to low resolutions. Since these architectures suffer from unacceptable computational and memory costs when consuming dense inputs, there is a pressing need in real-world applications to handle large-scale 3D point clouds. To bridge this gap, this paper presents a novel unsupervised neural architecture called RegGeoNet to parameterize an unstructured point set into a completely regular image structure dubbed as deep geometry image (DeepGI), such that spatial coordinates of unordered points are recorded in three-channel grid pixels. Intuitively, our goal is to embed irregular 3D surface points onto uniform 2D lattice grids, while trying to preserve local neighborhood consistency. Functionally, DeepGI serves as a generic representation modality for raw point cloud data and can be conveniently integrated into mature image processing pipelines. Driven by its unique structural characteristics, we are motivated to customize a set of efficient feature extractors that directly operate on DeepGls for achieving a rich variety of downstream tasks. To demonstrate the potential and universality of our proposed learning paradigms built upon DeepGls for large-scale point cloud processing, we conduct extensive experiments on various downstream tasks, including shape classification, object part segmentation, scene semantic segmentation, normal estimation, and geometry compression, where our frameworks achieve highly competitive performance, compared with state-of-the-art methods. The source code will be publicly available at htips://github.com/keeganhk/RegGeoNet.	[Zhang, Qijian; Hou, Junhui; Qian, Yue; Chan, Antoni B.] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China; [Zhang, Qijian; Hou, Junhui; Qian, Yue] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen, Peoples R China; [Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei, Peoples R China; [He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore	City University of Hong Kong; City University of Hong Kong; Chinese Academy of Sciences; University of Science & Technology of China, CAS; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Hou, JH (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.; Hou, JH (corresponding author), City Univ Hong Kong, Shenzhen Res Inst, Shenzhen, Peoples R China.	qijizhang3-c@my.cityu.edu.hk; jh.hou@cityu.edu.hk; yueqian4-c@my.cityu.edu.hk; abchan@cityu.edu.hk; juyong@ustc.edu.cn; yhe@ntu.edu.sg		Hou, Junhui/0000-0003-3431-2021; ZHANG, Qijian/0000-0003-4723-6136				ARMENI I, 2016, PROC CVPR IEEE, P1534, DOI DOI 10.1109/CVPR.2016.170; Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301; Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen C, 2019, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR.2019.00513; Chu L, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3459234; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P49, DOI 10.1111/cgf.13244; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Fan SQ, 2021, PROC CVPR IEEE, P14499, DOI 10.1109/CVPR46437.2021.01427; Gadelha M, 2019, IEEE I CONF COMP VIS, P22, DOI 10.1109/ICCV.2019.00011; Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128; Gojcic Z, 2020, PROC CVPR IEEE, P1756, DOI 10.1109/CVPR42600.2020.00183; Gu XF, 2002, ACM T GRAPHIC, V21, P355; Gu XY, 2019, PROC CVPR IEEE, P3249, DOI 10.1109/CVPR.2019.00337; Guo M., 2020, ARXIV; Haim N, 2019, IEEE I CONF COMP VIS, P632, DOI 10.1109/ICCV.2019.00072; Hanocka R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392415; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heckel R., 2019, P 7 INT C LEARN REPR; Hu Qingyong, 2020, CVPR, DOI DOI 10.1109/CVPR42600.2020.01112; Jang E., 2017, ICLR; Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053; Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702; Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479; Le Eric-Tuan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9500, DOI 10.1109/CVPR42600.2020.00952; Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Li Y., 2018, P 32 INT C NEURAL IN, P828; Lin YQ, 2020, PROC CVPR IEEE, P4292, DOI 10.1109/CVPR42600.2020.00435; Liu H, 2020, IEEE T BROADCAST, V66, P701, DOI 10.1109/TBC.2019.2957652; Liu YC, 2019, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2019.00534; Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910; Liu ZJ, 2019, ADV NEUR IN, V32; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Maddison Chris J, 2017, ICLR; Mao JG, 2019, IEEE I CONF COMP VIS, P1578, DOI 10.1109/ICCV.2019.00166; Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616; Masci J., 2015, P IEEE INT C COMP VI, P37; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Nezhadarya Ehsan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12953, DOI 10.1109/CVPR42600.2020.01297; Park C., 2022, P CVPR; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Qiu S, 2021, PROC CVPR IEEE, P1757, DOI 10.1109/CVPR46437.2021.00180; Rao YM, 2019, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2019.00054; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rethage D, 2018, LECT NOTES COMPUT SC, V11208, P625, DOI 10.1007/978-3-030-01225-0_37; Riba E, 2020, IEEE WINT CONF APPL, P3663, DOI 10.1109/WACV45572.2020.9093363; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981; Simonyan K, 2014, ADV NEUR IN, V27; Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91; Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14; Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167; Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275; Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696; Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xiang Tiange, 2021, P ICCV, P915; Xu M., 2021, P AAAI; Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319; Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563; Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344; Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238; Yu FG, 2019, PROC CVPR IEEE, P9483, DOI 10.1109/CVPR.2019.00972; Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12; Ze Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P326, DOI 10.1007/978-3-030-58592-1_20; Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169	90	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					3100	3122		10.1007/s11263-022-01682-w	http://dx.doi.org/10.1007/s11263-022-01682-w		SEP 2022	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI					2022-12-18	WOS:000860407600003
J	Braso, G; Cetintas, O; Leal-Taixe, L				Braso, Guillem; Cetintas, Orcun; Leal-Taixe, Laura			Multi-Object Tracking and Segmentation Via Neural Message Passing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-object tracking; Segmentation; Neural message passing; Graph neural networks	NETWORK; SET	Graphs offer a natural way to formulate Multiple Object Tracking (MOT) and Multiple Object Tracking and Segmentation (MOTS) within the tracking-by-detection paradigm. However, they also introduce a major challenge for learning methods, as defining a model that can operate on such structured domain is not trivial. In this work, we exploit the classical network flow formulation of MOT to define a fully differentiable framework based on Message Passing Networks. By operating directly on the graph domain, our method can reason globally over an entire set of detections and exploit contextual features. It then jointly predicts both final solutions for the data association problem and segmentation masks for all objects in the scene while exploiting synergies between the two tasks. We achieve state-of-the-art results for both tracking and segmentation in several publicly available datasets. Our code is available at https://github.com/ocetintas/MPNTrackSeg	[Braso, Guillem; Cetintas, Orcun; Leal-Taixe, Laura] Tech Univ Munich, Munich, Germany	Technical University of Munich	Cetintas, O (corresponding author), Tech Univ Munich, Munich, Germany.	guillem.braso@tum.de; orcun.cetintas@tum.de; leal.taixe@tum.de	Leal-Taixe, Laura/HFZ-8079-2022	Leal-Taixe, Laura/0000-0001-8709-1133; Cetintas, Orcun/0000-0003-2496-4157	Sofja Kovalevskaja Award of the Humboldt Foundation; German Federal Ministry of Education and Research (BMBF) [01IS18036B]	Sofja Kovalevskaja Award of the Humboldt Foundation; German Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF))	Open Access funding enabled and organized by Projekt DEAL. This project was partially funded by the Sofja Kovalevskaja Award of the Humboldt Foundation and by the German Federal Ministry of Education and Research (BMBF) underGrant No. 01IS18036B. The authors of this work take full responsibility for its content.	Ahuja R. K., 1993, NETWORK FLOWS THEORY; Baisa NL, 2021, J VIS COMMUN IMAGE R, V80, DOI 10.1016/j.jvcir.2021.103279; Battaglia PW, 2016, ADV NEUR IN, V29; Berclaz J., 2006, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2006.258; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103; Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003; Breitenstein MD, 2009, IEEE I CONF COMP VIS, P1515, DOI 10.1109/ICCV.2009.5459278; Bruna J, 2013, PROC INT C LEARN REP; Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347; Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16; Dai P, 2021, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR46437.2021.00247; Defferrard M, 2016, ADV NEUR IN, V29; Dendorfer P., 2020, ARXIV; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478; Geiger A., 2012, P IEEE COMP SOC C CO; Gilmer J, 2017, PR MACH LEARN RES, V70; Guo M, 2018, LECT NOTES COMPUT SC, V11205, P673, DOI 10.1007/978-3-030-01246-5_40; He JW, 2021, PROC CVPR IEEE, P5295, DOI 10.1109/CVPR46437.2021.00526; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Henschel R, 2019, IEEE COMPUT SOC CONF, P770, DOI 10.1109/CVPRW.2019.00105; Henschel R, 2018, IEEE COMPUT SOC CONF, P1509, DOI 10.1109/CVPRW.2018.00192; Henschel Roberto, 2017, IEEE C COMP VIS PATT; Hornakova A., 2021, MAKING HIGHER ORDER; Hornakova A, 2020, PR MACH LEARN RES, V119; Jiang Hao, 2007, IEEE C COMP VIS PATT, P2; Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9; Kasturi R, 2009, IEEE T PATTERN ANAL, V31, P319, DOI 10.1109/TPAMI.2008.57; Keuper M, 2020, IEEE T PATTERN ANAL, V42, P140, DOI 10.1109/TPAMI.2018.2876253; Kim A, 2021, IEEE INT CONF ROBOT, P11315, DOI 10.1109/ICRA48506.2021.9562072; Kim C., 2021, IEEE C COMPUTER VISI; Kipf T, 2018, PR MACH LEARN RES, V80; Kipf TN, 2016, P INT C LEARN REPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Leal-Taixe L., 2015, ARXIV; Leal-Taixe L, 2014, PROC CVPR IEEE, P3542, DOI 10.1109/CVPR.2014.453; Leal-Taixe L, 2012, PROC CVPR IEEE, P1987, DOI 10.1109/CVPR.2012.6247901; Leal-Taixe L, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Li JH, 2020, IEEE WINT CONF APPL, P708, DOI 10.1109/WACV45572.2020.9093347; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li ZW, 2018, ADV NEUR IN, V31; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin W., 2020, ARXIV; Liu QK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P530; Luiten J, 2020, IEEE ROBOT AUTOM LET, V5, P1803, DOI 10.1109/LRA.2020.2969183; Ma LQ, 2019, LECT NOTES COMPUT SC, V11362, P612, DOI 10.1007/978-3-030-20890-5_39; Meinhardt T., 2021, ARXIV; Milan A, 2015, PROC CVPR IEEE, P5397, DOI 10.1109/CVPR.2015.7299178; Narasimhan M., 2018, ARXIV; Osep A., 2017, IEEE INT C ROB AUT I; Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023; Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260; Porzi L, 2020, PROC CVPR IEEE, P6845, DOI 10.1109/CVPR42600.2020.00688; Qiao SY, 2021, PROC CVPR IEEE, P3996, DOI 10.1109/CVPR46437.2021.00399; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schulter S, 2017, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2017.292; Shao S., 2018, ARXIV; Shenoi A, 2020, IEEE INT C INT ROBOT, P10335, DOI 10.1109/IROS45743.2020.9341635; Shuai B, 2021, PROC CVPR IEEE, P12367, DOI 10.1109/CVPR46437.2021.01219; Stadler D, 2021, PROC CVPR IEEE, P10953, DOI 10.1109/CVPR46437.2021.01081; Suna Kim, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P98, DOI 10.1007/978-3-642-37431-9_8; Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394; Tokmakov P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10840, DOI 10.1109/ICCV48922.2021.01068; Voigtlaender P, 2019, PROC CVPR IEEE, P7934, DOI 10.1109/CVPR.2019.00813; Wang S., 2015, BRIT MACHINE VISION; Wang Tao, 2020, IEEE Trans Neural Netw Learn Syst, VPP, DOI 10.1109/TNNLS.2020.2997006; Wojke N, 2017, IEEE IMAGE PROC, P3645; Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217; Wu Z, 2011, PROC CVPR IEEE, P1185, DOI 10.1109/CVPR.2011.5995515; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28; Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409; Xu YH, 2020, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR42600.2020.00682; Xu Z., 2020, EUROPEAN C COMPUTER; Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255; Yu Q, 2007, PROC CVPR IEEE, P170; Zhang L., 2008, IEEE C COMPUTER VISI; Zhang Y, 2020, IEEE T IMAGE PROCESS, V29, P6694, DOI 10.1109/TIP.2020.2993073; Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7	105	0	0	7	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					3035	3053		10.1007/s11263-022-01678-6	http://dx.doi.org/10.1007/s11263-022-01678-6		SEP 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI		hybrid, Green Submitted			2022-12-18	WOS:000860950900002
J	Ju, YK; Shi, BX; Jian, MW; Qi, L; Dong, JY; Lam, KM				Ju, Yakun; Shi, Boxin; Jian, Muwei; Qi, Lin; Dong, Junyu; Lam, Kin-Man			NormAttention-PSN: A High-frequency Region Enhanced Photometric Stereo Network with Normalized Attention	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Photometric stereo; High-frequency surface normals; Non-Lambertian; Deep neural network	NEURAL-NETWORK; REFLECTANCE; SHAPE; SURFACES	Photometric stereo aims to recover the surface normals of a 3D object from various shading cues, establishing the relationship between two-dimensional images and the object geometry. Traditional methods usually adopt simplified reflectance models to approximate the non-Lambertian surface properties, while recently, photometric stereo based on deep learning has been widely used to deal with non-Lambertian surfaces. However, previous studies are limited in dealing with high-frequency surface regions, i.e., regions with rapid shape variations, such as crinkles, edges, etc., resulted in blurry reconstructions. To alleviate this problem, we present a normalized attention-weighted photometric stereo network, namely NormAttention-PSN, to improve surface orientation prediction, especially for those complicated structures. In order to address these challenges, in this paper, we (1) present an attention-weighted loss to produce better surface reconstructions, which applies a higher weight to the detail-preserving gradient loss in high-frequency areas, (2) adopt a double-gate normalization method for non-Lambertian surfaces, to explicitly distinguish whether the high-frequency representation is stimulated by surface structure or spatially varying reflectance, and (3) adopt a parallel high-resolution structure to generate deep features that can maintain the high-resolution details of surface normals. Extensive experiments on public benchmark data sets show that the proposed NormAttention-PSN significantly outperforms traditional calibrated photometric stereo algorithms and state-of-the-art deep learning-based methods.	[Ju, Yakun; Qi, Lin; Dong, Junyu] Ocean Univ China, Dept Comp Sci & Technol, Qingdao, Peoples R China; [Ju, Yakun; Lam, Kin-Man] Hong Kong Polytech Univ, Dept Elect & Informat Engn, Hung Hom, Hong Kong, Peoples R China; [Shi, Boxin] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing, Peoples R China; [Shi, Boxin] Peking Univ, Inst Artificial Intelligence, Beijing, Peoples R China; [Shi, Boxin] Peng Cheng Lab, Shenzhen, Peoples R China; [Jian, Muwei] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan, Peoples R China; [Jian, Muwei] Linyi Univ, Sch Informat Sci & Engn, Linyi, Shandong, Peoples R China; [Lam, Kin-Man] Ctr Adv Reliabil & Safety, Tai Po, Hong Kong, Peoples R China	Ocean University of China; Hong Kong Polytechnic University; Peking University; Peking University; Peng Cheng Laboratory; Shandong University of Finance & Economics; Linyi University	Dong, JY (corresponding author), Ocean Univ China, Dept Comp Sci & Technol, Qingdao, Peoples R China.; Lam, KM (corresponding author), Hong Kong Polytech Univ, Dept Elect & Informat Engn, Hung Hom, Hong Kong, Peoples R China.; Lam, KM (corresponding author), Ctr Adv Reliabil & Safety, Tai Po, Hong Kong, Peoples R China.	juyakun@stu.ouc.edu.cn; shiboxin@pku.edu.cn; jianmuweihk@163.com; qilin@ouc.edu.cn; dongjunyu@ouc.edu.cn; enkmlam@polyu.edu.hk		Ju, Yakun/0000-0003-4065-4108; Dong, Junyu/0000-0001-7012-2087	Key-Area Research and Development Program of Guangdong Province [2020B090928001]; Project of Strategic Importance Fund from The Hong Kong Polytechnic University [ZE1X]; National Key R&D Program of China [2018AAA0100602]; National Key Scientific Instrument and Equipment Development Projects of China [41927805]; National Natural Science Foundation of China [61872012, 62136001, 61976123, 61601427]; Key Development Program for Basic Research of Shandong Province [ZR2020ZD44]; Taishan Young Scholars Program of Shandong Province	Key-Area Research and Development Program of Guangdong Province; Project of Strategic Importance Fund from The Hong Kong Polytechnic University; National Key R&D Program of China; National Key Scientific Instrument and Equipment Development Projects of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Development Program for Basic Research of Shandong Province; Taishan Young Scholars Program of Shandong Province	The work was supported by the Key-Area Research and Development Program of Guangdong Province (2020B090928001), the Project of Strategic Importance Fund from The Hong Kong Polytechnic University (No. ZE1X), the National Key R&D Program of China under Grant (2018AAA0100602), the National Key Scientific Instrument and Equipment Development Projects of China (41927805), and the National Natural Science Foundation of China (61872012, 62136001, 61976123, 61601427), the Key Development Program for Basic Research of Shandong Province (ZR2020ZD44), and the Taishan Young Scholars Program of Shandong Province.	Ackermann J, 2013, FOUND TRENDS COMPUT, V9, P149, DOI 10.1561/0600000065; Alldrin N, 2008, PROC CVPR IEEE, P2447; Alldrin NG, 2007, IEEE I CONF COMP VIS, P417; Barsky S, 2003, IEEE T PATTERN ANAL, V25, P1239, DOI 10.1109/TPAMI.2003.1233898; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652; Chandraker M., 2007, P IEEE C COMPUTER VI, P1; Chandraker M, 2013, IEEE T PATTERN ANAL, V35, P2941, DOI 10.1109/TPAMI.2012.217; Chen GY, 2022, IEEE T PATTERN ANAL, V44, P129, DOI 10.1109/TPAMI.2020.3005397; Chen GY, 2019, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR.2019.00894; Chen GY, 2018, LECT NOTES COMPUT SC, V11213, P3, DOI 10.1007/978-3-030-01240-3_1; Cheng WC, 2006, IEEE IJCNN, P404; Chung HS, 2008, PROC CVPR IEEE, P3337; Einarsson P., 2006, RENDERING TECHNIQUES, P183, DOI [10.2312/EGWR/EGSR06/183-194, DOI 10.2312/EGWR/EGSR06/183-194]; Georghiades AS, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P816; Goldman DB, 2010, IEEE T PATTERN ANAL, V32, P1060, DOI 10.1109/TPAMI.2009.102; Guanying Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P745, DOI 10.1007/978-3-030-58568-6_44; Hartmann W, 2017, IEEE I CONF COMP VIS, P1595, DOI 10.1109/ICCV.2017.176; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Herbort S, 2011, 3D RES, V2, DOI 10.1007/3DRes.03(2011)4; Higo T, 2010, PROC CVPR IEEE, P1157, DOI 10.1109/CVPR.2010.5540084; Holroyd M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409086; Honzatko D, 2021, INT CONF 3D VISION, P394, DOI 10.1109/3DV53792.2021.00049; Hui Z, 2017, IEEE T PATTERN ANAL, V39, P2060, DOI 10.1109/TPAMI.2016.2623613; Ikehata S, 2018, LECT NOTES COMPUT SC, V11219, P3, DOI 10.1007/978-3-030-01267-0_1; Ikehata S, 2014, PROC CVPR IEEE, P2187, DOI 10.1109/CVPR.2014.280; Ikehata S, 2012, PROC CVPR IEEE, P318, DOI 10.1109/CVPR.2012.6247691; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; IWAHORI Y, 1993, IEEE IJCNN, P1181; Jian MW, 2020, IEEE T MULTIMEDIA, V22, P970, DOI 10.1109/TMM.2019.2937187; Johnson MK, 2011, PROC CVPR IEEE; Ju YK, 2020, IEEE I C VI COM I PR, P411; Ju YK, 2022, COMPUT VIS MEDIA, V8, P105, DOI 10.1007/s41095-021-0223-y; Ju YK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P694; Ju YK, 2021, IEEE T IMAGE PROCESS, V30, P3676, DOI 10.1109/TIP.2021.3064230; Li JX, 2019, PROC CVPR IEEE, P7560, DOI 10.1109/CVPR.2019.00775; Logothetis Fotios, 2021, P IEEE INT C COMPUTE, P12757; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; McAuley Stephen, 2012, ACM SIGGRAPH 2012 CO, P1; Miyazaki D, 2010, INT J COMPUT VISION, V86, P229, DOI 10.1007/s11263-009-0262-9; Mukaigawa Y, 2007, J OPT SOC AM A, V24, P3326, DOI 10.1364/JOSAA.24.003326; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; Santo H, 2022, IEEE T PATTERN ANAL, V44, P114, DOI 10.1109/TPAMI.2020.3005219; Santo H, 2017, IEEE INT CONF COMP V, P501, DOI 10.1109/ICCVW.2017.66; Shi BX, 2019, IEEE T PATTERN ANAL, V41, P271, DOI 10.1109/TPAMI.2018.2799222; Shi BX, 2012, LECT NOTES COMPUT SC, V7574, P455, DOI 10.1007/978-3-642-33712-3_33; Shi BX, 2014, IEEE T PATTERN ANAL, V36, P1078, DOI 10.1109/TPAMI.2013.196; SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103; Solomon F, 1996, IEEE T PATTERN ANAL, V18, P449, DOI 10.1109/34.491627; Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072; Taniai T, 2018, PR MACH LEARN RES, V80; Tozza S, 2016, J MATH IMAGING VIS, V56, P57, DOI 10.1007/s10851-016-0633-0; Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596; Verbiest F, 2008, PROC CVPR IEEE, P2886; Wang X, 2020, IEEE T IMAGE PROCESS, V29, P6032, DOI 10.1109/TIP.2020.2987176; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wiles Olivia, 2017, P BMVC; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008; Yao Z., 2020, P ADV NEURAL INFORM; Yeung SK, 2015, IEEE T PATTERN ANAL, V37, P890, DOI 10.1109/TPAMI.2014.2346195; Yu C, 2010, LECT NOTES COMPUT SC, V6314, P115; Zheng Q., 2020, VIRTUAL REALITY INTE, V2, P213, DOI [10.1016/j.vrih.2020.03.001, 2020, DOI 10.1016/J.VRIH.2020.03.001]; Zheng Q, 2019, IEEE I CONF COMP VIS, P8548, DOI 10.1109/ICCV.2019.00864	65	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					3014	3034		10.1007/s11263-022-01684-8	http://dx.doi.org/10.1007/s11263-022-01684-8		SEP 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI					2022-12-18	WOS:000860950900001
J	Xie, JH; Zhan, XH; Liu, ZW; Ong, YS; Loy, CC				Xie, Jiahao; Zhan, Xiaohang; Liu, Ziwei; Ong, Yew-Soon; Loy, Chen Change			Delving into Inter-Image Invariance for Unsupervised Visual Representations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised learning; Self-supervised learning; Representation learning; Contrastive learning; Inter-image invariance		Contrastive learning has recently shown immense potential in unsupervised visual representation learning. Existing studies in this track mainly focus on intra-image invariance learning. The learning typically uses rich intra-image transformations to construct positive pairs and then maximizes agreement using a contrastive loss. The merits of inter-image invariance, conversely, remainmuch less explored. Onemajor obstacle to exploit inter-image invariance is that it is unclear how to reliably construct inter-image positive pairs, and further derive effective supervision from them since no pair annotations are available. In this work, we present a comprehensive empirical study to better understand the role of inter-image invariance learning from three main constituting components: pseudo-label maintenance, sampling strategy, and decision boundary design. To facilitate the study, we introduce a unified and generic framework that supports the integration of unsupervised intra- and inter-image invariance learning. Through carefully-designed comparisons and analysis, multiple valuable observations are revealed: 1) online labels converge faster and perform better than offline labels; 2) semi-hard negative samples are more reliable and unbiased than hard negative samples; 3) a less stringent decision boundary is more favorable for inter-image invariance learning. With all the obtained recipes, our final model, namely InterCLR, shows consistent improvements over state-of-the-art intra-image invariance learning methods on multiple standard benchmarks. We hope this work will provide useful experience for devising effective unsupervised inter-image invariance learning. Code: https://github.com/open-mmlab/ mmselfsup.	[Xie, Jiahao; Liu, Ziwei; Ong, Yew-Soon; Loy, Chen Change] Nanyang Technol Univ, 50 Nanyang Ave, Singapore, Singapore; [Zhan, Xiaohang] Chinese Univ Hong Kong, Sha Tin, Hong Kong, Peoples R China	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Chinese University of Hong Kong	Loy, CC (corresponding author), Nanyang Technol Univ, 50 Nanyang Ave, Singapore, Singapore.	jiahao003@niu.edu.sg; xiaohangzhan@oudook.com; ziwei.liu@ntu.edu.sg; asysong@ntu.edu.sg; ccloy@ntu.edu.sg		Loy, Chen Change/0000-0001-5345-1591	RIE2020 Industry Alignment Fund-Industry Collaboration Projects (IAF-ICP) Funding Initiative; Singapore MOE AcRF Tier 2 [T2EP20120-0001]; Data Science and Artificial Intelligence Research Center at Nanyang Technological University	RIE2020 Industry Alignment Fund-Industry Collaboration Projects (IAF-ICP) Funding Initiative; Singapore MOE AcRF Tier 2(Ministry of Education, Singapore); Data Science and Artificial Intelligence Research Center at Nanyang Technological University	This study is supported under the RIE2020 Industry Alignment Fund-Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). The project is also supported by Singapore MOE AcRF Tier 2 (T2EP20120-0001), the Data Science and Artificial Intelligence Research Center at Nanyang Technological University.	Alwassel Humam, 2020, NEURIPS; Arora S, 2019, PR MACH LEARN RES, V97; Asano Yuki M., 2020, NEURIPS; Asano Yuki Markus, 2020, P ICLR; Bachman P, 2019, ADV NEUR IN, V32; Bojanowski P, 2017, PR MACH LEARN RES, V70; Caron M, 2019, IEEE I CONF COMP VIS, P2959, DOI 10.1109/ICCV.2019.00305; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Caron Mathilde, 2020, NEURIPS; Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626; Chen T, 2020, PR MACH LEARN RES, V119; Chen X, 2021, AUTOPHAGY, V17, P2054, DOI 10.1080/15548627.2020.1810918; Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549; Chuang Ching-Yao, 2020, NEURIPS; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Ding J., 2021, ARXIV; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Donahue J, 2019, ADV NEUR IN, V32; Donahue Jeff, 2017, INT C LEARN REPR ICL; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766, DOI [DOI 10.1109/TPAMI.2015.2496141, 10.48550/arXiv.1406.6909]; Dwibedi D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9568, DOI 10.1109/ICCV48922.2021.00945; Ericsson L, 2022, IEEE SIGNAL PROC MAG, V39, P42, DOI 10.1109/MSP.2021.3134634; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fang Wei, 2021, NEURIPS; Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal P, 2019, IEEE I CONF COMP VIS, P6400, DOI 10.1109/ICCV.2019.00649; Grill J.B., 2020, ADV NEUR IN; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; Harwood B, 2017, IEEE I CONF COMP VIS, P2840, DOI 10.1109/ICCV.2017.307; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He K., 2020, P IEEECVF C COMPUTER, P9729; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Henaff O.J., 2019, ARXIV; Henaff OJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10066, DOI 10.1109/ICCV48922.2021.00993; Hjelm R Devon, 2019, INT C LEARN REPR; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Ji X, 2019, IEEE I CONF COMP VIS, P9864, DOI 10.1109/ICCV.2019.00996; Kalantidis Yannis, 2020, ARXIV201001028; Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35; Lee D., 2013, INT C MACH LEARN ICM; Li Junnan, 2021, ICLR; Liao R., 2016, ADV NEURAL INFORM PR, P5076; Lim S, 2019, ADV NEUR IN, V32; Liu S., 2020, ARXIV; Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Loshchilov I., 2016, ARXIV; Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Morgado P, 2021, PROC CVPR IEEE, P12470, DOI 10.1109/CVPR46437.2021.01229; Morgado Pedro, 2021, CVPR; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Oord A.v.d., 2018, ARXIV; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pinheiro Pedro O, 2020, NEURIPS; Purushwalkam Senthil, 2020, DEMYSTIFYING CONTRAS, P2; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Robinson Joshua David, 2021, ICLR; Roh B, 2021, PROC CVPR IEEE, P1144, DOI 10.1109/CVPR46437.2021.00120; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Selvaraju RR, 2021, PROC CVPR IEEE, P11053, DOI 10.1109/CVPR46437.2021.01091; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Suh Y, 2019, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2019.00742; Tian Y., 2020, ECCV, P776, DOI [10.48550/arXiv.1906.05849, DOI 10.1007/978-3-030-58621-8_45]; Tian Y., 2020, ARXIV200510243, V33, P6827; TongzhouWang Phillip, 2020, ICML; Tosh C., 2021, ALGORITHMIC LEARNING, P1179; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang XL, 2021, PROC CVPR IEEE, P3023, DOI 10.1109/CVPR46437.2021.00304; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Wu Y., 2019, DETECTRON2; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Xiao Tete, 2021, ICCV; Xiao Tete, 2021, ICLR; Xie EZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8372, DOI 10.1109/ICCV48922.2021.00828; Xie J, 2021, NEURIPS; Xie JY, 2016, PR MACH LEARN RES, V48; Xie Z., 2021, CVPR, P16684; Yang CY, 2021, PROC CVPR IEEE, P3986, DOI 10.1109/CVPR46437.2021.00398; Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556; Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637; Zbontar J, 2021, PR MACH LEARN RES, V139; Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156; Zhan XH, 2020, PROC CVPR IEEE, P6687, DOI 10.1109/CVPR42600.2020.00672; Zhan XH, 2019, PROC CVPR IEEE, P1881, DOI 10.1109/CVPR.2019.00198; Zhang LH, 2019, PROC CVPR IEEE, P2542, DOI 10.1109/CVPR.2019.00265; Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhao Nanxuan, 2021, ICLR; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhuang CX, 2019, IEEE I CONF COMP VIS, P6001, DOI 10.1109/ICCV.2019.00610	99	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					2994	3013		10.1007/s11263-022-01681-x	http://dx.doi.org/10.1007/s11263-022-01681-x		SEP 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI		Green Submitted			2022-12-18	WOS:000857807300001
J	Li, Y; Zhu, Y; Li, RT; Wang, XT; Luo, Y; Shan, Y				Li, Yu; Zhu, Ye; Li, Ruoteng; Wang, Xintao; Luo, Yue; Shan, Ying			Hybrid Warping Fusion for Video Frame Interpolation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video frame interpolation; Hybrid warping; View synthesis		Video frame interpolation aims to synthesize new intermediate frames between existing ones, which is an important task in video enhancement. A classic direction in this field is flow-based which estimates motions in the form of optical flow, warps the frames, and synthesizes the final results. In this work, we explicitly investigate the warping step and propose a way to combine the strength from using both forward and backward warping. Our method, named HWFI, introduces hybrid warping fusion for frame interpolation. We also include edge information explicitly in our pipeline and employ channel attention in our synthesis network. Compared to the latest state-of-the-art method that only uses forward warping, our method produces better results with higher quality, especially in edge regions. Extensive experiments show that our method can obtain the best results qualitatively and quantitatively on multiple benchmark datasets.	[Li, Yu] Int Digital Econ Acad IDEA, Shenzhen 518048, Peoples R China; [Zhu, Ye; Wang, Xintao; Luo, Yue; Shan, Ying] Tencent PCG, ARC Lab, Shenzhen 518066, Peoples R China; [Li, Ruoteng] Natl Univ Singapore, Singapore 117583, Singapore	National University of Singapore	Li, Y (corresponding author), Int Digital Econ Acad IDEA, Shenzhen 518048, Peoples R China.	liyu@idea.edu.cn		LI, Yu/0000-0003-1865-8276				Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382; Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941; Bojanowski P, 2018, PR MACH LEARN RES, V80; Cheng XH, 2022, IEEE T PATTERN ANAL, V44, P7029, DOI 10.1109/TPAMI.2021.3100714; Cheng XH, 2020, AAAI CONF ARTIF INTE, V34, P10607; Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663; Choi Myungsub, 2020, P IEEE C COMP VIS PA, P9444, DOI DOI 10.1109/CVPR42600.2020.00946; Ding Tianyu, 2021, P IEEECVF C COMPUTER, P8001; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Fourure D., 2017, PROC BRIT MACH VIS C; Gu DH, 2019, IEEE INT CON MULTI, P1768, DOI 10.1109/ICME.2019.00304; Haoxian Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P474, DOI 10.1007/978-3-030-58595-2_29; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jaeyeon Kang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P701, DOI 10.1007/978-3-030-58607-2_41; Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938; Junheum Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P109, DOI 10.1007/978-3-030-58568-6_7; Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251; Kingma D.P, P 3 INT C LEARNING R; Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536; Lee S., 2022, P IEEECVF WINTER C A, P2839; Li HP, 2020, INT CONF ACOUST SPEE, P2613, DOI 10.1109/ICASSP40776.2020.9053987; Liu YL, 2019, AAAI CONF ARTIF INTE, P8794; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Long GC, 2015, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2015.7298729; Meyer S, 2018, PROC CVPR IEEE, P498, DOI 10.1109/CVPR.2018.00059; Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747; Nguyen-Phuoc TH., 2018, NEURIPS, V31, P7902; Niklaus S, 2021, IEEE WINT CONF APPL, P1098, DOI 10.1109/WACV48630.2021.00114; Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548; Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183; Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37; Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244; Peleg T, 2019, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2019.00250; Reda FA, 2018, LECT NOTES COMPUT SC, V11211, P747, DOI 10.1007/978-3-030-01234-2_44; Reda FA, 2019, IEEE I CONF COMP VIS, P892, DOI 10.1109/ICCV.2019.00098; Shen W, 2020, PROC CVPR IEEE, P5113, DOI 10.1109/CVPR42600.2020.00516; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Shi ZH, 2022, IEEE T MULTIMEDIA, V24, P426, DOI 10.1109/TMM.2021.3052419; Sim Hyeonjun, 2021, P IEEE CVF INT C COM, P14489; Siyao L., 2021, P IEEECVF C COMPUTER, P6587; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Teed Zachary, 2020, ECCV, DOI DOI 10.1007/978-3-030-58536-5_24; Tulyakov S, 2021, PROC CVPR IEEE, P16150, DOI 10.1109/CVPR46437.2021.01589; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wu CY, 2018, LECT NOTES COMPUT SC, V11212, P425, DOI 10.1007/978-3-030-01237-3_26; Xiang XY, 2020, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR42600.2020.00343; Xue F., 2021, 2021 IEEE INT C MULT, P1; Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2; Yihao Liu, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P41, DOI 10.1007/978-3-030-66823-5_3; Zhixiang Chi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P107, DOI 10.1007/978-3-030-58583-9_7	53	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					2980	2993		10.1007/s11263-022-01683-9	http://dx.doi.org/10.1007/s11263-022-01683-9		SEP 2022	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI					2022-12-18	WOS:000860938900001
J	Triess, LT; Rist, CB; Peter, D; Zollner, JM				Triess, Larissa T.; Rist, Christoph B.; Peter, David; Zoellner, J. Marius			A Realism Metric for Generated LiDAR Point Clouds	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Metric; Point cloud; LiDAR; Realism; Adversarial learning; Local features; Semantic segmentation		A considerable amount of research is concerned with the generation of realistic sensor data. LiDAR point clouds are generated by complex simulations or learned generative models. The generated data is usually exploited to enable or improve downstream perception algorithms. Two major questions arise from these procedures: First, how to evaluate the realism of the generated data? Second, does more realistic data also lead to better perception performance? This paper addresses both questions and presents a novel metric to quantify the realism of LiDAR point clouds. Relevant features are learned from real-world and synthetic point clouds by training on a proxy classification task. In a series of experiments, we demonstrate the application of our metric to determine the realism of generated LiDAR data and compare the realism estimation of our metric to the performance of a segmentation model. We confirm that our metric provides an indication for the downstream segmentation performance.	[Triess, Larissa T.; Rist, Christoph B.; Peter, David] Mercedes Benz AG, Stuttgart, Germany; [Triess, Larissa T.; Zoellner, J. Marius] Karlsruhe Inst Technol, Karlsruhe, Germany; [Peter, David] Robert Bosch GmbH, Stuttgart, Germany; [Zoellner, J. Marius] Res Ctr Informat Technol, Karlsruhe, Germany	Daimler AG; Helmholtz Association; Karlsruhe Institute of Technology; Bosch	Triess, LT (corresponding author), Mercedes Benz AG, Stuttgart, Germany.; Triess, LT (corresponding author), Karlsruhe Inst Technol, Karlsruhe, Germany.	larissa.triess@mercedes-benz.com; christoph_bernd.rist@mercedes-benz.com; david.peter@bosch.com; zoellner@fzi.de	; Peter, David/A-2669-2013	Triess, Larissa/0000-0003-0037-8460; Peter, David/0000-0001-7950-9915	Projekt DEAL	Projekt DEAL	Open Access funding enabled and organized by Projekt DEAL.	Achlioptas P, 2018, PR MACH LEARN RES, V80; [Anonymous], 2020, SCALE PANDASET; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arora S., 2018, P INT C LEARN REPR; Beutel Alex, 2017, ABS170700075 CORR; Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009; Caccia L, 2019, IEEE INT C INT ROBOT, P5034, DOI 10.1109/IROS40897.2019.8968535; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53; Che T, 2017, INT C LEARN REPR ICL; Chicco Davide, 2021, Methods Mol Biol, V2190, P73, DOI 10.1007/978-1-0716-0826-5_3; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gurumurthy Swaminathan, 2017, PROC CVPR IEEE, P166, DOI DOI 10.1109/CVPR.2017.525; Hensel M, 2017, ADV NEUR IN, V30; Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Hurl B, 2019, IEEE INT VEH SYM, P2522, DOI 10.1109/IVS.2019.8813809; Im D., 2016, GENERATING IMAGES RE; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Khrulkov V, 2018, PR MACH LEARN RES, V80; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lehmann Erich L, 2006, TESTING STAT HYPOTHE; Li D., 2022, P IEEE C COMPUTER VI; Lin ZA, 2018, ADV NEUR IN, V31; Lohdefink J., 2022, IMPROVING PERFORMANC; Lucic M, 2018, ADV NEUR IN, V31; Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762; Olsson C., 2018, SKILL RATING GENERAT; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Qi CR, 2017, ADV NEUR IN, V30; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Raff E, 2018, PR INT CONF DATA SC, P189, DOI 10.1109/DSAA.2018.00029; Ren K., 2017, P INT C LEARNING REP; Richardson E., 2018, ADV NEURAL INFORM PR, P5852; Salimans T, 2016, ADV NEUR IN, V29; Sallab A. E., 2019, P INT C MACH LEARN I; Santurkar S, 2018, PR MACH LEARN RES, V80; Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396; Srivastava A, 2017, ADV NEUR IN, V30; Theis Lucas, 2016, ICLR; Tolstikhin I. O., 2017, ADV NEURAL INFORM PR, P5424, DOI DOI 10.5555/3295222.3295294; Triess L. T., 2021, P GERMAN C PATTERN R; Triess LT, 2021, 2021 IEEE INTELLIGENT VEHICLES SYMPOSIUM WORKSHOPS (IV WORKSHOPS), P350, DOI 10.1109/IVWorkshops54471.2021.9669228; Triess LT, 2019, IEEE INT VEH SYM, P1512, DOI 10.1109/IVS.2019.8813771; Wang YX, 2016, ADV NEUR IN, V29; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI 10.1109/ICRA.2019.8793495; Xiang Sitao, 2017, EFFECTS BATCH WEIGHT; Xu Q., 2018, EMPIRICAL STUDY EVAL; Xu Y., 2021, SELF ENSEMBLING GAN; Yang J., 2017, P INT C LEARNING REP; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang ZF, 2018, IEEE WINT CONF APPL, P700, DOI 10.1109/WACV.2018.00082	62	0	0	4	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					2962	2979		10.1007/s11263-022-01676-8	http://dx.doi.org/10.1007/s11263-022-01676-8		SEP 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI		hybrid			2022-12-18	WOS:000856598600001
J	Han, MZ; Zhang, ZY; Jiao, ZY; Xie, X; Zhu, YX; Zhu, SC; Liu, HX				Han, Muzhi; Zhang, Zeyu; Jiao, Ziyuan; Xie, Xu; Zhu, Yixin; Zhu, Song-Chun; Liu, Hangxin			Scene Reconstruction with Functional Objects for Robot Autonomy	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Functional scene representation; 3D scene reconstruction; Actionable information; Volumetric panoptic mapping; Physical reasoning; Robot interaction	MOTION; DENSE; TASK	In this paper, we rethink the problem of scene reconstruction from an embodied agent's perspective: While the classic view focuses on the reconstruction accuracy, our new perspective emphasizes the underlying functions and constraints of the reconstructed scenes that provide actionable information for simulating interactions with agents. Here, we address this challenging problem by reconstructing a functionally equivalent and interactive scene from RGB-D data streams, where the objects within are segmented by a dedicated 3D volumetric panoptic mapping module and subsequently replaced by part-based articulated CAD models to afford finer-grained robot interactions. The object functionality and contextual relations are further organized by a graph-based scene representation that can be readily incorporated into robots' action specifications and task definition, facilitating their long-term task and motion planning in the scenes. In the experiments, we demonstrate that (i) our panoptic mapping module outperforms previous state-of-the-art methods in recognizing and segmenting scene entities, (ii) the geometric and physical reasoning procedure matches, aligns, and replaces object meshes with best-fitted CAD models, and (iii) the reconstructed functionally equivalent and interactive scenes are physically plausible and naturally afford actionable interactions; without any manual labeling, they are seamlessly imported to ROS-based robot simulators and VR environments for simulating complex robot interactions.	[Han, Muzhi; Zhang, Zeyu; Jiao, Ziyuan; Xie, Xu] Univ Calif Los Angeles, Ctr Vis Cognit Learning & Auton, Los Angeles, CA USA; [Zhang, Zeyu; Jiao, Ziyuan; Zhu, Song-Chun; Liu, Hangxin] Beijing Inst Gen Artificial Intelligence BIGAI, Beijing, Peoples R China; [Zhu, Yixin; Zhu, Song-Chun] Peking Univ, Inst Artificial Intelligence, Beijing, Peoples R China	University of California System; University of California Los Angeles; Peking University	Liu, HX (corresponding author), Beijing Inst Gen Artificial Intelligence BIGAI, Beijing, Peoples R China.; Zhu, YX (corresponding author), Peking Univ, Inst Artificial Intelligence, Beijing, Peoples R China.	muzhihan@ucla.edu; zeyuzhang@ucla.edu; zyjiao@ucla.edu; xiexu@ucla.edu; yixin.zhu@pku.edu.cn; sczhu@bigai.ai; huhx@bigai.ai		Jiao, Ziyuan/0000-0003-3404-3810; Liu, Hangxin/0000-0002-3003-8611; Han, Muzhi/0000-0002-2649-4577				Agin G. J., 1973, INT JOINT C ARTIFICI; [Anonymous], 2020, T PATTERN ANAL MACHI, DOI DOI 10.1109/TPAMI.2020.2976971; Armeni I, 2019, IEEE I CONF COMP VIS, P5663, DOI 10.1109/ICCV.2019.00576; Avetisyan A, 2019, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2019.00264; Avetisyan A, 2019, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2019.00272; Baoxiong Jia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P767, DOI 10.1007/978-3-030-58574-7_46; Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109; Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; Chang Angel X., 2015, ARXIV151203012CSGR P; Chang HJ, 2018, IEEE T PATTERN ANAL, V40, P2165, DOI 10.1109/TPAMI.2017.2748579; Chen Y., 2019, INT C COMPUTER VISIO; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Deitke M, 2020, PROC CVPR IEEE, P3161, DOI 10.1109/CVPR42600.2020.00323; Edmonds M, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aay4663; Edmonds M, 2017, IEEE INT C INT ROBOT, P3530; Fanbo Xiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11094, DOI 10.1109/CVPR42600.2020.01111; Furrer F, 2018, IEEE INT C INT ROBOT, P6835, DOI 10.1109/IROS.2018.8594391; Garrett CR, 2020, IEEE INT CONF ROBOT, P5678, DOI 10.1109/ICRA40945.2020.9196681; GIBSON JAMES J., 1966; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; Grinvald M, 2019, IEEE ROBOT AUTOM LET, V4, P3037, DOI 10.1109/LRA.2019.2923960; Gupta S, 2015, PROC CVPR IEEE, P4731, DOI 10.1109/CVPR.2015.7299105; Han L, 2020, PROC CVPR IEEE, P2937, DOI 10.1109/CVPR42600.2020.00301; Han M., 2021, IEEE INT C ROB AUT I; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Hoang DC, 2020, IEEE ROBOT AUTOM LET, V5, P1962, DOI 10.1109/LRA.2020.2970682; Hua BS, 2016, INT CONF 3D VISION, P92, DOI 10.1109/3DV.2016.18; Huang SY, 2018, LECT NOTES COMPUT SC, V11211, P194, DOI 10.1007/978-3-030-01234-2_12; Huang SY, 2018, ADV NEUR IN, V31; Ikeuchi K., 1992, IEEERSJ INT C INTELL; Jiang CFF, 2018, INT J COMPUT VISION, V126, P920, DOI 10.1007/s11263-018-1103-5; Jiao, 2022, IEEERSJ INT C INTELL; Jiao ZY, 2021, IEEE INT C INT ROBOT, P8288, DOI 10.1109/IROS51168.2021.9636554; Jiao ZY, 2021, IEEE INT C INT ROBOT, P979, DOI 10.1109/IROS51168.2021.9636351; JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710; Kaelbling LP, 2020, SCIENCE, V369, P915, DOI 10.1126/science.aaz7597; Kaelbling LP, 2011, IEEE INT CONF ROBOT, P1470; Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963; Knill D. C., 1996, PERCEPTION BAYESIAN, P1, DOI [10.1017/cbo9780511984037, DOI 10.1017/CBO9780511984037, 10.1017/CBO9780511984037]; Li XL, 2020, PROC CVPR IEEE, P3703, DOI 10.1109/CVPR42600.2020.00376; Li XT, 2019, PROC CVPR IEEE, P12360, DOI 10.1109/CVPR.2019.01265; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu HX, 2019, AAAI CONF ARTIF INTE, P8025; Liu HX, 2018, IEEE INT CONF ROBOT, P1947; Liu LB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201315; Malandain G, 2002, INT J COMPUT GEOM AP, V12, P489, DOI 10.1142/S0218195902001006; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Martin-Martin R, 2022, INT J ROBOT RES, V41, P741, DOI 10.1177/0278364919848850; McCormac John, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4628, DOI 10.1109/ICRA.2017.7989538; McCormac J., 2018, INT C 3D VISION 3DV; Min HQ, 2016, IEEE T COGN DEV SYST, V8, P237, DOI 10.1109/TCDS.2016.2614992; MINTON S, 1992, ARTIF INTELL, V58, P161, DOI 10.1016/0004-3702(92)90007-K; Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100; More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Myers A, 2015, IEEE INT CONF ROBOT, P1374, DOI 10.1109/ICRA.2015.7139369; Narita G, 2019, IEEE INT C INT ROBOT, P4205, DOI 10.1109/IROS40897.2019.8967890; Oleynikova H, 2017, IEEE INT C INT ROBOT, P1366; Pham Q. H., 2018, 3DOR P 11 EUROGRAPHI; Pronobis A, 2012, IEEE INT CONF ROBOT, P3515, DOI 10.1109/ICRA.2012.6224637; Qi SY, 2018, PROC CVPR IEEE, P5899, DOI 10.1109/CVPR.2018.00618; Pham QH, 2019, PROC CVPR IEEE, P8819, DOI 10.1109/CVPR.2019.00903; Pham QH, 2019, IEEE WINT CONF APPL, P1089, DOI 10.1109/WACV.2019.00121; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rosinol A, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI; Savva M, 2019, IEEE I CONF COMP VIS, P9338, DOI 10.1109/ICCV.2019.00943; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song S., 2015, C COMPUTER VISION PA; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Srivastava S, 2014, IEEE INT CONF ROBOT, P639, DOI 10.1109/ICRA.2014.6906922; Sturm J, 2011, J ARTIF INTELL RES, V41, P477, DOI 10.1613/jair.3229; Sui ZQ, 2020, IEEE ROBOT AUTOM LET, V5, P5913, DOI 10.1109/LRA.2020.3010443; Taguchi Y, 2013, IEEE INT CONF ROBOT, P5182, DOI 10.1109/ICRA.2013.6631318; Wada K., 2020, C COMPUTER VISION PA; Wald J, 2020, PROC CVPR IEEE, P3960, DOI 10.1109/CVPR42600.2020.00402; Wu Y., 2019, DETECTRON2; Xia F, 2020, IEEE ROBOT AUTOM LET, V5, P713, DOI 10.1109/LRA.2020.2965078; Xie X., 2019, P ACM TUR CEL C CHIN; Xu K, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818075; Yang SA, 2019, INT CONF MACH LEARN, P1; Yang SC, 2019, IEEE ROBOT AUTOM LET, V4, P3145, DOI 10.1109/LRA.2019.2924848; Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407; Yu LF, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964981; Yuan T, 2020, IEEE INT CONF ROBOT, P5972, DOI 10.1109/ICRA40945.2020.9197355; Zhang JY, 2019, IEEE ACCESS, V7, P179118, DOI 10.1109/ACCESS.2019.2958671; ZHANG KZ, 1989, SIAM J COMPUT, V18, P1245, DOI 10.1137/0218082; Zhang Z., 2020, IEEERSJ INT C INTELL; Zhang ZY, 2022, IEEE ROBOT AUTOM LET, V7, P9469, DOI 10.1109/LRA.2022.3191793; Zhao Y., 2011, ADV NEURAL INFORM PR, P73; Zhao YB, 2013, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2013.401; Zheng B, 2015, INT J COMPUT VISION, V112, P221, DOI 10.1007/s11263-014-0795-4; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018; Zhu YX, 2020, ENGINEERING-PRC, V6, P310, DOI 10.1016/j.eng.2020.01.011; Zhu YX, 2015, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2015.7298903; ZHU YX, 2016, PROC CVPR IEEE, P3823, DOI DOI 10.1109/CVPR.2016.415; Zou CH, 2019, INT J COMPUT VISION, V127, P143, DOI 10.1007/s11263-018-1133-z	99	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					2940	2961		10.1007/s11263-022-01670-0	http://dx.doi.org/10.1007/s11263-022-01670-0		SEP 2022	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI					2022-12-18	WOS:000855606900002
J	Wang, DK; Zhang, SL				Wang, Dongkai; Zhang, Shiliang			Unsupervised Person Re-Identification via Multi-Label Classification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Unsupervised learning; Multi-label classification	DISSIMILARITY	The challenge of unsupervised person re-identification (ReID) lies in learning discriminative features without true labels. Most of previous works predict single-class pseudo labels through clustering. To improve the quality of generated pseudo labels, this paper formulates unsupervised person ReID as a multi-label classification task to progressively seek true labels. Our method starts by assigning each person image with a single-class label, then evolves to multi-label classification by leveraging the updated ReID model for label prediction. We first investigate the effect of precision and recall rates of pseudo labels to the ReID accuracy. This study motivates the Clustering-guided Multi-class Label Prediction (CMLP), which adopts clustering and cycle consistency to ensure high recall rate and reasonably good precision rate in pseudo labels. To boost the unsupervised learning efficiency, we further propose the Memory-based Multi-label Classification Loss (MMCL). MMCL works with memory-based non-parametric classifier and integrates local loss and global loss to seek high optimization efficiency. CMLP and MMCL work iteratively and substantially boost the ReID performance. Experiments on several large-scale person ReID datasets demonstrate the superiority of our method in unsupervised person ReID. For instance, with fully unsupervised setting we achieve rank-1 accuracy of 90.1% on Market-1501, already outperforming many transfer learning and supervised learning methods.	[Wang, Dongkai; Zhang, Shiliang] Peking Univ, Sch Comp Sci, Beijing, Peoples R China; [Zhang, Shiliang] Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China	Peking University; Peng Cheng Laboratory	Zhang, SL (corresponding author), Peking Univ, Sch Comp Sci, Beijing, Peoples R China.; Zhang, SL (corresponding author), Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China.	dongkai.wang@pku.edu.cn; slzhang.jdl@pku.edu.cn	wang, dk/GXG-9446-2022		Natural Science Foundation of China [U20B2052, 61936011]; National Key Research and Development Program of China [2018YFE0118400]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China	This work is supported in part by Natural Science Foundation of China under Grant Nos. U20B2052, 61936011, in part by The National Key Research and Development Program of China under Grant Nos. 2018YFE0118400.	Arazo E, 2019, PR MACH LEARN RES, V97; Arpit D, 2017, PR MACH LEARN RES, V70; Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204; Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032; Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110; Ding G., 2019, BMVC; Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099; Durand T, 2019, PROC CVPR IEEE, P647, DOI 10.1109/CVPR.2019.00074; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316; Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621; Ge Yixiao, 2020, ARXIV200101526; Ge Yixiao, 2020, NIPS, P5; Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2019, ARXIV; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Iscen A, 2018, PROC CVPR IEEE, P7642, DOI 10.1109/CVPR.2018.00797; Jegou H, 2007, PROC CVPR IEEE, P9; Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29; Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kodirov E., 2015, BMVC, V3, P8; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343; Li JN, 2018, ADV NEUR IN, V31; Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin S., 2018, P BRIT MACH VIS C; Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345; Lin YT, 2019, AAAI CONF ARTIF INTE, P8738; Long M., 2015, ARXIV; Lv JM, 2018, PROC CVPR IEEE, P7948, DOI 10.1109/CVPR.2018.00829; Ma F, 2017, PR MACH LEARN RES, V70; Neverova N, 2019, ADV NEUR IN, V32; Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582; Tang YC, 2012, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2012.6247936; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang G., 2020, IEEECVF C COMPUTER V; Wang H., 2014, P BRIT MACHINE VISIO; Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wu AC, 2019, IEEE I CONF COMP VIS, P6921, DOI 10.1109/ICCV.2019.00702; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Wu Q., 2021, P IEEECVF C COMPUTER, P2993; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107; Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482; Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375; Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637; Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225; Yu HX, 2020, IEEE T PATTERN ANAL, V42, P956, DOI 10.1109/TPAMI.2018.2886878; Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113; Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904; Zhang C., 2016, ARXIV; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39; Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831; Zhang ZL, 2018, ADV NEUR IN, V31; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zheng L., 2016, ARXIV; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11; Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933; Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069; Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541; Zhong Zhun, 2017, PROC CVPR IEEE, P1318, DOI DOI 10.1109/CVPR.2017.389	76	0	0	5	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					2924	2939		10.1007/s11263-022-01680-y	http://dx.doi.org/10.1007/s11263-022-01680-y		SEP 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI		Green Submitted			2022-12-18	WOS:000855606900001
J	Sushko, V; Schonfeld, E; Zhang, D; Gall, J; Schiele, B; Khoreva, A				Sushko, Vadim; Schoenfeld, Edgar; Zhang, Dan; Gall, Juergen; Schiele, Bernt; Khoreva, Anna			OASIS: Only Adversarial Supervision for Semantic Image Synthesis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic image synthesis; GAN; Semantic segmentation; Label-to-image translation; Image editing		Despite their recent successes, generative adversarial networks (GANs) for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Previously, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limited the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity and with a better alignment to their input label maps, making the use of the perceptual loss superfluous. Furthermore, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image editing. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve a strong improvement in image synthesis quality over prior state-of-the-art models across the commonly used ADE20K, Cityscapes, and COCO-Stuff datasets using only adversarial supervision. In addition, we investigate semantic image synthesis under severe class imbalance and sparse annotations, which are common aspects in practical applications but were overlooked in prior works. To this end, we evaluate our model on LVIS, a dataset originally introduced for long-tailed object recognition. We thereby demonstrate high performance of our model in the sparse and unbalanced data regimes, achieved by means of the proposed 3D noise and the ability of our discriminator to balance class contributions directly in the loss function. Our code and pretrained models are available at https://github.com/boschresearch/OASIS.	[Sushko, Vadim; Schoenfeld, Edgar; Zhang, Dan; Khoreva, Anna] Bosch Ctr Artificial Intelligence, Renningen, Germany; [Zhang, Dan; Khoreva, Anna] Univ Tubingen, Tubingen, Germany; [Gall, Juergen] Univ Bonn, Bonn, Germany; [Schiele, Bernt] Max Planck Inst Informat, Saarbrucken, Germany	Eberhard Karls University of Tubingen; University of Bonn; Max Planck Society	Sushko, V (corresponding author), Bosch Ctr Artificial Intelligence, Renningen, Germany.	vadim.sushko@bosch.com; edgar.schoenfeld@bosch.com; dan.zhang2@bosch.com; gall@iai.uni-bonn.de; schiele@mpi-inf.mpg.de; anna.khoreva@bosch.com			Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy [EXC 2070 -390732324]; ERC Consolidator Grant FORHUE [101044724]	Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy(German Research Foundation (DFG)); ERC Consolidator Grant FORHUE	Juergen Gall has been supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC 2070 -390732324 and the ERC Consolidator Grant FORHUE (101044724).	Alharbi Y, 2020, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR42600.2020.00518; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arjovsky M, 2017, PR MACH LEARN RES, V70; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Brock A., 2019, INT C LEARNING REPRE; Bruna J., 2016, INT C LEARNING REPRE; Casanova Arantxa, 2021, ADV NEURAL INFORM PR; Chen D., 2020, ARXIV; Chen L.-C., 2018, EUROPEAN C COMPUTER; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; Hensel M, 2017, ADV NEUR IN, V30; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813; Karras T., 2021, ADV NEURAL INF PROCE, V34; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Karras Tero, 2020, ARXIV200606676, V33; Kingma D.P, P 3 INT C LEARNING R; Li K., 2018, ARXIV; Li K, 2019, IEEE I CONF COMP VIS, P4219, DOI 10.1109/ICCV.2019.00432; Li Y., 2021, INT C COMPUTER VISIO; Liu B., 2021, INT C LEARN REPR ICL; Liu XH, 2019, ADV NEUR IN, V32; Mirza M., 2014, ARXIV; Miyato Takeru, 2018, ARXIV180205637; Ntavelis Evangelos, 2020, EUR C COMP VIS ECCV; Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-7_19, DOI 10.48550/ARXIV.2007.15651]; Reed S, 2016, PR MACH LEARN RES, V48; Richardson Elad, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P2287, DOI 10.1109/CVPR46437.2021.00232; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sauer Axel, 2021, ADV NEURAL INFORM PR; Schonfeld E., 2021, INT C LEARNING REPRE; Schonfeld Edgar, 2020, P IEEE CVF C COMP VI, P8204, DOI [DOI 10.1109/CVPR42600.2020.00823, 10.1109/CVPR42600.2020.00823]; Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606; Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28; Tang, 2020, ARXIV; Tang H., 2020, C COMPUTER VISION PA; Tang H., 2020, ACM INT C MULTIMEDIA; Wang JQ, 2021, PROC CVPR IEEE, P9690, DOI 10.1109/CVPR46437.2021.00957; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang Y., 2021, ICCV; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Yazici Y., 2018, INT C LEARN REPR ICL; Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612; Zhang D., 2019, ADV NEURAL INFORM PR; Zhang Hang, 2022, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P2735, DOI 10.1109/CVPRW56347.2022.00309; Zhang H, 2021, PROC CVPR IEEE, P833, DOI 10.1109/CVPR46437.2021.00089; Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhu Z, 2020, PROC CVPR IEEE, P5466, DOI 10.1109/CVPR42600.2020.00551	68	0	0	4	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2022	130	12					2903	2923		10.1007/s11263-022-01673-x	http://dx.doi.org/10.1007/s11263-022-01673-x		SEP 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	5T8PI		hybrid			2022-12-18	WOS:000854437300001
J	Bailey, M; Hilton, A; Guillemaut, JY				Bailey, Matthew; Hilton, Adrian; Guillemaut, Jean-Yves			Finite Aperture Stereo	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D reconstruction; Depth from defocus; Multi-view stereo	MULTIVIEW STEREO; ENERGY MINIMIZATION; DEPTH; SHAPE; DEFOCUS	Multi-view stereo remains a popular choice when recovering 3D geometry, despite performance varying dramatically according to the scene content. Moreover, typical pinhole camera assumptions fail in the presence of shallow depth of field inherent to macro-scale scenes; limiting application to larger scenes with diffuse reflectance. However, the presence of defocus blur can itself be considered a useful reconstruction cue, particularly in the presence of view-dependent materials. With this in mind, we explore the complimentary nature of stereo and defocus cues in the context of multi-view 3D reconstruction; and propose a complete pipeline for scene modelling from a finite aperature camera that encompasses image formation, camera calibration and reconstruction stages. As part of our evaluation, an ablation study reveals how each cue contributes to the higher performance observed over a range of complex materials and geometries. Though of lesser concern with large apertures, the effects of image noise are also considered. By introducing pre-trained deep feature extraction into our cost function, we show a step improvement over per-pixel comparisons; as well as verify the cross-domain applicability of networks using largely in-focus training data applied to defocused images. Finally, we compare to a number of modern multi-view stereo methods, and demonstrate how the use of both cues leads to a significant increase in performance across several synthetic and real datasets.	[Bailey, Matthew; Hilton, Adrian; Guillemaut, Jean-Yves] Univ Surrey, Ctr Vis Speech & Signal Proc, Stag Hill, Guildford GU2 7XH, Surrey, England	University of Surrey	Bailey, M (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Stag Hill, Guildford GU2 7XH, Surrey, England.	m.j.bailey@surrey.ac.uk; a.hilton@surrey.ac.uk; j.guillemaut@surrey.ac.uk			EPSRC [EP/N 509772/1, EP/P022529/1]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This research was supported by the EPSRC (Grants EP/N 509772/1, EP/P022529/1)	Acharyya A, 2016, IEEE IMAGE PROC, P3444, DOI 10.1109/ICIP.2016.7532999; Anwar S, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-020-01162-6; Bailey M, 2021, IEEE INT CONF COMP V, P2474, DOI 10.1109/ICCVW54120.2021.00280; Bailey M, 2020, INT CONF 3D VISION, P1206, DOI 10.1109/3DV50981.2020.00131; Ben-Ari R, 2014, IEEE T PATTERN ANAL, V36, P1041, DOI 10.1109/TPAMI.2014.14; Bhavsar AV, 2012, INT J COMPUT VISION, V97, P167, DOI 10.1007/s11263-011-0476-5; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Bradley D., 2008, P IEEE C COMP VIS PA, P1; Carvalho M, 2019, LECT NOTES COMPUT SC, V11129, P307, DOI 10.1007/978-3-030-11009-3_18; Chakrabarti A, 2012, LECT NOTES COMPUT SC, V7576, P648, DOI 10.1007/978-3-642-33715-4_47; Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI [10.1109/ICCV.2015.104, 10.1109/ICCV.2015.312]; Chen RTQ., 2019, ARXIV; Chen Z., 2017, ARXIV; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Delaunoy A, 2014, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2014.193; Emerson D.R., 2019, 2019 IEEE APPL IM PA, P1, DOI [10.1109/AIPR47015.2019.9174568, DOI 10.1109/AIPR47015.2019.9174568]; Favaro P, 2005, IEEE T PATTERN ANAL, V27, P406, DOI 10.1109/TPAMI.2005.43; Favaro P., 2007, P IEEE INT C COMP VI, P1; Favaro P, 2008, IEEE T PATTERN ANAL, V30, P518, DOI 10.1109/TPAMI.2007.1175; Favaro P, 2010, PROC CVPR IEEE, P1133, DOI 10.1109/CVPR.2010.5540089; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Gheta I., 2007, GI JAHRESTAGUNG, P26; Gu X., 2019, CASCADE COST VOLUME, P1912; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hasinoff S, 2009, INT J COMPUT VISION, V81, P82, DOI 10.1007/s11263-008-0164-2; Hornung A., 2006, COMP VIS PATT REC 20, V1, P503, DOI 10.1109/CVPR.2006.135; Huang P., 2018, ARXIV; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Kar Abhishek, 2017, LEARNING MULTIVIEW S, P2; Kashiwagi M, 2019, IEEE I CONF COMP VIS, P4069, DOI 10.1109/ICCV.2019.00417; Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237; Kingslake R, 1992, OPTICS PHOTOGRAPHY; Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599; Kuhn A, 2020, INT CONF 3D VISION, P404, DOI 10.1109/3DV50981.2020.00050; Li C, 2016, IEEE T IMAGE PROCESS, V25, P589, DOI 10.1109/TIP.2015.2507403; Li F, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3500802; Li G, 2010, IEEE T PATTERN ANAL, V32, P72, DOI 10.1109/TPAMI.2008.270; Li ZX, 2016, IEEE T IMAGE PROCESS, V25, P864, DOI 10.1109/TIP.2015.2507400; Lin HT, 2015, IEEE I CONF COMP VIS, P3451, DOI 10.1109/ICCV.2015.394; Liu YB, 2010, IEEE T VIS COMPUT GR, V16, P407, DOI 10.1109/TVCG.2009.88; Logothetis F, 2019, IEEE I CONF COMP VIS, P1052, DOI 10.1109/ICCV.2019.00114; Luo KY, 2019, IEEE I CONF COMP VIS, P10451, DOI 10.1109/ICCV.2019.01055; Mannan F, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P326, DOI 10.1109/3DV.2015.44; Martinello M, 2015, IEEE INT CONF COMPUT; Mildenhall Ben, 2020, ECCV, P405, DOI [DOI 10.1007/978-3-030-58452-8_24, DOI 10.1007/978-3-030-58452-824]; Moeller M, 2015, IEEE T IMAGE PROCESS, V24, P5369, DOI 10.1109/TIP.2015.2479469; Namboodiri VP, 2008, IEEE IMAGE PROC, P1520, DOI 10.1109/ICIP.2008.4712056; NAYAR SK, 1994, IEEE T PATTERN ANAL, V16, P824, DOI 10.1109/34.308479; Olsson C, 2013, PROC CVPR IEEE, P1730, DOI 10.1109/CVPR.2013.226; Paramonov V, 2016, IEEE COMPUT SOC CONF, P910, DOI 10.1109/CVPRW.2016.118; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; Persch N, 2017, IMAGE VISION COMPUT, V57, P114, DOI 10.1016/j.imavis.2016.08.011; Rajagopalan AN, 2004, IEEE T PATTERN ANAL, V26, P1521, DOI 10.1109/TPAMI.2004.102; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rowlands A, 2017, IOP EXPAND PHYS, DOI 10.1088/978-0-7503-1242-4ch1; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Song G, 2018, IEEE IMAGE PROC, P1563, DOI 10.1109/ICIP.2018.8451201; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Takeda Y, 2013, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2013.34; Tang HX, 2017, PROC CVPR IEEE, P4773, DOI 10.1109/CVPR.2017.507; Tao MW, 2017, IEEE T PATTERN ANAL, V39, P546, DOI 10.1109/TPAMI.2016.2554121; Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89; Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Vogiatzis G, 2007, IEEE T PATTERN ANAL, V29, P2241, DOI 10.1109/TPAMI.2007.70712; Wang TC, 2016, PROC CVPR IEEE, P3717, DOI 10.1109/CVPR.2016.404; Watanabe M, 1998, INT J COMPUT VISION, V27, P203, DOI 10.1023/A:1007905828438; Wu CL, 2011, PROC CVPR IEEE, P969, DOI 10.1109/CVPR.2011.5995388; Xing Lin, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P95, DOI 10.1007/978-3-642-37447-0_8; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; ZAGORUYKO S, 2015, 2015 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2015.7299064; Zhang J., 2020, ARXIV; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zhu ZK, 2015, ISPRS J PHOTOGRAMM, V109, P47, DOI 10.1016/j.isprsjprs.2015.08.008	75	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2858	2884		10.1007/s11263-022-01658-w	http://dx.doi.org/10.1007/s11263-022-01658-w		SEP 2022	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ		hybrid			2022-12-18	WOS:000853289700001
J	Zhang, T; Fu, Y; Zhang, J				Zhang, Tao; Fu, Ying; Zhang, Jun			Guided Hyperspectral Image Denoising with Realistic Data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hyperspectral image denoising; Paired real dataset; Realistic synthetic data; Guided nonlocal denoising network	RESTORATION; RESOLUTION; SYSTEM; FILTER; MODEL	The hyperspectral image (HSI) denoising has been widely utilized to improve HSI qualities. Recently, learning-based HSI denoising methods have shown their effectiveness, but most of them are based on synthetic dataset and lack the generalization capability on real testing HSI. Moreover, there is still no public paired real HSI denoising dataset to learn HSI denoising network and quantitatively evaluate HSI methods. In this paper, we mainly focus on how to produce realistic dataset for learning and evaluating HSI denoising network. On the one hand, we collect a paired real HSI denoising dataset, which consists of short-exposure noisy HSIs and the corresponding long-exposure clean HSIs. On the other hand, we propose an accurate HSI noise model which matches the distribution of real data well and can be employed to synthesize realistic dataset. On the basis of the noise model, we present an approach to calibrate the noise parameters of the given hyperspectral camera. Besides, on the basis of observation of high signal-to-noise ratio of mean image of all spectral bands, we propose a guided HSI denoising network with guided dynamic nonlocal attention, which calculates dynamic nonlocal correlation on the guidance information, i.e., mean image of spectral bands, and adaptively aggregates spatial nonlocal features for all spectral bands. The extensive experimental results show that a network learned with only synthetic data generated by our noise model performs as well as it is learned with paired real data, and our guided HSI denoising network outperforms state-of-the-art methods under both quantitative metrics and visual quality.	[Zhang, Tao; Fu, Ying] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China; [Fu, Ying; Zhang, Jun] Beijing Inst Technol, Adv Res Inst Multidisciplinary Sci, Beijing 100081, Peoples R China	Beijing Institute of Technology; Beijing Institute of Technology	Fu, Y (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.; Fu, Y (corresponding author), Beijing Inst Technol, Adv Res Inst Multidisciplinary Sci, Beijing 100081, Peoples R China.	fuying@bit.edu.cn			National Natural Science Foundation of China [62171038, 61827901, 62088101]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China under Grants No. 62171038, No. 61827901 and No. 62088101.	Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182; Acito N, 2011, IEEE T GEOSCI REMOTE, V49, P2957, DOI 10.1109/TGRS.2011.2110657; [Anonymous], 2000, P 3 C FUS EARTH DAT, P99; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.187; Arad B, 2016, LECT NOTES COMPUT SC, V9911, P19, DOI 10.1007/978-3-319-46478-7_2; BASEDOW RW, 1995, P SOC PHOTO-OPT INS, V2480, P258, DOI 10.1117/12.210881; Bjorgan A, 2015, PROC SPIE, V9537, DOI 10.1117/12.2184155; Borengasser M., 2007, HYPERSPECTRAL REMOTE; Cai Y., 2021, ARXIV; Cao XY, 2018, IEEE T IMAGE PROCESS, V27, P2354, DOI 10.1109/TIP.2018.2799324; CHAKRABARTI A, 2011, PROC CVPR IEEE, P193, DOI DOI 10.1109/CVPR.2011.5995660; Chang Y, 2020, IEEE T CYBERNETICS, V50, P4558, DOI 10.1109/TCYB.2020.2983102; Chang Y, 2019, IEEE T GEOSCI REMOTE, V57, P667, DOI 10.1109/TGRS.2018.2859203; Chang Y, 2017, PROC CVPR IEEE, P5901, DOI 10.1109/CVPR.2017.625; CHARBONNIER P, 1994, IEEE IMAGE PROC, P168; Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328; Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347; Chen C, 2014, IEEE J-STARS, V7, P1047, DOI 10.1109/JSTARS.2013.2295610; Chen GY, 2011, IEEE T GEOSCI REMOTE, V49, P973, DOI 10.1109/TGRS.2010.2075937; Chen Y, 2018, IEEE T CYBERNETICS, V48, P1054, DOI 10.1109/TCYB.2017.2677944; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dong WS, 2019, IEEE T COMPUT IMAG, V5, P635, DOI 10.1109/TCI.2019.2911881; Dong WS, 2015, IEEE I CONF COMP VIS, P442, DOI 10.1109/ICCV.2015.58; Fu Y, 2019, PROC CVPR IEEE, P11653, DOI 10.1109/CVPR.2019.01193; Fu Y, 2018, IEEE T IMAGE PROCESS, V27, P5539, DOI 10.1109/TIP.2018.2855412; Fu Y, 2017, INT J COMPUT VISION, V122, P228, DOI 10.1007/s11263-016-0921-6; Gu SH, 2019, IEEE I CONF COMP VIS, P2511, DOI 10.1109/ICCV.2019.00260; Guo S, 2021, IEEE T IMAGE PROCESS, V30, P6930, DOI 10.1109/TIP.2021.3100312; He CX, 2021, SIGNAL PROCESS, V184, DOI 10.1016/j.sigpro.2021.108060; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; He W, 2019, PROC CVPR IEEE, P6861, DOI 10.1109/CVPR.2019.00703; He W, 2016, IEEE T GEOSCI REMOTE, V54, P176, DOI 10.1109/TGRS.2015.2452812; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; Holst GC, 1998, CCD ARRAYS CAMERAS D; Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22; Jiang HY, 2019, IEEE I CONF COMP VIS, P7323, DOI 10.1109/ICCV.2019.00742; Kawakami R, 2013, INT J COMPUT VISION, V105, P187, DOI 10.1007/s11263-013-0632-1; Kingma D.P, P 3 INT C LEARNING R; KRUSE FA, 1993, AIP CONF PROC, P192, DOI 10.1016/0034-4257(93)90013-N; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Kwon H, 2007, INT J COMPUT VISION, V71, P127, DOI 10.1007/s11263-006-6689-3; Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10; Lin BH, 2020, IEEE T IMAGE PROCESS, V29, P565, DOI 10.1109/TIP.2019.2928627; Liu L, 2020, PROC CVPR IEEE, P2237, DOI 10.1109/CVPR42600.2020.00231; Liu XF, 2012, IEEE T GEOSCI REMOTE, V50, P3717, DOI 10.1109/TGRS.2012.2187063; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu GL, 2014, J BIOMED OPT, V19, DOI 10.1117/1.JBO.19.1.010901; Ma CG, 2014, INT J COMPUT VISION, V110, P141, DOI 10.1007/s11263-013-0690-4; Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725; Monno Y, 2015, IEEE IMAGE PROC, P3861, DOI 10.1109/ICIP.2015.7351528; Morgan EC, 2011, ENERG CONVERS MANAGE, V52, P15, DOI 10.1016/j.enconman.2010.06.015; Nair V, 2010, P 27 INT C MACHINE L, P807; Ojha L, 2015, NAT GEOSCI, V8, P829, DOI [10.1038/ngeo2546, 10.1038/NGEO2546]; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Peng Y, 2014, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2014.377; Plotz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294; Porter WM., 1987, P SOC PHOTO-OPT INS, P22, DOI DOI 10.1117/12.942280; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schott J.R, 2007, REMOTE SENSING IMAGE; Shi Q, 2021, IEEE T GEOSCI REMOTE, V59, P10348, DOI 10.1109/TGRS.2020.3045273; Wang LZ, 2021, INT J COMPUT VISION, V129, P2907, DOI 10.1007/s11263-021-01481-9; Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070; Wang Y, 2018, IEEE J-STARS, V11, P1227, DOI 10.1109/JSTARS.2017.2779539; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wei KX, 2021, IEEE T NEUR NET LEAR, V32, P363, DOI 10.1109/TNNLS.2020.2978756; Xie Y, 2016, IEEE T GEOSCI REMOTE, V54, P4642, DOI 10.1109/TGRS.2016.2547879; Xiong F., 2021, IEEE T GEOSCIENCE RE; Xu L, 2015, PR MACH LEARN RES, V37, P1669; Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811; Yu-Chun Miao, 2022, IEEE Transactions on Geoscience and Remote Sensing, V60, DOI 10.1109/TGRS.2021.3106380; Yuan QQ, 2019, IEEE T GEOSCI REMOTE, V57, P1205, DOI 10.1109/TGRS.2018.2865197; Yuan QQ, 2012, IEEE T GEOSCI REMOTE, V50, P3660, DOI 10.1109/TGRS.2012.2185054; Zhang HY, 2014, IEEE T GEOSCI REMOTE, V52, P4729, DOI 10.1109/TGRS.2013.2284280; Zhang L, 2018, INT J COMPUT VISION, V126, P797, DOI 10.1007/s11263-018-1080-8; Zhang T., 2021, PROC IEEECVF INT C C, P2248; Zhang YZ, 2020, IEEE INT SYM BROADB, DOI 10.1109/BMSB49480.2020.9379596; Zhao B., 2022, IEEE T GEOSCI ELECT, V60, P1; Zheng HT, 2018, LECT NOTES COMPUT SC, V11210, P87, DOI 10.1007/978-3-030-01231-1_6; Zhou YM, 2021, PROC CVPR IEEE, P14837, DOI 10.1109/CVPR46437.2021.01460	79	0	0	8	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2885	2901		10.1007/s11263-022-01660-2	http://dx.doi.org/10.1007/s11263-022-01660-2		SEP 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ					2022-12-18	WOS:000853289700002
J	Du, DP; Chen, JW; Li, YX; Ma, K; Wu, GS; Zheng, YF; Wang, LM				Du, Dapeng; Chen, Jiawei; Li, Yuexiang; Ma, Kai; Wu, Gangshan; Zheng, Yefeng; Wang, Limin			Cross-Domain Gated Learning for Domain Generalization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Domain generalization; Representation learning; Dropout; Information bottleneck		Domain generalization aims to improve the generalization capacity of a model by leveraging useful information from the multi-domain data. However, learning an effective feature representation from such multi-domain data is challenging, due to the domain shift problem. In this paper, we propose an information gating strategy, termed cross-domain gating (CDG), to address this problem. Specifically, we try to distill the domain-invariant feature by adaptively muting the domain-related activations in the feature maps. This feature distillation process prevents the network from overfitting to the domain-related detailed information, and thereby improves the generalization ability of learned feature representation. Extensive experiments are conducted on three public datasets. The experimental results show that the proposed CDG training strategy can excellently enforce the network to exploit the intrinsic features of objects from the multi-domain data, and achieve a new state-of-the-art domain generalization performance on these benchmarks.	[Du, Dapeng; Wu, Gangshan; Wang, Limin] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China; [Chen, Jiawei; Li, Yuexiang; Ma, Kai; Zheng, Yefeng] Tencent Jarvis Lab, Shenzhen, Peoples R China	Nanjing University	Wang, LM (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.; Zheng, YF (corresponding author), Tencent Jarvis Lab, Shenzhen, Peoples R China.	dudp.nju@gmail.com; jiaweichen@tencent.com; vicyxli@tencent.com; kylekma@tencent.com; gswu@nju.edu.cn; yefengzheng@tencent.com; lmwang@nju.edu.cn	LI, Yue/GRS-8071-2022	Wang, Limin/0000-0002-3674-7718	National Science Foundation of China [62076119, 61921006]; National Key R & D Program of China [2018YFC2000702, 2020AAA0104100]; Fundamental Research Funds for the Central Universities [020214380091]; Collaborative Innovation Center of Novel Software Technology and Industrialization; Key-Area Research and Development Program of Guangdong Province [2018B010111001]	National Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R & D Program of China; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Collaborative Innovation Center of Novel Software Technology and Industrialization; Key-Area Research and Development Program of Guangdong Province	This work is supported by the National Science Foundation of China (No. 62076119, No. 61921006), National Key R & D Program of China (2018YFC2000702), the Scientific and Technical Innovation 2030-"New Generation Artificial Intelligence" Project (No. 2020AAA0104100), the Fundamental Research Funds for the Central Universities (No. 020214380091), Collaborative Innovation Center of Novel Software Technology and Industrialization, the Key-Area Research and Development Program of Guangdong Province (No. 2018B010111001). Part of this work was done when Dapeng Du was an intern at Tencent Jarvis Lab.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Balaji Y, 2018, ADV NEUR IN, V31; Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233; Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Dou Q, 2019, ADV NEUR IN, V32; Du DP, 2019, PROC CVPR IEEE, P11828, DOI 10.1109/CVPR.2019.01211; Du Y., 2020, EUR C COMP VIS; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fang C, 2013, IEEE I CONF COMP VIS, P1657, DOI 10.1109/ICCV.2013.208; Federici M., 2020, ICLR; Ganin Yaroslav, 2015, ICML; Ghiasi G, 2018, ADV NEUR IN, V31; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Girshick R., 2015, ICCV; Gong BQ, 2014, INT J COMPUT VISION, V109, P3, DOI 10.1007/s11263-014-0718-4; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang Z., 2020, EUROPEAN C COMPUTER; Kaiming He, 2020, IEEE Transactions on Pattern Analysis and Machine Intelligence, V42, P386, DOI 10.1109/TPAMI.2018.2844175; Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33; Kolchinsky A., 2019, ARXIV180807593; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566; Li H, 2020, LEUKEMIA, V34, P1503, DOI [10.1038/s41375-020-0848-3, 10.1007/s11263-020-01364-5]; Li YJ, 2019, PR MACH LEARN RES, V97; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long MS, 2015, PR MACH LEARN RES, V37, P97; McIlraith, 2017, AAAI C ARTIFICIAL IN; Moreno-Torres JG, 2012, PATTERN RECOGN, V45, P521, DOI 10.1016/j.patcog.2011.06.019; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Muandet Krikamol, 2013, ICML; Omeiza Daniel, 2019, ARXIV190801224; Park S, 2017, LECT NOTES COMPUT SC, V10112, P189, DOI 10.1007/978-3-319-54184-6_12; Park S, 2018, AAAI CONF ARTIF INTE, P3917; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Peng Xingchao, 2019, ARXIV191102054; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shankar Shiv, 2018, P INT C LEARN REPR I; Shujun Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P159, DOI 10.1007/978-3-030-58545-7_10; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Tishby Naftali, 2000, PHYSICS0004057 ARXIV; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Wang H., 2019, ICLR; Wang HF, 2020, IEEE COMPUT SOC CONF, P111, DOI 10.1109/CVPRW50498.2020.00020; Wang HH, 2019, ADV NEUR IN, V32; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2017, IEEE T IMAGE PROCESS, V26, P2055, DOI 10.1109/TIP.2017.2675339; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yue XY, 2019, IEEE I CONF COMP VIS, P2100, DOI 10.1109/ICCV.2019.00219; Zakharov S, 2019, IEEE I CONF COMP VIS, P532, DOI 10.1109/ICCV.2019.00062; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou Bolei, 2016, PLACES IMAGE DATABAS	68	0	0	11	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2842	2857		10.1007/s11263-022-01674-w	http://dx.doi.org/10.1007/s11263-022-01674-w		SEP 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ					2022-12-18	WOS:000850390600002
J	Sun, P; Zhang, WH; Li, SY; Guo, YL; Song, CL; Li, X				Sun, Peng; Zhang, Wenhu; Li, Songyuan; Guo, Yilin; Song, Congli; Li, Xi			Learnable Depth-Sensitive Attention for Deep RGB-D Saliency Detection with Multi-modal Fusion Architecture Search	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						RGB-D salient object detection; Depth-sensitive RGB feature; Multi-modal feature fusion	OBJECT DETECTION; NETWORK	RGB-D salient object detection (SOD) is usually formulated as a problem of classification or regression over two modalities, i.e., RGB and depth. Hence, effective RGB-D feature modeling and multi-modal feature fusion both play a vital role in RGB-D SOD. In this paper, we propose a depth-sensitive RGB feature modeling scheme using the depth-wise geometric prior of salient objects. In principle, the feature modeling scheme is carried out in a Depth-Sensitive Attention Module (DSAM), which leads to the RGB feature enhancement as well as the background distraction reduction by capturing the depth geometry prior. Furthermore, we extend and enhance the original DSAM to DSAMv2 by proposing a novel Depth Attention Generation Module (DAGM) to generate learnable depth attentionmaps formore robust depth-sensitiveRGBfeature extraction. Moreover, to perform effectivemulti-modal feature fusion, we further present an automatic neural architecture search approach for RGB-D SOD, which does well in finding out a feasible architecture from our specially designed multi-modal multi-scale search space. Extensive experiments on nine standard benchmarks have demonstrated the effectiveness of the proposed approach against the state-of-the-art. We name the enhanced learnable Depth-Sensitive Attention and Automatic multi-modal Fusion framework DSA2Fv2.	[Sun, Peng; Zhang, Wenhu; Li, Songyuan; Li, Xi] Zhejiang Univ, Hangzhou, Peoples R China; [Guo, Yilin; Song, Congli] Kuaishou Technol, Beijing, Peoples R China; [Li, Xi] Zhejiang Univ, Coll Comp Sci, Hangzhou, Peoples R China; [Li, Xi] Zhejiang Univ, Shanghai Inst Adv Study, Hangzhou, Peoples R China; [Li, Xi] Shanghai AI Lab, Shanghai, Peoples R China	Zhejiang University; Zhejiang University; Zhejiang University	Li, X (corresponding author), Zhejiang Univ, Hangzhou, Peoples R China.; Li, X (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou, Peoples R China.; Li, X (corresponding author), Zhejiang Univ, Shanghai Inst Adv Study, Hangzhou, Peoples R China.; Li, X (corresponding author), Shanghai AI Lab, Shanghai, Peoples R China.	sunpeng1996@zju.edu.cn; wenhuzhang@zju.edu.cn; leizungjyun@zju.edu.cn; xilizju@zju.edu.cn			National Key Research and Development Program of China [2020AAA0107400]; Zhejiang Provincial Natural Science Foundation of China [LR19F020004]; National Natural Science Foundation of China [U20A20222]	National Key Research and Development Program of China; Zhejiang Provincial Natural Science Foundation of China(Natural Science Foundation of Zhejiang Province); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported in part by National Key Research and Development Program of China under Grant 2020AAA0107400, Zhejiang Provincial Natural Science Foundation of China under Grant LR19F020004, National Natural Science Foundation of China under Grant U20A20222.	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Alok Aggarwal, 2019, Arxiv, DOI arXiv:1802.01548; Bender G, 2018, PR MACH LEARN RES, V80; Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833; Brock A., 2018, ARXIV; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Chang Huang, 2019, Arxiv, DOI arXiv:1808.00193; Chen H, 2020, IEEE T IMAGE PROCESS, V29, P8407, DOI 10.1109/TIP.2020.3014734; Chen H, 2020, IEEE T CYBERNETICS, V50, P4808, DOI 10.1109/TCYB.2019.2934986; Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322; Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104; Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007; Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063; Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289; Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI 10.1007/s11263-021-01490-8; Cheng Y., 2014, P INT C INTERNET MUL, P23; Cheng-Feng Sun, 2020, Learning and Collaboration Technologies. Human and Technology Ecosystems. 7th International Conference, LCT 2020. Held as Part of the 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12206), P520, DOI 10.1007/978-3-030-50506-6_35; Ciptadi A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.112; Colson B, 2007, ANN OPER RES, V153, P235, DOI 10.1007/s10479-007-0176-2; Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98; Fan D. P., 2020, EUROPEAN C COMPUTER; Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406; Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875; Fan Deng-Ping, 2018, IJCAI; Fan XX, 2014, INT CONF DIGIT SIG, P454, DOI 10.1109/ICDSP.2014.6900706; Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257; Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689; Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39; Guo J., 2016, PAPER PRESENTED IEEE, P1; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hong S, 2015, PR MACH LEARN RES, V37, P597; Ji W., 2020, EUROPEAN C COMPUTER; Jianqiang Ren, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P25, DOI 10.1109/CVPRW.2015.7301391; Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167; Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222; Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8; Li C., 2020, EUROPEAN C COMPUTER; Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689; Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359; Lin PW, 2020, PROC CVPR IEEE, P4202, DOI 10.1109/CVPR42600.2020.00426; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu GH, 2014, PROCEEDINGS OF 2013 INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CLOUD COMPUTING COMPANION (ISCC-C), P728, DOI 10.1109/ISCC-C.2013.21; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012; Mahadevan V, 2009, PROC CVPR IEEE, P1007, DOI 10.1109/CVPRW.2009.5206573; Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23; Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377; Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7; Perez-Rua JM, 2019, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2019.00713; Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735; Poole B, 2017, INT C LEARN REPR; Chen Q, 2021, Arxiv, DOI arXiv:2101.10241; Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981; Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323; Simonyan K., 2014, 3 INT C LEARN REPR I; Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277; Sun G., 2018, P IEEE CVF C COMP VI, P7132, DOI DOI 10.1109/CVPR.2018.00745; Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146; Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961; Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39; Xu H, 2019, IEEE I CONF COMP VIS, P6648, DOI 10.1109/ICCV.2019.00675; Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15; Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977; Zhang J., 2021, P IEEE CVF INT C COM, P4338; Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353; Zhang N., 2020, LEARNING SELECTIVE M; Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z; Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042; Zhu CB, 2017, IEEE INT CONF COMP V, P1509, DOI 10.1109/ICCVW.2017.178; Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355; Zoph B., 2017, NEURAL ARCHITECTURE	86	0	0	11	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2822	2841		10.1007/s11263-022-01646-0	http://dx.doi.org/10.1007/s11263-022-01646-0		SEP 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ					2022-12-18	WOS:000850390600001
J	Michieli, U; Zanuttigh, P				Michieli, Umberto; Zanuttigh, Pietro			Edge-Aware Graph Matching Network for Part-Based Semantic Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Part parsing; Semantic segmentation; Graph matching; Edge localization; Coarse-to-fine learning		Semantic segmentation of parts of objects is a marginally explored and challenging task in which multiple instances of objects and multiple parts within those objects must be recognized in an image. We introduce a novel approach (GMENet) for this task combining object-level context conditioning, part-level spatial relationships, and shape contour information. The first target is achieved by introducing a class-conditioning module that enforces class-level semantics when learning the part-level ones. Thus, intermediate-level features carry object-level prior to the decoding stage. To tackle part-level ambiguity and spatial relationships among parts we exploit an adjacency graph-based module that aims at matching the spatial relationships between parts in the ground truth and predicted maps. Last, we introduce an additional module to further leverage edges localization. Besides testing our framework on the already used Pascal-Part-58 and Pascal-Person-Part benchmarks, we further introduce two novel benchmarks for large-scale part parsing, i.e., a more challenging version of Pascal-Part with 108 classes and the ADE20K-Part benchmark with 544 parts. GMENet achieves state-of-the-art results in all the considered tasks and furthermore allows to improve object-level segmentation accuracy.	[Michieli, Umberto; Zanuttigh, Pietro] Univ Padua, Dept Informat Engn, I-35131 Padua, Italy	University of Padua	Zanuttigh, P (corresponding author), Univ Padua, Dept Informat Engn, I-35131 Padua, Italy.	michieli@dei.unipd.it; zanuttigh@dei.unipd.it			Universita degli Studi di Padova within the CRUI-CARE Agreement	Universita degli Studi di Padova within the CRUI-CARE Agreement	Open access funding provided by Universita degli Studi di Padova within the CRUI-CARE Agreement.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Azizpour H, 2012, LECT NOTES COMPUT SC, V7572, P836, DOI 10.1007/978-3-642-33718-5_60; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Cao KD, 2019, ADV NEUR IN, V32; Cermelli Fabio, 2020, P IEEE CVF C COMP VI, P9233; Chang WL, 2019, PROC CVPR IEEE, P1900, DOI 10.1109/CVPR.2019.00200; Chen L. C., 2020, DEEPLAB OFFICIAL TEN; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Chu Xiangxiang, 2021, ARXIV210413840; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Csurka G, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.32; Das D, 2018, IEEE IMAGE PROC, P3758, DOI 10.1109/ICIP.2018.8451152; de Geus D, 2021, PROC CVPR IEEE, P5481, DOI 10.1109/CVPR46437.2021.00544; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dhar P, 2019, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR.2019.00528; Dong J, 2014, PROC CVPR IEEE, P843, DOI 10.1109/CVPR.2014.113; Dosovitskiy A., 2020, ARXIV201011929; Douillard A, 2021, PROC CVPR IEEE, P4039, DOI 10.1109/CVPR46437.2021.00403; Emmert-Streib F, 2016, INFORM SCIENCES, V346, P180, DOI 10.1016/j.ins.2016.01.074; Eslami S.M.A, 2012, ADV NEURAL INFORM PR, V25, P100; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fang H. S., 2018, P IEEE C COMP VIS PA; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gholami A., 2021, ARXIV; Gonzalez-Garcia A, 2018, INT J COMPUT VISION, V126, P476, DOI 10.1007/s11263-017-1048-0; Guo YM, 2018, INT J MULTIMED INF R, V7, P87, DOI 10.1007/s13735-017-0141-z; Haggag H, 2016, IEEE SYS MAN CYBERN, P855, DOI 10.1109/SMC.2016.7844347; Han HY, 2021, IEEE T INTELL TRANSP, V22, P1041, DOI 10.1109/TITS.2019.2962094; He H., 2021, ARXIV; He J., 2021, ARXIV; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang Zilong, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3007032; Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069; Jiang HZ, 2019, IEEE I CONF COMP VIS, P3194, DOI 10.1109/ICCV.2019.00329; Jin Y, 2021, PATTERN RECOGN LETT, V148, P29, DOI 10.1016/j.patrec.2021.04.024; Kang Bingyi, 2019, ARXIV191009217, DOI 10.48550/arXiv.1910.09217; Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194; Li J., 2017, ARXIV; Li P., 2020, IEEE T PATTERN ANAL; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Liang TL, 2021, NEUROCOMPUTING, V461, P370, DOI 10.1016/j.neucom.2021.07.045; Liang X., 2017, P IEEE C COMP VIS PA, P1010; Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063; Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8; Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360; Liu XL, 2019, ARTIF INTELL REV, V52, P1089, DOI 10.1007/s10462-018-9641-3; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264; Livi L, 2013, PATTERN ANAL APPL, V16, P253, DOI 10.1007/s10044-012-0284-8; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu W., 2014, P BRIT MACH VIS C BM; Maracani Andrea, 2021, P IEEE CVF INT C COM, P7026, DOI DOI 10.1109/ICCV48922.2021.00694; Mel M, 2020, TECHNOLOGIES, V8, DOI 10.3390/technologies8010001; Michieli Umberto, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P397, DOI 10.1007/978-3-030-58598-3_24; Michieli U., 2021, ARXIV; Michieli U, 2021, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR46437.2021.00117; Michieli U, 2021, COMPUT VIS IMAGE UND, V205, DOI 10.1016/j.cviu.2021.103167; Michieli U, 2019, IEEE INT CONF COMP V, P3205, DOI 10.1109/ICCVW.2019.00400; Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Rebuffi Sylvestre-Alvise, 2017, PROC CVPR IEEE, P8, DOI DOI 10.1109/CVPR.2017.587; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruan T, 2019, AAAI CONF ARTIF INTE, P4814; Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368; Song YF, 2017, IEEE I CONF COMP VIS, P580, DOI 10.1109/ICCV.2017.70; Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717; Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422; Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wan WT, 2019, IEEE I CONF COMP VIS, P3404, DOI 10.1109/ICCV.2019.00350; Wang HY, 2015, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2015.7298788; Wang P, 2015, IEEE I CONF COMP VIS, P1573, DOI 10.1109/ICCV.2015.184; Wang Y, 2012, J MACH LEARN RES, V13, P3075; Xia F., 2015, ARXIV; Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644; Xia FT, 2016, LECT NOTES COMPUT SC, V9909, P648, DOI 10.1007/978-3-319-46454-1_39; Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P435, DOI 10.1007/978-3-030-58520-4_26; Xie E., 2021, ADV NEURAL INF PROCE, V34; Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101; YANG Y, 2011, PROC CVPR IEEE, P1385, DOI [10.1109/CVPR.2011.5995741, DOI 10.1109/CVPR.2011.5995741]; Yin J., 2021, IEEE T MULTIMEDIA, P1; Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2; Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20; Yuan Y., 2018, ARXIV; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zhang W., 2022, P IEEE C COMPUTER VI; Zhang ZJ, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2718-7; Zhang ZJ, 2019, LECT NOTES COMPUT SC, V11764, P442, DOI 10.1007/978-3-030-32239-7_49; Zhao J, 2017, IEEE COMPUT SOC CONF, P1595, DOI 10.1109/CVPRW.2017.204; Zhao YF, 2019, IEEE I CONF COMP VIS, P9176, DOI 10.1109/ICCV.2019.00927; Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681; Zhu L, 2011, INT J COMPUT VISION, V93, P1, DOI 10.1007/s11263-010-0375-1	98	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2797	2821		10.1007/s11263-022-01671-z	http://dx.doi.org/10.1007/s11263-022-01671-z		SEP 2022	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ		hybrid			2022-12-18	WOS:000849446400001
J	Huang, YK; Fu, XY; Li, L; Zha, ZJ				Huang, Yukun; Fu, Xueyang; Li, Liang; Zha, Zheng-Jun			Learning Degradation-Invariant Representation for Robust Real-World Person Re-Identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person Re-ID; Representation learning; Vision in bad weather; Deep learning; Low-light image enhancement	ENHANCEMENT; NETWORK	Person re-identification (Re-ID) in real-world scenarios suffers from various degradations, e.g., low resolution, weak lighting, and bad weather. These degradations hinders identity feature learning and significantly degrades Re-ID performance. To address these issues, in this paper, we propose a degradation invariance learning framework for robust person Re-ID. Concretely, we first design a content-degradation feature disentanglement strategy to capture and isolate task-irrelevant features contained in the degraded image. Then, to avoid the catastrophic forgetting problem, we introduce a memory replay algorithm to further consolidate invariance knowledge learned from the previous pre-training to improve subsequent identity feature learning. In this way, our framework is able to continuously maintain degradation-invariant priors from one or more datasets to improve the robustness of identity features, achieving state-of-the-art Re-ID performance on several challenging real-world benchmarks with a unified model. Furthermore, the proposed framework can be extended to low-level image processing, e.g., low-light image enhancement, demonstrating the potential of our method as a general framework for the various vision tasks. Code and trained models will be available at: https://github.com/hyk1996/Degradati on-Invariant-Re-D-pytorch.	[Huang, Yukun; Fu, Xueyang; Zha, Zheng-Jun] Univ Sci & Technol China, Hefei, Peoples R China; [Li, Liang] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Chinese Academy of Sciences; Institute of Computing Technology, CAS	Fu, XY (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.	xyfu@ustc.edu.cn		Fu, Xueyang/0000-0001-8036-4071	National Key R&D Program of China [2020AAA0105702]; National Natural Science Foundation of China (NSFC) [U19B2038, 61901433]; University Synergy Innovation Program of Anhui Province [GXXT-2019-025]; Fundamental Research Funds for the Central Universities [WK2100000024]; USTC Research Funds of the Double First-Class Initiative [YD2100002003]	National Key R&D Program of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); University Synergy Innovation Program of Anhui Province; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); USTC Research Funds of the Double First-Class Initiative	This work is supported by the National Key R&D Program of China under Grant 2020AAA0105702, the National Natural Science Foundation of China (NSFC) under Grants U19B2038 and 61901433, the University Synergy Innovation Program of Anhui Province under Grants GXXT-2019-025, the Fundamental Research Funds for the Central Universities under Grant WK2100000024, and the USTC Research Funds of the Double First-Class Initiative under Grant YD2100002003.	Bak S, 2018, LECT NOTES COMPUT SC, V11217, P193, DOI 10.1007/978-3-030-01261-8_12; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Burgess C.P., 2018, ARXIV; Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen M., 2021, ARXIV; Chen YC, 2019, AAAI CONF ARTIF INTE, P8215; Chen ZY, 2021, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR46437.2021.00710; Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68; Cheng ZY, 2020, PROC CVPR IEEE, P2602, DOI 10.1109/CVPR42600.2020.00268; Choksi B., 2021, ADV NEURAL INFORM PR, V34, P14069; Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223; Ge Y., 2021, INT C LEARNING REPRE; Ge YX, 2018, ADV NEUR IN, V31; George D, 2017, SCIENCE, V358, DOI 10.1126/science.aag2612; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Gulrajani I., 2017, ARXIV; Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185; Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474; Henderson P, 2020, INT J COMPUT VISION, V128, P835, DOI 10.1007/s11263-019-01219-8; Hermans Alexander, 2017, ARXIV170307737; Higgins I, 2016, BETA VAE LEARNING BA; Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735; Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Huang YK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P215, DOI 10.1109/ICCV48922.2021.00028; Huang YK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P365, DOI 10.1145/3343031.3350994; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jiang K., 2021, ARXIV; Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462; Jiao JN, 2018, AAAI CONF ARTIF INTE, P6967; Jing XY, 2015, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2015.7298669; Kanwal S, 2021, COMPUT ELECTR ENG, V96, DOI 10.1016/j.compeleceng.2021.107542; Ke Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P193, DOI 10.1007/978-3-030-58574-7_12; Kim H, 2018, PR MACH LEARN RES, V80; Kim M, 2019, IEEE I CONF COMP VIS, P2979, DOI 10.1109/ICCV.2019.00307; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kingma D.P., 2015, INT C LEARNING REPRE; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Koschmieder H., 1924, BEITR PHYS ATMOS, V12, P33, DOI DOI 10.1007/978-3-663-04661-5_2; Kviatkovsky I, 2013, IEEE T PATTERN ANAL, V35, P1622, DOI 10.1109/TPAMI.2012.246; Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022; Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z; Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951; Li CY, 2021, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604; Li H., 2021, P 29 ACM INT C MULT, P3115, DOI DOI 10.1145/3474085.3475455; Li H, 2020, LEUKEMIA, V34, P1503, DOI [10.1038/s41375-020-0848-3, 10.1007/s11263-020-01364-5]; Li S, 2020, INT J COMPUT VISION, V128, P2936, DOI 10.1007/s11263-020-01349-4; Li W, 2020, INT J COMPUT VISION, V128, P1635, DOI 10.1007/s11263-019-01274-1; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li X., 2019, INT C LEARNING REPRE; Li X, 2015, IEEE I CONF COMP VIS, P3765, DOI 10.1109/ICCV.2015.429; Li Y, 2020, INT J COMPUT VISION, V128, P2166, DOI 10.1007/s11263-019-01267-0; Li YJ, 2019, IEEE I CONF COMP VIS, P8089, DOI 10.1109/ICCV.2019.00818; Liu J., 2018, P BRIT MECH VIS C, P1; Liu Jiang-Jiang, 2019, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2019.00737; Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741; Locatello F, 2019, PR MACH LEARN RES, V97; Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190; Ma F, 2019, MULTIMED TOOLS APPL, V78, P337, DOI 10.1007/s11042-018-6239-3; Ma JX, 2019, PR MACH LEARN RES, V97; Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Mao SN, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P883; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mathieu E, 2019, PR MACH LEARN RES, V97; Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063; Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726; Nie Q, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01354-7; Pan ZQ, 2021, AAAI CONF ARTIF INTE, V35, P9285; Pang J, 2021, PROC S VLSI CIRCUITS, P1; Peebles William, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P581, DOI 10.1007/978-3-030-58539-6_35; Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908; Sanchez Eduardo Hugo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P205, DOI 10.1007/978-3-030-58542-6_13; Shen X. W., 2020, ARXIV; Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158; Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267; Shen Yujun, 2020, P IEEE CVF C COMP VI; Sonderby CK, 2016, ADV NEUR IN, V29; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tan MX, 2019, PR MACH LEARN RES, V97; Tran L, 2019, INT J COMPUT VISION, V127, P824, DOI 10.1007/s11263-019-01155-7; Varior RR, 2016, IEEE T IMAGE PROCESS, V25, P3395, DOI 10.1109/TIP.2016.2531280; Wang MJ, 2019, INT J COMPUT VISION, V127, P743, DOI 10.1007/s11263-019-01163-7; Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701; Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309; Wang Y., 2021, ARXIV; Wang Z., 2016, IJCAI, P2669; Wang Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3891; Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279; Yang MY, 2021, PROC CVPR IEEE, P9588, DOI 10.1109/CVPR46437.2021.00947; Yukun Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14072, DOI 10.1109/CVPR42600.2020.01409; Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782; Zhang GQ, 2021, IEEE T IMAGE PROCESS, V30, P8913, DOI 10.1109/TIP.2021.3120054; Zhang SS, 2021, INT J COMPUT VISION, V129, P1875, DOI 10.1007/s11263-021-01461-z; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng WS, 2022, INT J COMPUT VISION, V130, P136, DOI 10.1007/s11263-021-01518-z; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541; Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XP, 2021, INT J COMPUT VISION, V129, P1580, DOI 10.1007/s11263-021-01440-4	113	0	0	8	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2770	2796		10.1007/s11263-022-01666-w	http://dx.doi.org/10.1007/s11263-022-01666-w		SEP 2022	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ					2022-12-18	WOS:000849289700001
J	Zhao, Y; Ren, DY; Chen, Y; Jia, W; Wang, RG; Liu, XP				Zhao, Yang; Ren, Diya; Chen, Yuan; Jia, Wei; Wang, Ronggang; Liu, Xiaoping			Cartoon Image Processing: A Survey	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cartoon image processing; Cartoon style transfer; Cartoon colorization; Cartoon generation; Generative adversarial network	CARICATURE; GENERATION; NETWORKS; SKETCH	With the rapid development of cartoon industry, various studies on two-dimensional (2D) cartoon have been proposed for different application scenarios, such as quality assessment, style transfer, colorization, detection, compression, generation and editing. However, there is still a lack of literature to summarize and introduce these 2D cartoon image processing (CIP) works comprehensively. The cartoon images are usually composed of clear lines, smooth color patches and flat backgrounds, which are quite different from natural images. Therefore, based on the characteristics of cartoons, many specific CIP strategies are proposed. Especially with the development of deep learning technology, recent CIP methods have achieved better results than direct application of natural image processing algorithms. Thus, this paper reviews the commonalities and differences of 2D CIP methods according to different scenarios and applications, and focuses on recent deep-learning-based algorithms specifically. In addition, this paper also collects related CIP datasets, conducts experiments for some typical tasks, and discusses the future work.	[Zhao, Yang; Ren, Diya; Jia, Wei; Liu, Xiaoping] Hefei Univ Technol, Sch Comp & Informat, Danxia Rd, Hefei 230009, Peoples R China; [Zhao, Yang; Wang, Ronggang] Peng Cheng Lab, Dashi Rd, Shenzhen 518000, Peoples R China; [Chen, Yuan] Anhui Univ, Sch Internet, Feixi Rd, Hefei 230039, Peoples R China; [Wang, Ronggang] Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, 2199 Lishui Rd, Shenzhen 518055, Peoples R China	Hefei University of Technology; Peng Cheng Laboratory; Anhui University; Peking University; University Town of Shenzhen	Jia, W (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Danxia Rd, Hefei 230009, Peoples R China.	yzhao@hfut.edu.cn; 2019170962@mail.hfut.edu.cn; ychen@mail.hfut.edu.cn; jiawei@hfut.edu.cn; rgwang@pkusz.edu.cn; liu@hfut.edu.cn			Key R &D and Transformation Program of Qinghai Province [2021-GX-111]; Fundamental Research Funds for the Central Universities [JZ2022HGPA0309]; National Natural Science Foundation of China [61972129, 62072013, 62076086]; Shenzhen Cultivation of Excellent Scientific and Technological Innovation [RCJC20200714114435057]	Key R &D and Transformation Program of Qinghai Province; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Cultivation of Excellent Scientific and Technological Innovation	This work is supported by the Key R &D and Transformation Program of Qinghai Province No. 2021-GX-111, the Fundamental Research Funds for the Central Universities No. JZ2022HGPA0309, the National Natural Science Foundation of China (Nos. 61972129, 62072013, 62076086) and Shenzhen Cultivation of Excellent Scientific and Technological Innovation Talents RCJC20200714114435057.	Aizawa M, 2019, INT J NETW DISTRIB C, V7, P113, DOI 10.2991/ijndc.k.190711.001; Akita K., 2020, EUROGRAPHICS 2020 SH, V2; Akita K, 2019, SIGGRAPH '19 - ACM SIGGRAPH 2019 POSTERS, DOI 10.1145/3306214.3338585; Andersson F., 2020, PREPRINT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aneja D., 2019, PREPRINT; Anime4k, 2019, BLOC97; [Anonymous], 2019, YOUKU VIDEO SUPER RE; [Anonymous], 2022, REAL CUG; [Anonymous], 2021, NAV WEBT FAC; [Anonymous], 2021, MALN FAC; [Anonymous], 2021, DANBOORU2020 LARGE S; [Anonymous], 2017, SKETCHK; [Anonymous], 2018, WAIFU2X; Augereau O., 2016, P 1 INT WORKSH COMIC, P1, DOI DOI 10.1145/3011549.3011553; Augereau O, 2018, J IMAGING, V4, DOI 10.3390/jimaging4070087; Bahng H, 2018, LECT NOTES COMPUT SC, V11216, P443, DOI 10.1007/978-3-030-01258-8_27; Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311; Bonneel N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818107; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Brennan SE, 2007, LEONARDO, V40, P392, DOI 10.1162/leon.2007.40.4.392; Bryandlee, 2021, US; Cao K., 2018, PREPRINT; Chaudhari S., 2019, PREPRINT; Chen H., 2002, P 10 ACM INT C MULT, P171; Chen H., 2021, IEEE T MULTIMEDIA, DOI 10.1109/TMM.2021.3121875; Chen JB, 2018, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR.2018.00909; Chen Jie, 2019, INT S INTELLIGENCE C, P242; Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168; CHEN Y, 2021, IEEE T NEUR NET LEAR; Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986; Chen Y, 2020, IEEE T CIRC SYST VID, V30, P3282, DOI 10.1109/TCSVT.2019.2931589; Chen YG, 2020, LECT NOTES COMPUT SC, V11961, P176, DOI 10.1007/978-3-030-37731-1_15; Chen ZH, 2019, GEOFLUIDS, DOI 10.1155/2019/4657645; Cheng MM, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682628; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Chu WT, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P417, DOI 10.1145/3078971.3079031; Chu WQ, 2019, IEEE IMAGE PROC, P3282, DOI 10.1109/ICIP.2019.8803517; Ci YZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1536, DOI 10.1145/3240508.3240661; COHN N, 2017, MULTIMODAL COMMUNICA, V0006; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dumoulin V., 2016, PREPRINT; Dunst A, 2017, PROC INT CONF DOC, P15, DOI 10.1109/ICDAR.2017.286; Edwards P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925984; Favreau JD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925946; Fiser J., 2015, P WORKSH SKETCH BAS, P49; Frans K., 2017, PREPRINT; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Fujimoto A, 2016, PROCEEDINGS OF THE 1ST INTERNATIONAL WORKSHOP ON COMICS ANALYSIS, PROCESSING AND UNDERSTANDING (MANPU 2016), DOI 10.1145/3011549.3011551; Furukawa S., 2017, INT C ADV COMP ENT, P153; Furusawa C, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149430; Gatys L.A., 2015, PREPRINT; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; github, 2015, CHAIN DCGAN; Gong Julia, 2020, P IEEE CVF WINT C AP, P360; Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Grimm, 2012, JUST DRAW IT 3D SKET; GU Z, 2021, IEEE T MULTIMEDIA; Guerin C, 2013, PROC INT CONF DOC, P1145, DOI 10.1109/ICDAR.2013.232; Gupta T, 2018, LECT NOTES COMPUT SC, V11212, P610, DOI 10.1007/978-3-030-01237-3_37; Han XG, 2020, IEEE T VIS COMPUT GR, V26, P2349, DOI 10.1109/TVCG.2018.2886007; Hanser E., 2009, IR C ART INT COGN SC, P144; Hati Y, 2019, 16TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION (CVMP 2019), DOI 10.1145/3359998.3369401; Hensel M, 2017, ADV NEUR IN, V30; Hensman P, 2017, PROC INT CONF DOC, P72, DOI 10.1109/ICDAR.2017.295; Hicsonmez S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103886; Hoffman J, 2018, PR MACH LEARN RES, V80; Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745; Huang J., 2020, PREPRINT; Huang J., 2018, ASIAN C MACHINE LEAR, P566; Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536; Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Hung-Yu Tseng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P158, DOI 10.1007/978-3-030-58523-5_10; Huo J., 2017, PREPRINT; Ikuta H., 2016, SIGGRAPH ASIA 2016 T, P1; Illustrationgan, 2016, ILLUSTRATIONGAN; Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Ito K., 2015, SEPARATION MANGA LIN; Iyyer M, 2017, PROC CVPR IEEE, P6478, DOI 10.1109/CVPR.2017.686; Jampani V, 2017, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2017.336; Jang W, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459860; Jeromel A, 2020, MULTIMED TOOLS APPL, V79, P433, DOI 10.1007/s11042-019-08126-7; Jha S., 2018, PREPRINT; Jin Y., 2017, PREPRINT; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI DOI 10.1109/CVPR42600.2020.00813; Karras T., 2017, PREPRINT; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kataoka Y, 2017, 2017 18TH IEEE/ACIS INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, ARTIFICIAL INTELLIGENCE, NETWORKING AND PARALLEL/DISTRIBUTED COMPUTING (SNDP 2017), P495, DOI 10.1109/SNPD.2017.8022768; Kim H, 2019, IEEE I CONF COMP VIS, P9055, DOI 10.1109/ICCV.2019.00915; Kim J., PREPRINT; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D.P., 2013, PREPRINT; Kliegl R., 2017, RES METHODS PSYCHOLI, P68; Kodali N., 2017, PREPRINT; Kopf J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366159; Kowalski M, 2017, IEEE COMPUT SOC CONF, P2034, DOI 10.1109/CVPRW.2017.254; Laubrock J, 2020, TOP COGN SCI, V12, P274, DOI 10.1111/tops.12476; Lazarou C., 2020, PREPRINT; Lee G., SIGGRAPH ASIA 2019 T, P45; Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584; Lee Y., 2020, P IEEECVF C COMP VIS, P13906; Lee YJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964922; Lee YW, 2019, IEEE COMPUT SOC CONF, P752, DOI 10.1109/CVPRW.2019.00103; Lei CY, 2019, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2019.00387; Li B., 2021, PREPRINT; Li CZ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073675; Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272; Li H., 2019, SHORT PAPERS P EUROG, P65; Li J., 2018, PREPRINT; Li SM, 2020, COMPUT GRAPH FORUM, V39, P587, DOI 10.1111/cgf.14170; Li WB, 2020, NEURAL NETWORKS, V132, P66, DOI 10.1016/j.neunet.2020.08.011; Li XY, 2019, IEEE INT CON MULTI, P652, DOI 10.1109/ICME.2019.00118; Li Y., 2017, PREPRINT; Li Y, 2017, INT J COMPUT VISION, V122, P169, DOI 10.1007/s11263-016-0963-9; Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28; Li ZJ, 2009, 2009 ASIA-PACIFIC CONFERENCE ON INFORMATION PROCESSING (APCIP 2009), VOL 2, PROCEEDINGS, P545, DOI [10.1109/APCIP.2009.270, 10.1109/CSIE.2009.672]; Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882; Liang X., 2017, PREPRINT; Liu G., 2018, INT S INT COMP APPL, P190; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Liu XT, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818067; Liu ZQ, 2006, SOFT COMPUT, V10, P34, DOI 10.1007/s00500-005-0461-4; Lvmin Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P137, DOI 10.1007/978-3-030-58601-0_9; Ma MH, 2006, ARTIF INTELL REV, V25, P37, DOI 10.1007/s10462-007-9042-5; Maejima A, 2019, SIGGRAPH '19 - ACM SIGGRAPH 2019 POSTERS, DOI 10.1145/3306214.3338560; Mainberger M, 2011, PATTERN RECOGN, V44, P1859, DOI 10.1016/j.patcog.2010.08.004; Mao X., 2015, COMPUT VIS MEDIA, V1, P69, DOI [10.1007/s41095-015-0007-3, DOI 10.1007/S41095-015-0007-]; Mathews J, 2015, COMPUT ELECTR ENG, V43, P169, DOI 10.1016/j.compeleceng.2015.01.001; Mishra Ashutosh, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P35, DOI 10.1007/978-3-319-46604-0_3; Mo S., 2018, PREPRINT; Mo S., 2020, PREPRINT; Nguyen KHL, 2011, LECT NOTES COMPUT SC, V6523, P536; Ni ZK, 2018, IEEE T IMAGE PROCESS, V27, P4516, DOI 10.1109/TIP.2018.2839890; Nizan Ori, 2020, P IEEE CVF C COMP VI, P7860; Odena A, 2017, PR MACH LEARN RES, V70; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Park Taesung, 2020, ADV NEURAL INF PROCE, V33, P7198; Peng CL, 2020, IEEE T IMAGE PROCESS, V29, P8519, DOI 10.1109/TIP.2020.3016502; Pesko M, 2019, FUND INFORM, V168, P311, DOI 10.3233/FI-2019-1834; Pinkney J.N., 2020, PREPRINT; Qian Z., 2020, PREPRINT; Radford A., 2015, PREPRINT; Raj YA, 2019, CLUSTER COMPUT, V22, P361, DOI 10.1007/s10586-018-1994-5; Ran Yi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8214, DOI 10.1109/CVPR42600.2020.00824; Ren H, 2020, IEEE ACCESS, V8, P44599, DOI 10.1109/ACCESS.2019.2962579; Ren S., 2015, PREPRINT; Rosin P.L., 2017, BENCHMARKING NONPHOT; Rosin Paul, 2012, IMAGE VIDEO BASED AR; Royer A., 2018, DOMAIN ADAPTATION VI, P33; Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3; Saito M., 2015, SIGGRAPH ASIA 2015 T, P1; Salimans T, 2016, ADV NEUR IN, V29; SANCHES CL, 2016, P 1 INT WORKSHOP COM; Sato K., 2014, SIGGRAPH ASIA 2014 T, P1; Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024; Shet R.N., 2005, USE NEURAL NETWORKS; Shi M., 2020, PREPRINT; Shi YC, 2019, PROC CVPR IEEE, P10754, DOI 10.1109/CVPR.2019.01102; Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201370; Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132703; Siyao L., 2021, P IEEECVF C COMPUTER, P6587; Skora D., 2004, P 3 INT S NONPH AN R, P121; Song GX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459771; Sonka M., 2014, IMAGE PROCESSING ANA; Stricker M., 2018, PREPRINT; Su H., 2020, PREPRINT; Su SC, 2016, PROC CVPR IEEE, pCP40, DOI 10.1109/CVPR.2016.382; Sultan K., 2018, PREPRINT; Sultan KMA, 2020, PROC INT C TOOLS ART, P1175, DOI 10.1109/ICTAI50040.2020.00178; Sultana F., 2019, PREPRINT; Sun LY, 2019, FRONT INFORM TECH EL, V20, P1644, DOI 10.1631/FITEE.1900386; Sun R., 2018, PREPRINT; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Takayama K., 2012, IM EL VIS COMP WORKS, P48; Tang H., 2019, PREPRINT; Taylor S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073699; Taylor T., 2011, THESIS CASE W RESERV; Thasarathan Harrish, 2019, 2019 16th Conference on Computer and Robot Vision (CRV). Proceedings, P189, DOI 10.1109/CRV.2019.00033; Tsai YC, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P456, DOI 10.1109/MMSP.2006.285350; Tseng CC, 2007, LECT NOTES COMPUT SC, V4843, P314; Tsubota K, 2019, IEEE INT SYM MULTIM, P212, DOI 10.1109/ISM46123.2019.00046; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Wang M, 2012, IEEE T MULTIMEDIA, V14, P858, DOI 10.1109/TMM.2012.2187181; Wang NN, 2017, IEEE T IMAGE PROCESS, V26, P1264, DOI 10.1109/TIP.2017.2651375; Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759; Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166; Wilber MJ, 2017, IEEE I CONF COMP VIS, P1211, DOI 10.1109/ICCV.2017.136; Wu R., 2019, PREPRINT; Xiang S., 2019, PREPRINT; Xiang S., 2018, PREPRINT; Xie J, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P6695, DOI 10.1145/3025453.3025872; Xie MS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417873; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Xin YC, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010041; Xinrui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8087, DOI 10.1109/CVPR42600.2020.00811; Yang C, 2019, IEEE T IMAGE PROCESS, V28, P4845, DOI 10.1109/TIP.2019.2914583; Yang XH, 2019, IEEE ACCESS, V7, P123788, DOI 10.1109/ACCESS.2019.2938900; Yao CY, 2017, IEEE T VIS COMPUT GR, V23, P1070, DOI 10.1109/TVCG.2016.2525774; Yeh R., 2016, PREPRINT; Yi R, 2021, IEEE T PATTERN ANAL, V43, P3462, DOI 10.1109/TPAMI.2020.2987931; Yi R, 2019, PROC CVPR IEEE, P10735, DOI 10.1109/CVPR.2019.01100; Yonetsuji T., 2017, PAINTSCHAINER GITHUB; You S., 2019, PREPRINT; [于谦 Yu Qian], 2015, [高分子通报, Polymer Bulletin], P1; Yu Z. Z. H. Z. Z., 2017, PHOTOTO CARICATURE T; Zhang B, 2020, IMMUNE PHENOTYPING B, DOI 10.1101/2020.03.12.20035048; Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824; Zhang H, 2019, PR MACH LEARN RES, V97; Zhang LM, 2021, PROC CVPR IEEE, P5638, DOI 10.1109/CVPR46437.2021.00559; Zhang LM, 2021, PROC CVPR IEEE, P9884, DOI 10.1109/CVPR46437.2021.00976; Zhang LM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275090; Zhang LM, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P506, DOI 10.1109/ACPR.2017.61; Zhang Y., 2019, PREPRINT; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhao Yihao, 2020, COMPUTER VISION ECCV, P800, DOI DOI 10.1007/978-3-030-58545-7_46; Zheng Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2264, DOI 10.1145/3394171.3413726; Zhu J. Y., 2017, PREPRINT; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu MR, 2021, INT J COMPUT VISION, V129, P1820, DOI 10.1007/s11263-021-01442-2; Zhu MR, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1048; Zou C., 2018, PREPRINT; Zou CQ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356561	237	0	0	19	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2733	2769		10.1007/s11263-022-01645-1	http://dx.doi.org/10.1007/s11263-022-01645-1		SEP 2022	37	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ					2022-12-18	WOS:000849289700002
J	Lu, YH; Zhou, J; McDorman, ST; Zhang, CY; Scott, D; Bukuts, J; Wilder, C; Smith, KY; Wang, S				Lu, Yuhang; Zhou, Jun; McDorman, Sam T.; Zhang, Canyu; Scott, Deja; Bukuts, Jake; Wilder, Colin; Smith, Karen Y.; Wang, Song			Snowvision: Segmenting, Identifying, and Discovering Stamped Curve Patterns from Fragments of Pottery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Curve-pattern segmentation; Design identification; Curve-pattern matching; Curve-pattern clustering; Swift creek complicated stamped pottery	IMAGE SEGMENTATION; CRACK DETECTION; MEAN SHIFT; INVARIANT; EVOLUTION; SYSTEM	In southeastern North America, Indigenous potters and woodworkers carved complex, primarily abstract, designs into wooden pottery paddles, which were subsequently used to thin the walls of hand-built, clay vessels. Original paddle designs carry rich historical and cultural information, but pottery paddles from ancient times have not survived. Archaeologists have studied design fragments stamped on sherds to reconstruct complete or nearly complete designs, which is extremely laborious and time-consuming. In Snowvision, we aim to develop computer vision methods to assist archaeologists to accomplish this goal more efficiently and effectively. For this purpose, we identify and study three computer vision tasks: (1) extracting curve structures stamped on pottery sherds; (2) matching sherds to known designs; (3) clustering sherds with unknown designs. Due to the noisy, highly fragmented, composite-curve patterns, each task poses unique challenges to existing methods. To solve them, we propose (1) a weakly-supervised CNN-based curve structure segmentation method that takes only curve skeleton labels to predict full curve masks; (2) a patch-based curve pattern matching method to address the problem of partial matching in terms of noisy binary images; (3) a curve pattern clustering method consisting of pairwise curve matching, graph partitioning and sherd stitching. We evaluate the proposed methods on a set of collected sherds and extensive experimental results show the effectiveness of the proposed algorithms.	[Lu, Yuhang; Zhou, Jun; Zhang, Canyu; Scott, Deja; Bukuts, Jake; Wang, Song] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA; [McDorman, Sam T.] Univ South Carolina, Dept Anthropol & Sch Informat Sci, Columbia, SC 29208 USA; [Wilder, Colin] Univ South Carolina, Dept Hist, Columbia, SC 29208 USA; [Smith, Karen Y.] South Carolina Dept Nat Resources, Columbia, SC 29172 USA	University of South Carolina System; University of South Carolina Columbia; University of South Carolina System; University of South Carolina Columbia; University of South Carolina System; University of South Carolina Columbia	Wang, S (corresponding author), Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.; Smith, KY (corresponding author), South Carolina Dept Nat Resources, Columbia, SC 29172 USA.	yuhang@email.sc.edu; zhouj@mailbox.sc.edu; mcdorman@email.sc.edu; canyu@email.sc.edu; ds17@email.sc.edu; jbukuts@email.sc.edu; wildercf@mailbox.sc.edu; SmithKY@dnr.sc.gov; songwang@cec.sc.edu		Wang, Song/0000-0003-4152-5295	NationalEndowment for the Humanities (NEH) Digital Humanities Advancement Grant Program [HAA-266472-19]; National Science Foundation Archaeology and Archaeometry Grant Program [1658987]; National Center for Preservation Technology and Training Grant Program [P16AP00373]; Extreme Science and Engineering Discovery Environment (XSEDE) Science Gateway Program [DBS180011]; University of South Carolina ASPIRE II Program; University of South Carolina Social Science Provost Grant Program	NationalEndowment for the Humanities (NEH) Digital Humanities Advancement Grant Program; National Science Foundation Archaeology and Archaeometry Grant Program; National Center for Preservation Technology and Training Grant Program; Extreme Science and Engineering Discovery Environment (XSEDE) Science Gateway Program; University of South Carolina ASPIRE II Program; University of South Carolina Social Science Provost Grant Program	This research is supported by the NationalEndowment for the Humanities (NEH) Digital Humanities Advancement Grant Program (HAA-266472-19), the National Science Foundation Archaeology and Archaeometry Grant Program (1658987), the National Center for Preservation Technology and Training Grant Program (P16AP00373), the Extreme Science and Engineering Discovery Environment (XSEDE) Science Gateway Program (DBS180011), University of South Carolina ASPIRE II Program, and University of South Carolina Social Science Provost Grant Program. We would like to show our gratitude to Frankie Snow at South Georgia State College for sharing his pearls of wisdom and design images with us during the course of this research. We are very grateful to Dr. Matthew Compton, Curator of the R. M. Bogan Repository at Georgia Southern University, and Dr. Amanda Roberts Thompson, Operations Director of the Laboratory of Archaeology at the University of Georgia, for generously providing access to collections, and Scot Keith at New South Associates for his enthusiasm and encouragement of this research. We would like to thank Research Computing at University of South Carolina for highperformance computing support, and the South Carolina Department of Natural Resources for institutional support. The Muscogee (Creek) Nation has approved the scanning of Broyles Collection sherds shown in Figure 16 and requests consultation on any future use of the scans and images including but not limited to presentations, publications, manuscripts, social media posts, and news stories.	Alilou V.K., 2020, FINGERPRINT MATCHING; Anichini F., 2020, INTERNET ARCHAEOL, V52, DOI [10.11141/ia.52.7, DOI 10.11141/IA.52.7]; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Badrinarayanan V., 2015, ARXIV; Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410; Banterle F, 2017, PROC INT CONF DOC, P683, DOI 10.1109/ICDAR.2017.117; Barrow HG, 1977, P 5 INT JOINT C ART; Bay H., 2006, EUR C COMP VIS ECCV, P404, DOI [10.1007/11744023_32, DOI 10.1007/11744023_32]; Belongie S, 2001, ADV NEUR IN, V13, P831; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; Broyles BJ., 1968, SE ARCHAEOLOGICAL C, V8, P49; Brunelli R., 2009, TEMPLATE MATCHING TE, DOI [10.1002/9780470744055, DOI 10.1002/9780470744055]; Bundy A., 1984, CATALOGUE ARTIFICIAL, P30; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Carson C., 1998, P 3 INT C VIS INF SY, P509, DOI DOI 10.1007/3-540-48762-X_63; Chan J, 2017, PROC CVPR IEEE, P3020, DOI 10.1109/CVPR.2017.322; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626; Chao P, 2019, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2019.00365; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chia-Kai Yeh, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P738, DOI 10.1007/978-3-319-46604-0_51; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Daras P., 2013, J COMPUTING CULTURAL, V5, DOI 10.1145/2399180.2399183; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Frenkel M, 2003, LECT NOTES COMPUT SC, V2683, P35; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Fu F, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-18606-2; Fu L., 2019, PREHISTORIC MARITIME, P235; Gamble C., 2004, ARCHAEOLOGY BASICS; Gualandi ML, 2021, HERITAGE-BASEL, V4, P140, DOI 10.3390/heritage4010008; Han DJ, 2014, PATTERN RECOGN, V47, P296, DOI 10.1016/j.patcog.2013.06.022; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinami R, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P528, DOI 10.1145/3123266.3123312; Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7; Jain A, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P282, DOI 10.1109/ICIP.2001.958106; Jain A.K., 1988, ALGORITHMS CLUSTERIN; Kampel M., 2003, COMP VIS PATT REC WO, P4; Kong B, 2019, INT J COMPUT VISION, V127, P1738, DOI 10.1007/s11263-018-01143-3; LAM L, 1992, IEEE T PATTERN ANAL, V14, P869, DOI 10.1109/34.161346; Li HF, 2019, IEEE T INTELL TRANSP, V20, P2025, DOI 10.1109/TITS.2018.2856928; Li L, 2010, IEEE T IMAGE PROCESS, V19, P1, DOI 10.1109/TIP.2009.2032341; LI ZQ, 2015, PROC CVPR IEEE, P1356, DOI DOI 10.1109/CVPR.2015.7298741; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lorigo LM, 2001, MED IMAGE ANAL, V5, P195, DOI 10.1016/S1361-8415(01)00040-8; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Lu YH, 2018, AAAI CONF ARTIF INTE, P7235; Lucena M, 2016, MULTIMED TOOLS APPL, V75, P3677, DOI 10.1007/s11042-014-2063-6; MacDonald L.W., 2015, THESIS UCL; Markus N, 2019, IEEE T IMAGE PROCESS, V28, P279, DOI 10.1109/TIP.2018.2867270; Martinez-Carrillo, 2008, C COMPUTER APPL QUAN, V2, P6; Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Ono Yuki, 2018, ADV NEURAL INFORM PR, P6237; Ostertag C, 2020, PATTERN RECOGN LETT, V131, P336, DOI 10.1016/j.patrec.2020.01.012; Otto C, 2018, IEEE T PATTERN ANAL, V40, P289, DOI 10.1109/TPAMI.2017.2679100; Pirrone A, 2019, PROCEEDINGS OF THE 2019 WORKSHOP ON HISTORICAL DOCUMENT IMAGING AND PROCESSING (HIP' 19), P78, DOI 10.1145/3352631.3352646; Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340; Rasheed NA, 2020, J KING SAUD UNIV-COM, V32, P883, DOI 10.1016/j.jksuci.2018.09.019; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Roux V., 2019, CERAMICS SOC TECHNOL, DOI 10.1007/978-3-030-03973-8; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen W, 2016, PROC CVPR IEEE, P222, DOI 10.1109/CVPR.2016.31; Shen XL, 2019, PROC CVPR IEEE, P8124, DOI 10.1109/CVPR.2019.00832; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Smith KY., 2012, SE ARCHAEL, V31, P143, DOI 10.1179/sea.2012.31.2.002; Smith KY., 2018, SE ARCHAEOLOGY, V37, P112, DOI [10.1080/0734578X.2017.1416887, DOI 10.1080/0734578X.2017.1416887]; Smith P., 2010, 2010 IEEE COMP SOC C, P49, DOI DOI 10.1109/CVPRW.2010.5543523; Snow F., 1998, WORLD ENGRAVED ARCHA, P61; Snow F., 1975, EARLY GEORGIA, V3, P38; Son K, 2013, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2013.40; Stamatopoulos M.I., 2016, ARXIV; Talcott L., 1935, HESPERIA J AM SCH CL, V4, P477, DOI [10.2307/146463, DOI 10.2307/146463]; Tao XD, 2002, IEEE T MED IMAGING, V21, P513, DOI 10.1109/TMI.2002.1009387; Tico M, 2003, IEEE T PATTERN ANAL, V25, P1009, DOI 10.1109/TPAMI.2003.1217604; Tombari F, 2013, IEEE I CONF COMP VIS, P1265, DOI 10.1109/ICCV.2013.160; Tremeau A, 1997, PATTERN RECOGN, V30, P1191, DOI 10.1016/S0031-3203(96)00147-1; van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265; Wang S, 2004, ADV NEUR IN, V16, P1571; Wang S, 2003, IEEE T PATTERN ANAL, V25, P675, DOI 10.1109/TPAMI.2003.1201819; Wang ZD, 2019, PROC CVPR IEEE, P1117, DOI 10.1109/CVPR.2019.00121; Willis A., 2003, COMPUTER VISION PATT, V1, P5; Willis AR, 2004, PROC CVPR IEEE, P82; Wu JL, 2019, IEEE I CONF COMP VIS, P8149, DOI 10.1109/ICCV.2019.00824; Wu KL, 2007, PATTERN RECOGN, V40, P3035, DOI 10.1016/j.patcog.2007.02.006; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Zhan XH, 2018, LECT NOTES COMPUT SC, V11213, P576, DOI 10.1007/978-3-030-01240-3_35; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zou Q, 2012, PATTERN RECOGN LETT, V33, P227, DOI 10.1016/j.patrec.2011.11.004; Zunic J, 2010, PATTERN RECOGN, V43, P47, DOI 10.1016/j.patcog.2009.06.017	98	0	0	7	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2707	2732		10.1007/s11263-022-01669-7	http://dx.doi.org/10.1007/s11263-022-01669-7		AUG 2022	26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ					2022-12-18	WOS:000847089300001
J	Farnoosh, A; Ostadabbas, S				Farnoosh, Amirreza; Ostadabbas, Sarah			Dynamical Deep Generative Latent Modeling of 3D Skeletal Motion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D skeletal motion; Bayesian inference; Biologically valid interpretation; Generative models; Latent state modeling; Variational inference	SEGMENTATION; CAPTURE	In this paper, we propose a Bayesian switching dynamical model for segmentation of 3D pose data over time that uncovers interpretable patterns in the data and is generative. Our model decomposes highly correlated skeleton data into a set of few spatial basis of switching temporal processes in a low-dimensional latent framework. We parameterize these temporal processes with regard to a switching deep vector autoregressive prior in order to accommodate both multimodal and higher-order nonlinear inter-dependencies. This results in a dynamical deep generative latent model that parses the meaningful intrinsic states in the dynamics of 3D pose data using approximate variational inference, and enables a realistic low-level dynamical generation and segmentation of complex skeleton movements. Our experiments on four biological motion data containing bat flight, salsa dance, walking, and golf datasets substantiate superior performance of our model in comparison with the state-of-the-art methods.	[Farnoosh, Amirreza; Ostadabbas, Sarah] Northeastern Univ, Dept Elect & Comp Engn, Augmented Cognit Lab, Boston, MA 02115 USA	Northeastern University	Ostadabbas, S (corresponding author), Northeastern Univ, Dept Elect & Comp Engn, Augmented Cognit Lab, Boston, MA 02115 USA.	ostadabbas@ece.neu.edu						ACKERSON GA, 1970, IEEE T AUTOMAT CONTR, VAC15, P10, DOI 10.1109/TAC.1970.1099359; Bahadori M.T., 2014, P 27 INT C NEURAL IN, V2, P3491; Barbic J, 2004, PROC GRAPH INTERF, P185; Becker P, 2019, PR MACH LEARN RES, V97; Becker-Ehmck P, 2019, PR MACH LEARN RES, V97; Bergou AJ, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002297; Birch M. C., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1109, DOI 10.1109/ROBOT.2000.844747; Cai YJ, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P79, DOI 10.1145/2783258.2783348; CHANG CB, 1978, IEEE T AERO ELEC SYS, V14, P418, DOI 10.1109/TAES.1978.308603; Chang Y. Y., 2018, ARXIV; Chen LZ, 2018, AAAI CONF ARTIF INTE, P2844; Community B.O., 2018, BLENDER A 3D MODELLI; Farnoosh A., 2020, ARXIV; Fox E., 2008, ADV NEURAL INFORM PR, V21, P457; Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494; Ghahramani Z., 1996, TECH REP, DOI DOI 10.1109/JPROC.2014.2307357; Gong CH, 2016, INT J ROBOT RES, V35, P100, DOI 10.1177/0278364915593793; Gong D, 2014, IEEE T PATTERN ANAL, V36, P1414, DOI 10.1109/TPAMI.2013.244; HAMILTON JD, 1990, J ECONOMETRICS, V45, P39, DOI 10.1016/0304-4076(90)90093-9; Hoff J, 2016, ROBOTICS: SCIENCE AND SYSTEMS XII; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Karl M., 2017, PROC INT C LEARN REP, P1; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Kruger B, 2017, IEEE T MULTIMEDIA, V19, P797, DOI 10.1109/TMM.2016.2635030; Lai GK, 2018, ACM/SIGIR PROCEEDINGS 2018, P95, DOI 10.1145/3209978.3210006; Linderman SW, 2017, PR MACH LEARN RES, V54, P914; Meshry M., 2016, P IEEE WINT C APPL C, P1, DOI DOI 10.1109/WACV.2016.7477587; Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Murphy K. P, 1998, TECH REP; Nakamura T, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00067; Nassar J., 2019, INT C LEARNING REPRE; Papoutsakis K., 2017, P IEEE C COMP VIS PA, P6827; Paszke A, 2017, NIPS 2017 WORKSHOP; Patrona F, 2018, PATTERN RECOGN, V76, P612, DOI 10.1016/j.patcog.2017.12.007; Ranganath R., 2013, P 30 INT C MACH LEAR, V28, P298; Rangapuram SS, 2018, ADV NEUR IN, V31; Riskin DK, 2008, J THEOR BIOL, V254, P604, DOI 10.1016/j.jtbi.2008.06.011; Salinas D, 2020, INT J FORECASTING, V36, P1181, DOI 10.1016/j.ijforecast.2019.07.001; Santello M, 1998, J NEUROSCI, V18, P10105; Sen R, 2019, ADV NEUR IN, V32; Sun JZ, 2014, IEEE T SIGNAL PROCES, V62, P3499, DOI 10.1109/TSP.2014.2326618; Sun L., 2019, ARXIV; Takeuchi K, 2017, IEEE DATA MINING, P1105, DOI 10.1109/ICDM.2017.146; Tao Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10274, DOI 10.1109/CVPR42600.2020.01029; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167; Wang ZY, 2021, IEEE T VIS COMPUT GR, V27, P14, DOI 10.1109/TVCG.2019.2938520; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Wu Z., 2017, 2017 IEEE 15 INT C D, P780; Xia GY, 2021, IEEE T NEUR NET LEAR, V32, P1612, DOI 10.1109/TNNLS.2020.2985817; Xia GY, 2018, IEEE T IMAGE PROCESS, V27, P135, DOI 10.1109/TIP.2017.2738562; Xia J, 2020, MOL INFORM, V39, DOI 10.1002/minf.201900151; Yu Hsiang-Fu, 2016, ADV NEURAL INFORM PR, P847; Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137	58	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2695	2706		10.1007/s11263-022-01668-8	http://dx.doi.org/10.1007/s11263-022-01668-8		AUG 2022	12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ		Green Submitted			2022-12-18	WOS:000843995200002
J	Ye, P; Li, BP; Chen, T; Fan, JY; Mei, Z; Lin, C; Zuo, CY; Chi, QH; Ouyang, WL				Ye, Peng; Li, Baopu; Chen, Tao; Fan, Jiayuan; Mei, Zhen; Lin, Chen; Zuo, Chongyan; Chi, Qinghua; Ouyang, Wanli			Efficient Joint-Dimensional Search with Solution Space Regularization for Real-Time Semantic Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Solution Space Regularization; Joint-Dimensional Search; Real-Time Segmentation	MODEL	Semantic segmentation is a popular research topic in computer vision, and many efforts have been made on it with impressive results. In this paper, we intend to search an optimal network structure that can run in real-time for this problem. Towards this goal, we jointly search the depth, channel, dilation rate and feature spatial resolution, which results in a search space consisting of about 2.78 x 10(324) possible choices. To handle such a large search space, we leverage differential architecture search methods. However, the architecture parameters searched using existing differential methods need to be discretized, which causes the discretization gap between the architecture parameters found by the differential methods and their discretized version as the final solution for the architecture search. Hence, we relieve the problem of discretization gap from the innovative perspective of solution space regularization. Specifically, a novel Solution Space Regularization (SSR) loss is first proposed to effectively encourage the supernet to converge to its discrete one. Then, a new Hierarchical and Progressive Solution Space Shrinking method is presented to further achieve high efficiency of searching. In addition, we theoretically show that the optimization of SSR loss is equivalent to the Lo-norm regularization, which accounts for the improved search-evaluation gap. Comprehensive experiments show that the proposed search scheme can efficiently find an optimal network structure that yields an extremely fast speed (175 FPS) of segmentation with a small model size (1 M) while maintaining comparable accuracy.	[Ye, Peng; Chen, Tao; Mei, Zhen] Fudan Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China; [Li, Baopu] Oracle, Oracle Hlth & AI, Austin, TX USA; [Fan, Jiayuan] Fudan Univ, Acad Engn & Technol, Shanghai, Peoples R China; [Zuo, Chongyan; Chi, Qinghua] Huawei Inc China, Huawei, Peoples R China; [Lin, Chen] Univ Oxford, Oxford, England; [Ouyang, Wanli] Univ Sydney, Sydney, NSW, Australia; [Ouyang, Wanli] Shanghai AI Lab, Shanghai, Peoples R China	Fudan University; Oracle; Fudan University; Huawei Technologies; University of Oxford; University of Sydney	Chen, T (corresponding author), Fudan Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China.	eetchen@fudan.edu.cn			National Natural Science Foundation of China [62071127, U1909207]; Shanghai Municipal Science and Technology Major Project [2021SHZDZX0103]; Zhejiang Lab Project [2021KH0AB05]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai Municipal Science and Technology Major Project; Zhejiang Lab Project	This work is supported by National Natural Science Foundation of China (No. 62071127, U1909207), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103), and Zhejiang Lab Project (No. 2021KH0AB05).	Cai H, 2019, INT C LEARNING REPRE; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen Wuyang, 2019, ARXIV191210917; Chen XN, 2020, PR MACH LEARN RES, V119; Chen X, 2021, INT J COMPUT VISION, V129, P638, DOI 10.1007/s11263-020-01396-x; Chu Xiangxiang, 2020, EUR C COMP VIS, P465, DOI DOI 10.1007/978-3-030-58555-6_28; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Emara T, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P113; Guo JY, 2020, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR42600.2020.00158; Hu K, 2020, IEEE T IMAGE PROCESS, V29, P1890, DOI 10.1109/TIP.2019.2946469; Jiahui Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P702, DOI 10.1007/978-3-030-58571-6_41; Li GH, 2020, PROC CVPR IEEE, P1617, DOI 10.1109/CVPR42600.2020.00169; Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975; Liang F., 2019, INT C LEARNING REPRE; Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518; Lin PW, 2020, PROC CVPR IEEE, P4202, DOI 10.1109/CVPR42600.2020.00426; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu Hanxiao, 2018, INT C LEARNING REPRE; Orsic M, 2019, PROC CVPR IEEE, P12599, DOI 10.1109/CVPR.2019.01289; Paszke A., 2016, ARXIV PREPRINT ARXIV; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Qu W, 2020, IEEE J BIOMED HEALTH, V24, P2833, DOI 10.1109/JBHI.2020.2978004; Shikun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P661, DOI 10.1007/978-3-030-58610-2_39; Sun P, 2021, INT J COMPUT VISION, V129, P1506, DOI 10.1007/s11263-021-01433-3; Tan MX, 2019, PR MACH LEARN RES, V97; Tian GL, 2011, COMPUT STAT DATA AN, V55, P3381, DOI 10.1016/j.csda.2011.06.005; Wan Alvin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12962, DOI 10.1109/CVPR42600.2020.01298; Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163; Xianzhi Du, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11589, DOI 10.1109/CVPR42600.2020.01161; Xie S., 2018, INT C LEARNING REPRE; Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2; Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20; Yu Fisher, 2018, BDD100K DIVERSE DRIV, P6; Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0	39	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2674	2694		10.1007/s11263-022-01663-z	http://dx.doi.org/10.1007/s11263-022-01663-z		AUG 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ		Green Submitted			2022-12-18	WOS:000843995200001
J	Yu, TX; Lin, C; Zhang, SJ; Wang, CX; Ding, XH; An, HL; Liu, XX; Qu, T; Wan, L; You, SD; Wu, J; Zhang, JW				Yu, Tianxiu; Lin, Cong; Zhang, Shijie; Wang, Chunxue; Ding, Xiaohong; An, Huili; Liu, Xiaoxiang; Qu, Ting; Wan, Liang; You, Shaodi; Wu, Jian; Zhang, Jiawan			Artificial Intelligence for Dunhuang Cultural Heritage Protection: The Project and the Dataset	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cultural heritage protection; Dunhuang; Artificial intelligence; Computer vision		In this work, we introduce our project on Dunhuang cultural heritage protection using artificial intelligence. The Dunhuang Mogao Grottoes in China, also known as the Grottoes of the Thousand Buddhas, is a religious and cultural heritage located on the Silk Road. The grottoes were built from the 4th century to the 14th century. After thousands of years, the in grottoes decaying is serious. In addition, numerous historical records were destroyed throughout the years, making it difficult for archaeologists to reconstruct history. We aim to use modern computer vision and machine learning technologies to solve such challenges. First, we propose to use deep networks to automatically perform the restoration. Through out experiments, we find the automated restoration can provide comparable quality as those manually restored from an archaeologist. This can significantly speed up the restoration given the enormous size of the historical paintings. Second, we propose to use detection and retrieval for further analyzing the tremendously large amount of objects because it is unreasonable to manually label and analyze them. Several state-of-the-art methods are rigorously tested and quantitatively compared in different criteria and categorically. In this work, we created a new dataset, namely, AI for Dunhuang, to facilitate the research. Version v1.0 of the dataset comprises of data and label for the restoration, style transfer, detection, and retrieval. Specifically, the dataset has 10,000 images for restoration, 3455 for style transfer, and 6147 for property retrieval. Lastly, we propose to use style transfer to link and analyze the styles over time, given that the grottoes were build over 1000 years by numerous artists. This enables the possibly to analyze and study the art styles over 1000 years and further enable future researches on cross-era style analysis. We benchmark representative methods and conduct a comparative study on the results for our solution. The dataset will be publicly available along with this paper.	[Yu, Tianxiu; Wan, Liang; Zhang, Jiawan] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China; [Yu, Tianxiu; Wang, Chunxue; Ding, Xiaohong; An, Huili; Wu, Jian] Dunhuang Acad, Dunhuang, Gansu, Peoples R China; [Lin, Cong; Liu, Xiaoxiang; Qu, Ting] Jinan Univ, Guangzhou, Peoples R China; [Zhang, Shijie] Tianjin Med Univ, Tianjin, Peoples R China; [You, Shaodi] Univ Amsterdam, Amsterdam, Netherlands	Tianjin University; Jinan University; Tianjin Medical University; University of Amsterdam	Zhang, JW (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.; Wu, J (corresponding author), Dunhuang Acad, Dunhuang, Gansu, Peoples R China.; You, SD (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	s.you@uva.nl; wujian@dha.ac.cn; jwzhang@tju.edu.cn			National Key R&D Program of China [2020YFC1522705, 2020YFC1522701]; National Natural Science Foundation of China [62006101]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	Supported by National Key R&D Program of China, Grant No.2020YFC1522705 and Grant No.2020YFC1522701, and the National Natural Science Foundation of China, Grant No. 62006101.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anichini F., 2020, INTERNET ARCHAEOL, V52, DOI [10.11141/ia.52.7, DOI 10.11141/IA.52.7]; [Anonymous], DUNHUANGACADEMY E DU; Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036; Banno A, 2008, INT J COMPUT VISION, V78, P207, DOI 10.1007/s11263-007-0104-6; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Bok Y, 2011, INT J COMPUT VISION, V94, P36, DOI 10.1007/s11263-010-0397-8; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chen HB, 2021, PROC CVPR IEEE, P872, DOI 10.1109/CVPR46437.2021.00093; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Geiger A., 2012, P IEEE COMP SOC C CO; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Haliassos A., 2020, VISUAL COMPUTING CUL, P121, DOI DOI 10.1007/978-3-030-37191-3_7; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hou ML, 2018, J CULT HERIT, V34, P136, DOI 10.1016/j.culher.2018.04.004; Huang JB., 2014, ACM T GRAPHIC, V33, p1; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Ikeuchi K, 2013, 2013 INTERNATIONAL CONFERENCE ON CULTURE AND COMPUTING (CULTURE AND COMPUTING 2013), P1, DOI 10.1109/CultureComputing.2013.77; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karayev S., 2013, ARXIV; Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263; Levin A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P305; Li BY, 2019, AAAI CONF ARTIF INTE, P8577; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Lu M, 2019, IEEE I CONF COMP VIS, P5951, DOI 10.1109/ICCV.2019.00605; Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408; Pan G., 2018, P COMP GRAPH INT 201, P239, DOI [10.1145/3208159.3208160, DOI 10.1145/3208159.3208160]; Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091; Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-7_19, DOI 10.48550/ARXIV.2007.15651]; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43; Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860; Simakov Denis, 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587842; Sofiiuk K., 2020, P IEEECVF C COMPUTER; Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI 10.1007/978-3-030-01216-8_1; Sun DM, 2018, 2018 INTERNATIONAL CONFERENCE ON MICROWAVE AND MILLIMETER WAVE TECHNOLOGY (ICMMT2018); Wang H., 2019, DUNHUANG RES, V176, P26; Wang Zhizhong, 2020, P IEEE C COMP VIS PA, P7789; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434; Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913; Yu TX, 2019, IEEE INT CONF COMP V, P1447, DOI 10.1109/ICCVW.2019.00182; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	63	0	0	16	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2646	2673		10.1007/s11263-022-01665-x	http://dx.doi.org/10.1007/s11263-022-01665-x		AUG 2022	28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ		hybrid			2022-12-18	WOS:000843995200003
J	Peng, DZ; Jin, LW; Liu, YL; Luo, CJ; Lai, SX				Peng, Dezhi; Jin, Lianwen; Liu, Yuliang; Luo, Canjie; Lai, Songxuan			PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Handwritten Chinese text recognition; Page-level handwritten text recognition; Weakly supervised learning; Reading order	HISTORICAL DOCUMENTS; NEURAL-NETWORK; ONLINE; SEGMENTATION; CHARACTERS	Handwritten Chinese text recognition (HCTR) has been an active research topic for decades. However, most previous studies solely focus on the recognition of cropped text line images, ignoring the error caused by text line detection in real-world applications. Although some approaches aimed at page-level text recognition have been proposed in recent years, they either are limited to simple layouts or require very detailed annotations including expensive line-level and even character-level bounding boxes. To this end, we propose PageNet for end-to-end weakly supervised page-level HCTR. PageNet detects and recognizes characters and predicts the reading order between them, which is more robust and flexible when dealing with complex layouts including multi-directional and curved text lines. Utilizing the proposed weakly supervised learning framework, PageNet requires only transcripts to be annotated for real data; however, it can still output detection and recognition results at both the character and line levels, avoiding the labor and cost of labeling bounding boxes of characters and text lines. Extensive experiments conducted on five datasets demonstrate the superiority of PageNet over existing weakly supervised and fully supervised page-level methods. These experimental results may spark further research beyond the realms of existing methods based on connectionist temporal classification or attention. The source code is available at https://github.com/shannanyinxiang/PageNet.	[Peng, Dezhi; Jin, Lianwen; Luo, Canjie] South China Univ Technol, Guangzhou, Peoples R China; [Liu, Yuliang] Huazhong Univ Sci & Technol, Wuhan, Peoples R China; [Lai, Songxuan] Huawei Cloud Comp Technol, Shenzhen, Peoples R China; [Jin, Lianwen] Pazhou Lab Huangpu, Guangzhou, Peoples R China; [Jin, Lianwen] Peng Cheng Lab, Shenzhen, Peoples R China	South China University of Technology; Huazhong University of Science & Technology; Pazhou Lab; Peng Cheng Laboratory	Jin, LW (corresponding author), South China Univ Technol, Guangzhou, Peoples R China.; Jin, LW (corresponding author), Pazhou Lab Huangpu, Guangzhou, Peoples R China.; Jin, LW (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	eelwjin@scut.edu.cn		Jin, Lianwen/0000-0002-5456-0957	NSFC [61936003]; GD-NSF [2017A030312006, 2021A1515 011870]; Science and Technology Foundation of Guangzhou Huangpu Development District [2020GH17]	NSFC(National Natural Science Foundation of China (NSFC)); GD-NSF; Science and Technology Foundation of Guangzhou Huangpu Development District	This research is supported in part by NSFC (Grant No.: 61936003), GD-NSF (no.2017A030312006, No.2021A1515 011870), and the Science and Technology Foundation of Guangzhou Huangpu Development District (Grant 2020GH17).	Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959; Bluche T, 2016, ADV NEUR IN, V29; Bluche T, 2017, PROC INT CONF DOC, P1050, DOI 10.1109/ICDAR.2017.174; Carbonell M, 2019, PROC INT CONF DOC, P29, DOI 10.1109/ICDARW.2019.40077; Chung J, 2019, PROC INT CONF DOC, P35, DOI 10.1109/ICDARW.2019.40078; Dezhi Peng, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P25, DOI 10.1109/ICDAR.2019.00014; Du J, 2016, INT C PATT RECOG, P3428, DOI 10.1109/ICPR.2016.7900164; Feng W, 2021, INT J COMPUT VISION, V129, P619, DOI 10.1007/s11263-020-01388-x; Graves A., 2006, P 23 INT C MACH LEAR, P369; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jaderberg M, 2014, ABS14062227 CORR; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Keysers D, 2017, IEEE T PATTERN ANAL, V39, P1180, DOI 10.1109/TPAMI.2016.2572693; Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086; Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17; Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595; Liu YL, 2021, INT J COMPUT VISION, V129, P1972, DOI 10.1007/s11263-021-01459-7; Liu Z., 2020, INT J COMPUT VISION, V128, P1, DOI [10.1007/s11263-019-01215-y, DOI 10.1007/S11263-019-01215-Y]; Luo CJ, 2021, INT J COMPUT VISION, V129, P960, DOI 10.1007/s11263-020-01411-1; Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5; Ma WH, 2020, INT CONF FRONT HAND, P31, DOI 10.1109/ICFHR2020.2020.00017; Messina R, 2015, PROC INT CONF DOC, P171; Moysset B, 2017, PROC INT CONF DOC, P871, DOI 10.1109/ICDAR.2017.147; Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rodriguez-Serrano JA, 2015, INT J COMPUT VISION, V113, P193, DOI 10.1007/s11263-014-0793-6; Rui Zhang, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1577, DOI 10.1109/ICDAR.2019.00253; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Su TH, 2009, PATTERN RECOGN, V42, P167, DOI 10.1016/j.patcog.2008.05.012; Tensmeyer Chris, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1, DOI 10.1109/ICDAR.2019.00011; Wang QF, 2012, IEEE T PATTERN ANAL, V34, P1469, DOI 10.1109/TPAMI.2011.264; Wang S, 2016, INT CONF FRONT HAND, P84, DOI 10.1109/ICFHR.2016.25; Wang ZX, 2020, INT CONF FRONT HAND, P157, DOI 10.1109/ICFHR2020.2020.00038; Wang ZR, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107102; Wang ZR, 2018, INT J DOC ANAL RECOG, V21, P241, DOI 10.1007/s10032-018-0307-0; Wigington C, 2018, P EUROPEAN C COMPUTE, P367; Wu YC, 2017, PATTERN RECOGN, V65, P251, DOI 10.1016/j.patcog.2016.12.026; Wu YC, 2017, PROC INT CONF DOC, P79, DOI 10.1109/ICDAR.2017.22; Xie C., 2020, P INT WORKSH DOC AN, P45; Xie ZC, 2019, PROC CVPR IEEE, P6531, DOI 10.1109/CVPR.2019.00670; Xie ZC, 2019, NEUROCOMPUTING, V350, P271, DOI 10.1016/j.neucom.2019.04.001; Xie ZC, 2018, IEEE T PATTERN ANAL, V40, P1903, DOI 10.1109/TPAMI.2017.2732978; Xing LJ, 2019, IEEE I CONF COMP VIS, P9125, DOI 10.1109/ICCV.2019.00922; Yang HL, 2018, INT CONF FRONT HAND, P199, DOI 10.1109/ICFHR-2018.2018.00043; Yang HL, 2018, IEEE ACCESS, V6, P30174, DOI 10.1109/ACCESS.2018.2840218; Yaoxiong Huang, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P413, DOI 10.1109/ICDAR.2019.00073; Yin F, 2013, PROC INT CONF DOC, P1464, DOI 10.1109/ICDAR.2013.218; Yousef Mohamed, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14698, DOI 10.1109/CVPR42600.2020.01472; Yuhuan Xiu, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1464, DOI 10.1109/ICDAR.2019.00235; Zhan FN, 2018, LECT NOTES COMPUT SC, V11212, P257, DOI 10.1007/978-3-030-01237-3_16; Zhang HS, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107559; Zhang XY, 2018, IEEE T PATTERN ANAL, V40, P849, DOI 10.1109/TPAMI.2017.2695539; Zhou XD, 2013, IEEE T PATTERN ANAL, V35, P2413, DOI 10.1109/TPAMI.2013.49; Zhu ZY, 2020, INT CONF FRONT HAND, P288, DOI 10.1109/ICFHR2020.2020.00060	59	0	0	13	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2623	2645		10.1007/s11263-022-01654-0	http://dx.doi.org/10.1007/s11263-022-01654-0		AUG 2022	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ		Green Submitted			2022-12-18	WOS:000842143600001
J	Han, ZY; Fu, ZY; Chen, S; Yang, J				Han, Zongyan; Fu, Zhenyong; Chen, Shuo; Yang, Jian			Semantic Contrastive Embedding for Generalized Zero-Shot Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generalized zero-shot learning; Attribute; Semantic embedding; Contrastive learning		Generalized zero-shot learning (GZSL) aims to recognize objects from both seen and unseen classes when only the labeled examples from seen classes are provided. Recent feature generation methods learn a generative model that can synthesize the missing visual features of unseen classes to mitigate the data-imbalance problem in GZSL. However, the original visual feature space is suboptimal for GZSL recognition since it lacks semantic information, which is vital for recognizing the unseen classes. To tackle this issue, we propose to integrate the feature generation model with an embedding model. Our GZSL framework maps both the real and the synthetic samples produced by the generation model into an embedding space, where we perform the final GZSL classification. Specifically, we propose a semantic contrastive embedding (SCE) for our GZSL framework. Our SCE consists of attribute-level contrastive embedding and class-level contrastive embedding. They aim to obtain the transferable and discriminative information, respectively, in the embedding space. We evaluate our GZSL method with semantic contrastive embedding, named SCE-GZSL, on four benchmark datasets. The results show that our SCE-GZSL method can achieve the state-of-the-art or the second-best on these datasets.	[Han, Zongyan; Fu, Zhenyong; Chen, Shuo; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab,Minist Educ, Nanjing, Peoples R China; [Han, Zongyan; Fu, Zhenyong; Chen, Shuo; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing, Peoples R China; [Chen, Shuo] RIKEN Ctr Adv Intelligence Project, Tokyo, Japan	Nanjing University of Science & Technology; Nanjing University of Science & Technology; RIKEN	Fu, ZY; Yang, J (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab,Minist Educ, Nanjing, Peoples R China.; Fu, ZY; Yang, J (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing, Peoples R China.	hanzy@njust.edu.cn; z.fu@njust.edu.cn; shuo.chen.ya@riken.jp; csjyang@njust.edu.cn			National Science Foundation of China [U1713208, 61876085]; China Postdoctoral Science Foundation [2017M621748, 2020M681606, 2019T120430]; Postgraduate Research & Practice Innovation Program of Jiangsu Province [KYCX21_0302]	National Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); Postgraduate Research & Practice Innovation Program of Jiangsu Province	We would like to show our greatest appreciation to all editors and reviewers for their constructive comments on our paper. This work was supported by the National Science Foundation of China (Grant Nos. U1713208 and 61876085) and the China Postdoctoral Science Foundation (Grant Nos. 2017M621748, 2020M681606 and 2019T120430). This work was also supported by the Postgraduate Research & Practice Innovation Program of Jiangsu Province (No. KYCX21_0302).	Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793; Arjovsky M, 2017, PR MACH LEARN RES, V70; Atzmon Y, 2019, PROC CVPR IEEE, P11663, DOI 10.1109/CVPR.2019.01194; Bucher M, 2017, IEEE INT CONF COMP V, P2666, DOI 10.1109/ICCVW.2017.308; Bucher M, 2016, LECT NOTES COMPUT SC, V9909, P730, DOI 10.1007/978-3-319-46454-1_44; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Chen LC, 2018, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2018.00422; Chen S., 2021, CVPR; Chen Shiming, 2021, NEURIPS; Chen T, 2020, PR MACH LEARN RES, V119; Chen Xingyu, 2020, ECCV; Chen Yen-Chun, 2020, ECCV; Chen Zipei, 2021, ICCV; Huynh D, 2020, PROC CVPR IEEE, P4482, DOI 10.1109/CVPR42600.2020.00454; Dinh L., 2016, ARXIV; Dinh L., 2014, ARXIV; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2; Frome Andrea, 2013, NEURIPS; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Fu YW, 2014, LECT NOTES COMPUT SC, V8690, P584, DOI 10.1007/978-3-319-10605-2_38; Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007; Fu ZY, 2015, PROC CVPR IEEE, P2635, DOI 10.1109/CVPR.2015.7298879; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089; Jiang HJ, 2019, IEEE I CONF COMP VIS, P9764, DOI 10.1109/ICCV.2019.00986; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Keshari Rohit, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13297, DOI 10.1109/CVPR42600.2020.01331; Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kingma DP, 2018, ADV NEUR IN, V31; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170; Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758; Li K, 2019, IEEE I CONF COMP VIS, P3582, DOI 10.1109/ICCV.2019.00368; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu L, 2021, POSTGRAD MED, V133, P265, DOI 10.1080/00325481.2020.1803666; Liu SH, 2020, PROC CVPR IEEE, P2016, DOI 10.1109/CVPR42600.2020.00209; Liu SJ, 2018, ADV NEUR IN, V31; Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680; Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264; Mandal D, 2019, PROC CVPR IEEE, P9977, DOI 10.1109/CVPR.2019.01022; Mikolov T., 2013, ARXIV; Mishra A, 2018, IEEE COMPUT SOC CONF, P2269, DOI 10.1109/CVPRW.2018.00294; Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29; Oord A.v.d., 2018, ARXIV; Palatucci Mark, 2009, ADV NEURAL INFORM PR, P1410; Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Romera-Paredes B, 2017, ADV COMPUT VIS PATT, P11, DOI 10.1007/978-3-319-50077-5_2; Sariyildiz MB, 2019, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2019.00227; Schonfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844; Shaobo Min, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12661, DOI 10.1109/CVPR42600.2020.01268; Shen Yu, 2020, P EUR C COMP VIS ECC; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1016/J.PATREC.2014.01.008; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tong B, 2019, PROC CVPR IEEE, P11459, DOI 10.1109/CVPR.2019.01173; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450; Vyas Maunil R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P70, DOI 10.1007/978-3-030-58577-8_5; Wah C., 2011, TECH REP; Wang C., 2021, NEURIPS; Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717; Wang YC, 2017, ADV NEUR IN, V30; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961; Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676; Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474; Zhu YZ, 2019, IEEE I CONF COMP VIS, P9843, DOI 10.1109/ICCV.2019.00994; Zongyan Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12862, DOI 10.1109/CVPR42600.2020.01288	90	0	0	7	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2606	2622		10.1007/s11263-022-01656-y	http://dx.doi.org/10.1007/s11263-022-01656-y		AUG 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ					2022-12-18	WOS:000842360600001
J	Sinha, S; Ohashi, H; Nakamura, K				Sinha, Saptarshi; Ohashi, Hiroki; Nakamura, Katsuyuki			Class-Difficulty Based Methods for Long-Tailed Visual Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Class-imbalance; Weighted-loss; Data re-sampling; Class-wise difficulty; Image classification; Object detection; Instance segmentation; Video-action classification	SMOTE	Long-tailed datasets are very frequently encountered in real-world use cases where few classes or categories (known as majority or head classes) have higher number of data samples compared to the other classes (known as minority or tail classes). Training deep neural networks on such datasets gives results biased towards the head classes. So far, researchers have come up with multiple weighted loss and data re-sampling techniques in efforts to reduce the bias. However, most of such techniques assume that the tail classes are always the most difficult classes to learn and therefore need more weightage or attention. Here, we argue that the assumption might not always hold true. Therefore, we propose a novel approach to dynamically measure the instantaneous difficulty of each class during the training phase of the model. Further, we use the difficulty measures of each class to design a novel weighted loss technique called 'class-wise difficulty based weighted (CDB-W) loss' and a novel data sampling technique called 'class-wise difficulty based sampling (CDB-S)'. To verify the wide-scale usability of our CDB methods, we conducted extensive experiments on multiple tasks such as image classification, object detection, instance segmentation and video-action classification. Results verified that CDB-W loss and CDB-S could achieve state-of-the-art results on many class-imbalanced datasets such as ImageNet-LT, LVIS and EGTEA, that resemble real-world use cases.	[Sinha, Saptarshi; Ohashi, Hiroki] Hitachi Ltd, Intelligent Vis Res Dept, Kokubunji, Tokyo 1858601, Japan; [Nakamura, Katsuyuki] Hitachi Ltd, R&D Grp, Kokubunji, Tokyo 1858601, Japan	Hitachi Limited; Hitachi Limited	Sinha, S (corresponding author), Hitachi Ltd, Intelligent Vis Res Dept, Kokubunji, Tokyo 1858601, Japan.	saptarshi.sinha.hx@hitachi.com; hiroki.ohashi.uo@hitachi.com; katsuyuki.nakamura.xv@hitachi.com		sinha, saptarshi/0000-0002-5207-1551				Barua S, 2014, IEEE T KNOWL DATA EN, V26, P405, DOI 10.1109/TKDE.2012.232; Burges, 1998, MNIST DATABASE HANDW; Cao KD, 2019, ADV NEUR IN, V32; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fernandez A., 2018, LEARNING IMBALANCED, VVolume 10; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang C, 2020, IEEE T PATTERN ANAL, V42, P2781, DOI 10.1109/TPAMI.2019.2914680; Jamal M. A., 2020, IEEE C COMPUTER VISI; Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168; Kang B., 2020, 8 INT C LEARNING REP; Kay W., 2017, ARXIV PREPRINT ARXIV; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099; Li Y, 2018, LECT NOTES COMPUT SC, V11209, P639, DOI 10.1007/978-3-030-01228-1_38; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853; Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Ren MY, 2018, PR MACH LEARN RES, V80; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shu J, 2019, ADV NEUR IN, V32; Sinha Saptarshi, 2020, P AS C COMP VIS; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Tao Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P728, DOI 10.1007/978-3-030-58568-6_43; Wang T., 2019, PREPRINT; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Wang YB, 2017, ADV NEUR IN, V30	34	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2517	2531		10.1007/s11263-022-01643-3	http://dx.doi.org/10.1007/s11263-022-01643-3		AUG 2022	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI		Green Submitted, hybrid			2022-12-18	WOS:000842360600002
J	Castellano, G; Vessio, G				Castellano, Giovanna; Vessio, Gennaro			A Deep Learning Approach to Clustering Visual Arts	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cultural heritage; Digital humanities; Visual arts; Computer vision; Autoencoders; Deep clustering	REPRESENTATION; DISCOVERY	Clustering artworks is difficult for several reasons. On the one hand, recognizing meaningful patterns based on domain knowledge and visual perception is extremely hard. On the other hand, applying traditional clustering and feature reduction techniques to the highly dimensional pixel space can be ineffective. To address these issues, in this paper we propose DELIUS: a DEep learning approach to cLustering vIsUal artS. The method uses a pre-trained convolutional network to extract features and then feeds these features into a deep embedded clustering model, where the task of mapping the input data to a latent space is jointly optimized with the task of finding a set of cluster centroids in this latent space. Quantitative and qualitative experimental results show the effectiveness of the proposed method. DELIUS can be useful for several tasks related to art analysis, in particular visual link retrieval and historical knowledge discovery in painting datasets.	[Castellano, Giovanna; Vessio, Gennaro] Univ Bari Aldo Moro, Dept Comp Sci, Bari, Italy	Universita degli Studi di Bari Aldo Moro	Vessio, G (corresponding author), Univ Bari Aldo Moro, Dept Comp Sci, Bari, Italy.	gennaro.vessio@uniba.it	Vessio, Gennaro/AAG-9890-2019	Vessio, Gennaro/0000-0002-0883-2691	Italian Ministry of University and Research [PON AIM 1852414]	Italian Ministry of University and Research(Ministry of Education, Universities and Research (MIUR))	G.V. acknowledges the financial support of the Italian Ministry of University and Research through the PON AIM 1852414 Project.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arora RS, 2012, INT C PATT RECOG, P3541; Barnard K, 2001, PROC CVPR IEEE, P434; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bhowmik D, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2507-5; Cai D, 2011, IEEE T KNOWL DATA EN, V23, P902, DOI 10.1109/TKDE.2010.165; Cai H., 2015, 2015 IEEE Power & Energy Society General Meeting, P1, DOI 10.1109/PESGM.2015.7285596; Cai H., 2015, ARXIV; Calinski T., 1974, COMMUN STAT-THEOR M, V3, P1, DOI [DOI 10.1080/03610927408827101, 10.1080/03610927408827101]; Castellano G., 2022, KNOWL-BASED SYST, V248; Castellano G, 2021, INT C PATT RECOG, P2708, DOI 10.1109/ICPR48806.2021.9412438; Castellano G, 2021, NEURAL COMPUT APPL, V33, P12263, DOI 10.1007/s00521-021-05893-z; Cetinic E, 2018, EXPERT SYST APPL, V114, P107, DOI 10.1016/j.eswa.2018.07.026; Chen LY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2459, DOI 10.1145/3343031.3350977; Cornia M, 2020, PATTERN RECOGN LETT, V129, P166, DOI 10.1016/j.patrec.2019.11.018; Crowley EJ, 2015, LECT NOTES COMPUT SC, V8925, P54, DOI 10.1007/978-3-319-16178-5_4; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Elgammal A., 2017, ARXIV; Garcia N, 2020, INT J MULTIMED INF R, V9, P17, DOI 10.1007/s13735-019-00189-4; Gonthier N, 2019, LECT NOTES COMPUT SC, V11130, P692, DOI 10.1007/978-3-030-11012-3_53; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gultepe E, 2018, J CULT HERIT, V31, P13, DOI 10.1016/j.culher.2017.11.008; Guo XF, 2017, LECT NOTES COMPUT SC, V10635, P373, DOI 10.1007/978-3-319-70096-0_39; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hogan A, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3447772; Huang G., 2017, P IEEE C COMPUTER VI, P4700, DOI DOI 10.1109/CVPR.2017.243; James Melville, 2020, Arxiv, DOI arXiv:1802.03426; Karayev S., 2013, ARXIV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Leder H, 2004, BRIT J PSYCHOL, V95, P489, DOI 10.1348/0007126042369811; Lu R, 2019, IEEE-ACM T AUDIO SPE, V27, P1697, DOI 10.1109/TASLP.2019.2928140; Ren YZ, 2019, NEUROCOMPUTING, V325, P121, DOI 10.1016/j.neucom.2018.10.016; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Saleh B, 2016, MULTIMED TOOLS APPL, V75, P3565, DOI 10.1007/s11042-014-2193-x; Shen X., 2019, ARXIV; Spehr M., 2009, COMPUTATIONAL AESTHE, P57; Strezoski G., 2017, ARXIV; Tan WR, 2019, IEEE T IMAGE PROCESS, V28, P394, DOI 10.1109/TIP.2018.2866698; Tan WR, 2016, IEEE IMAGE PROC, P3703, DOI 10.1109/ICIP.2016.7533051; Tomei M, 2019, PROC CVPR IEEE, P5842, DOI 10.1109/CVPR.2019.00600; Vaigh C. B. E., 2021, ARXIV PREPRINT ARXIV; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; van Noord N, 2015, IEEE SIGNAL PROC MAG, V32, P46, DOI 10.1109/MSP.2015.2406955; Vinh NX, 2010, J MACH LEARN RES, V11, P2837; Xie JY, 2016, PR MACH LEARN RES, V48; Yang B, 2017, PR MACH LEARN RES, V70	55	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2590	2605		10.1007/s11263-022-01664-y	http://dx.doi.org/10.1007/s11263-022-01664-y		AUG 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ		hybrid, Green Submitted			2022-12-18	WOS:000841075300001
J	Pei, WJ; Feng, X; Fu, CM; Cao, Q; Lu, GM; Tai, YW				Pei, Wenjie; Feng, Xin; Fu, Canmiao; Cao, Qiong; Lu, Guangming; Tai, Yu-Wing			Learning Sequence Representations by Non-local Recurrent Neural Memory	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sequence representation learning; Non-local; recurrent; Neural memory; Long-range temporal dependencies	NETWORKS; MODELS	The key challenge of sequence representation learning is to capture the long-range temporal dependencies. Typical methods for supervised sequence representation learning are built upon recurrent neural networks to capture temporal dependencies. One potential limitation of these methods is that they only model one-order information interactions explicitly between adjacent time steps in a sequence, hence the high-order interactions between nonadjacent time steps are not fully exploited. It greatly limits the capability of modeling the long-range temporal dependencies since the temporal features learned by one-order interactions cannot be maintained for a long term due to temporal information dilution and gradient vanishing. To tackle this limitation, we propose the non-local recurrent neural memory (NRNM) for supervised sequence representation learning, which performs non-local operations by means of self-attention mechanism to learn full-order interactions within a sliding temporal memory block and models global interactions between memory blocks in a gated recurrent manner. Consequently, our model is able to capture long-range dependencies. Besides, the latent high-level features contained in high-order interactions can be distilled by our model. We validate the effectiveness and generalization of our NRNM on three types of sequence applications across different modalities, including sequence classification, step-wise sequential prediction and sequence similarity learning. Our model compares favorably against other state-of-the-art methods specifically designed for each of these sequence applications.	[Pei, Wenjie; Feng, Xin; Lu, Guangming] Harbin Inst Technol Shenzhen, Dept Comp Sci, Shenzhen 518057, Guangdong, Peoples R China; [Fu, Canmiao] Tecent, Shenzhen, Peoples R China; [Cao, Qiong] JD Explore Acad, Beijing, Peoples R China; [Tai, Yu-Wing] Kuaishou Technol, Beijing, Peoples R China	Harbin Institute of Technology	Lu, GM (corresponding author), Harbin Inst Technol Shenzhen, Dept Comp Sci, Shenzhen 518057, Guangdong, Peoples R China.	wenjiecoder@outlook.com; fengx_hit@outlook.com; fcm@pku.edu.cn; mathqiong2012@gmail.com; luguangm@hit.edu.cn; yuwing@gmail.com		Lu, Guangming/0000-0003-1578-2634	NSFC [U2013210, 62006060, 62176077]; Guangdong Basic and Applied Basic Research Foundation [2019Bl515120055, 2022A1515010306]; Shenzhen Key Technical Project [2020N046]; Shenzhen Fundamental Research Fund [JCYJ20210324132210025]; Shenzhen Stable Support Plan Fund for Universities [GXWD20201230155427003-20200824125730001]; Medical Biometrics Perception and Analysis Engineering Laboratory, Shenzhen, China; Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies [2022B1212010005]	NSFC(National Natural Science Foundation of China (NSFC)); Guangdong Basic and Applied Basic Research Foundation; Shenzhen Key Technical Project; Shenzhen Fundamental Research Fund; Shenzhen Stable Support Plan Fund for Universities; Medical Biometrics Perception and Analysis Engineering Laboratory, Shenzhen, China; Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies	This work was supported in part by the NSFC fund (U2013210, 62006060, 62176077), in part by the Guangdong Basic and Applied Basic Research Foundation under Grant (2019Bl515120055, 2022A1515010306), in part by the Shenzhen Key Technical Project under Grant 2020N046, in part by the Shenzhen Fundamental Research Fund under Grant (JCYJ20210324132210025), in part by the Shenzhen Stable Support Plan Fund for Universities (GXWD20201230155427003-20200824125730001), in part by the Medical Biometrics Perception and Analysis Engineering Laboratory, Shenzhen, China, and in part by Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies (2022B1212010005).	Ba J. L., 2016, ARXIV PREPRINT ARXIV; Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338; Brock A., 2019, INT C LEARNING REPRE; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Cao Yue, 2019, P IEEE CVF INT C COM; Carion N., 2020, COMPUTER VISION ECCV, P213, DOI DOI 10.1007/978-3-030-58452-8_13; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734; Dai AM, 2015, ADV NEUR IN, V28; Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dieng Adji B., 2017, 5 INT C LEARN REPR I; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Fu CM, 2019, IEEE I CONF COMP VIS, P6320, DOI 10.1109/ICCV.2019.00641; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Grave Edouard, 2017, 5 INT C LEARN REPR I; Graves A., 2014, ARXIV; Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Johnson R., 2014, ARXIV; Johnson R, 2016, PR MACH LEARN RES, V48; Kingma D.P, P 3 INT C LEARNING R; Kolen J.F., 2001, GRADIENT FLOW RECURR, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Krueger D., 2017, ICLR; Kumar A, 2016, PR MACH LEARN RES, V48; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115; Li Z, 2016, ARXIV; Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Maas A., 2011, P 49 ANN M ASS COMPU, P142; McCann B, 2017, ADV NEUR IN, V30; Minghao Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P191, DOI 10.1007/978-3-030-58555-6_12; Miyato Takeru, 2017, INT C LEARN REPR ICL; Morency LP, 2007, PROC CVPR IEEE, P2526; Mueller J, 2016, AAAI CONF ARTIF INTE, P2786; Pei W., 2016, ARXIV; Pei WJ, 2018, IEEE T NEUR NET LEAR, V29, P920, DOI 10.1109/TNNLS.2017.2651018; Pei WJ, 2017, PROC CVPR IEEE, P820, DOI 10.1109/CVPR.2017.94; Peng J., 2009, NEURIPS; Pollastri G, 2002, PROTEINS, V47, P228, DOI 10.1002/prot.10082; Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Radford A., 2017, ARXIV; Ramachandran P, 2019, ADV NEUR IN, V32; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sak H, 2014, INTERSPEECH, P338; Santoro A, 2018, NEURIPS, DOI [10.48550/arXiv.1806.01822, DOI 10.48550/ARXIV.1806.01822]; Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312; Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Simonyan K, 2014, ADV NEUR IN, V27; Soltani R., 2016, ARXIV; Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328; Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625; Sukhbaatar S, 2015, ADV NEUR IN, V28; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan CQ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4411; Tu JH, 2018, IEEE IMAGE PROC, P3478, DOI 10.1109/ICIP.2018.8451608; van der Maaten L.J.P., 2011, P INT C ART INT STAT; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang GL, 2003, BIOINFORMATICS, V19, P1589, DOI 10.1093/bioinformatics/btg224; Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang SB, 2016, SCI REP-UK, V6, DOI 10.1038/srep19444; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wang ZY, 2010, IEEE INT C BIOINFORM, P109, DOI 10.1109/BIBM.2010.5706547; Weston J., 2015, ICLR; Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037; Xia YC, 2018, PR MACH LEARN RES, V80; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Xiong CM, 2016, PR MACH LEARN RES, V48; Yan A, 2019, PROC CVPR IEEE, P7914, DOI 10.1109/CVPR.2019.00811; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang RQ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4699; Yang Zichao, 2018, ADV NEURAL INFORM PR; Yu F., 2016, P ICLR 2016; Zhang C, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5849; Zhang H, 2019, PR MACH LEARN RES, V97; Zhang PF, 2018, LECT NOTES COMPUT SC, V11213, P136, DOI 10.1007/978-3-030-01240-3_9; Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2017.233; Zhao H, 2020, P IEEECVF C COMPUTER, P10076, DOI 10.1109/CVPR42600.2020.01009; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49; Zhou J, 2014, INT CONF MACH LEARN, P71, DOI 10.1109/ICMLC.2014.7009094; Zhou JY, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2067-8	92	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2532	2552		10.1007/s11263-022-01648-y	http://dx.doi.org/10.1007/s11263-022-01648-y		AUG 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI					2022-12-18	WOS:000840388000001
J	Hu, XL; Tang, CF; Chen, H; Li, X; Li, JM; Zhang, ZX				Hu, Xiaolin; Tang, Chufeng; Chen, Hang; Li, Xiao; Li, Jianmin; Zhang, Zhaoxiang			Improving Image Segmentation with Boundary Patch Refinement	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image Segmentation; Boundary Refinement; Instance Segmentation; Semantic Segmentation; Panoptic Segmentation		Tremendous efforts have been made on image segmentation but the mask quality is still not satisfactory. The boundaries of predicted masks are usually imprecise due to the low spatial resolution of feature maps and the imbalance problem caused by the extremely low proportion of boundary pixels. To address these issues, we propose a conceptually simple yet effective post-processing refinement framework, termed BPR, to improve the boundary quality of the prediction of any image segmentation model. Following the idea of looking closer to segment boundaries better, we extract and refine a series of small boundary patches along the predicted boundaries. The refinement is accomplished by a boundary patch refinement network at the higher resolution. The trained BPR model can be easily transferred to refine the results of other models as well. Extensive experiments show that the proposed BPR framework yields significant improvements on the semantic, instance, and panoptic segmentation tasks over a variety of baselines on the Cityscapes dataset.	[Hu, Xiaolin; Tang, Chufeng; Chen, Hang; Li, Xiao; Li, Jianmin] Tsinghua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, BNRist,THUBosch JCML Ctr,Inst Artificial Intellig, Beijing, Peoples R China; [Hu, Xiaolin] Chinese Inst Brain Res CIBR, Beijing, Peoples R China; [Zhang, Zhaoxiang] Univ Chinese Acad Sci, HKISI CAS, Chinese Acad Sci, Inst Automat,Ctr Artificial Intelligence & Robot, Beijing, Peoples R China	Tsinghua University; Chinese Academy of Sciences; Institute of Automation, CAS; University of Chinese Academy of Sciences, CAS	Hu, XL (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, BNRist,THUBosch JCML Ctr,Inst Artificial Intellig, Beijing, Peoples R China.; Hu, XL (corresponding author), Chinese Inst Brain Res CIBR, Beijing, Peoples R China.	xlhu@mail.tsinghua.edu.cn	LI, JIAN/GRY-2197-2022; li, jian/GSE-0245-2022; Li, Jing/GYU-5036-2022	Hu, Xiaolin/0000-0002-4907-7354	National Natural Science Foundation of China [62061136001, 61836014, U19B2034]; National Science and Technology Major Project [2018ZX01028-102]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Science and Technology Major Project	This work was supported by the National Natural Science Foundation of China (Nos. 62061136001, 61836014, and U19B2034), the National Science and Technology Major Project (No. 2018ZX01028-102), and THU-Bosch JCML center.	Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305; Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925; Bowen Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12472, DOI 10.1109/CVPR42600.2020.01249; Chen Hao, 2020, CVPR; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen ZX, 2021, IEEE T IMAGE PROCESS, V30, P431, DOI 10.1109/TIP.2020.3037536; Cheng BW, 2021, PROC CVPR IEEE, P15329, DOI 10.1109/CVPR46437.2021.01508; Cheng Bowen, 2021, ARXIV211201527; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; deGeus D., 2018, ARXIV; Deng C., 2020, IEEE T PATTERN RECOG; Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gao NY, 2019, IEEE I CONF COMP VIS, P642, DOI 10.1109/ICCV.2019.00073; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7; Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656; Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963; Kirillov A, 2017, PROC CVPR IEEE, P7322, DOI 10.1109/CVPR.2017.774; Kirillov Alexander, 2020, CVPR; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Lazarow Justin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10717, DOI 10.1109/CVPR42600.2020.01073; Li, 2019, NVIDIA RTX 2080 TI D; Li YW, 2019, PROC CVPR IEEE, P7019, DOI 10.1109/CVPR.2019.00719; Liang Justin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9128, DOI 10.1109/CVPR42600.2020.00915; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu S, 2017, IEEE I CONF COMP VIS, P3516, DOI 10.1109/ICCV.2017.378; Liu Y, 2020, AAAI CONF ARTIF INTE, V34, P13172; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Minaee S., 2021, IEEE T PATTERN RECOG; MMSegmentation, 2020, MMSEGMENTATION OPENM; Mohan R, 2021, INT J COMPUT VISION, V129, P1551, DOI 10.1007/s11263-021-01445-z; Myungchul Kim, 2021, 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), P928, DOI 10.1109/WACV48630.2021.00097; Poudel R.P., 2019, ARXIV; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ren JW, 2021, AAAI CONF ARTIF INTE, V35, P2477; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sida Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8530, DOI 10.1109/CVPR42600.2020.00856; Sofiiuk K, 2019, IEEE I CONF COMP VIS, P7354, DOI 10.1109/ICCV.2019.00745; Tang CF, 2021, PROC CVPR IEEE, P13921, DOI 10.1109/CVPR46437.2021.01371; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Tianheng Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P660, DOI 10.1007/978-3-030-58568-6_39; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang, 2022, ASS ADVANCEMENT ARTI, P2397; Wang HY, 2021, PROC CVPR IEEE, P5459, DOI 10.1109/CVPR46437.2021.00542; Wang X., 2020, ADV NEURAL INF PROCE, V33, P17721; Wang YP, 2019, IEEE T IMAGE PROCESS, V28, P2813, DOI 10.1109/TIP.2019.2891055; Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P435, DOI 10.1007/978-3-030-58520-4_26; Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38; Xu WQ, 2019, IEEE I CONF COMP VIS, P5167, DOI 10.1109/ICCV.2019.00527; Yang Tien-Ju, 2019, ARXIV190205093; Ying H., 2019, ARXIV; Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P489, DOI 10.1007/978-3-030-58610-2_29; Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11; Zhang Gang, 2021, P IEEE CVF C COMP VI, P6861; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17; Zhou P., 2020, P IEEE CVF C COMP VI, P10558; Zhou SH, 2020, IEEE T IMAGE PROCESS, V29, P461, DOI 10.1109/TIP.2019.2919937	71	0	0	6	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2571	2589		10.1007/s11263-022-01662-0	http://dx.doi.org/10.1007/s11263-022-01662-0		AUG 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ					2022-12-18	WOS:000840004200002
J	Zhang, S; Zhang, J; Tao, DC				Zhang, Sen; Zhang, Jing; Tao, Dacheng			Information-Theoretic Odometry Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Odometry learning; Simultaneous localization and mapping; Information bottleneck; Generalization bound	VISUAL-INERTIAL ODOMETRY; FRAMEWORK; VISION	In this paper, we propose a unified information theoretic framework for learning-motivated methods aimed at odometry estimation, a crucial component of many robotics and vision tasks such as navigation and virtual reality where relative camera poses are required in real time. We formulate this problem as optimizing a variational information bottleneck objective function, which eliminates pose-irrelevant information from the latent representation. The proposed framework provides an elegant tool for performance evaluation and understanding in information-theoretic language. Specifically, we bound the generalization errors of the deep information bottleneck framework and the predictability of the latent representation. These provide not only a performance guarantee but also practical guidance for model design, sample collection, and sensor selection. Furthermore, the stochastic latent representation provides a natural uncertainty measure without the needs for extra structures or computations. Experiments on two well-known odometry datasets demonstrate the effectiveness of our method.	[Zhang, Sen; Zhang, Jing; Tao, Dacheng] Univ Sydney, Sydney, NSW, Australia	University of Sydney	Zhang, J (corresponding author), Univ Sydney, Sydney, NSW, Australia.	szha2609@uni.sydney.edu.au; jing.zhang1@sydney.edu.au; dacheng.tao@sydney.edu.au		ZHANG, JING/0000-0001-6595-7661; Zhang, Sen/0000-0002-8065-5095	ARC [FL-170100117, DP-180103424, IC-190100031, LE-200100049]	ARC(Australian Research Council)	This work is supported by ARC FL-170100117, DP-180103424, IC-190100031, and LE-200100049.	Alexander A. A., 2017, ICLR 2017 INT C LEAR; Bengio Y., 2014, ARXIV14061078; Bian JW, 2019, ADV NEUR IN, V32; Buesing L., 2018, ARXIV; Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033; Cabon Yohann, 2020, ARXIV; Chaudhari P, 2019, J STAT MECH-THEORY E, V2019, DOI 10.1088/1742-5468/ab39d9; Chen CG., 2020, ARXIV; Chen CH, 2019, PROC CVPR IEEE, P10534, DOI 10.1109/CVPR.2019.01079; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Clark R, 2017, AAAI CONF ARTIF INTE, P3995; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022; Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8; Gal Y, 2016, PR MACH LEARN RES, V48; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Gilitschenski I., 2019, INT C LEARN REPR; Goyal A., 2019, INT C LEARN REPR; Hafner D., 2020, ICLR; Hafner D, 2019, PR MACH LEARN RES, V97; Hu JS, 2014, IEEE INT CONF ROBOT, P3963, DOI 10.1109/ICRA.2014.6907434; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; KHATRI CG, 1977, J ROY STAT SOC B MET, V39, P95; Kingma D.P., 2013, P 2 INT C LEARN REPR; Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813; Loquercio A, 2020, IEEE ROBOT AUTOM LET, V5, P3153, DOI 10.1109/LRA.2020.2974682; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024; Paszke A, 2019, ADV NEUR IN, V32; Peretroukhin V, 2018, IEEE ROBOT AUTOM LET, V3, P2424, DOI 10.1109/LRA.2017.2778765; Poole B, 2019, PR MACH LEARN RES, V97; Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252; Shwartz-Ziv R., 2017, ARXIV; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Taketomi T., 2017, IBSJ T COMPUT VIS AP, V9, P16, DOI 10.1186/s41074-017-0027-2; Teed Zachary, 2020, ECCV, DOI DOI 10.1007/978-3-030-58536-5_24; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Tishby Naftali, 2000, PHYSICS0004057 ARXIV; Vera M, 2018, IEEE INT SYMP INFO, P1580; Wang S, 2018, INT J ROBOT RES, V37, P513, DOI 10.1177/0278364917734298; Wang Sen, 2017, 2017 IEEE INT C ROB, P2043, DOI 10.1109/ICRA.2017.7989236; Xu A., 2017, ADV NEURAL INFORM PR, P2521; Xu H., 2022, P IEEECVF C COMPUTER, P8121; Xu Y., 2021, ADV NEURAL INFORM PR, V34, P28522; Xue F, 2019, PROC CVPR IEEE, P8567, DOI 10.1109/CVPR.2019.00877; Yang N, 2020, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR42600.2020.00136; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Zhan HY, 2020, IEEE INT CONF ROBOT, P4203, DOI 10.1109/ICRA40945.2020.9197374; Zhang J, 2021, IEEE INTERNET THINGS, V8, P7789, DOI 10.1109/JIOT.2020.3039359; Zhang JW, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3109942; Zhang Q., ARXIV; Zhang S., 2022, ARXIV; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	57	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2022	130	11					2553	2570		10.1007/s11263-022-01659-9	http://dx.doi.org/10.1007/s11263-022-01659-9		AUG 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4Z7ZZ		hybrid, Green Submitted			2022-12-18	WOS:000840004200001
J	Chen, YW; Tsai, YH; Yang, MH				Chen, Yi-Wen; Tsai, Yi-Hsuan; Yang, Ming-Hsuan			Understanding Synonymous Referring Expressions via Contrastive Features	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Referring expression comprehension; Contrastive learning; Transfer learning; Synonymous sentences		Referring expression comprehension aims to localize objects identified by natural language descriptions. This is a challenging task as it requires understanding of both visual and language domains. One nature is that each object can be described by synonymous sentences with paraphrases, and such varieties in languages have critical impact on learning a comprehension model. While prior work usually treats each sentence and attends it to an object separately, we focus on learning a referring expression comprehension model that considers the property in synonymous sentences. To this end, we develop an end-to-end trainable framework to learn contrastive features on the image and object instance levels, where features extracted from synonymous sentences to describe the same object should be closer to each other after mapping to the visual domain. We conduct extensive experiments to evaluate the proposed algorithm on several benchmark datasets, and demonstrate that our method performs favorably against the state-of-the-art approaches. Furthermore, since the varieties in expressions become larger across datasets when they describe objects in different ways, we present the cross-dataset and transfer learning settings to validate the ability of our learned transferable features.	[Chen, Yi-Wen; Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95343 USA; [Tsai, Yi-Hsuan] Phiar, Redwood City, CA USA	University of California System; University of California Merced	Yang, MH (corresponding author), Univ Calif Merced, Merced, CA 95343 USA.	ychen319@ucmerced.edu; wasidennis@gmail.com; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304				Chen L, 2021, AAAI CONF ARTIF INTE, V35, P1036; Chen T, 2020, PR MACH LEARN RES, V119; Chen Yuxiao, 2019, BMVC; Chen Ze, 2020, CVPR; Deng J., 2021, ICCV; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Gan Zhe, 2020, ARXIV200606195, V33, P6616; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493; Huang BB, 2021, PROC CVPR IEEE, P16883, DOI 10.1109/CVPR46437.2021.01661; Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686; Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008; Jiasen Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10434, DOI 10.1109/CVPR42600.2020.01045; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kalantidis Yannis, 2020, ARXIV201001028; Kazemzadeh Sahar, 2014, P 2014 C EMP METH NA, P787, DOI DOI 10.3115/V1/D14-1086; Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Li X., 2020, ECCV, V12375, P121, DOI 10.1007/978-3-030-58577-88; Liao Yue, 2020, CVPR; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520; Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205; Lu JS, 2019, ADV NEUR IN, V32; Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48; Peters Matthew E., 2018, P 2018 C N AM CHAPT, V1, P2227, DOI DOI 10.18653/V1/N18-1202; Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49; Sadhu A, 2019, IEEE I CONF COMP VIS, P4693, DOI 10.1109/ICCV.2019.00479; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sibei Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P589, DOI 10.1007/978-3-030-58529-7_35; Sohn K, 2016, ADV NEUR IN, V29; van den Oord Aaron, 2019, REPRESENTATION LEARN, P4; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541; Yamaguchi M, 2017, IEEE I CONF COMP VIS, P1462, DOI 10.1109/ICCV.2017.162; Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474; Yang Ze, 2020, ECCV, P227, DOI DOI 10.1007/978-3-030-58589-1; Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478; Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375; Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5; Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437; Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041; Zhuang BH, 2018, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR.2018.00447	49	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2501	2516		10.1007/s11263-022-01647-z	http://dx.doi.org/10.1007/s11263-022-01647-z		AUG 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI		hybrid, Green Submitted			2022-12-18	WOS:000839412400001
J	Zhai, W; Luo, HC; Zhang, J; Cao, Y; Tao, DC				Zhai, Wei; Luo, Hongchen; Zhang, Jing; Cao, Yang; Tao, Dacheng			One-Shot Object Affordance Detection in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Affordance detection; One-Shot learning; Human purpose estimation and transfer		Affordance detection refers to identifying the potential action possibilities of objects in an image, which is a crucial ability for robot perception and manipulation. To empower robots with this ability in unseen scenarios, we first study the challenging one-shot affordance detection problem in this paper, i.e., given a support image that depicts the action purpose, all objects in a scene with the common affordance should be detected. To this end, we devise a One-Shot Affordance Detection Network (OSAD-Net) that firstly estimates the human action purpose and then transfers it to help detect the common affordance from all candidate images. Through collaboration learning, OSAD-Net can capture the common characteristics between objects having the same underlying affordance and learn a good adaptation capability for perceiving unseen affordances. Besides, we build a large-scale purpose-driven affordance dataset v2 (PADv2) by collecting and labeling 30k images from 39 affordance and 103 object categories. With complex scenes and rich annotations, our PADv2 dataset can be used as a test bed to benchmark affordance detection methods and may also facilitate downstream vision tasks, such as scene understanding, action recognition, and robot manipulation. Specifically, we conducted comprehensive experiments on PADv2 dataset by including 11 advanced models from several related research fields. Experimental results demonstrate the superiority of our model over previous representative ones in terms of both objective metrics and visual quality. The benchmark suite is available at https://github.com/lhc1224/OSAD_Net.	[Zhai, Wei; Luo, Hongchen; Cao, Yang] Univ Sci & Technol China, Hefei, Peoples R China; [Zhang, Jing; Tao, Dacheng] Univ Sydney, Sydney, NSW, Australia; [Tao, Dacheng] JD Explore Acad, Beijing, Peoples R China; [Cao, Yang] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; University of Sydney	Cao, Y (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.; Cao, Y (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China.	wzhai056@mail.ustc.edu.cn; lhc12@mail.ustc.edu.cn; jing.zhang1@sydney.edu.au; forrest@ustc.edu.cn; dacheng.tao@gmail.com	Cao, Yang/HGD-6463-2022		National Key R &D Program of China [2020AAA0105701]; National Natural Science Foundation of China (NSFC) [61872327]; ARC Project [FL-170100117]	National Key R &D Program of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); ARC Project(Australian Research Council)	This work was supported by National Key R &D Program of China under Grant 2020AAA0105701, National Natural Science Foundation of China (NSFC) under Grants 61872327. Dr. Jing Zhang is supported by the ARC Project FL-170100117.	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Nguyen A, 2017, IEEE INT C INT ROBOT, P5908; [Anonymous], ARXIV; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Cai JJ, 2015, IEEE T IMAGE PROCESS, V24, P261, DOI 10.1109/TIP.2014.2372616; Chen J., 2019, ARXIV; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen W, 2021, ARXIV; Chuang CY, 2018, PROC CVPR IEEE, P975, DOI 10.1109/CVPR.2018.00108; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deng SH, 2021, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR46437.2021.00182; DongN Xing EP, 2018, P BRIT MACHINE VI C; Dosovitskiy A., 2020, ARXIV201011929; Fan D.P., 2018, INT JOINT C ARTIFICI; Fan DP, 2021, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412; Fang K, 2018, PROC CVPR IEEE, P2139, DOI 10.1109/CVPR.2018.00228; Finn C, 2017, PR MACH LEARN RES, V70; Gao W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2866, DOI 10.1109/ICCV48922.2021.00288; Gibson JJ., 1977, PERCEIVING ACTING KN, P67, DOI DOI 10.2307/1421838; Hassan M., 2015, LECT NOTES COMPUTER, P220; Hassanin M., 2018, ARXIV; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hermans T., 2011, IEEE INT C ROB AUT I, P181; Ho J, 2016, ADV NEUR IN, V29; Howard A.G., 2017, MOBILENETS EFFICIENT; Johnander J., 2021, ARXIV; Kai Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13467, DOI 10.1109/CVPR42600.2020.01348; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Kjellstrom H, 2011, COMPUT VIS IMAGE UND, V115, P81, DOI 10.1016/j.cviu.2010.08.002; Kuan F, 2020, INT J ROBOT RES, V39, P202, DOI 10.1177/0278364919872545; Le Meur O, 2007, VISION RES, V47, P2483, DOI 10.1016/j.visres.2007.06.015; Li Gen, 2021, P IEEE CVF C COMP VI, P8334; Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926; Li XT, 2019, PROC CVPR IEEE, P12360, DOI 10.1109/CVPR.2019.01265; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu L., 2022, ARXIV; Luo H., 2021, INT JOINT C ARTIFICI; Luo H., 2021, ARXIV; Luo H., 2022, IEEE C COMPUTER VISI; Mi JP, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.00026; Myers A, 2015, IEEE INT CONF ROBOT, P1374, DOI 10.1109/ICRA.2015.7139369; Nagarajan T., 2020, ARXIV; Nagarajan T, 2019, IEEE I CONF COMP VIS, P8687, DOI 10.1109/ICCV.2019.00878; Patro S, 2015, ARXIV; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Qi Qian, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12161, DOI 10.1109/CVPR42600.2020.01218; Qi SY, 2017, IEEE I CONF COMP VIS, P1173, DOI 10.1109/ICCV.2017.132; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766; Ramakrishnan SK, 2021, INT J COMPUT VISION, V129, P1616, DOI 10.1007/s11263-021-01437-z; Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967; Ravi S., 2017, INT C LEARN REPR, P12; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Rusu A. A., 2018, ARXIV; Sawatzky J, 2017, IEEE INT CONF COMP V, P1383, DOI 10.1109/ICCVW.2017.164; Sawatzky J, 2017, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2017.552; Shaban A., 2017, ARXIV; Shang-Hua Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P702, DOI 10.1007/978-3-030-58539-6_42; Snell J, 2017, ADV NEUR IN, V30; Song HO, 2016, IEEE T AUTOM SCI ENG, V13, P798, DOI 10.1109/TASE.2015.2396014; Stark M, 2008, LECT NOTES COMPUT SC, V5008, P435; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Do TT, 2018, IEEE INT CONF ROBOT, P5882; Thermos S, 2017, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2017.13; Tian ZH, 2020, IEEE INTERNET THINGS, V7, P3901, DOI [10.1109/TPAMI.2020.3013717, 10.1109/JIOT.2019.2951620]; Vu TH, 2014, LECT NOTES COMPUT SC, V8693, P421, DOI 10.1007/978-3-319-10602-1_28; Ugur E, 2014, J IEEE I C DEVELOP L, P476, DOI 10.1109/DEVLRN.2014.6983026; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang HC, 2021, IEEE WINT CONF APPL, P525, DOI 10.1109/WACV48630.2021.00057; Wang Jingdong, 2020, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2020.2983686, 10.1109/tpami.2020.2983686]; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wang XL, 2017, PROC CVPR IEEE, P3366, DOI 10.1109/CVPR.2017.359; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Wei P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1297; Wu P., 2022, IEEE C COMPUTER VISI; Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403; Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753; Xu Yufei, 2021, INT C NEURAL INFORM; Yamanobe N, 2017, ADV ROBOTICS, V31, P1086, DOI 10.1080/01691864.2017.1394912; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536; Zhang J, 2021, INT J COMPUT VISION, V129, P2639, DOI 10.1007/s11263-021-01482-8; Zhang J, 2021, IEEE INTERNET THINGS, V8, P7789, DOI 10.1109/JIOT.2020.3039359; Zhang Q., 2022, ARXIV; Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887; Zhao X, 2020, NEURAL COMPUT APPL, V32, P14321, DOI 10.1007/s00521-019-04336-0; Zhong XB, 2021, INT J COMPUT VISION, V129, P1910, DOI 10.1007/s11263-021-01458-8; Zhu K, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1019; Zhu K, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4461; Zhu YX, 2015, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2015.7298903; Zhu YK, 2014, LECT NOTES COMPUT SC, V8690, P408, DOI 10.1007/978-3-319-10605-2_27	104	0	0	9	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2472	2500		10.1007/s11263-022-01642-4	http://dx.doi.org/10.1007/s11263-022-01642-4		AUG 2022	29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI		Green Submitted			2022-12-18	WOS:000837532600001
J	Tian, Y; Yan, YC; Zhai, GT; Guo, GD; Gao, ZY				Tian, Yuan; Yan, Yichao; Zhai, Guangtao; Guo, Guodong; Gao, Zhiyong			EAN: Event Adaptive Network for Enhanced Action Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; Dynamic neural networks; Vision transformers; Motion representation		Efficiently modeling spatial-temporal information in videos is crucial for action recognition. To achieve this goal, state-of-the-art methods typically employ the convolution operator and the dense interaction modules such as non-local blocks. However, these methods cannot accurately fit the diverse events in videos. On the one hand, the adopted convolutions are with fixed scales, thus struggling with events of various scales. On the other hand, the dense interaction modeling paradigm only achieves sub-optimal performance as action-irrelevant parts bring additional noises for the final prediction. In this paper, we propose a unified action recognition framework to investigate the dynamic nature of video content by introducing the following designs. First, when extracting local cues, we generate the spatial-temporal kernels of dynamic-scale to adaptively fit the diverse events. Second, to accurately aggregate these cues into a global video representation, we propose to mine the interactions only among a few selected foreground objects by a Transformer, which yields a sparse paradigm. We call the proposed framework as Event Adaptive Network because both key designs are adaptive to the input video content. To exploit the short-term motions within local segments, we propose a novel and efficient Latent Motion Code module, further improving the performance of the framework. Extensive experiments on several large-scale video datasets, e.g., Something-to-Something V1 &V2, Kinetics, and Diving48, verify that our models achieve state-of-the-art or competitive performances at low FLOPs. Codes are available at: https://github.com/tianyuan168326/EAN-Pytorch.	[Tian, Yuan; Yan, Yichao; Zhai, Guangtao; Guo, Guodong; Gao, Zhiyong] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai, Peoples R China; [Yan, Yichao] Shanghai Jiao Tong Univ, AI Inst, Shanghai, Peoples R China	Shanghai Jiao Tong University; Shanghai Jiao Tong University	Yan, YC; Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai, Peoples R China.; Yan, YC (corresponding author), Shanghai Jiao Tong Univ, AI Inst, Shanghai, Peoples R China.	ee_tianyuan@sjtu.edu.cn; yanyichao@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn; guoguodong01@baidu.com; zhiyong.gao@sjtu.edu.cn	Yan, Yichao/ADT-5511-2022	Yan, Yichao/0000-0003-3209-8965	NSFC [U19B2035, 61831015]; National Key R &D Program of China [2021YFE0206700]; Shanghai Municipal Science and Technology Major Project [2021SHZDZX0102]; CAAI-Huawei MindSpore Open Fund	NSFC(National Natural Science Foundation of China (NSFC)); National Key R &D Program of China; Shanghai Municipal Science and Technology Major Project; CAAI-Huawei MindSpore Open Fund	This work was supported by NSFC (61831015), National Key R &D Program of China (2021YFE0206700), NSFC (U19B2035), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and CAAI-Huawei MindSpore Open Fund.	Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676; Bensch R, 2017, INT J COMPUT VISION, V122, P502, DOI 10.1007/s11263-016-0934-1; Bertasius G., 2021, ARXIV; Bertasius G., 2018, ARXIV; Bulat Adrian, 2021, NEURIPS; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen X, 2021, INT J COMPUT VISION, V129, P2846, DOI 10.1007/s11263-021-01486-4; Cherian A, 2019, INT J COMPUT VISION, V127, P340, DOI 10.1007/s11263-018-1111-5; Cong Yuren, 2021, P IEEECVF INT C COMP, P16372; De Brabandere B, 2016, ADV NEUR IN, V29; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy A., 2020, ARXIV201011929; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fan H., 2021, ARXIV; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2020, INT J COMPUT VISION, V128, P420, DOI 10.1007/s11263-019-01225-w; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Ferryman JM, 2000, INT J COMPUT VISION, V37, P187, DOI 10.1023/A:1008155721192; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; Girdhar R., 2021, P IEEECVF INT C COMP, P13505; Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; Hara K, 2017, IEEE INT CONF COMP V, P3154, DOI 10.1109/ICCVW.2017.373; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heeseung Kwon, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P345, DOI 10.1007/978-3-030-58517-4_21; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209; Kanojia G, 2019, IEEE COMPUT SOC CONF, P2467, DOI 10.1109/CVPRW.2019.00302; Khowaja SA, 2020, INT J COMPUT VISION, V128, P393, DOI 10.1007/s11263-019-01248-3; Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099; Li YW, 2018, LECT NOTES COMPUT SC, V11210, P520, DOI 10.1007/978-3-030-01231-1_32; Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718; Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P11669; Liu Z, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-00708-7; Lu CW, 2019, INT J COMPUT VISION, V127, P993, DOI 10.1007/s11263-018-1129-8; Ma CY, 2018, PROC CVPR IEEE, P6790, DOI 10.1109/CVPR.2018.00710; Mahdisoltani F., 2018, ARXIV180409235, V5; Materzynska J, 2020, PROC CVPR IEEE, P1046, DOI 10.1109/CVPR42600.2020.00113; Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Srinivas A., 2021, ARXIV; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tian Y., 2021, P IEEECVF INT C COMP, P4490; Tian Y, 2019, IEEE INT CON MULTI, P272, DOI 10.1109/ICME.2019.00055; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang L. Lu, 2020, ACL NLP COVID WORKSH; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155; Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wu ZX, 2021, INT J COMPUT VISION, V129, P2965, DOI 10.1007/s11263-021-01508-1; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Yang B., 2019, ARXIV; Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104; Yuan Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P71, DOI 10.1007/978-3-030-58568-6_5; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zhang C.Y., 2020, ARXIV; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhang Y., 2021, PROC IEEE INT C COMP, P13577; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49; Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43	71	0	0	5	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2453	2471		10.1007/s11263-022-01661-1	http://dx.doi.org/10.1007/s11263-022-01661-1		AUG 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI		Green Submitted			2022-12-18	WOS:000837149500002
J	Zablocki, E; Ben-Younes, H; Perez, P; Cord, M				Zablocki, Eloi; Ben-Younes, Hedi; Perez, Patrick; Cord, Matthieu			Explainability of Deep Vision-Based Autonomous Driving Systems: Review and Challenges	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Review						Autonomous driving; Explainability; Interpretability; Black-box; Post-hoc interpretabililty	COMPUTER VISION; NEURAL-NETWORKS; TRUST; INTERPRETABILITY; VISUALIZATION; EXPLANATIONS	This survey reviews explainability methods for vision-based self-driving systems trained with behavior cloning. The concept of explainability has several facets and the need for explainability is strong in driving, a safety-critical application. Gathering contributions from several research fields, namely computer vision, deep learning, autonomous driving, explainable AI (X-AI), this survey tackles several points. First, it discusses definitions, context, and motivation for gaining more interpretability and explainability from self-driving systems, as well as the challenges that are specific to this application. Second, methods providing explanations to a black-box self-driving system in a post-hoc fashion are comprehensively organized and detailed. Third, approaches from the literature that aim at building more interpretable self-driving systems by design are presented and discussed in detail. Finally, remaining open-challenges and potential future research directions are identified and examined.	[Zablocki, Eloi; Ben-Younes, Hedi; Perez, Patrick; Cord, Matthieu] Valeo ai, Paris, France; [Cord, Matthieu] Sorbonne Univ, Paris, France	UDICE-French Research Universities; Sorbonne Universite	Zablocki, E (corresponding author), Valeo ai, Paris, France.	eloi.zablocki@valeo.com; hedi.ben-younes@valeo.com; patrick.perez@valeo.com; matthieu.cord@valeo.com		Zablocki, Eloi/0000-0003-2757-2036	Valeo	Valeo	This survey was funded by Valeo and no other funding was received to assist with the preparation of this manuscript.	Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052; Adebayo J, 2018, ADV NEUR IN, V31; Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522; Alipour K., 2020, SAFEAI AAAI; Alvarez-Melis D, 2018, ADV NEUR IN, V31; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2016, ADV NEUR IN, V29; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Arras L, 2022, INFORM FUSION, V81, P14, DOI 10.1016/j.inffus.2021.11.008; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Bailo O, 2019, IEEE COMPUT SOC CONF, P1039, DOI 10.1109/CVPRW.2019.00136; Banerjee S., 2005, P ACL WORKSH INTR EX; Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8; Bansal M, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Beaudouin V., 2020, SSRN ELECT J, DOI [http://doi.org/10.2139/ssrn.3559477, DOI 10.2139/SSRN.3559477]; Ben-Younes H., 2020, MACHINE LEARNING AUT; Ben-Younes H., 2022, CVPR WORKSHOP AUTONO; Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Besserve M., 2020, INT C LEARNING REPRE; Bojarski Mariusz, 2016, arXiv; Bojarski M., 2020, NVIDIA PILOTNET EXPT; Bojarski M, 2018, IEEE INT CONF ROBOT, P4701; Borg M., 2019, J AUTOMOTIVE SOFTWAR; Bowles C, 2018, ARXIV181010863; Brown K., 2020, ARXIV; Bykov K., 2020, MUCH CAN I TRUST YOU; Cadene R, 2019, ADV NEUR IN, V32; Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Caltagirone L, 2017, IEEE INT C INTELL TR; Casas Sergio, 2021, CVPR; Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321; Chan FH, 2017, LECT NOTES COMPUT SC, V10114, P136, DOI 10.1007/978-3-319-54190-7_9; Chang YL, 2019, IEEE COMPUT SOC CONF, P1785, DOI 10.1109/CVPRW.2019.00229; Chen Xingyu, 2020, ECCV; Chen Y, 2021, PROC CVPR IEEE, P7226, DOI 10.1109/CVPR46437.2021.00715; Chitta K., 2021, ICCV; Choi JK, 2015, INT J HUM-COMPUT INT, V31, P692, DOI 10.1080/10447318.2015.1070549; Choromanska A., 2017, ARXIV170407911; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Codevilla F, 2019, IEEE I CONF COMP VIS, P9328, DOI 10.1109/ICCV.2019.00942; Codevilla F, 2018, IEEE INT CONF ROBOT, P4693; Corbiere C, 2022, IEEE T PATTERN ANAL, V44, P6043, DOI 10.1109/TPAMI.2021.3085983; Corso A, 2019, IEEE INT C INTELL TR, P163, DOI 10.1109/ITSC.2019.8917242; Courtney PG, 2015, IEEE COMP SEMICON; Cui HG, 2019, IEEE INT CONF ROBOT, P2090, DOI 10.1109/ICRA.2019.8793868; Cultrera L, 2020, IEEE COMPUT SOC CONF, P1389, DOI 10.1109/CVPRW50498.2020.00178; Das Arun, 2020, ARXIV200611371; de Haan P, 2019, ADV NEUR IN, V32; Deng Y, 2020, INT CONF PERVAS COMP; Di X., 2020, ARXIV200705156; Dickmanns ED, 2002, IV'2002: IEEE INTELLIGENT VEHICLE SYMPOSIUM, PROCEEDINGS, P268; Ding S., 2021, NAACL; Djuric N, 2020, IEEE WINT CONF APPL, P2084, DOI 10.1109/WACV45572.2020.9093332; Doshi-Velez F, 2017, ARXIV171101134; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Du N, 2019, TRANSPORT RES C-EMER, V104, P428, DOI 10.1016/j.trc.2019.05.025; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Erhan D, 2009, 1341 U MONTR, V1341, P1, DOI DOI 10.2464/JILM.23.425; Espie E., 2005, TORCS OPEN RACING CA; Fellous JM, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.01346; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Frosst N., 2017, CEX AI IA, V2071; Gao Chen, 2020, ECCV; Garfinkel S., 2017, ALGORITHMIC TRANSPAR; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geng ZL, 2020, INT J COMPUT VISION, V128, P2744, DOI 10.1007/s11263-020-01361-8; Ghorbani A, 2019, AAAI CONF ARTIF INTE, P3681; Gilpin LH, 2018, PR INT CONF DATA SC, P80, DOI 10.1109/DSAA.2018.00018; Goyal Y, 2019, PR MACH LEARN RES, V97; Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009; Harradon M., 2018, CAUSAL LEARNING EXPL; Hase Peter, 2020, FINDINGS EMNLP; Haspiel J, 2018, ACMIEEE INT CONF HUM, P119, DOI 10.1145/3173386.3177057; Hecker S, 2020, IEEE INT C INT ROBOT, P2346, DOI 10.1109/IROS45743.2020.9341157; Hendricks LA, 2018, LECT NOTES COMPUT SC, V11206, P269, DOI 10.1007/978-3-030-01216-8_17; Hensel M, 2017, ADV NEUR IN, V30; Herman B., 2017, PROMISE PERIL HUMAN; Hooker S., 2019, BENCHMARK INTERPRETA; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Jacob P., 2021, ARXIV; Jacovi A., 2020, T ASSOC COMPUT LING; Jain Sarthak, 2019, ARXIV190210186, DOI [10.18653/v1/N19-1357, DOI 10.18653/V1/N19-1357]; Janai J., 2020, FOUND TRENDS COMPUTE; Jansen P., 2021, ARXIV; Jin Chen, 2020, CORR; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kim B, 2017, ARXIV PREPRINT ARXIV; Kim H, 2019, AAAI CONF ARTIF INTE, P978; Kim J., 2020, CVPR WORKSHOPS; Kim J, 2017, IEEE I CONF COMP VIS, P2961, DOI 10.1109/ICCV.2017.320; Kiran BR., 2020, DEEP REINFORCEMENT L; Koo J, 2015, INT J INTERACT DES M, V9, P269, DOI 10.1007/s12008-014-0227-2; Koren M, 2018, IEEE INT VEH SYM, P1898; Krahenbuhl P, 2018, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR.2018.00312; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lample G, 2017, ADV NEUR IN, V30; LEE J, 1992, ERGONOMICS, V35, P1243, DOI 10.1080/00140139208967392; Lee JD, 2004, HUM FACTORS, V46, P50, DOI 10.1518/hfes.46.1.50.30392; LEE JD, 1994, INT J HUM-COMPUT ST, V40, P153, DOI 10.1006/ijhc.1994.1007; Lee R., 2018, SDM; Leonard J, 2008, J FIELD ROBOT, V25, P727, DOI 10.1002/rob.20262; Li B., 2020, P IEEECVF C COMPUTER, P7880; Li B., 2020, NEURIPS; Li CX, 2020, IEEE INT C INT ROBOT, P10711, DOI 10.1109/IROS45743.2020.9341072; Li Q, 2018, LECT NOTES COMPUT SC, V11211, P570, DOI 10.1007/978-3-030-01234-2_34; Li Y., 2020, CAUSAL DISCOVERY PHY; Li Z., 2018, RETHINKING SELF DRIV; Lipton ZC, 2018, COMMUN ACM, V61, P36, DOI 10.1145/3233231; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Liu YC, 2020, INT CONF ACOUST SPEE, P2338, DOI 10.1109/ICASSP40776.2020.9053783; Lu JS, 2016, ADV NEUR IN, V29; Lu X., 2021, ECML PKDD; Lundberg SM, 2017, ADV NEUR IN, V30; Ly AO, 2021, IEEE T INTELL VEHICL, V6, P195, DOI 10.1109/TIV.2020.3002505; Ma WC, 2017, PROC CVPR IEEE, P4636, DOI 10.1109/CVPR.2017.493; Mac Aodha O, 2018, PROC CVPR IEEE, P3820, DOI 10.1109/CVPR.2018.00402; Madumal P, 2020, AAAI CONF ARTIF INTE, V34, P2493; Makino T., 2020, DIFFERENCES HUMAN MA; Malinowski M, 2017, INT J COMPUT VISION, V125, P110, DOI 10.1007/s11263-017-1038-2; Manzo U. G., 2020, SURVEY DEEP LEARNING; Maximov M, 2020, PROC CVPR IEEE, P5446, DOI 10.1109/CVPR42600.2020.00549; McAllister R, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4745; Mehrabi N, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3457607; Mehta A, 2018, ELEVENTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS AND IMAGE PROCESSING (ICVGIP 2018), DOI 10.1145/3293353.3293364; Michon J. A., 1985, HUMAN BEHAV TRAFFIC; Mohseni S., 2019, WORKSH UNC ROB DEEP; Moing G. L., 2021, CVPR; Molnar C., INTERPRETABLE MACHIN, DOI DOI 10.1007/S10290-014-0202-9; Moraffah Raha, 2020, ACM SIGKDD Explorations Newsletter, V22, P18, DOI 10.1145/3400051.3400058; Mordan T., 2020, DETECTING 32 PEDESTR; Morgulis N., 2019, FOOLING REAL CAR ADV; Mori K, 2019, IEEE INT VEH SYM, P1577, DOI 10.1109/IVS.2019.8813900; Morton J, 2017, IEEE INT C INTELL TR; Mullin M, 2019, CLIMATIC CHANGE, V152, P275, DOI 10.1007/s10584-018-2191-5; Narendra T, 2018, EXPLAINING DEEP LEAR; Omeiza D, 2022, IEEE T INTELL TRANSP, V23, P10142, DOI 10.1109/TITS.2021.3122865; Oramas J., 2019, INT C LEARN REPR ICL; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Patro BN, 2020, AAAI CONF ARTIF INTE, V34, P11848; Pearl J., 2009, CAUSALITY, DOI DOI 10.1017/CBO9780511803161; Pei K., 2019, DEEPXPLORE AUTOMATED; Pomerleau D.A., 1989, ALVINN AUTONOMOUS LA; Qiaoning Zhang, 2020, CHI EA '20: Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, P1, DOI 10.1145/3334480.3382986; Ramakrishnan S, 2018, ADV NEUR IN, V31; Ramanishka V, 2018, PROC CVPR IEEE, P7699, DOI 10.1109/CVPR.2018.00803; Rathi S., 2019, ARXIV190609293; Razavian Ali Sharif, 2014, P IEEE C COMP VIS PA, P806, DOI DOI 10.1109/CVPRW.2014.131; Ren ZZ, 2018, LECT NOTES COMPUT SC, V11205, P639, DOI 10.1007/978-3-030-01246-5_38; Rezvani T, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P682, DOI 10.1109/ITSC.2016.7795627; Ribeiro MT, 2018, AAAI CONF ARTIF INTE, P1527; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Richardson A, 2020, AAMAS; Riquelme F, 2020, IMAGE VISION COMPUT, V101, DOI 10.1016/j.imavis.2020.103968; Rodriguez P., 2021, ARXIV; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Sadat Abbas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P414, DOI 10.1007/978-3-030-58592-1_25; Salzmann Tim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P683, DOI 10.1007/978-3-030-58523-5_40; Samaras C., 2014, AUTONOMOUS VEHICLE T, DOI DOI 10.7249/RR443-2; Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820; Sato M, 2001, IEEE IJCNN, P1870, DOI 10.1109/IJCNN.2001.938448; Sauer A., 2018, P CORL, P237; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shariff A, 2017, NAT HUM BEHAV, V1, P694, DOI 10.1038/s41562-017-0202-6; Shen Y., 2020, EXPLAIN NOT EXPLAIN; Shrikumar A, 2017, PR MACH LEARN RES, V70; Singla S., 2020, ICLR; Srikanth S, 2019, IEEE INT C INT ROBOT, P942, DOI 10.1109/IROS40897.2019.8968553; Sun QR, 2018, PROC CVPR IEEE, P5050, DOI 10.1109/CVPR.2018.00530; Sundararajan M, 2017, PR MACH LEARN RES, V70; Suzuki T, 2018, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2018.00371; Tan S., 2021, CVPR; Thrun S, 2006, J FIELD ROBOT, V23, P661, DOI 10.1002/rob.20147; Tian YC, 2018, PROCEEDINGS 2018 IEEE/ACM 40TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE), P303, DOI 10.1145/3180155.3180220; Tjoa E, 2021, IEEE T NEUR NET LEAR, V32, P4793, DOI 10.1109/TNNLS.2020.3027314; Tomei M., 2021, CVPR WORKSHOPS; Tommasi T, 2017, ADV COMPUT VIS PATT, P37, DOI 10.1007/978-3-319-58347-1_2; Toromanoff M, 2020, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR42600.2020.00718; TORRALBA A, 2011, PROC CVPR IEEE, P1521, DOI DOI 10.1109/CVPR.2011.5995347; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Tung Phan-Minh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14062, DOI 10.1109/CVPR42600.2020.01408; Urmson C, 2008, J FIELD ROBOT, V25, P425, DOI 10.1002/rob.20255; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vilone G., 2020, ARXIV200600093; Wachter S., 2018, COUNTERFACTUAL EXPLA, V31; Wang DQ, 2019, IEEE INT CONF ROBOT, P8853, DOI 10.1109/ICRA.2019.8794224; Wojek C, 2011, PROC CVPR IEEE; Wojek C, 2013, IEEE T PATTERN ANAL, V35, P882, DOI 10.1109/TPAMI.2012.174; Xie Ning, 2020, ARXIV200414545, P1; Xu HZ, 2017, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2017.376; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xu YH, 2020, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR42600.2020.00682; Yang Z., 2020, RECOVERING SIMULATIN; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; You T., 2020, ECCV; Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271; Zahavy T, 2016, PR MACH LEARN RES, V48; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688; Zeng KH, 2017, PROC CVPR IEEE, P1330, DOI 10.1109/CVPR.2017.146; Zeng WY, 2019, PROC CVPR IEEE, P8652, DOI 10.1109/CVPR.2019.00886; Zhang HY, 2013, IEEE I CONF COMP VIS, P3056, DOI 10.1109/ICCV.2013.379; Zhang QS, 2018, FRONT INFORM TECH EL, V19, P27, DOI 10.1631/FITEE.1700808; Zhang QS, 2018, AAAI CONF ARTIF INTE, P4454; Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920; Zhao B., 2020, LAYOUT2IMAGE IMAGE G; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou Bolei, 2015, OBJECT DETECTORS EME, P2; Zhou B, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aaw6661; Zilke JR, 2016, LECT NOTES ARTIF INT, V9956, P457, DOI 10.1007/978-3-319-46307-0_29	227	0	0	27	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2425	2452		10.1007/s11263-022-01657-x	http://dx.doi.org/10.1007/s11263-022-01657-x		AUG 2022	28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI		Green Submitted			2022-12-18	WOS:000837149500001
J	Li, YX; Xu, N; Yang, WJ; See, J; Lin, WY				Li, Yuxi; Xu, Ning; Yang, Wenjie; See, John; Lin, Weiyao			Exploring the Semi-Supervised Video Object Segmentation Problem from a Cyclic Perspective	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								Modern video object segmentation (VOS) algorithms have achieved remarkably high performance in a sequential processing order, while most of currently prevailing pipelines still show some obvious inadequacy like accumulative error, unknown robustness or lack of proper interpretation tools. In this paper, we place the semi-supervised video object segmentation problem into a cyclic workflow and find the defects above can be collectively addressed via the inherent cyclic property of semi-supervised VOS systems. Firstly, a cyclic mechanism incorporated to the standard sequential flow can produce more consistent representations for pixel-wise correspondance. Relying on the accurate reference mask in the starting frame, we show that the error propagation problem can be mitigated. Next, a simple gradient correction module, which naturally extends the offline cyclic pipeline to an online manner, can highlight the high-frequent and detailed part of results to further improve the segmentation quality while keeping feasible computation cost. Meanwhile such correction can protect the network from severe performance degration resulted from interference signals. Finally we develop cycle effective receptive field (cycle-ERF) based on gradient correction process to provide a new perspective into analyzing object-specific regions of interests. We conduct comprehensive comparison and detailed analysis on challenging benchmarks of DAVIS16, DAVIS17 and Youtube-VOS, demonstrating that the cyclic mechanism is helpful to enhance segmentation quality, improve the robustness of VOS systems, and further provide qualitative comparison and interpretation on how different VOS algorithms work. The code of this project can be found at https://github.com/lyxok1/STM-Training.	[Li, Yuxi; Lin, Weiyao] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai, Peoples R China; [Xu, Ning] Adobe Res, San Jose, CA USA; [Yang, Wenjie] Shanghai Jiao Tong Univ, Dept Comp Sci, Shanghai, Peoples R China; [See, John] Heriot Watt Univ Malaysia, Putrajaya, Malaysia	Shanghai Jiao Tong University; Adobe Systems Inc.; Shanghai Jiao Tong University; Heriot Watt University	Lin, WY (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai, Peoples R China.	lyxok1@sjtu.edu.cn; nxu@adobe.com; 13633491388@sjtu.edu.cn; J.See@hw.ac.uk; wylin@sjtu.edu.cn	LI, YUXI/GQB-4474-2022; See, John/C-8633-2013	See, John/0000-0003-3005-4109	National Key Research and Development Program of China [2018AAA0100400]; National Natural Science Foundation of China [U21B2013, 61971277]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The paper is supported in part by the following grants: National Key Research and Development Program of China Grant (No.2018AAA0100400), National Natural Science Foundation of China (No. U21B2013, 61971277).	Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; Carl V., 2018, EUROPEAN C COMPUTER; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; github, PRETRAINING CODE SPA; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Jabri Allan, 2020, NEURIPS; Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9; Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916; Khoreva A, 2019, INT J COMPUT VISION, V127, P1175, DOI 10.1007/s11263-019-01164-6; Kingma D.P, P 3 INT C LEARNING R; Lin HJ, 2019, IEEE I CONF COMP VIS, P3948, DOI 10.1109/ICCV.2019.00405; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35; Meister S, 2018, AAAI CONF ARTIF INTE, P7251; Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/ICCV.2019.00932; Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372; Pont-Tuset J, 2017, ARXIV; Robinson Andreas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7404, DOI 10.1109/CVPR42600.2020.00743; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Seong Hongje, 2020, ECCV, P629, DOI DOI 10.1007/978-3-030-58542-6_38; Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960; Ventura C, 2019, PROC CVPR IEEE, P5272, DOI 10.1109/CVPR.2019.00542; Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971; Voigtlaender Paul, 2017, ARXIV170609364; Wang XG, 2019, PROC CVPR IEEE, P8868, DOI [10.1109/CVPR.2019.00908, 10.1109/CVPR.2019.00267]; Xu N., 2020, NEURAL INFORM PROCES; Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36; Yang Z, 2021, PROC ADV NEURAL INF; Yu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P735, DOI 10.1007/978-3-030-58607-2_43; Zeng XH, 2019, IEEE I CONF COMP VIS, P3928, DOI 10.1109/ICCV.2019.00403; Zhang YZ, 2020, PROC CVPR IEEE, P6947, DOI 10.1109/CVPR42600.2020.00698; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	39	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2408	2424		10.1007/s11263-022-01655-z	http://dx.doi.org/10.1007/s11263-022-01655-z		AUG 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI		Green Submitted			2022-12-18	WOS:000836755000001
J	Kordopatis-Zilos, G; Tzelepis, C; Papadopoulos, S; Kompatsiaris, I; Patras, I				Kordopatis-Zilos, Giorgos; Tzelepis, Christos; Papadopoulos, Symeon; Kompatsiaris, Ioannis; Patras, Ioannis			DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							KNOWLEDGE DISTILLATION; LOCALIZATION	In this paper, we address the problem of high performance and computationally efficient content-based video retrieval in large-scale datasets. Current methods typically propose either: (i) fine-grained approaches employing spatio-temporal representations and similarity calculations, achieving high performance at a high computational cost or (ii) coarse-grained approaches representing/indexing videos as global vectors, where the spatio-temporal structure is lost, providing low performance but also having low computational cost. In this work, we propose a Knowledge Distillation framework, called Distill-and-Select (DnS), that starting from a well-performing fine-grained Teacher Network learns: (a) Student Networks at different retrieval performance and computational efficiency trade-offs and (b) a Selector Network that at test time rapidly directs samples to the appropriate student to maintain both high retrieval performance and high computational efficiency. We train several students with different architectures and arrive at different trade-offs of performance and efficiency, i.e., speed and storage requirements, including fine-grained students that store/index videos using binary representations. Importantly, the proposed scheme allows Knowledge Distillation in large, unlabelled datasets-this leads to good students. We evaluate DnS on five public datasets on three different video retrieval tasks and demonstrate (a) that our students achieve state-of-the-art performance in several cases and (b) that the DnS framework provides an excellent trade-off between retrieval performance, computational speed, and storage space. In specific configurations, the proposed method achieves similar mAP with the teacher but is 20 times faster and requires 240 times less storage space. The collected dataset and implementation are publicly available: https://github.com/mever-team/distill-and-select.	[Kordopatis-Zilos, Giorgos; Papadopoulos, Symeon; Kompatsiaris, Ioannis] Ctr Res & Technol Hellas, Informat Technol Inst, Thessaloniki, Greece; [Kordopatis-Zilos, Giorgos; Tzelepis, Christos; Patras, Ioannis] Queen Mary Univ London, Mile End Rd, London E1 4NS, England	Centre for Research & Technology Hellas; University of London; Queen Mary University London	Kordopatis-Zilos, G (corresponding author), Ctr Res & Technol Hellas, Informat Technol Inst, Thessaloniki, Greece.; Kordopatis-Zilos, G (corresponding author), Queen Mary Univ London, Mile End Rd, London E1 4NS, England.	georgekordopatis@iti.gr; c.tzelepis@qmul.ac.uk; papadop@iti.gr; ikom@iti.gr; i.patras@qmul.ac.uk	; Tzelepis, Christos/O-6413-2015; Kompatsiaris, Ioannis (Yiannis)/P-8594-2015	Papadopoulos, Symeon/0000-0002-5441-7341; Kordopatis-Zilos, Giorgos/0000-0003-2297-4802; Tzelepis, Christos/0000-0002-2036-9089; Kompatsiaris, Ioannis (Yiannis)/0000-0001-6447-9020; Patras, Ioannis/0000-0003-3913-4738	European Commission [957252, 951911]; EPSRC [EP/R025290/1]	European Commission(European CommissionEuropean Commission Joint Research Centre); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work has been supported by the projects MediaVerse and AI4Media, partially funded by the European Commission under Contract Nos. 957252 and 951911, respectively, and DECSTER funded by EPSRC under Contract No. EP/R025290/1.	Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Ba J. L., 2016, ARXIV PREPRINT ARXIV; Baraldi L, 2018, PROC CVPR IEEE, P7804, DOI 10.1109/CVPR.2018.00814; Bhardwaj S, 2019, PROC CVPR IEEE, P354, DOI 10.1109/CVPR.2019.00044; Bishay Mina, 2019, ARXIV190709021; Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088; Cai Y., 2011, P ACM INT C MULT ACM; Cao Z., 2017, P IEEE INT C COMPUTE; Chou CL, 2015, IEEE T MULTIMEDIA, V17, P382, DOI 10.1109/TMM.2015.2391674; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807; Deng JJ, 2019, IEEE I CONF COMP VIS, P7022, DOI 10.1109/ICCV.2019.00712; Douze M, 2013, IEEE I CONF COMP VIS, P1825, DOI 10.1109/ICCV.2013.229; Douze M, 2010, IEEE T MULTIMEDIA, V12, P257, DOI 10.1109/TMM.2010.2046265; Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P55, DOI 10.1007/978-3-030-01264-9_4; Gao ZN, 2017, PROC CVPR IEEE, P2107, DOI 10.1109/CVPR.2017.227; Garcia NC, 2018, LECT NOTES COMPUT SC, V11212, P106, DOI 10.1007/978-3-030-01237-3_7; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gordo Albert, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P172, DOI 10.1007/978-3-030-58604-1_11; Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; Huang Z, 2010, IEEE T MULTIMEDIA, V12, P386, DOI 10.1109/TMM.2010.2050737; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Jiang QY, 2019, IEEE I CONF COMP VIS, P5280, DOI 10.1109/ICCV.2019.00538; Jiang YG, 2014, LECT NOTES COMPUT SC, V8692, P357, DOI 10.1007/978-3-319-10593-2_24; Kingma D.P, P 3 INT C LEARNING R; Kordopatis-Zilos G, 2019, IEEE I CONF COMP VIS, P6360, DOI 10.1109/ICCV.2019.00645; Kordopatis-Zilos G, 2019, IEEE T MULTIMEDIA, V21, P2638, DOI 10.1109/TMM.2019.2905741; Kordopatis-Zilos G, 2017, IEEE INT CONF COMP V, P347, DOI 10.1109/ICCVW.2017.49; Kordopatis-Zilos G, 2017, LECT NOTES COMPUT SC, V10132, P251, DOI 10.1007/978-3-319-51811-4_21; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lassance C, 2020, INT CONF ACOUST SPEE, P8484, DOI 10.1109/ICASSP40776.2020.9053986; Lee H., 2020, P IEEE C COMP VIS PA; Lee J, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P481, DOI 10.1145/3219819.3219856; Li Quanquan, 2017, CVPR; Liang, 2019, P IEEE INT C COMPUTE; Liang SY, 2020, LECT NOTES COMPUT SC, V11961, P752, DOI 10.1007/978-3-030-37731-1_61; Liao KY, 2019, IEEE T CIRC SYST VID, V29, P3743, DOI 10.1109/TCSVT.2018.2884941; Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404; Liu H, 2017, MULTIMED TOOLS APPL, V76, P24435, DOI 10.1007/s11042-016-4176-6; Liu Y, 2019, PROC CVPR IEEE, P3599, DOI 10.1109/CVPR.2019.00372; Luo ZL, 2018, LECT NOTES COMPUT SC, V11218, P174, DOI 10.1007/978-3-030-01264-9_11; Markatopoulou F, 2019, IEEE T CIRC SYST VID, V29, P1631, DOI 10.1109/TCSVT.2018.2848458; Markatopoulou F, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P412, DOI 10.1145/3078971.3079041; Miech A., 2017, ARXIV; Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409; Paszke A, 2019, ADV NEUR IN, V32; Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511; Piergiovanni AJ, 2020, PROC CVPR IEEE, P130, DOI 10.1109/CVPR42600.2020.00021; Poullot S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P381, DOI 10.1145/2733373.2806228; Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070; Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318; Shao J., 2021, P IEEE WINTER C APPL; Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Song J., 2011, P 19 ACM INT C MULT; Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344; Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746; Stroud JC, 2020, IEEE WINT CONF APPL, P614; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tan H. -K., 2009, P 17 ACM INT C MULT, P145, DOI DOI 10.1145/1631272.1631295; Tavakolian M., 2019, P IEEE INT C COMP VI; Thoker FM, 2019, IEEE IMAGE PROC, P6, DOI 10.1109/ICIP.2019.8802909; Tolias Giorgos, 2016, P ICLR; Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang K. H., 2021, PROC INT CONF PATT R; Wang L, 2017, LECT NOTES COMPUT SC, V10132, P576, DOI 10.1007/978-3-319-51811-4_47; Wu X., 2007, P ACM INT C MULT; Yalniz IZ, 2019, ARXIV; Yang YY, 2019, MULTIMED TOOLS APPL, V78, P311, DOI 10.1007/s11042-018-5862-3; Yang Z., 2016, P C N AM CHAPTER ASS; Yu-Gang Jiang, 2016, IEEE Transactions on Big Data, V2, P32, DOI 10.1109/TBDATA.2016.2530714; Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315; Zhang CR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1135; Zhang Z., 2020, P IEEE C COMPUTER VI; Zhao ZC, 2019, IEEE INT CONF COMP V, P1873, DOI 10.1109/ICCVW.2019.00234	79	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2385	2407		10.1007/s11263-022-01651-3	http://dx.doi.org/10.1007/s11263-022-01651-3		AUG 2022	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI		Green Submitted, hybrid			2022-12-18	WOS:000836496400001
J	Cheraghian, A; Rahman, S; Chowdhury, TF; Campbell, D; Petersson, L				Cheraghian, Ali; Rahman, Shafin; Chowdhury, Townim F.; Campbell, Dylan; Petersson, Lars			Zero-Shot Learning on 3D Point Cloud Objects and Beyond	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Zero-shot learning; 3D point clouds; Transductive learning; Hubness problem	NETWORK	Zero-shot learning, the task of learning to recognize new classes not seen during training, has received considerable attention in the case of 2D image classification. However, despite the increasing ubiquity of 3D sensors, the corresponding 3D point cloud classification problem has not been meaningfully explored and introduces new challenges. In this paper, we identify some of the challenges and apply 2D Zero-Shot Learning (ZSL) methods in the 3D domain to analyze the performance of existing models. Then, we propose a novel approach to address the issues specific to 3D ZSL. We first present an inductive ZSL process and then extend it to the transductive ZSL and Generalized ZSL (GZSL) settings for 3D point cloud classification. To this end, a novel loss function is developed that simultaneously aligns seen semantics with point cloud features and takes advantage of unlabeled test data to address some known issues (e.g., the problems of domain adaptation, hubness, and data bias). While designed for the particularities of 3D point cloud classification, the method is shown to also be applicable to the more common use-case of 2D image classification. An extensive set of experiments is carried out, establishing state-of-the-art for ZSL and GZSL on synthetic (ModelNet40, ModelNet10, McGill) and real (ScanObjectNN) 3D point cloud datasets.	[Cheraghian, Ali; Petersson, Lars] CSIRO, Data61, Canberra, ACT 2601, Australia; [Cheraghian, Ali] Australian Natl Univ, Sch Engn, Canberra, ACT 0200, Australia; [Rahman, Shafin; Chowdhury, Townim F.] North South Univ, Dept Elect & Comp Engn, Dhaka 1229, Bangladesh; [Campbell, Dylan] Univ Oxford, Dept Engn Sci, Visual Geometry Grp, Oxford, England	Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University; North South University (NSU); University of Oxford	Rahman, S (corresponding author), North South Univ, Dept Elect & Comp Engn, Dhaka 1229, Bangladesh.	ali.cheraghian@anu.edu.au; shafin.rahman@northsouth.edu; townim.faisal@northsouth.edu; dylan@robots.ox.ac.uk; lars.petersson@data61.csiro.au	Rahman, Shafin/N-1939-2019	Rahman, Shafin/0000-0001-7169-0318	North South University (NSU) Conference Travel and Research Grants (CTRG) 2020-2021 [CTRG-20/SEPS/04]; Continental AG	North South University (NSU) Conference Travel and Research Grants (CTRG) 2020-2021; Continental AG	This work was supported in part by North South University (NSU) Conference Travel and Research Grants (CTRG) 2020-2021 (Grant ID: CTRG-20/SEPS/04). D.C. is grateful for support from Continental AG.	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Atzmon Y, 2019, PROC CVPR IEEE, P11663, DOI 10.1109/CVPR.2019.01194; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Cheraghian A, 2019, PROCEEDINGS OF MVA 2019 16TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA); Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dinu Georgiana, 2014, ARXIV14126568; Do TT, 2019, PROC CVPR IEEE, P10396, DOI 10.1109/CVPR.2019.01065; Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Gao R, 2020, IEEE T IMAGE PROCESS, V29, P3665, DOI 10.1109/TIP.2020.2964429; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208; Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089; Izadi S., 2011, P 24 ANN ACM S US IN, P559, DOI DOI 10.1145/2047196.2047270; Jiang HJ, 2019, IEEE I CONF COMP VIS, P9764, DOI 10.1109/ICCV.2019.00986; Kingma D.P, P 3 INT C LEARNING R; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170; Lee D., 2013, INT C MACH LEARN ICM; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758; Li K, 2019, IEEE I CONF COMP VIS, P3582, DOI 10.1109/ICCV.2019.00368; Li RH, 2020, PROC CVPR IEEE, P6377, DOI 10.1109/CVPR42600.2020.00641; Li XP, 2021, NEURAL COMPUT APPL, V33, P5313, DOI 10.1007/s00521-020-05322-7; Li ZQ, 2019, AAAI CONF ARTIF INTE, P8682; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Liu SJ, 2018, ADV NEUR IN, V31; Mikolov T., 2013, ARXIV; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29; Palatucci Mark, 2009, ADV NEURAL INFORM PR, P1410; Qi Charles R, 2017, ARXIV170602413; Qiao R., 2017, BRIT MACH VIS C BMVC; Radovanovic M, 2010, J MACH LEARN RES, V11, P2487; Rahman S, 2020, INT J COMPUT VISION, V128, P2979, DOI 10.1007/s11263-020-01355-6; Rahman S, 2018, IEEE T IMAGE PROCESS, V27, P5652, DOI 10.1109/TIP.2018.2861573; Rosenblatt F., 1961, PRINCIPLES NEURODYNA, DOI 10.21236/AD0256582; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sariyildiz MB, 2019, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2019.00227; Schonfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9; Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8; Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48; Vyas Maunil R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P70, DOI 10.1007/978-3-030-58577-8_5; Wah C., 2011, TECH REP; Wang C., 2018, ARXIV; Wang YS, 2018, J BIOMED INFORM, V87, P12, DOI 10.1016/j.jbi.2018.09.008; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484; Xu Y., 2018, ARXIV; Yu YL, 2018, IEEE T CYBERNETICS, V48, P2908, DOI 10.1109/TCYB.2017.2751741; Yunlu Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P330, DOI 10.1007/978-3-030-58580-8_20; Zakharov S, 2017, IEEE INT C INT ROBOT, P552; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang L, 2020, IEEE T CIRC SYST VID, V30, P2843, DOI 10.1109/TCSVT.2020.2984666; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhao A, 2018, ADV NEUR IN, V31	80	0	0	5	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2364	2384		10.1007/s11263-022-01650-4	http://dx.doi.org/10.1007/s11263-022-01650-4		AUG 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI		Green Submitted			2022-12-18	WOS:000836126200001
J	Yudistira, N; Kavitha, MS; Kurita, T				Yudistira, Novanto; Kavitha, Muthu Subash; Kurita, Takio			Weakly-Supervised Action Localization, and Action Recognition Using Global-Local Attention of 3D CNN	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D CNN; Spatio-temporal attribution; Visual attribution; Weakly-supervised localization; Action recognition		3D convolutional neural network (3D CNN) captures spatial and temporal information on 3D data such as video sequences. However, due to the convolution and pooling mechanism, the information loss that occurs seems unavoidable. To improve the visual explanations and classification in 3D CNN, we propose two approaches; (i) aggregate layer-wise global to local (global-local) discrete gradient using trained 3DResNext network, and (ii) implement attention gating network to improve the accuracy of the action recognition. The proposed approach intends to show the usefulness of every layer termed as global-local attention in 3D CNN via visual attribution, weakly-supervised action localization, and action recognition. Firstly, the 3DResNext is trained and applied for action classification using backpropagation concerning the maximum predicted class. The gradient and activation of every layer are then up-sampled. Later, aggregation is used to produce more nuanced attention, which points out the most critical part of the predicted class's input videos. We use contour thresholding of final attention for final localization. We evaluate spatial and temporal action localization in trimmed videos using fine-grained visual explanation via 3DCAM. Experimental results show that the proposed approach produces informative visual explanations and discriminative attention. Furthermore, the action recognition via attention gating of each layer produces better classification results than the baseline model.	[Yudistira, Novanto] Brawijaya Univ, Fac Comp Sci, Informat Engn, Vet St 8, Malang 65145, East Java, Indonesia; [Kavitha, Muthu Subash] Nagasaki Univ, Sch Informat & Data Sci, 1-14 Bunkyo Machi, Nagasaki, Japan; [Kurita, Takio] Hiroshima Univ, Grad Sch Adv Sci & Engn, Higashihiroshima, Hiroshima 7398521, Japan	Brawijaya University; Nagasaki University; Hiroshima University	Yudistira, N (corresponding author), Brawijaya Univ, Fac Comp Sci, Informat Engn, Vet St 8, Malang 65145, East Java, Indonesia.	yudistira@ub.ac.id; kavitha@nagasaki-u.ac.jp; tkurita@hiroshima-u.ac.jp		Yudistira, Novanto/0000-0001-5330-5930	KAKENHI [16K00239]	KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	The authors would like to thank KAKENHI Project No. 16K00239 for funding the research.	Adebayo J, 2018, ADV NEUR IN, V31; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bargal SA, 2018, PROC CVPR IEEE, P1440, DOI 10.1109/CVPR.2018.00156; Bazzani L, 2016, IEEE WINT CONF APPL, DOI 10.1109/wacv.2016.7477688; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Choe J, 2020, PROC CVPR IEEE, P3130, DOI 10.1109/CVPR42600.2020.00320; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fukui H, 2019, PROC CVPR IEEE, P10697, DOI 10.1109/CVPR.2019.01096; Girdhar R, 2017, ADV NEUR IN, V30; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Kawaguchi K, 2019, NEURAL NETWORKS, V118, P167, DOI 10.1016/j.neunet.2019.06.009; Kuehne H, 2013, HIGH PERFORMANCE COMPUTING IN SCIENCE AND ENGINEERING '12: TRANSACTIONS OF THE HIGH PERFORMANCE COMPUTING CENTER, STUTTGART (HLRS) 2012, P571, DOI 10.1007/978-3-642-33374-3_41; Li W., 2017, P IEEE C COMPUTER VI; OQUAB M, 2015, PROC CVPR IEEE, P685, DOI DOI 10.1109/CVPR.2015.7298668; Preim B., 2013, VISUAL COMPUTING MED, V2nd ed; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schlemper J, 2019, MED IMAGE ANAL, V53, P197, DOI 10.1016/j.media.2019.01.012; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shamir O., 2018, ADV NEURAL INFORM PR; Shrikumar A, 2017, PR MACH LEARN RES, V70; Soomro K., 2012, COMPUT SCI; Sundararajan M, 2017, PR MACH LEARN RES, V70; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yudistira N., 2017, ARXIV; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x	32	0	0	6	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2022	130	10					2349	2363		10.1007/s11263-022-01649-x	http://dx.doi.org/10.1007/s11263-022-01649-x		AUG 2022	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	4L1FI		Green Submitted			2022-12-18	WOS:000833995800001
J	Zhao, YH; Fang, GC; Guo, YL; Guibas, L; Tombari, F; Birdal, T				Zhao, Yongheng; Fang, Guangchi; Guo, Yulan; Guibas, Leonidas; Tombari, Federico; Birdal, Tolga			3DPointCaps+plus : Learning 3D Representations with Capsule Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Capsule networks; 3D Point clouds; Representation learning; Autoencoder; Unsupervised learning; 3D Reconstruction; 3D Shapes		We present 3DPointCaps++ for learning robust, flexible and generalizable 3D object representations without requiring heavy annotation efforts or supervision. Unlike conventional 3D generative models, our algorithm aims for building a structured latent space where certain factors of shape variations, such as object parts, can be disentangled into independent sub-spaces. Our novel decoder then acts on these individual latent sub-spaces (i.e. capsules) using deconvolution operators to reconstruct 3D points in a self-supervised manner. We further introduce a cluster loss ensuring that the points reconstructed by a single capsule remain local and do not spread across the object uncontrollably. These contributions allow our network to tackle the challenging tasks of part segmentation, part interpolation/replacement as well as correspondence estimation across rigid / non-rigid shape, and across / within category. Our extensive evaluations on ShapeNet objects and human scans demonstrate that our network can learn generic representations that are robust and useful in many applications.	[Zhao, Yongheng; Tombari, Federico] Tech Univ Munich, Informat, Munich, Germany; [Fang, Guangchi; Guo, Yulan] Sun Yat Sen Univ, Sch Elect & Commun Engn, Shenzhen Campus, Shenzhen, Peoples R China; [Guibas, Leonidas] Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA; [Tombari, Federico] Google, Mountain View, CA 94043 USA; [Birdal, Tolga] Imperial Coll London, Dept Comp, London, England	Technical University of Munich; Sun Yat Sen University; Stanford University; Google Incorporated; Imperial College London	Birdal, T (corresponding author), Imperial Coll London, Dept Comp, London, England.	yongheng.zhao@tum.de; fanggch@mail2.sysu.edu.cn; guoyulan@sysu.edu.cn; guibas@cs.stanford.edu; tombari@google.com; tbirdal@imperial.ac.uk	guo, yu/GQZ-1392-2022	Birdal, Tolga/0000-0001-7915-7964	NSF [IIS-1763268]; National Natural Science Foundation of China [U20A20185, 61972435]; Natural Science Foundation of Guangdong Province [2019A1515011271]; Science and Technology Innovation Committee of Shenzhen Municipality [JCYJ20190807152209394]; Vannevar Bush Faculty fellowship	NSF(National Science Foundation (NSF)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Guangdong Province(National Natural Science Foundation of Guangdong Province); Science and Technology Innovation Committee of Shenzhen Municipality; Vannevar Bush Faculty fellowship	This project is partially supported by the Vannevar Bush Faculty fellowship, NSF Grant IIS-1763268, the National Natural Science Foundation of China (Nos. U20A20185, 61972435), the Natural Science Foundation of Guangdong Province (2019A1515011271), and the Science and Technology Innovation Committee of Shenzhen Municipality (JCYJ20190807152209394).	Achlioptas P, 2018, PR MACH LEARN RES, V80; Afshar P, 2020, PATTERN RECOGN LETT, V138, P638, DOI 10.1016/j.patrec.2020.09.010; Bogo F, 2017, PROC CVPR IEEE, P5573, DOI 10.1109/CVPR.2017.591; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen Z., 2018, ARXIV; Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Deng HW, 2018, LECT NOTES COMPUT SC, V11209, P620, DOI 10.1007/978-3-030-01228-1_37; Deng HW, 2019, PROC CVPR IEEE, P3239, DOI 10.1109/CVPR.2019.00336; Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028; Deprelle T, 2019, ADV NEUR IN, V32; Duarte K, 2018, ADV NEUR IN, V31; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Gojcic Z, 2021, PROC CVPR IEEE, P5688, DOI 10.1109/CVPR46437.2021.00564; Gojcic Z, 2020, PROC CVPR IEEE, P1756, DOI 10.1109/CVPR42600.2020.00183; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Groueix T, 2018, LECT NOTES COMPUT SC, V11206, P235, DOI 10.1007/978-3-030-01216-8_15; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Hermosilla P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275110; Hinton G.-E., 6 INT C LEARN REPR I; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Huang FC, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073654, 10.1145/3137609]; Jaiswal A, 2019, LECT NOTES COMPUT SC, V11131, P526, DOI 10.1007/978-3-030-11015-4_38; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jimenez-Sanchez A, 2018, LECT NOTES COMPUT SC, V11043, P150, DOI 10.1007/978-3-030-01364-6_17; Kong ZF, 2020, PR MACH LEARN RES, V108, P3599; Kosiorek AR, 2019, ADV NEUR IN, V32; LaLonde R., 2018, ARXIV; Lei H., 2018, ARXIV; Lenssen JE, 2018, ADV NEUR IN, V31; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Lin A., 2018, ARXIV; Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910; Malmgren C, 2019, COMP STUDY ROUTING M; Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616; Mobiny A, 2018, LECT NOTES COMPUT SC, V11071, P741, DOI 10.1007/978-3-030-00934-2_82; Naseer M, 2019, IEEE ACCESS, V7, P1859, DOI 10.1109/ACCESS.2018.2886133; Poulenard A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275102; Qi CR, 2017, ADV NEUR IN, V30; Quessard R., 2020, ARXIV; Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753; Ramirez I, 2020, NEUROCOMPUTING, V379, P64, DOI 10.1016/j.neucom.2019.09.101; Rempe Davis, 2020, ADV NEURAL INFORM PR, V3, P4; Ren H., 2018, ARXIV; Ribeiro FDS., 2020, ADV NEURAL INFORM PR, DOI 10.1145/3137609; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sabour S., 2020, ARXIV; Sabour S, 2017, ADV NEUR IN, V30; Saqur R., 2018, ARXIV; Srivastava N., 2019, ARXIV; Sun P., 2019, ARXIV; Sun W., 2020, ARXIV; Sun YB, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P361; Sung M, 2018, ADV NEUR IN, V31; Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Thomas N., 2018, ARXIV; Upadhyay Y., 2018, ARXIV; Uy Mikaela Angelina, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P397, DOI 10.1007/978-3-030-58571-6_24; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Wang D., 2018, ICLR WORKSH SUBM, DOI DOI 10.14722/NDSS.2018.23142; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Weiler M, 2018, ADV NEUR IN, V31; Wen X, 2020, IEEE T IMAGE PROCESS, V29, P8855, DOI 10.1109/TIP.2020.3019925; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yang ZS, 2021, IEEE WINT CONF APPL, P134, DOI 10.1109/WACV48630.2021.00018; Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238; Yongheng Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P1, DOI 10.1007/978-3-030-58452-8_1; Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295; Zhang LH, 2018, ADV NEUR IN, V31; Zhang S, 2018, INT S ART INT ROB, P301; Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472	77	0	0	9	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2321	2336		10.1007/s11263-022-01632-6	http://dx.doi.org/10.1007/s11263-022-01632-6		JUL 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG	35968252	hybrid, Green Published			2022-12-18	WOS:000833499200001
J	Koutaki, G; Ando, S; Shirai, K; Kishigami, T				Koutaki, Gou; Ando, Sakino; Shirai, Keiichiro; Kishigami, Tsuyoshi			ISHIGAKI Retrieval System Using 3D Shape Matching and Combinatorial Optimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cultural heritage; 3D measurement; ICP; Kumamoto Castle		In April 2016, a massive earthquake with a magnitude of 7.3 struck Kumamoto region, Japan, causing major devastation. One of the structures that were damaged in Kumamoto was Kumamoto Castle, a cultural asset of great significance in Japan. The stone retaining wall "ishigaki" that formed the foundation of the castle collapsed, and the superstructure was destroyed. The number of stones is estimated to be more than 70,000, and restoration work is anticipated to take more than 20 years. Since each of the stones is an important cultural asset, the broken stone structure needed to be restored to its original state in order not to lose its cultural value forever. In addition, the fallen stones need to be returned to their original positions in the ishigakis. In similar cases, non-automatic visual verification was used. However, for Kumamoto Castle, this would have been impossible because a large number of stones were displaced as a result of the collapse. The purpose of this project is to provide support for the restoration work by matching the stones that fell down after the collapse with those before the collapse using information technology, such as computer vision and optimization technologies. Specifically, we captured photographic images of the stones before and after the collapse to match them. The technical contributions of this study are as follows: (a) To estimate the scale and surface orientation of the stones, we exploit 3D model construction from the images. (b) To solve the jigsaw-puzzle-like problem of reassembling the stone fragments, we exploit the combination of a customized iterative closest point (ICP) algorithm for shape position matching and an assignment algorithm to find the best pairs of stones before and after the collapse by using the matching degree obtained from ICP. Here, only the 2D shape of the stones before the collapse can be used due to the small number of photos available. In contrast, a detailed 3D shape can be obtained from the stones after their collapse. We matched these asymmetric data in 2D and 3D to enable a comprehensive reconstruction. (c) We developed a user-friendly graphical user interface system that was used by actual masons without special knowledge. The developed system was used to match the ishigaki of a turret, Iidamaru. As a result, we succeeded in identifying 337 stones, or approximately 90% of the 370 images. These results are expected to be useful for and were used as a blueprint during actual restoration work.	[Koutaki, Gou; Ando, Sakino] Kumamoto Univ, Fac Adv Sci & Technol, Chuou Ku, 2-39-1 Kurokami, Kumamoto 8608555, Japan; [Shirai, Keiichiro] Shinshu Univ, Fac Engn, 4-17-1 Wakasato, Nagano 3808553, Japan; [Kishigami, Tsuyoshi] Toppan Printing CO LTD, Cultural Projects Div, Bunkyo Ku, 1-3-3 Suido, Tokyo 1128531, Japan	Kumamoto University; Shinshu University; Toppan Printing Co Ltd	Koutaki, G (corresponding author), Kumamoto Univ, Fac Adv Sci & Technol, Chuou Ku, 2-39-1 Kurokami, Kumamoto 8608555, Japan.	koutaki@cs.kumamoto-u.ac.jp; keiichi@shinshu-u.ac.jp; tsuyoshi.kishigami@toppan.co.jp			Japan Science and Technology Agency (JST), A-STEP	Japan Science and Technology Agency (JST), A-STEP	The authors would like to thank the Kumamoto Castle Research Institute of Kumamoto City for providing many valuable photographs, digital images, and other materials. I would also like to express my gratitude to the members of this project, Dr. Toda, Dr. Yamao, Dr. Matsunaga, Dr. Okajima, and Mr. Miyazawa, a graduate student at the time. This project was partially supported by the Japan Science and Technology Agency (JST), A-STEP.	Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13; Ando S, 2019, SUMAC'19: PROCEEDINGS OF THE 1ST WORKSHOP ON STRUCTURING AND UNDERSTANDING OF MULTIMEDIA HERITAGE CONTENTS, P69, DOI 10.1145/3347317.3357241; Collins, 1993, P CAA C, P19; GALE D, 1962, AM MATH MON, V69, P9, DOI 10.2307/2312726; Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925; Ikeuchi K, 2007, INT J COMPUT VISION, V75, P189, DOI 10.1007/s11263-007-0039-y; Kawakami R., 2020, INT C 3D VISION 3DV, P1; Kojima F, 2014, ECON THEOR, V55, P515, DOI 10.1007/s00199-013-0769-8; Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; OGrady, 2010, KUMAMOTO CASTLE JAPA; Papaioannou G, 2017, ACM J COMPUT CULT HE, V10, DOI 10.1145/3009905; Pohle F, 2003, CONSTR BUILD MATER, V17, P651, DOI 10.1016/S0950-0618(03)00062-X; SANDRON D, 2020, NOTRE DAME CATHEDRAL; Take Y, 2019, 2019 58TH ANNUAL CONFERENCE OF THE SOCIETY OF INSTRUMENT AND CONTROL ENGINEERS OF JAPAN (SICE), P976, DOI 10.23919/SICE.2019.8859910; Takey Y, 2020, 2020 59TH ANNUAL CONFERENCE OF THE SOCIETY OF INSTRUMENT AND CONTROL ENGINEERS OF JAPAN (SICE), P888; Tallon A, 2014, ARCHIT HIST-LONDON, V2, DOI 10.5334/ah.bo; Thuswaldner, 2009, ACM J COMPUT CULT HE, V2; Yamasaki Y, 2020, COMM COM INF SC, V1212, P43, DOI 10.1007/978-981-15-4818-5_4; Zhang K, 2015, IEEE I CONF COMP VIS, P2138, DOI 10.1109/ICCV.2015.247	20	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2286	2304		10.1007/s11263-022-01630-8	http://dx.doi.org/10.1007/s11263-022-01630-8		JUL 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG		hybrid			2022-12-18	WOS:000828942300002
J	Wu, XX; Zhao, WT; Luo, JB				Wu, Xinxiao; Zhao, Wentian; Luo, Jiebo			Learning Cooperative Neural Modules for Stylized Image Captioning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stylized image captioning; Cooperative modular networks; Reinforcement learning; Multi-pass decoding		Recent progress in stylized image captioning has been achieved through the encoder-decoder framework that generates a sentence in one-pass decoding process. However, it remains difficult for such a decoding process to simultaneously capture the syntactic structure, infer the semantic concepts and express the linguistic styles. Research in psycholinguistics has revealed that the language production process of humans involves multiple stages, starting with several rough concepts and ending with fluent sentences. With this in mind, we propose a novel stylized image captioning approach that generates stylized sentences in a multi-pass decoding process by training three cooperative neural modules under the reinforcement learning paradigm. A low-level neural module called syntax module first generates the overall syntactic structure of the stylized sentence. Next, two high-level neural modules, namely concept module and style module, incorporate the words that describe factual content and the words that express linguistic style, respectively. Since the three modules contribute to different aspects of the stylized sentence, i.e. the fluency, the relevancy of the factual content and the style accuracy, we encourage the modules to specialize in their own tasks by designing different rewards for different actions. We also design an attention mechanism to facilitate the communication between the high-level and low-level modules. With the help of the attention mechanism, the high-level modules are able to take the global structure of the sentence into consideration and maintain the consistency between the factual content and the linguistic style. Evaluations on several public benchmark datasets demonstrate that our method outperforms the existing one-pass decoding methods in terms of multiple different evaluation metrics.	[Wu, Xinxiao; Zhao, Wentian] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing, Peoples R China; [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA	Beijing Institute of Technology; University of Rochester	Wu, XX (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing, Peoples R China.	wuxinxiao@bit.edu.cn; wentian_zhao@bit.edu.cn; jluo@cs.rochester.edu		Luo, Jiebo/0000-0002-4516-9729	Natural Science Foundation of China (NSFC) [62072041]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported in part by the Natural Science Foundation of China (NSFC) under Grant No 62072041.	Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Banerjee S., 2005, P ACL WORKSH INTR EX, P65; Chen CK, 2019, AAAI CONF ARTIF INTE, P8151; Chen TL, 2018, LECT NOTES COMPUT SC, V11214, P527, DOI 10.1007/978-3-030-01249-6_32; Dethlefs Nina, 2010, P 6 INT NATURAL LANG, P37; Diao H., 2021, SIMILARITY REASONING; Fu ZX, 2018, AAAI CONF ARTIF INTE, P663; Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108; Gu JX, 2018, AAAI CONF ARTIF INTE, P6837; Guo LT, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P767; Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226; Guo LT, 2019, PROC CVPR IEEE, P4199, DOI 10.1109/CVPR.2019.00433; Huang QY, 2019, AAAI CONF ARTIF INTE, P8465; Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Kong XY, 2017, PROC CVPR IEEE, P7072, DOI 10.1109/CVPR.2017.748; Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356; Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3774; Mathews A, 2018, PROC CVPR IEEE, P8591, DOI 10.1109/CVPR.2018.00896; Mathews A, 2016, AAAI CONF ARTIF INTE, P3574; Panait L, 2005, AUTON AGENT MULTI-AG, V11, P387, DOI 10.1007/s10458-005-2631-2; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Peng Baolin, 2017, P 2017 C EMP METH NA, V1870, P2231, DOI DOI 10.18653/V1/D17-1237; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Shin Andrew, 2016, BMVC; Slevc LR, 2011, J EXP PSYCHOL LEARN, V37, P1503, DOI 10.1037/a0024350; Stolcke Andreas, 2002, P INT, P901; Sun XB, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3418; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815; Xia YC, 2017, ADV NEUR IN, V30; Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820; Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669; Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094; Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611; Zhao WT, 2020, AAAI CONF ARTIF INTE, V34, P12984; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	44	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2305	2320		10.1007/s11263-022-01636-2	http://dx.doi.org/10.1007/s11263-022-01636-2		JUL 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG					2022-12-18	WOS:000828942300001
J	Lee, S; Rameau, F; Im, S; Kweon, IS				Lee, Seokju; Rameau, Francois; Im, Sunghoon; Kweon, In So			Self-Supervised Monocular Depth and Motion Learning in Dynamic Scenes: Semantic Prior to Rescue	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D visual perception; Monocular depth estimation; Motion estimation; Self-supervised learning		We introduce an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion, and depth in a monocular camera setup without geometric supervision. Our technical contributions are three-fold. First, we highlight the fundamental difference between inverse and forward projection while modeling the individual motion of each rigid object, and propose a geometrically correct projection pipeline using a neural forward projection module. Second, we propose two types of residual motion learning frameworks to explicitly disentangle camera and object motions in dynamic driving scenes with different levels of semantic prior knowledge: video instance segmentation as a strong prior, and object detection as a weak prior. Third, we design a unified photometric and geometric consistency loss that holistically imposes self-supervisory signals for every background and object region. Lastly, we present a unsupervised method of 3D motion field regularization for semantically plausible object motion representation. Our proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI, Cityscapes, and Waymo open dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code, dataset, and models are publicly available	[Lee, Seokju] Korea Inst Energy Technol KENTECH, Sch Energy Engn, Naju, South Korea; [Rameau, Francois; Kweon, In So] Korea Adv Inst Sci & Technol KAIST, Sch Elect Engn, Daejeon, South Korea; [Im, Sunghoon] Daegu Gyeongbuk Inst Sci & Technol DGIST, Dept Informat & Commun Engn, Daegu, South Korea	Korea Advanced Institute of Science & Technology (KAIST); Daegu Gyeongbuk Institute of Science & Technology (DGIST)	Lee, S (corresponding author), Korea Inst Energy Technol KENTECH, Sch Energy Engn, Naju, South Korea.	seokju91@gmail.com; rameau.fr@gmail.com; sunghoonim@dgist.ac.kr; iskweon77@kaist.ac.kr			KENTECH [KRG2022-01-003]; DGIST R &D Program of the Ministry of Science and ICT [20-CoE-IT-01]; International Research and Development Program of the National Research Foundation of Korea (NRF) - Ministry of Science and ICT [NRF-2021K1A3A1A21040016]	KENTECH; DGIST R &D Program of the Ministry of Science and ICT(Ministry of Science, ICT & Future Planning, Republic of Korea); International Research and Development Program of the National Research Foundation of Korea (NRF) - Ministry of Science and ICT(National Research Foundation of Korea)	This work was supported by the KENTECH Research Grant (KRG2022-01-003), the DGIST R &D Program of the Ministry of Science and ICT (20-CoE-IT-01), and the International Research and Development Program of the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT under Grant NRF-2021K1A3A1A21040016.	Bangunharcana A., 2021, IROS; Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939; Bian JW, 2019, ADV NEUR IN, V32; Bian JW, 2021, INT J COMPUT VISION, V129, P2548, DOI 10.1007/s11263-021-01484-6; Casser V, 2019, IEEE COMPUT SOC CONF, P381, DOI 10.1109/CVPRW.2019.00051; Casser V, 2019, AAAI CONF ARTIF INTE, P8001; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen PY, 2019, PROC CVPR IEEE, P2619, DOI 10.1109/CVPR.2019.00273; Chen YH, 2019, IEEE I CONF COMP VIS, P7062, DOI 10.1109/ICCV.2019.00716; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai Q, 2020, IEEE COMPUT SOC CONF, P4326, DOI 10.1109/CVPRW50498.2020.00510; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Eigen D, 2014, ADV NEUR IN, V27; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Geiger A, 2014, IEEE T PATTERN ANAL, V36, P1012, DOI 10.1109/TPAMI.2013.185; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907; Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256; Guizilini Vitor, 2020, INT C LEARN REPR; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Hur Junhwa, 2020, P IEEE CVF C COMP VI, P7396; Jaderberg M, 2015, ADV NEUR IN, V28; Janai J, 2018, LECT NOTES COMPUT SC, V11220, P713, DOI 10.1007/978-3-030-01270-0_42; Kingma D.P, P 3 INT C LEARNING R; Klingner Marvin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P582, DOI 10.1007/978-3-030-58565-5_35; Lee S., 2021, ICCV; Lee S, 2019, IEEE INT C INT ROBOT, P1180, DOI 10.1109/IROS40897.2019.8967970; Lee S, 2021, AAAI CONF ARTIF INTE, V35, P1863; Li Hanhan, 2020, ARXIV201016404; Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Lv ZY, 2018, LECT NOTES COMPUT SC, V11209, P484, DOI 10.1007/978-3-030-01228-1_29; Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Meister S, 2018, AAAI CONF ARTIF INTE, P7251; Osep A., 2017, P 2017 IEEE INT C RO, P1995, DOI [DOI 10.1109/ICRA.2017.7989230, 10.1109/icra.2017.7989230]; Osep A, 2018, IEEE INT CONF ROBOT, P3494; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Pillai S, 2019, IEEE INT CONF ROBOT, P9250, DOI 10.1109/ICRA.2019.8793621; Ranjan A., 2019, CVPR, P12240, DOI DOI 10.1109/CVPR.2019.01252; Shashua A, 2004, 2004 IEEE INTELLIGENT VEHICLES SYMPOSIUM, P1; SHIN K, 2019, IEEE INT VEH SYM; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Voigtlaender P, 2019, PROC CVPR IEEE, P7934, DOI 10.1109/CVPR.2019.00813; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; WANG Y, 2018, PROC CVPR IEEE; Wang YJ, 2019, INT CONF ACOUST SPEE, P8063, DOI 10.1109/ICASSP.2019.8682578; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Yang ZH, 2018, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2018.00031; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Zhang CN, 2021, IEEE WINT CONF APPL, P3549, DOI 10.1109/WACV48630.2021.00359; Zhang Cheng, 2019, P BRIT MACHINE VISIO; Zhao ZT, 2019, 2018 7TH INTERNATIONAL CONFERENCE ON ADVANCED MATERIALS AND COMPUTER SCIENCE (ICAMCS 2018), P1, DOI 10.23977/icamcs.2018.001; Zhe Cao, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5587, DOI 10.1109/CVPR.2019.00574; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	60	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2265	2285		10.1007/s11263-022-01641-5	http://dx.doi.org/10.1007/s11263-022-01641-5		JUL 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG					2022-12-18	WOS:000827403200001
J	ElNaghy, H; Dorst, L				ElNaghy, Hanan; Dorst, Leo			Pairwise Alignment of Archaeological Fragments Through Morphological Characterization of Fracture Surfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Archaeological reconstruction; Fragment alignment; Scale space; Fracture representation; Boundary morphology; Morphological features; Lipschitz; ICP	REGISTRATION; OBJECTS; 2D	We design a computational method to align pairs of counter-fitting fracture surfaces of digitized archaeological artefacts. The challenge is to achieve an accurate fit, even though the data is inherently lacking material through abrasion, missing geometry of the counterparts, and may have been acquired by different scanning practices. We propose to use the non-linear complementarity-preserving properties of Mathematical Morphology to guide the pairwise fitting in a manner inherently insensitive to these aspects. In our approach, the fracture surface is tightly bounded by a concise set of characteristic multi-local morphological features. Such features and their descriptors are computed by analysing the discrete distance transform and its causal scale-space information. This compact morphological representation provides the information required for accurately aligning the fracture surfaces through applying a RANSAC-based algorithm incorporating weighted Procrustes to the morphological features, followed by ICP on morphologically selected 'flank' regions. We propose new criteria for evaluating the resulting pairwise alignment quality, taking into consideration both penetration and gap regions. Careful quantitative evaluation on real terracotta fragments confirms the accuracy of our method under the expected archaeological noise. We show that our morphological method outperforms a recent linear pairwise alignment method and briefly discuss our limitations and the effects of variations in digitization and abrasion on our proposed alignment technique.	[ElNaghy, Hanan; Dorst, Leo] Univ Amsterdam, Informat Inst, Comp Vis Lab, Amsterdam, Netherlands	University of Amsterdam	ElNaghy, H (corresponding author), Univ Amsterdam, Informat Inst, Comp Vis Lab, Amsterdam, Netherlands.	hanan.elnaghy@uva.nl			GRAVITATE project under EU2020 - REFLECTIVE-7-2014 Research and Innovation Action [665155]	GRAVITATE project under EU2020 - REFLECTIVE-7-2014 Research and Innovation Action	This research was funded by the GRAVITATE project (https://gravitate-project.eu/) under EU2020 - REFLECTIVE-7-2014 Research and Innovation Action, grant no. 665155. Datasets are from the GRAVITATE data collection. The archaeological artefacts are digitized and kept by the British Museum, Cyprus Institute and the Archaeology laboratory at the Hebrew University of Jerusalem. The 3D models for the comparison study are courtesy of Vienna University of Technology.	Altantsetseg E, 2014, VISUAL COMPUT, V30, P929, DOI 10.1007/s00371-014-0959-9; Alzaid Asma, 2019, 10th International Conference on Pattern Recognition Systems, P98; Andreadis A, 2015, 2015 DIGITAL HERITAGE INTERNATIONAL CONGRESS, VOL 2: ANALYSIS & INTERPRETATION THEORY, METHODOLOGIES, PRESERVATION & STANDARDS DIGITAL HERITAGE PROJECTS & APPLICATIONS, P549, DOI 10.1109/DigitalHeritage.2015.7419572; Attali D, 2009, MATH VIS, P109, DOI 10.1007/b106657_6; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Biasotti S, 2006, LECT NOTES COMPUT SC, V4105, P314; Brown BJ, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360683; Brown BJ., 2012, INT J HERITAGE DIGIT, V1, P313, DOI [10.1260/2047-4970.1.2.313, DOI 10.1260/2047-4970.1.2.313]; Bustos B, 2007, IEEE COMPUT GRAPH, V27, P22, DOI 10.1109/MCG.2007.80; Chang NC, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P987; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; Cignoni P., 2008, P EUR IT CHAPT C, P129, DOI DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136; De Vet SJ, 2015, METEORIT PLANET SCI, V50, pE1, DOI 10.1111/maps.12554; Derpanis K.G., 2010, IMAGE ROCHESTER NY, V4, P2; Dorst L, 2000, LECT NOTES COMPUT SC, V1888, P22; Dorst L., 2009, GEOMETRIC ALGEBRA CO; Dougherty E., 2018, MATH MORPHOLOGY IMAG; ElNaghy H., 2020, MATH MORPHOLOGY THEO, V4, P46, DOI [10.1515/mathm-2020-0101, DOI 10.1515/MATHM-2020-0101]; ElNaghy H, 2017, IEEE INT CONF COMP V, P2934, DOI 10.1109/ICCVW.2017.346; Fitzgibbon AW, 2003, IMAGE VISION COMPUT, V21, P1145, DOI 10.1016/j.imavis.2003.09.004; GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478; HARALICK RM, 1987, IEEE T PATTERN ANAL, V9, P532, DOI 10.1109/TPAMI.1987.4767941; Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508373; Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925; KALVIN A, 1986, INT J ROBOT RES, V5, P38, DOI 10.1177/027836498600500403; Koller D., 2006, J ROMAN ARCHAEOLO S, V61, P237; Kong WX, 2001, PROC CVPR IEEE, P583; Lavoue G, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P57; Leitao HCD, 2002, IEEE T PATTERN ANAL, V24, P1239, DOI 10.1109/TPAMI.2002.1033215; Li QH, 2020, IEEE ACCESS, V8, P6153, DOI 10.1109/ACCESS.2019.2961391; Liao SH, 2020, COMPUT METH PROG BIO, V197, DOI 10.1016/j.cmpb.2020.105756; Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156; Mavridis P., 2015, EG 2015 SHORT PAPERS, DOI 10.2312/egsh.20151005; McBride J.C., 2003, IEEE CVPR WORKSH, V1, P3, DOI DOI 10.1109/CVPRW.2003.10008; Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446; Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35; Palmas Gregorio, 2013, 2013 Digital Heritage International Congress (DigitalHeritage). Federating the 19th Int'I VSMM, 10th Eurographics GCH, & 2nd UNESCO Memory of the World Conferences, plus special sessions from CAA, Arqueologico 2.0, Space2Place, ICOMOS ICIP & CIPA, EU projects, et al. Proceedings, P529; Papaioannou G, 2002, IEEE T PATTERN ANAL, V24, P114, DOI 10.1109/34.982888; Papaioannou G, 2017, ACM J COMPUT CULT HE, V10, DOI 10.1145/3009905; Park KR, 1996, IEEE T PATTERN ANAL, V18, P1121, DOI 10.1109/34.544083; Sagiroglu M., 2005, P 6 INT C VIRT REAL, P137, DOI DOI 10.2312/VAST/VAST05/137-142; Scalas A, 2020, J CULT HERIT, V41, P113, DOI 10.1016/j.culher.2019.06.006; SERRA J, 1986, COMPUT VISION GRAPH, V35, P283, DOI 10.1016/0734-189X(86)90002-2; Serra J., 2012, MATH MORPHOLOGY ITS, DOI [10.1007/978-94-011-1040-2, DOI 10.1007/978-94-011-1040-2]; Shaffer E, 2001, IEEE VISUAL, P127, DOI 10.1109/VISUAL.2001.964503; SommellaMura A., 2011, DELICIAE FICTILES, VIV, P177; Son TG, 2018, VISUAL COMPUT, V34, P1371, DOI 10.1007/s00371-017-1419-0; Toler-Franklin C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866207; Vendrell-Vidal E, 2014, ACM J COMPUT CULT HE, V7, DOI 10.1145/2597178; Winkelbach S, 2008, INT J COMPUT VISION, V78, P1, DOI 10.1007/s11263-007-0121-5; Yang JQ, 2017, PATTERN RECOGN, V66, P375, DOI 10.1016/j.patcog.2017.01.017; Zhang K, 2015, IEEE I CONF COMP VIS, P2138, DOI 10.1109/ICCV.2015.247; Zhang YH, 2018, J CULT HERIT, V33, P191, DOI 10.1016/j.culher.2018.03.001	53	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2184	2204		10.1007/s11263-022-01635-3	http://dx.doi.org/10.1007/s11263-022-01635-3		JUL 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG		hybrid, Green Submitted			2022-12-18	WOS:000822474400001
J	Guo, H; Okura, F; Shi, BX; Funatomi, T; Mukaigawa, Y; Matsushita, Y				Guo, Heng; Okura, Fumio; Shi, Boxin; Funatomi, Takuya; Mukaigawa, Yasuhiro; Matsushita, Yasuyuki			Multispectral Photometric Stereo for Spatially-Varying Spectral Reflectances	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multispectral photometric stereo; Cultural asset; Spatially-varying spectral reflectance		Multispectral photometric stereo (MPS) aims at recovering the surface normal of a scene measured under multiple light sources with different wavelengths. While it opens up a capability of a single-shot measurement of surface normal, the problem has been known ill-posed. To make the problem well-posed, existing MPS methods rely on restrictive assumptions, such as shape prior, surfaces having a monochromatic with uniform albedo. This paper alleviates these restrictive assumptions in existing methods. We show that the problem becomes well-posed for surfaces with uniform chromaticity but spatially-varying albedos based on our new formulation. Specifically, if at least three (or two) scene points share the same chromaticity, the proposed method uniquely recovers their surface normals with the illumination of no less than four (or five) spectral lights in a closed-form. In addition, we show that a more general setting of spatially-varying both chromaticities and albedos can become well-posed if the light spectra and camera spectral sensitivity are calibrated. For this general setting, we derive a unique and closed-form solution for MPS using the linear bases extracted from a spectral reflectance database. Experiments on both synthetic and real captured data with spatially-varying reflectance demonstrate the effectiveness of our method and show the potential applicability for multispectral heritage preservation.	[Guo, Heng; Okura, Fumio; Matsushita, Yasuyuki] Osaka Univ, Grad Sch Informat Sci & Technol, Osaka, Japan; [Shi, Boxin] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing, Peoples R China; [Shi, Boxin] Peking Univ, Inst Artificial Intelligence, Beijing, Peoples R China; [Shi, Boxin] Beijing Acad Artificial Intelligence, Beijing, Peoples R China; [Shi, Boxin] Peng Cheng Lab, Shenzhen, Peoples R China; [Funatomi, Takuya; Mukaigawa, Yasuhiro] Nara Inst Sci & Technol, Grad Sch Sci & Technol, Ikoma, Japan	Osaka University; Peking University; Peking University; Peng Cheng Laboratory; Nara Institute of Science & Technology	Guo, H (corresponding author), Osaka Univ, Grad Sch Informat Sci & Technol, Osaka, Japan.	heng.guo@ist.osaka-u.ac.jp; okura@ist.osaka-u.ac.jp; shiboxin@pku.edu.cn; funatomi@is.naist.jp; mukaigawa@is.naist.jp; yasumat@ist.osaka-u.ac.jp	Guo, Heng/GPG-3465-2022	Guo, Heng/0000-0003-0047-3927	Japan Science and Technology Agency CREST [JPMJCR1764]; National Natural Science Foundation of China [62136001, 62088102, 61872012]	Japan Science and Technology Agency CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by Japan Science and Technology Agency CREST Grant Number JPMJCR1764, and National Natural Science Foundation of China under Grant Number 62136001, 62088102, and 61872012.	Anderson R., 2011, MVA, P369; Anderson R, 2011, IEEE I CONF COMP VIS, P2182, DOI 10.1109/ICCV.2011.6126495; Antensteiner D, 2019, IEEE COMPUT SOC CONF, P481, DOI 10.1109/CVPRW.2019.00065; Barsky S, 2003, IEEE T PATTERN ANAL, V25, P1239, DOI 10.1109/TPAMI.2003.1233898; Chakrabarti A, 2016, INT CONF 3D VISION, P258, DOI 10.1109/3DV.2016.34; Cho D, 2020, IEEE T PATTERN ANAL, V42, P232, DOI 10.1109/TPAMI.2018.2873295; Cox M. A. A., 2008, HDB DATA VISUALIZATI, P315; DREW MS, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P985, DOI 10.1109/CVPR.1994.323939; Dupuy J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275059; Fyffe G., 2011, P IEEE INT C COMP PH, P16, DOI [10.1109/ICCPHOT.2011.5753116, DOI 10.1109/ICCPH0T.2011.5753116]; Guo H, 2021, PROC CVPR IEEE, P963, DOI 10.1109/CVPR46437.2021.00102; Hain M., 2003, MEAS SCI REV, V3, P9; Hernandez C, 2007, IEEE I CONF COMP VIS, P873; Hernandez C, 2011, IEEE T PATTERN ANAL, V33, P419, DOI 10.1109/TPAMI.2010.181; JOHNSON RM, 1963, PSYCHOMETRIKA, V28, P259, DOI 10.1007/BF02289573; Ju YK, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107162; Ju YK, 2020, NEUROCOMPUTING, V375, P62, DOI 10.1016/j.neucom.2019.09.084; Ju YK, 2018, IEEE ACCESS, V6, P30804, DOI 10.1109/ACCESS.2018.2840138; KONTSEVICH LL, 1994, J OPT SOC AM A, V11, P1047, DOI 10.1364/JOSAA.11.001047; Matusik W., 2003, THESIS MASSACHUSETTS; McCamy C. S., 1976, Journal of Applied Photographic Engineering, V2, P95; Miyazaki D, 2019, J IMAGING, V5, DOI 10.3390/jimaging5070064; Miyazaki D, 2010, INT J COMPUT VISION, V86, P229, DOI 10.1007/s11263-009-0262-9; Mohammadi M., 2005, PROTOTYPE CALIBRATIO; Ozawa K, 2018, COMPUT VIS IMAGE UND, V171, P140, DOI 10.1016/j.cviu.2018.04.003; Picollo M, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20102843; Press W.H., 2007, NUMERICAL RECIPES; Shi BX, 2019, IEEE T PATTERN ANAL, V41, P271, DOI 10.1109/TPAMI.2018.2799222; Shi BX, 2014, IEEE T PATTERN ANAL, V36, P1078, DOI 10.1109/TPAMI.2013.196; Silver W.M., 1980, THESIS MIT; Vogiatzis G, 2012, INT J COMPUT VISION, V97, P91, DOI 10.1007/s11263-011-0482-7; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Xie WY, 2014, PROC CVPR IEEE, P2203, DOI 10.1109/CVPR.2014.282; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	35	0	0	4	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2166	2183		10.1007/s11263-022-01634-4	http://dx.doi.org/10.1007/s11263-022-01634-4		JUL 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG					2022-12-18	WOS:000824828200001
J	Sipiran, I; Mendoza, A; Apaza, A; Lopez, C				Sipiran, Ivan; Mendoza, Alexis; Apaza, Alexander; Lopez, Cristian			Data-Driven Restoration of Digital Archaeological Pottery with Point Cloud Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Cultural heritage restoration; Shape completion; Point cloud analysis; Shape analysis	SYMMETRY DETECTION; COMPLETION; SHAPE	The Josefina Ramos de Cox museum in Lima, Peru, decided to digitize hundreds of archaeological pieces from pre-Colombian cultures to support further research and create virtual educational environments. However, the 3D scanning procedure led to imperfections in the objects' surface, mainly due to the difficulty of manipulating the fragile objects during the acquisition. The problem was that many of the scanned artifacts do not contain the base because the contact surface during acquisition was not visible to the scanner. This paper proposes a method to repair the digital objects' surface using a data-driven approach. We design and train a point cloud neural network that learns to synthesize the missing geometry in an end-to-end manner. Our model consists of a novel architecture and training protocol that addresses the problem of point cloud completion. We propose an end-to-end neural network architecture that focuses on calculating the missing geometry and merging the known input and the predicted point cloud. Our method is composed of two neural networks: the missing part prediction network and the merging-refinement network. The first module focuses on extracting information from the incomplete input to infer the missing geometry. The second module merges both point clouds and improves the distribution of the points. Our approach is effective in repairing pottery objects with large imperfections during the scanning. Besides, our experiments on ShapeNet and Completion3D datasets show that our method is effective in a general setting for shape completion.	[Sipiran, Ivan] Univ Chile, Dept Comp Sci, Santiago, Chile; [Mendoza, Alexis; Apaza, Alexander] Univ Nacl San Agustin, Arequipa, Peru; [Lopez, Cristian] Univ Ingn & Tecnol, UTEC, Barranco, Peru	Universidad de Chile; Universidad Nacional de San Agustin de Arequipa; Universidad de Ingenieria Tecnologia UTEC	Sipiran, I (corresponding author), Univ Chile, Dept Comp Sci, Santiago, Chile.	isipiran@dcc.uchile.cl; amendozavil@unsa.edu.pe; aapazato@unsa.edu.pe; clopez@ulasalle.edu.pe	Sipiran, Ivan/GRR-8629-2022	Lopez-Del-Alamo, Cristian/0000-0002-2568-650X	ANID Chile-Research Initiation Program [11220211]; Proyecto de Mejoramiento y Ampliacion de los Servicios del Sistema Nacional de Ciencia Tecnologia e Innovacion Tecnologica(Banco Mundial, Concytec-Peru) [062-2018-FONDECYT-BM-IADT-AV]; Josefina Ramos de Cox Museum	ANID Chile-Research Initiation Program; Proyecto de Mejoramiento y Ampliacion de los Servicios del Sistema Nacional de Ciencia Tecnologia e Innovacion Tecnologica(Banco Mundial, Concytec-Peru); Josefina Ramos de Cox Museum	The work of Ivan Sipiran has been partially supported by ANID Chile-Research Initiation Program-Grant No 11220211. The work of Cristian Lopez has been supported by Proyecto de Mejoramiento y Ampliacion de los Servicios del Sistema Nacional de Ciencia Tecnologia e Innovacion Tecnologica(Banco Mundial, Concytec-Peru), Nr. Grant 062-2018-FONDECYT-BM-IADT-AV. Thanks to the Josefina Ramos de Cox Museum for the continuous support during the realization of this project.	Achlioptas P, 2018, PR MACH LEARN RES, V80; Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Gropp A, 2020, PR MACH LEARN RES, V119; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19; Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959; Harary G, 2014, COMPUT GRAPH FORUM, V33, P45, DOI 10.1111/cgf.12430; Harary G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532548; Hermoza R., 2018, P COMPUTER GRAPHICS, P5, DOI 10.1145/3208159.3208173; Hu T, 2020, AAAI CONF ARTIF INTE, V34, P10997; Hu T, 2019, IEEE INT CONF COMP V, P4114, DOI 10.1109/ICCVW.2019.00506; Huang H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366198; Jiang W, 2013, GRAPH MODELS, V75, P177, DOI 10.1016/j.gmod.2013.03.001; Koutsoudis A, 2010, J CULT HERIT, V11, P329, DOI 10.1016/j.culher.2010.02.002; Li E, 2014, INT C DIGITAL HOME, P157, DOI 10.1109/ICDH.2014.38; Liu F, 2019, IEEE I CONF COMP VIS, P9407, DOI 10.1109/ICCV.2019.00950; Liu MH, 2020, AAAI CONF ARTIF INTE, V34, P11596; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mavridis P, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12741; Pan L, 2021, PROC CVPR IEEE, P8520, DOI 10.1109/CVPR46437.2021.00842; Papaioannou G, 2017, ACM J COMPUT CULT HE, V10, DOI 10.1145/3009905; Pauly M., 2005, S GEOM PROC, P23; Pratikakis I, 2018, MULTIMED TOOLS APPL, V77, P12991, DOI 10.1007/s11042-017-4928-y; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446; Sipiran I., 2018, P 11 EUROGRAPHICS WO, P87; Sipiran I, 2017, IEEE INT CONF COMP V, P2925, DOI 10.1109/ICCVW.2017.345; Sipiran I, 2014, COMPUT GRAPH FORUM, V33, P131, DOI 10.1111/cgf.12481; Son K, 2013, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2013.40; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sun YB, 2020, IEEE WINT CONF APPL, P61, DOI 10.1109/WACV45572.2020.9093430; Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047; Thrun S, 2005, IEEE I CONF COMP VIS, P1824; Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087; Wen X, 2021, PROC CVPR IEEE, P7439, DOI 10.1109/CVPR46437.2021.00736; Wen X, 2021, PROC CVPR IEEE, P13075, DOI 10.1109/CVPR46437.2021.01288; Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545; Xie H., 2020, ECCV, P365; Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618454; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238; Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227; Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088; Zheng QA, 2010, ACTA OCEANOL SIN, V29, P1, DOI 10.1007/s13131-010-0044-9; Zitian Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7659, DOI 10.1109/CVPR42600.2020.00768	49	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2149	2165		10.1007/s11263-022-01637-1	http://dx.doi.org/10.1007/s11263-022-01637-1		JUN 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG					2022-12-18	WOS:000819324500001
J	Kim, N; Hwang, S; Kwak, S				Kim, Namyup; Hwang, Sehyun; Kwak, Suha			Learning to Detect Semantic Boundaries with Image-Level Class Labels	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Weakly supervised learning; Semantic boundary detection; Multiple instance learning		This paper presents the first attempt to learn semantic boundary detection using image-level class labels as supervision. Our method starts by estimating coarse areas of object classes through attentions drawn by an image classification network. Since boundaries will locate somewhere between such areas of different classes, our task is formulated as a multiple instance learning (MIL) problem, where pixels on a line segment connecting areas of two different classes are regarded as a bag of boundary candidates. Moreover, we design a new neural network architecture that can learn to estimate semantic boundaries reliably even with uncertain supervision given by the MIL strategy. Our network is used to generate pseudo semantic boundary labels of training images, which are in turn used to train fully supervised models. The final model trained with our pseudo labels achieves an outstanding performance on the SBD dataset, where it is as competitive as some of previous arts trained with stronger supervision.	[Kim, Namyup; Hwang, Sehyun; Kwak, Suha] POSTECH, Pohang Si, South Korea	Pohang University of Science & Technology (POSTECH)	Kwak, S (corresponding author), POSTECH, Pohang Si, South Korea.	namyup@postech.ac.kr; sehyun03@postech.ac.kr; suha.kwak@postech.ac.kr			IITP Grant - Ministry of Science and ICT, Korea [NRF-2018R1A5A1060031, IITP-2020-0-00842, IITP-2022-0-00926, IITP-2019-0-01906]; NRF Grant	IITP Grant - Ministry of Science and ICT, Korea(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); NRF Grant(National Research Foundation of Korea)	This work was supported by the NRF Grant and the IITP Grant funded by Ministry of Science and ICT, Korea (NRF-2018R1A5A1060031, IITP-2020-0-00842, IITP-2022-0-00926, IITP-2019-0-01906 Artificial Intelligence Graduate School ProgramPOSTECH).	Acuna D, 2019, PROC CVPR IEEE, P11067, DOI 10.1109/CVPR.2019.01133; Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231; Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Banica D, 2015, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2015.7298974; Bertasius G, 2016, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2016.392; Bertasius G, 2015, IEEE I CONF COMP VIS, P504, DOI 10.1109/ICCV.2015.65; Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067; Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chen Liyi, 2020, P EUR C COMP VIS, P347, DOI DOI 10.1007/978-3-030-58574-7_21; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Galasso F, 2013, IEEE I CONF COMP VIS, P3527, DOI 10.1109/ICCV.2013.438; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hong S, 2017, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2017.239; Hu Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P782; Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733; Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22; Karsch K, 2013, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2013.281; Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181; Khoreva A, 2016, PROC CVPR IEEE, P183, DOI 10.1109/CVPR.2016.27; Koh JY, 2017, LECT NOTES COMPUT SC, V10496, P153, DOI 10.1007/978-3-319-66709-6_13; Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1984; Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849; Manen S, 2013, IEEE I CONF COMP VIS, P2536, DOI 10.1109/ICCV.2013.315; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Remez T, 2018, LECT NOTES COMPUT SC, V11211, P39, DOI 10.1007/978-3-030-01234-2_3; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shan Q, 2014, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR.2014.511; Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024; Su SC, 2016, PROC CVPR IEEE, pCP40, DOI 10.1109/CVPR.2016.382; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315; Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Xu YQ, 2017, IEEE GLOBE WORK; Yang Jianwei, 2016, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2016.28; Yu ZD, 2018, LECT NOTES COMPUT SC, V11207, P400, DOI 10.1007/978-3-030-01219-9_24; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	59	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2131	2148		10.1007/s11263-022-01631-7	http://dx.doi.org/10.1007/s11263-022-01631-7		JUN 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG					2022-12-18	WOS:000817041400001
J	Zhuang, BH; Shen, CH; Tan, MK; Chen, P; Liu, LQ; Reid, I				Zhuang, Bohan; Shen, Chunhua; Tan, Mingkui; Chen, Peng; Liu, Lingqiao; Reid, Ian			Structured Binary Neural Networks for Image Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Binary neural networks; Quantization; Image classification; Semantic segmentation; Object detection	MIXTURES	In this paper, we propose to train binarized convolutional neural networks (CNNs) that are of significant importance for deploying deep learning to mobile devices with limited power capacity and computing resources. Previous works on quantizing CNNs often seek to approximate the floating-point information of weights and/or activations using a set of discrete values. Such methods, termed value approximation here, typically are built on the same network architecture of the full-precision counterpart. Instead, we take a new "structured approximation" view for network quantization - it is possible and valuable to exploit flexible architecture transformation when learning low-bit networks, which can achieve even better performance than the original networks in some cases. In particular, we propose a "group decomposition" strategy, termed GroupNet, which divides a network into desired groups. Interestingly, with our GroupNet strategy, each full-precision group can be effectively reconstructed by aggregating a set of homogeneous binary branches. We also propose to learn effective connections among groups to improve the representation capability. To improve the model capacity, we propose to dynamically execute sparse binary branches conditioned on input features while preserving the computational cost. More importantly, the proposed GroupNet shows strong flexibility for a few vision tasks. For instance, we extend the GroupNet for accurate semantic segmentation by embedding the rich context into the binary structure. The proposed GroupNet also shows strong performance on object detection. Experiments on image classification, semantic segmentation, and object detection tasks demonstrate the superior performance of the proposed methods over various quantized networks in the literature. Moreover, the speedup and runtime memory cost evaluation comparing with related quantization strategies is analyzed on GPU platforms, which serves as a strong benchmark for further research.	[Zhuang, Bohan; Chen, Peng] Monash Univ, Fac Informat Technol, Melbourne, Vic, Australia; [Shen, Chunhua] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China; [Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou, Peoples R China; [Tan, Mingkui] Minist Educ, Key Lab Big Data & Intelligent Robot, Beijing, Peoples R China; [Liu, Lingqiao; Reid, Ian] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia	Monash University; Zhejiang University; South China University of Technology; University of Adelaide	Shen, CH (corresponding author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.	bohan.zhuang@monash.edu; chunhuashen@zju.edu.cn; mingkuitan@scut.edu.cn; blueardour@gmail.com; lingqiao.liu@adelaide.edu.au; ian.reid@adelaide.edu.au		liu, lingqiao/0000-0003-3584-795X	Key Realm R &D Program of Guangzhou [202007030007]; Key-Area Research and Development Program Guangdong Province [2018B010107001]	Key Realm R &D Program of Guangzhou; Key-Area Research and Development Program Guangdong Province	This work was partially supported by Key Realm R &D Program of Guangzhou 202007030007, Key-Area Research and Development Program Guangdong Province 2018B010107001.	Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; BAI Y., 2019, P INT C LEARN REPR; Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1; Bethge J., 2018, ARXIV; Bethge J., 2018, ARXIV PREPRINT ARXIV; Bulat Adrian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P309, DOI 10.1007/978-3-030-58592-1_19; Bulat Adrian, 2021, ICLR; Cai ZW, 2017, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR.2017.574; Chandra S, 2016, LECT NOTES COMPUT SC, V9911, P402, DOI 10.1007/978-3-319-46478-7_25; Chen GB, 2017, ADV NEUR IN, V30; Chen HL, 2020, AAAI CONF ARTIF INTE, V34, P10526; Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723; Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen P, 2021, PROC CVPR IEEE, P104, DOI 10.1109/CVPR46437.2021.00017; Chen TQ, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P579; Choi Jungwook, 2018, ARXIV180506085; Chollet F., 2017, PROC CVPR IEEE, P1251, DOI DOI 10.1109/CVPR.2017.195; Courbariaux M, 2015, ADV NEUR IN, V28; Deng L, 2018, NEURAL NETWORKS, V100, P49, DOI 10.1016/j.neunet.2018.01.010; Ding RZ, 2019, PROC CVPR IEEE, P11400, DOI 10.1109/CVPR.2019.01167; Ehliar A, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY (FPT), P131, DOI 10.1109/FPT.2014.7082765; Esser, 2020, P INT C LEARN REPR; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fedus William, 2021, ARXIV PREPRINT ARXIV; Fromm J., 2018, ADV NEURAL INFORM PR, P4006; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Govindu G., 2004, Proceedings. 18th International Parallel and Distributed Processing Symposium; Guo YW, 2017, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2017.430; HAN SY, 2016, IEEE ICC; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Helwegen K, 2019, ADV NEUR IN, V32; Hou L., 2018, PROC INT C LEARN REP, P1; Hou L, 2017, IEEE GLOBE WORK; Howard A.G., 2017, MOBILENETS EFFICIENT; Hubara I, 2016, ADV NEUR IN, V29; Ignatov A, 2019, LECT NOTES COMPUT SC, V11133, P288, DOI 10.1007/978-3-030-11021-5_19; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Jung S, 2019, PROC CVPR IEEE, P4345, DOI 10.1109/CVPR.2019.00448; Kim H, 2020, DES AUT CON; [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1; Li RD, 2019, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR.2019.00292; Li ZF, 2017, IEEE I CONF COMP VIS, P2603, DOI 10.1109/ICCV.2017.282; LIN GS, 2016, PROC CVPR IEEE, P3194, DOI DOI 10.1109/CVPR.2016.348; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin SH, 2019, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR.2019.00290; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin XF, 2017, ADV NEUR IN, V30; Liu CL, 2019, PROC CVPR IEEE, P2686, DOI 10.1109/CVPR.2019.00280; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Liu ZC, 2020, INT J COMPUT VISION, V128, P202, DOI 10.1007/s11263-019-01227-8; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Martinez B., 2020, PROC INT C LEARN REP; Mehta S., 2018, ARXIV PREPRINT ARXIV; Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34; Mishra A., 2018, PROC REPREN INT C LE; Mullapudi RT, 2018, PROC CVPR IEEE, P8080, DOI 10.1109/CVPR.2018.00843; Pan X., 2018, LECT NOTES COMPUT SC, P267, DOI DOI 10.1007/978-3-030-01237-3_17; Park E, 2017, PROC CVPR IEEE, P7197, DOI 10.1109/CVPR.2017.761; Paszke A., 2017, AUTOMATIC DIFFERENTI; Polino Antonio, 2018, P REPR INT C LEARN; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tang W, 2017, AAAI CONF ARTIF INTE, P2625; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Umuroglu Y, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P65, DOI 10.1145/3020078.3021744; Wan DW, 2018, LECT NOTES COMPUT SC, V11206, P322, DOI 10.1007/978-3-030-01216-8_20; Yang HJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1209, DOI 10.1145/3123266.3129393; Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20; Yu C, 2017, INT CONF E BUS ENG, P1, DOI 10.1109/ICEBE.2017.11; Zechun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P143, DOI 10.1007/978-3-030-58568-6_9; Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23; Zhang JH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2272, DOI 10.1145/3343031.3350534; Zhang ZL, 2018, LECT NOTES COMPUT SC, V11214, P273, DOI 10.1007/978-3-030-01249-6_17; Zhou K, 2016, DESTECH TRANS COMP; Zhou S., 2016, ARXIV160606160; Zhu B, 2021, INT J NUMER ANAL MET, V45, P478, DOI 10.1002/nag.3169; Zhu Chenzhuo, 2017, ICLR; Zhu SL, 2019, PROC CVPR IEEE, P4918, DOI 10.1109/CVPR.2019.00506; Zhuang BH, 2019, PROC CVPR IEEE, P413, DOI 10.1109/CVPR.2019.00050; Zhuang BH, 2018, PROC CVPR IEEE, P7920, DOI 10.1109/CVPR.2018.00826; Zhuang ZW, 2018, ADV NEUR IN, V31	98	0	0	6	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2022	130	9					2081	2102		10.1007/s11263-022-01638-0	http://dx.doi.org/10.1007/s11263-022-01638-0		JUN 2022	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	3T2LG		Green Submitted			2022-12-18	WOS:000814459300001
J	Li, DS; Zhang, Y; Law, KL; Wang, XG; Qin, HW; Li, HS				Li, Dasong; Zhang, Yi; Law, Ka Lung; Wang, Xiaogang; Qin, Hongwei; Li, Hongsheng			Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Burst denoising; Poisson-Gaussian distribution; Variance stabilization; Sequential denoising; Multi-frequency denoising	IMAGE; ENHANCEMENT	With the growing popularity of smartphones, capturing high-quality images is of vital importance to smartphones. The cameras of smartphones have small apertures and small sensor cells, which lead to the noisy images in low light environment. Denoising based on a burst of multiple frames generally outperforms single frame denoising but with the larger compututional cost. In this paper, we propose an efficient yet effective burst denoising system. We adopt a three-stage design: noise prior integration, multi-frame alignment and multi-frame denoising. First, we integrate noise prior by pre-processing raw signals into a variance-stabilization space, which allows using a small-scale network to achieve competitive performance. Second, we observe that it is essential to adopt an explicit alignment for burst denoising, but it is not necessary to integrate an learning-based method to perform multi-frame alignment. Instead, we resort to a conventional and efficient alignment method and combine it with our multi-frame denoising network. At last, we propose a denoising strategy that processes multiple frames sequentially. Sequential denoising avoids filtering a large number of frames by decomposing multiple frames denoising into several efficient sub-network denoising. As for each sub-network, we propose an efficient multi-frequency denoising network to remove noise of different frequencies. Our three-stage design is efficient and shows strong performance on burst denoising. Experiments on synthetic and real raw datasets demonstrate that our method outperforms state-of-the-art methods, with less computational cost. Furthermore, the low complexity and high-quality performance make deployment on smartphones possible.	[Li, Dasong; Zhang, Yi; Wang, Xiaogang; Li, Hongsheng] Chinese Univ Hong Kong, Multimedia Lab, Shatin, Hong Kong, Peoples R China; [Law, Ka Lung; Qin, Hongwei] SenseTime Res, Beijing, Peoples R China; [Wang, Xiaogang; Li, Hongsheng] Ctr Perceptual & Interact Intelligence Ltd, Shatin, Hong Kong, Peoples R China	Chinese University of Hong Kong	Li, HS (corresponding author), Chinese Univ Hong Kong, Multimedia Lab, Shatin, Hong Kong, Peoples R China.; Qin, HW (corresponding author), SenseTime Res, Beijing, Peoples R China.; Li, HS (corresponding author), Ctr Perceptual & Interact Intelligence Ltd, Shatin, Hong Kong, Peoples R China.	dasongli@link.cuhk.edu.hk; zhangyi@link.cuhk.edu.hk; luojialong@sensetime.com; xgwang@ee.cuhk.edu.hk; qinhongwei@sensetime.com; hsli@ee.cuhk.edu.hk			Centre for Perceptual and Interactive Intelligence Limited; Research Grants Council of Hong Kong [14204021, 14207319, 14203118, 14208619]; Research Impact Fund [R5001-18]; CUHK Strategic Fund	Centre for Perceptual and Interactive Intelligence Limited; Research Grants Council of Hong Kong(Hong Kong Research Grants Council); Research Impact Fund; CUHK Strategic Fund	This work is supported in part by Centre for Perceptual and Interactive Intelligence Limited, in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants (Nos. 14204021, 14207319, 14203118, 14208619), in part by Research Impact Fund Grant No. R5001-18, in part by CUHK Strategic Fund.	[Anonymous], 2014, ARM NEON INTRINSICS; ANSCOMBE FJ, 1948, BIOMETRIKA, V35, P246, DOI 10.2307/2332343; Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; BARTLETT M. S., 1936, Journal of the Royal Statistical Society, V3, P68; BARTLETT MS, 1947, BIOMETRICS, V3, P39, DOI 10.2307/3001536; Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56; Curtiss JH, 1943, ANN MATH STAT, V14, P107, DOI 10.1214/aoms/1177731452; Dabov K., 2007, P 15 EUR SIGN PROC C, V1, P7; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Doob JL, 1935, ANN MATH STAT, V6, P160, DOI 10.1214/aoms/1177732594; Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399; FREEMAN MF, 1950, ANN MATH STAT, V21, P607, DOI 10.1214/aoms/1177729756; Gu SH, 2019, IEEE I CONF COMP VIS, P2511, DOI 10.1109/ICCV.2019.00260; Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Krasin Ivan, 2017, OPENIMAGES PUBLIC DA, V2, P18; Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060; Liu XH, 2014, IEEE T IMAGE PROCESS, V23, P4361, DOI 10.1109/TIP.2014.2347204; Liu ZW, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661277; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Maggioni M., 2021, EFFICIENT MULTISTAGE; Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324; Maggioni M, 2011, PROC SPIE, V7870, DOI 10.1117/12.872569; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; Makitalo M, 2012, INT CONF ACOUST SPEE, P1081, DOI 10.1109/ICASSP.2012.6288074; Makitalo M, 2013, IEEE T IMAGE PROCESS, V22, P91, DOI 10.1109/TIP.2012.2202675; Marinc T., 2019, CORRABS190205392; Menze Moritz, 2015, CVPR; Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; OLiba O., 2019, SIGGRAPH ASIA; Paszke A, 2019, ADV NEUR IN, V32; Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640; Rosten E, 2005, IEEE I CONF COMP VIS, P1508; Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Starck J.-L., 1998, IMAGE PROCESSING DAT; Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143; Vogels T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201388; Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247; Wei KX, 2020, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR42600.2020.00283; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2; Yue HJ, 2020, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR42600.2020.00237; Yuzhi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P1, DOI 10.1007/978-3-030-58539-6_1; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang Yi, 2021, P IEEECVF INT C COMP, P4593; Zhetong Liang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P150, DOI 10.1007/978-3-030-58595-2_10; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953	54	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					2060	2080		10.1007/s11263-022-01627-3	http://dx.doi.org/10.1007/s11263-022-01627-3		JUN 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP		Green Submitted			2022-12-18	WOS:000813757900001
J	Xu, QS; Su, WJ; Qi, YH; Tao, WB; Pollefeys, M				Xu, Qingshan; Su, Wanjuan; Qi, Yuhang; Tao, Wenbing; Pollefeys, Marc			Learning Inverse Depth Regression for Pixelwise Visibility-Aware Multi-View Stereo Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-view stereo networks; Visibility estimation; Anti-noise training strategy; Inverse depth regression; Average group-wise correlation; Ordinal-based high-resolution depth map refinement		Recently, learning-based multi-view stereo methods have achieved promising results. However, most of them overlook the visibility difference among different views, which leads to an indiscriminate multi-view similarity definition and greatly limits their performance on datasets with strong viewpoint variations. To deal with this problem, a pixelwise visibility-aware multi-view stereo network is proposed for robust dense 3D reconstruction. We present a pixelwise visibility estimation network to learn the visibility information for different neighboring images before computing the multi-view similarity, and then construct an adaptive weighted cost volume with the visibility information. Unlike previous methods that treat multi-view depth inference as a depth regression problem or an inverse depth classification problem, we recast multi-view depth inference as an inverse depth regression task. This allows our network to achieve sub-pixel estimation and be applicable to large-scale scenes. To achieve scalable high-resolution depth map estimation, we construct cost volumes by group-wise correlation and design an ordinal-based uncertainty estimation to progressively refine depth maps. Through extensive experiments on DTU dataset, Tanks and Temples dataset and ETH3D benchmark, we show that our method generalizes well to various datasets and achieves promising results, demonstrating its superior performance on robust dense 3D reconstruction.	[Xu, Qingshan; Su, Wanjuan; Qi, Yuhang; Tao, Wenbing] Huazhong Univ Sci & Technol, Wuhan, Peoples R China; [Pollefeys, Marc] Swiss Fed Inst Technol, Zurich, Switzerland; [Pollefeys, Marc] Microsoft, Redmond, WA USA	Huazhong University of Science & Technology; Swiss Federal Institutes of Technology Domain; ETH Zurich; Microsoft	Tao, WB (corresponding author), Huazhong Univ Sci & Technol, Wuhan, Peoples R China.	qingshanxu@hust.edu.cn; suwanjuan@hust.edu.cn; qiyuhang@hust.edu.cn; wenbingtao@hust.edu.cn; marc.pollefeys@inf.ethz.ch		Tao, Wenbing/0000-0003-3284-864X	National Natural Science Foundation of China [62176096, 61991412]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China under Grants 62176096 and 61991412.	Aanaes H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; Arno K., 2017, TANKS TEMPLES BENCHM; Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen R, 2021, IEEE T PATTERN ANAL, V43, P3695, DOI 10.1109/TPAMI.2020.2988729; Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162; Cheng S, 2020, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR42600.2020.00260; Christian S., 2021, ARXIV211114420; COLLINS RT, 1996, PROC CVPR IEEE, P358, DOI DOI 10.1109/CVPR.1996.517097; Fangjinhua W., 2021, ARXIV211205126; Fu ZH, 2018, IEEE WINT CONF APPL, P1321, DOI 10.1109/WACV.2018.00149; Fuhrmann S, 2014, EUROGRAPHICS WORKSHO, DOI 10.1016/j.cag.2015.09.003; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106; Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257; Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339; Haala N, 2012, PHOTOGRAMM FERNERKUN, P331, DOI 10.1127/1432-8364/2012/0121; Hartmann W, 2017, IEEE I CONF COMP VIS, P1595, DOI 10.1109/ICCV.2017.176; Heise P, 2015, IEEE I CONF COMP VIS, P882, DOI 10.1109/ICCV.2015.107; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298; Im S., 2019, ARXIV190500538; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Jianfeng Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P674, DOI 10.1007/978-3-030-58548-8_39; Jingyang Z., 2020, ARXIV200807928; Kar A, 2017, ADV NEUR IN, V30; Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim S, 2019, PROC CVPR IEEE, P205, DOI 10.1109/CVPR.2019.00029; Kim S, 2019, IEEE T IMAGE PROCESS, V28, P1299, DOI 10.1109/TIP.2018.2878325; Knapitsch Arno, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3072959.3073599; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82; Kuhn A, 2020, INT CONF 3D VISION, P404, DOI 10.1109/3DV50981.2020.00050; Li ZX, 2020, IEEE T IMAGE PROCESS, V29, P7176, DOI 10.1109/TIP.2020.2999853; Luo KY, 2020, PROC CVPR IEEE, P1587, DOI 10.1109/CVPR42600.2020.00166; Luo KY, 2019, IEEE I CONF COMP VIS, P10451, DOI 10.1109/ICCV.2019.01055; LUO WJ, 2016, PROC CVPR IEEE, P5695, DOI DOI 10.1109/CVPR.2016.614; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Poggi M, 2017, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2017.559; Pollefeys M, 2016, BRIT MACH VIS C BMVC; Qingshan X., 2020, ARXIV200707714; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Sormann C, 2020, INT CONF 3D VISION, P394, DOI 10.1109/3DV50981.2020.00049; Thomas S., ETH3D BENCHMARK; Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8; Tulyakov S, 2018, ADV NEUR IN, V31; Wang Fangjinhua, 2021, P IEEE C COMP VIS PA, P14194; Xu QS, 2020, AAAI CONF ARTIF INTE, V34, P12508; Xu QS, 2020, AAAI CONF ARTIF INTE, V34, P12516; Xu QS, 2019, PROC CVPR IEEE, P5478, DOI 10.1109/CVPR.2019.00563; Xu ZY, 2020, PROC CVPR IEEE, P5980, DOI 10.1109/CVPR42600.2020.00602; Xue YZ, 2019, IEEE I CONF COMP VIS, P4311, DOI 10.1109/ICCV.2019.00441; Yang JY, 2020, PROC CVPR IEEE, P4876, DOI 10.1109/CVPR42600.2020.00493; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yao Y, 2020, PROC CVPR IEEE, P1787, DOI 10.1109/CVPR42600.2020.00186; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; Yu ZH, 2020, PROC CVPR IEEE, P1946, DOI 10.1109/CVPR42600.2020.00202; Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767; Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027; Zhang XD, 2021, IEEE WINT CONF APPL, P3781, DOI 10.1109/WACV48630.2021.00383; Zheng EL, 2014, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2014.196	69	0	0	6	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					2040	2059		10.1007/s11263-022-01628-2	http://dx.doi.org/10.1007/s11263-022-01628-2		JUN 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP					2022-12-18	WOS:000812898000001
J	Qi, YG; Su, GY; Wang, Q; Yang, J; Pang, KY; Song, YZ				Qi, Yonggang; Su, Guoyao; Wang, Qiang; Yang, Jie; Pang, Kaiyue; Song, Yi-Zhe			Generative Sketch Healing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sketch healing; Graph convolutional networks; Perceptual distance; Sketch generative model; Sketch representation		To perceive and create a whole from parts is a prime trait of the human visual system. In this paper, we teach machines to perform a similar task by recreating a vectorised human sketch from its incomplete parts, dubbed as sketch healing. This is fundamentally different to prior works on image completion since (i) sketches exhibit a severe lack of visual cues and are of a sequential nature, and more importantly (ii) we ask for an agent that does not just fill in a missing part, but to recreate a novel sketch that closely resembles the partial input from scratch. We identify two key facets of sketch healing that are fundamental for effective learning. The first is encoding the incomplete sketches in a graph model that leverages the sequential nature of sketches to associate key visual parts centred around stroke junctions. The intuition is then that message passing within the graph topology will naturally provide the healing power when it comes to missing parts (nodes and edges). Second we show healing is a trade-off process between global semantic preservation and local structure reconstruction, and that it can only be effectively solved when both are taken into account and optimised together. Both qualitative and quantitative results suggest that the proposed method significantly outperforms the state-of-the-art alternatives on sketch healing. Last but not least, we show that sketch healing can be re-purposed to support the interesting application of sketch-based creativity assistant, which aims at generating a novel sketch from two partial sketches even without specifically trained so.	[Qi, Yonggang; Su, Guoyao; Wang, Qiang; Yang, Jie] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing, Peoples R China; [Qi, Yonggang; Pang, Kaiyue; Song, Yi-Zhe] Univ Surrey, SketchX Lab, CVSSP, Guildford, Surrey, England	Beijing University of Posts & Telecommunications; University of Surrey	Qi, YG (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing, Peoples R China.; Qi, YG (corresponding author), Univ Surrey, SketchX Lab, CVSSP, Guildford, Surrey, England.	qiyg@bupt.edu.cn		Qi, Yonggang/0000-0001-8280-3541	National Natural Science Foundation of China [61601042]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	Funding was provided by the National Natural Science Foundation of China (Grant No. 61601042).	[Anonymous], 2017, ARXIV170904121; Berger I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461964; Bhunia Ayan Kumar, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9776, DOI 10.1109/CVPR42600.2020.00980; Bhunia AK, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417840; Blau Y, 2019, PR MACH LEARN RES, V97; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Cao N., 2019, AAAI; Cao Y., 2010, INT C MULT, P1605; Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603; Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532; Das A., 2020, ECCV; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540; Ge S., 2020, ICLR; GORI M, 2005, IEEE IJCNN, P729, DOI DOI 10.1109/IJCNN.2005.1555942; Graves A, 2013, ARXIV13080850; Ha David, 2018, ICLR; Iandola Forrest, 2017, 2017 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS), DOI 10.1145/3125502.3125606; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kipf TN, 2016, P INT C LEARN REPR; Lahiri Avisek, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13693, DOI 10.1109/CVPR42600.2020.01371; Li K, 2018, LECT NOTES COMPUT SC, V11212, P593, DOI 10.1007/978-3-030-01237-3_36; Li YJ, 2017, ADV NEUR IN, V30; Liu F, 2019, PROC CVPR IEEE, P5823, DOI 10.1109/CVPR.2019.00598; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Meng-Li Shih, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8025, DOI 10.1109/CVPR42600.2020.00805; Muhammad UR, 2018, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2018.00836; Pang KY, 2019, PROC CVPR IEEE, P677, DOI 10.1109/CVPR.2019.00077; Pang KY, 2018, LECT NOTES COMPUT SC, V11219, P37, DOI 10.1007/978-3-030-01267-0_3; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50; Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43; Sagong MC, 2019, PROC CVPR IEEE, P11352, DOI 10.1109/CVPR.2019.01162; Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723; Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954; Shen YM, 2018, PROC CVPR IEEE, P3598, DOI 10.1109/CVPR.2018.00379; Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132703; Song JF, 2018, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2018.00090; Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI 10.1007/978-3-030-01216-8_1; Song Yuhang, 2018, ARXIV180503356; TALLON C, 1995, EUR J NEUROSCI, V7, P1285, DOI 10.1111/j.1460-9568.1995.tb01118.x; Tariq Taimoor, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P445, DOI 10.1007/978-3-030-58542-6_27; Theis Lucas, 2016, ICLR; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wilczkowiak M., 2005, P BRIT MACH VIS C; Xiong W, 2019, PROC CVPR IEEE, P5833, DOI 10.1109/CVPR.2019.00599; Xu P., 2019, TNNLS; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Yang LM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450284; Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42; Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457; [于谦 Yu Qian], 2015, [高分子通报, Polymer Bulletin], P1; Zang SC, 2021, NEURAL NETWORKS, V137, P138, DOI 10.1016/j.neunet.2021.01.006; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153; Zhou H, 2019, AAAI CONF ARTIF INTE, P9299; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	74	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					2006	2021		10.1007/s11263-022-01623-7	http://dx.doi.org/10.1007/s11263-022-01623-7		JUN 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP					2022-12-18	WOS:000807270100001
J	Zhang, Q; Chan, AB				Zhang, Qi; Chan, Antoni B.			Wide-Area Crowd Counting: Multi-view Fusion Networks for Counting in Large Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Crowd counting; Multi-view; Wide-area; DNNs fusion; Scale selection; Rotation selection	DETECTING PEDESTRIANS; PEOPLE	Crowd counting in single-view images has achieved outstanding performance on existing counting datasets. However, single-view counting is not applicable to large and wide scenes (e.g., public parks, long subway platforms, or event spaces) because a single camera cannot capture the whole scene in adequate detail for counting, e.g., when the scene is too large to fit into the field-of-view of the camera, too long so that the resolution is too low on faraway crowds, or when there are too many large objects that occlude large portions of the crowd. Therefore, to solve the wide-area counting task requires multiple cameras with overlapping fields-of-view. In this paper, we propose a deep neural network framework for multi-view crowd counting, which fuses information from multiple camera views to predict a scene-level density map on the ground-plane of the 3D world. We consider three versions of the fusion framework: the late fusion model fuses camera-view density map; the naive early fusion model fuses camera-view feature maps; and the multi-view multi-scale early fusion model ensures that features aligned to the same ground-plane point have consistent scales. A rotation selection module further ensures consistent rotation alignment of the features. We test our 3 fusion models on 3 multi-view counting datasets, PETS2009, DukeMTMC, and a newly collected multi-view counting dataset containing a crowded street intersection. Our methods achieve state-of-the-art results compared to other multi-view counting baselines.	[Zhang, Qi; Chan, Antoni B.] City Univ Hong Kong, Kowloon, Dept Comp Sci, Hong Kong, Peoples R China; [Zhang, Qi] Guangdong Lab Artificial Intelligence & Digital E, Shenzhen, Guangdong, Peoples R China	City University of Hong Kong	Zhang, Q (corresponding author), City Univ Hong Kong, Kowloon, Dept Comp Sci, Hong Kong, Peoples R China.; Zhang, Q (corresponding author), Guangdong Lab Artificial Intelligence & Digital E, Shenzhen, Guangdong, Peoples R China.	qzhang364-c@my.cityu.edu.hk; abchan@cityu.edu.hk		ZHANG, Qi/0000-0001-6212-9799	Research Grants Council of the Hong Kong Special Administrative Region, China [T32-101/15-R, CityU 11212518, CityU SRG 7005665, UGC GRF CityU 11215820]; City University of Hong Kong [7004887]; NVIDIA Corporation	Research Grants Council of the Hong Kong Special Administrative Region, China(Hong Kong Research Grants Council); City University of Hong Kong(City University of Hong Kong); NVIDIA Corporation	This work was supported by grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. [T32-101/15-R], CityU 11212518, CityU SRG 7005665 and UGC GRF CityU 11215820), and by a Strategic Research Grant from City University of Hong Kong (Project No. 7004887). We are grateful for the support of NVIDIA Corporation with the donation of the Tesla GPU used for this research.	Abbas A, 2019, IEEE INT CONF COMP V, P4095, DOI 10.1109/ICCVW.2019.00504; Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; Bhardwaj R, 2018, ACM T SENSOR NETWORK, V14, DOI 10.1145/3199667; Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Chen C. L., 2013, CROWD COUNTING PROFI; Chen C, 2019, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR.2019.00513; Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21; Cheng ZW, 2014, NEUROCOMPUTING, V136, P124, DOI 10.1016/j.neucom.2014.01.019; Cohen TS, 2016, PR MACH LEARN RES, V48; Dittrich F., 2017, ARXIV PREPRINT ARXIV; Eiselein V, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P19, DOI 10.1109/AVSS.2013.6636610; Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70; Gao HY, 2017, IEEE DATA MINING, P871, DOI 10.1109/ICDM.2017.107; Ge WN, 2010, LECT NOTES COMPUT SC, V6315, P324; Guerrero-Gomez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48; Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33; Jaderberg M, 2015, ADV NEUR IN, V28; Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629; Joachims T., 1998, TEXT CATEGORIZATION; Kang D., 2018, P BMVC, P89; Kang D, 2017, ADV NEUR IN, V30; Kang D, 2019, IEEE T CIRC SYST VID, V29, P1408, DOI 10.1109/TCSVT.2018.2837153; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laptev D, 2016, PROC CVPR IEEE, P289, DOI 10.1109/CVPR.2016.38; Leal-Taix L, 2015, ARXIV PREPRINT ARXIV; Lempitsky V, 2010, ADV NEURAL INFORM PR; Li JW, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P178, DOI 10.1109/AVSS.2012.54; Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120; Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192; Liu CC, 2019, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2019.00131; Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545; Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186; Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524; Ma HD, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2089094.2089107; Maddalena L, 2014, PATTERN RECOGN LETT, V36, P125, DOI 10.1016/j.patrec.2013.10.006; Marana AN, 1998, SIBGRAPI '98 - INTERNATIONAL SYMPOSIUM ON COMPUTER GRAPHICS, IMAGE PROCESSING, AND VISION, PROCEEDINGS, P354, DOI 10.1109/SIBGRA.1998.722773; Marcos D, 2017, IEEE I CONF COMP VIS, P5058, DOI 10.1109/ICCV.2017.540; Onoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38; Oord A.V.D., 2016, SSW; Paragios N, 2001, PROC CVPR IEEE, P1034; Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren WH, 2018, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2018.00561; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Ryan D, 2014, PATTERN RECOGN LETT, V44, P98, DOI 10.1016/j.patrec.2013.10.002; Sabzmeydani P, 2007, PROC CVPR IEEE, P1251; Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550; Shi MJ, 2019, PROC CVPR IEEE, P7271, DOI 10.1109/CVPR.2019.00745; Shi ZL, 2018, PROC CVPR IEEE, P5382, DOI 10.1109/CVPR.2018.00564; Jacques JCS, 2010, IEEE SIGNAL PROC MAG, V27, P66, DOI 10.1109/MSP.2010.937394; Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206; Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643; Tang NC, 2015, IEEE T IMAGE PROCESS, V24, P80, DOI 10.1109/TIP.2014.2363445; Pham VQ, 2015, IEEE I CONF COMP VIS, P3253, DOI 10.1109/ICCV.2015.372; Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552; Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839; Wang Y, 2016, IEEE IMAGE PROC, P3653, DOI 10.1109/ICIP.2016.7533041; Weiler M, 2018, PROC CVPR IEEE, P849, DOI 10.1109/CVPR.2018.00095; Worrall D, 2018, LECT NOTES COMPUT SC, V11209, P585, DOI 10.1007/978-3-030-01228-1_35; Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7; Xiyang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P241, DOI 10.1007/978-3-030-58586-0_15; Xu BL, 2016, IEEE WINT CONF APPL; Xu CF, 2019, IEEE I CONF COMP VIS, P8381, DOI 10.1109/ICCV.2019.00847; Yan ZY, 2019, IEEE I CONF COMP VIS, P952, DOI 10.1109/ICCV.2019.00104; Yang YF, 2020, PROC CVPR IEEE, P4373, DOI 10.1109/CVPR42600.2020.00443; Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684; Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849; Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zheng L., 2021, 2021 IEEE INT C MULT, P1	80	0	0	8	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					1938	1960		10.1007/s11263-022-01626-4	http://dx.doi.org/10.1007/s11263-022-01626-4		MAY 2022	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP		Green Submitted			2022-12-18	WOS:000802192500001
J	Guo, DD; Lu, RY; Chen, B; Zeng, ZQ; Zhou, MY				Guo, Dandan; Lu, Ruiying; Chen, Bo; Zeng, Zequn; Zhou, Mingyuan			Matching Visual Features to Hierarchical Semantic Topics for Image Paragraph Captioning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image paragraph generation; Deep topic model; Language model; Image and text		Observing a set of images and their corresponding paragraph-captions, a challenging task is to learn how to produce a semantically coherent paragraph to describe the visual content of an image. Inspired by recent successes in integrating semantic topics into this task, this paper develops a plug-and-play hierarchical-topic-guided image paragraph generation framework, which couples a visual extractor with a deep topic model to guide the learning of a language model. To capture the correlations between the image and text at multiple levels of abstraction and learn the semantic topics from images, we design a variational inference network to build the mapping from image features to textual captions. To guide the paragraph generation, the learned hierarchical topics and visual features are integrated into the language model, including Long Short-Term Memory and Transformer, and jointly optimized. Experiments on public datasets demonstrate that the proposed models, which are competitive with many state-of-the-art approaches in terms of standard evaluation metrics, can be used to both distill interpretable multi-layer semantic topics and generate diverse and coherent captions.	[Guo, Dandan; Lu, Ruiying; Chen, Bo; Zeng, Zequn] Xidian Univ, Collaborat Innovat Ctr Informat Sensing & Underst, Natl Lab Radar Signal Proc, Xian 710071, Peoples R China; [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA	Xidian University; University of Texas System; University of Texas Austin	Chen, B (corresponding author), Xidian Univ, Collaborat Innovat Ctr Informat Sensing & Underst, Natl Lab Radar Signal Proc, Xian 710071, Peoples R China.	gdd_xidian@126.com; ruiyinglu_xidian@163.com; bchen@mail.xidian.edu.cn; zzq8341@gmail.com; mingyuan.zhou@mccombs.utexas.edu			NSFC [U21B2006, 61771361]; Shaanxi Youth Innovation Team Project; 111 Project [B18039]; Program for Oversea Talent by Chinese Central Government; NSF [IIS-1812699]	NSFC(National Natural Science Foundation of China (NSFC)); Shaanxi Youth Innovation Team Project; 111 Project(Ministry of Education, China - 111 Project); Program for Oversea Talent by Chinese Central Government; NSF(National Science Foundation (NSF))	B. Chen acknowledges the support of NSFC (U21B2006 and 61771361), Shaanxi Youth Innovation Team Project, the 111 Project (No. B18039) and the Program for Oversea Talent by Chinese Central Government. M. Zhou acknowledges the support of NSF IIS-1812699.	Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bengio Y., 2014, ARXIV14061078; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Burkhardt S, 2019, J MACH LEARN RES, V20; Cong YL, 2017, PR MACH LEARN RES, V70; Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059; Demner-Fushman D, 2016, J AM MED INFORM ASSN, V23, P304, DOI 10.1093/jamia/ocv080; Denkowski Michael, 2014, P 9 WORKSH STAT MACH, P376, DOI DOI 10.3115/V1/W14-3348; Fan HH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3390891; Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Guo DD, 2020, PR MACH LEARN RES, V119; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473; Johnson A. E., 2019, ARXIVPREPRINT ARXIV1; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kingma D.P, P 3 INT C LEARNING R; Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902; Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010; Mao YZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4258; Melas-Kyriazi L, 2018, P 2018 C EMPIRICAL M, P757, DOI [10.18653/v1/D18-1084, DOI 10.18653/V1/D18-1084]; Miao YS, 2016, PR MACH LEARN RES, V48; Ordonez V, 2016, INT J COMPUT VISION, V119, P46, DOI 10.1007/s11263-015-0840-y; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Srivastava A., 2017, INT C LEARNING REPRE; Tang JH, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3291925; Vaswani A, 2017, ADV NEUR IN, V30; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang J, 2021, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR46437.2021.00136; Wang J, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P940; Xu C, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106590; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789; Yu NG, 2019, IEEE T IMAGE PROCESS, V28, P2743, DOI 10.1109/TIP.2018.2889922; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhao He, 2021, ARXIV210300498; Zhou M., 2012, ARTIF INTELL; Zhou MY, 2016, J MACH LEARN RES, V17, P1; Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503; Zhu ZH, 2018, IEEE IMAGE PROC, P2615, DOI 10.1109/ICIP.2018.8451083	49	0	0	7	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					1920	1937		10.1007/s11263-022-01624-6	http://dx.doi.org/10.1007/s11263-022-01624-6		MAY 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP		Green Submitted			2022-12-18	WOS:000805513000001
J	Kwon, MJ; Nam, SH; Yu, IJ; Lee, HK; Kim, C				Kwon, Myung-Joon; Nam, Seung-Hun; Yu, In-Jae; Lee, Heung-Kyu; Kim, Changick			Learning JPEG Compression Artifacts for Image Manipulation Detection and Localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image forensics; Multimedia forensics; Image manipulation detection; Double JPEG detection; Image processing	FORGERY; STEGANALYSIS	Detecting and localizing image manipulation are necessary to counter malicious use of image editing techniques. Accordingly, it is essential to distinguish between authentic and tampered regions by analyzing intrinsic statistics in an image. We focus on JPEG compression artifacts left during image acquisition and editing. We propose a convolutional neural network that uses discrete cosine transform (DCT) coefficients, where compression artifacts remain, to localize image manipulation. Standard CNNs cannot learn the distribution of DCT coefficients because the convolution throws away the spatial coordinates, which are essential for DCT coefficients. We illustrate how to design and train a neural network that can learn the distribution of DCT coefficients. Furthermore, we introduce Compression Artifact Tracing Network that jointly uses image acquisition artifacts and compression artifacts. It significantly outperforms traditional and deep neural network-based methods in detecting and localizing tampered regions.	[Kwon, Myung-Joon; Kim, Changick] Korea Adv Inst Sci & Technol KAIST, Sch Elect Engn, Daejeon, South Korea; [Nam, Seung-Hun] NAVER WEBTOON AI, Seongnam, South Korea; [Yu, In-Jae] Samsung Elect Co Ltd, Visual Display Business, Suwon, South Korea; [Lee, Heung-Kyu] Korea Adv Inst Sci & Technol KAIST, Sch Comp, Daejeon, South Korea	Korea Advanced Institute of Science & Technology (KAIST); Samsung; Samsung Electronics; Korea Advanced Institute of Science & Technology (KAIST)	Kim, C (corresponding author), Korea Adv Inst Sci & Technol KAIST, Sch Elect Engn, Daejeon, South Korea.	mjkwon2021@gmail.com; shnam1520@gmail.com; injae.yu@samsung.com; heunglee@kaist.ac.kr; changick@kaist.ac.kr		Nam, Seung-Hun/0000-0002-2576-7342; Kwon, Myung-Joon/0000-0002-9784-8440; South, Set/0000-0002-3881-3168; Yu, In-Jae/0000-0001-9865-2194	Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education [2021R1I1A1A01043600]	Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education(National Research Foundation of KoreaMinistry of Education (MOE), Republic of KoreaNational Research Council for Economics, Humanities & Social Sciences, Republic of Korea)	This research was partially supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2021R1I1A1A01043600).	Agarwal S., 2018, TR2018838 DARTM COLL; Amerini I, 2011, IEEE T INF FOREN SEC, V6, P1099, DOI 10.1109/TIFS.2011.2129512; Barni M, 2017, J VIS COMMUN IMAGE R, V49, P153, DOI 10.1016/j.jvcir.2017.09.003; Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5; Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953; Bi XL, 2019, IEEE COMPUT SOC CONF, P30, DOI 10.1109/CVPRW.2019.00010; Bianchi T, 2012, IEEE T INF FOREN SEC, V7, P1003, DOI 10.1109/TIFS.2012.2187516; Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749; Butora J, 2020, INT CONF ACOUST SPEE, P2762, DOI 10.1109/ICASSP40776.2020.9053696; Chierchia G, 2014, IEEE T INF FOREN SEC, V9, P554, DOI 10.1109/TIFS.2014.2302078; Choi CH, 2013, FORENSIC SCI INT, V226, P94, DOI 10.1016/j.forsciint.2012.12.014; Choi KS, 2006, PROC SPIE, V6069, DOI 10.1117/12.649775; Cozzolino D, 2015, IEEE T INF FOREN SEC, V10, P2284, DOI 10.1109/TIFS.2015.2455334; Cozzolino D, 2020, IEEE T INF FOREN SEC, V15, P144, DOI 10.1109/TIFS.2019.2916364; Dang-Nguyen D., 2015, ACM MULTIMEDIA SYSTE, P219, DOI [10.1145/2713168.2713194, DOI 10.1145/2713168.2713194]; de Carvalho TJ, 2013, IEEE T INF FOREN SEC, V8, P1182, DOI 10.1109/TIFS.2013.2265677; Ferrara P, 2012, IEEE T INF FOREN SEC, V7, P1566, DOI 10.1109/TIFS.2012.2202227; Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402; Fu DD, 2007, PROC SPIE, V6505, DOI 10.1117/12.704723; Gloe T., 2010, P 2010 ACM S APPL CO; Guan HY, 2019, IEEE WINT CONF APPL, P63, DOI 10.1109/WACVW.2019.00018; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huh M, 2018, LECT NOTES COMPUT SC, V11215, P106, DOI 10.1007/978-3-030-01252-6_7; Iakovidou C, 2018, J VIS COMMUN IMAGE R, V54, P155, DOI 10.1016/j.jvcir.2018.05.011; Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374; Kniaz VV, 2019, ADV NEUR IN, V32, P215; Korus P, 2017, DIGIT SIGNAL PROCESS, V71, P1, DOI 10.1016/j.dsp.2017.08.009; Korus P, 2017, IEEE T INF FOREN SEC, V12, P809, DOI 10.1109/TIFS.2016.2636089; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kwon MJ, 2021, IEEE WINT CONF APPL, P375, DOI 10.1109/WACV48630.2021.00042; Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177; Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin ZC, 2009, PATTERN RECOGN, V42, P2492, DOI 10.1016/j.patcog.2009.03.019; Liu B, 2020, INFORM SCIENCES, V526, P133, DOI 10.1016/j.ins.2020.03.099; Luk J., 2003, P DIG FOR RES WORKSH, P58; Lukas J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602; Lyu SW, 2014, INT J COMPUT VISION, V110, P202, DOI 10.1007/s11263-013-0688-y; Mahdian B, 2009, IMAGE VISION COMPUT, V27, P1497, DOI 10.1016/j.imavis.2009.02.001; Marra F, 2020, IEEE ACCESS, V8, P133488, DOI 10.1109/ACCESS.2020.3009877; Nam S.-H., 2020, IEEE T CIRC SYST VID; Pham NT, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11010083; Ng T. T., 2004, DATA SET AUTHENTIC S, P6; Nikoukhah T., 2019, P IEEE CVF C COMP VI; Novozamsky A, 2020, IEEE WINT CONF APPL, P71, DOI 10.1109/WACVW50321.2020.9096940; Park J, 2018, LECT NOTES COMPUT SC, V11209, P656, DOI 10.1007/978-3-030-01228-1_39; Paszke A, 2019, ADV NEUR IN, V32; Piva Alessandro, 2013, INT SCHOLARLY RES NO, V2013; Popescu AC, 2004, LECT NOTES COMPUT SC, V3200, P128; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Swaminathan A, 2008, IEEE T INF FOREN SEC, V3, P101, DOI 10.1109/TIFS.2007.916010; Tralic Dijana, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P49; Verma V., 2020, ARXIV; Wang JH, 2020, IEEE T CYBERNETICS, V50, P2971, DOI 10.1109/TCYB.2019.2891265; Wang Q, 2016, EURASIP J INF SECUR, DOI 10.1186/s13635-016-0047-y; Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339; Wu Y, 2019, PROC CVPR IEEE, P9535, DOI 10.1109/CVPR.2019.00977; Wu Y, 2018, IEEE WINT CONF APPL, P1907, DOI 10.1109/WACV.2018.00211; Ye SM, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P12; Yerushalmy I, 2011, INT J COMPUT VISION, V92, P71, DOI 10.1007/s11263-010-0403-1; Yousfi Y, 2020, IEEE SIGNAL PROC LET, V27, P830, DOI 10.1109/LSP.2020.2993959; Yu IJ, 2020, IEEE ACCESS, V8, P210837, DOI 10.1109/ACCESS.2020.3037735; Zampoglou M, 2017, MULTIMED TOOLS APPL, V76, P4801, DOI 10.1007/s11042-016-3795-2; Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116	66	0	0	16	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					1875	1895		10.1007/s11263-022-01617-5	http://dx.doi.org/10.1007/s11263-022-01617-5		MAY 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP		Green Submitted			2022-12-18	WOS:000802316600001
J	Peng, T; Tang, CY; Wu, YY; Cai, J				Peng, Tao; Tang, Caiyin; Wu, Yiyun; Cai, Jing			H-SegMed: A Hybrid Method for Prostate Segmentation in TRUS Images via Improved Closed Principal Curve and Improved Enhanced Machine Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Accurate prostate segmentation; Transrectal ultrasound; Principal curve; Greedy closed principal curve method; Memory-based adaptive differential evolution; Machine learning	DIFFERENTIAL EVOLUTION; VARIATIONAL MODEL; ULTRASOUND IMAGE; NEURAL-NETWORK; OPTIMIZATION; ALGORITHMS; FRAMEWORK; FEATURES; DESIGN; LUNG	Prostate segmentation is an important step in prostate volume estimation, multi-modal image registration, and patient-specific anatomical modeling for surgical planning and image-guided biopsy. Manual delineation of the prostate contour is time-consuming and prone to inter- and intra-observer variability. Accurate prostate segmentation in transrectal ultrasound images is particularly challenging due to the ambiguous boundary between the prostate and neighboring organs, the presence of shadow artifacts, heterogeneous intra-prostate image intensity, and inconsistent anatomical shapes. Therefore, in this study, we propose a novel hybrid segmentation method (H-SegMed) for accurate prostate segmentation in TRUS images. The method consists of two main steps: (1) an improved closed principal curve-based method was used to obtain the data sequence, in which only few radiologist-defined seed points were used as an approximate initialization; and (2) an enhanced machine learning method was used to achieve an accurate and smooth contour of the prostate. Our results show that the proposed model achieved superior segmentation performance compared with several other state-of-the-art models, achieving an average Dice similarity coefficient, Jaccard similarity coefficient (omega), and accuracy of 96.5, 95.1, and 96.3%, respectively.	[Peng, Tao; Cai, Jing] Hong Kong Polytech Univ, Dept Hlth Technol & Informat, Hong Kong, Peoples R China; [Tang, Caiyin] Taizhou Peoples Hosp, Dept Med Imaging, Taizhou, Jiangsu, Peoples R China; [Wu, Yiyun] Jiangsu Prov Hosp, Dept Med Technol, Nanjing, Jiangsu, Peoples R China	Hong Kong Polytechnic University; Nanjing Medical University	Cai, J (corresponding author), Hong Kong Polytech Univ, Dept Hlth Technol & Informat, Hong Kong, Peoples R China.	sdpengtao401@gmail.com; tangcaiyin@126.com; wuyi425@sina.com; jing.cai@polyu.edu.hk			 [ITS/080/19]		This work is partly supported by ITS/080/19.	Abu Anas EM, 2018, MED IMAGE ANAL, V48, P107, DOI 10.1016/j.media.2018.05.010; Akbari H, 2012, MED PHYS, V39, P2972, DOI 10.1118/1.4709607; Akbarinia A, 2018, INT J COMPUT VISION, V126, P1367, DOI 10.1007/s11263-017-1035-5; Ali MZ, 2017, IEEE T CYBERNETICS, V47, P2768, DOI 10.1109/TCYB.2016.2617301; AMARI S, 1993, NEUROCOMPUTING, V5, P185, DOI 10.1016/0925-2312(93)90006-O; Anas E.MA., 2017, LNCS, P365, DOI DOI 10.1007/978-3-319-66179-7_42; Arce-Santana ER, 2019, MED BIOL ENG COMPUT, V57, P565, DOI 10.1007/s11517-018-1896-y; Baioletti M, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8010069; Benaichouche AN, 2013, DIGIT SIGNAL PROCESS, V23, P1390, DOI 10.1016/j.dsp.2013.07.005; Bi H, 2020, COMPUT METH PROG BIO, V184, DOI 10.1016/j.cmpb.2019.105097; Chen MR, 2020, NEUROCOMPUTING, V391, P260; Cheng RD, 2017, I S BIOMED IMAGING, P749, DOI 10.1109/ISBI.2017.7950627; Ghose S, 2013, MED IMAGE ANAL, V17, P587, DOI 10.1016/j.media.2013.04.001; Gurari D, 2019, INT J COMPUT VISION, V127, P1198, DOI 10.1007/s11263-019-01172-6; Han SM, 2008, J DIGIT IMAGING, V21, pS121, DOI 10.1007/s10278-008-9106-3; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012; Jaouen V, 2019, IEEE T BIO-MED ENG, V66, P920, DOI 10.1109/TBME.2018.2865428; Jin JY, 2013, COMPUT MATH METHOD M, V2013, DOI 10.1155/2013/502013; Karimi D, 2019, MED IMAGE ANAL, V57, P186, DOI 10.1016/j.media.2019.07.005; Kegl B, 2000, IEEE T PATTERN ANAL, V22, P281, DOI 10.1109/34.841759; Khiyali Z, 2017, INT J PEDIATR-MASSHA, V5, P4821, DOI 10.22038/ijp.2016.7750; Leema N, 2016, APPL SOFT COMPUT, V49, P834, DOI 10.1016/j.asoc.2016.08.001; Lim S, 2019, IEEE T BIO-MED ENG, V66, P2527, DOI 10.1109/TBME.2019.2891240; Liu C, 2018, PATTERN RECOGN, V76, P367, DOI 10.1016/j.patcog.2017.11.019; Liu Y, 2019, SIGNAL PROCESS, V155, P193, DOI 10.1016/j.sigpro.2018.08.017; Mandavi SS, 2011, MED IMAGE ANAL, V15, P226, DOI 10.1016/j.media.2010.10.002; Mequanint EZ, 2019, IEEE T PATTERN ANAL, V41, P2438, DOI 10.1109/TPAMI.2018.2858243; Nouranian S, 2016, IEEE T MED IMAGING, V35, P921, DOI 10.1109/TMI.2015.2502540; Nouranian S, 2015, IEEE T MED IMAGING, V34, P950, DOI 10.1109/TMI.2014.2371823; Orlando N, 2020, MED PHYS, V47, P2413, DOI 10.1002/mp.14134; Palmero C, 2016, INT J COMPUT VISION, V118, P217, DOI 10.1007/s11263-016-0901-x; Peng T., 2020, COMPUT J; Peng T, 2019, IEEE ACCESS, V7, P137794, DOI 10.1109/ACCESS.2019.2941511; Peng T, 2020, IEEE ACCESS, V8, P73293, DOI 10.1109/ACCESS.2020.2987925; Peng T, 2018, J DIGIT IMAGING, V31, P520, DOI 10.1007/s10278-018-0058-y; Qiu W, 2014, IEEE T MED IMAGING, V33, P947, DOI 10.1109/TMI.2014.2300694; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shaaer A, 2020, BRACHYTHERAPY, V19, P599, DOI 10.1016/j.brachy.2020.06.014; Shaaer A, 2019, BRACHYTHERAPY, V18, P95, DOI 10.1016/j.brachy.2018.08.006; Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328; Sun YS, 2018, NEURAL COMPUT APPL, V29, P1357, DOI 10.1007/s00521-017-2904-0; Tong N, 2018, MED PHYS, V45, P4558, DOI 10.1002/mp.13147; Wang GT, 2019, IEEE T PATTERN ANAL, V41, P1559, DOI 10.1109/TPAMI.2018.2840695; Wang J, 2017, NEURAL NETWORKS, V89, P19, DOI 10.1016/j.neunet.2017.02.007; Wang L, 2015, EXPERT SYST APPL, V42, P855, DOI 10.1016/j.eswa.2014.08.018; Wang WR, 2021, INT J MED ROBOT COMP, V17, DOI 10.1002/rcs.2190; Wang Y, 2019, IEEE T MED IMAGING, V38, P2768, DOI 10.1109/TMI.2019.2913184; Wang Y, 2018, IEEE T MED IMAGING, V37, P1067, DOI 10.1109/TMI.2017.2777870; WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9; Wu GH, 2016, INFORM SCIENCES, V329, P329, DOI 10.1016/j.ins.2015.09.009; Xue C, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2021.101989; Yan PK, 2010, IEEE T BIO-MED ENG, V57, P1158, DOI 10.1109/TBME.2009.2037491; Yang SY, 2014, J COMPUT, V9, P1125, DOI 10.4304/jcp.9.5.1125-1130; Yu YY, 2016, COMPUT BIOL MED, V74, P74, DOI 10.1016/j.compbiomed.2016.05.002; Zhang JQ, 2009, IEEE T EVOLUT COMPUT, V13, P945, DOI 10.1109/TEVC.2009.2014613; Zhang JP, 2008, IEEE T INTELL TRANSP, V9, P666, DOI 10.1109/TITS.2008.2006780; Zhang Y, 2007, COMPUT BIOL MED, V37, P1591, DOI 10.1016/j.compbiomed.2007.02.008; Zhou S, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-14318-9; Zou DX, 2018, ENERGY, V147, P59, DOI 10.1016/j.energy.2018.01.029; Zou DX, 2016, APPL ENERG, V181, P375, DOI 10.1016/j.apenergy.2016.08.067	63	0	0	6	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2022	130	8					1896	1919		10.1007/s11263-022-01619-3	http://dx.doi.org/10.1007/s11263-022-01619-3		MAY 2022	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2X9AP					2022-12-18	WOS:000802316600002
J	Wang, A; Liu, A; Zhang, R; Kleiman, A; Kim, L; Zhao, D; Shirai, I; Narayanan, A; Russakovsky, O				Wang, Angelina; Liu, Alexander; Zhang, Ryan; Kleiman, Anat; Kim, Leslie; Zhao, Dora; Shirai, Iroha; Narayanan, Arvind; Russakovsky, Olga			REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision datasets; Bias mitigation; Tool	OPTIMAL NUMBER; OBJECT; CLASSIFICATION; FEATURES; CONTEXT	Machine learning models are known to perpetuate and even amplify the biases present in the data. However, these data biases frequently do not become apparent until after the models are deployed. Our work tackles this issue and enables the preemptive analysis of large-scale datasets. REvealing VIsual biaSEs (REVISE) is a tool that assists in the investigation of a visual dataset, surfacing potential biases along three dimensions: (1) object-based, (2) person-based, and (3) geography-based. Object-based biases relate to the size, context, or diversity of the depicted objects. Person-based metrics focus on analyzing the portrayal of people within the dataset. Geography-based analyses consider the representation of different geographic locations. These three dimensions are deeply intertwined in how they interact to bias a dataset, and REVISE sheds light on this; the responsibility then lies with the user to consider the cultural and historical context, and to determine which of the revealed biases may be problematic. The tool further assists the user by suggesting actionable steps that may be taken to mitigate the revealed biases. Overall, the key aim of our work is to tackle the machine learning bias problem early in the pipeline. REVISE is available at https://github.com/princetonvisualaikevise-tool.	[Wang, Angelina; Liu, Alexander; Zhang, Ryan; Kleiman, Anat; Kim, Leslie; Zhao, Dora; Shirai, Iroha; Narayanan, Arvind; Russakovsky, Olga] Princeton Univ, Princeton, NJ 08544 USA	Princeton University	Wang, A (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	angelina.wang@princeton.edu		Wang, Angelina/0000-0001-9140-3523	National Science Foundation [1763642, 1704444]	National Science Foundation(National Science Foundation (NSF))	This work is partially supported by the National Science Foundation Tunder Grant No. 1763642 and No. 1704444. We would also like to thank Felix Yu, Vikram Ramaswamy, and Zhiwei Deng for their helpful comments, and Zeyu Wang, Deniz Oktay, and Nobline Yoo for testing out the tool and providing feedback.	Alwassel H, 2018, LECT NOTES COMPUT SC, V11207, P264, DOI 10.1007/978-3-030-01219-9_16; Amazon, 2021, AM SAG CLAR; [Anonymous], 2019, METHODOLOGY; [Anonymous], AMAZON REKOGNITION; Balakrishnan Guha, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P547, DOI 10.1007/978-3-030-58523-5_32; Bao M., 2021, ARXIV210605498; Barocas S., 2019, FAIRNESS MACHINE LEA; Bellamy RKE, 2019, IBM J RES DEV, V63, DOI 10.1147/JRD.2019.2942287; Berg AC, 2012, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR.2012.6248100; Birdal T, 2019, IEEE T PATTERN ANAL; Birhane A, 2021, PATTERNS, V2, DOI 10.1016/j.patter.2021.100205; Birhane Abeba, 2021, ARXIV211001963; Bolya Daniel, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P558, DOI 10.1007/978-3-030-58580-8_33; Brown C., 2014, ARCH RECORDKEEPING T; Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011; Buolamwini J., 2018, ACM C FAIRNESS ACCOU; Cadene R, 2019, ADV NEUR IN, V32; Caliskan A, 2017, SCIENCE, V356, DOI 10.1126/science.aal4230; Choi MJ, 2012, PATTERN RECOGN LETT, V33, P853, DOI 10.1016/j.patrec.2011.12.004; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Denton E., 2020, ARXIV200707399; Denton E., 2019, P 2019 IEEE CVF C CO; DeVries T., 2019, C COMPUTER VISION PA; Ding F., 2021, ARXIV210804884; Dwork Cynthia, 2017, ARXIV170706613; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fabbrizzi S., 2021, ARXIV210707919; FDP, 2021, WIR WOLL FAIRN; FITZPATRICK TB, 1988, ARCH DERMATOL, V124, P869, DOI 10.1001/archderm.124.6.869; Gajane P., 2017, ARXIV171003184; Galleguillos C, 2008, PROC CVPR IEEE, P3552; Gebru T, 2017, P NATL ACAD SCI USA, V114, P13108, DOI 10.1073/pnas.1700035114; Google People + AI Research, 2021, KNOW YOUR DAT; Green B., 2018, MACHINE LEARNING; HAMIDI F, 2018, C HUMAN FACTORS COMP; Hanna A, 2020, FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P501, DOI 10.1145/3351095.3372826; Hardt M, 2016, ADV NEUR IN, V29; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hendricks LA, 2018, LECT NOTES COMPUT SC, V11207, P793, DOI 10.1007/978-3-030-01219-9_47; Hill Kashmir, 2020, NEW YORK TIMES; Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25; Holland S., 2018, ARXIV180503677; Hua JP, 2005, BIOINFORMATICS, V21, P1509, DOI 10.1093/bioinformatics/bti171; Jacobs Abigail Z., 2021, FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, P375, DOI 10.1145/3442188.3445901; JAIN AK, 1978, PATTERN RECOGN, V10, P365, DOI 10.1016/0031-3203(78)90008-0; JONCKHEERE AR, 1954, BIOMETRIKA, V41, P133, DOI 10.1093/biomet/41.1-2.133; Joulin Armand, 2016, ARXIV160701759; Kay M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3819, DOI 10.1145/2702123.2702520; Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12; Kleinberg J.M., 2017, 8 INNOVATIONS THEORE, DOI [10.4230/LIPIcs.ITCS.2017.43, DOI 10.4230/LIPICS.ITCS.2017.43]; Krasin Ivan, 2017, OPENIMAGES PUBLIC DA, V2, P18; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853; Mehrabi N, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3457607; Mitchell M, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P220, DOI 10.1145/3287560.3287596; Moulton J., 1981, SEXIST LANGUAGE; Oakes Jeannie., 1985, KEEPING TRACK SCH ST; Ojala M, 2010, J MACH LEARN RES, V11, P1833; Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009; Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100; Paullada A, 2021, PATTERNS, V2, DOI 10.1016/j.patter.2021.100336; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Peng K.L., 2021, 35 C NEUR INF PROC S; Pleiss G, 2017, ADV NEUR IN, V30; Prabhu Vinay Uday, 2020, ARXIV200616923; Quadrianto N., 2020, ARXIV200406524; Roll U, 2018, CONSERV BIOL, V32, P716, DOI 10.1111/cobi.13044; Rosenfeld A., 2018, ARXIV180803305; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salakhutdinov R, 2011, PROC CVPR IEEE, P1481, DOI 10.1109/CVPR.2011.5995720; Sattigeri P, 2019, IBM J RES DEV, V63, DOI 10.1147/JRD.2019.2945519; Scheuerman Morgan Klaus, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3476058; Scheuerman MK., 2020, P ACM HUM INT; Shankar S., 2017, NEURIPS WORKSHOP MAC; Sheeny M., 2021, IEEE INT C ROBOTICS; Sigurdsson GA, 2017, IEEE I CONF COMP VIS, P2156, DOI 10.1109/ICCV.2017.235; Steed Ryan, 2021, FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, P701, DOI 10.1145/3442188.3445932; Swinger N, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P305, DOI 10.1145/3306618.3314270; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Tommasi T, 2017, ADV COMPUT VIS PATT, P37, DOI 10.1007/978-3-319-58347-1_2; TORRALBA A, 2011, PROC CVPR IEEE, P1521, DOI DOI 10.1109/CVPR.2011.5995347; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; US Census Bureau, 2019, AM COMMUNITY SURVEY; van Miltenburg E., 2018, INT NATURAL LANGUAGE; Wang A., 2021, INT C MACHINE LEARNI; Wang Angelina, 2020, EUR C COMP VIS ECCV; Wilson Benjamin, 2019, ARXIV190211097; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yang JM, 2014, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2014.415; Yang K., 2021, ARXIV210306191; Yang KY, 2020, FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P547, DOI 10.1145/3351095.3375709; Yang KY, 2019, IEEE I CONF COMP VIS, P2051, DOI 10.1109/ICCV.2019.00214; Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626; Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271; Zhang BH, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P335, DOI 10.1145/3278721.3278779; Zhao D., 2021, CORR ARXIV210608503; ZhenWang Guosheng Hu, 2020, C COMP VIS PATT REC; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhu XX, 2014, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2014.122	109	0	0	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1790	1810		10.1007/s11263-022-01625-5	http://dx.doi.org/10.1007/s11263-022-01625-5		MAY 2022	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH		Green Submitted			2022-12-18	WOS:000800987800001
J	Zhang, KH; Luo, WH; Yu, YJ; Ren, WQ; Zhao, F; Li, CS; Ma, L; Liu, W; Li, HD				Zhang, Kaihao; Luo, Wenhan; Yu, Yanjiang; Ren, Wenqi; Zhao, Fang; Li, Changsheng; Ma, Lin; Liu, Wei; Li, Hongdong			Beyond Monocular Deraining: Parallel Stereo Deraining Network Via Semantic Prior	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo image deraining; Parallel stereo network; View fusion; Deep learning	REMOVE RAIN; SINGLE; MODEL	Rain is a common natural phenomenon. Taking images in the rain however often results in degraded quality of images, thus compromises the performance of many computer vision systems. Most existing de-rain algorithms use only one single input image and aim to recover a clean image. Few work has exploited stereo images. Moreover, even for single image based monocular deraining, many current methods fail to complete the task satisfactorily because they mostly rely on per pixel loss functions and ignore semantic information. In this paper, we present a Paired Rain Removal Network (PRRNet), which exploits both stereo images and semantic information. Specifically, we develop a Semantic-Aware Deraining Module (SADM) which solves both tasks of semantic segmentation and deraining of scenes, and a Semantic-Fusion Network (SFNet) and a View-Fusion Network (VFNet) which fuse semantic information and multi-view information respectively. In addition, we also introduce an Enhanced Paired Rain Removal Network (EPRRNet) which exploits semantic prior to remove rain streaks from stereo images. We first use a coarse deraining network to reduce the rain streaks on the input images, and then adopt a pre-trained semantic segmentation network to extract semantic features from the coarse derained image. Finally, a parallel stereo deraining network fuses semantic and multi-view information to restore finer results. We also propose new stereo based rainy datasets for benchmarking. Experiments on both monocular and the newly proposed stereo rainy datasets demonstrate that the proposed method achieves the state-of-the-art performance. hups://github.com/HDCVLab/Stereo-Image-Deraining.	[Zhang, Kaihao; Li, Hongdong] Australian Natl Univ, Canberra, ACT, Australia; [Luo, Wenhan; Ren, Wenqi] Sun Yat Sen Univ, Guangzhou, Peoples R China; [Yu, Yanjiang; Li, Changsheng] Beijing Inst Technol, Beijing, Peoples R China; [Zhao, Fang] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Ma, Lin] Meituan Grp, Beijing, Peoples R China; [Liu, Wei] Tencent, Shenzhen, Peoples R China	Australian National University; Sun Yat Sen University; Beijing Institute of Technology; Tencent	Luo, WH (corresponding author), Sun Yat Sen Univ, Guangzhou, Peoples R China.	kaihao.zhang@anu.edu.au; whluo.china@gmail.com; yuyanjiang87@gmail.com; rwq.renwenqi@gmail.com; fang.zhao@inceptioniai.org; lcs@bit.edu.cn; forest.linma@gmail.com; wl2223@columbia.edu; hongdong.li@anu.edu.au	Zhang, Kaihao/HGC-0368-2022; Luo, Wenhan/GZL-0535-2022		ARC [DP 190102261, DP220100800]; Ford Alliance URP grant; National Natural Science Foundation of China (NSFC) [62122013, U2001211]	ARC(Australian Research Council); Ford Alliance URP grant; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This research was funded in part by ARC-Discovery grant projects (DP 190102261 and DP220100800), a Ford Alliance URP grant, and National Natural Science Foundation of China (NSFC) under Grant 62122013, U2001211.	Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2; Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12; Brewer N, 2008, LECT NOTES COMPUT SC, V5342, P451, DOI 10.1007/978-3-540-89689-0_49; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen C., 2015, P IEEE INT C COMPUTE; Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696; Chen J, 2018, PROC CVPR IEEE, P6286, DOI 10.1109/CVPR.2018.00658; Chen J, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2013.2290595; Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Eslami SM, 2016, NEURIPS, V1; Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186; Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802; GARG K, 2004, PROC CVPR IEEE, P528, DOI DOI 10.1109/CVPR.2004.1315077; Garg K, 2006, ACM T GRAPHIC, V25, P996, DOI 10.1145/1141911.1141985; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821; Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759; Jeon DS, 2018, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2018.00185; Jiang TX, 2017, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2017.301; Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057; Kim JH, 2014, IEEE IMAGE PROC, P5432, DOI 10.1109/ICIP.2014.7026099; Kim JH, 2015, IEEE T IMAGE PROCESS, V24, P2658, DOI 10.1109/TIP.2015.2428933; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li B, 2018, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2018.00682; Li DX, 2021, PROC CVPR IEEE, P7717, DOI 10.1109/CVPR46437.2021.00763; Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029; Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173; Li RT, 2020, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR42600.2020.00324; Li RT, 2019, IEEE I CONF COMP VIS, P7303, DOI 10.1109/ICCV.2019.00740; Li SY, 2019, PROC CVPR IEEE, P3833, DOI 10.1109/CVPR.2019.00396; Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16; Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299; Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152; Liu JY, 2019, IEEE T IMAGE PROCESS, V28, P699, DOI 10.1109/TIP.2018.2869722; Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431; Liu P., 2009, COMPUTER INFORM SCI; Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; LUO WJ, 2016, PROC CVPR IEEE, P5695, DOI DOI 10.1109/CVPR.2016.614; Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388; Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108; photoshopessentials, P IEEE C COMP VIS PA; Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ren WH, 2017, PROC CVPR IEEE, P2838, DOI 10.1109/CVPR.2017.303; Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10; Riegler G, 2019, PROC CVPR IEEE, P7616, DOI 10.1109/CVPR.2019.00781; Santhaseelan V, 2015, INT J COMPUT VISION, V112, P71, DOI 10.1007/s11263-014-0759-8; Sen Deng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14548, DOI 10.1109/CVPR42600.2020.01457; Shao J, 2015, PROC CVPR IEEE, P4657, DOI 10.1109/CVPR.2015.7299097; Shen ZY, 2020, INT J COMPUT VISION, V128, P1829, DOI 10.1007/s11263-019-01288-9; Shen ZY, 2018, PROC CVPR IEEE, P8260, DOI 10.1109/CVPR.2018.00862; Tanaka Y, 2006, IEICE T INF SYST, VE89D, P2021, DOI 10.1093/ietisy/e89-d.7.2021; Tao Andrew, 2020, ARXIV200510821; Tripathi AK, 2012, IET IMAGE PROCESS, V6, P181, DOI 10.1049/iet-ipr.2010.0547; Wei W, 2017, IEEE I CONF COMP VIS, P2535, DOI 10.1109/ICCV.2017.275; Yang W, 2021, IEEE T PATTERN ANAL; Yang WH, 2021, IEEE T PATTERN ANAL, V43, P4059, DOI 10.1109/TPAMI.2020.2995190; YANG WH, 2017, PROC CVPR IEEE, P1685, DOI DOI 10.1109/CVPR.2017.183; Yasarla R, 2020, PROC CVPR IEEE, P2723, DOI 10.1109/CVPR42600.2020.00280; Yasarla R, 2020, IEEE T IMAGE PROCESS, V29, P4544, DOI 10.1109/TIP.2020.2973802; Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860; Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458; Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407; Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337; Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079; Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145; Zhang Jing, 2020, CVPR; Zhang K., 2021, ARXIV210311298; Zhang K., 2021, ARXIV210312318; Zhang K., 2020, EUROPEAN C COMPUTER; Zhang K., 2021, ARXIV210307051; Zhang K., 2022, ARXIV220110700; Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281; Zhang KH, 2019, AAAI CONF ARTIF INTE, P9203; Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733; Zhang XP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P461, DOI 10.1109/ICME.2006.262572; Zhao F, 2018, LECT NOTES COMPUT SC, V11219, P20, DOI 10.1007/978-3-030-01267-0_2; Zheng L., 2021, ARXIV210602809; Zheng Y. P., 2019, BMVC; Zhou SC, 2019, PROC CVPR IEEE, P10988, DOI 10.1109/CVPR.2019.01125; Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276	87	0	0	14	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1754	1769		10.1007/s11263-022-01620-w	http://dx.doi.org/10.1007/s11263-022-01620-w		MAY 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH		Green Submitted			2022-12-18	WOS:000796848800001
J	Xu, WJ; Xian, YQ; Wang, JI; Schiele, B; Akata, Z				Xu, Wenjia; Xian, Yongqin; Wang, Jiuniu; Schiele, Bernt; Akata, Zeynep			Attribute Prototype Network for Any-Shot Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Zero-shot learning; Few-shot learning; Attribute prototype; Attribute localization		Any-shot image classification allows to recognize novel classes with only a few or even zero samples. For the task of zero-shot learning, visual attributes have been shown to play an important role, while in the few-shot regime, the effect of attributes is under-explored. To better transfer attribute-based knowledge from seen to unseen classes, we argue that an image representation with integrated attribute localization ability would be beneficial for any-shot, i.e. zero-shot and few-shot, image classification tasks. To this end, we propose a novel representation learning framework that jointly learns discriminative global and local features using only class-level attributes. While a visual-semantic embedding layer learns global features, local features are learned through an attribute prototype network that simultaneously regresses and decorrelates attributes from intermediate features. Furthermore, we introduce a zoom-in module that localizes and crops the informative regions to encourage the network to learn informative features explicitly. We show that our locality augmented image representations achieve a new state-of-the-art on challenging benchmarks, i.e. CUB, AWA2, and SUN. As an additional benefit, our model points to the visual evidence of the attributes in an image, confirming the improved attribute localization ability of our image representation. The attribute localization is evaluated quantitatively with ground truth part annotations, qualitatively with visualizations, and through well-designed user studies.	[Xu, Wenjia] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing, Peoples R China; [Schiele, Bernt; Akata, Zeynep] Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany; [Xian, Yongqin] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland; [Akata, Zeynep] Univ Tubingen, Tubingen, Germany; [Akata, Zeynep] Max Planck Inst Intelligent Syst, Tubingen, Germany; [Wang, Jiuniu] City Univ Hong Kong, Hong Kong, Peoples R China; [Xu, Wenjia; Wang, Jiuniu] Chinese Acad Sci, Aerosp Informat Res Inst, Beijing, Peoples R China; [Xu, Wenjia; Wang, Jiuniu] Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing, Peoples R China	Beijing University of Posts & Telecommunications; Max Planck Society; Swiss Federal Institutes of Technology Domain; ETH Zurich; Eberhard Karls University of Tubingen; Max Planck Society; City University of Hong Kong; Chinese Academy of Sciences; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Xu, WJ (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing, Peoples R China.; Xu, WJ (corresponding author), Chinese Acad Sci, Aerosp Informat Res Inst, Beijing, Peoples R China.; Xu, WJ (corresponding author), Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing, Peoples R China.	xuwenjia16@mails.ucas.ac.cn; yongqin.xian@vision.ee.ethz.ch; wangjiuniu16@mails.ucas.ac.cn; schiele@mpi-inf.mpg.de; zeynep.akata@uni-tuebingen.de	Xu, Wenjia/GMX-3769-2022; Xu, Wenjia/HHD-3009-2022	Wang, Jiuniu/0000-0002-6113-0066	ERC [853489-DEXIM]; DFG-EXC [2064/1, 390727645]	ERC(European Research Council (ERC)European Commission); DFG-EXC	This work has been partially funded by the ERC 853489-DEXIM and by the DFG-EXC number 2064/1-Project number 390727645.	Akata Z., 2015, CVPR; Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Zeynep, 2020, ADV NEUR IN; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Arik S. O., 2019, ARXIV190206292; Cacheux Y. L., 2019, ICCV; Changpinyo S., 2016, CVPR; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fan Zhang, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P136, DOI 10.1007/978-3-030-67832-6_12; Finn C, 2017, PR MACH LEARN RES, V70; Fu J., 2017, CVPR; Gao TY, 2019, AAAI CONF ARTIF INTE, P6407; Geirhos R., 2019, P INT C LEARNING REP, P1; Guan JC, 2021, IEEE T PATTERN ANAL, V43, P2510, DOI 10.1109/TPAMI.2020.2965534; Guo YC, 2018, AAAI CONF ARTIF INTE, P6870; Hariharan B., 2017, IEEE ICCV; HE K, 2016, CVPR; Hjelm R Devon, 2019, INT C LEARN REPR; Hong J, 2021, PROC CVPR IEEE, P913, DOI 10.1109/CVPR46437.2021.00097; Huang H., 2019, CVPR; Jayaraman D., 2014, CVPR; Jiang H., 2019, ICCV; Kim JH, 2018, ADV NEUR IN, V31; Kingma D.P, P 3 INT C LEARNING R; KumarVerma V., 2018, CVPR; Lampert C. H., 2009, CVPR; Li AX, 2020, INT J COMPUT VISION, V128, P2810, DOI 10.1007/s11263-020-01342-x; Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758; Li K, 2019, IEEE I CONF COMP VIS, P3582, DOI 10.1109/ICCV.2019.00368; Li LH, 2017, AAAI CONF ARTIF INTE, P4133; Li O, 2018, AAAI CONF ARTIF INTE, P3530; Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779; Ling Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13387, DOI 10.1109/CVPR42600.2020.01340; Liu LC, 2014, INT C PATT RECOG, P2619, DOI 10.1109/ICPR.2014.452; Liu S., 2018, NEURIPS; Liu Y., 2019, ICCV; Luming Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14340, DOI 10.1109/CVPR42600.2020.01436; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Oreshkin BN, 2018, ADV NEUR IN, V31; Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z; Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610; Ravi S., 2017, INT C LEARN REPR, P12; Ren C, 2010, EUR J INORG CHEM, P5545, DOI 10.1002/ejic.201000731; Romera-Paredes Bernardino, 2015, ICML; Rusu Andrei A, 2019, ICLR; Schonfeld E., 2019, CVPR; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63649-6; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Sylvain T., 2020, ICLR; Tokmakov P, 2019, IEEE I CONF COMP VIS, P6381, DOI 10.1109/ICCV.2019.00647; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wah C., 2011, TECH REP; Wan ZY, 2021, INT J COMPUT VISION, V129, P1893, DOI 10.1007/s11263-021-01451-1; Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552; Wang K., 2019, ICCV; Wang Q, 2017, INT J COMPUT VISION, V124, P356, DOI 10.1007/s11263-017-1027-5; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Wei C, 2019, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2019.00201; Wei X. S., 2017, IEEE TIP, V26; Xian Y., 2018, CVPR; Xian Y., 2016, CVPR, P6977; Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xie G. S., 2019, CVPR; Yang H. M., 2018, CVPR; Yeh CK, 2018, ADV NEUR IN, V31; Yu YL, 2018, ADV NEUR IN, V31; Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129; Zhang HG, 2021, PROC CVPR IEEE, P9427, DOI 10.1109/CVPR46437.2021.00931; Zhang L., 2017, CVPR; Zhang X., 2016, CVPR; Zheng H., 2017, ICCV; Zhou B., 2016, CVPR; Zhou BL, 2019, IEEE T PATTERN ANAL, V41, P2131, DOI 10.1109/TPAMI.2018.2858759; Zhu PK, 2019, PR MACH LEARN RES, V97; Zhu Y., 2018, CVPR; Zhu YZ, 2019, IEEE I CONF COMP VIS, P9843, DOI 10.1109/ICCV.2019.00994; Zhu YZ, 2019, ADV NEUR IN, V32	87	0	0	5	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1735	1753		10.1007/s11263-022-01613-9	http://dx.doi.org/10.1007/s11263-022-01613-9		MAY 2022	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH		Green Submitted			2022-12-18	WOS:000793662600001
J	Yang, X; Yan, JC				Yang, Xue; Yan, Junchi			On the Arbitrary-Oriented Object Detection: Classification Based Approaches Revisited (vol 130, pg 1340, 2022)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Yang, Xue; Yan, Junchi] Shanghai Jiao Tong Univ, AI Inst, Dept Comp Sci & Engn, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China	Shanghai Jiao Tong University	Yan, JC (corresponding author), Shanghai Jiao Tong Univ, AI Inst, Dept Comp Sci & Engn, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.	yangxue-2019-sjtu@sjtu.edu.cn; yanjunchi@sjtu.edu.cn		Yan, Junchi/0000-0001-9639-7679				Yang X, 2022, INT J COMPUT VISION, V130, P1340, DOI 10.1007/s11263-022-01593-w	1	0	0	7	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1873	1874		10.1007/s11263-022-01618-4	http://dx.doi.org/10.1007/s11263-022-01618-4		MAY 2022	2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH		Bronze			2022-12-18	WOS:000791670100001
J	Deng, HW; Bui, M; Navab, N; Guibas, L; Ilic, S; Birdal, T				Deng, Haowen; Bui, Mai; Navab, Nassir; Guibas, Leonidas; Ilic, Slobodan; Birdal, Tolga			Deep Bingham Networks: Dealing with Uncertainty and Ambiguity in Pose Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D computer vision; Point clouds; Camera relocalization; 6D; Camera pose; Object pose; Rotation; Bingham distribution; Posterior distribution; Ambiguity; Uncertainty; Uncertainty estimation	SIMULTANEOUS LOCALIZATION	In this work, we introduce Deep Bingham Networks (DBN), a generic framework that can naturally handle pose-related uncertainties and ambiguities arising in almost all real life applications concerning 3D data. While existing works strive to find a single solution to the pose estimation problem, we make peace with the ambiguities causing high uncertainty around which solutions to identify as the best. Instead, we report a family of poses which capture the nature of the solution space. DBN extends the state of the art direct pose regression networks by (i) a multi-hypotheses prediction head which can yield different distribution modes; and (ii) novel loss functions that benefit from Bingham distributions on rotations. This way, DBN can work both in unambiguous cases providing uncertainty information, and in ambiguous scenes where an uncertainty per mode is desired. On a technical front, our network regresses continuous Bingham mixture models and is applicable to both 2D data such as images and to 3D data such as point clouds. We proposed new training strategies so as to avoid mode or posterior collapse during training and to improve numerical stability. Our methods are thoroughly tested on two different applications exploiting two different modalities: (i) 6D camera relocalization from images; and (ii) object pose estimation from 3D point clouds, demonstrating decent advantages over the state of the art. For the former we contributed our own dataset composed of five indoor scenes where it is unavoidable to capture images corresponding to views that are hard to uniquely identify. For the latter we achieve the top results especially for symmetric objects of ModelNet dataset (Wu et al., in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 1912-1920, 2015). The code and dataset accompanying this paper is provided under https://multimodal3dvision.github.io.	[Deng, Haowen; Bui, Mai; Navab, Nassir] Tech Univ Munich, Informat, Munich, Germany; [Deng, Haowen; Ilic, Slobodan] Corp Technol Siemens AG, Munich, Germany; [Guibas, Leonidas; Birdal, Tolga] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Technical University of Munich; Siemens AG; Siemens Germany; Stanford University	Birdal, T (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	haowen.deng@tum.de; mai.bui@tum.de; nassir.navab@tum.de; guibas@cs.stanford.edu; slobodan.ilic@tum.de; t.birdal@stanford.edu		Birdal, Tolga/0000-0001-7915-7964	Bacatec, Stanford-Ford Alliance, Directorate for Computer and Information Science and Engineering [1763268]; Vannevar Bush Faculty Fellowship; Stanford SAIL Toyota Research; Samsung GRO	Bacatec, Stanford-Ford Alliance, Directorate for Computer and Information Science and Engineering; Vannevar Bush Faculty Fellowship; Stanford SAIL Toyota Research; Samsung GRO(Samsung)	Funding was provided by Bacatec, Stanford-Ford Alliance, Directorate for Computer and Information Science and Engineering (Grant No. 1763268), Vannevar Bush Faculty Fellowship, Stanford SAIL Toyota Research, Samsung GRO.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733; Balntas V, 2018, LECT NOTES COMPUT SC, V11218, P782, DOI 10.1007/978-3-030-01264-9_46; Barfoot TD, 2014, IEEE T ROBOT, V30, P679, DOI 10.1109/TRO.2014.2298059; Berger J. O., 1985, STAT DECISION THEORY; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; BINGHAM C, 1974, ANN STAT, V2, P1201, DOI 10.1214/aos/1176342874; Birdal T., 2016, IEEE WACV 2016, P1; Birdal T, 2020, PROC CVPR IEEE, P1566, DOI 10.1109/CVPR42600.2020.00164; Birdal T, 2018, ADV NEUR IN, V31; Birdal T, 2019, PROC CVPR IEEE, P11097, DOI 10.1109/CVPR.2019.01136; Birdal T, 2017, IEEE I CONF COMP VIS, P133, DOI 10.1109/ICCV.2017.24; Birdal T, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P527, DOI 10.1109/3DV.2015.65; Bishop C.M., 1994, MIXTURE DENSITY NETW; Brachmann E., 2016, P IEEE C COMP VIS PA; Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489; Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267; Brahmbhatt S, 2018, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2018.00277; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Bui M., 2018, BRIT MACH VIS C BMVC; Bui M., 2019, INT C COMP VIS WORKS; Bui M., 2020, EUR C COMP VIS ECCV; Busam B, 2017, IEEE INT CONF COMP V, P2436, DOI 10.1109/ICCVW.2017.287; Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754; Chen J., 2022, IEEE C COMP VIS PATT; Clark R, 2017, PROC CVPR IEEE, P2652, DOI 10.1109/CVPR.2017.284; Corona E, 2018, IEEE INT C INT ROBOT, P7215, DOI 10.1109/IROS.2018.8594282; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Deng HW, 2018, LECT NOTES COMPUT SC, V11209, P620, DOI 10.1007/978-3-030-01228-1_37; Deng HW, 2019, PROC CVPR IEEE, P3239, DOI 10.1109/CVPR.2019.00336; Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng XK, 2021, IEEE T ROBOT, V37, P1328, DOI 10.1109/TRO.2021.3056043; Dey D, 2015, IEEE I CONF COMP VIS, P2947, DOI 10.1109/ICCV.2015.337; Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022; Falorsi L, 2019, PR MACH LEARN RES, V89; Feng W, 2016, PROC CVPR IEEE, P4049, DOI 10.1109/CVPR.2016.439; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gal Y, 2016, PR MACH LEARN RES, V48; Gilitschenski Igor, 2020, INT C LEARN REPR; Glover J, 2014, IEEE INT CONF ROBOT, P4133, DOI 10.1109/ICRA.2014.6907460; Glover J, 2012, ROBOTICS: SCIENCE AND SYSTEMS VII, P97; Grassia F.S., 1998, J GRAPH TOOLS, V3, P29, DOI [10.1080/10867651.1998.10487493, DOI 10.1080/10867651.1998.10487493]; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Guzm<prime>an-rivera Abner, 2012, ADV NEURAL INFORM PR; Haarbach A, 2018, INT CONF 3D VISION, P381, DOI 10.1109/3DV.2018.00051; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HERZ CS, 1955, ANN MATH, V61, P474, DOI 10.2307/1969810; Hinterstoisser Stefan, 2012, P AS C COMP VIS, P2, DOI DOI 10.1007/978-3-642-37331-2_42; Horaud R., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P500, DOI 10.1109/CVPR.1989.37893; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526; Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169; Kendall A., 2016, P INT C ROB AUT ICRA; Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694; Kendall A, 2016, IEEE INT CONF ROBOT, P4762, DOI 10.1109/ICRA.2016.7487679; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kume A, 2005, BIOMETRIKA, V92, P465, DOI 10.1093/biomet/92.2.465; Kurz G., 2017, ARXIV171209718; Kurz G, 2013, 2013 16TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1487; Labbe M, 2019, J FIELD ROBOT, V36, P416, DOI 10.1002/rob.21831; Liao S, 2019, PROC CVPR IEEE, P9751, DOI 10.1109/CVPR.2019.00999; Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684; Luc P, 2017, IEEE I CONF COMP VIS, P648, DOI 10.1109/ICCV.2017.77; Mahendran S, 2017, IEEE INT CONF COMP V, P2174, DOI 10.1109/ICCVW.2017.254; Makansi O, 2019, PROC CVPR IEEE, P7137, DOI 10.1109/CVPR.2019.00731; Manhardt F., 2019, INT C COMP VIS ICCV; Mardia KV, 2009, DIRECTIONAL STAT; Massiceti Daniela, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5118, DOI 10.1109/ICRA.2017.7989598; McLachlan G.J., 1988, MIXTURE MODELS INFER, V38; Morawiec A, 1996, PHILOS MAG A, V73, P1113, DOI 10.1080/01418619608243708; Murray R. M., 1994, MATH INTRO ROBOTIC M, V1; Okorn B, 2020, IEEE INT C INT ROBOT, P10580, DOI 10.1109/IROS45743.2020.9340860; Paszke Adam, 2017, NIPS AUT WORKSH, DOI DOI 10.1017/CBO9781107707221.009; Peretroukhin Valentin, 2020, P ROB SCI SYST RSS 2; Peretroukhin Valentin, 2019, ARXIV190403182; Piasco N, 2018, PATTERN RECOGN, V74, P90, DOI 10.1016/j.patcog.2017.09.013; Pitteri G, 2019, INT CONF 3D VISION, P614, DOI 10.1109/3DV.2019.00073; Prokudin S, 2018, LECT NOTES COMPUT SC, V11213, P542, DOI 10.1007/978-3-030-01240-3_33; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Qi Charles R, 2017, ARXIV170602413; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Rupprecht C, 2017, IEEE I CONF COMP VIS, P3611, DOI 10.1109/ICCV.2017.388; Salas-Moreno RF, 2013, PROC CVPR IEEE, P1352, DOI 10.1109/CVPR.2013.178; Sarlin PE, 2021, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR46437.2021.00326; Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342; Sattler T, 2015, IEEE I CONF COMP VIS, P2102, DOI 10.1109/ICCV.2015.243; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; Srivatsan RA, 2018, INT J ROBOT RES, V37, P1610, DOI 10.1177/0278364918778353; Subbarao R, 2009, INT J COMPUT VISION, V84, P1, DOI 10.1007/s11263-008-0195-8; Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43; Sundermeyer Martin, 2020, CVPR; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; ULLMAN S, 1979, PROC R SOC SER B-BIO, V203, P405, DOI 10.1098/rspb.1979.0006; Valentin J, 2015, PROC CVPR IEEE, P4400, DOI 10.1109/CVPR.2015.7299069; Wang QPA, 2008, J PHYS A-MATH THEOR, V41, DOI 10.1088/1751-8113/41/6/065004; Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362; Wang YL, 2019, ADV NEUR IN, V32; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Yang LW, 2019, IEEE I CONF COMP VIS, P42, DOI 10.1109/ICCV.2019.00013; Yongheng Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P1, DOI 10.1007/978-3-030-58452-8_1; Yuan W., 2018, ARXIV181111209; Zakharov S, 2019, IEEE I CONF COMP VIS, P532, DOI 10.1109/ICCV.2019.00062; Zakharov S, 2017, IEEE INT C INT ROBOT, P552; Zeisl B, 2015, IEEE I CONF COMP VIS, P2704, DOI 10.1109/ICCV.2015.310; Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29; Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zolfaghari Mohammadreza, 2019, ARXIV190503578	114	0	0	4	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1627	1654		10.1007/s11263-022-01612-w	http://dx.doi.org/10.1007/s11263-022-01612-w		MAY 2022	28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH		Green Submitted			2022-12-18	WOS:000790633300001
J	Ikeuchi, K; Morimoto, T; Kamakura, M; Kuchitsu, N; Kawano, K; Ikeda, T				Ikeuchi, Katsushi; Morimoto, Tetsuro; Kamakura, Mawo; Kuchitsu, Nobuaki; Kawano, Kazutaka; Ikeda, Tomoo			Kyushu Decorative Tumuli Project: From e-Heritage to Cyber-Archaeology	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						e-Heritage; Cultural assets; Cyber-archaeology; Decorative tumulus; Museum; Photometric data; 3D data; Reflectance; Color analysis; Simulation; Principle component analysis; Normalized cut; Restoration; Segmentation	CULTURAL-HERITAGE; REFLECTANCE	Digitization of cultural assets has become an important sub-area of computer vision (CV). Thus far, the value of digitization has been emphasized in terms of asset preservation and exhibition. The third aspect of digitization value is that the obtained digital data can be used to perform archaeological analysis based on physics and optics theories and simulations. This position paper emphasizes the importance of this third aspect, using our Kyushu decorative tumuli project as an illustrative example. In particular, we focus on the photometric approaches in the third aspect and explain the equipment and methods developed there as well as archaeological findings. This paper, then, proposes to establish this area as "cyber-archaeology" through categorizing and organizing those methodologies.	[Ikeuchi, Katsushi; Morimoto, Tetsuro; Kamakura, Mawo] Univ Tokyo, Tokyo, Japan; [Ikeuchi, Katsushi; Kamakura, Mawo] Microsoft, Redmond, WA 98052 USA; [Morimoto, Tetsuro] Toppan Printing, Tokyo, Japan; [Kuchitsu, Nobuaki] Tokyo Res Inst Cultural Properties, Tokyo, Japan; [Kawano, Kazutaka] Tokyo Natl Museum, Tokyo, Japan; [Ikeda, Tomoo] Kumamoto Prefecture Govt, Kumamoto, Japan	University of Tokyo; Microsoft; Toppan Printing Co Ltd	Ikeuchi, K (corresponding author), Univ Tokyo, Tokyo, Japan.; Ikeuchi, K (corresponding author), Microsoft, Redmond, WA 98052 USA.	katsushi.ikeuchi@ieee.org; tetsuro.morimoto@toppan.co.jp; mawo.kamakura@microsoft.com; kuchitsu@tobunken.go.jp; k-kawano@tnm.jp; ikeda-t-dr@pref.kumamoto.lg.jp						Ausonius J.D.C., 2016, BRYN MAWR CLASSICAL; Banno A, 2008, INT J COMPUT VISION, V78, P207, DOI 10.1007/s11263-007-0104-6; Beer A., 1852, ANN PHYS, V86, P78, DOI DOI 10.1002/ANDP.18521620505; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bok Y, 2011, INT J COMPUT VISION, V94, P36, DOI 10.1007/s11263-010-0397-8; Cucci C, 2016, ACCOUNTS CHEM RES, V49, P2070, DOI 10.1021/acs.accounts.6b00048; Dellepiane M., 2007, VAST, P117, DOI 10.2312/VAST/VAST07/117-124; Dessales H., 2020, VILLA DIOMEDES MAKIN, V1st; Fontana R, 2002, J CULT HERIT, V3, P325, DOI 10.1016/S1296-2074(02)01242-6; Fukiage T, 2014, INT SYM MIX AUGMENT, P63, DOI 10.1109/ISMAR.2014.6948410; Gomes L, 2014, PATTERN RECOGN LETT, V50, P3, DOI 10.1016/j.patrec.2014.03.023; Ikari A., 2005, 11 INT C VIRT SYST M; Ikeuchi K., 2004, P VIRT SYST MULT; Ikeuchi K, 2007, INT J COMPUT VISION, V75, P189, DOI 10.1007/s11263-007-0039-y; Ikeuchi K, 2013, 2013 INTERNATIONAL CONFERENCE ON CULTURE AND COMPUTING (CULTURE AND COMPUTING 2013), P1, DOI 10.1109/CultureComputing.2013.77; Ikeuchi Katsushi, 2008, DIGITALLY ARCHIVING; Kader VI., 2015, J BAVARIAN ACAD SCI, V53, P72; Kakuta T., 2008, DIGITALLY ARCHIVING, P457; Kamakura M., 2005, VIRTUAL SYSTEMS MULT, P751; Kamaroddin MFA, 2019, IOP C SER EARTH ENV, V268, DOI 10.1088/1755-1315/268/1/012057; Kuchitsu N., 2005, 22 ANN M JAP SOC CUL; Kuchitsu N., 2004, 21 ANN M JAP SOC CUL; Kyushu National Museum, 2015, NEW PROGR VR IM DEC; Kyushu National Museum, 2014, SEARCH ROOTS MYST PA; Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849; Mallik A., 2017, DIGITAL HAMPI PRESER; Masuda T., 2008, DIGITALLY ARCHIVING, P419; Matsuda, 2019, PUBLIC ARCHAEOLOGY S, P105; Miyazaki D, 2010, IEEE IMAGE PROC, P4057, DOI 10.1109/ICIP.2010.5650067; Miyazaki D, 2010, INT J COMPUT VISION, V86, P229, DOI 10.1007/s11263-009-0262-9; Morimoto T., 2013, 30 ANN M JAP SOC CUL; Morimoto T., 2012, 29 ANN M JAP SOC CUL; Morimoto T., 2011, 28 ANN M JAP SOC CUL; Morimoto T., 2010, 27 ANN M JAP SOC CUL; Morimoto T., 2014, 31 ANN M JAP SOC CUL; Morimoto T, 2010, PROC CVPR IEEE, P207, DOI 10.1109/CVPR.2010.5540211; Morimoto T, 2008, INT J AUTOM COMPUT, V5, P226, DOI 10.1007/s11633-008-0226-5; Narasimhan SG, 2003, PROC CVPR IEEE, P665; NAYAR SK, 1990, IEEE T ROBOTIC AUTOM, V6, P418, DOI 10.1109/70.59367; Pietroni N, 2011, IEEE T VIS COMPUT GR, V17, P1989, DOI 10.1109/TVCG.2011.165; Rushmeier H., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P99, DOI 10.1109/IM.1999.805339; Rushmeier H., 2005, MODELING VISUALIZATI, P183; Sato Y., 2014, J VIRTUAL REALITY SO, V19, P247; Sengoku-Haga K., 2015, NEW APPROACHES TEMPL; Sengoku-Haga K., 2017, ART BRONZ GREEKS THE; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Sundstedt V., 2004, P 3 INT C COMP GRAPH, P107, DOI DOI 10.1145/1029949.1029970; Szeliski R, 2006, LECT NOTES COMPUT SC, V3952, P16; Woodham R. J., 1978, Proceedings of the Society of Photo-Optical Instrumentation Engineers, vol.155. Image Understanding Systems and Industrial Applications, P136	49	0	0	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2022	130	7					1609	1626		10.1007/s11263-022-01609-5	http://dx.doi.org/10.1007/s11263-022-01609-5		MAY 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	2M4VH		hybrid			2022-12-18	WOS:000789728900001
J	Mustafa, A; Russell, C; Hilton, A				Mustafa, Armin; Russell, Chris; Hilton, Adrian			4D Temporally Coherent Multi-Person Semantic Reconstruction and Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dynamic 4D reconstruction; Segmentation; Scene understanding; Sports	ENERGY MINIMIZATION; 3D	We introduce the first approach to solve the challenging problem of automatic 4D visual scene understanding for complex dynamic scenes with multiple interacting people from multi-view video. Our approach simultaneously estimates a detailed model that includes a per-pixel semantically and temporally coherent reconstruction, together with instance-level segmentation exploiting photo-consistency, semantic and motion information. We further leverage recent advances in 3D pose estimation to constrain the joint semantic instance segmentation and 4D temporally coherent reconstruction. This enables per person semantic instance segmentation of multiple interacting people in complex dynamic scenes. Extensive evaluation of the joint visual scene understanding framework against state-of-the-art methods on challenging indoor and outdoor sequences demonstrates a significant (approximate to 40%) improvement in semantic segmentation, reconstruction and scene flow accuracy. In addition to the evaluation on several indoor and outdoor scenes, the proposed joint 4D scene understanding framework is applied to challenging outdoor sports scenes in the wild captured with manually operated wide-baseline broadcast cameras.	[Mustafa, Armin; Russell, Chris; Hilton, Adrian] Univ Surrey, Ctr Vis Speech & Signal Proc CVSSP, Guildford GU2 7XH, Surrey, England	University of Surrey	Mustafa, A (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc CVSSP, Guildford GU2 7XH, Surrey, England.	a.mustafa@surrey.ac.uk; chris.russell@surrey.ac.uk; a.hilton@surrey.ac.uk	Hilton, Adrian/N-3736-2014	Hilton, Adrian/0000-0003-4223-238X; Mustafa, Armin/0000-0002-1779-2775	Royal Academy of Engineering Research Fellowship [RF-201718-17177]; EPSRC Platform Grant on Audio-Visual Media Research [EP/P022529]	Royal Academy of Engineering Research Fellowship(Royal Academy of Engineering - UK); EPSRC Platform Grant on Audio-Visual Media Research(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This research was supported by the Royal Academy of Engineering Research Fellowship RF-201718-17177 and the EPSRC Platform Grant on Audio-Visual Media Research EP/P022529.	[Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; [Anonymous], CTR VIS SPEECH SIGN; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Ballan L, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778824; Basha T, 2010, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2010.5539791; Bi Sai, 2020, P ECCV; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Caliskan A., 2020, AS C COMP VIS ACCV; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chen HT, 2020, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR42600.2020.00154; Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen PY, 2019, PROC CVPR IEEE, P2619, DOI 10.1109/CVPR.2019.00273; Chiu WC, 2013, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.2013.48; Dai A, 2018, LECT NOTES COMPUT SC, V11214, P458, DOI 10.1007/978-3-030-01249-6_28; Djelouah A, 2016, INT CONF 3D VISION, P360, DOI 10.1109/3DV.2016.45; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Engelmann F, 2016, LECT NOTES COMPUT SC, V9796, P219, DOI 10.1007/978-3-319-45886-1_18; Evangelidis GD, 2008, IEEE T PATTERN ANAL, V30, P1858, DOI 10.1109/TPAMI.2008.113; Everingham M., 2012, PASCAL VISUAL OBJECT; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Floros G, 2012, PROC CVPR IEEE, P2823, DOI 10.1109/CVPR.2012.6248007; Gilbert A., 2020, BMVC; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Guerry J, 2017, IEEE INT CONF COMP V, P669, DOI 10.1109/ICCVW.2017.85; Guillemaut JY, 2011, INT J COMPUT VISION, V93, P73, DOI 10.1007/s11263-010-0413-z; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Hane C, 2017, IEEE T PATTERN ANAL, V39, P1730, DOI 10.1109/TPAMI.2016.2613051; Hasler N, 2009, PROC CVPR IEEE, P224, DOI 10.1109/CVPRW.2009.5206859; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Huang YH, 2017, INT CONF 3D VISION, P421, DOI 10.1109/3DV.2017.00055; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781; Kim H, 2012, IEEE T CIRC SYST VID, V22, P1611, DOI 10.1109/TCSVT.2012.2202185; Klodt M, 2018, LECT NOTES COMPUT SC, V11214, P713, DOI 10.1007/978-3-030-01249-6_43; Kundu A, 2016, PROC CVPR IEEE, P3168, DOI 10.1109/CVPR.2016.345; Kundu A, 2014, LECT NOTES COMPUT SC, V8694, P703, DOI 10.1007/978-3-319-10599-4_45; Kundu Abhijit, 2020, ECCV; Lai HY, 2019, PROC CVPR IEEE, P1890, DOI 10.1109/CVPR.2019.00199; Langguth F, 2016, LECT NOTES COMPUT SC, V9907, P469, DOI 10.1007/978-3-319-46487-9_29; Larsen ES, 2007, IEEE I CONF COMP VIS, P1440; Li X., 2020, ECCV, V12375, P121, DOI 10.1007/978-3-030-58577-88; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Luo B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1187, DOI 10.1145/2733373.2806313; Menze M, 2015, LECT NOTES COMPUT SC, V9358, P16, DOI 10.1007/978-3-319-24947-6_2; Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959; Mustafa A, 2019, IEEE I CONF COMP VIS, P10422, DOI 10.1109/ICCV.2019.01052; Mustafa A, 2019, IEEE T IMAGE PROCESS, V28, P1118, DOI 10.1109/TIP.2018.2872906; Mustafa A, 2017, INT CONF 3D VISION, P29, DOI 10.1109/3DV.2017.00014; Mustafa A, 2017, PROC CVPR IEEE, P5583, DOI 10.1109/CVPR.2017.592; Mustafa A, 2016, PROC CVPR IEEE, P4660, DOI 10.1109/CVPR.2016.504; Mustafa A, 2016, LECT NOTES COMPUT SC, V9905, P213, DOI 10.1007/978-3-319-46448-0_13; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ranjan A., 2019, CVPR, P12240, DOI DOI 10.1109/CVPR.2019.01252; Ranjan Anurag, 2018, BRIT MACH VIS C BMVC; Rodriguez A.L., 2020, BMVC; Rossi M., 2020, IEEE CVF C COMP VIS; Roussos A, 2012, INT SYM MIX AUGMENT, P31, DOI 10.1109/ISMAR.2012.6402535; Rusu RB, 2010, KUNSTL INTELL, V24, P345, DOI 10.1007/s13218-010-0059-6; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Sevilla-Lara L, 2016, PROC CVPR IEEE, P3889, DOI 10.1109/CVPR.2016.422; Siam M, 2018, IEEE IMAGE PROC, P1603, DOI 10.1109/ICIP.2018.8451495; Sorkine O., 2007, P 5 EUR S GEOM PROC, V4, P109, DOI [DOI 10.2312/SGP/SGP07/109-116, 10.2312/SGP/SGP07/109-116]; Szeliski R., 1999, CVPR; Taniai T, 2018, IEEE T PATTERN ANAL, V40, P2725, DOI 10.1109/TPAMI.2017.2766072; Tao M, 2012, COMPUT GRAPH FORUM, V31, P345, DOI 10.1111/j.1467-8659.2012.03013.x; Tome D, 2018, INT CONF 3D VISION, P474, DOI 10.1109/3DV.2018.00061; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Trager M, 2019, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2019.00031; Tsai YH, 2016, LECT NOTES COMPUT SC, V9908, P760, DOI 10.1007/978-3-319-46493-0_46; Ulusoy AO, 2017, PROC CVPR IEEE, P4531, DOI 10.1109/CVPR.2017.482; Vineet V, 2015, IEEE INT CONF ROBOT, P75, DOI 10.1109/ICRA.2015.7138983; Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696; Vogel C, 2015, INT J COMPUT VISION, V115, P1, DOI 10.1007/s11263-015-0806-0; Wang LJ, 2020, PROC CVPR IEEE, P538, DOI 10.1109/CVPR42600.2020.00062; Wedel A, 2011, INT J COMPUT VISION, V95, P29, DOI 10.1007/s11263-010-0404-0; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; WeiZeng S.K., 2020, BMVC; Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644; Xie J, 2016, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2016.401; Xu J, 2017, PROC CVPR IEEE, P5807, DOI 10.1109/CVPR.2017.615; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yujun Cai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P226, DOI 10.1007/978-3-030-58571-6_14; Zanfir A, 2015, IEEE I CONF COMP VIS, P4417, DOI 10.1109/ICCV.2015.502; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhong YR, 2019, PROC CVPR IEEE, P12087, DOI 10.1109/CVPR.2019.01237	93	0	0	3	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1583	1606		10.1007/s11263-022-01599-4	http://dx.doi.org/10.1007/s11263-022-01599-4		APR 2022	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU		hybrid			2022-12-18	WOS:000788466700001
J	Andre, AN; Sandoz, P; Jacquot, M; Laurent, GJ				Andre, A. N.; Sandoz, P.; Jacquot, M.; Laurent, G. J.			Pose Measurement at Small Scale by Spectral Analysis of Periodic Patterns	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Fiducial markers; Fourier analysis; Pose measurement	TRACKING; LOCALIZATION; CAMERA; FIDUCIALS; PRECISION; ACCURACY; POSITION	The retrieval of an observed object's pose is an essential computer vision problem. The challenge arises in many different fields, among them robotics control, contactless metrology, or augmented reality. When the observed object shrinks from the macroscopic scale to the microscopic, pose estimation is further complicated by the weaker perspective of imaging macroscale lenses down to the quasi-orthographic projection inherent to microscope objectives. This paper tackles this issue of microscale pose estimation in two complementary steps that rely on the use of planar periodic targets. We first consider the orthographic projection case as a means of presenting the theory of the method and showing how the pose of periodic patterns can be directly retrieved from the Fourier frequency spectrum of a given image. We then address the perspective case with long focal lengths, in which the full six-degrees of freedom (6-DOF) pose can be retrieved without ambiguities by following the same theoretical background. In addition to theoretically justifying pose retrieval via Fourier analysis of acquired images, this paper demonstrates the method's actual performance. Both simulations and experimentation are conducted to validate the method and confirm an experimental resolution lower than 1/1000th of a pixel for translations. For orientation measurement, resolutions below 1 mu rad. for in-plane orientation, and below 100 mu rad. for off-axis orientations can be achieved.	[Andre, A. N.; Sandoz, P.; Jacquot, M.; Laurent, G. J.] FEMTO ST Inst, 15B Ave Montboucons, F-25000 Besancon, France	Centre National de la Recherche Scientifique (CNRS); Universite de Franche-Comte; Universite de Technologie de Belfort-Montbeliard (UTBM)	Andre, AN (corresponding author), FEMTO ST Inst, 15B Ave Montboucons, F-25000 Besancon, France.	antoine.andre@femto-st.fr; patrick.sandoz@femto-st.fr; maxime.jacquot@femto-st.fr; guillaume.laurent@femto-st.fr	Jacquot, Maxime/A-7402-2012	Jacquot, Maxime/0000-0003-0285-204X; Laurent, Guillaume J./0000-0003-3586-4696; Sandoz, Patrick/0000-0003-3570-6196; ANDRE, Antoine N./0000-0003-3318-4769	Region Bourgogne Franche-Comte; ANR project Holo-Control [ANR-21-CE42-0009]; I-SITE BFC project HoloNet [ANR-15-IDEX-03]; Cross-disciplinary Research (EIPHI) Graduate School [ANR-17-EURE-0002]; ROBOTEX robotics network [ANR-10-EQPX-44-01]	Region Bourgogne Franche-Comte(Region Bourgogne-Franche-Comte); ANR project Holo-Control(French National Research Agency (ANR)); I-SITE BFC project HoloNet; Cross-disciplinary Research (EIPHI) Graduate School; ROBOTEX robotics network	This work was supported by Region Bourgogne Franche-Comte, by the ANR project Holo-Control (ANR-21-CE42-0009), by the I-SITE BFC project HoloNet (ANR-15-IDEX-03), by Cross-disciplinary Research (EIPHI) Graduate School (ANR-17-EURE-0002). The encoded target was realized thanks to the RENATECH technological network and its FEMTO-ST facility MIMENTO. The experiments was conducted within the ROBOTEX robotics network (ANR-10-EQPX-44-01) and its FEMTO-ST micro-nano-robotics center. Authors acknowledge G. Jutzi, L. Robert, M. Suarez and L. Gauthier-Manuel for technological and experimental assistance.	Abawi DF, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P260, DOI 10.1109/ISMAR.2004.8; Andre AN, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3009353; Andre AN, 2020, IEEE-ASME T MECH, V25, P1193, DOI 10.1109/TMECH.2020.2965211; Andreotti A, 2020, 2020 20TH IEEE INTERNATIONAL CONFERENCE ON ENVIRONMENT AND ELECTRICAL ENGINEERING AND 2020 4TH IEEE INDUSTRIAL AND COMMERCIAL POWER SYSTEMS EUROPE (EEEIC/I&CPS EUROPE); Azar ER, 2015, J INF TECHNOL CONSTR, V20, P213; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bomarito GF, 2017, OPT LASER ENG, V91, P73, DOI 10.1016/j.optlaseng.2016.11.005; Bouguet J.Y., 2015, CAMERA CALIBRATION T; Bruckstein AM, 2000, INT J COMPUT VISION, V39, P131, DOI 10.1023/A:1008123110489; Bruckstein AM, 1999, INT J COMPUT VISION, V35, P223, DOI 10.1023/A:1008156210387; Chen XC, 2020, OPT LASER ENG, V133, DOI 10.1016/j.optlaseng.2020.106121; Chen ZH, 2016, MEAS SCI TECHNOL, V27, DOI 10.1088/0957-0233/27/12/125018; Chu H. K., 2012, 2012 IEEE International Conference on Mechatronics and Automation (ICMA), P813, DOI 10.1109/ICMA.2012.6283247; Collins T, 2014, INT J COMPUT VISION, V109, P252, DOI 10.1007/s11263-014-0725-5; Didier JY, 2008, INT J IMAGE GRAPH, V8, P169, DOI 10.1142/S0219467808003039; Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620; Fiala M, 2005, PROC CVPR IEEE, P590; Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005; Guelpa V, 2014, SENSORS-BASEL, V14, P5056, DOI 10.3390/s140305056; Kato H., 1999, Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99), P85, DOI 10.1109/IWAR.1999.803809; Kim JA, 2018, REV SCI INSTRUM, V89, DOI 10.1063/1.5022717; Kim YS, 2015, SENSOR ACTUAT A-PHYS, V234, P48, DOI 10.1016/j.sna.2015.08.006; Li H, 2019, MECH SYST SIGNAL PR, V124, P111, DOI 10.1016/j.ymssp.2019.01.046; Liu A, 2016, INT J COMPUT VISION, V118, P1, DOI 10.1007/s11263-015-0866-1; Liu J, 2013, IEEE INT CONF ROBOT, P1724, DOI 10.1109/ICRA.2013.6630803; Loing V, 2018, INT J COMPUT VISION, V126, P1045, DOI 10.1007/s11263-018-1102-6; Marturi N, 2018, IEEE T AUTOM SCI ENG, V15, P45, DOI 10.1109/TASE.2016.2580660; Moreels P, 2007, INT J COMPUT VISION, V73, P263, DOI 10.1007/s11263-006-9967-1; Naimark L, 2002, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P27, DOI 10.1109/ISMAR.2002.1115065; Pentenrieder K., 2006, P DRITTR WORKSH VIRT; Ri S, 2014, OPT EXPRESS, V22, P9693, DOI 10.1364/OE.22.009693; Sandoz P, 2002, APPL OPTICS, V41, P5503, DOI 10.1364/AO.41.005503; Sattar J, 2007, FOURTH CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P165, DOI 10.1109/CRV.2007.34; Shang WF, 2016, SCI REP-UK, V6, DOI 10.1038/srep22534; Sugiura H, 2015, MICROMACHINES-BASEL, V6, P660, DOI 10.3390/mi6060660; Tamadazte B, 2010, INT J ROBOT RES, V29, P1416, DOI 10.1177/0278364910376033; Yamahata C, 2010, J MICROELECTROMECH S, V19, P1273, DOI 10.1109/JMEMS.2010.2067445; Yao S, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3065436; Zhong LS, 2019, INT J COMPUT VISION, V127, P973, DOI 10.1007/s11263-018-1119-x	39	0	0	7	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1566	1582		10.1007/s11263-022-01607-7	http://dx.doi.org/10.1007/s11263-022-01607-7		APR 2022	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU		Green Submitted			2022-12-18	WOS:000783738500001
J	Soviany, P; Ionescu, RT; Rota, P; Sebe, N				Soviany, Petru; Ionescu, Radu Tudor; Rota, Paolo; Sebe, Nicu			Curriculum Learning: A Survey	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Curriculum learning; Learning from easy to hard; Self-paced learning; Neural networks; Deep learning	NEURAL-NETWORKS	Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.	[Soviany, Petru; Ionescu, Radu Tudor] Univ Bucharest, Dept Comp Sci, Bucharest 010014, Romania; [Ionescu, Radu Tudor] Univ Bucharest, Romanian Young Acad, Bucharest 010014, Romania; [Rota, Paolo; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Povo, Italy	University of Bucharest; University of Bucharest; University of Trento	Ionescu, RT (corresponding author), Univ Bucharest, Dept Comp Sci, Bucharest 010014, Romania.; Ionescu, RT (corresponding author), Univ Bucharest, Romanian Young Acad, Bucharest 010014, Romania.	raducu.ionescu@gmail.com	Soviany, Petru/GYU-1663-2022; Ionescu, Radu Tudor/H-1914-2016	Sebe, Niculae/0000-0002-6597-7248; Rota, Paolo/0000-0003-0663-5659	Grant of the Romanian Ministry of Education and Research, CNCS -UEFISCDI, within PNCDI III [PN-III-P1-1.1-TE-2019-0235]; Romanian Young Academy - Stiftung Mercator; Alexander von Humboldt Foundation; European Union [951911 -AI4Media]	Grant of the Romanian Ministry of Education and Research, CNCS -UEFISCDI, within PNCDI III; Romanian Young Academy - Stiftung Mercator; Alexander von Humboldt Foundation(Alexander von Humboldt Foundation); European Union(European Commission)	This work was supported by a Grant of the Romanian Ministry of Education and Research, CNCS -UEFISCDI, Project Number PN-III-P1-1.1-TE-2019-0235, within PNCDI III. This article has also benefited from the support of the Romanian Young Academy, which is funded by Stiftung Mercator and the Alexander von Humboldt Foundation for the period 2020-2022. This work was also supported by European Union's Horizon 2020 research and innovation programme under Grant No. 951911 -AI4Media.	Allgower EL., 2003, SIAM, DOI 10.1137/1.9780898719154.fm; Almeida J., 2020, ARXIV200100238; Alsharid Mohammad, 2020, Med Ultrasound Preterm Perinat Paediatr Image Anal (2020), V12437, P75, DOI 10.1007/978-3-030-60334-2_8; Amodei D, 2016, PR MACH LEARN RES, V48; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bao S., 2020, ARXIV200616779; Bassich A., 2019, P SURL; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, 10.48550/arXiv.2005.14165]; Burduja M, 2021, IEEE IMAGE PROC, P3787, DOI 10.1109/ICIP42928.2021.9506067; Buyuktas B., 2020, P EUSIPCO; Carion N., 2020, COMPUTER VISION ECCV, P213, DOI DOI 10.1007/978-3-030-58452-8_13; Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951; Cascante-Bonilla Paola, 2020, ARXIV200106001; Castells Thibault, 2020, ADV NEURAL INFORM PR, V33, P1; Caubriere A, 2019, INTERSPEECH, P1198, DOI 10.21437/Interspeech.2019-1832; Chang E., 2021, P EACL, P727; Chang H. -S., 2017, ADV NEURAL INFORM PR, V30, P1002; Chen J., 2021, ARXIV210406468; Chen Jieneng, 2021, ARXIV210204306; Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168; Chen YH, 2018, LECT NOTES COMPUT SC, V11070, P91, DOI 10.1007/978-3-030-00928-1_11; Cheng H., 2019, P CVPR, P4748; Choi Jaehoon, 2019, ARXIV190800262; Chow J., 1991, 1991 IEEE International Sympoisum on Circuits and Systems (Cat. No.91CH3006-4), P2483, DOI 10.1109/ISCAS.1991.176030; Cirik Volkan, 2016, ARXIV161106204; Dai DX, 2020, INT J COMPUT VISION, V128, P1182, DOI 10.1007/s11263-019-01182-4; Dizaji KG, 2019, PROC CVPR IEEE, P4386, DOI 10.1109/CVPR.2019.00452; Dogan Urun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P174, DOI 10.1007/978-3-030-58526-6_11; Dosovitskiy A., 2021, ICLR; Eiband M, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P211, DOI 10.1145/3172944.3172961; ELMAN JL, 1993, COGNITION, V48, P71, DOI 10.1016/0010-0277(93)90058-4; Eppe M, 2019, J IEEE I C DEVELOP L, P183, DOI 10.1109/DEVLRN.2019.8850721; Fan YB, 2017, AAAI CONF ARTIF INTE, P1877; Fang LL, 2021, PATTERN ANAL APPL, V24, P1685, DOI 10.1007/s10044-021-01021-8; Fang M, 2019, ADV NEUR IN, V32; Feng D., 2020, P NEURIPS, P3141; Feng Zhengyang, 2020, ARXIV200408514; Florensa C., 2017, PROC 1 C ROBOT LEARN, P482; Foglino F., 2019, P WCGO, P720; Fournier P, 2021, IEEE T COGN DEV SYST, V13, P239, DOI 10.1109/TCDS.2019.2933371; Ganesh Madan Ravi, 2020, ARXIV200104529; Gao YH, 2021, LECT NOTES COMPUT SC, V12903, P61, DOI 10.1007/978-3-030-87199-4_6; Georgescu M., 2020, ARXIV201107491; Gong MG, 2019, IEEE T EVOLUT COMPUT, V23, P288, DOI 10.1109/TEVC.2018.2850769; Gong Y., 2021, P CIKM, P3034; Graves A, 2017, PR MACH LEARN RES, V70; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Guo JL, 2020, AAAI CONF ARTIF INTE, V34, P7839; Guo S, 2018, LECT NOTES COMPUT SC, V11214, P139, DOI 10.1007/978-3-030-01249-6_9; Guo Y., 2020, INTERNA TIONAL C MAC, P3822; Hacohen G, 2019, PR MACH LEARN RES, V97; Hatamizadeh Ali, 2021, ARXIV210310504; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu D., 2020, ARXIV200109414; Huang Ruozi, 2020, ARXIV200606119; Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594; Ionescu RT, 2016, PROC CVPR IEEE, P2157, DOI 10.1109/CVPR.2016.237; JAEGLE A, 2021, PR MACH LEARN RES, V139; Jafarpour B, 2021, INTERNLP 2021 - FIRST WORKSHOP ON INTERACTIVE LEARNING FOR NATURAL LANGUAGE PROCESSING, P40; Jiang L, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P547, DOI 10.1145/2647868.2654918; Jiang L, 2015, AAAI CONF ARTIF INTE, P2694; Jiang L, 2014, ADV NEUR IN, V27; Jimenez-Sanchez A, 2019, LECT NOTES COMPUT SC, V11769, P694, DOI 10.1007/978-3-030-32226-7_77; Khan Salman, 2021, ARXIV210101169; Kim D., 2019, ARXIV190200829; Kim T.H., 2018, ARXIV180100904; Kim Y, 2016, AAAI CONF ARTIF INTE, P2741; Klink P., 2020, P CORL, P513; Kocmi Tom, 2017, RANLP; Korkmaz Y., 2021, ARXIV210508059; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar Gaurav, 2019, NAACL HLT; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Kuo WC, 2019, P NATL ACAD SCI USA, V116, P22737, DOI 10.1073/pnas.1908021116; Lee YJ, 2011, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2011.5995523; Li BB, 2020, ETRA'20 FULL PAPERS: ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, DOI 10.1145/3379155.3391334; Li CS, 2017, AAAI CONF ARTIF INTE, P2175; Li S., 2017, P BMVC; Lin L, 2018, IEEE T PATTERN ANAL, V40, P7, DOI 10.1109/TPAMI.2017.2652459; Liu C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4223; Liu FL, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P3001; Liu JL, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3861; Liu XB, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P427; Lotter W, 2017, LECT NOTES COMPUT SC, V10553, P169, DOI 10.1007/978-3-319-67558-9_20; Lu J, 2018, PR MACH LEARN RES, V80; Luo S, 2020, IEEE IJCNN; Luthra A., 2021, P ICCVW; Luyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P608, DOI 10.1007/978-3-030-58568-6_36; Ma F, 2017, PR MACH LEARN RES, V70; Matiisen T, 2020, IEEE T NEUR NET LEAR, V31, P3732, DOI 10.1109/TNNLS.2019.2934906; MITCHELL T, 1989, ANNU REV COMPUT SCI, V4, P417; Morerio P, 2017, IEEE I CONF COMP VIS, P3564, DOI 10.1109/ICCV.2017.383; Murali A, 2018, IEEE INT CONF ROBOT, P6453; Nabli A., 2020, P NEURIPS, P7044; Narvekar S, 2020, J MACH LEARN RES, V21; Narvekar S, 2019, AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P25; Narvekar S, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P566; Nilesh Pathak Harsh, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P1637, DOI 10.1109/ICMLA.2019.00268; Oksuz I, 2019, MED IMAGE ANAL, V55, P136, DOI 10.1016/j.media.2019.04.009; Penha G., 2019, ARXIV191208555; PENTINA A, 2015, PROC CVPR IEEE, P5492, DOI [DOI 10.1109/CVPR.2015.7299188, 10.1109/CVPR.2015.7299188]; Pi Te, 2016, P 25 INT JOINT C ART, P1932; Platanios Emmanouil Antonios, 2019, P 2019 C N AM CHAPT, P1162, DOI DOI 10.18653/V1/N19-1119; Portelas R., 2020, ARXIV201108463; Portelas R., 2020, C ROBOT LEARNING, P835; Qin W, 2020, IEEE ACCESS, V8, P25990, DOI 10.1109/ACCESS.2020.2970726; Qing Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P438, DOI 10.1007/978-3-030-58610-2_26; Qu M, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P468, DOI 10.1145/3159652.3159711; Rajeswar Sai, 2017, ARXIV170510929; RAVANELLI M, 2018, IEEE W SP LANG TECH, P1021, DOI DOI 10.1109/SLT.2018.8639585; Ren MY, 2018, PR MACH LEARN RES, V80; Ren YZ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2641; Ren ZP, 2018, IEEE T NEUR NET LEAR, V29, P2216, DOI 10.1109/TNNLS.2018.2790981; Ristea N., 2021, ARXIV211006400; Ristea NC, 2021, INTERSPEECH, P2836, DOI 10.21437/Interspeech.2021-155; Rongchang Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P190, DOI 10.1007/978-3-030-58589-1_12; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruiter D, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2560; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sachan M, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P453; Sakaridis C, 2019, IEEE I CONF COMP VIS, P7373, DOI 10.1109/ICCV.2019.00747; Sangineto E, 2019, IEEE T PATTERN ANAL, V41, P712, DOI 10.1109/TPAMI.2018.2804907; Saxena Shreyas, 2019, ADV NEURAL INFORM PR, P11095; Shi MJ, 2016, LECT NOTES COMPUT SC, V9909, P105, DOI 10.1007/978-3-319-46454-1_7; Shi YY, 2015, COMPUT SPEECH LANG, V33, P136, DOI 10.1016/j.csl.2014.11.004; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Shu Y, 2019, AAAI CONF ARTIF INTE, P4951; Sinha S., 2020, ADV NEURAL INFORM PR, P21653; Soviany P, 2021, COMPUT VIS IMAGE UND, V204, DOI 10.1016/j.cviu.2021.103166; Soviany P, 2020, IEEE WINT CONF APPL, P3452, DOI 10.1109/WACV45572.2020.9093408; Soviany P, 2019, LECT NOTES COMPUT SC, V11132, P366, DOI 10.1007/978-3-030-11018-5_33; Soviany Petru., 2020, MRC ECAI; Spitkovsky V. I., 2009, P NIPS WORKSH GRAMM; Sun LJ, 2020, IEEE ACCESS, V8, P132012, DOI 10.1109/ACCESS.2020.3009988; Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tang K., 2012, ADV NEURAL INFORM PR; Tang Y., 2012, ACM INT C MULT, P833, DOI DOI 10.1145/2393347.2396324; Tang YP, 2019, AAAI CONF ARTIF INTE, P5117; Tang YX, 2018, LECT NOTES COMPUT SC, V11046, P249, DOI 10.1007/978-3-030-00919-9_29; Tay Y, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4922; Doan T, 2019, AAAI CONF ARTIF INTE, P3470; Tidd B., 2020, ARXIV201003848; Tsvetkov Y, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P130; Turchetta M., 2020, ARXIV200612136; Wang J, 2018, INT C PATT RECOG, P2416, DOI 10.1109/ICPR.2018.8546088; Wang W, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1282; Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512; Wei DL, 2018, PROC CVPR IEEE, P8052, DOI 10.1109/CVPR.2018.00840; Wei J., 2020, ARXIV200913698; Weinshall D, 2018, PR MACH LEARN RES, V80; Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009; Wu LJ, 2018, ADV NEUR IN, V31; Xu C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3974; Zaremba W, 2014, CORR; Zhan RZ, 2021, AAAI CONF ARTIF INTE, V35, P14310; Zhang B., 2021, P NEURIPS, V34; Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4; Zhang DW, 2017, PROC CVPR IEEE, P5340, DOI 10.1109/CVPR.2017.567; Zhang DW, 2015, IEEE I CONF COMP VIS, P594, DOI 10.1109/ICCV.2015.75; Zhang JR, 2021, AAAI CONF ARTIF INTE, V35, P3351; Zhang S., 2020, ARXIV200911138; Zhang W, 2021, INT SYM PERFORM ANAL, P90, DOI 10.1109/ISPASS51385.2021.00025; Zhang X, 2015, ADV NEUR IN, V28; Zhang XL, 2013, INT CONF ACOUST SPEE, P853, DOI 10.1109/ICASSP.2013.6637769; Zhang Xuan, 2018, ARXIV181100739; Zhang Xuan, 2019, PROC 2019 C N AM CHA, V1, P1903; Zhang Y., 2020, P NEURIPS, V33; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zhao MJ, 2020, AAAI CONF ARTIF INTE, V34, P9652; Zhao Q, 2015, AAAI CONF ARTIF INTE, P3196; Zhao YY, 2021, AAAI CONF ARTIF INTE, V35, P14540; Zheng SQ, 2019, INTERSPEECH, P4360, DOI 10.21437/Interspeech.2019-1440; Zheng W, 2020, PATTERN RECOGN LETT, V132, P4, DOI 10.1016/j.patrec.2018.06.029; Zhenghua He, 2020, Neural Information Processing. 27th International Conference, ICONIP 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12533), P202, DOI 10.1007/978-3-030-63833-7_17; Zhou T., 2018, P ICLR; Zhou T., 2020, ADV NEURAL INF PROCE, V33; Zhou Yikai, 2020, ACL, P6934; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu X., 2020, ARXIV201004159	208	0	0	8	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1526	1565		10.1007/s11263-022-01611-x	http://dx.doi.org/10.1007/s11263-022-01611-x		APR 2022	40	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU		Green Submitted			2022-12-18	WOS:000783738500003
J	Xiao, TH; Liu, SF; De Mello, S; Yu, ZD; Kautz, J; Yang, MH				Xiao, Taihong; Liu, Sifei; De Mello, Shalini; Yu, Zhiding; Kautz, Jan; Yang, Ming-Hsuan			Learning Contrastive Representation for Semantic Correspondence (Apr, 10.1007/s11263-022-01602-y, 2022)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Correction									[Xiao, Taihong; Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95343 USA; [Liu, Sifei; De Mello, Shalini; Yu, Zhiding; Kautz, Jan] Nvidia, Santa Clara, CA USA; [Yang, Ming-Hsuan] Yonsei Univ, Seoul, South Korea	University of California System; University of California Merced; Nvidia Corporation; Yonsei University	Yang, MH (corresponding author), Univ Calif Merced, Merced, CA 95343 USA.; Yang, MH (corresponding author), Yonsei Univ, Seoul, South Korea.	txiao3@ucmerced.edu; sifeil@nvidia.com; shalinig@nvidia.com; zhidingy@nvidia.com; jkautz@nvidia.com; mhyang@ucmerced.edu						Xiao TH, 2022, INT J COMPUT VISION, V130, P1293, DOI 10.1007/s11263-022-01602-y	1	0	0	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1607	1607		10.1007/s11263-022-01614-8	http://dx.doi.org/10.1007/s11263-022-01614-8		APR 2022	1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU		Bronze			2022-12-18	WOS:000783447900001
J	Cosmo, L; Minello, G; Bronstein, M; Rodola, E; Rossi, L; Torsello, A				Cosmo, Luca; Minello, Giorgia; Bronstein, Michael; Rodola, Emanuele; Rossi, Luca; Torsello, Andrea			3D Shape Analysis Through a Quantum Lens: the Average Mixing Kernel Signature	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape representation; Shape analysis; Partial Matching; Quantum walks		The Average Mixing Kernel Signature is a novel spectral signature for points on non-rigid three-dimensional shapes. It is based on a quantum exploration process of the shape surface, where the average transition probabilities between the points of the shape are summarised in the finite-time average mixing kernel. A band-filtered spectral analysis of this kernel then yields the AMKS. Crucially, we show that opting for a finite time-evolution allows the signature to account for a mixing of the Laplacian eigenspaces, similar to what is observed in the presence of noise, explaining the increased noise robustness of this signature when compared to alternative signatures. We perform an extensive experimental analysis of the AMKS under a wide range of problem scenarios, evaluating the performance of our descriptor under different sources of noise (vertex jitter and topological), shape representations (mesh and point clouds), as well as when only a partial view of the shape is available. Our experiments show that the AMKS consistently outperforms two of the most widely used spectral signatures, the Heat Kernel Signature and the Wave Kernel Signature, and suggest that the AMKS should be the signature of choice for various compute vision problems, including as input of deep convolutional architectures for shape analysis.	[Cosmo, Luca; Rodola, Emanuele] Sapienza Univ Rome, Rome, Italy; [Cosmo, Luca; Bronstein, Michael] Univ Lugano, Lugano, Switzerland; [Cosmo, Luca; Minello, Giorgia; Torsello, Andrea] CaFoscari Univ Venice, Venice, Italy; [Bronstein, Michael] Twitter, London, England; [Bronstein, Michael] Imperial Coll London, London, England; [Rossi, Luca] Queen Mary Univ London, London, England	Sapienza University Rome; Universita della Svizzera Italiana; Universita Ca Foscari Venezia; Twitter, Inc.; Imperial College London; University of London; Queen Mary University London	Cosmo, L (corresponding author), Sapienza Univ Rome, Rome, Italy.; Cosmo, L (corresponding author), Univ Lugano, Lugano, Switzerland.; Cosmo, L (corresponding author), CaFoscari Univ Venice, Venice, Italy.	luca.rossi@qmul.ac.uk	Cosmo, Luca/AAT-4569-2020; Torsello, Andrea/K-6352-2016	Cosmo, Luca/0000-0001-7729-4666; Torsello, Andrea/0000-0001-9189-4924; MINELLO, GIORGIA/0000-0002-0923-7640				Aubry Mathieu, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P122, DOI 10.1007/978-3-642-23123-0_13; Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Bai L, 2015, PATTERN RECOGN, V48, P344, DOI 10.1016/j.patcog.2014.03.028; Bell J.J, 2014, SPEAKABLE UNSPEAKABL, P160; Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491; Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1; Clarenz Ulrich, 2004, P 1 EUR C POINT BAS, P201; Corman E, 2015, LECT NOTES COMPUT SC, V8928, P283, DOI 10.1007/978-3-319-16220-1_20; Cosmo Luca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P1, DOI 10.1007/978-3-030-58565-5_1; Cosmo L, 2017, COMPUT GRAPH FORUM, V36, P209, DOI 10.1111/cgf.12796; Cosmo L., 2016, P 3DOR, P61, DOI DOI 10.2312/3DOR.20161089; Cosmo L, 2016, INT CONF 3D VISION, P1, DOI 10.1109/3DV.2016.10; Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845; Gasparetto A, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P344, DOI 10.1109/3DV.2015.46; Godsil C, 2013, J COMB THEORY A, V120, P1649, DOI 10.1016/j.jcta.2013.05.006; Huang FC, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073654, 10.1145/3137609]; Kempe J, 2003, CONTEMP PHYS, V44, P307, DOI 10.1080/00107151031000110776; Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974; Lahner Z, 2016, P EUR WORKSH 3D OBJ, V2; Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148; Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112; Minello G, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21030328; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; MURTHY DV, 1988, INT J NUMER METH ENG, V26, P293, DOI 10.1002/nme.1620260202; Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526; Portugal R., 2013, QUANTUM WALKS SEARCH, DOI [10.1007/978-1-4614-6336-8, DOI 10.1007/978-1-4614-6336-8]; Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011; Rodola E, 2017, COMPUT GRAPH FORUM, V36, P222, DOI 10.1111/cgf.12797; Rodola E., 2017, P EUR WORKSH 3D OBJ, P23; Rodola E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532; ROSSI L, 2016, JOINT IAPR INT WORKS, P474; Rossi L, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.022815; Rossi L, 2013, PHYS REV E, V88, DOI 10.1103/PhysRevE.88.032806; Rossi L, 2012, LECT NOTES COMPUT SC, V7626, P144, DOI 10.1007/978-3-642-34166-3_16; Rostami R, 2019, COMPUT GRAPH FORUM, V38, P356, DOI 10.1111/cgf.13536; Rustamov Raif M, 2007, P 5 EUR S GEOM PROC, P225, DOI DOI 10.2312/SGP/SGP07/225-233; Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275; Vestner M, 2017, INT CONF 3D VISION, P517, DOI 10.1109/3DV.2017.00065	40	0	0	2	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1474	1493		10.1007/s11263-022-01610-y	http://dx.doi.org/10.1007/s11263-022-01610-y		APR 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU		hybrid			2022-12-18	WOS:000780709100001
J	Wei, XX; Yan, HQ; Li, B				Wei, Xingxing; Yan, Huanqian; Li, Bo			Sparse Black-Box Video Attack with Reinforcement Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Adversarial examples; Black-box video attack; Reinforcement learning; Sparse attack	DEEP; NETWORKS; GAME; GO	Adversarial attacks on video recognition models have been explored recently. However, most existing works treat each video frame equally and ignore their temporal interactions. To overcome this drawback, a few methods try to select some key frames and then perform attacks based on them. Unfortunately, their selection strategy is independent of the attacking step, therefore the resulting performance is limited. Instead, we argue the frame selection phase is closely relevant with the attacking phase. The key frames should be adjusted according to the attacking results. For that, we formulate the black-box video attacks into a Reinforcement Learning (RL) framework. Specifically, the environment in RL is set as the recognition model, and the agent in RL plays the role of frame selecting. By continuously querying the recognition models and receiving the attacking feedback, the agent gradually adjusts its frame selection strategy and adversarial perturbations become smaller and smaller. We conduct a series of experiments with two mainstream video recognition models: C3D and LRCN on the public UCF-101 and HMDB-51 datasets. The results demonstrate that the proposed method can significantly reduce the adversarial perturbations with efficient query times.	[Wei, Xingxing] Beihang Univ, Inst Artificial Intelligence, Hangzhou Innovat Inst, Beijing, Peoples R China; [Yan, Huanqian; Li, Bo] Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media DML, Beijing, Peoples R China; [Yan, Huanqian; Li, Bo] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China	Beihang University; Beihang University; Beihang University	Wei, XX (corresponding author), Beihang Univ, Inst Artificial Intelligence, Hangzhou Innovat Inst, Beijing, Peoples R China.	xxwei@buaa.edu.cn; yanhq@buaa.edu.cn; boli@buaa.edu.cn			National Key R&D Program of China [2020AAA0104002]; National Natural Science Foundation of China [62076018]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported by National Key R&D Program of China (Grant No.2020AAA0104002), National Natural Science Foundation of China (No.62076018). We also thank anonymous reviewers for their valuable suggestions.	Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385; Bose AJ, 2018, IEEE INT WORKSH MULT; Cheng M., 2020, INT C LEARN REPR; Cheng Minhao, 2019, ICLR; Croce F, 2020, INT J COMPUT VISION, V128, P1028, DOI 10.1007/s11263-019-01213-0; Das N, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P196, DOI 10.1145/3219819.3219910; Deng LX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P112, DOI 10.1145/3343031.3351147; Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174; Dong WK, 2019, AAAI CONF ARTIF INTE, P8247; Dong YP, 2019, PROC CVPR IEEE, P7706, DOI 10.1109/CVPR.2019.00790; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goswami G, 2019, INT J COMPUT VISION, V127, P719, DOI 10.1007/s11263-019-01160-w; Guo Chuan, 2017, ARXIV171100117; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ilyas A, 2018, PR MACH LEARN RES, V80; Jia, 2019, ARXIV PREPRINT ARXIV; Jia XJ, 2019, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2019.00624; Jiang LX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P864, DOI 10.1145/3343031.3351088; Kingma D.P, P 3 INT C LEARNING R; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Li S., 2019, NETWORK DISTRIBUTED; Li Yuxi, 2017, ARXIV170107274; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Liu S., 2019, INT C LEARN REPR; Lu, 2017, ARXIV COMPUTER VISIO; Madry Aleksander, 2017, ARXIV; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Nezami O. M., 2020, PICK OBJECT ATTACK T; Prakash A, 2018, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR.2018.00894; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Soomro K., 2012, COMPUT SCI; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Teng SZ, 2021, INT J COMPUT VISION, V129, P719, DOI 10.1007/s11263-020-01402-2; Tramer Florian, 2018, INT C LEARN REPR ICL; Wei XX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P954; Wei XX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P18, DOI 10.1145/3240508.3240708; Wei XX, 2019, AAAI CONF ARTIF INTE, P8973; Wei Z., 2020, NATL C ART INT; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xie Cihang, 2017, ARXIV171101991; Zhang HC, 2019, IEEE I CONF COMP VIS, P421, DOI 10.1109/ICCV.2019.00051; Zhou KY, 2018, AAAI CONF ARTIF INTE, P7582	50	0	0	4	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1459	1473		10.1007/s11263-022-01604-w	http://dx.doi.org/10.1007/s11263-022-01604-w		APR 2022	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU		Green Submitted			2022-12-18	WOS:000779026600001
J	Varol, G; Momeni, L; Albanie, S; Afouras, T; Zisserman, A				Varol, Gul; Momeni, Liliane; Albanie, Samuel; Afouras, Triantafyllos; Zisserman, Andrew			Scaling Up Sign Spotting Through Sign Language Dictionaries	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sign language recognition; Sign spotting; Few-shot learning	RECOGNITION	The focus of this work is sign spotting-given a video of an isolated sign, our task is to identify whether and where it has been signed in a continuous, co-articulated sign language video. To achieve this sign spotting task, we train a model using multiple types of available supervision by: (1) watching existing footage which is sparsely labelled using mouthing cues; (2) reading associated subtitles (readily available translations of the signed content) which provide additional weak-supervision; (3) looking up words (for which no co-articulated labelled examples are available) in visual sign language dictionaries to enable novel sign spotting. These three tasks are integrated into a unified learning framework using the principles of Noise Contrastive Estimation and Multiple Instance Learning. We validate the effectiveness of our approach on low-shot sign spotting benchmarks. In addition, we contribute a machine-readable British Sign Language (BSL) dictionary dataset of isolated signs, BslDict, to facilitate study of this task. The dataset, models and code are available at our project page.	[Varol, Gul; Momeni, Liliane; Albanie, Samuel; Afouras, Triantafyllos; Zisserman, Andrew] Univ Oxford, Visual Geometry Grp, Oxford, England; [Varol, Gul] Univ Gustave Eiffel, LIGM, CNRS, Ecole Ponts, Marne La Vallee, France; [Albanie, Samuel] Univ Cambridge, Dept Engn, Cambridge, England	University of Oxford; Centre National de la Recherche Scientifique (CNRS); Ecole des Ponts ParisTech; Universite Gustave-Eiffel; ESIEE Paris; University of Cambridge	Varol, G (corresponding author), Univ Oxford, Visual Geometry Grp, Oxford, England.; Varol, G (corresponding author), Univ Gustave Eiffel, LIGM, CNRS, Ecole Ponts, Marne La Vallee, France.	gul@robots.ox.ac.uk; liliane@robots.ox.ac.uk; albanie@robots.ox.ac.uk; afourast@robots.ox.ac.uk; az@robots.ox.ac.uk			EPSRC; Royal Society Research Professorship	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Royal Society Research Professorship	This work was supported by EPSRC grant ExTol and a Royal Society Research Professorship. The authors would to like thankAbhishek Dutta, A. SophiaKoepke, AndrewBrown, NecatiCihan Camgoz, Neil Fox, Joon Son Chung, BencieWoll, and Hannah Bull for their help. The authors are also grateful to Daniel Mitchell who made signbsl.com webpage available. SA would like to thank Z. Novak and S. Carlson for enabling his contribution.	Afouras T., 2018, ABS180900496 ARXIV; Albanie S., 2021, ARXIV PREPRINT ARXIV; Albanie Samuel, 2020, EUR C COMP VIS; Aldersson R., 2007, BIRKBECK STUDIES APP, V2; Athitsos V, 2008, PROC CVPR IEEE, P1666; Bank R, 2011, SIGN LANG LINGUIST, V14, P248, DOI 10.1075/sll.14.2.02ban; Bilge Y.C., 2019, P BRIT MACH VIS C BM; Buehler P, 2009, PROC CVPR IEEE, P2953; Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004; Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI [10.1109/ICCV.2017.332, 10.1109/ICCVW.2017.364]; Cao Kaidi, 2020, CVPR; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chai X., 2014, VIPLTR14SLR001 CAS; Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753; Chung J.S., 2016, WORKSH BRAV NEW IDEA; Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367; Cooper H, 2009, PROC CVPR IEEE, P2560; Coucke A, 2019, INT CONF ACOUST SPEE, P6351, DOI 10.1109/ICASSP.2019.8683474; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Dutta Abhishek, 2019, MM '19: Proceedings of the 27th ACM International Conference on Multimedia, P2276, DOI 10.1145/3343031.3350535; Farhadi A, 2007, PROC CVPR IEEE, P2676; Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P55, DOI 10.1007/978-3-030-01264-9_4; Fenlon J., 2017, ROUTLEDGE HDB PHONOL; Fenlon Jordan, 2014, BSL SIGNBANK LEXICAL; Fillbrandt H, 2003, IEEE INTERNATIONAL WORKSHOP ON ANALYSIS AND MODELING OF FACE AND GESTURES, P181; Forster J, 2013, LECT NOTES COMPUT SC, V7887, P89; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; He K., 2020, P IEEECVF C COMPUTER, P9729; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang JJ, 2018, AAAI CONF ARTIF INTE, P6951; Joze Hamid Vaezi, 2019, BRIT MACH VIS C BMVC; Kadir T., 2004, P BMVC; Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013; Li DX, 2020, PROC CVPR IEEE, P6204, DOI 10.1109/CVPR42600.2020.00624; Li DX, 2020, IEEE WINT CONF APPL, P1448, DOI 10.1109/WACV45572.2020.9093512; Liu SB, 2011, PROC CVPR IEEE, P913, DOI 10.1109/CVPR.2011.5995334; Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990; Momeni L., 2020, ACCV; Momeni Liliane, 2021, CVPR; Momeni Liliane, 2020, BMVC; Motiian S., 2017, ADV NEURAL INF PROCE, V30, P1; Ong EJ, 2014, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2014.248; Ong EJ, 2012, PROC CVPR IEEE, P2200, DOI 10.1109/CVPR.2012.6247928; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Pfister T, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.20; Pfister T, 2014, LECT NOTES COMPUT SC, V8694, P814, DOI 10.1007/978-3-319-10599-4_52; Schembri A, 2013, LANG DOC CONSERV, V7, P136; SignumMcKee, 2000, SIGNS LANGUAGE REVIS; Stafylakis T, 2018, LECT NOTES COMPUT SC, V11208, P536, DOI 10.1007/978-3-030-01225-0_32; Starner T.E., 1995, VISUAL RECOGNITION A; Sutton-Spence R., 1999, LINGUISTICS BRIT SIG, DOI [10.1017/CBO9781139167048, DOI 10.1017/CBO9781139167048]; Sutton-Spence R., 2007, SIMULTANEITY SIGNED, P147, DOI DOI 10.1075/CILT.281.07SUT; TAMURA S, 1988, PATTERN RECOGN, V21, P343, DOI 10.1016/0031-3203(88)90048-9; van den Oord Aaron, 2018, ARXIV180703748; Veniat T, 2019, INT CONF ACOUST SPEE, P2842, DOI 10.1109/ICASSP.2019.8683305; Viitaniemi V, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P1892; von Agris Ulrich, 2008, Universal Access in the Information Society, V6, P323, DOI 10.1007/s10209-007-0104-x; von Agris U, 2008, IEEE INT CONF AUTOMA, P286; Wilbur R, 2006, TR0612 PURD U SCH EL, P167; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Yang HT, 2018, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2018.00157; Ye YC, 2018, IEEE COMPUT SOC CONF, P2145, DOI 10.1109/CVPRW.2018.00280; Zhang JY, 2019, IEEE INT CONF COMP V, P9, DOI 10.1109/ICCVW.2019.00008; Zhou H., 2020, ABS200203187 CORR	67	0	0	6	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2022	130	6					1416	1439		10.1007/s11263-022-01589-6	http://dx.doi.org/10.1007/s11263-022-01589-6		APR 2022	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	1N7TU		Green Submitted, Green Published, hybrid			2022-12-18	WOS:000779026700001
J	Shen, X; Champenois, R; Ginosar, S; Pastrolin, I; Rousselot, M; Bounou, O; Monnier, T; Gidaris, S; Bougard, F; Raverdy, PG; Limon, MF; Benevent, C; Smith, M; Poncet, O; Bender, K; Joyeux-Prunel, B; Honig, E; Efros, AA; Aubry, M				Shen, Xi; Champenois, Robin; Ginosar, Shiry; Pastrolin, Ilaria; Rousselot, Morgane; Bounou, Oumayma; Monnier, Tom; Gidaris, Spyros; Bougard, Francois; Raverdy, Pierre-Guillaume; Limon, Marie-Francoise; Benevent, Christine; Smith, Marc; Poncet, Olivier; Bender, K.; Joyeux-Prunel, Beatrice; Honig, Elizabeth; Efros, Alexei A.; Aubry, Mathieu			Spatially-Consistent Feature Matching and Learning for Heritage Image Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Feature learning; Self-supervised learning; Artwork analysis; Watermark recognition		Progress in the digitization of cultural assets leads to online databases that become too large for a human to analyze. Moreover, some analyses might be challenging, even for experts. In this paper, we explore two applications of computer vision to analyze historical data: watermark recognition and one-shot repeated pattern detection in artwork collections. Both problems present computer vision challenges which we believe to be representative of the ones encountered in cultural heritage applications: limited supervision is available, the tasks are fine-grained recognition, and the data comes in several different modalities. Both applications are also highly practical, as recognizing watermarks makes it possible to date and locate documents, while detecting repeated patterns allows exploring visual links between artworks. We demonstrate on both tasks the benefits of relying on deep mid-level features. More precisely, we define an image similarity score based on geometric verification of mid-level features and show how spatial consistency can be used to fine-tune out-of-the-box features for the target dataset with weak or no supervision. This paper relates and extends our previous works (Shen et al. in Discovering visual patterns in art collections with spatially-consistent feature learning, 2019; Shen et al. in Large-scale historical watermark recognition dataset and a new consistency-based approach, 2020). Our code and data are available at http://imagine.enpc.fr/similar to shenx/HisImgAnalysis/.	[Shen, Xi; Champenois, Robin; Monnier, Tom; Aubry, Mathieu] Univ Gustave Eiffel, UMR 8049, CNRS, Ecole Ponts,LIGM, Marne La Vallee, France; [Ginosar, Shiry; Honig, Elizabeth; Efros, Alexei A.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Pastrolin, Ilaria; Rousselot, Morgane; Benevent, Christine; Smith, Marc; Poncet, Olivier] Ecole Natl Chartes, Paris, France; [Limon, Marie-Francoise] Arch Natl, Paris, France; [Joyeux-Prunel, Beatrice] Univ Geneva, Geneva, Switzerland; [Bougard, Francois] IRHT, Paris, France; [Bounou, Oumayma; Raverdy, Pierre-Guillaume] INRIA, Paris, France; [Gidaris, Spyros] Valeo AI, Paris, France	Universite Gustave-Eiffel; ESIEE Paris; Centre National de la Recherche Scientifique (CNRS); Ecole des Ponts ParisTech; University of California System; University of California Berkeley; UDICE-French Research Universities; PSL Research University Paris; Ecole Nationale des Chartes; University of Geneva; Inria	Shen, X (corresponding author), Univ Gustave Eiffel, UMR 8049, CNRS, Ecole Ponts,LIGM, Marne La Vallee, France.	Xi.Shen@enpc.fr		SHEN, Xi/0000-0001-8043-9117; Efros, Alexei A./0000-0001-5720-8070				[Anonymous], BRUEGH FAM JAN BRUEG; Aubry M., 2014, ACM T GRAPHIC; Belongie S, 2001, ADV NEUR IN, V13, P831; Bender K., 2015, INT J DIGITAL ART HI; Bounou O., 2020, J DATA MINING DIGITA; Brendel W., 2019, APPROXIMATING CNNS B, DOI 1904.00760.; Briquet C. M., 1907, LES FILIGRANES; Crowley E. J., 2015, BMVC; Crowley EJ, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.39; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C., 2013, NEURIPS; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Doersch C, 2014, LECT NOTES COMPUT SC, V8691, P362, DOI 10.1007/978-3-319-10578-9_24; Dutta A., 2016, VGG IMAGE ANNOTATOR; Elezi Ismail, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P277, DOI 10.1007/978-3-030-58571-6_17; Frauenknecht E., 2015, KODIKOLOGIE PALAOGRA, P3; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Geirhos R., 2019, P INT C LEARNING REP, P1; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Ginosar S, 2015, LECT NOTES COMPUT SC, V8925, P101, DOI 10.1007/978-3-319-16178-5_7; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gonthier N., 2018, WEAKLY SUPERVISED OB; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hiary H., 2007, INT J DIGIT LIBRARIE; Hiary H., 2008, THESIS; Honig E., 2016, JAN BRUEGHEL SENSES; Jabri Allan, 2020, NEURIPS; Jenicek T., 2019, ICDAR; Karayev Sergey, 2014, P BRIT MACH VIS C, DOI [10.5244/c.28.122, DOI 10.5244/C.28.122]; Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330; Kingma D.P, P 3 INT C LEARNING R; Loing V, 2018, INT J COMPUT VISION, V126, P1045, DOI 10.1007/s11263-018-1102-6; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Massa F, 2016, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2016.648; Matin A, 2017, 2017 INTERNATIONAL CONFERENCE ON ELECTRICAL, COMPUTER AND COMMUNICATION ENGINEERING (ECCE), P1, DOI 10.1109/ECACE.2017.7912868; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Paumard MM, 2018, IEEE IMAGE PROC, P1018, DOI 10.1109/ICIP.2018.8451094; Picard D., 2015, IEEE SIGNAL PROC MAG; Picard D, 2016, CONF REC ASILOMAR C, P130, DOI 10.1109/ACSSC.2016.7869009; Piccard G., 1977, WASSERZEICHENKARTEI; Pondenkandath V., 2018, IDENTIFYING CROSS DE; Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610; Rad M, 2018, PROC CVPR IEEE, P4663, DOI 10.1109/CVPR.2018.00490; Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566; Rauber C., 1997, ELECT VISUALISATION; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rocco I, 2018, ADV NEUR IN, V31; Said J., 2016, MULTIMED TOOLS APPL; Seguin B., 2017, DH; Shen X., 2020, ICPR; Shen Xi, 2020, ECCV; Shrivastava A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024188; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Snell J, 2017, ADV NEUR IN, V30; Strezoski Gjorgji, 2017, ABS170800684 CORR; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sun BC, 2016, AAAI CONF ARTIF INTE, P2058; Tan WR, 2016, IEEE IMAGE PROC, P3703, DOI 10.1109/ICIP.2016.7533051; Teh Eu Wern, 2020, EUROPEAN C COMPUTER; Ubeda I, 2019, PROCEEDINGS OF THE 2019 WORKSHOP ON HISTORICAL DOCUMENT IMAGING AND PROCESSING (HIP' 19), P60, DOI 10.1145/3352631.3352645; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wallraven C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P257; Wang XG, 2019, PROC CVPR IEEE, P8868, DOI [10.1109/CVPR.2019.00908, 10.1109/CVPR.2019.00267]; Westlake N., 2016, ECCV; Wilber MJ, 2017, IEEE I CONF COMP VIS, P1211, DOI 10.1109/ICCV.2017.136; Yin R, 2016, INT CONF ACOUST SPEE, P2299, DOI 10.1109/ICASSP.2016.7472087; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	78	0	0	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1325	1339		10.1007/s11263-022-01576-x	http://dx.doi.org/10.1007/s11263-022-01576-x		MAR 2022	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		Green Submitted			2022-12-18	WOS:000780350300001
J	Teotia, D; Lapedriza, A; Ostadabbas, S				Teotia, Divyang; Lapedriza, Agata; Ostadabbas, Sarah			Interpreting Face Inference Models Using Hierarchical Network Dissection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Bias discovery; Face-based inference models; Face dictionary; Model interpretability; Network dissection		This paper presents Hierarchical Network Dissection, a general pipeline to interpret the internal representation of face-centric inference models. Using a probabilistic formulation, our pipeline pairs units of the model with concepts in our "Face Dictionary", a collection of facial concepts with corresponding sample images. Our pipeline is inspired by Network Dissection, a popular interpretability model for object-centric and scene-centric models. However, our formulation allows to deal with two important challenges of face-centric models that Network Dissection cannot address: (1) spacial overlap of concepts: there are different facial concepts that simultaneously occur in the same region of the image, like "nose" (facial part) and "pointy nose" (facial attribute); and (2) global concepts: there are units with affinity to concepts that do not refer to specific locations of the face (e.g. apparent age). We use Hierarchical Network Dissection to dissect different face-centric inference models trained on widely-used facial datasets. The results show models trained for different tasks learned different internal representations. Furthermore, the interpretability results can reveal some biases in the training data and some interesting characteristics of the face-centric inference tasks. Finally, we conduct controlled experiments on biased data to showcase the potential of Hierarchical Network Dissection for bias discovery. The results illustrate how Hierarchical Network Dissection can be used to discover and quantify bias in the training data that is also encoded in the model.	[Teotia, Divyang; Ostadabbas, Sarah] Northeastern Univ, Dept Elect & Comp Engn, Augmented Cognit Lab, Boston, MA 02115 USA; [Lapedriza, Agata] Univ Oberta Catalunya, Barcelona, Spain	Northeastern University; UOC Universitat Oberta de Catalunya	Ostadabbas, S (corresponding author), Northeastern Univ, Dept Elect & Comp Engn, Augmented Cognit Lab, Boston, MA 02115 USA.	ostadabbas@ece.neu.edu		Lapedriza, Agata/0000-0002-5248-0443	Spanish Ministry of Science, Innovation and Universities [RTI2018-095232-B-C22]	Spanish Ministry of Science, Innovation and Universities(Spanish Government)	This work was partially supported by the Spanish Ministry of Science, Innovation and Universities, RTI2018-095232-B-C22.	Nguyen A, 2016, ADV NEUR IN, V29; Bahng H., 2020, ARXIV191002806; Arrieta AB, 2020, INFORM FUSION, V58, P82, DOI 10.1016/j.inffus.2019.12.012; Bau D, 2020, P NATL ACAD SCI USA, V117, P30071, DOI 10.1073/pnas.1907375117; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Bau David, 2019, INT C LEARN REPR ICL; Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; Buolamwini JA, 2017, THESIS MIT; Cao Q., 2017, VGGFACE2 DATASET REC; Clark C., 2019, DONT TAKE EASY WAY O; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646; Gunning D., 2017, EXPLAINABLE ARTIFICI, V2, P1; Huang Gary B., 2007, 0749 U MASS, P7; Karkkainen K., 2019, ARXIV190804913 CORR; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Liang LY, 2018, INT C PATT RECOG, P1598, DOI 10.1109/ICPR.2018.8546038; Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212; Liu XD, 2019, IEEE COMPUT SOC CONF, P246, DOI 10.1109/CVPRW.2019.00034; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; RichardWebster B, 2018, LECT NOTES COMPUT SC, V11219, P263, DOI 10.1007/978-3-030-01267-0_16; Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sharma P, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901319; Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Tartaglione E., 2021, ARXIV210302023; Tatarunaite E, 2005, AM J ORTHOD DENTOFAC, V127, P676, DOI 10.1016/j.ajodo.2004.01.029; Wang A., 2020, EUR C PATT REC ECCV; Williford J. R., 2020, EUR C COMP; Xie QW, 2013, ASIAN J COMMUN, V23, P538, DOI 10.1080/01292986.2012.756046; Yin BJ, 2019, IEEE I CONF COMP VIS, P9347, DOI 10.1109/ICCV.2019.00944; Zee T, 2019, IEEE INT CONF COMP V, P514, DOI 10.1109/ICCVW.2019.00064; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463; ZhenWang Guosheng Hu, 2020, C COMP VIS PATT REC; Zhou BL, 2019, IEEE T PATTERN ANAL, V41, P2131, DOI 10.1109/TPAMI.2018.2858759; Zhou Bolei, 2015, OBJECT DETECTORS EME, P2	41	0	0	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1277	1292		10.1007/s11263-022-01603-x	http://dx.doi.org/10.1007/s11263-022-01603-x		MAR 2022	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		Green Submitted			2022-12-18	WOS:000772547700001
J	Ma, F; Zhu, LC; Yang, Y				Ma, Fan; Zhu, Linchao; Yang, Yi			Weakly Supervised Moment Localization with Decoupled Consistent Concept Prediction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video moment localization; Language query; Weakly supervised learning		Localizing moments in a video via natural language queries is a challenging task where models are trained to identify the start and the end timestamps of the moment in a video. However, it is labor intensive to obtain the temporal endpoint annotations. In this paper, we focus on a weakly supervised setting, where the temporal endpoints of moments are not available during training. We develop a decoupled consistent concept prediction (DCCP) framework to learn the relations between videos and query texts. Specifically, the atomic objects and actions are decoupled from the query text to facilitate the recognition of these concepts in videos. We introduce a concept pairing module to temporally localize the objects and actions in the video. The classification loss and the concept consistency loss are proposed to leverage the mutual benefits of object and action cues for building relations between languages and videos. Extensive experiments on DiDeMo, Charades-STA, and ActivityNet Captions demonstrate the effectiveness of our model.	[Ma, Fan; Zhu, Linchao] Univ Technol Sydney, Sydney, NSW, Australia; [Yang, Yi] Zhejiang Univ, Hangzhou, Peoples R China	University of Technology Sydney; Zhejiang University	Yang, Y (corresponding author), Zhejiang Univ, Hangzhou, Peoples R China.	fan.ma@student.uts.edu.au; zhulinchao7@gmail.com; yangyics@zju.edu.cn	yang, yang/HGT-7999-2022; Yang, Yi/B-9273-2017	Yang, Yi/0000-0002-0512-880X	ARC [DP200100938]	ARC(Australian Research Council)	This research was partially supported by ARC DP200100938.	Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/2951913.2976746, 10.1145/3022670.2976746]; Bird S., 2006, P ACL WORKSH EFF TOO, P69; Cer D, 2018, CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P169; Chen Jingyuan, 2018, P C EMP METH NAT LAN, P162; Chen K, 2018, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2018.00425; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chen SX, 2019, AAAI CONF ARTIF INTE, P8199; Datta S, 2019, IEEE I CONF COMP VIS, P2601, DOI 10.1109/ICCV.2019.00269; Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Escorcia Victor, 2019, ARXIV190712763; Gao J., 2017, ARXIV PREPRINT ARXIV; Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5; Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392; Gao MF, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1481; Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032; Girdhar R, 2017, ADV NEUR IN, V30; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; Hendricks L. A., 2018, PROC C EMPIRICAL MET, P1380; Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618; Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645; Javed, 2018, ARXIV PREPRINT ARXIV; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1; Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539; Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326; Minuk Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P156, DOI 10.1007/978-3-030-58604-1_10; Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186; Ning, 2018, ARXIV PREPRINT ARXIV; Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49; Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10; Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Simonyan K, 2014, ADV NEUR IN, V27; Vaswani A, 2017, ADV NEUR IN, V30; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang XK, 2021, IEEE T IND INFORM, V17, P2231, DOI [10.1109/TII.2020.2999901, 10.1109/TPAMI.2020.3015894]; Wang Y., 2021, FINDINGS ASS COMPUTA, P89; Wang Y., 2021, IEEE T MULTIMEDIA; Xiao FY, 2017, PROC CVPR IEEE, P5253, DOI 10.1109/CVPR.2017.558; Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yan Y, 2020, INT J COMPUT VISION, V128, P1414, DOI 10.1007/s11263-019-01244-7; Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463; Yeh RA, 2018, PROC CVPR IEEE, P6125, DOI 10.1109/CVPR.2018.00641; Yuankai Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9979, DOI 10.1109/CVPR42600.2020.01000; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23; Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134; Zhao H., 2017, ARXIV170509684; Zhu LG, 2019, IRONMAK STEELMAK, V46, P499, DOI 10.1080/03019233.2017.1405153; Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503	56	0	0	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1244	1258		10.1007/s11263-022-01600-0	http://dx.doi.org/10.1007/s11263-022-01600-0		MAR 2022	15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA					2022-12-18	WOS:000770721000001
J	Song, R; Zhang, W; Zhao, YT; Liu, YH				Song, Ran; Zhang, Wei; Zhao, Yitian; Liu, Yonghuai			Unsupervised Multi-View CNN for Salient View Selection and 3D Interest Point Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised 3D deep learning; Multi-view CNN; View-object consistency; View selection; 3D interest point detection	SHAPE; IMAGE	We present an unsupervised 3D deep learning framework based on a ubiquitously true proposition named by us view-object consistency as it states that a 3D object and its projected 2D views always belong to the same object class. To validate its effectiveness, we design a multi-view CNN instantiating it for salient view selection and interest point detection of 3D objects, which quintessentially cannot be handled by supervised learning due to the difficulty of collecting sufficient and consistent training data. Our unsupervised multi-view CNN, namely UMVCNN, branches off two channels which encode the knowledge within each 2D view and the 3D object respectively and also exploits both intra-view and inter-view knowledge of the object. It ends with a new loss layer which formulates the view-object consistency by impelling the two channels to generate consistent classification outcomes. The UMVCNN is then integrated with a global distinction adjustment scheme to incorporate global cues into salient view selection. We evaluate our method for salient view section both qualitatively and quantitatively, demonstrating its superiority over several state-of-the-art methods. In addition, we showcase that our method can be used to select salient views of 3D scenes containing multiple objects. We also develop a method based on the UMVCNN for 3D interest point detection and conduct comparative evaluations on a publicly available benchmark, which shows that the UMVCNN is amenable to different 3D shape understanding tasks.	[Song, Ran; Zhang, Wei] Shandong Univ, Sch Control Sci & Engn, Jinan, Peoples R China; [Song, Ran; Zhang, Wei] Shandong Univ, Inst Brain & Brain Inspired Sci, Jinan, Peoples R China; [Zhao, Yitian] Chinese Acad Sci, Cixi Inst Biomed Engn, Ningbo Inst Mat Technol & Engn, Ningbo, Peoples R China; [Liu, Yonghuai] Edge Hill Univ, Dept Comp Sci, Ormskirk, England	Shandong University; Shandong University; Chinese Academy of Sciences; Ningbo Institute of Materials Technology and Engineering, CAS; Edge Hill University	Zhang, W (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan, Peoples R China.; Zhang, W (corresponding author), Shandong Univ, Inst Brain & Brain Inspired Sci, Jinan, Peoples R China.	ransong@sdu.edu.cn; davidzhang@sdu.edu.cn; yitian.zhao@nimte.ac.cn; liuyo@edgehill.ac.uk			National Natural Science Foundation of China [62076148, 61991411, U1913204]; Young Taishan Scholars Program of Shandong Province [No.tsqn201909029]; Qilu Young Scholars Program of Shandong University	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Young Taishan Scholars Program of Shandong Province; Qilu Young Scholars Program of Shandong University	This work was supported by the National Natural Science Foundation of China under Grants 62076148, 61991411 and U1913204, the Young Taishan Scholars Program of Shandong Province No.tsqn201909029 and the Qilu Young Scholars Program of Shandong University.	Bai XY, 2020, PROC CVPR IEEE, P6358, DOI 10.1109/CVPR42600.2020.00639; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Blanz V, 1999, PERCEPTION, V28, P575, DOI 10.1068/p2897; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x; Chen XB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185525; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; CUTZU F, 1994, VISION RES, V34, P3037, DOI 10.1016/0042-6989(94)90277-1; Defferrard M, 2016, ADV NEUR IN, V29; Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028; Dutagaci H., 2010, ACM WORKSHOP, P45, DOI [10.1145/1877808.1877819, DOI 10.1145/1877808.1877819]; Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4; Freitag S, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P355; Giorgi D., 2007, SHREC COMPETITION, V8; Guerin J, 2018, IEEE INT C INT ROBOT, P1061, DOI 10.1109/IROS.2018.8593524; Han HL, 2014, I C VIRTUAL REALITY, P214, DOI 10.1109/ICVRV.2014.12; Hayward WG, 1998, J EXP PSYCHOL HUMAN, V24, P427, DOI 10.1037/0096-1523.24.2.427; He J., 2018, IEEE T VIS COMPUT GR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Huang FC, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073654, 10.1145/3137609]; Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702; Kim SH, 2017, COMPUT GRAPH FORUM, V36, P313, DOI 10.1111/cgf.13082; Koch C, 1999, NAT NEUROSCI, V2, P9, DOI 10.1038/4511; Kostrikov I, 2018, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR.2018.00269; Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244; Leifman G, 2016, IEEE T PATTERN ANAL, V38, P2544, DOI 10.1109/TPAMI.2016.2522437; Li JX, 2019, IEEE I CONF COMP VIS, P361, DOI 10.1109/ICCV.2019.00045; Li XL, 2018, SPRINGERBRIEF MATH, P1, DOI 10.1007/978-3-319-89617-5_1; Lienhard S, 2014, COMPUT GRAPH FORUM, V33, P361, DOI 10.1111/cgf.12317; Mezuman E., 2012, P NEURIPS, P719; Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z; Newman ME, 2008, MATH NETWORKS NEW PA; Novotny D, 2017, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2017.558; Page DL, 2003, IEEE IMAGE PROC, P229; Perron O, 1907, MATH ANN, V64, P248, DOI 10.1007/BF01449896; Polonsky O, 2005, VISUAL COMPUT, V21, P840, DOI 10.1007/s00371-005-0326-y; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Ran Song, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P454, DOI 10.1007/978-3-030-58529-7_27; Secord A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019628; Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504; Shilane P, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P108; Song R, 2020, IEEE T VIS COMPUT GR, V26, P2204, DOI 10.1109/TVCG.2018.2885750; Song R, 2018, VISUAL COMPUT, V34, P323, DOI 10.1007/s00371-016-1334-9; Song R, 2013, VISUAL COMPUT, V29, P695, DOI 10.1007/s00371-013-0806-4; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; TARR MJ, 1989, COGNITIVE PSYCHOL, V21, P233, DOI 10.1016/0010-0285(89)90009-1; Vazquez P.-P., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P273; Vieira T, 2009, COMPUT GRAPH FORUM, V28, P717, DOI 10.1111/j.1467-8659.2009.01412.x; WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Yamauchi H, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P265; Yew ZJ, 2018, LECT NOTES COMPUT SC, V11219, P630, DOI 10.1007/978-3-030-01267-0_37; Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748; Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29; Zhao L, 2015, VISUAL COMPUT, V31, P765, DOI 10.1007/s00371-015-1091-1; Zhao SH, 2016, VISUAL COMPUT, V32, P429, DOI 10.1007/s00371-015-1069-z; Zhu KY, 2020, IEEE INT C INT ROBOT, P5879, DOI 10.1109/IROS45743.2020.9341304	58	0	0	6	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1210	1227		10.1007/s11263-022-01592-x	http://dx.doi.org/10.1007/s11263-022-01592-x		MAR 2022	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	0V4WA					2022-12-18	WOS:000769832900001
J	Zhong, YR; Loop, C; Byeon, W; Birchfield, S; Dai, YC; Zhang, KH; Kamenev, A; Breuel, T; Li, HD; Kautz, J				Zhong, Yiran; Loop, Charles; Byeon, Wonmin; Birchfield, Stan; Dai, Yuchao; Zhang, Kaihao; Kamenev, Alexey; Breuel, Thomas; Li, Hongdong; Kautz, Jan			Displacement-Invariant Cost Computation for Stereo Matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo matching; Feature volume; Autonomous driving; Displacement-invariant cost computation		Although deep learning-based methods have dominated stereo matching leaderboards by yielding unprecedented disparity accuracy, their inference time is typically slow, i.e., less than 4 FPS for a pair of 540p images. The main reason is that the leading methods employ time-consuming 3D convolutions applied to a 4D feature volume. A common way to speed up the computation is to downsample the feature volume, but this loses high-frequency details. To overcome these challenges, we propose a displacement-invariant cost computation module to compute the matching costs without needing a 4D feature volume. Rather, costs are computed by applying the same 2D convolution network on each disparity-shifted feature map pair independently. Unlike previous 2D convolution-based methods that simply perform context mapping between inputs and disparity maps, our proposed approach learns to match features between the two images. We also propose an entropy-based refinement strategy to refine the computed disparity map, which further improves the speed by avoiding the need to compute a second disparity map on the right image. Extensive experiments on standard datasets (SceneFlow, KITTI, ETH3D, and Middlebury) demonstrate that our method achieves competitive accuracy with much less inference time. On typical image sizes (e.g., 540x960), our method processes over 100 FPS on a desktop GPU, making our method suitable for time-critical applications such as autonomous driving. We also show that our approach generalizes well to unseen datasets, outperforming 4D-volumetric methods. We will release the source code to ensure the reproducibility.	[Zhong, Yiran; Loop, Charles; Byeon, Wonmin; Birchfield, Stan; Kamenev, Alexey; Breuel, Thomas; Kautz, Jan] NVIDIA, Redmond, WA 98052 USA; [Dai, Yuchao] Northwestern Polytech Univ, Xian, Peoples R China; [Zhang, Kaihao; Li, Hongdong] Australia Natl Univ, Canberra, ACT, Australia	Northwestern Polytechnical University; Australian National University	Zhong, YR (corresponding author), NVIDIA, Redmond, WA 98052 USA.	zhongyiran@gmail.com	Zhang, Kaihao/HGC-0368-2022		CAUL	CAUL	Open Access funding enabled and organized by CAUL and its Member Institutions.	Badki A., 2021, CVPR; Badki A, 2020, PROC CVPR IEEE, P1597, DOI 10.1109/CVPR42600.2020.00167; Cai, 2020, 3DV; Chabra R, 2019, PROC CVPR IEEE, P11778, DOI 10.1109/CVPR.2019.01206; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Cheng X., 2020, ADV NEURAL INFORM PR, V33, P22158; Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448; Feihu Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P420, DOI 10.1007/978-3-030-58536-5_25; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Henkel, 1997, ICANN; Hernandez-Juarez D, 2016, PROCEDIA COMPUT SCI, V80, P143, DOI 10.1016/j.procs.2016.05.305; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Ilg E, 2018, LECT NOTES COMPUT SC, V11216, P626, DOI 10.1007/978-3-030-01258-8_38; Julesz B., 1971, FDN CYCLOPEAN PERCEP; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Khamis S., 2018, P EUR C COMP VIS ECC, P573; Kim S, 2019, PROC CVPR IEEE, P205, DOI 10.1109/CVPR.2019.00029; Kuzmin A., 2017, MLSP; Lee H, 2019, IEEE IMAGE PROC, P4280, DOI 10.1109/ICIP.2019.8803514; Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Menze Moritz, 2015, CVPR; Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108; Poggi M, 2020, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR42600.2020.00329; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Scharstein D, 1998, INT J COMPUT VISION, V28, P155, DOI 10.1023/A:1008015117424; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703; Smolyanskiy N., 2018, CVPRW; Taniai T, 2018, IEEE T PATTERN ANAL, V40, P2725, DOI 10.1109/TPAMI.2017.2766072; Teed Zachary, 2020, ECCV, DOI DOI 10.1007/978-3-030-58536-5_24; Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028; Weber, 2009, ICCVW; Yang GS, 2019, PROC CVPR IEEE, P5510, DOI 10.1109/CVPR.2019.00566; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620; Zbontar J, 2016, J MACH LEARN RES, V17; Zhang C, 2015, IEEE I CONF COMP VIS, P2057, DOI 10.1109/ICCV.2015.238; Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027; Zhong, 2018, ECCV; Zhong Yiran, 2017, ARXIV170900930; Zhou HZ, 2018, LECT NOTES COMPUT SC, V11220, P851, DOI 10.1007/978-3-030-01270-0_50	45	0	0	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1196	1209		10.1007/s11263-022-01595-8	http://dx.doi.org/10.1007/s11263-022-01595-8		MAR 2022	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		hybrid			2022-12-18	WOS:000769832900002
J	Wang, YD; Tan, DJ; Navab, N; Tombari, F				Wang, Yida; Tan, David Joseph; Navab, Nassir; Tombari, Federico			SoftPool plus plus : An Encoder-Decoder Network for Point Cloud Completion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Point cloud; Completion; SoftPool; Skip-connection		We propose a novel convolutional operator for the task of point cloud completion. One striking characteristic of our approach is that, conversely to related work it does not require any max-pooling or voxelization operation. Instead, the proposed operator used to learn the point cloud embedding in the encoder extracts permutation-invariant features from the point cloud via a soft-pooling of feature activations, which are able to preserve fine-grained geometric details. These features are then passed on to a decoder architecture. Due to the compression in the encoder, a typical limitation of this type of architectures is that they tend to lose parts of the input shape structure. We propose to overcome this limitation by using skip connections specifically devised for point clouds, where links between corresponding layers in the encoder and the decoder are established. As part of these connections, we introduce a transformation matrix that projects the features from the encoder to the decoder and vice-versa. The quantitative and qualitative results on the task of object completion from partial scans on the ShapeNet dataset show that incorporating our approach achieves state-of-the-art performance in shape completion both at low and high resolutions.	[Wang, Yida; Navab, Nassir; Tombari, Federico] Tech Univ Munich, Munich, Germany; [Tan, David Joseph; Tombari, Federico] Google, Zurich, Switzerland	Technical University of Munich; Google Incorporated	Wang, YD (corresponding author), Tech Univ Munich, Munich, Germany.	yida.wang@tum.de; djtan@google.com; nassir.navab@tum.de; tombari@google.com	Wang, Yida/HHR-8799-2022	Wang, Yida/0000-0003-4519-9108	Projekt DEAL	Projekt DEAL	Open Access funding enabled and organized by Projekt DEAL.	Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733; Azad R, 2019, IEEE INT CONF COMP V, P406, DOI 10.1109/ICCVW.2019.00052; Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53; Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284; Chibane J, 2020, PROC CVPR IEEE, P6968, DOI 10.1109/CVPR42600.2020.00700; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Fengting Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13961, DOI 10.1109/CVPR42600.2020.01398; Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397; Gong B., 2021, P IEEE CVF INT C COM, P12488; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Han ZZ, 2019, AAAI CONF ARTIF INTE, P8376; Huan Lei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11608, DOI 10.1109/CVPR42600.2020.01163; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kirillov Alexander, 2020, CVPR; Li PH, 2013, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2013.212; Li YY, 2018, ADV NEUR IN, V31; Lim I, 2019, COMPUT GRAPH FORUM, V38, P99, DOI 10.1111/cgf.13792; Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187; Liu MH, 2020, AAAI CONF ARTIF INTE, V34, P11596; Mao JG, 2019, IEEE I CONF COMP VIS, P1578, DOI 10.1109/ICCV.2019.00166; Mazaheri G., 2019, IEEE C COMP VIS PATT, P119; Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Pan L, 2021, PROC CVPR IEEE, P8520, DOI 10.1109/CVPR46437.2021.00842; Pan L, 2020, IEEE ROBOT AUTOM LET, V5, P4392, DOI 10.1109/LRA.2020.2994483; Park J, 2017, IEEE I CONF COMP VIS, P143, DOI 10.1109/ICCV.2017.25; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Ronneberger O, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49; Sauder Jonathan, 2019, ARXIV190108396; Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20; Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478; Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178; Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396; Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352; Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047; Wang H., P IEEE CVF INT C COM, P9782; Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054; Wang X., 2021, PROC IEEE INT C COMP, P13189; Wang Y., COMPUTER VISION ECCV, P70; Wang YD, 2019, IEEE I CONF COMP VIS, P8607, DOI 10.1109/ICCV.2019.00870; Wen X., 2020, ARXIV PREPRINT ARXIV; Wen X, 2021, PROC CVPR IEEE, P13075, DOI 10.1109/CVPR46437.2021.01288; Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545; Xie H., 2020, ECCV, P365; Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278; Xie HZ, 2020, INT J COMPUT VISION, V128, P2919, DOI 10.1007/s11263-020-01347-6; Xinjiang Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13356, DOI 10.1109/CVPR42600.2020.01337; Xun Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13703, DOI 10.1109/CVPR42600.2020.01372; Yang B, 2020, INT J COMPUT VISION, V128, P53, DOI 10.1007/s11263-019-01217-w; Yang B, 2017, IEEE INT CONF COMP V, P679, DOI 10.1109/ICCVW.2017.86; Yang B, 2019, IEEE T PATTERN ANAL, V41, P2820, DOI 10.1109/TPAMI.2018.2868195; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204; Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227; Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088; Zhang JQ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2831, DOI 10.1145/3394486.3403334; Zhang Wenrui, 2020, ARXIV200210085, P2; Zhou Liming, 2021, ARXIV PREPRINT ARXIV; Zitian Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7659, DOI 10.1109/CVPR42600.2020.00768	66	0	0	5	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1145	1164		10.1007/s11263-022-01588-7	http://dx.doi.org/10.1007/s11263-022-01588-7		MAR 2022	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		Green Submitted, hybrid			2022-12-18	WOS:000767739800001
