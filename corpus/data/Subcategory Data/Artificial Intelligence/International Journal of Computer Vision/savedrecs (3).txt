PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
J	Johansen, P; Ersboll, B				Johansen, P; Ersboll, B			Untitled - Introduction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Univ Copenhagen, DK-1168 Copenhagen, Denmark; Tech Univ Denmark, DK-2800 Lyngby, Denmark	University of Copenhagen; Technical University of Denmark	Johansen, P (corresponding author), Univ Copenhagen, DK-1168 Copenhagen, Denmark.								0	15	15	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		2001	42	1-2					5	5		10.1023/A:1011467125007	http://dx.doi.org/10.1023/A:1011467125007			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	437VX					2022-12-18	WOS:000169015200001
J	Joshi, T; Ahuja, N; Ponce, J				Joshi, T; Ahuja, N; Ponce, J			Structure and motion estimation from dynamic silhouettes under perspective projection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion estimation; 3D structure estimation; silhouettes; occluding contours; trinocular stereo; frontier points; epipolar curves	CONTOURS; SURFACE; CURVES	We address the problem of estimating the structure and motion of a smooth curved object from its silhouettes observed over time by a trinocular stereo rig under perspective projection. We first construct a model for the local structure along the silhouette for each frame in the temporal sequence. The local models are then integrated into a global surface description by estimating the motion between successive time instants. The algorithm tracks certain surface features (parabolic points) and image features (silhouette inflections and frontier points) which are used to bootstrap the motion estimation process. The entire silhouettes along with the reconstructed local structure are then used to refine the initial motion estimate. We have implemented the proposed approach and report results on real images.	Microsoft Corp, Redmond, WA 98052 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	Microsoft; University of Illinois System; University of Illinois Urbana-Champaign	Joshi, T (corresponding author), Microsoft Corp, 1 Microsoft Way, Redmond, WA 98052 USA.							ARBOGAST E, 1992, P 2 EUR C COMP VIS S, P467; BLAKE A, 1990, P EUR C COMP VIS, P465; BLAKE A, 1992, INT J COMPUT VISION, V9, P83; Boyer E, 1997, INT J COMPUT VISION, V22, P219, DOI 10.1023/A:1007978616082; CIPOLLA R, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P269, DOI 10.1109/ICCV.1995.466775; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; FAUGERAS O, 1992, GEOMETRIC INVARIANCE, P310; Giblin P., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P136; GIBLIN PJ, 1994, J OPT SOC AM A, V11, P1976, DOI 10.1364/JOSAA.11.001976; GIBLIN PJ, 1995, IMAGE VISION COMPUT, V13, P33, DOI 10.1016/0262-8856(95)91466-Q; Joshi T, 1997, IMAGE VISION COMPUT, V15, P479, DOI 10.1016/S0262-8856(97)00001-2; JOSHI T, 1994, P ARPA IM UND WORKSH, P1237; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321; KUTULAKOS KN, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P331, DOI 10.1109/CVPR.1994.323848; Press WH., 1994, NUMERICAL RECIPES C; RIEGER JH, 1986, OPT LETT, V11, P123, DOI 10.1364/OL.11.000123; SEALES B, 1994, 2 CAD BAS VIS WORKSH, P116; Szeliski R., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P666, DOI 10.1109/CVPR.1993.341037; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; *VIS NUM, 1991, IMSL MATH LIB VERS 2; ZHAO C, 1994, LECT NOTES COMPUTER, V800, P417; ZHENG JY, 1994, IEEE T PATTERN ANAL, V16, P163, DOI 10.1109/34.273734	24	15	15	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1999	31	1					31	50		10.1023/A:1008042709602	http://dx.doi.org/10.1023/A:1008042709602			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	186GM					2022-12-18	WOS:000079720600002
J	Havaldar, P; Medioni, G; Stein, F				Havaldar, P; Medioni, G; Stein, F			Perceptual grouping for generic recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							TWO-DIMENSIONAL IMAGES; OBJECT	We address the problem of recognition of generic objects from a single intensity image. This precludes the use of purely geometric methods which assume that models are geometrically and precisely designed. Instead, we propose to use descriptions in terms of features and their qualitative geometric relationships. To succeed, it is clear that these features need to be high level, rather than points or lines. We propose to detect groups using perceptual organization criteria such as proximity, symmetry, parallelism, and closure. The detection of these features is performed in an efficient way using proximity indexing. Since many groups are created, we also perform selection of relevant groups by organizing them into sets of similar perceptual content. Finally we present an implementation of a recognition system using these sets as primitives. It is an efficient colored graph matching algorithm using the adjacency matrix representation of a graph. Using indexing, we retrieve matching hypotheses, which are verified against each other with respect to topological constraints. Groups of consistent hypotheses represent detected model instances in a scene. The complete system is illustrated on real images. We also discuss further extensions.			Havaldar, P (corresponding author), UNIV SO CALIF,INST ROBOT & INTELLIGENT SYST,LOS ANGELES,CA 90089, USA.							Alavi Y., 1985, GRAPH THEORY APPL AL; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BREUEL TM, 1989, P DARPA IU WORKSHOP, P805; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CLEMENS DT, 1991, IEEE T PATTERN ANAL, V13, P1007, DOI 10.1109/34.99235; CORNEIL DG, 1970, J ACM, V17, P51, DOI 10.1145/321556.321562; DICKINSON SJ, 1992, IEEE T PATTERN ANAL, V14, P174, DOI 10.1109/34.121788; ETTINGER GJ, 1988, P IEEE CVPR ANN ARB; FAN TJ, 1990, DESCRIBING RECOGNIZI; FLYNN PJ, 1991, JUN P IEEE WORKSH DI, P115; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; Grimson W. E. L., 1990, OBJECT RECOGNITION C; HERAULT L, 1990, SEP P BRIT MACH VIS, P319; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; LAMDAN Y, 1988, P INT C COMP VIS, P218; LAMDAN Y, 1988, APR P IEEE INT C ROB, P1407; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; MOHAN R, 1989, IEEE T PATTERN ANAL, V11, P1121, DOI 10.1109/34.42852; NEVATIA R, 1992, IEEE T PATTERN ANAL, V4, P476; NOLTMEIER H, 1976, GRAPHENTHEORIE; Parvin B., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P393, DOI 10.1109/CVPR.1991.139722; ROBERTS LG, 1968, MACHINE PERCEPTION 3, P159; ROM H, 1993, IEEE T PATTERN ANAL, P973; ROTHWELL C, 1993, APPL INVARIANTS COMP, V2, P287; SAINTMARC P, 1990, P EURO C COMPUT VISI, P604; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P1198, DOI 10.1109/34.177385; STEIN F, 1993, P WORKSH COMP VIS SP, P453; STEIN F, THESIS USC IRIS; Tenenbaum Jay M, 1983, HUMAN MACHINE VISION, P481; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; ULUPINAR F, 1993, IEEE T PATTERN ANAL, P3; ZERROUG M, 1993, IMAGE UNDERSTANDING, P905; [No title captured]	36	15	15	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1996	20	1-2					59	80		10.1007/BF00144117	http://dx.doi.org/10.1007/BF00144117			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VQ240					2022-12-18	WOS:A1996VQ24000004
J	Carlsson, S				Carlsson, S			Projectively invariant decomposition and recognition of planar shapes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OBJECT RECOGNITION	An algorithm is presented for computing a decomposition of planar shapes into convex subparts represented by ellipses, The method is invariant to projective transformations of the shape, and thus the conic primitives can be used for matching and definition of invariants in the same way as points and lines. The method works for arbitrary planar shapes admitting at least four distinct tangents and it is based on finding ellipses with four points of contact to the given shape. The cross ratio computed from the four points on the ellipse can then be used as a projectively invariant index. It is demonstrated that a given shape has a unique parameter-free decomposition into a finite set of ellipses with unit cross ratio. For a given shape, each pair of ellipses can be used to compute two independent projective invariants. The set of invariants computed for each ellipse pair can be used as indexes to a hash table from which model hypothesis can be generated Examples of shape decomposition and recognition are given for synthetic shapes and shapes extracted from grey level images of real objects using edge detection.			Carlsson, S (corresponding author), ROYAL INST TECHNOL, DEPT NUMER ANAL & COMP SCI, CVAP, KTH, S-10044 STOCKHOLM, SWEDEN.							BARRETT EB, 1991, CVGIP-IMAG UNDERSTAN, V53, P46, DOI 10.1016/1049-9660(91)90004-9; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; BOOKSTEIN F, 1979, COMPUTER GRAPHICS IM, V8, P56; BRADY M, 1983, HUMAN MACHINE VISION, P39; CARLSSON S, 1992, ARTIF INT, P267; Forsyth D., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P598, DOI 10.1109/ICCV.1990.139604; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; LAMDAN Y, 1988, JUN P CVPR C ANN ARB, P335; LEE DT, 1982, IEEE T PATTERN ANAL, V4, P363, DOI 10.1109/TPAMI.1982.4767267; RICHARDS W, 1985, COMPUT VISION GRAPH, V31, P265, DOI 10.1016/0734-189X(85)90031-3; ROTHWELL C, 1992, 2ND P EUR C COMP VIS, P757; Rothwell C. A., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P109, DOI 10.1109/CVPR.1992.223219; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P1198, DOI 10.1109/34.177385; VANGOOL LJ, 1992, ARTIF INT, P157; WEISS I, 1988, JAN P IEEE C COMP VI, P291	17	15	24	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1996	17	2					193	209		10.1007/BF00058751	http://dx.doi.org/10.1007/BF00058751			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TZ498					2022-12-18	WOS:A1996TZ49800005
J	GRIMSON, WEL; HUTTENLOCHER, DP; JACOBS, DW				GRIMSON, WEL; HUTTENLOCHER, DP; JACOBS, DW			A STUDY OF AFFINE MATCHING WITH BOUNDED SENSOR ERROR	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OBJECT RECOGNITION; HOUGH TRANSFORM	Affine transformations of the plane have been used in a number of model-based recognition systems. Because the underlying mathematics are based on exact data, in practice various heuristics are used to adapt the methods to real data where there is positional uncertainty. This paper provides a precise analysis of affine point matching under uncertainty. We obtain an expression for the range of affine-invariant values that are consistent with a given set of four points, where each image point lies in an epsilon-disc of uncertainty. This range is shown to depend on the actual x-y-positions of the data points. In other words, given uncertainty in the data there are no representations that are invariant with respect to the Cartesian coordinate system of the data. This is problematic for methods, such as geometric hashing, that are based on affine-invariant representations. We also analyze the effect that uncertainty has on the probability that recognition methods using affine transformations will find false positive matches. We find that there is a significant probability of false positives with even moderate levels of sensor error, suggesting the importance of good verification techniques and good grouping techniques.	CORNELL UNIV,DEPT COMP SCI,ITHACA,NY 14853; XEROX CORP,PALO ALTO RES CTR,PALO ALTO,CA 94304; NEC RES INST,PRINCETON,NJ 08540	Cornell University; Xerox; NEC Corporation	GRIMSON, WEL (corresponding author), MIT,AI LAB,CAMBRIDGE,MA 02139, USA.							BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; Basri R., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P482, DOI 10.1109/CCV.1988.590027; BESI PJ, 1985, ACM COMPUT SURV, V17, P75; COSTA M, 1990, 6TH P ISR C AI, P35; CYGANSKI D, 1985, IEEE T PATTERN ANAL, V7, P662, DOI 10.1109/TPAMI.1985.4767722; DEMENTHON DF, 1992, 2 EUR C COMP VIS, P335; Efimov N. V., 1980, HIGHER GEOMETRY; ELLIS RE, 1989, IEEE INT C ROB AUT, P348; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; GAVRILA DM, 1992, PATTERN RECOGN LETT, V13, P263, DOI 10.1016/0167-8655(92)90077-D; Grimson W. E. L., 1990, OBJECT RECOGNITION C; GRIMSON WEL, 1990, IEEE T PATTERN ANAL, V12, P255, DOI 10.1109/34.49052; GRIMSON WEL, 1991, IEEE T PATTERN ANAL, V13, P1201, DOI 10.1109/34.106994; GRIMSON WEL, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P334; GRIMSON WEL, 1990, 1ST P EUR C COMP VIS, P552; GRIMSON WEL, 1992, P CVPR; GRIMSON WEL, 1991, MIT1250 AI LAB MEM; GUEZIEC A, 1992, 2ND P EUR C COMP VIS, P620; HORAUD R, 1989, COMPUT VISION GRAPH, V47, P33, DOI 10.1016/0734-189X(89)90052-2; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; JACOBS D, 1991, IEEE C COMP VIS PATT, P269; Klein F., 1939, ELEMENTARY MATH ADV; Korn G. A., 1968, MATH HDB SCI ENG; LAMDAN Y, 1990, IEEE T ROBOTIC AUTOM, V6, P578, DOI 10.1109/70.62047; Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995; LAMDAN Y, 1991, IEEE C COMP VIS PATT, P22; LAMDAN Y, 1988, JUN P CVPR C ANN ARB, P335; LAMDAN Y, 1988, APR P IEEE INT C ROB, P1407; RIGOUTSOS I, 1993, IEEE C COMP VIS PATT, P180; RIGOUTSOS I, 1991, 8TH ISR C ART INT CO; SARACHIK KB, 1992, GAUSSIAN ERROR MODEL; THOMPSON D, 1987, P IEEE C ROB AUT, P280; Van Gool L., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P454, DOI 10.1109/CVPR.1991.139735; WAYNER PC, 1991, IEEE C COMP VIS PATT, P473; WEISS I, 1988, DARPA IM UND WORKSH, P1125; WELLS WM, 1991, IEEE COMP SOC COMP V, P486; WOLFSON H, 1990, 1ST EUR C COMP VIS, P526; [No title captured]	38	15	17	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1994	13	1					7	32		10.1007/BF01420793	http://dx.doi.org/10.1007/BF01420793			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PK120					2022-12-18	WOS:A1994PK12000001
J	SHAKUNAGA, T; KANEKO, H				SHAKUNAGA, T; KANEKO, H			PERSPECTIVE ANGLE TRANSFORM - PRINCIPLE OF SHAPE FROM ANGLES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									NIPPON TELEGRAPH & TEL PUBL CORP, YOKOSUKA ELECT COMMUN LABS, VISUAL PERCEPT LAB, YOKOSUKA, KANAGAWA 23803, JAPAN; NIPPON TELEGRAPH & TEL PUBL CORP, YOKOSUKA ELECT COMMUN LABS, HUMAN INTERFACE LABS, YOKOSUKA, KANAGAWA 23803, JAPAN	Nippon Telegraph & Telephone Corporation; Nippon Telegraph & Telephone Corporation								BARNARD ST, 1983, ARTIF INTELL, V21, P435, DOI 10.1016/S0004-3702(83)80021-6; BARNARD ST, 1985, COMPUT VISION GRAPH, V29, P87, DOI 10.1016/S0734-189X(85)90152-5; HARALICK RM, 1980, COMPUTER GRAPHICS IM, V13, P49; Huffman D, 1971, MACHINE INTELLIGENCE; IKEUCHI K, 1984, ARTIF INTELL, V22, P49, DOI 10.1016/0004-3702(84)90025-0; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; KANATANI K, 1988, COMPUT VISION GRAPH, V41, P28, DOI 10.1016/0734-189X(88)90115-6; MACKWORTH AK, 1973, ARTIF INTELL, V4, P121, DOI 10.1016/0004-3702(73)90003-9; SHAFFER JA, 1983, GUT, V24, P182, DOI 10.1136/gut.24.3.182; Shakunaga T., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P594; SHAKUNAGA T, 1988, 2ND P INT C COMP VIS; 1977, ENCY DICT MATH, V2; 1977, ENCY DICT MATH, V1	13	15	15	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1989	3	3					239	254		10.1007/BF00133033	http://dx.doi.org/10.1007/BF00133033			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AP282					2022-12-18	WOS:A1989AP28200003
J	NALWA, VS				NALWA, VS			LINE-DRAWING INTERPRETATION - A MATHEMATICAL FRAMEWORK	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									STANFORD UNIV,DEPT COMP SCI,ROBOT LAB,STANFORD,CA 94305	Stanford University								ARNOLD VI, 1983, RUSS MATH SURV+, V38, P87, DOI 10.1070/RM1983v038n02ABEH003471; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; BARROW HG, 1981, ARTIF INTELL, V17, P75, DOI 10.1016/0004-3702(81)90021-7; BINFORD TO, 1981, ARTIF INTELL, V17, P205, DOI 10.1016/0004-3702(81)90025-4; BRADY M, 1982, COMPUT SURV, V14, P3, DOI 10.1145/356869.356871; BRADY M, 1983, 8TH P IJCAI, P969; CHAKRAVARTY I, 1979, IEEE T PATTERN ANAL, V1, P202, DOI 10.1109/TPAMI.1979.4766906; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; DRAPER SW, 1981, ARTIF INTELL, V17, P461, DOI 10.1016/0004-3702(81)90032-1; GANS D, 1969, TRANSFORMATIONS GEOM; Gans David, 1973, INTRO NONEUCLIDEAN G; Guillemin V., 2010, DIFFERENTIAL TOPOLOG, V370; Hilbert D., 1952, GEOMETRY IMAGINATION; Hirsch M. W., 1976, GRADUATE TEXTS MATH; Horn B., 1986, ROBOT VISION, P1; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; HUFFMAN DA, 1976, IEEE T COMPUT, V25, P1010, DOI 10.1109/TC.1976.1674542; Huffman David, 1977, MACHINE INTELLIGENCE, V8, P475; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; KANADE T, 1980, ARTIF INTELL, V13, P279, DOI 10.1016/0004-3702(80)90004-1; KENNEDY JM, 1974, MEDIA SYMBOLS FORMS, P211; KOENDERINK JJ, 1982, PERCEPTION, V11, P129, DOI 10.1068/p110129; KOENDERINK JJ, 1976, BIOL CYBERN, V24, P51, DOI 10.1007/BF00365595; KOENDERINK JJ, 1984, PERCEPTION, V13, P32184; LEE SJ, 1985, ARTIF INTELL, V26, P145, DOI 10.1016/0004-3702(85)90027-X; Lipschutz M., 1969, DIFFERENTIAL GEOMETR; LOWE DG, 1985, IEEE T PATTERN ANAL, V7, P320, DOI 10.1109/TPAMI.1985.4767660; MACKWORTH AK, 1973, ARTIF INTELL, V4, P121, DOI 10.1016/0004-3702(73)90003-9; MACKWORTH AK, 1977, MACH INTELL, V8, P510; MALIK J, 1985, THESIS STANFORD U CO; Marr D., 1982, VISION; MATHER JN, 1973, ANN MATH, V98, P226, DOI 10.2307/1970783; NALWA VS, 1988, IEEE T PAMI, V10; Pirenne M. H., 1970, OPTICS PAINTING PHOT; SHAPIRA R, 1978, IEEE T COMPUT, V27, P841, DOI 10.1109/TC.1978.1675204; Spivak M., 1965, CALCULUS MANIFOLDS, DOI DOI 10.1201/9780429501906; SUGIHARA K, 1984, IEEE T PATTERN ANAL, V6, P578, DOI 10.1109/TPAMI.1984.4767571; Turner K., 1974, THESIS U EDINBURGH; Waltz D., 1975, PSYCHOL COMPUTER VIS, P19; WHITNEY H, 1955, ANN MATH, V62, P374, DOI 10.2307/1970070	40	15	15	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1988	2	2					103	124		10.1007/BF00133696	http://dx.doi.org/10.1007/BF00133696			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC190					2022-12-18	WOS:A1988AC19000001
J	Bian, JW; Zhan, HY; Wang, NY; Li, ZC; Zhang, L; Shen, CH; Cheng, MM; Reid, I				Bian, Jia-Wang; Zhan, Huangying; Wang, Naiyan; Li, Zhichao; Zhang, Le; Shen, Chunhua; Cheng, Ming-Ming; Reid, Ian			Unsupervised Scale-Consistent Depth Learning from Video	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised depth estimation; Scale consistency; Visual SLAM; Pseudo-RGBD SLAM		We propose a monocular depth estimation method SC-Depth, which requires only unlabelled videos for training and enables the scale-consistent prediction at inference time. Our contributions include: (i) we propose a geometry consistency loss, which penalizes the inconsistency of predicted depths between adjacent views; (ii) we propose a self-discovered mask to automatically localize moving objects that violate the underlying static scene assumption and cause noisy signals during training; (iii) we demonstrate the efficacy of each component with a detailed ablation study and show high-quality depth estimation results in both KITTI and NYUv2 datasets. Moreover, thanks to the capability of scale-consistent prediction, we show that our monocular-trained deep networks are readily integrated into ORB-SLAM2 system for more robust and accurate tracking. The proposed hybrid Pseudo-RGBD SLAM shows compelling results in KITTI, and it generalizes well to the KAIST dataset without additional training. Finally, we provide several demos for qualitative evaluation. The source code is released on GitHub.	[Bian, Jia-Wang; Zhan, Huangying; Shen, Chunhua; Reid, Ian] Univ Adelaide, Adelaide, SA, Australia; [Bian, Jia-Wang; Zhan, Huangying; Shen, Chunhua; Reid, Ian] Australian Ctr Robot Vis, Brisbane, Qld, Australia; [Wang, Naiyan; Li, Zhichao] TuSimple, Beijing, Peoples R China; [Zhang, Le] Agcy Sci Technol & Res, Singapore, Singapore; [Cheng, Ming-Ming] Nankai Univ, CS, TKLNDST, Tianjin, Peoples R China	University of Adelaide; Australian Centre for Robotic Vision; Agency for Science Technology & Research (A*STAR); Nankai University	Bian, JW (corresponding author), Univ Adelaide, Adelaide, SA, Australia.; Bian, JW (corresponding author), Australian Ctr Robot Vis, Brisbane, Qld, Australia.	jiawang.bian@gmail.com	Cheng, Ming-Ming/A-2527-2009; Bian, Jia-Wang/AAP-2274-2020; Bian, Jia-Wang/AAH-4463-2019	Cheng, Ming-Ming/0000-0001-5550-8758; Bian, Jia-Wang/0000-0003-2046-3363; Bian, Jia-Wang/0000-0003-2046-3363	Australian Centre of Excellence for Robotic Vision [CE140100016]; ARC Laureate Fellowship [FL130100102]; Major Project for New Generation of AI [2018AAA0100403]; Tianjin Natural Science Foundation [18JCYBJC41300, 18ZXZNGX00110]; NSFC [61922046]	Australian Centre of Excellence for Robotic Vision; ARC Laureate Fellowship(Australian Research Council); Major Project for New Generation of AI; Tianjin Natural Science Foundation(Natural Science Foundation of Tianjin); NSFC(National Natural Science Foundation of China (NSFC))	This work was in part supported by the Australian Centre of Excellence for Robotic Vision CE140100016, and the ARC Laureate Fellowship FL130100102 to Prof. Ian Reid. This work was supported by Major Project for New Generation of AI (No. 2018AAA0100403), Tianjin Natural Science Foundation (No. 18JCYBJC41300 andNo. 18ZXZNGX00110), and NSFC (61922046) to Prof. Ming-Ming Cheng. We also thank anonymous reviewers for their valuable suggestions.	Angelova, 2019, ASS ADVANCEMENT ARTI; Angelova A., 2020, C ROB LEARN; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Bian J.-W., 2019, PROC BRIT MACH VIS C; Bloesch M, 2018, PROC CVPR IEEE, P2560, DOI 10.1109/CVPR.2018.00271; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Casser Vincent, 2019, CVPR WORKSH VIS OD C; Chen S, 2020, IEEE C COMP VIS PATT; Chen WJ, 2019, PROC CVPR IEEE, P7234, DOI 10.1109/CVPR.2019.00741; Chen YH, 2019, IEEE I CONF COMP VIS, P7062, DOI 10.1109/ICCV.2019.00716; Cheng M.-M., 2019, NEURAL INFORM PROCES; Cheng M-M, 2020, INT J COMPUT VISION, V4, P4181; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Djork-Arn, ICLR 2016; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eigen David, 2014, NEURIPS; Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577; Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Garg R, 2019, IEEE I CONF COMP VIS, P7627, DOI 10.1109/ICCV.2019.00772; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907; Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256; Guizilini Vitor, 2020, INT C LEARN REPR; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hirschmuller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kim A, 2019, INT J ROBOT RES; Kingma D.P, P 3 INT C LEARNING R; KLEIN G, 2007, IEEE ACM INT S MIX A, P225, DOI DOI 10.1109/ISMAR.2007.4538852; Klingner Marvin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P582, DOI 10.1007/978-3-030-58565-5_35; Koltun V, 2020, IEEE T PATTERN RECOG; Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lam Huynh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P581, DOI 10.1007/978-3-030-58574-7_35; Lee Seokju, 2021, P AAAI C ART INT AAA; Li J, 2017, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2017.365; Li RH, 2018, IEEE INT CONF ROBOT, P7286; Li Y, 2019, IEEE INT CONF ROBOT, P5439, DOI 10.1109/ICRA.2019.8793706; Li ZQ, 2019, PROC CVPR IEEE, P4516, DOI 10.1109/CVPR.2019.00465; Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; Liu Y.-J, 2020, IEEE C COMP VIS PATT; Loo SY, 2019, IEEE INT CONF ROBOT, P5218, DOI 10.1109/ICRA.2019.8794425; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo CX, 2020, IEEE T PATTERN ANAL, V42, P2624, DOI 10.1109/TPAMI.2019.2930258; Luo X, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392377; Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594; Mattoccia S, 2020, IEEE C COMP VIS PATT; Menze Moritz, 2015, CVPR; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Mur-Artal R, 2014, IEEE INT CONF ROBOT, P846, DOI 10.1109/ICRA.2014.6906953; Ng S.H, 2006, NEURAL INFORM PROCES; Pillai S, 2019, IEEE INT CONF ROBOT, P9250, DOI 10.1109/ICRA.2019.8793621; Pilzer A, 2018, INT CONF 3D VISION, P587, DOI 10.1109/3DV.2018.00073; Ranjan A., 2019, CVPR, P12240, DOI DOI 10.1109/CVPR.2019.01252; Reid I., ARXIV PREPRINT ARXIV; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Shakhnarovich G, 2016, NEURAL INFORM PROCES; Shen TW, 2019, IEEE INT CONF ROBOT, P6359, DOI 10.1109/ICRA.2019.8793479; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695; Tiwari Lokender, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P437, DOI 10.1007/978-3-030-58621-8_26; Torr, 2017, 170800783 ARXIV; Vijayanarasimhan Sudheendra, 2017, ARXIV170407804; Wang CY, 2019, INT CONF 3D VISION, P348, DOI 10.1109/3DV.2019.00046; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xian K, 2018, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2018.00040; Yang N, 2020, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR42600.2020.00136; Yang N, 2018, LECT NOTES COMPUT SC, V11212, P835, DOI 10.1007/978-3-030-01237-3_50; Yang Z., 2018, AAAI C ART INT; Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578; Yin XC, 2017, IEEE I CONF COMP VIS, P5871, DOI 10.1109/ICCV.2017.625; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Zeng W, 2019, IEEE INT C COMP VIS; Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043; Zhang Y, 2017, IEEE DECIS CONTR P; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561; Zhou Qian-Yi, 2018, ARXIV180109847; Zou Y., 2020, P 16 EUR C COMP VIS, P710; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	95	14	14	10	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2548	2564		10.1007/s11263-021-01484-6	http://dx.doi.org/10.1007/s11263-021-01484-6		JUN 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN		Green Submitted			2022-12-18	WOS:000663288700002
J	Chen, Z; Ouyang, WL; Liu, TL; Tao, DC				Chen, Zhe; Ouyang, Wanli; Liu, Tongliang; Tao, Dacheng			A Shape Transformation-based Dataset Augmentation Framework for Pedestrian Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pedestrian detection; Dataset augmentation; Pedestrian rendering	DEEP; MOTION	Deep learning-based computer vision is usually data-hungry. Many researchers attempt to augment datasets with synthesized data to improve model robustness. However, the augmentation of popular pedestrian datasets, such as Caltech and Citypersons, can be extremely challenging because real pedestrians are commonly in low quality. Due to the factors like occlusions, blurs, and low-resolution, it is significantly difficult for existing augmentation approaches, which generally synthesize data using 3D engines or generative adversarial networks (GANs), to generate realistic-looking pedestrians. Alternatively, to access much more natural-looking pedestrians, we propose to augment pedestrian detection datasets by transforming real pedestrians from the same dataset into different shapes. Accordingly, we propose the Shape Transformation-based Dataset Augmentation (STDA) framework. The proposed framework is composed of two subsequent modules, i.e. the shape-guided deformation and the environment adaptation. In the first module, we introduce a shape-guided warping field to help deform the shape of a real pedestrian into a different shape. Then, in the second stage, we propose an environment-aware blending map to better adapt the deformed pedestrians into surrounding environments, obtaining more realistic-looking pedestrians and more beneficial augmentation results for pedestrian detection. Extensive empirical studies on different pedestrian detection benchmarks show that the proposed STDA framework consistently produces much better augmentation results than other pedestrian synthesis approaches using low-quality pedestrians. By augmenting the original datasets, our proposed framework also improves the baseline pedestrian detector by up to 38% on the evaluated benchmarks, achieving state-of-the-art performance.	[Chen, Zhe; Ouyang, Wanli; Liu, Tongliang; Tao, Dacheng] Univ Sydney, Sydney, NSW, Australia	University of Sydney	Tao, DC (corresponding author), Univ Sydney, Sydney, NSW, Australia.	zhe.chen1@sydney.edu.au; wanli.ouyang@sydney.edu.au; tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au	Chen, Zhe/AAT-9716-2020; Liu, Tongliang/AAA-1506-2021	Chen, Zhe/0000-0001-5004-8975; Liu, Tongliang/0000-0002-9640-6472	Australian Research Council [FL-170100117, IH-180100002, IC-190100031, DP200103223]; Australian Medical Research Future Fund [MRFAI000085]	Australian Research Council(Australian Research Council); Australian Medical Research Future Fund(Medical Research Future Fund (MRFF))	This work was supported by Australian Research Council Projects FL-170100117, IH-180100002, IC-190100031, DP200103223, and Australian Medical Research Future Fund MRFAI000085.	Alcorn M. A., 2018, ARXIV181111553; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2018, ADV NEURAL INFORM PR; Bar-Hillel A, 2010, LECT NOTES COMPUT SC, V6314, P127, DOI 10.1007/978-3-642-15561-1_10; Brazil G, 2017, IEEE I CONF COMP VIS, P4960, DOI 10.1109/ICCV.2017.530; Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Chen Z, 2019, IEEE-CAA J AUTOMATIC, V6, P693, DOI 10.1109/JAS.2019.1911459; Chen ZJ, 2017, LECT NOTES COMPUT SC, V10634, P666, DOI 10.1007/978-3-319-70087-8_69; Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141; Du XZ, 2017, IEEE WINT CONF APPL, P953, DOI 10.1109/WACV.2017.111; Enzweiler M, 2009, IEEE T PATTERN ANAL, V31, P2179, DOI 10.1109/TPAMI.2008.260; Felzenszwalb P, 2008, PROC CVPR IEEE, P1984; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gulrajani I, 2017, P NIPS 2017; Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang SY, 2017, PROC CVPR IEEE, P4664, DOI 10.1109/CVPR.2017.496; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jaderberg M, 2015, ADV NEUR IN, V28; Lee D, 2018, ADV NEUR IN, V31; Lerer A, 2016, PR MACH LEARN RES, V48; Li C.-L., 2018, ARXIV PREPRINT ARXIV; Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508; Lin CZ, 2018, LECT NOTES COMPUT SC, V11213, P745, DOI 10.1007/978-3-030-01240-3_45; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74; Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431; Liu LL, 2019, IEEE I CONF COMP VIS, P6072, DOI 10.1109/ICCV.2019.00617; Liu Ming-Yu, 2017, NIPS; Liu TL, 2017, PR MACH LEARN RES, V70; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Loy C.C., 2019, ARXIV PREPRINT ARXIV; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Ouyang WL, 2018, IEEE T PATTERN ANAL, V40, P1874, DOI 10.1109/TPAMI.2017.2738645; Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257; Park D, 2010, LECT NOTES COMPUT SC, V6314, P241, DOI 10.1007/978-3-642-15561-1_18; Pishchulin L, 2011, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2011.5995574; Radford A., 2015, ARXIV PREPR ARXIV151; Ran Y, 2007, INT J COMPUT VISION, V71, P143, DOI 10.1007/s11263-006-8575-4; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Sajjadi Mehdi, 2016, NEURIPS; Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359; Song T, 2018, LECT NOTES COMPUT SC, V11211, P554, DOI 10.1007/978-3-030-01234-2_33; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Villegas R, 2017, PR MACH LEARN RES, V70; Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8; Vobecky A, 2019, IEEE INT CONF COMP V, P2367, DOI 10.1109/ICCVW.2019.00290; Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324; Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811; Wasserstein G.A.N., 2017, ARXIV170107875; Yan YC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P199, DOI 10.1145/3123266.3123277; Zanfir M, 2018, PROC CVPR IEEE, P5391, DOI 10.1109/CVPR.2018.00565; Zhang Chiyuan, 2016, ARXIV161103530; Zhang J., 2020, ARXIV200200537; Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28; Zhang SS, 2016, PROC CVPR IEEE, P1259, DOI 10.1109/CVPR.2016.141; Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhu Jun-Yan, 2017, ICCV	76	14	14	6	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1121	1138		10.1007/s11263-020-01412-0	http://dx.doi.org/10.1007/s11263-020-01412-0		JAN 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK		Green Submitted			2022-12-18	WOS:000606383400002
J	Wu, JW; Yin, F; Zhang, YM; Zhang, XY; Liu, CL				Wu, Jin-Wen; Yin, Fei; Zhang, Yan-Ming; Zhang, Xu-Yao; Liu, Cheng-Lin			Handwritten Mathematical Expression Recognition via Paired Adversarial Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Handwritten ME recognition; Paired adversarial learning; Semantic-invariant features; Convolutional decoder; Coverage of decoding	RETRIEVAL	Recognition of handwritten mathematical expressions (MEs) is an important problem that has wide applications in practice. Handwritten ME recognition is challenging due to the variety of writing styles and ME formats. As a result, recognizers trained by optimizing the traditional supervision loss do not perform satisfactorily. To improve the robustness of the recognizer with respect to writing styles, in this work, we propose a novel paired adversarial learning method to learn semantic-invariant features. Specifically, our proposed model, named PAL-v2, consists of an attention-based recognizer and a discriminator. During training, handwritten MEs and their printed templates are fed into PAL-v2 simultaneously. The attention-based recognizer is trained to learn semantic-invariant features with the guide of the discriminator. Moreover, we adopt a convolutional decoder to alleviate the vanishing and exploding gradient problems of RNN-based decoder, and further, improve the coverage of decoding with a novel attention method. We conducted extensive experiments on the CROHME dataset to demonstrate the effectiveness of each part of the method and achieved state-of-the-art performance.	[Wu, Jin-Wen; Yin, Fei; Zhang, Yan-Ming; Zhang, Xu-Yao; Liu, Cheng-Lin] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Wu, Jin-Wen; Zhang, Xu-Yao; Liu, Cheng-Lin] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China; [Liu, Cheng-Lin] CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing 100190, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Wu, JW (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Wu, JW (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.	jinwen.wu@nlpr.ia.ac.cn; fyin@nlpr.ia.ac.cn; ymzhang@nlpr.ia.ac.cn; xyz@nlpr.ia.ac.cn; liucl@nlpr.ia.ac.cn		Wu, Jin-Wen/0000-0003-1595-597X				Alvaro F, 2016, PATTERN RECOGN, V51, P135, DOI 10.1016/j.patcog.2015.09.013; Alvaro F, 2014, PATTERN RECOGN LETT, V35, P58, DOI 10.1016/j.patrec.2012.09.023; Anderson RH, 1967, S INTERACTIVE SYSTEM, P436, DOI [DOI 10.1145/2402536.2402585, 10.1145/2402536.2402585]; Aneja J, 2018, PROC CVPR IEEE, P5561, DOI 10.1109/CVPR.2018.00583; Le AD, 2017, PROC INT CONF DOC, P1056, DOI 10.1109/ICDAR.2017.175; Awal AM, 2014, PATTERN RECOGN LETT, V35, P68, DOI 10.1016/j.patrec.2012.10.024; Bai S, 2018, ARXIV18030127 CORR; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Chan KF, 2001, PATTERN RECOGN, V34, P1671, DOI 10.1016/S0031-3203(00)00102-3; Cho K, 2015, ARXIV151107916 CORR; Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Dauphin YN, 2017, PR MACH LEARN RES, V70; Deng Y, 2016, ARXIV16090493 CORR; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gehring J., 2017, P ICML; Ghiasi G, 2018, ADV NEUR IN, V31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Kam-Fai Chan, 2000, International Journal on Document Analysis and Recognition, V3, P3, DOI 10.1007/PL00013549; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krogh Anders, 1991, P NIPS, P950; Li LH, 2017, AAAI CONF ARTIF INTE, P4133; Liu Q, 2017, IEEE CONF COMPUT, P986; Liu Y, 2018, LECT NOTES COMPUT SC, V11209, P449, DOI 10.1007/978-3-030-01228-1_27; MacLean S, 2013, INT J DOC ANAL RECOG, V16, P139, DOI 10.1007/s10032-012-0184-x; Mahdavi M, 2019, ICDAR; Mouchere H, 2016, INT CONF FRONT HAND, P607, DOI [10.1109/ICFHR.2016.0116, 10.1109/ICFHR.2016.108]; Mouchere H, 2016, INT J DOC ANAL RECOG, V19, P173, DOI 10.1007/s10032-016-0263-5; Ordonez V, 2016, INT J COMPUT VISION, V119, P46, DOI 10.1007/s11263-015-0840-y; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Shi B, 2018, IEEE T PATTERN ANAL, V1, P1; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Su J, 2016, ANAL CELL PATHOL, V2016, P1, DOI 10.1155/2016/9535027; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tu ZP, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P76; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wu Jin-Wen, 2018, P JOINT EUR C MACH L, P18; Wu Y, 2018, ARXIV18060057 CORR; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zanibbi R, 2012, INT J DOC ANAL RECOG, V15, P331, DOI 10.1007/s10032-011-0174-4; Zhang JS, 2017, PROC INT CONF DOC, P902, DOI 10.1109/ICDAR.2017.152; Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689; Zhang JS, 2018, INT C PATT RECOG, P2245, DOI 10.1109/ICPR.2018.8546031; Zhang JS, 2017, PATTERN RECOGN, V71, P196, DOI 10.1016/j.patcog.2017.06.017; Zhang YP, 2018, PATTERN RECOGN LETT, V106, P20, DOI 10.1016/j.patrec.2018.02.006; Zhou XD, 2013, IEEE T PATTERN ANAL, V35, P2413, DOI 10.1109/TPAMI.2013.49	52	14	14	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2386	2401		10.1007/s11263-020-01291-5	http://dx.doi.org/10.1007/s11263-020-01291-5			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY					2022-12-18	WOS:000572233600003
J	Kim, B; Ponce, J; Ham, B				Kim, Beomjun; Ponce, Jean; Ham, Bumsub			Deformable Kernel Networks for Joint Image Filtering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Joint filtering; Convolutional neural networks; Depth map upsampling; Cross-modality image restoration; Texture removal; Semantic segmentation		Joint image filters are used to transfer structural details from a guidance picture used as a prior to a target image, in tasks such as enhancing spatial resolution and suppressing noise. Previous methods based on convolutional neural networks (CNNs) combine nonlinear activations of spatially-invariant kernels to estimate structural details and regress the filtering result. In this paper, we instead learn explicitly sparse and spatially-variant kernels. We propose a CNN architecture and its efficient implementation, called the deformable kernel network (DKN), that outputs sets of neighbors and the corresponding weights adaptively for each pixel. The filtering result is then computed as a weighted average. We also propose a fast version of DKN that runs about seventeen times faster for an image of size 640x480. We demonstrate the effectiveness and flexibility of our models on the tasks of depth map upsampling, saliency map upsampling, cross-modality image restoration, texture removal, and semantic segmentation. In particular, we show that the weighted averaging process with sparsely sampled 3 x 3 kernels outperforms the state of the art by a significant margin in all cases.	[Kim, Beomjun; Ham, Bumsub] Yonsei Univ, Sch Elect & Elect Engn, Seoul, South Korea; [Ponce, Jean] INRIA, Paris, France; [Ponce, Jean] PSL Univ, CNRS, Dept Informat ENS, DI ENS, Paris, France	Yonsei University; Inria; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Ham, B (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul, South Korea.	beomjun.kim@yonsei.ac.kr; jean.ponce@inria.fr; bumsub.ham@yonsei.ac.kr		HAM, BUMSUB/0000-0002-3443-8161	Samsung Research Funding & Incubation Center for Future Technology [SRFC-IT1802-06]; Louis Vuitton/ENS chair on artificial intelligence; Inria/NYU collaboration agreement; French government under Agence Nationale de la Recherche as part of the "Investissements d'avenir" program [ANR-19-P3IA-0001]	Samsung Research Funding & Incubation Center for Future Technology(Samsung); Louis Vuitton/ENS chair on artificial intelligence; Inria/NYU collaboration agreement; French government under Agence Nationale de la Recherche as part of the "Investissements d'avenir" program(French National Research Agency (ANR))	The authors would like to thank Yijun Li for helpful discussion. This work was supported in part by Samsung Research Funding & Incubation Center for Future Technology (SRFC-IT1802-06), the Louis Vuitton/ENS chair on artificial intelligence, the Inria/NYU collaboration agreement, and the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073708, 10.1145/3072959.3073703]; Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Choy CB, 2016, ADV NEUR IN, V29; Cohen M. F., 2007, P IEEE C COMP VIS PA; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; De Brabandere B, 2016, ADV NEUR IN, V29; Diebel J, 2006, ADV NEURAL INF PROCE; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666; Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127; Getreuer P, 2018, IEEE INT CONF COMPUT; Gu SH, 2017, PROC CVPR IEEE, P712, DOI 10.1109/CVPR.2017.83; Ham B, 2018, IEEE T PATTERN ANAL, V40, P192, DOI 10.1109/TPAMI.2017.2669034; Ham B, 2016, PROC CVPR IEEE, P3475, DOI 10.1109/CVPR.2016.378; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22; Jaderberg M, 2015, ADV NEUR IN, V28; Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403; Kingma D.P, P 3 INT C LEARNING R; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Li YJ, 2019, IEEE T PATTERN ANAL, V41, P1909, DOI 10.1109/TPAMI.2018.2890623; Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu S, 2014, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2014.433; Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39; Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265; Odena A, 2016, DISTILL, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003]; Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423; Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720; Riegler G, 2016, P BRIT MACH VIS C; Riegler G, 2016, P EUR C COMPUT VIS; Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shen XY, 2015, IEEE I CONF COMP VIS, P3406, DOI 10.1109/ICCV.2015.389; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Simonyan K, 2014, ADV NEUR IN, V27; Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szeliski R, 2006, ACM T GRAPHIC, V25, P1135, DOI 10.1145/1141911.1142005; Tan P, 2019, ARXIV190801238; Tomasi C., 1998, P INT C COMP VIS; Vogels T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201388; Voigtlaender P., 2019, PROC CVPR IEEE; Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211; Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xu L, 2015, PR MACH LEARN RES, V37, P1669; Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208; Yan Q, 2013, IEEE I CONF COMP VIS, P1537, DOI 10.1109/ICCV.2013.194; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yu F., 2016, P ICLR 2016; Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53; Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146	70	14	14	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					579	600		10.1007/s11263-020-01386-z	http://dx.doi.org/10.1007/s11263-020-01386-z		OCT 2020	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000577044200001
J	Luo, Y; Ye, JB; Adams, RB; Li, J; Newman, MG; Wang, JZ				Luo, Yu; Ye, Jianbo; Adams, Reginald B., Jr.; Li, Jia; Newman, Michelle G.; Wang, James Z.			ARBEE: Towards Automated Recognition of Bodily Expression of Emotion in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Body language; Emotional expression; Computer vision; Crowdsourcing; Video analysis; Perception; Statistical modeling	FACIAL EXPRESSION; BODY; FACE	Humans are arguably innately prepared to comprehend others' emotional expressions from subtle body movements. If robots or computers can be empowered with this capability, a number of robotic applications become possible. Automatically recognizing human bodily expression in unconstrained situations, however, is daunting given the incomplete understanding of the relationship between emotional expressions and body movements. The current research, as a multidisciplinary effort among computer and information sciences, psychology, and statistics, proposes a scalable and reliable crowdsourcing approach for collecting in-the-wild perceived emotion data for computers to learn to recognize body languages of humans. To accomplish this task, a large and growing annotated dataset with 9876 video clips of body movements and 13,239 human characters, named Body Language Dataset (BoLD), has been created. Comprehensive statistical analysis of the dataset revealed many interesting insights. A system to model the emotional expressions based on bodily movements, named Automated Recognition of Bodily Expression of Emotion (ARBEE), has also been developed and evaluated. Our analysis shows the effectiveness of Laban Movement Analysis (LMA) features in characterizing arousal, and our experiments using LMA features further demonstrate computability of bodily expression. We report and compare results of several other baseline methods which were developed for action recognition based on two different modalities, body skeleton and raw image. The dataset and findings presented in this work will likely serve as a launchpad for future discoveries in body language understanding that will enable future robots to interact and collaborate more effectively with humans.	[Luo, Yu; Ye, Jianbo; Wang, James Z.] Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA; [Ye, Jianbo] Amazon Lab126, Sunnyvale, CA USA; [Adams, Reginald B., Jr.; Newman, Michelle G.] Penn State Univ, Dept Psychol, University Pk, PA 16802 USA; [Li, Jia] Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Luo, Y; Wang, JZ (corresponding author), Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.	yzl5709@psu.edu; yelpoo@gmail.com; radams@psu.edu; jiali@psu.edu; mgn1@psu.edu; jwang@psu.edu	Newman, Michelle/I-4370-2013; Ye, Jianbo/Q-6990-2017	Newman, Michelle/0000-0003-0873-1409; Wang, James/0000-0003-4379-4173; Luo, Yu/0000-0001-7410-4417; Ye, Jianbo/0000-0003-4612-6429	Pennsylvania State University; National Science Foundation [ACI-1548562]; GPU; Amazon.com, Inc.	Pennsylvania State University; National Science Foundation(National Science Foundation (NSF)); GPU; Amazon.com, Inc.	This material is based upon work supported in part by The Pennsylvania State University. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation Grant No. ACI-1548562 (Towns et al. 2014). The work was also supported through a GPU gift from the NVIDIA Corporation. The authors are grateful to the thousands of Amazon Mechanical Turk independent contractors for their time and dedication in providing invaluable emotion ground truth labels for the video collection. Hanjoo Kim contributed in some of the discussions. Jeremy Yuya Ong supported the data collection and visualization effort. We thank Amazon.com, Inc. for supporting the expansion of this line of research.	Abu-El-Haija S., 2016, ARXIV; [Anonymous], SCI SOCIAL VISION; [Anonymous], ARXIV12120402V1; Aristidou A, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099566; Aristidou A, 2015, COMPUT GRAPH FORUM, V34, P262, DOI 10.1111/cgf.12598; Aviezer H, 2012, SCIENCE, V338, P1225, DOI 10.1126/science.1224313; Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600; Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003; Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carmichael L, 1937, J SOC PSYCHOL, V8, P115, DOI 10.1080/00224545.1937.9919994; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen M, 2011, INT CONF CLOUD COMPU, P316, DOI 10.1109/CCIS.2011.6045082; Dael N, 2012, EMOTION, V12, P1085, DOI 10.1037/a0025737; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23; DAWID AP, 1979, J ROY STAT SOC B MET, V41, P1; de Gelder B, 2006, NAT REV NEUROSCI, V7, P242, DOI 10.1038/nrn1872; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Douglas-Cowie E, 2007, LECT NOTES COMPUT SC, V4738, P488; EKMAN P, 1993, AM PSYCHOL, V48, P384, DOI 10.1037/0003-066X.48.4.384; EKMAN P, 1992, PSYCHOL REV, V99, P550, DOI 10.1037/0033-295X.99.3.550; EKMAN P, 1986, MOTIV EMOTION, V10, P159, DOI 10.1007/BF00992253; Ekman P., 1978, FACIAL ACTION CODING, DOI [10.1037/t27734-000, DOI 10.1037/T27734-000]; Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; Gunes H, 2005, IEEE SYS MAN CYBERN, P3437; Gunes H, 2007, J NETW COMPUT APPL, V30, P1334, DOI 10.1016/j.jnca.2006.09.007; Gwet K. L., 2014, HDB INTERRATER RELIA; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Iqbal U, 2017, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2017.495; Kantorov V, 2014, PROC CVPR IEEE, P2593, DOI 10.1109/CVPR.2014.332; Kipf Thomas N, 2016, 5 INT C LEARN REPR I; Kleinsmith A, 2006, INTERACT COMPUT, V18, P1371, DOI 10.1016/j.intcom.2006.04.003; Kleinsmith A, 2011, IEEE T SYST MAN CY B, V41, P1027, DOI 10.1109/TSMCB.2010.2103557; Kosti R, 2017, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2017.212; Krakovsky M, 2018, COMMUN ACM, V61, P18, DOI 10.1145/3185521; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laban R., 1971, MASTERY MOVEMENT; Lu X, 2017, INT CONF AFFECT, P440, DOI 10.1109/ACII.2017.8273637; Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Meeren HKM, 2005, P NATL ACAD SCI USA, V102, P16518, DOI 10.1073/pnas.0507650102; Mehrabian A, 1996, CURR PSYCHOL, V14, P261, DOI 10.1007/BF02686918; Mehrabian Albert, 1980, BASIC DIMENSIONS GEN, DOI DOI 10.1145/2901739.2901752; Nicolaou MA, 2011, IEEE T AFFECT COMPUT, V2, P92, DOI 10.1109/T-AFFC.2011.9; Noroozi F, 2021, IEEE T AFFECT COMPUT, V12, P505, DOI 10.1109/TAFFC.2018.2874986; Perronnin F, 2007, PROC CVPR IEEE, P2272; Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35; Ronchi MR, 2017, IEEE I CONF COMP VIS, P369, DOI 10.1109/ICCV.2017.48; Schindler K, 2008, NEURAL NETWORKS, V21, P1238, DOI 10.1016/j.neunet.2008.05.003; Simonyan K, 2014, ADV NEUR IN, V27; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Towns J, 2014, COMPUT SCI ENG, V16, P62, DOI 10.1109/MCSE.2014.80; Wakabayashi A, 2006, PERS INDIV DIFFER, V41, P929, DOI 10.1016/j.paid.2006.03.017; Wallbott HG, 1998, EUR J SOC PSYCHOL, V28, P879, DOI 10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Xu F, 2017, IEEE T AFFECT COMPUT, V8, P254, DOI 10.1109/TAFFC.2016.2518162; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Ye JB, 2019, IEEE T AFFECT COMPUT, V10, P115, DOI 10.1109/TAFFC.2017.2678472; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22	67	14	16	7	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2020	128	1					1	25		10.1007/s11263-019-01215-y	http://dx.doi.org/10.1007/s11263-019-01215-y			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	KI6WA	33664553	Green Accepted, Green Submitted			2022-12-18	WOS:000511490100001
J	Croitoru, I; Bogolin, SV; Leordeanu, M				Croitoru, Ioana; Bogolin, Simion-Vlad; Leordeanu, Marius			Unsupervised Learning of Foreground Object Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised learning; Foreground object segmentation; Object discovery in video; Transfer learning		Unsupervised learning represents one of the most interesting challenges in computer vision today. The task has an immense practical value with many applications in artificial intelligence and emerging technologies, as large quantities of unlabeled images and videos can be collected at low cost. In this paper, we address the unsupervised learning problem in the context of segmenting the main foreground objects in single images. We propose an unsupervised learning system, which has two pathways, the teacher and the student, respectively. The system is designed to learn over several generations of teachers and students. At every generation the teacher performs unsupervised object discovery in videos or collections of images and an automatic selection module picks up good frame segmentations and passes them to the student pathway for training. At every generation multiple students are trained, with different deep network architectures to ensure a better diversity. The students at one iteration help in training a better selection module, forming together a more powerful teacher pathway at the next iteration. In experiments, we show that the improvement in the selection power, the training of multiple students and the increase in unlabeled data significantly improve segmentation accuracy from one generation to the next. Our method achieves top results on three current datasets for object discovery in video, unsupervised image segmentation and saliency detection. At test time, the proposed system is fast, being one to two orders of magnitude faster than published unsupervised methods. We also test the strength of our unsupervised features within a well known transfer learning setup and achieve competitive performance, proving that our unsupervised approach can be reliably used in a variety of computer vision tasks.	[Croitoru, Ioana; Bogolin, Simion-Vlad; Leordeanu, Marius] Romanian Acad, Inst Math, 21 Calea Grivitei, Bucharest, Romania; [Leordeanu, Marius] Univ Politehn Bucuresti, 313 Splaiul Independentei, Bucharest, Romania	Institute of Mathematics of the Romanian Academy; Romanian Academy of Sciences; University of Bucharest; Polytechnic University of Bucharest	Leordeanu, M (corresponding author), Romanian Acad, Inst Math, 21 Calea Grivitei, Bucharest, Romania.; Leordeanu, M (corresponding author), Univ Politehn Bucuresti, 313 Splaiul Independentei, Bucharest, Romania.	ioana.croi@gmail.com; vladbogolin@gmail.com; marius.leordeanu@imar.ro			UEFISCDI [PN-III-P4-ID-ERC-2016-0007, PN-III-P2-2.1-PED-2016-1842, PN-III-P1-1.1-TE-2016-2182, PN-III-P1-1.2-PCCDI-2017-0734]	UEFISCDI(Consiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii (UEFISCDI))	This work was supported by UEFISCDI, under Projects PN-III-P4-ID-ERC-2016-0007, PN-III-P2-2.1-PED-2016-1842, PN-III-P1-1.1-TE-2016-2182 and PN-III-P1-1.2-PCCDI-2017-0734.	Abadi M, 2015, P 12 USENIX S OPERAT; Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13; Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226; Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613; Bau D., 2017, INT C COMP VIS PATT; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30; Chen L, 2014, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2014.135; Cheng Jingchun, 2017, ICCV; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; CHO M, 2015, PROC CVPR IEEE, P1201, DOI DOI 10.1109/CVPR.2015.7298724; Croitoru I, 2017, IEEE I CONF COMP VIS, P4345, DOI 10.1109/ICCV.2017.465; Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909; Deselaers T, 2012, INT J COMPUT VISION, V100, P275, DOI 10.1007/s11263-012-0538-3; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Donahue J., 2016, ARXIV160509782; Dutt Jain S., 2017, IEEE C COMP VIS PATT; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Haller E, 2017, IEEE I CONF COMP VIS, P5095, DOI 10.1109/ICCV.2017.544; Hou X, 2007, 2007 IEEE C COMP VIS, V800, P1, DOI DOI 10.1109/CVPR.2007.383267; Jegou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156; JOULIN A, 2010, PROC CVPR IEEE, P1943, DOI DOI 10.1109/CVPR.2010.5539868; Joulin A, 2014, LECT NOTES COMPUT SC, V8694, P253, DOI 10.1007/978-3-319-10599-4_17; Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719; JunKoh Y., 2016, CVPR; Kalogeiton V, 2016, IEEE T PATTERN ANAL, V38, P2327, DOI 10.1109/TPAMI.2016.2551239; Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181; Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239; Kingma D.P, P 3 INT C LEARNING R; Krahenbuhl P., 2015, ARXIV151106856; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuettel D, 2012, LECT NOTES COMPUT SC, V7578, P459, DOI 10.1007/978-3-642-33786-4_34; Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35; Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79; Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471; Leordeanu M., 2005, CVPR; Leordeanu M, 2012, INT J COMPUT VISION, V96, P28, DOI 10.1007/s11263-011-0442-2; Li D, 2016, LECT NOTES COMPUT SC, V9908, P678, DOI 10.1007/978-3-319-46493-0_41; Li NY, 2015, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2015.7299158; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Liu D., 2007, CVPR; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32; Nguyen M., 2009, CVPR; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48; Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223; Parikh D., 2007, P AS C COMP VIS; Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Radenovi F., 2016, ECCV; Raiko T, 2012, P INT C ART INT STAT, V22, P924; Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592; Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789; Rochan M, 2014, LECT NOTES COMPUT SC, V8887, P172, DOI 10.1007/978-3-319-14249-4_17; ROCK I, 1990, SCI AM, V263, P84, DOI 10.1038/scientificamerican1290-84; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253; Rubio Jose C, 2012, ACCV; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Siva P, 2013, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2013.416; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Stretcu Otilia, 2015, BMVC, V1, P4; Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190; Thomee B., 2015, ARXIV150301817; Tokmakov P., 2016, ECCV, V1, P6; Tokmakov P, 2017, PROC CVPR IEEE, P531, DOI 10.1109/CVPR.2017.64; Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256; Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530; Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Yu F., 2016, P ICLR 2016; Zhang D, 2017, P AMER CONTR CONF, P4042, DOI 10.23919/ACC.2017.7963575; Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165; Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360	88	14	15	4	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2019	127	9					1279	1302		10.1007/s11263-019-01183-3	http://dx.doi.org/10.1007/s11263-019-01183-3			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IL9YV		Green Submitted, hybrid			2022-12-18	WOS:000477642300006
J	Chen, JC; Ranjan, R; Sankaranarayanan, S; Kumar, A; Chen, CH; Patel, VM; Castillo, CD; Chellappa, R				Chen, Jun-Cheng; Ranjan, Rajeev; Sankaranarayanan, Swami; Kumar, Amit; Chen, Ching-Hui; Patel, Vishal M.; Castillo, Carlos D.; Chellappa, Rama			Unconstrained Still/Video-Based Face Verification with Deep Convolutional Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Face detection/association; Fiducial detection; Face verification; Metric learning	TRACKING; CASCADE	Over the last 5 years, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements for object detection and recognition problems. This has been made possible due to the availability of large annotated datasets, a better understanding of the non-linear mapping between input images and class labels as well as the affordability of GPUs. In this paper, we present the design details of a deep learning system for unconstrained face recognition, including modules for face detection, association, alignment and face verification. The quantitative performance evaluation is conducted using the IARPA Janus Benchmark A (IJB-A), the JANUS Challenge Set 2 (JANUS CS2), and the Labeled Faces in the Wild (LFW) dataset. The IJB-A dataset includes real-world unconstrained faces of 500 subjects with significant pose and illumination variations which are much harder than the LFW and Youtube Face datasets. JANUS CS2 is the extended version of IJB-A which contains not only all the images/frames of IJB-A but also includes the original videos. Some open issues regarding DCNNs for face verification problems are then discussed.	[Chen, Jun-Cheng; Ranjan, Rajeev; Sankaranarayanan, Swami; Kumar, Amit; Chen, Ching-Hui; Castillo, Carlos D.; Chellappa, Rama] Univ Maryland, AV Williams 4455, College Pk, MD 20740 USA; [Patel, Vishal M.] Rutgers State Univ, Dept Elect & Comp Engn, 508 CoRE 94 Brett Rd, Piscataway, NJ 08854 USA	University System of Maryland; University of Maryland College Park; Rutgers State University New Brunswick	Chen, JC (corresponding author), Univ Maryland, AV Williams 4455, College Pk, MD 20740 USA.	pullpull@cs.umd.edu	Chen, Jun-Cheng/ABC-4218-2020; Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/AAV-8690-2020; Sankaranarayanan, Swami/AFP-9228-2022; Chellappa, Rama/B-6573-2012	Chen, Jun-Cheng/0000-0002-0209-8932; Castillo, Carlos/0000-0001-5308-4824	Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA RD [2014-14071600012]	Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA RD	This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. We thank professor Alice O'Toole for carefully reading the manuscript and suggesting improvements in the presentation of this work.	AbdAlmageed W, 2016, IEEE WINT CONF APPL; Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Ahuja R. K., 1993, NETWORK FLOWS THEORY; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2014, ARXIV PREPRINT ARXIV; Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737; Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159; Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23; Bodla N., 2017, IEEE WINT C APPL COM; Breitenstein MD, 2009, IEEE I CONF COMP VIS, P1515, DOI 10.1109/ICCV.2009.5459278; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao X., 2014, US Patent App, Patent No. [13/ 728,584, 13728584]; Chen D, 2014, LECT NOTES COMPUT SC, V8694, P109, DOI 10.1007/978-3-319-10599-4_8; Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41; Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389; Chen J., 2015, ARXIV150801722; Chen J, 2015, IEEE ICC, P1801, DOI 10.1109/ICC.2015.7248586; Chen JC, 2015, INT CONF BIOMETR THE; Chen YC, 2012, LECT NOTES COMPUT SC, V7577, P766, DOI 10.1007/978-3-642-33783-3_55; Cheney J, 2015, INT CONF BIOMETR, P229, DOI 10.1109/ICB.2015.7139089; Comaschi F, 2015, 2015 12TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS); Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cristinacce D., 2006, P BRIT MACH VIS C, V3, P929; Crosswhite N, 2016, ARXIV160303958; Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042; Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094; Donahue J, 2013, ARXIV 1310 1531; Du M., 2012, EUR C COMP VIS ECCV; Duffner S, 2013, IEEE T IMAGE PROCESS, V22, P272, DOI 10.1109/TIP.2012.2210238; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Farfade SS, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P643, DOI 10.1145/2671188.2749408; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Giryes R., 2014, ARXIV14125896; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197; Haeffele B. D., 2015, ARXIV PREPRINT ARXIV; HASSNER T, 2015, PROC CVPR IEEE, P4295, DOI DOI 10.1109/CVPR.2015.7299058; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Huang C, 2008, LECT NOTES COMPUT SC, V5303, P788, DOI 10.1007/978-3-540-88688-4_58; Huang G.B., 2008, WORKSH FAC REAL LIF; Jain V., 2010, UMCS2010009; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803; Koestinger M., 2011, ICCV WORKSH, DOI [10.1109/ICCVW.2011.6130513, DOI 10.1109/ICCVW.2011.6130513]; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar A., 2016, CORR; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Li HX, 2013, IEEE I CONF COMP VIS, P793, DOI 10.1109/ICCV.2013.103; Li JG, 2013, PROC CVPR IEEE, P3468, DOI 10.1109/CVPR.2013.445; Liao SC, 2016, IEEE T PATTERN ANAL, V38, P211, DOI 10.1109/TPAMI.2015.2448075; Long MS, 2015, PR MACH LEARN RES, V37, P97; Lui YM, 2010, IEEE T SYST MAN CY A, V40, P437, DOI 10.1109/TSMCA.2010.2041655; Mallat S, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0203; Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35; Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47; Mignon A, 2012, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2012.6247987; National institute of standards and technology (NIST), IARPA JAN BENCHM A P; Nguyen Hieu V, 2010, P AS C COMP VIS, P709, DOI DOI 10.1007/978-3-642-19309-5_55; Parkhi Omkar M., 2015, BRIT MACH VIS C; Ranjan R., 2016, ARXIV161100851; Ranjan R., 2017, ARXIV PREPRINT ARXIV; Ranjan R., 2016, ARXIV160301249; Ranjan Rajeev, 2017, Proceedings of the Indian National Science Academy Part B Biological Sciences, V87, P377, DOI 10.1007/s40011-015-0618-6; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Roth M., 2012, INT C PATT REC ICPR; RoyChowdhury A, 2016, IEEE WINT CONF APPL; Sankaranarayanan S., 2016, P IEEE INT C BIOMETR, P1; Sankaranarayanan S., 2016, ARXIV160203418; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Sun Y., 2015, ARXIV PREPRINT ARXIV; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Taigman Y., 2009, BMVC, P1; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang D., 2015, ARXIV150707242; Wang P, 2008, IEEE T IMAGE PROCESS, V17, P1189, DOI 10.1109/TIP.2008.924287; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wolf L, 2009, IEEE I CONF COMP VIS, P897, DOI 10.1109/ICCV.2009.5459323; Xiong L., 2017, ARXIV PREPRINT ARXIV; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yan JJ, 2014, IMAGE VISION COMPUT, V32, P790, DOI 10.1016/j.imavis.2013.12.004; Yang B, 2015, IEEE I CONF COMP VIS, P82, DOI 10.1109/ICCV.2015.18; Yang J., 2016, ARXIV160305474; Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yoon JH, 2015, IEEE CUST INTEGR CIR; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zhu S., 2015, IEEE CONFERENCE ON C; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	105	14	15	0	30	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		272	291		10.1007/s11263-017-1029-3	http://dx.doi.org/10.1007/s11263-017-1029-3			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA		Green Submitted			2022-12-18	WOS:000425619100008
J	Lynen, S; Bosse, M; Siegwart, R				Lynen, Simon; Bosse, Michael; Siegwart, Roland			Trajectory-Based Place-Recognition for Efficient Large Scale Localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Localization; Loop-closure; Place recognition	PROBABILISTIC LOCALIZATION; FAB-MAP; SPACE; SLAM	Place recognition is a core competency for any visual simultaneous localization and mapping system. Identifying previously visited places enables the creation of globally accurate maps, robust relocalization, and multi-user mapping. To match one place to another, most state-of-the-art approaches must decide a priori what constitutes a place, often in terms of how many consecutive views should overlap, or how many consecutive images should be considered together. Unfortunately, such threshold dependencies limit their generality to different types of scenes. In this paper, we present a placeless place recognition algorithm using a novel match-density estimation technique that avoids heuristically discretizing the space. Instead, our approach considers place recognition as a problem of continuous matching between image streams, automatically discovering regions of high match density that represent overlapping trajectory segments. The algorithm uses well-studied statistical tests to identify the relevant matching regions which are subsequently passed to an absolute pose algorithm to recover the geometric alignment. We demonstrate the efficiency and accuracy of our methodology on three outdoor sequences, including a comprehensive evaluation against ground-truth from publicly available datasets that shows our approach outperforms several state-of-the-art algorithms for place recognition. Furthermore we compare our overall algorithm to the currently best performing system for global localization and show how we outperform the approach on challenging indoor and outdoor datasets.	[Lynen, Simon; Bosse, Michael; Siegwart, Roland] Swiss Fed Inst Technol, Autonomous Syst Lab, Leonhardstr 21, CH-8092 Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Lynen, S (corresponding author), Swiss Fed Inst Technol, Autonomous Syst Lab, Leonhardstr 21, CH-8092 Zurich, Switzerland.	simon.lynen@mavt.ethz.ch; michael.bosse@mavt.ethz.ch; roland.siegwart@mavt.ethz.ch	Siegwart, Roland/A-4495-2008	Siegwart, Roland/0000-0002-2760-7983				Agarwal S., CERES SOLVER; Alahi Alexandre, 2012, P EUR C COMP VIS ECC; Arandjelovic R., 2013, 2013 IEEE C COMP VIS; Arth Clemens, 2009, P INT S MIX AUGM REA; Babenko Artem, 2012, P IEEE C COMP VIS PA; Bay H., 2006, ECCV; Bay H., 2006, IEEE EUR C COMP VIS; Blanco-Claraco JL, 2014, INT J ROBOT RES, V33, P207, DOI 10.1177/0278364913507326; Bosse M, 2009, ROBOT AUTON SYST, V57, P1211, DOI 10.1016/j.robot.2009.07.009; Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961; Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483; Darling D. A., 1957, ANN MATH STAT; Dong Zilong, 2009, P INT C COMP VIS ICC; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158; Ge T., 2014, IEEE T PATTERN ANAL, V33, P117; Glover A. J., 2010, IEEE INT C ROB AUT I; Grunert J.A., 1841, GRUNERTS ARCHIV MATH, V1, P238; Hesch J. A., 2014, INT J ROBOTICS RES; Hesch Joel A., 2011, P INT C COMP VIS ICC; Irschara A, 2009, P IEEE C COMP VIS PA; Jegou H., IEEE T PATTERN ANAL; Jegou H, 2008, P EUR C COMP VIS ECC; Johns E, 2014, INT J COMPUT VISION, V106, P297, DOI 10.1007/s11263-013-0648-6; Kendall A., 2015, P IEEE INT C COMP VI; Kneip L, 2013, IEEE I CONF COMP VIS, P2352, DOI 10.1109/ICCV.2013.292; Kuiper N., 1960, P KON NED AK WET; Leutenegger S., 2011, P INT C COMP VIS ICC; Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813; Li Y., 2010, P EUR C COMP VIS ECC; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lynen S., 2014, 3DV; Maddern W, 2012, INT J ROBOT RES, V31, P429, DOI 10.1177/0278364912438273; Mei Christopher, 2010, P IEEE RSJ C INT ROB; Middelberg Sven, 2014, P EUR C COMP VIS ECC; Milford M. J., 2012, IEEE INT C ROB AUT I; Mourikis AI, 2009, IEEE T ROBOT, V25, P264, DOI 10.1109/TRO.2009.2012342; Murphy Liz, 2014, P IEEE INT C ROB AUT; Naseer Tayyab, 2014, AAAI; Neyman J., 1992, PROBLEM THEMOST EFFI; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; NISTER D, 2003, P INT C COMP VIS ICC; Nister D., 2006, P 2006 IEEE COMP SOC; Pepperell E, 2014, IEEE INT CONF ROBOT, P1612, DOI 10.1109/ICRA.2014.6907067; Philbin J, 2007, CVPR; Sattler T., 2011, P INT C COMP VIS ICC; Sattler T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.76; Sattler Torsten, 2012, P EUR C COMP VIS ECC; Se S, 2005, IEEE T ROBOT, V21, P364, DOI 10.1109/TRO.2004.839228; Sivic J., 2003, IEEE INT C COMP VIS; Smith M., 2009, INT J ROBOTICS RES; Stewenius H, 2006, ISPRS J PHOTOGRAMM, V60, P284, DOI 10.1016/j.isprsjprs.2006.03.005; Stewenius Henrik, 2012, P EUR C COMP VIS ECC; Stumm Elena, 2013, P IEEE RSJ C INT ROB; Sunderhauf N., 2011, 2011 IEEE RSJ INT C; Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Svarm L, 2014, P IEEE C COMP VIS PA; Trzcinski T, 2012, PATTERN RECOGN LETT, V33, P2173, DOI 10.1016/j.patrec.2012.08.006; Wang O, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601208; Wendel A, 2011, IEEE INT CONF ROBOT	60	14	14	1	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2017	124	1					49	64		10.1007/s11263-016-0947-9	http://dx.doi.org/10.1007/s11263-016-0947-9			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FA2JD					2022-12-18	WOS:000405265900003
J	Agudo, A; Moreno-Noguer, F				Agudo, Antonio; Moreno-Noguer, Francesc			Combining Local-Physical and Global-Statistical Models for Sequential Deformable Shape from Motion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sequential non-rigid structure from motion; Particle dynamics; Bundle adjustment; Low-rank models	STRUCTURE-FROM-MOTION	In this paper, we simultaneously estimate camera pose and non-rigid 3D shape from a monocular video, using a sequential solution that combines local and global representations. We model the object as an ensemble of particles, each ruled by the linear equation of the Newton's second law of motion. This dynamic model is incorporated into a bundle adjustment framework, in combination with simple regularization components that ensure temporal and spatial consistency. The resulting approach allows to sequentially estimate shape and camera poses, while progressively learning a global low-rank model of the shape that is fed back into the optimization scheme, introducing thus, global constraints. The overall combination of local (physical) and global (statistical) constraints yields a solution that is both efficient and robust to several artifacts such as noisy and missing data or sudden camera motions, without requiring any training data at all. Validation is done in a variety of real application domains, including articulated and non-rigid motion, both for continuous and discontinuous shapes. Our on-line methodology yields significantly more accurate reconstructions than competing sequential approaches, being even comparable to the more computationally demanding batch methods.	[Agudo, Antonio; Moreno-Noguer, Francesc] UPC, CSIC, Inst Robot & Informat Ind, Llorens Artigas 4-6, Barcelona 08028, Spain	Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i Informatica Industrial (IRII); Universitat Politecnica de Catalunya	Agudo, A (corresponding author), UPC, CSIC, Inst Robot & Informat Ind, Llorens Artigas 4-6, Barcelona 08028, Spain.	aagudo@iri.upc.edu; fmoreno@iri.upc.edu	Agudo, Antonio/C-5147-2017	Agudo, Antonio/0000-0001-6845-4998	Spanish Ministry of Science and Innovation under Project RobInstruct [TIN2014-58178-R]; Spanish MECD [FPU12/04886]; ERA-net CHISTERA Project VISEN [PCIN-2013-047]; ERA-net CHISTERA Project I-DRESS [PCIN-2015-147]	Spanish Ministry of Science and Innovation under Project RobInstruct(Ministry of Science and Innovation, Spain (MICINN)); Spanish MECD(Spanish Government); ERA-net CHISTERA Project VISEN; ERA-net CHISTERA Project I-DRESS	We would like to thank the anonymous reviewers for their insights and comments that have significantly contributed to improving this manuscript. This work has been partially supported by the Spanish Ministry of Science and Innovation under Project RobInstruct TIN2014-58178-R; by a scholarship FPU12/04886 from the Spanish MECD; and by the ERA-net CHISTERA Projects VISEN PCIN-2013-047 and I-DRESS PCIN-2015-147. The authors also thank Chris Russell, Lourdes Agapito and Paulo Gotardo for making their data available.	Agudo A., 2014, BRIT MACH VIS C; Agudo A, 2016, IEEE T PATTERN ANAL, V38, P979, DOI 10.1109/TPAMI.2015.2469293; Agudo A, 2015, PROC CVPR IEEE, P2179, DOI 10.1109/CVPR.2015.7298830; Agudo A, 2014, PROC CVPR IEEE, P1558, DOI 10.1109/CVPR.2014.202; Agudo A, 2012, PROC CVPR IEEE, P1418; AKHTER I, 2008, NEURAL INFORM PROCES, V41, P41; Baraff D., 1989, Computer Graphics, V23, P223, DOI 10.1145/74334.74356; Bartoli, 2014, BRIT MACH VIS C; Brand M, 2001, PROC CVPR IEEE, P456; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Brubaker MA, 2009, IEEE I CONF COMP VIS, P2389, DOI 10.1109/ICCV.2009.5459407; Dai YC, 2012, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2012.6247905; Del Bue A, 2006, P IEEE C COMP VIS PA, V1, P1191; Fayad J, 2010, LECT NOTES COMPUT SC, V6314, P297, DOI 10.1007/978-3-642-15561-1_22; Garg R, 2013, PROC CVPR IEEE, P1272, DOI 10.1109/CVPR.2013.168; Gotardo P. F. U., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3065, DOI 10.1109/CVPR.2011.5995560; Gotardo PFU, 2011, IEEE I CONF COMP VIS, P802, DOI 10.1109/ICCV.2011.6126319; Jongwoo Lim, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3489, DOI 10.1109/CVPR.2011.5995511; Koh W., 2014, P ACM SIGGRAPH EUR S, P159; Lee M, 2013, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR.2013.169; Ma Y, 2001, INT J COMPUT VISION, V44, P219, DOI 10.1023/A:1012276232049; Maier-Hein L, 2014, IEEE T MED IMAGING, V33, P1913, DOI 10.1109/TMI.2014.2325607; Marques M., 2008, P IEEE WORKSH MOT VI, P1; METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727; Moreno-Noguer F, 2011, PROC CVPR IEEE, P1289, DOI 10.1109/CVPR.2011.5995532; Paladini Marco, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2898, DOI 10.1109/CVPRW.2009.5206602; Paladini M, 2010, LECT NOTES COMPUT SC, V6312, P15, DOI 10.1007/978-3-642-15552-9_2; Park HS, 2010, LECT NOTES COMPUT SC, V6313, P158; Popovic Z, 1999, COMP GRAPH, P11, DOI 10.1145/311535.311536; Russell C., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3009, DOI 10.1109/CVPR.2011.5995383; Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38; Salzmann M, 2011, IEEE I CONF COMP VIS, P2064, DOI 10.1109/ICCV.2011.6126480; Sayd P., 2008, P IEEE C COMP VIS PA, P1; Shaji Appu, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563071; Tao LL, 2013, COMPUT VIS IMAGE UND, V117, P1287, DOI 10.1016/j.cviu.2013.03.005; Taylor J, 2010, PROC CVPR IEEE, P2761, DOI 10.1109/CVPR.2010.5540002; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Valmadre J, 2012, PROC CVPR IEEE, P1394, DOI 10.1109/CVPR.2012.6247826; Varol A, 2009, IEEE I CONF COMP VIS, P1811, DOI 10.1109/ICCV.2009.5459403; Vondrak M., 2008, P IEEE C COMP VIS PA, P1; Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9	42	14	14	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		371	387		10.1007/s11263-016-0972-8	http://dx.doi.org/10.1007/s11263-016-0972-8			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS		Green Submitted			2022-12-18	WOS:000398162200010
J	Hara, K; Chellappa, R				Hara, Kota; Chellappa, Rama			Growing Regression Tree Forests by Classification for Continuous Object Pose Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose estimation; Direction estimation; Orientation estimation; Regression tree; Random regression forest; Decision tree; Mean shift	MEAN SHIFT	We propose a novel node splitting method for regression trees and incorporate it into the random regression forest framework. Unlike traditional binary splitting, where the splitting rule is selected from a predefined set of binary splitting rules via trial-and-error, the proposed node splitting method first finds clusters in the training data which at least locally minimize the empirical loss without considering the input space. Then splitting rules which preserve the found clusters as much as possible, are determined by casting the problem as a classification problem. Consequently, our new node splitting method enjoys more freedom in choosing the splitting rules, resulting in more efficient tree structures. In addition to the algorithm for the ordinary Euclidean target space, we present a variant which can naturally deal with a circular target space by the proper use of circular statistics. In order to deal with challenging, ambiguous image-based pose estimation problems, we also present a voting-based ensemble method using the mean shift algorithm. Furthermore, to address data imbalanceness problems present in some of the datasets, we propose a bootstrap sampling method using a sample weighting technique. We apply the proposed random regression forest algorithm to head pose estimation, car direction estimation and pedestrian orientation estimation tasks, and demonstrate its competitive performance.	[Hara, Kota; Chellappa, Rama] Univ Maryland, Ctr Automat Res, UMIACS, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Hara, K (corresponding author), Univ Maryland, Ctr Automat Res, UMIACS, College Pk, MD 20742 USA.	kotahara@umiacs.umd.edu; rama@umiacs.umd.edu	Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/B-6573-2012		MURI Grant from the US Office of Naval Research [N00014-10-1-0934]	MURI Grant from the US Office of Naval Research	This research was supported by a MURI Grant from the US Office of Naval Research under N00014-10-1-0934.	Andriluka M., 2010, CVPR 2010; Bailly K., 2009, INT C COMP AN IM PAT; Baltieri D., 2012, EUR C COMP VIS; Berzal F, 2004, INFORM SCIENCES, V165, P73, DOI 10.1016/j.ins.2003.09.018; Bissacco A., 2007, 2007 IEEE C COMP VIS, P1, DOI [10.1109/CVPR.2007.383129, DOI 10.1109/CVPR.2007.383129]; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2017, CLASSIFICATION REGRE; Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chang-Chien SJ, 2012, SOFT COMPUT, V16, P1043, DOI 10.1007/s00500-012-0802-z; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Chen C., 2004, USING RANDOM FOREST; Chen C., 2011, INT C ADV VID SIGN B; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; CHOU PA, 1991, IEEE T PATTERN ANAL, V13, P340, DOI 10.1109/34.88569; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Criminisi A., 2013, DECISION FORESTCOM; Criminisi A, 2011, LECT NOTES COMPUT SC, V6533, P106, DOI 10.1007/978-3-642-18421-5_11; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dantone M., 2012, 2012 IEEE C COMP VIS; Dobra A., 2002, P 8 ACM SIGKDD INT C; Dollar P., 2010, 2010 IEEE C COMP VIS; Domingos P., 1999, P 50 ACM SIGKDD INT; Drucker H, 1997, ADV NEUR IN, V9, P155; Drummond C., 2003, WORKSH LEARN IMB DAT, V11, P1; DUIN RPW, 1976, IEEE T COMPUT, V25, P1175, DOI 10.1109/TC.1976.1674577; Enzweiler M., 2010, CVPR 2010; Fanelli G., 2011, 2011 IEEE C COMP VIS; FAYYAD UM, 1993, IJCAI-93, VOLS 1 AND 2, P1022; Fenzi M., 2015, P IEEE INT C COMP VI; Fenzi M., 2013, P IEEE C COMP VIS PA; Fenzi M., 2014, BRIT MACH VIS C; Fisher N.I., 1995, STAT ANAL CIRCULAR D; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Gaile G. L., 1980, DIRECTIONAL STAT CON; Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755; Gandhi T., 2008, INT VEH S; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Girshick R., 2011, 2011 IEEE INT C COMP; Goto K., 2011, IEEE INT VEH S IV; Gourier N., 2004, PROC ICPR INT WORKSH, P1; HABBEMA JDF, 1977, TECHNOMETRICS, V19, P487, DOI 10.2307/1267890; Haj M. A., 2012, 2012 IEEE C COMP VIS; Hara K., 2014, EUR C COMP VIS ECCV; Hara K., 2013, P IEEE C COMP VIS PA; He K., 2014, EUR C COMP VIS ECCV; Herdtweck C., 2013, INT VEH S IVS; Ho H. T., 2012, 2012 19 IEEE INT C I; Huang C., 2010, 2010 20 INT C PATT R; Kafai M., 2010, CVPR WORKSH; KASHYAP RL, 1977, IEEE T AUTOMAT CONTR, V22, P715, DOI 10.1109/TAC.1977.1101594; Kobayashi T., 2010, 2010 20 INT C PATT R; Kubat M., 1997, P ECML 97 10 EUR C M; LOH WY, 1988, J AM STAT ASSOC, V83, P715, DOI 10.2307/2289295; Mardia K.V., 2000, DIRECTIONAL STAT; Nakajima C, 2003, PATTERN RECOGN, V36, P1997, DOI 10.1016/S0031-3203(03)00061-X; Orozco J., 2009, P BRIT MACH VIS C BM; Ozuysal M., 2009, 2009 IEEE C COMP VIS; Pazzani Michael J., 1994, P 11 INT C MACHINE L, P217, DOI DOI 10.1016/B978-1-55860-335-6.50034-9; Pelleg D., 2000, P 17 INT C MACH LEAR, DOI DOI 10.1038/S41598-021-86770-6; Redondo-cabrera C., 2014, 25 BRIT MACH VIS C B; Rosipal R, 2002, J MACH LEARN RES, V2, P97, DOI 10.1162/15324430260185556; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Shimizu H., 2004, INT VEH S IVS; Sun M, 2012, PROC CVPR IEEE, P3394, DOI 10.1109/CVPR.2012.6248079; Tao J, 2013, ICCV WORKSH; Torgo L., 2013, PORT C ART INT; Torgo L., 1996, BRAZ S ART INT; Torki M., 2011, 2011 INT C COMP VIS; Vapnik V., 1998, STAT LEARNING THEORY; Weiss SM, 1995, J ARTIF INTELL RES, V3, P383, DOI 10.1613/jair.199; Wu KL, 2007, PATTERN RECOGN, V40, P3035, DOI 10.1016/j.patcog.2007.02.006; Yan Y, 2013, IEEE I CONF COMP VIS, P1177, DOI 10.1109/ICCV.2013.150; Yang L., 2014, EUR C COMP VIS; Zhang H., 2013, JOINT OBJECT POSE RE; Zhao Guangzhe, 2012, Journal of Electronics (China), V29, P72, DOI 10.1007/s11767-012-0814-y; Zhen X., 2015, P IEEE C COMP VIS PA	78	14	14	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		292	312		10.1007/s11263-016-0942-1	http://dx.doi.org/10.1007/s11263-016-0942-1			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS					2022-12-18	WOS:000398162200007
J	Singh, N; Hinkle, J; Joshi, S; Fletcher, PT				Singh, Nikhil; Hinkle, Jacob; Joshi, Sarang; Fletcher, P. Thomas			Hierarchical Geodesic Models in Diffeomorphisms	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Longitudinal modeling; Diffeomorphisms; Mixed-effects modeling; LDDMM	REGRESSION	Hierarchical linear models (HLMs) are a standard approach for analyzing data where individuals are measured repeatedly over time. However, such models are only applicable to longitudinal studies of Euclidean data. This paper develops the theory of hierarchical geodesic models (HGMs), which generalize HLMs to the manifold setting. Our proposed model quantifies longitudinal trends in shapes as a hierarchy of geodesics in the group of diffeomorphisms. First, individuallevel geodesics represent the trajectory of shape changes within individuals. Second, a grouplevel geodesic represents the average trajectory of shape changes for the population. Our proposed HGM is applicable to longitudinal data from unbalanced designs, i.e., varying numbers of timepoints for individuals, which is typical in medical studies. We derive the solution of HGMs on diffeomorphisms to estimate individuallevel geodesics, the group geodesic, and the residual diffeomorphisms. We also propose an efficient parallel algorithm that easily scales to solve HGMs on a large collection of 3D images of several individuals. Finally, we present an effective model selection procedure based on cross validation. We demonstrate the effectiveness of HGMs for longitudinal analysis of synthetically generated shapes and 3D MRI brain scans.	[Singh, Nikhil] Univ N Carolina, Chapel Hill, NC USA; [Hinkle, Jacob; Joshi, Sarang; Fletcher, P. Thomas] Univ Utah, Salt Lake City, UT USA	University of North Carolina; University of North Carolina Chapel Hill; Utah System of Higher Education; University of Utah	Singh, N (corresponding author), Univ N Carolina, Chapel Hill, NC USA.	nikhilpratapsingh@gmail.com	Hinkle, Jacob/AAM-6795-2021; singh, Nikhil/AAX-5380-2020	Hinkle, Jacob/0000-0002-7751-1760; 	NIH [U01NS082086, 5R01EB007688, U01AG024904, R01MH084795, P41 RR023953]; NSF [1054057]; National Institutes of Health [U01 AG024904]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This research is supported by NIH Grants U01NS082086, 5R01EB007688, U01AG024904, R01MH084795 and P41 RR023953, and NSF Grant 1054057. National Institutes of Health Grant U01 AG024904.	Adams J. F., 1969, LECT LIE GROUPS; AMIT Y, 1991, J AM STAT ASSOC, V86, P376, DOI 10.2307/2290581; ARNOLD V, 1966, ANN I FOURIER, V16, P319, DOI 10.5802/aif.233; Burke SN, 2006, NAT REV NEUROSCI, V7, P30, DOI 10.1038/nrn1809; Chevalley C., 1999, THEORY OF LIE GROUPS, V1; Davis BC, 2010, INT J COMPUT VISION, V90, P255, DOI 10.1007/s11263-010-0367-1; Durrleman S, 2009, LECT NOTES COMPUT SC, V5761, P297, DOI 10.1007/978-3-642-04268-3_37; Fishbaugh J., 2012, MICCAI; Fitzmaurice G.M., 2012, APPL LONGITUDINAL AN, V998; Fletcher PT, 2013, INT J COMPUT VISION, V105, P171, DOI 10.1007/s11263-012-0591-y; Fox NC, 2004, LANCET, V363, P392, DOI 10.1016/S0140-6736(04)15441-X; Grenander U, 1998, Q APPL MATH, V56, P617, DOI 10.1090/qam/1668732; Hinkle J, 2012, LECT NOTES COMPUT SC, V7574, P1, DOI 10.1007/978-3-642-33712-3_1; Hong Y, 2012, LECT NOTES COMPUT SC, V7512, P197, DOI 10.1007/978-3-642-33454-2_25; LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876; Lorenzi M., 2012, MICCAI WORKSH NOV IM, P145; Lorenzi M., 2011, MICCAI 2011; Marcus DS, 2010, J COGNITIVE NEUROSCI, V22, P2677, DOI 10.1162/jocn.2009.21407; Miller M, 1997, Stat Methods Med Res, V6, P267, DOI 10.1191/096228097673360480; Miller MI, 2006, J MATH IMAGING VIS, V24, P209, DOI 10.1007/s10851-005-3624-0; Miller MI, 2004, NEUROIMAGE, V23, pS19, DOI 10.1016/j.neuroimage.2004.07.021; Muralidharan P, 2012, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2012.6247780; Niethammer M, 2011, LECT NOTES COMPUT SC, V6892, P655, DOI 10.1007/978-3-642-23629-7_80; Pinheiro J. C., 1995, J COMPUTATIONAL GRAP, V4, P12, DOI [DOI 10.2307/1390625, 10.1080/10618600.1995.10474663, DOI 10.1080/10618600.1995.10474663]; Raz N, 2006, NEUROSCI BIOBEHAV R, V30, P730, DOI 10.1016/j.neubiorev.2006.07.001; Reuter M, 2012, NEUROIMAGE, V61, P1402, DOI 10.1016/j.neuroimage.2012.02.084; Reuter M, 2010, NEUROIMAGE, V53, P1181, DOI 10.1016/j.neuroimage.2010.07.020; Singh N., 2014, P 2014 IEEE INT S BI; Singh N, 2014, LECT NOTES COMPUT SC, V8674, P121, DOI 10.1007/978-3-319-10470-6_16; Singh Nikhil, 2013, Inf Process Med Imaging, V23, P560, DOI 10.1007/978-3-642-38868-2_47; Singh N, 2013, I S BIOMED IMAGING, P1219; Sowell ER, 2003, NAT NEUROSCI, V6, P309, DOI 10.1038/nn1008; THOMPSON D. W., 1942, On growth and form.; Thompson P. M., 2002, Computing and Visualization in Science, V5, P13, DOI 10.1007/s00791-002-0084-6; Winer B. J., 1962, STAT PRINCIPLES EXPT, DOI [10.1037/11774-000, DOI 10.1037/11774-000]; Younes L, 2010, APPL MATH SCI, V171, P1, DOI 10.1007/978-3-642-12055-8; Younes L, 2008, J MATH IMAGING VIS, V32, P41, DOI 10.1007/s10851-008-0074-5; Younes L, 2009, NEUROIMAGE, V45, pS40, DOI 10.1016/j.neuroimage.2008.10.050; Zhang Miaomiao, 2013, Inf Process Med Imaging, V23, P37, DOI 10.1007/978-3-642-38868-2_4	41	14	14	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2016	117	1					70	92		10.1007/s11263-015-0849-2	http://dx.doi.org/10.1007/s11263-015-0849-2			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DF2FG		Green Submitted			2022-12-18	WOS:000371156500004
J	Bermudez-Cameo, J; Lopez-Nicolas, G; Guerrero, JJ				Bermudez-Cameo, J.; Lopez-Nicolas, G.; Guerrero, J. J.			Automatic Line Extraction in Uncalibrated Omnidirectional Cameras with Revolution Symmetry	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Omnidirectional vision; Line features; Uncalibrated images; Fisheye; Catadioptric; Single-image; Matlab toolbox	SELF-CALIBRATION; WIDE-ANGLE; DISTORTION; IMAGES; MODEL	Revolution symmetry is a realistic assumption for modelling the majority of catadioptric and dioptric cameras. In central systems it can be described by a projection model based on radially symmetric distortion. In these systems straight lines are projected on curves called line-images. These curves have in general more than two degrees of freedom and their shape strongly depends on the particular camera configuration. Therefore, the existing line-extraction methods for this kind of omnidirectional cameras require the camera calibration by contrast with the perspective case where the calibration is not involved in the shape of the projected line-image. However, this drawback can be considered as an advantage because the shape of the line-images can be used for self-calibration. In this paper, we present a novel method to extract line-images in uncalibrated omnidirectional images which is valid for radially symmetric central systems. In this method we propose using the plumb-line constraint to find closed form solutions for different types of camera systems, dioptric or catadioptric. The inputs of the proposed method are points belonging to the line-images and their intensity gradient. The gradient information allows to reduce the number of points needed in the minimal solution improving the result and the robustness of the estimation. The scheme is used in a line-image extraction algorithm to obtain lines from uncalibrated omnidirectional images without any assumption about the scene. The algorithm is evaluated with synthetic and real images showing good performance. The results of this work have been implemented in an open source Matlab toolbox for evaluation and research purposes.	[Bermudez-Cameo, J.; Lopez-Nicolas, G.; Guerrero, J. J.] Univ Zaragoza, I3A, Zaragoza, Spain	University of Zaragoza	Bermudez-Cameo, J (corresponding author), Univ Zaragoza, I3A, Zaragoza, Spain.	bermudez@unizar.es; gonlopez@unizar.es; josechu.guerrero@unizar.es	Guerrero, Jose J/K-5435-2014	Guerrero, Jose J/0000-0001-5209-2267; Bermudez-Cameo, Jesus/0000-0002-8479-1748; Lopez Nicolas, Gonzalo/0000-0001-9347-5969	Spanish Project VINEA [DPI2012-31781]; FEDER funds; FPU program [AP2010-3849]	Spanish Project VINEA; FEDER funds(European Commission); FPU program(Spanish Government)	This work was supported by the Spanish Project VINEA DPI2012-31781 and FEDER funds. First author was supported by the FPU program AP2010-3849. Thanks to J. P. Barreto from ISR Coimbra for the set of high resolution paracatadioptric images.	Aleman-Flores M, 2014, PATTERN RECOGN LETT, V36, P261, DOI 10.1016/j.patrec.2013.06.020; Alvarez L, 2009, J MATH IMAGING VIS, V35, P36, DOI 10.1007/s10851-009-0153-2; Baker S, 1999, INT J COMPUT VISION, V35, P175, DOI 10.1023/A:1008128724364; Barreto JP, 2006, COMPUT VIS IMAGE UND, V101, P151, DOI 10.1016/j.cviu.2005.07.002; Barreto JP, 2005, IEEE T PATTERN ANAL, V27, P1327, DOI 10.1109/TPAMI.2005.163; Bazin JC, 2010, COMPUT VIS IMAGE UND, V114, P254, DOI 10.1016/j.cviu.2009.04.006; Bermudez-Cameo J, 2012, ROBOT AUTON SYST, V60, P755, DOI 10.1016/j.robot.2012.02.008; Bermudez-Cameo J., 2012, 11 AS C COMP VIS ACC, V7727; Bermudez-Cameo J., 2013, 24 BRIT MACH VIS C B; BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855; Bukhari F, 2013, J MATH IMAGING VIS, V45, P31, DOI 10.1007/s10851-012-0342-2; Courbon J, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P1689; Cucchiara R, 2003, 12TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P182, DOI 10.1109/ICIAP.2003.1234047; Devernay F, 2001, MACH VISION APPL, V13, P14, DOI 10.1007/PL00013269; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fitzgibbon AW, 2001, PROC CVPR IEEE, P125; Gasparini S, 2011, INT J COMPUT VISION, V94, P361, DOI 10.1007/s11263-011-0435-1; Gasparini S, 2009, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2009.5459336; Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135; Geyer Christopher, 2000, LNCS, P445, DOI DOI 10.1007/3-540-45053-X_29; Kannala J., 2008, WILEY ENCY COMPUTER; Kannala J, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P28; Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153; Kingslake R., 1989, HIST PHOTOGRAPHIC LE; Mei C, 2007, IEEE INT CONF ROBOT, P3945, DOI 10.1109/ROBOT.2007.364084; Melo R., 2013, IEEE 14 INT C COMP V, P1; Puig L, 2012, COMPUT VIS IMAGE UND, V116, P120, DOI 10.1016/j.cviu.2011.08.003; Puig L, 2011, INT J COMPUT VISION, V93, P101, DOI 10.1007/s11263-010-0411-1; Ray S. F., 2002, APPL PHOTOGRAPHIC OP, DOI 10.4324/9780080499253; Rosten E, 2011, MACH VISION APPL, V22, P77, DOI 10.1007/s00138-009-0196-9; Scaramuzza D, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P5695, DOI 10.1109/IROS.2006.282372; Schneider D, 2009, ISPRS J PHOTOGRAMM, V64, P259, DOI 10.1016/j.isprsjprs.2009.01.001; Stevenson DE, 1996, THIRD IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV '96, PROCEEDINGS, P214, DOI 10.1109/ACV.1996.572058; Strand R., 2005, 16 BRIT MACH VIS C B; Sturm P, 2010, FOUND TRENDS COMPUT, V6, P1, DOI 10.1561/0600000023; Swaminathan R, 2000, IEEE T PATTERN ANAL, V22, P1172, DOI 10.1109/34.879797; Tardif JP, 2006, LECT NOTES COMPUT SC, V3954, P186; Thormahlen T., 2003, MIRAGE 2003, P105; Wang AQ, 2009, J MATH IMAGING VIS, V35, P165, DOI 10.1007/s10851-009-0162-1; Wu YH, 2005, IEEE I CONF COMP VIS, P1547; Xianghua Ying, 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P412, DOI 10.1109/IROS.2005.1545166; Ying X., 2004, 8 EUR C COMP VIS ECC; Ying XH, 2004, INT C PATT RECOG, P839, DOI 10.1109/ICPR.2004.1333903	43	14	14	0	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2015	114	1					16	37		10.1007/s11263-014-0792-7	http://dx.doi.org/10.1007/s11263-014-0792-7			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CO1UF					2022-12-18	WOS:000358940300002
J	Bilen, H; Namboodiri, VP; Van Gool, LJ				Bilen, Hakan; Namboodiri, Vinay P.; Van Gool, Luc J.			Object and Action Classification with Latent Window Parameters	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object classification; Action classification; Latent SVM		In this paper we propose a generic framework to incorporate unobserved auxiliary information for classifying objects and actions. This framework allows us to automatically select a bounding box and its quadrants from which best to extract features. These spatial subdivisions are learnt as latent variables. The paper is an extended version of our earlier work Bilen et al. (Proceedings of The British Machine Vision Conference, 2011), complemented with additional ideas, experiments and analysis. We approach the classification problem in a discriminative setting, as learning a max-margin classifier that infers the class label along with the latent variables. Through this paper we make the following contributions: (a) we provide a method for incorporating latent variables into object and action classification; (b) these variables determine the relative focus on foreground versus background information that is taken account of; (c) we design an objective function to more effectively learn in unbalanced data sets; (d) we learn a better classifier by iterative expansion of the latent parameter space. We demonstrate the performance of our approach through experimental evaluation on a number of standard object and action recognition data sets.	[Bilen, Hakan; Van Gool, Luc J.] Katholieke Univ Leuven, ESAT PSI iMinds, B-3001 Heverlee, Belgium; [Namboodiri, Vinay P.] Alcatel Lucent Bell Labs, B-2018 Antwerp, Belgium; [Van Gool, Luc J.] ETH, Comp Vis Lab, CH-8092 Zurich, Switzerland	IMEC; KU Leuven; Alcatel-Lucent; Swiss Federal Institutes of Technology Domain; ETH Zurich	Bilen, H (corresponding author), Katholieke Univ Leuven, ESAT PSI iMinds, Kasteelpk Arenberg 10, B-3001 Heverlee, Belgium.	hakan.bilen@esat.kuleuven.be; vinay.namboodiri@alcatel-lucent.com; luc.vangool@esat.kuleuven.be	Bilen, Hakan/AAG-3202-2022; Bilen, Hakan/ACY-3128-2022; Bilen, Hakan/H-9130-2016	Bilen, Hakan/0000-0002-6947-6918; Namboodiri, Vinay/0000-0001-5262-9722	EU Project FP7 [AXES ICT-269980]	EU Project FP7	This work was supported by the EU Project FP7 AXES ICT-269980.	[Anonymous], 2007, PASCAL VISUAL OBJECT; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bilen H., 2012, P DAGM OAGM C; Bilen H, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.17; Blaschko M. B., 2010, P ADV NEUR INF PROC; Blaschko MB, 2008, PROC CVPR IEEE, P93, DOI 10.1109/cvpr.2008.4587586; Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Demsar J, 2006, J MACH LEARN RES, V7, P1; DUNN OJ, 1961, J AM STAT ASSOC, V56, P52, DOI 10.2307/2282330; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Friedman M, 1937, J AM STAT ASSOC, V32, P675, DOI 10.2307/2279372; Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378; Laptev Ivan, 2008, P COMP VIS PATT REC; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Messing R, 2009, P INT C COMP VIS ICC; Nemenyi P., 1963, THESIS PRINCETON; Nguyen M. H., 2009, P INT C COMP VIS; Opelt A, 2006, IEEE T PATTERN ANAL, V28, P416, DOI 10.1109/TPAMI.2006.54; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Pinz A, 2005, FOUND TRENDS COMPUT, V1, DOI 10.1561/0600000003; Ranjbar M, 2012, PROC CVPR IEEE, P2304, DOI 10.1109/CVPR.2012.6247941; Satkin S, 2010, LECT NOTES COMPUT SC, V6311, P536, DOI 10.1007/978-3-642-15549-9_39; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shapovalova N., 2012, P EUR C COMP VIS ECC; Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093; Taskar B., 2005, P 22 INT C MACH LEAR, P896, DOI DOI 10.1145/1102351.1102464; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; Vedaldi A., 2009, P ADV NEUR INF PROC; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183; Wang Heng, 2009, BMVC, P1; Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018; Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271; Yu C.-N. J., 2009, P 26 ANN INT C MACHI, P1169, DOI [10.1145/1553374.1553523, DOI 10.1145/1553374.1553523]; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958; Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11; Zhu L, 2010, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2010.5540096; [No title captured]	44	14	15	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2014	106	3			SI		237	251		10.1007/s11263-013-0646-8	http://dx.doi.org/10.1007/s11263-013-0646-8			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AA3DC		Green Accepted			2022-12-18	WOS:000330972100002
J	Payet, N; Todorovic, S				Payet, Nadia; Todorovic, Sinisa			SLEDGE: Sequential Labeling of Image Edges for Boundary Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							SEGMENTATION; CONTOURS; REGIONS; COLOR	Our goal is to detect boundaries of objects or surfaces occurring in an arbitrary image. We present a new approach that discovers boundaries by sequential labeling of a given set of image edges. A visited edge is labeled as on or off a boundary, based on the edge's photometric and geometric properties, and evidence of its perceptual grouping with already identified boundaries. We use both local Gestalt cues (e.g., proximity and good continuation), and the global Helmholtz principle of non-accidental grouping. A new formulation of the Helmholtz principle is specified as the entropy of a layout of image edges. For boundary discovery, we formulate a new, policy iteration algorithm, called SLEDGE. Training of SLEDGE is iterative. In each training image, SLEDGE labels a sequence of edges, which induces loss with respect to the ground truth. These sequences are then used as training examples for learning SLEDGE in the next iteration, such that the total loss is minimized. For extracting image edges that are input to SLEDGE, we use our new, low-level detector. It finds salient pixel sequences that separate distinct textures within the image. On the benchmark Berkeley Segmentation Datasets 300 and 500, our approach proves robust and effective. We outperform the state of the art both in recall and precision for different input sets of image edges.	[Payet, Nadia; Todorovic, Sinisa] Oregon State Univ, Sch EECS, Corvallis, OR 97331 USA	Oregon State University	Todorovic, S (corresponding author), Oregon State Univ, Sch EECS, Kelley Engn Bldg, Corvallis, OR 97331 USA.	payetn@onid.orst.edu; sinisa@eecs.oregonstate.edu						Ahuja N., 2007, ICCV; Ahuja N., 2008, CVPR; [Anonymous], 2007, COMP VIS 2007 ICCV 2; [Anonymous], 1985, PERCEPTUAL ORG VISUA; Arbelaez P., 2006, COMP VIS PATT REC WO, P182, DOI DOI 10.1109/CVPRW.2006.48; Arbelaez P, 2010, TPAMI 99 RAP POSTS; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BIEDERMAN I, 1988, COGNITIVE PSYCHOL, V20, P38, DOI 10.1016/0010-0285(88)90024-2; Borenstein E, 2002, LECT NOTES COMPUT SC, V2351, P109; BORGEFORS G, 1988, IEEE T PATTERN ANAL, V10, P849, DOI 10.1109/34.9107; BRICE CR, 1970, ARTIF INTELL, V1, P205, DOI 10.1016/0004-3702(70)90008-1; Brin S., 1998, 7 INT WORLD WID WEB; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Coughlan JM, 2002, NEURAL COMPUT, V14, P1929, DOI 10.1162/089976602760128072; Daume lll H, 2009, MACHINE LEARNING J; Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985; Desolneux A, 2003, IEEE T PATTERN ANAL, V25, P508, DOI 10.1109/TPAMI.2003.1190576; Desolneux A, 2001, J MATH IMAGING VIS, V14, P271, DOI 10.1023/A:1011290230196; Desolneux A, 2000, INT J COMPUT VISION, V40, P7, DOI 10.1023/A:1026593302236; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; Dollar P., 2006, P IEEE COMP SOC C CO, V2, P1964, DOI DOI 10.1109/CVPR.2006.298; Donoser M, 2010, CVPR; Drummond C., 2003, WORKSHOP LEARNING IM; Felzenszwalb P., 2006, POCV; Freund Yoav, 2001, P 8 INT WORKSH ART I; Fridman A, 2003, P NATL ACAD SCI USA, V100, P8092, DOI 10.1073/pnas.0731829100; Geman D, 1996, IEEE T PATTERN ANAL, V18, P1, DOI 10.1109/34.476006; Greminger MA, 2008, INT J COMPUT VISION, V78, P29, DOI 10.1007/s11263-007-0076-6; Guy G, 1996, INT J COMPUT VISION, V20, P113, DOI 10.1007/BF00144119; Helmholtz H., 1962, TREATISE PHYSL OPTIC; HOCHBERG JE, 1957, PSYCHOL REV, V64, P73, DOI 10.1037/h0043738; Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500; Jain A, 2010, LECT NOTES COMPUT SC, V6314, P199, DOI 10.1007/978-3-642-15561-1_15; Jermyn IH, 2001, IEEE T PATTERN ANAL, V23, P1075, DOI 10.1109/34.954599; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Kim G., 2008, CVPR; Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881; Koffka K., 1935, PRINCIPLES GESTALT P; Kokkinos I., 2010, CVPR; Kokkinos I., 2010, ECCV; Konishi S., 1999, CVPR; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; LEE YJ, 2009, CVPR; Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773; Mahamud S, 2003, IEEE T PATTERN ANAL, V25, P433, DOI 10.1109/TPAMI.2003.1190570; Mairal J, 2008, LECT NOTES COMPUT SC, V5304, P43, DOI 10.1007/978-3-540-88690-7_4; Maire M., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587420; Martin D. R., 2001, ICCV; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; MORRONE MC, 1987, PATTERN RECOGN LETT, V6, P303, DOI 10.1016/0167-8655(87)90013-4; Palmer S.E., 1999, VISION SCI PHOTONS P; Perona P., 1990, ICCV; PORRILL J, 1991, IMAGE VISION COMPUT, V9, P45, DOI 10.1016/0262-8856(91)90048-T; Ren X., 2008, ECCV; Ren XF, 2008, INT J COMPUT VISION, V77, P47, DOI 10.1007/s11263-007-0092-6; Rubner Y, 1996, IMAGE UNDERSTANDING WORKSHOP, 1996 PROCEEDINGS, VOLS I AND II, P927; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Sharon E, 2001, PROC CVPR IEEE, P469; Shashua A, 1988, ICCV; Taskar B., 2004, NIPS; TEH CH, 1989, IEEE T PATTERN ANAL, V11, P859, DOI 10.1109/34.31447; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Varma M, 2003, PROC CVPR IEEE, P691; Wang S, 2005, IEEE T PATTERN ANAL, V27, P546, DOI 10.1109/TPAMI.2005.84; Will S, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P877, DOI 10.1109/ICIP.2000.899596; Williams LR, 1999, INT J COMPUT VISION, V34, P81, DOI 10.1023/A:1008187804026; WILLIAMS LR, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P408, DOI 10.1109/ICCV.1995.466910; XIONG W, 2007, CVPR; YU SX, 2005, CVPR; Zhu Q., 2007, INT C COMP VIS, p[2065, 2066, 2067, 2068, 2070], DOI DOI 10.1109/ICCV.2007.4408929; Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1170, DOI 10.1109/34.809110	73	14	17	2	36	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2013	104	1					15	37		10.1007/s11263-013-0612-5	http://dx.doi.org/10.1007/s11263-013-0612-5			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	179LY		Green Submitted			2022-12-18	WOS:000321523000002
J	Li, RN; Chellappa, R; Zhou, SK				Li, Ruonan; Chellappa, Rama; Zhou, Shaohua Kevin			Recognizing Interactive Group Activities Using Temporal Interaction Matrices and Their Riemannian Statistics	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Event analysis; Activity recognition	RECOGNITION; REPRESENTATION	While video-based activity analysis and recognition has received much attention, a large body of existing work deals with activities of a single subject. Modeling and recognition of coordinated multi-subject activities, or group activities, present in a variety of applications such as surveillance, sports, and biological monitoring records, etc., is the main objective of this paper. Unlike earlier attempts which model the complex spatial temporal constraints among multiple subjects with a parametric Bayesian network, we propose a compact and discriminative descriptor referred to as the Temporal Interaction Matrix for representing a coordinated group motion pattern. Moreover, we characterize the space of the Temporal Interaction Matrices using the Discriminative Temporal Interaction Manifold (DTIM), and use it as a framework within which we develop a data-driven strategy to characterize the group motion pattern without employing specific domain knowledge. In particular, we establish probability densities on the DTIM for compactly describing the statistical properties of the coordinations and interactions among multiple subjects in a group activity. For each class of group activity, we learn a multi-modal density function on the DTIM. A Maximum a Posteriori (MAP) classifier on the manifold is then designed for recognizing new activities. In addition, we have extended this model to one with which we can explicitly distinguish the participants from non-participants. We demonstrate how the framework can be applied to motions represented by point trajectories as well as articulated human actions represented by images. Experiments on both cases show the effectiveness of the proposed approach.	[Li, Ruonan] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA; [Chellappa, Rama] Univ Maryland, UMIACS, Ctr Automat Res, College Pk, MD 20742 USA; [Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA; [Zhou, Shaohua Kevin] Siemens Corp, Corp Res & Technol, Princeton, NJ 08540 USA	Harvard University; University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park; Siemens AG	Li, RN (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	ruonanli@seas.harvard.edu; rama@umiacs.umd.edu; shaohua.zhou@siemens.com	Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAV-8690-2020; Li, Ruonan/J-5126-2014		DARPA VIRAT program; MURI program from the Office of Naval Research [N00014-10-1-0934]	DARPA VIRAT program; MURI program from the Office of Naval Research	Li and Chellappa were supported by the DARPA VIRAT program and a MURI program from the Office of Naval Research under the grant N00014-10-1-0934.	Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653; Choi Wongun, 2009, 2009 IEEE 12 INT C C, P1282; Cutler R, 2000, IEEE T PATTERN ANAL, V22, P781, DOI 10.1109/34.868681; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Dryden I.L., 1998, STAT SHAPE ANAL, DOI [DOI 10.5555/1046920.1088707, 10.1002/9781119072492]; Gong S., 2003, P 2003 IEEE C COMP V, V2, P742; Grant M., 2014, CVX MATLAB SOFTWARE; Hakeem A, 2007, ARTIF INTELL, V171, P586, DOI 10.1016/j.artint.2007.04.002; Hongeng S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P84, DOI 10.1109/ICCV.2001.937608; Hoogs A., 2009, SNOWB LEARN WORKSH S; Hoogs A, 2008, 2008 IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING, P71; Huang CL, 2006, IEEE T MULTIMEDIA, V8, P749, DOI 10.1109/TMM.2006.876289; Intille SS, 2001, COMPUT VIS IMAGE UND, V81, P414, DOI 10.1006/cviu.2000.0896; Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686; Joo SW, 2007, IEEE T IMAGE PROCESS, V16, P2849, DOI 10.1109/TIP.2007.906254; Junejo IN, 2011, IEEE T PATTERN ANAL, V33, P172, DOI 10.1109/TPAMI.2010.68; Kass R., 1997, GEOMETRIC FDN ASYMPT; KHAN SM, 2005, ACM MULTIMEDIA; Kim K, 2012, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2012.6247809; Kim M., 2006, IEEE C COMP VIS PATT; Klaser Alexander, 2008, BMVC; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Lan T., 2010, ADV NEURAL INFORM PR; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Lazarescu M, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P181; Li RN, 2009, PROC CVPR IEEE, P2442; Liu TY, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P164; Liu XH, 2006, IMAGE VISION COMPUT, V24, P166, DOI 10.1016/j.imavis.2005.09.024; Ma X, 2009, IEEE T CIRC SYST VID, V19, P397, DOI 10.1109/TCSVT.2009.2013510; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Morariu VI, 2011, PROC CVPR IEEE; Ni B., 2009, IEEE C COMP VIS PATT; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Perse M, 2009, COMPUT VIS IMAGE UND, V113, P612, DOI 10.1016/j.cviu.2008.03.001; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Rosset S., 2002, ADV NEURAL INFORM PR, V15, P641; Ryoo MS, 2011, INT J COMPUT VISION, V93, P183, DOI 10.1007/s11263-010-0355-5; Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361; Scovanner P., 2007, ACM MM, P357; Srivastava A., 2007, IEEE C COMP VIS PATT, P1; Todorovic S., 2011, IEEE INT C COMP VIS; Vaswani N, 2005, IEEE T IMAGE PROCESS, V14, P1603, DOI 10.1109/TIP.2005.852197; Veeraraghavan A, 2008, IEEE T PATTERN ANAL, V30, P463, DOI 10.1109/TPAMI.2007.70707; Wongun Choi, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3273, DOI 10.1109/CVPR.2011.5995707; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Zhang D, 2006, IEEE T MULTIMEDIA, V8, P509, DOI 10.1109/TMM.2006.870735; Zhou Y, 2008, PROC CVPR IEEE, P3682	49	14	14	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	2					305	328		10.1007/s11263-012-0573-0	http://dx.doi.org/10.1007/s11263-012-0573-0			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	081BW					2022-12-18	WOS:000314291600005
J	Wang, LA; Wang, YZ; Gao, W				Wang, Liang; Wang, Yizhou; Gao, Wen			Mining Layered Grammar Rules for Action Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; Layered-grammar model; Action parse tree; Emerging pattern mining	REPRESENTATION	We propose a layered-grammar model to represent actions. Using this model, an action is represented by a set of grammar rules. The bottom layer of an action instance's parse tree contains action primitives such as spatiotemporal (ST) interest points. At each layer above, we iteratively mine grammar rules and "super rules" that account for the high-order compositional feature structures. The grammar rules are categorized into three classes according to three different ST-relations of their action components, namely the strong relation, weak relation and stochastic relation. These ST-relations characterize different action styles (degree of stiffness), and they are pursued in terms of grammar rules for the purpose of action recognition. By adopting the Emerging Pattern (EP) mining algorithm for relation pursuit, the learned production rules are statistically significant and discriminative. Using the learned rules, the parse tree of an action video is constructed by combining a bottom-up rule detection step and a top-down ambiguous rule pruning step. An action instance is recognized based on the discriminative configurations generated by the production rules of its parse tree. Experiments confirm that by incorporating the high-order feature statistics, the proposed method largely improves the recognition performance over the bag-of-words models.	[Wang, Liang; Wang, Yizhou; Gao, Wen] Peking Univ, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China; [Wang, Yizhou; Gao, Wen] Peking Univ, Key Lab Machine Percept MoE, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China; [Wang, Liang] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150006, Heilongjiang, Peoples R China	Peking University; Peking University; Harbin Institute of Technology	Wang, YZ (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.	wangliang@jdl.ac.cn; Yizhou.Wang@pku.edu.cn; wgao@pku.edu.cn			NSFC [60872077]; National Basic Research Program of China (973 Program) [2009CB320904]	NSFC(National Natural Science Foundation of China (NSFC)); National Basic Research Program of China (973 Program)(National Basic Research Program of China)	This work was supported by NSFC grants 60872077 and National Basic Research Program of China (973 Program) 2009CB320904.	Agrawal R., 1994, P 20 INT C VER LARG, P487; Alhammady H, 2006, IEEE T KNOWL DATA EN, V18, P865, DOI 10.1109/TKDE.2006.116; Allen J. E., 1994, Journal of Logic and Computation, V4, P531, DOI 10.1093/logcom/4.5.531; [Anonymous], 2008, P IEEE C COMP VIS PA; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Dong G., 1999, LECT NOTES COMPUTER, V1721, P737; DONG G, 2004, P ACM SIGKDD INT C K, P43; Gilbert A, 2008, LECT NOTES COMPUT SC, V5302, P222, DOI 10.1007/978-3-540-88682-2_18; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686; Joo SW, 2006, IEEE IMAGE PROC, P2897, DOI 10.1109/ICIP.2006.313035; Ke Y, 2005, IEEE I CONF COMP VIS, P166; Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378; Laptev I., 2008, P INT C COMP VIS PAT; Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3; LEONARDIS A, 1995, INT J COMPUT VISION, V14, P253, DOI 10.1007/BF01679685; Lin L, 2009, PATTERN RECOGN LETT, V30, P180, DOI 10.1016/j.patrec.2008.02.023; Liu J., 2008, P INT C COMP VIS PAT; Liu J., 2009, P IEEE INT C COMP VI; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MORI G., 2009, P IEEE C COMP VIS PA; Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4; Nowozin S., 2007, P INT C COMP VIS; Quack T., 2007, P ICCV; Quelhas P, 2007, IEEE T PATTERN ANAL, V29, P1575, DOI 10.1109/TPAMI.2007.1155; Rapantzikos K., 2009, P C COMP VIS PATT RE, P43; Rodriguez M.D., 2008, P INT C COMP VIS PAT; Ryoo MS, 2009, INT J COMPUT VISION, V82, P1, DOI 10.1007/s11263-008-0181-1; Schneider P, 2009, P IEEE C COMP VIS PA, P1; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; SIVIC J, 2004, P INT C COMP VIS PAT; Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721; Vilalta R, 2002, ARTIF INTELL REV, V18, P77, DOI 10.1023/A:1019956318069; Wong S., 2007, P IEEE INT C COMP VI; YAO B, 2009, P INT C COMP VIS	36	14	14	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2011	93	2					162	182		10.1007/s11263-010-0393-z	http://dx.doi.org/10.1007/s11263-010-0393-z			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	740PD					2022-12-18	WOS:000288806000004
J	Wokes, DS; Palmer, PL				Wokes, David S.; Palmer, P. L.			Perspective Reconstruction of a Spheroid from an Image Plane Ellipse	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Spheroid; Image plane; Perspective projection; Perspective reconstruction		The perspective reconstruction of a spheroid's position and orientation from a single image plane ellipse is derived, by direct inversion of the projection equations, assuming the semi-major and semi-minor axes are known. Attention is paid to the geometric interpretation of the reconstruction. The reconstruction is formulated and reduced to an eigenvalue problem, to yield 2 solutions for the spheroid's position and orientation. The symmetry of the polar planes for these solutions are then deduced.	[Wokes, David S.; Palmer, P. L.] Univ Surrey, Surrey Space Ctr, Guildford GU2 5XH, Surrey, England	University of Surrey	Wokes, DS (corresponding author), Univ Surrey, Surrey Space Ctr, Guildford GU2 5XH, Surrey, England.	D.Wokes@googlemail.com; P.Palmer@surrey.ac.uk						CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chellali R, 2003, IEEE T IND ELECTRON, V50, P685, DOI 10.1109/TIE.2003.814764; DINTEN JM, 1990, 10 INT C PATT REC RE, P153; Eberly D., 2007, RECONSTRUCTING ELLIP; EBERLY D, 2005, PERSPECTIVE PROJECTI; Gartner B., 1997, SMALLEST ENCLOSING E; HORAUD R, 1995, IEEE 5 INT C COMP VI, P426; LIU JS, 2003, IEEE INT S COMP INT, V2, P729; MILLAR AV, 1913, DESCRIPTIVE GEOMETRY, pCH1; Osterman M, 2007, FOCAL ENCY PHOTOGRAP; POIGNET P, 2003, IEEE RSJ INT C INT R, V4, P3300; Roberts L, 1965, MACHINE PERCEPTION 3; Semple J. G., 1998, ALGEBRAIC PROJECTIVE; Springer C. E., 1964, GEOMETRY ANAL PROJEC; Todd MJ, 2007, DISCRETE APPL MATH, V155, P1731, DOI 10.1016/j.dam.2007.02.013; WOKES DS, 2010, THESIS U SURREY GUIL; Wylie Jr C. R., 1970, INTRO PROJECTIVE GEO; ZISSERMAN A, 1992, GEOMETRIC INVARIANCE	18	14	16	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2010	90	3					369	379		10.1007/s11263-010-0368-0	http://dx.doi.org/10.1007/s11263-010-0368-0			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	662BV					2022-12-18	WOS:000282782700007
J	Ghanem, B; Ahuja, N				Ghanem, Bernard; Ahuja, Narendra			Dinkelbach NCUT: An Efficient Framework for Solving Normalized Cuts Problems with Priors and Convex Constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Normalized graph cuts; Graph theory; Graph cuts; Interactive segmentation; Graph-based image segmentation; Dinkelbach method for fractional programming; Quadratic programming; Conjugate gradient method; Eigen-decomposition; Multi-way segmentation		In this paper, we propose a novel framework, called Dinkelbach NCUT (DNCUT), which efficiently solves the normalized graph cut (NCUT) problem under general, convex constraints, as well as, under given priors on the nodes of the graph. Current NCUT methods use generalized eigen-decomposition, which poses computational issues especially for large graphs, and can only handle linear equality constraints. By using an augmented graph and the iterative Dinkelbach method for fractional programming (FP), we formulate the DNCUT framework to efficiently solve the NCUT problem under general convex constraints and given data priors. In this framework, the initial problem is converted into a sequence of simpler sub-problems (i.e. convex, quadratic programs (QP's) subject to convex constraints). The complexity of finding a global solution for each sub-problem depends on the complexity of the constraints, the convexity of the cost function, and the chosen initialization. However, we derive an initialization, which guarantees that each sub-problem is a convex QP that can be solved by available convex programming techniques. We apply this framework to the special case of linear constraints, where the solution is obtained by solving a sequence of sparse linear systems using the conjugate gradient method. We validate DNCUT by performing binary segmentation on real images both with and without linear/nonlinear constraints, as well as, multi-class segmentation. When possible, we compare DNCUT to other NCUT methods, in terms of segmentation performance and computational efficiency. Even though the new formulation is applied to the problem of spectral graph-based, low-level image segmentation, it can be directly applied to other applications (e.g. clustering).	[Ghanem, Bernard; Ahuja, Narendra] Univ Illinois, Dept Elect & Comp Engn, Beckman Inst Adv Sci & Technol, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Ghanem, B (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Beckman Inst Adv Sci & Technol, 1406 W Green St, Urbana, IL 61801 USA.	bghanem2@vision.ai.uiuc.edu; ahuja@vision.ai.uiuc.edu	Ghanem, Bernard/J-7605-2017	Ghanem, Bernard/0000-0002-5534-587X	Office of Naval Research [N00014-09-1-0017]; National Science Foundation [IIS 08-12188]	Office of Naval Research(Office of Naval Research); National Science Foundation(National Science Foundation (NSF))	The support of the Office of Naval Research under grant N00014-09-1-0017 and the National Science Foundation under grant IIS 08-12188 are gratefully acknowledged.	Bhatia Rajendra, 1997, MATRIX ANAL, DOI 10.1007/978-1-4612-0653-8; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y.Y., 2001, ICCV, V1, P105, DOI DOI 10.1109/ICCV.2001.937505; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; Cour T, 2005, PROC CVPR IEEE, P1124; COUR T, 2007, INT C ART INT STAT, V11; Dinkelbach W., 1967, MANAGE SCI, V13, P492, DOI 10.1287/mnsc.13.7.492; ERIKSSON AP, 2007, INT C COMP VIS; Georgescu B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P456; GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x; KOLMOGOROV V, 2007, INT C COMP VIS; Lehoucq RB, 1996, SIAM J MATRIX ANAL A, V17, P789, DOI 10.1137/S0895479895281484; Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Paragios N, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P145, DOI 10.1007/0-387-28831-7_9; PARDALOS PM, 2004, J GLOBAL OPTIM, V1, P15; POTHEN A, 1990, SIAM J MATRIX ANAL A, V11, P430, DOI 10.1137/0611030; Rodenas R. G., 1999, TOP, V7, P33; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Thompson R., 1970, B AUST MATH SOC, V3, P23; YU S, 2004, T PATTERN ANAL MACHI, V2, P173; [No title captured]	24	14	17	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2010	89	1					40	55		10.1007/s11263-010-0321-2	http://dx.doi.org/10.1007/s11263-010-0321-2			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	584RF					2022-12-18	WOS:000276769500003
J	Liu, XW; Shi, YG; Dinov, I; Mio, W				Liu, Xiuwen; Shi, Yonggang; Dinov, Ivo; Mio, Washington			A Computational Model of Multidimensional Shape	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multidimensional shape; Shape of surfaces; Elastic shapes	MANIFOLDS; METRICS; SPACES	We develop a computational model of shape that extends existing Riemannian models of curves to multidimensional objects of general topological type. We construct shape spaces equipped with geodesic metrics that measure how costly it is to interpolate two shapes through elastic deformations. The model employs a representation of shape based on the discrete exterior derivative of parametrizations over a finite simplicial complex. We develop algorithms to calculate geodesics and geodesic distances, as well as tools to quantify local shape similarities and contrasts, thus obtaining a formulation that accounts for regional differences and integrates them into a global measure of dissimilarity. The Riemannian shape spaces provide a common framework to treat numerous problems such as the statistical modeling of shapes, the comparison of shapes associated with different individuals or groups, and modeling and simulation of shape dynamics. We give multiple examples of geodesic interpolations and illustrations of the use of the models in brain mapping, particularly, the analysis of anatomical variation based on neuroimaging data.	[Mio, Washington] Florida State Univ, Dept Math, Tallahassee, FL 32306 USA; [Liu, Xiuwen] Florida State Univ, Dept Comp Sci, Tallahassee, FL 32306 USA; [Shi, Yonggang; Dinov, Ivo] UCLA Sch Med, Lab Neuro Imaging, Los Angeles, CA 90095 USA	State University System of Florida; Florida State University; State University System of Florida; Florida State University; University of California System; University of California Los Angeles; University of California Los Angeles Medical Center; David Geffen School of Medicine at UCLA	Mio, W (corresponding author), Florida State Univ, Dept Math, Tallahassee, FL 32306 USA.	mio@math.fsu.edu		Dinov, Ivo/0000-0003-3825-4375	NATIONAL CENTER FOR RESEARCH RESOURCES [U54RR021813] Funding Source: NIH RePORTER; NCRR NIH HHS [U54 RR021813, U54 RR021813-05] Funding Source: Medline	NATIONAL CENTER FOR RESEARCH RESOURCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR)); NCRR NIH HHS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR))		Bhattacharya R, 2003, ANN STAT, V31, P1; do Carmo M., 1994, RIEMANNIAN GEOMETRY; Dryden I.L., 1998, STAT SHAPE ANAL, DOI [DOI 10.5555/1046920.1088707, 10.1002/9781119072492]; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; FLETCHER PT, 2008, IEEE INT C COMP VIS; Fuchs M, 2009, J MATH IMAGING VIS, V35, P86, DOI 10.1007/s10851-009-0156-z; Joshi S., 2007, IEEE C COMP VIS PATT; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; Kendall D.G., 1999, SHAPE SHAPE THEORY; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kilian M, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276457, 10.1145/1239451.1239515]; Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333; LE HL, 1993, ANN STAT, V21, P1225, DOI 10.1214/aos/1176349259; LIU X, 2009, ISBI; LIU X, 2008, MICCAI; Michor PW, 2007, APPL COMPUT HARMON A, V23, P74, DOI 10.1016/j.acha.2006.07.004; Mio W, 2004, Q APPL MATH, V62, P359, DOI 10.1090/qam/2054604; Mio W, 2007, INT J COMPUT VISION, V73, P307, DOI [10.1007/s11263-006-9968-0, 10.1007/s11263-006-996S-0]; Mio W, 2009, INT J COMPUT VISION, V82, P96, DOI 10.1007/s11263-008-0190-0; Morra JH, 2009, NEUROIMAGE, V45, pS3, DOI 10.1016/j.neuroimage.2008.10.043; Munkres J.R., 1984, ELEMENTS ALGEBRAIC T; Schmidt FR, 2006, LECT NOTES COMPUT SC, V4174, P142; Shi YG, 2007, NEUROIMAGE, V37, P792, DOI 10.1016/j.neuroimage.2007.05.016; Shi YG, 2007, MED IMAGE ANAL, V11, P207, DOI 10.1016/j.media.2007.02.001; Srivastava A, 2005, IEEE T PATTERN ANAL, V27, P590, DOI 10.1109/TPAMI.2005.86; Sundaramoorthi G, 2007, INT J COMPUT VISION, V73, P345, DOI 10.1007/s11263-006-0635-2; Younes L, 1999, IMAGE VISION COMPUT, V17, P381, DOI 10.1016/S0262-8856(98)00125-5; Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685; YOUNES L, 2007, ATTI ACCAD SFMNRLMA, V9	29	14	14	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2010	89	1					69	83		10.1007/s11263-010-0323-0	http://dx.doi.org/10.1007/s11263-010-0323-0			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	584RF	21057668	Green Submitted, Bronze, Green Published, Green Accepted			2022-12-18	WOS:000276769500005
J	Liu, J; Huan, ZD; Huang, HY; Zhang, HL				Liu, Jun; Huan, Zhongdan; Huang, Haiyang; Zhang, Haili			An Adaptive Method for Recovering Image from Mixed Noisy Data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image deblurring; Image denoising; EM algorithm; Non-uniformly distributed noise	REMOVAL	In this paper, we present a new version of the famous Rudin-Osher-Fatemi (ROF) model to restore image. The key point of the model is that it could reconstruct images with blur and non-uniformly distributed noise. We develop this approach by adding several statistical control parameters to the cost functional, and these parameters could be adaptively determined by the given observed image. In this way, we could adaptively balance the performance of the fit-to-data term and the regularization term. The Numerical experiments have demonstrated the significant effectiveness and robustness of our model in restoring blurred images with mixed Gaussian noise or salt-and-pepper noise.	[Liu, Jun; Huan, Zhongdan; Huang, Haiyang; Zhang, Haili] Beijing Normal Univ, Lab Math & Complex Syst, Sch Math Sci, Beijing 100875, Peoples R China	Beijing Normal University	Liu, J (corresponding author), Beijing Normal Univ, Lab Math & Complex Syst, Sch Math Sci, Beijing 100875, Peoples R China.	liujn121@gmail.com			National Science Foundation of China (NSFC) [10531040]	National Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	We thank the anonymous reviewers for their constructive comments leading to improvements of the manuscript. The research has been supported by National Science Foundation of China (NSFC, No. 10531040).	ACAR R, 1994, INVERSE PROBL, V10, P1217, DOI 10.1088/0266-5611/10/6/003; Bar L, 2006, INT J COMPUT VISION, V70, P279, DOI 10.1007/s11263-006-6468-1; BECT J, 2004, LECT NOTES COMPUTER, V3024, P1; Bilmes J.A., 1998, INT COMPUT SCI I, V4, P126; Chan RH, 2005, IEEE T IMAGE PROCESS, V14, P1479, DOI 10.1109/TIP.2005.852196; Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187; He L, 2005, INT J IMAG SYST TECH, V15, P74, DOI 10.1002/ima.20040; LAGENDIJK RL, 1988, IEEE T ACOUST SPEECH, V36, P1874, DOI 10.1109/29.9032; McLachlan, 1997, EM ALGORITHM EXTENSI; Michael K. N., 1999, SIAM J SCI COMPUT, V21, P851; Nikolova M, 2004, J MATH IMAGING VIS, V20, P99, DOI 10.1023/B:JMIV.0000011920.58935.9c; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Rudin L. I., 1994, Proceedings ICIP-94 (Cat. No.94CH35708), P31, DOI 10.1109/ICIP.1994.413269; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Shi YY, 2008, APPL NUMER MATH, V58, P602, DOI 10.1016/j.apnum.2007.01.007; Shi YY, 2006, APPL MATH COMPUT, V179, P121, DOI 10.1016/j.amc.2005.11.085; Vogel CR, 1998, IEEE T IMAGE PROCESS, V7, P813, DOI 10.1109/83.679423; Vogel RV, 2002, COMPUTATIONAL METHOD	18	14	15	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2009	85	2					182	191		10.1007/s11263-009-0254-9	http://dx.doi.org/10.1007/s11263-009-0254-9			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	488JD					2022-12-18	WOS:000269344500004
J	Sandler, R; Lindenbaum, M				Sandler, Roman; Lindenbaum, Michael			Optimizing Gabor Filter Design for Texture Edge Detection and Classification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Texture; Gabor kernels; Edge detection; Classification		An effective and efficient texture analysis method, based on a new criterion for designing Gabor filter sets, is proposed. The commonly used filter sets are usually designed for optimal signal representation. We propose here an alternative criterion for designing the filter set. We consider a set of filters and its response to pairs of harmonic signals. Two signals are considered separable if the corresponding two sets of vector responses are disjoint in at least one of the components. We propose an algorithm for deriving the set of Gabor filters that maximizes the fraction of separable harmonic signal pairs in a given frequency range. The resulting filters differ significantly from the traditional ones. We test these maximal harmonic discrimination (MHD) filters in several texture analysis tasks: clustering, recognition, and edge detection. It turns out that the proposed filters perform much better than the traditional ones in these tasks. They can achieve performance similar to that of state-of-the-art, distribution based (texton) methods, while being simpler and more computationally efficient.	[Sandler, Roman; Lindenbaum, Michael] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Sandler, R (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.	romats@cs.technion.ac.il; mic@cs.technion.ac.il			MUSCLE Network of Excellence	MUSCLE Network of Excellence	The authors would like to thank Manik Varma for supplying the edited CUReT database used in his papers. This work was supported by the MUSCLE Network of Excellence.	BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; FOGEL I, 1989, BIOL CYBERN, V61, P103, DOI 10.1007/BF00204594; GREENSPAN H, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P222, DOI 10.1109/CVPR.1994.323833; Lee TS, 1996, IEEE T PATTERN ANAL, V18, P959, DOI 10.1109/34.541406; Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638; Liu XW, 2003, IEEE T IMAGE PROCESS, V12, P661, DOI 10.1109/TIP.2003.812327; MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261; Sandler R., 2006, COMP VIS PATT REC WO, P178; Tsai DM, 2001, IMAGE VISION COMPUT, V19, P299, DOI 10.1016/S0262-8856(00)00078-0; Varma M, 2003, PROC CVPR IEEE, P691; Varma M, 2002, LECT NOTES COMPUT SC, V2352, P255; Weldon TP, 1996, PATTERN RECOGN, V29, P2005, DOI 10.1016/S0031-3203(96)00047-7	19	14	15	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2009	84	3					308	324		10.1007/s11263-009-0237-x	http://dx.doi.org/10.1007/s11263-009-0237-x			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	458MS					2022-12-18	WOS:000267028600005
J	Van den Bergh, M; Koller-Meier, E; Van Gool, L				Van den Bergh, Michael; Koller-Meier, Esther; Van Gool, Luc			Real-Time Body Pose Recognition Using 2D or 3D Haarlets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose estimation; Pose recognition; Silhouettes; 3D hulls; LDA; ANMM; Haarlets		This article presents a novel approach to markerless real-time pose recognition in a multicamera setup. Body pose is retrieved using example-based classification based on Haar wavelet-like features to allow for real-time pose recognition. Average Neighborhood Margin Maximization (ANMM) is introduced as a powerful new technique to train Haar-like features. The rotation invariant approach is implemented for both 2D classification based on silhouettes, and 3D classification based on visual hulls.	[Van den Bergh, Michael; Van Gool, Luc] ETH, Comp Vis Lab, Zurich, Switzerland; [Koller-Meier, Esther; Van Gool, Luc] Katholieke Univ Leuven, ESAT PSI VISICS, Louvain, Belgium	Swiss Federal Institutes of Technology Domain; ETH Zurich; KU Leuven	Van den Bergh, M (corresponding author), ETH, Comp Vis Lab, Zurich, Switzerland.	vamichae@vision.ee.ethz.ch; ebmeier@vision.ee.ethz.ch; vangool@vision.ee.ethz.ch			EU [FP6 511092]; Swiss NCCR project IM2	EU(European Commission); Swiss NCCR project IM2(Swiss National Science Foundation (SNSF))	This work has been carried out in the context of the Sixth Framework Programme of the European Commission: EU Project FP6 511092 ( CyberWalk) and Swiss NCCR project IM2.	BAUMBERG A, 1994, LECT NOTES COMPUTER, V800, P299; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581; Cheung GKM, 2003, PROC CVPR IEEE, P77; Cohen I, 2003, IEEE INTERNATIONAL WORKSHOP ON ANALYSIS AND MODELING OF FACE AND GESTURES, P74; Delamarre Q., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P716, DOI 10.1109/ICCV.1999.790292; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056; Griesser A., 2005, P VIS MOD VIS VMV, P319; Ioffe S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P690, DOI 10.1109/ICCV.2001.937589; Kakadiaris I, 2000, IEEE T PATTERN ANAL, V22, P1453, DOI 10.1109/34.895978; Ke Y, 2005, IEEE I CONF COMP VIS, P166; Kehl R, 2005, PROC CVPR IEEE, P129; Lienhart R, 2002, IEEE IMAGE PROC, P900; Mikic I, 2001, PROC CVPR IEEE, P455; Mori G, 2002, LECT NOTES COMPUT SC, V2352, P666; Nummiaro K, 2003, IMAGE VISION COMPUT, V21, P99, DOI 10.1016/S0262-8856(02)00129-4; Papageorgiou CP, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P555, DOI 10.1109/ICCV.1998.710772; Ren L, 2005, ACM T GRAPHIC, V24, P1303, DOI 10.1145/1095878.1095882; Rosales R, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P19, DOI 10.1109/HUMO.2000.897366; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Sminchisescu C, 2003, INT J ROBOT RES, V22, P371, DOI 10.1177/0278364903022006003; Syeda-Mahmood T, 2007, PROC CVPR IEEE, P132; VANDENBERGH M, 2008, IEEE VISUAL MOTION C; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Yamamoto M, 1998, PROC CVPR IEEE, P2, DOI 10.1109/CVPR.1998.698580; YANG J, 2000, INT C CONTR AUT ROB; Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883; Zhao W, 1998, PROC CVPR IEEE, P164, DOI 10.1109/CVPR.1998.698604	29	14	14	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2009	83	1					72	84		10.1007/s11263-009-0218-0	http://dx.doi.org/10.1007/s11263-009-0218-0			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	413JY		Green Submitted, Green Published			2022-12-18	WOS:000263790600005
J	Cheung, V; Frey, BJ; Jojic, N				Cheung, Vincent; Frey, Brendan J.; Jojic, Nebojsa			Video epitomes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Conference on Computer Vision and Pattern Recognition	JUN 20-25, 2005	San Diego, CA	IEEE Comp Soc		epitome; video summarization; em algorithm; variational technique; super-resolution; inpainting; object removal; image restoration; missing data		Recently, "epitomes" were introduced as patch-based probability models that are learned by compiling together a large number of examples of patches from input images. In this paper, we describe how epitomes can be used to model video data and we describe significant computational speedups that can be incorporated into the epitome inference and learning algorithm. In the case of videos, epitomes are estimated so as to model most of the small spacetime cubes from the input data. Then, the epitome can be used for various modeling and reconstruction tasks, of which we show results for video super-resolution, video interpolation, and object removal. Besides computational efficiency, an interesting advantage of the epitome as a representation is that it can be reliably estimated even from videos with large amounts of missing data. We illustrate this ability on the task of reconstructing the dropped frames in video broadcast using only the degraded video and also in denoising a severely corrupted video.	[Cheung, Vincent; Frey, Brendan J.] Univ Toronto, Toronto, ON M5S 3G4, Canada; [Jojic, Nebojsa] Microsoft Res, Machine Learning & Appl Stat, Redmond, WA 98052 USA	University of Toronto; Microsoft	Cheung, V (corresponding author), Univ Toronto, Toronto, ON M5S 3G4, Canada.	vincent@psi.toronto.edu; frey@psi.toronto.edu; jojic@microsoft.com						Bishop C.M., 2003, INT WORKSH ART INT S, P25; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Criminisi A, 2003, PROC CVPR IEEE, P721; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Frey BJ, 2005, IEEE T PATTERN ANAL, V27, P1392, DOI 10.1109/TPAMI.2005.169; Jepson A., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P760, DOI 10.1109/CVPR.1993.341161; Jojic N, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P34; Rosales R, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P472; WANG JYA, 1994, P SPIE DIG VID COMP, P116; Wexler Y, 2004, PROC CVPR IEEE, P120; ZHU SC, 2002, P 7 EUR C COMP VIS 4, P793	13	14	17	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2008	76	2					141	152		10.1007/s11263-006-0001-4	http://dx.doi.org/10.1007/s11263-006-0001-4			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	255VH					2022-12-18	WOS:000252685400005
J	Wang, S; Stahl, JS; Bailey, A; Dropps, M				Wang, Song; Stahl, Joachim S.; Bailey, Adam; Dropps, Michael			Global detection of salient convex boundaries	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						boundary detection; convexity; edge detection; edge grouping; graph models; perceptual organization	COMPLETION; CONTOURS	As an important geometric property of many structures or structural components, convexity plays an important role in computer vision and image understanding. In this paper, we describe a general approach that can force various edge-grouping algorithms to detect only convex structures from a set of boundary fragments. The basic idea is to remove some fragments and fragment connections so that, on the remaining ones, a prototype edge-grouping algorithm that detects closed boundaries without the convexity constraint can only produce convex closed boundaries. We show that this approach takes polynomial time and preserves the grouping optimality by not excluding any valid convex boundary from the search space. Choosing the recently developed ratio-contour algorithm as the prototype grouping algorithm, we develop a new convex-grouping algorithm, which can detect convex salient boundaries with good continuity and proximity in a globally optimal fashion. To facilitate the application of this convex-grouping algorithm, we develop a new fragment-connection method based on four-point Bezier curves. We demonstrate the performance of this convex-grouping algorithm by conducting experiments on both synthetic and real images. In addition, we provide a comparison with some prior edge-grouping algorithms. Finally, we show that the proposed convex-grouping algorithm can be further extended to detect convex open boundaries, derive region-based image hierarchies, and incorporate some simple human-computer interactions.	Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA	University of South Carolina; University of South Carolina System; University of South Carolina Columbia	Wang, S (corresponding author), Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.	songwang@cse.sc.edu; stahlj@cse.sc.edu; baileya6@cse.sc.edu; droppsm@cse.sc.edu		Wang, Song/0000-0003-4152-5295				Alter T, 1998, INT J COMPUT VISION, V27, P51, DOI 10.1023/A:1007953729443; Amir A, 1998, IEEE T PATTERN ANAL, V20, P168, DOI 10.1109/34.659934; [Anonymous], 1985, PERCEPTUAL ORG VISUA; [Anonymous], 1938, SOURCE BOOK GESTALT; Bartels RH, 1987, INTRO SPLINES USE CO; Bertamini M, 2001, PERCEPTION, V30, P1295, DOI 10.1068/p3197; BOEHM W, 2002, BEZIER B SPLINE TECH; Borra S, 1997, IEEE T PATTERN ANAL, V19, P1306, DOI 10.1109/34.632991; BRUCKSTEIN AM, 1990, COMPUT VISION GRAPH, V49, P283, DOI 10.1016/0734-189X(90)90105-5; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Elder J., 1996, EUR C COMP VIS, P399; Elder JH, 2003, IEEE T PATTERN ANAL, V25, P661, DOI 10.1109/TPAMI.2003.1201818; ESTRADA F, 2004, CSRG482 U TOR DEP CO; Estrada FJ, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334008; Foley J.D., 1995, COMPUTER GRAPHICS PR; Forsyth David A, 2012, COMPUTER VISION MODE; Guy G, 1996, INT J COMPUT VISION, V20, P113, DOI 10.1007/BF00144119; HUTTENLOCHER DP, 1992, INT J COMPUT VISION, V8, P7, DOI 10.1007/BF00126398; JACOBS D, 1992, AI1416 MIT ART INT L; JACOBS D, 1996, CONVEX GROUPING CODE; Jacobs D. W., 1987, Proceedings of the IEEE Computer Society Workshop on Computer Vision (Cat. No.87TH0210-5), P164; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; KANISZA G, 1976, VISION ARTIFACT; Liu ZL, 1999, VISION RES, V39, P4244, DOI 10.1016/S0042-6989(99)00141-8; Mahamud S, 2003, IEEE T PATTERN ANAL, V25, P433, DOI 10.1109/TPAMI.2003.1190570; Mio W, 2004, Q APPL MATH, V62, P359, DOI 10.1090/qam/2054604; Mumford D., 1994, ALGEBRAIC GEOMETRY I, V5681, P491, DOI DOI 10.1007/978-1-4612-2628-4_31; NATTKEMPER TW, 2004, P 11 WORLD C MED INF; Parvin B., 1995, Proceedings International Symposium on Computer Vision (Cat. No.95TB100006), P295, DOI 10.1109/ISCV.1995.477017; Sarkar S, 1996, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.1996.517115; SARKAR S, 1994, COMPUTING PERCEPTUAL; Saund E, 2003, IEEE T PATTERN ANAL, V25, P475, DOI 10.1109/TPAMI.2003.1190573; SAUND E, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P597, DOI 10.1109/ICCV.1995.466884; Sharon E, 2000, IEEE T PATTERN ANAL, V22, P1117, DOI 10.1109/34.879792; SRIVASTAVA A, P 2 IEEE WORKSH VAR; Stahl JS, 2005, IEEE I CONF COMP VIS, P946; Ullman S., 1988, P 2 INT C COMP VIS, P321, DOI DOI 10.1109/CCV.1988.590008; Wang JW, 2004, NITRIC OXIDE-BIOL CH, V11, P298, DOI 10.1016/j.niox.2004.10.003; Wang S, 2005, IEEE T PATTERN ANAL, V27, P546, DOI 10.1109/TPAMI.2005.84; WANG S, 2003, NEURAL INFORMATION P, P1571; WILLIAMS L, 1997, NEURAL COMPUT, V9, P849; Williams LR, 1999, INT J COMPUT VISION, V34, P81, DOI 10.1023/A:1008187804026	42	14	16	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2007	71	3					337	359		10.1007/s11263-006-8427-2	http://dx.doi.org/10.1007/s11263-006-8427-2			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	104CB					2022-12-18	WOS:000241932400004
J	O'Donnell, T; Funka-Lea, G; Tek, H; Jolly, MP; Rasch, M; Setser, R				O'Donnell, Thomas; Funka-Lea, Gareth; Tek, Huseyin; Jolly, Marie-Pierre; Rasch, Matthias; Setser, Randolph			Comprehensive cardiovascular image analysis using MR and CT at Siemens Corporate Research	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						medical image analysis; cardiovascular; cardiac; support vector machines; level sets; model-based registration	REGISTRATION; ANGIOGRAPHY; FUSION	At Siemens Corporate Research we have created a set of tools for the analysis of MR and CT cardiovascular images in the applications Argus, Vessel View, and Proteus. Argus is designed to assess cardiovascular function by reporting measures of morphology and tissue health using a 2-D approach. Vessel View, a 3-D application, is capable of quantifying vascular integrity and provides tools for segmenting vessels. Lastly, Proteus has functionality for registering 3-D cardiac data sets (e.g., MR and CT). Taken together, these applications allow for a comprehensive analysis of MR and CT cardiovascular studies. Throughout this paper we will illustrate the capabilities of our tools via their application to an actual clinical case. Our contribution lies in combining several computer vision technologies and applying them to practical, real world problems.	Siemens Corp res, Imaging & Visualizat Dept, Princeton, NJ 08540 USA; Cleveland Clin Fdn, Dept Diagnost Radiol, Cleveland, OH 44195 USA	Siemens AG; Cleveland Clinic Foundation	O'Donnell, T (corresponding author), Siemens Corp res, Imaging & Visualizat Dept, Princeton, NJ 08540 USA.	tom.odonnell@siemens.com; setserr@ccf.org						*AHA, 2001, HEART STROK STAT UPD; Avants BB, 2000, LECT NOTES COMPUT SC, V1935, P707; Aylward S, 1996, PROCEEDINGS OF THE IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, P131, DOI 10.1109/MMBIA.1996.534065; Bonneville M, 1998, P SOC PHOTO-OPT INS, V3338, P264, DOI 10.1117/12.310900; Bullitt E, 1999, LECT NOTES COMPUT SC, V1613, P308; Chung ACS, 1999, LECT NOTES COMPUT SC, V1679, P82; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Cristianini N., 2000, INTRO SUPPORT VECTOR; Deschamps T, 2001, MED IMAGE ANAL, V5, P281, DOI 10.1016/S1361-8415(01)00046-9; Faber TL, 2004, J NUCL MED, V45, P745; Frangi AF, 2002, IEEE T MED IMAGING, V21, P1005, DOI 10.1109/TMI.2002.804442; FRANGI AF, 1998, MED IM C COMP ASS IN, P82; Gokturk SB, 2001, IEEE T MED IMAGING, V20, P1251, DOI 10.1109/42.974920; GRAY H, 1957, GRAYS ANATOMY; Kim RJ, 2000, NEW ENGL J MED, V343, P1445, DOI 10.1056/NEJM200011163432003; KOLLER TM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P864, DOI 10.1109/ICCV.1995.466846; LORENZ C, 1999, J CARDIOVASCULAR MAG, P7; LORIGO LM, 2000, IEEE C COMP VIS PATT; Murray C., 2002, WORLD HLTH REPORT RE; Nalwa V. S., 1993, GUIDED TOUR COMPUTER; NEKOLA S, 2001, COREGISTRATION FUSIO, V322, P144; Noble NMI, 2004, LECT NOTES COMPUT SC, V3217, P890; O'Donnell T, 2000, PROC CVPR IEEE, P790, DOI 10.1109/CVPR.2000.854960; ODONNELL T, 2003, SPIE MED IM; PAL NR, 1993, PATTERN RECOGN, V26, P1277, DOI 10.1016/0031-3203(93)90135-J; Petitjean C, 2005, J CARDIOVASC MAGN R, V7, P501, DOI 10.1081/JCMR-200053610; REHR RB, 1985, RADIOLOGY, V156, P717, DOI 10.1148/radiology.156.3.4023232; Sato Y, 1998, Med Image Anal, V2, P143, DOI 10.1016/S1361-8415(98)80009-1; Schiller N B, 1989, J Am Soc Echocardiogr, V2, P358; Schindler TH, 1999, INT J CARDIAC IMAG, V15, P357, DOI 10.1023/A:1006232407637; SETHIAN JA, 1999, LEVEL SET METHODS F; Setser RM, 2005, RADIOLOGY, V237, P465, DOI 10.1148/radiol.2372040236; SHEEHAN FH, 1987, CIRCULATION, V75, P817, DOI 10.1161/01.CIR.75.4.817; SIDDIQI K, 2001, INT WORKSH EN MIN ME, P336; Sturm B, 2003, INT J CARDIOVAS IMAG, V19, P281, DOI 10.1023/A:1025481929472; Tek HY, 2001, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P228, DOI 10.1109/MMBIA.2001.991738; Turkington TG, 1997, IEEE T NUCL SCI, V44, P235, DOI 10.1109/23.568814; Walimbe V, 2003, INT J CARDIOVAS IMAG, V19, P483, DOI 10.1023/B:CAIM.0000004325.48512.5a; Wilson DL, 1997, LECT NOTES COMPUT SC, V1230, P423; WIRTH MA, 1997, IEEE INT C IMAGE PRO, P780; Yezzi A, 2001, PROC CVPR IEEE, P87; YU JN, 1995, J NUCL MED, V36, P2333; [No title captured]	43	14	14	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2006	70	2					165	178		10.1007/s11263-006-7937-2	http://dx.doi.org/10.1007/s11263-006-7937-2			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	076SO					2022-12-18	WOS:000239978200005
J	Li, G; Zucker, SW				Li, Gang; Zucker, Steven W.			Contextual inference in contour-based stereo correspondence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	2nd IEEE Workshop on Variational, Geometric and Level Set Methods held in Conjunction with the IEEE International Conference on Computer Vision	OCT, 2003	Nice, FRANCE	IEEE, French Natl Inst Res Comp Sci & Control, Siemens Corp Res, Imaging & Visualizat Dept		stereo correspondence; curve matching; position disparity; orientation disparity; relaxation labeling	VISION; ALGORITHM; GEOMETRY	Standard approaches to stereo correspondence have difficulty when scene structure does not lie in or near the frontal parallel plane, in part because an orientation disparity as well as a positional disparity is introduced. We propose a correspondence algorithm based on differential geometry. that takes explicit advantage of both disparities. The algorithm relates the 2D differential structure (position, tangent. and curvature) of curves in the left and right images to the Frenet approximation of the (3D) space curve. A compatibility function is defined via transport of the Frenet frames, and they ire matched by relaxing this compatibility function on overlapping neighborhoods along the curve. The remaining, false matches are concurrently eliminated by a model of "near" and "far" neurons derived from neurobiology. Examples on scenes with complex 3D structures are provided.	Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA	Yale University	Li, G (corresponding author), Yale Univ, Dept Comp Sci, POB 2158, New Haven, CT 06520 USA.	gang.li@yale.edu; steven.zucker@yale.edu						ALIBHAI S, 2000, ECCV; BARNARD ST, 1982, COMPUT SURV, V14, P553, DOI 10.1145/356893.356896; Ben-Shahar O, 2003, IEEE T PATTERN ANAL, V25, P401, DOI 10.1109/TPAMI.2003.1190568; Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; CHRISTMAS WJ, 1995, IEEE T PATTERN ANAL, V17, P749, DOI 10.1109/34.400565; CIPOLLA R, 1992, INT J COMPUT VISION, V8, P53, DOI 10.1007/BF00126400; Cipolla R., 2000, VISUAL MOTION CURVES; DAVID C, 1990, INT J COMPUT VISION, V5, P219, DOI 10.1007/BF00126500; DHOND UR, 1989, IEEE T SYST MAN CYB, V19, P1489, DOI 10.1109/21.44067; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; Faugeras O, 1996, INT J COMPUT VISION, V18, P5, DOI 10.1007/BF00126137; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Howard I.P., 1995, BINOCULAR VISION STE; HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085; HUMMEL RA, 1983, IEEE T PATTERN ANAL, V5, P267, DOI 10.1109/TPAMI.1983.4767390; IVERSON LA, 1995, IEEE T PATTERN ANAL, V17, P982, DOI 10.1109/34.464562; JONES DG, 1992, ECCV; KROL JD, 1980, PERCEPTION, V9, P651, DOI 10.1068/p090651; LEHKY SR, 1990, J NEUROSCI, V10, P2281; Maciel J, 2003, IEEE T PATTERN ANAL, V25, P187, DOI 10.1109/TPAMI.2003.1177151; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; Marr D., 1982, VISION; MEDIONI G, 1985, COMPUT VISION GRAPH, V31, P2, DOI 10.1016/S0734-189X(85)80073-6; NASRABADI NM, 1992, IEEE T PATTERN ANAL, V14, P566, DOI 10.1109/34.134060; Nemhauser G.L., 1988, INTEGER COMBINATORIA; PARENT P, 1989, IEEE T PATTERN ANAL, V11, P823, DOI 10.1109/34.31445; POGGIO GF, 1984, ANNU REV NEUROSCI, V7, P379, DOI 10.1146/annurev.ne.07.030184.002115; POGGIO GF, 1977, J NEUROPHYSIOL, V40, P1392, DOI 10.1152/jn.1977.40.6.1392; POLLARD SB, 1985, PERCEPTION, V14, P449, DOI 10.1068/p140449; RICHARD AF, 1985, PRIMATES IN NATURE; Robert L., 1991, P IEEE C COMP VIS PA; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Schmid C, 2000, INT J COMPUT VISION, V40, P199, DOI 10.1023/A:1008135310502; Shan Y, 2002, INT J COMPUT VISION, V46, P157, DOI 10.1023/A:1013591914229; WILDES RP, 1991, IEEE T PATTERN ANAL, V13, P761, DOI 10.1109/34.85667; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zitnick CL, 2000, IEEE T PATTERN ANAL, V22, P675, DOI 10.1109/34.865184; Zucker SW, 1989, NEURAL COMPUT, V1, P68, DOI 10.1162/neco.1989.1.1.68; ZUCKER SW, 2004, IN PRESS PROBLEMS SY	42	14	16	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	1					59	75		10.1007/s11263-006-6853-9	http://dx.doi.org/10.1007/s11263-006-6853-9			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	063QH					2022-12-18	WOS:000239034100005
J	Shimizu, M; Okutomi, M				Shimizu, Masao; Okutomi, Masatoshi			Multi-parameter simultaneous estimation on area-based matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						area-based matching; sub-pixel estimation; similarity interpolation; motion parameter; non-iterative computation	REGISTRATION	Area-based matching is a fundamental image processing method that obtains displacement between image regions. In addition, the similarity interpolation method to estimate sub-pixel displacement is commonly used to enhance resolution. This paper proposes a novel 2D sub-pixel displacement estimation method based on similarity interpolation. The method estimates the displacement as an intersection point of two lines, which are approximations of zero positions of the partial derivatives with respect to each motion parameter. The proposed method requires a non-iterative computation. Furthermore, the method engenders only slightly higher calculation costs than the conventional similarity interpolation method. Moreover, the method is suitable for hardware implementation. We show that the proposed method can be extended to obtain the N-parameter of image deformation with non-iterative computation. Using similarity measures obtained at discrete positions in the parameter space, our method provides a highly accurate maximum position of the similarity in sub-sampling resolution; that position corresponds to the image deformation parameters. Experimental results using both synthetic and real images demonstrate that our method can estimate parameters more accurately than conventional methods.	Tokyo Inst Technol, Grad Sch Sci & Engn, Meguro Ku, Tokyo 1528550, Japan	Tokyo Institute of Technology	Shimizu, M (corresponding author), Tokyo Inst Technol, Grad Sch Sci & Engn, Meguro Ku, 2-12-1 Ookayama, Tokyo 1528550, Japan.	mas@ok.ctrl.titech.ac.jp; mxo@ctrl.titech.ac.jp		Okutomi, Masatoshi/0000-0001-5787-0742				Altunbasak Y, 2003, IEEE T IMAGE PROCESS, V12, P395, DOI 10.1109/TIP.2003.809012; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; BERGEN JR, 1992, LECT NOTES COMPUT SC, V588, P237; BROWN LG, 1992, COMPUT SURV, V24, P325, DOI 10.1145/146370.146374; Cech J, 2005, LECT NOTES COMPUT SC, V3540, P598; Chang JY, 2002, IEEE T CONSUM ELECTR, V48, P108, DOI 10.1109/TCE.2002.1010098; DVORNYCHENKO VN, 1983, IEEE T PATTERN ANAL, V5, P206, DOI 10.1109/TPAMI.1983.4767373; Gleicher M, 1997, PROC CVPR IEEE, P331, DOI 10.1109/CVPR.1997.609345; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; IRANI M, 1994, INT J COMPUT VISION, V12, P5, DOI 10.1007/BF01420982; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Shimizu M, 2005, INT J COMPUT VISION, V63, P207, DOI 10.1007/s11263-005-6878-5; SHIMIZU M, 2004, P AS C COMP VIS JEJ, P854; SHIMIZU M, 2004, P IEEE 2 WORKSH IM V; Shum HY, 2002, INT J COMPUT VISION, V48, P151, DOI 10.1023/A:1016051024520; Shum HY, 2000, INT J COMPUT VISION, V36, P101, DOI 10.1023/A:1008195814169; SUN C, 2003, P AUSTR JAP ADV WORK; Szeliski R, 2002, LECT NOTES COMPUT SC, V2351, P525; TIAN Q, 1986, COMPUT VISION GRAPH, V35, P220, DOI 10.1016/0734-189X(86)90028-9; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	23	14	18	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2006	67	3					327	342		10.1007/s11263-006-5632-3	http://dx.doi.org/10.1007/s11263-006-5632-3			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	052JA					2022-12-18	WOS:000238228800005
J	Liu, B; Yu, MY; Maier, D; Manner, R				Liu, B; Yu, MY; Maier, D; Manner, R			An efficient and accurate method for 3D-point reconstruction from multiple views	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D-point reconstruction; structure estimate; bundle adjustment; resection; intersection; maximum likelihood estimation; first-order approximation		In this paper we consider the problem of finding the position of a point in space given its projections in multiple images taken by cameras with known calibration and pose. Ideally the 3D point can be obtained as the intersection of multiple known rays in space. However, with noise the rays do not meet at a single point generally. Therefore, it is necessary to find a best point of intersection. In this paper we propose a modification of the method (Ma et al., 2001. Journal of Communications in Information and Systems, (1):51-73) based on the multiple-view epipolar constraints. The solution is simple in concept and straightforward to implement. It includes generally two steps: first, image points are corrected through approximating the error model to the first order, and then the 3D point can be reconstructed from the corrected image points using any generic triangulation method. Experiments are conducted both on simulated data and on real data to test the proposed method against previous methods. It is shown that results obtained with the proposed method are consistently more accurate than those of other linear methods. When the measurement error of image points is relatively small, its results are comparable to those of maximum likelihood estimation using Newton-type optimizers; and when processing image-point correspondences cross a small number of views, the proposed method is by far more efficient than the Newton-type optimizers.	Univ Mannheim, Lehrstuhl Informat 5, D-68131 Mannheim, Germany	University of Mannheim	Liu, B (corresponding author), Univ Mannheim, Lehrstuhl Informat 5, B6,23-29C, D-68131 Mannheim, Germany.							Bartoli A, 2002, INT C PATT RECOG, P560, DOI 10.1109/ICPR.2002.1048365; BURINGTON R, 1970, HDB PROBABILITY STAT; CARTER E, GENERATING GAUSSIAN; CHEN Q, 1996, P ECCV; Golub Gene H., 2013, MATRIX COMPUTATION, V3; HARTLEY R, 2004, P IEEE C COMP VIS PA; HARTLEY R, 1997, COMPUTER VISION IMAG, V68; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Kanatani K., 1996, STAT OPTIMIZATION GE; MA Y, 1999, J COMPUTER VISION, V44, P219; MA Y, 2001, J COMMUNICATIONS INF, P51; MAHAMUD S, 2001, P IEEE C COMP VIS PA; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074; Zhang Z., 2001, MSRTR0154	15	14	16	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2005	65	3					175	188		10.1007/s11263-005-3670-5	http://dx.doi.org/10.1007/s11263-005-3670-5			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	999ZE					2022-12-18	WOS:000234429900004
J	Felsberg, M; Duits, R; Florack, L				Felsberg, M; Duits, R; Florack, L			The monogenic scale space on a rectangular domain and its features	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	4th International Conference on Scale Space Methods in Computer Vision	JUN 10-12, 2003	ISLE SKYE, SCOTLAND	British Machine Vis Assoc, Kings Coll London, IT Univ Copenhagen		scale space; monogenic; Poisson kernel; Riesz transform; local phase; DCT		In this paper we present a novel method to implement the monogenic scale space on a rectangular domain. The monogenic scale space is a vector valued scale space based on the Poisson scale space, which establishes a sophisticated alternative to the Gaussian scale space. Previous implementations of the monogenic scale space are Fourier transform based, and therefore suffer from the implicit periodicity in case of finite domains. The features of the monogenic scale space, including local amplitude, local phase, local orientation, local frequency, and phase congruency, are much easier to interpret in terms of image features evolving through scale than in the Gaussian case. Furthermore, applying results from harmonic analysis, relations between the features are obtained which improve the understanding of image analysis. As applications, we present a very simple but still accurate approach to image reconstruction from local amplitude and local phase and a method for extracting the evolution of lines and edges through scale.	Linkoping Univ, Dept Elect Engn, Comp Vis Lab, S-58183 Linkoping, Sweden	Linkoping University	Felsberg, M (corresponding author), Linkoping Univ, Dept Elect Engn, Comp Vis Lab, S-58183 Linkoping, Sweden.	mfe@isy.liu.se; R.Duits@tue.nl; L.M.J.Florack@tue.nl		Felsberg, Michael/0000-0002-6096-3648				AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784; BEHAR J, 1992, IEEE T SIGNAL PROCES, V40, P736, DOI 10.1109/78.127948; COLTON DL, 1988, PARTIAL DIFFERENTIAL; DAMON J, 1995, J DIFFER EQUATIONS, V115, P368, DOI 10.1006/jdeq.1995.1019; Duits R, 2004, J MATH IMAGING VIS, V20, P267, DOI 10.1023/B:JMIV.0000024043.96722.aa; Duits R, 2003, LECT NOTES COMPUT SC, V2695, P494; Felsberg M, 2004, J MATH IMAGING VIS, V21, P5, DOI 10.1023/B:JMIV.0000026554.79537.35; Felsberg M, 2001, IEEE T SIGNAL PROCES, V49, P3136, DOI 10.1109/78.969520; FELSBERG M, 2002, LECT NOTES COMPUTER, V2449, P248; Felsberg M., 2000, MUSTERERKENNUNG 2000, P195; FELSBERG M, 2001, LECT NOTES COMPUTER, V2191, P124; FELSBERG M, 2002, THESIS CHRISTIAN ALB; Florack L, 2000, J MATH IMAGING VIS, V12, P65, DOI 10.1023/A:1008304909717; Granlund G.H., 1995, SIGNAL PROCESSING CO; HEIN W, 1990, STRUKTUR DARSTELLUNG; Jain A. K., 1989, FUNDAMENTALS DIGITAL; KOEDERINK JJ, 1989, IEEE T PATTERN ANAL, V11, P1222; KOVESI P, 1999, VIDERE J COMPUTER VI, V1; Larkin KG, 2001, J OPT SOC AM A, V18, P1862, DOI 10.1364/JOSAA.18.001862; Papoulis A., 1962, FOURIER INTEGRAL ITS; PAUWELS EJ, 1995, IEEE T PATTERN ANAL, V17, P691, DOI 10.1109/34.391411; Silvester P., 1983, FINITE ELEMENTS ELEC; Weickert J, 1999, J MATH IMAGING VIS, V10, P237, DOI 10.1023/A:1008344623873	24	14	16	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2005	64	2-3					187	201		10.1007/s11263-005-1843-x	http://dx.doi.org/10.1007/s11263-005-1843-x			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	961YC					2022-12-18	WOS:000231696700008
J	Maybank, SJ				Maybank, SJ			The Fisher-Rao metric for projective transformations of the line	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						asymptotic expansion; canonical volume; Fisher-Rao metric; heat equation; probability of false detection; projective transformation of the line; Riemannian manifold		A conditional probability density function is defined for measurements arising from a projective transformation of the line. The conditional density is a member of a parameterised family of densities in which the parameter takes values in the three dimensional manifold of projective transformations of the line. The Fisher information of the family defines on the manifold a Riemannian metric known as the Fisher-Rao metric. The Fisher-Rao metric has an approximation which is accurate if the variance of the measurement errors is small. It is shown that the manifold of parameter values has a finite volume under the approximating metric. These results are the basis of a simple algorithm for detecting those projective transformations of the line which are compatible with a given set of measurements. The algorithm searches a finite list of representative parameter values for those values compatible with the measurements. Experiments with the algorithm suggest that it can detect a projective transformation of the line even when the correspondences between the components of the measurements in the domain and the range of the projective transformation are unknown.	Univ London Birkbeck Coll, Sch Comp Sci & Informat Syst, London WC1E 7HX, England	University of London; Birkbeck University London	Maybank, SJ (corresponding author), Univ London Birkbeck Coll, Sch Comp Sci & Informat Syst, Malet St, London WC1E 7HX, England.	sjmaybank@dcs.bbk.ac.uk						Abramowitz M., 1948, HDB MATH FUNCTIONS F, V55; AMARI SI, 1985, LECT NOTES COMPUTER, V28; Chavel I., 1984, EIGENVALUES RIEMANNI; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FERRYMAN JM, 2001, PETS 2001 DATABASE; Fisher R.A., 1922, PHILOS T R SOC LON A, V222, P309, DOI [DOI 10.1098/RSTA.1922.0009, 10.1098/rsta.1922.0009]; Forsyth David A, 2012, COMPUTER VISION MODE; Gallot S., 1990, RIEMANNIAN GEOMETRY; GOLUB GH, 1996, MAYTRIX COMPUTATIONS; GONZALEZ R., 2002, DIGITAL IMAGE PROCES; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; KANATANI K, 1996, STAT COMPUTATION GEO; KOTZ S, 1992, SPRINGER SERIES STAT, V1; MAYBANK SJ, 2004, IEEE T PATTER ANAL M, V26; MAYBANK SJ, 2003, P ROY SOC LOND A MAT, V459, P1; Myung IJ, 2000, P NATL ACAD SCI USA, V97, P11170, DOI 10.1073/pnas.170283897; Rao C.R., 1945, BULL CALCUTTA MATH S, V37, P81, DOI DOI 10.1007/978-1-4612-0919-5_15; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; TORR PHS, 1993, IMAGE VISION COMPUT, V11, P180, DOI 10.1016/0262-8856(93)90034-E; WERMAN M, 1999, P COMP VIS PATT REC, V2, P552; Wolfram S, 1999, MATH BOOK	22	14	14	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2005	63	3					191	206		10.1007/s11263-005-6877-6	http://dx.doi.org/10.1007/s11263-005-6877-6			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	928YR		Green Accepted, Green Submitted			2022-12-18	WOS:000229308500002
J	Yu, YH; Chang, JT				Yu, YH; Chang, JT			Shadow graphs and 3D texture reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D texture; surface geometry; shape-from-shadow; shadow graph; shading; photometric stereo; optimization	REFLECTANCE; SHAPE; SURFACES	We present methods for recovering surface height fields such as geometric details of 3D textures by incorporating shadow constraints. We introduce shadow graphs which give a new graph-based representation for shadow constraints. It can be shown that the shadow graph alone is sufficient to solve the shape-from-shadow problem from a dense set of images. Shadow graphs provide a simpler and more systematic approach to represent and integrate shadow constraints from multiple images. To recover height fields from a sparse set of images, we propose a method for integrated shadow and shading constraints. Previous shape-from-shadow algorithms do not consider shading constraints while shape-from-shading usually assumes there is no shadow. Our method is based on collecting a set of images from a fixed viewpoint as a known light source changes its position. It first builds a shadow graph from shadow constraints from which an upper bound for each pixel can be derived if the height values of a small number of pixels are initialized correctly. Finally, a constrained optimization procedure is designed to make the results from shape-from-shading consistent with the height bounds derived from the shadow constraints. Our technique is demonstrated on both synthetic and real imagery.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; CALTECH, Jet Prop Lab, Pasadena, CA 91109 USA	University of Illinois System; University of Illinois Urbana-Champaign; California Institute of Technology; National Aeronautics & Space Administration (NASA); NASA Jet Propulsion Laboratory (JPL)	Yu, YH (corresponding author), Univ Illinois, Dept Comp Sci, 201 N Goodwin Ave, Urbana, IL 61801 USA.	yyz@cs.uiuc.edu; johnny.t.chang@jpl.nasa.gov	YU, YIZHOU/D-1603-2013; /F-3345-2010	/0000-0002-0470-5548				Belhumeur PN, 1997, PROC CVPR IEEE, P1060, DOI 10.1109/CVPR.1997.609461; Belhumeur PN, 1998, INT J COMPUT VISION, V28, P245, DOI 10.1023/A:1008005721484; Branca A, 2002, INT C PATT RECOG, P214, DOI 10.1109/ICPR.2002.1044656; Dana KJ, 1997, PROC CVPR IEEE, P151, DOI 10.1109/CVPR.1997.609313; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; DANA KJ, 1998, P IEEE C COMP VIS PA; DANA KJ, 1999, INT C COMP VIS; Daum M, 1998, PROC CVPR IEEE, P461, DOI 10.1109/CVPR.1998.698646; DONG J, 2002, 2 INT WORKSH TEXT AN, P41; Dupuis P., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P259, DOI 10.1007/BFb0028359; Georghiades AS, 1999, IEEE WORKSHOP ON MULTI-VIEW MODELING & ANALYSIS OF VISUAL SCENES (MVIEW'99). PROCEEDINGS, P47, DOI 10.1109/MVIEW.1999.781082; Hampel F., 1986, ROBUST STAT; HASSIN R, 1994, INFORM PROCESS LETT, V51, P133, DOI 10.1016/0020-0190(94)00086-7; HATZITHEODOROU M, 1989, P IM UND WORKSH, P1012; HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3; IKEUCHI K, 1981, IEEE T PATTERN ANAL, V3, P661, DOI 10.1109/TPAMI.1981.4767167; Jacobs D, 1997, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.1997.609321; Karp RM., 1972, COMPLEXITY COMPUTER, P85; KENDER J, 1987, INT C COMP VIS, P539; KRIEGMAN DJ, 1998, COMPUTER VISION ECCV; Langer M.S., 1995, P IEEE RSJ IROS, P390; Leclerc Y. G., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P552, DOI 10.1109/CVPR.1991.139752; Leung T., 1999, INT C COMP VIS; Liu XG, 2001, COMP GRAPH, P97; MEDIONI G, 1983, IEEE C COMP VIS PATT, P73; NAYAR SK, 1990, IEEE T ROBOTIC AUTOM, V6, P418, DOI 10.1109/70.59367; NAYAR SK, 1991, INT J COMPUT VISION, V6, P2; Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520; Press WH, 1988, NUMERICAL RECIPES C; Shafer S. A., 1985, SHADOWS SILHOUETTES; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Stewart AJ, 1997, IEEE T PATTERN ANAL, V19, P1020, DOI 10.1109/34.615450; TERZOPOULOS D, 1988, IEEE T PATTERN ANAL, V10, P417, DOI 10.1109/34.3908; Van Gool L, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P286, DOI 10.1109/TDPVT.2002.1024073; Woodham R. J., 1989, SHAPE SHADING, P513; YU Y, 2002, 7 EUR C COMP VIS, V2, P31	36	14	22	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-MAY	2005	62	1-2					35	60						26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	866XV					2022-12-18	WOS:000224807600004
J	Lin, ZC; Wong, TT; Shum, HY				Lin, ZC; Wong, TT; Shum, HY			Relighting with the reflected irradiance field: Representation, sampling and reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						sampling; BRDF; light field; Lumigraph; plenoptic functions; image-based rendering; relighting		Image-based relighting (IBL) is a technique to change the illumination of an image-based object/scene. In this paper, we define a representation called the reflected irradiance field which records the light reflected from a scene as viewed at a fixed viewpoint as a result of moving a point light source on a plane. It synthesizes a novel image under a different illumination by interpolating and superimposing appropriate recorded samples. Furthermore, we study the minimum sampling problem of the reflected irradiance field, i.e., how many light source positions are needed. We find that there exists a geometry-independent bound for the sampling interval whenever the second-order derivatives of the surface BRDF and the minimum depth of the scene are bounded. This bound ensures that when the novel light source is on the plane, the error in the reconstructed image is controlled by a given tolerance, regardless of the geometry. We also analyze the bound of depth error so that the extra reconstruction error can also be governed when the novel light source is off-plane. Experiments on both synthetic and real surfaces are conducted to verify our analysis.	Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China	Chinese University of Hong Kong								Amanatides J., 1992, Proceedings. Graphics Interface '92, P86; Belhumeur PN, 1996, PROC CVPR IEEE, P270, DOI 10.1109/CVPR.1996.517085; Bouguet J. Y., 2000, CAMERA CALIBRATION T; Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932; Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395; Cook R. L., 1984, Computers & Graphics, V18, P137; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884; Ferwerda J. A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P143, DOI 10.1145/258734.258818; Fournier A., 1993, Proceedings Graphics Interface '93, P254; GIBSON S, 2000, EUR WORKSH REND REND, P365; Golub G. H., 1996, MATRIX COMPUTATIONS; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; Haeberli P., 1992, SYNTHETIC LIGHTING P; HANRAHAN P, 1993, COMPUTER GRAPHICS, P43; Horn B., 1986, ROBOT VISION, P1; Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929; Jeffry N., 1994, EUR WORKSH REND, P359; Lafortune E. P. F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P117, DOI 10.1145/258734.258801; LAFORTUNE EPF, 2000, COMMUNICATIONS; Landy M.S., 1991, COMPUTATIONAL MODELS; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Lin ZC, 2001, PROC CVPR IEEE, P561; Lin ZC, 2000, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2000.855873; Loscos C, 2000, IEEE T VIS COMPUT GR, V6, P289, DOI 10.1109/2945.895874; Magda S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P391, DOI 10.1109/ICCV.2001.937652; McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398; Miller G., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P281; Oren M., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P239, DOI 10.1145/192161.192213; Pattanaik S. N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P287, DOI 10.1145/280814.280922; Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812; PRESS WH, 1997, NUMERICAL RECIPIES C; Sato Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P379, DOI 10.1145/258734.258885; Shashua A, 1997, INT J COMPUT VISION, V21, P99, DOI 10.1023/A:1007975506780; Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573; Wang R., 1999, NUMERICAL APPROXIMAT; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; WONG TT, 2001, IN PRESS IEEE T MULT; WONG TT, 2001, IRRAD DEMO VERSION 1; WONG TT, 1997, EUR REND WORKSH 1997, P13; YU Y, 1998, COMPUTER GRAPHICS, P207; Yu YZ, 1999, COMP GRAPH, P215; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zhang ZY, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1041, DOI 10.1109/ICCV.1998.710845	44	14	15	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP-OCT	2002	49	2-3					229	246		10.1023/A:1020153824351	http://dx.doi.org/10.1023/A:1020153824351			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	590TA					2022-12-18	WOS:000177837100007
J	Yuille, AL; Coughlan, JM; Wu, Y; Zhu, SC				Yuille, AL; Coughlan, JM; Wu, Y; Zhu, SC			Order parameters for detecting target curves in images: When does high level knowledge help ?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Bayesian inference; curve detection; order parameters; minimax entropy	PERFORMANCE; AMBIGUITIES	Many problems in vision can be formulated as Bayesian inference. It is important to determine the accuracy of these inferences and how they depend on the problem domain. In this paper, we provide a theoretical framework based on Bayesian decision theory which involves evaluating performance based on an ensemble of problem instances. We pay special attention to the task of detecting a target in the presence of background clutter. This framework is then used to analyze the detectability of curves in images. We restrict ourselves to the case where the probability models are ergodic (both for the geometry of the curve and for the imaging). These restrictions enable us to use techniques from large deviation theory to simplify the analysis. We show that the detectability of curves depend on a parameter K which is a function of the probability distributions characterizing the problem. At critical values of K the target becomes impossible to detect on average. Our framework also enables us to determine whether a simpler approximate model is sufficient to detect the target curve and hence clarify how much information is required to perform specific tasks. These results generalize our previous work (Yuille, A.L. and Coughlan, J.M. 2000. Pattern Analysis and Machine Intelligence PAMI, 22(2):160-173) by placing it in a Bayesian decision theory framework, by extending the class of probability models which can be analyzed, and by analysing the case where approximate models are used for inference.	Smith Kettlewell Eye Res Inst, San Francisco, CA 94115 USA; Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; Ohio State Univ, Dept Informat & Comp Sci, Columbus, OH 43210 USA	The Smith-Kettlewell Eye Research Institute; University of California System; University of California Los Angeles; University System of Ohio; Ohio State University	Yuille, AL (corresponding author), Smith Kettlewell Eye Res Inst, 2318 Fillmore St, San Francisco, CA 94115 USA.	yuille@ski.org; coughlan@ski.org; ywu@math.ucla.edu; szhu@cis.ohio-state.edu		Yuille, Alan L./0000-0001-5207-9249				AMARI SI, 1982, ANN STAT, V10, P357, DOI 10.1214/aos/1176345779; Amit D. J, 1989, MODELLING BRAIN FUNC; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BOWYE KW, 1998, EMPIRICAL EVALUATION; COUGHLA JM, 1999, SPRINGER VERLAG LECT, V1654, P189; COVE TM, 1991, ELEMENTS INFORMATION; DeGroot M. H., 1970, OPTIMAL STAT DECISIO; Dembo A., 1998, LARGE DEVIATION TECH; GEIGER D, 1997, EMMCVPR97, P295; Geman D, 1996, IEEE T PATTERN ANAL, V18, P1, DOI 10.1109/34.476006; Green DM, 1988, SIGNAL DETECTION THE; Grenander U, 1998, IEEE T PATTERN ANAL, V20, P790, DOI 10.1109/34.709572; GRIFFITHS RB, 1971, COMMUN MATH PHYS, V23, P169, DOI 10.1007/BF01877738; Heath MD, 1997, IEEE T PATTERN ANAL, V19, P1338, DOI 10.1109/34.643893; Hoover A, 1996, IEEE T PATTERN ANAL, V18, P673, DOI 10.1109/34.506791; Kass M., 1987, International Journal of Computer Vision, V1, P321, DOI 10.1007/BF00133570; Knill DC, 1996, PERCEPTION BAYESIAN; Konishi S., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P573, DOI 10.1109/CVPR.1999.786996; Lanford III O., 1973, STAT MECH MATH PROBL; Lewis J. T., 1995, MARKOV PROCESS RELAT, V1, P319; Murray M. K., 1993, MONOGRAPHS STAT APPL, V48; OSULLIVAN JA, 1998, IEEE T INFORMATION T, V44; Pearl Judea, 1984, HEURISTICS; Rajagopalan AN, 1998, INT J COMPUT VISION, V30, P175, DOI 10.1023/A:1008019215914; RATCHES JA, 1997, IEEE T PAMI, V19; Szeliski R, 1997, IEEE T PATTERN ANAL, V19, P506, DOI 10.1109/34.589211; TSOTSOS JK, 1990, BEHAV BRAIN SCI, V13, P423, DOI 10.1017/S0140525X00079577; Vapnik V.N, 1998, STAT LEARNING THEORY; Wu YN, 2000, INT J COMPUT VISION, V38, P247, DOI 10.1023/A:1008199424771; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903; Yuille A. L., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P631, DOI 10.1109/CVPR.1999.784990; Yuille AL, 2000, IEEE T PATTERN ANAL, V22, P160, DOI 10.1109/34.825754; Yuille AL, 2000, PATTERN RECOGN, V33, P603, DOI 10.1016/S0031-3203(99)00075-8; YUILLE AL, 2000, P SOC PHOTO-OPT INS, P333; YUILLE AL, 2000, ORDER PARAMETERS MIN; Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1170, DOI 10.1109/34.809110; Zhu SC, 1997, NEURAL COMPUT, V9, P1627, DOI 10.1162/neco.1997.9.8.1627	37	14	14	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		2001	41	1-2					9	33		10.1023/A:1011156931605	http://dx.doi.org/10.1023/A:1011156931605			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	427YL					2022-12-18	WOS:000168434300003
J	Rajagopalan, AN; Chaudhuri, S				Rajagopalan, AN; Chaudhuri, S			Performance analysis of maximum likelihood estimator for recovery of depth from defocused images and optimal selection of camera parameters	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						depth from defocus; Gaussian blur; blur identification; auto-regressive process; maximum likelihood estimator; optimality criterion; Cramer-Rao bound; log-likelihood function	EXPECTATION-MAXIMIZATION ALGORITHM; NONCAUSAL BLURS; IDENTIFICATION; RESTORATION; FOCUS	The recovery of depth from defocused images involves calculating the depth of various points in a scene by modeling the effect that the focal parameters of the camera have on images acquired with a small depth of field. In the existing methods on depth from defocus (DFD), two defocused images of a scene are obtained by capturing the scene with different sets of camera parameters. Although the DFD technique is computationally simple, the accuracy is somewhat limited compared to the stereo algorithms. Further, an arbitrary selection of the camera settings can result in observed images whose relative blurring is insufficient to yield a good estimate of the depth. In this paper, we address the DFD problem as a maximum likelihood (ML) based blur identification problem. We carry out performance analysis of the ML estimator and study the effect of the degree of relative blurring on the accuracy of the estimate of the depth. We propose a criterion for optimal selection of camera parameters to obtain an improved estimate of the depth. The optimality criterion is based on the Cramer-Rao bound of the variance of the error in the estimate of blur. A number of simulations as well as experimental results on real images are presented to substantiate our claims.	Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Bombay	Rajagopalan, AN (corresponding author), Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India.			Ambasamudram, Rajagopalan/0000-0002-0006-6961				Andrews H.C., 1977, DIGITAL IMAGE RESTOR; BIEMOND J, 1988, IEEE T CIRCUITS SYST, V35, P385, DOI 10.1109/31.1753; Born M., 1968, PRINCIPLES OPTICS; BOVE VM, 1993, J OPT SOC AM A, V10, P561, DOI 10.1364/JOSAA.10.000561; ENS J, 1993, IEEE T PATTERN ANAL, V15, P97, DOI 10.1109/34.192482; Gonzalez R. C., 1987, DIGITAL IMAGE PROCES; Goodman J. W., 2005, MCGRAW HILL PHYS QUA; HORN BKP, 1987, ROBOT VISION; Hwang T.-l., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P476, DOI 10.1109/CVPR.1989.37890; JAIN AK, 1981, P IEEE, V69, P502, DOI 10.1109/PROC.1981.12021; JARVIS RA, 1983, IEEE T PATTERN ANAL, V5, P122, DOI 10.1109/TPAMI.1983.4767365; Krotkov EP, 1989, ACTIVE COMPUTER VISI; LAGENDIJK RL, 1990, IEEE T ACOUST SPEECH, V38, P1180, DOI 10.1109/29.57545; LAGENDIJK RL, 1990, OPT ENG, V29, P422, DOI 10.1117/12.55611; LAGENDIJK RL, 1990, P IEEE INT C AC SPEE, P1889; LAGENDIJK RL, 1988, P IEEE INT C AC SPEE, P992; LAY KT, 1990, OPT ENG, V29, P436, DOI 10.1117/12.55612; MENDEL JM, 1987, LESSONS DIGITAL ESTI; Nayar SK, 1996, IEEE T PATTERN ANAL, V18, P1186, DOI 10.1109/34.546256; Pavlovic G, 1992, IEEE T IMAGE PROCESS, V1, P496, DOI 10.1109/83.199919; PENTLAND A, 1994, J OPT SOC AM A, V11, P2925, DOI 10.1364/JOSAA.11.002925; Pentland A., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P256, DOI 10.1109/CVPR.1989.37858; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; PENTLAND AP, 1982, P IM UND WORKSH; Rajagopalan AN, 1997, IEEE T PATTERN ANAL, V19, P1158, DOI 10.1109/34.625126; Rajagopalan AN, 1997, COMPUT VIS IMAGE UND, V68, P309, DOI 10.1006/cviu.1997.0534; Rajagopalan AN, 1997, PROC CVPR IEEE, P219, DOI 10.1109/CVPR.1997.609323; Rao C.R., 1965, LINEAR STAT INFERENC, V2nd; SCHREIBER WF, 1986, FUNDAMENTALS ELECT I; Subbarao M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P773, DOI 10.1109/CVPR.1992.223176; SUBBARAO M, 1988, P INT C COMP VIS, P149; Surya G., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P61, DOI 10.1109/CVPR.1993.340978; TEKALP AM, 1986, IEEE T ACOUST SPEECH, V34, P963, DOI 10.1109/TASSP.1986.1164886; WATANABE M, 1996, IEEE C COMP VIS PATT, P431	34	14	15	0	7	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1998	30	3					175	190		10.1023/A:1008019215914	http://dx.doi.org/10.1023/A:1008019215914			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	156FL					2022-12-18	WOS:000077990900002
J	Casadei, S; Mitter, S				Casadei, S; Mitter, S			Hierarchical image segmentation - Part I: Detection of regular curves in a vector graph	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							ORGANIZATION; RELAXATION; VISION	The problem of edge detection is viewed as a hierarchy of detection problems where the geometric objects to be detected (e.g., edge points, curves, regions) have increasing complexity and spatial extent. An early stage of the proposed hierarchy consists in detecting the regular portions of the visible edges. The input to this stage is given by a graph whose vertices are tangent vectors representing local and uncertain information about the edges. A model relating the input vector graph to the curves to be detected is proposed. An algorithm with linear time complexity is described which solves the corresponding detection problem in a worst-case scenario. The stability of curve reconstruction in the presence of uncertain information and multiple responses to the same edge is analyzed and addressed explicitly by the proposed algorithm.	MIT, Informat & Decis Syst Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Casadei, S (corresponding author), MIT, Informat & Decis Syst Lab, Cambridge, MA 02139 USA.	casadei@lids.mit.edu						[Anonymous], 1985, PERCEPTUAL ORG VISUA; Bienenstock E., 1995, HDB BRAIN THEORY NEU, V2nd Edn, P223, DOI [10.1098/rstb.2019.0306, DOI 10.1098/RSTB.2019.0306]; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Casadei S., 1996, P IEEE C COMP VIS PA; CASADEI S, 1995, LIDSTH2306 MIT LAB I; CASADEI S, 1996, P 4 EUR C COMP VIS; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; Cohen LD, 1996, PROC CVPR IEEE, P666, DOI 10.1109/CVPR.1996.517144; DAVID C, 1990, INT J COMPUT VISION, V5, P219, DOI 10.1007/BF00126500; DERICHE R, 1993, P IEEE C COMP VIS PA; DOLAN J, 1992, P IEEE C COMP VIS PA; ELDER J, 1996, P 4 EUR C COMP VIS, P57; Geiger D, 1996, PROC CVPR IEEE, P155, DOI 10.1109/CVPR.1996.517068; GEIGER D, 1996, P 4 EUR C COMP VIS, P413; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; HANCOCK ER, 1990, IEEE T PATTERN ANAL, V12, P165, DOI 10.1109/34.44403; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; IVERSON LA, 1995, IEEE T PATTERN ANAL, V17, P982, DOI 10.1109/34.464562; Kanizsa Gaetano, 1979, ORG VISION ESSAYS GE; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; MARROQUIN J, 1987, J AM STAT ASSOC, V82, P76, DOI 10.2307/2289127; MOHAN R, 1992, IEEE T PATTERN ANAL, V14, P616, DOI 10.1109/34.141553; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Nayar SK, 1996, PROC CVPR IEEE, P471, DOI 10.1109/CVPR.1996.517114; Nitzberg M., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P138, DOI 10.1109/ICCV.1990.139511; Nitzberg M., 1993, LECT NOTES COMPUTER, V662; PARENT P, 1989, IEEE T PATTERN ANAL, V11; Perona P., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P52, DOI 10.1109/ICCV.1990.139492; PERONA P, 1992, P EUR C COMP VIS, P3; RICHARDSON T, 1994, GEOMETRY DRIVEN DIFF; ROHR K, 1992, INT J COMPUTER VISIO, V9; ROSENTHALER L, 1992, P 2 EUR C COMP VIS; SARKAR S, 1993, IEEE T SYST MAN CYB, V23, P382, DOI 10.1109/21.229452; SHAASHUA A, 1988, P 2 INT C COMP VIS, P321, DOI DOI 10.1109/CCV.1988.590008; SHAH J, 1996, P IEEE C COMP VIS PA, P1836; SUBIRANAVILANOV.JB, 1992, IMAGE UNDERSTANDING; WILLIAMS LR, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P408, DOI 10.1109/ICCV.1995.466910; ZHU S, 1995, CICSP454; Zucker S. W., 1988, P 2 INT C COMP VIS	40	14	14	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1998	27	1					71	100		10.1023/A:1007905813513	http://dx.doi.org/10.1023/A:1007905813513			30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZH371					2022-12-18	WOS:000073101800004
J	Navab, N; Faugeras, OD				Navab, N; Faugeras, OD			The critical sets of lines for camera displacement estimation: A mixed euclidean-projective and constructive approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							3-DIMENSIONAL MOTION; CORRESPONDENCES; CALIBRATION; ALGORITHM; SURFACES; OBJECTS	The problem of the recovery of the motion, and the structure from motion is relevant to many computer vision applications. Many algorithms have been proposed to solve this problem. Some of these use line correspondences. For obvious practical reasons, it is important to study the limitation of such algorithms. In this paper, we are concerned with the problem of recovering the relative displacements of a camera by using line matches in three views. In particular, we want to know whether there exist sets of 3D lines such that no matter how many lines we observe there will always be several solutions to the relative displacement estimation problem. Such sets of lines may be called critical in the sense that they defeat the corresponding algorithm. This question has been studied in detail in the case of point matches by early-century Austrian photogrammeters and, independently, in the mid-seventies and early-eighties by computer vision scientists. The answer lies in the idea of a critical surface. The case of lines has been much less studied. Recently, Buchanan (1992a, 1992b) provided a first analysis of the problem in which he gave a positive answer: there exist critical sets of lines and they are pretty big (infinity(2) lines). In general these sets are algorithm dependent, for example the critical set of lines for the Liu-Huang algorithm introduced in (Buchanan, 1992a), but Buchanan has shown that there is a critical set that defeats any algorithm. This paper is an attempt to build on his work and extend it in several directions. First, we cast his purely projective analysis in a more euclidean framework better suited to applications and, currently, more familiar to most of the computer vision community. Second, we clearly relate his critical set to those of previously published algorithms, in particular (Liu and Huang, 1988a, 1988b). Third, we provide an effective, i.e., computational, approach for describing these critical sets in terms of simple geometric properties. This has allowed us to scrutinize the structure of the critical sets which we found to be both intricate and beautiful.	INRIA SOPHIA ANTIPOLIS, F-06561 VALBONNE, FRANCE		Navab, N (corresponding author), SIEMENS CORP RES, 755 COLL RD E, PRINCETON, NJ 08540 USA.							Abdel-Aziz Y., 1971, P S CLOSE RANGE PHOT, P1, DOI [10.14358/PERS.81.2.103, DOI 10.1080/10671188.1967.10616517]; ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855; BUCHANAN T, 1992, GEOMETRIAE DEDICATA, V44, P223; BUCHANAN T, 1992, P 2 EUR C COMP VIS S, P730; Chasles M., 1855, NOUV ANN MATH, V14, P50; FAIG W, 1975, PHOTOGRAMM ENG REM S, V41, P1479; Faugeras O. D., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P15; Faugeras O. D., 1990, International Journal of Imaging Systems and Technology, V2, P356, DOI 10.1002/ima.1850020410; Faugeras O. D., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P25; FAUGERAS OD, 1990, INT J COMPUT VISION, V4, P225, DOI 10.1007/BF00054997; FAUGERAS OD, 1990, 1157 INRIA; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FINSTERWALDER S, 1987, JAHRESBER DTSCH MATH, V6, P1; HILDRETH E, 1983, MEASUREMENT VISUAL M; HOFMANN W, 1950, THESIS FAKULTAT BAUW; HORN B, 1981, ARTIF INTELL, V20, P199; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; HORN BKP, 1987, INT J COMPUT VISION, V1, P263; KANATANI K, 1992, GEOMETRIC COMPUTATIO; Krames J., 1940, MONATSHEFTE MATH PHY, V49, P327, DOI DOI 10.1007/BF01707311; LIU YC, 1988, COMPUT VISION GRAPH, V43, P37, DOI 10.1016/0734-189X(88)90041-2; LIU YC, 1988, COMPUT VISION GRAPH, V44, P35, DOI 10.1016/S0734-189X(88)80030-6; LIU YC, 1990, IEEE T PATTERN ANAL, V12, P28, DOI 10.1109/34.41381; LIU YC, 1986, OCT P INT C PATT REC, P306; Longuet-Higgins H. C., 1984, First Conference on Artificial Intelligence Applications (Cat. No. 84CH2107-1), P395; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LONGUETHIGGINS HC, 1988, P ROY SOC LOND A MAT, V418, P1, DOI 10.1098/rspa.1988.0071; LUONG T, 1994, IN PRESS 3 EUR C COM; Maybank S., 1992, THEORY RECONSTRUCTIO; MAYBANK SJ, 1990, PHILOS T ROY SOC A, V332, P1, DOI 10.1098/rsta.1990.0099; MAYBANK SJ, 1987, THESIS U LONDON LOND; Navab N., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P254, DOI 10.1109/CVPR.1993.340981; NAVAB N, 1993, P 4 INT C COMP VIS B; NAVAB N, 1993, THESIS U PARIS 11 OR; OKAMOTO A, 1984, PHOTOGRAMM ENG REM S, V50, P705; OKAMOTO A, 1981, PHOTOGRAMM ENG REM S, V47, P1437; Semple J., 1949, INTRO ALGEBRAIC GEOM; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; STURM R, 1909, LEHRE GEOMETRISCHEN; Subbarao M., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P687; TSAI R, 1982, IEEE T ASSP, P30; Tsai R.Y., 1986, P IEEE C COMP VIS PA, P364; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; VIEVILLE T, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P517; WAXMAN AM, 1986, IEEE T PATTERN ANAL, V8, P715, DOI 10.1109/TPAMI.1986.4767853; WENG J, 1992, IEEE T PAMI, V14; WOLF PR, ELEMENTS PHOTOGRAMME; WONG KW, 1975, PHOTOGRAMM ENG REM S, V41, P1355; WUNDERLICH W, 1941, MONATSHEFTE MATH PHY, V50, P151; ZELLER M, 1952, TXB PHOTOGRAMMETRY	52	14	14	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	1997	23	1					17	44		10.1023/A:1007911807871	http://dx.doi.org/10.1023/A:1007911807871			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XK559					2022-12-18	WOS:A1997XK55900002
J	Cass, TA				Cass, TA			Polynomial-time geometric matching for object recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								This paper considers the task of recognition and position determination, by computer, of a 2D or 3D object where the input is a single 2D brightness image, and a model of the object is known a priori. The primary contribution of this paper is a novel formulation and methods for local geometric feature matching. This formulation is based on analyzing geometric constraints on transformations of the model features which geometrically align it with a substantial subset of image features. Specifically, the formulation and algorithms for geometric feature matching presented here provide a guaranteed method for finding all feasible interpretations of the data in terms of the model. This method is robust to measurement uncertainty in the data features and to the presence of spurious scene features, and its time and space requirements are only polynomial in the size of the feature sets. This formulation provides insight into the fundamental nature of the matching problem, and the algorithms commonly used in computer vision for solving it.			Cass, TA (corresponding author), XEROX CORP,PALO ALTO RES CTR,3333 COYOTE HILL RD,PALO ALTO,CA 94304, USA.							ALT H, 1988, DISCRETE COMPUT GEOM, V3, P237, DOI 10.1007/BF02187910; ALTER DT, 1992, THESIS MIT; ARKIN E, 1991, P 2 ACM SIAM S DISCR; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BAIRD HS, 1985, MODEL BASED IMAGE MA; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; BREUEL TM, 1990, 1259 MIT AI; CASS T, 1992, THESIS MIT; CASS TA, 1996, P EUR C COMP VIS CAM; CASS TA, 1988, THESIS MIT; CASS TA, 1990, P INT C COMP VIS OS; CHEW L, 1993, P 5 CAN C COMP GEOM; CLEMENS DT, 1986, THESIS MIT; COSTA M, 1990, 6TH P ISR C AI, P35; Edelsbrunner H., 1987, ALGORITHMS COMBINATO; Goodrich M. T., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P103, DOI 10.1145/177424.177572; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; GRIMSON WEL, 1990, ARTIF INTELL, V44, P121, DOI 10.1016/0004-3702(90)90100-E; HEFFERNAN P, 1994, COMPUTATIONAL GEOMET; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; HUTTENLOCHER DP, 1992, P EUR C COMP VIS GEN; JAZCOBS DW, 1992, THESIS MIT; Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995; Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; RIGOUTSOS I, 1995, COMPUT VIS IMAGE UND, V62, P11, DOI 10.1006/cviu.1995.1038; STOCKMAN G, 1982, IEEE T PATTERN ANAL, V4; THOMPSON DW, 1987, IEEE J ROBOTIC AUTOM, P208; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; WELLS WM, 1991, P IEEE C COMP VIS PA	30	14	17	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1997	21	1-2					37	61		10.1023/A:1007971405872	http://dx.doi.org/10.1023/A:1007971405872			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WM797					2022-12-18	WOS:A1997WM79700003
J	Stewart, CV; Flatland, RY; Bubna, K				Stewart, CV; Flatland, RY; Bubna, K			Geometric constraints and stereo disparity computation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							SURFACE RECONSTRUCTION; M-ESTIMATORS; VISION; ALGORITHM; IMAGES	Most stereo techniques compute disparity assuming that it varies slowly along surfaces. We quantify and justify this assumption, using weak assumptions about surface orientation distributions in the world to derive the density of disparity surface orientations. The small disparity change assumption is justified by the orientation density's heavy bias toward disparity surfaces that are nearly parallel to the image plane. In addition, the bias strengthens with smaller baselines, larger focal lengths, and as surfaces move farther from the cameras. To analyze current stereo techniques, we derive three densities from the first density, those of the disparity gradient magnitude, the directional derivative of disparity, and the difference in disparity between neighboring surface points. The latter may be used in Bayesian algorithms computing dense disparity fields. The directional derivative density and the disparity difference density both show that feature-based algorithms should strongly favor small disparity changes, contrary to several well-known algorithms. Finally, we use our original surface orientation density and the gradient magnitude density to derive two new ''surfaces-from-stereo'' techniques, techniques combining feature-based matching and surface reconstruction. The first uses the densities to severely restrict the search range for the optimum fit. The second incorporates the surface orientation density into the optimization criteria, producing a Bayesian formulation. Both algorithms are shown to be efficient and effective.			Stewart, CV (corresponding author), RENSSELAER POLYTECH INST, DEPT COMP SCI, TROY, NY 12180 USA.							ARNOLD RD, 1980, SPIE, V238, P281; AYACHE N, 1987, INT J COMPUT VISION, V1, P107, DOI 10.1007/BF00123161; BAKER HH, 1981, 7TH P INT JOINT C AR, P631; Ballard D.H., 1982, COMPUTER VISION; BARNARD ST, 1980, IEEE T PATTERN ANAL, V2, P333, DOI 10.1109/TPAMI.1980.4767032; BARNARD ST, 1982, COMPUT SURV, V14, P553, DOI 10.1145/356893.356896; BARNARD ST, 1989, INT J COMPUT VISION, V3, P17, DOI 10.1007/BF00054836; BEATON AE, 1974, TECHNOMETRICS, V16, P147, DOI 10.2307/1267936; Belhumeur P. N., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P506, DOI 10.1109/CVPR.1992.223143; Belhumeur P. N., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P431, DOI 10.1109/ICCV.1993.378184; Besl P. J., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P591, DOI 10.1109/CCV.1988.590039; BOLLES RC, 1993, P DARPA IM UND WORKS, P263; Boult T. E., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P118, DOI 10.1109/CCV.1988.589980; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; DHOND UR, 1989, IEEE T SYST MAN CYB, V19, P1489, DOI 10.1109/21.44067; DRUMHELLER M, 1986, IEEE J ROBOTIC AUTOM, P1439; EASTMAN RD, 1987, COMPUT VISION GRAPH, V39, P73, DOI 10.1016/S0734-189X(87)80203-7; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GRIMSON WEL, 1985, IEEE T PATTERN ANAL, V7, P17, DOI 10.1109/TPAMI.1985.4767615; GRIMSON WEL, 1994, COMMUN ACM, V37, P45, DOI 10.1145/175247.175251; GRIMSON WEL, 1981, IMAGES SURFACES; HAMPEL FR, 1981, J AM STAT ASSOC, V76, P643, DOI 10.2307/2287524; Hampel FR., 2011, WILEY SERIES PROBABI; HOFF W, 1989, IEEE T PATTERN ANAL, V11, P121, DOI 10.1109/34.16709; HOLLAND PW, 1977, COMMUN STAT A-THEOR, V6, P813, DOI 10.1080/03610927708827533; Horn B., 1986, ROBOT VISION, P1; Huber P., 1981, ROBUST STATISTICS, DOI [10.1002/0471725250, 10.1002/0471725250.ch1]; ILLINGWORTH J, 1988, COMPUT VISION GRAPH, V44, P87, DOI 10.1016/S0734-189X(88)80033-1; JENKIN MRM, 1991, CVGIP-IMAG UNDERSTAN, V53, P14, DOI 10.1016/1049-9660(91)90002-7; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482; MATTHIES L, 1992, INT J COMPUT VISION, V8, P71, DOI 10.1007/BF00126401; MCLAUCHLAN PF, 1991, IMAGE VISION COMPUT, V9, P20, DOI 10.1016/0262-8856(91)90044-P; MCLAUCHLAN PF, 1990, THESIS U SHEFFIELD; MEDIONI G, 1985, COMPUT VISION GRAPH, V31, P2, DOI 10.1016/S0734-189X(85)80073-6; MIRZA MJ, 1993, IEEE T ROBOTIC AUTOM, V9, P75, DOI 10.1109/70.210797; NIELSEN M, 1993, P 4 BRIT MACH VIS C, P135; NIELSEN M, 1995, P IEEE INT C COMP VI; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; OLSEN SI, 1990, IEEE T PATTERN ANAL, V12, P309, DOI 10.1109/34.49055; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; POLLARD SB, 1985, PERCEPTION, V14, P449, DOI 10.1068/p140449; PRAZDNY K, 1985, BIOL CYBERN, V52, P93, DOI 10.1007/BF00363999; Shah J., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P34, DOI 10.1109/CVPR.1993.341004; Stewart C. V., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P769, DOI 10.1109/CVPR.1992.223177; WEISBERG S, 1985, APPLIED LINEAR REGRE; YUILLE A, 1990, 1ST P EUR C COMP VIS, P73	49	14	14	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1996	20	3					143	168						26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VW629					2022-12-18	WOS:A1996VW62900001
J	Basri, R				Basri, R			Paraperspective affine	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							MOTION	It is shown that the set of all paraperspective images with arbitrary reference point and the set of all affine images of a 3-D object are identical. Consequently, all uncalibrated paraperspective images of an object can be constructed from a 3-D model of the object by applying an affine transformation to the model, and every affine image of the object represents some uncalibrated paraperspective image of the object. It follows that the paraperspective images of an object can be expressed as linear combinations of any two non-degenerate images of the object. When the image position of the reference point is given the parameters of the affine transformation (and, likewise, the coefficients of the linear combinations) satisfy two quadratic constraints. Conversely, when the values of parameters are given the image position of the reference point is determined by solving a bi-quadratic equation.			Basri, R (corresponding author), WEIZMANN INST SCI,DEPT APPL MATH,IL-76100 REHOVOT,ISRAEL.							ALOIMONOS JY, 1990, IMAGE VISION COMPUT, V8, P177; BASRI R, 1993, HDB PATTERN RECOGNIT, P863; FAUGERAS OD, 1992, EUR C COMP VIS, P564; JACOBS D, 1994, IN PRESS INT J COMPU; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; OHTA Y, 1981, P INT JOINT C ART IN, P746; POELMAN CJ, 1994, EUR C COMP VIS; POGGIO T, 1990, 900503 TR IRST; SUGIMOTO A, 1993, P SOC PHOTO-OPT INS, V1904, P183, DOI 10.1117/12.146690; SUGIMOTO A, 1995, IN PRESS INT J COMPU; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P92; WEINSHALL D, 1992, IN PRESS INT J COMP	14	14	15	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1996	19	2					169	179		10.1007/BF00055803	http://dx.doi.org/10.1007/BF00055803			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VE939					2022-12-18	WOS:A1996VE93900004
J	HelOr, Y; Werman, M				HelOr, Y; Werman, M			Constraint fusion for recognition and localization of articulated objects	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							TWO-DIMENSIONAL IMAGES; RANGE	This paper presents a method for localization and interpretation of modeled objects that is general enough to cover articulated and other types of constrained models. The flexibility between the components of the model is expressed as spatial constraints that are fused into the pose estimation during the interpretation process. The constraint fusion assists in obtaining a precise and stable pose of each of the object's components and in finding the correct interpretation. The proposed method can handle any constraint (including inequalities) between any number of different components of the model. The framework is based on Kalman filtering.	HEBREW UNIV JERUSALEM,DEPT COMP SCI,IL-91904 JERUSALEM,ISRAEL	Hebrew University of Jerusalem	HelOr, Y (corresponding author), WEIZMANN INST SCI,DEPT APPL MATH & COMP SCI,IL-76100 REHOVOT,ISRAEL.							Anderson B. D. O., 1979, OPTIMAL FILTERING; [Anonymous], [No title captured]; AYACHE N, 1987, 1ST P INT C COMP VIS, P73; Ayache N, 1991, ARTIFICIAL VISION MO; BEINGLASS A, 1991, C COMP VIS PATT REC, P461; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FAUGERAS OD, 1984, P AI APPLICATIONS C, P218; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; GRIMSON WEL, 1989, INT J COMPUT VISION, V2, P353, DOI 10.1007/BF00133555; GRIMSON WEL, 1987, 1ST P INT C COMP VIS, P93; GRIMSON WEL, 1990, EUR C COMP VIS, P489; HELOR Y, 1995, IEEE T PATTERN ANAL, V17, P195, DOI 10.1109/34.368169; HELOR Y, 1993, THESIS HEBREW U JERU; HELOR Y, 1993, IEEE T COMPUT AID D, V26, P426; Himmelblau D M, 1972, APPL NONLINEAR PROGR; Horn B., 1986, ROBOT VISION, P1; Jazwinski A.H., 1970, STOCHASTIC PROCESSES; Levenberg K., 1944, Q APPL MATH, V2, P164, DOI 10.1090/qam/10666; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043; LOWE DG, 1990, EUR C COMP VIS, P408; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; Maybeck P. S., 1982, STOCHASTIC MODELS ES; SABATA B, 1991, CVGIP-IMAG UNDERSTAN, V54, P309, DOI 10.1016/1049-9660(91)90032-K; SHAKUNAGA T, 1991, C COMP VIS PATT REC, P566; SHMUEL A, 1990, 10TH P INT C PATT RE, P48; Strang G., 1986, INTRO APPL MATH; WENG J, 1989, JUN P IEEE C COMP VI, P144	29	14	15	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	1996	19	1					5	28		10.1007/BF00131146	http://dx.doi.org/10.1007/BF00131146			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VB109		Green Submitted			2022-12-18	WOS:A1996VB10900001
J	MAYBANK, SJ				MAYBANK, SJ			PROBABILISTIC ANALYSIS OF THE APPLICATION OF THE CROSS RATIO TO MODEL-BASED VISION - MISCLASSIFICATION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								The cross ratio of four colinear points is of fundamental importance in model based vision, because it is the simplest numerical property of an object that is invariant under projection to an image. It provides a basis for algorithms to recognise objects from images without first estimating the position and orientation of the camera. A quantitative analysis of the effectiveness of the cross ratio in model based vision is made. A given image I of four colinear points is classified by making comparisons between the measured cross ratio tau of the four image points and the cross ratios stored in the model database. The image I is accepted as a projection of an object O-sigma with cross ratio a if \tau - sigma\ less than or equal to ntu, where n is the standard deviation of the image noise, t is a threshold and u = \del tau\. The performance of the cross ratio is described quantitatively by the probability of rejection R, the probability of false alarm F and the probability of misclassification p(sigma)(sigma), defined for two model cross ratios sigma, sigma. The trade off between these different probabilities is determined by t. It is assumed that in the absence of an object the image points have identical Gaussian distributions, and that in the presence of an object the image points have the appropriate conditional densities. The measurements of the image points are subject to small random Gaussian perturbations. Under these assumptions the trade offs between R, F and p(sigma)(sigma) are given to a good approximation by R = 2(1 - Phi(t)), F = r(F) epsilon t, root p(sigma)(sigma) = e(sigma)epsilon t\sigma - sigma\(-1), where epsilon is the relative noise level, Phi is the cumulative distribution function for the normal distribution, r(F) is constant, and e(sigma) is a function of a only. The trade off between R and F is obtained in Maybank (1994). In this paper the trade off between R and p(sigma)(S) is obtained. It is conjectured that the general form of the above trade offs between R, F and p(sigma)(sigma) is the same for a range of invariants useful in model based vision. The conjecture prompts the following definition: an invariant which has trade offs between R, F, p(sigma)(sigma) of the above form is said to be non-degenerate for model based vision. The consequences of the trade off between R and p(sigma)(sigma) are examined. In particular, it is shown that for a fixed overall probability of misclassification there is a maximum possible model cross ratio sigma(m), and there is a maximum possible number N of models. Approximate expressions for sigma(m) and N are obtained. They indicate that in practice a model database containing only cross ratio values can have a size of order at most ten, for a physically plausible level of image noise, and for a probability of misclassification of the order 0.1.			MAYBANK, SJ (corresponding author), GEC LTD,MARCONI HIRST RES CTR,ELSTREE WAY,BOREHAMWOOD WD6 1RX,HERTS,ENGLAND.							CRAMER H, 1991, PRINCETON MATH SERIE, V9; Devijver PA, 1982, PATTERN RECOGNITION; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; Helstrom C., 1960, INT SERIES MONOGRAPH; Maybank S.J., 1994, LECT NOTES COMPUTER, V825, P453; MAYBANK SJ, 1994, UNPUB EXPT INVESTIGA; MAYBANK SJ, 1993, APPLICATIONS INVARIA, V2, P113; MAYBANK SJ, 1994, UNPUB IJCV; MAYBANK SJ, 1994, IN PRESS J APPLIED S; Mundy J., 1992, GEOMETRIC INVARIANCE; MUNDY JL, 1992, GEOMETRIC INVARIANCE, P1; MUNDY JL, IN PRESS LECTURE NOT; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; Wolfram S., 1991, MATH SYSTEM DOING MA	14	14	14	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1995	14	3					199	210		10.1007/BF01679682	http://dx.doi.org/10.1007/BF01679682			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QU485					2022-12-18	WOS:A1995QU48500001
J	Liu, JY; Xu, DJ; Yang, WH; Fan, MH; Huang, HF				Liu, Jiaying; Xu, Dejia; Yang, Wenhan; Fan, Minhao; Huang, Haofeng			Benchmarking Low-Light Image Enhancement and Beyond	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Low-light enhancement; Benchmark; Dataset; Face detection	DYNAMIC HISTOGRAM EQUALIZATION; CONTRAST ENHANCEMENT; QUALITY ASSESSMENT; RETINEX; COMPENSATION; ALGORITHM	In this paper, we present a systematic review and evaluation of existing single-image low-light enhancement algorithms. Besides the commonly used low-level vision oriented evaluations, we additionally consider measuring machine vision performance in the low-light condition via face detection task to explore the potential of joint optimization of high-level and low-level vision enhancement. To this end, we first propose a large-scale low-light image dataset serving both low/high-level vision with diversified scenes and contents as well as complex degradation in real scenarios, called Vision Enhancement in the LOw-Light condition (VE-LOL). Beyond paired low/normal-light images without annotations, we additionally include the analysis resource related to human, i.e. face images in the low-light condition with annotated face bounding boxes. Then, efforts are made on benchmarking from the perspective of both human and machine visions. A rich variety of criteria is used for the low-level vision evaluation, including full-reference, no-reference, and semantic similarity metrics. We also measure the effects of the low-light enhancement on face detection in the low-light condition. State-of-the-art face detection methods are used in the evaluation. Furthermore, with the rich material of VE-LOL, we explore the novel problem of joint low-light enhancement and face detection. We develop an enhanced face detector to apply low-light enhancement and face detection jointly. The features extracted by the enhancement module are fed to the successive layer with the same resolution of the detection module. Thus, these features are intertwined together to unitedly learn useful information across two phases, i.e. enhancement and detection. Experiments on VE-LOL provide a comparison of state-of-the-art low-light enhancement algorithms, point out their limitations, and suggest promising future directions. Our dataset has supported the Track "Face Detection in Low Light Conditions" of CVPR UG2+ Challenge (2019-2020) ().	[Liu, Jiaying; Xu, Dejia; Yang, Wenhan; Fan, Minhao; Huang, Haofeng] Peking Univ, Wangxuan Inst Comp Technol, Beijing, Peoples R China	Peking University	Liu, JY; Yang, WH (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing, Peoples R China.	liujiaying@pku.edu.cn; dejia@pku.edu.cn; yangwenhan@pku.edu.cn; fanminhao@pku.edu.cn; huang6013@pku.edu.cn			National Key Research and Development Program of China [2018AAA0102702]; Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China [61772043]	National Key Research and Development Program of China; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Key Research and Development Program of China under Grant 2018AAA0102702, the Fundamental Research Funds for the Central Universities, and the National Natural Science Foundation of China under contract No. 61772043.	Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734; Anaya J, 2018, J VIS COMMUN IMAGE R, V51, P144, DOI 10.1016/j.jvcir.2018.01.012; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.596; Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548; Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129; Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431; Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218; Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513; Chang Y., 2016, 2016 IEEE FRONT ED C, P1, DOI DOI 10.1109/NEWCAS.2016.7604803; Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328; Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347; Chen X., 2018, ARXIV181210695; Chen ZY, 2006, IEEE T IMAGE PROCESS, V15, P2303, DOI 10.1109/TIP.2006.875201; Chi C., 2018, ABS180902693 CORR; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dai DX, 2018, IEEE INT C INTELL TR, P3819, DOI 10.1109/ITSC.2018.8569387; Dang-Nguyen D., 2015, ACM MULTIMEDIA SYSTE, P219, DOI [10.1145/2713168.2713194, DOI 10.1145/2713168.2713194]; Dong XC, 2011, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2011.115; Fan MH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2317, DOI 10.1145/3394171.3413757; Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304; Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185; Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450; Hordley SD, 2004, INT C PATT RECOG, P76, DOI 10.1109/ICPR.2004.1334009; Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706; Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280; Jiang HY, 2019, IEEE I CONF COMP VIS, P7323, DOI 10.1109/ICCV.2019.00742; Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462; Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272; Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kalinin, 2018, ARXIV180906839; Kim G., 2019, ARXIV190605119; Kim G, 2019, IEEE IMAGE PROC, P2811, DOI 10.1109/ICIP.2019.8803328; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108; Lee CH, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P43, DOI 10.1109/SITIS.2013.19; Lee C, 2014, IEEE T CIRC SYST VID, V24, P576, DOI 10.1109/TCSVT.2013.2276154; Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059; Lee JT, 2014, IEEE IMAGE PROC, P4527, DOI 10.1109/ICIP.2014.7025918; Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951; Li J, 2019, PROC CVPR IEEE, P5055, DOI 10.1109/CVPR.2019.00520; Li L, 2015, IEEE IMAGE PROC, P3730, DOI 10.1109/ICIP.2015.7351501; Li MD, 2018, IEEE IMAGE PROC, P1118, DOI 10.1109/ICIP.2018.8451278; Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539; Liang ZT, 2016, IEEE T IMAGE PROCESS, V25, P673, DOI 10.1109/TIP.2015.2507405; Lim J, 2015, IEEE IMAGE PROC, P4131, DOI 10.1109/ICIP.2015.7351583; Liu J., 2018, P BRIT MECH VIS C, P1; Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006; Liu XM, 2015, INT CONF ACOUST SPEE, P2274, DOI 10.1109/ICASSP.2015.7178376; Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010; Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008; Lv J. W. Feifan, 2018, BR MACH VIS C; Ma, 2017, ARXIV E PRINTS; Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726; Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050; Nada H, 2018, INT CONF BIOMETR THE; Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522; Nakai K, 2013, I S INTELL SIG PROC, P445, DOI 10.1109/ISPACS.2013.6704591; Paszke Adam, 2017, NIPS AUT WORKSH, DOI DOI 10.1017/CBO9781107707221.009; Pierre F, 2016, IEEE IMAGE PROC, P4067, DOI 10.1109/ICIP.2016.7533124; Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340; Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412; Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427; Saad MA, 2011, IEEE IMAGE PROC; Sakaridis C, 2022, IEEE T PATTERN ANAL, V44, P3139, DOI 10.1109/TPAMI.2020.3045882; Sakaridis C, 2019, IEEE I CONF COMP VIS, P7373, DOI 10.1109/ICCV.2019.00747; Sasagawa Yukihiro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P345, DOI 10.1007/978-3-030-58589-1_21; Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Su HA, 2017, INT CONF ACOUST SPEE, P1977, DOI 10.1109/ICASSP.2017.7952502; Tan PN, 2005, INTRO DATA MINING, V1st; Tao L, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP); Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Vonikakis V, 2013, MEAS SCI TECHNOL, V24, DOI 10.1088/0957-0233/24/7/074024; Vonikakis V, 2018, MULTIMED TOOLS APPL, V77, P9211, DOI 10.1007/s11042-017-4783-x; Wang F, 2018, LECT NOTES COMPUT SC, V11213, P780, DOI 10.1007/978-3-030-01240-3_47; Wang JH, 2018, IEEE INT CONF COMM; Wang LQ, 2014, IEEE T IMAGE PROCESS, V23, P3381, DOI 10.1109/TIP.2014.2324813; Wang RZ, 2019, IEEE I CONF COMP VIS, P3056, DOI 10.1109/ICCV.2019.00315; Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309; Wang XZ, 2010, IEEE T INF TECHNOL B, V14, P1355, DOI 10.1109/TITB.2010.2076378; Wang Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2015, DOI 10.1145/3343031.3350983; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wending Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P473, DOI 10.1007/978-3-030-58610-2_28; Wu XM, 2017, IEEE IMAGE PROC, P3190; Xu J., 2019, ARXIV190606690; Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880; Xu KQ, 2017, INT CONF ACOUST SPEE, P1363, DOI 10.1109/ICASSP.2017.7952379; Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235; Xueyang Fu, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1190, DOI 10.1109/ICASSP.2014.6853785; Yang Q, 2018, IEEE IMAGE PROC, P3199, DOI 10.1109/ICIP.2018.8451840; Yang W., 2020, P IEEE INT C COMP VI; Ye Z, 2007, SE SYM SYS THRY, P315; Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725; Ying ZJ, 2017, IEEE INT SUPERCOND E; Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4; Ying Zhenqiang, 2017, ARXIV171100591; Yu SY, 2019, IEEE T CIRC SYST VID, V29, P28, DOI 10.1109/TCSVT.2017.2763180; Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416; Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730; Zhang Q, 2016, IEEE T VIS COMPUT GR, V22, P1773, DOI 10.1109/TVCG.2015.2461157; Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30; Zhang XD, 2012, INT C PATT RECOG, P2034; Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926; Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106	113	13	16	25	61	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1153	1184		10.1007/s11263-020-01418-8	http://dx.doi.org/10.1007/s11263-020-01418-8		JAN 2021	32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000607058700002
J	Hehn, TM; Kooij, JFP; Hamprecht, FA				Hehn, Thomas M.; Kooij, Julian F. P.; Hamprecht, Fred A.			End-to-End Learning of Decision Trees and Forests	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Decision forests; End-to-end learning; Efficient inference; Interpretability	CLASSIFICATION	Conventional decision trees have a number of favorable properties, including a small computational footprint, interpretability, and the ability to learn from little training data. However, they lack a key quality that has helped fuel the deep learning revolution: that of being end-to-end trainable. Kontschieder et al. (ICCV, 2015) have addressed this deficit, but at the cost of losing a main attractive trait of decision trees: the fact that each sample is routed along a small subset of tree nodes only. We here present an end-to-end learning scheme for deterministic decision trees and decision forests. Thanks to a new model and expectation-maximization training scheme, the trees are fully probabilistic at train time, but after an annealing process become deterministic at test time. In experiments we explore the effect of annealing visually and quantitatively, and find that our method performs on par or superior to standard learning algorithms for oblique decision trees and forests. We further demonstrate on image datasets that our approach can learn more complex split functions than common oblique ones, and facilitates interpretability through spatial regularization.	[Hehn, Thomas M.; Kooij, Julian F. P.] Delft Univ Technol, Intelligent Vehicles Grp, Mekelweg 2, NL-2628 CD Delft, Netherlands; [Hamprecht, Fred A.] Heidelberg Univ, HCI IWR, D-69120 Heidelberg, Germany	Delft University of Technology; Ruprecht Karls University Heidelberg	Hehn, TM (corresponding author), Delft Univ Technol, Intelligent Vehicles Grp, Mekelweg 2, NL-2628 CD Delft, Netherlands.	T.M.Hehn@tudelft.nl; J.F.P.Kooij@tudelft.nl; fred.hamprecht@iwr.uni-heidelberg.de		Kooij, Julian/0000-0001-9919-0710				Barros RC, 2012, IEEE T SYST MAN CY C, V42, P291, DOI 10.1109/TSMCC.2011.2157494; Bolukbasi T, 2017, PR MACH LEARN RES, V70; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2017, CLASSIFICATION REGRE; Cardona A, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000502; Cordts M., 2016, 2016 IEEE COMP SOC C; Cordts M, 2017, IEEE T PATTERN ANAL, V39, P1444, DOI 10.1109/TPAMI.2016.2592911; Criminisi A., 2013, DECISION FORESTCOM; de Ville, 2006, DECISION TREES BUSIN; Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479; Dua D., 2019, UCI MACHINE LEARNING; Duarte MF, 2004, J PARALLEL DISTR COM, V64, P826, DOI 10.1016/j.jpdc.2004.03.020; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; Fan R. E., 2011, LIBSVM DATA CLASSIFI; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; FREY PW, 1991, MACH LEARN, V6, P161, DOI 10.1023/A:1022606404104; Frosst Nicholas, 2017, ARXIV171109784; Gall J, 2009, PROC CVPR IEEE, P1022, DOI 10.1109/CVPRW.2009.5206740; Guh RS, 2011, EXPERT SYST APPL, V38, P4437, DOI 10.1016/j.eswa.2010.09.112; Hehn T. M., 2018, GERM C PATT REC, P612; Huang Gao, 2018, ICLR; Huang GM, 2015, BMC BIOINFORMATICS, V16, DOI 10.1186/1471-2105-16-S1-S5; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Hyafil L., 1976, Information Processing Letters, V5, P15, DOI 10.1016/0020-0190(76)90095-8; Ioannou Y, 2016, ARXIV PREPRINT ARXIV; Jordan M. I., 1994, Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, COLT 94, P13, DOI 10.1145/180139.175372; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Kingma D.P, P 3 INT C LEARNING R; Kontschieder P., 2013, 2013 IEEE COMP SOC C; Kontschieder P, 2015, IEEE I CONF COMP VIS, P1467, DOI 10.1109/ICCV.2015.172; Laptev D, 2014, LECT NOTES COMPUT SC, V8753, P95, DOI 10.1007/978-3-319-11752-2_8; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lepetit V, 2005, PROC CVPR IEEE, P775; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; McGill M., 2017, P 34 INT C MACH LEAR, V70, p2363~2372; Menze BH, 2011, LECT NOTES ARTIF INT, V6912, P453, DOI 10.1007/978-3-642-23783-6_29; Montillo A., 2013, DECISION FORESTCOM, P273; Murthy K. V. S., 1996, THESIS; Norouzi M., 2015, ARXIV150606155; Norouzi M, 2015, ADV NEUR IN, V28; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pinhas-Hamiel O, 2013, INT J EAT DISORDER, V46, P819, DOI 10.1002/eat.22138; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; Quinlan J. R., 1993, C4 5 PROGRAMS MACHIN; Richmond D., 2016, P BRIT MACH VIS C BM, P1441, DOI 10.5244/C.30.144; ROSE K, 1990, PHYS REV LETT, V65, P945, DOI 10.1103/PhysRevLett.65.945; Rota Bulo S., 2014, 2014 IEEE COMP SOC C; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schulter S, 2013, PROC CVPR IEEE, P508, DOI 10.1109/CVPR.2013.72; SETHI IK, 1990, P IEEE, V78, P1605, DOI 10.1109/5.58346; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Suarez A, 1999, IEEE T PATTERN ANAL, V21, P1297, DOI 10.1109/34.817409; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wang Jung-Ying, 2002, APPL SUPPORT VECTOR; Wang S, 2017, OXID MED CELL LONGEV, V2017, DOI 10.1155/2017/4371714; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Welbl J., 2014, GCPR; Worachartcheewan Apilak, 2010, Diabetes Res Clin Pract, V90, pe15, DOI 10.1016/j.diabres.2010.06.009; Xiao H., 2017, FASHION MNIST NOVEL; Zhang L, 2017, PROC CVPR IEEE, P5825, DOI 10.1109/CVPR.2017.617	60	13	13	6	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		997	1011		10.1007/s11263-019-01237-6	http://dx.doi.org/10.1007/s11263-019-01237-6			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Green Published, hybrid			2022-12-18	WOS:000525393600017
J	Anirudh, R; Thiagarajan, JJ; Kailkhura, B; Bremer, PT				Anirudh, Rushil; Thiagarajan, Jayaraman J.; Kailkhura, Bhavya; Bremer, Peer-Timo			MimicGAN: Robust Projection onto Image Manifolds with Corruption Mimicking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generative adversarial networks; Robustness; Adversarial defense; Anomaly detection; Domain adaptation		In the past few years, Generative Adversarial Networks (GANs) have dramatically advanced our ability to represent and parameterize high-dimensional, non-linear image manifolds. As a result, they have been widely adopted across a variety of applications, ranging from challenging inverse problems like image completion, to problems such as anomaly detection and adversarial defense. A recurring theme in many of these applications is the notion of projecting an image observation onto the manifold that is inferred by the generator. In this context, Projected Gradient Descent (PGD) has been the most popular approach, which essentially optimizes for a latent vector that minimizes the discrepancy between a generated image and the given observation. However, PGD is a brittle optimization technique that fails to identify the right projection (or latent vector) when the observation is corrupted, or perturbed even by a small amount. Such corruptions are common in the real world, for example images in the wild come with unknown crops, rotations, missing pixels, or other kinds of non-linear distributional shifts which break current encoding methods, rendering downstream applications unusable. To address this, we propose corruption mimicking-a new robust projection technique, that utilizes a surrogate network to approximate the unknown corruption directly at test time, without the need for additional supervision or data augmentation. The proposed method is significantly more robust than PGD and other competing methods under a wide variety of corruptions, thereby enabling a more effective use of GANs in real-world applications. More importantly, we show that our approach produces state-of-the-art performance in several GAN-based applications-anomaly detection, domain adaptation, and adversarial defense, that benefit from an accurate projection.	[Anirudh, Rushil; Thiagarajan, Jayaraman J.; Kailkhura, Bhavya; Bremer, Peer-Timo] Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA	United States Department of Energy (DOE); Lawrence Livermore National Laboratory	Anirudh, R (corresponding author), Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA.	anirudh1@llnl.gov			U.S. Department of Energy by Lawrence Livermore National Laboratory [DE-AC52-07NA27344]	U.S. Department of Energy by Lawrence Livermore National Laboratory(United States Department of Energy (DOE))	This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.	Abadi M, 2015, P 12 USENIX S OPERAT; Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453; Akcay S, 2019, LECT NOTES COMPUT SC, V11363, P622, DOI 10.1007/978-3-030-20893-6_39; An J., 2015, SPEC LECT, V2, P1, DOI DOI 10.1007/BF00758335; [Anonymous], 2017, ARXIV171209196; Asim M., 2018, ARXIV180204073 CORR; Athalye A, 2018, PR MACH LEARN RES, V80; Azulay A, 2019, J MACH LEARN RES, V20; Becker BC, 2013, IEEE COMPUT SOC CONF, P904, DOI 10.1109/CVPRW.2013.133; BigGANEx, 2018, DIV LAT SPAC BIGGAN; Bojanowski P, 2018, PR MACH LEARN RES, V80; Bora A, 2017, PR MACH LEARN RES, V70; Brock A., 2019, INT C LEARNING REPRE; Burges, 1998, MNIST DATABASE HANDW; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Donahue Jeff, 2017, INT C LEARN REPR ICL; Dumoulin Vincent, 2017, ICLR; Faghri Fartash, 2016, ARXIV PREPRINT ARXIV; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Foo C.S., 2018, PROC WORKSHOP INT C; Ganin Y, 2016, J MACH LEARN RES, V17; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow IJ, 2014, 3 INT C LEARNING REP; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hoffman J, 2018, PR MACH LEARN RES, V80; Hogan T. A, 2018, ARXIV181103733; Hoshen Y., 2018, P EUR C COMP VIS ECC, P436; Hoshen Y, 2018, ADV NEUR IN, V31; Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lipton Zachary C, 2017, ICLR WORKSH, P2; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; LIU MY, 2017, P ASM 36 INT C OC; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Madry Aleksander, 2017, ARXIV; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pinto M, 2011, INNOVATIONS IN SMES AND CONDUCTING E-BUSINESS: TECHNOLOGIES, TRENDS AND SOLUTIONS, P35, DOI 10.4018/978-1-60960-765-4.ch003; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Samangouei Pouya, 2018, ARXIV180506605; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Santhanam G. K., 2018, ARXIV180510652; Seguy V., 2018, INT C LEARN REPR; Shah V, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4609; Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329; StyleGAN, 2019, ENC OFF TENS IMPL; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Xiao H., 2017, FASHION MNIST NOVEL; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	61	13	13	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2459	2477		10.1007/s11263-020-01310-5	http://dx.doi.org/10.1007/s11263-020-01310-5		MAR 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Green Submitted			2022-12-18	WOS:000561636900001
J	Cao, J; Hu, YB; Zhang, HW; He, R; Sun, ZA				Cao, Jie; Hu, Yibo; Zhang, Hongwen; He, Ran; Sun, Zhenan			Towards High Fidelity Face Frontalization in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face frontalization; Realistic face generation; Pose-invariant face recognition	VIEWPOINT INVARIANT	Face frontalization refers to the process of synthesizing the frontal view of a face from a given profile. Due to self-occlusion and appearance distortion in the wild, it is extremely challenging to recover faithful high-resolution results meanwhile preserve texture details. This paper proposes a high fidelity pose in-variant model (HF-PIM) to produce photographic and identity-preserving results. HF-PIM frontalizes the profiles through a novel texture fusion warping procedure and leverages a dense correspondence field to bind the 2D and 3D surface spaces. We decompose the prerequisite of warping into dense correspondence field estimation and facial texture map recovering, which are both well addressed by deep networks. Different from those reconstruction methods relying on 3D data, we also propose adversarial residual dictionary learning to supervise facial texture map recovering with only monocular images. Furthermore, a multi-perception guided loss is proposed to address the practical misalignment between the ground truth frontal and profile faces, allowing HF-PIM to effectively utilize multiple images during training. Quantitative and qualitative evaluations on five controlled and uncontrolled databases show that the proposed method not only boosts the performance of pose-invariant face recognition but also improves the visual quality of high-resolution frontalization appearances.	[Cao, Jie; Hu, Yibo; Zhang, Hongwen; He, Ran; Sun, Zhenan] CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; [Cao, Jie; Hu, Yibo; Zhang, Hongwen; He, Ran; Sun, Zhenan] CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Cao, Jie; Hu, Yibo; Zhang, Hongwen; He, Ran; Sun, Zhenan] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China; [Cao, Jie; Hu, Yibo; Zhang, Hongwen; He, Ran; Sun, Zhenan] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences	Sun, ZA (corresponding author), CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.; Sun, ZA (corresponding author), CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China.; Sun, ZA (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; Sun, ZA (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.	jie.cao@cripac.ia.ac.cn; yibo.hu@cripac.ia.ac.cn; hongwen.zhang@cripac.ia.ac.cn; rhe@nlpr.ia.ac.cn; znsun@nlpr.ia.ac.cn	Zhang, Hongwen/ABI-2791-2020	Zhang, Hongwen/0000-0001-8633-4551; cao, jie/0000-0001-6368-4495	National Key Research and Development Program of China [2016YFB1001001, 2017YFC0821602]; National Natural Science Foundation of China [61622310, 61427811, U1836217]; Beijing Natural Science Foundation [JQ18017]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work is funded by the National Key Research and Development Program of China (Grant Nos. 2016YFB1001001, 2017YFC0821602), the National Natural Science Foundation of China (Grant Nos. 61622310, 61427811, U1836217), and Beijing Natural Science Foundation (Grant No. JQ18017).	AbdAlmageed W, 2016, IEEE WINT CONF APPL; Arjovsky M, 2017, PR MACH LEARN RES, V70; Ashraf AB, 2008, PROC CVPR IEEE, P3208; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Booth J, 2014, IEEE IMAGE PROC, P4672, DOI 10.1109/ICIP.2014.7025947; Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116; Cao J, 2019, IEEE T INF FOREN SEC, V14, P2028, DOI 10.1109/TIFS.2019.2891116; Cao J, 2018, ADV NEUR IN, V31; Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cimpoi M., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299007; Cole F, 2017, PROC CVPR IEEE, P3386, DOI 10.1109/CVPR.2017.361; Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741; Dovgard R, 2004, LECT NOTES COMPUT SC, V3022, P99; Ferrari C, 2016, INT C PATT RECOG, P1047, DOI 10.1109/ICPR.2016.7899774; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Guler RA, 2017, PROC CVPR IEEE, P2614, DOI 10.1109/CVPR.2017.280; Han CR, 2018, LECT NOTES COMPUT SC, V11213, P120, DOI 10.1007/978-3-030-01240-3_8; HASSNER T, 2015, PROC CVPR IEEE, P4295, DOI DOI 10.1109/CVPR.2015.7299058; Hassner T, 2013, IEEE I CONF COMP VIS, P3607, DOI 10.1109/ICCV.2013.448; Hensel M, 2017, ADV NEUR IN, V30; Hu YB, 2018, PROC CVPR IEEE, P8398, DOI 10.1109/CVPR.2018.00876; Huang Gary B., 2007, 0749 U MASS, P7; Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243; KLARE BF, 2015, PROC CVPR IEEE, P1931, DOI DOI 10.1109/CVPR.2015.7298803; Li JH, 2016, P IEEE RAS-EMBS INT, P1068, DOI 10.1109/BIOROB.2016.7523773; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Lucey S, 2008, INT J COMPUT VISION, V80, P58, DOI 10.1007/s11263-007-0119-z; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523; Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35; Paszke A., 2017, AUTOMATIC DIFFERENTI; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Radford A., 2016, 4 INT C LEARNING REP; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sagonas C, 2015, IEEE I CONF COMP VIS, P3871, DOI 10.1109/ICCV.2015.441; Sengupta S, 2016, IEEE WINT CONF APPL; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Sun XX, 2018, IEEE IMAGE PROC, P346, DOI 10.1109/ICIP.2018.8451701; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tran L, 2019, IEEE T PATTERN ANAL, V41, P3007, DOI 10.1109/TPAMI.2018.2868350; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; van Gemert JC, 2008, LECT NOTES COMPUT SC, V5304, P696; Wu W, 2019, IEEE ICC; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; YAN J, 2018, EUR C COMP VIS ECCV; Yang Jimei, 2015, NIPS; Yim J, 2015, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2015.7298667; Yin X, 2017, IEEE I CONF COMP VIS, P4010, DOI 10.1109/ICCV.2017.430; Zhang H, 2017, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR.2017.309; Zhang ZG, 2018, DESTECH TRANS ENVIR; ZHAO J, 2018, IEEE T PATTERN ANAL; Zhao J, 2017, ADV NEUR IN, V30; Zhao J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1184; Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23; Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679	67	13	13	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1485	1504		10.1007/s11263-019-01229-6	http://dx.doi.org/10.1007/s11263-019-01229-6		OCT 2019	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL9ON					2022-12-18	WOS:000492258500001
J	Liu, J; Rahmani, H; Akhtar, N; Mian, A				Liu, Jian; Rahmani, Hossein; Akhtar, Naveed; Mian, Ajmal			Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human action recognition; Cross-view; Cross-subject; Depth sensor; CNN; GAN	HISTOGRAMS; VIEWS	We propose Human Pose Models that represent RGB and depth images of human poses independent of clothing textures, backgrounds, lighting conditions, body shapes and camera viewpoints. Learning such universal models requires training images where all factors are varied for every human pose. Capturing such data is prohibitively expensive. Therefore, we develop a framework for synthesizing the training data. First, we learn representative human poses from a large corpus of real motion captured human skeleton data. Next, we fit synthetic 3D humans with different body shapes to each pose and render each from 180 camera viewpoints while randomly varying the clothing textures, background and lighting. Generative Adversarial Networks are employed to minimize the gap between synthetic and real image distributions. CNN models are then learned that transfer human poses to a shared high-level invariant space. The learned CNN models are then used as invariant feature extractors from real RGB and depth frames of human action videos and the temporal variations are modelled by Fourier Temporal Pyramid. Finally, linear SVM is used for classification. Experiments on three benchmark human action datasets show that our algorithm outperforms existing methods by significant margins for RGB only and RGB-D action recognition.	[Liu, Jian; Akhtar, Naveed; Mian, Ajmal] Univ Western Australia, Sch Comp Sci & Software Engn, 35 Stirling Highway, Crawley, WA 6009, Australia; [Rahmani, Hossein] Univ Lancaster, Sch Comp & Commun, Lancaster, Lancs, England	University of Western Australia; Lancaster University	Liu, J (corresponding author), Univ Western Australia, Sch Comp Sci & Software Engn, 35 Stirling Highway, Crawley, WA 6009, Australia.	jian.liu@research.uwa.cdu.au; h.rahmani@lancaster.ac.uk; naveed.akhtar@uwa.edu.au; ajmal.mian@uwa.edu.au	Rahmani, Hossein/S-5134-2019; AKHTAR, NAVEED/AAT-1283-2020	Rahmani, Hossein/0000-0003-1920-0371; AKHTAR, NAVEED/0000-0003-3406-673X; Mian, Ajmal/0000-0002-5206-3842	Australian Research Council [DP160101458]	Australian Research Council(Australian Research Council)	This research was sponsored by the Australian Research Council Grant DP160101458. The Tesla K-40 GPU used for this research was donated by the NVIDIA Corporation.	[Anonymous], 2005, THESIS MIT CAMBRIDGE; [Anonymous], 2007, 2007 IEEE C COMP VIS; [Anonymous], 2017, IEEE C COMP VIS PATT; [Anonymous], IEEE C COMP VIS PATT; [Anonymous], 2016, P ECCV; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772; Farhadi A, 2008, LECT NOTES COMPUT SC, V5302, P154, DOI 10.1007/978-3-540-88682-2_13; Farhadi A, 2009, IEEE I CONF COMP VIS, P948, DOI 10.1109/ICCV.2009.5459350; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Gupta A, 2014, PROC CVPR IEEE, P2601, DOI 10.1109/CVPR.2014.333; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He Y, 2016, LECT NOTES COMPUT SC, V9915, P11, DOI 10.1007/978-3-319-49409-8_2; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jia CC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P87, DOI 10.1145/2647868.2654928; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kerola T, 2017, COMPUT VIS IMAGE UND, V154, P108, DOI 10.1016/j.cviu.2016.10.004; Kong Y, 2017, INT J COMPUT VISION, V123, P350, DOI 10.1007/s11263-016-0982-6; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li BL, 2012, PROC CVPR IEEE, P1362, DOI 10.1109/CVPR.2012.6247822; Li RN, 2012, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2012.6248011; Li Y, 2016, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2016.215; Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50; Luo ZL, 2017, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR.2017.751; McInnes L, 2017, INT CONF DAT MIN WOR, P33, DOI 10.1109/ICDMW.2017.12; Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76; Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98; Parameswaran V, 2006, INT J COMPUT VISION, V66, P83, DOI 10.1007/s11263-005-3671-4; Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222; Qadir O, 2011, IEEE C EVOL COMPUTAT, P208; Rahmani H, 2018, IEEE T PATTERN ANAL, V40, P667, DOI 10.1109/TPAMI.2017.2691768; Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167; Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389; Rahmani H, 2015, PROC CVPR IEEE, P2458, DOI 10.1109/CVPR.2015.7298860; Rao C, 2002, INT J COMPUT VISION, V50, P203, DOI 10.1023/A:1020350100748; Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Shahroudy A, 2016, IEEE T PATTERN ANAL, V38, P2123, DOI 10.1109/TPAMI.2015.2505295; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Simonyan K, 2014, ADV NEUR IN, V27; Soomro K., 2012, CRCVTR1201; Su B, 2016, LECT NOTES COMPUT SC, V9908, P202, DOI 10.1007/978-3-319-46493-0_13; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339; Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191; Wang Y, 2016, PROC CVPR IEEE, P2698, DOI 10.1109/CVPR.2016.295; Weinland D, 2007, IEEE I CONF COMP VIS, P170; Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013; Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108; Yilmaz A, 2005, PROC CVPR IEEE, P984; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Yu MY, 2016, IEEE T PATTERN ANAL, V38, P1651, DOI 10.1109/TPAMI.2015.2491925; Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297; Zhang Z, 2013, PROC CVPR IEEE, P2690, DOI 10.1109/CVPR.2013.347; Zheng JJ, 2013, IEEE I CONF COMP VIS, P3176, DOI 10.1109/ICCV.2013.394; Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219	73	13	14	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1545	1564		10.1007/s11263-019-01192-2	http://dx.doi.org/10.1007/s11263-019-01192-2			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL		Green Submitted, Green Accepted			2022-12-18	WOS:000485320300009
J	Chrysos, GG; Favaro, P; Zafeiriou, S				Chrysos, Grigorios G.; Favaro, Paolo; Zafeiriou, Stefanos			Motion Deblurring of Faces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Learning motion deblurring; Face deblurring; Data-driven networks	IMAGE; IDENTIFICATION; RESTORATION	Face analysis lies at the heart of computer vision with remarkable progress in the past decades. Face recognition and tracking are tackled by building invariance to fundamental modes of variation such as illumination, 3D pose. A much less standing mode of variation is motion deblurring, which however presents substantial challenges in face analysis. Recent approaches either make oversimplifying assumptions, e.g. in cases of joint optimization with other tasks, or fail to preserve the highly structured shape/identity information. We introduce a two-step architecture tailored to the challenges of motion deblurring: the first step restores the low frequencies; the second restores the high frequencies, while ensuring that the outputs span the natural images manifold. Both steps are implemented with a supervised data-driven method; to train those we devise a method for creating realistic motion blur by averaging a variable number of frames. The averaged images originate from the 2MF2 dataset with 19 million facial frames, which we introduce for the task. Considering deblurring as an intermediate step, we conduct a thorough experimentation on high-level face analysis tasks, i.e. landmark localization and face verification, on blurred images. The experimental evaluation demonstrates the superiority of our method.	[Chrysos, Grigorios G.; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England; [Favaro, Paolo] Univ Bern, Dept Informat, Neubruckstr 10, CH-3012 Bern, Switzerland	Imperial College London; University of Bern	Chrysos, GG (corresponding author), Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England.	g.chrysos@imperial.ac.uk; paolo.favaro@inf.unibe.ch; s.zafeiriou@imperial.ac.uk	Chrysos, Grigorios/ABE-2026-2021	Chrysos, Grigorios/0000-0002-0650-1856				Arjovsky M., 2017, ARXIV170107875; Babacan SD, 2012, LECT NOTES COMPUT SC, V7577, P341, DOI 10.1007/978-3-642-33783-3_25; Bansal A., 2017, IEEE P INT C COMP VI; Bulat A., 2017, IEEE P INT C COMP VI; Cao Q., 2017, IEEE P INT C COMP VI; Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14; Chen Weifeng, 2016, NEURIPS, P2, DOI DOI 10.5555/3157096.3157178; CHO CM, 1991, IEEE IJCNN, P2558, DOI 10.1109/IJCNN.1991.170774; Chrysos G. G., 2017, IEEE P INT C COMP VI; Chrysos GG, 2018, INT J COMPUT VISION, V126, P198, DOI 10.1007/s11263-017-0999-5; Chrysos GG, 2017, IEEE COMPUT SOC CONF, P2015, DOI 10.1109/CVPRW.2017.252; Chrysos GG, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P954, DOI 10.1109/ICCVW.2015.126; Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6; Nguyen DT, 2015, SENSORS-BASEL, V15, P21898, DOI 10.3390/s150921898; Denton Emily L, 2015, NEURIPS, V2, P4; Ding CX, 2018, IEEE T PATTERN ANAL, V40, P1002, DOI 10.1109/TPAMI.2017.2700390; Dosovitskiy Alexey, 2016, NEURIPS; Dumoulin Vincent, 2016, ARXIV E PRINTS; Gong D., 2017, IEEE P INT C COMP VI; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2012, IEEE T PATTERN ANAL, V34, P1220, DOI 10.1109/TPAMI.2012.15; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Grossberg MD, 2003, IEEE T PATTERN ANAL, V25, P1455, DOI 10.1109/TPAMI.2003.1240119; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; HaCohen Y, 2013, IEEE I CONF COMP VIS, P2384, DOI 10.1109/ICCV.2013.296; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hradi M., 2015, P BRIT MACH VIS C BM; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Huang YH, 2015, 2015 INTERNATIONAL AUTOMATIC CONTROL CONFERENCE (CACS), P13, DOI 10.1109/CACS.2015.7378358; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; KIM TH, 2018, TPAMI, V40, P2374, DOI DOI 10.1109/TPAMI.2017.2761348; Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521; Kupyn O., 2018, IEEE P INT C COMP VI; Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815; Liao SC, 2016, IEEE T PATTERN ANAL, V38, P211, DOI 10.1109/TPAMI.2015.2448075; Liu M.-Y., 2017, ARXIV170704993; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mirza M., 2014, ARXIV; Nah S., 2017, IEEE P INT C COMP VI; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nishiyama M, 2011, IEEE T PATTERN ANAL, V33, P838, DOI 10.1109/TPAMI.2010.203; Noroozi M, 2017, LECT NOTES COMPUT SC, V10496, P65, DOI 10.1007/978-3-319-66709-6_6; Nowozin S, 2016, ADV NEUR IN, V29; Odena A, 2016, INT C MACH LEARN WOR; Pan J., 2016, IEEE P INT C COMP VI; Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371; Pan JS, 2014, LECT NOTES COMPUT SC, V8695, P47, DOI 10.1007/978-3-319-10584-0_4; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ramakrishnan S., 2017, IEEE P INT C COMP VI; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3; Salimans T, 2016, ADV NEUR IN, V29; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418; Shen J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1003, DOI 10.1109/ICCVW.2015.132; Son S, 2018, INT CONF BIG DATA, P721, DOI 10.1109/BigComp.2018.00135; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; TAI YW, 2013, TPAMI, V35, P2498, DOI DOI 10.1109/TPAMI.2013.40; TEKALP AM, 1986, IEEE T ACOUST SPEECH, V34, P963, DOI 10.1109/TASSP.1986.1164886; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Whitelam C., 2017, IEEE P INT C COMP VI; Wieschollek P., 2017, IEEE P INT C COMP VI, V4; Wolf L., 2011, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2011.5995566; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36; Yang J, 2017, IEEE P INT C COMP VI; Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31; Zafeiriou S., 2017, IEEE P INT C COMP VI; Zhang HC, 2013, PROC CVPR IEEE, P1051, DOI 10.1109/CVPR.2013.140; Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134	78	13	13	3	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		801	823		10.1007/s11263-018-1138-7	http://dx.doi.org/10.1007/s11263-018-1138-7			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD		Green Published, hybrid			2022-12-18	WOS:000468525900015
J	Duong, CN; Luu, K; Quach, KG; Bui, TD				Chi Nhan Duong; Khoa Luu; Kha Gia Quach; Bui, Tien D.			Deep Appearance Models: A Deep Boltzmann Machine Approach for Face Modeling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep Boltzmann machines; Deep appearance models; Active appearance models; Principal component analysis; Facial super-resolution reconstruction; Facial off-angle reconstruction; Face frontalization; Age estimation	CONVOLUTIONAL NETWORK; RECOGNITION; AGE; REPRESENTATION; ALIGNMENT	The interpretation through synthesis approach to analyze face images, particularly Active Appearance Models (AAMs) method, has become one of the most successful face modeling approaches over the last two decades. AAM models have ability to represent face images through synthesis using a controllable parameterized Principal Component Analysis (PCA) model. However, the accuracy and robustness of the synthesized faces of AAMs are highly depended on the training sets and inherently on the generalizability of PCA subspaces. This paper presents a novel Deep Appearance Models (DAMs) approach, an efficient replacement for AAMs, to accurately capture both shape and texture of face images under large variations. In this approach, three crucial components represented in hierarchical layers are modeled using the Deep Boltzmann Machines (DBM) to robustly capture the variations of facial shapes and appearances. DAMs are therefore superior to AAMs in inferencing a representation for new face images under various challenging conditions. The proposed approach is evaluated in various applications to demonstrate its robustness and capabilities, i.e. facial super-resolution reconstruction, facial off-angle reconstruction or face frontalization, facial occlusion removal and age estimation using challenging face databases, i.e. Labeled Face Parts in the Wild, Helen and FG-NET. Comparing to AAMs and other deep learning based approaches, the proposed DAMs achieve competitive results in those applications, thus this showed their advantages in handling occlusions, facial representation, and reconstruction.	[Chi Nhan Duong; Kha Gia Quach; Bui, Tien D.] Concordia Univ, Comp Sci & Software Engn Dept, Montreal, PQ, Canada; [Chi Nhan Duong; Kha Gia Quach] Carnegie Mellon Univ, CyLab Biometr Ctr, Pittsburgh, PA 15213 USA; [Chi Nhan Duong; Kha Gia Quach] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA; [Khoa Luu] Univ Arkansas, Comp Sci & Comp Engn Dept, Fayetteville, AR 72701 USA	Concordia University - Canada; Carnegie Mellon University; Carnegie Mellon University; University of Arkansas System; University of Arkansas Fayetteville	Duong, CN (corresponding author), Concordia Univ, Comp Sci & Software Engn Dept, Montreal, PQ, Canada.; Duong, CN (corresponding author), Carnegie Mellon Univ, CyLab Biometr Ctr, Pittsburgh, PA 15213 USA.; Duong, CN (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.	c_duon@encs.concordia.ca; khoaluu@uark.edu; k_q@encs.concordia.ca; bui@encs.concordia.ca	Luu, Khoa/AAQ-8540-2021; Quach, Kha Gia/Z-5464-2019	Luu, Khoa/0000-0003-2104-0901; Quach, Kha Gia/0000-0001-6952-306X; Duong, Chi Nhan/0000-0002-5177-0393	Natural Sciences and Engineering Research Council (NSERC) of Canada	Natural Sciences and Engineering Research Council (NSERC) of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada.	Alabort-i-Medina J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P679, DOI 10.1145/2647868.2654890; Alabort-i-Medina J, 2017, INT J COMPUT VISION, V121, P26, DOI 10.1007/s11263-016-0916-3; Alabort-i-Medina J, 2015, PROC CVPR IEEE, P3679, DOI 10.1109/CVPR.2015.7298991; Alabort-i-Medina J, 2014, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2014.439; Amberg B, 2009, PROC CVPR IEEE, P1714, DOI 10.1109/CVPRW.2009.5206788; Anderson R, 2013, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2013.434; [Anonymous], 2010, MIR 10 P 2010 ACM IN; Antonakos E, 2016, IEEE IMAGE PROC, P1649, DOI 10.1109/ICIP.2016.7532638; Antonakos E, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2431445; Antonakos E, 2014, IEEE IMAGE PROC, P224, DOI 10.1109/ICIP.2014.7025044; Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319; Duong N, 2015, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2015.7299111; Cohn J. F, 2016, P IEEE C COMP VIS PA, P87; Cootes T. F., 1998, FG, P300; Cootes T.F., 2006, P BRIT MACH VIS C, P919; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Donner R, 2006, IEEE T PATTERN ANAL, V28, P1690, DOI 10.1109/TPAMI.2006.206; Duong C. N., 2011, INT C AC SPEECH SIGN; Edwards G. J., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P581, DOI 10.1007/BFb0054766; Eslami SMA, 2014, INT J COMPUT VISION, V107, P155, DOI 10.1007/s11263-013-0669-1; Ferrari C, 2016, INT C PATT RECOG, P1047, DOI 10.1109/ICPR.2016.7899774; Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847; Gao SH, 2015, IEEE T INF FOREN SEC, V10, P2108, DOI 10.1109/TIFS.2015.2446438; Ge YX, 2013, J VIS COMMUN IMAGE R, V24, P627, DOI 10.1016/j.jvcir.2013.04.011; Gross R, 2005, IMAGE VISION COMPUT, V23, P1080, DOI 10.1016/j.imavis.2005.07.009; Haase D, 2014, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2014.185; HASSNER T, 2015, PROC CVPR IEEE, P4295, DOI DOI 10.1109/CVPR.2015.7299058; Hendriks E., 2010, COMP VIS PATT REC WO, P34; Hou XW, 2001, PROC CVPR IEEE, P828; Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968; Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Li CS, 2012, PROC CVPR IEEE, P2570, DOI 10.1109/CVPR.2012.6247975; Li C, 2014, LECT NOTES COMPUT SC, V8693, P218, DOI 10.1007/978-3-319-10602-1_15; Liu LQ, 2016, IEEE T MULTIMEDIA, V18, P64, DOI 10.1109/TMM.2015.2500730; Luu K., 2011, INT JOINT C BIOM IJC; Luu K., 2011, INT C AUT FAC GEST R; Luu K, 2009, 2009 IEEE 3 INT C BI, P1, DOI DOI 10.1109/BTAS.2009.5339053; Luu K., 2010, COMP VIS PATT REC WO; Martnez A., 1998, THE AR FACE DATABASE; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Mollahosseini A, 2013, IEEE COMPUT SOC CONF, P875, DOI 10.1109/CVPRW.2013.129; Navarathna R, 2011, IEEE I CONF COMP VIS, P1919, DOI 10.1109/ICCV.2011.6126461; Ngiam J, 2011, P 28 INT C MACH LEAR, V28, P689, DOI DOI 10.5555/3104482.3104569; Papandreou George, 2008, COMP VIS PATT REC 20, P1; Pizarro D., 2008, CVPR, P1; Sagonas C, 2015, IEEE I CONF COMP VIS, P3871, DOI 10.1109/ICCV.2015.441; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Salakhutdinov R., 2009, P 23 ANN C NEURAL IN, P1598; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Saragih J, 2007, IEEE I CONF COMP VIS, P2173; Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI [DOI 10.1162/NEC0_A_00311, DOI 10.1109/CVPR.2013.49]; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Sung J, 2008, IEEE T SYST MAN CY A, V38, P852, DOI 10.1109/TSMCA.2008.923047; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tang Y., 2012, ICML; Tang YC, 2012, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2012.6247936; Taylor GW, 2010, PROC CVPR IEEE, P631, DOI 10.1109/CVPR.2010.5540157; Tzimiropoulos G, 2017, INT J COMPUT VISION, V122, P17, DOI 10.1007/s11263-016-0950-1; Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79; Wang B, 2015, IEEE IMAGE PROC, P1648, DOI 10.1109/ICIP.2015.7351080; Wang XL, 2015, IEEE WINT CONF APPL, P534, DOI 10.1109/WACV.2015.77; Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649; Wu Y, 2013, PROC CVPR IEEE, P3452, DOI 10.1109/CVPR.2013.443; Xing JL, 2014, PROC CVPR IEEE, P1829, DOI 10.1109/CVPR.2014.236; Yang CY, 2013, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR.2013.146; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yildirim I., 2015, COGSCI; Zhai HH, 2015, LECT NOTES COMPUT SC, V9242, P341, DOI 10.1007/978-3-319-23989-7_35; Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342; Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286; Zhu CC, 2017, ADV COMPUT VIS PATT, P57, DOI 10.1007/978-3-319-61657-5_3; Zhu JK, 2006, LECT NOTES COMPUT SC, V3951, P186; Zhu Z., 2014, NIPS, P217; Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21	80	13	15	0	29	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2019	127	5					437	455		10.1007/s11263-018-1113-3	http://dx.doi.org/10.1007/s11263-018-1113-3			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HO7XB		Green Submitted			2022-12-18	WOS:000461162200002
J	Tang, M; Marin, D; Ben Ayed, I; Boykov, Y				Tang, Meng; Marin, Dmitrii; Ben Ayed, Ismail; Boykov, Yuri			Kernel Cuts: Kernel and Spectral Clustering Meet Regularization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Segmentation; Markov random fields; Spectral clustering; Kernel methods; Bound optimization	IMAGE SEGMENTATION; GRAPH CUTS; OPTIMIZATION; TEXTURE; CLASSIFICATION; APPROXIMATIONS; MINIMIZATION; SCENE	This work bridges the gap between two popular methodologies for data partitioning: kernel clustering and regularization-based segmentation. While addressing closely related practical problems, these general methodologies may seem very different based on how they are covered in the literature. The differences may show up in motivation, formulation, and optimization, e.g.spectral relaxation versus max-flow. We explain how regularization and kernel clustering can work together and why this is useful. Our joint energy combines standard regularization, e.g.MRF potentials, and kernel clustering criteria like normalized cut. Complementarity of such terms is demonstrated in many applications using our bound optimization Kernel Cut algorithm for the joint energy (code is publicly available). While detailing combinatorial move-making, our main focus are new linear kernel and spectral bounds for kernel clustering criteria allowing their integration with any regularization objectives with existing discrete or continuous solvers.	[Tang, Meng; Marin, Dmitrii; Boykov, Yuri] Univ Waterloo, Comp Sci, Waterloo, ON, Canada; [Marin, Dmitrii] Western Univ, Comp Sci, London, ON, Canada; [Ben Ayed, Ismail] ETS Montreal, Montreal, PQ, Canada	University of Waterloo; Western University (University of Western Ontario); University of Quebec; Ecole de Technologie Superieure - Canada	Tang, M (corresponding author), Univ Waterloo, Comp Sci, Waterloo, ON, Canada.	m62tang@uwaterloo.ca; dmitrii.a.marin@gmail.com; ismail.benayed@etsmtl.ca; yboykov@uwaterloo.ca						Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x; Aggarwal CC, 2014, CH CRC DATA MIN KNOW, P1; [Anonymous], 2006, NONLINEAR PROGRAMMIN; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bach FR, 2004, ADV NEUR IN, V16, P305; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Belongie S., 1998, P EUR C COMP VIS ECC; Ben Ayed I, 2005, IEEE T PATTERN ANAL, V27, P793, DOI 10.1109/TPAMI.2005.106; Ben Ayed I, 2013, PROC CVPR IEEE, P1304, DOI 10.1109/CVPR.2013.172; Ben Salah M, 2010, IEEE T IMAGE PROCESS, V19, P220, DOI 10.1109/TIP.2009.2032940; Bishop C.M, 2006, PATTERN RECOGN; Boyd S, 2004, CONVEX OPTIMIZATION; Boykov Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P26; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Breiman L, 1996, MACH LEARN, V24, P41, DOI 10.1023/A:1018094028462; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Carreira-Perpinan M. A., 2013, ARXIV13046478V1CSLG; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chambolle A, 2004, J MATH IMAGING VIS, V20, P89; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chew SE, 2015, IEEE I CONF COMP VIS, P1716, DOI 10.1109/ICCV.2015.200; Chitta R., 2011, P 17 ACM SIGKDD INT, P895; Collins MD, 2014, LECT NOTES COMPUT SC, V8691, P282, DOI 10.1007/978-3-319-10578-9_19; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Cox I. J., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P557, DOI 10.1109/ICPR.1996.546886; Cox T., 2000, MULTIDIMENSIONAL SCA; Cremers D, 2007, INT J COMPUT VISION, V72, P195, DOI 10.1007/s11263-006-8711-1; Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z; Deng Z., 2015, INT C COMP VIS ICCV; Dhillon I., 2004, KDD; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; Dou MS, 2015, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2015.7298647; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Eriksson A, 2011, J MATH IMAGING VIS, V39, P45, DOI 10.1007/s10851-010-0223-5; Fanti C, 2004, ADV NEUR IN, V16, P1603; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Girolami M, 2002, IEEE T NEURAL NETWOR, V13, P780, DOI 10.1109/TNN.2002.1000150; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Gottfried Jens-Malte, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P758; Gulshan V., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1127, DOI 10.1109/ICCVW.2011.6130376; Hein M, 2004, LECT NOTES COMPUT SC, V3175, P270; Hochbaum DS, 2010, IEEE T PATTERN ANAL, V32, P889, DOI 10.1109/TPAMI.2009.80; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Jayasumana S, 2015, IEEE T PATTERN ANAL, V37, P2464, DOI 10.1109/TPAMI.2015.2414422; Jermyn IH, 2001, IEEE T PATTERN ANAL, V23, P1075, DOI 10.1109/34.954599; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Kearns M., 1997, C UNC ART INT UAI; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Kolmogorov V, 2007, IEEE I CONF COMP VIS, P644; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Krahenbuhl P., 2011, P ADV NEUR INF PROC; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis B, 2009, MACH LEARN, V74, P1, DOI 10.1007/s10994-008-5084-4; Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605; Lempitsky V., 2008, ECCV; Lempitsky V, 2009, IEEE I CONF COMP VIS, P277, DOI 10.1109/ICCV.2009.5459262; Li S, 2009, MARKOV RANDOM FIELD; Li T., 2004, INT C MOB LEARN; Linli Xu, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2866, DOI 10.1109/CVPRW.2009.5206561; Louppe G., 2013, ADV NEURAL INFORM PR, V26, P431, DOI DOI 10.5555/2999611.2999660; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; Marin D., 2018, KERNEL CLUSTERING DE, DOI [10. 1109/TPAMI. 2017. 2780166, DOI 10.1109/TPAMI.2017.2780166]; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; McGuinness K, 2010, PATTERN RECOGN, V43, P434, DOI 10.1016/j.patcog.2009.03.008; Menze M., 2015, C COMP VIS PATT REC; Mitiche A, 2010, SPRINGER TOP SIGN PR, V5, P1; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Narasimhan M., 2005, UAI, P404; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Ng AY, 2002, ADV NEUR IN, V14, P849; Nieuwenhuis C, 2013, IEEE T PATTERN ANAL, V35, P1234, DOI 10.1109/TPAMI.2012.183; Ochs P, 2011, IEEE I CONF COMP VIS, P1583, DOI 10.1109/ICCV.2011.6126418; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568; Park K., 2012, ECCV; Pock T., 2009, IEEE C COMP VIS PATT; Ren XF, 2012, PROC CVPR IEEE, P2759, DOI 10.1109/CVPR.2012.6247999; Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788; Roth V, 2003, IEEE T PATTERN ANAL, V25, P1540, DOI 10.1109/TPAMI.2003.1251147; Rother C., 2004, ACM T GRAPHICS SIGGR; Rousson M., 2002, WORKSH MOT VID COMP; Shawe-Tayler J, 2004, KERNEL METHODS PATTE; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Silberman N., 2012, ECCV; Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648; Tang M., 2015, INT C COMP VIS ICCV; Tang M., 2016, EUR C COMP VIS ECCV; Tang M., 2016, ARXIV150607439; Tang M., 2013, INT C COMP VIS ICCV; Tang M, 2014, LECT NOTES COMPUT SC, V8693, P691, DOI 10.1007/978-3-319-10602-1_45; Vapnik V.N, 1998, STAT LEARNING THEORY; Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4; Veksler O, 2018, IEEE T PATTERN ANAL; Vicente S., 2009, INT C COMP VIS ICCV; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wang S, 2003, IEEE T PATTERN ANAL, V25, P675, DOI 10.1109/TPAMI.2003.1201819; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085; Yu S., 2003, INT C COMP VIS ICCV; Yu SX, 2004, IEEE T PATTERN ANAL, V26, P173, DOI 10.1109/TPAMI.2004.1262179; Yu YZ, 2015, IEEE I CONF COMP VIS, P1368, DOI 10.1109/ICCV.2015.161; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343	110	13	14	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2019	127	5					477	511		10.1007/s11263-018-1115-1	http://dx.doi.org/10.1007/s11263-018-1115-1			35	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HO7XB					2022-12-18	WOS:000461162200004
J	Wang, HX; Zhu, XT; Gong, SG; Xiang, T				Wang, Hanxiao; Zhu, Xiatian; Gong, Shaogang; Xiang, Tao			Person Re-identification in Identity Regression Space	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Feature embedding space; Regression; Incremental learning; Active learning	ALGORITHM; VECTORS	Most existing person re-identification (re-id) methods are unsuitable for real-world deployment due to two reasons: Unscalability to large population size, and Inadaptability over time. In this work, we present a unified solution to address both problems. Specifically, we propose to construct an identity regression space (IRS) based on embedding different training person identities (classes) and formulate re-id as a regression problem solved by identity regression in the IRS. The IRS approach is characterised by a closed-form solution with high learning efficiency and an inherent incremental learning capability with human-in-the-loop. Extensive experiments on four benchmarking datasets (VIPeR, CUHK01, CUHK03 and Market-1501) show that the IRS model not only outperforms state-of-the-art re-id methods, but also is more scalable to large re-id population size by rapidly updating model and actively selecting informative samples with reduced human labelling effort.	[Wang, Hanxiao] Boston Univ, Elect & Comp Engn Dept, Boston, MA 02215 USA; [Zhu, Xiatian; Gong, Shaogang; Xiang, Tao] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England; [Zhu, Xiatian] Vis Semant Ltd, London E1 4NS, England	Boston University; University of London; Queen Mary University London	Zhu, XT (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.; Zhu, XT (corresponding author), Vis Semant Ltd, London E1 4NS, England.	hxw@bu.edu; eddy@visionsemantics.com; s.gong@qmul.ac.uk; t.xiang@qmul.ac.uk	Zhu, Xiatian/Y-1601-2019	Zhu, Xiatian/0000-0002-9284-2955	China Scholarship Council; Vision Semantics Ltd; Royal Society Newton Advanced Fellowship Programme [NA150459]; Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety [98111-571149]	China Scholarship Council(China Scholarship Council); Vision Semantics Ltd; Royal Society Newton Advanced Fellowship Programme; Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety(UK Research & Innovation (UKRI)Innovate UK)	This work was partially supported by the China Scholarship Council, Vision Semantics Ltd, Royal Society Newton Advanced Fellowship Programme (NA150459), and Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety (98111-571149).	Ahmed E., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299016; Akaike H., 1973, 2 INT S INF THEOR, P267, DOI DOI 10.1007/978-1-4612-1694-0_15; [Anonymous], 2001, P 2000 C ADV NEUR IN; Cai D, 2008, IEEE T KNOWL DATA EN, V20, P1, DOI 10.1109/TKDE.2007.190669; Cebron N, 2009, DATA MIN KNOWL DISC, V18, P283, DOI 10.1007/s10618-008-0115-0; Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142; Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9; Chen SZ, 2016, IEEE T IMAGE PROCESS, V25, P2353, DOI 10.1109/TIP.2016.2545929; Chen YC, 2017, IEEE T CIRC SYST VID, V27, P1661, DOI 10.1109/TCSVT.2016.2515309; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Das A, 2015, IEEE IMAGE PROC, P4263, DOI 10.1109/ICIP.2015.7351610; Duda R. O., 2012, PATTERN CLASSIFICATI; Ebert S, 2012, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2012.6248108; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Geng M., 2016, ARXIV161105244; Gong SG, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4_1; Gray D., 2007, IEEE INT WORKSH PERF, V3, P1; Guo YF, 2006, PATTERN RECOGN, V39, P2248, DOI 10.1016/j.patcog.2006.05.009; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; Hastie T, 2009, ELEMENTS STAT LEARNI; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; Horn R.A., 2012, MATRIX ANAL, DOI [DOI 10.1017/CBO9780511810817, 10.1017/CBO9780511810817]; Hospedales TM, 2012, LECT NOTES COMPUT SC, V7576, P453, DOI 10.1007/978-3-642-33715-4_33; Joshi Ajay J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2372, DOI 10.1109/CVPRW.2009.5206627; Kading C., 2015, IEEE C COMP VIS PATT; Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499; Kang JH, 2004, LECT NOTES ARTIF INT, V3056, P384; Karanam S., 2016, COMPREHENSIVE EVALUA; Kodirov E, 2016, LECT NOTES COMPUT SC, V9905, P178, DOI 10.1007/978-3-319-46448-0_11; Kostinger Martin, 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247939; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; Layne R, 2013, ARTEMIS; Li W., 2013, LNCS, V7724, P31, DOI [10.1007/978-3-642-37331-2, DOI 10.1007/978-3-642-37331-2]; Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463; Liao S., 2010, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2010.5539817; Liao S., 2014, ARXIV14080872; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477; Lisanti G., 2014, P INT C DISTRIBUTED, P1, DOI DOI 10.1145/2659021.2659036; Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055; Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62; Loy C. C., 2012, IEEE C COMP VIS PATT; Ma AJ, 2013, IEEE I CONF COMP VIS, P3567, DOI 10.1109/ICCV.2013.443; Ma BP, 2012, LECT NOTES COMPUT SC, V7583, P413, DOI 10.1007/978-3-642-33863-2_41; Martinel N, 2016, LECT NOTES COMPUT SC, V9908, P858, DOI 10.1007/978-3-319-46493-0_52; Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152; McLaughlin A, 2015, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2015.10; Mika S., 1999, NEURAL NETWORKS SIGN, V1999, P41, DOI DOI 10.1109/NNSP.1999.788121; Moghaddam B, 2000, PATTERN RECOGN, V33, P1771, DOI 10.1016/S0031-3203(99)00179-X; Osugi T., 2005, IEEE INT C DAT MIN; PAIGE CC, 1982, ACM T MATH SOFTWARE, V8, P43, DOI 10.1145/355984.355989; Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Park CH, 2005, SIAM J MATRIX ANAL A, V27, P87, DOI 10.1137/S0895479804442334; Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426; Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146; Penrose R., 1955, P CAMBRIDGE PHILOS S, V51, P406, DOI [10.1017/S0305004100030401, DOI 10.1017/S0305004100030401]; Prosser B. J., 2010, PROC BRIT MACH VIS C, P6, DOI DOI 10.5244/C.24.21; Ristin M., 2014, IEEE C COMP VIS PATT; Settles B., 2012, SYNTHESIS LECT ARTIF, V6, P1, DOI [10.2200/s00429ed1v01y201207aim018, DOI 10.2200/S00429ED1V01Y201207AIM018]; Settles B, 2008, P 2008 C EMP METH NA, ppp1070, DOI DOI 10.3115/1613715.1613855; Sharma A., 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247923; Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44; Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30; Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9; Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48; Wang F, 2016, IEEE CONF COMPUT; Wang H., 2014, PROC IEEE INT C CONS, DOI 10.5244/C.28.48; Wang HX, 2016, LECT NOTES COMPUT SC, V9908, P405, DOI 10.1007/978-3-319-46493-0_25; Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418; Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45; Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207; Wang ZM, 2016, LECT NOTES COMPUT SC, V9907, P453, DOI 10.1007/978-3-319-46487-9_28; Woodbury M.A., 1950, 42 PRINC U; Wu SX, 2016, IEEE WINT CONF APPL; Xiang T., 2016, BRIT MACH VIS C; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1; Yang L, 2006, DISTANCE METRIC LEAR; Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35; Yi D., 2014, DEEP METRIC LEARNING; Yu H, 2001, PATTERN RECOGN, V34, P2067, DOI 10.1016/S0031-3203(00)00162-X; Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139; Zhang Y., 2013, IEEE C COMP VIS PATT; Zhang Y, 2016, PROC CVPR IEEE, P5985, DOI 10.1109/CVPR.2016.644; Zhang ZH, 2010, J MACH LEARN RES, V11, P2199; Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zheng L., 2016, ARXIV; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138; Zheng WM, 2005, IEEE T NEURAL NETWOR, V16, P1, DOI 10.1109/TNN.2004.836239; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhu F, 2016, LECT NOTES COMPUT SC, V9911, P305, DOI 10.1007/978-3-319-46478-7_19	99	13	14	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2018	126	12			SI		1288	1310		10.1007/s11263-018-1105-3	http://dx.doi.org/10.1007/s11263-018-1105-3			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GZ3KT	30930537	Green Published, hybrid, Green Submitted			2022-12-18	WOS:000449286200003
J	Bergmann, R; Fitschen, JH; Persch, J; Steidl, G				Bergmann, Ronny; Fitschen, Jan Henrik; Persch, Johannes; Steidl, Gabriele			Iterative Multiplicative Filters for Data Labeling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Labeling; Supervised partitioning; Multiplicative filter; Partitioning; Manifold-valued images	IMAGE SEGMENTATION; RELAXATION; APPROXIMATIONS	Based on an idea in strom et al. (J Math ImagingVis, doi10.1007/s10851-016-0702-4, 2017) we propose a new iterative multiplicative filtering algorithm for label assignment matrices which can be used for the supervised partitioning of data. Starting with a row-normalized matrix containing the averaged distances between prior features and observed ones, the method assigns in a very efficient way labels to the data. We interpret the algorithm as a gradient ascent method with respect to a certain function on the product manifold of positive numbers followed by a reprojection onto a subset of the probability simplex consisting of vectors whose components are bounded away from zero by a small constant. While such boundedness away from zero is necessary to avoid an arithmetic underflow, our convergence results imply that they are also necessary for theoretical reasons. Numerical examples show that the proposed simple and fast algorithm leads to very good results. In particular we apply the method for the partitioning of manifold-valued images.	[Bergmann, Ronny; Fitschen, Jan Henrik; Persch, Johannes; Steidl, Gabriele] Tech Univ Kaiserslautern, Dept Math, Postfach 3049, D-67653 Kaiserslautern, Germany	University of Kaiserslautern	Bergmann, R (corresponding author), Tech Univ Kaiserslautern, Dept Math, Postfach 3049, D-67653 Kaiserslautern, Germany.	bergmann@mathematik.uni-kl.de; fitschen@mathematik.uni-kl.de; persch@mathematik.uni-kl.de; steidl@mathematik.uni-kl.de	Bergmann, Ronny/H-3806-2019	Bergmann, Ronny/0000-0001-8342-7218	German Research Foundation (DFG) [STE 571/13-1, BE 5888/2-1]; German Federal Ministry of Education and Research (BMBF) [05M13UKA]	German Research Foundation (DFG)(German Research Foundation (DFG)); German Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF))	We are grateful to Ch. Schnorr (University of Heidelberg) for stimulating discussions. Many thanks to R. Hielscher (University of Chemnitz) for supporting the work on EBSD data. We thank the referees for requesting a discussion of condition (PI). Funding by the German Research Foundation (DFG) within the Project STE 571/13-1 & BE 5888/2-1 and within the Research Training Group 1932 "Stochastic Models for Innovations in the Engineering Sciences", project area P3, is gratefully acknowledged. Furthermore, G. Steidl acknowledges the support by the German Federal Ministry of Education and Research (BMBF) through Grant 05M13UKA (AniS).	ADAMS BL, 1993, METALL TRANS A, V24, P819, DOI 10.1007/BF02656503; Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965; Astrom F, 2017, J MATH IMAGING VIS, V58, P211, DOI 10.1007/s10851-016-0702-4; Astrom F., 2016, P 2 INT WORKSH DIFF; Bachmann F., 2005, MTEX MATLAB TOOLBOX; Bachmann F, 2011, ULTRAMICROSCOPY, V111, P1720, DOI 10.1016/j.ultramic.2011.08.002; Bae E, 2011, INT J COMPUT VISION, V92, P112, DOI 10.1007/s11263-010-0406-y; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Buades A, 2010, SIAM REV, V52, P113, DOI 10.1137/090773908; Buades A, 2006, NUMER MATH, V105, P1, DOI [10.1007/s00211-006-0029-y, 10.1007/s00211-006-0029-v]; Burger M, 2016, OPERATOR SPLITTINGS; Cai XH, 2013, SIAM J IMAGING SCI, V6, P368, DOI 10.1137/120867068; Chambolle A, 2012, SIAM J IMAGING SCI, V5, P1113, DOI 10.1137/110856733; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286; Chaux C, 2011, J MATH IMAGING VIS, V41, P23, DOI 10.1007/s10851-010-0241-3; Cook PA, 2006, P 13 SCI M, P2759; Deledalle CA, 2009, IEEE T IMAGE PROCESS, V18, P2661, DOI 10.1109/TIP.2009.2029593; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Folland G. B., 1999, REAL ANAL; Frobenius G. F., 1912, UBER MATRIZEN NICHTN; Gilboa G, 2007, MULTISCALE MODEL SIM, V6, P595, DOI 10.1137/060669358; Graf M, 2012, ADV COMPUT MATH, V37, P379, DOI 10.1007/s10444-011-9214-3; Hauser S, 2013, INT J COMPUT MATH, V90, P62, DOI 10.1080/00207160.2012.688960; HERAULT L, 1993, IEEE T PATTERN ANAL, V15, P899, DOI 10.1109/34.232076; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; Horn R.A., 2013, MATRIX ANAL, Vsecond; HUMMEL RA, 1983, IEEE T PATTERN ANAL, V5, P267, DOI 10.1109/TPAMI.1983.4767390; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; KUNZE K, 1993, TEXTURE MICROSTRUCT, V20, P41, DOI 10.1155/TSM.20.41; Laus F., 2016, SIAM J IMAGING SCI; Lellmann J, 2011, SIAM J IMAGING SCI, V4, P1049, DOI 10.1137/100805844; Lellmann J, 2013, J MATH IMAGING VIS, V47, P239, DOI 10.1007/s10851-012-0390-7; Moakher M, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P285, DOI 10.1007/3-540-31272-2_17; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Nolze G, 2016, J APPL CRYSTALLOGR; ORLAND H, 1985, J PHYS LETT-PARIS, V46, pL763, DOI 10.1051/jphyslet:019850046017076300; Pelillo M, 1997, J MATH IMAGING VIS, V7, P309, DOI 10.1023/A:1008255111261; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Perron O, 1907, MATH ANN, V64, P248, DOI 10.1007/BF01449896; Petra Stefania, 2016, P ECCV; Peyre G., 2015, ARXIV150206216V3; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; Wielandt H, 1950, MATH Z, V52, P642, DOI DOI 10.1007/BF02230720; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085	53	13	13	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2017	123	3					435	453		10.1007/s11263-017-0995-9	http://dx.doi.org/10.1007/s11263-017-0995-9			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EX9EO		Green Submitted			2022-12-18	WOS:000403559600007
J	Aghdam, H; Heravi, E; Puig, D				Habibi Aghdam, Hamed; Jahani Heravi, Elnaz; Puig, Domenec			A Practical and Highly Optimized Convolutional Neural Network for Classifying Traffic Signs in Real-Time	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Convolutional neural network; Traffic sign classification; Ensemble construction; Visualizing convolutional neural networks	RECOGNITION; COLOR	Classifying traffic signs is an indispensable part of Advanced Driver Assistant Systems. This strictly requires that the traffic sign classification model accurately classifies the images and consumes as few CPU cycles as possible to immediately release the CPU for other tasks. In this paper, we first propose a new ConvNet architecture. Then, we propose a new method for creating an optimal ensemble of ConvNets with highest possible accuracy and lowest number of ConvNets. Our experiments show that the ensemble of our proposed ConvNets (the ensemble is also constructed using our method) reduces the number of arithmetic operations 88 and compared with two state-of-art ensemble of ConvNets. In addition, our ensemble is more accurate than one of the state-of-art ensembles and it is only less accurate than the other state-of-art ensemble when tested on the same dataset. Moreover, ensemble of our compact ConvNets reduces the number of the multiplications 95 and , yet, the classification accuracy drops only 0.2 and 0.4% compared with these two ensembles. Besides, we also evaluate the cross-dataset performance of our ConvNet and analyze its transferability power in different layers. We show that our network is easily scalable to new datasets with much more number of traffic sign classes and it only needs to fine-tune the weights starting from the last convolution layer. We also assess our ConvNet through different visualization techniques. Besides, we propose a new method for finding the minimum additive noise which causes the network to incorrectly classify the image by minimum difference compared with the highest score in the loss vector.	[Habibi Aghdam, Hamed; Jahani Heravi, Elnaz; Puig, Domenec] Univ Rovira & Virgili, Dept Comp Engn & Math, Intelligent Robot & Comp Vis Grp, Tarragona, Spain	Universitat Rovira i Virgili	Aghdam, H (corresponding author), Univ Rovira & Virgili, Dept Comp Engn & Math, Intelligent Robot & Comp Vis Grp, Tarragona, Spain.	hamed.habibi@urv.cat; elnaz.jahani@urv.cat; domenec.puig@urv.cat			Generalitat de Catalunya's Agecia de Gestio d'Ajuts Universitaris i de Recerca (AGAUR) through FI-DGR; Marti Franques fellowships	Generalitat de Catalunya's Agecia de Gestio d'Ajuts Universitaris i de Recerca (AGAUR) through FI-DGR; Marti Franques fellowships	The authors are grateful for the support granted by Generalitat de Catalunya's Agecia de Gestio d'Ajuts Universitaris i de Recerca (AGAUR) through FI-DGR 2015 and Marti Franques 2015 fellowships.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; Baro X, 2009, IEEE T INTELL TRANSP, V10, P113, DOI 10.1109/TITS.2008.2011702; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30; Donahue J, 2014, PR MACH LEARN RES, V32; Dosovitskiy A, 2015, ARXIV150602753, P1; Fleyeh H, 2011, IET INTELL TRANSP SY, V5, P190, DOI 10.1049/iet-its.2010.0159; Gao XW, 2006, J VIS COMMUN IMAGE R, V17, P675, DOI 10.1016/j.jvcir.2005.10.003; Greenhalgh J, 2012, IEEE T INTELL TRANSP, V13, P1498, DOI 10.1109/TITS.2012.2208909; Habibi Aghdam Hamed, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P87; He K., 2015, ARXIV150201852; Hsu SH, 2001, IMAGE VISION COMPUT, V19, P119, DOI 10.1016/S0262-8856(00)00050-0; Huang GF, 2013, J NANOMATER, V2013, DOI 10.1155/2013/371356; Jin JQ, 2014, IEEE T INTELL TRANSP, V15, P1991, DOI 10.1109/TITS.2014.2308281; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larsson F, 2011, LECT NOTES COMPUT SC, V6688, P238, DOI 10.1007/978-3-642-21227-7_23; Liu H, 2014, J HAZARD MATER, V266, P75, DOI 10.1016/j.jhazmat.2013.12.013; Lu K, 2012, IEEE T INTELL TRANSP, V13, P1515, DOI 10.1109/TITS.2012.2220965; Maas AL, 2013, INT C MACH LEARN ICM, V30; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Bascon SM, 2010, COMPUT VIS IMAGE UND, V114, P373, DOI 10.1016/j.cviu.2009.12.002; Maldonado-Bascon S, 2007, IEEE T INTELL TRANSP, V8, P264, DOI 10.1109/TITS.2007.895311; Mathias M, 2013, IEEE IJCNN; Mogelmose A, 2012, IEEE T INTELL TRANSP, V13, P1484, DOI 10.1109/TITS.2012.2209421; Moiseev B, 2013, LECT NOTES COMPUT SC, V8192, P576, DOI 10.1007/978-3-319-02895-8_52; Paclik P, 2000, PATTERN RECOGN LETT, V21, P1165, DOI 10.1016/S0167-8655(00)00078-7; Piccioli G, 1996, IMAGE VISION COMPUT, V14, P209, DOI 10.1016/0262-8856(95)01057-2; Ruta A, 2010, IEEE T INTELL TRANSP, V11, P846, DOI 10.1109/TITS.2010.2051427; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Sermanet P, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2809, DOI 10.1109/IJCNN.2011.6033589; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016; Sun ZL, 2014, NEUROCOMPUTING, V128, P153, DOI 10.1016/j.neucom.2012.11.057; Szegedy C., 2014, ARXIV13126199V4; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Timofte R, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.61; Timofte R, 2014, MACH VISION APPL, V25, P633, DOI 10.1007/s00138-011-0391-3; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; WANG JJ, 2010, PROC CVPR IEEE, P3360, DOI DOI 10.1109/CVPR.2010.5540018; Yosinski J, 2014, ADV NEUR IN, V27; Yuan X, 2014, IEEE T INTELL TRANSP, V15, P1466, DOI 10.1109/TITS.2014.2298912; Zaklouta F, 2014, ROBOT AUTON SYST, V62, P16, DOI 10.1016/j.robot.2012.07.019; Zaklouta F, 2012, IEEE T INTELL TRANSP, V13, P1507, DOI 10.1109/TITS.2012.2225618; Zaklouta F, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2151, DOI 10.1109/IJCNN.2011.6033494; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeng YJ, 2015, LECT NOTES COMPUT SC, V9242, P272, DOI 10.1007/978-3-319-23989-7_28	49	13	14	1	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		246	269		10.1007/s11263-016-0955-9	http://dx.doi.org/10.1007/s11263-016-0955-9			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS					2022-12-18	WOS:000398162200005
J	Alchatzidis, S; Sotiras, A; Zacharaki, EI; Paragios, N				Alchatzidis, Stavros; Sotiras, Aristeidis; Zacharaki, Evangelia I.; Paragios, Nikos			A Discrete MRF Framework for Integrated Multi-Atlas Registration and Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-atlas segmentation; Medical imaging; Markov random fields; Discrete optimization	IMAGE SEGMENTATION; LABEL FUSION; SIMILARITY; STRATEGIES; VALIDATION; TRUTH	Multi-atlas segmentation has emerged in recent years as a simple yet powerful approach in medical image segmentation. It commonly comprises two steps: (1) a series of pairwise registrations that establish correspondences between a query image and a number of atlases, and (2) the fusion of the available segmentation hypotheses towards labeling objects of interest. In this paper, we introduce a novel approach that solves simultaneously for the underlying segmentation labels and the multi-atlas registration. The proposed approach is formulated as a pairwise Markov Random Field, where registration and segmentation nodes are coupled towards simultaneously recovering all atlas deformations and labeling the query image. The coupling is achieved by promoting the consistency between selected deformed atlas segmentations and the estimated query segmentation. Additional membership fields are estimated, determining the participation of each atlas in labeling each voxel. Inference is performed by using a sequential relaxation scheme. The proposed approach is validated on the IBSR dataset and is compared against standard post-registration label fusion strategies. Promising results demonstrate the potential of our method.	[Alchatzidis, Stavros; Zacharaki, Evangelia I.; Paragios, Nikos] INRIA Saclay, Equipe GALEN, Ile de France, Orsay, France; [Alchatzidis, Stavros; Zacharaki, Evangelia I.; Paragios, Nikos] Ecole Cent Paris, Dept Appl Math, Ctr Visual Comp, F-92295 Chatenay Malabry, France; [Sotiras, Aristeidis] Univ Penn, Dept Radiol, Sect Biomed Image Anal, Philadelphia, PA 19104 USA	UDICE-French Research Universities; Universite Paris Saclay; University of Pennsylvania	Alchatzidis, S (corresponding author), INRIA Saclay, Equipe GALEN, Ile de France, Orsay, France.; Alchatzidis, S (corresponding author), Ecole Cent Paris, Dept Appl Math, Ctr Visual Comp, F-92295 Chatenay Malabry, France.	stavros.alchatzidis@ecp.fr; aristeidis.sotiras@uphs.upenn.edu; evangelia.zacharaki@centralesupelec.fr; nikos.paragios@ecp.fr	Zacharaki, Evangelia/AAC-6661-2020	Zacharaki, Evangelia/0000-0001-8228-0437; Sotiras, Aristeidis/0000-0003-0795-8820	European Research Council Grant Diocles [ERC-STG-259112]	European Research Council Grant Diocles	This research was partially supported by European Research Council Grant Diocles (ERC-STG-259112).	Akhondi-Asl A, 2013, IEEE T MED IMAGING, V32, P1840, DOI 10.1109/TMI.2013.2266258; Alchatzidis S., 2014, BRIT MACH VIS C; Artaechevarria X, 2009, IEEE T MED IMAGING, V28, P1266, DOI 10.1109/TMI.2009.2014372; Ashburner J, 2005, NEUROIMAGE, V26, P839, DOI 10.1016/j.neuroimage.2005.02.018; Asman AJ, 2013, LECT NOTES COMPUT SC, V8149, P759, DOI 10.1007/978-3-642-40811-3_95; Asman AJ, 2013, MED IMAGE ANAL, V17, P194, DOI 10.1016/j.media.2012.10.002; Asman AJ, 2011, IEEE T MED IMAGING, V30, P1779, DOI 10.1109/TMI.2011.2147795; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Cardoso MJ, 2013, MED IMAGE ANAL, V17, P671, DOI 10.1016/j.media.2013.02.006; Chen XH, 2005, LECT NOTES COMPUT SC, V3565, P126; Chen XH, 2004, LECT NOTES COMPUT SC, V3216, P663; Coupe P, 2011, NEUROIMAGE, V54, P940, DOI 10.1016/j.neuroimage.2010.09.018; Doshi J, 2013, ACAD RADIOL, V20, P1566, DOI 10.1016/j.acra.2013.09.010; Dowling JA, 2011, LECT NOTES COMPUT SC, V6963, P10, DOI 10.1007/978-3-642-23944-1_2; Fonov V., 2012, MICCAI 2012 WORKSH M, P63; Glocker B, 2011, ANNU REV BIOMED ENG, V13, P219, DOI 10.1146/annurev-bioeng-071910-124649; Heckemann RA, 2006, NEUROIMAGE, V33, P115, DOI 10.1016/j.neuroimage.2006.05.061; Iglesias JE, 2013, MED IMAGE ANAL, V17, P1181, DOI 10.1016/j.media.2013.08.001; Isgum I, 2009, IEEE T MED IMAGING, V28, P1000, DOI 10.1109/TMI.2008.2011480; Klein S, 2008, MED PHYS, V35, P1407, DOI 10.1118/1.2842076; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Kybic J, 2003, IEEE T IMAGE PROCESS, V12, P1427, DOI 10.1109/TIP.2003.813139; Langerak TR, 2010, IEEE T MED IMAGING, V29, P2000, DOI 10.1109/TMI.2010.2057442; Ou YM, 2009, PROC CVPR IEEE, P188; Parisot S, 2012, LECT NOTES COMPUT SC, V7511, P651, DOI 10.1007/978-3-642-33418-4_80; Rohlfing T, 2003, LECT NOTES COMPUT SC, V2732, P210; Rohlfing T, 2005, TOP BIOMED ENGN, P435; Rohlfing T, 2012, IEEE T MED IMAGING, V31, P153, DOI 10.1109/TMI.2011.2163944; Rousseau F, 2011, IEEE T MED IMAGING, V30, P1852, DOI 10.1109/TMI.2011.2156806; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Sabuncu MR, 2010, IEEE T MED IMAGING, V29, P1714, DOI 10.1109/TMI.2010.2050897; Sdika M, 2010, MED IMAGE ANAL, V14, P219, DOI 10.1016/j.media.2009.12.004; Seghier ML, 2008, NEUROIMAGE, V41, P1253, DOI 10.1016/j.neuroimage.2008.03.028; Tang XY, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0065591; van Rikxoort EM, 2010, MED IMAGE ANAL, V14, P39, DOI 10.1016/j.media.2009.10.001; Wang HZ, 2013, IEEE T PATTERN ANAL, V35, P611, DOI 10.1109/TPAMI.2012.143; Warfield SK, 2004, IEEE T MED IMAGING, V23, P903, DOI 10.1109/TMI.2004.828354; Warfield SK, 2002, LECT NOTES COMPUT SC, V2488, P298; Wyatt PP, 2003, MED IMAGE ANAL, V7, P539, DOI 10.1016/S1361-8415(03)00067-7; Xue Z, 2010, COMPUT MED IMAG GRAP, V34, P55, DOI 10.1016/j.compmedimag.2009.05.007	40	13	14	0	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	1					169	181		10.1007/s11263-016-0925-2	http://dx.doi.org/10.1007/s11263-016-0925-2			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EI3HN		Green Submitted			2022-12-18	WOS:000392380900008
J	Hadfield, S; Lebeda, K; Bowden, R				Hadfield, Simon; Lebeda, Karel; Bowden, Richard			Hollywood 3D: What are the Best 3D Features for Action Recognition?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; In the wild; 3D; Structure; Depth; 3D motion; Hollywood 3D; Benchmark		Action recognition "in the wild" is extremely challenging, particularly when complex 3D actions are projected down to the image plane, losing a great deal of information. The recent growth of 3D data in broadcast content and commercial depth sensors, makes it possible to overcome this. However, there is little work examining the best way to exploit this new modality. In this paper we introduce the Hollywood 3D benchmark, which is the first dataset containing "in the wild" action footage including 3D data. This dataset consists of 650 stereo video clips across 14 action classes, taken from Hollywood movies. We provide stereo calibrations and depth reconstructions for each clip. We also provide an action recognition pipeline, and propose a number of specialised depth-aware techniques including five interest point detectors and three feature descriptors. Extensive tests allow evaluation of different appearance and depth encoding schemes. Our novel techniques exploiting this depth allow us to reach performance levels more than triple those of the best baseline algorithm using only appearance information. The benchmark data, code and calibrations are all made available to the community.	[Hadfield, Simon; Lebeda, Karel; Bowden, Richard] Univ Surrey, CVSSP, Guildford GU27XH, Surrey, England	University of Surrey	Hadfield, S (corresponding author), Univ Surrey, CVSSP, Guildford GU27XH, Surrey, England.	s.hadfield@surrey.ac.uk; k.lebeda@surrey.ac.uk; r.bowden@surrey.ac.uk	Bowden, Richard/AAF-8283-2019	Bowden, Richard/0000-0003-3285-8020; Hadfield, Simon/0000-0001-8637-5054	EPSRC Project "Learning to Recognise Dynamic Visual Content from Broadcast Footage" [EP/I011811/1]; Swiss National Science Foundation SMILE; Engineering and Physical Sciences Research Council [EP/I011811/1] Funding Source: researchfish; EPSRC [EP/I011811/1] Funding Source: UKRI	EPSRC Project "Learning to Recognise Dynamic Visual Content from Broadcast Footage"(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Swiss National Science Foundation SMILE; Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by the EPSRC Project "Learning to Recognise Dynamic Visual Content from Broadcast Footage" (EP/I011811/1) and the Swiss National Science Foundation SMILE "Scalable Multimodal sign language technology for sign language learning and assessment".	Beaudet P., 1978, JOINT C PATT REC; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Blank M., 2005, ICCV; Brand M, 1997, CVPR; Cheng Z., 2012, ECCV WORKSH; Dalal N., 2006, P ECCV GRAZ AUSTR; Desai C., 2012, 12 EUR C COMP VIS EC; Dollar P, 2005, VIS SURV PERF EV TRA; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gilbert A., 2014, ACCV; Gilbert A, 2011, IEEE T PATTERN ANAL, V33, P883, DOI 10.1109/TPAMI.2010.144; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Hadfield S., 2014, ECCV; Hadfield S., 2011, ICCV; Hadfield S., 2014, BMVC; Hadfield S., 2013, CVPR; Hadfield S., 2013, HOLLYWOOD 3D DATASET, DOI 10.15126/surreydata.00808228; Hadfield S, 2014, IEEE T PATTERN ANAL, V36, P564, DOI 10.1109/TPAMI.2013.162; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hoai Minh, 2014, BMVC; Huguet F., 2007, ICCV; Iosifidis A., 2014, ARTIFICIAL INTELLIGE; Iosifidis A, 2014, EUR SIGNAL PR CONF, P1317; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Konda K, 2013, UNSUPERVISED LEARNIN; Kukelova Zuzana, 2008, BMVC; Laptev I., 2007, ICCV; Laptev I., 2003, ICCV; Laptev I., 2008, CVPR; Lebeda K., 2012, BMVC; Li W., 2010, CVPR WORKSH; Liu Z., 2013, CVPR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mademlis I., 2014, S COMP INT; Marszalek M., 2009, CVPR; Messing R., 2009, ICCV; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Oshin O., 2011, IEEE INT C AUT FAC G; Richardt Christian, 2010, ECCV; Saff EB, 1997, MATH INTELL, V19, P5, DOI 10.1007/BF03024331; Sapienza M., 2012, BMVC; Scharstein D., 2003, IEEE COMP SOC C CVPR, V1; Schuldt C, 2004, ICPR; Scovanner P., 2007, INT C MULT; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Torr P., 1998, ICCV; Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017; Vieira Antonio W., 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P252, DOI 10.1007/978-3-642-33275-3_31; Wang H., 2011, CVPR; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Willems G., 2008, ECCV; Yang X., 2012, ACM INT C MULT; Yao Bangpeng, 2012, ECCV	55	13	13	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	1					95	110		10.1007/s11263-016-0917-2	http://dx.doi.org/10.1007/s11263-016-0917-2			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EI3HN	32355409	hybrid, Green Published			2022-12-18	WOS:000392380900004
J	Law, MT; Thome, N; Cord, M				Law, Marc T.; Thome, Nicolas; Cord, Matthieu			Learning a Distance Metric from Relative Comparisons between Quadruplets of Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Metric learning; Relative attributes; Web mining; Change detection	REPRESENTATION; PROXIMITIES	This paper is concerned with the problem of learning a distance metric by considering meaningful and discriminative distance constraints in some contexts where rich information between data is provided. Classic metric learning approaches focus on constraints that involve pairs or triplets of images. We propose a general Mahalanobis-like distance metric learning framework that exploits distance constraints over up to four different images. We show how the integration of such constraints can lead to unsupervised or semi-supervised learning tasks in some applications. We also show the benefit on recognition performance of this type of constraints, in rich contexts such as relative attributes, class taxonomies and temporal webpage analysis.	[Law, Marc T.; Thome, Nicolas; Cord, Matthieu] UPMC Univ Paris 06, Sorbonne Univ, UMR 7606, LIP6, F-75005 Paris, France	UDICE-French Research Universities; Sorbonne Universite	Law, MT (corresponding author), UPMC Univ Paris 06, Sorbonne Univ, UMR 7606, LIP6, F-75005 Paris, France.	Marc.Law@lip6.fr; Nicolas.Thome@lip6.fr; Matthieu.Cord@lip6.fr						Adar E., 2009, ACM CHI C HUM FACT C; Adar E., 2009, ACM WSDM C SER WEB S; Agarwal Sameer, 2007, J MACHINE LEARNING R, P11; Avila S, 2013, COMPUT VIS IMAGE UND, V117, P453, DOI 10.1016/j.cviu.2012.09.007; Ben Saad M., 2011, JOINT C DIG LIB JCDL; Borg I., 2005, SPRINGER SERIES STAT, DOI 10.1007/0-387-28981-X; Boyd S, 2004, CONVEX OPTIMIZATION; Boyd S., 2008, NOTES STANFORD U; Cai D., 2003, MSRTR2003792003; Chapelle O, 2010, INFORM RETRIEVAL, V13, P201, DOI 10.1007/s10791-009-9109-9; Chapelle O, 2007, NEURAL COMPUT, V19, P1155, DOI 10.1162/neco.2007.19.5.1155; Chechik G, 2010, J MACH LEARN RES, V11, P1109; Cord M, 2008, COGN TECHNOL, P1, DOI 10.1007/978-3-540-75171-7; Davis J. V., 2007, INT C MACH LEARN ICM; DOUZE M, 2009, ACM INT C IM VID RET; Finley T., 2005, INT C MACHINE LEARNI, P217, DOI DOI 10.1145/1102351.1102379; Finley T., 2008, SUPERVISED K MEANS C; FROME A, 2006, ADV NEURAL INFORM PR; Frome A, 2007, IEEE I CONF COMP VIS, P94; Goh H., 2012, EUR C COMP VIS ECCV; Guillaumin Matthieu, 2009, IEEE INT C COMP VIS; Hocking TD, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-164; Hwang S. J., 2011, ADV NEURAL INFORM PR; Hwang S. J., 2013, INT C MACH LEARN ICM; Jain P., 2008, IEEE C COMP VIS PATT; Joachims T, 2006, PROC 22 ACM SIGKDD I, P217, DOI DOI 10.1145/1150402.1150429; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Keerthi SS, 2005, J MACH LEARN RES, V6, P341; Kendall M.G., 1990, RANK CORRELATION MET, V5th; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Kumar M., 2007, IEEE INT C COMP VIS; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; LAJUGIE R, 2014, ICML, P297; Lampert C., 2009, IEEE C COMP VIS PATT; Law M. T., 2012, 10 WORKSH CONT BAS M; Law M. T., 2012, ACM S DOC ENG DOCENG; Law MT, 2013, IEEE I CONF COMP VIS, P249, DOI 10.1109/ICCV.2013.38; Luo P., 2009, ACM S DOC ENG DOCENG; McFee B., 2010, INT C MACH LEARN ICM; McFee B., 2009, P 26 ANN INT C MACHI, P721; Mensink T., 2012, EUR C COMP VIS ECCV; Mignon A., 2012, IEEE C COMP VIS PATT; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Parikh D., 2012, EUR C COMP VIS ECCV; Parikh D., 2011, IEEE INT C COMP VIS; Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56; Shaw B, 2011, ADV NEURAL INFORM PR, P1899; SHEPARD RN, 1962, PSYCHOMETRIKA, V27, P125, DOI 10.1007/BF02289630; SHEPARD RN, 1962, PSYCHOMETRIKA, V27, P219, DOI 10.1007/BF02289621; Sivic J., 2003, IEEE INT C COMP VIS; Socher R., 2009, CVPR09; Song R., 2004, WORLD WID WEB C WWW; Spengler A., 2010, ACM S DOC ENG DOCENG; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Theriault C, 2013, IEEE T IMAGE PROCESS, V22, P764, DOI 10.1109/TIP.2012.2222900; Torresani L., 2007, NEURIPS; Verma N., 2012, IEEE C COMP VIS PATT; Weinberger K., 2008, ADV NEURAL INFORM PR, P1737; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Xing E., 2002, P ADV NEUR INF PROC, V15, P1; Yang J., 2009, IEEE C COMP VIS PATT	64	13	13	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2017	121	1					65	94		10.1007/s11263-016-0923-4	http://dx.doi.org/10.1007/s11263-016-0923-4			30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	EI3HN		Green Submitted			2022-12-18	WOS:000392380900003
J	Liu, CW; Wu, XX; Jia, YD				Liu, Cuiwei; Wu, Xinxiao; Jia, Yunde			A Hierarchical Video Description for Complex Activity Understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Activity understanding; Hierarchical video description; Atomic action; Latent structural model		This paper addresses the challenging problem of complex human activity understanding from long videos. Towards this goal, we propose a hierarchical description of an activity video, referring to the "which" of activities, "what" of atomic actions, and "when" of atomic actions happening in the video. In our work, each complex activity is characterized as a composition of simple motion units (called atomic actions), and different atomic actions are explained by different video segments. We develop a latent discriminative structural model to detect the complex activity and atomic actions, while learning the temporal structure of atomic actions simultaneously. A segment-annotation mapping matrix is introduced for relating video segments to their associational atomic actions, allowing different video segments to explain different atomic actions. The segment-annotation mapping matrix is treated as latent information in the model, since its ground-truth is not available during both training and testing. Moreover, we present a semi-supervised learning method to automatically predict the atomic action labels of unlabeled training videos when the labeled training data is limited, which could greatly alleviate the laborious and time-consuming annotations of atomic actions for training data. Experiments on three activity datasets demonstrate that our method is able to achieve promising activity recognition results and obtain rich and hierarchical descriptions of activity videos.	[Liu, Cuiwei; Wu, Xinxiao; Jia, Yunde] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China	Beijing Institute of Technology	Wu, XX (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.	liucuiwei@bit.edu.cn; wuxinxiao@bit.edu.cn; jiayunde@bit.edu.cn			Natural Science Foundation of China (NSFC) [61203274, 61375044, 61472038]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported in part by the Natural Science Foundation of China (NSFC) under Grant Nos. 61203274, 61375044 and 61472038.	Bhattacharya S., 2014, IEEE INT C COMP VIS; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Do T. M. T., 2009, IEEE INT C MACH LEAR; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726; Gaidon A, 2011, PROC CVPR IEEE; Gaidon A, 2014, INT J COMPUT VISION, V107, P219, DOI 10.1007/s11263-013-0677-1; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Hu N., 2014, IEEE INT C ROB AUT I; Izadinia H., 2012, EUR C COMP VIS ECCV; Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31; Kliper O., 2012, EUR C COMP VIS ECCV; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laxton B., 2007, IEEE C COMP VIS PATT; Le QV, 2011, PROC CVPR IEEE; Li W., 2012, NEUR INF PROC SYST C; Li WQ, 2008, IEEE T CIRC SYST VID, V18, P1499, DOI 10.1109/TCSVT.2008.2005597; Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461; Lillo I, 2014, PROC CVPR IEEE, P812, DOI 10.1109/CVPR.2014.109; Liu J., 2011, IEEE INT C COMP VIS; Hoai M, 2011, PROC CVPR IEEE; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Pirsiavash H., 2014, IEEE INT C COMP VIS; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Ryu J, 2012, IEEE ICC; Sadanand S., 2012, IEEE INT C COMP VIS; Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219; Sun C., 2013, IEEE INT C COMP VIS; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang L., 2013, IEEE INT C COMP VIS; Wang LM, 2014, IEEE T IMAGE PROCESS, V23, P810, DOI 10.1109/TIP.2013.2295753; Wang Y., 2010, NEUR INF PROC SYST C; Weinland D, 2007, IEEE I CONF COMP VIS, P170; Wu X., 2011, IEEE INT C COMP VIS; Yilmaz A, 2005, IEEE I CONF COMP VIS, P150; Yu C. N. J., 2009, IEEE INT C MACH LEAR; Yu G., 2012, EUR C COMP VIS ECCV; Zhou Q., 2012, EUR C COMP VIS ECCV; Zhou Q., 2013, IEEE INT C COMP VIS	39	13	13	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2016	118	2			SI		240	255		10.1007/s11263-016-0897-2	http://dx.doi.org/10.1007/s11263-016-0897-2			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DO0OE					2022-12-18	WOS:000377477400008
J	Zhu, XT; Loy, CC; Gong, SG				Zhu, Xiatian; Loy, Chen Change; Gong, Shaogang			Learning from Multiple Sources for Video Summarisation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-source data; Heterogeneous data; Visual surveillance; Event recognition; Video summarisation	SCALE	Many visual surveillance tasks, e.g. video summarisation, is conventionally accomplished through analysing imagery-based features. Relying solely on visual cues for public surveillance video understanding is unreliable, since visual observations obtained from public space CCTV video data are often not sufficiently trustworthy and events of interest can be subtle. We believe that non-visual data sources such as weather reports and traffic sensory signals can be exploited to complement visual data for video content analysis and summarisation. In this paper, we present a novel unsupervised framework to learn jointly from both visual and independently-drawn non-visual data sources for discovering meaningful latent structure of surveillance video data. In particular, we investigate ways to cope with discrepant dimension and representation whilst associating these heterogeneous data sources, and derive effective mechanism to tolerate with missing and incomplete data from different sources. We show that the proposed multi-source learning framework not only achieves better video content clustering than state-of-the-art methods, but also is capable of accurately inferring missing non-visual semantics from previously-unseen videos. In addition, a comprehensive user study is conducted to validate the quality of video summarisation generated using the proposed multi-source model.	[Zhu, Xiatian; Gong, Shaogang] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London, England; [Loy, Chen Change] Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China	University of London; Queen Mary University London; Chinese University of Hong Kong	Zhu, XT (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, London, England.	xiatian.zhu@qmul.ac.uk; ccloy@ie.cuhk.edu.hk; s.gong@qmul.ac.uk	Zhu, Xiatian/Y-1601-2019	Zhu, Xiatian/0000-0002-9284-2955; Loy, Chen Change/0000-0001-5345-1591	Engineering and Physical Sciences Research Council [EP/E028594/1] Funding Source: researchfish; EPSRC [EP/E028594/1] Funding Source: UKRI	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))		Boccaletti S, 2006, PHYS REP, V424, P175, DOI 10.1016/j.physrep.2005.10.009; Bosch A, 2007, IEEE I CONF COMP VIS, P1863; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2003, SIAM WORKSH UC BERK; Breiman L., 2017, CLASSIFICATION REGRE; Cai X, 2011, PROC CVPR IEEE; Caruana R., 2008, INT C MACH LEARN; Chan AB, 2008, IEEE T PATTERN ANAL, V30, P909, DOI 10.1109/TPAMI.2007.70738; Chu W. S., 2015, IEEE C COMP VIS PATT, V30, P3584; Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951; Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1561/0600000035, 10.1501/0000000035]; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Fanti C, 2004, ADV NEUR IN, V16, P1603; Feng S., 2012, IEEE C COMP VIS PATT; Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128; Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70; Gong S., 2011, VISUAL ANAL HUMANS, P455, DOI DOI 10.1007/978-0-85729-997-0_23; Gong YH, 2003, EURASIP J APPL SIG P, V2003, P160, DOI 10.1155/S1110865703211082; Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928; Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33; Heer J., 2001, 1 SIAM ICDM WORKSH W, P51; Hospedales TM, 2011, IEEE T PATTERN ANAL, V33, P2451, DOI 10.1109/TPAMI.2011.81; Huang H. C., 2012, IEEE C COMP VIS PATT; Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011; Kang H.-W., 2006, COMPUTER VISION PATT, P1331, DOI DOI 10.1109/CVPR.2006.284; Karydis I., 2009, P INT SOC MUS INFORM, P159; Khalidov V, 2011, NEURAL COMPUT, V23, P517, DOI 10.1162/NECO_a_00074; Khosla A, 2013, PROC CVPR IEEE, P2698, DOI 10.1109/CVPR.2013.348; Kim C, 2002, IEEE T CIRC SYST VID, V12, P1128, DOI 10.1109/TCSVT.2002.806813; Kim G, 2014, PROC CVPR IEEE, P4225, DOI 10.1109/CVPR.2014.538; Kratz L, 2012, LECT NOTES COMPUT SC, V7575, P558, DOI 10.1007/978-3-642-33765-9_40; Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820; Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111; Liu B., 2000, C INF KNOWL MAN; Loog M, 2004, IEEE T PATTERN ANAL, V26, P732, DOI 10.1109/TPAMI.2004.13; Loy CC, 2012, IEEE T PATTERN ANAL, V34, P1799, DOI 10.1109/TPAMI.2011.246; Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350; Mairal J, 2010, J MACH LEARN RES, V11, P19; Martin JK, 1997, MACH LEARN, V28, P257, DOI 10.1023/A:1007367629006; Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002; Moosmann F, 2008, IEEE T PATTERN ANAL, V30, P1632, DOI 10.1109/TPAMI.2007.70822; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Perbet F., 2009, BRIT MACH VIS C; Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35; Pritch Y, 2008, IEEE T PATTERN ANAL, V30, P1971, DOI 10.1109/TPAMI.2008.29; Russell S. J, 2002, ADV NEURAL INFORM PR, P12, DOI DOI 10.5555/2968618.2968683; Schulter S., 2013, IEEE C COMP VIS PATT; Schulter S, 2013, IEEE I CONF COMP VIS, P417, DOI 10.1109/ICCV.2013.59; Shi T, 2006, J COMPUT GRAPH STAT, V15, P118, DOI 10.1198/106186006X94072; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735; Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51; Taskiran CM, 2006, IEEE T MULTIMEDIA, V8, P775, DOI 10.1109/TMM.2006.876282; Toderici G., 2010, IEEE C COMP VIS PATT; Topchy A, 2005, IEEE T PATTERN ANAL, V27, P1866, DOI 10.1109/TPAMI.2005.237; Truong B. T., 2007, ACM T MULTIMEDIA COM; Wagstaff K., 2001, ICML, V1, P577, DOI DOI 10.1109/TPAMI.2002.1017616; Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041; Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87; Wang ZS, 2010, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2010.5540125; Wolf W, 1996, IEEE INT C AC SPEECH; Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882; Zhang D. Q., 2004, IEEE INT C MULT EXP; Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4; Zhao B, 2014, PROC CVPR IEEE, P2513, DOI 10.1109/CVPR.2014.322; Zhao Y, 2004, MACH LEARN, V55, P311, DOI 10.1023/B:MACH.0000027785.44527.d6; Zhu XT, 2014, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2014.188	68	13	15	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2016	117	3					247	268		10.1007/s11263-015-0864-3	http://dx.doi.org/10.1007/s11263-015-0864-3			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DL1TF		Green Submitted			2022-12-18	WOS:000375414600003
J	Wang, ZL; Feng, JS; Yan, SC				Wang, Zilei; Feng, Jiashi; Yan, Shuicheng			Collaborative Linear Coding for Robust Image Classification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image classification; Collaborative linear coding (CLC); Robust image representation; Noisy pixel	SCENE; CODEBOOKS	How to generate robust image representations, when there is contamination from noisy pixels within the images, is critical for boosting the performance of image classification methods. However, such an important problem is not fully explored yet. In this paper, we propose a novel image representation learning method, i.e., collaborative linear coding (CLC), to alleviate the negative influence of noisy features in classifying images. Specifically, CLC exploits the correlation among local features in the coding procedure, in order to suppress the interference of noisy features via weakening their responses on coding basis. CLC implicitly divides the extracted local features into different feature subsets, and such feature allocation is indicated by the introduced latent variables. Within each subset, the features are ensured to be highly correlated, and the produced codes for them are encouraged to activate on the identical basis. Through incorporating such regularization in the coding model, the responses of noisy local features are dominated by the responses of informative features due to their rarity compared with the informative features. Thus the final image representation is more robust and distinctive for following classification, compared with the coding methods without considering such high order correlation. Though CLC involves a set of complicated optimization problems, we investigate the special structure of the problems and then propose an efficient alternative optimization algorithm. We verified the effectiveness and robustness of the proposed CLC on multiple image classification benchmark datasets, including Scene 15, Indoor 67, Flower 102, Pet 37, and PASCAL VOC 2011. Compared with the well established baseline LLC, CLC consistently enhances the classification accuracy, especially for the images containing more noises.	[Wang, Zilei] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Anhui, Peoples R China; [Feng, Jiashi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore	Chinese Academy of Sciences; University of Science & Technology of China, CAS; National University of Singapore	Wang, ZL (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Anhui, Peoples R China.	zlwang@ustc.edu.cn	Yan, Shuicheng/HCI-1431-2022; Feng, Jiashi/AGX-6209-2022		National Natural Science Foundation of China [61203256, 61233003]; Natural Science Foundation of Anhui Province [1408085 MF112]; Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative; Fundamental Research Funds for the Central Universities [WK2100100018, WK2100100021]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Anhui Province(Natural Science Foundation of Anhui Province); Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative(National Research Foundation, Singapore); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work is supported partially by the National Natural Science Foundation of China under Grant 61203256 and 61233003, Natural Science Foundation of Anhui Province (1408085 MF112), the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office, and the Fundamental Research Funds for the Central Universities (WK2100100018 and WK2100100021).	Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598; Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716; BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Carreira J, 2012, INT J COMPUT VISION, V98, P243, DOI 10.1007/s11263-011-0507-2; Coates Adam, 2011, P 28 INT C MACH LEAR, P921; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Gao SH, 2013, IEEE T PATTERN ANAL, V35, P92, DOI 10.1109/TPAMI.2012.63; Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943; Gu CH, 2009, PROC CVPR IEEE, P1030, DOI 10.1109/CVPRW.2009.5206727; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; Khan FS, 2009, IEEE I CONF COMP VIS, P979, DOI 10.1109/ICCV.2009.5459362; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534; Long MS, 2013, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2013.59; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Moosmann F., 2006, P C ECCV06 WORKSH RE; Nilsback ME, 2010, IMAGE VISION COMPUT, V28, P1049, DOI 10.1016/j.imavis.2009.10.001; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Su H., 2010, ADV NEURAL PROCESSIN, V1, P1378; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52; van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183; WANG JJ, 2010, PROC CVPR IEEE, P3360, DOI DOI 10.1109/CVPR.2010.5540018; Wang ZX, 2010, LECT NOTES COMPUT SC, V6311, P706, DOI 10.1007/978-3-642-15549-9_51; Wang ZL, 2013, IEEE T IMAGE PROCESS, V22, P4341, DOI 10.1109/TIP.2013.2272514; Wang ZL, 2013, IEEE T IMAGE PROCESS, V22, P537, DOI 10.1109/TIP.2012.2218826; Wu JX, 2009, IEEE I CONF COMP VIS, P630, DOI 10.1109/ICCV.2009.5459178; Xu H, 2012, IEEE T PATTERN ANAL, V34, P187, DOI 10.1109/TPAMI.2011.177; Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757; Yao BP, 2011, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR.2011.5995368; Yu Kai, 2009, ADV NEURAL INFORM PR, P2223; Yu Kai, 2010, ICML, P1215; Yuan XT, 2010, PROC CVPR IEEE, P3493, DOI 10.1109/CVPR.2010.5539967; Zarzoso V, 2010, IEEE T NEURAL NETWOR, V21, P248, DOI 10.1109/TNN.2009.2035920; Zhang TZ, 2013, IEEE I CONF COMP VIS, P281, DOI 10.1109/ICCV.2013.42	45	13	15	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2015	114	2-3			SI		322	333		10.1007/s11263-014-0739-z	http://dx.doi.org/10.1007/s11263-014-0739-z			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CP7MJ					2022-12-18	WOS:000360071900013
J	Tighe, J; Niethammer, M; Lazebnik, S				Tighe, Joseph; Niethammer, Marc; Lazebnik, Svetlana			Scene Parsing with Object Instance Inference Using Regions and Per-exemplar Detectors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image parsing; Semantic segmentation; Scene understanding; Object segmentation	IMAGE; SEGMENTATION; STUFF	This paper describes a system for interpreting a scene by assigning a semantic label at every pixel and inferring the spatial extent of individual object instances together with their occlusion relationships. First we present a method for labeling each pixel aimed at achieving broad coverage across hundreds of object categories, many of them sparsely sampled. This method combines region-level features with per-exemplar sliding window detectors. Unlike traditional bounding box detectors, per-exemplar detectors perform well on classes with little training data and high intra-class variation, and they allow object masks to be transferred into the test image for pixel-level segmentation. Next, we use per-exemplar detections to generate a set of candidate object masks for a given test image. We then select a subset of objects that explain the image well and have valid overlap relationships and occlusion ordering. This is done by minimizing an integer quadratic program either using a greedy method or a standard solver. We alternate between using the object predictions to refine the pixel labels and using the pixel labels to improve the object predictions. The proposed system obtains promising results on two challenging subsets of the LabelMe dataset, the largest of which contains 45,676 images and 232 classes.	[Tighe, Joseph; Niethammer, Marc] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27514 USA; [Lazebnik, Svetlana] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA	University of North Carolina; University of North Carolina Chapel Hill; University of Illinois System; University of Illinois Urbana-Champaign	Tighe, J (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27514 USA.	jtighe@cs.unc.edu; mn@cs.unc.edu; slazebni@illinois.edu			NSF [IIS 1228082, CIF 1302438]; DARPA Computer Science Study Group [D12AP00305]; Microsoft Research Faculty Fellowship; Sloan Foundation; Xerox	NSF(National Science Foundation (NSF)); DARPA Computer Science Study Group; Microsoft Research Faculty Fellowship(Microsoft); Sloan Foundation(Alfred P. Sloan Foundation); Xerox	This research was supported in part by NSF grants IIS 1228082 and CIF 1302438, DARPA Computer Science Study Group (D12AP00305), Microsoft Research Faculty Fellowship, Sloan Foundation, and Xerox. We thank Arun Mallya for helping to adapt the LDA detector code of Hariharan et al. (2012).	Adelson EH, 2001, PROC SPIE, V4299, P1, DOI 10.1117/12.429489; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Boykov Y., 2003, INT C COMP VIS ICCV; Brostow G. J., 2008, EUR C COMP VIS ECCV; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dean T., 2013, IEEE COMP SOC C COMP; Eigen D., 2012, IEEE C COMP VIS PATT; Everingham M., 2008, PASCAL VISUAL OBJECT; Farabet C., 2012, INT C MACH LEARN ICM; Floros G, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.79; Gould S., 2009, INT C COMP VIS ICCV; Guo R., 2012, EUR C COMP VIS ECCV; Hariharan B., 2012, EUR C COMP VIS ECCV; Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4; IBM, 2013, CPL OPT; Isola P, 2013, IEEE I CONF COMP VIS, P3048, DOI 10.1109/ICCV.2013.457; Kim B., 2012, ECCV WORKSH HIGH ORD; Kim J., 2012, EUR C COMP VIS ECCV; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Krahenbuhl P., 2011, ANN C NEUR INF PROC; Ladicky L., 2010, 11 EUR C COMP VIS EC; Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131; Malisiewicz T., 2011, 13 INT C COMP VIS IC; Myeong H. J., 2012, C COMP VIS PATT REC; Rahimi A., 2007, P 21 ANN C NEUR INF; riggs B. T, 2005, IN 2005 IEEE COMP VI; Rother C., 2004, SPECIAL INTEREST GRO; Russell B. C., 2009, IEEE COMP SOC C COMP; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Sturgess P., 2009, BRIT MACH VIS C BMVC; Tighe J., 2014, IEEE C COMP VIS PATT; Tighe J., 2013, IEEE C COMP VIS PATT; Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z; Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x; Xiao J., 2010, 23 IEEE C COMP VIS P; Yang Y, 2012, IEEE T PATTERN ANAL, V34, P1731, DOI 10.1109/TPAMI.2011.208; Yao J., 2012, C COMP VIS PATT REC; Zhang C., 2010, 11 EUR C COMP VIS EC	39	13	13	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2015	112	2			SI		150	171		10.1007/s11263-014-0778-5	http://dx.doi.org/10.1007/s11263-014-0778-5			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CE0TD					2022-12-18	WOS:000351518500003
J	Fernando, B; Fromont, E; Tuytelaars, T				Fernando, Basura; Fromont, Elisa; Tuytelaars, Tinne			Mining Mid-level Features for Image Classification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Frequent itemset mining; Image classification; Discriminative patterns; Mid-level features	PATTERNS	Mid-level or semi-local features learnt using class-level information are potentially more distinctive than the traditional low-level local features constructed in a purely bottom-up fashion. At the same time they preserve some of the robustness properties with respect to occlusions and image clutter. In this paper we propose a new and effective scheme for extracting mid-level features for image classification, based on relevant pattern mining. In particular, we mine relevant patterns of local compositions of densely sampled low-level features. We refer to the new set of obtained patterns as Frequent Local Histograms or FLHs. During this process, we pay special attention to keeping all the local histogram information and to selecting the most relevant reduced set of FLH patterns for classification. The careful choice of the visual primitives and an extension to exploit both local and global spatial information allow us to build powerful bag-of-FLH-based image representations. We show that these bag-of-FLHs are more discriminative than traditional bag-of-words and yield state-of-the-art results on various image classification benchmarks, including Pascal VOC.	[Fernando, Basura; Tuytelaars, Tinne] Katholieke Univ Leuven, ESAT PSI, iMinds, Heverlee, Belgium; [Fromont, Elisa] Univ St Etienne, UMR CNRS 5516, Lab Hubert Curien, Univ Lyon, F-42000 St Etienne, France	IMEC; KU Leuven; Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite Jean Monnet	Fernando, B (corresponding author), Katholieke Univ Leuven, ESAT PSI, iMinds, Heverlee, Belgium.	basura.fernando@esat.kuleuven.be	Tuytelaars, Tinne/B-4319-2015	Tuytelaars, Tinne/0000-0003-3307-9723; Fernando, Basura/0000-0002-6920-9916; Fromont, Elisa/0000-0003-0133-3491	iMinds Impact project Beeldcanon; FP7 ERC [240530]; PASCAL 2 Network of Excellence	iMinds Impact project Beeldcanon; FP7 ERC; PASCAL 2 Network of Excellence	The authors acknowledge the support of the iMinds Impact project Beeldcanon, the FP7 ERC Starting Grant 240530 COGNIMUND and PASCAL 2 Network of Excellence.	Agarwal A, 2008, INT J COMPUT VISION, V78, P15, DOI 10.1007/s11263-007-0072-x; Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072; Agrawal R., 1994, P 20 INT C VER LARG, P487; Boiman O., 2008, CVPR; Bourdev L., 2009, INT C COMP VIS ICCV; Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Cheng H, 2007, PROC INT CONF DATA, P691; Chum O., 2009, CVPR, DOI [10.1109/CVPR.2009.5206531, DOI 10.1109/CVPR.2009.5206531]; Cinbis R. G., 2012, CVPR; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Endres I., 2013, IEEE C COMP VIS PATT; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Fernando B., 2012, CVPR; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Fernando B, 2012, LECT NOTES COMPUT SC, V7572, P214, DOI 10.1007/978-3-642-33718-5_16; Gilbert A, 2009, IEEE I CONF COMP VIS, P925, DOI 10.1109/ICCV.2009.5459335; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124; Khan FS, 2009, IEEE I CONF COMP VIS, P979, DOI 10.1109/ICCV.2009.5459362; Kim S., 2010, 10 INT WORKSH MULT D, DOI [10.1145/1814245.1814252, DOI 10.1145/1814245.1814252]; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lee AJT, 2009, J SYST SOFTWARE, V82, P603, DOI 10.1016/j.jss.2008.08.028; Lee Y. J., 2013, INT C COMP VIS; Ling H., 2007, ICCV; LIU D, 2008, CVPR; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Nowozin S., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383171; Opelt A, 2004, LECT NOTES COMPUT SC, V3022, P71; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Quack T., 2007, ICCV; Quack T, 2006, LECT NOTES COMPUT SC, V4071, P360; Rematas K, 2012, ACCV, V7724, P176; SAVARESE S, 2006, CVPR; SHARMA G, 2013, PROC CVPR IEEE, P652, DOI DOI 10.1109/CVPR.2013.90; Simonyan K., 2013, ADV NEURAL INFORM PR; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Sivic J., 2004, CVPR, DOI [10.1109/CVPR.2004.1315071, DOI 10.1109/CVPR.2004.1315071]; Tuytelaars T, 2011, IEEE I CONF COMP VIS, P1824, DOI 10.1109/ICCV.2011.6126449; Uno T, 2003, P IEEE ICDM WORKSH F, V90; van de Weijer J, 2007, IEEE IMAGE PROC, P1621; Xie NH, 2010, PROC CVPR IEEE, P2313, DOI 10.1109/CVPR.2010.5539917; Yan X., 2005, ACM SIGKDD; Yang Y., 2011, ICCV; Yao Bangpeng, 2010, CVPR, DOI DOI 10.1109/CVPR.2010.5540234; Yimeng Zhang T. C., 2009, CVPR; Yuan J., 2008, CVPR, DOI [10.1109/CVPR.2008.4587347, DOI 10.1109/CVPR.2008.4587347]; Yuan JS, 2011, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2011.5995476; Yuan J, 2007, PROC CVPR IEEE, P1930, DOI 10.1109/CVPR.2007.383222; Yun U., 2005, SDM 05	55	13	14	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2014	108	3					186	203		10.1007/s11263-014-0700-1	http://dx.doi.org/10.1007/s11263-014-0700-1			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AH8NO		Green Submitted			2022-12-18	WOS:000336394900002
J	Galleguillos, C; McFee, B; Lanckriet, GRG				Galleguillos, Carolina; McFee, Brian; Lanckriet, Gert R. G.			Iterative Category Discovery via Multiple Kernel Metric Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Category discovery; Metric learning; Multiple kernel learning; Iterative discovery		The goal of an object category discovery system is to annotate a pool of unlabeled image data, where the set of labels is initially unknown to the system, and must therefore be discovered over time by querying a human annotator. The annotated data is then used to train object detectors in a standard supervised learning setting, possibly in conjunction with category discovery itself. Category discovery systems can be evaluated in terms of both accuracy of the resulting object detectors, and the efficiency with which they discover categories and annotate the training data. To improve the accuracy and efficiency of category discovery, we propose an iterative framework which alternates between optimizing nearest neighbor classification for known categories with multiple kernel metric learning, and detecting clusters of unlabeled image regions likely to belong to a novel, unknown categories. Experimental results on the MSRC and PASCAL VOC2007 data sets show that the proposed method improves clustering for category discovery, and efficiently annotates image regions belonging to the discovered classes.	[Galleguillos, Carolina] SET Media Inc, San Francisco, CA 94108 USA; [McFee, Brian] Columbia Univ, New York, NY USA; [Lanckriet, Gert R. G.] Univ Calif San Diego, San Diego, CA 92103 USA	Columbia University; University of California System; University of California San Diego	Galleguillos, C (corresponding author), SET Media Inc, San Francisco, CA 94108 USA.	cgalleguillos@gmail.com; brm2132@columbia.edu; gert@ece.ucsd.edu		McFee, Brian/0000-0001-6261-9747				Barthe E, 2009, POLICE PRACT RES, V10, P255, DOI 10.1080/15614260802381067; Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32; Collins B., 2008, COMPUTER VISION ECCV; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; DEFAYS D, 1977, COMPUT J, V20, P364, DOI 10.1093/comjnl/20.4.364; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Faktor A, 2012, LECT NOTES COMPUT SC, V7578, P474, DOI 10.1007/978-3-642-33786-4_35; Forsyth D. A., 1995, COMPUT J, V1144, P335; Frome A, 2007, IEEE I CONF COMP VIS, P94; Galleguillos C., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2665, DOI 10.1109/CVPR.2011.5995527; Galleguillos C, 2010, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2010.5540223; Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169; Globerson A., 2007, INT C ART INT STAT A; Grauman K., 2006, COMPUTER VISION PATT; Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4; Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kang H., 2012, EUR C COMP VIS ECCV, P794; Lee Y., 2010, COMPUTER VISION PATT; Lee YJ, 2011, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2011.5995523; McFee B., 2010, P 27 INT C MACHINE L, P775; MEILA M, 2001, ADV NEURAL INFORM PR; Rabinovich A., 2006, COMPUTER VISION PATT; Russell B., 2006, COMPUTER VISION PATT; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; SIVIC J, 2005, INT C COMP VIS ICCV; Sivic J, 2008, PROC CVPR IEEE, P2182; Tian Y, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383282; Todorovic S., 2006, COMPUTER VISION PATT, P927; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Tuytelaars T, 2010, INT J COMPUT VISION, V88, P284, DOI 10.1007/s11263-009-0271-8; Varma M, 2007, IEEE I CONF COMP VIS, P369; Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183; Vijayanarasimhan S., 2009, COMPUTER VISION PATT; Wang G., 2010, COMPUTER VISION PATT; Weinberger Kilian Q, 2006, ADV NEURAL INFORM PR, P1473, DOI DOI 10.1007/978-3-319-13168-9_; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; Winn J, 2005, IEEE I CONF COMP VIS, P1800; ZHAO Y, 2001, MACHINE LEARNING; Zhu JY, 2012, PROC CVPR IEEE, P3218, DOI 10.1109/CVPR.2012.6248057	43	13	13	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2014	108	1-2			SI		115	132		10.1007/s11263-013-0679-z	http://dx.doi.org/10.1007/s11263-013-0679-z			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AG7BT					2022-12-18	WOS:000335573700008
J	Lellmann, J; Lellmann, B; Widmann, F; Schnorr, C				Lellmann, Jan; Lellmann, Bjoern; Widmann, Florian; Schnoerr, Christoph			Discrete and Continuous Models for Partitioning Problems	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-class labeling; Segmentation; Partitioning problem; Graph cut; Convex relaxation; Variational methods	OPTIMIZATION; APPROXIMATION; MINIMIZATION; RELAXATION; ALGORITHMS; METRICS; FLOW; CUTS	Recently, variational relaxation techniques for approximating solutions of partitioning problems on continuous image domains have received considerable attention, since they introduce significantly less artifacts than established graph cut-based techniques. This work is concerned with the sources of such artifacts. We discuss the importance of differentiating between artifacts caused by discretization and those caused by relaxation and provide supporting numerical examples. Moreover, we consider in depth the consequences of a recent theoretical result concerning the optimality of solutions obtained using a particular relaxation method. Since the employed regularizer is quite tight, the considered relaxation generally involves a large computational cost. We propose a method to significantly reduce these costs in a fully automatic way for a large class of metrics including tree metrics, thus generalizing a method recently proposed by Strekalovskiy and Cremers (IEEE conference on computer vision and pattern recognition, pp. 1905-1911, 2011).	[Lellmann, Jan] Univ Cambridge, Dept Appl Math & Theoret Phys, Cambridge CB3 9EW, England; [Lellmann, Bjoern; Widmann, Florian] Univ London Imperial Coll Sci Technol & Med, Dept Comp, London, England; [Schnoerr, Christoph] Heidelberg Univ, Image & Pattern Anal Grp, Heidelberg, Germany; [Schnoerr, Christoph] Heidelberg Univ, Dept Math & Comp Sci, HCI, Heidelberg, Germany	University of Cambridge; Imperial College London; Ruprecht Karls University Heidelberg; Ruprecht Karls University Heidelberg	Lellmann, J (corresponding author), Univ Cambridge, Dept Appl Math & Theoret Phys, Silver St, Cambridge CB3 9EW, England.	j.lellmann@damtp.cam.ac.uk; b.lellmann10@imperial.ac.uk; f.widmann@imperial.ac.uk; schnoerr@math.uni-heidelberg.de	Lellmann, Jan/AAL-4077-2021	Lellmann, Bjorn/0000-0002-5335-1838	Engineering and Physical Sciences Research Council (EPSRC) [EP/H016317/1]; King Abdullah University of Science and Technology (KAUST) [KUK-I1- 007-43]; EPSRC [EP/H016317/1, EP/J009539/1] Funding Source: UKRI; Engineering and Physical Sciences Research Council [EP/H016317/1, EP/J009539/1] Funding Source: researchfish	Engineering and Physical Sciences Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); King Abdullah University of Science and Technology (KAUST)(King Abdullah University of Science & Technology); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The second and third author were supported by Engineering and Physical Sciences Research Council (EPSRC)-Project EP/H016317/1. This publication is partly based on work supported by Award No. KUK-I1- 007-43, made by King Abdullah University of Science and Technology (KAUST).	Ambrosio L., 2000, OX MATH M, DOI 10.1017/S0024609301309281; Appleton B, 2006, IEEE T PATTERN ANAL, V28, P106, DOI 10.1109/TPAMI.2006.12; Bae E, 2011, INT J COMPUT VISION, V92, P112, DOI 10.1007/s11263-010-0406-y; Bartal Y., 1998, ACM S THEOR COMP; Bertsekas D.P., 1998, NETWORK OPTIMIZATION; Boykov Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P26; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Braides A., 2002, GAMMA CONVERGENCE BE; Calinescu G, 2000, J COMPUT SYST SCI, V60, P564, DOI 10.1006/jcss.1999.1687; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chambolle A, 1999, RAIRO-MATH MODEL NUM, V33, P261; Chambolle A, 2011, SIAM J IMAGING SCI, V4, P277, DOI 10.1137/090752754; Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286; Charikar M, 1998, ANN IEEE SYMP FOUND, P379, DOI 10.1109/SFCS.1998.743488; COMBETTES PL, 2010, FIXED POINT ALGORITH; Couprie C, 2011, SIAM J IMAGING SCI, V4, P905, DOI 10.1137/100799186; Couprie C, 2011, IEEE T PATTERN ANAL, V33, P1384, DOI 10.1109/TPAMI.2010.200; DAHLHAUS E, 1994, SIAM J COMPUT, V23, P864, DOI 10.1137/S0097539792225297; Dal Maso, 1993, INTRO GAMMA CONVERGE; De Giorgi E., 1975, ATTI ACCAD NAZ LIN, V58, P842; Dixit N., 2005, 0507 CERTIS ENPC; Esser E, 2010, SIAM J IMAGING SCI, V3, P1015, DOI 10.1137/09076934X; Fakcharoenphol J, 2004, J COMPUT SYST SCI, V69, P485, DOI 10.1016/j.jcss.2004.04.01; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gobbino M, 2001, P ROY SOC EDINB A, V131, P567, DOI 10.1017/S0308210500001001; Goldluecke B, 2010, LECT NOTES COMPUT SC, V6315, P225, DOI 10.1007/978-3-642-15555-0_17; GOLDSCHLAGER LM, 1982, THEOR COMPUT SCI, V21, P105, DOI 10.1016/0304-3975(82)90092-5; Grady L.J., 2010, DISCRETE CALCULUS AP; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kleinberg J., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P14, DOI 10.1109/SFFCS.1999.814572; Klodt M, 2008, LECT NOTES COMPUT SC, V5302, P332, DOI 10.1007/978-3-540-88682-2_26; Koller D., 2009, PROBABILISTIC GRAPHI; Kolmogorov V, 2005, IEEE I CONF COMP VIS, P564; Komodakis N, 2007, IEEE T PATTERN ANAL, V29, P1436, DOI 10.1109/TPAMI.2007.1061; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Lellmann J, 2011, SIAM J IMAGING SCI, V4, P1049, DOI 10.1137/100805844; Lellmann J., 2011, ENERGY MINIMIZATION; Lellmann J., 2011, THESIS U HEIDELBERG; Lellmann J, 2013, J MATH IMAGING VIS, V47, P239, DOI 10.1007/s10851-012-0390-7; Lellmann J, 2010, LECT NOTES COMPUT SC, V6312, P494, DOI 10.1007/978-3-642-15552-9_36; Lellmann J, 2009, LECT NOTES COMPUT SC, V5567, P150, DOI 10.1007/978-3-642-02256-2_13; Lempitsky V, 2007, IEEE I CONF COMP VIS, P620; Lempitsky V, 2010, IEEE T PATTERN ANAL, V32, P1392, DOI 10.1109/TPAMI.2009.143; Lie J, 2006, MATH COMPUT, V75, P1155, DOI 10.1090/S0025-5718-06-01835-7; Morel J.-M., 1995, VARIATIONAL METHODS; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Olsson C., 2009, INT C COMP VIS; Olsson C., 2009, THESIS LUND U; PARAGIOS N., 2006, HDB MATH MODELS COMP; Pock T, 2008, LECT NOTES COMPUT SC, V5304, P792, DOI 10.1007/978-3-540-88690-7_59; Pock T, 2009, PROC CVPR IEEE, P810, DOI 10.1109/CVPRW.2009.5206604; Rickett J., 1999, 100 STANDF EXPL PROJ; Scherzer O, 2009, APPL MATH SCI, V167, P3; Sinop AK, 2007, IEEE I CONF COMP VIS, P1016, DOI 10.1109/iccv.2007.4408927; STRANG G, 1983, MATH PROGRAM, V26, P123, DOI 10.1007/BF02592050; Strekalovskiy E., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1905, DOI 10.1109/CVPR.2011.5995573; Strekalovskiy E, 2011, IEEE I CONF COMP VIS, P2328, DOI 10.1109/ICCV.2011.6126514; Trobin W., 2008, EUR C COMP VIS, V4, P667; Zach C., 2008, VISION MODELING VISU	65	13	13	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2013	104	3			SI		241	269		10.1007/s11263-013-0621-4	http://dx.doi.org/10.1007/s11263-013-0621-4			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	190VT					2022-12-18	WOS:000322371100003
J	Logvinenko, AD				Logvinenko, Alexander D.			Object-Colour Manifold	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Colour; Colour space; Colour equivalence; Colour theory	CONSTANCY	Colorimetry can predict which lights will look alike. Such lights are called metameric. Two lights are metameric if they have the same tri-stimulus values. Using the tri-stimulus values as Cartesian coordinates one can represent light colours as points in a 3D space (referred to as the colorimetric space). All the light colours make a tri-dimensional manifold which can be represented as a circular cone in the colorimetric space. Furthermore, colorimetry also claims that reflecting objects illuminated by the same light will look alike as soon as they reflect metameric lights. All the object colours are then represented as a closed solid inscribed in the light colour cone provided the illumination is fixed. However, as argued in this article, the reflected light metamerism does not guarantee that the reflecting objects will look identical (referred to as colour equivalence), especially when there are multiple illuminants. Moreover, colour equivalence cannot be derived from metamerism. The colour of a reflecting object under various illuminations is shown to be specified by six numbers (referred to as its six-stimulus values) that can be established by experiment. Using the six-stimulus values one can represent the colours of all the reflecting objects illuminated by various illuminants as a cone (without a vertex) through a 5D ball.	Glasgow Caledonian Univ, Dept Vis Sci, Glasgow G4 0BA, Lanark, Scotland	Glasgow Caledonian University	Logvinenko, AD (corresponding author), Glasgow Caledonian Univ, Dept Vis Sci, City Campus, Glasgow G4 0BA, Lanark, Scotland.	a.logvinenko@gcu.ac.uk			EPSRC [EP/C010353/1]; Engineering and Physical Sciences Research Council [EP/C010353/1] Funding Source: researchfish	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by a research grant (EP/C010353/1) from EPSRC. I wish to thank Brian Funt for fruitful discussions and encouragement while working on the article. I am also very grateful to Michael H. Brill for valuable criticism of the early draft.	AREND L, 1994, LIGHTNESS BRIGHTNESS, P159; Barnard K, 2002, IEEE T IMAGE PROCESS, V11, P972, DOI 10.1109/TIP.2002.802531; Brainard D. H., 2010, OSA HDB OPTICS, VIII; Brainard D. H, 2009, SAGE ENCY PERCEPTION, P253; Brainard DH, 1997, J OPT SOC AM A, V14, P2091, DOI 10.1364/JOSAA.14.002091; Brainard DH, 2004, VISUAL NEUROSCIENCES, P948; CRAVEN BJ, 1992, VISION RES, V32, P1359, DOI 10.1016/0042-6989(92)90228-B; Ebner M., 2007, COMPUTER VISION; Fairchild M. D., 2005, COLOR APPEARANCE MOD, V2nd, P26; FORSYTH DA, 1990, INT J COMPUT VISION, V5, P5, DOI 10.1007/BF00056770; Foster DH, 2011, VISION RES, V51, P674, DOI 10.1016/j.visres.2010.09.006; Foster DH, 2001, P NATL ACAD SCI USA, V98, P8151, DOI 10.1073/pnas.141505198; FOSTER DH, 1994, P ROY SOC B-BIOL SCI, V257, P115, DOI 10.1098/rspb.1994.0103; Gelb A., 1929, HDB NORMAL PATHOLOGI, P594, DOI [10.1007/978-3-642-91031-9_19, DOI 10.1007/978-3-642-91031-9_19]; Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224; Gilchrist A., 2006, SEEING BLACK WHITE; GILCHRIST AL, 1977, SCIENCE, V195, P185, DOI 10.1126/science.831266; Godau Christoph, 2010, Eighteenth Color and Imaging Conference. Color Science and Engineering Systems, Technologies, and Applications, P334; Godau C, 2012, COLOR RES APPL, V37, P117, DOI 10.1002/col.20680; Hordley SD, 2006, COLOR RES APPL, V31, P303, DOI 10.1002/col.20226; Hurlbert A., PERCEPTUAL CONSTANCY, V1998, P283; JAMESON D, 1989, ANNU REV PSYCHOL, V40, P1, DOI 10.1146/annurev.ps.40.020189.000245; Kaiser P, 1996, HUMAN COLOR VISION; Katz D, 1999, WORLD COLOUR; Kingdom FAA, 2011, VISION RES, V51, P652, DOI 10.1016/j.visres.2010.09.012; Koenderink J., 2010, COLOR SCI; Koffka K., 1935, PRINCIPLES GESTALT P; KRANTZ DH, 1975, J MATH PSYCHOL, V12, P283, DOI 10.1016/0022-2496(75)90026-7; KRANTZ DH, 1968, J MATH PSYCHOL, V5, P1, DOI 10.1016/0022-2496(68)90056-4; Kuehni RG., 2008, COLOR ORDERED SURVEY; Kulikowski JJ, 1997, ACTA PSYCHOL, V97, P25, DOI 10.1016/S0001-6918(97)00022-X; LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108; LOGVINENKO A, 1994, PERCEPTION, V23, P1007, DOI 10.1068/p231007; Logvinenko A. D., 2012, FDN COLOUR SCI; Logvinenko A. D., 2012, P CG42012 IS T 6 EUR; Logvinenko A. D., 2009, P 11 C INT COL ASS A; Logvinenko AD, 2006, PERCEPT PSYCHOPHYS, V68, P76, DOI 10.3758/BF03193657; Logvinenko AD, 2005, SPATIAL VISION, V18, P337, DOI 10.1163/1568568054089357; Logvinenko AD, 2012, J VISION, V12, DOI 10.1167/12.4.17; Logvinenko AD, 2011, SEEING PERCEIVING, V24, P407, DOI 10.1163/187847511X588746; Logvinenko AD, 2012, COLOR RES APPL, V37, P109, DOI 10.1002/col.20661; Logvinenko AD, 2009, J VISION, V9, DOI 10.1167/9.11.5; MacLeod D. I. A., 2003, COLOUR PERCEPTION MI, P205; MacLeod DIA, 2003, TRENDS COGN SCI, V7, P97, DOI 10.1016/S1364-6613(03)00022-6; Maloney L., 2003, COLOUR PERCEPTION MI, P279, DOI 10.1093/acprof:oso/9780198505006.003.0009; Maloney Laurence T, 1999, COLOR VISION GENES P, P1, DOI [10.1167/11.5.1, DOI 10.1167/11.5.1]; MALONEY LT, 1986, J OPT SOC AM A, V3, P29, DOI 10.1364/JOSAA.3.000029; Mausfeld R., 2003, COLOUR PERCEPTION MI, P381, DOI 10.1093/acprof:oso/9780198505006.001.0001; Mausfeld R., 1998, COLOR VISION PERSPEC, P219; NIKOLAEV PP, 1985, BIOFIZIKA+, V30, P112; Pokorny J., 1991, PERCEPTION COLOUR, V6, P43; Riemann G. F. B, 1867, ABHANDLUNGEN KONIGLI, V13; Schanda J, 2007, COLORIMETRY: UNDERSTANDING THE CIE SYSTEM, P1, DOI 10.1002/9780470175637; Schrodinger E, 1920, ANN PHYS-BERLIN, V62, P603; Schrodinger E, 1970, SOURCES COLOR SCI, P134; Shepard R.N., 1992, ADAPTED MIND EVOLUTI, P495; Shevell SK, 2008, ANNU REV PSYCHOL, V59, P143, DOI 10.1146/annurev.psych.59.103006.093619; Suppes Patrick., 1989, FDN MEASUREMENT, VII; Tokunaga R, 2008, VISUAL NEUROSCI, V25, P395, DOI 10.1017/S0952523808080395; Tokunaga R, 2010, J OPT SOC AM A, V27, P2551, DOI 10.1364/JOSAA.27.002551; Tokunaga R, 2010, OPHTHAL PHYSL OPT, V30, P611, DOI 10.1111/j.1475-1313.2010.00733.x; Tokunaga R, 2010, VISION RES, V50, P1740, DOI 10.1016/j.visres.2010.05.030; TVERSKY A, 1977, PSYCHOL REV, V84, P327, DOI 10.1037/h0026750; van Trigt C, 2007, J OPT SOC AM A, V24, P2684, DOI 10.1364/JOSAA.24.002684; Volbrecht V. J., 1998, COLOR VISION PERSPEC, P187; von Helmholtz H., 1867, HDB PHYSL OPTIK, VVol. 9, DOI 10.1007/BF01708548; WEINBERG JW, 1976, GEN RELAT GRAVIT, V7, P135, DOI 10.1007/BF00762021; Whittle P., 2003, COLOUR PERCEPTION MI, P115; Wyszecki G., 2000, COLOR SCI CONCEPTS M, V2nd	69	13	14	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	1					143	160		10.1007/s11263-012-0555-2	http://dx.doi.org/10.1007/s11263-012-0555-2			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	080XV					2022-12-18	WOS:000314278500006
J	Kohli, P; Nickisch, H; Rother, C; Rhemann, C				Kohli, Pushmeet; Nickisch, Hannes; Rother, Carsten; Rhemann, Christoph			User-Centric Learning and Evaluation of Interactive Segmentation Systems	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Interactive systems; Image segmentation; Learning	COST	Many successful applications of computer vision to image or video manipulation are interactive by nature. However, parameters of such systems are often trained neglecting the user. Traditionally, interactive systems have been treated in the same manner as their fully automatic counterparts. Their performance is evaluated by computing the accuracy of their solutions under some fixed set of user interactions. In this paper, we study the problem of evaluating and learning interactive segmentation systems which are extensively used in the real world. The key questions in this context are how to measure (1) the effort associated with a user interaction, and (2) the quality of the segmentation result as perceived by the user. We conduct a user study to analyze user behavior and answer these questions. Using the insights obtained from these experiments, we propose a framework to evaluate and learn interactive segmentation systems which brings the user in the loop. The framework is based on the use of an active robot user-a simulated model of a human user. We show how this approach can be used to evaluate and learn parameters of state-of-the-art interactive segmentation systems. We also show how simulated user models can be integrated into the popular max-margin method for parameter learning and propose an algorithm to solve the resulting optimisation problem.	[Kohli, Pushmeet; Rother, Carsten] Microsoft Res Cambridge, Cambridge, England; [Nickisch, Hannes] MPI Intelligent Syst, Tubingen, Germany; [Rhemann, Christoph] Vienna Univ Technol, A-1040 Vienna, Austria	Microsoft; Max Planck Society; Technische Universitat Wien	Kohli, P (corresponding author), Microsoft Res Cambridge, Cambridge, England.	pkohli@microsoft.com; hannnes@nickisch.org; carrot@microsoft.com; rhemann@ims.tuwien.ac.at	Nickisch, Hannes/I-7049-2017	Nickisch, Hannes/0000-0003-1604-6647	Vienna Science and Technology Fund (WWTF) [ICT08-019]	Vienna Science and Technology Fund (WWTF)	Christoph Rhemann was supported by the Vienna Science and Technology Fund (WWTF) under project ICT08-019.	[Anonymous], 2008, CVPR; Bai X., 2007, ICCV; Batra D., 2010, CVPR; Blake A, 2011, MARKOV RANDOM FIELDS FOR VISION AND IMAGE PROCESSING, P1; BLAKE A, 2004, ECCV; Duchenne O., 2008, CVPR; GRADY L, 2006, IEEE T PATTERN ANAL, V0028, P00001, DOI DOI 10.1109/TPAMI.2006.233; Gulshan Varun, 2010, CVPR; Hofmann T., 2004, P 21 INT C MACH LEAR, P104, DOI 10.1145/1015330.1015341; Joachims T., 2008, ICML; Jolly M., 2001, ICCV; Kohli P., 2008, CVPR; KOHLI P, 2005, ICCV; Li Y., 2004, SIGGRAPH, V23; Liu J., 2009, SIGGRAPH; McGuinness K., 2011, CVIU; McGuinness K, 2010, PATTERN RECOGN, V43, P434, DOI 10.1016/j.patcog.2009.03.008; Nickisch H., 2009, LEARNING INTERACTIVE; Nickisch H., 2010, ICVGIP; Nowozin S., 2009, CVPR; Rother C., 2004, SIGGRAPH; Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Singaraju D., 2009, CVPR; Sorokin A., 2008, INT VIS WORKSH CVPR; SZELISKI R, 2006, ECCV; Szummer M., 2008, ECCV; Taskar B., 2004, ICML; Vijayanarasimhan S, 2011, CVPR; Vijayanarasimhan S, 2011, INT J COMPUT VISION, V91, P24, DOI 10.1007/s11263-010-0372-4; Vijayanarasimhan S, 2009, PROC CVPR IEEE, P2262, DOI 10.1109/CVPRW.2009.5206705; Von Ahn Luis, 2004, P SIGCHI C HUM FACT, P319, DOI DOI 10.1145/985692.985733; [No title captured]; [No title captured]	35	13	13	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2012	100	3					261	274		10.1007/s11263-012-0537-4	http://dx.doi.org/10.1007/s11263-012-0537-4			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	008KV		Green Submitted			2022-12-18	WOS:000308956700003
J	Gasparini, S; Caglioti, V				Gasparini, Simone; Caglioti, Vincenzo			Line Localization from Single Catadioptric Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Non-central catadioptric cameras; Line localization; Reconstruction from single image	REFLECTIVE SURFACES	Indoor environments often contain several line segments. The 3D reconstruction of such environments can thus be reduced to the localization of lines in the 3D space. Multi-view reconstruction requires the solution of the correspondence problem. The use of a single image to localize space lines is attractive, since the correspondence problem can be avoided. However, using a central camera the line localization from single image is an ill-posed problem, because there are infinitely many lines sharing the same image. In this work we relaxed the constraint on single viewpoint imaging and considered a wide class of non-central catadioptric cameras, constituted by an axial symmetric mirror and a perspective camera placed at a generic relative position. In the paper we report the results of our study on line localization for such cameras, reporting the conditions that allow a line to be localized from a single image. We show how the analysis can be extended to other classes of non-central devices sharing a similar imaging model. We also present a brief overview of the main algorithms for line localization from single image that have been proposed.	[Gasparini, Simone] INRIA Grenoble Rhone Alpes, F-38334 Montbonnot St Martin, France; [Caglioti, Vincenzo] Politecn Milan, Dipartimento Elettron & Informaz, I-20133 Milan, Italy	Polytechnic University of Milan	Gasparini, S (corresponding author), INRIA Grenoble Rhone Alpes, 655 Ave Europe, F-38334 Montbonnot St Martin, France.	simone.gasparini@inrialpes.fr; vincenzo.caglioti@polimi.it		Caglioti, Vincenzo/0000-0003-2741-7474; Gasparini, Simone/0000-0001-8239-8005				Avidan S, 2000, IEEE T PATTERN ANAL, V22, P348, DOI 10.1109/34.845377; Baker S, 1999, INT J COMPUT VISION, V35, P175, DOI 10.1023/A:1008128724364; BAKSTEIN H, 2001, COMP VIS WINT WORKSH; Barreto JP, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1359; BOGNER SL, 1995, P IEEE INT C SYST MA, V4, P3099; BREWSTER DS, 1838, TREATISE OPTICS; Bronnimann H, 2005, DISCRETE COMPUT GEOM, V34, P381, DOI 10.1007/s00454-005-1183-1; Caglioti V, 2005, PROC CVPR IEEE, P1129; CAGLIOTI V, 2007, P 7 WORKSH OMN VIS O; CAGLIOTI V, 2006, P IEEE INT C COMP VI, V1, P1266, DOI DOI 10.1109/CVPR.2006.1; CAGLIOTI V, 2005, P 6 WORKSH OMN VIS O; CAUCHOIS C, 1999, P IEEE INT C ROB AUT, V2, P1287; Chahl JS, 1997, APPL OPTICS, V36, P8275, DOI 10.1364/AO.36.008275; CROTEAU A, 2000, P IEEE 9 INT C TRANS, P83; Feldman D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P988; Fermuller C, 2000, IEEE WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P11, DOI 10.1109/OMNVIS.2000.853797; FIROOZFAM P, 2002, P OCEANS MTS IEEE C, V3, P1595, DOI DOI 10.1109/OCEANS.2002.1191873; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658; Furukawa Y, 2009, PROC CVPR IEEE, P1422, DOI 10.1109/CVPRW.2009.5206867; GASPARINI S, 2007, THESIS POLITECNICO M; Gasparini S, 2009, ADV PATTERN RECOGNIT, P197, DOI 10.1007/978-1-84882-299-3_9; Geyer C, 2002, IEEE T PATTERN ANAL, V24, P687, DOI 10.1109/34.1000241; Gregory RL, 1966, EYE BRAIN PSYCHOL SE; Grossberg MD, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P108, DOI 10.1109/ICCV.2001.937611; Hartley R., 2004, ROBOTICA; Hassner T., 2006, CVPR WORKSH, P15; HICKS R, 2000, P IEEE INT C COMP VI, V1, P545, DOI DOI 10.1109/CVPR.2000.855867; Hicks RA, 2001, IMAGE VISION COMPUT, V19, P773, DOI 10.1016/S0262-8856(00)00104-9; Hilbert D., 1932, GEOMETRY IMAGINATION; HONG JW, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P620, DOI 10.1109/ROBOT.1991.131651; ISHIGURO H, 1992, IEEE T PATTERN ANAL, V14, P257, DOI 10.1109/34.121792; Kanbara M, 2006, INT C PATT RECOG, P874; Lanman D, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P89; MARCHESE FM, 2001, LECT NOTES COMPUTER, P179; Micusik B, 2004, PROC CVPR IEEE, P58; Pajdla T, 2002, INT J COMPUT VISION, V47, P161, DOI 10.1023/A:1014593824520; PELEG S, 1999, P IEEE INT C COMP VI, V1; Pless R, 2003, PROC CVPR IEEE, P587, DOI 10.1109/cvpr.2003.1211520; Ponce J, 2009, PROC CVPR IEEE, P1526, DOI 10.1109/CVPRW.2009.5206668; Rees D. W., 1970, US Patent, Patent No. 3505465; Semple J.G., 1998, OXFORD CLASSIC TEXTS; SHUM HY, 1999, P IEEE INT C COMP VI, V1, P22, DOI DOI 10.1109/ICCV.1999.791193; Sturm P, 2005, PROC CVPR IEEE, P206; Sturm P, 2000, IEEE WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P119, DOI 10.1109/OMNVIS.2000.853818; Swaminathan R, 2000, IEEE T PATTERN ANAL, V22, P1172, DOI 10.1109/34.879797; Swaminathan R, 2006, INT J COMPUT VISION, V66, P211, DOI 10.1007/s11263-005-3220-1; SWAMINATHAN R, 2008, P 8 WORKSH OMN VIS O, P2008; Teller S., 1999, Journal of Graphics Tools, V4, P11, DOI 10.1080/10867651.1999.10487506; Thrun S. a. o., 2002, EXPLORING ARTIFICIAL, V1, P1; Wilczkowiak M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P142, DOI 10.1109/ICCV.2001.937510; Yagi Y., 1990, Proceedings. IROS '90. IEEE International Workshop on Intelligent Robots and Systems '90. Towards a New Frontier of Applications (Cat. No.90TH0332-7), P181, DOI 10.1109/IROS.1990.262385; Yu JY, 2004, LECT NOTES COMPUT SC, V3022, P14; Zheng J. Y., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P161, DOI 10.1109/ICPR.1990.118082	54	13	13	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2011	94	3					361	374		10.1007/s11263-011-0435-1	http://dx.doi.org/10.1007/s11263-011-0435-1			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816AC		Green Submitted			2022-12-18	WOS:000294570100006
J	Liu, YH				Liu, Yonghuai			Replicator Dynamics in the Iterative Process for Accurate Range Image Matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Replicator dynamics; Iterative process; Accurate matching; Overlapping range images; Fitness; Probability of a possible correspondence being real	AUTOMATIC REGISTRATION; OBJECT RECOGNITION; 3D; ALGORITHM	Iterative algorithms are often used for range image matching. In this paper, we treat the iterative process of range image matching as a live biological system: evolving from one generation to another. Whilst different generations of the population are regarded as range images captured at different viewpoints, the iterative process is simulated using time. The well-known replicator equations in theoretical biology are then adapted to estimate the probabilities of possible correspondences established using the traditional closest point criterion. To reduce the effect of image resolutions on the final results for efficient and robust overlapping range image matching, the relative fitness difference (rather than the absolute fitness difference) is employed in the replicator equations in order to model the probability change of possible correspondences being real over successive iterations. The fitness of a possible correspondence is defined as the negative of a power of its squared Euclidean distance. While the replicator dynamics penalize those individuals with low fitness, they are further penalised with a parameter, since distant points are often unlikely to represent their real replicators. While the replicator equations assume that all individuals are equally likely to meet each other and thus treat them equally, we penalise those individuals competing for the same points as their possible replicators. The estimated probabilities of possible correspondences being real are finally embedded into the powerful deterministic annealing scheme for global optimization, resulting in the camera motion parameters being estimated in the weighted least squares sense. A comparative study based on real range images with partial overlap has shown that the proposed algorithm is promising for automatic matching of overlapping range images.	Aberystwyth Univ, Dept Comp Sci, Ceredigion SY23 3DB, Wales	Aberystwyth University	Liu, YH (corresponding author), Aberystwyth Univ, Dept Comp Sci, Ceredigion SY23 3DB, Wales.	yyl@aber.ac.uk	Liu, Yonghuai/ABF-3794-2020					Alboszta J, 2004, J THEOR BIOL, V231, P175, DOI 10.1016/j.jtbi.2004.06.012; Allen PK, 2003, IEEE INT CONF ROBOT, P145; Andreetto M, 2004, IEEE T IMAGE PROCESS, V13, P352, DOI 10.1109/TIP.2003.821351; ASHBROOK AP, 1998, P 5 ECCV, V2, P185; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Brusco N, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P262, DOI 10.1109/3DIM.2005.5; Brusco N, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P592, DOI 10.1109/TDPVT.2004.1335293; Chang NC, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P987; Chen CS, 1999, IEEE T PATTERN ANAL, V21, P1229, DOI 10.1109/34.809117; Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; Dewaele G, 2004, LECT NOTES COMPUT SC, V3021, P495; Dorai C, 1998, IEEE T PATTERN ANAL, V20, P83, DOI 10.1109/34.655652; Fisher R.A., 1930, GENETICAL THEORY NAT; Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; Gold S, 1998, PATTERN RECOGN, V31, P1019, DOI 10.1016/S0031-3203(98)80010-1; Granger S, 2002, LECT NOTES COMPUT SC, V2353, P418; Huber DF, 2003, IMAGE VISION COMPUT, V21, P637, DOI 10.1016/S0262-8856(03)00060-X; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Liu LH, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P2280, DOI 10.1109/IROS.2000.895308; Liu YG, 2005, PATTERN RECOGN, V38, P1615, DOI 10.1016/j.patcog.2005.01.008; Liu YH, 2004, ROBOT AUTON SYST, V47, P11, DOI 10.1016/j.robot.2004.02.002; Liu YH, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P232; Liu YH, 2006, IMAGE VISION COMPUT, V24, P762, DOI 10.1016/j.imavis.2006.01.009; Liu YH, 2006, ROBOT AUTON SYST, V54, P428, DOI 10.1016/j.robot.2006.02.008; Lomonosov E, 2006, PATTERN RECOGN LETT, V27, P1201, DOI 10.1016/j.patrec.2005.07.018; Makadia A., 2006, P IEEE COMP SOC C CO, P1297; Nowak MA, 2004, SCIENCE, V303, P793, DOI 10.1126/science.1093411; PAJDLA T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P390, DOI 10.1109/ICCV.1995.466913; Pelillo M, 1999, NEURAL COMPUT, V11, P1933, DOI 10.1162/089976699300016034; Pulli K., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P160, DOI 10.1109/IM.1999.805346; PUZICHA J, 1997, P 15 IMACS WORLD C S, V6, P445; Pykh YA, 2005, 2005 International Conference on Physics and Control (PHYSCON), P56; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Santamaria J, 2007, SOFT COMPUT, V11, P819, DOI 10.1007/s00500-006-0132-0; Schutz C, 1998, INT C PATT RECOG, P982, DOI 10.1109/ICPR.1998.711852; Sharp GC, 2002, IEEE T PATTERN ANAL, V24, P90, DOI 10.1109/34.982886; Silva L, 2005, IEEE T PATTERN ANAL, V27, P762, DOI 10.1109/TPAMI.2005.108; Stadler BMR, 2003, ADV COMPLEX SYST, V6, P47, DOI 10.1142/S0219525903000724; Stewart CV, 2003, IEEE T MED IMAGING, V22, P1379, DOI 10.1109/TMI.2003.819276; Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241; Xiao GY, 2007, IMAGE VISION COMPUT, V25, P934, DOI 10.1016/j.imavis.2006.07.006; Yamany SM, 2002, IEEE T PATTERN ANAL, V24, P1105, DOI 10.1109/TPAMI.2002.1023806; Zagorchev L, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P126, DOI 10.1109/3DIM.2005.10; Zhao W, 2004, PROC CVPR IEEE, P964; Zhu L, 2007, INT J ADV MANUF TECH, V32, P505, DOI 10.1007/s00170-005-0370-9; [No title captured]	48	13	13	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2009	83	1					30	56		10.1007/s11263-009-0210-8	http://dx.doi.org/10.1007/s11263-009-0210-8			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	413JY					2022-12-18	WOS:000263790600003
J	Chen, P; Suter, D				Chen, Pei; Suter, David			Rank Constraints for Homographies over Two Views: Revisiting the Rank Four Constraint	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Homography; Rank constraint; First order perturbation	FACTORIZATION METHOD; COMPUTER VISION; MOTION; SHAPE; IMAGES	It is well known that one can collect the coefficients of five (or more) homographies between two views into a large, rank deficient matrix. In principle, this implies that one can refine the accuracy of the estimates of the homography coefficients by exploiting the rank constraint. However, the standard rank-projection approach is impractical for two different reasons: it requires many homographies to even score a modest gain; and, secondly, correlations between the errors in the coefficients will lead to poor estimates. In this paper we study these problems and provide solutions to each. Firstly, we show that the matrices of the homography coefficients can be recast into two parts, each consistent with ranks of only one. This immediately establishes the prospect of realistically (that is, with as few as only three or four homographies) exploiting the redundancies of the homographies over two views. We also tackle the remaining issue: correlated coefficients. We compare our approach with the "gold standard"; that is, non-linear bundle adjustment (initialized from the ground truth estimate-the ideal initialization). The results confirm our theory and show one can implement rank-constrained projection and come close to the gold standard in effectiveness. Indeed, our algorithm (by itself), or our algorithm further refined by a bundle adjustment stage; may be a practical algorithm: providing generally better results than the "standard" DLT (direct linear transformation) algorithm, and even better than the bundle adjustment result with the DLT result as the starting point. Our unoptimized version has roughly the same cost as bundle adjustment and yet can generally produce close to the "gold standard" estimate (as illustrated by comparison with bundle adjustment initialized from the ground truth). Independent of the merits or otherwise of our algorithm, we have illuminated why the naive approach of direct rank-projection is relatively doomed to failure. Moreover, in revealing that there are further rank constraints, not previously known; we have added to the understanding of these issues, and this may pave the way for further improvements.	[Chen, Pei] Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510275, Guangdong, Peoples R China; [Chen, Pei] CAS CUHK, Shenzhen Inst Adv Integrat Technol, Shenzhen, Peoples R China; [Suter, David] Monash Univ, ARC Ctr Percept & Intelligent Machines Complex En, Dept Elect & Comp Syst Engn, Melbourne, Vic 3004, Australia	Sun Yat Sen University; Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; Monash University	Chen, P (corresponding author), Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510275, Guangdong, Peoples R China.	chenpei75@yahoo.com		Suter, David/0000-0001-6306-3023				Aguiar P. M., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P178, DOI 10.1109/CVPR.1999.786936; Aguiar PMQ, 2003, IEEE T PATTERN ANAL, V25, P1134, DOI 10.1109/TPAMI.2003.1227988; Aguiar PMQ, 2000, IEEE IMAGE PROC, P549, DOI 10.1109/ICIP.2000.901017; Aguiar PMQ, 2001, IEEE T IMAGE PROCESS, V10, P1541, DOI 10.1109/83.951539; AGUIAR PMQ, 1999, P INT C  IM PROC; Anandan P, 2002, INT J COMPUT VISION, V49, P101, DOI 10.1023/A:1020137420717; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; BASRI R, 1999, P INT C COMP VIS, P383; Brand M, 2002, LECT NOTES COMPUT SC, V2350, P707; Chen P, 2007, J MATH IMAGING VIS, V28, P191, DOI 10.1007/s10851-007-0003-z; Chen P, 2006, INT J COMPUT VISION, V68, P83, DOI 10.1007/s11263-006-6659-9; CHEN P, 2008, J MATH IMAG IN PRESS; CHEN P, 2004, THESIS MONASH U; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Hartley R., 2003, MULTIPLE VIEW GEOMET; IRANI M, 2000, P EUR C COMP VIS, V49, P539; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; Manton JH, 2003, IEEE T SIGNAL PROCES, V51, P500, DOI 10.1109/TSP.2002.807002; Morita T, 1997, IEEE T PATTERN ANAL, V19, P858, DOI 10.1109/34.608289; POELMAN C, 1994, P EUR C COMP VIS, P206; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448; Ramamoorthi R, 2002, IEEE T PATTERN ANAL, V24, P1322, DOI 10.1109/TPAMI.2002.1039204; SHASHUA A, 1996, P EUR C COMP VIS, P196; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TOMASI C, 1990, P INT C COMP VIS; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Zelnik-Manor L, 2002, IEEE T PATTERN ANAL, V24, P214, DOI 10.1109/34.982901; Zelnik-Manor L., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P710, DOI 10.1109/ICCV.1999.790291; ZELNIKMANOR L, 1999, P C COMP VIS PATT RE	32	13	14	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2009	81	2					205	225		10.1007/s11263-008-0167-z	http://dx.doi.org/10.1007/s11263-008-0167-z			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	393VI					2022-12-18	WOS:000262401600008
J	Troccoli, A; Allen, P				Troccoli, Alejandro; Allen, Peter			Building illumination coherent 3D models of large-scale outdoor scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D modeling; inverse rendering; texture acquisition	RECONSTRUCTION; REFLECTANCE; IMAGES	Systems for the creation of photorealistic models using range scans and digital photographs are becoming increasingly popular in a wide range of fields, from reverse engineering to cultural heritage preservation. These systems employ a range finder to acquire the geometry information and a digital camera to measure color detail. But bringing together a set of range scans and color images to produce an accurate and usable model is still an area of research with many unsolved problems. In this paper we address the problem of how to build illumination coherent integrated texture maps from images that were taken under different illumination conditions. To achieve this we present two different solutions. The first one is to align all the images to the same illumination, for which we have developed a technique that computes a relighting operator over the area of overlap of a pair of images that we then use to relight the entire image. Our proposed method can handle images with shadows and can effectively remove the shadows from the image, if required. The second technique uses the ratio of two images to factor out the diffuse reflectance of an image from its illumination. We do this without any light measuring device. By computing the actual reflectance we remove from the images any effects of the illumination, allowing us to create new renderings under novel illumination conditions.	[Allen, Peter] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA	Columbia University	Troccoli, A (corresponding author), 2701 San Tomas Expy, Santa Clara, CA 95054 USA.	atroccoli@acm.org						Agathos A, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P139, DOI 10.1109/IM.2003.1240243; ALLEN P, 2004, P 2 INT S 3D DAT PRO; Allen PK, 2003, IEEE COMPUT GRAPH, V23, P32, DOI 10.1109/MCG.2003.1242380; Bannai N, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P558; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; BEAUCHESNE E, 2003, P COMP VIS PATT REC; Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309; Chuang YY, 2003, ACM T GRAPHIC, V22, P494, DOI 10.1145/882262.882298; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; DEBEVEC P, 2004, ESTIMATING SURFACE R; Debevec PaulE., 1996, P 23 ANN C COMP GRAP, P11; FROLOVA D, 2004, P EUR C COMP VIS, V1, P574; FUNKALEA G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P203, DOI 10.1109/ICCV.1995.466785; Hertzmann A, 2005, IEEE T PATTERN ANAL, V27, P1254, DOI 10.1109/TPAMI.2005.158; IKEUCHI K, 1991, IEEE T PATTERN ANAL, V13, P1139, DOI 10.1109/34.103274; Lensch HPA, 2003, ACM T GRAPHIC, V22, P234, DOI 10.1145/636886.636891; MARSCHNER SR, 1997, P 5 COL IM C; Narasimhan SG, 2001, PROC CVPR IEEE, P186; Pulli K., 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P23; Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; Ramamoorthi R, 2002, IEEE T PATTERN ANAL, V24, P1322, DOI 10.1109/TPAMI.2002.1039204; Reda I., 2003, SOLAR POSITION ALGOR; Salvador E, 2004, COMPUT VIS IMAGE UND, V95, P238, DOI 10.1016/j.cviu.2004.03.008; Shashua A, 2001, IEEE T PATTERN ANAL, V23, P129, DOI 10.1109/34.908964; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; TROCCOLI A, 2004, 2 IEEE WORKSH IM VID; TROCCOLI A, 2006, THESIS COLUBIA U; TROCCOLI A, 2005, P 3DIM 05; TROCCOLI A, 2006, P 3 INT S 3D DAT PRO; TROCCOLI A, 2006, ANIMATION RELIGHTED; Wang HT, 2004, PROC CVPR IEEE, P498; Wang LF, 2001, PROC CVPR IEEE, P347; XU C, 2006, P 3 INT S 3D DAT PRO; YU Y, 1998, P SIGGRAPH 98, P207	36	13	14	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2008	78	2-3					261	280		10.1007/s11263-007-0100-x	http://dx.doi.org/10.1007/s11263-007-0100-x			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	275RK		Green Submitted			2022-12-18	WOS:000254089100009
J	Reyes, L; Medioni, G; Bayro, E				Reyes, Leo; Medioni, Gerard; Bayro, Eduardo			Registration of 3D points using geometric algebra and tensor voting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						computer vision; 3D motion estimation; tensor voting; geometric algebra		We address the problem of finding the correspondences of two point sets in 3D undergoing a rigid transformation. Using these correspondences the motion between the two sets can be computed to perform registration. Our approach is based on the analysis of the rigid motion equations as expressed in the Geometric Algebra framework. Through this analysis it was apparent that this problem could be cast into a problem of finding a certain 3D plane in a different space that satisfies certain geometric constraints. In order to find this plane in a robust way, the Tensor Voting methodology was used. Unlike other common algorithms for point registration (like the Iterated Closest Points algorithm), ours does not require an initialization, works equally well with small and large transformations, it cannot be trapped in "local minima" and works even in the presence of large amounts of outliers. We also show that this algorithm is easily extended to account for multiple motions and certain non-rigid or elastic transformations.	CINVESTAV Unidad Guadalajara, Elect Engn & Comp Sci, Zapopan 45010, Jalisco, Mexico; Univ So Calif, Inst Robot & Intelligent Syst, Los Angeles, CA 90089 USA	CINVESTAV - Centro de Investigacion y de Estudios Avanzados del Instituto Politecnico Nacional; University of Southern California	Bayro, E (corresponding author), CINVESTAV Unidad Guadalajara, Elect Engn & Comp Sci, Av Cient 1145,Colonia Bajio, Zapopan 45010, Jalisco, Mexico.	edb@gdl.cinvestav.mx						Bayro-Corrochano E., 2001, GEOMETRIC COMPUTING; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; BORGEFORS G, 1988, IEEE T PATTERN ANAL, V10, P849, DOI 10.1109/34.9107; CHAMPLEBOUX G, 1992, IEEE COMP SOC C COMP, P83; Chua CS, 1996, INT J COMPUT VISION, V17, P77, DOI 10.1007/BF00127819; Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186; Chui HL, 2000, PROC CVPR IEEE, P44, DOI 10.1109/CVPR.2000.854733; Cunnington S. J., 1999, BMVC99. Proceedings of the 10th British Machine Vision Conference, P234; Eggert DW, 1998, COMPUT VIS IMAGE UND, V69, P253, DOI 10.1006/cviu.1998.0667; Feldmar J, 1996, PROCEEDINGS OF THE IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, P84, DOI 10.1109/MMBIA.1996.534060; Fitzgibbon AW, 2003, IMAGE VISION COMPUT, V21, P1145, DOI 10.1016/j.imavis.2003.09.004; Fookes C, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P447, DOI 10.1109/ICIP.2000.899444; GRIMSON WEL, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P430, DOI 10.1109/CVPR.1994.323862; Guest E, 2001, IEEE T PATTERN ANAL, V23, P165, DOI 10.1109/34.908967; Hestenes D, 1966, SPACE TIME ALGEBRA; Hestenes D, 2012, CLIFFORD ALGEBRA GEO; HU G, 1995, IEEE INT C SYST MAN, V3, P2718; IONESCU D, 1993, CAN C EL COMP ENG, V2, P710; Johnson AE, 1997, INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P234, DOI 10.1109/IM.1997.603871; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kang EY, 2002, INT C PATT RECOG, P256, DOI 10.1109/ICPR.2002.1047445; Kybic J, 2003, IEEE T IMAGE PROCESS, V12, P1427, DOI 10.1109/TIP.2003.813139; LOUNESTO P, 1987, A248 I MATH; LOUNESTO P, 1987, CLICAL CALCULATOR TY; Lounesto P, 1997, CLIFFORD ALGEBRAS SP; LU F, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P935, DOI 10.1109/CVPR.1994.323928; LUO B, 1999, 10 BRIT MACH VIS C, P43; Medioni G., 2000, COMPUTATIONAL FRAMEW; Simon D A, 1995, J Image Guid Surg, V1, P17, DOI 10.3109/10929089509106822; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; Stoddart A. J., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P40, DOI 10.1109/ICPR.1996.546720; Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241; Wells WM, 1997, INT J COMPUT VISION, V21, P63, DOI 10.1023/A:1007923522710; ZHANG Z, 1990, 1658 INRIA	34	13	13	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2007	75	3					351	369		10.1007/s11263-007-0038-z	http://dx.doi.org/10.1007/s11263-007-0038-z			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	211QS					2022-12-18	WOS:000249539000003
J	Quan, L; Wang, JD; Tan, P; Yuan, L				Quan, Long; Wang, Jingdong; Tan, Ping; Yuan, Lu			Image-based modeling by joint segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; image-based modeling; reconstruction; segmentation	RECONSTRUCTION	The paper first traces the image-based modeling back to feature tracking and factorization that have been developed in the group led by Kanade since the eighties. Both feature tracking and factorization have inspired and motivated many important algorithms in structure from motion, 3D reconstruction and modeling. We then revisit the recent quasi-dense approach to structure from motion. The key advantage of the quasi-dense approach is that it not only delivers the structure from motion in a robust manner for practical modeling purposes, but also it provides a cloud of sufficiently dense 3D points that allows the objects to be explicitly modeled. To structure the available 3D points and registered 2D image information, we argue that a joint segmentation of both 3D and 2D is the fundamental stage for the subsequent modeling. We finally propose a probabilistic framework for the joint segmentation. The optimal solution to such a joint segmentation is still generally intractable, but approximate solutions are developed in this paper. These methods are implemented and validated on real data set.	Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Quan, L (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.	quan@cs.ust.hk; welleast@cs.ust.hk; ptan@cs.ust.hk; luyuan@cs.ust.hk	Wang, Jingdong/E-9920-2017	Wang, Jingdong/0000-0002-4888-4445				BLAKE A, 2004, ECCV, P428; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y.Y., 2001, ICCV, V1, P105, DOI DOI 10.1109/ICCV.2001.937505; Criminisi A., 2006, IEEE C COMP VIS PATT, V1, P53, DOI DOI 10.1109/CVPR.2006.69; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; FORSTNER W, 1994, P EUR C COMP VIS, P383; FUA P, 1991, P 12 INT JOINT C ART; Gargarella R, 2005, ICON-INT J CONST LAW, V3, P1, DOI 10.1093/icon/moi001; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; HARTLEY R, 1992, P 2 EUR C COMP VIS, P579; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; KOENDERINK JJ, 1989, J OPT SOC AM A, V8, P377; Kolmogorov V, 2005, PROC CVPR IEEE, P407; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; LAVEAU S, 1996, THESIS ECOLE POLYTEC; Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44; Lhuillier M, 2002, IEEE T PATTERN ANAL, V24, P1140, DOI 10.1109/TPAMI.2002.1023810; LHUILLIER M, 1998, P 9 BRIT MACH VIS C, P700; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384; MOHR R, 1995, INT J ROBOT RES, V14, P619, DOI 10.1177/027836499501400607; MOHR R, 1992, 84IIMAGLIFIA RT; MORAHAN-MARTIN J., 1998, CYBERPSYCHOLOGY BEHA, V1, P3, DOI DOI 10.1089/CPB.1998.1.3; Moravec H.P., 1979, P 6 INT JOINT C ART, P598; NISTER D, 2001, THESIS NADA KTH SWED; Patras I, 2001, IEEE T PATTERN ANAL, V23, P326, DOI 10.1109/34.910886; POLLARD SB, 1985, PERCEPTION, V14, P449, DOI 10.1068/p140449; Pollefeys M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P90, DOI 10.1109/ICCV.1998.710705; QUAN L, 1995, IEEE T PATTERN ANAL, V17, P34; QUAN L, 2006, P ACM SIGGRAPH; ROTHER C, 2004, P ACM SIGGRAPH, P309; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; SUN J, 2006, ECCV; TANNER MA, 1987, J AM STAT ASSOC, V82, P528, DOI 10.2307/2289457; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TOMASI C, 1991, THESIS CARNEGIE MELL; Torr PHS, 2001, IEEE T PATTERN ANAL, V23, P297, DOI 10.1109/34.910882; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Triggs B, 1996, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.1996.517170; TRIGGS B, 2004, EUR C COMP VIS; Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239; Tuytelaars T., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P412; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; WEI Y, 2005, P ACM SIGGRAPH 2005, V27; WILLSJ, 2003, CVPR, P37; XIAO J, 2004, CVPR, P972; Yang L, 2004, INT C PATT RECOG, P303, DOI 10.1109/ICPR.2004.1334180; ZABIH R, 2004, CVPR, P437; ZENG G, IN PRESS IEEE T PATT; ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4; ZHU X, 2005, ICML, P1052	60	13	16	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2007	75	1					135	150		10.1007/s11263-007-0044-1	http://dx.doi.org/10.1007/s11263-007-0044-1			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	197RM					2022-12-18	WOS:000248574200008
J	Ilic, S; Salzmann, M; Fua, P				Ilic, Slobodan; Salzmann, Mathieu; Fua, Pascal			Implicit meshes for effective silhouette handling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3-D modelina; silhouettes; tracking; deformable surfaces; implicit surfaces; bundle-adjustment	MODEL; SHAPE	Using silhouettes in uncontrolled environments typically requires handling occlusions as well as changing or cluttered backgrounds, which limits the applicability of most silhouette based methods. For the put-pose of 3-D shape modeling, we show that representing generic 3-D surfaces as implicit surfaces lets us effectively address these issues. This desirable behavior is completely independent from the way the surface deformations are parametrized. To show this, we demonstrate our technique in three very different cases: Modeling the deformations of a piece of paper represented by an ordinary triangulated mesh; reconstruction and tracking a person's shoulders whose deformations are expressed in terms of Dirichlet Free Form Deformations; reconstructing the shape of a human face parametrized in terms of a Principal Component Analysis model.	Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Ilic, S (corresponding author), Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland.	Slobodan.Ilic@epfl.ch; Mathieu.Salzmann@epfl.ch; Pascal.Fua@epfl.ch	Fua, Pascal/H-3928-2011	Fua, Pascal/0000-0002-6702-9970; Salzmann, Mathieu/0000-0002-8347-8637; Ilic, Slobodan/0000-0002-3413-1936				Agarwal A., 2004, C COMP VIS PATT REC; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Boyer E, 1997, INT J COMPUT VISION, V22, P219, DOI 10.1023/A:1007978616082; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; CROSS G, 2000, NATO ADV RES WORKSH, P25; DAVID P, 2002, EUR C COMP VIS COP D; DAVIS JW, 1998, WORKSH MOD MOT CAPT, P12; DIMITRIJEVIC M, 2004, C COMP VIS PATT REC; Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620; Fua P, 2000, INT J COMPUT VISION, V38, P153, DOI 10.1023/A:1008105802790; FUA P, 1999, INT C COMP VIS SEPT, P46; GAVRILA DM, 1995, IEEE INT S COMP VIS, P253; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; ILIC S, 2002, EUR C COMP VIS COP D; ILIC S, 2003, ICCV WORKSH HIGH LEV; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Moccozet L, 1997, COMP ANIM CONF PROC, P93, DOI 10.1109/CA.1997.601047; Paragios NK, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1139, DOI 10.1109/ICCV.1998.710859; Press W.H., 1992, NUMERICAL RECIPES AR, V2nd; Rosten E., 2003, BRIT MACH VIS C, P719; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; SMINCHISESCU C, 2003, CVPR, V1, P69; SULLIVAN S, 1994, IEEE T PATTERN ANAL, V16, P1183, DOI 10.1109/34.387489; Sullivan S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P510, DOI 10.1109/ICCV.1998.710765; Szeliski R, 1998, INT J COMPUT VISION, V28, P27, DOI 10.1023/A:1008050630628; TERZOPOULOS D, 1991, IEEE T PATTERN ANAL, V13, P703, DOI 10.1109/34.85659; VACCHETTI L, 2004, INT S MIX AUG REAL A; VAILLANT R, 1992, IEEE T PATTERN ANAL	31	13	13	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2007	72	2					159	178		10.1007/s11263-006-8595-0	http://dx.doi.org/10.1007/s11263-006-8595-0			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	127XW		Green Submitted			2022-12-18	WOS:000243620700003
J	Nister, D; Schaffalitzky, F				Nister, David; Schaffalitzky, Frederik			Four points in two or three calibrated views: Theory and practice	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	8th European Conference on Computer Vision	MAY 11-14, 2004	Prague, CZECH REPUBLIC	Business Informat Grp as, Camea spol sro, Casablanca INT sro, ECVision, Microsoft Res, Miracle Network sro, Neovision sro, Toyota		structure-from-motion; multi-view geometry; minimal methods; RANSAC; Kruppa constraint; algebraic curves		Suppose two perspective views of four world points are given and that the intrinsic parameters are known but the camera poses and the world point positions are not. We prove that the epipole in each view is then constrained to lie on a curve of degree ten. We derive the equation for the curve and establish many of the curve's properties. For example, we show that the curve has four branches through each of the image points and that it has four additional points on each conic of the pencil of conics through the four image points. We show how to compute the four curve points on each conic in closed form. We show that orientation constraints allow only parts of the curve and find that there are impossible configurations of four corresponding point pairs. We give a novel algorithm that solves for the essential matrix given three corresponding points and one of the epipoles. We then use the theory to create the most efficient solution yet to the notoriously difficult problem of solving for the pose of three views given four corresponding points. The solution is a search over a one-dimensional parameter domain, where each point in the search can be evaluated in closed form. The intended use for the solution is in a hypothesise-and-test architecture to solve for structure and motion.	Sarnoff Corp, Princeton, NJ 08530 USA; Univ Oxford, Robot Res Grp, Oxford, England	Sarnoff Corporation; University of Oxford	Nister, D (corresponding author), Univ Kentucky, Dept Comp Sci, Ctr Visualizat & Virtual Environm, Lexington, KY 40506 USA.	dnister@cs.uky.edu; fsm@robots.ox.ac.uk						Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; HARALICK RM, 1994, INT J COMPUT VISION, V13, P331, DOI 10.1007/BF02028352; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HOLT R, 1995, IEEE T PATTERN ANAL, V17; KIRWAN F, 1995, COMPLEX ALGEBRAIC CU; MAYBANK S, 1993, THEORY RECONSTRUCTIO; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Nister D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P199; Nister D, 2003, PROC CVPR IEEE, P195; QUAN L, 2003, IN PRESS J MATH IMAG; Schaffalitzky F, 2000, LECT NOTES COMPUT SC, V1842, P632; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; Werner T, 2003, PROC CVPR IEEE, P203	14	13	13	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2006	67	2					211	231		10.1007/s11263-005-4265-x	http://dx.doi.org/10.1007/s11263-005-4265-x			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	048ZO					2022-12-18	WOS:000237985300005
J	Kanatani, K; Nakatsuji, A; Sugaya, Y				Kanatani, K; Nakatsuji, A; Sugaya, Y			Stabilizing the focal length computation for 3-D reconstruction from two uncalibrated views	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	6th Asian Conference on Computer Vision	JAN 28-30, 2004	Cheju Isl, SOUTH KOREA			structure from motion; two-view analysis; focal length estimation; fundamental matrix; fixated images	METRIC RECONSTRUCTION; MOTION	In order to reconstruct 3-D shape from two uncalibrated views, one needs to resolve two problems: (i) the computed focal lengths can be imaginary; (ii) the computation fails for fixated images. We present a practical remedy for these by subsampling feature points and fixing the focal length. We first summarize theoretical backgrounds and then do simulations, which reveal a rather surprising fact that when the focal length is actually fixed, not using that knowledge yields better results for non-fixated images. We give an explanation to this seeming paradox and derive a hybrid method switching the computation by judging whether or not the images are fixated. Doing simulations and real image experiments, we demonstrate the effectiveness of our method.	Okayama Univ, Dept Comp Sci, Okayama 7008530, Japan; NEC Engn Ltd, Internet Terminals Div, Yokohama, Kanagawa 1838502, Japan; Okayama Univ, Dept Comp Sci, Okayama 7008530, Japan	Okayama University; Okayama University	Kanatani, K (corresponding author), Okayama Univ, Dept Comp Sci, Okayama 7008530, Japan.	kanatani@suri.it.okayama-u.ac.jp; a-nakatsuji@pb.jp.nec.com; sugaya@suri.it.okayana-u.ac.jp						Bougnoux S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P790, DOI 10.1109/ICCV.1998.710808; Brooks MJ, 1998, IMAGE VISION COMPUT, V16, P989, DOI 10.1016/S0262-8856(98)00064-X; Chojnacki W, 2000, IEEE T PATTERN ANAL, V22, P1294, DOI 10.1109/34.888714; Hart Adam G., 2002, Acta Ethologica, V5, P1, DOI 10.1007/s10211-002-0062-5; HARTLEY R, 1992, P 2 EUR C COMP VIS, P579; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Heyden A, 1997, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.1997.609362; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P1310, DOI 10.1109/34.41368; Kanatani K, 2004, IEEE T PATTERN ANAL, V26, P1307, DOI 10.1109/TPAMI.2004.93; Kanatani K, 2003, J ELECTRON IMAGING, V12, P478, DOI 10.1117/1.1579018; Kanatani K, 1998, INT J COMPUT VISION, V26, P171, DOI 10.1023/A:1007948927139; Kanatani K., 1993, GEOMETRIC COMPUTATIO; KANATANI K, 2000, P 6 S SENS VIA IM IN, P291; Kanatani K., 1996, STAT OPTIMIZATION GE; Kanatani K., 2000, P 809 AS C COMP VIS, V1, P128; Kanazawa Y., 2004, P 6 AS C COMP VIS JE, V2, P1128; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; Newsam G. N., 1996, INT ARCH PHOTOGRAM R, P575; PAN HP, 1995, P SPIE VIDEOMETRICS, V4, P174; PAN HP, 1995, P SPIE VIDEOMETRICS, V4, P162; Sturm P, 2001, PROC CVPR IEEE, P145; UESHIBA T, 2003, P AUSTR JAP ADV WORK, P118; ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4	24	13	15	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2006	66	2					109	122		10.1007/s11263-005-3952-y	http://dx.doi.org/10.1007/s11263-005-3952-y			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	022CY					2022-12-18	WOS:000236033500002
J	Cooper, MC				Cooper, MC			Wireframe projections: Physical realisability of curved objects and unambiguous reconstruction of simple polyhedra	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						wireframe model; hidden-line drawing; physical realisability; impossible object; Necker cube; Penrose triangle	INTERPRETING LINE-DRAWINGS; WIRE FRAMES; MODELS; COMPLEXITY; SCENES	The reconstruction of an object from a single 2D projection of a 3D wireframe model is a vision problem with applications in CAD/CAM and computer graphics. We propose an algorithm for the interpretation of wireframe projections based on assigning semantic and numerical depth labels to lines. This method allows us to state necessary and sufficient conditions for the physical realisability of a wireframe projection of a curved object. The presence of linear features provides further constraints on the positions of object vertices. For example, each straight line gives rise to a coplanarity constraint between a set of object vertices. We show that extra information, such as vanishing points, parallel lines or user-entered depth-parity information, is sufficient to uniquely determine the face-circuits in wireframe projections of polyhedra with simple trihedral vertices. In fact, a polyhedron with simple trihedral vertices can be unambiguously reconstructed from its 3D wireframe model.	Univ Toulouse 3, IRIT, F-31062 Toulouse, France	Universite de Toulouse; Universite Toulouse III - Paul Sabatier	Cooper, MC (corresponding author), Univ Toulouse 3, IRIT, 118 Rue Narbonne, F-31062 Toulouse, France.	cooper@irit.fr	Cooper, Martin/AAE-8777-2020; Cooper, Martin/AAV-1705-2021	Cooper, Martin/0000-0003-4853-053X; Cooper, Martin/0000-0003-4853-053X				AGARWAL SC, 1992, COMPUT AIDED DESIGN, V24, P123, DOI 10.1016/0010-4485(92)90032-6; Alevizos P. D., 1991, Proceedings IROS '91. IEEE/RSJ International Workshop on Intelligent Robots and Systems '91. Intelligence for Mechanical Systems (Cat. No.91TH0375-6), P595, DOI 10.1109/IROS.1991.174541; [Anonymous], [No title captured]; CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; Cooper MC, 2001, INT J COMPUT VISION, V43, P75, DOI 10.1023/A:1011166601983; Cooper MC, 1999, ARTIF INTELL, V108, P31, DOI 10.1016/S0004-3702(98)00118-0; COOPER MC, 1993, IMAGE VISION COMPUT, V11, P82, DOI 10.1016/0262-8856(93)90074-Q; Cooper MC, 1997, IMAGE VISION COMPUT, V15, P263, DOI 10.1016/S0262-8856(96)01135-3; Cooper MC, 2000, ARTIF INTELL, V119, P235, DOI 10.1016/S0004-3702(00)00008-4; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; Jain PK, 1999, P I MECH ENG B-J ENG, V213, P597, DOI 10.1243/0954405991517056; KIROUSIS LM, 1988, J COMPUT SYST SCI, V37, P14, DOI 10.1016/0022-0000(88)90043-8; Kuo MH, 1998, LECT NOTES COMPUT SC, V1389, P265; LECLERC YG, 1992, INT J COMPUT VISION, V9, P113, DOI 10.1007/BF00129683; Lipson H, 1996, COMPUT AIDED DESIGN, V28, P651, DOI 10.1016/0010-4485(95)00081-X; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; MARILL T, 1991, INT J COMPUT VISION, V6, P147, DOI 10.1007/BF00128154; MARKOWSKY G, 1980, IBM J RES DEV, V24, P582, DOI 10.1147/rd.245.0582; Mortenson M.E., 1997, GEOMETRIC MODELLING; PARODI P, 1994, ARTIF INTELL, V70, P239, DOI 10.1016/0004-3702(94)90107-4; PENROSE LS, 1958, BRIT J PSYCHOL; ROS L, 2002, IEEE T PATTERN ANAL, V24; Shpitalni M, 1996, IEEE T PATTERN ANAL, V18, P1000, DOI 10.1109/34.541409; SUGIHARA K, 1978, COMPUT VISION GRAPH, V8, P382, DOI 10.1016/0146-664X(78)90064-3; Sugihara K., 1986, MACHINE INTERPRETATI; Syeda-Mahmood T, 1999, IEEE T PATTERN ANAL, V21, P737, DOI 10.1109/34.784287; Varley P. A. C., 2001, International Journal of Shape Modeling, V7, P23, DOI 10.1142/S0218654301000035; Vosniakos G, 1998, COMPUT INTEGR MANUF, V11, P53, DOI 10.1016/S0951-5240(98)00009-3; VOSNIAKOS G, 1997, INT J ADV MANUF TECH, V14, P199; WESLEY MA, 1981, IBM J RES DEV, V25, P934, DOI 10.1147/rd.256.0934	30	13	14	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2005	64	1					69	88		10.1007/s11263-005-1087-9	http://dx.doi.org/10.1007/s11263-005-1087-9			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	950KB					2022-12-18	WOS:000230854900003
J	Niethammer, M; Betelu, S; Sapiro, G; Tannenbaum, A; Giblin, PJ				Niethammer, M; Betelu, S; Sapiro, G; Tannenbaum, A; Giblin, PJ			Area-based medial axis of planar curves	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						medial axis; affine invariant; symmetry; area; shape; pattern recognition	SYMMETRY SETS; SKELETONS; SHAPE; TRANSFORM; IMAGES; VISION	A new definition of affine invariant medial axis of planar closed curves is introduced. A point belongs to the affine medial axis if and only if it is equidistant from at least two points of the curve, with the distance being a minimum and given by the areas between the curve and its corresponding chords. The medial axis is robust, eliminating the need for curve denoising. In a dynamical interpretation of this affine medial axis, the medial axis points are the affine shock positions of the affine erosion of the curve. We propose a simple method to compute the medial axis and give examples. We also demonstrate how to use this method to detect affine skew symmetry in real images.	Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA; Univ N Texas, Dept Math, Denton, TX 76203 USA; Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA; Univ Liverpool, Dept Math, Liverpool L69 3BX, Merseyside, England	University System of Georgia; Georgia Institute of Technology; University of North Texas System; University of North Texas Denton; University of Minnesota System; University of Minnesota Twin Cities; University of Liverpool	Niethammer, M (corresponding author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA.	guille@ece.umn.edu			NATIONAL CENTER FOR RESEARCH RESOURCES [P41RR013218] Funding Source: NIH RePORTER; NCRR NIH HHS [P41 RR013218] Funding Source: Medline	NATIONAL CENTER FOR RESEARCH RESOURCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR)); NCRR NIH HHS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Research Resources (NCRR))		AUGUST J, 1999, P INT C COMP VIS IEE, V1, P315; Betelu S, 2001, PATTERN RECOGN, V34, P943, DOI 10.1016/S0031-3203(00)00045-5; BETELU S, 2001, P INT C COMP VIS IEE, V2, P174; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154; BRADY M, 1984, INT J ROBOT RES, V3, P36, DOI 10.1177/027836498400300302; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; Bruce J.W., 1992, CURVES SINGULARITIES; BRUCE JW, 1986, P ROY SOC EDINB A, V104, P179, DOI 10.1017/S030821050001917X; BRUCE JW, 1985, P ROY SOC EDINB A, V101, P163, DOI 10.1017/S0308210500026263; CALABI L, 1968, AM MATH MON, V75, P335, DOI 10.2307/2313409; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; CHAM TJ, 1995, IMAGE VISION COMPUT, V13, P439, DOI 10.1016/0262-8856(95)99731-F; GIBLIN P, 1998, P INT C COMP VIS; Giblin P. J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P385, DOI 10.1109/ICCV.1999.791246; Giblin PJ, 2000, CH CRC RES NOTES, V412, P173; Giblin PJ, 1998, GEOMETRIAE DEDICATA, V71, P237, DOI 10.1023/A:1005099011913; GONIEWICZ RL, 1993, DISCRETE VORONOI SKE; HOLTOM P, 2000, THESIS LIVERPOOL U; Jain R., 1995, MACHINE VISION; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KATZ RA, 2002, UNPUB INT J COMPUTER; Kichenassamy S, 1996, ARCH RATION MECH AN, V134, P275, DOI 10.1007/BF00379537; Kimia BB, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P581, DOI 10.1109/ICIP.2001.958559; KLEE VL, 1949, AM MATH MONTHLY, V56, P247, DOI 10.2307/2304766; KOVACS I, 1994, NATURE, V370, P644, DOI 10.1038/370644a0; LAM L, 1992, IEEE T PATTERN ANAL, V14, P869, DOI 10.1109/34.161346; LEE SW, 1993, PATTERN RECOGN, V7, P1203; LEE TS, 1995, INVESTIGATIVE OPTHAL; LEYMARIE F, 1992, IEEE T PATTERN ANAL, V14, P56, DOI 10.1109/34.107013; Leyton M., 1992, SYMMETRY CAUSALITY M; MEYER F, 1990, SPIE, V1360, P251; Moisan L, 1998, IEEE T IMAGE PROCESS, V7, P411, DOI 10.1109/83.661191; MOTTSMITH JC, 1970, PICTURE PROCESSING P, P267; MOTZKIN TS, 1983, SELECTED PAPERS, P138; NIBLACK CW, 1990, P INT C PATT REC, V1, P881; Ogniewicz R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P63, DOI 10.1109/CVPR.1992.223226; Pizer SM, 1998, COMPUT VIS IMAGE UND, V69, P55, DOI 10.1006/cviu.1997.0563; PONCE J, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P49, DOI 10.1109/ROBOT.1989.99966; PREPARATA FP, 1990, TEXTS MONOGRAPHS COM; Pudney C, 1998, COMPUT VIS IMAGE UND, V72, P404, DOI 10.1006/cviu.1998.0680; Ranwez V, 2002, PATTERN RECOGN LETT, V23, P687, DOI 10.1016/S0167-8655(01)00146-5; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; Serra J, 1988, IMAGE ANAL MATH MORP; SHAKED D, 1996, THESIS TECHNION; Smoller J., 1983, GRUNDLEHREN MATH WIS; Svensson S, 1999, J VIS COMMUN IMAGE R, V10, P379, DOI 10.1006/jvci.1999.0425; VANGOOL L, 1995, COMPUT VIS IMAGE UND, V61, P138, DOI 10.1006/cviu.1995.1010; VanGool L, 1996, PROC CVPR IEEE, P285, DOI 10.1109/CVPR.1996.517087; Verwer B. J. H., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P1287, DOI 10.1142/S0218001493000637; WANG S, 1982, IEEE T PATTERN ANAL, V4, P419, DOI 10.1109/TPAMI.1982.4767274; WRIGHT MW, 1995, IMAGE VISION COMPUT, V13, P367, DOI 10.1016/0262-8856(95)99723-E	53	13	14	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2004	60	3					203	224		10.1023/B:VISI.0000036835.28674.d0	http://dx.doi.org/10.1023/B:VISI.0000036835.28674.d0			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	843OL	23710110	Green Accepted			2022-12-18	WOS:000223093300002
J	Yang, J; Li, Q; Zhuang, YT				Yang, J; Li, Q; Zhuang, YT			Towards data-adaptive and user-adaptive image retrieval by peer indexing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						user-adaptive; data-adaptive; peer indexing; content-based image retrieval; pseudo feedback		Adaptation to the characteristics of specific images and the preferences of individual users is critical to the success of an image retrieval system but insufficiently addressed by the existing approaches. In this paper, we propose an elegant and effective approach to data-adaptive and user-adaptive image retrieval based on the idea of peer indexing-describing an image through semantically relevant peer images. Specifically, we associate each image with a two-level peer index that models the "data characteristics" of the image as well as the "user characteristics" of individual users with respect to this image. Based on two-level image peer indexes, a set of retrieval parameters including query vectors and similarity metric are optimized towards both data and user characteristics by applying the pseudo feedback strategy. A cooperative framework is proposed under which peer indexes and image visual features are integrated to facilitate data- and user-adaptive image retrieval. Simulation experiments conducted on real-world images have verified the effectiveness of our approach in a relatively restricted setting.	Carnegie Mellon Univ, Sch Comp Sci, Language Technol Inst, Pittsburgh, PA 15213 USA; City Univ Hong Kong, Dept Comp Engn & Informat Technol, Kowloon, Hong Kong, Peoples R China; Zhejiang Univ, Dept Comp Sci, Hangzhou 310027, Peoples R China	Carnegie Mellon University; City University of Hong Kong; Zhejiang University	Yang, J (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Language Technol Inst, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	juny1@andrew.cmu.edu; itqli@cityu.edu.hk; yzhuang@cs.zju.edu.cn	Li, Qing/H-4100-2011	Li, Qing/0000-0003-3370-471X				Balabanovic M, 1997, COMMUN ACM, V40, P66, DOI 10.1145/245108.245124; Brusilovsky P, 2001, USER MODEL USER-ADAP, V11, P87, DOI 10.1023/A:1011143116306; Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596; GONG Y, 1994, P IEEE INT C MULT CO, P121; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Ishikawa Y., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P218; LEE C, 1998, INFORMATION EMBEDDIN; LU Y, 2000, P ACM MULT, P31; Minka TP, 1996, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.1996.517110; PAEK S, 1999, P ACM SIGIR WORKSH M; Pazzani M, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P54; Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413; Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510; Rui Y, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P815, DOI 10.1109/ICIP.1997.638621; Rui Y, 2000, PROC CVPR IEEE, P236, DOI 10.1109/CVPR.2000.855825; SALTON G, 1982, INTRO MODERN INFORMA; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; TAMURA H, 1984, PATTERN RECOGN, V17, P29, DOI 10.1016/0031-3203(84)90033-5; W-Y Ma, 1999, HDB MULTIMEDIA COMPU; ZHU L, 2000, P ACM MULT 2000 LOS, P157	20	13	15	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN-FEB	2004	56	1-2			SI		47	63		10.1023/B:VISI.0000004836.59343.e9	http://dx.doi.org/10.1023/B:VISI.0000004836.59343.e9			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	745MJ		Green Submitted			2022-12-18	WOS:000186692200005
J	Gomes, J; Faugeras, O				Gomes, J; Faugeras, O			The Vector Distance Functions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	3rd International Conference on Scale-Space and Morphology held in conjunction with the 8th International Conference on Computer Vision	JUL 07-08, 2001	VANCOUVER, CANADA	IEEE Tech Comm Pattern Anal & Machine Intelligence		implicit representations of shape; mean curvature motion in arbitrary codimension; change of dimension; manifolds with a boundary	MEAN-CURVATURE FLOW; LEVEL SET APPROACH; VISCOSITY SOLUTIONS; ALGORITHMS; SHAPE; CURVES	We present a novel method for representing and evolving objects of arbitrary dimension. The method, called the Vector Distance Function (VDF) method, uses the vector that connects any point in space to its closest point on the object. It can deal with smooth manifolds with and without boundaries and with shapes of different dimensions. It can be used to evolve such objects according to a variety of motions, including mean curvature. If discontinuous velocity fields are allowed the dimension of the objects can change. The evolution method that we propose guarantees that we stay in the class of VDF's and therefore that the intrinsic properties of the underlying shapes such as their dimension, curvatures can be read off easily from the VDF and its spatial derivatives at each time instant. The main disadvantage of the method is its redundancy: the size of the representation is always that of the ambient space even though the object we are representing may be of a much lower dimension. This disadvantage is also one of its strengths since it buys us flexibility.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA; INRIA, Sophia Antipolis, France; MIT, Cambridge, MA USA	International Business Machines (IBM); Inria; Massachusetts Institute of Technology (MIT)	Gomes, J (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.							ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; Adalsteinsson D, 1999, J COMPUT PHYS, V148, P2, DOI 10.1006/jcph.1998.6090; Ambrosio L, 1996, J DIFFER GEOM, V43, P693; AMBROSIO L, 1996, IN PRESS J GEOM ANAL; Arnol'd V I, 1973, ORDINARY DIFFERENTIA; ARNOL'D V. I., 1983, GEOMETRICAL METHODS; BARLES G, 1993, SIAM J CONTROL OPTIM, V31, P439, DOI 10.1137/0331021; Barles G., 1994, MATH APPL; Bertalmio M, 1999, LECT NOTES COMPUT SC, V1682, P46; Blake A., 1998, ACTIVE CONTOURS, DOI [10.1007/978-1-4471-1555-7, DOI 10.1007/978-1-4471-1555-7]; BURCHARD P, 2000, MOTION CURVES 3 SPAT; Caselles V., 1996, ICAOS'96. 12th International Conference on Analysis and Optimization of Systems. Images, Wavelets and PDEs, P43; Caselles V, 1996, SIAM J NUMER ANAL, V33, P2445, DOI 10.1137/S0036142994275044; Caselles V, 1997, IEEE T PATTERN ANAL, V19, P394, DOI 10.1109/34.588023; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; CHEN YG, 1991, J DIFFER GEOM, V33, P749; CHOPP DL, 1993, J COMPUT PHYS, V106, P77, DOI 10.1006/jcph.1993.1092; Cipolla R., 2000, VISUAL MOTION CURVES; CRANDALL MG, 1992, B AM MATH SOC, V27, P1, DOI 10.1090/S0273-0979-1992-00266-5; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; do Carmo M. P., 1992, RIEMANNIAN GEOMETRY; Epstein C L, 1987, WAVE MOTION THEORY M; Evans L. C., 1998, GRADUATE STUDIES MAT, V19; EVANS LC, 1991, J DIFFER GEOM, V33, P635, DOI 10.4310/jdg/1214446559; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; GAGE M, 1986, J DIFFER GEOM, V23, P69; GAGE ME, 1984, INVENT MATH, V76, P357, DOI 10.1007/BF01388602; Gomes J, 1999, LECT NOTES COMPUT SC, V1682, P70; Gomes J, 2000, J VIS COMMUN IMAGE R, V11, P209, DOI 10.1006/jvci.1999.0439; GOMES J, 2000, 4011 INRIA; GRAYSON MA, 1987, J DIFFER GEOM, V26, P285; GRAYSON MA, 1989, DUKE MATH J, V58, P555, DOI 10.1215/S0012-7094-89-05825-0; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, P 5 INT C COMP VIS B; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; KIMMEL R, 1997, GRAPH MODEL IM PROC, V60, P112; Lorigo LM, 1999, LECT NOTES COMPUT SC, V1613, P126; MALLADI R, 1994, LECT NOTES COMPUTER, V800; MONTAGNAT J, 2000, 3954 INRIA; OENDERINK JJ, 1990, SOLID SHAPE; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758; PRESS ICS, 1995, P 5 INT C COMP VIS B; RUUTH S, 1998, 9847 UCLA COMP APPL; RUUTH SJ, 1999, 9922 UCLA COMP APPL; SAPIRO G, 1994, J FUNCT ANAL, V119, P79, DOI 10.1006/jfan.1994.1004; Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591; SETHIAN JA, 1990, J DIFFER GEOM, V31, P131; Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703; Sochen N, 1998, IEEE T IMAGE PROCESS, V7, P310, DOI 10.1109/83.661181; SPIVAK M., 1979, COMPREHENSIVE INTRO, VI; Steinhoff J, 2000, J COMPUT PHYS, V157, P683, DOI 10.1006/jcph.1999.6389; TEK H, 1995, P 5 INT C COMP VIS B; TERZOPOULOS D, 1988, ARTIF INTELL, V36, P91, DOI 10.1016/0004-3702(88)90080-X; TSITSIKLIS JN, 1995, IEEE T AUTOMAT CONTR, V40, P1528, DOI 10.1109/9.412624; Zhao HK, 1996, J COMPUT PHYS, V127, P179, DOI 10.1006/jcph.1996.0167	57	13	13	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY-JUN	2003	52	2-3					161	187		10.1023/A:1022956108418	http://dx.doi.org/10.1023/A:1022956108418			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	659EL					2022-12-18	WOS:000181764500007
J	Maragos, P				Maragos, P			Algebraic and PDE approaches for lattice scale-spaces with global constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	3rd International Conference on Scale-Space and Morphology held in conjunction with the 8th International Conference o n Computer Vision	JUL 07-08, 2001	VANCOUVER, CANADA	IEEE Tech Comm Pattern Analy & Machine Intelligence		nonlinear scale-spaces; morphology; lattice operators; levelings; PDEs	FILTERS; RECONSTRUCTION; REPRESENTATION; ALGORITHMS; EQUATIONS; EVOLUTION; LEVELINGS	This paper begins with analyzing the theoretical connections between levelings on lattices and scale-space erosions on reference semilattices. They both represent large classes of self-dual morphological operators that exhibit both local computation and global constraints. Such operators are useful in numerous image analysis and vision tasks including edge-preserving multiscale smoothing, image simplification, feature and object detection, segmentation, shape and motion analysis. Previous definitions and constructions of levelings were either discrete or continuous using a PDE. We bridge this gap by introducing generalized levelings based on triphase operators that switch among three phases, one of which is a global constraint. The triphase operators include as special cases useful classes of semilattice erosions. Algebraically, levelings are created as limits of iterated or multiscale triphase operators. The subclass of multiscale geodesic triphase operators obeys a semigroup, which we exploit to find PDEs that can generate geodesic levelings and continuous-scale semilattice erosions. We discuss theoretical aspects of these PDEs, propose discrete algorithms for their numerical solution which converge as iterations of triphase operators, and provide insights via image experiments.	Natl Tech Univ Athens, Sch Elect & Comp Engn, Athens 15773, Greece	National Technical University of Athens	Maragos, P (corresponding author), Natl Tech Univ Athens, Sch Elect & Comp Engn, Athens 15773, Greece.	maragos@cs.ntua.gr						ALVAREZ L, 1993, ARCH RATION MECH AN, V123, P199, DOI 10.1007/BF00375127; BROCKETT RW, 1994, IEEE T SIGNAL PROCES, V42, P3377, DOI 10.1109/78.340774; Heijmans H., 1994, MORPHOLOGICAL IMAGE; HEIJMANS HJA, 2000, P INT C IM PROC VANC; HEIJMANS HJA, 2001, PNAR0101 CWI; Keshet R., 2000, Fundamenta Informaticae, V41, P33; MARAGOS P, 1989, IEEE T PATTERN ANAL, V11, P701, DOI 10.1109/34.192465; Maragos P, 1999, LECT NOTES COMPUT SC, V1682, P363; MATHERON G., 1975, RANDOM SETS INTEGRAL; Matheron G., 1997, NIVELLEMENTS; MEYER F, 1989, SIGNAL PROCESS, V16, P303, DOI 10.1016/0165-1684(89)90028-5; Meyer F, 2000, J VIS COMMUN IMAGE R, V11, P245, DOI 10.1006/jvci.1999.0447; MEYER F, 1998, MATH MORPHOLOGY ITS; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053; SALEMBIER P, 1995, IEEE T IMAGE PROCESS, V4, P1153, DOI 10.1109/83.403422; SAPIRO G, 1993, PATTERN RECOGN, V26, P1363, DOI 10.1016/0031-3203(93)90142-J; Serra J., 2000, Fundamenta Informaticae, V41, P147; Serra J, 1982, IMAGE ANAL MATH MORP; Serra J, 1988, IMAGE ANAL MATH MORP; Vincent L, 1993, IEEE T IMAGE PROCESS, V2, P176, DOI 10.1109/83.217222	21	13	14	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAY-JUN	2003	52	2-3					121	137		10.1023/A:1022999923439	http://dx.doi.org/10.1023/A:1022999923439			17	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	659EL					2022-12-18	WOS:000181764500004
J	Liebowitz, D; Carlsson, S				Liebowitz, D; Carlsson, S			Uncalibrated motion capture exploiting articulated structure constraints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion capture; calibration; multiple views; 3D reconstruction	SELF-CALIBRATION; AFFINE; CAMERA	We present an algorithm for 3D reconstruction of dynamic articulated structures, such as humans, from uncalibrated multiple views. The reconstruction exploits constraints associated with a dynamic articulated structure, specifically the conservation over time of length between rotational joints. These constraints admit reconstruction of metric structure from at least two different images in each of two uncalibrated parallel projection cameras. As a by product, the calibration of the cameras can also be computed. The algorithm is based on a stratified approach, starting with affine reconstruction from factorization, followed by rectification to metric structure using the articulated structure constraints. The exploitation of these specific constraints admits reconstruction and self-calibration with fewer feature points and views compared to standard self-calibration. The method is extended to pairs of cameras that are zooming, where calibration of the cameras allows compensation for the changing scale factor in a scaled orthographic camera. Results are presented in the form of stick figures and animated 3D reconstructions using pairs of sequences from broadcast television. The technique shows promise as a means of creating 3D animations of dynamic activities such as sports events.	Royal Inst Technol, Computat Vis & Act Percept Lab, Stockholm, Sweden	Royal Institute of Technology	Liebowitz, D (corresponding author), Royal Inst Technol, Computat Vis & Act Percept Lab, Stockholm, Sweden.							[Anonymous], P IEEE C COMP VIS PA; Bregler C., 1998, P IEEE C COMP VIS PA; FAUGERAS O, 1995, J OPT SOC AM A, V12, P465, DOI 10.1364/JOSAA.12.000465; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; FAUGERAS OD, 1990, INT J COMPUT VISION, V4, P225, DOI 10.1007/BF00054997; HARTLEY RI, 1992, LECT NOTES COMPUT SC, V588, P579; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; Kreysig E., 1993, ADV ENG MATH; LEVENTON ME, 1998, TR9806 MERL; Liebowitz D, 1998, PROC CVPR IEEE, P482, DOI 10.1109/CVPR.1998.698649; LIEBOWITZ D, 1999, P 7 INT C COMP VIS K; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; Mundy J., 1992, GEOMETRIC INVARIANCE; POELMAN C, 1994, P 3 EUR C COMP VIS S, V2, P97; Pollefeys M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P31, DOI 10.1007/BFb0015521; Quan L, 1996, INT J COMPUT VISION, V19, P93, DOI 10.1007/BF00131149; Reid ID, 1996, INT J COMPUT VISION, V18, P41, DOI 10.1007/BF00126139; SHAPIRO L, 1994, LNCS, V800; SHAPIRO LS, 1995, INT J COMPUT VISION, V16, P147, DOI 10.1007/BF01539553; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; WEBB JA, 1982, ARTIF INTELL, V19, P107, DOI 10.1016/0004-3702(82)90023-6; Weinshall D., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P675, DOI 10.1109/ICCV.1993.378147	23	13	14	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	2003	51	3					171	187		10.1023/A:1021897717694	http://dx.doi.org/10.1023/A:1021897717694			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	633TA					2022-12-18	WOS:000180297500001
J	Shimshoni, I; Ponce, J				Shimshoni, I; Ponce, J			Probabilistic 3D object recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object recognition; probabilistic reasoning; probabilistic peaking effect	MODEL-BASED RECOGNITION; PERSPECTIVE; LOCALIZATION; IMAGE; POSE	A probabilistic 3D object recognition algorithm is presented. In order to guide the recognition process the probability that match hypotheses between image features and model features are correct is computed. A model is developed which uses the probabilistic peaking effect of measured angles and ratios of lengths by tracing iso-angle and iso-ratio curves on the viewing sphere. The model also accounts for various types of uncertainty in the input such as incomplete and inexact edge detection. For each match hypothesis the pose of the object and the pose uncertainty which is due to the uncertainty in vertex position are recovered. This is used to find sets of hypotheses which reinforce each other by matching features of the same object with compatible uncertainty regions. A probabilistic expression is used to rank these hypothesis sets. The hypothesis sets with the highest rank are output. The algorithm has been fully implemented, and tested on real images.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Shimshoni, I (corresponding author), Technion Israel Inst Technol, Dept Ind Engn & Management, IL-32000 Haifa, Israel.	ilans@ie.technion.ac.il						ALTER TD, 1994, IEEE T PATTERN ANAL, V16, P802, DOI 10.1109/34.308475; ALTER TD, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P892, DOI 10.1109/CVPR.1994.323920; [Anonymous], 1985, PERCEPTUAL ORG VISUA; ARNOLD RD, 1980, P SPIE M SAN DIEG CA; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BENARIE J, 1990, IEEE T PATTERN ANAL, V12, P760, DOI 10.1109/34.57667; BESL PJ, 1985, COMPUT SURV, V17, P75, DOI 10.1145/4078.4081; BINFORD TO, 1987, WORKSH UNC ART INT; Breuel T. M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P445, DOI 10.1109/CVPR.1992.223152; BURNETT RH, 1993, J VINYL TECHNOL, V15, P1; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; CASS TA, 1994, ICPR, pA477; CHIN RT, 1986, COMPUT SURV, V18, P67, DOI 10.1145/6462.6464; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; GASTON PC, 1984, IEEE T PATT ANAL MAC, V6; Grimson W. E. L., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P316, DOI 10.1109/CVPR.1992.223257; GRIMSON WEL, 1991, IEEE T PATTERN ANAL, V13, P1201, DOI 10.1109/34.106994; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; HARALICK RM, 1994, INT J COMPUT VISION, V13, P331, DOI 10.1007/BF02028352; HORNEGGER J, 1995, ICCV, P914; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; Kriegman DJ, 1997, INT J ROBOT RES, V16, P448, DOI 10.1177/027836499701600402; KRIEGMAN DJ, 1991, CURVES SURFACES, P267; Morgan A. P., 1987, SOLVING POLYNOMIAL S; OLSEN CF, 1993, P IEEE C COMP VIS PA, P387; OLSON CF, 1995, IEEE T PATTERN ANAL, V17, P518, DOI 10.1109/34.391391; RUCKLIDGE W, 1995, INT C COMP VIS, P457; Shimshoni I, 1999, COMPUT VIS IMAGE UND, V74, P163, DOI 10.1006/cviu.1999.0755; WEINSHALL D, 1994, P 3 EUR C COMP VIS S, P24; WHEELER MD, 1995, IEEE T PATTERN ANAL, V17, P252, DOI 10.1109/34.368190; WU YY, 1994, IEEE T PATTERN ANAL, V16, P961, DOI 10.1109/34.329012	32	13	14	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2000	36	1					51	70		10.1023/A:1008172423811	http://dx.doi.org/10.1023/A:1008172423811			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	280DJ					2022-12-18	WOS:000085086700003
J	Geiger, D; Liu, TL; Donahue, MJ				Geiger, D; Liu, TL; Donahue, MJ			Sparse representations for image decompositions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						sparse representation; template matching; occlusion; recognition	PROJECTION PURSUIT REGRESSION	We are given an image I and a library of templates L, such that L is an overcomplete basis for I. The templates can represent objects, faces, features, analytical functions, or be single pixel templates (canonical templates). There are infinitely many ways to decompose I as a linear combination of the library templates. Each decomposition defines a representation for the image I, given L. What is an optimal representation for I given L and how to select it? We are motivated to select a sparse/compact representation for I, and to account for occlusions and noise in the image. We present a concave cost function criterion on the linear decomposition coefficients that satisfies our requirements. More specifically, we study a "weighted L-p norm" with 0 < p < 1. We prove a result that allows us to generate all local minima for the L-p norm, and the global minimum is obtained by searching through the local ones. Due to the computational complexity, i.e., the large number of local minima, we also study a greedy and iterative "weighted L-p Matching Pursuit" strategy.	NYU, Courant Inst, New York, NY 10012 USA; Acad Sinica, Inst Sci Informat, Tokyo 115, Japan; Univ Minnesota, IMA, Minneapolis, MN 55455 USA	New York University; University of Minnesota System; University of Minnesota Twin Cities	Geiger, D (corresponding author), NYU, Courant Inst, New York, NY 10012 USA.							BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Donawa M E, 1996, Med Device Technol, V7, P12; FIELD DJ, 1994, NEURAL COMPUT, V6, P559, DOI 10.1162/neco.1994.6.4.559; FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; GRENANDER U, 1989, ANN STAT, V17, P1, DOI 10.1214/aos/1176347002; HUBER PJ, 1985, ANN STAT, V13, P435, DOI 10.1214/aos/1176349519; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; JONES LK, 1987, ANN STAT, V15, P880, DOI 10.1214/aos/1176350382; KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390; Krumm J, 1996, PROC CVPR IEEE, P55, DOI 10.1109/CVPR.1996.517053; Leonardis A, 1996, PROC CVPR IEEE, P453, DOI 10.1109/CVPR.1996.517111; Olshausen BA, 1996, NETWORK-COMP NEURAL, V7, P333, DOI 10.1088/0954-898X/7/2/014; POGGIO T, 1990, SCIENCE, V247, P978, DOI 10.1126/science.247.4945.978; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; SPYROPOU.K, 1973, COMPUT J, V16, P180, DOI 10.1093/comjnl/16.2.180; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; [No title captured]; [No title captured]; [No title captured]	20	13	13	0	4	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1999	33	2					139	156		10.1023/A:1008146126392	http://dx.doi.org/10.1023/A:1008146126392			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	247MF					2022-12-18	WOS:000083224000003
J	Kalitzin, SN; Romeny, BMTH; Viergever, MA				Kalitzin, SN; Romeny, BMTH; Viergever, MA			Invertible apertured orientation filters in image analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						integral transformations; orientation analysis; scale space; complete systems	GABOR	A unitary approach for locally apertured orientation analysis of 2D and 3D scalar images is proposed. The size of the local aperture (the scale) needed for the orientation representation induces in general a lost of spatial acuity, or blur. Our construction permits a compensation of the blur by a reconstruction procedure. For this purpose, a special scale-dependent orientation bundle (map of the visual space into function of both position and orientation) is build from the local Gaussian-derivatives jet of a scalar image. In this construction there is an invertible relation between the orientation bundle and the original image. This invertible transformation is used to regain the original acuity in the spatial domain after analyzing orientation features at any given scale. The approach turns out to be highly effective for the detection of elongated structures and for removal of elongated artifacts in 2D images.	Univ Utrecht Hosp, Image Sci Inst, NL-3508 GA Utrecht, Netherlands	Utrecht University; Utrecht University Medical Center	Kalitzin, SN (corresponding author), Univ Utrecht Hosp, Image Sci Inst, HP E01-334,POB 85500, NL-3508 GA Utrecht, Netherlands.		Romenij, Bart M. ter Haar/A-5323-2013; Viergever, Max A/J-1215-2014	Kalitzin, Stiliyan/0000-0002-7028-7778				CONWELL JF, 1990, GROUP THEORY PHYSICS, V2; Florack L, 1996, INT J COMPUT VISION, V18, P61, DOI 10.1007/BF00126140; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; GREENSPAN H, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P222, DOI 10.1109/CVPR.1994.323833; KARASARIDIS N, 1996, P ICASSP96 ATL GA; Lee TS, 1996, IEEE T PATTERN ANAL, V18, P959, DOI 10.1109/34.541406; LINDEBERG T, 1994, KLUWER INT SERIES EN; MICHAELIS M, 1995, THESIS TECHNISCHE FA; Miller W., 1972, SYMMETRY GROUPS THEI; OR YH, 1996, STANCSTN9628; OR YH, 1996, STANCSTN9633; PERONA P, 1994, IEEE CVPR, P222; Romeny B. M. T. H., 1997, FRONT END VISION MUL; Segman J., 1993, Journal of Mathematical Imaging and Vision, V3, P51, DOI 10.1007/BF01248403; SIMONCELLI E, 1991, IEEE T INFORMATION T, V32, P587; SIMONCELLI E, 1994, 1 INT C IM PROC TEX; Simoncelli EP, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P185, DOI 10.1109/ICIP.1996.560415; SIMONCELLI EP, 1996, IEEE T IMAGE PROCESS; SIMONCELLI EP, 1995, 2 ANN IEEE INT C IM; Zibulski M, 1997, APPL COMPUT HARMON A, V4, P188, DOI 10.1006/acha.1997.0209	21	13	14	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1999	31	2-3					145	158		10.1023/A:1008013815039	http://dx.doi.org/10.1023/A:1008013815039			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	209NA					2022-12-18	WOS:000081053100004
J	Rieger, JH				Rieger, JH			Topographical properties of generic images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							EDGE-DETECTION; SURFACES; SINGULARITIES	Topographical curves in images are defined by certain extremality conditions involving the gradient of the greyvalue function. Curves extracted by some edge operators and watersheds studied in geography are both examples of such topographical curves. In the first part of this article we list the possible geometrical features of certain topographical curves-such as intersection points, singular points, endpoints, curvature extrema and inflections-for generic images and for generic 1-parameter families of, linearly or non-linearly, diffused images (intuitively, generic phenomena arise with probability 1). In a second experimental part the same topographical curves are computed in discrete images and in Gaussian blurred families of images. It turns out that the geometrical features of the smooth classification also figure in these discrete approximations. The consequences of these results for edge-linking algorithms are briefly discussed.			Rieger, JH (corresponding author), UNIV HAMBURG, FB INFORMAT, VOGT KOLLN STR 30, D-22527 HAMBURG, GERMANY.							ALVAREZ L, 1992, SIAM J NUMER ANAL, V29, P845, DOI 10.1137/0729052; BANCHOFF T, 1982, PITMAN RES NOTES MAT, V55; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; DAMON J, 1995, J DIFFER EQUATIONS, V115, P368, DOI 10.1006/jdeq.1995.1019; DAMON J, 1995, IN PRESS ARCH RAT ME; DECHAMP B, 1854, CR HEBD ACAD SCI, V39, P647; DESAINTVENANT, 1852, B SOC PHILOMATH 0306; DREWNIOK C, 1994, INT J REMOTE SENS, V15, P3743, DOI 10.1080/01431169408954356; DUPUISTORCY, 1808, J EC POLYT, V7, P262; GUEZIEC A, 1992, P 4 ICCV BERL, P650; IZUMIYA S, 1994, B LOND MATH SOC, V26, P69, DOI 10.1112/blms/26.1.69; JORDAN C, 1872, CR HEBD ACAD SCI, V74, P1457; KERGOSIEN YL, 1980, CR ACAD SCI A MATH, V290, P705; KOENDERINK JJ, 1994, PATTERN RECOGN LETT, V15, P439, DOI 10.1016/0167-8655(94)90134-1; KOENDERINK JJ, 1979, BIOL CYBERN, V33, P151, DOI 10.1007/BF00337293; LANDIS EE, 1981, FUNCT ANAL APPL+, V15, P103, DOI 10.1007/BF01082281; Maxwell J.C., 1870, PHILOS MAGAZ J SCI, V40, P421; MAXWELL JC, 1890, SCI PAPERS, V2, P233; MULLER E, 1919, LEHRBUCH DARSTELLEND; Porteous I., 1971, J DIFFER GEOM, V5, P543; PORTEOUS IR, 1983, P SYMP PURE MATH, V40, P379; Rieger J. H., 1995, Journal of Mathematical Imaging and Vision, V5, P207, DOI 10.1007/BF01248372; RIEGER JH, 1992, BIOL CYBERN, V66, P497, DOI 10.1007/BF00204114; Rothe R., 1915, SITZUNGSBERICHTE BER, V14, P51; TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769	25	13	13	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	1997	23	1					79	92		10.1023/A:1007915908780	http://dx.doi.org/10.1023/A:1007915908780			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XK559					2022-12-18	WOS:A1997XK55900004
J	Niessen, WJ; Romeny, BMT; Florack, LMJ; Viergever, MA				Niessen, WJ; Romeny, BMT; Florack, LMJ; Viergever, MA			A general framework for geometry-driven evolution equations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							HAMILTON-JACOBI EQUATIONS; SCALE-SPACE; EDGE-DETECTION; ANISOTROPIC DIFFUSION; NONLINEAR DIFFUSION; VISCOSITY SOLUTIONS; CURVE EVOLUTION; SHOCK FILTERS; PLANE-CURVES; ALGORITHMS	This paper presents a general framework to generate multi-scale representations of image data. The process is considered as an initial value problem with an acquired image as initial condition and a geometrical invariant as ''driving force'' of an evolutionary process. The geometrical invariants are extracted using the family of Gaussian derivative operators. These operators naturally deal with scale as a free parameter and solve the ill-posedness problem of differentiation. Stability requirements for numerical approximation of evolution schemes using Gaussian derivative operators are derived and establish an intuitive connection between the allowed time-step and scale. This approach has been used to generalize and implement a variety of nonlinear diffusion schemes. Results on test images and medical images are shown.			Niessen, WJ (corresponding author), UNIV UTRECHT HOSP, IMAGE SCI INST, NL-3584 CX UTRECHT, NETHERLANDS.		Romenij, Bart M. ter Haar/A-5323-2013; Viergever, Max A/J-1215-2014	Niessen, Wiro/0000-0002-5822-1995				ALVAREZ L, 1993, ARCH RATION MECH AN, V123, P199, DOI 10.1007/BF00375127; ALVAREZ L, 1992, SIAM J NUMER ANAL, V29, P845, DOI 10.1137/0729052; ALVAREZ L, 1994, SIAM J NUMER ANAL, V31, P590, DOI 10.1137/0731032; AMES WF, 1972, NONLINEAR PARTIAL DI, V1; ANGENENT S, 1991, J DIFFER GEOM, V33, P601; ANGENENT S, 1991, ANN MATH, V133, P171, DOI 10.2307/2944327; BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749; BROCKETT RW, 1992, INT C AC SPEECH SIGN; CASELLES V, 1992, 9210 U PAR DAUPH; CATTE F, 1992, SIAM J NUMER ANAL, V29, P182, DOI 10.1137/0729012; CHEN MH, 1989, IEEE T PATTERN ANAL, V11, P694, DOI 10.1109/34.192464; COHIGNAC T, 1993, AFFINE MORPHOLOGICAL; CRANDALL MG, 1983, T AM MATH SOC, V277, P1, DOI 10.2307/1999343; Epstein C L, 1987, WAVE MOTION THEORY M; FALCONE M, 1994, NUMER MATH, V67, P315, DOI 10.1007/s002110050031; Florack L. M. J., 1993, Journal of Mathematical Imaging and Vision, V3, P327, DOI 10.1007/BF01664793; Florack L. M. J., 1994, Journal of Mathematical Imaging and Vision, V4, P325, DOI 10.1007/BF01262401; FLORACK LMJ, 1992, IMAGE VISION COMPUT, V10, P376, DOI 10.1016/0262-8856(92)90024-W; FLORACK LMJ, 1994, NATO ASI SERIES F, V126, P651; GAGE M, 1986, J DIFFER GEOM, V23, P69; GAGE ME, 1984, INVENT MATH, V76, P357, DOI 10.1007/BF01388602; GERIG G, 1992, IEEE T MED IMAGING, V11, P221, DOI 10.1109/42.141646; GRAYSON MA, 1987, J DIFFER GEOM, V26, P285; HILLEN B, 1993, ELSEVIERS INTERACTIV; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; KIMIA BB, 1990, LECT NOTES COMPUT SC, V427, P402; KIMIA BB, 1990, THSIS MCGILL U; LeVeque RJ., 1992, LECT MATH ETH ZURICH, V2nd; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591; MURAGOS P, 1987, OPTICAL ENG, V26, P623; NIESSEN W, 1994, GEOMETRY DRIVEN DIFF, P393; NIESSEN WJ, 1994, IEEE C COMP VIS PATT; Olver P., 1994, GEOMETRY DRIVEN DIFF, V1, P255; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; OSHER S, 1991, SIAM J NUMER ANAL, V28, P907, DOI 10.1137/0728049; OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Romeny B.M., 1994, GEOMETRY DRIVEN DIFF; Romeny B.M.H., 1994, P 1 IEEE INT C IM PR, P21; RUDIN L, 1987, 525087 CALTECH CS DE; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; SAPIRO G, 1993, PATTERN RECOGN, V26, P1363, DOI 10.1016/0031-3203(93)90142-J; SAPIRO G, 1993, INT J COMPUT VISION, V11, P25, DOI 10.1007/BF01420591; SAPIRO G, 1994, J FUNCT ANAL, V119, P79, DOI 10.1006/jfan.1994.1004; SAPIRO G, 1994, IEEE IMAGE PROC, P472, DOI 10.1109/ICIP.1994.413615; Schwartz L., 1966, THEORIE DISTRIBUTION; Sethian J.A., 1982, THESIS U CALIFORNIA; SETHIAN JA, 1985, COMMUN MATH PHYS, V101, P487, DOI 10.1007/BF01210742; SETHIAN JA, 1990, J DIFFER GEOM, V31, P131; VANDENBOOMGAARD R, 1994, IEEE T PATTERN ANAL, V16, P1101, DOI 10.1109/34.334389; VINCKEN KL, 1991, 3DCV9120 UTR U; WEICKERT J, 1994, 110 LAB TECHN; WHITAKER R, 1994, GEOMETRY DRIVEN DIFF, P93	55	13	14	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1997	21	3					187	205		10.1023/A:1007995731951	http://dx.doi.org/10.1023/A:1007995731951			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WN532					2022-12-18	WOS:A1997WN53200002
J	Freeman, WT				Freeman, WT			Exploiting the generic viewpoint assumption	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							VISUAL-CORTEX; IMAGES; SHAPE; DISTRIBUTIONS; ORIENTATION; SURFACES; RECOVERY	The ''generic viewpoint'' assumption states that an observer is not in a special position relative to the scene. It is commonly used to disqualify scene interpretations that assume special viewpoints, following a binary decision that the viewpoint was either generic or accidental. In this paper, we apply Bayesian statistics to quantify the probability of a view, and so derive a useful tool to estimate scene parameters. Generic variables can include viewpoint, object orientation, and lighting position. By considering the image as a (differentiable) function of these variables, we derive the probability that a set of scene parameters created a given image. This scene probability equation has three terms: the fidelity of the scene interpretation to the image data; the prior probability of the scene interpretation; and a new genericity term, which favors scenes likely to produce the observed image. The genericity term favors image interpretations for which the image is stable with respect to changes in the generic variables. It results from integration over the generic variables, using a low-noise approximation common in Bayesian statistics. This approach may increase the scope and accuracy of scene estimates. It applies to a range of vision problems. We show shape from shading examples, where we rank shapes or reflectance functions in cases where these are otherwise unknown. The rankings agree with the perceived values.										ALBERT MK, 1995, GEOMETRIC REPRESENTATIONS OF PERCEPTUAL PHENOMENA, P95; ALBRECHT DG, 1991, VISUAL NEUROSCI, V7, P531, DOI 10.1017/S0952523800010336; BELHUMEUR PN, 1996, PERCEPTION BAYESIAN; Berger J.O., 1985, STAT DECISION THEORY, P74; Bichsel M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P459, DOI 10.1109/CVPR.1992.223150; BIEDERMAN I, 1985, COMPUT VISION GRAPH, V32, P29, DOI 10.1016/0734-189X(85)90002-7; BINFORD TO, 1981, ARTIF INTELL, V17, P205, DOI 10.1016/0004-3702(81)90025-4; Bleistein N., 1987, ASYMPTOTIC EXPANSION; Box G.E.P., 2011, BAYESIAN INFERENCE S; BOX GEP, 1964, BIOMETRIKA, V51, P153, DOI 10.1093/biomet/51.1-2.153; BRAINARD DH, 1994, P SPIE, V2179; BROOKS MJ, 1992, INT J COMPUT VISION, V7, P119, DOI 10.1007/BF00128131; BROOKS MJ, 1979, SHAPE SHADING, pCH3; BULTHOFF HH, 1991, J THEORETICAL BIOL, V2; CARANDINI M, 1994, SCIENCE, V264, P1333, DOI 10.1126/science.8191289; COOK RL, 1981, SIGGRAPH81; Cornsweet T., 1970, VISUAL PERCEPTION; Darrell T., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P112, DOI 10.1109/ICCV.1990.139506; DICKINSON SJ, 1992, IEEE T PATTERN ANAL, V14, P174, DOI 10.1109/34.121788; FISHER RA, 1959, STATISTICAL METHODS; Freeman W. T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P347, DOI 10.1109/ICCV.1993.378193; FREEMAN WT, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P210, DOI 10.1109/ICCV.1995.466784; FREEMAN WT, 1996, PERCEPTION BAYESIAN; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GRIMSON WEL, 1984, COMPUT VISION GRAPH, V28, P19, DOI 10.1016/0734-189X(84)90137-3; GULL SF, 1989, FUND THEOR, V36, P53; GULL SF, 1988, MAXIMUM ENTROPY BAYE, V1; HEEGER DJ, 1992, SPATIAL VISION HUMAN; Horn B.K.P., 1989, SHAPE SHADING; HORN BKP, 1993, IEEE T PATTERN ANAL, V15, P166, DOI 10.1109/34.192489; HORN BKP, 1978, 490 MIT ART INT LAB; HORN BKP, 1989, 1105 MIT ART INT LAB; Jeffreys H, 1961, THEORY PROBABILITY; JEPSON AD, 1992, SPATIAL VISION HUMAN; JOHNSON RA, 1970, ANN MATH STAT, V41, P851, DOI 10.1214/aoms/1177696963; KERSTEN D, 1991, COMPUTATIONAL MODELS, pCH15; Knill DC, 1996, PERCEPTION BAYESIAN; KOENDERINK JJ, 1979, BIOL CYBERN, V32, P211, DOI 10.1007/BF00337644; Laplace P.S., 1820, THEORIE ANAL PROBABI; Leclerc Y. G., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P552, DOI 10.1109/CVPR.1991.139752; LECLERC YG, 1989, INT J COMPUT VISION, V3, P73, DOI 10.1007/BF00054839; LEE CH, 1989, SHAPE SHADING, pCH11; LINDLEY DV, 1972, BAYESIAN STATISTICS; LOWE DG, 1985, IEEE T PATTERN ANAL, V7, P320, DOI 10.1109/TPAMI.1985.4767660; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; NAKAYAMA K, 1992, SCIENCE, V257, P1357, DOI 10.1126/science.1529336; Papoulis A., 2002, PROBABILITY RANDOM V; PENTLAND AP, 1990, INT J COMPUT VISION, V4, P107, DOI 10.1007/BF00127812; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P170, DOI 10.1109/TPAMI.1984.4767501; PENTLAND AP, 1990, P 3 INT C COMP VIS; PENTLAND T, 1985, NATURE, V317, P114; Press W.H., 1992, NUMERICAL RECIPES C, V2; RICHARDS WA, 1987, J OPT SOC AM A, V4, P1168, DOI 10.1364/JOSAA.4.001168; SCHREIBER WF, 1986, FUNDAMENTALS ELECT I; SKILLING J, 1989, FUND THEOR, V36, P45; SZELISKI R, 1989, BAYESIAN MODELING UN; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P413, DOI 10.1109/TPAMI.1986.4767807; Tikhonov A. N., 1977, SOLUTIONS ILL POSED; WEINSHALL D, 1994, P 3 EUR C COMP VIS S; WITKIN AP, 1981, ARTIF INTELL, V17, P17, DOI 10.1016/0004-3702(81)90019-9; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Yuille A. L., 1996, PERCEPTION BAYESIAN, P123, DOI DOI 10.1017/CBO9780511984037.006; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658; HUMAN VISION VISUAL, V5	66	13	13	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1996	20	3					243	261						19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VW629					2022-12-18	WOS:A1996VW62900005
J	PROKOPOWICZ, PN; COOPER, PR				PROKOPOWICZ, PN; COOPER, PR			THE DYNAMIC RETINA - CONTRAST AND MOTION DETECTION FOR ACTIVE VISION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							PERCEPTION	This paper presents an efficient, biologically-inspired early vision architecture, the dynamic retina, that is well-suited to highly active and responsive vision platforms. The dynamic retina exploits normally undesirable camera motion as a necessary step in detecting image contrast, by using dynamic receptive fields instead of traditional spatial-neighborhood operators. We analyze the continuous miniature ''noise'' movements made by active imaging systems, and show that they can be exploited to detect contrast. We then develop an appropriate photoreceptor response function, based on light-adaptation models for vertebrate receptors. Together, the movements and response function over time compute image contrast. The dynamic retina is also useful for motion analysis, since moving objects processed by the system leave a clear signature from which motion parameters can be extracted. Results from a number of experiments with real video sequences demonstrate the effectiveness of the system for both contrast detection and motion analysis.			PROKOPOWICZ, PN (corresponding author), NORTHWESTERN UNIV,INST LEARNING SCI,EVANSTON,IL 60201, USA.							ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BOAHEN KA, 1992, NEURAL INFORMATION P, V4; BROWN P, 1994, UNPUB VELOCITY SINGL; Carpenter RHS, 1977, MOVEMENTS EYES; DELBRUCK T, 1989, ADV NEURAL INFORMATI, V1; FELDMAN JA, 1988, 88011 INT COMP SCI I; Fleet DJ, 1992, MEASUREMENT IMAGE VE; HEEGER DJ, 1992, SPATIAL VISION HUMAN; HEEGER DJ, 1988, INT J COMPUT VISION, P279; Hildreth E., 1984, MEASUREMENT VISUAL M; HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455; JAIN R, 1977, 5TH P INT JOINT C AR, P612; JAIN R, 1979, IEEE PAMI, V1, P236; KAHN P, 1988, P COMP VIS PAT REC; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; MAHOWALD MA, 1991, SCI AM, V264, P76, DOI 10.1038/scientificamerican0591-76; MAHOWALD MA, 1991, VISUAL INFORMATION P, V1473; MANN J, 1991, VISUAL INFORMATION P, V1473; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Marr D., 1982, VISION; MEAD C, 1989, ANALOG VLSI IMPLEMEN, P177; MEAD CA, 1988, NEURAL NETWORKS, V1, P91, DOI 10.1016/0893-6080(88)90024-X; NAKA KI, 1966, J PHYSIOL-LONDON, V185, P587, DOI 10.1113/jphysiol.1966.sp008003; NELSON RC, 1991, INT J COMPUT VISION, V7, P33, DOI 10.1007/BF00130488; NORMANN RA, 1979, J PHYSIOL-LONDON, V286, P491, DOI 10.1113/jphysiol.1979.sp012633; ROBINSON DA, 1987, INVEST OPHTH VIS SCI, V28, P1912; Shapley R., 1984, PROG RETIN RES, V3, P263, DOI [DOI 10.1016/0278-4327(84)90011-7, 10.1016/0278-4327(84)90011-7]; ULLMAN S, 1981, COMPUTER, V14, P57, DOI 10.1109/C-M.1981.220564; WHITTLE P, 1969, VISION RES, V9, P1095, DOI 10.1016/0042-6989(69)90050-9; Yarbus A. L., 1967, EYE MOVEMENTS VISION, P171	32	13	13	1	6	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1995	16	3					191	204		10.1007/BF01539626	http://dx.doi.org/10.1007/BF01539626			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TC768					2022-12-18	WOS:A1995TC76800001
J	Ramakrishnan, SK; Jayaraman, D; Grauman, K				Ramakrishnan, Santhosh K.; Jayaraman, Dinesh; Grauman, Kristen			An Exploration of Embodied Visual Exploration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual navigation; Visual exploration; Learning for navigation		Embodied computer vision considers perception for robots in novel, unstructured environments. Of particular importance is the embodied visual exploration problem: how might a robot equipped with a camera scope out a new environment? Despite the progress thus far, many basic questions pertinent to this problem remain unanswered: (i) What does it mean for an agent to explore its environment well? (ii) Which methods work well, and under which assumptions and environmental settings? (iii) Where do current approaches fall short, and where might future work seek to improve? Seeking answers to these questions, we first present a taxonomy for existing visual exploration algorithms and create a standard framework for benchmarking them. We then perform a thorough empirical study of the four state-of-the-art paradigms using the proposed framework with two photorealistic simulated 3D environments, a state-of-the-art exploration architecture, and diverse evaluation metrics. Our experimental results offer insights and suggest new performance metrics and baselines for future work in visual exploration. Code, models and data are publicly available.	[Ramakrishnan, Santhosh K.; Grauman, Kristen] Univ Texas Austin, Austin, TX 78712 USA; [Ramakrishnan, Santhosh K.; Grauman, Kristen] Facebook AI Res, Austin, TX 78712 USA; [Jayaraman, Dinesh] Univ Penn, Philadelphia, PA 19104 USA; [Jayaraman, Dinesh] Facebook AI Res, Menlo Pk, CA USA	University of Texas System; University of Texas Austin; Facebook Inc; University of Pennsylvania; Facebook Inc	Ramakrishnan, SK (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.; Ramakrishnan, SK (corresponding author), Facebook AI Res, Austin, TX 78712 USA.	sramakrishnan@utexas.edu; dineshj@seas.upenn.edu; grauman@cs.utexas.edu		Ramakrishnan, Santhosh Kumar/0000-0003-2833-7038	NSF AI Institute on Foundations of Machine Learning; DARPA Lifelong Learning Machines; GCP Research Credits Program	NSF AI Institute on Foundations of Machine Learning; DARPA Lifelong Learning Machines; GCP Research Credits Program	UT Austin is supported in part by the NSF AI Institute on Foundations of Machine Learning, DARPA Lifelong Learning Machines, and the GCP Research Credits Program. The authors thank Ziad-Al Halah for the helpful discussions. The authors thank Tao Chen for clarifying details about the exploration architecture implementation.	ALOIMONOS J, 1987, INT J COMPUT VISION, V1, P333; Ammirato, 2016, ICRA; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Anderson Peter, 2018, EVALUATION EMBODIED, V1; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; Batra Dhruv, 2020, OBJECTNAV REV EVALUA, P3; Bellemare M., 2016, NEURIPS; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Burda Y., 2018, ARXIV180804355; Carion Nicolas, 2020, EUR C COMP VIS ECCV; Cassandra AR, 1996, IROS 96 - PROCEEDINGS OF THE 1996 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - ROBOTIC INTELLIGENCE INTERACTING WITH DYNAMIC WORLDS, VOLS 1-3, P963, DOI 10.1109/IROS.1996.571080; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; Chaplot Devendra Singh, 2019, INT C LEARN REPR; Chen Ting, 2019, ICLR; Chung J., 2014, ARXIV14123555; Das A, 2018, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2018.00008; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Duan Y, 2016, PR MACH LEARN RES, V48; Fang K, 2019, PROC CVPR IEEE, P538, DOI 10.1109/CVPR.2019.00063; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024; Goyal P, 2019, IEEE I CONF COMP VIS, P6400, DOI 10.1109/ICCV.2019.00649; Gupta, 2017, ARXIV171208125; Gupta, 2018, GAMES BRINGING EXPLO; Gupta S, 2017, PROC CVPR IEEE, P7272, DOI 10.1109/CVPR.2017.769; Haber N, 2018, ADV NEUR IN, V31; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Henriques JF, 2018, PROC CVPR IEEE, P8476, DOI 10.1109/CVPR.2018.00884; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jayaraman D, 2018, PROC CVPR IEEE, P1238, DOI 10.1109/CVPR.2018.00135; Jayaraman D, 2019, IEEE T PATTERN ANAL, V41, P1601, DOI 10.1109/TPAMI.2018.2840991; Kadian A., 2019, ARXIV191206321; Kay W., 2017, ARXIV PREPRINT ARXIV; Kingma D.P, P 3 INT C LEARNING R; Kolve Eric, 2017, ARXIV171205474; Kostrikov I., 2018, PYTORCH IMPLEMENTATI; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lopes M., 2012, ADV NEURAL INFORM PR, V1; Lovejoy WS, 1991, ANN OPER RES, V28, P47, DOI 10.1007/BF02055574; Mahmood A. R., 2018, PROC 2 C ROBOT LEARN, P561; Malmir M., 2015, BMVC; Mishkin D., 2019, ARXIV190110915; Narasimhan, 2020, ARXIV PREPRINT ARXIV; Ostrovski G, 2017, PR MACH LEARN RES, V70; Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271; Pathak D, 2017, IEEE COMPUT SOC CONF, P488, DOI 10.1109/CVPRW.2017.70; Pathak D, 2019, PR MACH LEARN RES, V97; Qi W., 2020, ARXIV PREPRINT ARXIV; Ramakrishnan SK, 2018, LECT NOTES COMPUT SC, V11216, P424, DOI 10.1007/978-3-030-01258-8_26; Ramakrishnan SK, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aaw6326; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Savarese S., 2017, JOINT 2D 3D SEMANTIC; Savinov, 2018, ARXIV181002274; Savva M, 2019, IEEE I CONF COMP VIS, P9338, DOI 10.1109/ICCV.2019.00943; Savva Manolis, 2017, ARXIV171203931; SCHMIDHUBER J, 1991, IEEE IJCNN, P1458, DOI 10.1109/IJCNN.1991.170605; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Seifi, 2019, ARXIV PREPRINT ARXIV; Soomro K., 2012, ARXIV; Stachniss C., 2005, ROBOTICS SCI SYSTEMS, V2, P65, DOI DOI 10.15607/RSS.2005.I.009; Straub Julian, 2019, ARXIV190605797; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Sun Y., 2011, INT C ART GEN INT, P41, DOI DOI 10.1007/978-3-642-22887-2_5; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tang HL, 2017, INT CONF MEAS, P1, DOI [10.1109/ICMTMA.2017.0009, 10.1109/ICMTMA.2017.8]; Tsotsos J. K., 1992, COMPUTER VISION PATT; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wijmans E, 2019, PROC CVPR IEEE, P6652, DOI 10.1109/CVPR.2019.00682; Xia F, 2018, PROC CVPR IEEE, P9068, DOI 10.1109/CVPR.2018.00945; Yamauchi B, 1997, 1997 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION - CIRA '97, PROCEEDINGS, P146, DOI 10.1109/CIRA.1997.613851; Yang J, 2019, ARXIV PREPRINT ARXIV; Yang ZW, 2021, ANIM BIOTECHNOL, V32, P67, DOI 10.1080/10495398.2019.1653901; Zamir AR, 2016, LECT NOTES COMPUT SC, V9907, P535, DOI 10.1007/978-3-319-46487-9_33; Zhang, 2016, ARXIV PREPRINT ARXIV; Zhu Y, 2017, IEEE I CONF COMP VIS, P483, DOI 10.1109/ICCV.2017.60	77	12	12	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1616	1649		10.1007/s11263-021-01437-z	http://dx.doi.org/10.1007/s11263-021-01437-z		FEB 2021	34	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000622676800001
J	Mostafavi, M; Wang, L; Yoon, KJ				Mostafavi, Mohammad; Wang, Lin; Yoon, Kuk-Jin			Learning to Reconstruct HDR Images from Events, with Applications to Depth and Flow Prediction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Event camera; Conditional generative adversarial network; Image and video reconstruction; High dynamic range; High frame rate; Optical flow; Depth	QUALITY ASSESSMENT; VISION	Event cameras have numerous advantages over traditional cameras, such as low latency, high temporal resolution, and high dynamic range (HDR). We initially investigate the potential of creating intensity images/videos from an adjustable portion of the event data stream via event-based conditional generative adversarial networks (cGANs). Using the proposed framework, we further show the versatility of our method in directly handling similar supervised tasks, such as optical flow and depth prediction. Stacks of space-time coordinates of events are used as the inputs while the proposed framework is trained to predict either the intensity images, optical flows, or depth outputs according to the target task. We further demonstrate the unique capability of our approach in generating HDR images even under extreme illumination conditions, creating non-blurred images under rapid motion, and generating very high frame rate videos up to the temporal resolution of event cameras. The proposed framework is evaluated using a publicly available real-world dataset and a synthetic dataset we prepared by utilizing an event camera simulator.	[Mostafavi, Mohammad] GIST, Comp Vis Lab, Gwangju, South Korea; [Wang, Lin; Yoon, Kuk-Jin] Korea Adv Inst Sci & Technol, Visual Intelligence Lab, Daejeon, South Korea	Gwangju Institute of Science & Technology (GIST); Korea Advanced Institute of Science & Technology (KAIST)	Yoon, KJ (corresponding author), Korea Adv Inst Sci & Technol, Visual Intelligence Lab, Daejeon, South Korea.	mostafavi@gist.ac.kr; wanglin@kaist.ac.kr; kjyoon@kaist.ac.kr		Mostafavi Isfahani, Sayed Mohammad/0000-0002-5883-3844; yun, gugjin/0000-0002-1634-2756	National Research Foundation of Korea (NRF) - Korea government (MSIT) [NRF 2018R1A2B3008640]; Next Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT [NRF-2017M3C4A7069369]; Institute of Information & Communications Technology Planning & Evaluation (IITP) - Korea government (MSIT) [2020-0-00440]	National Research Foundation of Korea (NRF) - Korea government (MSIT); Next Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT; Institute of Information & Communications Technology Planning & Evaluation (IITP) - Korea government (MSIT)	This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF 2018R1A2B3008640), the Next Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT (NRF-2017M3C4A7069369) and Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by Korea government (MSIT) (No.2020-0-00440, Development of Artificial Intelligence Technology that Continuously Improves Itself as the Situation Changes in the Real World).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2019, RVIZ 3D VISUALIZATIO; Atapour-Abarghouei Amir, 2018, P IEEE C COMP VIS PA, P2800, DOI DOI 10.1109/CVPR.2018.00296; BARDOW P, 2016, PROC CVPR IEEE, P884, DOI DOI 10.1109/CVPR.2016.102; Barua S., 2016, P IEEE REAL TIM SYST, P1; Benosman R, 2012, NEURAL NETWORKS, V27, P32, DOI 10.1016/j.neunet.2011.11.001; Binas J., 2017, PROC ICML; Chen NFY, 2018, IEEE COMPUT SOC CONF, P757, DOI 10.1109/CVPRW.2018.00107; Community B.O., 2018, BLENDER 3D MODELLING; Cook M, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P770, DOI 10.1109/IJCNN.2011.6033299; Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413; Gallego G, 2018, PROC CVPR IEEE, P3867, DOI 10.1109/CVPR.2018.00407; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Karacan L., 2016, ARXIV161200215; Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21; Kim Hanme, 2014, BRIT MACH VIS C BMVC, DOI [10.5244/C.28.26, DOI 10.5244/C.28.26]; Kingma D.P, P 3 INT C LEARNING R; Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337; Maqueda AI, 2018, PROC CVPR IEEE, P5419, DOI 10.1109/CVPR.2018.00568; Mathieu M., 2016, INT C LEARN REPR ICL; Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050; Moeys DP, 2017, IEEE INT SYMP CIRC S, P638; Moeys DP, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/EBCCSP.2016.7605233; Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115; Munda G, 2018, INT J COMPUT VISION, V126, P1381, DOI 10.1007/s11263-018-1106-2; Nguyen A., REALTIME 6DOF POSE R; Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398; Rebecq H, 2018, INT J COMPUT VISION, V126, P1394, DOI 10.1007/s11263-017-1050-6; Rebecq H, 2017, IEEE ROBOT AUTOM LET, V2, P593, DOI 10.1109/LRA.2016.2645143; Rebecq Henri, 2018, C ROB LEARN, P2; Reinbacher C., 2016, P BRIT MACH VIS C; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3; Scheerlinck C., 2019, P IEEE C COMP VIS PA; Scheerlinck Cedric, 2018, AS C COMP VIS ACCV; Shedligeri Prasan A, 2018, ARXIV180506140; van der Ouderaa TFA, 2019, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR.2019.00485; Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang Z., 2020, IEEE T PATTERN ANAL; Ye C., 2018, ARXIV180908625; Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhu A. Z., 2018, IEEE ROBOT AUTOM LET, V3, P2032, DOI [DOI 10.1109/LRA.2018.2800793, 10.1109/lra.2018.2800793]; Zhu AZ, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	53	12	12	4	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					900	920		10.1007/s11263-020-01410-2	http://dx.doi.org/10.1007/s11263-020-01410-2		JAN 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000605122800003
J	Zhang, ZC; Sattler, T; Scaramuzza, D				Zhang, Zichao; Sattler, Torsten; Scaramuzza, Davide			Reference Pose Generation for Long-term Visual Localization via Learned Features and View Synthesis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual localization; Benchmark construction; Learned local features	PLACE RECOGNITION	Visual Localization is one of the key enabling technologies for autonomous driving and augmented reality. High quality datasets with accurate 6 Degree-of-Freedom (DoF) reference poses are the foundation for benchmarking and improving existing methods. Traditionally, reference poses have been obtained via Structure-from-Motion (SfM). However, SfM itself relies on local features which are prone to fail when images were taken under different conditions, e.g., day/night changes. At the same time, manually annotating feature correspondences is not scalable and potentially inaccurate. In this work, we propose a semi-automated approach to generate reference poses based on feature matching between renderings of a 3D model and real images via learned features. Given an initial pose estimate, our approach iteratively refines the pose based on feature matches against a rendering of the model from the current pose estimate. We significantly improve the nighttime reference poses of the popular Aachen Day-Night dataset, showing that state-of-the-art visual localization methods perform better (up to 47%) than predicted by the original reference poses. We extend the dataset with new nighttime test images, provide uncertainty estimates for our new reference poses, and introduce a new evaluation criterion. We will make our reference poses and our framework publicly available upon publication.	[Zhang, Zichao; Scaramuzza, Davide] Univ Zurich, Robot & Percept Grp, Zurich, Switzerland; [Sattler, Torsten] Czech Tech Univ, Czech Inst Informat Robot & Cybernet, Prague, Czech Republic	University of Zurich; Czech Technical University Prague	Zhang, ZC (corresponding author), Univ Zurich, Robot & Percept Grp, Zurich, Switzerland.	zzhang@ifi.uzh.ch; torsten.sattler@cvut.cz; sdavide@ifi.uzh.ch	Sattler, Torsten/AAM-3155-2021	Zhang, Zichao/0000-0002-6526-0706	University of Zurich	University of Zurich	Open access funding provided by University of Zurich.	Aiger D., 2019, 35 INT S COMP GEOM S; Albl C, 2016, PROC CVPR IEEE, P3355, DOI 10.1109/CVPR.2016.365; Alcantarilla PF, 2011, IEEE INT CONF ROBOT; Arandjelovi R., 2014, ACCV; Arandjelovi R., 2016, CVPR; Armagan A, 2017, PROC CVPR IEEE, P4590, DOI 10.1109/CVPR.2017.488; Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591009; Badino H, 2011, IEEE INT VEH SYM, P794, DOI 10.1109/IVS.2011.5940504; Balntas V., 2019, SILDA SCAPE IMPERIAL; Balntas V, 2018, LECT NOTES COMPUT SC, V11218, P782, DOI 10.1007/978-3-030-01264-9_46; Balntas Vassileios, 2016, BMVC, V2, DOI DOI 10.5244/C.30.119; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Benbihi A, 2019, IEEE I CONF COMP VIS, P7939, DOI 10.1109/ICCV.2019.00803; Brachmann E, 2019, IEEE I CONF COMP VIS, P7524, DOI 10.1109/ICCV.2019.00762; Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489; Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267; Brachmann Eric, 2020, ARXIV200212324; Brahmbhatt S, 2018, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2018.00277; Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54; Budvytis I, 2019, BMVC, P31; Camposeco Federico, 2019, IEEE C COMP VIS PATT; Cao S, 2014, PROC CVPR IEEE, P461, DOI 10.1109/CVPR.2014.66; Cao S, 2013, PROC CVPR IEEE, P700, DOI 10.1109/CVPR.2013.96; Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638; Castle R, 2008, TWELFTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P15, DOI 10.1109/ISWC.2008.4911577; Cavallari T., 2019, IEEE T PATTERN ANAL; Cavallari T, 2019, INT CONF 3D VISION, P564, DOI 10.1109/3DV.2019.00068; Cavallari T, 2017, PROC CVPR IEEE, P218, DOI 10.1109/CVPR.2017.31; Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610; Cheng WT, 2019, IEEE I CONF COMP VIS, P1032, DOI 10.1109/ICCV.2019.00112; Choudhary S, 2012, LECT NOTES COMPUT SC, V7576, P130, DOI 10.1007/978-3-642-33715-4_10; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; Clark R, 2017, PROC CVPR IEEE, P2652, DOI 10.1109/CVPR.2017.284; Crandall D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3001, DOI 10.1109/CVPR.2011.5995626; Dai Angela, 2017, ACM T GRAPHICS 2017, P1, DOI DOI 10.1145/3054739; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060; Ding MY, 2019, IEEE I CONF COMP VIS, P2871, DOI 10.1109/ICCV.2019.00296; Donoser M., 2014, CVPR; Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828; DuToit Ryan C., 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P6253, DOI 10.1109/ICRA.2017.7989741; Dymczyk M., 2018, IEEE ROBOT AUTOM LET, V3, P1418, DOI [DOI 10.1109/LRA.2018.2800113, 10.1109/LRA.2018.2800113]; Dymczyk M, 2015, IEEE INT CONF ROBOT, P2767, DOI 10.1109/ICRA.2015.7139575; Ebel P, 2019, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2019.00034; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Garg S, 2022, INT J ROBOT RES, V41, P573, DOI 10.1177/0278364919839761; Germain H, 2019, INT CONF 3D VISION, P513, DOI 10.1109/3DV.2019.00063; HARALICK RM, 1994, INT J COMPUT VISION, V13, P331, DOI 10.1007/BF02028352; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He R, 2018, INT CONF MEAS, P118, DOI 10.1109/ICMTMA.2018.00035; Heng L, 2019, IEEE INT CONF ROBOT, P4695, DOI 10.1109/ICRA.2019.8793949; Hinterstoisser Stefan, 2012, P AS C COMP VIS, P2, DOI DOI 10.1007/978-3-642-37331-2_42; Huang XY, 2020, IEEE T PATTERN ANAL, V42, P2702, DOI 10.1109/TPAMI.2019.2926463; Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069; Irschara Arnold, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2599, DOI 10.1109/CVPRW.2009.5206587; Jones ES, 2011, INT J ROBOT RES, V30, P407, DOI 10.1177/0278364910388963; Kasyanov A, 2017, IEEE INT C INT ROBOT, P6662; Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237; Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Kneip L., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2969, DOI 10.1109/CVPR.2011.5995464; Kukelova Zuzana, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P216, DOI 10.1007/978-3-642-19309-5_17; Kukelova Z, 2013, IEEE I CONF COMP VIS, P2816, DOI 10.1109/ICCV.2013.350; Larsson C. T. V., 2016, BMVC; Larsson M, 2019, IEEE I CONF COMP VIS, P31, DOI 10.1109/ICCV.2019.00012; Larsson V, 2017, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2017.254; Laskar Z, 2017, IEEE INT CONF COMP V, P920, DOI 10.1109/ICCVW.2017.113; Lebeda K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.95; Li Y, 2010, LECT NOTES COMPUT SC, V6313, P790; Li YP, 2012, LECT NOTES COMPUT SC, V7572, P15, DOI 10.1007/978-3-642-33718-5_2; Liu L, 2017, IEEE I CONF COMP VIS, P2391, DOI 10.1109/ICCV.2017.260; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lynen S, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498; Massiceti Daniela, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5118, DOI 10.1109/ICRA.2017.7989598; Melekhov I, 2017, IEEE INT CONF COMP V, P870, DOI 10.1109/ICCVW.2017.107; Meng LL, 2018, IEEE INT C INT ROBOT, P6827, DOI 10.1109/IROS.2018.8593505; Meng LL, 2017, IEEE INT C INT ROBOT, P6886; Middelberg S, 2014, LECT NOTES COMPUT SC, V8690, P268, DOI 10.1007/978-3-319-10605-2_18; Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623; Mishchuk Anastasiya, 2017, ADV NEURAL INFORM PR; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Mur-Artal R, 2017, IEEE ROBOT AUTOM LET, V2, P796, DOI 10.1109/LRA.2017.2653359; Nam J, 2012, BIOMICROFLUIDICS, V6, DOI 10.1063/1.4718719; Naseer Tayyab, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2614, DOI 10.1109/ICRA.2017.7989305; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374; Ono Y, 2018, ADV NEUR IN, V31; Pittaluga F, 2019, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2019.00023; Qianqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P757, DOI 10.1007/978-3-030-58452-8_44; Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566; Radwan N, 2018, IEEE ROBOT AUTOM LET, V3, P4407, DOI 10.1109/LRA.2018.2869640; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Revaud J, 2019, ADV NEUR IN, V32; Robertson D., 2004, BMVC; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Saha Soham, 2018, BMVC; Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499; Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300; Sattler T, 2019, RANSACLIB TEMPLATE B; Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342; Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897; Sattler T, 2017, PROC CVPR IEEE, P6175, DOI 10.1109/CVPR.2017.654; Sattler T, 2017, IEEE T PATTERN ANAL, V39, P1744, DOI 10.1109/TPAMI.2016.2611662; Sattler T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.76; Sattler T, 2011, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2011.6126302; Schonberger JL, 2018, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR.2018.00721; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Schops T, 2019, PROC CVPR IEEE, P134, DOI 10.1109/CVPR.2019.00022; Se S, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P226, DOI 10.1109/IRDS.2002.1041393; Seymour Z., 2019, BMVC; Shan Q., 2014, 2014 2 INT C 3D VIS, VVolume 1, P525; Shi TX, 2019, IEEE IMAGE PROC, P315, DOI 10.1109/ICIP.2019.8802957; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; Sibbing D., 2013, 3DV; Simo-Serra E, 2015, ICCV; Simonyan K, 2014, IEEE T PATTERN ANAL, V36, P1573, DOI 10.1109/TPAMI.2014.2301163; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Stenborg E, 2018, IEEE INT CONF ROBOT, P6484; Sun X, 2017, PROC CVPR IEEE, P5641, DOI 10.1109/CVPR.2017.598; Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Svarm L, 2017, IEEE T PATTERN ANAL, V39, P1455, DOI 10.1109/TPAMI.2016.2598331; Taira H, 2019, IEEE I CONF COMP VIS, P4372, DOI 10.1109/ICCV.2019.00447; Taira Hajime, 2018, CVPR; Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649; Tian Yurun, 2019, IEEE C COMP VIS PATT; Toft C, 2018, LECT NOTES COMPUT SC, V11206, P391, DOI 10.1007/978-3-030-01216-8_24; Toft C, 2017, IEEE INT CONF COMP V, P650, DOI 10.1109/ICCVW.2017.83; Torii A, 2018, IEEE T PATTERN ANAL, V40, P257, DOI 10.1109/TPAMI.2017.2667665; Torii A, 2015, IEEE T PATTERN ANAL, V37, P2346, DOI 10.1109/TPAMI.2015.2409868; Torii A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCVW.2011.6130230; Valada A, 2018, IEEE INT CONF ROBOT, P6939; Valentin J., 2016, 3D VISION 3DV, P323; Valentin J, 2016, INT CONF 3D VISION, P323, DOI 10.1109/3DV.2016.41; Valentin J, 2015, PROC CVPR IEEE, P4400, DOI 10.1109/CVPR.2015.7299069; Ventura J, 2014, IEEE T VIS COMPUT GR, V20, P531, DOI 10.1109/TVCG.2014.27; Walch F, 2017, IEEE I CONF COMP VIS, P627, DOI 10.1109/ICCV.2017.75; Williams B, 2007, IEEE I CONF COMP VIS, P2244; Xue F, 2019, IEEE I CONF COMP VIS, P2841, DOI 10.1109/ICCV.2019.00293; Yang LW, 2019, IEEE I CONF COMP VIS, P42, DOI 10.1109/ICCV.2019.00013; Yang Tsun-Yi, 2020, ARXIV200107252; Zeisl B, 2015, IEEE I CONF COMP VIS, P2704, DOI 10.1109/ICCV.2015.310; Zetao Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3223, DOI 10.1109/ICRA.2017.7989366; Zhang J, 2019, IEEE I CONF COMP VIS, P1685, DOI 10.1109/ICCV.2019.00177; Zhang W, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P33, DOI 10.1109/3dpvt.2006.80; Zheng EL, 2015, IEEE I CONF COMP VIS, P2075, DOI 10.1109/ICCV.2015.240; Zhou H, 2016, LECT NOTES COMPUT SC, V9915, P724, DOI 10.1007/978-3-319-49409-8_60; Zhou L, 2020, PROC CVPR IEEE, P4918, DOI 10.1109/CVPR42600.2020.00497; Zhou Qunjie, 2019, IEEE INT C ROB AUT I	152	12	12	5	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					821	844		10.1007/s11263-020-01399-8	http://dx.doi.org/10.1007/s11263-020-01399-8		DEC 2020	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK	34720404	Green Accepted, Green Submitted, Green Published, hybrid			2022-12-18	WOS:000601485200002
J	Yin, J; Wu, A; Zheng, WS				Yin, Jiahang; Wu, Ancong; Zheng, Wei-Shi			Fine-Grained Person Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Fine-grained cross-view matching; Visual surveillance		Person re-identification (re-id) plays a critical role in tracking people via surveillance systems by matching people across non-overlapping camera views at different locations. Although most re-id methods largely depend on the appearance features of a person, such methods always assume that the appearance information (particularly color) is distinguishable. However, distinguishing people who dress in very similar clothes (especially the same type of clothes, e.g. uniform) is ineffective if relying only on appearance cues. We call this problem the fine-grained person re-identification (FG re-id) problem. To solve this problem, rather than relying on clothing color, we propose to exploit two types of local dynamic pose features: motion-attentive local dynamic pose feature and joint-specific local dynamic pose feature. They are complementary to each other and describe identity-specific pose characteristics, which are found to be more unique and discriminative against similar appearance between people. A deep neural network is formed to learn these local dynamic pose features and to jointly quantify motion and global visual cues. Due to the lack of a suitable benchmark dataset for evaluating the FG re-id problem, we also contribute a fine-grained person re-identification (FGPR) dataset, which contains 358 identities. Extensive evaluations on the FGPR dataset show that our proposed model achieves the best performance compared with related person re-id and fine-grained recognition methods for FG re-id. In addition, we verify that our method is still effective for conventional video-based person re-id.	[Yin, Jiahang] SUN YAT SEN Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China; [Wu, Ancong] SUN YAT SEN Univ, Sch Elect Informat & Technol, Guangzhou, Guangdong, Peoples R China; [Wu, Ancong] Guangdong Prov Key Lab Informat Secur, Guangzhou 510275, Peoples R China; [Zheng, Wei-Shi] SUN YAT SEN Univ, Dept Comp Sci, Guangzhou, Guangdong, Peoples R China; [Zheng, Wei-Shi] Peng Cheng Lab, Shenzhen, Peoples R China; [Zheng, Wei-Shi] Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou, Peoples R China	Sun Yat Sen University; Sun Yat Sen University; Sun Yat Sen University; Peng Cheng Laboratory	Zheng, WS (corresponding author), SUN YAT SEN Univ, Dept Comp Sci, Guangzhou, Guangdong, Peoples R China.; Zheng, WS (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.; Zheng, WS (corresponding author), Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou, Peoples R China.	yinjh5@mail2.sysu.edu.cn; wuancong@mail2.sysu.edu.cn; wszheng@ieee.org		Wu, Ancong/0000-0002-7969-3190				Ahmed E., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299016; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Branson S., 2014, PROC BRIT MACH VIS C; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68; Chung D., 2017, CVPR; Dai J, 2019, IEEE T IMAGE PROCESS, V28, P1366, DOI 10.1109/TIP.2018.2878505; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476; Ge YX, 2018, ADV NEUR IN, V31; Gou Mengran, 2016, BMVC; Gray D., 2007, IEEE INT WORKSH PERF, V3, P1; Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9; Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; Kostinger Martin, 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247939; Kviatkovsky I, 2013, IEEE T PATTERN ANAL, V35, P1622, DOI 10.1109/TPAMI.2012.246; Li SY, 2018, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2018.00683; Li W., 2013, LNCS, V7724, P31, DOI [10.1007/978-3-642-37331-2, DOI 10.1007/978-3-642-37331-2]; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu JY, 2018, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2018.00341; Liu X, 2017, AAAI CONF ARTIF INTE, P4190; Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46; Liu Y., 2017, CVPR; Liu Yu-Lun, 2019, P 33 C ART INT AAAI, P2; Makihara Y., 2017, CVPR; Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152; Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899; Qian X, 2017, POSE NORMALIZED IMAG; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Rida I, 2016, IEEE SIGNAL PROC LET, V23, P154, DOI 10.1109/LSP.2015.2507200; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562; Song G., 2017, ABS171108766 CORR; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279; Wu L, 2019, IEEE T NEUR NET LEAR, V30, P3347, DOI 10.1109/TNNLS.2019.2891244; Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507; Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; You JJ, 2016, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2016.150; Zhang W, 2019, IEEE T NEUR NET LEAR, V30, P3847, DOI 10.1109/TNNLS.2019.2899588; Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138; Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717; Zhu XK, 2018, IEEE T IMAGE PROCESS, V27, P5683, DOI 10.1109/TIP.2018.2861366	67	12	16	2	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1654	1672		10.1007/s11263-019-01259-0	http://dx.doi.org/10.1007/s11263-019-01259-0			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN					2022-12-18	WOS:000534910600006
J	Zhao, JJ; Han, JG; Shao, L; Snoek, CGM				Zhao, Jiaojiao; Han, Jungong; Shao, Ling; Snoek, Cees G. M.			Pixelated Semantic Colorization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image colorization; Semantic segmentation; Pixelated semantics	COLOR; IMAGE	While many image colorization algorithms have recently shown the capability of producing plausible color versions from gray-scale photographs, they still suffer from limited semantic understanding. To address this shortcoming, we propose to exploit pixelated object semantics to guide image colorization. The rationale is that human beings perceive and distinguish colors based on the semantic categories of objects. Starting from an autoregressive model, we generate image color distributions, from which diverse colored results are sampled. We propose two ways to incorporate object semantics into the colorization model: through a pixelated semantic embedding and a pixelated semantic generator. Specifically, the proposed network includes two branches. One branch learns what the object is, while the other branch learns the object colors. The network jointly optimizes a color embedding loss, a semantic segmentation loss and a color generation loss, in an end-to-end fashion. Experiments on Pascal VOC2012 and COCO-stuff reveal that our network, when trained with semantic segmentation labels, produces more realistic and finer results compared to the colorization state-of-the-art.	[Zhao, Jiaojiao; Snoek, Cees G. M.] Univ Amsterdam, Amsterdam, Netherlands; [Han, Jungong] Univ Warwick, Coventry, W Midlands, England; [Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates	University of Amsterdam; University of Warwick	Zhao, JJ (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	j.zhao3@uva.nl; jungonghan77@gmail.com; ling.shao@ieee.org; cgmsnoek@uva.nl	Han, Jungong/ABE-6812-2020	Shao, Ling/0000-0002-8264-6117				Bugeau A, 2014, IEEE T IMAGE PROCESS, V23, P298, DOI 10.1109/TIP.2013.2288929; Cao Y, 2017, LECT NOTES ARTIF INT, V10534, P151, DOI 10.1007/978-3-319-71249-9_10; Charpiat G, 2008, LECT NOTES COMPUT SC, V5304, P126, DOI 10.1007/978-3-540-88690-7_10; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55; Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190; Comaniciu D., 1997, P IEEE COMP SOC C CO; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143; Deshpande A, 2015, IEEE I CONF COMP VIS, P567, DOI 10.1109/ICCV.2015.72; Deshpande Aditya, 2017, P IEEE C COMP VIS PA, P6837; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Frans K., 2017, ABS170408834 CORR; Gijsenij A, 2010, INT J COMPUT VISION, V86, P127, DOI 10.1007/s11263-008-0171-3; Gupta R.K., 2012, P 20 ACM INT C MULTI, P369; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365; IIZUKA S, 2016, ACM T GRAPHIC, V35, DOI DOI 10.1145/2897824.2925974; Ironi R., 2005, P 16 EUROGRAPHICS C, P201; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Khan F. S., 2009, ICCV; Khan FS, 2012, PROC CVPR IEEE, P3306, DOI 10.1109/CVPR.2012.6248068; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35; Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu XP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409105; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lou Z., 2015, BMVC; Luan Q., 2007, EGSR07, P309; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Perez P., 2002, ECCV; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Qu BT, 2017, SA'17: SIGGRAPH ASIA 2017 SYMPOSIUM ON VISUALIZATION, DOI 10.1145/3139295.3139314; Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017; Radford A., 2016, ICLR 2016 INT C LEAR, DOI DOI 10.1007/S11280-018-0565-2; Royer A., 2017, BMVC; Salimans Tim, 2017, ICLR; Sanchez J. M., 2000, ICME; Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723; Shlens J., 2017, ARXIV170507208; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487; Tai YW, 2005, PROC CVPR IEEE, P747; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; VANDERHORST GJ, 1969, J OPT SOC AM, V59, P1482, DOI 10.1364/JOSA.59.001482; Vondrick C., 2018, ECCV, P391; Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576; Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231; Yi-Chin Huang, 2005, 13th Annual ACM International Conference on Multimedia, P351, DOI 10.1145/1101149.1101223; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhao J., 2018, ARXIV180801597	57	12	14	1	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		818	834		10.1007/s11263-019-01271-4	http://dx.doi.org/10.1007/s11263-019-01271-4			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Green Submitted, hybrid, Green Published			2022-12-18	WOS:000525393600005
J	Jang, Y; Song, Y; Kim, CD; Yu, Y; Kim, Y; Kim, G				Jang, Yunseok; Song, Yale; Kim, Chris Dongjoo; Yu, Youngjae; Kim, Youngjin; Kim, Gunhee			Video Question Answering with Spatio-Temporal Reasoning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						VQA; Spatio-temporal reasoning; Large-scale video QA dataset; Spatial and temporal attention		Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention and show its effectiveness over conventional VQA techniques through empirical evaluations.	[Jang, Yunseok; Kim, Chris Dongjoo; Yu, Youngjae; Kim, Youngjin; Kim, Gunhee] Seoul Natl Univ, Seoul, South Korea; [Song, Yale] Microsoft AI & Res, Redmond, WA USA	Seoul National University (SNU)	Kim, G (corresponding author), Seoul Natl Univ, Seoul, South Korea.	yunseok.jang@snu.ac.kr; yalesong@microsoft.com; cdjkim@vision.snu.ac.kr; youngjae.yu@vision.snu.ac.kr; youngjin.kim@vision.snu.ac.kr; gunhee@snu.ac.kr			IITP Grant [2019-0-01082, 2017-0-01772]; Brain Research Program through the NRF - Korea government (MSIT) [2017M3C7A1047860]; Academic Research Program in Yahoo Research	IITP Grant(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); Brain Research Program through the NRF - Korea government (MSIT)(National Research Foundation of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Academic Research Program in Yahoo Research	This work was supported by IITP Grant (No. 2019-0-01082, SW StarLab) (No. 2017-0-01772, Video Turing Test), Brain Research Program through the NRF (2017M3C7A1047860) funded by the Korea government (MSIT) and Academic Research Program in Yahoo Research. Gunhee Kim is the corresponding author.	Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522; Andreas J., 2016, ANN C N AM CHAPT ASS; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ba J. L., 2016, ARXIV PREPRINT ARXIV; Bakhshi S., 2016, CHI; Chomsky Noam, 1971, CONDITIONS TRANSFORM; Daiber J., 2013, I SEM 2013 9 INT C S, P121, DOI [10.1145/2506182.2506198, 10.1145/, DOI 10.1145/2506182.2506198]; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; Davis E, 2015, COMMUN ACM, V58, P92, DOI 10.1145/2701413; Denkowski M., 2011, EMNLP; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Farneback G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50; Fellbaum Christiane, 1998, WORDNET ELECT DATABA; Fukui Akira, 2016, ARXIV160601847; Gao HY, 2015, ADV NEUR IN, V28; Gao Jiyang, 2018, P IEEE C COMP VIS PA; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Gygli Michael, 2016, CVPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Isola Phillip, 2015, CVPR; Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Karen Kipper-Schuler, 2005, THESIS; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kim Jin-Hwa, 2016, ADV NEURAL INFORM PR, P1; Kim K., 2017, IJCAI; Kingma D.P, P 3 INT C LEARNING R; Kiros J. R., 2015, NIPS; Lei Jie, 2018, EMNLP, P1369, DOI DOI 10.18653/V1/D18-1167; Levi G., 2015, P ACM INT C MULT INT; Levy O., 2015, ICCV; Li YC, 2016, PROC CVPR IEEE, P4641, DOI 10.1109/CVPR.2016.502; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin X., 2015, P IEEE C COMP VIS PA; Maharaj Tegan, 2017, CVPR; Malinowski M., 2015, P INT C COMP VIS; Malinowski M., 2014, NIPS; Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010; Mun J., 2017, ICCV; Na S., 2018, ICCV; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Pham V., 2014, ICFHR; PiotrBojanwoski E. G., 2017, TACL; Ren M., 2015, P 28 INT C NEUR INF, V2, P2953, DOI [10.5555/2969442.2969570, DOI 10.5555/2969442.2969570]; Rohrbach A, 2017, INT J COMPUT VISION, V123, P94, DOI 10.1007/s11263-016-0987-1; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Soomro K., 2012, ARXIV; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tapaswi M., 2016, IEEE C COMP VIS PATT; Vaswani Ashish, NIPS; Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515; Wu Q., 2016, P CVPR; Xie Saining, 2018, ECCV; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z., 2015, ARXIV151102274; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yu L, 2015, IEEE INT C COMP VIS; Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347; Zhao Z., 2017, IJCAI; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7; Zhu Y., 2016, P IEEE C COMP VIS PA; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	67	12	12	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1385	1412		10.1007/s11263-019-01189-x	http://dx.doi.org/10.1007/s11263-019-01189-x			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL					2022-12-18	WOS:000485320300001
J	Arrigoni, F; Fusiello, A				Arrigoni, Federica; Fusiello, Andrea			Synchronization Problems in Computer Vision with Closed-Form Solutions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Synchronization; Averaging; Graph optimization; Multiple point-set registration; Structure from motion; Multi-view matching	MOTION; REGISTRATION; VIEWS; EIGENVECTORS; LOCALIZATION; ALGORITHM; NETWORKS	In this paper we survey and put in a common framework several works that have been developed in different contexts, all dealing with the same abstract problem, called synchronization by some authors, or averaging, or graph optimization by others. The problem consists in recovering some variables from a set of pairwise relation measurements. In particular, we concentrate on instances where the variables and the measures belong to a (semi-)group and the measures are their mutual differences (or ratios, depending on how the group operation is called). The groups we deal with have a matrix representation, which leads to an elegant theory and closed-form solutions.	[Arrigoni, Federica; Fusiello, Andrea] Univ Udine, DPIA, Via Sci 208, Udine, Italy; [Arrigoni, Federica] Czech Tech Univ, Czech Inst Informat Robot & Cybernet, Jugoslavskych Partyzanu 3, Prague, Czech Republic	University of Udine; Czech Technical University Prague	Fusiello, A (corresponding author), Univ Udine, DPIA, Via Sci 208, Udine, Italy.	arrigoni.federica@spes.uniud.it; andrea.fusiello@uniud.it	Fusiello, Andrea/GOJ-9893-2022	Fusiello, Andrea/0000-0003-2963-0316; Arrigoni, Federica/0000-0003-0331-4032	European Regional Development Fund under the project IMPACT [CZ.02.1.01/0.0/0.0/15_003/0000468]	European Regional Development Fund under the project IMPACT	The authors would like to thank Carlo Comin and Romeo Rizzi for pointing out the connection with the Group Feedback Edge Set problem, and Eleonora Maset who co-authored the works on permutation synchronization. We are also grateful to Fabio Crosilla for drawing our attention to the leveling problem in topography. This work was supported by the European Regional Development Fund under the project IMPACT (Reg. No. CZ.02.1.01/0.0/0.0/15_003/0000468).	Aftab K, 2015, IEEE T PATTERN ANAL, V37, P728, DOI 10.1109/TPAMI.2014.2353625; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; [Anonymous], 1984, GRADUATE TEXTS MATH; Aragues R, 2012, SYST CONTROL LETT, V61, P773, DOI 10.1016/j.sysconle.2012.04.008; Arie-Nachimson Mica, 2012, P JOINT 3DIM 3DPVT C; Arrigoni F., 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P491, DOI 10.1109/3DV.2014.48; Arrigoni F., 2014, INT ARCH PHOTOGRAMM, VXL-5, P63; Arrigoni F., 2015, P INT C 3D VIS 3DV, P300; Arrigoni F, 2018, COMPUT VIS IMAGE UND, V174, P95, DOI 10.1016/j.cviu.2018.08.001; Arrigoni F, 2019, IEEE T PATTERN ANAL, V41, P2049, DOI 10.1109/TPAMI.2018.2848225; Arrigoni F, 2017, LECT NOTES COMPUT SC, V10485, P70, DOI 10.1007/978-3-319-68548-9_7; Arrigoni F, 2016, INT CONF 3D VISION, P546, DOI 10.1109/3DV.2016.64; Arrigoni F, 2016, SIAM J IMAGING SCI, V9, P1963, DOI 10.1137/16M1060248; Arrigoni F, 2016, LECT NOTES COMPUT SC, V9908, P489, DOI 10.1007/978-3-319-46493-0_30; Barooah P, 2008, IEEE T SIGNAL PROCES, V56, P2181, DOI 10.1109/TSP.2007.912270; Barooah P, 2007, IEEE CONTR SYST MAG, V27, P57, DOI 10.1109/MCS.2007.384125; Bartoli A, 2003, INT J COMPUT VISION, V52, P45, DOI 10.1023/A:1022318524906; Belta C, 2002, P I MECH ENG C-J MEC, V216, P47, DOI 10.1243/0954406021524909; Benjemaa R., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P34, DOI 10.1007/BFb0054732; Bernard F, 2019, PATTERN RECOGN, V92, P146, DOI 10.1016/j.patcog.2019.03.021; Bernard Florian, 2015, P IEEE C COMP VIS PA; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bhowmick Brojeshwar, 2014, 12 AS C COMP VIS ACC; BJERHAMMAR A, 1973, THEORY ERRORS GEN MA; Bollobas B., 1998, MODERN GRAPH THEORY; Boumal N., 2013, P IEEE INT C ROB AUT; Boumal N, 2014, INF INFERENCE, V3, P1, DOI 10.1093/imaiai/iat006; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Brand M., 2004, P EUR C COMP VIS; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Cardoso J., 1999, PREPRINT, P99; Carlone L, 2016, IEEE T ROBOT, V32, P545, DOI 10.1109/TRO.2016.2544304; Carlone L, 2014, IEEE T ROBOT, V30, P475, DOI 10.1109/TRO.2013.2291626; Carlone L, 2012, ROBOTICS: SCIENCE AND SYSTEMS VII, P41; Carlone Luca, 2015, P IEEE INT C ROB AUT; Castellani U, 2002, COMPUT VIS IMAGE UND, V87, P78, DOI 10.1006/cviu.2002.0984; Chatterjee A., 2013, P INT C COMP VIS; Chen YX, 2014, PR MACH LEARN RES, V32, P100; Chudnovsky M, 2008, COMBINATORICA, V28, P145, DOI 10.1007/s00493-008-2157-8; Crandall D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3001, DOI 10.1109/CVPR.2011.5995626; Cygan M, 2012, LECT NOTES COMPUT SC, V7551, P194, DOI 10.1007/978-3-642-34611-8_21; EDELMAN PH, 1979, J GRAPH THEOR, V3, P135, DOI 10.1002/jgt.3190030205; Enfissi EMA, 2005, PLANT BIOTECHNOL J, V3, P17, DOI 10.1111/j.1467-7652.2004.00091.x; Enqvist O., 2011, 11 WORKSH OMN VIS CA; Fantoni S, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P73, DOI 10.1109/3DIMPVT.2012.63; Fredriksson Johan, 2012, P AS C COMP VIS; Fusiello A, 2002, LECT NOTES COMPUT SC, V2351, P805; Giridhar A, 2006, IEEE DECIS CONTR P, P4915, DOI 10.1109/CDC.2006.377325; Goldstein T, 2016, LECT NOTES COMPUT SC, V9911, P289, DOI 10.1007/978-3-319-46478-7_18; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Govindu V. M, 2001, P IEEE C COMP VIS PA; Govindu V. M., 2017, MOTION AVERAGING FRA; Govindu VM, 2014, IEEE T IMAGE PROCESS, V23, P1289, DOI 10.1109/TIP.2013.2246517; Govindu VM, 2006, LECT NOTES COMPUT SC, V3852, P457; Govindu VM, 2004, PROC CVPR IEEE, P684; Guillemot S, 2011, DISCRETE OPTIM, V8, P61, DOI 10.1016/j.disopt.2010.05.003; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848; HOLLAND PW, 1977, COMMUN STAT A-THEOR, V6, P813, DOI 10.1080/03610927708827533; Jiang N., 2013, P INT C COMP VIS; Joglekar M, 2012, DISCRETE MATH, V312, P1542, DOI 10.1016/j.disc.2011.09.021; Karp R., 2003, TECHNICAL REPORT; Kavitha T, 2009, COMPUT SCI REV, V3, P199, DOI 10.1016/j.cosrey.2009.08.001; Kawarabayashi K, 2006, J COMB THEORY B, V96, P296, DOI 10.1016/j.jctb.2005.08.001; KELLER JB, 1975, MATH MAG, V48, P192; Kennedy R, 2012, IEEE INT C INT ROBOT, P194, DOI 10.1109/IROS.2012.6386132; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Khatri C. G., 1968, SANKHYA, V30, P167, DOI DOI 10.2307/25049527; Krishnan Shankar, 2007, International Journal of Intelligent Systems Technologies and Applications, V3, P319, DOI 10.1504/IJISTA.2007.014267; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Levi N, 2003, PROC CVPR IEEE, P518; Liu S., 2008, INT J INFORM SYSTEMS, V4, P160, DOI 10.1155/2016/8301709; Lov asz, 2007, TECHNICAL REPORT; Martinec D., 2007, P IEEE C COMP VIS PA; Maset E., 2017, P IEEE INT C COMP VI, V2, P5; Megret R., 2014, P AS C COMP VIS; Meyer CD., 2000, MATRIX ANAL APPL LIN; Minka T., 2000, OLD NEW MATRIX ALGEB; Moakher M, 2002, SIAM J MATRIX ANAL A, V24, P1, DOI 10.1137/S0895479801383877; Molavi P., 2011, IEEE AM CONTR C; Moulon P, 2013, IEEE I CONF COMP VIS, P3248, DOI 10.1109/ICCV.2013.403; Olsson C, 2011, LECT NOTES COMPUT SC, V6688, P524, DOI 10.1007/978-3-642-21227-7_49; Ozyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X; Ozyosil O, 2015, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2015.7298883; Pachauri D., 2013, ADV NEURAL INFORM PR, V26, P1860; PENNEC X, 1996, 16 LEEDS ANN STAT WO, P178; Pulli K., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P160, DOI 10.1109/IM.1999.805346; Rosen D. M., 2017, P INT C INT ROB SYST; Rosen DM, 2015, IEEE INT CONF ROBOT, P5822, DOI 10.1109/ICRA.2015.7140014; Russell WJ, 2011, IEEE T SIGNAL PROCES, V59, P2834, DOI 10.1109/TSP.2011.2117422; Santellani E., 2018, ISPRS ANN PHOTOGRAMM, V4, P247; Saunderson J, 2015, SIAM J OPTIMIZ, V25, P1314, DOI 10.1137/14096339X; Schroeder Pierre, 2011, 2011 IEEE WORKSH APP, P650, DOI DOI 10.1109/WACV.2011.5711566; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Sharp GC, 2002, LECT NOTES COMPUT SC, V2351, P587; Shen Yujia, 2016, ADV NEURAL INFORM PR; Singer A, 2011, SIAM J IMAGING SCI, V4, P543, DOI 10.1137/090767777; Singer A, 2011, APPL COMPUT HARMON A, V30, P20, DOI 10.1016/j.acha.2010.02.001; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Solis R, 2006, IEEE DECIS CONTR P, P2737; Thunberg J, 2011, IEEE DECIS CONTR P, P1962, DOI 10.1109/CDC.2011.6161295; Toldo R., 2010, P 5 INT S 3D DAT PRO, P109; Toldo R, 2015, COMPUT VIS IMAGE UND, V140, P127, DOI 10.1016/j.cviu.2015.05.011; Torsello A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2441, DOI 10.1109/CVPR.2011.5995565; Tron R., 2015, IEEE AM CONTR C; Tron R, 2017, IEEE I CONF COMP VIS, P4077, DOI 10.1109/ICCV.2017.437; Tron R, 2014, IEEE T AUTOMAT CONTR, V59, P3325, DOI 10.1109/TAC.2014.2351912; Tron Roberto, 2014, P EUR C COMP VIS; Tron Roberto, 2016, COMP VIS PATT REC WO; Van Loan CF, 2000, J COMPUT APPL MATH, V123, P85, DOI 10.1016/S0377-0427(00)00393-9; Wahlstrom M, 2014, HALF INTEGRALITY LP, P1762; Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5; Wilson K, 2016, LECT NOTES COMPUT SC, V9911, P255, DOI 10.1007/978-3-319-46478-7_16; Wright S, 1997, SOC IND APPL MATH; Yu JG, 2016, PATTERN RECOGN, V51, P255, DOI 10.1016/j.patcog.2015.09.029; Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801; Zhao SY, 2016, AUTOMATICA, V69, P334, DOI 10.1016/j.automatica.2016.03.010; Zheng YQ, 2012, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2012.6247828; Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459	123	12	12	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2020	128	1					26	52		10.1007/s11263-019-01240-x	http://dx.doi.org/10.1007/s11263-019-01240-x		SEP 2019	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KI6WA		Green Submitted			2022-12-18	WOS:000488905800001
J	Yoon, JH; Lee, CR; Yang, MH; Yoon, KJ				Yoon, Ju Hong; Lee, Chang-Ryeol; Yang, Ming-Hsuan; Yoon, Kuk-Jin			Structural Constraint Data Association for Online Multi-object Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-object tracking; Data association; Structural constraint	MULTITARGET TRACKING; APPEARANCE	Online two-dimensional (2D) multi-object tracking (MOT) is a challenging task when the objects of interest have similar appearances. In that case, the motion of objects is another helpful cue for tracking and discriminating multiple objects. However, when using a single moving camera for online 2D MOT, observable motion cues are contaminated by global camera movements and, thus, are not always predictable. To deal with unexpected camera motion, we propose a new data association method that effectively exploits structural constraints in the presence of large camera motion. In addition, to reduce incorrect associations with mis-detections and false positives, we develop a novel event aggregation method to integrate assignment costs computed by structural constraints. We also utilize structural constraints to track missing objects when they are re-detected again. By doing this, identities of the missing objects can be retained continuously. Experimental results validated the effectiveness of the proposed data association algorithm under unexpected camera motions. In addition, tracking results on a large number of benchmark datasets demonstrated that the proposed MOT algorithm performs robustly and favorably against various online methods in terms of several quantitative metrics, and that its performance is comparable to offline methods.	[Yoon, Ju Hong] Korea Elect Technol Inst, Seongnam, Gyeonggi Do, South Korea; [Lee, Chang-Ryeol] Gwangju Inst Sci & Technol, Elect Engn & Comp Sci, Gwangju, South Korea; [Yang, Ming-Hsuan] Univ Calif Merced, Elect Engn & Comp Sci, Merced, CA 95344 USA; [Yoon, Kuk-Jin] Korea Adv Inst Sci & Technol, Dept Mech Engn, 291 Daehak Ro, Daejeon 34141, South Korea	Korea Electronics Technology Institute (KETI); Gwangju Institute of Science & Technology (GIST); University of California System; University of California Merced; Korea Advanced Institute of Science & Technology (KAIST)	Yoon, KJ (corresponding author), Korea Adv Inst Sci & Technol, Dept Mech Engn, 291 Daehak Ro, Daejeon 34141, South Korea.	jhyoon@keti.re.kr; crlee@gist.ac.kr; mhyang@ucmerced.edu; kjyoon@kaist.ac.kr	Yoon, Kuk-Jin/F-4329-2018; Yang, Ming-Hsuan/T-9533-2019; Yang, Ming-Hsuan/AAE-7350-2019	Yang, Ming-Hsuan/0000-0003-4848-2304; yun, gugjin/0000-0002-1634-2756	'The Cross-Ministry Giga KOREA Project' grant - Korea government (MSIT) [GK18P0200, GK18P0300]; IITP grant - Korea government (MSIP) [2014-0-00059]; Samsung Research Funding Center of Samsung Electronics [SRFC-TC1603-05]; NSF CAREER Grant [1149783]	'The Cross-Ministry Giga KOREA Project' grant - Korea government (MSIT); IITP grant - Korea government (MSIP); Samsung Research Funding Center of Samsung Electronics(Samsung); NSF CAREER Grant(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was partially supported by 'The Cross-Ministry Giga KOREA Project' grant funded by the Korea government (MSIT) (No. GK18P0200, Development of 4D reconstruction and dynamic deformable action model based hyper-realistic service technology and No. GK18P0300, Real-time 4D reconstruction of dynamic objects for ultra-realistic service). Theworkwas also partially supported by IITP grant funded by the Korea government (MSIP) (2014-0-00059) and Samsung Research Funding Center of Samsung Electronics under Project Number SRFC-TC1603-05. M.-H. Yang is supported in part by the the NSF CAREER Grant #1149783.	Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159; Bar-Shalom Y., 1995, MULTITARGET MULTISEN; Bernardin K., 2008, J IMAGE VIDEO PROCES, V2008, P1; Betke M., 2016, DATA ASS MULTIOBJECT; Blackman S. S., 1999, DESIGN ANAL MODERN T; Breitenstein MD, 2011, IEEE T PATTERN ANAL, V33, P1820, DOI 10.1109/TPAMI.2010.232; Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347; Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479; Duan GQ, 2012, LECT NOTES COMPUT SC, V7574, P129, DOI 10.1007/978-3-642-33712-3_10; Ess A., 2008, P IEEE C COMP VIS PA; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fowlkes C. C., 2015, P BRIT MACH VIS C; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A, 2013, THESIS; Geiger A, 2014, IEEE T PATTERN ANAL, V36, P1012, DOI 10.1109/TPAMI.2013.185; Grabner H, 2010, PROC CVPR IEEE, P1285, DOI 10.1109/CVPR.2010.5539819; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Leal-Taixe L, 2011, P IEEE INT C COMP VI; Leal-Taixe L., 2016, P IEEE C COMP VIS PA, P33; Leal-Taixe L, 2014, PROC CVPR IEEE, P3542, DOI 10.1109/CVPR.2014.453; Lenz P, 2015, IEEE I CONF COMP VIS, P4364, DOI 10.1109/ICCV.2015.496; Li XR, 2003, IEEE T AERO ELEC SYS, V39, P1333, DOI 10.1109/TAES.2003.1261132; McLaughlin N, 2015, IEEE WINT CONF APPL, P71, DOI 10.1109/WACV.2015.17; Milan A, 2013, P IEEE C COMP VIS PA; Milan A., 2015, P IEEE C COMP VIS PA; Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103; Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260; Poiesi F, 2013, CVIU, P257; Rezatofighi SH, 2016, PROC CVPR IEEE, P136, DOI 10.1109/CVPR.2016.22; Rezatofighi SH, 2015, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2015.349; Suna Kim, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P98, DOI 10.1007/978-3-642-37431-9_8; Takala V, 2007, P IEEE C COMP VIS PA; Wang XY, 2015, IEEE T PATTERN ANAL, V37, P2071, DOI 10.1109/TPAMI.2015.2389830; Wu Z, 2016, COMPUT VIS IMAGE UND, V143, P25, DOI 10.1016/j.cviu.2015.10.006; Xing JL, 2009, PROC CVPR IEEE, P1200, DOI 10.1109/CVPRW.2009.5206745; Yang B, 2014, INT J COMPUT VISION, V107, P203, DOI 10.1007/s11263-013-0666-4; Yang B, 2012, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR.2012.6247892; Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155; Yoon JH, 2015, IEEE WINT CONF APPL, P33, DOI 10.1109/WACV.2015.12; Zhang L, 2013, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2013.240	45	12	15	1	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2019	127	1					1	21		10.1007/s11263-018-1087-1	http://dx.doi.org/10.1007/s11263-018-1087-1			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HK6AD					2022-12-18	WOS:000458050000001
J	Plotz, T; Roth, S				Ploetz, Tobias; Roth, Stefan			Automatic Registration of Images to Untextured Geometry Using Average Shading Gradients	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	IEEE International Conference on Computer Vision (ICCV)	DEC 11-18, 2015	Santiago, CHILE	CPS, IEEE Comp Soc, Amazon, Microsoft, SENSETIME, Baidu, Intel, Facebook, Adobe, Panasonic, Google, OMRON, Blippar, iRobot, HISCENE, NVIDIA, Viscovery, AiCUre, M Tec, Inst Elect & Elect Engineers, Comp Vis Fdn		Registration; 3D geometry; Gradients; Dense correspondence; Pose estimation	RECOGNITION; SCENE; SHAPE	Many existing approaches for image-to-geometry registration assume that either a textured 3D model or a good initial guess of the 3D pose is available to bootstrap the registration process. In this paper we consider the registration of photographs to 3D models even when no texture information is available. This is very challenging as we cannot rely on texture gradients, and even shading gradients are hard to estimate since the lighting conditions are unknown. To that end, we propose average shading gradients, a rendering technique that estimates the average gradient magnitude over all lighting directions under Lambertian shading. We use this gradient representation as the building block of a registration pipeline based on matching sparse features. To cope with inevitable false matches due to the missing texture information and to increase robustness, the pose of the 3D model is estimated in two stages. Coarse pose hypotheses are first obtained from a single correct match each, subsequently refined using SIFT flow, and finally verified. We apply our algorithm to registering images of real-world objects to untextured 3D meshes of limited accuracy. Moreover, we show that registration can be performed even for paintings despite lacking photo-realism.	[Ploetz, Tobias; Roth, Stefan] Tech Univ Darmstadt, Darmstadt, Germany	Technical University of Darmstadt	Plotz, T (corresponding author), Tech Univ Darmstadt, Darmstadt, Germany.	tobias.ploetz@visinf.tu-darmstadt.de; stefan.roth@visinf.tu-darmstadt.de		Plotz, Tobias/0000-0003-1386-939X; Roth, Stefan/0000-0001-9002-9832	European Union [323567]	European Union(European Commission)	This work was supported by the European Union FP7 Project "Harvest4D" (No. 323567).	Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148; Arandjelovic R, 2011, IEEE I CONF COMP VIS, P375, DOI 10.1109/ICCV.2011.6126265; Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591009; Baatz G, 2012, LECT NOTES COMPUT SC, V7573, P517, DOI 10.1007/978-3-642-33709-3_37; Baboud L, 2011, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2011.5995727; Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642; Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Chum O., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P623; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Corsini M, 2013, INT J COMPUT VISION, V102, P91, DOI 10.1007/s11263-012-0552-5; Corsini M, 2009, COMPUT GRAPH FORUM, V28, P1755, DOI 10.1111/j.1467-8659.2009.01552.x; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354; Dellepiane Matteo, 2013, 2013 Digital Heritage International Congress (DigitalHeritage). Federating the 19th Int'I VSMM, 10th Eurographics GCH, & 2nd UNESCO Memory of the World Conferences, plus special sessions from CAA, Arqueologico 2.0, Space2Place, ICOMOS ICIP & CIPA, EU projects, et al. Proceedings, P39; Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fuhrmann S, 2015, COMPUT GRAPH-UK, V53, P44, DOI 10.1016/j.cag.2015.09.003; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Irschara Arnold, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2599, DOI 10.1109/CVPRW.2009.5206587; Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470; Kendall A, 2016, IEEE INT CONF ROBOT, P4762, DOI 10.1109/ICRA.2016.7487679; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Kneip L., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2969, DOI 10.1109/CVPR.2011.5995464; Krull A, 2015, IEEE I CONF COMP VIS, P954, DOI 10.1109/ICCV.2015.115; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Li XW, 2008, LECT NOTES COMPUT SC, V5302, P427; Li YP, 2010, LECT NOTES COMPUT SC, V6312, P791; Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372; Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu LY, 2012, COMPUT VIS IMAGE UND, V116, P25, DOI 10.1016/j.cviu.2011.07.009; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043; Massa F, 2016, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2016.648; Matzen K, 2014, LECT NOTES COMPUT SC, V8695, P615, DOI 10.1007/978-3-319-10584-0_40; NEUGEBAUER PJ, 1999, EUR WORKSH 99, V18, P245; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Plotz T, 2015, IEEE I CONF COMP VIS, P2030, DOI 10.1109/ICCV.2015.235; Russell B. C., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P545, DOI 10.1109/ICCVW.2011.6130291; Shan Q., 2014, 2014 2 INT C 3D VIS, VVolume 1, P525; Shanmugam P, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P73; Shrivastava A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024188; Sibbing D, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P56, DOI 10.1109/3DV.2013.16; Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y; Stark M., 2010, P BRIT MACH VIS C, P1, DOI [10.5244/C.24.106, DOI 10.5244/C.24.106]; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Viola P, 1997, INT J COMPUT VISION, V24, P137, DOI 10.1023/A:1007958904918; Wendel A., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P5792, DOI 10.1109/ICRA.2011.5980317; Zia MZ, 2013, IEEE T PATTERN ANAL, V35, P2608, DOI 10.1109/TPAMI.2013.87	53	12	12	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2017	125	1-3			SI		65	81		10.1007/s11263-017-1022-x	http://dx.doi.org/10.1007/s11263-017-1022-x			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	FL2TO		hybrid			2022-12-18	WOS:000414072800006
J	Cevikalp, H; Triggs, B				Cevikalp, Hakan; Triggs, Bill			Visual Object Detection Using Cascades of Binary and One-Class Classifiers	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Rejection cascade; One-Class Classifier; Latent training	SUPPORT; SYSTEM	We describe an efficient approach to visual object detection that uses short cascades of asymmetric 'one class' classifiers to quickly reject negatives (windows not centered on an object of the desired class) within a sliding window framework. Current detectors typically use binary discriminants such as Support Vector Machines or Boosting to implement each stage of the cascade. These treat the positive and negative classes symmetrically. We argue that this is suboptimal because object detectors typically see a great many negative windows with extremely diverse contents and only a few positive ones with comparatively coherent contents. We show that asymmetric representations that focus on tightly modeling the extent of the rare, coherent positive class can lead to simpler classifiers and faster rejection. Our cascades use asymmetric classifiers based on simple convex models to progressively tighten the bound on the positive class. They typically start with a conventional linear SVM for initial pruning, followed by a cascade of linear distance-to-hyperplane and interior-of-hypersphere classifiers and finishing with a kernelized hypersphere classifier. We show that the resulting detectors have competitive performance on the Labeled Faces in the Wild dataset and state-of-the-art performance on the FDDB face detection, ESOGU face detection and INRIA Person datasets. The results on the PASCAL VOC 2007 dataset are also respectable given that they use neither object parts nor context. The one-class formulations provide significant reductions in classifier complexity relative to the corresponding two-class ones, making them suitable for real-world applications.	[Cevikalp, Hakan] Eskisehir Osmangazi Univ, Elect & Elect Engn Dept, Eskisehir, Turkey; [Triggs, Bill] Lab Jean Kuntzmann, Grenoble, France	Eskisehir Osmangazi University; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Inria	Cevikalp, H (corresponding author), Eskisehir Osmangazi Univ, Elect & Elect Engn Dept, Eskisehir, Turkey.	hakan.cevikalp@gmail.com; Bill.Triggs@imag.fr			Scientific and Technological Research Council of Turkey (TUBITAK) [EEEAG-109E279]	Scientific and Technological Research Council of Turkey (TUBITAK)(Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK))	This work was supported in part by the Scientific and Technological Research Council of Turkey (TUBITAK) under Grant Number EEEAG-109E279.	Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Aldavert D., 2010, CVPR; Amit Y, 1999, NEURAL COMPUT, V11, P1691, DOI 10.1162/089976699300016197; [Anonymous], 2015, P BMVC; [Anonymous], 2012, CVPR; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Benenson R., 2010, CVPR; Burges C. J. C., 1996, INT C MACH LEARN; Cevikalp H., 2008, INT C MACH LEARN; Cevikalp H., 2012, CVPR; Cevikalp H, 2013, IEEE INT CONF AUTOMA; Cevikalp H, 2010, J SIGNAL PROCESS SYS, V61, P61, DOI 10.1007/s11265-008-0313-4; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Felzenszwalb P., 2008, CVPR; Felzenszwalb P. F., 2010, CVPR; Gasimov RN, 2006, OPTIM METHOD SOFTW, V21, P527, DOI 10.1080/10556780600723252; Girshick R., 2014, CVPR4; Harzallah H., 2009, ICCV; Huang G.B., 2008, WORKSHOP FACESREAL L; Hussain S., 2011, THESIS; Hussain S., 2010, BMVC; Jain V.., 2010, FDDB BENCHMARK FACE; Jin H., 2004, INT C AUT FAC GEST R; Kalal Z., 2008, BMVC; Lampert C., 2008, CVPR; Levi K., 2004, CVPR; Li H., 2015, CVPR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Malisiewicz T., 2011, ICCV; Mangasarian OL, 2006, IEEE T PATTERN ANAL, V28, P69, DOI 10.1109/TPAMI.2006.17; Mika S., 1999, NEURAL INFORM PROCES; Murat Dundar M., 2008, INT C MACH LEARN; Murthy SK, 1994, J ARTIF INTELL RES, V2, P1, DOI 10.1613/jair.63; Orozco J, 2015, IMAGE VISION COMPUT, V42, P47, DOI 10.1016/j.imavis.2015.07.002; Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689; Perrotton X., 2010, CVPR, P2; Platt J C, 1999, ADV KERNEL METHODS S; PORIKLI F, 2005, CVPR; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256; Scholkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Shams L., 1996, WORLD C NEUR NETW; Sizintsev M., 2010, CVPR; Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Tenmoto H, 1998, PATTERN RECOGN, V31, P1627, DOI 10.1016/S0031-3203(98)00016-8; Ullman S, 2000, LECT NOTES COMPUT SC, V1811, P73; Varma M., 2007, ICCV; Vedaldi A., 2009, ICCV; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207; Wei Y, 2010, CVPR; Zhu L., 2010, CVPR; ZHU X, 2012, BMVC	58	12	13	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2017	123	3					334	349		10.1007/s11263-016-0986-2	http://dx.doi.org/10.1007/s11263-016-0986-2			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EX9EO					2022-12-18	WOS:000403559600002
J	Kiechle, M; Habigt, T; Hawe, S; Kleinsteuber, M				Kiechle, Martin; Habigt, Tim; Hawe, Simon; Kleinsteuber, Martin			A Bimodal Co-sparse Analysis Model for Image Processing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Co-sparse analysis model; Bimodal image reconstruction; Bimodal image registration; Gradient methods on manifolds; Analysis operator learning	INFORMATION-BASED REGISTRATION; SUPERRESOLUTION; ALGORITHM	The success of many computer vision tasks lies in the ability to exploit the interdependency between different image modalities such as intensity and depth. Fusing corresponding information can be achieved on several levels, and one promising approach is the integration at a low level. Moreover, sparse signal models have successfully been used in many vision applications. Within this area of research, the so-called co-sparse analysis model has attracted considerably less attention than its well-known counterpart, the sparse synthesis model, although it has been proven to be very useful in various image processing applications. In this paper, we propose a bimodal co-sparse analysis model that is able to capture the interdependency of two image modalities. It is based on the assumption that a pair of analysis operators exists, so that the co-supports of the corresponding bimodal image structures have a large overlap. We propose an algorithm that is able to learn such a coupled pair of operators from registered and noise-free training data. Furthermore, we explain how this model can be applied to solve linear inverse problems in image processing and how it can be used as a prior in bimodal image registration tasks. This paper extends the work of some of the authors by two major contributions. Firstly, a modification of the learning process is proposed that a priori guarantees unit norm and zero-mean of the rows of the operator. This accounts for the intuition that local texture carries the most important information in image modalities independent of brightness and contrast. Secondly, the model is used in a novel bimodal image registration algorithm, which estimates the transformation parameters of unregistered images of different modalities.	[Kiechle, Martin; Habigt, Tim; Hawe, Simon; Kleinsteuber, Martin] Tech Univ Munich, Dept Elect Elect & Comp Engn, D-80333 Munich, Germany	Technical University of Munich	Kiechle, M (corresponding author), Tech Univ Munich, Dept Elect Elect & Comp Engn, Arcisstr 21, D-80333 Munich, Germany.	martin.kiechle@tum.de; tim@tum.de; simon.hawe@tum.de; kleinsteuber@tum.de			German Federal Ministry of Economics and Technology (BMWi) [KF3057001TL2]; Cluster of Excellence CoTeSys - Cognition for Technical Systems - German Research Foundation (DFG)	German Federal Ministry of Economics and Technology (BMWi)(Federal Ministry for Economic Affairs and Energy (BMWi)); Cluster of Excellence CoTeSys - Cognition for Technical Systems - German Research Foundation (DFG)(German Research Foundation (DFG))	This work was supported by the German Federal Ministry of Economics and Technology (BMWi) through Project KF3057001TL2 and by the Cluster of Excellence CoTeSys - Cognition for Technical Systems, funded by the German Research Foundation (DFG).	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210; BROWN LG, 1992, COMPUT SURV, V24, P325, DOI 10.1145/146370.146374; Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; CHAN D, 2008, P ECCV WORKSH MULT M; Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043; Cole-Rhodes AA, 2003, IEEE T IMAGE PROCESS, V12, P1495, DOI 10.1109/TIP.2003.819237; COLLIGNON A, 1995, COMP IMAG VIS, V3, P263; Dai YH, 2001, ANN OPER RES, V103, P33, DOI 10.1023/A:1012930416777; Diebel James, 2005, NEURAL INF PROCESS S, P291; Fan XF, 2010, IEEE T GEOSCI REMOTE, V48, P2580, DOI 10.1109/TGRS.2010.2040390; Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075; Hawe S, 2013, IEEE T IMAGE PROCESS, V22, P2138, DOI 10.1109/TIP.2013.2246175; Hyder MM, 2009, IEEE SIGNAL PROC LET, V16, P1091, DOI 10.1109/LSP.2009.2028107; Jia K, 2013, IEEE T PATTERN ANAL, V35, P367, DOI 10.1109/TPAMI.2012.95; Kiechle M., 2013, P INT C COMP VIS; Klein S, 2010, IEEE T MED IMAGING, V29, P196, DOI 10.1109/TMI.2009.2035616; Krotosky SJ, 2007, COMPUT VIS IMAGE UND, V106, P270, DOI 10.1016/j.cviu.2006.10.008; Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5; Lu JB, 2011, INT CONF ACOUST SPEE, P985; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Mattes D, 2003, IEEE T MED IMAGING, V22, P120, DOI 10.1109/TMI.2003.809072; Mishali M, 2008, IEEE T SIGNAL PROCES, V56, P4692, DOI 10.1109/TSP.2008.927802; Nam S, 2013, APPL COMPUT HARMON A, V34, P30, DOI 10.1016/j.acha.2012.03.006; Ophir B., 2011, EUR SIGN SPROC C EUS; Orchard J, 2007, IEEE T IMAGE PROCESS, V16, P2526, DOI 10.1109/TIP.2007.904956; Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282; Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867; Ravishankar S, 2013, IEEE T SIGNAL PROCES, V61, P1072, DOI 10.1109/TSP.2012.2226449; Rubinstein R, 2013, IEEE T SIGNAL PROCES, V61, DOI 10.1109/TSP.2012.2226445; Scharstein D, 2003, PROC CVPR IEEE, P195; Studholme C, 1999, PATTERN RECOGN, V32, P71, DOI 10.1016/S0031-3203(98)00091-0; Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030; Viola P, 1997, INT J COMPUT VISION, V24, P137, DOI 10.1023/A:1007958904918; Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930; Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yanjie Li, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P152, DOI 10.1109/ICME.2012.30; Zeyde R., 2012, CURVES SURFACES; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	44	12	13	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2015	114	2-3			SI		233	247		10.1007/s11263-014-0786-5	http://dx.doi.org/10.1007/s11263-014-0786-5			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CP7MJ		Green Submitted			2022-12-18	WOS:000360071900008
J	Stuckler, J; Behnke, S				Stueckler, Joerg; Behnke, Sven			Efficient Dense Rigid-Body Motion Segmentation and Estimation in RGB-D Video	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion segmentation; Rigid multi-body registration; Multibody structure-from-motion	FLOW	Motion is a fundamental grouping cue in video. Many current approaches to motion segmentation in monocular or stereo image sequences rely on sparse interest points or are dense but computationally demanding. We propose an efficient expectation-maximization (EM) framework for dense 3D segmentation of moving rigid parts in RGB-D video. Our approach segments images into pixel regions that undergo coherent 3D rigid-body motion. Our formulation treats background and foreground objects equally and poses no further assumptions on the motion of the camera or the objects than rigidness. While our EM-formulation is not restricted to a specific image representation, we supplement it with efficient image representation and registration for rapid segmentation of RGB-D video. In experiments, we demonstrate that our approach recovers segmentation and 3D motion at good precision.	[Stueckler, Joerg; Behnke, Sven] Univ Bonn, Comp Sci Inst 6, D-53113 Bonn, Germany	University of Bonn	Stuckler, J (corresponding author), Univ Bonn, Comp Sci Inst 6, Friedrich Ebert Allee 144, D-53113 Bonn, Germany.	stueckler@ais.uni-bonn.de; behnke@cs.uni-bonn.de	Behnke, Sven/B-5509-2013; Stückler, Jörg/AAK-9145-2021	Behnke, Sven/0000-0002-5040-7525; Stückler, Jörg/0000-0002-2328-4363				AGRAWAL M, 2005, P IEEE WORKSH MOT; Ayvaci A., 2009, P IEEE ICCV WORKSH; Bishop C.M, 2006, PATTERN RECOGN; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y. Y., 2001, P IEEE INT C COMP VI; Brox T, 2006, LECT NOTES COMPUT SC, V3951, P471; Cremers D, 2005, INT J COMPUT VISION, V62, P249, DOI 10.1007/s11263-005-4882-4; Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z; Drost B., 2010, IEEE INT C COMP VIS; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fitzpatrick P., 2003, P IEEE RSJ INT C INT; Gruber A., 2004, P IEEE INT C COMP VI; Hadfield S, 2014, IEEE T PATTERN ANAL, V36, P564, DOI 10.1109/TPAMI.2013.162; Herbst E., 2014, INT C ROB AUT ICRA; Herbst E, 2013, IEEE INT CONF ROBOT, P2276, DOI 10.1109/ICRA.2013.6630885; Hornacek M., 2014, P IEEE C COMP VIS PA; Huguet F., 2007, P IEEE INT C COMP VI; Kenney J., 2009, P IEEE ICRA; Kumar M. P., 2005, P INT C COMP VIS ICC; Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242; Quiroga J., 2013, P IEEE INT C IM PROC; Ross DA, 2010, INT J COMPUT VISION, V88, P214, DOI 10.1007/s11263-010-0325-y; Rothganger F, 2007, IEEE T PATTERN ANAL, V29, P477, DOI 10.1109/TPAMI.2007.57; Roussos A., 2012, P IEEE INT S MIX AUG; Saito M, 2012, PROC CVPR IEEE, P1680, DOI 10.1109/CVPR.2012.6247862; Schindler K, 2006, IEEE T PATTERN ANAL, V28, P983, DOI 10.1109/TPAMI.2006.130; Sekkati H, 2006, IEEE T IMAGE PROCESS, V15, P641, DOI 10.1109/TIP.2005.863699; Stuckler J, 2014, J VIS COMMUN IMAGE R, V25, P137, DOI 10.1016/j.jvcir.2013.02.008; Stuckler J, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.51; Unger M, 2012, PROC CVPR IEEE, P1878, DOI 10.1109/CVPR.2012.6247887; Van den Bergh M., 2012, IEEE WORKSH APPL COM; Wang SY, 2013, J SIGNAL PROCESS SYS, V71, P21, DOI 10.1007/s11265-012-0675-5; Weber J, 1997, IEEE T PATTERN ANAL, V19, P139, DOI 10.1109/34.574794; Wedel A, 2011, STEREO SCENE FLOW FOR 3D MOTION ANALYSIS, P1, DOI 10.1007/978-0-85729-965-9; Zelnik-Manor L, 2006, INT J COMPUT VISION, V68, P27, DOI 10.1007/s11263-005-4840-1; Zhang G., 2011, P IEEE INT C COMP VI	36	12	12	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2015	113	3			SI		233	245		10.1007/s11263-014-0796-3	http://dx.doi.org/10.1007/s11263-014-0796-3			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CK3BZ					2022-12-18	WOS:000356091900007
J	Ranzato, M; Hinton, G; Lecun, Y				Ranzato, Marc'Aurelio; Hinton, Geoffrey; LeCun, Yann			Guest Editorial: Deep Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Ranzato, Marc'Aurelio; LeCun, Yann] Facebook AI Res, Menlo Pk, CA 94025 USA; [Hinton, Geoffrey] Univ Toronto, Dept Comp Sci, Toronto, Canada; [Hinton, Geoffrey] Google Inc, Mountain View, CA USA; [LeCun, Yann] NYU, Courant Inst Math Sci, New York, NY USA	Facebook Inc; University of Toronto; Google Incorporated; New York University	Ranzato, M (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.	ranzato@fb.com; geoffrey.hinton@gmail.com; yann@cs.nyu.edu							0	12	12	2	111	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2015	113	1			SI		1	2		10.1007/s11263-015-0813-1	http://dx.doi.org/10.1007/s11263-015-0813-1			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CH6GX		Bronze			2022-12-18	WOS:000354135700001
J	Wang, YX; Lee, CM; Cheong, LF; Toh, KC				Wang, Yu-Xiang; Lee, Choon Meng; Cheong, Loong-Fah; Toh, Kim-Chuan			Practical Matrix Completion and Corruption Recovery Using Proximal Alternating Robust Subspace Minimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Low rank matrix completion; Robust matrix factorization; Non-convex optimization; SfM; Photometric stereo	LEAST-SQUARES; MISSING DATA; FACTORIZATION; ALGORITHM; OPTIMIZATION; GRADIENT; SPARSITY	Low-rank matrix completion is a problem of immense practical importance. Recent works on the subject often use nuclear norm as a convex surrogate of the rank function. Despite its solid theoretical foundation, the convex version of the problem often fails to work satisfactorily in real-life applications. Real data often suffer from very few observations, with support not meeting the randomness requirements, ubiquitous presence of noise and potentially gross corruptions, sometimes with these simultaneously occurring. This paper proposes a Proximal Alternating Robust Subspace Minimization method to tackle the three problems. The proximal alternating scheme explicitly exploits the rank constraint on the completed matrix and uses the pseudo-norm directly in the corruption recovery step. We show that the proposed method for the non-convex and non-smooth model converges to a stationary point. Although it is not guaranteed to find the global optimal solution, in practice we find that our algorithm can typically arrive at a good local minimizer when it is supplied with a reasonably good starting point based on convex optimization. Extensive experiments with challenging synthetic and real data demonstrate that our algorithm succeeds in a much larger range of practical problems where convex optimization fails, and it also outperforms various state-of-the-art algorithms.	[Wang, Yu-Xiang] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Wang, Yu-Xiang; Lee, Choon Meng; Cheong, Loong-Fah; Toh, Kim-Chuan] Natl Univ Singapore, Singapore 117548, Singapore	Carnegie Mellon University; National University of Singapore	Wang, YX (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	luke.yxwang@gmail.com; leechoonmeng@nus.edu.sg; eleclf@nus.edu.sg; mattohkc@nus.edu.sg	Toh, Kim-Chuan/A-6068-2010	Wang, Yu-Xiang/0000-0002-6403-212X	Singapore PSF [1321202075]	Singapore PSF	This work was supported by the Singapore PSF grant 1321202075.	[Anonymous], 2015, INT J COMPUT VIS, V111, P315; Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; Balzano L., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Becker S, 2012, TFOCS TEMPLATES 1 OR; Becker SR, 2011, MATH PROGRAM COMPUT, V3, P165, DOI 10.1007/s12532-011-0029-5; Bennett James, 2007, P KDD CUP WORKSH, V2007, P35; Buchanan AM, 2005, PROC CVPR IEEE, P316; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Chen P, 2008, INT J COMPUT VISION, V80, P125, DOI 10.1007/s11263-008-0135-7; Chen P, 2011, SIAM J NUMER ANAL, V49, P1417, DOI 10.1137/100799988; Clarke Frank H., 1990, OPTIMIZATION NONSMOO, V5; Del Bue A, 2012, IEEE T PATTERN ANAL, V34, P1496, DOI 10.1109/TPAMI.2011.238; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Eriksson A, 2010, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2010.5540139; Friedland S., 2006, 2006 IEEE INT C AC S, V2, pII; Funk S., 2006, NETFLIX UPDATE TRY T; GABRIEL KR, 1979, TECHNOMETRICS, V21, P489, DOI 10.2307/1268288; Grant M., 2013, CVX MATLAB SOFTWARE; Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7; Hartley R., 2003, AUSTR JAP ADV WORKSH, V74, P76; He J., 2011, IT 2011; HORN BKP, 1990, INT J COMPUT VISION, V5, P37, DOI 10.1007/BF00056771; Jain P., 2012, ARXIV12120467; Ke QF, 2005, PROC CVPR IEEE, P739; Keshavan RH, 2009, ANN ALLERTON CONF, P1216, DOI 10.1109/ALLERTON.2009.5394534; Kiraly F., 2012, P 29 INT C MACH LEAR, P967; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Koren Yehuda, 2009, NETFLIX PRIZE DOCUME, V81; Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92; Liang Xiong, 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P844, DOI 10.1109/ICDM.2011.52; Liu Y. J., 2009, MATH PROGRAM, P1; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226; Oh S. Y., 2010, WIRELESS DAYS WD 201, P1, DOI DOI 10.1109/WD.2010.5657708; Okatani T, 2007, INT J COMPUT VISION, V72, P329, DOI 10.1007/s11263-006-9785-5; Overton M. L., NLCG NONLINEAR CONJU; Paladini Marco, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2898, DOI 10.1109/CVPRW.2009.5206602; Rabaud V., VINCENTS STRUCTURES; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Recht B., 2009, ARXIV09100651; Rudin W., 1987, REAL ANAL, V84; Shi X., 2011, ACM SIGKDD EXPLORATI, V12, P16; Srebro N., 2003, P 20 INT C MACHINE L, P720; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; Toh KC, 2010, PAC J OPTIM, V6, P615; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Wang Y. X., 2012, P 29 INT C MACH LEAR, P417; Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1; Wu B, 2014, SIAM J OPTIMIZ, V24, P766, DOI 10.1137/110827144; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Yang JF, 2013, SIAM J OPTIMIZ, V23, P857, DOI 10.1137/120864192; Yu HF, 2012, IEEE DATA MINING, P765, DOI 10.1109/ICDM.2012.168; Yudong Chen, 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2313, DOI 10.1109/ISIT.2011.6033975	62	12	14	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2015	111	3					315	344		10.1007/s11263-014-0746-0	http://dx.doi.org/10.1007/s11263-014-0746-0			30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CB9XK		Green Submitted			2022-12-18	WOS:000349987400004
J	Potesil, V; Kadir, T; Platsch, G; Brady, SM				Potesil, Vaclav; Kadir, Timor; Platsch, Guenther; Brady, Sir Michael			Personalized Graphical Models for Anatomical Landmark Localization in Whole-Body Medical Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Landmark localization; Parts-based graphical model; Model personalization; Image registration	KNOWLEDGE-BASED SEGMENTATION; ACTIVE APPEARANCE MODELS; OBJECT RECOGNITION; LABEL FUSION; REGISTRATION	The goal of this work is to accurately and reliably localize anatomical landmarks in 3D Computed Tomography (CT) scans of the upper bodies of cancer patients even in the presence of pathologies and imaging artifacts that may markedly change the appearances of anatomical structures. We propose a method based on dense matching of parts-based graphical models. For landmark localization, we replace population averaged models by personalized models that are adapted to each test image at runtime. We do so by jointly leveraging weighted combinations of labeled training exemplars. We report results for localizing standard anatomical landmarks in clinical 3D CT volumes, using a database of 83 lung cancer patients. We compare our method against both (baseline) population averaged graphical models and against atlas-based deformable registration and show the method is in each case able to localize landmarks with significantly improved reliability and accuracy.	[Potesil, Vaclav] Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England; [Kadir, Timor] Mirada Med Ltd, Oxford, England; [Platsch, Guenther] Siemens Mol Imaging Ltd, Oxford, England; [Brady, Sir Michael] Univ Oxford, Dept Oncol, Oxford, England	University of Oxford; Siemens AG; University of Oxford	Potesil, V (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	vaclav@robots.ox.ac.uk						Beichel R, 2005, IEEE T MED IMAGING, V24, P1151, DOI 10.1109/TMI.2005.853237; Besbes A, 2011, I S BIOMED IMAGING, P989, DOI 10.1109/ISBI.2011.5872568; Besbes A, 2009, PROC CVPR IEEE, P1295, DOI 10.1109/CVPRW.2009.5206649; Buehler Patrick, 2008, P BRIT MACH VIS C; CHUM O., 2007, P INT C COMP VIS; COOTES TF, 1994, IMAGE VISION COMPUT, V12, P355, DOI 10.1016/0262-8856(94)90060-4; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Corso J., 2008, P MED IM COMP COMP A; Crandall D., 2005, P IEEE C COMP VIS PA; Crandall DJ, 2006, LECT NOTES COMPUT SC, V3951, P16; Criminisi A, 2011, LECT NOTES COMPUT SC, V6533, P106, DOI 10.1007/978-3-642-18421-5_11; Criminisi Antonio, 2009, MED IMAGE COMPUTING, P69; Donner R., 2007, P BRIT MACH VIS C; Eichner M., 2009, P BRIT MACH VIS C; Felzenszwalb P., 2008, P IEEE C COMP VIS PA; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Fergus R, 2007, INT J COMPUT VISION, V71, P273, DOI 10.1007/s11263-006-8707-x; Fischler M., 1973, IEEE T COMPUT, V100, P67; LAN X, 2005, P INT C COMP VIS; Langerak TR, 2010, IEEE T MED IMAGING, V29, P2000, DOI 10.1109/TMI.2010.2057442; Langs G, 2007, LECT NOTES COMPUT SC, V4791, P968; Ou Y, 2010, I S BIOMED IMAGING, P400, DOI 10.1109/ISBI.2010.5490324; Potesil V., 2010, P BRIT MACH VIS C; Ramanan D., 2007, P NEUR INF PROC SYST; Roche A, 1998, LECT NOTES COMPUT SC, V1496, P1115, DOI 10.1007/BFb0056301; Sabuncu MR, 2010, IEEE T MED IMAGING, V29, P1714, DOI 10.1109/TMI.2010.2050897; Sapp B, 2010, PROC CVPR IEEE, P422, DOI 10.1109/CVPR.2010.5540182; Schmidt S., 2007, P INF PROC MED IM; Sigal L., 2006, P IEEE C COMP VIS PA; Toews M, 2007, IEEE T MED IMAGING, V26, P497, DOI 10.1109/TMI.2007.892510; Wang CH, 2010, LECT NOTES COMPUT SC, V6363, P189; Warfield SK, 2004, IEEE T MED IMAGING, V23, P903, DOI 10.1109/TMI.2004.828354; Worz S, 2006, MED IMAGE ANAL, V10, P41, DOI 10.1016/j.media.2005.02.003; Wright T., 2008, TRUED DEFORMABLE REG; Xiang B, 2012, 2012 9TH IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING (ISBI), P1425, DOI 10.1109/ISBI.2012.6235836; Zhang P., 2012, IEEE T MED IMAGING, V99, P1; Zhang P., 2010, P MED IM COMP COMP A; Zhang P, 2011, LECT NOTES COMPUT SC, V6801, P636, DOI 10.1007/978-3-642-22092-0_52	38	12	12	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2015	111	1					29	49		10.1007/s11263-014-0731-7	http://dx.doi.org/10.1007/s11263-014-0731-7			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AZ6RC					2022-12-18	WOS:000348345400003
J	Satkin, S; Rashid, M; Lin, J; Hebert, M				Satkin, Scott; Rashid, Maheen; Lin, Jason; Hebert, Martial			3DNN: 3D Nearest Neighbor Data-Driven Geometric Scene Understanding Using 3D Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision; Machine learning; Scene understanding; Geometry estimation; 3D data	OBJECT	In this paper, we describe a data-driven approach to leverage repositories of 3D models for scene understanding. Our ability to relate what we see in an image to a large collection of 3D models allows us to transfer information from these models, creating a rich understanding of the scene. We develop a framework for auto-calibrating a camera, rendering 3D models from the viewpoint an image was taken, and computing a similarity measure between each 3D model and an input image. We demonstrate this data-driven approach in the context of geometry estimation and show the ability to find the identities, poses and styles of objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints. Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-before-seen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation, as well as two novel applications: affordance estimation and photorealistic object insertion.	[Satkin, Scott] Google Inc, Mountain View, CA 94043 USA; [Rashid, Maheen; Hebert, Martial] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Lin, Jason] Microsoft Corp, Redmond, WA 98052 USA	Google Incorporated; Carnegie Mellon University; Microsoft	Satkin, S (corresponding author), Google Inc, Mountain View, CA 94043 USA.	satkin@google.com; maheenr@andrew.cmu.edu; jasonlin@alumni.cmu.edu; hebert@ri.cmu.edu			ONR MURI [N000141010934]	ONR MURI(MURIOffice of Naval Research)	This work was supported in part by ONR MURI N000141010934.	[Anonymous], 2012, ACM T GRAPHIC, DOI DOI 10.1145/2366145.2366155; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Baatz G., 2012, P EUR C COMP VIS ECC; Baboud L., 2011, P IEEE C COMP VIS PA; Bao S., 2010, P IEEE C COMP VIS PA; Brooks R. A., 1981, ARTIFICIAL INTELLIGE, V17; Choi Wongun, 2013, P IEEE C COMP VIS PA; Colburn A, 2013, IEEE T VIS COMPUT GR, V19, P56, DOI 10.1109/TVCG.2012.95; Debevec P., 1998, P 25 ANN C COMP GRAP; Deng J., 2009, 2009 IEEE C COMP VIS, P248, DOI [DOI 10.1109/CVPR.2009.5206848, 10.1109/CVPR.2009.5206848]; Efros A.A, 2008, P IEEE C COMP VIS PA; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929; Fisher M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866204; Fouhey D. F., 2013, P IEEE INT C COMP VI; Fouhey D.F., 2012, P EUR C COMP VIS ECC; Girshick RB, 2012, DISCRIMINATIVELY TRA; Google Inc, 2000, GOOGL SKETCHUP; Grabner H, 2011, PROC CVPR IEEE, P1529, DOI 10.1109/CVPR.2011.5995327; Grimson W. E. L., 1990, OBJECT RECOGNITION C; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; Guo R., 2011, P IEEE C COMP VIS PA; Gupta A., 2010, P EUR C COMP VIS ECC; Gupta Abhinav, 2011, P IEEE C COMP VIS PA; Hays J., 2007, P SIGGRAPH, DOI [10.1145/1275808.1276382, DOI 10.1145/1275808.1276382]; Hedau V., 2010, P EUR C COMP VIS ECC; Hedau V., 2012, P IEEE C COMP VIS PA; Hedau Varsha, 2009, P IEEE INT C COMP VI; Herbrich R, 1999, IEE CONF PUBL, P97, DOI 10.1049/cp:19991091; Hoiem D., 2008, P IEEE C COMP VIS PA; Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y; IKEA, 2013, IKEA CAT; Karsch K., 2014, ACM T GRAPH IN PRESS; Karsch K., 2011, P 2011 SIGGRAPH AS C; LAI K, 2009, P ROB SCI SYST SEATT; Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239454, 10.1145/1276377.1276381]; Lee D. C., 2009, P IEEE C COMP VIS PA; Lee D.C., 2010, P C NEUR INF PROC SY; Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168; Li L. J., 2010, NIPS P, V2, P5; Lim Joseph J, 2013, P IEEE INT C COMP VI; Liu C., 2008, P EUR C COMP VIS ECC; Liu M. Y., 2010, P IEEE C COMP VIS PA; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; Microsoft Corporation, 2010, KIN XBOX 360; Munoz D., 2010, P EUR C COMP VIS ECC; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Oliva A., 2006, PROGR BRAIN RES, V42, P145; Pero L. D., 2011, P IEEE C COMP VIS PA; Pero L. D., 2013, P IEEE C COMP VIS PA; Pero L. D., 2012, P IEEE C COMP VIS PA; Polantis, 2010, POL 3D CAT; Ramalingam S., 2010, IEEE RSJ INT C INT R; Satkin S., 2013, P IEEE INT C COMP VI; Satkin S., 2012, CMU 3D ANNOTATED SCE; Satkin S., 2013, THESIS CARNEGIE MELL; Satkin S, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.128; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Schwing A. G., 2012, P EUR C COMP VIS ECC; Silberman N., 2012, P EUR C COMP VIS ECC; Sing G., 2013, P IEEE C COMP VIS PA; Tighe J., 2010, P EUR C COMP VIS ECC; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Torralba A, 2010, P IEEE, V98, P1467, DOI 10.1109/JPROC.2010.2050290; Triggs B, 2005, P IEEE INT C COMP VI; Trimble Inc, 2012, TRIMBL 3D WAR; Vondrick C., 2013, INVERTING VISUALIZIN; Wang H., 2010, P EUR C COMP VIS ECC; Xiao J, 2010, P IEEE C COMP VIS PA; Yu S., 2008, P IEEE WORKSH PERC O; Zhao Y., 2013, P IEEE C COMP VIS PA	72	12	13	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2015	111	1					69	97		10.1007/s11263-014-0734-4	http://dx.doi.org/10.1007/s11263-014-0734-4			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AZ6RC					2022-12-18	WOS:000348345400005
J	Couture, V; Martin, N; Roy, S				Couture, Vincent; Martin, Nicolas; Roy, Sebastien			Unstructured Light Scanning Robust to Indirect Illumination and Depth Discontinuities	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Active reconstruction; Global illumination; Indirect illumination; Depth discontinuities	APPROXIMATE NEAREST-NEIGHBOR; OPTIMAL HASHING ALGORITHMS; STRUCTURED LIGHT; ACQUISITION	Reconstruction from structured light can be greatly affected by indirect illumination such as interreflections between surfaces in the scene and sub-surface scattering. This paper introduces band-pass white noise patterns designed specifically to reduce the effects of indirect illumination, and still be robust to standard challenges in scanning systems such as scene depth discontinuities, defocus and low camera-projector pixel ratio. While this approach uses unstructured light patterns that increase the number of required projected images, it is up to our knowledge the first method that is able to recover scene disparities in the presence of both indirect illumination and scene discontinuities. Furthermore, the method does not require calibration (geometric nor photometric) or post-processing such as phase unwrapping or interpolation from sparse correspondences. We show results for a few challenging scenes and compare them to correspondences obtained with the Phase-shift method and the recently introduced method by Gupta et al., designed specifically to handle indirect illumination.	[Couture, Vincent; Martin, Nicolas; Roy, Sebastien] Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3T IJ4, Canada	Universite de Montreal	Couture, V (corresponding author), Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3T IJ4, Canada.	chapdelv@iro.umontreal.ca; martinic@iro.umontreal.ca; roys@iro.umontreal.ca						Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494; Andoni A, 2006, ANN IEEE SYMP FOUND, P459; BOYER KL, 1987, IEEE T PATTERN ANAL, V9, P14, DOI 10.1109/TPAMI.1987.4767869; Bracewell R, 1986, PENTAGRAM NOTATION C, V2nd, P192; Caspi D, 1998, IEEE T PATTERN ANAL, V20, P470, DOI 10.1109/34.682177; Chen T., 2008, IEEE C COMP VIS PATT; Couture V., 2011, IEEE INT C COMP VIS; Davis J, 2005, IEEE T PATTERN ANAL, V27, P296, DOI 10.1109/TPAMI.2005.37; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Goddyn L., 2003, ELECTRON J COMB, V10, P27; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; Gu J., 2011, IEEE INT C COMP VIS, P1; Guhring J, 2001, PROC SPIE, V4309, P220; Gupta M, 2012, PATTERN DISCOVERY USING SEQUENCE DATA MINING: APPLICATIONS AND STUDIES, P1, DOI 10.4018/978-1-61350-056-9.ch001; Gupta M, 2011, PROC CVPR IEEE, P713, DOI 10.1109/CVPR.2011.5995321; Hermans C, 2009, PROC CVPR IEEE, P1865, DOI 10.1109/CVPRW.2009.5206610; Inokuchi S., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P806; Kushnir Anner, 2007, 3DTV Conference, 2007, P1; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Minou M., 1981, Transactions of the Institute of Electronics and Communication Engineers of Japan, Section E (English), VE64, P521; Nayar SK, 2006, ACM T GRAPHIC, V25, P935, DOI 10.1145/1141911.1141977; Peers P., 2009, COMPRESSIVE LIGHT TR; POSDAMER JL, 1982, COMPUT VISION GRAPH, V18, P1, DOI 10.1016/0146-664X(82)90096-X; Proesmans M., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P336, DOI 10.1109/ICPR.1996.546966; Salvi J, 2004, PATTERN RECOGN, V37, P827, DOI 10.1016/j.patcog.2003.10.002; Salvi J, 1998, PATTERN RECOGN LETT, V19, P1055, DOI 10.1016/S0167-8655(98)00085-3; Scharstein D, 2003, PROC CVPR IEEE, P195; Tardif JP, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P217, DOI 10.1109/im.2003.1240253; VUYLSTEKE P, 1990, IEEE T PATTERN ANAL, V12, P148, DOI 10.1109/34.44402; Wexler Y, 2003, PROC CVPR IEEE, P209; Wust C., 1991, Machine Vision and Applications, V4, P193, DOI 10.1007/BF01230201; Xu Y, 2009, IEEE T VIS COMPUT GR, V15, P465, DOI 10.1109/TVCG.2008.97; Zhang L, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P24, DOI 10.1109/TDPVT.2002.1024035; Zhang S, 2007, OPT ENG, V46, DOI 10.1117/1.2802546	34	12	12	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2014	108	3					204	221		10.1007/s11263-014-0701-0	http://dx.doi.org/10.1007/s11263-014-0701-0			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AH8NO					2022-12-18	WOS:000336394900003
J	Haines, TSF; Xiang, T				Haines, Tom S. F.; Xiang, Tao			Active Rare Class Discovery and Classification Using Dirichlet Processes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Active learning; Rare class discovery; Classification		Classification is used to solve countless problems. Many real world computer vision problems, such as visual surveillance, contain uninteresting but common classes alongside interesting but rare classes. The rare classes are often unknown, and need to be discovered whilst training a classifier. Given a data set active learning selects the members within it to be labelled for the purpose of constructing a classifier, optimising the choice to get the best classifier for the least amount of effort. We propose an active learning method for scenarios with unknown, rare classes, where the problems of classification and rare class discovery need to be tackled jointly. By assuming a non-parametric prior on the data the goals of new class discovery and classification refinement are automatically balanced, without any tunable parameters. The ability to work with any specific classifier is maintained, so it may be used with the technique most appropriate for the problem at hand. Results are provided for a large variety of problems, demonstrating superior performance.	[Haines, Tom S. F.; Xiang, Tao] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England	University of London; Queen Mary University London	Haines, TSF (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.	thaines@eecs.qmul.ac.uk; txiang@eecs.qmul.ac.uk		Haines, Tom/0000-0002-7181-3223				Abe N., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P1; Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1023/A:1022821128753; [Anonymous], 2010, BAYESIAN NONPARAMETR; BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Culotta Aron, 2005, AAAI, P746; Dagan I., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P150; Dupuit J, 1844, INT EC PAPERS, V8, P256; ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Frank A., 2010, UCI MACHINE LEARNING; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197; Haines T. S. F., 2011, P BMVC; Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38; He J., 2007, NEURAL INFORM PROCES, V21; Hodge VJ, 2004, ARTIF INTELL REV, V22, P85, DOI 10.1023/B:AIRE.0000045502.10941.a9; Hospedales T. M., 2011, PAC AS C KNOWL DISC, V15; Huang Gary B., 2007, 0749 U MASS, P7; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; Lee Y. J., 2010, P CVPR; Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3; Loy CC, 2012, P CVPR; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Maloof MA, 2003, MACH LEARN, V53, P157, DOI 10.1023/A:1025623527461; McCallumzy A.K., 1998, ICML, V98, P359; MITCHELL TM, 1982, ARTIF INTELL, V18, P203, DOI 10.1016/0004-3702(82)90040-6; Olsson Fredrik, 2009, LIT SURVEY ACTIVE MA; Pelleg D., 2004, ADV NEURAL INF PROCE, P1073; PICARD RW, 1995, MULTIMEDIA SYST, V3, P3, DOI 10.1007/BF01236575; Platt JC, 2000, ADV NEUR IN, P61; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Roy Nicholas, 2001, P 18 INT C MACH LEAR, P441; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Settles B., 2009, ACTIVE LEARNING LIT; Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417; Sillito RR, 2007, LECT NOTES COMPUT SC, V4668, P58; Stokes J. W., 2008, 20082024 MICR RES; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Tin Kam Ho, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P278, DOI 10.1109/ICDAR.1995.598994; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Vatturi P, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P847; Vlachos A., 2010, P 2010 WORKSH GEOMET, P57	47	12	12	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2014	106	3			SI		315	331		10.1007/s11263-013-0630-3	http://dx.doi.org/10.1007/s11263-013-0630-3			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AA3DC		Green Submitted, Green Accepted			2022-12-18	WOS:000330972100007
J	Worz, S; Rohr, K				Woerz, Stefan; Rohr, Karl			Spline-Based Hybrid Image Registration using Landmark and Intensity Information based on Matrix-Valued Non-radial Basis Functions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Elastic image registration; Hybrid registration; Gaussian elastic body splines; Analytic solution	ELASTIC REGISTRATION; NONRIGID REGISTRATION; ALGORITHM; ROBUST; DENSE	We introduce a new approach for spline-based elastic image registration using both point landmarks and intensity information. With this approach, both types of information as well as a regularization based on the Navier equation are directly integrated in a single energy minimizing functional. For this functional we have derived an analytic solution, which is based on matrix-valued non-radial basis functions. With our approach the full 3D intensity information is exploited, i.e., all voxels are considered and subsampling using a grid is not required. A special case of our hybrid approach is obtained by disregarding the landmark information, which results in a pure intensity-based elastic registration approach. We have successfully applied our approach to 3D synthetic images, 2D MR images of the human brain, 2D gel electrophoresis images, and 3D CT lung images.	[Woerz, Stefan; Rohr, Karl] Heidelberg Univ, Dept Bioinformat & Funct Genom, Biomed Comp Vis Grp, BIOQUANT,IPMB, D-69120 Heidelberg, Germany; [Woerz, Stefan; Rohr, Karl] DKFZ Heidelberg, D-69120 Heidelberg, Germany	Ruprecht Karls University Heidelberg; Helmholtz Association; German Cancer Research Center (DKFZ)	Worz, S (corresponding author), Heidelberg Univ, Dept Bioinformat & Funct Genom, Biomed Comp Vis Grp, BIOQUANT,IPMB, Neuenheimer Feld 267, D-69120 Heidelberg, Germany.	s.woerz@dkfz.de			Deutsche Forschungsgemeinschaft (DFG) within the project ELASTIR [RO 2471/2]	Deutsche Forschungsgemeinschaft (DFG) within the project ELASTIR(German Research Foundation (DFG))	This work has been funded by the Deutsche Forschungsgemeinschaft (DFG) within the project ELASTIR (RO 2471/2). We thank Marie-Luise Winz and Andreas Biesdorf for performing the experiments on gel electrophoresis images and synthetic images, respectively. The original MR images and the tumor outlines (see Fig. 5) have kindly been provided by Prof. Dr. med. U. Spetzger, Department of Neurosurgery, Stadtisches Klinikum Karlsruhe, Germany, and Prof. Dr. med. J.M. Gilsbach, Neurosurgical Clinic, University Hospital Aachen of the RWTH, Germany. The gel electrophoresis images (see Fig. 7) are courtesy of Prof. G-Z. Yang, Royal Society/Wolfson MIC Laboratory, Dept. of Computing, Imperial College of Science, Technology, and Medicine, London/UK.	ARAD N, 1995, COMPUT GRAPH FORUM, V14, P35, DOI 10.1111/1467-8659.1410035; B Delhay, 2006, P WORKSH STAT ATL PE, P87; Bergvall E, 2008, IEEE T MED IMAGING, V27, P1045, DOI 10.1109/TMI.2008.917244; Berth M, 2007, APPL MICROBIOL BIOT, V76, P1223, DOI 10.1007/s00253-007-1128-0; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Cachier P, 2004, J MATH IMAGING VIS, V20, P251, DOI 10.1023/B:JMIV.0000024042.88755.4f; CACHIER P, 2001, LNCS, V2208, P734; Cao KL, 2010, PROC SPIE, V7623, DOI 10.1117/12.844541; Chou PC, 1992, ELASTICITY TENSOR DY; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Clatz O, 2005, IEEE T MED IMAGING, V24, P1417, DOI 10.1109/TMI.2005.856734; Crum WR, 2004, BRIT J RADIOL, V77, pS140, DOI 10.1259/bjr/25329214; Davis MH, 1997, IEEE T MED IMAGING, V16, P317, DOI 10.1109/42.585766; Dowsey AW, 2003, PROTEOMICS, V3, P1567, DOI 10.1002/pmic.200300459; Fischer B, 2004, METHOD INFORM MED, V43, P327; Fornefett M, 2001, IMAGE VISION COMPUT, V19, P87, DOI 10.1016/S0262-8856(00)00057-3; GEE JC, 1994, P SOC PHOTO-OPT INS, V2167, P327, DOI 10.1117/12.175067; Glocker B, 2008, MED IMAGE ANAL, V12, P731, DOI 10.1016/j.media.2008.03.006; Gorbunova V, 2010, I S BIOMED IMAGING, P340, DOI 10.1109/ISBI.2010.5490341; Hartkens T., 2002, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2002. 5th International Conference. Proceedings, Part II (Lecture Notes in Computer Science Vol.2489), P565; Hellier P, 2003, IEEE T MED IMAGING, V22, P217, DOI 10.1109/TMI.2002.808365; Holden M, 2008, IEEE T MED IMAGING, V27, P111, DOI 10.1109/TMI.2007.904691; Hub M, 2009, IEEE T MED IMAGING, V28, P1708, DOI 10.1109/TMI.2009.2021063; Johnson HJ, 2002, IEEE T MED IMAGING, V21, P450, DOI 10.1109/TMI.2002.1009381; Joshi SC, 2000, IEEE T IMAGE PROCESS, V9, P1357, DOI 10.1109/83.855431; Kabus S., 2008, 1 INT WORKSH PULM IM, P73; Kabus S, 2009, LECT NOTES COMPUT SC, V5761, P747, DOI 10.1007/978-3-642-04268-3_92; Kaus MR, 2007, INT J RADIAT ONCOL, V68, P572, DOI 10.1016/j.ijrobp.2007.01.056; Klein S, 2007, IEEE T IMAGE PROCESS, V16, P2879, DOI 10.1109/TIP.2007.909412; Kohlrausch J, 2005, J MATH IMAGING VIS, V23, P253, DOI 10.1007/s10851-005-0483-7; Kybic J, 2003, IEEE T IMAGE PROCESS, V12, P1427, DOI 10.1109/TIP.2003.813139; Lu HX, 2011, I S BIOMED IMAGING, P594, DOI 10.1109/ISBI.2011.5872477; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; Marsland S, 2004, IEEE T MED IMAGING, V23, P1006, DOI 10.1109/TMI.2004.831228; Meyer C R, 1997, Med Image Anal, V1, P195, DOI 10.1016/S1361-8415(97)85010-4; Murphy K, 2011, MED IMAGE ANAL, V15, P71, DOI 10.1016/j.media.2010.07.005; Osechinskiy S., 2011, ANATOMY RES INT, V287860, P17; Papademetris X, 2004, LECT NOTES COMPUT SC, V3216, P763; Paquin D, 2007, MATH BIOSCI ENG, V4, P711, DOI 10.3934/mbe.2007.4.711; Park HJ, 2004, MED IMAGE ANAL, V8, P465, DOI 10.1016/j.media.2004.03.001; Pekar V, 2006, PHYS MED BIOL, V51, P361, DOI 10.1088/0031-9155/51/2/012; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Reinhardt JM., 2009, P 2 INT WORKSHOP PUL, P137; Rogers M, 2007, IEEE T IMAGE PROCESS, V16, P624, DOI 10.1109/TIP.2007.891342; Rohde GK, 2003, IEEE T MED IMAGING, V22, P1470, DOI 10.1109/TMI.2003.819299; Rohr K, 2004, PATTERN RECOGN, V37, P1035, DOI 10.1016/j.patcog.2003.10.004; Rohr K, 2001, IEEE T MED IMAGING, V20, P526, DOI 10.1109/42.929618; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Sarrut D, 2006, MED PHYS, V33, P605, DOI 10.1118/1.2161409; Sato M, 1999, P SOC PHOTO-OPT INS, V3661, P774, DOI 10.1117/12.348634; Serifovic-Trbalic A., 2006, INT J CARS, V33, P605; Skrinjar O, 2009, I S BIOMED IMAGING, P891, DOI 10.1109/ISBI.2009.5193196; Sorzano COS, 2005, IEEE T BIO-MED ENG, V52, P652, DOI 10.1109/TBME.2005.844030; Sotiras A, 2010, LECT NOTES COMPUT SC, V6362, P676; Stewart CV, 2004, LECT NOTES COMPUT SC, V3216, P870; Teng CC, 2006, I S BIOMED IMAGING, P462; Thevenaz P, 1998, IEEE T IMAGE PROCESS, V7, P27, DOI 10.1109/83.650848; Vandemeulebroucke J, 2007, 15 INT C US COMP RAD, P195; Veeser S, 2001, PROTEOMICS, V1, P856; von Berg J, 2007, PROC SPIE, V6511, DOI 10.1117/12.709395; Worz S, 2008, COMPUT VIS IMAGE UND, V111, P263, DOI 10.1016/j.cviu.2007.12.003; Worz S, 2008, PROC SPIE, V6914, DOI 10.1117/12.769448; Worz S, 2008, I S BIOMED IMAGING, P1135, DOI 10.1109/ISBI.2008.4541201; Wu GR, 2010, NEUROIMAGE, V49, P2225, DOI 10.1016/j.neuroimage.2009.10.065; Xiuying Wang, 2004, Proceedings. Third International Conference on Image and Graphics, P208; Yang JZ, 2011, PATTERN RECOGN, V44, P764, DOI 10.1016/j.patcog.2010.10.009; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	67	12	13	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2014	106	1					76	92		10.1007/s11263-013-0642-z	http://dx.doi.org/10.1007/s11263-013-0642-z			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	288QM					2022-12-18	WOS:000329626800004
J	Jahanbin, S; Jahanbin, R; Bovik, AC				Jahanbin, Sina; Jahanbin, Rana; Bovik, Alan C.			Passive Three Dimensional Face Recognition Using Iso-Geodesic Contours and Procrustes Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face recognition; 3-D surface representation; Classifier fusion; Stepwise-LDA; Iso-geodesic contours	3D; SHAPE	We introduce a new model for personal recognition based on the 3-D geometry of the face. The model is designed for application scenarios where the acquisition conditions constrain the facial position. The 3-D structure of a facial surface is compactly represented by sets of contours (facial contours) extracted around automatically pinpointed nose tip and inner eye corners. The metric used to decide whether a point on the face belongs to a facial contour is its geodesic distance from a given landmark. Iso-geodesic contours are inherently robust to head pose variations, including in-depth rotations of the face. Since these contours are extracted from rigid parts of the face, the resulting recognition algorithms are insensitive to changes in facial expressions. The facial contours are encoded using innovative pose invariant features, including Procrustean distances defined on pose-invariant curves. The extracted features are combined in a hierarchical manner to create three parallel face recognizers. Inspired by the effectiveness of region ensembles approaches, the three recognizers constructed around the nose tip and inner corners of the eyes are fused both at the feature-level and the match score-level to create a unified face recognition algorithm with boosted performance. The performances of the proposed algorithms are evaluated and compared with other algorithms from the literature on a large public database appropriate for the assumed constrained application scenario.	[Jahanbin, Sina] KLA Tencor Corp, Milpitas, CA 95035 USA; [Jahanbin, Rana] Harmonic Inc, San Jose, CA 95134 USA; [Bovik, Alan C.] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	KLA Corporation; University of Texas System; University of Texas Austin	Jahanbin, S (corresponding author), KLA Tencor Corp, Milpitas, CA 95035 USA.	sina.jahanbin@utexas.edu; rana_jahanbin@alumni.utexas.net; bovik@ece.utexas.edu	Bovik, Alan/B-6717-2012	Bovik, Alan/0000-0001-6067-710X	ADIR; IRIS International	ADIR; IRIS International	The authors thank Dr. Kenneth Castleman, ADIR, and IRIS International for not only providing the Texas 3-D Face Recognition Database of facial range and portrait images, but also for assisting the research financially. This work draws continued inspiration from Dr. Castleman's ideas.	Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Boehnen C, 2009, LECT NOTES COMPUT SC, V5558, P12, DOI 10.1007/978-3-642-01793-3_2; Bowyer KW, 2006, COMPUT VIS IMAGE UND, V101, P1, DOI 10.1016/j.cviu.2005.05.005; Bowyer KW, 2004, SURVEY 3D MULTIMODAL; Bronstein AM, 2005, INT J COMPUT VISION, V64, P5, DOI 10.1007/s11263-005-1085-y; Chang KI, 2003, ACM WORKSH MULT US A, P25; Chin-Seng Chua, 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P233, DOI 10.1109/AFGR.2000.840640; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Egan J.P., 1975, SIGNAL DETECTION THE; Faltemier TC, 2008, IEEE T INF FOREN SEC, V3, P62, DOI 10.1109/TIFS.2007.916287; Gokberk B, 2005, LECT NOTES COMPUT SC, V3546, P1019; Grother P, 2003, LECT NOTES COMPUT SC, V2688, P937; GUPTA S, 2007, PATTERN RECOGN, P63; GUPTA S, 2010, IEEE SW S IM AN INT, P25; Gupta S, 2010, INT J COMPUT VISION, V90, P331, DOI 10.1007/s11263-010-0360-8; Heseltine T, 2004, LECT NOTES COMPUT SC, V3212, P684; Hesher C, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 2, PROCEEDINGS, P201, DOI 10.1109/ISSPA.2003.1224850; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; Jahanbin S, 2008, 2008 IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS & INTERPRETATION, P25, DOI 10.1109/SSIAI.2008.4512276; Jahanbin Sina, 2008, 2 IEEE INT C BIOM TH, P1; Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017; Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431; KUHL FP, 1982, COMPUT VISION GRAPH, V18, P236, DOI 10.1016/0146-664X(82)90034-X; LATTA N, 2004, US VISIT KEEPING AM; Lu XG, 2006, IEEE T PATTERN ANAL, V28, P31, DOI 10.1109/TPAMI.2006.15; Lu XG, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P156; Mahoor MH, 2009, PATTERN RECOGN, V42, P445, DOI 10.1016/j.patcog.2008.08.012; Maurer T, 2005, P IEEE C COMP VIS PA, P154, DOI DOI 10.1109/CVPR.2005.581; MCKEON R, 2010, 4 IEEE INT C BIOM TH, P1; Milnor J., 1969, ANN MATH STUDIES, V51; MOKHTARIAN F, 1986, IEEE T PATTERN ANAL, V8, P34, DOI 10.1109/TPAMI.1986.4767750; MOORTHY AK, 2010, IEEE 4 INT C BIOM TH; MORGAN D, 2005, BIOMETRIC IDENTIFIER; Mpiperis I, 2007, PROCEEDINGS OF THE FOURTH IASTED INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, PATTERN RECOGNITION, AND APPLICATIONS, P328; Mpiperis I, 2007, IEEE T INF FOREN SEC, V2, P537, DOI 10.1109/TIFS.2007.902326; NAGAMINE T, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL I, P324, DOI 10.1109/ICPR.1992.201567; PEURA M, 1997, ASPECTS VISUAL FORM, P443; Peyre G, 2006, INT J COMPUT VISION, V69, P145, DOI 10.1007/s11263-006-6859-3; Phillips P., 2003, FACE RECOGNITION VEN; Phillips PJ, 2010, IEEE T PATTERN ANAL, V32, P831, DOI 10.1109/TPAMI.2009.59; Phillips PJ, 2005, PROC CVPR IEEE, P947; Queirolo CC, 2010, IEEE T PATTERN ANAL, V32, P206, DOI 10.1109/TPAMI.2009.14; Rizvi SA, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P48, DOI 10.1109/AFGR.1998.670924; Rosin PL, 2003, MACH VISION APPL, V14, P172, DOI 10.1007/s00138-002-0118-6; Ross A.A., 2006, HDB MULTIBIOMETRICS; RUSS T, 2006, IEEE C COMP VIS PATT, V2, P1391; Samir C, 2006, IEEE T PATTERN ANAL, V28, P1858, DOI 10.1109/TPAMI.2006.235; Sharma S, 1995, APPL MULTIVARIATE TE; Sun Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P263, DOI 10.1109/ICCV.2001.937634; Trier OD, 1996, PATTERN RECOGN, V29, P641, DOI 10.1016/0031-3203(95)00118-2; ZOU SCL, 2007, IEEE T INF FOREN SEC, V2, P513; [No title captured]	53	12	13	1	34	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2013	105	1					87	108		10.1007/s11263-013-0631-2	http://dx.doi.org/10.1007/s11263-013-0631-2			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	194HL					2022-12-18	WOS:000322621800005
J	Taylor, S; Drummond, T				Taylor, Simon; Drummond, Tom			Binary Histogrammed Intensity Patches for Efficient and Robust Matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image matching; Keypoint recognition; Feature extraction; Keypoint matching; Matching by classification; Pose estimation; Real-time system		This paper describes a method for feature-based matching which offers very fast runtime performance due to the simple quantised patches used for matching and a tree-based lookup scheme which prevents the need for exhaustively comparing each query patch against the entire feature database. The method enables seven independently moving targets in a test sequence to be localised in an average total processing time of 6.03 ms per frame. A training phase is employed to identify the most repeatable features from a particular range of viewpoints and to learn a model for the patches corresponding to each feature. Feature models consist of independent histograms of quantised intensity for each pixel in the patch, which we refer to as Histogrammed Intensity Patches (HIPs). The histogram values are thresholded and the feature model is stored in a compact binary representation which requires under 60 bytes of memory per feature and permits the rapid computation of a matching score using bitwise operations. The method achieves better matching robustness than the state-of-the-art fast localisation schemes introduced by Wagner et al. (IEEE International Symposium on Mixed and Augmented Reality, 2008). Additionally both the runtime memory usage and computation time are reduced by a factor of more than four.	[Taylor, Simon] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England; [Drummond, Tom] Monash Univ, Dept Elect & Comp Syst Engn, Clayton, Vic 3800, Australia	University of Cambridge; Monash University	Taylor, S (corresponding author), Univ Cambridge, Dept Engn, Trumpington St, Cambridge CB2 1PZ, England.	sjt59@cam.ac.uk; Tom.Drummond@monash.edu	Drummond, Tom/A-4696-2011	Drummond, Tom/0000-0001-8204-5904	Boeing Company	Boeing Company	This work was supported by The Boeing Company.	BALLARD DH, 1987, READINGS COMPUTER VI, V1, P714; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Brown M, 2005, PROC CVPR IEEE, P510; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; CROES GA, 1958, OPER RES, V6, P791, DOI 10.1287/opre.6.6.791; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Heikkila M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014; HINTERSTOISSER S, 2008, BRIT MACH VIS C; Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; MCILROY P, 2010, BRIT MACH VIS C; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561; MIKOLAJCZYK K, 2002, ECCV, P128; Moravec, 1981, IJCAI, V81, P785, DOI DOI 10.1007/S00427-011-0383-3; Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730; OZUYSAL M, 2007, IEEE COMP SOC C COMP; Rosten E., 2006, ECCV, P430, DOI DOI 10.1007/11744023_; Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215; Taylor S., 2009, BRIT MACH VIS C; Taylor S., 2009, IEEE CVPR WORKSH FEA; Wagner D., 2008, IEEE INT S MIX AUGM; WINDER S, 2007, IEEE COMP SOC C COMP	25	12	12	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2011	94	2					241	265		10.1007/s11263-011-0430-6	http://dx.doi.org/10.1007/s11263-011-0430-6			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	775VB					2022-12-18	WOS:000291490400005
J	Astola, L; Florack, L				Astola, Laura; Florack, Luc			Finsler Geometry on Higher Order Tensor Fields and Applications to High Angular Resolution Diffusion Imaging	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Finsler geometry; Higher order tensors; High Angular Resolution Diffusion Imaging (HARDI)		We study 3D-multidirectional images, using Finsler geometry. The application considered here is in medical image analysis, specifically in High Angular Resolution Diffusion Imaging (HARDI) (Tuch et al. in Magn. Reson. Med. 48(6):1358-1372, 2004) of the brain. The goal is to reveal the architecture of the neural fibers in brain white matter. To the variety of existing techniques, we wish to add novel approaches that exploit differential geometry and tensor calculus. In Diffusion Tensor Imaging (DTI), the diffusion of water is modeled by a symmetric positive definite second order tensor, leading naturally to a Riemannian geometric framework. A limitation is that it is based on the assumption that there exists a single dominant direction of fibers restricting the thermal motion of water molecules. Using HARDI data and higher order tensor models, we can extract multiple relevant directions, and Finsler geometry provides the natural geometric generalization appropriate for multi-fiber analysis. In this paper we provide an exact criterion to determine whether a spherical function satisfies the strong convexity criterion essential for a Finsler norm. We also show a novel fiber tracking method in Finsler setting. Our model incorporates a scale parameter, which can be beneficial in view of the noisy nature of the data. We demonstrate our methods on analytic as well as simulated and real HARDI data.	[Astola, Laura; Florack, Luc] Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5600 MB Eindhoven, Netherlands	Eindhoven University of Technology	Astola, L (corresponding author), Eindhoven Univ Technol, Dept Math & Comp Sci, POB 513, NL-5600 MB Eindhoven, Netherlands.	l.j.astola@tue.nl; l.m.j.florack@tue.nl			Netherlands Organization for Scientific Research (NWO)	Netherlands Organization for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	The Netherlands Organization for Scientific Research (NWO) is gratefully acknowledged for financial support. We want to thank Vesna Prckovska and Paulo Rodrigues for kindly providing a program to simulate crossings in HARDI data. We are specially grateful to Evgeniya Balmashnova and Jim Portegies for their valuable advice and to Biomedical Image Analysis group at Eindhoven University of Technology for providing the human brain data.	[Anonymous], 2001, LECT FINSLER GEOMETR; ASTOLA L, 2008, CVPR, V20, P1; Astola L, 2007, LECT NOTES COMPUT SC, V4584, P642; Bao D., 2000, INTRO RIEMANN FINSLE, V200; Barmpoutis A, 2007, LECT NOTES COMPUT SC, V4584, P308; Bulow T, 2004, IEEE T PATTERN ANAL, V26, P1650, DOI 10.1109/TPAMI.2004.129; Carmo MP., 1993, RIEMANNIAN GEOMETRY; Descoteaux M, 2007, MAGN RESON MED, V58, P497, DOI 10.1002/mrm.21277; FLORACK L, 2008, P 18 INT C COMP GRAP, P26; FLORACK L, 2008, CVPR, P20; Florack L. M. J., 2008, MICCAI WORKSH COMP D, P94; Frank LR, 2002, MAGN RESON MED, V47, P1083, DOI 10.1002/mrm.10156; FUSTER A, 2009, P 13 INT C COMP AN I; Jansons KM, 2003, INVERSE PROBL, V19, P1031, DOI 10.1088/0266-5611/19/5/303; Jian B, 2007, NEUROIMAGE, V37, P164, DOI 10.1016/j.neuroimage.2007.03.074; Lenglet C, 2004, LECT NOTES COMPUT SC, V2034, P127; Melonakos J, 2008, IEEE T PATTERN ANAL, V30, P412, DOI 10.1109/TPAMI.2007.70713; Mori S., 2009, INTRO DIFFUSION TENS, DOI DOI 10.1016/B978-0-444-52828-5.X5014-5; O'Donnell L, 2002, LECT NOTES COMPUT SC, V2488, P459; Ozarslan E, 2003, MAGN RESON MED, V50, P955, DOI 10.1002/mrm.10596; Ozarslan E, 2006, NEUROIMAGE, V31, P1086, DOI 10.1016/j.neuroimage.2006.01.024; PRADOS E, 2006, CVPR 06, P1076; Rosner B., 2006, FUNDAMENTALS BIOSTAT; STEJSKAL EO, 1965, J CHEM PHYS, V42, P288, DOI 10.1063/1.1695690; TUCH D, 2004, MAGNETIC RESONANCE M, V52, P577; Tuch DS, 2004, MAGN RESON MED, V52, P1358, DOI 10.1002/mrm.20279; YOKONUMA T, 1992, TENSOR SPACES EXTERI, V108	27	12	12	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2011	92	3					325	336		10.1007/s11263-010-0377-z	http://dx.doi.org/10.1007/s11263-010-0377-z			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	729DS		Bronze, Green Submitted			2022-12-18	WOS:000287929400006
J	Brunet, F; Gay-Bellile, V; Bartoli, A; Navab, N; Malgouyres, R				Brunet, Florent; Gay-Bellile, Vincent; Bartoli, Adrien; Navab, Nassir; Malgouyres, Remy			Feature-Driven Direct Non-Rigid Image Registration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Direct registration; Inverse compositional image alignment; Deformable model	DEFORMATIONS; LANDMARK	The direct registration problem for images of a deforming surface has been well studied. Parametric flexible warps based, for instance, on the Free-Form Deformation or a Radial Basis Function such as the Thin-Plate Spline, are often estimated using additive Gauss-Newton-like algorithms. The recently proposed compositional framework has been shown to be more efficient, but cannot be directly applied to such non-groupwise warps. Our main contribution in this paper is the Feature-Driven framework. It makes possible the use of compositional algorithms for most parametric warps such as those above mentioned. Two algorithms are proposed to demonstrate the relevance of our Feature-Driven framework: the Feature-Driven Inverse Compositional and the Feature-Driven Learning-based algorithms. As another contribution, a detailed derivation of the Feature-Driven warp parameterization is given for the Thin-Plate Spline and the Free-Form Deformation. We experimentally show that these two types of warps have a similar representational power. Experimental results show that our Feature-Driven registration algorithms are more efficient in terms of computational cost, without loss of accuracy, compared to existing methods.	[Brunet, Florent; Bartoli, Adrien] Univ Auvergne, ISIT, Clermont Ferrand, France; [Brunet, Florent; Navab, Nassir] TUM, CAMPAR, Munich, Germany; [Gay-Bellile, Vincent] CEA LIST, Vis & Content Engn Lab, F-91191 Gif Sur Yvette, France; [Malgouyres, Remy] LIMOS, UMR 6158, Clermont Ferrand, France	Universite Clermont Auvergne (UCA); Technical University of Munich; CEA; Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA)	Brunet, F (corresponding author), Univ Auvergne, ISIT, Clermont Ferrand, France.	florent@florentbrunet.com						Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; BARTOLI A, 2004, P BRIT MACH VIS C; BENHIMANE S, 2004, P INT C INT ROB SYST; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Boor C. D., 2001, PRACTICAL GUIDE SPLI; BREGLER C, 2000, P INT C COMP VIS PAT; CHARPIAT G, 2005, P INT C COMP VIS; COOTES TF, 2004, P EUR C COMP VIS; Cootes Timothy F, 1998, P EUR C COMP VIS; FORNEFETT M, 1999, P INT C COMP VIS PAT; GAYBELLILE, 2007, P BRIT MACH VIS C; GAYBELLILE V, 2006, P INT C IM PROC; GEORGEL P, 2008, P BRIT MACH VIS C; Haber E, 2006, LECT NOTES COMPUT SC, V4191, P726; Johnson HJ, 2002, IEEE T MED IMAGING, V21, P450, DOI 10.1109/TMI.2002.1009381; Joshi SC, 2000, IEEE T IMAGE PROCESS, V9, P1357, DOI 10.1109/83.855431; Jurie F, 2002, IEEE T PATTERN ANAL, V24, P996, DOI 10.1109/TPAMI.2002.1017625; LIM J, 2005, P INT C COMP VIS PAT; Little JA, 1997, COMPUT VIS IMAGE UND, V66, P223, DOI 10.1006/cviu.1997.0608; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Meyer C R, 1997, Med Image Anal, V1, P195, DOI 10.1016/S1361-8415(97)85010-4; PILET J, 2005, P INT C COMP VIS PAT; Pizarro D, 2007, LECT NOTES COMPUT SC, V4522, P928; Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867; Romdhani S., 2003, P INT C COMP VIS; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009; TORR PHS, 1999, WORKSH VIS ALG THEOR; Wahba G., 1990, CBMS NSF REGIONAL C, V59	30	12	12	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2011	93	1					33	52		10.1007/s11263-010-0407-x	http://dx.doi.org/10.1007/s11263-010-0407-x			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	738WJ					2022-12-18	WOS:000288673000003
J	Rumpf, M; Wirth, B				Rumpf, Martin; Wirth, Benedikt			An Elasticity-Based Covariance Analysis of Shapes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape analysis; Principal components; Non-rigid registration; Nonlinear elasticity; Covariance metric; Phase field approximation; Finite element discretization	METRICS; APPROXIMATION; CONSTRUCTION; EXISTENCE; ANATOMY; MODELS; ATLAS; SPACE	We introduce the covariance of a number of given shapes if they are interpreted as boundary contours of elastic objects. Based on the notion of nonlinear elastic deformations from one shape to another, a suitable linearization of geometric shape variations is introduced. Once such a linearization is available, a principal component analysis can be investigated. This requires the definition of a covariance metric-an inner product on linearized shape variations. The resulting covariance operator robustly captures strongly nonlinear geometric variations in a physically meaningful way and allows to extract the dominant modes of shape variation. The underlying elasticity concept represents an alternative to Riemannian shape statistics. In this paper we compare a standard L (2)-type covariance metric with a metric based on the Hessian of the nonlinear elastic energy. Furthermore, we explore the dependence of the principal component analysis on the type of the underlying nonlinear elasticity. For the built-in pairwise elastic registration, a relaxed model formulation is employed which allows for a non-exact matching. Shape contours are approximated by single well phase fields, which enables an extension of the method to a covariance analysis of image morphologies. The model is implemented with multilinear finite elements embedded in a multi-scale approach. The characteristics of the approach are demonstrated on a number of illustrative and real world examples in 2D and 3D.	[Rumpf, Martin; Wirth, Benedikt] Univ Bonn, D-53113 Bonn, Germany	University of Bonn	Wirth, B (corresponding author), Univ Bonn, D-53113 Bonn, Germany.	martin.rumpf@ins.uni-bonn.de; benedikt.wirth@ins.uni-bonn.de			German Science Foundation via the Hausdorff Center for Mathematics; Bonn International Graduate School;  [SPP 1253]	German Science Foundation via the Hausdorff Center for Mathematics; Bonn International Graduate School; 	The authors thank Guillermo Sapiro for pointing them to the issue of an elastic principal component analysis. We are grateful to Heiko Schlarb from adidas, Herzogenaurach, Germany, for providing 3D scans of feet. Furthermore, we acknowledge support by the German Science Foundation via the Hausdorff Center for Mathematics and the SPP 1253. Benedikt Wirth has been supported by the Bonn International Graduate School.	AMBROSIO L, 1990, COMMUN PUR APPL MATH, V43, P999, DOI 10.1002/cpa.3160430805; AMBROSIO L, 1992, B UNIONE MAT ITAL, V6B, P105; BALL JM, 1981, P ROY SOC EDINB A, V88, P315, DOI 10.1017/S030821050002014X; Bhatia KK, 2007, LECT NOTES COMPUT SC, V4792, P544; Bhatia KK, 2004, 2004 2ND IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: MACRO TO NANO, VOLS 1 AND 2, P908; Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1; Chalmond B, 1999, IEEE T PATTERN ANAL, V21, P422, DOI 10.1109/34.765654; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chan TF, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P161, DOI 10.1109/VLSM.2001.938895; Charpiat G, 2005, FOUND COMPUT MATH, V5, P1, DOI 10.1007/s10208-003-0094-x; CHARPIAT G, 2006, ACOUSTICS SPEECH SIG, V5; CHIPOT M, 1986, P ROY SOC EDINB A, V102, P291, DOI 10.1017/S0308210500026378; CIARLET PG, 1988, 3 DEMENSIONAL ELASTI; Cremers D, 2003, PATTERN RECOGN, V36, P1929, DOI 10.1016/S0031-3203(03)00056-6; DALMASO G, 1992, ACTA MATH-DJURSHOLM, V168, P89, DOI 10.1007/BF02392977; Dambreville S, 2006, LECT NOTES COMPUT SC, V4141, P173; DEGIORGI E, 1989, ARCH RATION MECH AN, V108, P195, DOI 10.1007/BF01052971; Faugeras O, 2004, NEUROIMAGE, V23, pS46, DOI 10.1016/j.neuroimage.2004.07.015; Fletcher PT, 2003, PROC CVPR IEEE, P95; FLETCHER T, 2008, IEEE C COMP VIS PATT; FUCHS M, 2008, SHAPE METRICS BASED; Fuchs M, 2009, J MATH IMAGING VIS, V35, P86, DOI 10.1007/s10851-009-0156-z; Hafner BJ, 2000, MED BIOL ENG COMPUT, V38, P9, DOI 10.1007/BF02344682; Joshi S, 2004, NEUROIMAGE, V23, pS151, DOI 10.1016/j.neuroimage.2004.07.068; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kilian M, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276457, 10.1145/1239451.1239515]; Leventon M.E., 2002, 5 IEEE EMBS INT SUMM; Marsland S, 2003, LECT NOTES COMPUT SC, V2879, P771; Memoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y; Miller MI, 2002, ANNU REV BIOMED ENG, V4, P375, DOI 10.1146/annurev.bioeng.4.092101.125733; Miller MI, 2001, INT J COMPUT VISION, V41, P61, DOI 10.1023/A:1011161132514; MOREL J.-M., 1988, REV MAT COMPLUT, V1, P169; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Perperidis D, 2005, LECT NOTES COMPUT SC, V3750, P402, DOI 10.1007/11566489_50; Rathi Y, 2006, LECT NOTES COMPUT SC, V4241, P96; RUECKERT D, 2001, LECT NOTES COMPUTER, V2208, P77; Rumpf M, 2009, SIAM J IMAGING SCI, V2, P800, DOI 10.1137/080738337; Rumpf M, 2009, LECT NOTES COMPUT SC, V5567, P709; Sohn M, 2005, PHYS MED BIOL, V50, P5893, DOI 10.1088/0031-9155/50/24/009; Srivastava A, 2006, LECT NOTES COMPUT SC, V3851, P612; Studholme C, 2003, LECT NOTES COMPUT SC, V2717, P81; Wirth B, 2009, LECT NOTES COMPUT SC, V5681, P288, DOI 10.1007/978-3-642-03641-5_22; Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685	45	12	12	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2011	92	3					281	295		10.1007/s11263-010-0358-2	http://dx.doi.org/10.1007/s11263-010-0358-2			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	729DS					2022-12-18	WOS:000287929400003
J	Breitenreicher, D; Schnorr, C				Breitenreicher, Dirk; Schnoerr, Christoph			Model-Based Multiple Rigid Object Detection and Registration in Unstructured Range Data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Range data; Point set registration; Multiple object detection; Geometric optimization; Sparse convex programming	OPTIMIZATION; RECOGNITION; ALGORITHMS; RECOVERY	We present a two-stage approach to the simultaneous detection and registration of multiple instances of industrial 3D objects in unstructured noisy range data. The first non-local processing stage takes all data into account and computes in parallel multiple localizations of the object along with rough pose estimates. The second stage computes accurate registrations for all detected object instances individually by using local optimization. Both stages are designed using advanced numerical techniques, large-scale sparse convex programming, and second-order geometric optimization on the Euclidean manifold, respectively. They complement each other in that conflicting interpretations are resolved through non-local convex processing, followed by accurate non-convex local optimization based on sufficiently good initializations. As input data a sparse point sample of the object's surface is required exclusively. Our experiments focus on industrial applications where multiple 3D object instances are randomly assembled in a bin, occlude each other, and unstructured noisy range data is acquired by a laser scanning device.	[Breitenreicher, Dirk; Schnoerr, Christoph] Heidelberg Univ, Image & Pattern Anal Grp IPA, Heidelberg Collaboratory Image Proc HCI, D-69115 Heidelberg, Germany	Ruprecht Karls University Heidelberg	Breitenreicher, D (corresponding author), Heidelberg Univ, Image & Pattern Anal Grp IPA, Heidelberg Collaboratory Image Proc HCI, Speyerer Str 6, D-69115 Heidelberg, Germany.	breitenreicher@math.uni-heidelberg.de; schnoerr@math.uni-heidelberg.de			VMT Vision Machine Technic Bildverarbeitungssysteme GmbH, a company of the Pepperl + Fuchs Group	VMT Vision Machine Technic Bildverarbeitungssysteme GmbH, a company of the Pepperl + Fuchs Group	The authors would like to thank the VMT Vision Machine Technic Bildverarbeitungssysteme GmbH, a company of the Pepperl + Fuchs Group, for supporting this research work, and to Stefania Petra, Jan Lellmann and Florian Becker from our IPA group for continuously discussing with us various optimization issues.	Adler RL, 2002, IMA J NUMER ANAL, V22, P359, DOI 10.1093/imanum/22.3.359; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Benhimane S, 2006, IEEE INT CONF ROBOT, P526, DOI 10.1109/ROBOT.2006.1641764; Bennett KP, 2006, J MACH LEARN RES, V7, P1265; Bertsimas D., 2005, OPTIMIZATION INTEGER; BESL PJ, 1985, COMPUT SURV, V17, P75, DOI 10.1145/4078.4081; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Birgin EG, 2000, SIAM J OPTIMIZ, V10, P1196, DOI 10.1137/S1052623497330963; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; BREITENREICHER D, 2009, 7 INT WORKSH EN MIN; Breitenreicher D, 2010, MACH VISION APPL, V21, P601, DOI 10.1007/s00138-009-0227-6; Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; CHIN RT, 1986, COMPUT SURV, V18, P67, DOI 10.1145/6462.6464; Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186; Chui HL, 2000, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P190, DOI 10.1109/MMBIA.2000.852377; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; do Carmo M. P., 1992, RIEMANNIAN GEOMETRY; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Drummond T, 2002, IMAGE VISION COMPUT, V20, P427, DOI 10.1016/S0262-8856(02)00013-6; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fitzgibbon AW, 2003, IMAGE VISION COMPUT, V21, P1145, DOI 10.1016/j.imavis.2003.09.004; Frome A., 2004, P EUR C COMP VIS; GELFAND N, 2005, P S GEOM PROC; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Greenspan MA, 2002, IEEE T PATTERN ANAL, V24, P495, DOI 10.1109/34.993557; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; JIAN B, 2005, P INT C COMP VIS; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Krishnan Shankar, 2007, International Journal of Intelligent Systems Technologies and Applications, V3, P319, DOI 10.1504/IJISTA.2007.014267; Lavva I, 2008, IEEE T SYST MAN CY B, V38, P826, DOI 10.1109/TSMCB.2008.918567; Li H, 2007, PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON STRUCTURE HEALTH MONITORING & INTELLIGENT INFRASTRUCTURE: STRUCTURAL HEALTH MONITORING & INTELLIGENT INFRASTRUCTURE; Matsushima Y., 1972, DIFFERENTIABLE MANIF; MITRA NJ, 2004, P S GEOM PROC; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Olson CF, 1997, INT J COMPUT VISION, V23, P131, DOI 10.1023/A:1007906812782; Olsson C, 2009, IEEE T PATTERN ANAL, V31, P783, DOI 10.1109/TPAMI.2008.131; Pottmann H, 2006, INT J COMPUT VISION, V67, P277, DOI 10.1007/s11263-006-5167-2; RANGARAJAN A, 1997, P INT C INF PROC MED; Reyes L, 2007, INT J COMPUT VISION, V75, P351, DOI 10.1007/s11263-007-0038-z; ROCKAFELLAR R. T., 1998, GRUNDLEHREN MATH WIS, V317; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Salvi J, 2007, IMAGE VISION COMPUT, V25, P578, DOI 10.1016/j.imavis.2006.05.012; Shang LM, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P297; SHI Q, 2006, INT C INT ROB SYST; Subbarao R, 2009, INT J COMPUT VISION, V84, P1, DOI 10.1007/s11263-008-0195-8; Taylor CJ, 1994, 9405 YAL U CTR SYST; Teboulle M, 2007, J MACH LEARN RES, V8, P65; Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; TSIN Y, 2004, P EUR C COMP VIS, V3, P558; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; WANG F, 2006, P EUR C COMP VIS; WIEDEMANN C, 2008, PATTERN RECOGN; Wolfson HJ, 1997, IEEE COMPUT SCI ENG, V4, P10, DOI 10.1109/99.641604; Zhu L, 2007, INT J ADV MANUF TECH, V32, P505, DOI 10.1007/s00170-005-0370-9	59	12	14	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2011	92	1					32	52		10.1007/s11263-010-0401-3	http://dx.doi.org/10.1007/s11263-010-0401-3			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	729GH					2022-12-18	WOS:000287936200002
J	Ribnick, E; Papanikolopoulos, N				Ribnick, Evan; Papanikolopoulos, Nikolaos			3D Reconstruction of Periodic Motion from a Single View	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Periodic motion; Single-view 3D reconstruction; Human motion analysis		Periodicity has been recognized as an important cue for tasks like activity recognition and gait analysis. However, most existing techniques analyze periodic motions only in image coordinates, making them very dependent on the viewing angle. In this paper we show that it is possible to reconstruct a periodic trajectory in 3D given only its appearance in image coordinates from a single camera view. We draw a strong analogy between this problem and that of reconstructing an object from multiple views, which allows us to rely on well-known theoretical results from the multi-view geometry domain and obtain significant guarantees regarding the solvability of the estimation problem. We present two different formulations of the problem, along with techniques for performing the reconstruction in both cases, and an algorithm for estimating the period of motion from its image-coordinate trajectory. Experimental results demonstrate the feasibility of the proposed techniques.	[Papanikolopoulos, Nikolaos] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA; [Ribnick, Evan] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities; University of Minnesota System; University of Minnesota Twin Cities	Papanikolopoulos, N (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	ribnick@cs.umn.edu; npapas@cs.umn.edu			Department of Homeland Security; Center for Transportation Studies; ITS Institute at the University of Minnesota; Minnesota Department of Transportation; U.S. Army Research Laboratory; U.S. Army Research Office [911NF-08-1-0463, 55111-CI]; National Science Foundation [IIS-0219863, CNS-0224363, CNS-0324864, CNS-0420836, IIP-0443945, IIP-0726109, CNS-0708344]	Department of Homeland Security(United States Department of Homeland Security (DHS)); Center for Transportation Studies; ITS Institute at the University of Minnesota(University of Minnesota System); Minnesota Department of Transportation; U.S. Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL)); U.S. Army Research Office; National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported in part by the Department of Homeland Security, the Center for Transportation Studies and the ITS Institute at the University of Minnesota, the Minnesota Department of Transportation, the U.S. Army Research Laboratory and the U.S. Army Research Office under contract # 911NF-08-1-0463 (Proposal 55111-CI), and the National Science Foundation through grants # IIS-0219863, # CNS-0224363, # CNS-0324864, # CNS-0420836, # IIP-0443945, # IIP-0726109, and # CNS-0708344.	ALLMEN M, 1990, P IEEE INT C PATT RE, V1, P365; [Anonymous], 1999, NUMERICAL OPTIMIZATI; Belongie S., 2004, 1 INT WORKSH P SPAT, P16; Boyd S, 2004, CONVEX OPTIMIZATION; Briassouli A, 2007, IEEE T PATTERN ANAL, V29, P1244, DOI 10.1109/TPAMI.2007.1042.; Cohen CJ, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P60, DOI 10.1109/AFGR.1996.557244; Cutler R, 2000, IEEE T PATTERN ANAL, V22, P781, DOI 10.1109/34.868681; Davis J, 2000, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2000.855878; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Eggert DW, 1997, MACH VISION APPL, V9, P272, DOI 10.1007/s001380050048; Elgammal A, 2004, PROC CVPR IEEE, P681; FOSSATI A, 2008, P CVPR WORKSH NONR S; Fujii S, 2004, ELECTRON COMM JPN 1, V87, P1, DOI 10.1002/ecja.10181; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hurley N, 2008, MACHINE LEARN SIGN P, P55, DOI 10.1109/MLSP.2008.4685455; Little J., 1995, Proceedings International Symposium on Computer Vision (Cat. No.95TB100006), P235, DOI 10.1109/ISCV.1995.477007; Liu F., 1998, P IEEE C COMP VIS PA; Ormoneit D, 2005, IMAGE VISION COMPUT, V23, P1264, DOI 10.1016/j.imavis.2005.09.004; Orriols X., 2003, P IEEE INT C IM PROC, V1; Polana R, 1997, INT J COMPUT VISION, V23, P261, DOI 10.1023/A:1007975200487; Ramanan D, 2007, IEEE T PATTERN ANAL, V29, P65, DOI 10.1109/TPAMI.2007.250600; Ribnick E., 2009, P IEEE RSJ INT C INT; Ribnick E., 2008, P EUR C COMP VIS; Ribnick E, 2009, IEEE T PATTERN ANAL, V31, P938, DOI 10.1109/TPAMI.2008.247; Schaible S, 2003, OPTIM METHOD SOFTW, V18, P219, DOI 10.1080/1055678031000105242; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Seitz SM, 1997, INT J COMPUT VISION, V25, P231, DOI 10.1023/A:1007928103394; Sigal L., 2006, P IEEE C COMP VIS PA; TSAI PS, 1994, PATTERN RECOGN, V27, P1591, DOI 10.1016/0031-3203(94)90079-5; Zhang ZH, 2007, NEURAL COMPUT, V19, P1400, DOI 10.1162/neco.2007.19.5.1400	30	12	12	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2010	90	1					28	44		10.1007/s11263-010-0334-x	http://dx.doi.org/10.1007/s11263-010-0334-x			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	616GA		Green Submitted			2022-12-18	WOS:000279195800002
J	Withagen, PJ; Schutte, K; Groen, FCA				Withagen, P. J.; Schutte, K.; Groen, F. C. A.			Global Intensity Correction in Dynamic Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Intensity correction; Intensity variation; Camera calibration; Pixel classification; Time-varying imagery	ILLUMINATION	Changing image intensities causes problems for many computer vision applications operating in unconstrained environments. We propose generally applicable algorithms to correct for global differences in intensity between images recorded with a static or slowly moving camera, regardless of the cause of intensity variation. The proposed intensity correction is based on intensity-quotient estimation. Various intensity estimation methods are compared. Usability is evaluated with background classification as example application. For this application we introduced the PIPE error measure evaluating performance and robustness to parameter setting. Our approach retains local intensity information, is always operational and can cope with fast changes in intensity. We show that for intensity estimation, robustness to outliers is essential for dynamic scenes. For image sequences with changing intensity, the best performing algorithm (MofQ) improves foreground-background classification results up to a factor two to four on real data.	[Withagen, P. J.] Philips Healthcare, Xray Imaging Innovat, NL-5680 DA Best, Netherlands; [Schutte, K.] TNO Def Secur & Safety, Electro Opt Grp, NL-2509 JG The Hague, Netherlands; [Groen, F. C. A.] Univ Amsterdam, Fac Sci, Inst Informat, NL-1098 SJ Amsterdam, Netherlands	Philips; Philips Healthcare; Netherlands Organization Applied Science Research; University of Amsterdam	Withagen, PJ (corresponding author), Philips Healthcare, Xray Imaging Innovat, Veenpluis 6,POB 10-000, NL-5680 DA Best, Netherlands.	Paul.Withagen@philips.com; Klamer.Schutte@tno.nl; Groen@science.uva.nl						Altunbasak Y, 2003, IEEE T IMAGE PROCESS, V12, P395, DOI 10.1109/TIP.2003.809012; Cox IJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pB366; Greiffenhagen M, 2001, PROC CVPR IEEE, P704; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Harville M, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P3, DOI 10.1109/EVENT.2001.938860; He YH, 2003, 2003 IEEE INTELLIGENT TRANSPORTATION SYSTEMS PROCEEDINGS, VOLS. 1 & 2, P515; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; HISCHOF H, 2004, COMPUTER VISION IMAG, V95, P86; HORPRASERT T, 1999, P IEEE ICCV 99 FRAME, P751; Hsieh JW, 2003, IMAGE VISION COMPUT, V21, P505, DOI 10.1016/S0262-8856(03)00030-1; Jabri S, 2000, INT C PATT RECOG, P627, DOI 10.1109/ICPR.2000.902997; Jacobs DW, 1998, PROC CVPR IEEE, P610, DOI 10.1109/CVPR.1998.698668; Javed O, 2002, LECT NOTES COMPUT SC, V2353, P343; Kamikura K, 1998, IEEE T CIRC SYST VID, V8, P988, DOI 10.1109/76.736731; McKenna SJ, 2000, COMPUT VIS IMAGE UND, V80, P42, DOI 10.1006/cviu.2000.0870; OHTA N, 2001, P IEEE INT C COMP VI, P751; PAVLIDIS I, 2001, P EUR WORKSH ADV VID, P285; PRIEBE CE, 1994, J AM STAT ASSOC, V89, P796; Provost F, 2001, MACH LEARN, V42, P203, DOI 10.1023/A:1007601015854; Scott M. J. J., 1998, BMVC 98. Proceedings of the Ninth British Machine Vision Conference, P306; Siebert A, 2001, PATTERN RECOGN LETT, V22, P249, DOI 10.1016/S0167-8655(00)00107-0; Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677; Toth D., 2000, 4th IEEE Southwest Symposium on Image Analysis and Interpretation, P3, DOI 10.1109/IAI.2000.839561; TOYAMA K, 1999, INT C COMP VIS, P255, DOI DOI 10.1109/ICCV.1999.791228; Tsin Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P480, DOI 10.1109/ICCV.2001.937555; Withagen P, 2004, INT C PATT RECOG, P31, DOI 10.1109/ICPR.2004.1333998; Withagen PJ, 2008, IEEE IMTC P, P119, DOI 10.1109/IMTC.2008.4547015; Withagen PJ, 2007, IEEE T INSTRUM MEAS, V56, P199, DOI 10.1109/TIM.2006.887667; WITHAGEN PJ, 2006, THESIS U AMSTERDAM; WITHAGEN PJ, 2005, P IEEE INSTR MEAS TE, P2232; Xie BL, 2004, IMAGE VISION COMPUT, V22, P117, DOI 10.1016/j.imavis.2003.07.003	31	12	12	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2010	86	1					33	47		10.1007/s11263-009-0247-8	http://dx.doi.org/10.1007/s11263-009-0247-8			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	534MZ		Green Submitted, Green Published, Bronze			2022-12-18	WOS:000272903100002
J	Bartoli, A				Bartoli, Adrien			On Computing the Prediction Sum of Squares Statistic in Linear Least Squares Problems with Multiple Parameter or Measurement Sets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						PRESS; Cross-validation; Registration; Image warp; Deformation centre; Thin-Plate Spline	CROSS-VALIDATION; SELECTION; WARPS	The prediction sum of squares is a useful statistic for comparing different models. It is based on the principle of leave-one-out or ordinary cross-validation, whereby every measurement is considered in turn as a test set, for the model parameters trained on all but the held out measurement. As for linear least squares problems, there is a simple well-known non-iterative formula to compute the prediction sum of squares without having to refit the model as many times as the number of measurements. We extend this formula to cases where the problem has multiple parameter or measurement sets. We report experimental results on the fitting of a warp between two images, for which the number of deformation centres is automatically selected, based on one of the proposed non-iterative formulae.	Univ Clermont Ferrand, CNRS, LASMEA, Clermont Ferrand, France	Centre National de la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA)	Bartoli, A (corresponding author), Univ Clermont Ferrand, CNRS, LASMEA, Clermont Ferrand, France.	Adrien.Bartoli@gmail.com						ALLEN DM, 1974, TECHNOMETRICS, V16, P125, DOI 10.2307/1267500; BARTOLI A, 2007, INT C COMP VIS PATT; Bartoli A, 2008, J MATH IMAGING VIS, V31, P133, DOI 10.1007/s10851-007-0062-1; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; DUCHON J, 1976, REV FR AUTOMAT INFOR, V10, P5; Gentle J. E., 2004, HDB COMPUTATIONAL ST; Gheissari N, 2008, IMAGE VISION COMPUT, V26, P1636, DOI 10.1016/j.imavis.2008.04.001; Hartley R., 2003, MULTIPLE VIEW GEOMET; Luong QT, 1996, COMPUT VIS IMAGE UND, V64, P193, DOI 10.1006/cviu.1996.0055; Montgomery D.C., 1992, INTRO LINEAR REGRESS; Tarpey T, 2000, AM STAT, V54, P116, DOI 10.2307/2686028; WAHBA G, 1975, COMMUN STAT, V4, P1, DOI 10.1080/03610927508827223	12	12	12	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2009	85	2					133	142		10.1007/s11263-009-0253-x	http://dx.doi.org/10.1007/s11263-009-0253-x			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	488JD					2022-12-18	WOS:000269344500001
J	Tejos, C; Irarrazaval, P; Cardenas-Blanco, A				Tejos, Cristian; Irarrazaval, Pablo; Cardenas-Blanco, Arturo			Simplex Mesh Diffusion Snakes: Integrating 2D and 3D Deformable Models and Statistical Shape Knowledge in a Variational Framework	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Segmentation; Deformable Models; Active Contours; Statistical Shape Knowledge; MRI; Occlusion	ACTIVE CONTOUR MODELS; SEGMENTATION; GRADIENT	In volumetric medical imaging the boundaries of structures are frequently blurred due to insufficient resolution. This artefact is particularly serious in structures such as articular joints, where different cartilage surfaces appear to be linked at the contact regions. Traditional image segmentation techniques fail to separate such erroneously linked structures, and a sensible approach has been the introduction of prior-knowledge to the segmentation process. Although several 3D prior-knowledge based techniques that could successfully segment these structures have been published, most of them are pixel-labelling schemes that generate pixellated images with serious geometric distortions. The Simplex Mesh Diffusion Snakes segmentation technique presented here is an extension of the two dimensional Diffusion Snakes, but without any restriction on the number of dimensions of the data set. This technique integrates a Simplex Mesh, a region-based deformable model and Statistical Shape Knowledge into a single energy functional, so that it takes into account both the image information available directly from the data set, and the shape statistics obtained from a training process. The resulting segmentations converge correctly to well defined boundaries and provide a feasible location for those removed boundaries. The algorithm has been evaluated using 2D and 3D data sets obtained with Magnetic Resonance Imaging (MRI) and has proved to be robust to most of the MRI artefacts, providing continuous and smooth curves or surfaces with sub-pixel resolution. Additionally, this novel technique opens a wide range of opportunities for segmentation and tracking time-dependent 3D structures or data sets with more than three dimensions, due to its non-restrictive mathematical formulation.	[Tejos, Cristian; Irarrazaval, Pablo] Pontificia Univ Catolica Chile, Dept Elect Engn, Biomed Imaging Ctr, Santiago, Chile; [Cardenas-Blanco, Arturo] Ottawa Hosp, Ottawa Hlth Res Inst, Dept Diagnost Imaging, Ottawa, ON K1H 8L6, Canada	Pontificia Universidad Catolica de Chile; University of Ottawa; Ottawa Hospital Research Institute	Tejos, C (corresponding author), Pontificia Univ Catolica Chile, Dept Elect Engn, Biomed Imaging Ctr, Av Vicuna Mackenna 4860, Santiago, Chile.	ctejos@puc.cl	Irarrazaval, Pablo/F-8232-2013; Tejos, Cristian/F-8270-2013	Cardenas-Blanco, Arturo/0000-0001-8683-8741; Tejos, Cristian/0000-0002-8367-155X				BAJESY R, 1989, COMPUT VIS GRAPH IMA, V46, P1; Blake A., 2000, ACTIVE CONTOURS; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chen YM, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P145, DOI 10.1109/VLSM.2001.938893; COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675; Cootes T., 1992, P BRIT MACH VIS C, P9, DOI DOI 10.1007/978-1-4471-3201-1_2; Cremers D, 2003, PATTERN RECOGN, V36, P1929, DOI 10.1016/S0031-3203(03)00056-6; Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915; CREMERS D, 2002, THESIS U MANNHEIM MA; CREMERS D, 2003, P 2 IEEE WORKSH VAR, P169; Dam E, 2004, LECT NOTES COMPUT SC, V3217, P1008; Davies R., 2002, THESIS U MANCHESTER; DELINGETTE H, 2004, GEN OBJECT RECONSTRU; DELINGETTE H, 2000, LNCS, V1843, P381; DRYDEN IL, 2002, STAT SHAPE ANAL; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; KLEIN P, 1999, CALCULUS VECTORS MAT; Lachaud J O, 1999, Med Image Anal, V3, P187, DOI 10.1016/S1361-8415(99)80012-7; LEITNER F, 1991, P SPIE C ADV INT ROB, V1609; LEROY B, 1996, P 12 INT C AN OPT SY; LEVENTON ME, 2000, PROC CVPR IEEE, P316, DOI DOI 10.1109/CVPR.2000.855835; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7; McInerney T, 1999, IEEE T MED IMAGING, V18, P840, DOI 10.1109/42.811261; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; PAPADOPOULO T, 2004, ESTIMATING JACOBIAN; Paragios N, 2002, INT J COMPUT VISION, V46, P223, DOI 10.1023/A:1014080923068; PARAGIOS N, 1999, P IEEE INT C COMP VI, V2, P926; PARAGIOS N, 2002, P ECCV, V2, P78; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P715, DOI 10.1109/34.85660; Pitiot A, 2003, LECT NOTES COMPUT SC, V2879, P644; Pizer SM, 2003, INT J COMPUT VISION, V55, P85, DOI 10.1023/A:1026313132218; Romdhani S, 1999, P 10 BRIT MACH VIS C, P483, DOI [10.5244/C.13.48, DOI 10.5244/C.13.48]; RONFARD R, 1994, INT J COMPUT VISION, V13, P229, DOI 10.1007/BF01427153; Rousson M, 2002, LECT NOTES COMPUT SC, V2351, P78; SAMSON C, 1999, P INT C SCAL SPAC TH, P306; SINGH A, 1998, DEFORMABLE MODELS ME; Staib L. H., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P98, DOI 10.1109/CVPR.1989.37834; Tejos C, 2004, P ANN INT IEEE EMBS, V26, P1648; TEJOS C, 2005, P 13 INT C INT SOC M; Tsai A, 2001, PROC CVPR IEEE, P463; Tsai A, 2003, IEEE T MED IMAGING, V22, P137, DOI 10.1109/TMI.2002.808355; TWINING CJ, 2001, P BRIT MACH VIS C, P23; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; Yezzi A.  Jr., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P898, DOI 10.1109/ICCV.1999.790317; YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343	56	12	12	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2009	85	1					19	34		10.1007/s11263-009-0241-1	http://dx.doi.org/10.1007/s11263-009-0241-1			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	470UY					2022-12-18	WOS:000268008000002
J	Vanhamel, I; Mihai, C; Sahli, H; Katartzis, A; Pratikakis, I				Vanhamel, I.; Mihai, C.; Sahli, H.; Katartzis, A.; Pratikakis, I.			Scale Selection for Compact Scale-Space Representation of Vector-Valued Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scale selection; Localization scale; Scale-space discretization	ANISOTROPIC DIFFUSION; EDGE-DETECTION; SEGMENTATION; RESTORATION	This paper investigates the scale selection problem for nonlinear diffusion scale-spaces. This topic comprises the notions of localization scale selection and scale space discretization. For the former, we present a new approach. It aims at maximizing the image content's presence by finding the scale that has a maximum correlation with the noise-free image. For the latter, we propose to adapt the optimal diffusion stopping time criterion of Mrazek and Navara in such a way that it may identify multiple scales of importance.	[Vanhamel, I.; Mihai, C.; Sahli, H.] Vrije Univ Brussel, ETRO IRIS, B-1050 Brussels, Belgium; [Katartzis, A.] Univ London Imperial Coll Sci Technol & Med, CSP Grp, EEE Dep, London SW7 2AZ, England; [Pratikakis, I.] NCSR Demokritos, IIT, Computat Intelligence Lab, Athens 15310, Greece	Vrije Universiteit Brussel; Imperial College London; National Centre of Scientific Research "Demokritos"	Mihai, C (corresponding author), Vrije Univ Brussel, ETRO IRIS, Pl Laan 2, B-1050 Brussels, Belgium.	iuvanham@etro.vub.ac.be; cmihai@etro.vub.ac.be; hsahli@etro.vub.ac.be; a.katartzis@imperial.ac.uk; ipratika@iit.demokritos.gr	PRATIKAKIS, IOANNIS/AAD-3387-2019	PRATIKAKIS, IOANNIS/0000-0002-4124-3688; Sahli, Hichem/0000-0002-1774-2970				Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192; Blomgren P, 1998, IEEE T IMAGE PROCESS, V7, P304, DOI 10.1109/83.661180; BRESSON X, 2005, 5 INT C SCAL SPAC PD, P167; CATTE F, 1992, SIAM J NUMER ANAL, V29, P182, DOI 10.1137/0729012; CHARBONNIER P, 1994, THESIS U NICE SOPHIA; GEMAN D, 1992, IEEE T PATTERN ANAL, V14, P367, DOI 10.1109/34.120331; GILBOA G, 2004, 499 CCIT; HADJIDEMETRIOU E, 2002, EUR C COMP VIS, P220; HAMPEL FR, 1974, J AM STAT ASSOC, V69, P383, DOI 10.2307/2285666; Katartzis A, 2005, IEEE T GEOSCI REMOTE, V43, P548, DOI 10.1109/TGRS.2004.842405; Keeling SL, 2002, INVERSE PROBL, V18, P175, DOI 10.1088/0266-5611/18/1/312; LIN Z, 1999, INT C IM AN PROC, P102; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Mrazek P, 2003, INT J COMPUT VISION, V52, P189, DOI 10.1023/A:1022908225256; Mrazek P, 2001, LECT NOTES COMPUT SC, V2106, P290; Mukhopadhyay S, 2003, IEEE T IMAGE PROCESS, V12, P533, DOI 10.1109/TIP.2003.810757; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Petrovic A, 2004, IEEE T IMAGE PROCESS, V13, P1104, DOI 10.1109/TIP.2004.828431; Pratikakis I, 2006, IEE P-VIS IMAGE SIGN, V153, P313, DOI 10.1049/ip-vis:20050061; PRATIKAKIS I, 1998, THESIS VRIJE U BRUSS; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Sporring J, 1999, IEEE T INFORM THEORY, V45, P1051, DOI 10.1109/18.761342; Sporring J, 2000, IEEE IMAGE PROC, P920, DOI 10.1109/ICIP.2000.901110; Vanhamel I, 2003, IEEE T IMAGE PROCESS, V12, P617, DOI 10.1109/TIP.2003.811490; VANHAMEL I, 2006, THESIS VRIJE U BRUSS; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Weickert J, 1999, IMAGE VISION COMPUT, V17, P201, DOI 10.1016/S0262-8856(98)00102-4; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; WHITAKER R, 1994, GEOMETRY DRIVEN DIFF, P93; WHITAKER R, 1993, THESIS U N CAROLINA; You YL, 1996, IEEE T IMAGE PROCESS, V5, P1539, DOI 10.1109/83.541424	32	12	12	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2009	84	2					194	204		10.1007/s11263-008-0154-4	http://dx.doi.org/10.1007/s11263-008-0154-4			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	451NO					2022-12-18	WOS:000266477100006
J	Zaharescu, A; Horaud, R				Zaharescu, Andrei; Horaud, Radu			Robust Factorization Methods Using a Gaussian/Uniform Mixture Model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Robust factorization; 3-D reconstruction; Multiple camera calibration; Data clustering; Expectation-maximization; EM; M-estimators; Outlier rejection	MOTION; SHAPE; UNCERTAINTY; ALGORITHM	In this paper we address the problem of building a class of robust factorization algorithms that solve for the shape and motion parameters with both affine (weak perspective) and perspective camera models. We introduce a Gaussian/uniform mixture model and its associated EM algorithm. This allows us to address parameter estimation within a data clustering approach. We propose a robust technique that works with any affine factorization method and makes it resilient to outliers. In addition, we show how such a framework can be further embedded into an iterative perspective factorization scheme. We carry out a large number of experiments to validate our algorithms and to compare them with existing ones. We also compare our approach with factorization methods that use M-estimators.	[Zaharescu, Andrei; Horaud, Radu] INRIA Grenoble Rhone Alpes, F-38330 Montbonnot St Martin, France		Horaud, R (corresponding author), INRIA Grenoble Rhone Alpes, 655 Ave Europe, F-38330 Montbonnot St Martin, France.	andrei.zaharescu@inrialpes.fr; Radu.Horaud@inrialpes.fr	Horaud, Radu/AAR-5982-2021	Horaud, Radu/0000-0001-5232-024X	European Community's Sixth Framework Programme [VISION-TRAIN RTN-CT-2004-005439]	European Community's Sixth Framework Programme	This research was supported by the VISION-TRAIN RTN-CT-2004-005439 Marie Curie Action within the European Community's Sixth Framework Programme.	Aanaes H, 2002, IEEE T PATTERN ANAL, V24, P1215, DOI 10.1109/TPAMI.2002.1033213; Anandan P, 2002, INT J COMPUT VISION, V49, P101, DOI 10.1023/A:1020137420717; [Anonymous], 2004, EMERGING TOPICS COMP; Bishop C.M, 2006, PATTERN RECOGN; Bouguet J.-Y., 2001, INTEL CORP; Brandt S., 2002, Proceedings of the Statistical Methods in Video Processing Workshop, P109; Christy S, 1996, IEEE T PATTERN ANAL, V18, P1098, DOI 10.1109/34.544079; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131; Golub G. H., 1996, MATRIX COMPUTATIONS; Gruber A, 2004, PROC CVPR IEEE, P707; GRUBER A, 2003, P NEUR INF PROC SYST; HAJDER L, 2004, P 2 INT S 3D DAT PRO; HARTLEY R, 2003, POWERFACTORIZATION 3; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Huynh DQ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P585; HUYNH DQ, 2002, P 7 INT C CONTR AUT; Kanatani K, 2004, IEEE T PATTERN ANAL, V26, P1307, DOI 10.1109/TPAMI.2004.93; Kanatani K, 1998, INT J COMPUT VISION, V26, P171, DOI 10.1023/A:1007948927139; LUONG QT, 2001, GEOMETRY MULTIPLE IM; LUONG QT, 1997, INT J COMPUT VISION, V1, P5; Mahamud S, 2000, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2000.854872; MAHAMUD S, 2001, IEEE C COMP VIS PATT; McLachlan, 1997, EM ALGORITHM EXTENSI; Miller DJ, 2003, IEEE T PATTERN ANAL, V25, P1468, DOI 10.1109/TPAMI.2003.1240120; Miyagawa I, 2006, IEEE T PATTERN ANAL, V28, P1176, DOI 10.1109/TPAMI.2006.147; Morris DD, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P696, DOI 10.1109/ICCV.1998.710793; Oliensis J, 2007, IEEE T PATTERN ANAL, V29, P2217, DOI 10.1109/TPAMI.2007.1132; Rousseeuw P., 1999, COMPUTING SCI STAT, V31, P451; ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718; Roweis S, 1998, ADV NEUR IN, V10, P626; SHUM HY, 1995, IEEE T PATTERN ANAL, V17, P854, DOI 10.1109/34.406651; Stewart CV, 1999, SIAM REV, V41, P513, DOI 10.1137/S0036144598345802; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; TARDIF JP, 2007, P IEEE C COMP VIS PA; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TRAJKOVIC M, 2007, P BRIT MACH VIS C SE; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; Vidal R, 2004, PROC CVPR IEEE, P310; WALKER MW, 1991, CVGIP-IMAG UNDERSTAN, V54, P358, DOI 10.1016/1049-9660(91)90036-O; Wiberg T, 1976, P 2 S COMP STAT, P229; ZAHARESCU A, 2007, LNCS; ZAHARESCU A, 2006, P 3 INT S 3D DAT PRO; Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	48	12	12	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2009	81	3					240	258		10.1007/s11263-008-0169-x	http://dx.doi.org/10.1007/s11263-008-0169-x			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	394GK		Green Submitted			2022-12-18	WOS:000262433800002
J	de la Fraga, LG; Schutze, O				de la Fraga, Luis Gerardo; Schuetze, Oliver			Direct Calibration by Fitting of Cuboids to a Single Image Using Differential Evolution	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Camera calibration; 3D modeling; Differential evolution; Non-linear optimization	CAMERA	In this article we propose a new method to calibrate directly the camera by which it was taken an image of a cuboid, and to find at the same time the orientation and side lengths of the cuboid. This is a highly non-linear optimization problem that is solved directly using a heuristic called differential evolution. We show in this paper that this problem is very difficult if one tries to solve it with a conventional scalar optimization procedure. Although differential evolution is a heuristic, we find valid results in 100% of the executions. We test our method with synthetic and real images.	[de la Fraga, Luis Gerardo; Schuetze, Oliver] CINVESTAV, Dept Comp Sci, Mexico City 07360, DF, Mexico	CINVESTAV - Centro de Investigacion y de Estudios Avanzados del Instituto Politecnico Nacional	de la Fraga, LG (corresponding author), CINVESTAV, Dept Comp Sci, Av IPN 2508, Mexico City 07360, DF, Mexico.	fraga@cs.cinvestav.mx; schuetze@cs.cinvestav.mx	de la Fraga, Luis Gerardo/B-1631-2017	de la Fraga, Luis Gerardo/0000-0002-9373-9837	CONACyT, Mexico [80965]	CONACyT, Mexico(Consejo Nacional de Ciencia y Tecnologia (CONACyT))	This work was partially supported by CONACyT, Mexico under grant 80965.	CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813; CIPOLLA R, 1999, BMVC99, P10; Coello C.A.C., 2007, EVOLUTIONARY ALGORIT, V5; de la Fraga LG, 2008, IEEE C EVOL COMPUTAT, P3266, DOI 10.1109/CEC.2008.4631240; HANSEN N, 2006, COMPARISONS RESULTS; HARTLEY RI, 1994, P 2 JOINT EUR US WOR, P237; Jelinek D, 2001, IEEE T PATTERN ANAL, V23, P767, DOI 10.1109/34.935850; Mezura-Montes E, 2006, GECCO 2006: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOL 1 AND 2, P485; Pollefeys M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P90, DOI 10.1109/ICCV.1998.710705; Price K. V., 1999, INTRO DIFFERENTIAL E, P79; Wilczkowiak M, 2005, IEEE T PATTERN ANAL, V27, P194, DOI 10.1109/TPAMI.2005.40; Wilczkowiak M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P142, DOI 10.1109/ICCV.2001.937510; Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	14	12	13	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2009	81	2					119	127		10.1007/s11263-008-0183-z	http://dx.doi.org/10.1007/s11263-008-0183-z			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	393VI					2022-12-18	WOS:000262401600001
J	Preusser, T; Scharr, H; Krajsek, K; Kirby, RM				Preusser, Tobias; Scharr, Hanno; Krajsek, Kai; Kirby, Robert M.			Building blocks for computer vision with stochastic partial differential equations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image processing; error propagation; random fields; polynomial chaos; stochastic partial differential equations; stochastic galerkin method; stochastic finite element method	OPTIC FLOW COMPUTATION; MODELING UNCERTAINTY; POLYNOMIAL CHAOS; EDGE-DETECTION; DIFFUSION; WIENER	We discuss the basic concepts of computer vision with stochastic partial differential equations (SPDEs). In typical approaches based on partial differential equations (PDEs), the end result in the best case is usually one value per pixel, the "expected" value. Error estimates or even full probability density functions PDFs are usually not available. This paper provides a framework allowing one to derive such PDFs, rendering computer vision approaches into measurements fulfilling scientific standards due to full error propagation. We identify the image data with random fields in order to model images and image sequences which carry uncertainty in their gray values, e.g. due to noise in the acquisition process. The noisy behaviors of gray values is modeled as stochastic processes which are approximated with the method of generalized polynomial chaos (Wiener-Askey-Chaos). The Wiener-Askey polynomial chaos is combined with a standard spatial approximation based upon piecewise multi-linear finite elements. We present the basic building blocks needed for computer vision and image processing in this stochastic setting, i.e. we discuss the computation of stochastic moments, projections, gradient magnitudes, edge indicators, structure tensors, etc. Finally we show applications of our framework to derive stochastic analogs of well known PDEs for de-noising and optical flow extraction. These models are discretized with the stochastic Galerkin method. Our selection of SPDE models allows us to draw connections to the classical deterministic models as well as to stochastic image processing not based on PDEs. Several examples guide the reader through the presentation and show the usefulness of the framework.	[Preusser, Tobias] Univ Bremen, Ctr Complex Syst & Visualizat, Bremen, Germany; [Scharr, Hanno; Krajsek, Kai] Forschungszentrum Julich, Inst Phytosphere 3, Inst Chem & Dynam Geosphere, Julich, Germany; [Kirby, Robert M.] Univ Utah, Sch Comp, Salt Lake City, UT USA; [Kirby, Robert M.] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT USA	University of Bremen; Helmholtz Association; Research Center Julich; Utah System of Higher Education; University of Utah; Utah System of Higher Education; University of Utah	Preusser, T (corresponding author), Univ Bremen, Ctr Complex Syst & Visualizat, Bremen, Germany.	tp@mevis.de; h.scharr@fz-juelich.de; k.krajsek@fz-juelich.de; kirby@cs.utah.edu	Scharr, Hanno/D-9051-2015; Krajsek, Kai/S-1057-2016	Scharr, Hanno/0000-0002-8555-6416; Krajsek, Kai/0000-0003-3417-161X				Amiaz T, 2006, INT J COMPUT VISION, V68, P111, DOI 10.1007/s11263-005-6206-0; Avriel M., 2003, NONLINEAR PROGRAMMIN; Bao YF, 2004, IEEE T PATTERN ANAL, V26, P63, DOI 10.1109/TPAMI.2004.1261079; Black M. J., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P296, DOI 10.1109/CVPR.1991.139705; Black MJ, 1993, P 4 INT C COMP VIS, P231, DOI DOI 10.1109/ICCV.1993.378214; Bruhn A., 2002, Pattern Recognition. 24th DAGM Symposium. Proceedings (Lecture Notes in Computr Science Vol.2449), P454; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; CATTE F, 1992, SIAM J NUMER ANAL, V29, P182, DOI 10.1137/0729012; CHORIN AJ, 1971, J COMPUT PHYS, V8, P472, DOI 10.1016/0021-9991(71)90025-8; CHORIN AJ, 1974, J FLUID MECH, V63, P21, DOI 10.1017/S0022112074000991; COHEN I, 1993, SCIA93, P523; Deb MK, 2001, COMPUT METHOD APPL M, V190, P6359, DOI 10.1016/S0045-7825(01)00237-7; DELAPLACE, 1812, THEORIE ANAL PROBABI; Fermuller C, 2001, COMPUT VIS IMAGE UND, V82, P1, DOI 10.1006/cviu.2000.0900; Forsyth David A, 2012, COMPUTER VISION MODE; GAUSS CF, 1987, CLASSICS APPL MATH, V11; Ghanem R, 1999, J HEAT TRANS-T ASME, V121, P290, DOI 10.1115/1.2825979; Ghanem RG, 2003, STOCHASTIC FINITE EL; HAUSSECKER H, 1998, P INT S REALT IM DYN; HAUSSECKER H, 1999, HDB COMPUTER VISION, P309; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Iijima T., 1962, BULL ELECT LAB, V26, P368; IIJIMA T, 1963, ELECTR COMMUN, P123; Jahne B, 1993, LECT NOTES COMPUTER; KEESE A, 2004, THESIS TU BRAUNSCHWE; Kichenassamy S, 1997, SIAM J APPL MATH, V57, P1328, DOI 10.1137/S003613999529558X; Le Maitre OP, 2002, J COMPUT PHYS, V181, P9, DOI 10.1006/jcph.2002.7104; Lucas Bruce D, 1981, P 7 INT JOINT C ART, DOI DOI 10.1042/CS0730285; Lucor D, 2004, INT J NUMER METH ENG, V60, P571, DOI 10.1002/nme.976; Malliavin P., 1997, STOCHASTIC ANAL; MALTZ FH, 1979, J COMPUT PHYS, V32, P345, DOI 10.1016/0021-9991(79)90150-5; MEECHAM WC, 1968, J FLUID MECH, V32, P225, DOI 10.1017/S0022112068000698; Mikula K., 2004, Computing and Visualization in Science, V6, P197, DOI 10.1007/s00791-004-0129-0; Narayanan VAB, 2004, INT J NUMER METH ENG, V60, P1569, DOI 10.1002/nme.1015; NESTARES O, 2000, CVPR 00, V1; NESTARES O, 2003, IEEE INT C IM PROC I, V3, P77; Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; PREUSSER T, 1999, P SCAL SPAC 99, P223; Reagan MT, 2004, COMBUST THEOR MODEL, V8, P607, DOI 10.1088/1364-7830/8/3/010; Reagan MT, 2005, INT J CHEM KINET, V37, P368, DOI 10.1002/kin.20081; Scharr H, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P840; Scharr H, 2006, LECT NOTES COMPUT SC, V4174, P51; SUHLING M, 2006, THESIS SWISS FEDERAL; Thomee V., 1984, GALERKIN FINITE ELEM; Van Huffel S., 1991, FRONTIERS APPL MATH, V9; WEBER J, 1994, INT J COMPUT VISION, V14, P5; Weickert J, 2001, J MATH IMAGING VIS, V14, P245, DOI 10.1023/A:1011286029287; WEICKERT J, 1998, P COMP VIS MOB ROB W, P115; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; Xiu DB, 2003, INT J HEAT MASS TRAN, V46, P4681, DOI 10.1016/S0017-9310(03)00299-0; Xiu DB, 2003, J COMPUT PHYS, V187, P137, DOI 10.1016/S0021-9991(03)00092-5; Xiu DB, 2002, SIAM J SCI COMPUT, V24, P619, DOI 10.1137/S1064827501387826; Xiu DB, 2002, COMPUT METHOD APPL M, V191, P4927, DOI 10.1016/S0045-7825(02)00421-8	56	12	12	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2008	80	3					375	405		10.1007/s11263-008-0145-5	http://dx.doi.org/10.1007/s11263-008-0145-5			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	350RJ		Green Submitted			2022-12-18	WOS:000259370500006
J	Lucey, S; Chen, T				Lucey, Simon; Chen, Tsuhan			A viewpoint invariant, sparsely registered, patch based, face verifier	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	24th Conference on Computer Vision and Pattern Recognition	JUN 17-22, 2006	New York, NY			face verification; patch-whole modeling; viewpoint invariance	RECOGNITION	Sparsely registering a face (i.e., locating 2-3 fiducial points) is considered a much easier task than densely registering one; especially with varying viewpoints. Unfortunately, the converse tends to be true for the task of viewpoint-invariant face verification; the more registration points one has the better the performance. In this paper we present a novel approach to viewpoint invariant face verification which we refer to as the "patch-whole" algorithm. The algorithm is able to obtain good verification performance with sparsely registered faces. Good performance is achieved by not assuming any alignment between gallery and probe view faces, but instead trying to learn the joint likelihood functions for faces of similar and dissimilar identities. Generalization is encouraged by factorizing the joint gallery and probe appearance likelihood, for each class, into an ensemble of "patch-whole" likelihoods. We make an additional contribution in this paper by reviewing existing approaches to viewpoint-invariant face verification and demonstrating how most of them fall into one of two categories; namely viewpoint-generative or viewpoint-discriminative. This categorization is instructive as it enables us to compare our "patch-whole" algorithm to other paradigms in viewpoint-invariant face verification and also gives deeper insights into why the algorithm performs so well.	[Lucey, Simon; Chen, Tsuhan] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Lucey, S (corresponding author), Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	slucey@ieee.org; tsuhan@cmu.edu	Lucey, Simon/HDO-1716-2022; Lucey, Simon/B-7556-2011	Chen, Tsuhan/0000-0003-3951-7931				Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BEYMER D, 1995, INT C COMP VIS; Bishop C.M, 2006, PATTERN RECOGN; Blanz V, 2005, PROC CVPR IEEE, P454; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Blanz V, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P617; Everingham M, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P441; Gross R, 2004, IEEE T PATTERN ANAL, V26, P449, DOI 10.1109/TPAMI.2004.1265861; Jebara T., 2004, KLUWER INT SER ENG C, DOI 10.1007/978-1-4419-9011-2; Kanade T, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, VOLS I-III, PROCEEDINGS, P954; Kim TK, 2005, IEEE T PATTERN ANAL, V27, P318, DOI 10.1109/TPAMI.2005.58; Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881; Liu XM, 2005, PROC CVPR IEEE, P502; LUCEY S, 2006, HCSNET WORKSH US VIS; LUCEY S, 2006, IEEE C COMP VIS PATT, V1, P909; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790; Sanderson C, 2006, PATTERN RECOGN, V39, P288, DOI 10.1016/j.patcog.2005.07.001; Wen Yi Zhao, 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P285, DOI 10.1109/AFGR.2000.840648	19	12	12	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2008	80	1					58	71		10.1007/s11263-007-0119-z	http://dx.doi.org/10.1007/s11263-007-0119-z			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	344CB		Green Submitted			2022-12-18	WOS:000258901900005
J	Wang, Y; Zhu, SC				Wang, Yizhou; Zhu, Song-Chun			Perceptual scale-space and its applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						scale-space; image pyramid; primal sketch; graph grammar; generative modeling	PRIMAL SKETCH; STATISTICS; SHAPE	When an image is viewed at varying resolutions, it is known to create discrete perceptual jumps or transitions amid the continuous intensity changes. In this paper, we study a perceptual scale-space theory which differs from the traditional image scale-space theory in two aspects. (i) In representation, the perceptual scale-space adopts a full generative model. From a Gaussian pyramid it computes a sketch pyramid where each layer is a primal sketch representation (Guo et al. in Comput. Vis. Image Underst. 106(1):5-19, 2007)-an attribute graph whose elements are image primitives for the image structures. Each primal sketch graph generates the image in the Gaussian pyramid, and the changes between the primal sketch graphs in adjacent layers are represented by a set of basic and composite graph operators to account for the perceptual transitions. (ii) In computation, the sketch pyramid and graph operators are inferred, as hidden variables, from the images through Bayesian inference by stochastic algorithm, in contrast to the deterministic transforms or feature extraction, such as computing zero-crossings, extremal points, and inflection points in the image scale-space. Studying the perceptual transitions under the Bayesian framework makes it convenient to use the statistical modeling and learning tools for (a) modeling the Gestalt properties of the sketch graph, such as continuity and parallelism etc; (b) learning the most frequent graph operators, i.e. perceptual transitions, in image scaling; and (c) learning the prior probabilities of the graph operators conditioning on their local neighboring sketch graph structures. In experiments, we learn the parameters and decision thresholds through human experiments, and we show that the sketch pyramid is a more parsimonious representation than a multi-resolution Gaussian/Wavelet pyramid. We also demonstrate an application on adaptive image display-showing a large image in a small screen (say PDA) through a selective tour of its image pyramid. In this application, the sketch pyramid provides a means for calculating information gain in zooming-in different areas of an image by counting a number of operators expanding the primal sketches, such that the maximum information is displayed in a given number of frames.	[Wang, Yizhou; Zhu, Song-Chun] Univ Calif Los Angeles, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Zhu, SC (corresponding author), Univ Calif Los Angeles, 8125 Math Sci Bldg,Box 951554, Los Angeles, CA 90095 USA.	sczhu@stat.ucla.edu						Agrawal R., 1994, P 20 INT C VER LARG; AHUJA N, 1993, P COMP VIS PATT REC, V15, P780; Brady M., 2001, INT J COMPUTER VISIO; Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844; Cootes Timothy F, 1998, P EUR C COMP VIS; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; GAUCH JM, 1993, IEEE T PATTERN ANAL, V15, P635, DOI 10.1109/34.216734; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; GUO C, 2003, P INT C COMP VIS; Guo CE, 2007, COMPUT VIS IMAGE UND, V106, P5, DOI 10.1016/j.cviu.2005.09.004; JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0; Klein PN, 2001, SIAM PROC S, P781; KOENDERINK JJ, 1984, BIOL CYBERNETICS; LIFSHITZ LM, 1990, IEEE T PATTERN ANAL, V12, P529, DOI 10.1109/34.56189; LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346; Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; LINDEBERG T, 1992, IMAGE VISION COMPUT, V10, P3, DOI 10.1016/0262-8856(92)90079-I; Lindeberg T., 1994, SCALE SPACE THEORY C; LOWE DG, 2004, INT J COMPUTER VISIO; MA YF, 2002, ACM MULTIMEDIA, V12; Mallat S., 1999, WAVELET TOUR SIGNAL; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Marr D., 1983, VISION; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Mumford D, 2001, Q APPL MATH, V59, P85, DOI 10.1090/qam/1811096; Olsen O. F., 1997, Image Analysis and Processing. 9th International Conference, ICIAP '97 Proceedings, P6; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Romeny B. M. T. H., 1997, FRONT END VISION MUL; RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006; Shokoufandeh A, 2002, LECT NOTES COMPUT SC, V2352, P759; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725; SPORRING J, 1996, GAUSSIAN SCALE SPACE; Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444; WANG Y, 2005, P INT C COMP VIS; WANG Y, 2004, P IEEE C COMP VIS PA; Witkin A. P., 1983, INT JOINT C AI PAL A; WU Y, 2007, Q APPL MATH; XIE X, 2003, IEEE T MULTIMEDIA; XU ZJ, 2005, P IEEE C COMP VIS PA; YAO Z, 2007, 6 INT C EMMCVPR; Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1170, DOI 10.1109/34.809110; Zhu SC, 1996, INT J COMPUT VISION, V20, P187; Zhu SC, 1997, NEURAL COMPUT, V9, P1627, DOI 10.1162/neco.1997.9.8.1627	45	12	14	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2008	80	1					143	165		10.1007/s11263-008-0138-4	http://dx.doi.org/10.1007/s11263-008-0138-4			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	344CB		Green Submitted			2022-12-18	WOS:000258901900010
J	Zheng, JY; Shi, M				Zheng, Jiang Yu; Shi, Min			Scanning depth of route panorama based on stationary blur	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						slit scanner; line sensor; route panorama; stationary blur; motion blur; sampling; depth estimation; vision system; geographic information system; large scale 3D space	MOTION	This work achieves an efficient acquisition of scenes and their depths along long streets. A camera is mounted on a vehicle moving along a straight or a mildly curved path and a sampling line properly set in the camera frame scans the 1D images over scenes continuously to form a 2D route panorama. This paper proposes a method to estimate the depth from the camera path by analyzing a phenomenon called stationary blur in the route panorama. This temporal blur is a perspective effect in parallel projection yielded from the sampling slit with a physical width. We analyze the behavior of the stationary blur with respect to the scene depth, vehicle path, and camera properties. Based on that, we develop an adaptive filter to evaluate the degree of the blur for depth estimation, which avoids error-prone feature matching or tracking in capturing complex street scenes and facilitates real time sensing. The method also uses much less data than the structure from motion approach so that it can extend the sensing area significantly. The resulting route panorama with depth information is useful for urban visualization, monitoring, navigation, and modeling.	[Zheng, Jiang Yu; Shi, Min] Indiana Univ Purdue Univ, Dept Comp Sci, Indianapolis, IN 46202 USA	Indiana University System; Indiana University-Purdue University Indianapolis	Zheng, JY (corresponding author), Indiana Univ Purdue Univ, Dept Comp Sci, Indianapolis, IN 46202 USA.	jzheng@cs.iupui.edu						Agarwala A, 2006, ACM T GRAPHIC, V25, P853, DOI 10.1145/1141911.1141966; ALIAGA DG, 2001, SIGGRAPH01; Baker H. H., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P2, DOI 10.1109/CVPR.1988.196209; Ben-Ezra M, 2003, PROC CVPR IEEE, P657; Chen SN, 1995, SWIMMING THROUGH TROUBLED WATER, P29; Chowdhury AKR, 2003, INT J COMPUT VISION, V55, P27, DOI 10.1023/A:1024488407740; FLORA G, 2007, ACM MULTIMEDIA 07; FOX J, 1988, IEEE COMPUTER VISION, P360; Frueh C, 2003, PROC CVPR IEEE, P562; Gupta R, 1997, IEEE T PATTERN ANAL, V19, P963, DOI 10.1109/34.615446; Ikeuchi K, 2004, INT J COMPUT VISION, V58, P237, DOI 10.1023/B:VISI.0000019686.74089.5d; ISHIGURO H, 1992, IEEE T PATTERN ANAL, V14, P257, DOI 10.1109/34.121792; Jiang Yu Zheng, 2005, 13th Annual ACM International Conference on Multimedia, P986, DOI 10.1145/1101149.1101354; Kawanishi T, 1998, INT C PATT RECOG, P485, DOI 10.1109/ICPR.1998.711187; KIM J, 2003, INT C COMP VIS, V3, P1033; Li SG, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P570, DOI 10.1109/IROS.1998.724679; Li Y, 2004, IEEE T PATTERN ANAL, V26, P45, DOI 10.1109/TPAMI.2004.1261078; Potmesil M., 1983, Computer Graphics, V17, P389, DOI 10.1145/964967.801169; Roman A, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P537, DOI 10.1109/VISUAL.2004.50; Seitz SM, 2003, IEEE COMPUT GRAPH, V23, P16, DOI 10.1109/MCG.2003.1242377; Seitz SM, 2002, INT J COMPUT VISION, V48, P21, DOI 10.1023/A:1014851111084; SHI M, 2005, IEEE C COMP VIS PATT; Srinivasan S, 2000, INT J COMPUT VISION, V37, P203, DOI 10.1023/A:1008111923880; TANG CK, 2001, INT C COMP VIS; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Weng J, 1997, IEEE T PATTERN ANAL, V19, P451, DOI 10.1109/34.589205; Xiao JJ, 2005, IEEE T PATTERN ANAL, V27, P1644, DOI 10.1109/TPAMI.2005.202; ZHAO H, 2003, IEEE T SYST MAN CYB, V33; Zheng DL, 2003, J UNIV SCI TECHNOL B, V10, P68; Zheng JY, 2006, INT C PATT RECOG, P709; Zheng JY, 2006, IEEE T VIS COMPUT GR, V12, P155, DOI 10.1109/TVCG.2006.37; ZHENG JY, 1992, INT J COMPUT VISION, V9, P55; Zheng JY, 2005, COMPUT ANIMAT VIRT W, V16, P97, DOI 10.1002/cav.66; Zheng JY, 2004, INT C PATT RECOG, P348; ZHENG JY, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P558; Zheng JY, 1998, COMPUT VIS IMAGE UND, V72, P237, DOI 10.1006/cviu.1998.0678; ZHENG JY, 2006, 7 ACCV, V1, P509; ZHU Z, 2004, IJCV, V61, P233; ZHU Z, 2001, ICCV 2001, V2, P723; Zhu ZG, 2004, IEEE T PATTERN ANAL, V26, P226, DOI 10.1109/TPAMI.2004.1262190; ZOMET D, 2003, IEEE T PAMI, P741	41	12	14	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2008	78	2-3					169	186		10.1007/s11263-007-0080-x	http://dx.doi.org/10.1007/s11263-007-0080-x			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	275RK					2022-12-18	WOS:000254089100004
J	Greminger, MA; Nelson, BJ				Greminger, Michael A.; Nelson, Bradley J.			A deformable object tracking algorithm based on the boundary element method that is robust to occlusions and spurious edges	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						deformable object tracking; boundary element method; robust tracking; edge detection; robust statistics; artificial neural networks		The manipulation of deformable objects is an important problem in robotics and arises in many applications including biomanipulation, microassembly, and robotic surgery. For some applications, the robotic manipulator itself may be deformable. Vision-based deformable object tracking can provide feedback for these applications. Computer vision is a logical sensing choice for tracking deformable objects because the large amount of data that is collected by a vision system allows many points within the deformable object to be tracked simultaneously. This article introduces a template based deformable object tracking algorithm, based on the boundary element method, that is able to track a wide range of deformable objects. The robustness of this algorithm to occlusions and to spurious edges in the source image is also demonstrated. A robust error measure is used to handle the problem of occlusion and an improved edge detector based on the Canny edge operator is used to suppress spurious edges. This article concludes by quantifying the performance increase provided by the robust error measure and the robust edge detector. The performance of the algorithm is also demonstrated through the tracking of a sequence of cardiac MRI images.	[Greminger, Michael A.] Univ Minnesota, Dept Mech Engn, Minneapolis, MN 55455 USA; [Nelson, Bradley J.] Swiss Fed Inst Technol, Inst Robot & Intelligent Syst, CH-8092 Zurich, Switzerland	University of Minnesota System; University of Minnesota Twin Cities; Swiss Federal Institutes of Technology Domain; ETH Zurich	Greminger, MA (corresponding author), Univ Minnesota, Dept Mech Engn, 111 Church St SE, Minneapolis, MN 55455 USA.	grem@me.umn.edu	Nelson, Bradley/B-7761-2013	Nelson, Bradley/0000-0001-9070-6987				Beer G., 2001, PROGRAMMING BOUNDARY; CANNY JF, 1986, PAMI, V8, P6, DOI DOI 10.1109/TPAMI.1986.4767851; Draper N.R., 1998, APPL REGRESSION ANAL, V326; Fletcher R, 1987, PRACTICAL METHODS OP, V1; Galassi M., 2001, GNU SCI LIB REFERENC; Greminger MA, 2004, IEEE T PATTERN ANAL, V26, P290, DOI 10.1109/TPAMI.2004.1262305; Jawson M. A., 1977, INTEGRAL EQUATION ME; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; McInerney T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P518, DOI 10.1109/ICCV.1993.378169; Metaxas D., 1997, PHYS BASED DEFORMABL; Nakamura Y, 2001, IEEE INT CONF ROBOT, P2014, DOI 10.1109/ROBOT.2001.932903; Press WH., 1993, NUMERICAL RECIPES C; RIZZO FJ, 1967, Q APPL MATH, V25, P83, DOI 10.1090/qam/99907; SOKOLNIKOFF IS, 1983, MATH THEORY ELASTICI; Stewart CV, 1999, SIAM REV, V41, P513, DOI 10.1137/S0036144598345802; Sun Y, 2002, INT J ROBOT RES, V21, P861, DOI 10.1177/0278364902021010833; Tsap LV, 1998, COMPUT VIS IMAGE UND, V69, P330, DOI 10.1006/cviu.1998.0663; Wang XY, 2001, SENSOR ACTUAT A-PHYS, V94, P142, DOI 10.1016/S0924-4247(01)00705-1; YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169	19	12	12	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2008	78	1					29	45		10.1007/s11263-007-0076-6	http://dx.doi.org/10.1007/s11263-007-0076-6			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	270YE		Green Submitted			2022-12-18	WOS:000253755300003
J	Bar-Hillel, A; Weinshall, D				Bar-Hillel, Aharon; Weinshall, Daphna			Efficient learning of relational object class models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object class recognition; object localization; generative models; boosting; weakly supervised learning		We present an efficient method for learning part-based object class models from unsegmented images represented as sets of salient features. A model includes parts' appearance, as well as location and scale relations between parts. The object class is generatively modeled using a simple Bayesian network with a central hidden node containing location and scale information, and nodes describing object parts. The model's parameters, however, are optimized to reduce a loss function of the training error, as in discriminative methods. We show how boosting techniques can be extended to optimize the relational model proposed, with complexity linear in the number of parts and the number of features per image. This efficiency allows our method to learn relational models with many parts and features. The method has an advantage over purely generative and purely discriminative approaches for learning from sets of salient features, since generative method often use a small number of parts and features, while discriminative methods tend to ignore geometrical relations between parts. Experimental results are described, using some bench-mark data sets and three sets of newly collected data, showing the relative merits of our method in recognition and localization tasks.	[Bar-Hillel, Aharon] Intel Res Israel, IL-31015 Haifa, Israel; [Weinshall, Daphna] Hebrew Univ Jerusalem, Dept Comp Sci, IL-91904 Jerusalem, Israel; [Weinshall, Daphna] Hebrew Univ Jerusalem, Ctr Neural Computat, IL-91904 Jerusalem, Israel	Intel Corporation; Hebrew University of Jerusalem; Hebrew University of Jerusalem	Bar-Hillel, A (corresponding author), Intel Res Israel, POB 1659, IL-31015 Haifa, Israel.	aharon.bar-hillel@intel.com	Hillel, Aharon Bar/R-2656-2016	Bar-Hillel, Aharon/0000-0002-7303-0687				Agarwal S, 2002, LECT NOTES COMPUT SC, V2353, P113; Agarwal V, 2004, LANGMUIR, V20, P11, DOI 10.1021/la035375n; [Anonymous], 2001, NIPS; Bar-Hillel A., 2005, CVPR; BARHILLEL A, 2005, ICCV; Borenstein E, 2004, IEEE WORKSH PERC ORG, P46; Chan AB, 2004, FAMILY PROBABILISTIC; Csurka G., 2004, ECCV; DORKO G, 2005, UNPUB IEEE T PATTERN; Everingham M, 2006, LECT NOTES ARTIF INT, V3944, P117; FEIFEI L, 2003, ICCV; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Fergus R., 2003, CVPR; FERGUS R, 2005, CVPR; FREUND Y, 1996, INT C MACH LEARN, P148; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; GAO D., 2004, NIPS; HOLUB A, 2005, CVPR; HOLUB AD, 2005, ICCV; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Leibe B., 2004, STAT LEARNING COMPUT; Li Y, 2005, IEEE I CONF COMP VIS, P1605; LOEFF N, 2005, NIPS; Lowe DG, 2001, PROC CVPR IEEE, P682; Mason L, 2000, ADV NEUR IN, V12, P512; MURPHY K, 2003, NIPS; Opelt A., 2004, ECCV; OPELT A, 2004, UNPUB PAMI; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Serre T., 2005, CVPR; Thureson J, 2004, LECT NOTES COMPUT SC, V3022, P518; TORRALBA A, 2004, NIPS; Ullman S, 2002, NAT NEUROSCI, V5, P682, DOI 10.1038/nn870; ULUSOY I, 2005, CVPR, V2, P258; Vapnik V.N, 1998, STAT LEARNING THEORY; Vidal-Naquet M., 2003, ICCV; Viola P., 2001, P 2001 IEEE COMP SOC, pI, DOI [10.1109/CVPR.2001.990517, DOI 10.1109/CVPR.2001.990517]; [No title captured]	38	12	12	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2008	77	1-3					175	198		10.1007/s11263-007-0091-7	http://dx.doi.org/10.1007/s11263-007-0091-7			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	267RE					2022-12-18	WOS:000253526100011
J	Barnard, K; Fan, QF; Swaminathan, R; Hoogs, A; Collins, R; Rondot, P; Kaufhold, J				Barnard, Kobus; Fan, Quanfu; Swaminathan, Ranjini; Hoogs, Anthony; Collins, Roderic; Rondot, Pascale; Kaufhold, John			Evaluation of localized semantics: Data, methodology, and experiments	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image annotation; region labeling; ground truth data; segmetation; image semantics; WordNet	SPARSE	We present a new data set of 1014 images with manual segmentations and semantic labels for each segment, together with a methodology for using this kind of data for recognition evaluation. The images and segmentations are from the UCB segmentation benchmark database (Martin et al., in International conference on computer vision, vol. II, pp. 416-421, 2001). The database is extended by manually labeling each segment with its most specific semantic concept in WordNet (Miller et al., in Int. J. Lexicogr. 3(4): 235-244, 1990). The evaluation methodology establishes protocols for mapping algorithm specific localization (e. g., segmentations) to our data, handling synonyms, scoring matches at different levels of specificity, dealing with vocabularies with sense ambiguity (the usual case), and handling ground truth regions with multiple labels. Given these protocols, we develop two evaluation approaches. The first measures the range of semantics that an algorithm can recognize, and the second measures the frequency that an algorithm recognizes semantics correctly. The data, the image labeling tool, and programs implementing our evaluation strategy are all available on-line (kobus.ca//research/data/IJCV_2007). We apply this infrastructure to evaluate four algorithms which learn to label image regions from weakly labeled data. The algorithms tested include two variants of multiple instance learning (MIL), and two generative multi-modal mixture models. These experiments are on a significantly larger scale than previously reported, especially in the case of MIL methods. More specifically, we used training data sets up to 37,000 images and training vocabularies of up to 650 words. We found that one of the mixture models performed best on image annotation and the frequency correct measure, and that variants of MIL gave the best semantic range performance. We were able to substantively improve the performance of MIL methods on the other tasks (image annotation and frequency correct region labeling) by providing an appropriate prior.	[Barnard, Kobus; Fan, Quanfu; Swaminathan, Ranjini] Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA; [Hoogs, Anthony; Collins, Roderic] GE Global Res, Moscow 123098, Russia; [Rondot, Pascale] Lockheed Martin Corp, Aeronaut, Ft Worth, TX 76108 USA; [Kaufhold, John] SAIC Corp, Adv Concepts Business Unit, Mclean, VA 22102 USA	University of Arizona; General Electric; Lockheed Martin; Science Applications International Corporation (SAIC)	Barnard, K (corresponding author), Univ Arizona, Dept Comp Sci, POB 210077, Tucson, AZ 85721 USA.	kobus@cs.arizona.edu; quanfu@cs.arizona.edu; ranjini@cs.arizona.edu; hoogs@research.ge.com; collins@research.ge.com; pascale.rondot@lmco.com; kaufholdj@saic.com		Barnard, Kobus/0000-0002-8568-9518; Kaufhold, John/0000-0002-2236-7909				Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108; AGARWAL S, 2002, UIUC IMAGE DATABASE; AGIRRE E, 1995, 1 INT C REC ADV NAT; ANDREWS S, 2002, MULTIPLE INSTANCE LE; ANDREWS S, 2002, ADV NEURAL INFORM PR, V15; ANDREWS S, 2004, ADV NEUR INFORM PROC; Barnard K, 2003, PROC CVPR IEEE, P675; Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214; Barnard K, 2001, PROC CVPR IEEE, P434; Barnard K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P408, DOI 10.1109/ICCV.2001.937654; Berg AC, 2005, PROC CVPR IEEE, P26; CARBONETTO P, 2004, EU C COMP VIS; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen YX, 2004, J MACH LEARN RES, V5, P913; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fei-Fei L, 2004, WORKSH GEN MOD BAS V; FERGUS R, 1998, IEEE C COMP VIS PATT; GABBUR P, 2003, THESIS U ARIZONA TUC; GALE W, 1992, DARPA WORKSH SPEECH, P233; Jeon Jiwoon, 2003, SIGIR; JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710; Karov Y, 1998, COMPUT LINGUIST, V24, P41; Lavrenko V., 2003, ADV NEURAL INFORM PR; LEIBE B, 2003, TU DARMSTADT DATABAS; Maron O., 1998, THESIS MIT; MARON O, 1998, NEURAL INFORM PROCES; Maron O., 1998, 15 INT C MACH LEARN; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; MIHALCEA R, 1998, COLING ACL WORKSH US; Miller G.A., 1990, INT J LEXICOGRAPHY, V3, P235, DOI DOI 10.1093/IJL/3.4.235; OPELT A, 2004, TU GRAZ 02 DATABASE; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; TAO Q, 2004, IEE INT C TOOLS ART; TAO Q, 2004, P 17 INT FLOR ART IN, P550; Tao Q., 2004, 21 INT C MACH LEARN, P779; Torralba A, 2004, PROC CVPR IEEE, P762; TORRALBA A, 2003, MIT CSAIL DATABASE O; TRAUPMAN J, 2003, EXPT IMPROVING UNSUP; VIVARELLI F, 1997, IEE INT C ART NEUR N; WEBER M, 2000, 6 EUR C COMP VIS LON, P18; YAROWSKY D, 1995, 33 C APPL NAT LANG P; ZHANG Q, 2001, NEURAL INFORM PROCES	43	12	13	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2008	77	1-3					199	217		10.1007/s11263-007-0068-6	http://dx.doi.org/10.1007/s11263-007-0068-6			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	267RE					2022-12-18	WOS:000253526100012
J	Aharon, M; Kimmel, R				Aharon, Michal; Kimmel, Ron			Representation analysis and synthesis of lip images using dimensionality reduction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						automatic lipreading; image sequence processing; speech synthesis; multidimensional scaling; dimension reduction; locally linear embedding		Understanding facial expressions in image sequences is an easy task for humans. Some of us are capable of lipreading by interpreting the motion of the mouth. Automatic lipreading by a computer is a challenging task, with so far limited success. The inverse problem of synthesizing real looking lip movements is also highly non-trivial. Today, the technology to automatically generate an image series that imitates natural postures is far from perfect. We introduce a new framework for facial image representation, analysis and synthesis, in which we focus just on the lower half of the face, specifically the mouth. It includes interpretation and classification of facial expressions and visual speech recognition, as well as a synthesis procedure of facial expressions that yields natural looking mouth movements. Our image analysis and synthesis processes are based on a parametrization of the mouth configuration set of images. These images are represented as points on a two-dimensional flat manifold that enables us to efficiently define the pronunciation of each word and thereby analyze or synthesize the motion of the lips. We present some examples of automatic lips motion synthesis and lipreading, and propose a generalization of our solution to the problem of lipreading different subjects.	Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Aharon, M (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.	michalo@cs.technion.ac.il						AHARON M, 2004, CIS200401 ISR I TECH; BERGEN JR, 1992, IEEE T PAMI, V14; Borg I., 1997, MODERN MULTIDIMENSIO; BREGLER C, 1998, COMPUTER VISION MAN; BREGLER C, 1994, ADV NEURAL INFORMATI, V6, P43; BREGLER C, 1993, P INT C AC SPEECH SI, V1, P557; BREGLER C, 1997, COMPUTER GRAPHICS, V31, P353; Cramer J.S., 2003, ORIGINS LOGISTIC REG; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100; DUCHNOWSKI P, 1995, P INT C AC SPEECH SI, P109; FISHER CG, 1968, J SPEECH HEAR RES, V11, P796, DOI 10.1044/jshr.1104.796; Jacobs DW, 1998, PROC CVPR IEEE, P610, DOI 10.1109/CVPR.1998.698668; Kalberer GA, 2001, PROC SPIE, V4309, P16; Li N., 1997, MOTION BASED RECOGNI, P345; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; LUETTIN J, 1997, VISUAL SPEECH SPEAKE; MALONE SW, 2000, P STAT COMP SECT; MASE A, 1991, 117 MIT MED LAB; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; SCHWARTZ EL, 1989, IEEE T PATTERN ANAL, V11, P1005, DOI 10.1109/34.35506; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; VANROOSE P, 2002, P BSIT; Yacoob Y, 1996, IEEE T PATTERN ANAL, V18, P636, DOI 10.1109/34.506414	24	12	12	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2006	67	3					297	312		10.1007/s11263-006-5166-3	http://dx.doi.org/10.1007/s11263-006-5166-3			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	052JA		Green Submitted			2022-12-18	WOS:000238228800003
J	Gumerov, NA; Zandifar, A; Duraiswami, R; Davis, LS				Gumerov, NA; Zandifar, A; Duraiswami, R; Davis, LS			3D structure recovery and unwarping of surfaces applicable to planes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D structure recovery; unwarping; applicable surface; differential geometry; single view		The deformation of applicable surfaces such as sheets of paper satisfies the differential geometric constraints of isometry (lengths and areas are conserved) and vanishing Gaussian curvature. We show that these constraints lead to a closed set of equations that allow recovery of the full geometric structure from a single image of the surface and knowledge of its undeformed shape. We show that these partial differential equations can be reduced to the Hopf equation that arises in non-linear wave propagation, and deformations of the paper can be interpreted in terms of the characteristics of this equation. A new exact integration of these equations is developed that relates the 3-D structure of the applicable surface to an image. The solution is tested by comparison with particular exact Solutions. We present results for both the forward and the inverse 3D structure recovery problem.	Univ Maryland, Perceptual Interfaces & Real Lab, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Gumerov, NA (corresponding author), Univ Maryland, Perceptual Interfaces & Real Lab, College Pk, MD 20742 USA.	ramani@umiacs.umd.edu	Gumerov, Nail/G-7529-2011; Duraiswami, Ramani/J-6070-2012	Duraiswami, Ramani/0000-0002-5596-8460				Brown M. S., 2001, INT C COMP VIS ICCV; Clark P., 2001, P BRIT MACH VIS C; DOCORMO M, 1976, DIFFERENTIAL GEOMETR; FAUGERAS O, 1990, 3 DIMENSIONAL COMPUT; Gray Alfred., 1998, MODERN DIFFERENTIAL; GUMEROV N, 2004, EUR C COMP VIS ECCV; KANUNGO T, 1994, INT J IMAG SYST TECH, V5, P220, DOI 10.1002/ima.1850050305; KERGOSIEN YL, 1994, IEEE COMPUT GRAPH, V14, P40, DOI 10.1109/38.250917; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321; Korn G. A., 1968, MATH HDB SCI ENG; Liebowitz D, 1998, PROC CVPR IEEE, P482, DOI 10.1109/CVPR.1998.698649; McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7; MCIVOR AM, 1995, 2 AS C COMP VIS, V2, P275; PENNA MA, 1992, CVGIP-IMAG UNDERSTAN, V56, P366, DOI 10.1016/1049-9660(92)90048-8; PILU M, 2001, P IEEE C COM VIS PAT; PILU M, 2001, IEEE COMP VIS PATT R; Press WH., 1993, NUMERICAL RECIPES C; WANG YF, 1987, IEEE T PATTERN ANAL, V9, P129, DOI 10.1109/TPAMI.1987.4767878; Whitham B., 1974, LINEAR NONLINEAR WAV; YOU Y, 1994, INT J PATTERN RECOGI, V8, P351; Zandifar A, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P325, DOI 10.1109/ICMI.2002.1167016	22	12	13	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2006	66	3					261	281		10.1007/s11263-005-3678-x	http://dx.doi.org/10.1007/s11263-005-3678-x			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	028GP		Green Submitted			2022-12-18	WOS:000236475400003
J	Borzi, A; Grossauer, H; Scherzer, O				Borzi, A; Grossauer, H; Scherzer, O			Analysis of iterative methods for solving a Ginzburg-Landau equation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	4th International Conference on Scale Space Methods in Computer Vision	JUN 10-12, 2003	ISLE SKYE, SCOTLAND	British Machine Vis Assoc, Kings Coll London, IT Univ Copenhagen		Ginzburg-Landau equation; inpainting; diffusion filtering; non-linear partial differential equations; variational problems		Very recently we have proposed to use a complex Ginzburg-Landau equation for high contrast inpainting, to restore higher dimensional (volumetric) data (which has applications in frame interpolation), improving sparsely sampled data and to fill in fragmentary surfaces. In this paper we review digital inpainting algorithms and compare their performance with a Ginzburg-Landau inpainting model. For the solution of the Ginzburg-Landau equation we compare the performance of several numerical algorithms. A stability and convergence analysis is given and the consequences for applications to digital inpainting are discussed.	Graz Univ, Math Inst, A-8010 Graz, Austria; Univ Innsbruck, Dept Comp Sci, A-6020 Innsbruck, Austria	University of Graz; University of Innsbruck	Borzi, A (corresponding author), Graz Univ, Math Inst, Heinrichstr 36, A-8010 Graz, Austria.	harald.grossauer@uibk.ac.at; otmar.scherzer@uibk.ac.at	Scherzer, Otmar/AAA-4132-2019; Borzì, Alfio/AAF-6111-2020	Borzi, Alfio/0000-0002-8050-1336				Ambrosio L, 2000, CALCULUS OF VARIATIONS AND PARTIAL DIFFERENTIAL EQUATIONS, P5; Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036; Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261; Bertalmio M., 2000, SIGGRAPH 2000; BERTALMIO M, 2001, IEEE CVPR 2001 HAW U; Bohr T, 1997, PHYSICA D, V106, P95, DOI 10.1016/S0167-2789(97)00025-0; BORZI A, 2004, NUMER MATH, V54, P1050; Caselles V, 1996, PROG NONLIN, V25, P35; Chan TF, 2003, SIAM J APPL MATH, V63, P564; Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487; Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844; Chen LQ, 1998, COMPUT PHYS COMMUN, V108, P147, DOI 10.1016/S0010-4655(97)00115-X; DAVIS J, 2002, 1 INT S 3D DAT PROC; EFROS A, 1999, P 7 INT C COMP VIS C; Esedoglu S, 2002, EUR J APPL MATH, V13, P353, DOI 10.1017/S0956792501004904; Ginzburg V.L., 1950, SUPERCONDUCTIVITY SU, V20, P1064, DOI DOI 10.1007/978-3-540-68008-6_4; Grossauer H, 2004, LECT NOTES COMPUT SC, V3022, P214; GROSSAUER H, 2003, LECT NOTES COMPUTER, V2695; GROSSAUER H, 2004, P ECCV 04, V2; HELEIN H.F., 1994, PROGR NONLINEAR DIFF, V13; IGEHY H, 1997, P 1997 IEEE INT C IM; Ipsen M, 2000, PHYS REV LETT, V84, P2389, DOI 10.1103/PhysRevLett.84.2389; JOYEUX L, 1999, P CVPR 99 IEEE INT C; Masnou S, 2002, IEEE T IMAGE PROCESS, V11, P68, DOI 10.1109/83.982815; MASNOUS, 1998, P 1998 IEEE INT C IM, P259; Morton K. W., 1994, NUMERICAL SOLUTION P; OLIVEIRA MM, 2001, P INT C VIS IM IM PR, P261; RAMASUBRAMANIAN M, 1999, ANN C SERIES, P73; THENAZ P, 2000, HDB MED IMAGING PROC, P393; Thomas J. W., 1995, TEXTS APPL MATH, V22; Unser M, 2000, P IEEE, V88, P569, DOI 10.1109/5.843002; VANHECKE M, 1995, PHYS REV LETT, V75, P3830, DOI 10.1103/PhysRevLett.75.3830; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; Yamauchi H, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P120, DOI 10.1109/CGI.2003.1214456; MAGAZIN COMPUTER TEC, P190	35	12	12	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2005	64	2-3					203	219		10.1007/s11263-005-1844-9	http://dx.doi.org/10.1007/s11263-005-1844-9			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	961YC					2022-12-18	WOS:000231696700009
J	Zalesny, A; Ferrari, V; Caenen, G; Van Gool, L				Zalesny, A; Ferrari, V; Caenen, G; Van Gool, L			Composite texture synthesis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						texture synthesis; texture analysis; statistical texture modeling; composite textures; hierarchical texture model		Many textures require complex models to describe their intricate structures. Their modeling can be simplified if they are considered composites of simpler subtextures. After an initial, unsupervised segmentation of the composite texture into the subtextures, it can be described at two levels. One is a label map texture, which captures the layout of the different subtextures. The other consists of the different subtextures. This scheme has to be refined to also include mutual influences between textures, mainly found near their boundaries. The proposed composite texture model also includes these. The paper describes an improved implementation of this idea. Whereas in a previous implementation subtextures and their interactions were synthesized sequentially, this paper proposes a parallel implementation, which yields results of higher quality.	ETH, D ITET BIWI, Zurich, Switzerland; Katholieke Univ Leuven, ESAT PSI Vis, Louvain, Belgium	Swiss Federal Institutes of Technology Domain; ETH Zurich; KU Leuven	Zalesny, A (corresponding author), ETH, D ITET BIWI, Zurich, Switzerland.	zalesny@vision.ce.ethz.ch; ferrari@vision.ee.ethz.ch; caenen@esat.kuleuven.ac.be; vangool@vision.ee.ethz.ch						Aherne FJ, 1998, KYBERNETIKA, V34, P363; ASLAM J, 2000, WORKSH ALG ENG; COMANICIU D, 2000, P ICPR, V3, P629; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Ferrari V, 2001, PROC CVPR IEEE, P226; GAGALOWICZ A, 1985, COMPUT VISION GRAPH, V30, P289, DOI 10.1016/0734-189X(85)90162-8; GIMELFARB G, 1999, IMAGE TEXTURES GIBBS, P250; GOUSSEAU Y, 2002, P INT WORKSH TEXT AN, P53; GRAHAM RL, 1995, HDB COMBINATORICS, V2, P1890; MALIK J, 2000, PERCEPTUAL ORG ARTIF; PAGET R, 1999, THESIS U QUEENSLAND; Puzicha J, 1999, PATTERN RECOGN LETT, V20, P899, DOI 10.1016/S0167-8655(99)00056-2; PUZICHA J, 2000, MODEL BASED HALFTONI; Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407; Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009; ZALESNY A, 2002, P ECCV, P180; Zalesny A., 2001, LECT NOTES COMPUTER, V2018, P124	18	12	18	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-MAY	2005	62	1-2					161	176		10.1007/s11263-005-4640-7	http://dx.doi.org/10.1007/s11263-005-4640-7			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	866XV		Green Published, Green Submitted			2022-12-18	WOS:000224807600010
J	Kain, J; Ostrov, DN				Kain, J; Ostrov, DN			Numerical shape-from-shading for discontinuous photographic images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shape-from-shading; viscosity solutions; discontinuous Hamilton-Jacobi equations; convergent numerical methods	HAMILTON-JACOBI EQUATIONS; VISCOSITY SOLUTIONS; ALGORITHMS; UNIQUENESS	The height, u(x, y), of a continuous, Lambertian surface of known albedo (i.e., grayness) is related to n(x, y), information recoverable from a black and white flash photograph of the surface, by the partial differential equation rootu(x)(2) + u(y)(2) - n = 0. We review the notion of a unique viscosity solution for this equation when n is continuous and a recent unique extension of the viscosity solution when n is discontinuous. We prove convergence to this extension for a wide class of the numerical algorithms that converge when n is continuous. After discussing the properties of the extension and the order of error in the algorithms simulating the extension, we point out warning signs which, when observed in the numerical solution, usually indicate that the surface is not continuous or that the viscosity solution or its extension does not correspond to the actual surface. Finally, we discuss a method that, in some of these cases, allows us to correct the simulation and recover the actual surface again.										Barles G., 1991, Asymptotic Analysis, V4, P271; Boue M, 1999, SIAM J NUMER ANAL, V36, P667, DOI 10.1137/S0036142997323521; BRUCKSTEIN AM, 1988, COMPUT VISION GRAPH, V44, P139, DOI 10.1016/S0734-189X(88)80002-1; BRUSS AR, 1982, J MATH PHYS, V23, P890, DOI 10.1063/1.525441; CRANDALL MG, 1984, T AM MATH SOC, V282, P487, DOI 10.2307/1999247; CRANDALL MG, 1983, T AM MATH SOC, V277, P1, DOI 10.2307/1999343; DUPUIS P, 1993, ANN APPL PROBAB, V4, P287; EASTWOOD M, 1995, ADV APPL MATH, V16, P259, DOI 10.1006/aama.1995.1012; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; Horn B.K.P., 1989, SHAPE SHADING; Horn Berthold K. P., 1975, PSYCHOL COMPUTER VIS, P115; ISHII H, 1987, P AM MATH SOC, V100, P247, DOI 10.2307/2045953; ISHII H., 1985, B FAC SCI ENG CHUO U, V28, P33; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P47, DOI 10.1006/cviu.1995.1040; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P360, DOI 10.1006/cviu.1995.1060; KIMMEL R, 1995, INT J COMPUT VISION, V16, P107, DOI 10.1007/BF01539551; KIMMEL R, 1992, THESIS TECHNION ISRA; Kushner H.J., 1992, NUMERICAL METHODS ST; LIONS PL, 1993, NUMER MATH, V64, P323, DOI 10.1007/BF01388692; LIONS PL, 1982, PITMAN RES NOTES SER; Ostrov DN, 2000, NONLINEAR ANAL-THEOR, V42, P709, DOI 10.1016/S0362-546X(99)00164-9; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; SAXBERG BVH, 1992, INT J ROBOT RES, V11, P202, DOI 10.1177/027836499201100304; Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591; SOUGANIDIS PE, 1985, J DIFFER EQUATIONS, V59, P1, DOI 10.1016/0022-0396(85)90136-6; TOURIN A, 1992, NUMER MATH, V62, P75, DOI 10.1007/BF01396221; TSITSIKLIS JN, 1995, IEEE T AUTOMAT CONTR, V40, P1528, DOI 10.1109/9.412624	27	12	12	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP-OCT	2001	44	3					163	173		10.1023/A:1012235914303	http://dx.doi.org/10.1023/A:1012235914303			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	496NM					2022-12-18	WOS:000172402900001
J	Bruckstein, AM; Holt, RJ; Huang, TS; Netravali, AN				Bruckstein, AM; Holt, RJ; Huang, TS; Netravali, AN			New devices for 3D pose estimation: Mantis eyes, agam paintings, sundials, and other space fiducials	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						pose determination; space fiducials	CAMERA CALIBRATION; VISION	Several unconventional ideas for viewer/camera pose estimation are discussed. The methods proposed so far advocate the use of advanced image processing for identification and precise location of calibration objects in the images acquired, and base pose recovery on the identification of the viewing dependent deformations of these objects. We propose to more fully exploit the freedom in the design of "space fiducials" or calibration objects showing that we can build objects whose images directly encode, in easily identifiable gray-level/color or temporal patterns, the pose of their viewer. We also show how to construct high-precision fiducials, which can determine a viewing direction quite accurately when it is known to lie within a relatively narrow range.	Lucent Technol, Bell Labs, Murray Hill, NJ 07974 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	Alcatel-Lucent; Lucent Technologies; AT&T; University of Illinois System; University of Illinois Urbana-Champaign	Bruckstein, AM (corresponding author), Lucent Technol, Bell Labs, Murray Hill, NJ 07974 USA.			Bruckstein, Alfred/0000-0001-5669-0037				BASU A, 1995, IEEE T SYST MAN CYB, V25, P256, DOI 10.1109/21.364838; Bergkvist, 1979, U.S. Patent, Patent No. 4166699; Bruckstein AM, 1999, INT J COMPUT VISION, V35, P223, DOI 10.1023/A:1008156210387; Debevec P., 1997, P ACM SIGGRAPH 1997, DOI [DOI 10.1145/258734.258884, 10.1145/258734.258884]; GANAPATHY S, 1984, PATTERN RECOGN LETT, V2, P401, DOI 10.1016/0167-8655(84)90007-2; HARALICK RM, 1994, INT J COMPUT VISION, V13, P331, DOI 10.1007/BF02028352; HEALEY G, 1991, SPIE OPTICS ILLUMINA, V1614, P121; HOLT RJ, 1991, CVGIP-IMAG UNDERSTAN, V54, P368, DOI 10.1016/1049-9660(91)90037-P; Kunkel B., 1987, U.S. Patent, Patent No. 4710620; Nalwa V. S., 1993, GUIDED TOUR COMPUTER; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; Robert L, 1996, COMPUT VIS IMAGE UND, V63, P314, DOI 10.1006/cviu.1996.0021; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109	13	12	12	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2000	39	2					131	139		10.1023/A:1008123110489	http://dx.doi.org/10.1023/A:1008123110489			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	359FD					2022-12-18	WOS:000089600100005
J	Basri, R; Moses, Y				Basri, R; Moses, Y			When is it possible to identify 3D objects from single images using class constraints?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object recognition; class-based recognition; viewpoint invariance	PROJECTIVE RECONSTRUCTION; UNCALIBRATED IMAGES; 6 POINTS; INVARIANTS; RECOGNITION; AFFINE; MODELS; MOTION; VIEW	One approach to recognizing objects seen from arbitrary viewpoint is by extracting invariant properties of the objects from single images. Such properties are found in images of 3D objects only when the objects are constrained to belong to certain classes (e.g., bilaterally symmetric objects). Existing studies that follow this approach propose how to compute invariant representations for a handful of classes of objects. A fundamental question regarding the invariance approach is whether it can be applied to a wide range of classes. To answer this question it is essential to study the set of classes for which invariance exists. This paper introduces a new method for determining the existence of invariant functions for classes of objects together with the set of images from which these invariants can be computed. We develop algebraic tests that determine whether the objects in a given class can be identified from single images. These tests apply to classes of objects undergoing affine projection. In addition, these tests allow us to determine the set of views of the objects which are degenerate. We apply these tests to several classes of objects and determine which of them is identifiable and which of their views are degenerate.	Weizmann Inst Sci, Dept Comp Sci, IL-76100 Rehovot, Israel	Weizmann Institute of Science	Basri, R (corresponding author), Weizmann Inst Sci, Dept Comp Sci, IL-76100 Rehovot, Israel.							[Anonymous], 1985, PERCEPTUAL ORG VISUA; Basri R, 1996, INT J COMPUT VISION, V19, P169, DOI 10.1007/BF00055803; BURNETT RH, 1993, J VINYL TECHNOL, V15, P1; CLEMENS DT, 1991, IEEE T PATTERN ANAL, V13, P1007, DOI 10.1109/34.99235; FAUGERAS OD, 1992, 2 EUR C COMP VIS ECC, P563; FAWCETT R, 1994, IMAGE VISION COMPUT, V12, P615, DOI 10.1016/0262-8856(94)90015-9; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233; HARTLEY R, 1993, P DARPA IM UND WORKS, P737; HARTLEY RI, 1994, IEEE T PATTERN ANAL, V16, P1036, DOI 10.1109/34.329005; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; JACOBS D, 1992, IEEE C COMP VIS PATT, P439; JONES MJ, 1995, INT C COMP VIS, P531; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; Lamdan Y., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P238, DOI 10.1109/CCV.1988.589995; MITSUMOTO H, 1992, IEEE T PATTERN ANAL, V14, P941, DOI 10.1109/34.161352; MOSES Y, 1992, 2 EUR C COMP VIS, P820; MUNDY JL, 1994, LECT NOTES COMPUTER, V825; POGGIO T, 1992, 1347 MIT AI; QUAN L, 1995, IEEE T PATTERN ANAL, V17, P34; Quan L, 1996, IEEE T PATTERN ANAL, V18, P151, DOI 10.1109/34.481540; Rothwell C.A., 1995, OBJECT RECOGNITION I; ROTHWELL CA, 1992, P 2 EUR C COMP VIS, P757; SPARR G, 1992, IMAGE VISION COMPUT, V10, P683, DOI 10.1016/0262-8856(92)90013-S; SUGIMOTO A, 1994, P 12 INT C PATT REC, V1, P190; Thompson D. W., 1987, Proceedings of the 1987 IEEE International Conference on Robotics and Automation (Cat. No.87CH2413-3), P208; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; WEINSHALL D, 1993, INT J COMPUT VISION, V10, P27, DOI 10.1007/BF01440845; Weinshall D, 1997, IEEE T PATTERN ANAL, V19, P97, DOI 10.1109/34.574783; WEISS I, 1988, DARPA IM UND WORKSH, P1125; WERMAN M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P473, DOI 10.1109/ICCV.1995.466902; Yan X, 1997, PATTERN RECOGN, V30, P513, DOI 10.1016/S0031-3203(96)00088-X; ZISSERMAN A, 1995, ARTIF INTELL, V78, P239, DOI 10.1016/0004-3702(95)00023-2	34	12	13	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1999	33	2					95	116		10.1023/A:1008169328072	http://dx.doi.org/10.1023/A:1008169328072			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	247MF					2022-12-18	WOS:000083224000001
J	de Ves, E; Diaz, ME; Ayala, G; Domingo, J; Simo, A				de Ves, E; Diaz, ME; Ayala, G; Domingo, J; Simo, A			Robust descriptors of binary shapes with applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						2D binary shape description; stochastic mathematical morphology; granulometry; geometric covariogram; shape matching		The subject of this paper is to propose and test a set of numerical descriptors of 2D binary planar shapes. Given a shape, A, the transformations of A with a given mathematical morphological operation and different structuring elements are considered. The measures of this family of transformed sets provide a numerical description of the original set A. These descriptors are very robust against noise and maintain a reasonable discriminatory power. The robustness against different levels of contour degradation is tested by simulation. Starting with a clean (without noise) set, Lambda, it is assumed that the observed set, A, is a noisy version (with contour degradation) of Lambda. The performance of the descriptors, when they are used to compare different shapes or shapes from a scene with models, is studied and compared with related descriptors based on the granulometric analysis of the original set, which are the closest previous alternative to our approach in the literature.	Univ Valencia, Inst Robot, E-46003 Valencia, Spain; Univ Jaume 1, Dept Matemat, Castello, Spain	University of Valencia; Universitat Jaume I	de Ves, E (corresponding author), Univ Valencia, Inst Robot, Avda Vicent Andres Estelles S-N, E-46003 Valencia, Spain.		Ayala, Guillermo/A-8077-2008; Ayala, Guillermo/N-5766-2019; Simó, Amelia/D-6321-2017; de Ves Cuenca, Esther/R-8157-2018; Diaz, Maria Elena/H-5996-2011; Domingo, Juan/E-9709-2018; Domingo, Juan/AAA-2379-2019	Ayala, Guillermo/0000-0002-6231-2865; Ayala, Guillermo/0000-0002-6231-2865; Simó, Amelia/0000-0001-5507-2907; de Ves Cuenca, Esther/0000-0002-7702-8808; Diaz, Maria Elena/0000-0002-6818-6943; Domingo, Juan/0000-0003-4728-6256; Domingo, Juan/0000-0003-4728-6256				BILLINGSLEY P, 1989, PROBABILITY; COSTER M, 1985, PRECIS ANAL IMAGES; Gonzalez R. C., 1987, DIGITAL IMAGE PROCES; Haralick R.M., 1992, COMPUTER ROBOT VISIO, V1; HEIJMANS HJA, 1994, NATO ASI SERIES F, V126, P147; Kotz S, 1983, ENCY STAT SCI, V4; Matheron G, 1970, CAHIERS CMM FASCICUL; MATHERON G., 1975, RANDOM SETS INTEGRAL; MATTIOLI J, 1994, NATO ASI SERIES F, V126, P177; MOORE M, 1986, CANADIAN J STAT, V14, P104; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Rachev S. T., 1991, PROBABILITY METRICS; RIPLEY BD, 1986, CAN J STAT, V14, P83, DOI 10.2307/3314656; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; Sonka M., 1993, IMAGE PROCESSING ANA, DOI DOI 10.1007/978-1-4899-3216-7_4; Stoyan, 1994, FRACTALS RANDOM SHAP, V302; Stoyan D., 1995, STOCHASTIC GEOMETRY; URAS C, 1994, NATO ASI SERIES F, V126, P81; Ying-Lie O, 1994, NATO ASI SERIES F, V126	19	12	12	0	5	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1999	34	1					5	17		10.1023/A:1008164518969	http://dx.doi.org/10.1023/A:1008164518969			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	248WN					2022-12-18	WOS:000083299900001
J	Kimmel, R; Kiryati, N; Bruckstein, AM				Kimmel, R; Kiryati, N; Bruckstein, AM			Analyzing and synthesizing images by evolving curves with the Osher-Sethian method	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shape from shading; halftoning; offsets; distance maps; minimal geodesics; segmentation; numerical methods	LEVEL SETS; SHORTEST PATHS; DISTANCE MAPS; SHAPE; SURFACES; PROPAGATION; CURVATURE; EVOLUTION; FRONTS	Numerical analysis of conservation laws plays an important role in the implementation of curve evolution equations. This paper reviews the relevant concepts in numerical analysis and the relation between curve evolution, Hamilton-Jacobi partial differential equations, and differential conservation laws. This close relation enables us to introduce finite difference approximations, based on the theory of conservation laws, into curve evolution. It is shown how curve evolution serves as a powerful tool for image analysis, and how these mathematical relations enable us to construct efficient and accurate numerical schemes. Some examples demonstrate the importance of the CFL condition as a necessary condition for the stability of the numerical schemes.	TECHNION ISRAEL INST TECHNOL, DEPT ELECT ENGN, IL-32000 HAIFA, ISRAEL; TECHNION ISRAEL INST TECHNOL, DEPT COMP SCI, IL-32000 HAIFA, ISRAEL	Technion Israel Institute of Technology; Technion Israel Institute of Technology	Kimmel, R (corresponding author), UNIV CALIF BERKELEY, LAWRENCE BERKELEY LAB, MAIL STOP 50A-2152, BERKELEY, CA 94720 USA.			Kiryati, Nahum/0000-0003-1436-2275				ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; ALVAREZ L, 1992, SIAM J NUMER ANAL, V29, P845, DOI 10.1137/0729052; ALVAREZ L, 1993, ARCH RATIONAL MECH, V123; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; BRUCKSTEIN AM, 1988, COMPUT VISION GRAPH, V44, P139, DOI 10.1016/S0734-189X(88)80002-1; BRUCKSTEIN AM, 1994, P IEEE ICIP AUST TEX, V1, P11; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; CHOPP DL, 1993, J COMPUT PHYS, V106, P77, DOI 10.1006/jcph.1993.1092; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; COURANT R, 1967, IBM J RES DEV, V11, P215, DOI 10.1147/rd.112.0215; Courant R, 1928, MATH ANN, V100, P32, DOI 10.1007/BF01448839; Epstein C L, 1987, WAVE MOTION THEORY M; Feynman R. P., 1964, FEYNMAN LECT PHYSICS; GAGE M, 1986, J DIFF GEOM, V23; GRAYSON M, 1987, J DIFF GEOM, V26; HARTEN A, 1976, COMMUN PUR APPL MATH, V29, P297, DOI 10.1002/cpa.3160290305; KIMMEL R, 1995, COMPUT MATH APPL, V29, P49, DOI 10.1016/0898-1221(94)00228-D; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P47, DOI 10.1006/cviu.1995.1040; KIMMEL R, 1995, IEEE T PATTERN ANAL, V17, P635, DOI 10.1109/34.387512; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P382, DOI 10.1006/cviu.1995.1062; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P360, DOI 10.1006/cviu.1995.1060; KIMMEL R, 1995, INT J COMPUT VISION, V16, P107, DOI 10.1007/BF01539551; KIMMEL R, 1993, COMPUT AIDED DESIGN, V25, P154, DOI 10.1016/0010-4485(93)90040-U; KIMMEL R, 1994, INT C PATT RECOG, P367; KIMMEL R, 1992, THESIS ISRAEL I TECH; KIMMEL R, 1996, IN PRESS J MATH IMAG; KIMMEL R, 1995, THESIS ISRAEL I TECH; KIRYATI N, 1993, PATTERN RECOGN, V26, P1623, DOI 10.1016/0031-3203(93)90018-R; Kuznetsov NN., 1976, USSR COMP MATH MATH+, V16, P105, DOI [10.1016/0041-5553(76)90046-X, DOI 10.1016/0041-5553(76)90046-X]; LAX PD, 1973, HYPERBOLIC STSTEMS C; LeVeque RJ., 1992, LECT MATH ETH ZURICH, V2nd; MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; OSHER S, 1991, SIAM J NUMER ANAL, V28, P907, DOI 10.1137/0728049; Pnueli Y., 1994, Visual Computer, V10, P277, DOI 10.1007/BF01901584; Pnueli Y, 1996, GRAPH MODEL IM PROC, V58, P38, DOI 10.1006/gmip.1996.0003; SAPIRO G, 1993, PATTERN RECOGN, V26, P1363, DOI 10.1016/0031-3203(93)90142-J; SAPIRO G, 1993, INT J COMPUT VISION, V11, P25, DOI 10.1007/BF01420591; SAPIRO G, 1995, IEEE T PAMI, V17; SCHROEDER M, 1983, MATH INTELL, V5, P36, DOI 10.1007/BF03023504; SETHIAN JA, 1985, COMMUN MATH PHYS, V101, P487, DOI 10.1007/BF01210742; SETHIAN JA, 1990, J DIFFER GEOM, V31, P131; SHAKED D, 1995, IN PRESS CGIU; Smoller J., 1983, GRUNDLEHREN MATH WIS; SOD GA, NUMERICAL METHODS FL; Weickert J., 1995, THESIS KAISERSLAUTER; [No title captured]	48	12	51	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1997	24	1					37	55		10.1023/A:1007970107971	http://dx.doi.org/10.1023/A:1007970107971			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XV644					2022-12-18	WOS:A1997XV64400003
J	AZENCOTT, R; CHALMOND, B; COLDEFY, F				AZENCOTT, R; CHALMOND, B; COLDEFY, F			MARKOV FUSION OF A PAIR OF NOISY IMAGES TO DETECT INTENSITY VALLEYS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								Our presentation is related to a non-destructive control industrial task: the detection of defects on pairs of gamma-radiographic images. The images are very noisy and have a strong luminosity gradient. Defects are identified with intensity valleys. First we present a Bayes-Markov model in order to estimate the noise, the gradient and the valley bottom lines of defects for a single image. Then, we define a Markov fusion model for a pair incorporating a criterion of similarity between matched images. The proposed Markov models are general and can be used in other situations for detecting valley bottoms in noisy images.	ECOLE NORMALE SUPER,DIAM CMLA,CHACHAN,FRANCE; UNIV CERGY PONTOISE,DEPT PHYS,CERGY,FRANCE; UNIV PARIS 11,ORSAY,FRANCE	CY Cergy Paris Universite; UDICE-French Research Universities; Universite Paris Saclay	AZENCOTT, R (corresponding author), SUDIMAGE,47 AV CARNOT,F-94230 CHACHAN,FRANCE.							AZENCOTT R, 1992, LECTURES NOTES STATI, P75; AZENCOTT R, 1992, LECT NOTES STAT, V74, P46; AZENCOTT R, 1992, 11TH P INT C PATT RE; AZENCOTT R, 1987, P INT C IND APPL MAT; BESAG J, 1988, J ROYAL STATIS SOC, pB148; BROWNLEE KA, 1965, STATISTICAL THEORY M; CA J, 1973, REV AUTOM INFORM REC; CAROLL RJ, 1988, J AM STAT ASSOC, V83, P1045; CHALMOND B, 1989, PATTERN RECOGN, V22, P747, DOI 10.1016/0031-3203(89)90011-3; CHALMOND B, 1988, SIGNAL PROCESS, V15, P115, DOI 10.1016/0165-1684(88)90065-5; CHALMOND B, 1991, HP219143 EDF DER INT; CHALMOND B, 1986, STAT ANAL DONNES, V11, P1; CHALMOND B, 1993, IN RPESS WAVLETS IMA; CHALMOND B, 1982, P COMPUT STATIST, V2; CLARK R, 1990, DATA FUSION SENSORYI; COLDEFY F, 1993, THESIS PARIS S U ORS; DAVID C, 1990, INT J COMPUT VISION, V5, P219, DOI 10.1007/BF00126500; Do Carmo M.P., 2016, DIFFERENTIAL GEOMETR, Vsecond; EUBANK P, 1990, SPLINE SMOOTHING NON; GAUCH JM, 1993, IEEE T PATTERN ANAL, P635; GEMAN D, 1990, IEEE T PATTERN ANAL, V12, P609, DOI 10.1109/34.56204; GEMAN D, 1992, LECTURE NOTES MATH E, V1427, P111; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; MARROQUIN J, 1987, J AM STAT ASSOC, V82, P76, DOI 10.2307/2289127; ORD K, 1975, J AM STAT ASSOC, V70, P120, DOI 10.2307/2285387; PARENT P, 1989, IEEE T PATTERN ANAL, V11, P823, DOI 10.1109/34.31445; PRENTER PM, 1971, SPLINES VARIATIONAL; TAN HL, 1991, IEEE T PATTERN ANAL, V14, P3; YUILLE AL, 1990, LECTURE NOTES COMPUT, V427	29	12	12	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1995	16	2					135	145		10.1007/BF01539552	http://dx.doi.org/10.1007/BF01539552			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TC767					2022-12-18	WOS:A1995TC76700002
J	DICKINSON, SJ; METAXAS, D				DICKINSON, SJ; METAXAS, D			INTEGRATING QUALITATIVE AND QUANTITATIVE SHAPE RECOVERY	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							GLOBAL DEFORMATIONS; MODELS; SUPERQUADRICS; RECOGNITION; REPRESENTATION; COMPONENTS; IMAGES; VISION; FORM	Recent work in qualitative shape recovery and object recognition has focused on solving the ''what is it'' problem, while avoiding the ''where is it'' problem. In contrast, typical CAD-based recognition systems have focused on the ''where is it'' problem, while assuming they know what the object is. Although each approach addresses an important aspect of the 3-D object recognition problem, each falls short in addressing the complete problem of recognizing and localizing 3-D objects from a large database. In this paper, we first synthesize a new approach to shape recovery for 3-D object recognition that decouples recognition from localization by combining basic elements from these two approaches. Specifically, we use qualitative shape recovery and recognition techniques to provide strong fitting constraints on physics-based deformable model recovery techniques. Secondly, we extend our previously developed technique of fitting deformable models to occluding image contours to the case of image data captured under general orthographic, perspective, and stereo projections. On one hand, integrating qualitative knowledge of-the object being fitted to the data, along with knowledge of occlusion supports a much more robust and accurate quantitative fitting. On the other hand, recovering object pose and quantitative surface shape not only provides a richer description for indexing, but supports interaction with the world when object manipulation is required. This paper presents the approach in detail and applies it to real imagery.	UNIV TORONTO,DEPT COMP SCI,TORONTO,ON M5S 1A4,CANADA; UNIV PENN,DEPT COMP & INFORMAT SCI,PHILADELPHIA,PA 19104	University of Toronto; University of Pennsylvania	DICKINSON, SJ (corresponding author), RUTGERS STATE UNIV,CTR COGNIT SCI,POB 1179,PISCATAWAY,NJ 08855, USA.							[Anonymous], 1985, PERCEPTUAL ORG VISUA; [Anonymous], 1980, PRINCIPLES ARTIFICIA; Barr A. H., 1981, IEEE Computer Graphics and Applications, V1, P11, DOI 10.1109/MCG.1981.1673799; BERGEVIN R, 1989, NOV P IEEE WORKSH IN, P68; BIEDERMAN I, 1985, COMPUT VISION GRAPH, V32, P29, DOI 10.1016/0734-189X(85)90002-7; BIEDERMAN I, 1992, P SPIE APPLICATIONS, V10, P570; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; CLEMENS D, 1991, AITR1307 MIT ART INT; DICKINSON S, 1993, 8TH P SCAND C IM AN; DICKINSON S, 1990, VISION CONVERGENCE D; DICKINSON SJ, 1992, IEEE T PATTERN ANAL, V14, P174, DOI 10.1109/34.121788; DICKINSON SJ, 1992, CVGIP-IMAG UNDERSTAN, V55, P130, DOI 10.1016/1049-9660(92)90013-S; FAIRWOOD RC, 1991, IMAGE VISION COMPUT, V9, P113, DOI 10.1016/0262-8856(91)90021-G; GUPTA A, 1991, MSCIS9145 U PENNS TE; HUTTENLOCHER D, 1988, MIT1045 ART INT LAB; JACOTDESCOMBES A, 1992, P SPIE APPL ARTIFICI, V10, P579; KOENDERINK JJ, 1979, BIOL CYBERN, V32, P211, DOI 10.1007/BF00337644; LEE JSJ, 1987, IEEE T ROBOTIC AUTOM, V3, P142, DOI 10.1109/JRA.1987.1087088; Metaxas D., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P337, DOI 10.1109/CVPR.1991.139712; METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P715, DOI 10.1109/34.85660; PENTLAND AP, 1990, INT J COMPUT VISION, V4, P107, DOI 10.1007/BF00127812; PENTLAND AP, 1986, ARTIF INTELL, V28, P293, DOI 10.1016/0004-3702(86)90052-4; RAJA NS, 1992, IMAGE VISION COMPUT, V10, P179, DOI 10.1016/0262-8856(92)90069-F; SAINTMARC P, 1991, IEEE T PATTERN ANAL, V13, P514, DOI 10.1109/34.87339; Shabana A. A, 1989, DYNAMICS MULTIBODY S; SOLINA F, 1990, IEEE T PATTERN ANAL, V12, P131, DOI 10.1109/34.44401; TERZOPOULOS D, 1991, IEEE T PATTERN ANAL, V13, P703, DOI 10.1109/34.85659; TERZOPOULOS D, 1988, ARTIF INTELL, V36, P91, DOI 10.1016/0004-3702(88)90080-X; TERZOPOULOS D, 1988, IEEE COMPUT GRAPH, V8, P41, DOI 10.1109/38.20317; THOMPSON D, 1987, P DARPA IMAGE UNDERS, P93; [No title captured]	32	12	12	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1994	13	3					311	330		10.1007/BF02028351	http://dx.doi.org/10.1007/BF02028351			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QF170		Green Published			2022-12-18	WOS:A1994QF17000004
J	NEGAHDARIPOUR, S				NEGAHDARIPOUR, S			CRITICAL SURFACE PAIRS AND TRIPLETS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									UNIV HAWAII,DEPT ELECT ENGN,HONOLULU,HI 96822	University of Hawaii System								ADIV G, 1985, IEEE T PAMI, V7; ADIV G, 1985, JUN P IEEE C COMP VI; BALLARD DH, 1983, COMPUTER VISION, V22; BARNARD ST, 1980, IEEE T PAMI, V2; BRUSS AR, 1983, COMPUT VISION GRAPHI, V21; BUXTON BF, 1984, 6TH P EUR C ART INT; HAY JC, 1966, PSYCHOL REV, V73; HOFFMAN DD, 1982, J OPT SOC AM, V72; HORN BKP, 1987, INT J COMPUT VISION, V1; JERIAN C, 1984, IEEE T PAMI, V6; KANATANI K, 1987, COMPUT VISION GRAPHI, V38; KANATANI K, 1986, COMPUT VISION GRAPHI, V35; Longuet-Higgins H. C, 1984, P ROY SOC LOND B BIO, P223; Longuet-Higgins H. C., 1980, P ROYAL SOC LOND B, V208; MAYBANK SJ, 1985, P ROY SOC LOND A MAT, P401; NEGAHDARIPOUR S, 1987, 1ST P INT C COMP VIS; NEGAHDARIPOUR S, 1987, IEEE T PAMI, V9; NEGAHDARIPOUR S, 1986, THESIS MIT CAMBRIDGE; SUBBARAO M, 1987, 1ST P INT C COMP VIS; SUBBARAO M, 1986, COMPUT VISION GRAPH, P36; SUGIHARA K, 1984, COMPUT VISION GRAPH, P27; TSAI R, 1982, IEEE T ASSP, P30; Tsai R. Y., 1984, IEEE T PAMI, V6; ULLMAN S, 1983, AI706 MIT AI LAB MEM; WAXMAN AM, 1987, INT J COMPUT VISION, V1	25	12	12	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1989	3	4					293	312		10.1007/BF00132601	http://dx.doi.org/10.1007/BF00132601			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CC907					2022-12-18	WOS:A1989CC90700002
J	FUNT, B; HO, JA				FUNT, B; HO, JA			COLOR FROM BLACK AND WHITE	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									SIMON FRASER UNIV,DEPT COMP SCI,BURNABY V5A 1S6,BC,CANADA	Simon Fraser University								Born M., 1980, PRINCIPLES OPTICS, P180; BRAINARD DH, 1986, J OPT SOC AM A, V3, P1651, DOI 10.1364/JOSAA.3.001651; COHEN J, 1964, PSYCHON SCI, V1, P369, DOI 10.3758/BF03342963; DEANE B, 1964, J OPT SOC AM, V54, P1031; FUNT B, 1988, JUN P IEEE C COMP VI; Garcia L., 1972, INTRO CLASSICAL THER; GERSHON R, 1987, 10TH P INT JOINT C A; HO J, 1988, CSSLCCRTR8818 S FRAS; HO J, 1988, CSSLCCRTR8817 S FRAS; HOPKINS HH, 1955, PROC R SOC LON SER-A, V231, P91, DOI 10.1098/rspa.1955.0158; HOWARTH PA, 1986, VISION RES, V26, P361, DOI 10.1016/0042-6989(86)90034-9; Klein M., 1970, ENCY BR; KRINOV EL, 1947, TT439 NAT RES COUNC; Land E. H., 1974, Proceedings of the Royal Institution of Great Britain, P23; MALONEY LT, 1985, THESIS STANFORD U; Marr D., 1980, P R SOC LONDON, V207, P187; STOKSETH PA, 1969, J OPT SOC AM, V59, P1314, DOI 10.1364/JOSA.59.001314; WANDELL BA, 1987, IEEE T PATTERN ANAL, V9, P2, DOI 10.1109/TPAMI.1987.4767868	18	12	12	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1989	3	2					109	117		10.1007/BF00126427	http://dx.doi.org/10.1007/BF00126427			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AH264					2022-12-18	WOS:A1989AH26400001
J	Zhang, J; Chen, Z; Tao, DC				Zhang, Jing; Chen, Zhe; Tao, Dacheng			Towards High Performance Human Keypoint Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human Pose Estimation; Deep Nerual Networks; Sub-pixel Refinement; Context		Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM's representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark.	[Zhang, Jing; Chen, Zhe; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia	University of Sydney	Tao, DC (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia.	zhe.chen1@sydney.edu.au; dacheng.tao@sydney.edu.au	Chen, Zhe/AAT-9716-2020	Chen, Zhe/0000-0001-5004-8975				Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542; Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Cai Y., 2020, LNCS, V12348, P455, DOI [10.1007/978-3-030-58580-827, DOI 10.1007/978-3-030-58580-827, DOI 10.1007/978-3-030-58580-8_27]; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742; Chen Z, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01370-7; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Felzenszwalb P, 2008, PROC CVPR IEEE, P1984; Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044; Hattori H, 2018, INT J COMPUT VISION, V126, P1027, DOI 10.1007/s11263-018-1077-3; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Holt B., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1196, DOI 10.1109/ICCVW.2011.6130386; Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang SL, 2017, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2017.329; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Li Wenbo, 2019, ARXIV190100148; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Ma B., 2020, ADV NEURAL INFORM PR, V33; Mazhar O., 2018, 2018 IEEERSJ INT C I, P1, DOI [10.1109/IROS.2018.8594385, DOI 10.1109/IROS.2018.8594385]; Newell A, 2017, ADV NEUR IN, V30; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Ni BB, 2018, IEEE T NEUR NET LEAR, V29, P3715, DOI 10.1109/TNNLS.2017.2731775; Ouyang WL, 2016, INT J COMPUT VISION, V120, P14, DOI 10.1007/s11263-016-0890-9; Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17; Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rhodin H, 2018, LECT NOTES COMPUT SC, V11214, P765, DOI 10.1007/978-3-030-01249-6_46; Rogez G, 2012, INT J COMPUT VISION, V99, P25, DOI 10.1007/s11263-012-0516-9; Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Varadarajan J, 2018, INT J COMPUT VISION, V126, P410, DOI 10.1007/s11263-017-1026-6; Wagemans J, 2012, PSYCHOL BULL, V138, P1172, DOI 10.1037/a0029333; Wang F, 2013, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2013.83; Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551; Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712; Zhang Han, 2019, ICLR; Zhang J, 2021, IEEE INTERNET THINGS, V8, P4628, DOI 10.1109/JIOT.2020.3026732; Zhang SH, 2019, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2019.00098	57	11	11	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2639	2662		10.1007/s11263-021-01482-8	http://dx.doi.org/10.1007/s11263-021-01482-8		JUL 2021	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN		Green Submitted			2022-12-18	WOS:000668844300001
J	Chen, YH; Wang, HR; Li, W; Sakaridis, C; Dai, DX; Van Gool, L				Chen, Yuhua; Wang, Haoran; Li, Wen; Sakaridis, Christos; Dai, Dengxin; Van Gool, Luc			Scale-Aware Domain Adaptive Faster R-CNN	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Domain adaptation; Scale-aware algorithm; Scene understanding	ADAPTATION; VISION	Object detection typically assumes that training and test samples are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch may lead to a significant performance drop. In this work, we present Scale-aware Domain Adaptive Faster R-CNN, a model aiming at improving the cross-domain robustness of object detection. In particular, our model improves the traditional Faster R-CNN model by tackling the domain shift on two levels: (1) the image-level shift, such as image style, illumination, etc., and (2) the instance-level shift, such as object appearance, size, etc. The two domain adaptation modules are implemented by learning domain classifiers in an adversarial training manner. Moreover, we observe that the large variance in object scales often brings a crucial challenge to cross-domain object detection. Thus, we improve our model by explicitly incorporating the object scale into adversarial training. We evaluate our proposed model on multiple cross-domain scenarios, including object detection in adverse weather, learning from synthetic data, and cross-camera adaptation, where the proposed model outperforms baselines and competing methods by a significant margin. The promising results demonstrate the effectiveness of our proposed model for cross-domain object detection. The implementation of our model is available at https://github.com/yuhuayc/sa-da-faster.	[Li, Wen] Univ Elect Sci & Technol China, Chengdu, Peoples R China; [Chen, Yuhua; Wang, Haoran; Sakaridis, Christos; Dai, Dengxin; Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland; [Van Gool, Luc] Katholieke Univ Leuven, Leuven, Belgium	University of Electronic Science & Technology of China; Swiss Federal Institutes of Technology Domain; ETH Zurich; KU Leuven	Li, W (corresponding author), Univ Elect Sci & Technol China, Chengdu, Peoples R China.	yuhua.yc@gmail.com; haowang@student.ethz.ch; liwen@uestc.edu.cn; csakarid@vision.ee.ethz.ch; dai@vision.ee.ethz.ch; vangool@vision.ee.ethz.ch		Wang, Haoran/0000-0001-6177-9706; Chen, Yuhua/0000-0002-1278-4960	Major Project for New Generation of AI [2018AAA0100400]; Armasuisse; Toyota TRACE	Major Project for New Generation of AI; Armasuisse; Toyota TRACE	This work is partially supported by the Major Project for New Generation of AI under Grant No. 2018AAA0100400. The authors would like to acknowledge support by Armasuisse and Toyota TRACE, and thank AWS for providing cloud credits.	[Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88; Cai Q., 2019, C COMP VIS PATT REC; Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542; Chellappa R., 2019, ARXIV PREPRINT ARXIV; Chen Y., 2016, COMPUTER VISION PATT; Chen YH, 2019, PROC CVPR IEEE, P1841, DOI 10.1109/CVPR.2019.00194; Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Dalal N., 2005 IEEE COMP SOC C; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P1667, DOI 10.1109/TPAMI.2011.265; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gool L. V, 2019, C COMP VIS PATT REC; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Haeusser P, 2017, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2017.301; Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006; He ZW, 2019, IEEE I CONF COMP VIS, P6667, DOI 10.1109/ICCV.2019.00677; Hoffman J., 2016, ARXIV PREPRINT ARXIV; Hoiem D., 2012, EUR C COMP VIS ECCV; Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Johnson-Roberson M., 2017, INT J SOC ROBOT; Khodabandeh M, 2019, IEEE I CONF COMP VIS, P480, DOI 10.1109/ICCV.2019.00057; Kim C, 2019, INT C COMP VIS ICCV; Kim J, 2017, INT C COMP VIS ICCV; Kim T, 2019, PROC CVPR IEEE, P12448, DOI 10.1109/CVPR.2019.01274; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Lempitsky V, 2015, INT C MACH LEARN ICM; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624; Li Y., 2016, ADV NEURAL INFORM PR; Lin T.Y., 2016, ARXIV PREPRINT ARXIV; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu MY, 2017, ADV NEUR IN, V30; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Long MS, 2015, PR MACH LEARN RES, V37, P97; LU H, 2017, IEEE I CONF COMP VIS, P599, DOI DOI 10.1109/ICCV.2017.72; Mathieu M., 2013, INT C LEARNING REPRE; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723; Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151; Raj A., 2015, ARXIV PREPRINT ARXIV; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; RoyChowdhury A, 2019, PROC CVPR IEEE, P780, DOI 10.1109/CVPR.2019.00087; Saenko K., 2015, ARXIV PREPRINT ARXIV; Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Savvides M., 2019, ARXIV PREPRINT ARXIV; Sener O., 2016, ADV NEURAL INFORM PR; Shan YH, 2019, NEUROCOMPUTING, V367, P31, DOI 10.1016/j.neucom.2019.08.022; Sun B., 2014, BRIT MACH VIS C BMVC; Tang K., 2012, ADV NEURAL INFORM PR, P638; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Tzeng Eric, 2018, ARXIV181200929; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Xu JL, 2014, IEEE T PATTERN ANAL, V36, P2367, DOI 10.1109/TPAMI.2014.2327973; Zhang L, 2019, P IEEE INT C COMP VI; Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28; Zhang Yu, 2017, SURVEY MULTITASK LEA; Zhao SS, 2019, PROC CVPR IEEE, P9780, DOI 10.1109/CVPR.2019.01002; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XG, 2019, PROC CVPR IEEE, P687, DOI 10.1109/CVPR.2019.00078	78	11	12	10	26	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2223	2243		10.1007/s11263-021-01447-x	http://dx.doi.org/10.1007/s11263-021-01447-x		MAY 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW					2022-12-18	WOS:000649252600001
J	Zhan, YB; Yu, J; Yu, T; Tao, DC				Zhan, Yibing; Yu, Jun; Yu, Ting; Tao, Dacheng			Multi-task Compositional Network for Visual Relationship Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual relationship detection; Object detection; Predicate detection; Significance detection; Multi-task		Previous methods treat visual relationship detection as a combination of object detection and predicate detection. However, natural images likely contain hundreds of objects and thousands of object pairs. Relying only on object detection and predicate detection is insufficient for effective visual relationship detection because the significant relationships are easily overwhelmed by the dominant less-significant relationships. In this paper, we propose a novel subtask for visual relationship detection, the significance detection, as the complement of object detection and predicate detection. Significance detection refers to the task of identifying object pairs with significant relationships. Meanwhile, we propose a novel multi-task compositional network (MCN) that simultaneously performs object detection, predicate detection, and significance detection. MCN consists of three modules, an object detector, a relationship generator, and a relationship predictor. The object detector detects objects. The relationship generator provides useful relationships, and the relationship predictor produces significance scores and predicts predicates. Furthermore, MCN proposes a multimodal feature fusion strategy based on visual, spatial, and label features and a novel correlated loss function to deeply combine object detection, predicate detection, and significance detection. MCN is validated on two datasets: visual relationship detection dataset and visual genome dataset. The experimental results compared with state-of-the-art methods verify the competitiveness of MCN and the usefulness of significance detection in visual relationship detection.	[Zhan, Yibing; Yu, Jun; Yu, Ting] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou, Peoples R China; [Yu, Ting] Zhejiang Univ Finance & Econ Dongfang Coll, Hangzhou, Peoples R China; [Tao, Dacheng] Univ Sydney, Fac Engn, UBTECH Sydney Artificial Intelligence Ctr, Sch Comp Sci, 6 Cleveland St, Darlington, NSW 2008, Australia	Hangzhou Dianzi University; Zhejiang University of Finance & Economics; University of Sydney	Yu, J (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou, Peoples R China.	zybjy@hdu.edu.cn; yujun@hdu.edu.cn; yuting@zufedfc.edu.cn; dacheng.tao@sydney.edu.au			National Key R&D Program of China [2018AAA0100603]; National Nature Science Foundation of China [61836002]; Australian Research Council [FL-170100117]	National Key R&D Program of China; National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); Australian Research Council(Australian Research Council)	This work was supported in part by the National Key R&D Program of China: Grant No. 2018AAA0100603, in part by the National Nature Science Foundation of China: Grant No. 61836002, and in part by the Australian Research Council Project: FL-170100117.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632; Chen Xinlei, 2017, ARXIV170202138; Desai C, 2012, LECT NOTES COMPUT SC, V7575, P158, DOI 10.1007/978-3-642-33765-9_12; du Plessis MC, 2015, PR MACH LEARN RES, V37, P1386; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Galleguillos C, 2008, PROC CVPR IEEE, P3552; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x; Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207; Han CJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P510, DOI 10.1145/3240508.3240611; Hsieh CJ, 2015, PR MACH LEARN RES, V37, P2445; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Hwang S. Jae, 2018, IEEE C COMP VIS PATT; Kaji H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2301; Kanehira A, 2016, PROC CVPR IEEE, P5138, DOI 10.1109/CVPR.2016.555; Kong Y, 2017, INT J COMPUT VISION, V123, P350, DOI 10.1007/s11263-016-0982-6; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Li XY, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON PUBLIC ADMINISTRATION (5TH), VOL I, P259; Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766; Li YB, 2017, INT CON DISTR COMP S, P1261, DOI 10.1109/ICDCS.2017.54; Liang KM, 2018, AAAI CONF ARTIF INTE, P7098; Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433; Ouyang WL, 2016, INT J COMPUT VISION, V120, P14, DOI 10.1007/s11263-016-0890-9; Palmero C, 2016, INT J COMPUT VISION, V118, P217, DOI 10.1007/s11263-016-0901-x; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Peyre J, 2017, IEEE I CONF COMP VIS, P5189, DOI 10.1109/ICCV.2017.554; Platanios EA, 2017, ADV NEUR IN, V30; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ruder Sebastian, 2017, ARXIV170605098; Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711; Sansone E., 2018, IEEE T PATTERN ANAL; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Yang X, 2018, LECT NOTES COMPUT SC, V11216, P38, DOI 10.1007/978-3-030-01258-8_3; Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42; Yin GJ, 2018, LECT NOTES COMPUT SC, V11207, P330, DOI 10.1007/978-3-030-01219-9_20; Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121; Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340; Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202; Zhan YB, 2019, PROC CVPR IEEE, P5123, DOI 10.1109/CVPR.2019.00527; Zhang HW, 2017, IEEE I CONF COMP VIS, P4243, DOI 10.1109/ICCV.2017.454; Zhang Hanwang, 2017, PROC CVPR IEEE, P5532, DOI [DOI 10.1109/CVPR.2017.331, DOI 10.1109/CVPR.2018.00611]; Zhang J, 2019, PROC CVPR IEEE, P11527, DOI 10.1109/CVPR.2019.01180; Zhang J, 2019, AAAI CONF ARTIF INTE, P9185; Zhang X, 2017, AAAI CONF ARTIF INTE, P2907; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhou Joey Tianyi, 2012, AS C MACH LEARN, V25, P555; Zhu YH, 2018, AAAI CONF ARTIF INTE, P7623; Zhuang BH, 2017, IEEE I CONF COMP VIS, P589, DOI 10.1109/ICCV.2017.71	54	11	11	20	67	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2146	2165		10.1007/s11263-020-01353-8	http://dx.doi.org/10.1007/s11263-020-01353-8		JUL 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG					2022-12-18	WOS:000555672000001
J	Ververas, E; Zafeiriou, S				Ververas, Evangelos; Zafeiriou, Stefanos			SliderGAN: Synthesizing Expressive Face Images by Sliding 3D Blendshape Parameters	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						GAN; Image translation; Facial expression synthesis; Speech synthesis; Blendshape models; Action units; 3DMM fitting; Relativistic discriminator; Emotionet; 4DFAB; LRW		Image-to-image (i2i) translation is the dense regression problem of learning how to transform an input image into an output using aligned image pairs. Remarkable progress has been made in i2i translation with the advent of deep convolutional neural networks and particular using the learning paradigm of generative adversarial networks (GANs). In the absence of paired images, i2i translation is tackled with one or multiple domain transformations (i.e., CycleGAN, StarGAN etc.). In this paper, we study the problem of image-to-image translation, under a set of continuous parameters that correspond to a model describing a physical process. In particular, we propose the SliderGAN which transforms an input face image into a new one according to the continuous values of a statistical blendshape model of facial motion. We show that it is possible to edit a facial image according to expression and speech blendshapes, using sliders that control the continuous values of the blendshape model. This provides much more flexibility in various tasks, including but not limited to face editing, expression transfer and face neutralisation, comparing to models based on discrete expressions or action units.	[Ververas, Evangelos; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, London SW7 2AZ, England	Imperial College London	Ververas, E (corresponding author), Imperial Coll London, Dept Comp, London SW7 2AZ, England.	e.ververas16@imperial.ac.uk; s.zafeiriou@imperial.ac.uk		Ververas, Evangelos/0000-0003-4345-1744	Teaching Fellowship of Imperial College London; EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans [EP/S010203/1]; EPSRC [EP/S010203/1] Funding Source: UKRI	Teaching Fellowship of Imperial College London; EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	E. Ververas was supported by the Teaching Fellowship of Imperial College London. S. Zafeiriou acknowledges funding from the EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans (EP/S010203/1). Additionally, we would like to thank the reviewers for their valuable comments that helped us to improve this paper.	Alami Mejjati Y, 2018, UNSUPERVISED ATTENTI, P3693; Amos Brandon, 2016, CMUCS16118; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Benitez-Quiroz CF, 2017, IEEE I CONF COMP VIS, P3990, DOI 10.1109/ICCV.2017.428; Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600; Benitez-Quiroz F, 2018, IEEE T PATTERN ANAL, V29, P1683; Booth J, 2018, IEEE T PATTERN ANAL, V40, P2638, DOI 10.1109/TPAMI.2018.2832138; Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580; Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598; Cheng S, 2018 IEEE C COMP PAT; Choi Y, IEEE C COMP VIS PATT; Chung J. S, P AS C COMP VIS; Deng Jiankang, 2018, ARXIV180107698; Ekman P., 2002, FACIAL ACTION CODING; Fried O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323028; Geng Z, 2019, 3D GUIDED FINE GRAIN; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Isola P, CVPR; Jolicoeur-Martineau Alexia, 2018, ARXIV180700734; Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283; Kingma D.P, P 3 INT C LEARNING R; Li M., 2016, ARXIV161005586 CORR; Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Neumann T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508417; Perarnau G, ARXIV161106355 CORR; Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50; Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589; Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640; Tewari A, 2017, ARXIV171202859, V2; Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035; Thies Justus, 2016, CVPR, DOI DOI 10.1109/CVPR.2016.262; Tzirakis P, ARXIV190407002; Usman B, IEEE INT C COMP VIS; Wang LW, 2005, IEEE T PATTERN ANAL, V27, P1334, DOI 10.1109/TPAMI.2005.165; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wiles O, 2018, P ECCV; Wiles O, 2018, EUR C COMP VIS; Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892; Zhu Jun-Yan, 2017, ICCV	42	11	11	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2629	2650		10.1007/s11263-020-01338-7	http://dx.doi.org/10.1007/s11263-020-01338-7		JUN 2020	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		hybrid, Green Submitted			2022-12-18	WOS:000539947500001
J	Zhang, XY; Dong, H; Hu, Z; Lai, WS; Wang, F; Yang, MH				Zhang, Xinyi; Dong, Hang; Hu, Zhe; Lai, Wei-Sheng; Wang, Fei; Yang, Ming-Hsuan			Gated Fusion Network for Degraded Image Super Resolution	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Super resolution; Image restoration; Deep learning		Single image super resolution aims to enhance image quality with respect to spatial content, which is a fundamental task in computer vision. In this work, we address the task of single frame super resolution with the presence of image degradation, e.g., blur, haze, or rain streaks. Due to the limitations of frame capturing and formation processes, image degradation is inevitable, and the artifacts would be exacerbated by super resolution methods. To address this problem, we propose a dual-branch convolutional neural network to extract base features and recovered features separately. The base features contain local and global information of the input image. On the other hand, the recovered features focus on the degraded regions and are used to remove the degradation. Those features are then fused through a recursive gate module to obtain sharp features for super resolution. By decomposing the feature extraction step into two task-independent streams, the dual-branch model can facilitate the training process by avoiding learning the mixed degradation all-in-one and thus enhance the final high-resolution prediction results. We evaluate the proposed method in three degradation scenarios. Experiments on these scenarios demonstrate that the proposed method performs more efficiently and favorably against the state-of-the-art approaches on benchmark datasets.	[Zhang, Xinyi] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Shaanxi, Peoples R China; [Dong, Hang; Wang, Fei] Xi An Jiao Tong Univ, Coll Artificial Intelligence, Xian 710049, Shaanxi, Peoples R China; [Hu, Zhe] Hikvis Res Amer, Santa Clara, CA USA; [Lai, Wei-Sheng; Yang, Ming-Hsuan] Univ Calif Merced, Elect Engn & Comp Sci, Merced, CA USA	Xi'an Jiaotong University; Xi'an Jiaotong University; University of California System; University of California Merced	Wang, F (corresponding author), Xi An Jiao Tong Univ, Coll Artificial Intelligence, Xian 710049, Shaanxi, Peoples R China.	jacqueline@stu.xjtu.edu.cn; dhunter@stu.xjtu.edu.cn; zhe.hu@hikvision.com; wlai24@ucmerced.edu; wfx@mail.xjtu.edu.cn; mhyang@ucmerced.edu	Hu, Zhe/AAE-7207-2021; Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304; Zhang, Xinyi/0000-0002-6577-7705				Agustsson Eirikur, 2017, IEEE C COMP VIS PATT; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Bai YC, 2018, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2018.00010; Bao W., 2017, IEEE INT C IM PROC; Bascle B., 1996, EUR C COMP VIS; Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185; Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Fan Z., 2017, ACM INT C MULT; Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671; Geiger A., 2012, P IEEE COMP SOC C CO; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Gong D, 2017, PROC CVPR IEEE, P3806, DOI 10.1109/CVPR.2017.405; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; He Kaiming, 2015, CVPR, DOI [10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hradi M., 2015, BRIT MACH VIS C; Hu Z, 2014, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2014.432; Huang Jia-Bin, 2015, CVPR, DOI DOI 10.1109/CVPR.2015.7299156; Jiang TX, 2017, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2017.301; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kim J, 2016, IEEE CONF COMPUT; Kim TH, 2013, IEEE I CONF COMP VIS, P3160, DOI 10.1109/ICCV.2013.392; Kingma D.P, P 3 INT C LEARNING R; Kohler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304; Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951; Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511; Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636; Li X, 2018, LECT NOTES COMPUT SC, V11206, P287, DOI [10.1007/978-3-030-01216-8_18, 10.1007/978-3-030-01267-0_22]; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Liu JY, 2018, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2018.00341; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Mei K., 2018, P AS C COMP VIS; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Nimisha TM, 2017, IEEE I CONF COMP VIS, P4762, DOI 10.1109/ICCV.2017.509; Noroozi M, 2017, LECT NOTES COMPUT SC, V10496, P65, DOI 10.1007/978-3-319-66709-6_6; Pan JS, 2016, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2016.306; Paramanand C, 2013, PROC CVPR IEEE, P1115, DOI 10.1109/CVPR.2013.148; Park H, 2017, IEEE I CONF COMP VIS, P4623, DOI 10.1109/ICCV.2017.494; Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343; Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10; Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361; Sajjadi Mehdi S. M., 2017, ICCV, DOI DOI 10.1109/CVPR.2019.00817; Schmidt U., 2013, IEEE C COMP VIS PATT; Schmidt U, 2011, PROC CVPR IEEE; Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8; Xu L., 2013, IEEE C COMP VIS PATT; Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36; Yamaguchi T., 2010, P AS C COMP VIS; Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183; Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101; Zhang H, 2017, ARXIV COMPUTER VISIO; Zhang HC, 2011, IEEE I CONF COMP VIS, P595, DOI [10.1109/ICCV.2011.6126293, 10.1109/APAP.2011.6180470]; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang LP, 2010, SIGNAL PROCESS, V90, P848, DOI 10.1016/j.sigpro.2009.09.002; Zhang X., 2018, IEEE INT C AC SPEECH; Zhang Yulun, 2018, P EUROPEAN C COMPUTE, P286; Zhao T, 2018, IEEE INT CONF COMP; Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423	75	11	13	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1699	1721		10.1007/s11263-019-01285-y	http://dx.doi.org/10.1007/s11263-019-01285-y			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN		Green Submitted			2022-12-18	WOS:000534910600008
J	Chen, XJ; Hong, WJ; Nie, FP; Huang, JZ; Shen, L				Chen, Xiaojun; Hong, Weijun; Nie, Feiping; Huang, Joshua Zhexue; Shen, Li			Enhanced Balanced Min Cut	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Clustering; Spectral clustering; Normalized cut	ALGORITHM	Spectral clustering is a hot topic and many spectral clustering algorithms have been proposed. These algorithms usually solve the discrete cluster indicator matrix by relaxing the original problems, obtaining the continuous solution and finally obtaining a discrete solution that is close to the continuous solution. However, such methods often result in a non-optimal solution to the original problem since the different steps solve different problems. In this paper, we propose a novel spectral clustering method, named as Enhanced Balanced Min Cut (EBMC). In the new method, a new normalized cut model is proposed, in which a set of balance parameters are learned to capture the differences among different clusters. An iterative method with proved convergence is used to effectively solve the new model without eigendecomposition. Theoretical analysis reveals the connection between EBMC and the classical normalized cut. Extensive experimental results show the effectiveness and efficiency of our approach in comparison with the state-of-the-art methods.	[Chen, Xiaojun; Hong, Weijun; Huang, Joshua Zhexue] Shenzhen Univ, Coll Comp Sci & Software, Shenzhen 518060, Peoples R China; [Nie, Feiping] Northwestern Polytech Univ, Comp Sci, Xian 710072, Shanxi, Peoples R China; [Nie, Feiping] Northwestern Polytech Univ, Ctr OPTIMAL, Xian 710072, Shanxi, Peoples R China; [Shen, Li] Tencent AI Lab, Shenzhen 518060, Peoples R China	Shenzhen University; Northwestern Polytechnical University; Northwestern Polytechnical University; Tencent	Chen, XJ (corresponding author), Shenzhen Univ, Coll Comp Sci & Software, Shenzhen 518060, Peoples R China.	xjchen@szu.edu.cn; 280996118@qq.com; feipingnie@gmail.com; zx.huang@szu.edu.cn; mathshenli@gmail.com			NSFC [61773268, 61502177]; Natural Science Foundation of SZU [000346]; Shenzhen Research Foundation for Basic Research, China [JCYJ20180305124149387]	NSFC(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of SZU; Shenzhen Research Foundation for Basic Research, China	This research was supported by NSFC under Grant Nos. 61773268, 61502177 and Natural Science Foundation of SZU (Grant No. 000346) and the Shenzhen Research Foundation for Basic Research, China (Nos. JCYJ20180305124149387).	Buhler T., 2009, P 26 ANN INT C MACH, P81, DOI DOI 10.1145/1553374.1553385; Chen XJ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1206, DOI 10.1145/3219819.3220039; Chen XJ, 2018, PATTERN RECOGN, V76, P404, DOI 10.1016/j.patcog.2017.10.026; Chen XJ, 2017, IEEE I CONF COMP VIS, P2080, DOI 10.1109/ICCV.2017.227; Chen XJ, 2013, IEEE T KNOWL DATA EN, V25, P932, DOI 10.1109/TKDE.2011.262; Chen XJ, 2012, PATTERN RECOGN, V45, P434, DOI 10.1016/j.patcog.2011.06.004; De Bie T, 2006, J MACH LEARN RES, V7, P1409; de Souto MCP, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-497; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; HAGEN L, 1992, IEEE T COMPUT AID D, V11, P1074, DOI 10.1109/43.159993; Huang L, 2013, 2013 1ST INTERNATIONAL FUTURE ENERGY ELECTRONICS CONFERENCE (IFEEC 2013), P431, DOI 10.1109/IFEEC.2013.6687544; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588; Lu CY, 2016, IEEE T IMAGE PROCESS, V25, P2833, DOI 10.1109/TIP.2016.2553459; Ng AY, 2002, ADV NEUR IN, V14, P849; Nie FP, 2016, AAAI CONF ARTIF INTE, P1969; Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726; Nie FP, 2011, IEEE T NEURAL NETWOR, V22, P1796, DOI 10.1109/TNN.2011.2162000; Sander J, 1998, DATA MIN KNOWL DISC, V2, P169, DOI 10.1023/A:1009745219419; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wang C, 2017, INT C PAR DISTRIB SY, P151, DOI 10.1109/ICPADS.2017.00031; Xiao Cai, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1977, DOI 10.1109/CVPR.2011.5995740; Yu SX, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P313, DOI 10.1109/iccv.2003.1238361	25	11	11	3	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2020	128	7					1982	1995		10.1007/s11263-020-01320-3	http://dx.doi.org/10.1007/s11263-020-01320-3		MAR 2020	14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MC3NG					2022-12-18	WOS:000521879400001
J	Lan, L; Wang, XC; Hua, G; Huang, TMS; Tao, DC				Lan, Long; Wang, Xinchao; Hua, Gang; Huang, Thomas S.; Tao, Dacheng			Semi-online Multi-people Tracking by Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-object tracking; Semi-online methods; Combinatory optimization; Deep learning	MULTITARGET TRACKING; MULTIOBJECT TRACKING; ENERGY MINIMIZATION; SELECTION	In this paper, we propose a novel semi-online approach to tracking multiple people. In contrast to conventional offline approaches that take the whole image sequence as input, our semi-online approach tracks people in a frame-by-frame manner by exploring the time, space and multi-camera relationship of detection hypotheses in the near future frames. We cast the multi-people tracking task as a re-identification problem, and explicitly account for objects' appearance changes and longer-term associations. We model our approach using a Multi-Label Markov Random Field, and introduce a fast alpha-expansion algorithm to solve it efficiently. To our best knowledge, this is the first semi-online approach achieved by re-identification. It yields very promising tracking results especially in challenging cases, such as scenarios of the crowded streets where pedestrians frequently occlude each other, scenes captured with moving cameras where objects may disappear and reappear randomly, and videos under changing illuminations wherein the appearances of objects are influenced.	[Lan, Long] Natl Univ Def Technol, Coll Comp, Inst Quantum Informat, Changsha 410073, Peoples R China; [Lan, Long] Natl Univ Def Technol, Coll Comp, State Key Lab High Performance Comp, Changsha 410073, Peoples R China; [Wang, Xinchao] Stevens Inst Technol, Dept Comp Sci, Hoboken, NJ 07030 USA; [Hua, Gang] Wormpex AI Res, Bellevue, WA USA; [Huang, Thomas S.] UIUC, Beckman Inst, Image Format & Proc Grp IFP, Urbana, IL USA; [Tao, Dacheng] Univ Sydney, Fac Engn, UBTECH Sydney Artificial Intelligence Ctr, 6 Cleveland St, Darlington, NSW 2008, Australia; [Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, 6 Cleveland St, Darlington, NSW 2008, Australia	National University of Defense Technology - China; National University of Defense Technology - China; Stevens Institute of Technology; University of Illinois System; University of Illinois Urbana-Champaign; University of Sydney; University of Sydney	Wang, XC (corresponding author), Stevens Inst Technol, Dept Comp Sci, Hoboken, NJ 07030 USA.	long.lan@nudt.edu.cn; xinchao.wang@stevens.edu; ganghua@gmail.com; t-huang1@illinois.edu; dacheng.tao@sydney.edu.au	Wang, Xinchao/L-7655-2018	Wang, Xinchao/0000-0003-0057-1404				Arora C, 2013, IEEE I CONF COMP VIS, P177, DOI 10.1109/ICCV.2013.29; Bae SH, 2018, IEEE T PATTERN ANAL, V40, P595, DOI 10.1109/TPAMI.2017.2691769; Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159; Benfold B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3457, DOI 10.1109/CVPR.2011.5995667; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103; Boros E, 2002, DISCRETE APPL MATH, V123, P1; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Breitenstein MD, 2011, IEEE T PATTERN ANAL, V33, P1820, DOI 10.1109/TPAMI.2010.232; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128; Chen JH, 2017, IEEE COMPUT SOC CONF, P2143, DOI 10.1109/CVPRW.2017.266; Chen L, 2018, IEEE INT CON MULTI; Chen S, 2014, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2014.148; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347; Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205; DEHGHAN A, 2015, PROC CVPR IEEE, P4091, DOI DOI 10.1109/CVPR.2015; Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z; Fagot-Bouquet L, 2016, LECT NOTES COMPUT SC, V9912, P774, DOI 10.1007/978-3-319-46484-8_47; Fan JL, 2013, IEEE T IMAGE PROCESS, V22, P549, DOI 10.1109/TIP.2012.2218827; Fan JL, 2012, IEEE T PATTERN ANAL, V34, P1633, DOI 10.1109/TPAMI.2011.257; Fleuret F, 2014, ADV COMPUT VIS PATT, P309, DOI 10.1007/978-1-4471-6296-4_15; FORTMANN TE, 1983, IEEE J OCEANIC ENG, V8, P173, DOI 10.1109/JOE.1983.1145560; Fu Y, 2019, AAAI CONF ARTIF INTE, P8295; Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI 10.1109/ICCV.2017.129; Geng M., 2016, ARXIV161105244; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Henschel R, 2019, IEEE COMPUT SOC CONF, P770, DOI 10.1109/CVPRW.2019.00105; Hofmann M, 2013, PROC CVPR IEEE, P3650, DOI 10.1109/CVPR.2013.468; Hu WM, 2012, IEEE T PATTERN ANAL, V34, P2420, DOI 10.1109/TPAMI.2012.42; Jerripothula KR, 2016, LECT NOTES COMPUT SC, V9911, P187, DOI 10.1007/978-3-319-46478-7_12; Keuper M, 2016, ARXIV160706317; Kuo CH, 2011, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2011.5995384; Kuo CH, 2010, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2010.5540148; LAN L, 2018, TIP; LAN L, 2016, IJCAI, P3396; Leal-Taixe L, 2012, PROC CVPR IEEE, P1987, DOI 10.1109/CVPR.2012.6247901; Lenz P, 2015, IEEE I CONF COMP VIS, P4364, DOI 10.1109/ICCV.2015.496; Levinkov E, 2017, PROC CVPR IEEE, P1904, DOI 10.1109/CVPR.2017.206; Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Liu JC, 2013, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2013.239; Liwicki S, 2015, IEEE T IMAGE PROCESS, V24, P2955, DOI 10.1109/TIP.2015.2428052; Liwicki S, 2012, IEEE T NEUR NET LEAR, V23, P1624, DOI 10.1109/TNNLS.2012.2208654; Maksai A, 2016, PROC CVPR IEEE, P972, DOI 10.1109/CVPR.2016.111; McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148; Milan A., 2016, MOT16 BENCHMARK MULT; Milan A, 2015, PROC CVPR IEEE, P5397, DOI 10.1109/CVPR.2015.7299178; Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103; Mori G, 2016, ARXIV160702568; Nillius P., 2006, COMP VIS PATT REC 20, V2, P2187, DOI [DOI 10.1109/CVPR.2006.198, 10.1109/CVPR.2006.198]; Pang Y, 2015, 2015 18TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1938; Possegger H, 2014, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2014.170; QIU J, 2020, TPAMI; Qiu JY, 2019, PROC CVPR IEEE, P8485, DOI 10.1109/CVPR.2019.00869; Ramanan D, 2007, IEEE T PATTERN ANAL, V29, P65, DOI 10.1109/TPAMI.2007.250600; Rezatofighi SH, 2015, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2015.349; ROBERTO H, 2017, ARXIV170508314; Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41; Sanchez-Matilla R, 2016, LECT NOTES COMPUT SC, V9914, P84, DOI 10.1007/978-3-319-48881-3_7; Shen CC, 2019, AAAI CONF ARTIF INTE, P3068; SHITRIT HB, 2014, IEEE T PATTERN ANAL, V36, P1614, DOI DOI 10.1109/TPAMI.2013.210; Shu G, 2012, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2012.6247879; Song J., 2019, ADV NEURAL INFORM PR, P6179; Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30; Sullivan J, 2006, LECT NOTES COMPUT SC, V3953, P619, DOI 10.1007/11744078_48; Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394; TANG SY, 2015, PROC CVPR IEEE, P5033; Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5; Turetken E, 2017, IEEE T MED IMAGING, V36, P942, DOI 10.1109/TMI.2016.2640859; Wang HW, 2009, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE AND ENGINEERING MANAGEMENT, P124; Wang J, 2019, IEEE I CONF COMP VIS, P4662, DOI 10.1109/ICCV.2019.00476; Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418; WANG X, 2017, TIP; Wang XC, 2014, LECT NOTES COMPUT SC, V8689, P17, DOI 10.1007/978-3-319-10590-1_2; Wang XC, 2014, COMPUT VIS IMAGE UND, V119, P102, DOI 10.1016/j.cviu.2013.11.010; Wang XC, 2011, IEEE T IMAGE PROCESS, V20, P2627, DOI 10.1109/TIP.2011.2114354; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Wen LY, 2017, INT J COMPUT VISION, V122, P313, DOI 10.1007/s11263-016-0943-0; Wu Z, 2016, COMPUT VIS IMAGE UND, V143, P25, DOI 10.1016/j.cviu.2015.10.006; Wu Z, 2012, PROC CVPR IEEE, P1948, DOI 10.1109/CVPR.2012.6247896; Yang M, 2007, PROC CVPR IEEE, P1590, DOI 10.1109/CVPR.2007.383178; Ye JW, 2019, PROC CVPR IEEE, P2824, DOI 10.1109/CVPR.2019.00294; Yin XQ, 2018, LECT NOTES COMPUT SC, V11214, P475, DOI 10.1007/978-3-030-01249-6_29; Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155; Yoon JH, 2015, IEEE WINT CONF APPL, P33, DOI 10.1109/WACV.2015.12; YU S, 2016, ARXIV160407468; Yu XY, 2017, PROC CVPR IEEE, P67, DOI 10.1109/CVPR.2017.15; Zhang J., 2017, ARXIV171207257; Zhang LJ, 2008, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD006286.pub2; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng WS, 2016, IEEE T PATTERN ANAL, V38, P591, DOI 10.1109/TPAMI.2015.2453984; Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23	114	11	11	4	26	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2020	128	7					1937	1955		10.1007/s11263-020-01314-1	http://dx.doi.org/10.1007/s11263-020-01314-1		MAR 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MC3NG					2022-12-18	WOS:000520662000001
J	Esteves, C; Allen-Blanchette, C; Makadia, A; Daniilidis, K				Esteves, Carlos; Allen-Blanchette, Christine; Makadia, Ameesh; Daniilidis, Kostas			Learning SO(3) Equivariant Representations with Spherical CNNs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	15th European Conference on Computer Vision (ECCV)	SEP 08-14, 2018	Munich, GERMANY			Equivariance; Sphere; Spherical CNN; 3D vision		We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard 3D shape retrieval and classification benchmarks.	[Esteves, Carlos; Allen-Blanchette, Christine; Daniilidis, Kostas] Univ Penn, Philadelphia, PA 19104 USA; [Makadia, Ameesh] Google Res, New York, NY USA	University of Pennsylvania; Google Incorporated	Esteves, C (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	machc@seas.upenn.edu	Esteves, Carlos/AAU-6517-2020	Esteves, Carlos/0000-0001-9413-1201; Daniilidis, Kostas/0000-0003-0498-0758				Arfken G, 1966, MATH METHODS PHYS, V2; Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J., 2013, ARXIV13013537; Bruna J, 2013, PROC INT C LEARN REP; Chang A. X., 2015, ARXIV151203012V1 COR; Cohen TS, 2016, PR MACH LEARN RES, V48; Cohen Taco S, 2018, ICLR; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Dieleman S, 2015, MON NOT R ASTRON SOC, V450, P1441, DOI 10.1093/mnras/stv632; DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008; Frome A., 2004, LECT NOTES COMPUTER, V3023; Furuya T., 2016, P BMVC; Gens R., 2014, NIPS; Healy DM, 2003, J FOURIER ANAL APPL, V9, P341, DOI 10.1007/s00041-003-0018-9; HelOr Y, 1996, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.1996.517165; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kanezaki Asako, 2018, P IEEE INT C COMP VI; Kazhdan M., 2002, ACM SIGGRAPH 2002 C, P191; Kipf TN, 2016, P INT C LEARN REPR; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Lebedev N. N., 1972, SPECIAL FUNCTIONS TH; Li J., 2018, ARXIV180304249V4 COR; Makadia A, 2010, INT J COMPUT VISION, V89, P193, DOI 10.1007/s11263-009-0280-7; Marcos D., 2016, ARXIV161209346 CORR; Masci J., 2015, P IEEE INT C COMP VI, P37; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Monti F., 2016, ARXIV161108402; Qi C. R., 2017, IEEE P COMPUT VIS PA, V1, P4, DOI DOI 10.1109/CVPR.2017.16; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Qi Charles R, 2017, ARXIV170602413; Rippel O., 2015, ARXIV150603767 CORR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Savva M, 2017, PROC EUROGR WORKSHOP, P39; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; SEGMAN J, 1992, IEEE T PATTERN ANAL, V14, P1171, DOI 10.1109/34.177382; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Tatsuma A, 2009, VISUAL COMPUT, V25, P785, DOI 10.1007/s00371-008-0304-2; Thurston W.P., 1997, PRINCETON MATH SERIE, V35; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Worrall D.E., 2016, CORR; Worrall DE, 2017, PROC CVPR IEEE, P7168, DOI 10.1109/CVPR.2017.758; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Yi L., 2016, ARXIV161200606; Zhang R, 2019, PR MACH LEARN RES, V97; Zhou YZ, 2017, PROC CVPR IEEE, P4961, DOI 10.1109/CVPR.2017.527	49	11	12	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		588	600		10.1007/s11263-019-01220-1	http://dx.doi.org/10.1007/s11263-019-01220-1			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	KU1MV		Green Submitted			2022-12-18	WOS:000519475600003
J	Shao, WQ; Li, JY; Ren, JM; Zhang, RM; Wang, XG; Luo, P				Shao, Wenqi; Li, Jingyu; Ren, Jiamin; Zhang, Ruimao; Wang, Xiaogang; Luo, Ping			SSN: Learning Sparse Switchable Normalization via SparsestMax	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Normalization; Classification; Optimization		Normalization method deals with parameters training of convolution neural networks (CNNs) in which there are often multiple convolution layers. Despite the fact that layers in CNN are not homogeneous in the role they play at representing a prediction function, existing works often employ identical normalizer in different layers, making performance away from idealism. To tackle this problem and further boost performance, a recently-proposed switchable normalization (SN) provides a new perspective for deep learning: it learns to select different normalizers for different convolution layers of a ConvNet. However, SN uses softmax function to learn importance ratios to combine normalizers, not only leading to redundant computations compared to a single normalizer but also making model less interpretable. This work addresses this issue by presenting sparse switchable normalization (SSN) where the importance ratios are constrained to be sparse. Unlike l(1) and l(0) regularizations that impose difficulties in tuning layer-wise regularization coefficients, we turn this sparse-constrained optimization problem into feed-forward computation by proposing SparsestMax, which is a sparse version of softmax. SSN has several appealing properties. (1) It inherits all benefits from SN such as applicability in various tasks and robustness to a wide range of batch sizes. (2) It is guaranteed to select only one normalizer for each normalization layer, avoiding redundant computations and improving interpretability of normalizer selection. (3) SSN can be transferred to various tasks in an end-to-end manner. Extensive experiments show that SSN outperforms its counterparts on various challenging benchmarks such as ImageNet, COCO, Cityscapes, ADE20K, Kinetics and MegaFace. Models and code are available at https://github.com/switchablenorms/Sparse_SwitchNorm.	[Shao, Wenqi; Zhang, Ruimao; Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China; [Li, Jingyu; Ren, Jiamin] SenseTime Res, Shenzhen, Guangdong, Peoples R China; [Luo, Ping] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China	Chinese University of Hong Kong; University of Hong Kong	Luo, P (corresponding author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.	weqish@link.cuhk.edu.hk; lijingyu@sensetime.com; renjiamin@sensetime.com; ruimao.zhang@cuhk.edu.hk; xgwang@ee.cuhk.edu.hk; pluo@cs.hku.hk	Luo, Ping/HGE-7623-2022; Li, Jingyu/GSD-1924-2022; Li, Jing/GYU-5036-2022; Luo, Ping/GPG-2707-2022	Luo, Ping/0000-0002-6685-7950; 	HKU Seed Funding for Basic Research; SenseTime's Donation for Basic Research; SenseTime Group Limited; General Research Fund through the Research Grants Council of HongKong [CUHK14202217, CUHK14203118, CUHK14207319]	HKU Seed Funding for Basic Research(University of Hong Kong); SenseTime's Donation for Basic Research; SenseTime Group Limited; General Research Fund through the Research Grants Council of HongKong	Thank Xinjiang Wang and Tianjian Meng for their helpful discussions. Ping Luo is partially supported by the HKU Seed Funding for Basic Research and SenseTime's Donation for Basic Research. This work was supported in part by SenseTime Group Limited and in part by the General Research Fund through the Research Grants Council of HongKong underGrants CUHK14202217, CUHK14203118, CUHK14207319.	Advani Madhu S., 2017, ARXIV171003667, Patent No. [1710.03667, 171003667]; [Anonymous], 2013, ARXIV13126098; Bartlett PL, 1999, ADV NEUR IN, V11, P190; BENTLEY JL, 1993, SOFTWARE PRACT EXPER, V23, P1249, DOI 10.1002/spe.4380231105; Bertsekas D. P, 2014, CONSTRAINED OPTIMIZA; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Condat L, 2016, MATH PROGRAM, V158, P575, DOI 10.1007/s10107-015-0946-6; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng Jiankang, 2018, ARXIV180107698; Dentinel Zarembaw, 2014, NEURIPS, P1269; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goyal Priya, 2017, ARXIV170602677; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Held M., 1974, Mathematical Programming, V6, P62, DOI 10.1007/BF01580223; Hinton, 2016, ARXIV PREPRINT ARXIV; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jang E., 2016, ARXIV; Kay W., 2017, ARXIV PREPRINT ARXIV; Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527; Laurent C, 2016, INT CONF ACOUST SPEE, P2657, DOI 10.1109/ICASSP.2016.7472159; Li Yanghao, 2016, ARXIV160304779; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Hanxiao, 2018, ARXIV180609055; Louizos C, 2017, ADV NEUR IN, V30; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Luo P., 2018, ARXIV180610779; Luo P, 2018, ARXIV PREPRINT ARXIV; Ma Ningning, 2018, P EUR C COMP VIS ECC; Maddison Chris J, 2016, ARXIV161100712; Malaviya C., 2018, ARXIV180508241; Martins A. F. T., 2016, ARXIV160202068 CORR; Martins FT, 2017, P 2017 C EMP METH NA, P349, DOI DOI 10.18653/V1/D17-1036; Pan X., 2018, P ECCV, P464; Paszke A., 2017, AUTOMATIC DIFFERENTI; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Raghu M, 2017, PR MACH LEARN RES, V70; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Santurkar S, 2018, ADV NEUR IN, V31; Scardapane S, 2017, NEUROCOMPUTING, V241, P81, DOI 10.1016/j.neucom.2017.02.029; Schoenholz S. S., 2019, ARXIV190208129; Shen YT, 2018, INT CONF CLOUD COMPU, P762, DOI 10.1109/CCIS.2018.8691323; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tai C., 4 INT C LEARN REPR I; Tartaglione E, 2018, ADV NEUR IN, V31; Teye M., 2018, P MACHINE LEARNING R, P4914; Ulyanov Dmitry, 2017, INSTANCE NORMALIZATI, P3; van den Berg E, 2008, SIAM J SCI COMPUT, V31, P890, DOI 10.1137/080714488; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wen W, 2016, ADV NEUR IN, V29; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xie Sirui, 2018, ARXIV181209926, P2; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang C., 2019, ARXIV190201996; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zheng YD, 2017, CHINESE PHYS B, V26, DOI 10.1088/1674-1056/26/7/070302; Zoph B., 2016, ARXIV161101578; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	69	11	11	2	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2107	2125		10.1007/s11263-019-01269-y	http://dx.doi.org/10.1007/s11263-019-01269-y		DEC 2019	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG		Green Submitted			2022-12-18	WOS:000505388800001
J	Lu, CW; Shi, JP; Wang, WM; Jia, JY				Lu, Cewu; Shi, Jianping; Wang, Weiming; Jia, Jiaya			Fast Abnormal Event Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Abnormal event; Realtime detection; Event detection; Video analysis	ANOMALY DETECTION; ALGORITHM	Fast abnormal event detection meets the growing demand to process an enormous number of surveillance videos. Based on the inherent redundancy of video structures, we propose an efficient sparse combination learning framework with both batch and online solvers. It achieves decent performance in the detection phase without compromising result quality. The extremely fast execution speed is guaranteed owing to the fact that our method effectively turns the original complicated problem into a few small-scale least square optimizations. Our method reaches high detection rates on benchmark datasets at a speed of 1000-1200 frames per second on average when computing on an ordinary single core desktop PC using MATLAB.	[Lu, Cewu] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China; [Shi, Jianping] Sensetime Grp Ltd, Beijing, Peoples R China; [Wang, Weiming] Shanghai Jiao Tong Univ, Sch Mech Engn, Shanghai, Peoples R China; [Jia, Jiaya] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China	Shanghai Jiao Tong University; Shanghai Jiao Tong University; Chinese University of Hong Kong	Lu, CW (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.; Wang, WM (corresponding author), Shanghai Jiao Tong Univ, Sch Mech Engn, Shanghai, Peoples R China.	lucewu06@hotmail.com; wangweiming@sjtu.edu.cn	Jia, Jiaya/I-3251-2012		National Science Foundation China [61772332,51675342,61133009]; Research Grants Council of the Hong Kong SAR [2150760]	National Science Foundation China(National Natural Science Foundation of China (NSFC)); Research Grants Council of the Hong Kong SAR(Hong Kong Research Grants Council)	This work is supported by the National Science Foundation China, under Grant 61772332,51675342,61133009 and by a Grant from the Research Grants Council of the Hong Kong SAR (Project No. 2150760).	Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825; Antic B, 2011, IEEE I CONF COMP VIS, P2415, DOI 10.1109/ICCV.2011.6126525; Basharat A, 2008, PROC CVPR IEEE, P1301; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Benezeth Y., 2009, IEEE C COMP VIS PATT; Bertsekas D, 1999, NONLINEAR PROGRAMMIN; Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524; CHENG KW, 2015, PROC CVPR IEEE, P2909; Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090; Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fu XW, 2007, DASC 2007: THIRD IEEE INTERNATIONAL SYMPOSIUM ON DEPENDABLE, AUTONOMIC AND SECURE COMPUTING, PROCEEDINGS, P145, DOI 10.1109/DASC.2007.18; Hastie T, 2009, ELEMENTS STAT LEARNI; Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569; Jager M, 2008, IEEE T IMAGE PROCESS, V17, P1700, DOI 10.1109/TIP.2008.2001043; Jiang F, 2008, INT CONF ACOUST SPEE, P2129; Jiang F, 2011, COMPUT VIS IMAGE UND, V115, P323, DOI 10.1016/j.cviu.2010.10.008; Kaltsa V, 2015, IEEE T IMAGE PROCESS, V24, P2153, DOI 10.1109/TIP.2015.2409559; Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771; Kwon J, 2012, PROC CVPR IEEE, P1266, DOI 10.1109/CVPR.2012.6247810; Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111; Lu CY, 2013, IEEE I CONF COMP VIS, P1345, DOI 10.1109/ICCV.2013.170; Lu CW, 2013, PROC CVPR IEEE, P415, DOI 10.1109/CVPR.2013.60; Ma Y, 2008, SIAM REV, V50, P413, DOI 10.1137/060655523; MAHADEVAN V, 2010, PROC CVPR IEEE, P1975, DOI DOI 10.1109/CVPR.2010.5539872; Mairal J, 2010, J MACH LEARN RES, V11, P19; Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641; Nowak R. D, 2007, IEEE J SELECTED TOPI; Osborne M. R., 2000, IMA J NUMERICAL ANAL, V20; Peng X, 2013, IEEE C COMP VIS PATT; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Pruteanu-Malinici I, 2008, IEEE T IMAGE PROCESS, V17, P811, DOI 10.1109/TIP.2008.919359; Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917; Shet V. D., 2006, EUR C COMP VIS ECCV; Shi JP, 2011, PROC CVPR IEEE, P1809, DOI 10.1109/CVPR.2011.5995592; Szabo Z., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2865, DOI 10.1109/CVPR.2011.5995712; Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244; Wang XQ, 2007, INT J THERM SCI, V46, P1, DOI 10.1016/j.ijthermalsci.2006.06.010; Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882; Xinyi Cui, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3161, DOI 10.1109/CVPR.2011.5995558; Xu Dan, 2015, ARXIV151001553; Yang JF, 2011, SIAM J SCI COMPUT, V33, P250, DOI 10.1137/090777761; Yinghuan Shi, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3653, DOI 10.1109/ICPR.2010.891; Zhang D., 2005, IEEE C COMP VIS PATT; Zhong H., 2004, IEEE C COMP VIS PATT	47	11	15	1	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					993	1011		10.1007/s11263-018-1129-8	http://dx.doi.org/10.1007/s11263-018-1129-8			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM					2022-12-18	WOS:000474559000002
J	Ikeuchi, K; Ma, ZY; Yan, ZQ; Kudoh, S; Nakamura, M				Ikeuchi, Katsushi; Ma, Zhaoyuan; Yan, Zengqiang; Kudoh, Shunsuke; Nakamura, Minako			Describing Upper-Body Motions Based on Labanotation for Learning-from-Observation Robots	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Humanoid robot; Programing-by-demonstration; Labanotation; Task model; Gesture recognition; Action recognition; Key pose	ACTION RECOGNITION	We have been developing a paradigm that we call learning-from-observation for a robot to automatically acquire a robot program to conduct a series of operations, or for a robot to understand what to do, through observing humans performing the same operations. Since a simple mimicking method to repeat exact joint angles or exact end-effector trajectories does not work well because of the kinematic and dynamic differences between a human and a robot, the proposed method employs intermediate symbolic representations, tasks, for conceptually representing what-to-do through observation. These tasks are subsequently mapped to appropriate robot operations depending on the robot hardware. In the present work, task models for upper-body operations of humanoid robots are presented, which are designed on the basis of Labanotation. Given a series of human operations, we first analyze the upper-body motions and extract certain fixed poses from key frames. These key poses are translated into tasks represented by Labanotation symbols. Then, a robot performs the operations corresponding to those task models. Because tasks based on Labanotation are independent of robot hardware, different robots can share the same observation module, and only different task-mapping modules specific to robot hardware are required. The system was implemented and demonstrated that three different robots can automatically mimic human upper-body operations with a satisfactory level of resemblance.	[Ikeuchi, Katsushi] Microsoft Corp, Redmond, WA 98052 USA; [Ma, Zhaoyuan] Worcester Polytech Inst, Worcester, MA 01609 USA; [Yan, Zengqiang] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China; [Kudoh, Shunsuke] Univ Electrocommun, Tokyo, Japan; [Nakamura, Minako] Ochanomizu Univ, Tokyo, Japan	Microsoft; Worcester Polytechnic Institute; Hong Kong University of Science & Technology; University of Electro-Communications - Japan; Ochanomizu University	Ikeuchi, K (corresponding author), Microsoft Corp, Redmond, WA 98052 USA.	katsuike@microsoft.com; zma3@wpi.edu; zyanad@connect.ust.hk; s-kudoh@uec.ac.jp; nakamura.minako@ocha.ac.jp			Microsoft Research Asia Core project	Microsoft Research Asia Core project(Microsoft)	A part of this work was supported by Microsoft Research Asia Core project 2016 and 2017. Yoshihiro Sato, Naohiro Hayashi, Masaaki Fukumoto, and Ambrosio Blanco helped us to set up the experimental environment including two home-made robots, RABOT1 and RABOT2. Discussions with David Baumert, Yutaka Suzue and John Lee were also valuable. The authors acknowledge and appreciate their help.	Aboshosha A, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2859; Ben Amor B, 2016, IEEE T PATTERN ANAL, V38, P1, DOI 10.1109/TPAMI.2015.2439257; Bobick AF, 1997, PHILOS T ROY SOC B, V352, P1257, DOI 10.1098/rstb.1997.0108; Cheng G, 2008, IEEE INT CONF ROBOT, P1772; Cheron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368; Choensawat W., 2010, T VIRTUAL REALITY SO, V15, P379; Dombre E, 2003, IEEE T ROBOTIC AUTOM, V19, P876, DOI 10.1109/TRA.2003.817067; Gams A, 2015, ROBOTICA, V33, P1049, DOI 10.1017/S0263574714001477; Huang L., 2003, YALEUDCSRR1253; Hutchinson Guest A., 2005, LABANOTATION SYSTEM; IKEUCHI K, 1994, IEEE T ROBOTIC AUTOM, V10, P368, DOI 10.1109/70.294211; IKEUCHI K, 1993, PROCEEDINGS : IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, pB688; Ikeuchi K., 1991, ANNU RES REV, P37; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Kagami S., 2000, 4 INT WORKSH ALG FDN; Kosuge K, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P3459; Kovashka A, 2010, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2010.5539881; Kuroki Y, 2003, IEEE INT CONF ROBOT, P471; LaViers A, 2012, P AMER CONTR CONF, P4327; Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557; MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158; Nakaoka S., 2004, P 10 INT C VIRT SYST, P1142; Nakaoka S, 2007, INT J ROBOT RES, V26, P829, DOI 10.1177/0278364907079430; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Pollard NS, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P1390, DOI 10.1109/ROBOT.2002.1014737; Rett J, 2007, INT C REHAB ROBOT, P257, DOI 10.1109/ICORR.2007.4428436; Riley M., 2000, AAAI CMU WORKSH INT; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Roy A, 2009, IEEE T ROBOT, V25, P569, DOI 10.1109/TRO.2009.2019783; Shiratori T, 2006, IEEE INT CONF ROBOT, P3654, DOI 10.1109/ROBOT.2006.1642260; Suehiro T., 1992, P INT C INT ROB SYST, V3, P2095; Takamatsu J, 2006, IEEE T ROBOT, V22, P65, DOI 10.1109/TRO.2005.855988; Tamiya Y., 1999, J ROBOTICS SOC JAPAN, V17, P268, DOI DOI 10.7210/jrsj.17.268; Treptow A, 2005, 2005 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P3610, DOI 10.1109/IROS.2005.1545530; Truong A, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0202-5; Vuga R, 2013, IEEE INT CONF ROBOT, P5284, DOI 10.1109/ICRA.2013.6631333; Wada K, 2007, IEEE T ROBOT, V23, P972, DOI 10.1109/TRO.2007.906261; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Witkin A., 1984, P IEEE INT C AC SPEE, V9, P150, DOI DOI 10.1109/ICASSP.1984.1172729; Yamane K, 2003, IEEE T ROBOTIC AUTOM, V19, P421, DOI 10.1109/TRA.2003.810579; Yoshii K, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P1749	43	11	11	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2018	126	12			SI		1415	1429		10.1007/s11263-018-1123-1	http://dx.doi.org/10.1007/s11263-018-1123-1			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GZ3KT		hybrid, Green Submitted			2022-12-18	WOS:000449286200010
J	Yang, CY; Liu, SF; Yang, MH				Yang, Chih-Yuan; Liu, Sifei; Yang, Ming-Hsuan			Hallucinating Compressed Face Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face hallucination; Super resolution; JPEG compression; Image denoising; Landmark points	SPARSE REPRESENTATION; BLOCKING ARTIFACTS; SUPERRESOLUTION; DEBLOCKING; REDUCTION; DCT	A face hallucination algorithm is proposed to generate high-resolution images from JPEG compressed low-resolution inputs by decomposing a deblocked face image into structural regions such as facial components and non-structural regions like the background. For structural regions, landmarks are used to retrieve adequate high-resolution component exemplars in a large dataset based on the estimated head pose and illumination condition. For non-structural regions, an efficient generic super resolution algorithm is applied to generate high-resolution counterparts. Two sets of gradient maps extracted from these two regions are combined to guide an optimization process of generating the hallucination image. Numerous experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art hallucination methods on JPEG compressed face images with different poses, expressions, and illumination conditions.	[Yang, Chih-Yuan; Liu, Sifei; Yang, Ming-Hsuan] Univ Calif Merced, Sch Engn, Elect Engn & Comp Sci, 5200 North Lake Rd, Merced, CA 95343 USA	University of California System; University of California Merced	Yang, MH (corresponding author), Univ Calif Merced, Sch Engn, Elect Engn & Comp Sci, 5200 North Lake Rd, Merced, CA 95343 USA.	cyang35@ucmerced.edu; sliu32@ucmerced.edu; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019; Yang, Ming-Hsuan/AAE-7350-2019; Liu, Sifei/AGE-1968-2022	Yang, Ming-Hsuan/0000-0003-4848-2304; Liu, Sifei/0000-0002-6011-3686	NSF CAREER [1149783]	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work is supported by NSF CAREER Grant 1149783, and gifts from Adobe and Nvidia.	Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210; Barnes C., 2010, P EUR C COMP VIS; Buades Antoni, 2005, CVPR; Choi I., 2013, P IEEE INT C COMP VI; Figueiredo M. A. T., 2006, P IEEE INT C IM PROC; Gross R., 2008, P IEEE C AUT FAC GES; JIA K, 2005, P IEEE INT C COMP VI; Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320; Kim K. I., 2008, EXAMPLE BASED LEARNI; Kumar N., 2009, P IEEE INT C COMP VI; Li Y., 2014, P EUR C COMP VIS; Liang Y, 2014, PATTERN RECOGN, V47, P3327, DOI 10.1016/j.patcog.2014.03.027; Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5; Liu SZ, 2002, IEEE T CIRC SYST VID, V12, P1139, DOI 10.1109/TCSVT.2002.806819; Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019; Mairal J., 2009, P IEEE INT C COMP VI; Park JS, 2008, IEEE T IMAGE PROCESS, V17, P1806, DOI 10.1109/TIP.2008.2001394; Singh S, 2007, DIGIT SIGNAL PROCESS, V17, P225, DOI 10.1016/j.dsp.2005.08.003; Tappen M. F., 2012, P EUR C COMP VIS, pFF249; Timofte Radu, 2014, P AS C COMP VIS; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Voska R, 2001, JPGQ JPEG QUALITY ES; Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9; Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xiong Xuehan, 2013, P IEEE C COMP VIS PA; Yang J., 2008, P IEEE C COMP PATT R; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yang M.-H., 2013, P IEEE C COMP VIS PA; Yang M. H., 2014, P IEEE INT C IM PROC; Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P735, DOI 10.1109/TMM.2008.922849; Zhu X., 2012, P IEEE C COMP VIS PA	34	11	11	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2018	126	6					597	614		10.1007/s11263-017-1044-4	http://dx.doi.org/10.1007/s11263-017-1044-4			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	GC3EO					2022-12-18	WOS:000429667300002
J	Tan, DJ; Tombari, F; Navab, N				Tan, David Joseph; Tombari, Federico; Navab, Nassir			Real-Time Accurate 3D Head Tracking and Pose Estimation with Consumer RGB-D Cameras	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3d temporal tracking; Head pose estimation; Real time; Random forests; Multi camera	REGISTRATION	We demonstrate how 3D head tracking and pose estimation can be effectively and efficiently achieved from noisy RGB-D sequences. Our proposal leverages on a random forest framework, designed to regress the 3D head pose at every frame in a temporal tracking manner. One peculiarity of the algorithm is that it exploits together (1) a generic training dataset of 3D head models, which is learned once offline; and, (2) an online refinement with subject-specific 3D data, which aims for the tracker to withstand slight facial deformations and to adapt its forest to the specific characteristics of an individual subject. The combination of these works allows our algorithm to be robust even under extreme poses, where the user's face is no longer visible on the image. Finally, we also propose another solution that utilizes a multi-camera system such that the data simultaneously acquired from multiple RGB-D sensors helps the tracker to handle challenging conditions that affect a subset of the cameras. Notably, the proposed multi-camera frameworks yields a real-time performance of approximately 8 ms per frame given six cameras and one CPU core, and scales up linearly to 30 fps with 25 cameras.	[Tan, David Joseph; Tombari, Federico; Navab, Nassir] Tech Univ Munich, Munich, Germany; Univ Bologna, Bologna, Italy	Technical University of Munich; University of Bologna	Tan, DJ (corresponding author), Tech Univ Munich, Munich, Germany.	tanda@in.tum.de; tombari@in.tum.de; navab@in.tum.de						Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240; Bar T, 2012, IEEE INT C INTELL TR, P1797, DOI 10.1109/ITSC.2012.6338678; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bradski G, 2000, DR DOBBS J, V25, P120; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breitenstein M. D., 2008, C COMP VIS PATT REC; Cai Q, 2010, LECT NOTES COMPUT SC, V6313, P229; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Dantone M., 2012, C COMP VIS PATT REC; Fanelli G., 2011, C COMP VIS PATT REC; Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0; Geng X., 2014, C COMP VIS PATT REC; Kan M., 2014, C COMP VIS PATT REC; Kazemi V., 2014, C COMP VIS PATT REC; Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968; Koterba S., 2005, INT C COMP VIS; Li S., 2015, IEEE T PATTERN ANAL; Martin Manuel, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P641, DOI 10.1109/3DV.2014.54; Meyer G. P., 2015, INT C COMP VIS; Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106; Padeleris P., 2012, 2012 IEEE COMP SOC C, P42, DOI DOI 10.1109/CVPRW.2012.6239236; Papazov C, 2015, PROC CVPR IEEE, P4722, DOI 10.1109/CVPR.2015.7299104; Redondo-Cabrera C., 2014, BRIT MACH VIS C; Rekik Ahmed, 2013, Proceedings of the 8th International Conference on Computer Vision Theory and Applications. VISAPP 2013, P223; Riegler G, 2014, BRIT MACH VIS C; SADOURNY R, 1968, MON WEATHER REV, V96, P351, DOI 10.1175/1520-0493(1968)096<0351:IOTNBV>2.0.CO;2; Schulter S., 2013, INT C COMP VIS; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tan D. J., 2015, INT C COMP VIS; Tan D. J., 2014, C COMP VIS PATT REC; Tan DJ, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P500, DOI 10.1109/3DV.2015.62; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Weiss TG, 2007, GLOB INST, P1	34	11	11	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		158	183		10.1007/s11263-017-0988-8	http://dx.doi.org/10.1007/s11263-017-0988-8			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA					2022-12-18	WOS:000425619100003
J	Jayaraman, D; Grauman, K				Jayaraman, Dinesh; Grauman, Kristen			Learning Image Representations Tied to Egomotion from Unlabeled Video	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	IEEE International Conference on Computer Vision (ICCV)	DEC 11-18, 2015	Santiago, CHILE	CPS, IEEE Comp Soc, Amazon, Microsoft, SENSETIME, Baidu, Intel, Facebook, Adobe, Panasonic, Google, OMRON, Blippar, iRobot, HISCENE, NVIDIA, Viscovery, AiCUre, M Tec, Inst Elect & Elect Engineers, Comp Vis Fdn				Understanding how images of objects and scenes behave in response to specific egomotions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose a new "embodied" visual learning paradigm, exploiting proprioceptive motor signals to train visual representations from egocentric video with no manual supervision. Specifically, we enforce that our learned features exhibit equivariance i.e., they respond predictably to transformations associated with distinct egomotions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.	[Jayaraman, Dinesh; Grauman, Kristen] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Jayaraman, D (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	dineshj@cs.utexas.edu; grauman@cs.utexas.edu	Jayaraman, Dinesh/AAI-2527-2021	Jayaraman, Dinesh/0000-0002-6888-3095	ONR PECASE Award [N00014-15-1-2291]; Texas Advanced Computing Center	ONR PECASE Award; Texas Advanced Computing Center	This research is supported in part by ONR PECASE Award N00014-15-1-2291. We also thank Texas Advanced Computing Center for their generous support, Pulkit Agrawal for sharing models and code and for helpful discussions, Ruohan Gao for helpful discussions, and our anonymous reviewers for constructive suggestions.	Agrawal Pulkit, 2015, CVPR, V2; [Anonymous], 2015, NIPS; [Anonymous], 2011, ICANN; Bojarski M., 2016, ARXIV160407316; Bromley J., 1993, IJPRAI; Cadieu CF, 2012, NEURAL COMPUT, V24, P827, DOI 10.1162/NECO_a_00247; Chen C., 2013, CVPR; Cohen T., 2015, ICLR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J., 2009, CVPR; Dosovitskiy A., 2014, NIPS; Gao R., 2016, ACCV; Geiger A., 2013, IJRR; Geiger A., 2012, CVPR; Glorot X., 2010, P 13 INT C ART INT S, VVolume 9, P249; Goroshin R., 2015, ICCV; Grauman K., 2016, CVPR; Hadsell R., 2006, P CVPR; HELD R, 1963, J COMP PHYSIOL PSYCH, V56, P872, DOI 10.1037/h0040546; Ioffe S., 2015, P 32 INT C INT C MAC, V37, P448; Jayaraman D., 2016, ECCV; Jia Y., 2014, P 22 INT C MULT ACM, P675; Kivinen J. J., 2011, ICANN; Kornhauser C. C. A. S. A., 2015, ICCV; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kulkarni T. D., 2015, NIPS; LeCun Y., 2004, CVPR; Levine S., 2015, ARXIV150400702; Li Y., 2013, ICCV; Lies JP, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003468; Lowe D. G., 1999, ICCV; Memisevic R., 2013, PAMI; Michalski V., 2014, NIPS; Mobahi H., 2009, ICML; Nakamura T., 1995, IJCAI; Ranzato MarcAurelio, 2014, VIDEO LANGUAGE MODEL; Ren Xiaofeng, 2010, CVPR; Schmidt U., 2012, CVPR; Simard P. Y., 2003, ICDAR; Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239; Sohn K., 2012, 29 INT C MACH LEARN, P1311; Tulsiani S., 2015, ICCV; Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017; Vedaldi A., 2015, CVPR; Vincent Pascal., 2008, ICML; Wang X., 2015, CVPR; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938; Wu Z., 2015, CVPR, V1, P2; Xiao J., 2010, CVPR; Xu C., 2012, ECCV; Yamada K., 2012, PSIVT; Zhang Ning, 2014, ICML; Zou W., 2012, P NIPS, P1	54	11	12	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2017	125	1-3			SI		136	161		10.1007/s11263-017-1001-2	http://dx.doi.org/10.1007/s11263-017-1001-2			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	FL2TO					2022-12-18	WOS:000414072800010
J	Ummenhofer, B; Brox, T				Ummenhofer, Benjamin; Brox, Thomas			Global, Dense Multiscale Reconstruction for a Billion Points	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	IEEE International Conference on Computer Vision (ICCV)	DEC 11-18, 2015	Santiago, CHILE	CPS, IEEE Comp Soc, Amazon, Microsoft, SENSETIME, Baidu, Intel, Facebook, Adobe, Panasonic, Google, OMRON, Blippar, iRobot, HISCENE, NVIDIA, Viscovery, AiCUre, M Tec, Inst Elect & Elect Engineers, Comp Vis Fdn		3D reconstruction; Large scale; Finite element method; Adaptive grid; Depth map fusion	GENERATION	We present a variational approach for surface reconstruction from a set of oriented points with scale information. We focus particularly on scenarios with nonuniform point densities due to images taken from different distances. In contrast to previous methods, we integrate the scale information in the objective and globally optimize the signed distance function of the surface on a balanced octree grid. We use a finite element discretization on the dual structure of the octree minimizing the number of variables. The tetrahedral mesh is generated efficiently with a lookup table which allows to map octree cells to the nodes of the finite elements. We optimize memory efficiency by data aggregation, such that robust data terms can be used even on very large scenes. The surface normals are explicitly optimized and used for surface extraction to improve the reconstruction at edges and corners.	[Ummenhofer, Benjamin; Brox, Thomas] Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany	University of Freiburg	Ummenhofer, B (corresponding author), Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany.	ummenhof@cs.uni-freiburg.de; brox@cs.uni-freiburg.de			ERC; EU	ERC(European Research Council (ERC)European Commission); EU(European Commission)	We acknowledge funding by the ERC Starting Grant VideoLearn and the EU project Trimbot2020.	[Anonymous], 2014, P EUR WORKSH GRAPH C; [Anonymous], 2008, P INT S 3D DAT PROC; Bailer C, 2012, LECT NOTES COMPUT SC, V7574, P398, DOI 10.1007/978-3-642-33712-3_29; BLACKER TD, 1993, ENG COMPUT, V9, P83, DOI 10.1007/BF01199047; BOLITHO M., 2007, P 5 EUR S GEOM PROC, P69; Calakli F, 2011, COMPUT GRAPH FORUM, V30, P1993, DOI 10.1111/j.1467-8659.2011.02058.x; Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3057, DOI 10.1109/CVPR.2011.5995552; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Estellers Virginia, 2015, Scale Space and Variational Methods in Computer Vision. 5th International Conference, SSVM 2015. Proceedings: LNCS 9087, P525, DOI 10.1007/978-3-319-18461-6_42; Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27; Fuhrmann S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601163; Fuhrmann S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024182; GARGANTINI I, 1982, COMPUT VISION GRAPH, V20, P365, DOI 10.1016/0146-664X(82)90058-2; Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933; Hiep VH, 2009, PROC CVPR IEEE, P1430, DOI 10.1109/CVPRW.2009.5206617; Hirschmuller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56; Jancosek M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3121, DOI 10.1109/CVPR.2011.5995693; Ju T, 2002, ACM T GRAPHIC, V21, P339; Kazhdan M., 2007, P 5 EUR S GEOM PROC, P125; Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237; Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265; Labatut P, 2009, COMPUT GRAPH FORUM, V28, P2275, DOI 10.1111/j.1467-8659.2009.01530.x; Lewiner T, 2010, COMPUT GRAPH FORUM, V29, P1661, DOI 10.1111/j.1467-8659.2010.01775.x; Li X, 2014, EFFECT MILD MOXIBUST, P1; Manson J, 2008, COMPUT GRAPH FORUM, V27, P1411, DOI 10.1111/j.1467-8659.2008.01281.x; Marechal L, 2009, PROCEEDINGS OF THE 18TH INTERNATIONAL MESHING ROUNDTABLE, P65, DOI 10.1007/978-3-642-04319-2_5; MAVRIPLIS DJ, 1995, J COMPUT PHYS, V117, P90, DOI 10.1006/jcph.1995.1047; Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154; Pock T, 2011, LECT NOTES COMPUT SC, V6570, P245, DOI 10.1007/978-3-642-19391-0_18; RUPPERT J, 1992, DISCRETE COMPUT GEOM, V7, P227, DOI 10.1007/BF02187840; Sagawa R, 2005, IEEE T PATTERN ANAL, V27, P392, DOI 10.1109/TPAMI.2005.46; Schaefer S, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P70, DOI 10.1109/PCCGA.2004.1348336; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Shewchuk J. R., 1998, Proceedings of the Fourteenth Annual Symposium on Computational Geometry, P86, DOI 10.1145/276884.276894; Ummenhofer B, 2015, IEEE I CONF COMP VIS, P1341, DOI 10.1109/ICCV.2015.158; Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5; Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25	37	11	11	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2017	125	1-3			SI		82	94		10.1007/s11263-017-1017-7	http://dx.doi.org/10.1007/s11263-017-1017-7			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	FL2TO		Green Submitted			2022-12-18	WOS:000414072800007
J	Ghodrati, A; Diba, A; Pedersoli, M; Tuytelaars, T; Van Gool, L				Ghodrati, Amir; Diba, Ali; Pedersoli, Marco; Tuytelaars, Tinne; Van Gool, Luc			DeepProposals: Hunting Objects and Actions by Cascading Deep Convolutional Layers	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object Proposals; Action proposals; Object detection; Action localization		In this paper, a new method for generating object and action proposals in images and videos is proposed. It builds on activations of different convolutional layers of a pretrained CNN, combining the localization accuracy of the early layers with the high informativeness (and hence recall) of the later layers. To this end, we build an inverse cascade that, going backward from the later to the earlier convolutional layers of the CNN, selects the most promising locations and refines them in a coarse-to-fine manner. The method is efficient, because (i) it re-uses the same features extracted for detection, (ii) it aggregates features using integral images, and (iii) it avoids a dense evaluation of the proposals thanks to the use of the inverse coarse-to-fine cascade. The method is also accurate. We show that DeepProposals outperform most of the previous object proposal and action proposal approaches and, when plugged into a CNN-based object detector, produce state-of-the-art detection performance.	[Ghodrati, Amir] Univ Amsterdam, QUVA Lab, Amsterdam, Netherlands; [Diba, Ali; Tuytelaars, Tinne; Van Gool, Luc] Katholieke Univ Leuven, IMinds, ESAT PSI, Leuven, Belgium; [Pedersoli, Marco] Ecole Technol Super, Montreal, PQ, Canada	University of Amsterdam; IMEC; KU Leuven; University of Quebec; Ecole de Technologie Superieure - Canada	Ghodrati, A (corresponding author), Univ Amsterdam, QUVA Lab, Amsterdam, Netherlands.	a.ghodrati@uva.nl	Tuytelaars, Tinne/B-4319-2015	Tuytelaars, Tinne/0000-0003-3307-9723	DBOF PhD scholarship; KU Leuven CAMETRON Project; FWO Project "Monitoring of Abnormal Activity with Camera Systems"	DBOF PhD scholarship; KU Leuven CAMETRON Project; FWO Project "Monitoring of Abnormal Activity with Camera Systems"(FWO)	This work was supported by a DBOF PhD scholarship, the KU Leuven CAMETRON Project and the FWO Project "Monitoring of Abnormal Activity with Camera Systems".	Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bergh M., 2013, ICCV; Bilen H, 2015, PROC CVPR IEEE, P1081, DOI 10.1109/CVPR.2015.7298711; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414; Cinbis R. G., 2013, ICCV; Deselaers T, 2010, LECT NOTES COMPUT SC, V6314, P452, DOI 10.1007/978-3-642-15561-1_33; Dollar P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231; Tran D, 2014, IEEE T PATTERN ANAL, V36, P404, DOI 10.1109/TPAMI.2013.137; Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Ghodrati A, 2015, IEEE I CONF COMP VIS, P2578, DOI 10.1109/ICCV.2015.296; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; Hariharan B., 2014, ARXIV14115752; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908; Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100; Krahenbuhl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lan T., 2011, ICCV; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Manen S., 2013, ICCV; Oneata D., 2014, ECCV; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Pinheiro Pedro O., 2015, ADV NEURAL INFORM PR, V3, P5; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Song HO, 2014, PR MACH LEARN RES, V32, P1611; Soomro K., 2012, ARXIV; van de Sande Koen E. A., 2011, ICCV; vanGemert J., 2015, BMVC; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang X., 2013, ICCV; Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362; Xinggang W., 2015, CVPR; Yu G, 2015, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2015.7298735; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhao Q., 2014, BMVC; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	49	11	13	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	2					115	131		10.1007/s11263-017-1006-x	http://dx.doi.org/10.1007/s11263-017-1006-x			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FC3PK		Green Published, Green Submitted			2022-12-18	WOS:000406751100001
J	Varano, V; Gabriele, S; Teresi, L; Dryden, IL; Puddu, PE; Torromeo, C; Piras, P				Varano, Valerio; Gabriele, Stefano; Teresi, Luciano; Dryden, Ian L.; Puddu, Paolo E.; Torromeo, Concetta; Piras, Paolo			The TPS Direct Transport: A New Method for Transporting Deformations in the Size-and-Shape Space	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Geometric Morphometrics; Shape analysis; Inter-individual difference; Riemannian manifold; Deformation cycle; Parallel transport; Trajectory analysis; Thin plate spline	DIFFEOMORPHOMETRY; SPLINES; SYSTEMS	Modern shape analysis allows the fine comparison of shape changes occurring between different objects. Very often the classic machineries of generalized Procrustes analysis and principal component analysis are used in order to contrast the shape change occurring among configurations represented by homologous landmarks. However, if size and shape data are structured in different groups thus constituting different morphological trajectories, a data centering is needed if onewants to compare solely the deformation representing the trajectories. To do that, inter-individual variation must be filtered out. This maneuver is rarely applied in studies using simulated or real data. A geometrical procedure named parallel transport, that can be based on various connection types, is necessary to perform such kind of data centering. Usually, the Levi Civita connection is used for interpolation of curves in a Riemannian space. It can also be used to transport a deformation. We demonstrate that this procedure does not preserve some important characters of the deformation, even in the affine case. We propose a novel procedure called 'TPS Direct Transport' which is able to perfectly transport deformation in the affine case and to better approximate non affine deformation in comparison to existing tools. We recommend to center shape data using the methods described here when the differences in deformation rather than in shape are under study.	[Varano, Valerio; Gabriele, Stefano] Roma Tre Univ, Dept Architecture, Rome, Italy; [Teresi, Luciano] Roma Tre Univ, Dept Math & Phys, Rome, Italy; [Dryden, Ian L.] Univ Nottingham, Sch Math Sci, Nottingham, England; [Puddu, Paolo E.; Torromeo, Concetta; Piras, Paolo] Sapienza Univ, Dept Cardiovasc Resp Nephrol & Geriatr Sci, Rome, Italy; [Piras, Paolo] Sapienza Univ, Dept Struct Engn & Geotech, Rome, Italy	Roma Tre University; Roma Tre University; University of Nottingham; Sapienza University Rome; Sapienza University Rome	Varano, V (corresponding author), Roma Tre Univ, Dept Architecture, Rome, Italy.	valerio.varano@uniroma3.it	Gabriele, Stefano/J-1091-2019; Gabriele, Stefano/G-5280-2010; Dryden, Ian/C-8742-2017	Gabriele, Stefano/0000-0001-8264-2792; Gabriele, Stefano/0000-0001-8264-2792; Piras, Paolo/0000-0001-9832-2742; Varano, Valerio/0000-0003-0298-6398; Dryden, Ian/0000-0003-4900-3571; TORROMEO, Concetta/0000-0003-0936-2780	National Group of Mathematical Physics (GNFM-INdAM), Italy	National Group of Mathematical Physics (GNFM-INdAM), Italy	We are grateful to Lena R. Zastrow and Antonio DiCarlo for hints and advices; their helpful discussions stimulated us to go far beyond our initial intuitions. The authors wish also to express their gratitude to Willem Gorissen, Clinical Market Manager Cardiac Ultrasound at Toshiba Medical Systems Europe, Zoetermeer, The Netherland, for his continuous support and help. We thank Antonio Profico for his unvaluable support in building deformetrics R package. We also thank three anonymous reviewers for their help in improving the manuscript. L.T., V.V. and S.G acknowledge the National Group of Mathematical Physics (GNFM-INdAM), Italy, for support.	Bookstein F. L., 1997, P 1997 INT C SHAP MO; Bookstein F. L., 1991, MORPHOMETRIC TOOLS L; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Boyer DM, 2011, P NATL ACAD SCI USA, V108, P18221, DOI 10.1073/pnas.1112822108; Charlier B, 2017, FOUND COMPUT MATH, V17, P287, DOI 10.1007/s10208-015-9288-2; Collyer ML, 2013, HYSTRIX, V24, P75, DOI 10.4404/hystrix-24.1-6298; Cootes TF, 2008, IMAGE VISION COMPUT, V26, P326, DOI 10.1016/j.imavis.2006.12.005; Crampin M., 1986, APPLICABLE DIFFERENT; Dryden I.L., 1998, STAT SHAPE ANAL, DOI [DOI 10.5555/1046920.1088707, 10.1002/9781119072492]; Duchateau Nicolas, 2012, Spatio-temporal Image Analysis for Longitudinal and Time-Series Image Data. Proceedings Second International Workshop, STIA 2012. Held in Conjunction with MICCAI 2012, P25, DOI 10.1007/978-3-642-33555-6_3; Erikson AP, 2012, SPRINGER PROC MATH, V6, P93, DOI 10.1007/978-3-642-20236-0_5; Fiot Jean-Baptiste, 2012, Spatio-temporal Image Analysis for Longitudinal and Time-Series Image Data. Proceedings Second International Workshop, STIA 2012. Held in Conjunction with MICCAI 2012, P13, DOI 10.1007/978-3-642-33555-6_2; Glaunes J., 2005, THESIS; Huckemann S, 2010, IEEE T PATTERN ANAL, V32, P593, DOI 10.1109/TPAMI.2009.117; Kendall D.G., 1999, SHAPE SHAPE THEORY; KENDALL DG, 1977, ADV APPL PROBAB, V9, P428, DOI 10.2307/1426091; Klingenberg W., 1982, AKSUTIKA; Kume A, 2007, BIOMETRIKA, V94, P513, DOI 10.1093/biomet/asm047; Le H, 2000, J MICROSC-OXFORD, V200, P140, DOI 10.1046/j.1365-2818.2000.00744.x; Le HL, 2003, J LOND MATH SOC, V68, P511, DOI 10.1112/S0024610703004393; Lorenzi M, 2014, J MATH IMAGING VIS, V50, P5, DOI 10.1007/s10851-013-0470-3; Lorenzi M, 2013, INT J COMPUT VISION, V105, P111, DOI 10.1007/s11263-012-0598-4; Lorenzi M, 2011, LECT NOTES COMPUT SC, V6892, P663, DOI 10.1007/978-3-642-23629-7_81; Marle C.-M., 2007, BANACH CTR PUBLICATI, V76; Marsland S., 2015, ARXIV151103355; Miller M. I., 2013, Patent n. US, pB2, Patent No. 8600131; Miller MI, 2001, INT J COMPUT VISION, V41, P61, DOI 10.1023/A:1011161132514; Miller MI, 2015, ANNU REV BIOMED ENG, V17, P447, DOI 10.1146/annurev-bioeng-071114-040601; Miller MI, 2014, TECHNOLOGY, V2, P36, DOI 10.1142/S2339547814500010; Miller MI, 2009, NEUROIMAGE, V45, pS16, DOI 10.1016/j.neuroimage.2008.10.044; Niethammer M., 2013, P 4 MICCAI WORKSH MA, P1; Pennec X., 2011, C GRETSI 11 BORD FRA; Peter AM, 2009, IEEE T PATTERN ANAL, V31, P337, DOI 10.1109/TPAMI.2008.69; Piras P., SCI REPORTS; Piras P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086896; Pokrass J, 2013, NUMER MATH-THEORY ME, V6, P223, DOI 10.4208/nmtma.2013.mssvm12; Qiu AQ, 2009, NEUROIMAGE, V45, pS51, DOI 10.1016/j.neuroimage.2008.10.039; Rohlf FJ, 2003, SYST BIOL, V52, P66, DOI 10.1080/10635150390132759; Schouten J.A., 1954, RICCI CALCULUS; Spivak M., 1999, COMPREHENSIVE INTRO, VII; Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184; Sundaramoorthi G, 2011, SIAM J IMAGING SCI, V4, P109, DOI 10.1137/090781139; Tang XY, 2015, HUM BRAIN MAPP, V36, P2093, DOI 10.1002/hbm.22758; Trouve A, 2005, FOUND COMPUT MATH, V5, P173, DOI 10.1007/s10208-004-0128-z; Trouve A., 1995, TECHNICAL REPORT; Twining Carole, 2011, Energy Minimization Methods in Computer Vision and Pattern Recognition. Proceedings 8th International Conference, EMMCVPR 2011, P399, DOI 10.1007/978-3-642-23094-3_29; Vaillant M, 2005, LECT NOTES COMPUT SC, V3565, P381; Varano V, 2015, BIOMAT, P351; Xie Q., 2013, IEEE INT C COMP VIS; Yezzi AJ, 2003, INT J COMPUT VISION, V53, P153, DOI 10.1023/A:1023048024042; Younes L, 2008, J MATH IMAGING VIS, V32, P41, DOI 10.1007/s10851-008-0074-5; Younes L, 2007, Q APPL MATH, V65, P113, DOI 10.1090/S0033-569X-07-01027-5	52	11	11	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	3					384	408		10.1007/s11263-017-1031-9	http://dx.doi.org/10.1007/s11263-017-1031-9			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FE1ER		Green Accepted			2022-12-18	WOS:000407961700007
J	Chen, DP; Yuan, ZJ; Wang, JD; Chen, BD; Hua, G; Zheng, NN				Chen, Dapeng; Yuan, Zejian; Wang, Jingdong; Chen, Badong; Hua, Gang; Zheng, Nanning			Exemplar-Guided Similarity Learning on Polynomial Kernel Feature Map for Person Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Explicit polynomial kernel feature map; Exemplar-guided similarity function; Multiple visual cues; Similarity learning; Person re; identification		Person re-identification is a crucial problem for video surveillance, aiming to discover the correct matches for a probe person image from a set of gallery person images. To directly describe the image pair, we present a novel organization of polynomial kernel feature map in a high dimensional feature space to break down the variability of positive person pairs. An exemplar-guided similarity function is built on the map, which consists of multiple sub-functions. Each sub-function is associated with an "exemplar" image being responsible for a particular type of image pair, thus excels at separating the persons with similar appearance. We formulate a unified learning problem including a relaxed loss term as well as two kinds of regularization strategies particularly designed for the feature map. The corresponding optimization algorithm jointly optimizes the coefficients of all the sub-functions and selects the proper exemplars for a better discrimination. The proposed method is extensively evaluated on six public datasets, where we thoroughly analyze the contribution of each component and verify the generalizability of our approach by cross-dataset experiments. Results show that the new method can achieve consistent improvements over state-of-the-art methods.	[Chen, Dapeng; Yuan, Zejian; Chen, Badong; Zheng, Nanning] Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China; [Wang, Jingdong; Hua, Gang] Microsoft Res Asia, Beijing, Peoples R China	Xi'an Jiaotong University; Microsoft; Microsoft Research Asia	Yuan, ZJ (corresponding author), Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China.; Wang, JD (corresponding author), Microsoft Res Asia, Beijing, Peoples R China.	dapengchen@xjtu.edu.cn; yuan.ze.jian@mail.xjtu.edu.cn; jingdw@microsoft.com; chenbd@mail.xjtu.edu.cn; ghua@stevens.edu; nnzheng@mail.xjtu.edu.cn	Wang, Jingdong/E-9920-2017; Chen, Badong/F-4211-2015	Wang, Jingdong/0000-0002-4888-4445; Chen, Badong/0000-0003-1710-3818	National Key Research and Development Program of China [2016YFB1001001]; National Basic Research Program of China [2015CB351703, 2012CB316400]; National Natural Science Foundation of China [61573280, 91648121, 61603022, 61573273]	National Key Research and Development Program of China; National Basic Research Program of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Key Research and Development Program of China (No. 2016YFB1001001), the National Basic Research Program of China (No. 2015CB351703, No. 2012CB316400), the National Natural Science Foundation of China (No. 61573280, No. 91648121, No. 61603022, No. 61573273).	Ahmed E., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299016; [Anonymous], WORKSH DEM COMP VIS; Bak Slawomir, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P435, DOI 10.1109/AVSS.2010.34; Baltieri D, 2011, LECT NOTES COMPUT SC, V6978, P197, DOI 10.1007/978-3-642-24085-0_21; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Chen D., 2016, C COMP VIS PATT REC; Chen DP, 2015, PROC CVPR IEEE, P1565, DOI 10.1109/CVPR.2015.7298764; Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Gong S., 2014, ADV COMPUTER VISION; Gray D., 2007, P IEEE INT WORKSH PE, V3, P1; Hirzer M, 2012, LECT NOTES COMPUT SC, V7577, P780, DOI 10.1007/978-3-642-33783-3_56; Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Kjems U., 2000, NIPS, P549; Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939; Li W., 2013, COMPUTER VISION PATT; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li Y., 2015, WINT C APPL COMP VIS; Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Liao SC, 2010, PROC CVPR IEEE, P1301, DOI 10.1109/CVPR.2010.5539817; Loy CC, 2013, IEEE IMAGE PROC, P3567, DOI 10.1109/ICIP.2013.6738736; Ma BP, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.57; Ma LY, 2014, IEEE T IMAGE PROCESS, V23, P3656, DOI 10.1109/TIP.2014.2331755; Mignon A, 2012, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2012.6247987; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794; Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426; Prosser B. J., 2010, PROC BRIT MACH VIS C, P6, DOI DOI 10.5244/C.24.21; Schwartz WR, 2009, SIBGRAPI, P322, DOI 10.1109/SIBGRAPI.2009.42; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Wu ZY, 2015, IEEE T PATTERN ANAL, V37, P1095, DOI 10.1109/TPAMI.2014.2360373; Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1; Xu YL, 2013, IEEE I CONF COMP VIS, P3152, DOI 10.1109/ICCV.2013.391; Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35; Zhang Ziming, 2014, ECCV WORKSH; Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138; Zheng Wei-Shi, 2009, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.23.23	48	11	11	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2017	123	3					392	414		10.1007/s11263-017-0991-0	http://dx.doi.org/10.1007/s11263-017-0991-0			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EX9EO					2022-12-18	WOS:000403559600005
J	Ozgur, E; Bartoli, A				Ozgur, Erol; Bartoli, Adrien			Particle-SfT: A Provably-Convergent, Fast Shape-from-Template Algorithm	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						convergent; fast; isometric; elastic; particle; shape-from-template	SURFACE DETECTION	The Shape-from-Template (SfT) problem is to recover the 3D shape of a deformable object from a single image, given a 3D template and a deformation constraint. We propose Particle-SfT, a new SfT algorithm which handles isometric and non-isometric deformations. We build Particle-SfT upon a particle system guided by deformation and reprojection constraint projections. Reconstruction is achieved by evolving particles to a globally attractive equilibrium, while taking observable external forces such as gravity into account, if any. Particle-SfT may be used to refine an existing initial shape. However, in practice we simply use the template as initial guess. This is because, as opposed to the existing refining methods, Particle-SfT has an extremely wide convergence basin. Particle-SfT is also faster than the existing refining methods. This is because it moves pieces of the shape's mesh independently to achieve larger step size by optimal constraint projection. We proved its convergence to a fixed-point. We experimented it with synthetic and real data. It has the same accuracy as the best performing isometric method and consistently outperforms all existing elastic methods in almost all cases, while being much faster.	[Ozgur, Erol; Bartoli, Adrien] Clermont Univ, Clermont Ferrand, France		Ozgur, E (corresponding author), Clermont Univ, Clermont Ferrand, France.	ErolOzgur@gmail.com			EU's FP7 through the ERC [307483 FLEXABLE]	EU's FP7 through the ERC	We thank the authors of (Brunet et al. 2014; Chhatkuli et al. 2014; Bartoli et al. 2015; Ostlund et al. 2012; Salzmann and Fua 2011; Varol et al. 2012; Malti et al. 2015) for making their source codes and/or datasets available to us. This research has received funding from the EU's FP7 through the ERC Research Grant 307483 FLEXABLE.	Agarwal R. P., 2001, CAMB TRACT MATH; Agudo A., 2014, CVPR; Agudo A., 2015, CVPR; Agudo A, 2016, IEEE T PATTERN ANAL, V38, P979, DOI 10.1109/TPAMI.2015.2469293; Alcantarilla P. F., 2012, ECCV; Bartoli A, 2015, IEEE T PATTERN ANAL, V37, P2099, DOI 10.1109/TPAMI.2015.2392759; BELLUCE LP, 1969, CAN MATH BULLETIN, V12, P481, DOI 10.4153/CMB-1969-062-9; BROUWER L, 1910, MATH ANN, V71, P97; Brunet F, 2014, COMPUT VIS IMAGE UND, V125, P138, DOI 10.1016/j.cviu.2014.04.003; Chhatkuli A., 2014, CVPR; Goldstein H., 2002, CLASSICAL MECH; Haouchine N., 2014, ISMAR; Ilic S., 2007, ICCV; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Lowe D. G., 1999, ICCV; Lyapunov AM, 1892, THESIS; Malti A., 2013, CVPR; Malti A., 2015, CVPR; Marton Z.C., 2009, ICRA; METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727; Moreno-Noguer F., 2009, CVPR; Muller Matthias, 2006, WORKSH VIRT REAL INT; Ostlund J.O.M., 2012, ECCV; Perriollat M, 2011, INT J COMPUT VISION, V95, P124, DOI 10.1007/s11263-010-0352-8; Pilet J, 2008, INT J COMPUT VISION, V76, P109, DOI 10.1007/s11263-006-0017-9; Pizarro D, 2012, INT J COMPUT VISION, V97, P54, DOI 10.1007/s11263-011-0452-0; PROVOT X, 1995, GRAPH INTER, P147; RHOADES BE, 1977, T AM MATH SOC, V226, P257, DOI 10.2307/1997954; Salzmann M., 2008, ECCV; Salzmann M., 2009, CVPR; Salzmann M., 2011, ICCV; Salzmann M, 2011, IEEE T PATTERN ANAL, V33, P931, DOI 10.1109/TPAMI.2010.158; Varol A., 2012, CVPR	33	11	11	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2017	123	2					184	205		10.1007/s11263-016-0968-4	http://dx.doi.org/10.1007/s11263-016-0968-4			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EU2OM					2022-12-18	WOS:000400868800003
J	Li, SJ; Zhang, WC; Chan, AB				Li, Sijin; Zhang, Weichen; Chan, Antoni B.			Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structured learning; Deep learning; Human pose estimation		This paper focuses on structured-output learning using deep neural networks for 3D human pose estimation from monocular images. Our network takes an image and 3D pose as inputs and outputs a score value, which is high when the image-pose pair matches and low otherwise. The network structure consists of a convolutional neural network for image feature extraction, followed by two sub-networks for transforming the image features and pose into a joint embedding. The score function is then the dot-product between the image and pose embeddings. The image-pose embedding and score function are jointly trained using a maximum-margin cost function. Our proposed framework can be interpreted as a special form of structured support vector machines where the joint feature space is discriminatively learned using deep neural networks. We also propose an efficient recurrent neural network for performing inference with the learned image-embedding. We test our framework on the Human3.6m dataset and obtain state-of-the-art results compared to other recent methods. Finally, we present visualizations of the image-pose embedding space, demonstrating the network has learned a high-level embedding of body-orientation and pose-configuration.	[Li, Sijin; Zhang, Weichen; Chan, Antoni B.] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China	City University of Hong Kong	Li, SJ (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.	sijin.li@my.cityu.edu.hk; wczhang4-c@my.cityu.edu.hk; abchan@cityu.edu.hk	CHAN, Antoni B./D-7858-2013	CHAN, Antoni B./0000-0002-2886-2513	Research Grants Council of the Hong Kong Special Administrative Region, China [CityU 123212]; City University of Hong Kong [7004417, 7004682]	Research Grants Council of the Hong Kong Special Administrative Region, China(Hong Kong Research Grants Council); City University of Hong Kong(City University of Hong Kong)	This work was supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (CityU 123212), and by a Strategic Research Grant from City University of Hong Kong (Project Nos. 7004417 and 7004682). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.	Andrew Galen, 2013, ICML; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Yoshua, 2013, INT C MACHINE LEARNI, P552; Bregler C, 2004, INT J COMPUT VISION, V56, P179, DOI 10.1023/B:VISI.0000011203.00237.9b; Bregler C., 2014, ICLR; Burenius M, 2013, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2013.464; CALAMAI PH, 1987, MATH PROGRAM, V39, P93, DOI 10.1007/BF02592073; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Chen X., 2014, P 27 ANN C NEURAL IN, P1736, DOI DOI 10.1109/CVPR.2018.00742; Chu X, 2015, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2015.383; Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c; Dhungel N., 2014, CORR; Eichner M., 2009, BMVC, V2, P5; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Ionescu C, 2011, IEEE I CONF COMP VIS, P2220, DOI 10.1109/ICCV.2011.6126500; Ionescu C, 2009, IEEE I CONF COMP VIS, P1157, DOI 10.1109/ICCV.2009.5459346; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Koller D., 2009, PROBABILISTIC GRAPHI; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LI S, 2014, IJCV, P1, DOI DOI 10.13140/RG.2.1.4181.3604; Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326; Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23; Murray R. M., 1994, MATH INTRO ROBOTIC M; Nair V, 2010, P 27 INT C MACHINE L, P807; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Osadchy M, 2007, J MACH LEARN RES, V8, P1197; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Rodriguez J. A., 2013, BMVC; Rumelhart DE, 1985, TECHNICAL REPORT, DOI 10.1016/b978-1-4832-1446-7.50035-2; Sapp B., 2013, P IEEE C CVPR; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Srivastava Nitish, 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; YANG Y, 2011, PROC CVPR IEEE, P1385, DOI [10.1109/CVPR.2011.5995741, DOI 10.1109/CVPR.2011.5995741]; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	44	11	11	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2017	122	1					149	168		10.1007/s11263-016-0962-x	http://dx.doi.org/10.1007/s11263-016-0962-x			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EL2AF		Green Submitted			2022-12-18	WOS:000394421800007
J	Agarwal, S; Lee, HL; Sturmfels, B; Thomas, RR				Agarwal, Sameer; Lee, Hon-Leung; Sturmfels, Bernd; Thomas, Rekha R.			On the Existence of Epipolar Matrices	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Epipolar geometry; Algebraic geometry		This paper considers the foundational question of the existence of a fundamental (resp. essential) matrix given m point correspondences in two views. We present a complete answer for the existence of fundamental matrices for any value of m. We disprove the widely held beliefs that fundamental matrices always exist whenever . At the same time, we prove that they exist unconditionally when . Under a mild genericity condition, we show that an essential matrix always exists when . We also characterize the six and seven point configurations in two views for which all matrices satisfying the epipolar constraint have rank at most one.	[Agarwal, Sameer] Google Inc, Seattle, WA 98103 USA; [Lee, Hon-Leung; Thomas, Rekha R.] Univ Washington, Dept Math, Seattle, WA 98195 USA; [Sturmfels, Bernd] Univ Calif Berkeley, Dept Math, Berkeley, CA 94720 USA	Google Incorporated; University of Washington; University of Washington Seattle; University of California System; University of California Berkeley	Agarwal, S (corresponding author), Google Inc, Seattle, WA 98103 USA.	sameeragarwal@google.com; hllee@uw.edu; bernd@berkeley.edu; rrthomas@uw.edu			NSF [DMS-1418728, DMS-1419018]	NSF(National Science Foundation (NSF))	Lee and Thomas were partially supported by NSF Grant DMS-1418728, and Sturmfels by NSF Grant DMS-1419018.	Agarwal S., 2014, CERTIFYING EXISTENCE; [Anonymous], 1997, MATRIX ALGEBRA STAT; [Anonymous], 2005, GROBNER BASIS METHOD; Chum O, 2005, PROC CVPR IEEE, P772; Cox D., 2007, IDEALS VARIETIES ALG; DALBEC J, 1995, INVARIANT METHODS IN DISCRETE AND COMPUTATIONAL GEOMETRY, P37; Demazure M., 1988, 992 INRIA; Faugeras O. D., 1992, Computer Vision - ECCV '92. Second European Conference on Computer Vision Proceedings, P563; FAUGERAS OD, 1990, INT J COMPUT VISION, V4, P225, DOI 10.1007/BF00054997; Flanders Harley, 1962, J LONDON MATH SOC, V1, P10; Gel'fand IsraelM., 1994, MATH THEORY APPL, DOI 10.1007/978-0-8176-4771-1_10; Harris J., 1992, ALGEBRAIC GEOMETRY 1, V133; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; Hartley R., TECH REP; Hartley R. I., 2003, MULTIVIEW GEOMETRY C; HARTLEY RI, 1994, IEEE T PATTERN ANAL, V16, P1036, DOI 10.1109/34.329005; HARTLEY RI, 1992, LECT NOTES COMPUT SC, V588, P579; Kneip L., 2012, EUR C COMP VIS; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Ma Yi, 2012, INVITATION 3 D VISIO, V26; Marshall, 2008, POSITIVE POLYNOMIALS, V146; Maybank S., 1993, THEORY RECONSTRUCTIO, V28; MESHULAM R, 1985, Q J MATH, V36, P225, DOI 10.1093/qmath/36.2.225; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Semple J. G., 1998, ALGEBRAIC PROJECTIVE; Shafarevich Igor R., 2013, BASIC ALGEBRAIC GEOM, Vthird; Sturm R., 1869, MATH ANN, V1, P533; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003	28	11	11	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2017	121	3					403	415		10.1007/s11263-016-0949-7	http://dx.doi.org/10.1007/s11263-016-0949-7			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EK9UY		Green Submitted			2022-12-18	WOS:000394270600005
J	Xu, JL; Ramos, S; Vazquez, D; Lopez, AM				Xu, Jiaolong; Ramos, Sebastian; Vazquez, David; Lopez, Antonio M.			Hierarchical Adaptive Structural SVM for Domain Adaptation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							FACE RECOGNITION	A key topic in classification is the accuracy loss produced when the data distribution in the training (source) domain differs from that in the testing (target) domain. This is being recognized as a very relevant problem for many computer vision tasks such as image classification, object detection, and object category recognition. In this paper, we present a novel domain adaptation method that leverages multiple target domains (or sub-domains) in a hierarchical adaptation tree. The core idea is to exploit the commonalities and differences of the jointly considered target domains. Given the relevance of structural SVM (SSVM) classifiers, we apply our idea to the adaptive SSVM (A-SSVM; Xu et al., IEEE Trans Pattern Anal Mach Intell 36(12): 2367-2380, 2014a), which only requires the target domain samples together with the existing source-domain classifier for performing the desired adaptation. Altogether, we term our proposal as hierarchical A-SSVM (HA-SSVM). As proof of concept we use HA-SSVM for pedestrian detection, object category recognition and face recognition. In the former we apply HA-SSVM to the deformable part-based model (DPM; Felzenszwalb et al., IEEE Trans Pattern Anal Mach Intell 32(9): 1627-1645, 2010) while in the rest HA-SSVM is applied to multi-category classifiers. We will show how HA-SSVM is effective in increasing the detection/recognition accuracy with respect to adaptation strategies that ignore the structure of the target data. Since, the sub-domains of the target data are not always known a priori, we shown how HA-SSVM can incorporate sub-domain discovery for object category recognition.	[Xu, Jiaolong; Ramos, Sebastian; Vazquez, David; Lopez, Antonio M.] Comp Vis Ctr, Barcelona, Spain; [Xu, Jiaolong; Ramos, Sebastian; Lopez, Antonio M.] Univ Autonoma Barcelona, Dept Comp Sci, Barcelona, Spain	Centre de Visio per Computador (CVC); Autonomous University of Barcelona	Xu, JL (corresponding author), Comp Vis Ctr, Barcelona, Spain.; Xu, JL (corresponding author), Univ Autonoma Barcelona, Dept Comp Sci, Barcelona, Spain.	jiaolong@cvc.uab.es; sramosp@cvc.uab.es; dvazquez@cvc.uab.es; antonio@cvc.uab.es	López, Antonio M/L-5303-2014; Vázquez, David/P-3306-2019	López, Antonio M/0000-0002-6979-5783; Vázquez, David/0000-0002-2845-8158	Spanish MEC Project [TRA2014-57088-C2-1-R]; Spanish DGT Project [SPIP2014-01352]; Generalitat de Catalunya [2014-SGR-1506]; Jiaolong Xu's Chinese Scholarship Council (CSC) [2011611023]; Sebastian Ramos' FPI Grant [BES-2012-058280]	Spanish MEC Project(Spanish Government); Spanish DGT Project; Generalitat de Catalunya(Generalitat de Catalunya); Jiaolong Xu's Chinese Scholarship Council (CSC); Sebastian Ramos' FPI Grant	This work is supported by the Spanish MEC Project TRA2014-57088-C2-1-R, the Spanish DGT Project SPIP2014-01352, the Generalitat de Catalunya Project 2014-SGR-1506, Jiaolong Xu's Chinese Scholarship Council (CSC) Grant No. 2011611023, and Sebastian Ramos' FPI Grant BES-2012-058280. Finally, we also want to thank the NVIDIA Corporation for the generous support in the form of different GPU hardware units.	[Anonymous], 2007, P 15 ACM INT C MULTI; Aytar Y., 2011, P INT C COMP VIS SIN; Behley J., 2013, IEEE INT C INT ROB S; Ben-David S., 2010, ML, V79, P151, DOI DOI 10.1007/S10994-009-5152-4]; Bergamo A., 2010, ADV NEURAL INFORM PR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Daume III H, 2007, M ASS COMP LING PRAG; Daume III H., 2009, UAI; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Duan L., 2012, INT C MACH LEARN; Duan L., 2009, INT C MACH LEARN; Ess A, 2007, IEEE I CONF COMP VIS, P2065; Finkel J., 2009, NAACL; Geiger A., 2011, ADV NEURAL INFORM PR; Geiger A., 2012, P IEEE COMP SOC C CO; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Girshick RB, 2012, DISCRIMINATIVELY TRA; Girshick Ross Brook, 2012, RIGID TEMPLATES GRAM; Gong B., 2013, ADV NEURAL INFORM PR; Gong BQ, 2014, INT J COMPUT VISION, V109, P3, DOI 10.1007/s11263-014-0718-4; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gong Boqing, 2013, INT C MACH LEARN; Gopalan R., 2011, INT C COMP VIS; Gourier N., 2004, INT C PATT REC NEW Y; Griffin Gregory, 2007, CALTECH 256 OBJECT C; Hoffman J, 2012, EUR C COMP VIS; Hoffman J., 2013, INT C LEARN REPR AR; Hoffman J, 2014, INT J COMPUT VISION, V109, P28, DOI 10.1007/s11263-014-0719-3; Yebes JJ, 2015, SENSORS-BASEL, V15, P9228, DOI 10.3390/s150409228; Jia Y., 2014, P 22 ACM INT C MULT, P675; Jiang J, 2008, TECHNICAL REPORT; Kan MN, 2014, INT J COMPUT VISION, V109, P94, DOI 10.1007/s11263-013-0693-1; Kulis B, 2011, IEEE C COMP VIS PATT; Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92; Lu B., 2015, BRIT MACH VIS C, P1; Mansour Y., 2008, ADV NEURAL INF PROCE, V21; Mirrashed F., 2013, INT C COMP VIS; Mosek, 2013, OPT TOOLK; Nguyen HV, 2015, IEEE T IMAGE PROCESS, V24, P5479, DOI 10.1109/TIP.2015.2479405; Ni J., 2013, IEEE C COMP VIS PATT; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Papadimitratos P, 2012, IEEE ICC; Park D, 2010, LECT NOTES COMPUT SC, V6314, P241, DOI 10.1007/978-3-642-15561-1_18; Pepikj B., 2015, IEEE T PATTERN ANAL; Premebida C, 2014, IEEE INT C INT ROBOT, P4112, DOI 10.1109/IROS.2014.6943141; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Tang K., 2012, ADV NEURAL INFORM PR; Teh Y. W., 2007, P ADV NEUR INF PROC, P1353; Vazquez D., 2012, INT C PATT REC TSUK; Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163; Xu H., 2015, BRIT MACH VIS C; Xu JL, 2014, IEEE T PATTERN ANAL, V36, P2367, DOI 10.1109/TPAMI.2014.2327973; Xu JL, 2014, IEEE T INTELL TRANSP, V15, P2121, DOI 10.1109/TITS.2014.2310138; Zhu L., 2010, IEEE C COMP VIS PATT	55	11	13	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	2					159	178		10.1007/s11263-016-0885-6	http://dx.doi.org/10.1007/s11263-016-0885-6			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FA		Green Submitted			2022-12-18	WOS:000380269600004
J	Khan, SH; Bennamoun, M; Sohel, F; Togneri, R; Naseem, I				Khan, Salman H.; Bennamoun, Mohammed; Sohel, Ferdous; Togneri, Roberto; Naseem, Imran			Integrating Geometrical Context for Semantic Labeling of Indoor Scenes using RGBD Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene parsing; Graphical models; Geometric reasoning; Structured learning	OBJECT RECOGNITION; FEATURES; SCALE; TEXTURE	Inexpensive structured light sensors can capture rich information from indoor scenes, and scene labeling problems provide a compelling opportunity to make use of this information. In this paper we present a novel conditional random field (CRF) model to effectively utilize depth information for semantic labeling of indoor scenes. At the core of the model, we propose a novel and efficient plane detection algorithm which is robust to erroneous depthmaps. Our CRF formulation defines local, pairwise and higher order interactions between image pixels. At the local level, we propose a novel scheme to combine energies derived from appearance, depth and geometry-based cues. The proposed local energy also encodes the location of each object class by considering the approximate geometry of a scene. For the pairwise interactions, we learn a boundary measure which defines the spatial discontinuity of object classes across an image. To model higher-order interactions, the proposed energy treats smooth surfaces as cliques and encourages all the pixels on a surface to take the same label. We show that the proposed higher-order energies can be decomposed into pairwise submodular energies and efficient inference can be made using the graph-cuts algorithm. We follow a systematic approach which uses structured learning to fine-tune the model parameters. We rigorously test our approach on SUN3D and both versions of the NYU-Depth database. Experimental results show that our work achieves superior performance to state-of-the-art scene labeling techniques.	[Khan, Salman H.; Bennamoun, Mohammed; Sohel, Ferdous] Univ Western Australia, Sch CSSE, 35 Stirling Highway, Crawley, WA 6009, Australia; [Togneri, Roberto] Univ Western Australia, Sch EECE, 35 Stirling Highway, Crawley, WA 6009, Australia; [Naseem, Imran] Karachi Inst Econ & Technol, Dept Engn, Karachi 75190, Pakistan	University of Western Australia; University of Western Australia	Khan, SH (corresponding author), Univ Western Australia, Sch CSSE, 35 Stirling Highway, Crawley, WA 6009, Australia.	salman.khan@research.uwa.edu.au; mohammed.bennamoun@uwa.edu.au; ferdous.sohel@uwa.edu.au; roberto.togneri@uwa.edu.au; imrannaseem@pafkiet.edu.pk	Bennamoun, Mohammed/C-2789-2013; Sohel, Ferdous/C-2428-2013; Khan, Salman Hameed/M-4834-2016	Bennamoun, Mohammed/0000-0002-6603-3257; Sohel, Ferdous/0000-0003-1557-4907; Khan, Salman Hameed/0000-0002-9502-1749; Togneri, Roberto/0000-0002-3778-4633	IPRS scholarship from The University of Western Australia; Australian Research Council (ARC) [DP110102166, DP150104251, DE120102960]	IPRS scholarship from The University of Western Australia; Australian Research Council (ARC)(Australian Research Council)	This research was supported by the IPRS scholarship from The University of Western Australia and the Australian Research Council (ARC) Grants DP110102166, DP150104251 and DE120102960. The authors would especially like to thank the anonymous reviewers and the Associate Editor for their valuable comments and suggestions to improve the quality of the manuscript.	Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Blake A, 2011, MARKOV RANDOM FIELDS FOR VISION AND IMAGE PROCESSING, P1; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Cadena C., 2014, SEMANTIC SEGMENTATIO; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Couprie Camille, 2013, 1 INT C LEARN REPR I; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Edwards W, 2007, ADVANCES IN DECISION ANALYSIS: FROM FOUNDATIONS TO APPLICATIONS, P1, DOI 10.1017/CBO9780511611308; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79; Hall M., 2008, WEKA DATA MINING SOF, V11, P10, DOI [10.1145/1656274.1656278, DOI 10.1145/1656274.1656278]; Hayat M, 2015, IEEE T PATTERN ANAL, V37, P713, DOI 10.1109/TPAMI.2014.2353635; He XM, 2004, PROC CVPR IEEE, P695; Jiang Y, 2012, INT J ROBOT RES, V31, P1021, DOI 10.1177/0278364912438781; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Khan S., 2015, IEEE CVPR; Khan S., 2014, IEEE CVPR; Khan SH, 2014, LECT NOTES COMPUT SC, V8689, P679, DOI 10.1007/978-3-319-10590-1_44; Kohli P., 2007, P IEEE C COMP VIS PA, P1; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Koppula H. S., 2011, ADV NEURAL INFORM PR, P244; Ladicky L., 2013, IJCV, P1; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; Lai K, 2011, IEEE INT CONF ROBOT, P1817; Lempitsky V., 2011, NIPS, V24, P1485; Li YJ, 2013, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2013.14; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Qixing Huang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1953, DOI 10.1109/CVPR.2011.5995571; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Quigley M, 2009, IEEE INT CONF ROBOT, P3604; Rao D, 2010, IEEE INT C INT ROBOT, P2578, DOI 10.1109/IROS.2010.5650493; Ren XF, 2012, PROC CVPR IEEE, P2759, DOI 10.1109/CVPR.2012.6247999; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Silberman N., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P601, DOI 10.1109/ICCVW.2011.6130298; Silberman Nathan, 2012, EUR C COMP VIS, DOI 10.1007/978-3-642-33715-4_54; Szummer M, 2008, LECT NOTES COMPUT SC, V5303, P582, DOI 10.1007/978-3-540-88688-4_43; Tahir R., 2006, INT ARCH PHOTOGRAMM, V36, P248, DOI DOI 10.1111/1750-3841.12802; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; van de Weijer J, 2006, LECT NOTES COMPUT SC, V3952, P334; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; Woodford OJ, 2009, IEEE I CONF COMP VIS, P2319, DOI 10.1109/ICCV.2009.5459434; Xiao J., 2013, BMVC; Xiong X., 2010, BMVC, P45	55	11	14	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2016	117	1					1	20		10.1007/s11263-015-0843-8	http://dx.doi.org/10.1007/s11263-015-0843-8			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DF2FG		Green Submitted			2022-12-18	WOS:000371156500001
J	Ordonez, V; Liu, W; Deng, J; Choi, YJ; Berg, AC; Berg, TL				Ordonez, Vicente; Liu, Wei; Deng, Jia; Choi, Yejin; Berg, Alexander C.; Berg, Tamara L.			Predicting Entry-Level Categories	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Recognition; Categorization; Entry-level categories; Psychology		Entry-level categories-the labels people use to name an object-were originally defined and studied by psychologists in the 1970s and 1980s. In this paper we extend these ideas to study entry-level categories at a larger scale and to learn models that can automatically predict entry-level categories for images. Our models combine visual recognition predictions with linguistic resources like WordNet and proxies for word "naturalness" mined from the enormous amount of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people, and for learning mappings between concepts predicted by existing visual recognition systems and entry-level concepts. In this work we make use of recent successful efforts on convolutional network models for visual recognition by training classifiers for 7404 object categories on ConvNet activation features. Results for category mapping and entry-level category prediction for images show promise for producing more natural human-like labels. We also demonstrate the potential applicability of our results to the task of image description generation.	[Ordonez, Vicente; Liu, Wei; Berg, Alexander C.; Berg, Tamara L.] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27514 USA; [Deng, Jia] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA; [Choi, Yejin] Univ Washington, Comp Sci & Engn, Seattle, WA 98195 USA	University of North Carolina; University of North Carolina Chapel Hill; University of Michigan System; University of Michigan; University of Washington; University of Washington Seattle	Ordonez, V (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27514 USA.	vicente@cs.unc.edu			NSF [1444234, 1445409]; Div Of Information & Intelligent Systems [1444234] Funding Source: National Science Foundation	NSF(National Science Foundation (NSF)); Div Of Information & Intelligent Systems(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This work was supported by NSF Career Award #1444234 and NSF Award #1445409.	[Anonymous], 2012, ICML; BARNARD K, 2006, INFORM THEORY APPL; Bird S., 2006, COLING ACL; Brants Thorsten, 2006, LINGUISTIC DATA CONS; Chen X., 2013, ICCV; Dean T. L, 2013, CVPR; Deng J., 2009, CVPR; Deng J., 2012, CVPR; Deng J., 2010, ECCV; Divvala SK, 2014, COMPUTER VISION PATT; Donahue J., 2013, 31 INT C MACH LEARN; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Farhadi A., 2010, ECCV; Fellbaum Christiane, 1998, WORDNET ELECT DATABA; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Feng S., 2015, AAAI; Gupta A., 2012, 26 AAAI C ART INT; Jia Y., 2013, CAFFE OPEN SOURCE CO; JOLICOEUR P, 1984, COGNITIVE PSYCHOL, V16, P243, DOI 10.1016/0010-0285(84)90009-4; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162; Kuznetsova P., 2012, ACL; Kuznetsova Polina, 2014, T ASSOC COMPUT LING, V2, P351, DOI DOI 10.1162/TACL_A_00188; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Mason R., 2014, ACL; Mitchell Margaret, 2012, EACL; Ordonez V., 2011, NIPS; Ordonez V, 2013, ICCV; Perronnin F., 2012, CVPR; Platt J., 1999, ADV LARGE MARGIN CLA; Ramnath K., 2014, WACV; Rosch E., 1978, COGNITION CATEGORIZA, P27, DOI DOI 10.1037/0012-1649.16.5.391; Russakovsky O., 2014, ARXIV, P1, DOI [10.48550/arXiv.1409.0575, DOI 10.48550/ARXIV.1409.0575]; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Simonyan K., 2014, ARXIV E PRINTS; Szegedy C, 2014, GOING DEEPER CONVOLU; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Xiao J., 2010, CVPR; Yanai K., 2005, MIR ACM; Yang Y., 2011, EMNLP	41	11	11	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2015	115	1					29	43		10.1007/s11263-015-0815-z	http://dx.doi.org/10.1007/s11263-015-0815-z			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CS0MN					2022-12-18	WOS:000361754600002
J	Cao, S; Snavely, N				Cao, Song; Snavely, Noah			Graph-Based Discriminative Learning for Location Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Location recognition; Discriminative learning; Image graphs		Recognizing the location of a query image by matching it to an image database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of an image database by representing it as a graph, and show how the rich information embedded in such a graph can improve bag-of-words-based location recognition methods. In particular, starting from a graph based on visual connectivity, we propose a method for selecting a set of overlapping subgraphs and learning a local distance function for each subgraph using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.	[Cao, Song; Snavely, Noah] Cornell Univ, Ithaca, NY 14850 USA	Cornell University	Cao, S (corresponding author), Cornell Univ, Ithaca, NY 14850 USA.	caosong@cs.cornell.edu			National Science Foundation [IIS-1149393, IIS-0964027]; Intel Corporation	National Science Foundation(National Science Foundation (NSF)); Intel Corporation(Intel Corporation)	This work was supported in part by the National Science Foundation (Grants IIS-1149393 and IIS-0964027) and Intel Corporation. We also thank Flickr users for use of their photos.	Agarwal S., 2009, ICCV; Arandjelovic R, 2012, CVPR; Cao S., 2012, ECCV WORKSH WEB SCAL; Chen H., 2006, ACM SIGIR; Chum O., 2010, PAMI; Chum O., 2010, CVPR; Doersch C., 2012, SIGGRAPH; Frahm J.-M., 2010, ECCV; Frome A., 2007, ICCV; Guha S, 1998, ALGORITHMICA, V20, P374, DOI 10.1007/PL00009201; Havlena M., 2010, ECCV; Hays J., 2008, CVPR; Irschara A., 2009, CVPR; Johns E., 2011, ICCV; Knopp J., 2010, ECCV; Li Xiaowei, 2008, ECCV, P2; Li Y., 2009, ICCV; Li Y., 2012, ECCV; Li Yunpeng, 2010, ECCV; Malisiewicz T., 2009, NIPS; Malisiewicz T., 2011, ICCV; Mikulik A., 2010, ECCV; Philbin J., 2007, CVPR; Platt JC, 2000, ADV NEUR IN, P61; Robertson Stephen E., 1997, READINGS INFORM RETR, P281; Sattler T., 2011, ICCV; Sattler T., 2012, BMVC; Sattler Torsten, 2012, ECCV; Schindler G., 2007, CVPR; Shrivastava A., 2011, SIGGRAPH ASIA; Sivic J., 2003, ICCV; Torii A., 2011, ICCV WORKSH; Turcot P., 2009, WORKSH EM ISS LARG A; Zheng Y.T., 2009, CVPR	35	11	11	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2015	112	2			SI		239	254		10.1007/s11263-014-0774-9	http://dx.doi.org/10.1007/s11263-014-0774-9			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CE0TD		Green Submitted			2022-12-18	WOS:000351518500008
J	Imre, E; Hilton, A				Imre, Evren; Hilton, Adrian			Order Statistics of RANSAC and Their Practical Application	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						RANSAC; Robust regression; Camera calibration; Structure-from-motion	MLESAC	For statistical analysis purposes, RANSAC is usually treated as a Bernoulli process: each hypothesis is a Bernoulli trial with the outcome outlier-free/contaminated; a run is a sequence of such trials. However, this model only covers the special case where all outlier-free hypotheses are equally good, e.g. generated from noise-free data. In this paper, we explore a more general model which obviates the noise-free data assumption: we consider RANSAC a random process returning the best hypothesis, , among a number of hypotheses drawn from a finite set (). We employ the rank of within for the statistical characterisation of the output, present a closed-form expression for its exact probability mass function, and demonstrate that -distribution is a good approximation thereof. This characterisation leads to two novel termination criteria, which indicate the number of iterations to come arbitrarily close to the global minimum in with a specified probability. We also establish the conditions defining when a RANSAC process is statistically equivalent to a cascade of shorter RANSAC processes. These conditions justify a RANSAC scheme with dedicated stages to handle the outliers and the noise separately. We demonstrate the validity of the developed theory via Monte-Carlo simulations and real data experiments on a number of common geometry estimation problems. We conclude that a two-stage RANSAC process offers similar performance guarantees at a much lower cost than the equivalent one-stage process, and that a cascaded set-up has a better performance than LO-RANSAC, without the added complexity of a nested RANSAC implementation.	[Imre, Evren; Hilton, Adrian] Univ Surrey, CVSSP, Guildford GU2 7XH, Surrey, England	University of Surrey	Imre, E (corresponding author), Univ Surrey, CVSSP, Guildford GU2 7XH, Surrey, England.	h.imre@surrey.ac.uk	Hilton, Adrian/N-3736-2014	Hilton, Adrian/0000-0003-4223-238X	Technology Strategy Board(TSB) [TP/11/CII/6/I/AJ307D, 11702-76150]; European Commission ICT-7th Framework Program project "IMPART: Intelligent Management Platform for Advanced Real-Time Media Processes" [316564]; EPSRC [EP/F02827X/1] Funding Source: UKRI; Engineering and Physical Sciences Research Council [EP/F02827X/1] Funding Source: researchfish	Technology Strategy Board(TSB); European Commission ICT-7th Framework Program project "IMPART: Intelligent Management Platform for Advanced Real-Time Media Processes"; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work is supported by the Technology Strategy Board(TSB) projects i3Dlive: interactive 3D methods for live-action media (TP/11/CII/6/I/AJ307D), "SYMMM: Synchronising Multimodal Movie Metadata" (11702-76150), and the European Commission ICT-7th Framework Program project "IMPART: Intelligent Management Platform for Advanced Real-Time Media Processes" (316564).	[Anonymous], [No title captured]; Arnold B.C., 2008, 1 COURSE ORDER STAT, VVolume 54; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Haralick R. M., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P592, DOI 10.1109/CVPR.1991.139759; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hughes-Hallett D., 2017, CALCULUS SINGLE VARI, V7; Imre Evren, 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P260, DOI 10.1109/3DIMPVT.2011.40; Johnson N.L., 1995, CONTINUOUS UNIVARIAT, V1; Johnson NL, 2005, WILEY SER PROBAB ST, P1, DOI 10.1002/0471715816; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Nister D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P199; Pollefeys M., 2013, LEUVEN CASTLE; Powell M.J.D., 1970, NUMERICAL METHODS NO, P87; Raguram R, 2009, IEEE I CONF COMP VIS, P2074, DOI 10.1109/ICCV.2009.5459456; Tordoff BJ, 2005, IEEE T PATTERN ANAL, V27, P1523, DOI 10.1109/TPAMI.2005.199; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Tran Q.-H., 2012, DEFENCE RANSAC OUTLI, P274; WALD A, 1945, ANN MATH STAT, V16, P117, DOI 10.1214/aoms/1177731118	21	11	12	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2015	111	3					276	297		10.1007/s11263-014-0745-1	http://dx.doi.org/10.1007/s11263-014-0745-1			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CB9XK					2022-12-18	WOS:000349987400002
J	Sfar, AR; Boujemaa, N; Geman, D				Sfar, Asma Rejeb; Boujemaa, Nozha; Geman, Donald			Confidence Sets for Fine-Grained Categorization and Plant Species Identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Fine-grained categorization; Hierarchical representation; Confidence set; Plant identification; Semi-automated	LEAF IMAGES; CLASSIFICATION; RECOGNITION; SEGMENTATION	We present a new hierarchical strategy for fine-grained categorization. Standard, fully automated systems report a single estimate of the category, or perhaps a ranked list, but have non-neglible error rates for most realistic scenarios, which limits their utility. Instead, we propose a semi-automated system which outputs a it confidence set (CS)-a variable-length list of categories which contains the true one with high probability (e.g., a 99 % CS). Performance is then measured by the expected size of the CS, reflecting the effort required for final identification by the user. The implementation is based on a hierarchical clustering of the full set of categories. This tree of subsets provides a graded family of candidate CS's containing visually similar categories. There is also a learned discriminant score for deciding between each subset and all others combined. Selection of the CS is based on the joint score likelihood under a Bayesian network model. We apply this method to determining the species of a plant from an image of a leaf against either a uniform or natural background. Extensive experiments are reported. We obtain superior results relative to existing methods for point estimates for scanned leaves and report the first useful results for natural images at the expense of asking the user to initialize the process by identifying specific landmarks.	[Sfar, Asma Rejeb; Boujemaa, Nozha] INRIA Saclay, Palaiseau, France; [Geman, Donald] Johns Hopkins Univ, Baltimore, MD USA	Inria; Johns Hopkins University	Sfar, AR (corresponding author), INRIA Saclay, Palaiseau, France.	asma.rejeb_sfar@inria.fr; nozha.boujemaa@inria.fr; geman@jhu.edu						Angelova A., 2013, CVPR; Belhumeur PN, 2008, LECT NOTES COMPUT SC, V5305, P116, DOI 10.1007/978-3-540-88693-8_9; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32; Burl MC, 1998, PROC CVPR IEEE, P535, DOI 10.1109/CVPR.1998.698657; Caballero C., 2010, P ACM INT C IM VID R, P327, DOI [10.1145/1816041.1816089, DOI 10.1145/1816041.1816089]; Casanova D., 2012, CLEF; Casanova D., 2011, CLEF; Cook N. R., 2005, CONFIDENCE INTERVALS; Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073; Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6; Du JX, 2005, LECT NOTES COMPUT SC, V3497, P281; Duan K, 2012, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2012.6248089; El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605; Elpel T., 2004, BOT DAY PATTERNS MET; Fan XD, 2004, INT C PATT RECOG, P65, DOI 10.1109/ICPR.2004.1334470; Farrell R, 2011, IEEE I CONF COMP VIS, P161, DOI 10.1109/ICCV.2011.6126238; Fei-Fei L, 2005, PROC CVPR IEEE, P524; FELZENSZWALB PF, 2007, CVPR; FERECATU M, 2005, THESIS U VERSAILLES; Fergus R, 2010, LECT NOTES COMPUT SC, V6311, P762, DOI 10.1007/978-3-642-15549-9_55; Fernandez A, 2008, J CLASSIF, V25, P43, DOI 10.1007/s00357-008-9004-x; Goeau H., 2012, CLEF; Goeau H, 2011, CLEF; Grall-Maes E, 2009, IEEE T PATTERN ANAL, V31, P2073, DOI 10.1109/TPAMI.2008.239; Gu X, 2005, LECT NOTES COMPUT SC, V3644, P253; GUPTA SS, 1965, TECHNOMETRICS, V7, P225, DOI 10.2307/1266672; Ha TM, 1997, IEEE T PATTERN ANAL, V19, P608, DOI 10.1109/34.601248; Horiuchi T, 1998, PATTERN RECOGN, V31, P1579, DOI 10.1016/S0031-3203(97)00136-2; Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81; Jia D, 2012, PROC CVPR IEEE, P3450, DOI 10.1109/CVPR.2012.6248086; del Coz JJ, 2009, J MACH LEARN RES, V10, P2273; Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lee P., 1989, BAYEISAN STAT INTRO, V2; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13; Manh AG, 2001, J AGR ENG RES, V80, P139, DOI 10.1006/jaer.2001.0725; Martinez-Munoz G, 2009, PROC CVPR IEEE, P549, DOI 10.1109/CVPRW.2009.5206574; Mouine S., 2013, P 3 ACM C INT C MULT, P127; Neyman J., 1937, PHILOS T R SOC A, V236, P333, DOI [10.1098/rsta.1937.0005, DOI 10.1098/RSTA.1937.0005]; Nilsback M-E., 2006, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2006., DOI 10.1109/CVPR.2006.42]; Otsu N., 1979, MAN CYBERNETICS IEEE; Rejeb Sfar A., 2013, P 3 ACM C INT C MULT, P191, DOI DOI 10.1145/2461466.2461499; Rothe R., 2014, ICCV, DOI DOI 10.1109/ICCV.2007.4409064; Sfar AR, 2013, PROC CVPR IEEE, P835, DOI 10.1109/CVPR.2013.113; SODERKVIST OJO, 2001, THESIS LINKOPING U L; Teng CH, 2009, LECT NOTES COMPUT SC, V5627, P937, DOI 10.1007/978-3-642-02611-9_92; Tversky B., 1984, EXPT PSYCHOL GEN; Wah C, 2011, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2011.6126539; Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018; Wang XF, 2005, LECT NOTES COMPUT SC, V3644, P87; Wang XF, 2008, APPL MATH COMPUT, V205, P916, DOI 10.1016/j.amc.2008.05.108; WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967; Wu J., 2008, CVPR; Wu S. G., 2007, CORR; Yang S., 2012, P ADV NEUR INF PROC, P3131; Yao BP, 2012, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2012.6248088; Yuan M, 2010, J MACH LEARN RES, V11, P111; Zhang N, 2012, PROC CVPR IEEE, P3665, DOI 10.1109/CVPR.2012.6248364	62	11	12	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2015	111	3					255	275		10.1007/s11263-014-0743-3	http://dx.doi.org/10.1007/s11263-014-0743-3			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CB9XK					2022-12-18	WOS:000349987400001
J	Pedersoli, M; Timofte, R; Tuytelaars, T; Van Gool, L				Pedersoli, Marco; Timofte, Radu; Tuytelaars, Tinne; Van Gool, Luc			An Elastic Deformation Field Model for Object Detection and Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Deformation field; Tracking; Deformable Part Models; Latent SVM	RECOGNITION; INFERENCE; MRFS	Deformable Parts Models (DPM) are the current state-of-the-art for object detection. Nevertheless they seem sub-optimal in the representation of deformations. Object deformations are often continuous and not confined to big parts. Therefore we propose to replace the DPM star model based on big parts by a deformation field. This consists of a grid of small parts connected with pairwise constraints which can better handle continuous deformations. The naive application of this model for object detection would consist of a bounded sliding window approach: for each possible location of the image the best part configuration within a limited bound around this location is found. This is computationally very expensive. Instead, we propose a different inference procedure, where an iterative image-level search finds the best object hypothesis. We show that this approach is faster than bounded sliding windows yet produces comparable accuracy. Experiments further show that the deformation field can better approximate real object deformations and therefore, for certain classes, produces even better detection accuracy than state-of-the-art DPM. Finally, the same approach is adapted to model-free tracking, showing improved accuracy also in this case.	[Pedersoli, Marco; Timofte, Radu; Tuytelaars, Tinne; Van Gool, Luc] Katholieke Univ Leuven, ESAT PSI VISICS iMinds, B-3001 Leuven, Belgium	IMEC; KU Leuven	Pedersoli, M (corresponding author), Katholieke Univ Leuven, ESAT PSI VISICS iMinds, Kasteelpk Arenberg 10, B-3001 Leuven, Belgium.	marco.pedersoli@esat.kuleuven.be	Tuytelaars, Tinne/B-4319-2015; Timofte, Radu/AAK-6022-2021	Tuytelaars, Tinne/0000-0003-3307-9723; Timofte, Radu/0000-0002-1478-0402; Pedersoli, Marco/0000-0002-7601-8640	Toyota Motor Corporation; FP7 ERC Starting Grant [240530 COGN-IMUND]	Toyota Motor Corporation; FP7 ERC Starting Grant	This work was partially supported by Toyota Motor Corporation and FP7 ERC Starting Grant 240530 COGN-IMUND.	Alahari K, 2010, IEEE T PATTERN ANAL, V32, P1846, DOI 10.1109/TPAMI.2009.194; Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754; [Anonymous], 2007, PASCAL VISUAL OBJECT; Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226; BATRA D, 2012, P EUR C COMP VIS; Bergtholdt Martin, 2010, International Journal of Computer Vision, V87, P93, DOI 10.1007/s11263-009-0209-1; Bourdev L, 2010, LECT NOTES COMPUT SC, V6316, P168, DOI 10.1007/978-3-642-15567-3_13; Boykov Y, 1998, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.1998.698673; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Crandall D, 2005, PROC CVPR IEEE, P10; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Desai Chaitanya, 2009, P IEEE INT C COMP VI; Duchenne O., 2011, GRAPH MATCHING KERNE, P1056; Felzenszwalb P., 2004, TECHNICAL REPORT; Felzenszwalb P, 2008, 2008 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2008.4587597; Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906; Fergus R, 2003, PROC CVPR IEEE, P264; Glocker B, 2008, MED IMAGE ANAL, V12, P731, DOI 10.1016/j.media.2008.03.006; Hoeim D., 2008, P IEEE C COMP VIS PA; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231; Kapoor A, 2006, LECT NOTES COMPUT SC, V3953, P302, DOI 10.1007/11744078_24; Kohli P, 2007, IEEE T PATTERN ANAL, V29, P2079, DOI 10.1109/TPAMI.2007.1128; Komodakis N, 2008, COMPUT VIS IMAGE UND, V112, P14, DOI 10.1016/j.cviu.2008.06.007; LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173; Ladicky L., 2012, P BRIT MACH VIS C, P101, DOI DOI 10.1-10.11; Ladicky L, 2010, LECT NOTES COMPUT SC, V6314, P424, DOI 10.1007/978-3-642-15561-1_31; Li Z., 2010, INT C LOG ENG INT TR, P1, DOI [10.1109/LEITS.2010.5664931, DOI 10.1109/LEITS.2010.5664931]; Lucas B.D., 1981, IJCAI 81 P 7 INT JOI, P674, DOI DOI 10.1109/HPDC.2004.1323531; Pedersoli M, 2011, PROC CVPR IEEE, P1353, DOI 10.1109/CVPR.2011.5995668; Quattoni A., 2004, PROC ADV NEURAL INF, V17, P1097; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Vedaldi A., 2009, P ADV NEUR INF PROC, P1928; Vedaldi A, 2012, PROC CVPR IEEE, P2320, DOI 10.1109/CVPR.2012.6247943; Wang Y, 2011, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR.2011.5995519; Yang Y., 2012, MAN IN THE MIDDLE AT, DOI 10.1049/cp.2012.1831; Yuille AL, 2002, ADV NEUR IN, V14, P1033; Zhang L.T., 2013, IEEE MTT S INT MICRO, P1, DOI DOI 10.1109/IMW.2013.6582122	39	11	13	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2015	111	2					137	152		10.1007/s11263-014-0736-2	http://dx.doi.org/10.1007/s11263-014-0736-2			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AZ6RD		Green Accepted, Green Submitted			2022-12-18	WOS:000348345500001
J	Kapoor, A; Caicedo, JC; Lischinski, D; Kang, SB				Kapoor, Ashish; Caicedo, Juan C.; Lischinski, Dani; Kang, Sing Bing			Collaborative Personalization of Image Enhancement	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image enhancement; Personalization; Collaborative filtering; Crowdsourcing		This paper presents methods for personalization of image enhancement, which could be deployed in photo editing software and also in cloud-based image sharing services. We observe that users do have different preferences for enhancing images and that there are groups of people that share similarities in preferences. Our goal is to predict enhancements for novel images belonging to a particular user based on her specific taste, to facilitate the retouching process on large image collections. To that end, we describe an enhancement framework that can learn user preferences in an individual or collaborative way. The proposed system is based on a novel interactive application that allows to collect user's enhancement preferences. We propose algorithms to predict personalized enhancements by learning a preference model from the provided information. Furthermore, the algorithm improves prediction performance as more enhancement examples are progressively added. We conducted experiments via Amazon Mechanical Turk to collect preferences from a large group of people. Results show that the proposed framework can suggest image enhancements more targeted to individual users than commercial tools with global auto-enhancement functionalities.	[Kapoor, Ashish; Kang, Sing Bing] Microsoft Res, Redmond, WA 98052 USA; [Caicedo, Juan C.] Univ Illinois, Urbana, IL 61801 USA; [Lischinski, Dani] Hebrew Univ Jerusalem, IL-91904 Jerusalem, Israel	Microsoft; University of Illinois System; University of Illinois Urbana-Champaign; Hebrew University of Jerusalem	Kapoor, A (corresponding author), Microsoft Res, One Microsoft Way, Redmond, WA 98052 USA.	akapoor@microsoft.com; caicedo@illinois.edu; danix@mail.huji.ac.il; sbkang@microsoft.com		Lischinski, Dani/0000-0002-6191-0361				BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7; Bychkovsky V., 2011, CVPR 11; Caicedo J., 2011, CVPR 11; Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Dale K, 2009, IEEE I CONF COMP VIS, P2217, DOI 10.1109/ICCV.2009.5459473; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Farid H, 2001, J OPT SOC AM A, V18, P2072, DOI 10.1364/JOSAA.18.002072; Farid H, 2001, IEEE T IMAGE PROCESS, V10, P1428, DOI 10.1109/83.951529; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Fogarty J., 2008, C HUM FACT COMP SYST; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Gehler PV, 2008, PROC CVPR IEEE, P3291; Gijsenij A., 2007, CVPR; Grabler F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531372; Hays J, 2008, COMMUN ACM, V51, P87, DOI 10.1145/1400181.1400202; Hsu E, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1399504.1360669, 10.1145/1360612.1360669]; Jain P., 2008, NIPS; Joshi N, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731050; Kang S. B., 2010, CVPR; Kang S. B., 2007, CVPR; Kapoor A., 2005, WORKSH MULT CLASS SY; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Krause A, 2008, J MACH LEARN RES, V9, P235; Lawrence N. D., 2009, P 26 ANN INT C MACH; Lin S, 2004, PROC CVPR IEEE, P938; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; Marks J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P389, DOI 10.1145/258734.258887; Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Salakhutdinov R, 2007, ADV NEURAL INFORM PR, V20, DOI 10.1145/1390156.1390267; Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI DOI 10.1145/371920.372071; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Shapira L., 2009, IMAGE APPEARANCE EXP; Stanikunas R, 2004, NEURAL NETWORKS, V17, P327, DOI 10.1016/j.neunet.2003.12.002; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tresp V., 2001, NEURAL INFORM PROCES; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; Wyszecki G., 2000, COLOR SCI CONCEPTS M, V2nd	40	11	13	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2014	108	1-2			SI		148	164		10.1007/s11263-013-0675-3	http://dx.doi.org/10.1007/s11263-013-0675-3			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AG7BT					2022-12-18	WOS:000335573700010
J	Ion, A; Carreira, J; Sminchisescu, C				Ion, Adrian; Carreira, Joao; Sminchisescu, Cristian			Probabilistic Joint Image Segmentation and Labeling by Figure-Ground Composition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image segmentation; Image labeling; Semantic segmentation; Statistical models; Learning and categorization	FEATURES; LAYOUT	We propose a layered statistical model for image segmentation and labeling obtained by combining independently extracted, possibly overlapping sets of figure-ground (FG) segmentations. The process of constructing consistent image segmentations, called tilings, is cast as optimization over sets of maximal cliques sampled from a graph connecting all non-overlapping figure-ground segment hypotheses. Potential functions over cliques combine unary, Gestalt-based figure qualities, and pairwise compatibilities among spatially neighboring segments, constrained by T-junctions and the boundary interface statistics of real scenes. Building on the segmentation layer, we further derive a joint image segmentation and labeling model (JSL) which, given a bag of FGs, constructs a joint probability distribution over both the compatible image interpretations (tilings) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as first sampling tilings, followed by sampling labelings conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on maximum likelihood with a novel estimation procedure we refer to as incremental saddle-point approximation. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that are rated as probable by candidate models during learning. State of the art results are reported on the Berkeley, Stanford and Pascal VOC datasets, where an improvement of 28 % was achieved for the segmentation task only (tiling), and an accuracy of 47.8 % was obtained on the test set of VOC12 for semantic labeling (JSL).	[Ion, Adrian] Vienna Univ Technol, Fac Informat, A-1040 Vienna, Austria; [Carreira, Joao] Univ Coimbra, Inst Syst & Robot, Coimbra, Portugal; [Sminchisescu, Cristian] Lund Univ, Fac Engn, Dept Math, Lund, Sweden; [Sminchisescu, Cristian] Romanian Acad, Inst Math, Bucharest, Romania	Technische Universitat Wien; Universidade de Coimbra; Lund University; Institute of Mathematics of the Romanian Academy; Romanian Academy of Sciences; University of Bucharest	Sminchisescu, C (corresponding author), Lund Univ, Fac Engn, Dept Math, Lund, Sweden.	ion@prip.tuwien.ac.at; joaoluis@isr.uc.pt; cristian.sminchisescu@math.lth.se			CNCS-UEFICSDI [PCE-2011-3-0438, CT-ERC-2012-1]; FCT [PTDC/EEA-CRO/122812/2010]	CNCS-UEFICSDI(Consiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii (UEFISCDI)); FCT(Portuguese Foundation for Science and TechnologyEuropean Commission)	This work was supported, in part, by CNCS-UEFICSDI, under PCE-2011-3-0438, and CT-ERC-2012-1, and by FCT under PTDC/EEA-CRO/122812/2010. The authors thank the anonymous reviewers for their useful comments and suggestions.	Arbelaez P., 2012, IEEE INT C COMP VIS; Arbelaez P., 2009, IEEE INT C COMP VIS; Bagon S., 2008, EUR C COMP VIS; Barbu A, 2005, IEEE T PATTERN ANAL, V27, P1239, DOI 10.1109/TPAMI.2005.161; Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214; Bomze IM, 2000, IEEE T NEURAL NETWOR, V11, P1228, DOI 10.1109/72.883403; Bomze ImmanuelM., 1999, HDB COMBINATORIAL OP, P1; Brendel W., 2010, ADV NEURAL INFORM PR; Carreira J., 2012, EUR C COMP VIS; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Carreira J, 2012, INT J COMPUT VISION, V98, P243, DOI 10.1007/s11263-011-0507-2; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Cour T., 2005, ARTIFICIAL INTELLIGE; Csurka G., 2010, INT J COMPUT VISION, P1; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dann C., 2012, P DAGM OAGM S; Endres I., 2010, EUR C COMP VIS; Everingham M., 2012, PASCAL VISUAL OBJECT; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Fulkerson B., 2009, IEEE I CONF COMP VIS, P670, DOI [10.1109/ICCV.2009.5459175, DOI 10.1109/ICCV.2009.5459175]; Ghose T., 2005, JV, V5, P970; Gonfaus J. M., 2010, IEEE INT C COMP VIS; Gould S., 2009, ADV NEURAL INFORM PR; Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; HE X, 2004, IEEE INT C COMP VIS; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y; Huggins P., 2001, IEEE INT C COMP VIS; Ion A., 2011, ADV NEURAL INFORM PR; Ion A, 2011, IEEE I CONF COMP VIS, P2110, DOI 10.1109/ICCV.2011.6126486; Kohli P., 2008, IEEE INT C COMP VIS; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kumar M. P., 2010, IEEE INT C COMP VIS; Kumar S., 2005, ENERGY MINIMIZATION; Ladicky L., 2010, EUR C COMP VIS; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; Leichter I., 2009, IEEE INT C COMP VIS; Li F., 2010, P DAGM S; Lim J., 2009, IEEE INT C COMP VIS; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Malisiewicz T., 2007, BRIT MACH VIS C; Martin D., 2001, IEEE INT C COMP VIS; Nowozin S., 2010, EUR C COMP VIS; PANTOFARU C, 2008, EUR C COMP VIS; Rahimi A., 2007, ADV NEURAL INFORM PR; Ren X., 2006, EUR C COMP VIS; Sarawagi S., 2004, ADV NEURAL INFORM PR; Sharon E, 2006, NATURE, V442, P810, DOI 10.1038/nature04977; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Tu Z., 2003, IEEE INT C COMP VIS; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; Xia W., 2012, EUR C COMP VIS; Yang Y, 2012, IEEE T PATTERN ANAL, V34, P1731, DOI 10.1109/TPAMI.2011.208; [No title captured]	58	11	12	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2014	107	1					40	57		10.1007/s11263-013-0663-7	http://dx.doi.org/10.1007/s11263-013-0663-7			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AB2RT					2022-12-18	WOS:000331640500003
J	Hartley, R; Kahl, F; Olsson, C; Seo, Y				Hartley, Richard; Kahl, Fredrik; Olsson, Carl; Seo, Yongduek			Verifying Global Minima for L-2 Minimization Problems in Multiple View Geometry	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Geometric optimization; Reconstruction; Convex programming	RECONSTRUCTION; OPTIMIZATION; ALGORITHM	We consider the least-squares (L-2) minimization problems in multiple view geometry for triangulation, homography, camera resectioning and structure -and-motion with known rotation, or known plane. Although optimal algorithms have been given for these problems under an L-infinity cost function, finding optimal least-squares solutions to these problems is difficult, since the cost functions are not convex, and in the worst case may have multiple minima. Iterative methods can be used to find a good solution, but this may be a local minimum. This paper provides a method for verifying whether a local-minimum solution is globally optimal, by providing a simple and rapid test involving the Hessian of the cost function. The basic idea is that by showing that the cost function is convex in a restricted but large enough neighbourhood, a sufficient condition for global optimality is obtained. The method is tested on numerous problem instances of real data sets. In the vast majority of cases we are able to verify that the solutions are optimal, in particular, for small to medium-scale problems.	[Hartley, Richard] Australian Natl Univ, Canberra, ACT, Australia; [Hartley, Richard] NICTA, Canberra, ACT, Australia; [Kahl, Fredrik; Olsson, Carl] Lund Univ, Ctr Math Sci, Lund, Sweden; [Seo, Yongduek] Sogang Univ, Dept Media Technol, Seoul, South Korea	Australian National University; Australian National University; Lund University; Sogang University	Hartley, R (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.	Richard.Hartley@anu.edu.au		SEO, Yongduek/0000-0002-0570-2197; Hartley, Richard/0000-0002-5005-0191	NICTA; Australian Government; Australian Research Council through the ICT Centre of Excellence; Swedish Research Council [2007-6476]; Swedish Foundation for Strategic Research (SSF; Wearable Visual Information Systems; European Research Council [209480]; Sogang University [201210029.1]	NICTA(NICTA); Australian Government(Australian GovernmentCGIAR); Australian Research Council through the ICT Centre of Excellence(Australian Research Council); Swedish Research Council(Swedish Research CouncilEuropean Commission); Swedish Foundation for Strategic Research (SSF(Swedish Foundation for Strategic Research); Wearable Visual Information Systems; European Research Council(European Research Council (ERC)European Commission); Sogang University	This research was supported by (i) NICTA, a research centre funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy, (ii) the Australian Research Council through the ICT Centre of Excellence program, (iii) the Swedish Research Council (Grant No. 2007-6476), (iv) the Swedish Foundation for Strategic Research (SSF) through the programmes Future Research Leaders and Wearable Visual Information Systems, (v) the European Research Council (GlobalVision Grant No. 209480), and (vi) Sogang University Research Grant of 2012 (201210029.1).	Hartley R, 2004, PROC CVPR IEEE, P504; Hartley R., 2004, ROBOTICA; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Hartley RI, 1998, INT J COMPUT VISION, V26, P41, DOI 10.1023/A:1007984508483; Kahl F., 2007, AS C COMP VIS TOK JA; Kahl F, 2008, IEEE T PATTERN ANAL, V30, P1603, DOI 10.1109/TPAMI.2007.70824; Kahl F, 2008, INT J COMPUT VISION, V79, P271, DOI 10.1007/s11263-007-0117-1; Ke Q, 2007, IEEE T PATTERN ANAL, V29, P1834, DOI 10.1109/TPAMI.2007.1083; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Lu FF, 2007, LECT NOTES COMPUT SC, V4844, P279; Nister D., 2007, C COMP VIS PATT REC; Nister D., 2007, INT C COMP VIS RIO D; Olsson C., 2009, SCAND C IM AN OSL NO; Rother C, 2002, INT J COMPUT VISION, V49, P117, DOI 10.1023/A:1020189404787; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Stewenius H, 2005, IEEE I CONF COMP VIS, P686; Vedaldi A., 2007, C COMP VIS PATT REC	17	11	13	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	2					288	304		10.1007/s11263-012-0569-9	http://dx.doi.org/10.1007/s11263-012-0569-9			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	081BW					2022-12-18	WOS:000314291600004
J	Direkoglu, C; Dahyot, R; Manzke, M				Direkoglu, Cem; Dahyot, Rozenn; Manzke, Michael			On Using Anisotropic Diffusion for Skeleton Extraction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Skeletonization; Feature extraction; Heat flow; Computer vision	SHAPE	We present a novel and effective skeletonization algorithm for binary and gray-scale images, based on the anisotropic heat diffusion analogy. We diffuse the image in the direction normal to the feature boundaries and also allow tangential diffusion (curvature decreasing diffusion) to contribute slightly. The proposed anisotropic diffusion provides a high quality medial function in the image: it removes noise and preserves prominent curvatures of the shape along the level-sets (skeleton features). The skeleton strength map, which provides the likelihood of a point to be part of the skeleton, is defined by the mean curvature measure. Finally, thin and binary skeleton is obtained by non-maxima suppression and hysteresis thresholding of the skeleton strength map. Our method outperforms the most related and the popular methods in skeleton extraction especially in noisy conditions. Results show that the proposed approach is better at handling noise in images and preserving the skeleton features at the centerline of the shape.	[Direkoglu, Cem; Dahyot, Rozenn; Manzke, Michael] Trinity Coll Dublin, Sch Comp Sci & Stat, Dublin, Ireland	Trinity College Dublin	Direkoglu, C (corresponding author), Trinity Coll Dublin, Sch Comp Sci & Stat, Dublin, Ireland.	cdirekoglu@googlemail.com; rozenn.dahyot@scss.tcd.ie; michael.manzke@cs.tcd.ie	Direkoglu, Cem/H-2893-2013; Dahyot, Rozenn/AAN-4260-2020	Direkoglu, Cem/0000-0001-7709-4082; Dahyot, Rozenn/0000-0003-0983-3052; Manzke, Michael/0000-0002-2183-8318	Sony-Toshiba-IBM [IP-2007-505]; Enterprise Ireland [IP-2007-505]	Sony-Toshiba-IBM; Enterprise Ireland	This work is supported by an Innovation Partnership between Sony-Toshiba-IBM and Enterprise Ireland (IP-2007-505) and forms part of Trinity's Sony-Toshiba-IBM European Cell/B.E. Center of Competence.	ARCELLI C, 1992, PATTERN RECOGN LETT, V13, P237, DOI 10.1016/0167-8655(92)90074-A; Aslan C, 2008, IEEE T PATTERN ANAL, V30, P2188, DOI 10.1109/TPAMI.2007.70842; Bai X, 2007, IEEE T PATTERN ANAL, V29, P449, DOI 10.1109/TPAMI.2007.59; Blum H., 1967, MODELS PERCEPTION SP, P363; Direkoglu C., 2009, THESIS U SOUTHAMPTON; Direkoglu C., 2010, P BRIT MACH VIS C, P611, DOI [10.5244/C.24.61, DOI 10.5244/C.24.61]; Direkoglu C, 2007, LECT NOTES COMPUT SC, V4678, P553; Gorelick L, 2006, IEEE T PATTERN ANAL, V28, P1991, DOI 10.1109/TPAMI.2006.253; Hassouna MS, 2005, PROC CVPR IEEE, P458; Hummel R. A., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P204; KIMIA BB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P113, DOI 10.1109/CVPR.1994.323817; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P382, DOI 10.1006/cviu.1995.1062; Krinidis S, 2009, IEEE T IMAGE PROCESS, V18, P1, DOI 10.1109/TIP.2008.2007351; LAM L, 1992, IEEE T PATTERN ANAL, V14, P869, DOI 10.1109/34.161346; Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850; Le Bourgeois F., 2007, P IEEE INT C IM PROC, V3, P33; Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773; Macrini D, 2008, P IEEE C COMP VIS PA, P1; Malandain G, 1998, IMAGE VISION COMPUT, V16, P317, DOI 10.1016/S0262-8856(97)00074-7; Manay S, 2003, IEEE T IMAGE PROCESS, V12, P1310, DOI 10.1109/TIP.2003.818039; OGNIEWICZ RL, 1995, PATTERN RECOGN, V28, P343, DOI 10.1016/0031-3203(94)00105-U; Shen W, 2011, PATTERN RECOGN, V44, P196, DOI 10.1016/j.patcog.2010.08.021; Siddiqi K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P828, DOI 10.1109/ICCV.1999.790307; Tari ZSG, 1997, COMPUT VIS IMAGE UND, V66, P133, DOI 10.1006/cviu.1997.0612; Ursell T. S., 2007, DIFFUSION EQUATION M; Ward AD, 2010, IEEE T PATTERN ANAL, V32, P1084, DOI 10.1109/TPAMI.2009.81; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186; Yu ZY, 2004, PROC CVPR IEEE, P415; [No title captured]	31	11	14	0	24	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2012	100	2					170	189		10.1007/s11263-012-0540-9	http://dx.doi.org/10.1007/s11263-012-0540-9			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	000CQ		Green Accepted			2022-12-18	WOS:000308364500005
J	Veksler, O				Veksler, Olga			Multi-label Moves for MRFs with Truncated Convex Priors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Discrete optimization; Markov random fields (MRF); Graph cuts; Truncated convex priors	MARKOV RANDOM-FIELDS; ENERGY MINIMIZATION; STEREO; IMAGE	Optimization with graph cuts became very popular in recent years. While exact optimization is possible in a few cases, many useful energy functions are NP hard to optimize. One approach to approximate optimization is the so-called move making algorithms. At each iteration, a move-making algorithm makes a proposal (move) for a pixel p to either keep its old label or switch to a new label. Two move-making algorithms based on graph cuts are in wide use, namely the swap and expansion. Both of these moves are binary in nature, that is they give each pixel a choice of only two labels. An evaluation of optimization techniques shows that the expansion and swap algorithms perform very well for energies where the underlying MRF has the Potts prior. However for more general priors, the swap and expansion algorithms do not perform as well. The main contribution of this paper is to develop multi-label moves. A multi-label move, unlike expansion and swap, gives each pixel has a choice of more than two labels to switch to. In particular, we develop several multi-label moves for truncated convex priors. We evaluate our moves on image restoration, inpainting, and stereo correspondence. We get better results than expansion and swap algorithms, both in terms of the energy value and accuracy.	Univ Western Ontario, Dept Comp Sci, London, ON, Canada	Western University (University of Western Ontario)	Veksler, O (corresponding author), Univ Western Ontario, Dept Comp Sci, London, ON, Canada.	olga@csd.uwo.ca	Veksler, Olga/B-6549-2015	Veksler, Olga/0000-0002-9664-6601	NSERC; CFI; ERA	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CFI(Canada Foundation for Innovation); ERA	This research was supported in part by NSERC, CFI, ERA.	Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718; Agarwala A, 2006, ACM T GRAPHIC, V25, P853, DOI 10.1145/1141911.1141966; Alahari K., 2008, P IEEE C COMP VIS PA, P1; BESAG J, 1986, J R STAT SOC B, V48, P259; Blake A, 2004, LECT NOTES COMPUT SC, V3021, P428; Boykov Y, 1998, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.1998.698673; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Boykov Y.Y., 2001, ICCV, V1, P105, DOI DOI 10.1109/ICCV.2001.937505; Carr P, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P532, DOI 10.1109/DICTA.2009.90; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gould S, 2009, PROC CVPR IEEE, P903, DOI 10.1109/CVPRW.2009.5206650; Ishikawa H., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P232, DOI 10.1007/BFb0055670; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82; Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1480, DOI 10.1109/TPAMI.2006.193; Kolmogorov V, 2006, LECT NOTES COMPUT SC, V3952, P1; Kolmogorov V, 2009, DISCRETE OPTIM, V6, P378, DOI 10.1016/j.disopt.2009.04.006; Kumar MP, 2011, J MACH LEARN RES, V12, P31; Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Liu X., 2008, CVPR, P1; Nguyen MH, 2008, COMPUT GRAPH FORUM, V27, P627, DOI 10.1111/j.1467-8659.2008.01160.x; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rother C, 2005, PROC CVPR IEEE, P589; Rother C., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91; Roy S, 2000, INT C PATT RECOG, P1041, DOI 10.1109/ICPR.2000.903724; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scharstein D, 2003, PROC CVPR IEEE, P195; Schlesinger D., 2006, TUDFI0601 DRESD U TE; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Torr P., 2009, ADV NEURAL INFORM PR, P889; Torr P. H. S., 2008, COMMUNICATION; VEKSLER O, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383249; Veksler O, 2009, LECT NOTES COMPUT SC, V5681, P1, DOI 10.1007/978-3-642-03641-5_1; Wills J, 2003, PROC CVPR IEEE, P37	39	11	11	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2012	98	1					1	14		10.1007/s11263-011-0491-6	http://dx.doi.org/10.1007/s11263-011-0491-6			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	919PY		Green Submitted			2022-12-18	WOS:000302341900001
J	Ng, TT; Pahwa, RS; Bai, JM; Tan, KH; Ramamoorthi, R				Ng, Tian-Tsong; Pahwa, Ramanpreet Singh; Bai, Jiamin; Tan, Kar-Han; Ramamoorthi, Ravi			From the Rendering Equation to Stratified Light Transport Inversion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Inverse light transport; Projector radiometric compensation; Matrix inversion; Inter-reflection cancellation; Rendering equation		Recent advances in fast light transport acquisition have motivated new applications for forward and inverse light transport. While forward light transport enables image relighting, inverse light transport provides new possibilities for analyzing and cancelling interreflections, to enable applications like projector radiometric compensation and light bounce separation. With known scene geometry and diffuse reflectance, inverse light transport can be easily derived in closed form. However, with unknown scene geometry and reflectance properties, we must acquire and invert the scene's light transport matrix to undo the effects of global illumination. For many photometric setups such as that of a projector-camera system, the light transport matrix often has a size of 10(5)x10(5) or larger. Direct matrix inversion is accurate but impractical computationally at these resolutions. In this work, we explore a theoretical analysis of inverse light transport, relating it to its forward counterpart, expressed in the form of the rendering equation. It is well known that forward light transport has a Neumann series that corresponds to adding bounces of light. In this paper, we show the existence of a similar inverse series, that zeroes out the corresponding physical bounces of light. We refer to this series solution as stratified light transport inversion, since truncating to a certain number of terms corresponds to cancelling the corresponding interreflection bounces. The framework of stratified inversion is general and may provide insight for other problems in light transport and beyond, that involve large-size matrix inversion. It is also efficient, requiring only sparse matrix-matrix multiplications. Our practical application is to radiometric compensation, where we seek to project patterns onto real-world surfaces, undoing the effects of global illumination. We use stratified light transport inversion to efficiently invert the acquired light transport matrix for a static scene, after which interreflection cancellation is a simple matrix-vector multiplication to compensate the input image for projection.	[Ng, Tian-Tsong] Inst Infocomm Res, Singapore 128632, Singapore; [Pahwa, Ramanpreet Singh] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; [Bai, Jiamin; Ramamoorthi, Ravi] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA; [Tan, Kar-Han] HP Labs, Palo Alto, CA 94304 USA	Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R); University of Illinois System; University of Illinois Urbana-Champaign; University of California System; University of California Berkeley; Hewlett-Packard	Ng, TT (corresponding author), Inst Infocomm Res, 1 Fusionopolis Way,21-01 Connexis, Singapore 128632, Singapore.	ttng@i2r.a-star.edu.sg		Ng, Tian Tsong/0000-0002-6713-2310; Tan, Kar Han/0000-0001-9294-2932	A*STAR Graduate Academy of Singapore; ONR [N00014-10-1-0032, N00014-09-1-0741]; NSF [09-24968]	A*STAR Graduate Academy of Singapore(Agency for Science Technology & Research (A*STAR)); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	We are grateful to Tony Quee-Seng Quek for giving feedback on an earlier draft of the manuscript. We wish to thank Manmohan Chandraker and Mao-Pei Tsui for their great suggestions on various technical issues. We would also like to thank Ya-suyuki Matsushita, Jiaping Wang and Yue Dong of Microsoft Research Asia for discussion and suggestions on experimental techniques. Finally, we would like to thank Zhiyong Huang and Joo Hwee Lim for their support on this project. Ramanpreet Singh Pahwa and Jiamin Bai are supported by National Science Scholarships awarded by A*STAR Graduate Academy of Singapore. Ravi Ramamoorthi is supported by ONR YIP grant N00014-10-1-0032, as well as NSF CAREER Award 09-24968 and ONR PECASE grant N00014-09-1-0741.	Arvo J., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P75, DOI 10.1145/192161.192179; Ashdown M., 2006, P IEEE INT WORKSH PR; Bai J., 2010, P EUR C COMP VIS; Bimber O, 2005, COMPUTER, V38, P48, DOI 10.1109/MC.2005.17; Bimber O, 2006, IDEA GROUP PUB, P64; Bimber O, 2006, P IEEE VIRT REAL ANN, P151, DOI 10.1109/VR.2006.34; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; Ding Y., 2009, P COMP VIS PATT REC; Fujii K., 2005, P COMP VIS PATT REC; Gortler S. J., 1993, Computer Graphics Proceedings, P221, DOI 10.1145/166117.166146; Habe H., 2007, P COMP VIS PATT REC; HANRAHAN P, 1991, COMP GRAPH, V25, P197; Kajiya J.T., 1986, SIGGRAPH, P143, DOI [DOI 10.1145/15922.15902, 10.1145/15886.15902, DOI 10.1145/15886.15902]; Langville A. N., 2003, CRSCTR0334 N CAR STA; Liu S., 2010, P EUR C COMP VIS; Majumder A., 1999, P ACM INT C MULT; Marschner S. R, 1998, THESIS CORNELL U; Masselus V, 2003, ACM T GRAPHIC, V22, P613, DOI 10.1145/882262.882315; Mukaigawa Y., 2006, P ACM S VIRT REAL SO, P268; Nayar S., 2003, P IEEE INT WORKSH PR; Nayar SK, 2006, ACM T GRAPHIC, V25, P935, DOI 10.1145/1141911.1141977; Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280; Ng T.-T., 2009, P INT C COMP VIS; O'Toole M., 2010, P ACM SIGGRAPH AS; Peers P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477929; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; Ramamoorthi R, 2007, FOUND TRENDS COMPUT, V3, P1, DOI 10.1561/0600000021; Raskar R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P179, DOI 10.1145/280814.280861; Raskar R., 1999, SPATIALLY AUGMENTED; Seitz SM, 2005, IEEE I CONF COMP VIS, P1440; Sen P, 2005, ACM T GRAPHIC, V24, P745, DOI 10.1145/1073204.1073257; Sen P, 2009, COMPUT GRAPH FORUM, V28, P609, DOI 10.1111/j.1467-8659.2009.01401.x; Song P., 2005, P IEEE INT WORKSH PR; Wang JP, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531335; Wetzstein G, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P391, DOI 10.1109/PG.2007.47; Yang R., 2004, P APPL COMP VIS WORK; Yu YZ, 1999, COMP GRAPH, P215	37	11	12	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2012	96	2					235	251		10.1007/s11263-011-0467-6	http://dx.doi.org/10.1007/s11263-011-0467-6			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	876AW					2022-12-18	WOS:000299080200006
J	Uijlings, JRR; Smeulders, AWM; Scha, RJH				Uijlings, J. R. R.; Smeulders, A. W. M.; Scha, R. J. H.			The Visual Extent of an Object	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Content based image retrieval; Visual extent; Context	FEATURES; TEXTURE	The visual extent of an object reaches beyond the object itself. This is a long standing fact in psychology and is reflected in image retrieval techniques which aggregate statistics from the whole image in order to identify the object within. However, it is unclear to what degree and how the visual extent of an object affects classification performance. In this paper we investigate the visual extent of an object on the Pascal VOC dataset using a Bag-of-Words implementation with (colour) SIFT descriptors. Our analysis is performed from two angles. (a) Not knowing the object location, we determine where in the image the support for object classification resides. We call this the normal situation. (b) Assuming that the object location is known, we evaluate the relative potential of the object and its surround, and of the object border and object interior. We call this the ideal situation. Our most important discoveries are: (i) Surroundings can adequately distinguish between groups of classes: furniture, animals, and land-vehicles. For distinguishing categories within one group the surroundings become a source of confusion. (ii) The physically rigid plane, bike, bus, car, and train classes are recognised by interior boundaries and shape, not by texture. The non-rigid animals dog, cat, cow, and sheep are recognised primarily by texture, i.e. fur, as their projected shape varies greatly. (iii) We confirm an early observation from human psychology (Biederman in Perceptual Organization, pp. 213-263, 1981): in the ideal situation with known object locations, recognition is no longer improved by considering surroundings. In contrast, in the normal situation with unknown object locations, the surroundings significantly contribute to the recognition of most classes.	[Uijlings, J. R. R.; Smeulders, A. W. M.] Inst Informat, ISIS Lab, NL-1098 XG Amsterdam, Netherlands; [Scha, R. J. H.] Inst Log Language & Computat, Amsterdam, Netherlands		Uijlings, JRR (corresponding author), Inst Informat, ISIS Lab, Sci Pk 107, NL-1098 XG Amsterdam, Netherlands.	JRR.Uijlings@uva.nl						Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108; Bar M, 2004, NAT REV NEUROSCI, V5, P617, DOI 10.1038/nrn1476; Biedermann I., 1981, PERCEPTUAL ORG, P213, DOI [10.4324/9781315512372-8, 10.4324/9781315512372]; Bishop C. M., 2006, INFOR MATION SCI STA, V2nd, DOI DOI 10.1117/1.2819119; Blaschko M., 2009, P BMVC; Burl M. C., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P628, DOI 10.1007/BFb0054769; Carbonetto P, 2004, LECT NOTES COMPUT SC, V3021, P350; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fergus R, 2003, PROC CVPR IEEE, P264; Fulkerson B., 2009, IEEE I CONF COMP VIS, P670, DOI [10.1109/ICCV.2009.5459175, DOI 10.1109/ICCV.2009.5459175]; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; Harzallah H, 2009, IEEE I CONF COMP VIS, P237, DOI 10.1109/ICCV.2009.5459257; Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5; Jiang Y G, 2007, P 6 ACM INT C IM VID, P494, DOI DOI 10.1145/1282280.1282352; Jurie F, 2005, IEEE I CONF COMP VIS, P604; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Maji S, 2008, PROC CVPR IEEE, P2245; Malisiewicz T., 2009, NIPS; Malisiewicz T., 2007, P BRIT MACH VIS C UK, DOI 10.5244/C.21.55; Marszalek M., 2007, ICCV PASC VOC 2007 C; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Moosmann F., 2006, NIPS, P985; Nedovic V, 2010, IEEE T PATTERN ANAL, V32, P1673, DOI 10.1109/TPAMI.2009.174; Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009; Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Singhal A, 2003, PROC CVPR IEEE, P235; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Smeaton A.F., 2006, MIR 2006 P 8 ACM INT, P321, DOI DOI 10.1145/1178677.1178722; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; Tahir M. A., 2008, ECCV PASC VOC 2008 C; Tuytelaars T, 2007, IEEE I CONF COMP VIS, P754; Uijlings JRR, 2009, PROC CVPR IEEE, P770, DOI 10.1109/CVPRW.2009.5206663; Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027; Ullah M., 2010, BRIT MACH VIS C; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; Vedaldi A, 2010, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2010.5539949; Wolf L, 2006, INT J COMPUT VISION, V69, P251, DOI 10.1007/s11263-006-7538-0; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4	46	11	12	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2012	96	1					46	63		10.1007/s11263-011-0443-1	http://dx.doi.org/10.1007/s11263-011-0443-1			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	872LF		Bronze, Green Published			2022-12-18	WOS:000298810300003
J	Hauberg, S; Pedersen, KS				Hauberg, Soren; Pedersen, Kim Steenstrup			Predicting Articulated Human Motion from Spatial Processes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion analysis; Articulated human motion; Articulated tracking; Prediction; Inverse kinematics; Particle filtering	TRACKING; SURFACE	We present a probabilistic interpretation of inverse kinematics and extend it to sequential data. The resulting model is used to estimate articulated human motion in visual data. The approach allows us to express the prior temporal models in spatial limb coordinates, which is in contrast to most recent work where prior models are derived in terms of joint angles. This approach has several advantages. First of all, it allows us to construct motion models in low dimensional spaces, which makes motion estimation more robust. Secondly, as many types of motion are easily expressed in spatial coordinates, the approach allows us to construct high quality application specific motion models with little effort. Thirdly, the state space is a real vector space, which allows us to use off-the-shelf stochastic processes as motion models, which is rarely possible when working with joint angles. Fourthly, we avoid the problem of accumulated variance, where noise in one joint affects all joints further down the kinematic chains. All this combined allows us to more easily construct high quality motion models. In the evaluation, we show that an activity independent version of our model is superior to the corresponding state-of-the-art model. We also give examples of activity dependent models that would be hard to phrase directly in terms of joint angles.	[Hauberg, Soren; Pedersen, Kim Steenstrup] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark	University of Copenhagen	Hauberg, S (corresponding author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.	hauberg@diku.dk; kimstp@diku.dk	Hauberg, Søren/L-2104-2016; Steenstrup Pedersen, Kim/L-8748-2016	Hauberg, Søren/0000-0001-7223-877X; Steenstrup Pedersen, Kim/0000-0003-3713-0960				ABEND W, 1982, BRAIN, V105, P331, DOI 10.1093/brain/105.2.331; [Anonymous], 1999, NUMERICAL OPTIMIZATI; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bishop C.M, 2006, PATTERN RECOGN; Bregler C, 2004, INT J COMPUT VISION, V56, P179, DOI 10.1023/B:VISI.0000011203.00237.9b; Cappe O, 2007, P IEEE, V95, P899, DOI 10.1109/JPROC.2007.893250; CARREIRAPERPINA.MA, 2007, J MACHINE LEARNING R, V2, P59; Courty N, 2008, LECT NOTES COMPUT SC, V5098, P1, DOI 10.1007/978-3-540-70517-8_1; Elgammal A, 2009, IEEE T PATTERN ANAL, V31, P520, DOI 10.1109/TPAMI.2008.101; Engell-Norregard M. P., 2010, COMPUTER GRAPHICS IN; ENGELLNORREGARD M, 2009, P VRIPHYS 09; Erleben K., 2005, PHYS BASED ANIMATION; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; GANESH S, 2009, THESIS U CALIFORNIA; Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755; Hauberg S, 2011, LECT NOTES COMPUT SC, V6494, P758, DOI 10.1007/978-3-642-19318-7_59; Hauberg S, 2010, LECT NOTES COMPUT SC, V6311, P425, DOI 10.1007/978-3-642-15549-9_31; Hauberg S, 2009, LECT NOTES COMPUT SC, V5681, P235, DOI 10.1007/978-3-642-03641-5_18; Herda L, 2004, LECT NOTES COMPUT SC, V3022, P405; Horaud R, 2009, IEEE T PATTERN ANAL, V31, P158, DOI 10.1109/TPAMI.2008.108; Kerlow I.V., 2003, ART 3D COMPUTER ANIM; KJELLSTROM H, 2010, CVPR 10; Knossow D, 2008, INT J COMPUT VISION, V79, P247, DOI 10.1007/s11263-007-0116-2; LU Z, 2008, ADV NEURAL INFORM PR, V20, P1705; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; MORASSO P, 1981, EXP BRAIN RES, V42, P223; Murray R. M., 1994, MATH INTRO ROBOTIC M; Poon E, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P151, DOI 10.1109/MOTION.2002.1182228; Poppe R, 2007, COMPUT VIS IMAGE UND, V108, P4, DOI 10.1016/j.cviu.2006.10.016; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; ROSENHAHN B, 2008, IEEE C COMP VIS PATT, P1; SALZMANN M, 2010, P CVPR 10; Sidenbladh H., 2000, LNCS, V2, P702; Sminchisescu C, 2003, PROC CVPR IEEE, P69; Sminchisescu C., 2004, ICML; Tournier M, 2009, COMPUT GRAPH FORUM, V28, P355, DOI 10.1111/j.1467-8659.2009.01375.x; Urtasun R, 2005, IEEE I CONF COMP VIS, P403; URTASUN R, 2008, ICML, P1080; Urtasun R., 2006, 2006 IEEE COMP SOC C, V1, P238, DOI DOI 10.1109/CVPR.2006.15; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167; ZHAO JM, 1994, ACM T GRAPHIC, V13, P313, DOI 10.1145/195826.195827	41	11	12	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2011	94	3					317	334		10.1007/s11263-011-0433-3	http://dx.doi.org/10.1007/s11263-011-0433-3			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816AC					2022-12-18	WOS:000294570100004
J	Yeo, C; Ahammad, P; Ramchandran, K				Yeo, Chuohao; Ahammad, Parvez; Ramchandran, Kannan			Coding of Image Feature Descriptors for Distributed Rate-efficient Visual Correspondences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Distributed source coding; Camera calibration; Camera networks; Visual correspondences; Distributed feature matching	INFORMATION; ALGORITHMS; SYSTEM	Establishing visual correspondences is a critical step in many computer vision tasks involving multiple views of a scene. In a dynamic environment and when cameras are mobile, visual correspondences need to be updated on a recurring basis. At the same time, the use of wireless links between camera motes imposes tight rate constraints. This combination of issues motivates us to consider the problem of establishing visual correspondences in a distributed fashion between cameras operating under rate constraints. We propose a solution based on constructing distance preserving hashes using binarized random projections. By exploiting the fact that descriptors of regions in correspondence are highly correlated, we propose a novel use of distributed source coding via linear codes on the binary hashes to more efficiently exchange feature descriptors for establishing correspondences across multiple camera views. A systematic approach is used to evaluate rate vs visual correspondences retrieval performance; under a stringent matching criterion, our proposed methods demonstrate superior performance to a baseline scheme employing transform coding of descriptors.	[Yeo, Chuohao] Inst Infocomm Res, Signal Proc Dept, Singapore 118936, Singapore; [Ahammad, Parvez] Howard Hughes Med Inst, Ashburn, VA 20147 USA; [Ramchandran, Kannan] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R); Howard Hughes Medical Institute; University of California System; University of California Berkeley	Yeo, C (corresponding author), Inst Infocomm Res, Signal Proc Dept, Singapore 118936, Singapore.	chyeo@i2r.a-star.edu.sg; parvez@ieee.org; kannanr@eecs.berkeley.edu						AHLSWEDE R, 1981, IEEE T INFORM THEORY, V27, P398, DOI 10.1109/TIT.1981.1056381; Avidan S, 1998, IEEE T VIS COMPUT GR, V4, P293, DOI 10.1109/2945.765324; Barton-Sweeney A., 2006, P IEEE BAS; Berg AC, 2005, PROC CVPR IEEE, P26; BICKEL PJ, 2000, MATH STAT BASIC IDEA, V1; CAI H, 2008, P BRIT MACH VIS C; Chandrasekhar Vijay, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2504, DOI 10.1109/CVPRW.2009.5206733; CHANDRASEKHAR V, 2009, P SPIE VISUAL COMMUN; Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965; CHEN PWC, 2008, UCBEECS200850 EECS D; Cheng ZL, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/57034; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; DEVARAJAN D, 2004, P WORKSH BROADB VANC; Ferrari V, 2004, LECT NOTES COMPUT SC, V3021, P40; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Franke U, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P273, DOI 10.1109/IVS.2000.898354; Gallager RG, 1963, LOW DENSITY PARITY C; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; JAIN P, 2008, INT S LOW POW EL DES, P1; KORNER J, 1979, IEEE T INFORM THEORY, V25, P219, DOI 10.1109/TIT.1979.1056022; Larsen, 1999, P 5 ACM SIGKDD INT C, P16, DOI [DOI 10.1145/312129.312186, 10.1145/312129.312186]; LEE H, 2006, P 4 ACM INT WORKSH V, P9; LIN YC, 2007, P IEEE INT C IM PROC; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma Y., 2004, INVITATION 3 D VISIO; MARTINIAN E, 2005, P ALL C COMM CONTR C; Matusik W, 2004, ACM T GRAPHIC, V23, P814, DOI 10.1145/1015706.1015805; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mikolajczyk K., 2007, P IEEE 11 INT C COMP, P1, DOI DOI 10.1109/ICCV.2007.4408871; Oh S, 2007, P IEEE, V95, P234, DOI 10.1109/JPROC.2006.887296; Rahimi M., 2005, P 3 INT C EMB NETW S; Richardson TJ, 2001, IEEE T INFORM THEORY, V47, P599, DOI 10.1109/18.910577; ROY S, 2007, P IEEE INT C IM PROC; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Schaffalitzky F, 2002, LECT NOTES COMPUT SC, V2350, P414; SE S, 2002, P IEEE RSJ INT C INT, V1; Shum HY, 2000, PROC SPIE, V4067, P2, DOI 10.1117/12.386541; SLEPIAN D, 1973, IEEE T INFORM THEORY, V19, P471, DOI 10.1109/TIT.1973.1055037; Szewczyk R, 2004, COMMUN ACM, V47, P34, DOI 10.1145/990680.990704; TEIXEIRA T, 2006, P WORSH DISTR CANC B; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; Winder SAJ, 2007, PROC CVPR IEEE, P17; WYNER AD, 1976, IEEE T INFORM THEORY, V22, P1, DOI 10.1109/TIT.1976.1055508; Yeo C., 2008, P IEEE INT C IM PROC; Yeo C., 2009, P IEEE INT C AC SPEE; YEO CH, 2008, P SPIE VIS COMM IM P; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561; [No title captured]	51	11	13	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2011	94	3					267	281		10.1007/s11263-011-0427-1	http://dx.doi.org/10.1007/s11263-011-0427-1			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	816AC					2022-12-18	WOS:000294570100001
J	Hisatomi, K; Katayama, M; Tomiyama, K; Iwadate, Y				Hisatomi, Kensuke; Katayama, Miwa; Tomiyama, Kimihiro; Iwadate, Yuichi			3D Archive System for Traditional Performing Arts Application of 3D Reconstruction Method Using Graph-cuts	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-view; Graph-cuts; 3D Reconstruction; Arbitrary viewpoint	MULTIVIEW STEREO	We developed a 3D archive system for Japanese traditional performing arts. The system generates sequences of 3D actor models of the performances from multi-view video by using a graph-cuts algorithm and stores them with CG background models and related information. The system can show a scene from any viewpoint as follows; the 3D actor model is integrated with the background model and the integrated model is projected to a viewpoint that the user indicates with a viewpoint controller. A challenge of generating the actor models is how to reconstruct thin or slender parts. Japanese traditional costumes for performances include slender parts such as long sleeves, fans and strings that may be manipulated during the performance. The graph-cuts algorithm is a powerful 3D reconstruction tool but it tends to cut off those parts because it uses an energy-minimization process. Hence, the search for a way to reconstruct such parts is important for us to preserve these arts for future generations. We therefore devised an adaptive erosion method that works on the visual hull and applied it to the graph-cuts algorithm to extract interior nodes in the thin parts and to prevent the thin parts from being cut off. Another tendency of the reconstruction method using the graph-cuts algorithm is over-shrinkage of the reconstructed models. This arises because the energy can also be reduced by cutting inside the true surface. To avoid this tendency, we applied a silhouette-rim constraint defined by the number of the silhouette-rims passing through each node. By applying the adaptive erosion process and the silhouette-rim constraint, we succeeded in constructing a virtual performance with costumes including thin parts. This paper presents the results of the 3D reconstruction using the proposed method and some outputs of the 3D archive system.	[Hisatomi, Kensuke; Katayama, Miwa; Tomiyama, Kimihiro; Iwadate, Yuichi] NHK Sci & Technol Res Labs, Tokyo, Japan	NHK Japan Broadcasting Corp	Hisatomi, K (corresponding author), NHK Sci & Technol Res Labs, 1-10-11 Kinuta Setagaya, Tokyo, Japan.	hisatomi.k-ko@nhk.or.jp			Ministry of Education, Culture and Sports, Science and Technology Japan	Ministry of Education, Culture and Sports, Science and Technology Japan	The support given by the Ministry of Education, Culture and Sports, Science and Technology Japan is gratefully acknowledged.	AGUIAR E, 2008, P SIGGRAPH; Banno A, 2008, INT J COMPUT VISION, V78, P207, DOI 10.1007/s11263-007-0104-6; Boykov Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P26; DEBEVEC P, 1998, 9 EUR WORKSH REND, P105; Eisemann M., 2008, EUROGRAPHICS, V27; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; GOLDLUCKE B, 2003, P IEEE INT C IM PROC, V3, P713; Hisatomi K, 2009, SMPTE MOTION IMAG J, V118, P29, DOI 10.5594/J15983; KATAYAMA M, 2006, EVA DIGITAL CULTURAL, P51; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; KENMOCHI Y, 1999, P 1999 INT C IM PROC, P361; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; NOBUHARA S, 2006, DEFORMABLE MESH MODE; ORZAN A, 2005, ROM C COMP HUM INT, P133; SANDE K, 2004, THESIS U AMSTERDAM B; Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526; SINHA S, 2007, INT C COMP VIS ICCV; STARCK J, 2006, BRIT MACH VIS C; TOMIYAMA K, 2005, 2 EUR C VIS PROD, P68; TRAN S, 2006, P ECCV, P218; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696; Vogiatzis G, 2005, PROC CVPR IEEE, P391; Vogiatzis G, 2007, IEEE T PATTERN ANAL, V29, P2241, DOI 10.1109/TPAMI.2007.70712; INTRO NOH KYOGEN	25	11	13	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2011	94	1					78	88		10.1007/s11263-011-0434-2	http://dx.doi.org/10.1007/s11263-011-0434-2			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	760JM					2022-12-18	WOS:000290320600006
J	Okamoto, Y; Oishi, T; Ikeuchi, K				Okamoto, Yasuhide; Oishi, Takeshi; Ikeuchi, Katsushi			Image-Based Network Rendering of Large Meshes for Cloud Computing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Huge mesh rendering; Image-based rendering; Network rendering		Recent advances in sensing and software technologies enable us to obtain large-scale, yet fine 3D mesh models of cultural assets. However, such large models cannot be displayed interactively on consumer computers because of the performance limitation of the hardware. Cloud computing technology is a solution that can process a very large amount of information without adding to each client user's processing cost. In this paper, we propose an interactive rendering system for large 3D mesh models, stored on a remote environment through a network of relatively small capacity machines, based on the cloud computing concept. Our system uses both model- and image-based rendering methods for efficient load balance between a server and clients. On the server, the 3D models are rendered by the model-based method using a hierarchical data structure with Level of Detail (LOD). On the client, an arbitrary view is constructed by using a novel image-based method, referred to as the Grid-Lumigraph, which blends colors from sampling images received from the server. The resulting rendering system can efficiently render any image in real time. We implemented the system and evaluated the rendering and data transferring performance.	[Okamoto, Yasuhide; Oishi, Takeshi; Ikeuchi, Katsushi] Univ Tokyo, Inst Ind Sci, Dept Ikeuchi Lab 3, Meguro Ku, Tokyo 1538505, Japan	University of Tokyo	Okamoto, Y (corresponding author), Univ Tokyo, Inst Ind Sci, Dept Ikeuchi Lab 3, Meguro Ku, 4-6-1 Komaba, Tokyo 1538505, Japan.	okamoto@cvl.iis.u-tokyo.ac.jp		Oishi, Takeshi/0000-0002-2010-2608	Japanese Ministry of Education, Culture, Sports, Science and Technology	Japanese Ministry of Education, Culture, Sports, Science and Technology(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT))	The research described herein was supported, in part, by the Japanese Ministry of Education, Culture, Sports, Science and Technology. The 3D models of cultural assets were digitized with the cooperation of the Japanese Government Team for Safeguarding Angkor (JSA) and the National Museum of Western Art, Tokyo.	Cignoni P, 2004, ACM T GRAPHIC, V23, P796, DOI 10.1145/1015706.1015802; Dobbyn S, 2005, ACM T GRAPHIC, V24, P933, DOI 10.1145/1073204.1073290; GARLAND M, 1997, P SIGGRAPH 97, P209, DOI DOI 10.1145/258734.258849; Gobbetti E, 2005, ACM T GRAPHIC, V24, P878, DOI 10.1145/1073204.1073277; GOBBETTI E, 2004, COMPUTERS GRAPHICS, V28; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216; IKEUCHI K, 2004, P 10 INT C VIRT SYST, P334; Jeschke S., 2005, P ACM SIGGRAPH S INT, P103; Karypis G, 1998, J PARALLEL DISTR COM, V48, P96, DOI 10.1006/jpdc.1997.1404; Koller D, 2004, ACM T GRAPHIC, V23, P695, DOI 10.1145/1015706.1015782; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849; Luebke D., 2003, LEVEL DETAIL 3D GRAP; Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940; RUSINKIEWICZ S, 2001, P 2001 S INT 3D GRAP, P63; SHADE J, 1996, P SIGGRAPH 96, P75	17	11	13	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2011	94	1					12	22		10.1007/s11263-010-0383-1	http://dx.doi.org/10.1007/s11263-010-0383-1			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	760JM					2022-12-18	WOS:000290320600002
J	Chang, LB; Jin, Y; Zhang, W; Borenstein, E; Geman, S				Chang, Lo-Bin; Jin, Ya; Zhang, Wei; Borenstein, Eran; Geman, Stuart			Context, Computation, and Optimal ROC Performance in Hierarchical Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Context; Hierarchy; Parts; Composition; ROC performance; Bottom-up processing; Top-down processing; Coarse-to-fine search		It is widely recognized that human vision relies on contextual information, typically arising from each of many levels of analysis. Local gradient information, otherwise ambiguous, is seen as part of a smooth contour or sharp angle in the context of an object's boundary or corner. A stroke or degraded letter, unreadable by itself, contributes to the perception of a familiar word in the context of the surrounding strokes and letters. The iconic Dalmatian dog stays invisible until a multitude of clues about body parts and posture, and figure and ground, are coherently integrated. Context is always based on knowledge about the composition of parts that make up a whole, as in the arrangement of strokes that make up a letter, the arrangement of body parts that make up an animal, or the poses and postures of individuals that make up a mob. From this point of view, the hierarchy of contextual information available to an observer derives from the compositional nature of the world being observed. We will formulate this combinatorial viewpoint in terms of probability distributions and examine the computational implications. Whereas optimal recognition performance in this formulation is NP-complete, we will give mathematical and experimental evidence that a properly orchestrated computational algorithm can achieve nearly optimal recognition within a feasible number of operations. We will interpret the notions of bottom-up and top-down processing as steps in the staging of one such orchestration.	[Chang, Lo-Bin; Jin, Ya; Zhang, Wei; Borenstein, Eran; Geman, Stuart] Brown Univ, Div Appl Math, Providence, RI 02912 USA	Brown University	Geman, S (corresponding author), Brown Univ, Div Appl Math, Providence, RI 02912 USA.	lo-bin_chang@brown.edu; jin.ya76@gmail.com; weizhang.brown@gmail.com; eran.borenstein@gmail.com; stuart_geman@brown.edu			Office of Naval Research [N000140610749]; National Science Foundation [ITR-0427223, DMS-1007593]	Office of Naval Research(Office of Naval Research); National Science Foundation(National Science Foundation (NSF))	Partially supported by the Office of Naval Research under N000140610749, and the National Science Foundation under ITR-0427223 and DMS-1007593.	AHUJA N, 2008, CVPR 08; Amit Y, 1999, NEURAL COMPUT, V11, P1691, DOI 10.1162/089976699300016197; AMIT Y, 2010, MORE YOU LOOK MORE Y; AMIT Y, 2007, INT J COMPUTER VISIO, V75; BAHADUR RR, 1960, ANN MATH STAT, V31, P1015, DOI 10.1214/aoms/1177705674; Barlow H. B., 1994, LARGE SCALE NEURONAL, P1; Bengio Y., 2007, LARGE SCALE KERNEL M; Blanchard G, 2005, ANN STAT, V33, P1155, DOI 10.1214/009053605000000174; BORENSTEIN E, 2002, ECCV, P109; BURL MC, 1998, CVPR; CHANG LB, 2010, THESIS BROWN U; Chen Y., 2007, NIPS; EPSHTEIN B, 2005, ICCV 05; FELZENSZWALB PF, 2010, TR201002 U CHICAGO; FIDLER S, 2007, CVPR 07; Fleuret F, 2001, INT J COMPUT VISION, V41, P85, DOI 10.1023/A:1011113216584; FODOR JA, 1988, COGNITION, V28, P3, DOI 10.1016/0010-0277(88)90031-5; Harrison M. T., 2005, THESIS BROWN U; Jin Y., 2006, CVPR, V2, P2145; KOKKINOS I, 2006, CVPR 06; MOREELS P, 2008, ECCV; OMMER B, 2007, CVPR 07; Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56; SHIEBER SM, 1992, CONSTRAINT BASED GRA; SUDDERTH EB, 2005, IEEE INT C COMPUTER; Viola P, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P747; WARREN WH, 2010, ENCY PERCEPTION; Wu TF, 2011, INT J COMPUT VISION, V93, P226, DOI 10.1007/s11263-010-0346-6; ZHANG W, 2009, THESIS BROWN U; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	30	11	11	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2011	93	2					117	140		10.1007/s11263-010-0391-1	http://dx.doi.org/10.1007/s11263-010-0391-1			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	740PD					2022-12-18	WOS:000288806000002
J	Zhu, L; Chen, YH; Lin, CX; Yuille, A				Zhu, Long (Leo); Chen, Yuanhao; Lin, Chenxi; Yuille, Alan			Max Margin Learning of Hierarchical Configural Deformable Templates (HCDTs) for Efficient Object Parsing and Pose Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hierarchy; Shape representation; Object parsing; Segmentation; Structure learning; Max margin		In this paper we formulate a hierarchical configurable deformable template (HCDT) to model articulated visual objects-such as horses and baseball players-for tasks such as parsing, segmentation, and pose estimation. HCDTs represent an object by an AND/OR graph where the OR nodes act as switches which enables the graph topology to vary adaptively. This hierarchical representation is compositional and the node variables represent positions and properties of subparts of the object. The graph and the node variables are required to obey the summarization principle which enables an efficient compositional inference algorithm to rapidly estimate the state of the HCDT. We specify the structure of the AND/OR graph of the HCDT by hand and learn the model parameters discriminatively by extending Max-Margin learning to AND/OR graphs. We illustrate the three main aspects of HCDTs-representation, inference, and learning-on the tasks of segmenting, parsing, and pose (configuration) estimation for horses and humans. We demonstrate that the inference algorithm is fast and that max-margin learning is effective. We show that HCDTs gives state of the art results for segmentation and pose estimation when compared to other methods on benchmarked datasets.	[Zhu, Long (Leo)] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; [Chen, Yuanhao] Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China; [Lin, Chenxi] Alibaba Grp R&D, Hangzhou, Zhejiang, Peoples R China; [Yuille, Alan] Univ Calif Los Angeles, Dept Stat Psychol & Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles; Chinese Academy of Sciences; University of Science & Technology of China, CAS; Alibaba Group; University of California System; University of California Los Angeles	Zhu, L (corresponding author), Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.	lzhu@stat.ucla.edu; yhchen4@ustc.edu; chenxi.lin@alibaba-inc.com; yuille@stat.ucla.edu		Yuille, Alan L./0000-0001-5207-9249	National Science Foundation [0413214, IIS-0917141]; W.M. Keck Foundation	National Science Foundation(National Science Foundation (NSF)); W.M. Keck Foundation(W.M. Keck Foundation)	We gratefully acknowledge support from the National Science Foundation with NSF grant number 0413214, IIS-0917141, and from the W.M. Keck Foundation.	Altun Y., 2003, ICML, P3; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BORENSTEIN E, 2002, ECCV, P109; Borenstein E., 2006, CVPR, V1, P969, DOI DOI 10.1109/CVPR.2006.276; CHEN H, 2006, CVPR2006, P943; CHEN X, 2005, CVPR; Chen Y., 2007, NIPS; Chui HL, 2000, PROC CVPR IEEE, P44, DOI 10.1109/CVPR.2000.854733; Coughlan J, 2000, COMPUT VIS IMAGE UND, V78, P303, DOI 10.1006/cviu.2000.0842; COUGHLAN J, 2002, ECCV, V3, P453; COUGHLAN J, 1998, CVPR; Cour T., 2007, CVPR; CRAMMER K, 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628; Cristianini N., 2000, INTRO SUPPORT VECTOR; Dechter R, 2007, ARTIF INTELL, V171, P73, DOI 10.1016/j.artint.2006.11.003; Felzenszwalb P., 2008, P IEEE CVPR; Hofmann T., 2004, P 21 INT C MACH LEAR, P104, DOI 10.1145/1015330.1015341; Jin Y., 2006, CVPR, V2, P2145; KUMAR MP, 2005, CVPR, V1, P18; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Lee MW, 2004, PROC CVPR IEEE, P334; LEIBE B, 2004, ECCV WORKSH STAT LEA, P17; Levin A, 2006, LECT NOTES COMPUT SC, V3954, P581; Manning CD, 1999, FDN STAT NATURAL LAN; Meila M, 2001, J MACH LEARN RES, V1, P1, DOI 10.1162/153244301753344605; Mori G, 2005, IEEE I CONF COMP VIS, P1417; MORI G, 2004, CVPR, V2, P326; Oren M, 1997, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.1997.609319; Osuna E, 1997, PROC CVPR IEEE, P130, DOI 10.1109/CVPR.1997.609310; Pereira F. C. N., 2001, P 18 INT C MACH LEAR, P282; Platt J. C., 1998, NIPS, V11, P557; Ramanan D., 2006, NIPS, P1129; Ren X., 2005, NIPS; Ren XF, 2005, IEEE I CONF COMP VIS, P824; Ronfard R, 2002, LECT NOTES COMPUT SC, V2353, P700; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Sigal L., 2006, CVPR, P2041; Srinivasan P., 2007, CVPR, p[2265, 2269]; Srinivasan P, 2007, LECT NOTES COMPUT SC, V4679, P153; Taskar B., 2004, EMNLP; Taskar Ben, 2003, NIPS; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Viola P. A., 2005, NIPS; Winn J, 2005, IEEE I CONF COMP VIS, P756; ZHANG J, 2006, CVPR 06, P1536; ZHU L, 2009, IEEE T PATTERN ANAL; Zhu L., 2008, CVPR; Zhu L., 2008, ECCV; ZHU L, 2005, NIPS; ZHU L, 2006, NIPS, P1617; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	52	11	11	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2011	93	1					1	21		10.1007/s11263-010-0375-1	http://dx.doi.org/10.1007/s11263-010-0375-1			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	738WJ		Green Submitted, Bronze			2022-12-18	WOS:000288673000001
J	Bertelli, L; Chandrasekaran, S; Gibou, F; Manjunath, BS				Bertelli, Luca; Chandrasekaran, Shivkumar; Gibou, Frederic; Manjunath, B. S.			On the Length and Area Regularization for Multiphase Level Set Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image segmentation; Multiphase flows; Level set methods; Length regularization; Area regularization	IMAGE SEGMENTATION; ACTIVE CONTOURS; SHAPE PRIORS; FRAMEWORK; MUMFORD; SNAKES; MODEL	In this paper we introduce novel regularization techniques for level set segmentation that target specifically the problem of multiphase segmentation. When the multiphase model is used to obtain a partitioning of the image in more than two regions, a new set of issues arise with respect to the single phase case in terms of regularization strategies. For example, if smoothing or shrinking each contour individually could be a good model in the single phase case, this is not necessarily true in the multiphase scenario. In this paper, we address these issues designing enhanced length and area regularization terms, whose minimization yields evolution equations in which each level set function involved in the multiphase segmentation can "sense" the presence of the other level set functions and evolve accordingly. In other words, the coupling of the level set function, which before was limited to the data term (i.e. the proper segmentation driving force), is extended in a mathematically principled way to the regularization terms as well. The resulting regularization technique is more suitable to eliminate spurious regions and other kind of artifacts. An extensive experimental evaluation supports the model we introduce in this paper, showing improved segmentation performance with respect to traditional regularization techniques.	[Bertelli, Luca; Chandrasekaran, Shivkumar; Manjunath, B. S.] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA; [Gibou, Frederic] Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA; [Gibou, Frederic] Univ Calif Santa Barbara, Dept Mech Engn, Santa Barbara, CA 93106 USA	University of California System; University of California Santa Barbara; University of California System; University of California Santa Barbara; University of California System; University of California Santa Barbara	Bertelli, L (corresponding author), Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.	lbertelli@ece.ucsb.edu; shiv@ece.ucsb.edu; fgibou@engineering.ucsb.edu; manj@ece.ucsb.edu	Gibou, Frederic/AAU-4944-2020; Manjunath, B S/AAM-8190-2020	Manjunath, B S/0000-0003-2804-3611	NSF [0331697, III-0808772]	NSF(National Science Foundation (NSF))	The authors would like to thank the reviewers for their constructive comments and Dr. Ruggero Carli for numerous fruitful discussions. This research was supported by NSF ITR grant #0331697 and NSF award III-0808772.	Aubert G, 2003, SIAM J APPL MATH, V63, P2128, DOI 10.1137/S0036139902408928; Bertelli L, 2008, IEEE T PATTERN ANAL, V30, P1400, DOI 10.1109/TPAMI.2007.70785; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chang PC, 1998, ELEC SOC S, V98, P98; CHUNG G, 2009, COMPUTING VISUALIZAT, V12, P1634; CREMERS D, 2004, EUR C COMP VIS ECCV, P74; Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5; Cremers D, 2006, IEEE T PATTERN ANAL, V28, P1262, DOI 10.1109/TPAMI.2006.161; Dambreville Samuel, 2006, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, P977; Evans LC., 1992, MEASURE THEORY FINE; Goldenberg R, 2001, IEEE T IMAGE PROCESS, V10, P1467, DOI 10.1109/83.951533; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KICHENASSAMY S, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P810, DOI 10.1109/ICCV.1995.466855; Kim JM, 2005, IEEE T IMAGE PROCESS, V14, P1486, DOI 10.1109/TIP.2005.854442; MALLADI R, 1994, ECCV94, V1, P3; MALLADI R, 2004, SPIE C GEOM METH COM, P246; Mumford D., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P22; Nguyen HT, 2003, IEEE T PATTERN ANAL, V25, P330, DOI 10.1109/TPAMI.2003.1182096; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Paragios N, 2002, INT J COMPUT VISION, V46, P223, DOI 10.1023/A:1014080923068; PARAGIOS N, 1999, ICCV, V2, P926; Park J, 2001, IEEE T PATTERN ANAL, V23, P1201, DOI 10.1109/34.954609; Rathi Y., 2006, INT C SIGN IM PROC; RAVIV TR, 2005, INT C COMP VIS, P204; RAVIV TR, 2006, INT J COMPU IN PRESS; Rousson M, 2003, PROC CVPR IEEE, P699; Samson C, 2000, IEEE T PATTERN ANAL, V22, P460, DOI 10.1109/34.857003; Samson C, 2000, INT J COMPUT VISION, V40, P187, DOI 10.1023/A:1008183109594; SAMSON C, 1999, P SCAL SPAC THEOR CO; Sapiro G, 1997, COMPUT VIS IMAGE UND, V68, P247, DOI 10.1006/cviu.1997.0562; Sapiro G, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL I, P817, DOI 10.1109/ICIP.1996.559624; Tsai A, 2001, IEEE T IMAGE PROCESS, V10, P1169, DOI 10.1109/83.935033; Vese L, 2003, GEOMETRIC LEVEL SET METHODS IN IMAGING, VISION AND GRAPHICS, P175, DOI 10.1007/0-387-21810-6_10; Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076; Yezzi A.  Jr., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P898, DOI 10.1109/ICCV.1999.790317; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343; ZHU SC, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P416, DOI 10.1109/ICCV.1995.466909	39	11	11	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2010	90	3					267	282		10.1007/s11263-010-0348-4	http://dx.doi.org/10.1007/s11263-010-0348-4			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	662BV		Bronze, Green Published			2022-12-18	WOS:000282782700001
J	Wang, CW; Hunter, A				Wang, Ching-Wei; Hunter, Andrew			Robust Pose Recognition of the Obscured Human Body	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose estimation; Covered body pose recognition; Sleep apnoea	SLEEP-APNEA; TRACKING; PREVALENCE; SHAPE	This paper presents a robust automated noninvasive video monitoring approach to recover the human pose in conditions with persistent heavy obscuration. The proposed methods are compared with Ramanan's stylized pose detection method and Wang's sequential pose model. The experimental results show that the proposed method performs significantly better than Ramanan's approach, is able to estimate the obscured body pose with various postures and obscuration levels in different environments, and is not sensitive to illumination changes. The system is evaluated in two domains: sleeping human subjects obscured by a bed cover, and pedestrians with a cluttered background scene, low feature contrast and baggy clothing. The body part detectors are trained in the sleep monitoring domain but are still able to estimate the pose in the pedestrian domain, demonstrating the robustness of the proposed technique.	[Wang, Ching-Wei] Natl Taiwan Univ Sci & Technol, Grad Inst Biomed Engn, Taipei, Taiwan; [Hunter, Andrew] Lincoln Univ, Lincoln LN6 7TS, England	National Taiwan University of Science & Technology; University of Lincoln	Wang, CW (corresponding author), Natl Taiwan Univ Sci & Technol, Grad Inst Biomed Engn, Taipei, Taiwan.	cweiwang@ieee.org	Wang, Ching-Wei/GXV-5212-2022; Hunter, Andrew/E-3880-2015	Wang, Ching-Wei/0000-0001-9992-6863; Hunter, Andrew/0000-0003-3786-4008	United Lincolnshire Hospitals NHS Trust; University of Lincoln	United Lincolnshire Hospitals NHS Trust; University of Lincoln	This research is jointly supported by United Lincolnshire Hospitals NHS Trust (ULH collaborative Research Grant) and University of Lincoln for the PhD scholarship of C.-W. Wang. The authors gratefully acknowledge the support of Dr. Neil Gravill and Dr. Simon Matusiewicz for their valuable comments in obstructive sleep apnoea, Dr. Chris Hacking for his generous help in clinical engineering and the editor and reviewers for their precious comments on the paper. Research Ethics approval was gained from Derbyshire Research Ethics Committee (REC number 08/H0401/12).	AGARWAL A, 2004, CVPR, V2, P882; Andriluka M., 2009, P C COMP VIS PATT RE; Balan AO, 2008, LECT NOTES COMPUT SC, V5303, P15, DOI 10.1007/978-3-540-88688-4_2; Barrow HG, 1977, P 5 INT JOINT C ART; BORDEFORS G, 1988, IEEE T PATTERN ANAL, V10, P849; Boult TE, 1999, SECOND IEEE WORKSHOP ON VISUAL SURVEILLANCE (VS'99), PROCEEDINGS, P48; CHALMOND B, 2004, IEEE T IMAGE PROCESS, P2644; Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205; Cremers D, 2008, J SCI COMPUT, V35, P132, DOI 10.1007/s10915-008-9220-x; Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c; Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197; Elgammal A, 2004, PROC CVPR IEEE, P681; Eng HL, 2004, INT C PATT RECOG, P257; Enzweiler M, 2009, IEEE T PATTERN ANAL, V31, P2179, DOI 10.1109/TPAMI.2008.260; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468; Flemons WW, 2003, CHEST, V124, P1543, DOI 10.1378/chest.124.4.1543; Fleuret F, 2001, INT J COMPUT VISION, V41, P85, DOI 10.1023/A:1011113216584; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; GASTAUT H., 1966, BRAIN RES, V1, P167, DOI 10.1016/0006-8993(66)90117-X; GAVRILA DM, 2000, P EUR C COMP VIS, P37; GHAFOOR A, 2003, P 4 EURASIP C FOC VI, V1, P155; Gibson GJ, 2005, BRIT MED BULL, V72, P49, DOI 10.1093/bmb/ldh044; Guo F, 2006, INT C PATT RECOG, P43; Haba-Rubio J, 2005, SLEEP MED, V6, P225, DOI 10.1016/j.sleep.2004.08.009; Hasler N, 2009, COMPUT GRAPH-UK, V33, P211, DOI 10.1016/j.cag.2009.03.026; HOEY J, 2006, P BRIT MACH VIS C, V1, P367; HUANG ZQ, 2005, P DIG IM COMP TECHN, P161; JAEGGLI T, 2005, P IEEE MOTION, P248; Javaheri S, 2004, EUR HEART J, V25, P260, DOI 10.1016/j.ehj.2003.10.032; LAN X, 2004, P IEEE C COMP VIS PA, V1, P722; Lee M.W., 2007, P 2007 IEEE WORKSH M, P23; Lee MW, 2006, IEEE T PATTERN ANAL, V28, P905, DOI 10.1109/TPAMI.2006.110; Li B, 2008, PATTERN RECOGN, V41, P418, DOI 10.1016/j.patcog.2007.06.002; MATUSIEWICZ S, 2006, COMMUNICATION; Mori G, 2004, PROC CVPR IEEE, P326; Neven AK, 1998, THORAX, V53, P638, DOI 10.1136/thx.53.8.638; Nguyen HT, 2004, IEEE T PATTERN ANAL, V26, P1099, DOI 10.1109/TPAMI.2004.45; Quinlan JR, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P725; RAMANAN D, 2006, P COMP VIS PATT REC; RAMANAN D, 2009, WEB HOMEPAGE DEVA RA; Ramanan D., 2007, ADV NEURAL INFORM PR, V19, P1129; RAMANAN D, 2003, CVPR, V2, P467; Ramanan D, 2007, IEEE T PATTERN ANAL, V29, P65, DOI 10.1109/TPAMI.2007.250600; REN X, 2003, P INT C COMP VIS, V1, P824; Rosenhahn B, 2007, MACH VISION APPL, V18, P25, DOI 10.1007/s00138-006-0046-y; Sigal L, 2004, PROC CVPR IEEE, P421; Sigal L., 2006, PROC IEEE C COMPUT V, P2041; Sminchisescu C, 2005, PROC CVPR IEEE, P390; THAYANANTHAN A, 2003, COMPUTER VISION PATT, V1, P1; TOBIAS J, 2005, P IEEE WORKSH MOT VI, V2, P248; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; *VIS, 2008, VIS 3 DIG VID SYST; Wang CW, 2006, LECT NOTES COMPUT SC, V4141, P404; WANG CW, 2008, P 8 INT C IEEE BIOIN; WANG CW, 2009, INT J APPL INTELLIGE; Wang Y, 2008, LECT NOTES COMPUT SC, V5304, P710, DOI 10.1007/978-3-540-88690-7_53; Winn J., 2006, CVPR; Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7	59	11	12	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2010	90	3					313	330		10.1007/s11263-010-0365-3	http://dx.doi.org/10.1007/s11263-010-0365-3			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	662BV					2022-12-18	WOS:000282782700004
J	Nguyen, MH; de la Torre, F				Nguyen, Minh Hoai; de la Torre, Fernando			Metric Learning for Image Alignment	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image alignment; Metric learning; Template matching; Active appearance models	MODELS; TRACKING	Image alignment has been a long standing problem in computer vision. Parameterized Appearance Models (PAMs) such as the Lucas-Kanade method, Eigentracking, and Active Appearance Models are commonly used to align images with respect to a template or to a previously learned model. While PAMs have numerous advantages relative to alternate approaches, they have at least two drawbacks. First, they are especially prone to local minima in the registration process. Second, often few, if any, of the local minima of the cost function correspond to acceptable solutions. To overcome these problems, this paper proposes a method to learn a metric for PAMs that explicitly optimizes that local minima occur at and only at the places corresponding to the correct fitting parameters. To the best of our knowledge, this is the first paper to address the problem of learning a metric to explicitly model local properties of the PAMs' error surface. Synthetic and real examples show improvement in alignment performance in comparison with traditional approaches. In addition, we show how the proposed criteria for a good metric can be used to select good features to track.	[Nguyen, Minh Hoai; de la Torre, Fernando] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Nguyen, MH (corresponding author), Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	minhhoai@cmu.edu; ftorre@cs.cmu.edu						Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; BAKER S, 2001, P IEEE C COMP VIS PA; BERGEN JR, 1992, EUR C COMP VIS, P237; Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436; Blanz Volker, 1999, ACM SIGGRAPH; COOTES T, 2001, PATTERN ANAL MACHINE, V23; Cootes TF, 2001, STAT MODELS APPEARAN; De la Torre F, 2003, COMPUT VIS IMAGE UND, V91, P53, DOI 10.1016/S1077-3142(03)00076-6; De la Torre F, 2000, INT C PATT RECOG, P1106, DOI 10.1109/ICPR.2000.903739; De la Torre F., 2008, P IEEE C COMP VIS PA; DELATORRE F, 2007, IEEE C COMP VIS PATT; Grant M., 2013, CVX MATLAB SOFTWARE; Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7; Grossmann V, 2007, BE J THEOR ECON, V7; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Jones MJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P683, DOI 10.1109/ICCV.1998.710791; Kanatani K., 1996, STAT OPTIMIZATION GE; Learned-Miller EG, 2006, IEEE T PATTERN ANAL, V28, P236, DOI 10.1109/TPAMI.2006.34; Liu X., 2007, IEEE C COMP VIS PATT; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Matei BC, 2006, IEEE T PATTERN ANAL, V28, P1537, DOI 10.1109/TPAMI.2006.205; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16; McKenna S. J., 2000, SURVEY FACE RECOGNIT, V58; NAYAR SK, 1996, EARLY VISUAL LEARNIN; NGUYEN MH, 2008, P IEEE C COMP VIS PA; NGUYEN MH, 2008, 8 IEEE INT C AUT FAC; Rudin W, 1976, PRINCIPLES MATH ANAL, V3rd; SARAGIH J, 2007, INT C COMP VIS; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Taskar B, 2004, ADV NEUR IN, V16, P25; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Vapnik V., 1998, STAT LEARNING THEORY; VETTER T, 1997, INT C AUT FAC GEST R; WIMMER M, 2006, P BRIT MACH VIS C; Wu H., 2008, P IEEE C COMP VIS PA; Xiao J, 2004, PROC CVPR IEEE, P535; Yang L, 2006, DISTANCE METRIC LEAR	40	11	11	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	88	1					69	84		10.1007/s11263-009-0299-9	http://dx.doi.org/10.1007/s11263-009-0299-9			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	571LS					2022-12-18	WOS:000275753900004
J	Carroll, RE; Seitz, SM				Carroll, Robert E.; Seitz, Steven M.			Rectified Surface Mosaics	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image registration and mosaicing; Pose estimation; Endoscopy	REGISTRATION	We approach mosaicing as a camera tracking problem within a known parameterized surface. From a video of a camera moving within a surface, we compute a mosaic representing the texture of that surface, flattened onto a planar image. Our approach works by defining a warp between images as a function of surface geometry and camera pose. Globally optimizing this warp to maximize alignment across all frames determines the camera trajectory, and the corresponding flattened mosaic image. In contrast to previous mosaicing methods which assume planar or distant scenes, or controlled camera motion, our approach enables mosaicing in cases where the camera moves unpredictably through proximal surfaces, such as in medical endoscopy applications.	[Carroll, Robert E.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Seitz, Steven M.] Univ Washington, Seattle, WA 98195 USA	University of California System; University of California Berkeley; University of Washington; University of Washington Seattle	Carroll, RE (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	carroll@cs.berkeley.edu			Pentax; NSF [IIS-0413198]	Pentax; NSF(National Science Foundation (NSF))	The support of Pentax and NSF grant IIS-0413198 are gratefully acknowledged. We wish to thank Eric Seibel for suggesting this problem and for helping to guide our research.	Agarwala A, 2004, ACM T GRAPHIC, V23, P584, DOI 10.1145/1015706.1015764; Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718; Agarwala A, 2006, ACM T GRAPHIC, V25, P853, DOI 10.1145/1141911.1141966; BAKER S, 2002, LUCASKANADE 20 YEA 1; Carrasco JBI, 2007, SCRIPTA NOVA, V11, P1; Bricault I, 1998, IEEE T MED IMAGING, V17, P703, DOI 10.1109/42.736022; CARRASCOSA P, 2006, ABDOMINAL IMAGING; Chen SN, 1995, SWIMMING THROUGH TROUBLED WATER, P29; Fattal R, 2002, ACM T GRAPHIC, V21, P249; Hartley R., 2004, ROBOTICA; HELFERTY J, 2004, SPIE MED IMAGING 200, V5369, P150; Konen W, 2007, BILDVERARBEITUNG MED, P298; Lucas B.D., 1981, ITERATIVE IMAGE REGI, P674; Miranda-Luna R, 2008, IEEE T BIO-MED ENG, V55, P541, DOI 10.1109/TBME.2007.903520; MORI K, 2000, SPIE, V3978, P122; Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269; RAI L, 2006, SPIE, V6141, P1; Reeff M, 2006, GI JAHRESTAGUNG, V1, P467; Rousso B, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P945, DOI 10.1109/ICCV.1998.710830; Seitz SM, 2002, INT J COMPUT VISION, V48, P21, DOI 10.1023/A:1014851111084; Seshamani S, 2006, LECT NOTES COMPUT SC, V4190, P355; Szeliski R, 1996, IEEE COMPUT GRAPH, V16, P22, DOI 10.1109/38.486677; Szeliski R., 1997, SIGGRAPH 97, P251; TRUONG T, 2006, SPIE, V6143, P412; Vercauteren T, 2006, MED IMAGE ANAL, V10, P673, DOI 10.1016/j.media.2006.06.006; Warmath JR, 2005, P SOC PHOTO-OPT INS, V5744, P425, DOI 10.1117/12.595263; WOOD DN, 1997, SIGGRAPH 97, P243; [No title captured]	28	11	12	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2009	85	3					307	315		10.1007/s11263-009-0264-7	http://dx.doi.org/10.1007/s11263-009-0264-7			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	502CO		Green Published, Bronze			2022-12-18	WOS:000270432200008
J	Hartley, R; Schaffalitzky, F				Hartley, Richard I.; Schaffalitzky, Frederik			Reconstruction from Projections Using Grassmann Tensors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Grassman tensor; Projective reconstruction; Multiview tensor; Exterior algebra		In this paper a general procedure is given for reconstruction of a set of feature points in an arbitrary dimensional projective space from their projections into lower dimensional spaces. This extends the methods applied in the well-studied problem of reconstruction of scene points in a"similar to(3) given their projections in a set of images. In this case, the bifocal, trifocal and quadrifocal tensors are used to carry out this computation. It is shown that similar methods will apply in a much more general context, and hence may be applied to projections from a"similar to (n) to a"similar to (m) , which have been used in the analysis of dynamic scenes, and in radial distortion correction. For sufficiently many generic projections, reconstruction of the scene is shown to be unique up to projectivity, except in the case of projections onto one-dimensional image spaces (lines), in which case there are two solutions. Projections from a"similar to (n) to a"similar to(2) have been considered by Wolf and Shashua (in International Journal of Computer Vision 48(1): 53-67, 2002), where they were applied to several different problems in dynamic scene analysis. They analyzed these projections using tensors, but no general way of defining such tensors, and computing the projections was given. This paper settles the general problem, showing that tensor definition and retrieval of the projections is always possible.	[Hartley, Richard I.] Natl ICT Australia, Canberra, ACT, Australia; [Hartley, Richard I.; Schaffalitzky, Frederik] Australian Natl Univ, Canberra, ACT, Australia	NICTA; Australian National University	Hartley, R (corresponding author), Natl ICT Australia, Canberra, ACT, Australia.	Richard.Hartley@anu.edu.au		Hartley, Richard/0000-0002-5005-0191	National ICT Australia (NICTA); Department of Broad-band, Communications and the Digital Economy; Australian Research Council through the ICT Centre of Excellence program	National ICT Australia (NICTA); Department of Broad-band, Communications and the Digital Economy; Australian Research Council through the ICT Centre of Excellence program(Australian Research Council)	This research has been supported partly by National ICT Australia (NICTA). NICTA is funded by the Australian Government as represented by the Department of Broad-band, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.	Faugeras O, 2000, IEEE T PATTERN ANAL, V22, P1179, DOI 10.1109/34.879801; Hartley R., 2004, ROBOTICA; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hartley RI, 1998, PHILOS T R SOC A, V356, P1175, DOI 10.1098/rsta.1998.0216; HARTLEY RI, 1998, LNCS, V1406, P20; Hartley R, 2008, LECT NOTES COMPUT SC, V5302, P276, DOI 10.1007/978-3-540-88682-2_22; Heyden A, 1998, INT J COMPUT VISION, V30, P5, DOI 10.1023/A:1008020228557; HEYDEN A, 1998, P 5 EUR C COMP VIS F, P3; Quan L, 1997, IEEE T PATTERN ANAL, V19, P834, DOI 10.1109/34.608285; Quan L, 2001, IEEE T PATTERN ANAL, V23, P212, DOI 10.1109/34.908971; Semple J., 1979, ALGEBRAIC PROJECTIVE; Thirthala S, 2005, IEEE I CONF COMP VIS, P1539; Thirthala S, 2005, PROC CVPR IEEE, P321; Wolf L, 2002, INT J COMPUT VISION, V48, P53, DOI 10.1023/A:1014855311993	14	11	11	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2009	83	3					274	293		10.1007/s11263-009-0225-1	http://dx.doi.org/10.1007/s11263-009-0225-1			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	424EI		Green Submitted			2022-12-18	WOS:000264547900004
J	Kawasaki, H; Furukawa, R				Kawasaki, Hiroshi; Furukawa, Ryo			Shape Reconstruction and Camera Self-Calibration Using Cast Shadows and Scene Geometries	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape reconstruction; Shape from cast shadow; Camera calibration; Self-calibration	SEGMENTATION	Recently, various techniques of shape reconstruction using cast shadows have been proposed. These techniques have the advantage that they can be applied to various scenes, including outdoor scenes, without using special devices. Previously proposed techniques usually require calibration of camera parameters and light source positions, and such calibration processes limit the range of application of these techniques. In this paper, we propose a method to reconstruct 3D scenes even when the camera parameters or light source positions are unknown. The technique first recovers the shape with 4-DOF indeterminacy using coplanarities obtained by cast shadows of straight edges or visible planes in a scene, and then upgrades the shape using metric constraints obtained from the geometrical constraints in the scene. In order to circumvent the need for calibrations and special devices, we propose both linear and nonlinear methods in this paper. Experiments using simulated and real images verified the effectiveness of this technique.	[Kawasaki, Hiroshi] Saitama Univ, Fac Engn, Sakura Ku, Saitama 3388570, Japan; [Furukawa, Ryo] Hiroshima City Univ, Fac Informat Sci, Asaminami Ku, Hiroshima, Japan	Saitama University	Kawasaki, H (corresponding author), Saitama Univ, Fac Engn, Sakura Ku, 255 Shimo Okubo, Saitama 3388570, Japan.	kawasaki@cgv.ics.saitama-u.ac.jp; ryo-f@cs.hiroshima-cu.ac.jp	Furukawa, Ryo/GWZ-2117-2022	Furukawa, Ryo/0000-0002-2063-1008	Ministry of Internal Affairs and Communications, Japan [072103013]; Ministry of Education, Science, Sports and Culture, Japan [19700098, 19700157]	Ministry of Internal Affairs and Communications, Japan; Ministry of Education, Science, Sports and Culture, Japan(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT))	This work was supported in part by SCOPE No. 072103013 (Ministry of Internal Affairs and Communications, Japan) and Grant-in-Aid for Scientific Research Nos. 19700098 and 19700157 (Ministry of Education, Science, Sports and Culture, Japan).	Bartoli A, 2003, INT J COMPUT VISION, V52, P45, DOI 10.1023/A:1022318524906; Bouguet J.-Y., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P514, DOI 10.1109/CVPR.1999.786986; BOUGUET JY, 1998, INT C COMP VIS, P129; CASPI Y, 2006, CVPR, P2309; Criminisi A, 2000, INT J COMPUT VISION, V40, P123, DOI 10.1023/A:1026598000963; Daum M, 1998, PROC CVPR IEEE, P461, DOI 10.1109/CVPR.1998.698646; HAMBRICK LN, 1987, IEEE T PATTERN ANAL, V9, P597, DOI 10.1109/TPAMI.1987.4767954; HATZITHEODOROU M, 1988, CVPR, P486; JIANG CX, 1994, CVGIP-IMAG UNDERSTAN, V59, P213, DOI 10.1006/ciun.1994.1014; KAWASAKI H, 2007, INT C 3D DIG IM MOD; Kriegman DJ, 2001, J OPT SOC AM A, V18, P1804, DOI 10.1364/JOSAA.18.001804; RAVIV D, 1989, IEEE T ROBOTIC AUTOM, V5, P701, DOI 10.1109/70.88087; Salvador E., 2004, COMPUT VIS IMAGE UND, V95, P238, DOI DOI 10.1016/j.cviu.2004.03.008; Sato K., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P657; Savarese S, 2007, INT J COMPUT VISION, V71, P305, DOI 10.1007/s11263-006-8323-9; SHAFER SA, 1983, COMPUT VISION GRAPH, V22, P145, DOI 10.1016/0734-189X(83)90099-3; Sugihara K., 1986, MACHINE INTERPRETATI; YAMAZAKI S, 2007, INT C COMP VIS; Yu YH, 2005, INT J COMPUT VISION, V62, P35, DOI 10.1007/s11263-005-4634-5	19	11	13	1	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2009	83	2					135	148		10.1007/s11263-008-0188-7	http://dx.doi.org/10.1007/s11263-008-0188-7			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	423UE					2022-12-18	WOS:000264520400002
J	Sochman, J; Matas, J				Sochman, Jan; Matas, Jiri			Learning Fast Emulators of Binary Decision Processes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Boosting; AdaBoost; Sequential probability ratio test; Sequential decision making; WaldBoost; Interest point detectors; Machine learning	SCALE; COLOR	Computation time is an important performance characteristic of computer vision algorithms. The paper shows how existing (slow) binary decision algorithms can be approximated by a (fast) trained WaldBoost classifier. WaldBoost learning minimises the decision time of the classifier while guaranteeing predefined precision. We show that the WaldBoost algorithm together with bootstrapping is able to efficiently handle an effectively unlimited number of training examples provided by the implementation of the approximated algorithm. Two interest point detectors, the Hessian-Laplace and the Kadir-Brady saliency detectors, are emulated to demonstrate the approach. Experiments show that while the repeatability and matching scores are similar for the original and emulated algorithms, a 9-fold speed-up for the Hessian-Laplace detector and a 142-fold speed-up for the Kadir-Brady detector is achieved. For the Hessian-Laplace detector, the achieved speed is similar to SURF, a popular and very fast handcrafted modification of Hessian-Laplace; the WaldBoost emulator approximates the output of the Hessian-Laplace detector more precisely.	[Sochman, Jan; Matas, Jiri] Czech Tech Univ, Fac Elect Engn, Ctr Machine Percept, Dept Cybernet, Prague 16627 6, Czech Republic	Czech Technical University Prague	Sochman, J (corresponding author), Czech Tech Univ, Fac Elect Engn, Ctr Machine Percept, Dept Cybernet, Tech 2, Prague 16627 6, Czech Republic.	jan.sochman@cmp.felk.cvut.cz	, Matas/AAW-3282-2020		EC [FP6-IST-027113 eTRIMS]; Czech Science Foundation [102/07/1317]	EC(European CommissionEuropean Commission Joint Research Centre); Czech Science Foundation(Grant Agency of the Czech Republic)	The authors were supported by EC project FP6-IST-027113 eTRIMS (JS) and by Czech Science Foundation Project 102/07/1317 (JM).; We thank the anonymous reviewers for their valuable comments that led to an improvement of the manuscript.	BAKER S, 1996, INT C PATT REC, V2, P869; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310; Brubaker SC, 2008, INT J COMPUT VISION, V77, P65, DOI 10.1007/s11263-007-0060-1; DOLLAR P, 2006, IEEE COMP SOC C COMP, V2, P1964, DOI DOI 10.1109/CVPR.2006.298; Fergus R, 2005, PROC CVPR IEEE, P380; Friedman J., 1998, ADDITIVE LOGISTIC RE; Froba B, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P91, DOI 10.1109/AFGR.2004.1301514; GRABNER M, 2006, ACCV, V1, P918; Hare JS, 2004, LECT NOTES COMPUT SC, V3115, P317; Huang C, 2007, IEEE T PATTERN ANAL, V29, P671, DOI 10.1109/TPAMI.2007.1011; Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Kalal Z., 2008, BRIT MACH VIS C; Lepetit V, 2005, PROC CVPR IEEE, P775; Lienhart R, 2002, IEEE IMAGE PROC, P900; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; MATAS J, 2007, INT C ROB AUT; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; MIKOLAJCZYK K, 2008, COMMUNICATION; Mikolajczyk K., 2002, THESIS INPG GRENOBLE; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Rivest R. L., 1987, Machine Learning, V2, P229, DOI 10.1023/A:1022607331053; Rosten E., 2006, ECCV, P430, DOI DOI 10.1007/11744023_; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Siegmund DO, 1985, SEQUENTIAL ANAL TEST; Sochman J, 2005, PROC CVPR IEEE, P150; SOCHMAN J, 2007, AS C COMP VIS, V2, P236; Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648; Viola P., 2001, INT WORKSH STAT COMP; Wald A., 1947, SEQUENTIAL ANAL; Xiao R, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P709; ZHU L, 2006, ADV NEURAL INFORM PR, P1617	34	11	11	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2009	83	2					149	163		10.1007/s11263-009-0229-x	http://dx.doi.org/10.1007/s11263-009-0229-x			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	423UE					2022-12-18	WOS:000264520400003
J	Zunic, J; Rosin, PL				Zunic, Jovisa; Rosin, Paul L.			An Alternative Approach to Computing Shape Orientation with an Application to Compound Shapes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape; Orientation; Image processing; Early vision	PRINCIPAL AXES; REPRESENTATION	We consider the method that computes the shape orientation as the direction alpha that maximises the integral of the length of projections, taken to the power of 2N, of all the straight line segments whose end points belong to the shape, to a line that has the slope alpha. We show that for N=1 such a definition of shape orientation is consistent with the shape orientation defined by the axis of the least second moment of inertia. For N > 1 this is not the case, and consequently our new method can produce different results. As an additional benefit our approach leads to a new method for computation of the orientation of compound objects.	[Zunic, Jovisa] Univ Exeter, Dept Comp Sci, Exeter EX4 4QF, Devon, England; [Zunic, Jovisa] Serbian Acad Arts & Sci, Math Inst, Belgrade, Serbia; [Rosin, Paul L.] Cardiff Univ, Sch Comp Sci, Cardiff CF24 3AA, S Glam, Wales	University of Exeter; Serbian Academy of Sciences & Arts; Cardiff University	Zunic, J (corresponding author), Univ Exeter, Dept Comp Sci, Exeter EX4 4QF, Devon, England.	J.Zunic@ex.ac.uk; Paul.Rosin@cs.cf.ac.uk						Boutsen L, 2001, PERCEPT PSYCHOPHYS, V63, P404, DOI 10.3758/BF03194408; Chandra DVS, 1998, IEEE T AERO ELEC SYS, V34, P1009, DOI 10.1109/7.705915; Cortadellas J, 2004, PATTERN RECOGN LETT, V25, P591, DOI 10.1016/j.patrec.2003.12.003; Fahlbusch S, 2005, J MATER PROCESS TECH, V167, P371, DOI 10.1016/j.jmatprotec.2005.06.022; Ha VHS, 2005, IEEE T IMAGE PROCESS, V14, P1687, DOI 10.1109/TIP.2005.857271; Jafari-Khouzani K, 2005, IEEE T PATTERN ANAL, V27, P1004, DOI 10.1109/TPAMI.2005.126; Jain R., 1995, MACHINE VISION; JIANG XY, 1991, PATTERN RECOGN, V24, P801, DOI 10.1016/0031-3203(91)90047-9; Kim S, 2008, PATTERN RECOGN, V41, P754, DOI 10.1016/j.patcog.2007.03.018; Kim WY, 1999, IEEE T PATTERN ANAL, V21, P768, DOI 10.1109/34.784290; Klette R., 2004, DIGITAL GEOMETRY; Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151; Lin JC, 1996, PATTERN RECOGN, V29, P477, DOI 10.1016/0031-3203(95)00095-X; LIN JC, 1993, PATTERN RECOGN, V26, P485, DOI 10.1016/0031-3203(93)90104-5; Mardia K.V., 2000, DIRECTIONAL STAT, P15, DOI [10.1002/9780470316979, DOI 10.1002/9780470316979]; MARENDAZ C, 1993, J EXP PSYCHOL HUMAN, V19, P1266, DOI 10.1037/0096-1523.19.6.1266; Martinez-Alajarin J, 2005, IEEE T SYST MAN CY C, V35, P488, DOI 10.1109/TSMCC.2004.843236; Palmer S.E., 1999, VISION SCI PHOTONS P; PALMER SE, 1980, COGNITIVE PSYCHOL, V12, P285, DOI 10.1016/0010-0285(80)90012-2; ROCK I, 1994, PERCEPTION, V23, P1409, DOI 10.1068/p231409; Shen D, 1996, ELECTRON LETT, V32, P1873, DOI 10.1049/el:19961249; Shi B, 1998, ENG GEOL, V50, P59, DOI 10.1016/S0013-7952(97)00082-3; TSAI WH, 1985, COMPUT VISION GRAPH, V29, P377, DOI 10.1016/0734-189X(85)90133-1; TSAI WH, 1991, PATTERN RECOGN, V24, P95, DOI 10.1016/0031-3203(91)90080-O; Wang L, 2003, IEEE T IMAGE PROCESS, V12, P1120, DOI 10.1109/TIP.2003.815251; Wei L, 2006, IEEE DATA MINING, P711; Zunic J, 2006, PATTERN RECOGN, V39, P856, DOI 10.1016/j.patcog.2005.11.010; Zunic J, 2006, IEEE T IMAGE PROCESS, V15, P3478, DOI 10.1109/TIP.2006.877527	28	11	11	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2009	81	2					138	154		10.1007/s11263-008-0149-1	http://dx.doi.org/10.1007/s11263-008-0149-1			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	393VI					2022-12-18	WOS:000262401600003
J	Leitao, HCG; Stolfi, J				Leitao, HCG; Stolfi, J			Measuring the information content of fracture lines	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						curve matching; jigsaw puzzles; information content; fractals; shape recognition	PARTIAL SURFACE; OBJECTS	Reassembling unknown broken objects from a large collection of fragments is a common problem in archaeology and other fields. Computer tools have recently been developed, by the authors and by others, which try to help by identifying pairs of fragments with matching outline shapes. Those tools have been successfully tested on small collections of fragments; here we address the question of whether they can be expected to work also for practical instances of the problem (10(3) to 10(5) fragments). To that end, we describe here a method to measure the average amount of information contained in the shape of a fracture line of given length. This parameter tells us how many false matches we can expect to find for it among a given set of fragments. In particular, the numbers we obtained for ceramic fragments indicate that fragment outline comparison should give useful results even for large instances of the problem.	Univ Fed Fluminense, Inst Comp, BR-24210240 Niteroi, RJ, Brazil; Univ Estadual Campinas, Inst Comp, BR-13083970 Campinas, SP, Brazil	Universidade Federal Fluminense; Universidade Estadual de Campinas	Leitao, HCG (corresponding author), Univ Fed Fluminense, Inst Comp, Rua Passo da Patria 156, BR-24210240 Niteroi, RJ, Brazil.	hcgl@ic.uff.br; stolfi@ic.unicamp.br	Stolfi, Jorge/B-3304-2012					Barequet G, 1999, COMP GEOM-THEOR APPL, V12, P45, DOI 10.1016/S0925-7721(98)00034-0; Barequet G, 1997, IEEE T PATTERN ANAL, V19, P929, DOI 10.1109/34.615444; BROWN CA, 1993, WEAR, V161, P61, DOI 10.1016/0043-1648(93)90453-S; Brown CA, 1997, PROCEEDINGS OF THE TWELFTH ANNUAL MEETING OF THE AMERICAN SOCIETY FOR PRECISION ENGINEERING, P118; BURDEA GC, 1989, IEEE T ROBOTIC AUTOM, V5, P752, DOI 10.1109/70.88097; KAMPEL M, 2003, P IEEE CVPR WORKSH A; Lathi B.P., 1998, MODERN DIGITAL ANALO; Leitao HCD, 2002, IEEE T PATTERN ANAL, V24, P1239, DOI 10.1109/TPAMI.2002.1033215; LEITAO HCG, 2000, 8 INT C CENTR EUR CO, V2, P389; LEITAO HCG, 1999, THESIS U CAMPINAS I; LEVOY M, 1999, SCANNING FRAGMENTS F; MCBRIDE JC, 2003, P IEEE CVPR WORKSH A; Papaioannou G, 2003, IMAGE VISION COMPUT, V21, P401, DOI 10.1016/S0262-8856(03)00008-8; Papamichail I, 2002, J GLOBAL OPTIM, V24, P1, DOI 10.1023/A:1016259507911; TAYLOR RI, 1994, IEE P-VIS IMAGE SIGN, V141, P422, DOI 10.1049/ip-vis:19941429; Ucoluk G, 1999, COMPUT GRAPH-UK, V23, P573, DOI 10.1016/S0097-8493(99)00075-8; WOLFSON HJ, 1990, IEEE T PATTERN ANAL, V12, P483, DOI 10.1109/34.55108; Wolfson HJ, 1997, IEEE COMPUT SCI ENG, V4, P10, DOI 10.1109/99.641604	18	11	11	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2005	65	3					163	174		10.1007/s11263-005-3226-8	http://dx.doi.org/10.1007/s11263-005-3226-8			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	999ZE					2022-12-18	WOS:000234429900003
J	Habuka, K; Shinagawa, Y				Habuka, K; Shinagawa, Y			Image interpolation using enhanced multiresolution critical-point filters	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						image matching; image interpolation; correspondence problem; optical flow; multiresolution critical-point filter	MAPPING FUNCTIONS; SELF-CALIBRATION; MOTION; CAMERA	There are increasing demands for image interpolation in various fields such as virtual reality, computer animation, and video transmission. The critical-point filters (CPF) we have proposed previously enable completely automatic matching of two images. In our previous method, however, it has taken a long time to compute the optimal parameter values by iterative searches. This paper proposes an improved algorithm called the enhanced critical-point filters (ECPF) where the parameters are stably computed using together the inverse mapping from the destination to the source. It takes only a second to match images whose size is 64 x 64. The algorithm is also improved in its precision by directly handling color images while the previous algorithm has taken only the intensity value into account. We apply ECPF to keyframe interpolation of video sequences. We also apply ECPF to interpolating two different views of an object to generate any intermediate views. In this case, there are many constraints that can be used to determine the mapping between the images. We propose a method to use such constraints to improve the accuracy of the mappings. As the results, image-based pseudo-3D models are easily created from a set of views without any special devices.	Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Habuka, K (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.							Alvarez L, 2000, INT J COMPUT VISION, V39, P41, DOI 10.1023/A:1008170101536; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309; Catmull E., 1980, Computer Graphics, V14, P279, DOI 10.1145/965105.807505; Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153; Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395; Debrunner C, 1998, IEEE T PATTERN ANAL, V20, P206, DOI 10.1109/34.659941; FANT KM, 1986, IEEE COMPUT GRAPH, V6, P71, DOI 10.1109/MCG.1986.276613; Gao PS, 1998, VISUAL COMPUT, V14, P390, DOI 10.1007/s003710050150; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; GOSHTASBY A, 1986, PATTERN RECOGN, V19, P459, DOI 10.1016/0031-3203(86)90044-0; GOSHTASBY A, 1987, PATTERN RECOGN, V20, P525, DOI 10.1016/0031-3203(87)90079-3; GUPTA NC, 1995, ARTIF INTELL, V78, P45, DOI 10.1016/0004-3702(95)00031-3; HAN M, 1999, CMURITR9922; Harder RL, 1972, J AIRCR, V9, P189, DOI [10.2514/3.44330., DOI 10.2514/3.44330]; Horn B., 1986, ROBOT VISION, P1; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Luong QT, 1997, INT J COMPUT VISION, V22, P261, DOI 10.1023/A:1007982716991; OTTE M, 1995, ARTIF INTELL, V78, P5, DOI 10.1016/0004-3702(95)00033-X; Pratt W, 1991, DIGITAL IMAGE PROCES; Russ J. C., 2016, IMAGE PROCESSING HDB; SEITZ SM, 1996, P SIGGRAPH 96, P21; Shinagawa Y, 1998, IEEE T PATTERN ANAL, V20, P994, DOI 10.1109/34.713364; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388; Wolberg G, 1990, DIGITAL IMAGE WARPIN	27	11	12	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2004	58	1					19	35		10.1023/B:VISI.0000016145.44583.5f	http://dx.doi.org/10.1023/B:VISI.0000016145.44583.5f			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	823OD					2022-12-18	WOS:000221621600003
J	Hadjidemetriou, E; Grossberg, MD; Nayar, SK				Hadjidemetriou, E; Grossberg, MD; Nayar, SK			Histogram preserving image transformations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						histogram preservation; Hamiltonian transformation; local transformation; weak perspective projection; paraperspective projection; histogram based recognition	MOTION FIELD; OPTICAL-FLOW; SINGULARITIES; RECOGNITION	Histograms are used to analyze and index images. They have been found experimentally to have low sensitivity to certain types of image morphisms, for example, viewpoint changes and object deformations. The precise effect of these image morphisms on the histogram, however, has not been studied. In this work we derive the complete class of local transformations that preserve or scale the magnitude of the histogram of all images. We also derive a more general class of local transformations that preserve the histogram relative to a particular image. To achieve this, the transformations are represented as solutions to families of vector fields acting on the image. The local effect of fixed points of the fields on the histograms is also analyzed. The analytical results are verified with several examples. We also discuss several applications and the significance of these transformations for histogram indexing.	Columbia Univ, Dept Comp Sci, New York, NY 10027 USA	Columbia University	Hadjidemetriou, E (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.	stathis@cs.columbia.edu						Abraham R., 1978, FDN MECH; ANASTASSIOU D, 1989, IEEE T CIRCUITS SYST, V36, P1175, DOI 10.1109/31.34663; Andronov A.A., 1973, QUALITATIVE THEORY 2; Arneodo A, 1999, PHYS REV LETT, V83, P1255, DOI 10.1103/PhysRevLett.83.1255; Arnold V. I., 1989, GRAD TEXTS MATH, V60; Bach JR, 1996, P SOC PHOTO-OPT INS, V2670, P76, DOI 10.1117/12.234785; Basri R, 1996, INT J COMPUT VISION, V19, P169, DOI 10.1007/BF00055803; Bouzouba K, 2000, PATTERN RECOGN LETT, V21, P691, DOI 10.1016/S0167-8655(00)00028-3; Cohen S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1076, DOI 10.1109/ICCV.1999.790393; FINLAYSON GD, 1996, P 4 EUR C COMP VIS, V2, P16; Foley James D, 1996, COMPUTER GRAPHICS PR, V12110; FORD RM, 1994, CVGIP-GRAPH MODEL IM, V56, P75, DOI 10.1006/cgip.1994.1007; Giachetti A, 1996, INT J COMPUT VISION, V18, P255, DOI 10.1007/BF00123144; GINNEKEN BV, 2000, J VISUAL COMMUNICATI, V11, P196; GLASBEY CA, 1993, CVGIP-GRAPH MODEL IM, V55, P532, DOI 10.1006/cgip.1993.1040; Griffin LD, 1997, IMAGE VISION COMPUT, V15, P369, DOI 10.1016/S0262-8856(97)87979-6; HAASER NB, 1971, REAL ANAL; Hadjidemetriou E, 2000, PROC CVPR IEEE, P410, DOI 10.1109/CVPR.2000.855848; HALSEY TC, 1986, PHYS REV A, V33, P1141, DOI 10.1103/PhysRevA.33.1141; Huang T. S., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P361, DOI 10.1109/ICPR.1990.118129; JAGERSAND M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P195, DOI 10.1109/ICCV.1995.466786; KASS M, 1987, COMPUT VISION GRAPH, V37, P362, DOI 10.1016/0734-189X(87)90043-0; Koenderink JJ, 1999, INT J COMPUT VISION, V31, P159, DOI 10.1023/A:1008065931878; Levy Vehel J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P661, DOI 10.1109/CVPR.1992.223207; Marsden J.E, 1988, VECTOR CALCULUS; MOGHADDAM B, 1995, INT WORKSH AUT FAC G, P122; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173; Pass G., 1996, P 4 ACM INT C MULT, P65, DOI DOI 10.1145/244130.244148; RAO AR, 1992, IEEE T PATTERN ANAL, V14, P693, DOI 10.1109/34.142908; Rose J.S., 1994, COURSE GROUP THEORY; Royden H.L., 1968, REAL ANAL; SANDER PT, 1992, IEEE T PATTERN ANAL, V14, P309, DOI 10.1109/34.120326; Schiele B, 2000, INT J COMPUT VISION, V36, P31, DOI 10.1023/A:1008120406972; SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519; Smith J., 1996, P SOC PHOTO-OPT INS, V2670, P1630; Spivak M., 1965, CALCULUS MANIFOLDS, DOI DOI 10.1201/9780429501906; Sporring J, 1999, IEEE T INFORM THEORY, V45, P1051, DOI 10.1109/18.761342; STRICKER M, 1995, P SOC PHOTO-OPT INS, V2420, P381; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487; Swarninathan R., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P413, DOI 10.1109/CVPR.1999.784714; TSALLIS C, 1988, J STAT PHYS, V52, P479, DOI 10.1007/BF01016429; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; ULICHNEY RA, 1988, P IEEE, V76, P56, DOI 10.1109/5.3288; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; VERRI A, 1989, J OPT SOC AM A, V6, P698, DOI 10.1364/JOSAA.6.000698; Weng J., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P246, DOI 10.1109/ICPR.1990.118105; Wu HS, 1998, IEEE T SYST MAN CY B, V28, P227, DOI 10.1109/3477.662762; Zhang H. J., 1993, MULTIMEDIA SYSTEMS, V1, P10, DOI DOI 10.1007/BF01210504; ZHANG HJ, 1995, P ACM MULT 95 SAN FR, P15, DOI DOI 10.1145/217279.215068	50	11	11	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2001	45	1					5	23		10.1023/A:1012356022268	http://dx.doi.org/10.1023/A:1012356022268			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	487JN					2022-12-18	WOS:000171872100001
J	Cooper, MC				Cooper, MC			The interpretation of line drawings with contrast failure and shadows	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						line drawing labelling; contrast failure; shadows; coplanarity constraints; extended junction constraints; constraint satisfaction problem	CURVED OBJECTS; SHAPE; IMAGE; COLOR	In line drawings derived from real images, lines may be missing due to contrast failure and objects with curved surfaces may cast shadows from multiple light sources. This paper shows that it is the presence of shadows, rather than contrast failure, that renders the line drawing labelling problem NP-complete. However, shadows are a valuable visual cue, since their presence is formally shown to reduce the average ambiguity of drawings. This is especially true when constraints concerning shadow formation are employed to differentiate shadow and non-shadow lines. The extended junction constraint, concerning straight lines colinear with junctions, compensates the loss of information caused by contrast failure. In fact, we observe the contrast failure paradox: a drawing is sometimes less ambiguous when lines are partly missing due to contrast failure. It is known that the coplanarity of sets of object vertices can be deduced from the presence of straight lines in the drawing. This paper shows that these coplanarity constraints are robust to the presence of contrast failure.	Univ Toulouse 3, IRIT, F-31062 Toulouse, France	Universite de Toulouse; Universite Toulouse III - Paul Sabatier	Cooper, MC (corresponding author), Univ Toulouse 3, IRIT, 118 Route Narbonne, F-31062 Toulouse, France.		Cooper, Martin/AAV-1705-2021; Cooper, Martin/AAE-8777-2020	Cooper, Martin/0000-0003-4853-053X; Cooper, Martin/0000-0003-4853-053X				CLOWES MB, 1971, ARTIF INTELL, V2, P79, DOI 10.1016/0004-3702(71)90005-1; Cooper MC, 1997, ARTIF INTELL, V90, P1, DOI 10.1016/S0004-3702(96)00018-5; Cooper MC, 1999, ARTIF INTELL, V108, P31, DOI 10.1016/S0004-3702(98)00118-0; COOPER MC, 1993, IMAGE VISION COMPUT, V11, P82, DOI 10.1016/0262-8856(93)90074-Q; Cooper MC, 1997, IMAGE VISION COMPUT, V15, P263, DOI 10.1016/S0262-8856(96)01135-3; Cooper MC, 2000, ARTIF INTELL, V119, P235, DOI 10.1016/S0004-3702(00)00008-4; COOPER MC, IN PRESS FUZZY SETS; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FREUDER EC, 1991, PROCEEDINGS : NINTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P227; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; KIROUSIS LM, 1988, J COMPUT SYST SCI, V37, P14, DOI 10.1016/0022-0000(88)90043-8; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; MALIK J, 1989, IEEE T PATTERN ANAL, V11, P555, DOI 10.1109/34.24791; MELHORN K, 1974, GRAPH ALGORITHMS NP; NALWA VS, 1988, INT J COMPUT VISION, V2, P103, DOI 10.1007/BF00133696; PARODI P, 1994, ARTIF INTELL, V70, P239, DOI 10.1016/0004-3702(94)90107-4; RUBIN JM, 1982, BIOL CYBERN, V45, P215, DOI 10.1007/BF00336194; Shimshoni I, 1997, COMPUT VIS IMAGE UND, V65, P296, DOI 10.1006/cviu.1996.0569; STRACHAN NJC, 1993, IMAGE VISION COMPUT, V11, P2, DOI 10.1016/0262-8856(93)90027-E; SUGIHARA K, 1984, IEEE T PATTERN ANAL, V6, P578, DOI 10.1109/TPAMI.1984.4767571; SUGIHARA K, 1982, IEEE T PATTERN ANAL, V4, P458, DOI 10.1109/TPAMI.1982.4767289; Sugihara K., 1986, MACHINE INTERPRETATI; TAI A, 1993, IMAGE VISION COMPUT, V11, P240, DOI 10.1016/0262-8856(93)90042-F; Tsang E., 1993, FDN CONSTRAINT SATIS; Waltz D., 1975, PSYCHOL COMPUTER VIS, P19	26	11	16	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2001	43	2					75	97		10.1023/A:1011166601983	http://dx.doi.org/10.1023/A:1011166601983			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	451EH					2022-12-18	WOS:000169787800001
J	Maybank, S; Tan, T				Maybank, S; Tan, T			A special section on visual surveillance - Introduction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Univ Reading, Dept Comp Sci, Reading, Berks, England; Chinese Acad Sci, Natl Lab Pattern Recognit, Beijing, Peoples R China	University of Reading; Chinese Academy of Sciences	Maybank, S (corresponding author), Univ Reading, Dept Comp Sci, Reading, Berks, England.								0	11	18	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2000	37	2					173	173		10.1023/A:1008151520284	http://dx.doi.org/10.1023/A:1008151520284			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	341FD					2022-12-18	WOS:000088579600003
J	Marchand, E; Chaumette, F				Marchand, E; Chaumette, F			An autonomous active vision system for complete and accurate 3D scene reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						scene reconstruction; active vision; Bayes nets; exploration; visual serving	PERCEPTION; ROBOTICS; MOTION; SHAPE; VIEW	We propose in this paper an active vision approach for performing the 3D reconstruction of static scenes. The perception-action cycles are handled at various levels: from the definition of perception strategies for scene exploration downto the automatic generation of camera motions using visual servoing. To perform the reconstruction, we use a structure from controlled motion method which allows an optimal estimation of geometrical primitive parameters. As this method is based on particular camera motions, perceptual strategies able to appropriately perform a succession of such individual primitive reconstructions are proposed in order to recover the complete spatial structure of the scene. Two algorithms are proposed to ensure the exploration of the scene. The former is an incremental reconstruction algorithm based on the use of a prediction/verification scheme managed using decision theory and Bayes nets. It allows the visual system to get a high level description of the observed part of the scene. The latter, based on the computation of new viewpoints ensures the complete reconstruction of the scene. Experiments carried out on a robotic cell have demonstrated the validity of our approach.	Inst Natl Rech Informat & Automat, IRISA, F-35042 Rennes, France		Marchand, E (corresponding author), Inst Natl Rech Informat & Automat, IRISA, Campus Beaulieu, F-35042 Rennes, France.		Francois, Chaumette/AAH-1481-2021; Marchand, Eric/AAF-2809-2019	Francois, Chaumette/0000-0002-1238-4385; Marchand, Eric/0000-0001-7096-5236				ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; ALOIMONOS J, 1990, 10TH P INT C PATT RE, V1, P346; ALOIMONOS Y, 1987, INT J COMPUT VISION, V1, P333; BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968; BALL SB, 1991, ORGAN BEHAV HUM DEC, V48, P1, DOI 10.1016/0749-5978(91)90002-B; Boukir S, 1998, MACH VISION APPL, V10, P321, DOI 10.1007/s001380050082; BOUTHEMY P, 1989, IEEE T PATTERN ANAL, V11, P499, DOI 10.1109/34.24782; BUXTON H, 1995, ARTIF INTELL, V78, P431, DOI 10.1016/0004-3702(95)00041-0; Chaumette F, 1996, IEEE T PATTERN ANAL, V18, P492, DOI 10.1109/34.494639; CHIEN CH, 1989, IEEE T PATTERN ANAL, V11, P372, DOI 10.1109/34.19034; CONNOLLY C, 1985, IEEE INT C ROB AUT, P432; COWAN GK, 1988, IEEE T PATTERN ANAL, V10, P407; DJIAN D, 1995, P INT C ADV ROB ICAR, P895; ESPIAU B, 1992, IEEE T ROBOTIC AUTOM, V8, P313, DOI 10.1109/70.143350; ESPIAU B, 1987, IEEE INT C ROB AUT R, V3, P1436; Hager GD, 1998, COMPUT VIS IMAGE UND, V69, P23, DOI 10.1006/cviu.1997.0586; HASHIMOTO K, 1993, WORLD SCI SERIES ROB, V7; Hutchinson S, 1996, IEEE T ROBOTIC AUTOM, V12, P651, DOI 10.1109/70.538972; HUTCHINSON SA, 1989, IEEE T ROBOTIC AUTOM, V5, P765, DOI 10.1109/70.88098; KRAUSE P, 1993, REPRESENTING UNCERTA; KUTULAKOS KN, 1994, INT J COMPUT VISION, V12, P113, DOI 10.1007/BF01421200; LEGUERNIC P, 1991, P IEEE, V79, P1321, DOI 10.1109/5.97301; Marchand E, 1997, IEEE T CONTR SYST T, V5, P200, DOI 10.1109/87.556025; MARCHAND E, 1996, THESIS U RENNES 1; MARCHAND E, 1996, IEEE RSJ INT C INT R, V3, P1083; MARCHAND E, 1996, IEEE INT C COMP VIS, P169; Marr D., 1982, VISION COMPUTATIONAL; MAVER J, 1993, IEEE T PATTERN ANAL, V15, P417, DOI 10.1109/34.211463; Neapolitan R.E., 1990, PROBABILISTIC REASON; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; REED MK, 1997, IEEE INT C COMP VIS, P72; RIMEY RD, 1994, INT J COMPUT VISION, V12, P173, DOI 10.1007/BF01421202; RIMEY RD, 1991, INT J COMPUT VISION, V7, P47, DOI 10.1007/BF00130489; Samson C., 1991, ROBOT CONTROL TASK F; SWAIN MJ, 1993, INT J COMPUT VISION, V11, P109, DOI 10.1007/BF01469224; TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P86, DOI 10.1109/70.345940; TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P72, DOI 10.1109/70.345939; TRIGGS B, 1995, IEEE INT C ROB AUT N, V2, P1732; WENG J, 1990, IAPR INT C PATT REC, V1, P168; WIXSON L, 1994, IEEE INT C COMP VIS, P800; XIE M, 1989, IEEE WORKSH INT 3D S, P91	41	11	11	2	7	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1999	32	3					171	194		10.1023/A:1008161528515	http://dx.doi.org/10.1023/A:1008161528515			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	229HK					2022-12-18	WOS:000082187500001
J	Holt, RJ; Netravali, AN				Holt, RJ; Netravali, AN			Number of solutions for motion and structure from multiple frame correspondence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							POLYNOMIAL SYSTEMS; COMPUTATION	Much of the dynamic computer vision literature deals with the determination of motion and structure by observing two frames captured at two instants of time. Motion prediction and understanding can be improved significantly, particularly in the presence of noise, by analyzing an image sequence containing more than two frames. In this paper, we assume knowledge of correspondence of points on the surface of an object which is moving with constant motion, i.e., constant translation and constant rotation around an unknown center. We give a new formulation of the problem and prove that the following results hold in general for the number of solutions to motion and structure values (i.e., values of translation, rotation, and depth): (a) For three point correspondences over three views, there are at most two solutions, only one of which has all positive depth values; (b) For two point correspondences over four views, there is a unique solution; (c) For one point correspondence over five views, there can be up to ten solutions; (d) For one point correspondence over six views, there is a unique solution. The method of solution for each of the above formulations requires the solving of a system of multivariate polynomials, whose coefficients are functions of the observed data. In order to determine the number of solutions to these systems, we use theorems from algebraic geometry which imply that under a few mild conditions, the number of solutions at one set of data points provides an upper bound on the number of solutions for almost all sets of data points. Thus a bound on the number of solutions is obtained when a single system is solved by a method such as homotopy continuation, which we use here.			Holt, RJ (corresponding author), AT&T BELL LABS,MURRAY HILL,NJ 07974, USA.		Holt, Robert J/B-5460-2009					AGGARWAL JK, 1988, P IEEE, V76, P917, DOI 10.1109/5.5965; BOGEN R, 1983, MACSYMA REFERENCE MA; Borel A., 1969, LINEAR ALGEBRAIC GRO; BROIDA TJ, 1986, MAY P IEEE WORKSH MO, P95; Buchberger B., 1979, LECT NOTES COMPUT SC, V72, P3; Canny, 1988, COMPLEXITY ROBOT MOT; CHAR BW, 1985, MAPLE USERS GUIDE; HARTSHORNE R, 1977, ALGEBRAIC GEOMETRY; HOFFMAN DD, 1986, BIOL CYBERN, V54, P71, DOI 10.1007/BF00320477; Holt R., 1993, Journal of Visual Communication and Image Representation, V4, P14, DOI 10.1006/jvci.1993.1002; HOLT RJ, 1991, CVGIP-IMAG UNDERSTAN, V54, P368, DOI 10.1016/1049-9660(91)90037-P; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HORN BKP, 1987, INT J COMPUT VISION, V1, P259, DOI 10.1007/BF00127824; Huang T. S., 1986, Proceedings of the Workshop on Motion: Representation and Analysis (Cat. No.86CH2322-6), P125; HUANG TS, 1994, P IEEE, V82, P252, DOI 10.1109/5.265351; HUANG TS, 1987, ENCY ARTIFICIAL INTE, P620; Limb J. O., 1975, Computer Graphics and Image Processing, V4, P311, DOI 10.1016/0146-664X(75)90001-5; MORGAN A, 1987, APPL MATH COMPUT, V24, P101, DOI 10.1016/0096-3003(87)90063-4; MORGAN AP, 1989, APPL MATH COMPUT, V29, P123, DOI 10.1016/0096-3003(89)90099-4; MORGAN AP, 1989, ACM T MATH SOFTWARE, V15, P93, DOI 10.1145/63522.64124; Netravali A. N., 1989, International Journal of Imaging Systems and Technology, V1, P78, DOI 10.1002/ima.1850010110; NETRAVALI AN, 1985, AT&T TECH J, V64, P335, DOI 10.1002/j.1538-7305.1985.tb00436.x; NETRAVALI AN, 1979, AT&T TECH J, V58, P631, DOI 10.1002/j.1538-7305.1979.tb02238.x; Shafarevich IR., 1974, BASIC ALGEBRAIC GEOM, DOI 10.1007/978-3-642-96200-4; SHARIAT H, 1990, IEEE T PATTERN ANAL, V12, P417, DOI 10.1109/34.55102; SHARIAT H, 1986, THESIS U SO CALIFORN; SHARIAT H, 1986, 202 IRIS U SO CAL DE; STILLMAN M, 1989, MACAULAY USERS MANUA; SUBBARAO M, 1988, INTERPRETATION VISUA; WAXMAN AM, 1984, P 1 C ART INT, P12; [No title captured]	31	11	13	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	1997	23	1					5	15		10.1023/A:1007966223801	http://dx.doi.org/10.1023/A:1007966223801			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XK559					2022-12-18	WOS:A1997XK55900001
J	Vieville, T; Zeller, C; Robert, L				Vieville, T; Zeller, C; Robert, L			Using collineations to compute motion and structure in an uncalibrated image sequence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							SCENE	We address the well-known problem of estimating the motion and structure of a plane, but in the case where the visual system is not calibrated and in a monocular image-sequence. We first define plane collineations and analyse some of their properties when used to analyse the retinal motion in an uncalibrated image sequence. We show how to relate them to the Euclidean parameters of the scene. In particular, we discuss how to detect and estimate the collineation of the plane at infinity and use this quantity for auto-calibration. More precisely: We have been able to elaborate a method to estimate robustly any collineation in the image as soon as at least four projections have been established, especially for points at infinity and the collineation of this virtual infinite plane; It is shown that, given at least four points of a stationary plane and two stationary points not on the plane (or equivalently 2 planes) we can compute the focus of expansion; A step further, we have defined a bi-ratio of distances for a point with respect to a plane which allows us to analyse not only the relative position of this point with respect to the plane but also quantify this distance; Moreover a necessary and sufficient condition for a collineation to correspond to a stationary plane is given in the affine case; It is also discussed that when given three views and the plane at infinity, the intrinsics calibration parameters of the camera can be recovered from linear equations. Robust estimations of collineation and statistical tests are then developed and illustrated by some experimental results.			Vieville, T (corresponding author), INRIA SOPHIA, BP93, F-06902 VALBONNE, FRANCE.							AYACHE N, 1985, IEEE T PATTERN ANAL, V7, P384; BLAKE M, 1993, 4 ICCV IEEE SOC, P231; BUCHANAN T, 1988, COMPUT VISION GRAPH, V42, P130, DOI 10.1016/0734-189X(88)90146-6; COLLINS RT, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P400; DERICHE R, 1991, OR M; DERICHE R, 1990, 1ST P EUR C COMP VIS, P259; FAUGERAS O, 1992, 2 ECCV GEN; FAUGERAS O, 1993, 3 DIMENSIOANL COMPUT; FAUGERAS O, 1993, 2013 INRIA; Faugeras O. D., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P25; FRANCOIS E, 1991, C COMPUTER VISION PA, P166; GIAICHECA B, 1993, RR1906 INRIA; Hartley R., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P549, DOI 10.1109/CVPR.1993.341076; HUANG T, 1990, SIGNAL PROCESSING 1; JUPP PE, 1989, INT STAT REV, P57; Kumar P, 1986, STOCHASTIC SYSTEMS E; LAVEST J, 1993, INTELLIGENT AUTONOMO; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LUONG Q, 1993, IEEE P CVPR93 NEW YO, P194; LUONG Q, 1992, RR1894 INRIA; LUONG QT, 1994, 3 ECCV STOCKH; LUONG QT, 1992, THESIS U PARISSUD OR; LYONG Q, 1993, RR1894 INRIA; Murray D. W., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P403, DOI 10.1109/ICCV.1993.378187; MURRAY DW, 1987, IEEE T PATTERN ANAL, V9, P220, DOI 10.1109/TPAMI.1987.4767896; PAHLAVAN K, 1992, 2ND P EUR C COMP VIS, P526; Peleg S., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P109, DOI 10.1109/ICPR.1990.118074; ROBERT L, 1993, 4 ICCV BERL; Semple J., 1979, ALGEBRAIC PROJECTIVE; SHALOM YB, 1988, TRACKING DATA ASS; STEPHENS M, 1989, COMPUTER VISION PATT, P556; THACKER NA, 1992, BRIT MACHINE VISION; TOSCANI G, 1987, P INT WORKSH MACH IN; TRIVEDI H, 1991, IMAGE VISION COMPUT, P9; TSAI RY, 1982, IEEE T ACOUST SPEECH, V30, P525, DOI 10.1109/TASSP.1982.1163931; TSAI RY, 1989, ROBOTICS REV, V1, P147; VIEVILLE T, 1994, IMAGE VISION COMPUT, P12; VIEVILLE T, 1990, 1ST P ECCV ANT, P281; VIEVILLE T, 1993, HDB ACTIVE VISION CO; VIEVILLE T, 1992, RR1669 INRIA; VIEVILLE T, 1994, IN PRESS MACHINE VIS; VIEVILLE T, 1994, IN PRESS INT J COMPU; VIEVILLE T, 1992, 7TH IEEE S INT CONTR, P348; WAXMAN AM, 1985, INT J ROBOT RES, P4; ZHANG Z, 1994, IN PRESS P INT S YOU	45	11	11	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1996	20	3					213	242						30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VW629					2022-12-18	WOS:A1996VW62900004
J	WU, TH; CHELLAPPA, R; ZHENG, QF				WU, TH; CHELLAPPA, R; ZHENG, QF			EXPERIMENTS ON ESTIMATING EGOMOTION AND STRUCTURE PARAMETERS USING LONG MONOCULAR IMAGE SEQUENCES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							3-DIMENSIONAL MOTION PARAMETERS; RIGID PLANAR PATCH; NOISY IMAGES; UNIQUENESS; OBJECTS; FRAMES; CORRESPONDENCES; NAVIGATION	This paper presents a simple but robust model based approach to estimating the kinematics of a moving camera and the structure of the objects in a stationary environment using long, noisy, monocular image sequences. Both batch and recursive algorithms are presented and the problem due to occlusion is addressed. The approach is based on representing the constant translational velocity and constant angular velocity of the camera motion using nine rectilinear motion parameters, which are 3-D vectors of the position of the rotation center, linear and angular velocities. The structure parameters are 3-D coordinates of the salient feature points in the inertial coordinate system. Due to redundancies in parameterization, the total number of independent parameters to be estimated is 3M + 7, where M is the number of feature points. The image plane coordinates of these feature points in each frame are first detected and matched over the frames. These noisy image coordinates serve as the input to our algorithms. Due to the nonlinear nature of perspective projection, a nonlinear least squares method is formulated for the batch algorithm, and a conjugate gradient method is then applied to find the solution. A recursive method using an Iterated Extended Kalman Filter (IEKF) for incremental estimation of motion and structure is also presented. Since the plant model is simple in our formulation, closed form solutions for the state and covariance transition equations are easily derived. Experimental results for simulated imagery as well as several real image sequences are included.	UNIV MARYLAND,CTR AUTOMAT RES,DEPT ELECT ENGN,COLLEGE PK,MD 20742; UNIV MARYLAND,INST ADV COMP STUDIES,COLLEGE PK,MD 20742; UNIV MARYLAND,CTR AUTOMAT RES,COMP VIS LAB,COLLEGE PK,MD 20742	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	WU, TH (corresponding author), UNIV SO CALIF,INST SIGNAL & IMAGE PROC,DEPT ELECT ENGN SYST,LOS ANGELES,CA 90089, USA.		ZHENG, Qin/T-2925-2019; Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/AAV-8690-2020					AGGARWAL JK, 1985, 3RD P IEEE WORKSH VI, P127; Ando H., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P101, DOI 10.1109/WVM.1991.212781; BROIDA TJ, 1991, IEEE T PATTERN ANAL, V13, P497, DOI 10.1109/34.87338; BROIDA TJ, 1990, IEEE T AERO ELEC SYS, V26, P639, DOI 10.1109/7.55557; BROIDA TJ, 1986, IEEE T PATTERN ANAL, V8, P90, DOI 10.1109/TPAMI.1986.4767755; BROIDA TJ, 1989, J OPT SOC AM A, V6, P879, DOI 10.1364/JOSAA.6.000879; CHANDRASHEKHAR S, 1992, J ROBOTIC SYST, V9, P729; CUI N, 1991, P IEEE WORKSHOP VISU, P75; DAUGMAN JG, 1988, P IEEE INT C NEURAL, P547; Dickmanns E.D., 1988, MACH VISION APPL, V1, P241; DICKMANNS ED, 1988, MACH VISION APPL, V1, P233; Dutta R., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P159, DOI 10.1109/CVPR.1989.37844; FANG JQ, 1984, IEEE T PATTERN ANAL, V6, P545, DOI 10.1109/TPAMI.1984.4767569; FRANZEN WO, 1991, USCIRIS266 U SO CAL; GENNERY DB, 1992, INT J COMPUT VISION, V7, P243, DOI 10.1007/BF00126395; HEEL J, 1990, AI1190 MIT AI LAB ME; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; Huang T. S., 1986, Proceedings of the Workshop on Motion: Representation and Analysis (Cat. No.86CH2322-6), P45; HUANG TS, 1981, IMAGE SEQUENCE ANAL; Jazwinski A.H., 1970, STOCHASTIC PROCESSES; JERIAN CP, 1991, IEEE T SYST MAN CYB, V21, P572, DOI 10.1109/21.97478; KUMAR R, 1990, P DARPA IMAGE UNDERS, P660; Longuet-Higgins H. C., 1984, First Conference on Artificial Intelligence Applications (Cat. No. 84CH2107-1), P395; Manjunath B. S., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P373, DOI 10.1109/CVPR.1992.223162; MATTHIES L, 1987, IEEE T ROBOTIC AUTOM, V3, P239, DOI 10.1109/JRA.1987.1087097; Maybeck P. S., 1982, STOCHASTIC MODELS ES, V2; Meyer F., 1992, Proceedings. 11th IAPR International Conference on Pattern Recognition. Vol.1. Conference A: Computer Vision and Applications, P78, DOI 10.1109/ICPR.1992.201512; NEGAHDARIPOUR S, 1987, IEEE T PATTERN ANAL, V9, P168, DOI 10.1109/TPAMI.1987.4767884; Oliensis J., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P8, DOI 10.1109/WVM.1991.212794; PORAT M, 1988, IEEE T PATTERN ANAL, V10, P452, DOI 10.1109/34.3910; RANADE S, 1980, PATTERN RECOGN, V12, P269, DOI 10.1016/0031-3203(80)90067-9; ROACH JW, 1980, IEEE T PATTERN ANAL, V2, P554, DOI 10.1109/TPAMI.1980.6447703; Sawhney H. S., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P179, DOI 10.1109/CVPR.1991.139684; SETHI IK, 1987, IEEE T PATTERN ANAL, V9, P56, DOI 10.1109/TPAMI.1987.4767872; SHARIAT H, 1990, IEEE T PATTERN ANAL, V12, P417, DOI 10.1109/34.55102; Taylor C. J., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P242, DOI 10.1109/WVM.1991.212801; THOMAS JI, 1992, P DARPA IMAGE UNDERS, P507; TIAN Q, 1986, COMPUT VISION GRAPH, V35, P220, DOI 10.1016/0734-189X(86)90028-9; Tomasi C., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P21, DOI 10.1109/WVM.1991.212792; TSAI RY, 1982, IEEE T ACOUST SPEECH, V30, P525, DOI 10.1109/TASSP.1982.1163931; TSAI RY, 1981, IEEE T ACOUST SPEECH, V29, P1147, DOI 10.1109/TASSP.1981.1163710; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; TSAI RY, 1984, IEEE T ACOUST SPEECH, V32, P213, DOI 10.1109/TASSP.1984.1164313; WENG JY, 1987, IEEE T PATTERN ANAL, V9, P370, DOI 10.1109/TPAMI.1987.4767920; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P318, DOI 10.1109/34.120327; WILLIAMS LR, 1988, P DARPA IM UND WORKS, P1047; WU TH, 1992, CARTR646 U MAR CTR A; YOUNG GSJ, 1990, IEEE T PATTERN ANAL, V12, P735, DOI 10.1109/34.57666; ZHANG ZY, 1992, INT J COMPUT VISION, V7, P211, DOI 10.1007/BF00126394; ZHENG Q, 1992, UNPUB INT J COMPUTER; ZHENG Q, 1992, ICPR, P193; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658; [No title captured]	53	11	11	0	6	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1995	15	1-2					77	103		10.1007/BF01450850	http://dx.doi.org/10.1007/BF01450850			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QY042					2022-12-18	WOS:A1995QY04200004
J	GRIMSON, WEL				GRIMSON, WEL			ON THE RECOGNITION OF PARAMETERIZED 2D OBJECTS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									MIT, ARTIFICIAL INTELLIGENCE LAB, CAMBRIDGE, MA 02139 USA	Massachusetts Institute of Technology (MIT)								AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; AYACHE NJ, 1982, 6TH P INT C PATT REC; BAIRD H, 1986, MODEL BASED IMAGE MA; BESL PJ, 1985, COMPUT SURV, V17, P75, DOI 10.1145/4078.4081; BHANU B, 1984, IEEE T PAMI, V6; Binford T. O., 1982, INT J ROBOT RES, V1, P18; Bolles R. C., 1982, INT J ROBOT RES, V1, P57; BOLLES RC, 1986, INT J ROBOT RES, V5, P3, DOI 10.1177/027836498600500301; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; DAVIS LS, 1979, IEEE T PATTERN ANAL, V1, P60, DOI 10.1109/TPAMI.1979.4766876; FAUGERAS OD, 1983, 8TH P INT JOINT C AR, P996; FREUDER EC, 1982, J ACM, V29, P24, DOI 10.1145/322290.322292; FREUDER EC, 1978, COMMUN ACM, V21, P958, DOI 10.1145/359642.359654; GOAD C, 1983, P DARPA IMAGE UNDERS; GRIMSON WEL, 1986, IEEE J ROBOT AUTOM, V2, P196, DOI 10.1109/JRA.1986.1087057; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; GRIMSON WEL, 1986, J ACM, V33, P658, DOI 10.1145/6490.6492; GRIMSON WEL, 1987, MIT AI983 LAB MEM; GRIMSON WEL, 1988, IN PRESS IEEE T PAMI; GRIMSON WEL, 1988, MIT1019 ART INT LAB; HARALICK RM, 1980, ARTIF INTELL, V14, P263, DOI 10.1016/0004-3702(80)90051-X; HARALICK RM, 1979, IEEE T PATTERN ANAL, V1, P173, DOI 10.1109/TPAMI.1979.4766903; HUTTENLOCHER DP, 1988, THESIS MIT; HUTTENLOCHER DP, 1987, 1ST P INT C COMP VIS, P478; IKEUCHI K, 1987, INT J COMPUT VISION, V1, P145, DOI 10.1007/BF00123163; JACOBS D, 1988, THESIS MIT; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; MACKWORTH AK, 1977, ARTIF INTELL, V8, P99, DOI 10.1016/0004-3702(77)90007-8; MACKWORTH AK, 1985, ARTIF INTELL, V25, P65, DOI 10.1016/0004-3702(85)90041-4; MONTANAR.U, 1974, INFORM SCIENCES, V7, P95, DOI 10.1016/0020-0255(74)90008-5; STOCKMAN G, TR84002 MICH STAT U; THOMPSON DW, 1987, IEEE J ROBOTIC AUTOM, P208; TOMITA F, 1985, ROBOTICS RES, V2, P35; Waltz D., 1975, PSYCHOL COMPUTER VIS, P19; [No title captured]	36	11	11	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1989	2	4					353	372		10.1007/BF00133555	http://dx.doi.org/10.1007/BF00133555			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC192					2022-12-18	WOS:A1989AC19200001
J	MURRAY, DW; COOK, DB				MURRAY, DW; COOK, DB			USING THE ORIENTATION OF FRAGMENTARY 3D EDGE SEGMENTS FOR POLYHEDRAL OBJECT RECOGNITION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article											MURRAY, DW (corresponding author), GEC RES LTD,HIRST RES CTR,LONG RANGE RES LAB,WEMBLEY HA9 7PP,MIDDX,ENGLAND.							BOLLES RC, 1983, 8TH P IJCAI KARLSR, P116; CANNY JF, 1983, MIT AITR729 REP; FAUGERAS OD, 1983, 8TH P INT JOINT C AR, P996; FAUGERAS OD, 1983, 1ST P INT S ROB RES; GASTON PC, 1984, IEEE T PATTERN ANAL, V6, P257, DOI 10.1109/TPAMI.1984.4767518; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; GRIMSON WEL, 1984, MIT AI MEMO, V763; GRIMSON WEL, 1985, IEEE J ROBOTIC AUTOM, P61; GRIMSONWEL, 1985, 3RD P INT S ROB RES; Marr D., 1982, VISION COMPUTATIONAL; MURRAY DW, 1987, COMPUT VISION GRAPH, V40, P250, DOI 10.1016/S0734-189X(87)80118-4; MURRAY DW, 1987, INT J COMPUTER VISIO; POLLARD SB, 1987, IMAGE VISION COMPUT, V5, P73, DOI 10.1016/0262-8856(87)90030-8; POLLARD SB, 1985, PERCEPTION, V14, P449, DOI 10.1068/p140449; PRIDMORE TP, 1986, AIVRU MEMO, V12; QUARENDON P, 1984, IBM UKSC123 UK SCI C	16	11	11	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1988	2	2					153	169		10.1007/BF00133698	http://dx.doi.org/10.1007/BF00133698			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC190					2022-12-18	WOS:A1988AC19000003
J	Yang, X; Yan, JC				Yang, Xue; Yan, Junchi			On the Arbitrary-Oriented Object Detection: Classification Based Approaches Revisited	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Arbitrary-oriented object detection; Boundary problem; Circular smooth label; Densely coded labels; Object heading detection	NEURAL-NETWORK; ROTATION	Arbitrary-oriented object detection has been a building block for rotation sensitive tasks. We first show that the boundary problem suffered in existing dominant regression-based rotation detectors, is caused by angular periodicity or corner ordering, according to the parameterization protocol. We also show that the root cause is that the ideal predictions can be out of the defined range. Accordingly, we transform the angular prediction task from a regression problem to a classification one. For the resulting circularly distributed angle classification problem, we first devise a Circular Smooth Label technique to handle the periodicity of angle and increase the error tolerance to adjacent angles. To reduce the excessive model parameters by Circular Smooth Label, we further design a Densely Coded Labels, which greatly reduces the length of the encoding. Finally, we further develop an object heading detection module, which can be useful when the exact heading orientation information is needed e.g. for ship and plane heading detection. We release our OHD-SJTU dataset and OHDet detector for heading detection. Extensive experimental results on three large-scale public datasets for aerial images i.e. DOTA, HRSC2016, OHD-SJTU, and face dataset FDDB, as well as scene text dataset ICDAR2015 and MLT, show the effectiveness of our approach.	[Yang, Xue; Yan, Junchi] Shanghai Jiao Tong Univ, AI Inst, Dept Comp Sci & Engn, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China	Shanghai Jiao Tong University	Yan, JC (corresponding author), Shanghai Jiao Tong Univ, AI Inst, Dept Comp Sci & Engn, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.	yangxue-2019-sjtu@sjtu.edu.cn; yanjunchi@sku.edu.cn		Yan, Junchi/0000-0001-9639-7679	National Key Research and Development Program of China [2020AAA0107600]; Shanghai Municipal Science and Technology Major Project [2021SHZDZX0102]; National Natural Science Foundation of China [U20B2068, 61972250]; Wu Wen Jun Honorary Doctoral Scholarship, AI Institute, Shanghai Jiao Tong University	National Key Research and Development Program of China; Shanghai Municipal Science and Technology Major Project; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Wu Wen Jun Honorary Doctoral Scholarship, AI Institute, Shanghai Jiao Tong University	This work was partly supported by National Key Research and Development Program of China (2020AAA0107600), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (U20B2068, 61972250). Xue Yang is partly supported by Wu WeYn Jun Honorary Doctoral Scholarship, AI Institute, Shanghai Jiao Tong University.	Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/2951913.2976746, 10.1145/3022670.2976746]; An QZ, 2019, IEEE T GEOSCI REMOTE, V57, P8333, DOI 10.1109/TGRS.2019.2920534; Azimi SM, 2021, INT C PATT RECOG, P6920, DOI 10.1109/ICPR48806.2021.9412353; Berg T.L., 2005, ADV NEURAL INFORM PR, V17, P137; Chen YP, 2017, ADV NEUR IN, V30; Chen Zhiming, 2020, P EUR C COMP VIS, P195; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Deng D, 2018, AAAI CONF ARTIF INTE, P6773; Ding J, 2019, PROC CVPR IEEE, P2844, DOI 10.1109/CVPR.2019.00296; Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667; Feng PM, 2020, INT CONF ACOUST SPEE, P4057, DOI 10.1109/ICASSP40776.2020.9053562; Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917; Fu K, 2020, ISPRS J PHOTOGRAMM, V161, P294, DOI 10.1016/j.isprsjprs.2020.01.025; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Gray F., 1953, U.S. Patent, Patent No. 2632058; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87; HEATH FG, 1972, SCI AM, V227, P76, DOI 10.1038/scientificamerican0872-76; Hou LP, 2020, IEEE INT CON MULTI; Huang C, 2007, IEEE T PATTERN ANAL, V29, P671, DOI 10.1109/TPAMI.2007.1011; Jain V., 2010, UMCS2010009; Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942; Kim KR, 2019, IEEE I CONF COMP VIS, P273, DOI 10.1109/ICCV.2019.00036; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; Li CY, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091435; Li CZ, 2019, IEEE IMAGE PROC, P3886, DOI 10.1109/ICIP.2019.8803521; Li YY, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12030389; Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474; Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin Youtian, 2019, ARXIV191200969; Liu K, 2015, IEEE GEOSCI REMOTE S, V12, P1938, DOI 10.1109/LGRS.2015.2439517; Liu L., 2017, LEARNING ROTATION IN; Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595; Liu YL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3052; Liu ZK, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P324, DOI 10.5220/0006120603240331; Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020; Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Pan Xingjia, 2020, P IEEE CVF C COMP VI, P11207, DOI DOI 10.1109/CVPR42600.2020.01122; Qian W, 2021, AAAI CONF ARTIF INTE, V35, P2458; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rowley HA, 1998, PROC CVPR IEEE, P38, DOI 10.1109/CVPR.1998.698585; Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371; Shi XP, 2018, PROC CVPR IEEE, P2295, DOI 10.1109/CVPR.2018.00244; Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4; Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436; Wang JW, 2021, IEEE T GEOSCI REMOTE, V59, P4307, DOI 10.1109/TGRS.2020.3010051; Wang JW, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11242930; Wang W., 2021, IEEE T PATTERN ANAL, V2; Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853; Wang YS, 2019, IEEE ACCESS, V7, P173855, DOI 10.1109/ACCESS.2019.2956569; Wei HR, 2020, ISPRS J PHOTOGRAMM, V169, P268, DOI 10.1016/j.isprsjprs.2020.09.022; WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9; Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418; Xiao ZF, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12060908; Xie Enze, 2021, IEEE T PATTERN ANAL; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38; Xu YC, 2021, IEEE T PATTERN ANAL, V43, P1452, DOI 10.1109/TPAMI.2020.2974745; Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589; Xue Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P677, DOI 10.1007/978-3-030-58598-3_40; Yang F, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061686; Yang QP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1071; Yang X., ARXIV PREPRINT ARXIV; Yang X, 2021, PROC CVPR IEEE, P15814, DOI 10.1109/CVPR46437.2021.01556; Yang X, 2021, AAAI CONF ARTIF INTE, V35, P3163; Yang X, 2019, IEEE I CONF COMP VIS, P8231, DOI 10.1109/ICCV.2019.00832; Yang X, 2018, IEEE ACCESS, V6, P50839, DOI 10.1109/ACCESS.2018.2869884; Yang X, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010132; Yi JR, 2021, IEEE WINT CONF APPL, P2149, DOI 10.1109/WACV48630.2021.00220; Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255; Zhang GJ, 2019, IEEE T GEOSCI REMOTE, V57, P10015, DOI 10.1109/TGRS.2019.2930982; Zhang ZH, 2018, IEEE GEOSCI REMOTE S, V15, P1745, DOI 10.1109/LGRS.2018.2856921; Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17; Zhou K, 2016, DESTECH TRANS COMP; Zhou L, 2020, IEEE ACCESS, V8, P223373, DOI 10.1109/ACCESS.2020.3041025; Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094; Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283; Zhu YX, 2020, IEEE T GEOSCI REMOTE, V58, P7247, DOI 10.1109/TGRS.2020.2981203; Zou FH, 2020, NEURAL COMPUT APPL, V32, P14549, DOI 10.1007/s00521-020-04893-9	90	10	10	11	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1340	1365		10.1007/s11263-022-01593-w	http://dx.doi.org/10.1007/s11263-022-01593-w		MAR 2022	26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	0V4WA		Green Submitted			2022-12-18	WOS:000776215200001
J	Liu, Y; Cheng, MM; Fan, DP; Zhang, L; Bian, JW; Tao, DC				Liu, Yun; Cheng, Ming-Ming; Fan, Deng-Ping; Zhang, Le; Bian, Jia-Wang; Tao, Dacheng			Semantic Edge Detection with Diverse Deep Supervision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic edge detection; Diverse deep supervision; Information converter	CONVOLUTIONAL FEATURES; BOUNDARIES; COLOR	Semantic edge detection (SED), which aims at jointly extracting edges as well as their category information, has far-reaching applications in domains such as semantic segmentation, object proposal generation, and object recognition. SED naturally requires achieving two distinct supervision targets: locating fine detailed edges and identifying high-level semantics. Our motivation comes from the hypothesis that such distinct targets prevent state-of-the-art SED methods from effectively using deep supervision to improve results. To this end, we propose a novel fully convolutional neural network using diverse deep supervision within a multi-task framework where bottom layers aim at generating category-agnostic edges, while top layers are responsible for the detection of category-aware semantic edges. To overcome the hypothesized supervision challenge, a novel information converter unit is introduced, whose effectiveness has been extensively evaluated on SBD and Cityscapes datasets.	[Liu, Yun; Cheng, Ming-Ming; Fan, Deng-Ping] Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China; [Zhang, Le] Univ Elect Sci & Technol China, Chengdu, Peoples R China; [Bian, Jia-Wang] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia; [Tao, Dacheng] JD Explore Acad JD Com, Beijing, Peoples R China	Nankai University; University of Electronic Science & Technology of China; University of Adelaide	Cheng, MM (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.	cmm@nankai.edu.cn	Cheng, Ming-Ming/A-2527-2009; Bian, Jia-Wang/AAP-2274-2020; Bian, Jia-Wang/AAH-4463-2019; Fan, Deng-Ping/ABD-4052-2020	Cheng, Ming-Ming/0000-0001-5550-8758; Bian, Jia-Wang/0000-0003-2046-3363; Bian, Jia-Wang/0000-0003-2046-3363; Fan, Deng-Ping/0000-0002-5245-7518	National Key Research and Development Program of China [2018AAA0100400]; NSFC [61922046]	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC))	This research was supported by the National Key Research and Development Program of China Grant No. 2018AAA0100400 and NSFC (No. 61922046).	Acuna D, 2019, PROC CVPR IEEE, P11067, DOI 10.1109/CVPR.2019.01133; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bertasius G, 2016, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2016.392; Bertasius G, 2015, IEEE I CONF COMP VIS, P504, DOI 10.1109/ICCV.2015.65; Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067; Bian JW, 2021, INT J COMPUT VISION, V129, P2548, DOI 10.1007/s11263-021-01484-6; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625; CHEN LC, 2016, PROC CVPR IEEE, P4545, DOI DOI 10.1109/CVPR.2016.492; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144; Ganin Y, 2015, LECT NOTES COMPUT SC, V9004, P536, DOI 10.1007/978-3-319-16808-1_36; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; Hayder Z, 2017, PROC CVPR IEEE, P587, DOI 10.1109/CVPR.2017.70; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; HOU Q, 2018, ARXIV PREPRINT ARXIV; Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688; Hu XW, 2018, NEUROCOMPUTING, V313, P377, DOI 10.1016/j.neucom.2018.05.088; Hu Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P782; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Khoreva A, 2016, PROC CVPR IEEE, P183, DOI 10.1109/CVPR.2016.27; Kirillov A, 2017, PROC CVPR IEEE, P7322, DOI 10.1109/CVPR.2017.774; Kokkinos I.., 2016, P 4 INT C LEARN REPR, P1; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P864; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Nair V, 2010, P 27 INT C MACHINE L, P807; Ramalingam S, 2010, IEEE INT C INT ROBOT, P3816, DOI 10.1109/IROS.2010.5649105; Shan Q, 2014, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR.2014.511; Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024; Sobel I., 1970, CAMERA MODELS MACHIN; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357; Wang YP, 2019, IEEE T IMAGE PROCESS, V28, P1285, DOI 10.1109/TIP.2018.2874279; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yang Jianwei, 2016, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2016.28; Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403; Yu F., 2016, P ICLR 2016; Yu ZD, 2018, LECT NOTES COMPUT SC, V11207, P400, DOI 10.1007/978-3-030-01219-9_24; Yu ZD, 2017, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2017.191; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391	63	10	10	21	37	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					179	198		10.1007/s11263-021-01539-8	http://dx.doi.org/10.1007/s11263-021-01539-8		NOV 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC		Green Submitted			2022-12-18	WOS:000723525700001
J	Huang, Y; Wu, Q; Xu, JS; Zhong, Y; Zhang, ZX				Huang, Yan; Wu, Qiang; Xu, Jingsong; Zhong, Yi; Zhang, Zhaoxiang			Unsupervised Domain Adaptation with Background Shift Mitigating for Person Re-Identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Unsupervised domain adaptation; Background suppression; Image generation; Virtual label estimation		Unsupervised domain adaptation has been a popular approach for cross-domain person re-identification (re-ID). There are two solutions based on this approach. One solution is to build a model for data transformation across two different domains. Thus, the data in source domain can be transferred to target domain where re-ID model can be trained by rich source domain data. The other solution is to use target domain data plus corresponding virtual labels to train a re-ID model. Constrains in both solutions are very clear. The first solution heavily relies on the quality of data transformation model. Moreover, the final re-ID model is trained by source domain data but lacks knowledge of the target domain. The second solution in fact mixes target domain data with virtual labels and source domain data with true annotation information. But such a simple mixture does not well consider the raw information gap between data of two domains. This gap can be largely contributed by the background differences between domains. In this paper, a Suppression of Background Shift Generative Adversarial Network (SBSGAN) is proposed to mitigate the gaps of data between two domains. In order to tackle the constraints in the first solution mentioned above, this paper proposes a Densely Associated 2-Stream (DA-2S) network with an update strategy to best learn discriminative ID features from generated data that consider both human body information and also certain useful ID-related cues in the environment. The built re-ID model is further updated using target domain data with corresponding virtual labels. Extensive evaluations on three large benchmark datasets show the effectiveness of the proposed method.	[Huang, Yan; Wu, Qiang; Xu, Jingsong] Univ Technol Sydney, Sch Elect & Data Engn, Ultimo, Australia; [Zhong, Yi] Beijing Inst Technol, Sch Informat & Elect, Haidian, Peoples R China; [Zhang, Zhaoxiang] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China	University of Technology Sydney; Beijing Institute of Technology; Chinese Academy of Sciences; Institute of Automation, CAS	Wu, Q (corresponding author), Univ Technol Sydney, Sch Elect & Data Engn, Ultimo, Australia.	yan.huang-3@student.uts.edu.au; qiang.wu@uts.edu.au; jingsong.xu@uts.edu.au; yi.zhong@bit.edu.cn; zhaoxiang.zhang@ia.ac.cn	; Huang, Yan/N-3447-2018	Wu, Qiang/0000-0001-5641-2483; Huang, Yan/0000-0002-1363-5318	Australian Government Research Training Program Scholarship; Beijing Institute of Technology Research Fund Program for Young Scholars	Australian Government Research Training Program Scholarship(Australian GovernmentDepartment of Industry, Innovation and Science); Beijing Institute of Technology Research Fund Program for Young Scholars	This research was supported in part by the Australian Government Research Training Program Scholarship and in part by the Beijing Institute of Technology Research Fund Program for Young Scholars.	Abdulla W., 2017, GITHUB REPOSITORY, DOI DOI 10.1109/CVPR.2017.106; Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bak S, 2018, LECT NOTES COMPUT SC, V11217, P193, DOI 10.1007/978-3-030-01261-8_12; Campello Ricardo J. G. B., 2013, Advances in Knowledge Discovery and Data Mining. 17th Pacific-Asia Conference (PAKDD 2013). Proceedings, P160, DOI 10.1007/978-3-642-37456-2_14; Chen D, 2018, LECT NOTES COMPUT SC, V11211, P764, DOI 10.1007/978-3-030-01234-2_45; Chen GY, 2019, IEEE I CONF COMP VIS, P9636, DOI 10.1109/ICCV.2019.00973; Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844; Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379; Damodaran BB, 2018, LECT NOTES COMPUT SC, V11208, P467, DOI 10.1007/978-3-030-01225-0_28; Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hermans Alexander, 2017, ARXIV170307737; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962; Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715; Huang Y, 2017, NEUROCOMPUTING, V241, P191, DOI 10.1016/j.neucom.2017.02.055; Huang Y, 2016, IEEE IMAGE PROC, P4255, DOI 10.1109/ICIP.2016.7533162; Gulrajani I, 2017, ADV NEUR IN, V30; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117; Kingma D.P, P 3 INT C LEARNING R; Kurmi VK, 2019, PROC CVPR IEEE, P491, DOI 10.1109/CVPR.2019.00058; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801; Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737; Long MS, 2015, PR MACH LEARN RES, V37, P97; Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508; Nair V., 2010, ICML, P807; Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149; Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129; Song JF, 2019, PROC CVPR IEEE, P719, DOI 10.1109/CVPR.2019.00081; Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Sun YF, 2021, IEEE T PATTERN ANAL, V43, P902, DOI 10.1109/TPAMI.2019.2938523; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242; Wang XM, 2019, AAAI CONF ARTIF INTE, P5345; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wu AC, 2019, IEEE I CONF COMP VIS, P6921, DOI 10.1109/ICCV.2019.00702; Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375; Yu TY, 2019, IEEE I CONF COMP VIS, P552, DOI 10.1109/ICCV.2019.00064; Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831; Zheng L., 2016, ARXIV; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069; Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	74	10	10	2	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2244	2263		10.1007/s11263-021-01474-8	http://dx.doi.org/10.1007/s11263-021-01474-8		MAY 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW					2022-12-18	WOS:000650112400002
J	Weinzaepfel, P; Rogez, G				Weinzaepfel, Philippe; Rogez, Gregory			Mimetics: Towards Understanding Human Actions Out of Context	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Biases in action recognition; Mimes; Pose-based action recognition		Recent methods for video action recognition have reached outstanding performances on existing benchmarks. However, they tend to leverage context such as scenes or objects instead of focusing on understanding the human action itself. For instance, a tennis field leads to the prediction playing tennis irrespectively of the actions performed in the video. In contrast, humans have a more complete understanding of actions and can recognize them without context. The best example of out-of-context actions are mimes, that people can typically recognize despite missing relevant objects and scenes. In this paper, we propose to benchmark action recognition methods in such absence of context and introduce a novel dataset, Mimetics, consisting of mimed actions for a subset of 50 classes from the Kinetics benchmark. Our experiments show that (a) state-of-the-art 3D convolutional neural networks obtain disappointing results on such videos, highlighting the lack of true understanding of the human actions and (b) models leveraging body language via human pose are less prone to context biases. In particular, we show that applying a shallow neural network with a single temporal convolution over body pose features transferred to the action recognition problem performs surprisingly well compared to 3D action recognition methods.	[Weinzaepfel, Philippe; Rogez, Gregory] NAVER Labs Europe, 6 Chemin Maupertuis, F-38240 Meylan, France		Weinzaepfel, P (corresponding author), NAVER Labs Europe, 6 Chemin Maupertuis, F-38240 Meylan, France.	Philippe.Weinzaepfel@naverlabs.com; Gregory.Rogez@naverlabs.com						[Anonymous], 2016, P ECCV; Bahng Hyojin, 2019, ARXIV191002806; Cao C., 2016, IJCAI, P3324; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Cheron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368; Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734; Courtney PG, 2015, IEEE COMP SEMICON; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Du WB, 2017, IEEE I CONF COMP VIS, P3745, DOI 10.1109/ICCV.2017.402; Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Gall J., 2017, INT C AUT FAC GEST R; Ghadiyaram Deepti, 2019, CVPR; Girdhar R, 2017, ADV NEUR IN, V30; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Jacquot Vincent, 2020, Conf Comput Vis Pattern Recognit Workshops, V2020; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472; Kay W., 2017, ARXIV PREPRINT ARXIV; Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Li Y, 2019, PROC CVPR IEEE, P9564, DOI 10.1109/CVPR.2019.00980; Li YW, 2018, LECT NOTES COMPUT SC, V11210, P520, DOI 10.1007/978-3-030-01231-1_32; Liu J., 2016, DESTECH TRANS COMP; Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; McNally W, 2019, 2019 16TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2019), P49, DOI 10.1109/CRV.2019.00015; Naqvi S.M, 2018, ARXIV PREPRINT ARXIV; Ren S., 2015, P 28 INT C NEUR INF, DOI DOI 10.5555/2969239.2969250; Rogez G, 2020, IEEE T PATTERN ANAL, V42, P1146, DOI 10.1109/TPAMI.2019.2892985; Saha S., 2016, BMVC, P581; Sevilla-Lara L., 2018, GERM C PATT REC, P281, DOI DOI 10.1007/978-3-030-12939-2_20; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Singh G, 2017, IEEE I CONF COMP VIS, P3657, DOI 10.1109/ICCV.2017.393; Soomro K., 2012, ARXIV; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Wang W., 2018, ARXIV PREPRINT ARXIV; Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362; Weng JW, 2018, LECT NOTES COMPUT SC, V11211, P142, DOI 10.1007/978-3-030-01234-2_9; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yao A, 2012, INT J COMPUT VISION, V100, P16, DOI 10.1007/s11263-012-0532-9; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhu J. etal, 2018, ARXIV PREPRINT ARXIV; Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697; Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316	59	10	10	2	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1675	1690		10.1007/s11263-021-01446-y	http://dx.doi.org/10.1007/s11263-021-01446-y		MAR 2021	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000624439800002
J	Chen, X; Xie, LX; Wu, J; Tian, Q				Chen, Xin; Xie, Lingxi; Wu, Jun; Tian, Qi			Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Neural architecture search; Optimization gap; Progressive DARTS	NEURAL-NETWORKS	With the rapid development of neural architecture search (NAS), researchers found powerful network architectures for a wide range of vision tasks. Like the manually designed counterparts, we desire the automatically searched architectures to have the ability of being freely transferred to different scenarios. This paper formally puts forward this problem, referred to as NAS in the wild, which explores the possibility of finding the optimal architecture in a proxy dataset and then deploying it to mostly unseen scenarios. We instantiate this setting using a currently popular algorithm named differentiable architecture search (DARTS), which often suffers unsatisfying performance while being transferred across different tasks. We argue that the accuracy drop originates from the formulation that uses a super-network for search but a sub-network for re-training. The different properties of these stages have resulted in a significant optimization gap, and consequently, the architectural parameters "over-fit" the super-network. To alleviate the gap, we present a progressive method that gradually increases the network depth during the search stage, which leads to the Progressive DARTS (P-DARTS) algorithm. With a reduced search cost (7 hours on a single GPU), P-DARTS achieves improved performance on both the proxy dataset (CIFAR10) and a few target problems (ImageNet classification, COCO detection and three ReID benchmarks). Our code is available at https://github.com/chenxin061/pdarts.	[Chen, Xin] Tongji Univ, Shanghai, Peoples R China; [Xie, Lingxi; Tian, Qi] Huawei Inc, Shenzhen, Peoples R China; [Wu, Jun] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China; [Wu, Jun] Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China	Tongji University; Huawei Technologies; Fudan University; Fudan University	Wu, J (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China.; Wu, J (corresponding author), Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China.	1410452@tongji.edu.cn; 198808xc@gmail.com; wujun@fudan.edu.cn; tian.qi1@huawei.com	Xie, Lingxi/ABF-6996-2020		National Natural Science Foundation of China [61831018, 61631017]; Guangdong Province Key Research and Development Program Major Science and Technology Projects [2018B010115002]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Guangdong Province Key Research and Development Program Major Science and Technology Projects	This work was supported in part by the National Natural Science Foundation of China under Grant Nos. 61831018, and 61631017, and Guangdong Province Key Research and Development Program Major Science and Technology Projects under Grant 2018B010115002.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baker Bowen, 2017, ICLR; Bi Kaifeng, 2019, ARXIV191011831; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cai Han, 2019, INT C LEARN REPR; Chen X, 2019, IEEE I CONF COMP VIS, P1294, DOI 10.1109/ICCV.2019.00138; Chen YK, 2019, ADV NEUR IN, V32; Chu Xiangxiang, 2021, ARXIV190701845, P12239; Cubuk Ekin D., 2018, ARXIV180509501; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186; Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667; Elsken Thomas, 2018, ARXIV180805377, DOI [10.1007/978-3-030-05318-5_11, DOI 10.1007/978-3-030-05318-5_11]; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Goyal Priya, 2017, ARXIV170602677; Guan Melody, 2018, ICML, P4095; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Howard A.G., 2017, MOBILENETS EFFICIENT; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larsson G., 2017, INT C LEARN REPR ICL; Le, 2017, ARXIV PREPRINT ARXIV; Li JW, 2018, INT J COMPUT VISION, V126, P855, DOI 10.1007/s11263-018-1075-5; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2018, LECT NOTES COMPUT SC, V11210, P203, DOI 10.1007/978-3-030-01231-1_13; Liu Hanxiao, 2018, ICLR; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Ma Ningning, 2018, P EUR C COMP VIS ECC; Mao HZ, 2017, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2017.241; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shu Yao, 2020, INT C LEARN REPR; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Suganuma M, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P497, DOI 10.1145/3071178.3071229; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tan MX, 2019, PR MACH LEARN RES, V97; Tan Mingxing, 2020, CVPR; Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166; Wang HX, 2018, INT J COMPUT VISION, V126, P1288, DOI 10.1007/s11263-018-1105-3; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Xie SN, 2019, IEEE I CONF COMP VIS, P1284, DOI 10.1109/ICCV.2019.00137; Xie Sirui, 2019, ICLR, V1, P13; Xu Y, 2020, PLANT SOIL, V449, P133, DOI 10.1007/s11104-020-04435-1; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zela A., 2020, INT C LEARN REPRESEN; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	72	10	10	2	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					638	655		10.1007/s11263-020-01396-x	http://dx.doi.org/10.1007/s11263-020-01396-x		NOV 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC		Green Submitted			2022-12-18	WOS:000585070700001
J	Kamann, C; Rother, C				Kamann, Christoph; Rother, Carsten			Benchmarking the Robustness of Semantic Segmentation Models with Respect to Common Corruptions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic segmentation; Corruption robustness; Common image corruptions; Realistic image corruptions	CALIBRATION; CAMERA; NOISE	When designing a semantic segmentation model for a real-world application, such as autonomous driving, it is crucial to understand the robustness of the network with respect to a wide range of image corruptions. While there are recent robustness studies for full-image classification, we are the first to present an exhaustive study for semantic segmentation, based on many established neural network architectures. We utilize almost 400,000 images generated from the Cityscapes dataset, PASCAL VOC 2012, and ADE20K. Based on the benchmark study, we gain several new insights. Firstly, many networks perform well with respect to real-world image corruptions, such as a realistic PSF blur. Secondly, some architecture properties significantly affect robustness, such as a Dense Prediction Cell, designed to maximize performance on clean data only. Thirdly, the generalization capability of semantic segmentation models depends strongly on the type of image corruption. Models generalize well for image noise and image blur, however, not with respect to digitally corrupted data or weather corruptions.	[Kamann, Christoph; Rother, Carsten] Heidelberg Univ, Visual Learning Lab, HCI IWR, Heidelberg, Germany	Ruprecht Karls University Heidelberg	Kamann, C (corresponding author), Heidelberg Univ, Visual Learning Lab, HCI IWR, Heidelberg, Germany.	of182@uni-heidelberg.de; carsten.rother@iwr.uni-heidelberg.de		Kamann, Christoph/0000-0001-9094-9065	Projekt DEAL	Projekt DEAL	Open Access funding enabled and organized by Projekt DEAL.	Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/2951913.2976746, 10.1145/3022670.2976746]; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arnab A, 2018, PROC CVPR IEEE, P888, DOI 10.1109/CVPR.2018.00099; Azulay A, 2019, J MACH LEARN RES, V20; Badrinarayanan V., 2015, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Boopathy A., 2019, AAAI; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini Nicholas, 2017, P 10 ACM WORKSHOP AR, P3, DOI [10.1145/3128572.3140444, DOI 10.1145/3128572.3140444]; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chollet F., 2017, PROC CVPR IEEE, P1251, DOI DOI 10.1109/CVPR.2017.195; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Cubuk E. D., 2018, INTRIGUING PROPERTIE; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Dai DX, 2018, IEEE INT C INTELL TR, P3819, DOI 10.1109/ITSC.2018.8569387; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dodge S. F., 2016, QUOMEX; Dodge S, 2017, 2017 26TH INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATION AND NETWORKS (ICCCN 2017); ELdin A. K., 2017, IEEE INT S CIRC SYST, P1; Engstrom L, 2019, PR MACH LEARN RES, V97; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fawzi A., 2015, BRIT MACH VIS C; Fitzgibbon AW, 2001, PROC CVPR IEEE, P125; FORD N, 2019, ARXIV190110513; Geirhos R., 2018, C NEUR INF PROC SYST; Geirhos R., 2019, P INT C LEARNING REP, P1; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; Gu S., 2014, ARXIV14125068; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hasirlioglu S, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2242, DOI 10.1109/ITSC.2016.7795918; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; Hendrycks D., 2019, ICLR, P1; Hendrycks Dan, 2020, P INT C LEARN REPR I; Holschneider M., 1990, WAVELETS, p286?297, DOI DOI 10.1007/978-3-642-75988-8_28; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang XW, 2017, LECT NOTES COMPUT SC, V10426, P3, DOI 10.1007/978-3-319-63387-9_1; Jahne B, 1997, DIGITAL IMAGE PROCES, P4; Joshi N, 2008, PROC CVPR IEEE, P3823; Kamann C., 2020, IEEE CVF C COMP VIS; Kannan Harini, 2018, ARXIV180306373; Ke TW, 2017, PROC CVPR IEEE, P4067, DOI 10.1109/CVPR.2017.433; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laermann J, 2019, LECT NOTES COMPUT SC, V11824, P360, DOI 10.1007/978-3-030-33676-9_25; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu C, 2008, IEEE T PATTERN ANAL, V30, P299, DOI [10.1109/TPAMI.2007.1176, 10.1109/TPAMI.20071176]; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lopes R.G., 2019, ARXIV190602611; Lukas J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602; Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12; Metzen J. H., 2017, 5 INT C LEARNING REP, DOI DOI 10.1109/ICCV.2017.300; Michaelis Claudio, 2019, ARXIV190707484; Orhan A. E., 2019, ARXIV190707640; Papandreou G, 2015, PROC CVPR IEEE, P390, DOI 10.1109/CVPR.2015.7298636; Paszke A., 2016, ARXIV PREPRINT ARXIV; Pham H, 2018, PR MACH LEARN RES, V80; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Rusak Evgenia, 2020, ARXIV200106057, P53, DOI [DOI 10.1007/978-3-030-58580-8_4, 10.1007/978, DOI 10.1007/978]; Sakaridis C, 2019, IEEE I CONF COMP VIS, P7373, DOI 10.1109/ICCV.2019.00747; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shah S, 1996, PATTERN RECOGN, V29, P1775, DOI 10.1016/0031-3203(96)00038-6; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Takahashi R, 2020, IEEE T CIRC SYST VID, V30, P2917, DOI 10.1109/TCSVT.2019.2935128; Vasiljevic I., 2016, ARXIV161105760CSCV; Volk G, 2019, IEEE INT C INTELL TR, P285, DOI 10.1109/ITSC.2019.8917269; Wang YJ, 2018, ELECTROCHEM ENERGY R, V1, P1, DOI 10.1007/s41918-018-0002-3; WILLSON RG, 1994, P SOC PHOTO-OPT INS, V2350, P170, DOI 10.1117/12.189130; Xie Qizhe, 2019, ARXIV191104252; Young IT, 1998, FUNDAMENTALS IMAGE P; Yu F, 2016, INT NANOELECTR CONF; Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612; Zendel O, 2018, LECT NOTES COMPUT SC, V11210, P407, DOI 10.1007/978-3-030-01231-1_25; Zendel O, 2017, INT J COMPUT VISION, V125, P95, DOI 10.1007/s11263-017-1020-z; Zhang R, 2019, PR MACH LEARN RES, V97; Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zhou YR, 2017, INT CONF ACOUST SPEE, P1213, DOI 10.1109/ICASSP.2017.7952349; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907; Zoran D., 2018, ARXIV180404438	98	10	10	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					462	483		10.1007/s11263-020-01383-2	http://dx.doi.org/10.1007/s11263-020-01383-2		SEP 2020	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		hybrid			2022-12-18	WOS:000574133000001
J	Tremblay, M; Halder, SS; de Charette, R; Lalonde, JF				Tremblay, Maxime; Halder, Shirsendu Sukanta; de Charette, Raoul; Lalonde, Jean-Francois			Rain Rendering for Evaluating and Improving Robustness to Bad Weather	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Adverse weather; Vision and rain; Physics-based rendering; Image to image translation; GAN	VISION	Rain fills the atmosphere with water particles, which breaks the common assumption that light travels unaltered from the scene to the camera. While it is well-known that rain affects computer vision algorithms, quantifying its impact is difficult. In this context, we present a rain rendering pipeline that enables the systematic evaluation of common computer vision algorithms to controlled amounts of rain. We present three different ways to add synthetic rain to existing images datasets: completely physic-based; completely data-driven; and a combination of both. The physic-based rain augmentation combines a physical particle simulator and accurate rain photometric modeling. We validate our rendering methods with a user study, demonstrating our rain is judged as much as 73% more realistic than the state-of-the-art. Using our generated rain-augmented KITTI, Cityscapes, and nuScenes datasets, we conduct a thorough evaluation of object detection, semantic segmentation, and depth estimation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection, 60% for semantic segmentation, and 6-fold increase in depth estimation error. Finetuning on our augmented synthetic data results in improvements of 21% on object detection, 37% on semantic segmentation, and 8% on depth estimation.	[Tremblay, Maxime; Lalonde, Jean-Francois] Univ Laval, Quebec City, PQ, Canada; [Halder, Shirsendu Sukanta; de Charette, Raoul] INRIA, Paris, France	Laval University; Inria	Tremblay, M (corresponding author), Univ Laval, Quebec City, PQ, Canada.	maxime.tremblay.17@ulaval.ca		Halder, Shirsendu/0000-0002-6548-8663; Lalonde, Jean-Francois/0000-0002-6583-2364	Service de cooperation et d'action culturelle du Consulat general de France a Quebec; FRQ-NT with the Samuel-de-Champlain grant	Service de cooperation et d'action culturelle du Consulat general de France a Quebec; FRQ-NT with the Samuel-de-Champlain grant	This work was partially supported by the Service de cooperation et d'action culturelle du Consulat general de France a Quebec, as well as the FRQ-NT with the Samuel-de-Champlain grant. We gratefully thank Pierre Bourre for his priceless technical help, Aitor Gomez Torres for his initial input, and Srinivas Narasimhan for letting us reuse the physical simulator. We also thank the Nvidia corporation for the donation of the GPU used in this research.	ATLAS D, 1973, REV GEOPHYS, V11, P1, DOI 10.1029/RG011i001p00001; Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8; Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2; Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bijelic Mario, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11679, DOI 10.1109/CVPR42600.2020.01170; Brock A., 2018, ICLR, P1; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Cameron C., 2005, TECHNICAL REPORT; Chen YL, 2013, INT SYMP NEXTGEN; Chu MY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073643; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Creus C, 2013, COMPUT GRAPH-UK, V37, P33, DOI 10.1016/j.cag.2012.12.002; Dai JF, 2016, ADV NEUR IN, V29; de Charette R., 2012, IEEE INT C COMP PHOT, P1, DOI 10.1109/ICCPhot.2012.6215217; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Garg K, 2005, IEEE I CONF COMP VIS, P1067; GARG K, 2004, PROC CVPR IEEE, P528, DOI DOI 10.1109/CVPR.2004.1315077; Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6; Garg K, 2006, ACM T GRAPHIC, V25, P996, DOI 10.1145/1141911.1141985; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A., 2012, P IEEE COMP SOC C CO; Ghosal A, 2019, IEEE IC COMP COM NET; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gruber T, 2019, INT CONF 3D VISION, P95, DOI 10.1109/3DV.2019.00020; Halimeh JC, 2009, IEEE INT VEH SYM, P610, DOI 10.1109/IVS.2009.5164347; Hao Z, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3277992; Hao ZX, 2019, IEEE INT CONF COMP V, P4340, DOI 10.1109/ICCVW.2019.00534; Hold-Geoffroy Y, 2019, PROC CVPR IEEE, P6920, DOI 10.1109/CVPR.2019.00709; Hold-Geoffroy Y, 2017, PROC CVPR IEEE, P2373, DOI 10.1109/CVPR.2017.255; Horn B., 1986, ROBOT VISION, P1; HUANG X, 2018, EUR C COMP VIS; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jacobs N, 2007, IEEE I CONF COMP VIS, P1305; Jaritz M, 2018, INT CONF 3D VISION, P52, DOI 10.1109/3DV.2018.00017; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Johnson-Roberson Matthew, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P746, DOI 10.1109/ICRA.2017.7989092; Khan S, 2019, CONSUM COMM NETWORK; Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101; Lalonde JF, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618477; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee JH, 2020, ARXIV190710326CS; Li Peilun, 2018, BMVC; Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299; Liu MY, 2017, ADV NEUR IN, V30; Liu SC, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511149; Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741; Luo Y, 2015, IEEE INFOCOM SER; Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498; MARSHALL JS, 1948, J METEOROL, V5, P165, DOI 10.1175/1520-0469(1948)005<0165:TDORWS>2.0.CO;2; Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34; Narasimhan SG, 2002, LECT NOTES COMPUT SC, V2352, P148; Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723; Pizzati F., 2020, EUR C COMP VIS; Porav H, 2019, IEEE INT CONF ROBOT, P7087, DOI 10.1109/ICRA.2019.8793486; Potmesil M., 1981, Computer Graphics, V15, P297, DOI 10.1145/965161.806818; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Roser Martin, 2010, Computer Vision - ACCV 2010 Workshops. ACCV 2010 International Workshops. Revised Selected Papers, P235, DOI 10.1007/978-3-642-22819-3_24; Roser M., 2009, 2009 IEEE 12 INT C C, P570; Rousseau P, 2006, COMPUT GRAPH-UK, V30, P507, DOI 10.1016/j.cag.2006.03.013; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212; Tasar O, 2020, INT GEOSCI REMOTE SE, P1837, DOI 10.1109/IGARSS39084.2020.9323711; Tatarchuk N., 2006, ACM SIGGRAPH 2006 CO, P23; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Van Boxel J.H., 1997, WORKSH WIND WAT ER, P77; Weber Y, 2015, COMPUT GRAPH-UK, V50, P61, DOI 10.1016/j.cag.2015.04.007; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Yan BN, 2019, INT EL DEVICES MEET; Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183; Yu Fisher, 2018, BDD100K DIVERSE DRIV, P6; Zendel O, 2018, LECT NOTES COMPUT SC, V11210, P407, DOI 10.1007/978-3-030-01231-1_25; Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407; Zhang H, 2018, IEEE ICC; Zhang JJ, 2019, PROC CVPR IEEE, P5468, DOI 10.1109/CVPR.2019.00562; Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	83	10	12	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					341	360		10.1007/s11263-020-01366-3	http://dx.doi.org/10.1007/s11263-020-01366-3		SEP 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Submitted			2022-12-18	WOS:000569779200001
J	Lin, MB; Ji, RR; Liu, H; Sun, XS; Chen, S; Tian, Q				Lin, Mingbao; Ji, Rongrong; Liu, Hong; Sun, Xiaoshuai; Chen, Shen; Tian, Qi			Hadamard Matrix Guided Online Hashing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Binary code; Online hashing; Hadamard matrix; Image retrieval		Online image hashing has attracted increasing research attention recently, which receives large-scale data in a streaming manner to update the hash functions on-the-fly. Its key challenge lies in the difficulty of balancing the learning timeliness and model accuracy. To this end, most works follow a supervised setting, i.e., using class labels to boost the hashing performance, which defects in two aspects: first, strong constraints, e.g., orthogonal or similarity preserving, are used, which however are typically relaxed and lead to large accuracy drops. Second, large amounts of training batches are required to learn the up-to-date hash functions, which largely increase the learning complexity. To handle the above challenges, a novel supervised online hashing scheme termed Hadamard Matrix Guided Online Hashing (HMOH) is proposed in this paper. Our key innovation lies in introducing Hadamard matrix, which is an orthogonal binary matrix built via Sylvester method. In particular, to release the need of strong constraints, we regard each column of Hadamard matrix as the target code for each class label, which by nature satisfies several desired properties of hashing codes. To accelerate the online training, LSH is first adopted to align the lengths of target code and to-be-learned binary code. We then treat the learning of hash functions as a set of binary classification problems to fit the assigned target code. Finally, extensive experiments on four widely-used benchmarks demonstrate the superior accuracy and efficiency of HMOH over various state-of-the-art methods. Codes can be available at https:// github. com/lmbxmu/ mycode..	[Lin, Mingbao; Ji, Rongrong; Liu, Hong; Sun, Xiaoshuai; Chen, Shen; Tian, Qi] Xiamen Univ, Dept Artificial Intelligence Sch Informat, Media Analyt & Comp Lab, Xiamen, Peoples R China; [Tian, Qi] Huawei Noahs Ark Lab, Shenzhen, Peoples R China	Xiamen University; Huawei Technologies	Ji, RR (corresponding author), Xiamen Univ, Dept Artificial Intelligence Sch Informat, Media Analyt & Comp Lab, Xiamen, Peoples R China.	lmbxmu@stu.xmu.edu.cn; rrji@xmu.cdu.cn; lynnliu.xmu@gmail.com; xiaoshuaisun.hit@gmail.com; chenshen@stu.xmu.edu.cn; tian.qi1@huawei.com			Nature Science Foundation of China [U1705262, 61772443, 61572410, 61802324, 61702136]; National Key RD Program [2017YFC0113000, 2016YFB1001503]	Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key RD Program	This work is supported by the Nature Science Foundation of China (Nos. U1705262, 61772443, 61572410, 61802324 and 61702136) and National Key R&D Program (Nos. 2017YFC0113000 and 2016YFB1001503).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Babenko Boris, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1346, DOI 10.1109/ICCVW.2009.5457453; Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55; Cakir F, 2017, COMPUT VIS IMAGE UND, V156, P162, DOI 10.1016/j.cviu.2016.10.009; Cakir F, 2015, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2015.125; Chen XX, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Crammer K, 2006, J MACH LEARN RES, V7, P551; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; GOLDBERG K, 1966, P AM MATH SOC, V17, P744; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475; Horadam K. J., 2012, HADAMARD MATRICES TH, DOI 10.1515/9781400842902; Huang L., 2013, IJCAI, P1422; Huang LK, 2018, IEEE T NEUR NET LEAR, V29, P2309, DOI 10.1109/TNNLS.2017.2689242; Jiang JY, 2009, PROC CVPR IEEE, P1810, DOI 10.1109/CVPRW.2009.5206761; Kittler J, 2001, COMPUTER VISION PATT, V21, P1163; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Leng C, 2015, PROC CVPR IEEE, P2503, DOI 10.1109/CVPR.2015.7298865; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Lin MB, 2019, AAAI CONF ARTIF INTE, P8722; Lin MB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1635, DOI 10.1145/3240508.3240519; Liu H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1589, DOI 10.1145/3240508.3240684; Liu W., 2014, ADV NEURAL INFORM PR, V4, P3419; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Lu Yichao, 2013, ADV NEURAL INFORM PR, P369; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Novikoff A. B., 1963, TECHNICAL REPORTS ST; Ockwig NW, 2005, ACCOUNTS CHEM RES, V38, P176, DOI 10.1021/ar020022l; Paley R.E.A.C., 1933, J MATH PHYS, V12, P311, DOI DOI 10.1002/SAPM1933121311; Peterson W. W., 1972, ERROR CORRECTING COD, V2nd; Sablayrolles A, 2017, INT CONF ACOUST SPEE, P1732, DOI 10.1109/ICASSP.2017.7952453; Schapire R. E., 1997, MACH LEARN P 14 INT, P313; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Sylvester J., 1867, PHILOS MAG, V34, P461, DOI [10.1080/14786446708639914, DOI 10.1080/14786446708639914]; Wang J, 2017, IEEE INFOCOM SER, DOI 10.1007/s12083-017-0556-6; Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; Williamson J, 1944, DUKE MATH J, V11, P65, DOI [10.1215/S0012-7094-44-01108-7, DOI 10.1215/S0012-7094-44-01108-7]; Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863; Zhao B, 2013, PROC CVPR IEEE, P3350, DOI 10.1109/CVPR.2013.430; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415	50	10	12	1	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2279	2306		10.1007/s11263-020-01332-z	http://dx.doi.org/10.1007/s11263-020-01332-z		MAY 2020	28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG		Green Submitted			2022-12-18	WOS:000530783200001
J	Sui, Y; Zhang, ZM; Wang, GH; Tang, YF; Zhang, L				Sui, Yao; Zhang, Ziming; Wang, Guanghui; Tang, Yafei; Zhang, Li			Exploiting the Anisotropy of Correlation Filter Learning for Visual Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object tracking; Anisotropy; Correlation filtering; Loss function; Sparsity; Robustness; Sensitivity		Correlation filtering based tracking model has received significant attention and achieved great success in terms of both tracking accuracy and computational complexity. However, due to the limitation of the loss function, current correlation filtering paradigm could not reliably respond to the abrupt appearance changes of the target object. This study focuses on improving the robustness of the correlation filter learning. An anisotropy of the filter response is observed and analyzed for the correlation filtering based tracking model, through which the overfitting issue of previous methods is alleviated. Three sparsity related loss functions are proposed to exploit the anisotropy, leading to three implementations of visual trackers, correspondingly resulting in improved overall tracking performance. A large number of experiments are conducted and these experimental results demonstrate that the proposed approach greatly improves the robustness of the learned correlation filter. The proposed trackers performs comparably against state-of-the-art tracking methods on four latest standard tracking benchmark datasets.	[Sui, Yao] Harvard Univ, Harvard Med Sch, Boston, MA 02115 USA; [Zhang, Ziming] MERL, Cambridge, MA 02139 USA; [Wang, Guanghui] Univ Kansas, Dept Elect Engn & Comp Sci, Lawrence, KS 66045 USA; [Tang, Yafei] China Unicom Res Inst, Beijing 100032, Peoples R China; [Zhang, Li] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China	Harvard University; Harvard Medical School; University of Kansas; Tsinghua University	Sui, Y (corresponding author), Harvard Univ, Harvard Med Sch, Boston, MA 02115 USA.	suiyao@gmail.com; zzhang@merl.com; ghwang@ku.edu; tangyf24@chinaunicom.cn; chinazhangli@tsinghua.edu.cn	Zhang, ziming/HGA-8604-2022		National Natural Science Foundation of China (NSFC) [61132007, 61573351, U1533132]; Kansas NASA EPSCoR Program [KNEP-PDG-10-2017-KU]; Civil Aviation Administration [U1533132]	National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Kansas NASA EPSCoR Program; Civil Aviation Administration	This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grants 61132007 and 61573351, in part by the Kansas NASA EPSCoR Program under Grant KNEP-PDG-10-2017-KU, and in part by the joint fund of Civil Aviation Research by the National Natural Science Foundation of China (NSFC) and Civil Aviation Administration under Grant U1533132.	[Anonymous], 2016, ECCV; [Anonymous], 2005, BMVC; Bach F, 2012, OPTIMIZATION FOR MACHINE LEARNING, P19; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertinetto L., 2015, ICCV VOT WORKSH; Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bogun I., 2015, ICCV VOT WORKSH; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Chen B., 2017, ICCV VOT WORKSH; Chen K., 2016, CONVOLUTIONAL REGRES; Chi Z., 2016, ECCV VOT WORKSH; Danelljan M., 2017, ICCV VOT WORKSH; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29; Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490; Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143; Duffner S, 2016, IEEE T CIRC SYST VID, V26, P2215, DOI 10.1109/TCSVT.2015.2504739; Gao J., 2016, ECCV VOT WORKSH; Gundogdu E., 2017, GOOD FEATURES CORREL; Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251; He Z., 2017, ICCV VOT WORKSHOP; Henriques F., 2012, ECCV; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hu T., 2016, ECCV VOT WORKSH; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; KIM J, 2011, IEEE INT CONF ROBOT; Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982; Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821; Lee H., 2016, ECCV VOT WORKSH; Lee J. Y., 2016, ECCV VOT WORKSH; Lee J. Y., 2015, ICCV VOT WORKSH; Li Y., 2015, ICCV VOT WORKSH; Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18; Liu S., 2016, CVPR; Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124; Lukezic A., 2015, ICCV VOT WORKSH; Lukezic A., 2017, ICCV VOT WORKSH; Lukezic A., 2016, DEFORMABLE PARTS COR; Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515; Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352; Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177; MEHRABADI NR, 2017, ICCV VOT WORKSH; Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66; Mocanu B., 2017, ICCV VOT WORKSH; Nam B. M. H., 2016, MODELING PROPAGATING; NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465; Poostchi M., 2017, ICCV VOT WORKSH; Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823; Qi Y., 2016, ECCV VOT WORKSH; Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466; Schmid C., 2015, ICCV; Singh S., 2017, ICCV VOT WORKSH; Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230; Sui Y., 2015, ICCV; Sui Y., 2016, ECCV; Sui Y, 2018, INT J COMPUT VISION, V126, P515, DOI 10.1007/s11263-017-1049-z; Sui Y, 2018, IEEE T IMAGE PROCESS, V27, P1282, DOI 10.1109/TIP.2017.2779275; Sui Y, 2018, IEEE T CYBERNETICS, V48, P1290, DOI 10.1109/TCYB.2017.2690860; Sui Y, 2016, INT J COMPUT VISION, V119, P110, DOI 10.1007/s11263-016-0881-x; Sui Y, 2015, IEEE T IMAGE PROCESS, V24, P4686, DOI 10.1109/TIP.2015.2462076; Sui Y, 2015, PATTERN RECOGN, V48, P2872, DOI 10.1016/j.patcog.2015.03.007; Sui Y, 2015, IEEE SIGNAL PROC LET, V22, P1331, DOI 10.1109/LSP.2015.2402313; Sun C., 2017, ICCV VOT WORKSH; Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348; Vojir T., 2015, ONLINE ADAPTIVE HIDD; Vojir T, 2014, PATTERN RECOGN LETT, V49, P250, DOI 10.1016/j.patrec.2014.03.025; Walsh R., 2016, ECCV VOT WORKSH; Wang D., 2013, CVPR; Wang L., 2016, ECCV VOT WORKSH; Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357; Wang LM, 2016, PROC CVPR IEEE, P2708, DOI 10.1109/CVPR.2016.296; Wang N., 2015, ICML; Wang N., 2015, ARXIV150104587; Wen L., 2016, ECCV VOT WORKSH; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Xu Z., 2016, ECCV VOT WORKSH; Yang L., 2017, ICCV VOT WORKSH; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Zhang J., 2014, EUR C COMP VIS; Zhang K., 2014, ECCV; Zhang M., 2015, ICCV VOT WORKSH; Zhang M., 2016, ECCV VOT WORKSH; Zhang SL, 2015, IEEE T IMAGE PROCESS, V24, P5723, DOI 10.1109/TIP.2015.2484068; Zhang SL, 2015, PATTERN RECOGN, V48, P3881, DOI 10.1016/j.patcog.2015.06.005; Zhang T., 2017, ICCV VOT WORKSH; Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI 10.1109/CVPR.2017.512; Zhang TZ, 2015, PROC CVPR IEEE, P150, DOI 10.1109/CVPR.2015.7298610; Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34; Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908; Zhang Z, 2017, IEEE INT WORK SIGN P; Zhu G, 2016, PROC CVPR IEEE, P943, DOI 10.1109/CVPR.2016.108; Zhu Z., 2017, ICCV VOT WORKSH	95	10	11	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					1084	1105		10.1007/s11263-019-01156-6	http://dx.doi.org/10.1007/s11263-019-01156-6			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM					2022-12-18	WOS:000474559000007
J	Bulow, H; Birk, A				Buelow, Heiko; Birk, Andreas			Scale-Free Registrations in 3D: 7 Degrees of Freedom with Fourier Mellin SOFT Transforms	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Registration; Fourier-Mellin; SO(3) Fourier transform; Spherical harmonics; Multidimensional signal processing; Object recognition; Object modeling; Mapping	UNIQUE SIGNATURES; SURFACE; PHASE; REPRESENTATION; RECOGNITION; HISTOGRAMS; ALGORITHM; ROTATION; GEOMETRY; IMAGES	Fourier Mellin SOFT (FMS) as a novel method for global registration of 3D data is presented. It determines the seven degrees of freedom (7-DoF) transformation, i.e., the 6-DoF rigid motion parameters plus 1-DoF scale, between two scans, i.e., two noisy, only partially overlapping views on objects or scenes. It is based on a sequence of the 3D Fourier transform, the Mellin transform and the SO(3) Fourier transform. This combination represents a non-trivial complete 3Dextension of the well known Fourier-Mellin registration for 2D images. It is accordingly based on decoupling rotation and scale from translation. First, rotation-which is the main challenge for the extension to 3D data - is tackled with a SO(3) Fourier Transform (SOFT) based on Spherical Harmonics. In a second step, scale is determined via a 3D Mellin transform. Finally, translation is calculated by Phase-Matching. Experiments are presented with simulated data sets for ground truth comparisons and with real world data including object recognition and localization in Magnetic Resonance Tomography (MRT) data, registration of 2.5D RGBD scans from a Microsoft Kinect with a scale-free 3D model generated by Multi-View Vision, and 3D mapping by registration of a sequence of consecutive scans from a low-cost actuated Laser Range Finder. The results show that the method is fast and that it can robustly handle partial overlap, interfering structures, and noise. It is also shown that the method is a very interesting option for 6-DoF registration, i.e., when scale is known.	[Buelow, Heiko; Birk, Andreas] Jacobs Univ Bremen, Sch Sci & Engn, Robot Comp Sci & Elect Engn, D-28759 Bremen, Germany	Jacobs University	Birk, A (corresponding author), Jacobs Univ Bremen, Sch Sci & Engn, Robot Comp Sci & Elect Engn, D-28759 Bremen, Germany.	h.buelow@jacobs-university.de; a.birk@jacobs-university.de	Birk, Andreas/W-9259-2019	Birk, Andreas/0000-0003-4577-2525				Bariya P, 2012, INT J COMPUT VISION, V99, P232, DOI 10.1007/s11263-012-0526-7; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bulow H, 2013, IEEE T PATTERN ANAL, V35, P954, DOI 10.1109/TPAMI.2012.173; CASASENT D, 1977, P IEEE, V65, P77, DOI 10.1109/PROC.1977.10432; Censi A, 2005, IEEE INT CONF ROBOT, P2739; Censi A., 2009, P 2009 IEEE INT C RO; CHEN QS, 1994, IEEE T PATTERN ANAL, V16, P1156; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186; CIDECIYAN AV, 1995, IEEE ENG MED BIOL, V14, P52, DOI 10.1109/51.340749; DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008; Fiolka Torsten, 2012, Spatial Cognition VIII. Proceedings of the International Conference, Spatial Cognition 2012, P74, DOI 10.1007/978-3-642-32732-2_5; Fischer D, 2000, J INTELL ROBOT SYST, V29, P389, DOI 10.1023/A:1008164101786; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Foroosh H, 2002, IEEE T IMAGE PROCESS, V11, P188, DOI 10.1109/83.988953; Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279; Goodman J., 1996, OPT ENG, V2nd, DOI DOI 10.1117/1.601121; Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y; Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y; He WF, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P470; Healy DM, 2003, J FOURIER ANAL APPL, V9, P341, DOI 10.1007/s00041-003-0018-9; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; HORNER JL, 1984, APPL OPTICS, V23, P812, DOI 10.1364/AO.23.000812; Kazhdan M., 2003, EUR S GEOM PROC; Keller Y., 2005, P IEEE C COMP VIS PA; Keller Y, 2006, IEEE T SIGNAL PROCES, V54, P4323, DOI 10.1109/TSP.2006.881217; Kostelec PJ, 2008, J FOURIER ANAL APPL, V14, P145, DOI 10.1007/s00041-008-9013-5; Kostelec PJ., 2003, SANTA FE I WORKING P; LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Lowe DG, 2001, PROC CVPR IEEE, P682; Lucchese L, 2002, IEEE T PATTERN ANAL, V24, P1468, DOI 10.1109/TPAMI.2002.1046160; Makadia A., 2006, P IEEE C COMP VIS PA, V1, P1297, DOI DOI 10.1109/CVPR.2006.122; Mian AS, 2006, INT J COMPUT VISION, V66, P19, DOI 10.1007/s11263-005-3221-0; Oppenheim A.V., 1989, DISCRETE TIME SIGNAL; OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022; Ozden KE, 2004, COMPUT VIS IMAGE UND, V96, P453, DOI 10.1016/j.cviu.2004.03.015; Pan W, 2009, IEEE T PATTERN ANAL, V31, P400, DOI 10.1109/TPAMI.2008.83; Pathak K, 2010, IEEE T ROBOT, V26, P424, DOI 10.1109/TRO.2010.2042989; Pathak K, 2010, J FIELD ROBOT, V27, P52, DOI 10.1002/rob.20322; Pfingsthorn M., 2012, INT C ROB AUT ICRA; Pomerleau F, 2013, AUTON ROBOT, V34, P133, DOI 10.1007/s10514-013-9327-2; Pottmann H, 2006, INT J COMPUT VISION, V67, P277, DOI 10.1007/s11263-006-5167-2; Reddy BS, 1996, IEEE T IMAGE PROCESS, V5, P1266, DOI 10.1109/83.506761; Rodola E, 2013, INT J COMPUT VISION, V102, P129, DOI 10.1007/s11263-012-0568-x; Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34; Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Rusu R., 2009, IEEE INT C ROB AUT I, P3212, DOI DOI 10.1109/R0B0T.2009.5152473; Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011; Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710; Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; Stone HS, 2003, J VIS COMMUN IMAGE R, V14, P114, DOI 10.1016/S1047-3203(03)00002-6; Thirion JP, 1996, INT J COMPUT VISION, V18, P121, DOI 10.1007/BF00054999; Tombari F, 2013, INT J COMPUT VISION, V102, P198, DOI 10.1007/s11263-012-0545-4; Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Tzimiropoulos G, 2010, IEEE T PATTERN ANAL, V32, P1899, DOI 10.1109/TPAMI.2010.107; VANDERLUGT A, 1964, IEEE T INFORM THEORY, V10, P139; Vaskevicius N, 2010, ADV ROBOTICS, V24, P1169, DOI 10.1163/016918610X501291; WENG J, 1993, INT J COMPUT VISION, V11, P211, DOI 10.1007/BF01469343; Zuiderveld K., 1994, GRAPHIC GEMS	65	10	11	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2018	126	7					731	750		10.1007/s11263-018-1067-5	http://dx.doi.org/10.1007/s11263-018-1067-5			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GH0DS					2022-12-18	WOS:000433072800004
J	Slavcheva, M; Kehl, W; Navab, N; Ilic, S				Slavcheva, Miroslava; Kehl, Wadim; Navab, Nassir; Ilic, Slobodan			SDF-2-SDF Registration for Real-Time 3D Reconstruction from RGB-D Data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Signed distance field; Registration; 3D reconstruction; Camera tracking; Global optimization; RGB-D sensors	LEVEL-SET; INTEGRATION; IMAGES; RESOLUTION	We tackle the task of dense 3D reconstruction from RGB-D data. Contrary to the majority of existing methods, we focus not only on trajectory estimation accuracy, but also on reconstruction precision. The key technique is SDF-2-SDF registration, which is a correspondence-free, symmetric, dense energy minimization method, performed via the direct voxel-wise difference between a pair of signed distance fields. It has a wider convergence basin than traditional point cloud registration and cloud-to-volume alignment techniques. Furthermore, its formulation allows for straightforward incorporation of photometric and additional geometric constraints. We employ SDF-2-SDF registration in two applications. First, we perform small-to-medium scale object reconstruction entirely on the CPU. To this end, the camera is tracked frame-to-frame in real time. Then, the initial pose estimates are refined globally in a lightweight optimization framework, which does not involve a pose graph. We combine these procedures into our second, fully real-time application for larger-scale object reconstruction and SLAM. It is implemented as a hybrid system, whereby tracking is done on the GPU, while refinement runs concurrently over batches on the CPU. To bound memory and runtime footprints, registration is done over a fixed number of limited-extent volumes, anchored at geometry-rich locations. Extensive qualitative and quantitative evaluation of both trajectory accuracy and model fidelity on several public RGB-D datasets, acquired with various quality sensors, demonstrates higher precision than related techniques.	[Slavcheva, Miroslava; Kehl, Wadim; Navab, Nassir; Ilic, Slobodan] Tech Univ Munich, Munich, Germany; [Slavcheva, Miroslava; Ilic, Slobodan] Siemens CT, Munich, Germany; [Kehl, Wadim] Toyota Res Inst, Los Altos, CA USA	Technical University of Munich; Siemens AG; Siemens Germany; Toyota Motor Corporation	Slavcheva, M (corresponding author), Tech Univ Munich, Munich, Germany.; Slavcheva, M (corresponding author), Siemens CT, Munich, Germany.	mira.slavcheva@tum.de; wadim.kehl@tri.global; nassir.navab@tum.de; slobodan.ilic@siemens.com		Slavcheva, Miroslava/0000-0001-5591-4887; Ilic, Slobodan/0000-0002-3413-1936				ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; Adorno BV, 2011, IEEE INT CONF ROBOT; Alexandre L.A., 2012, WORKSH COL DEPTH CAM, P7; [Anonymous], 2008, ZPRINTER 650 HARDW M; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bylow E., 2013, ROBOTICS SCI SYSTEMS; Bylow E., 2014, INT C PATT REC ICPR; Canelhas D., 2017, SDF TRACKER ROS WIKI; Canelhas DR, 2013, IEEE RSJ INT C INT R; Chen JW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461940; Chen Y., 1991, IEEE INT C ROB AUT I; Choi S., 2016, ARXIV160202481; Choi S., 2015, IEEE C COMP VIS PATT; Clarenz U, 2004, IEEE T VIS COMPUT GR, V10, P516, DOI 10.1109/TVCG.2004.34; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Dalela PK, 2014, IEEE I C ADV NETW TE; Dimashova M., 2013, 3 WORKSH SEM PERC MA; Drost B., 2010, IEEE C COMP VIS PATT; Endres F., 2012, IEEE INT C ROB AUT I; Fioraio N., 2015, IEEE C COMP VIS PATT; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gelfand N., 2005, 3 EUR S GEOM PROC SG; Henry P., 2013, INT C 3D VIS 3DV; Henry P., 2010, INT S EXP ROB; Holzer S., 2012, EUR C COMP VIS ECCV; Houston B, 2006, ACM T GRAPHIC, V25, P151, DOI 10.1145/1122501.1122508; Ioannou Yani, 2012, 2 INT C 3D IM MOD PR; Izadi S., 2011, P ACM S US INT SOFTW, P16; Johnson AE, 1999, IMAGE VISION COMPUT, V17, P135, DOI 10.1016/S0262-8856(98)00117-6; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kahler O, 2015, IEEE T VIS COMPUT GR, V21, P1241, DOI 10.1109/TVCG.2015.2459891; Kehl W., 2016, BRIT MACH VIS C BMVC; Kehl W., 2014, P BRIT MACH VIS C BM; Keller G, 2013, CONF P INDIUM PHOSPH; Kerl C., 2013, IEEE INT C ROB AUT I; Kerl C, 2017, GITHUB TUM VISION DV; Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437; Klein G., 2007, IEEE ACM INT S MIX A; Kummerle R., 2011, IEEE INT C ROB AUT I; Lai K, 2011, IEEE INT CONF ROBOT; Lorensen W, 1987, 14 ANN C COMP GRAPH; Losasso F, 2006, COMPUT FLUIDS, V35, P995, DOI 10.1016/j.compfluid.2005.01.006; Ma Y., 2003, INVITATION 3 D VISIO; Masuda T, 2002, COMPUT VIS IMAGE UND, V87, P51, DOI 10.1006/cviu.2002.0982; Meilland M., 2013, 2013 IEEE RSJ INT C; Narayan K.S., 2015, IEEE INT C ROB AUT I; Newcombe R. A., 2011, 10 INT S MIX AUGM RE; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Nielsen MB, 2006, J SCI COMPUT, V26, P261, DOI 10.1007/s10915-005-9062-8; Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374; Osher S., 2003, APPL MATH SCI, V153; Pirker K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.115; Ren C. Y., 2012, EUR C COMP VIS 2 WOR; Roth H, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.112; RUSINKIEWICZ S, 2001, 3 INT C 3D DIG IM MO; Rusu R B, 2009, IEEE RSJ INT C INT R; Schutz C., 1998, INT C PATT REC ICPR; Segal A., 2009, ROBOTICS SCI SYSTEMS; Slavcheva M., 2016, BRIT MACH VIS C BMVC; Slavcheva M., 2016, EUR C COMP VIS ECCV; Steder Bastian, 2010, WORKSH DEF SOLV REAL, V44, P2; Steinbrucker F., 2014, IEEE INT C ROB AUT I; Steinbrucker F, 2013, IEEE I CONF COMP VIS, P3264, DOI 10.1109/ICCV.2013.405; Sturm J., 2017, P INT C INT ROB SYST; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Tombari F, 2013, INT J COMPUT VISION, V102, P198, DOI 10.1007/s11263-012-0545-4; Van Gool L, 2006, 18 INT C PATT REC IC; Vijayanagar KR, 2014, MOBILE NETW APPL, V19, P414, DOI 10.1007/s11036-013-0458-7; Wasenmuller O, 2016, IEEE WINT CONF APPL; Wasenmuller O., 2016, AS C COMP VIS ACCV I; Whelan T., 2012, KINTINUOUS SPATIALLY; Whelan T., 2013, IEEE RSJ INT C INT R; Whelan T., 2013, IEEE INT CONF AUTOMA; Whelan T, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237; Whitaker RT, 1998, INT J COMPUT VISION, V29, P203, DOI 10.1023/A:1008036829907; Zach C., 2007, P 11 IEEE INT C COMP, P1, DOI DOI 10.1109/ICCV.2007.4408983; Zeng M, 2013, GRAPH MODELS, V75, P126, DOI 10.1016/j.gmod.2012.09.002; Zhou Q., 2013, IEEE INT C COMP VIS; Zhou QY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461919	80	10	10	0	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2018	126	6					615	636		10.1007/s11263-017-1057-z	http://dx.doi.org/10.1007/s11263-017-1057-z			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GC3EO					2022-12-18	WOS:000429667300003
J	Fernando, B; Gould, S				Fernando, Basura; Gould, Stephen			Discriminatively Learned Hierarchical Rank Pooling Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Rank pooling; Action recognition; Activity recognition; Convolutional neural networks		Rank pooling is a temporal encoding method that summarizes the dynamics of a video sequence to a single vector which has shown good results in human action recognition in prior work. In this work, we present novel temporal encoding methods for action and activity classification by extending the unsupervised rank pooling temporal encoding method in two ways. First, we present discriminative rank pooling in which the shared weights of our video representation and the parameters of the action classifiers are estimated jointly for a given training dataset of labelled vector sequences using a bilevel optimization formulation of the learning problem. When the frame level features vectors are obtained from a convolutional neural network (CNN), we rank pool the network activations and jointly estimate all parameters of the model, including CNN filters and fully-connected weights, in an end-to-end manner which we coined as end-to-end trainable rank pooled CNN. Importantly, this model can make use of any existing convolutional neural network architecture (e.g., AlexNet or VGG) without modification or introduction of additional parameters. Then, we extend rank pooling to a high capacity video representation, called hierarchical rank pooling. Hierarchical rank pooling consists of a network of rank pooling functions, which encode temporal semantics over arbitrary long video clips based on rich frame level features. By stacking non-linear feature functions and temporal sub-sequence encoders one on top of the other, we build a high capacity encoding network of the dynamic behaviour of the video. The resulting video representation is a fixed-length feature vector describing the entire video clip that can be used as input to standard machine learning classifiers. We demonstrate our approach on the task of action and activity recognition. We present a detailed analysis of our approach against competing methods and explore variants such as hierarchy depth and choice of non-linear feature function. Obtained results are comparable to state-of-the-art methods on three important activity recognition benchmarks with classification performance of 76.7% mAP on Hollywood2, 69.4% on HMDB51, and 93.6% on UCF101.	[Fernando, Basura; Gould, Stephen] Australian Natl Univ, Res Sch Engn, ACRV, Canberra, ACT, Australia	Australian National University	Fernando, B (corresponding author), Australian Natl Univ, Res Sch Engn, ACRV, Canberra, ACT, Australia.	basura.fernando@anu.edu.au		Fernando, Basura/0000-0002-6920-9916	Australian Research Council Centre of Excellence for Robotic Vision [CE140100016]	Australian Research Council Centre of Excellence for Robotic Vision(Australian Research Council)	This research was supported by the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016).	Abu-El-Haija S, 2016, YOUTUBE 8M LARGE SCA; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2015, P INT C MACH LEARN; Bard J. F., 1998, PRACTICAL BILEVEL OP; BILEN H, 2016, ARXIV161200738; Bilen Hakan, 2016, CVPR, DOI DOI 10.1109/CVPR.2016.331; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bregler C, 1997, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.1997.609382; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chollet F., 2015, KERAS; Courtney PG, 2015, IEEE COMP SEMICON; Dempe S, 2016, COMPUT OPTIM APPL, V63, P685, DOI 10.1007/s10589-015-9795-8; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Do C. B., 2007, NIPS; Domke Justin, 2012, INT C ARTIFICIAL INT; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Fernando B, 2016, ICML; Fernando B., 2016, IEEE T PATTERN ANAL, P1; Fernando B, 2016, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR.2016.212; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Golub G. H., 2012, MATRIX COMPUTATIONS; Gould Stephen, 2016, ARXIV160705447; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hoai M., 2014, ACCV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hughes M. C., 2012, IEEE C COMP VIS PATT, P25; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Joachims T., 2006, P 12 ACM SIGKDD INT, V06, P217, DOI DOI 10.1145/1150402.1150429; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Klatzer T., 2015, COMP VIS WINT WORKSH, P39; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuehne H., 2011, P INT C COMP VIS, DOI DOI 10.1109/ICCV.2011.6126543; Lan T., 2015, ICCV; Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299; Liu T-Y., 2009, FOUND TRENDS INF RET, V3, P225, DOI DOI 10.1561/1500000016; Lu L, 2002, IEEE T SPEECH AUDI P, V10, P504, DOI 10.1109/TSA.2002.804546; Ochs P., 2015, INT C SCAL SPAC VAR, V9087, P654, DOI DOI 10.1007/978-3-319-18461-6_52; Peng XJ, 2014, LECT NOTES COMPUT SC, V8691, P660, DOI 10.1007/978-3-319-10578-9_43; Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Ryoo MS, 2015, PROC CVPR IEEE, P896, DOI 10.1109/CVPR.2015.7298691; Samuel K. G. G., 2009, CVPR; Sener O, 2015, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2015.509; Shinozaki K, 2003, CURR OPIN PLANT BIOL, V6, P410, DOI 10.1016/S1369-5266(03)00092-X; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Snoek C., 2016, ACTIVITYNET LARGE SC; Song Y, 2013, CVPR; Soomro K., 2012, ARXIV; Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Willsky A. S., 2009, ADV NEURAL INFORM PR, P549; Wu JJ, 2014, PROC CVPR IEEE, P256, DOI 10.1109/CVPR.2014.40; Zha S., 2015, P BRIT MACH VIS C	71	10	10	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	3					335	355		10.1007/s11263-017-1030-x	http://dx.doi.org/10.1007/s11263-017-1030-x			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FE1ER		Green Submitted			2022-12-18	WOS:000407961700005
J	Jain, M; van Gemert, J; Jegou, H; Bouthemy, P; Snoek, CGM				Jain, Mihir; van Gemert, Jan; Jegou, Herve; Bouthemy, Patrick; Snoek, Cees G. M.			Tubelets: Unsupervised Action Proposals from Spatiotemporal Super-Voxels	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action localization; Video representation; Action classification	DESCRIPTORS; RECOGNITION	This paper considers the problem of localizing actions in videos as sequences of bounding boxes. The objective is to generate action proposals that are likely to include the action of interest, ideally achieving high recall with few proposals. Our contributions are threefold. First, inspired by selective search for object proposals, we introduce an approach to generate action proposals from spatiotemporal super-voxels in an unsupervised manner, we call them Tubelets. Second, along with the static features from individual frames our approach advantageously exploits motion. We introduce independent motion evidence as a feature to characterize how the action deviates from the background and explicitly incorporate such motion information in various stages of the proposal generation. Finally, we introduce spatiotemporal refinement of Tubelets, for more precise localization of actions, and pruning to keep the number of Tubelets limited. We demonstrate the suitability of our approach by extensive experiments for action proposal quality and action localization on three public datasets: UCF Sports, MSR-II and UCF101. For action proposal quality, our unsupervised proposals beat all other existing approaches on the three datasets. For action localization, we show top performance on both the trimmed videos of UCF Sports and UCF101 as well as the untrimmed videos of MSR-II.	[Jain, Mihir] Qualcomm Res, Amsterdam, Netherlands; [Jegou, Herve] Facebook Al Res, Paris, France; [Jain, Mihir; Jegou, Herve; Bouthemy, Patrick] INRIA, Rennes, France; [Jain, Mihir; Jegou, Herve; Snoek, Cees G. M.] Univ Amsterdam, Amsterdam, Netherlands; [van Gemert, Jan] Delft Univ Technol, Delft, Netherlands	Facebook Inc; Inria; University of Amsterdam; Delft University of Technology	Jain, M (corresponding author), Qualcomm Res, Amsterdam, Netherlands.; Jain, M (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	mijain@qti.qualcomm.com; J.C.vanGemert@tudelft.nl; rvj@fb.com; patrick.bouthemy@inria.fr; cgmsnoek@uva.nl						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Cao L., 2010, P IEEE C COMP VIS PA; Chen W., 2015, P IEEE INT C COMP VI; Chen W, 2014, PROC CVPR IEEE, P748, DOI 10.1109/CVPR.2014.101; Cheron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368; CLEVELAND WS, 1979, J AM STAT ASSOC, V74, P829, DOI 10.2307/2286407; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Delaitre V., 2010, BMVC, DOI DOI 10.5244/C.24.97; Derpanis KG, 2013, IEEE T PATTERN ANAL, V35, P527, DOI 10.1109/TPAMI.2012.141; Dollar Piotr, 2005, VISUAL SURVEILLANCE; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Tran D, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995416; Everts I, 2014, IEEE T IMAGE PROCESS, V23, P1569, DOI 10.1109/TIP.2014.2302677; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908; Huber P., 1981, ROBUST STAT; Jain M., 2014, P IEEE C COMP VIS PA; Jain M., 2016, FRONTIERS ICT COMPUT, V2, P28, DOI DOI 10.3389/FICT.2015.00028; Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95; Ke Y., 2005, P IEEE INT C COMP VI; Kim M, 2010, LECT NOTES COMPUT SC, V6313, P649; Klaser A., 2012, P EUR C COMP VIS, V6553, P219; Klaser Alexander, 2008, BMVC; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kwak S., 2015, ICCV; LAMPERT C. H., 2008, P IEEE C COMP VIS PA; Lan T., 2011, P IEEE INT C COMP VI; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Ma SG, 2013, IEEE I CONF COMP VIS, P2744, DOI 10.1109/ICCV.2013.341; Maji S., 2011, CVPR, DOI DOI 10.1109/CVPR.2011.5995631; Manen S, 2013, IEEE I CONF COMP VIS, P2536, DOI 10.1109/ICCV.2013.315; Mosabbeb E. A., 2014, ACCV; ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029; Oneata D., 2014, P EUR C COMP VIS; Oneata D., 2013, P IEEE INT C COMP VI; Oneata D, 2014, PROC CVPR IEEE, P2545, DOI 10.1109/CVPR.2014.326; Perronnin F, 2007, P IEEE C COMP VIS PA; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Piriou G, 2006, IEEE T IMAGE PROCESS, V15, P3417, DOI 10.1109/TIP.2006.881963; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Puscas M., 2015, P IEEE INT C COMP VI; Raptis M., 2012, P IEEE C COMP VIS PA; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Soomro K., 2012, ARXIV; Soomro Khurram, 2015, P IEEE INT C COMP VI, V1, P3; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tian Y., 2013, P IEEE C COMP VIS PA; Tran D., 2012, ADV NEURAL INFORM PR; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; van Gemert J. C., 2015, P BRIT MACH VIS C; vandeSande K. E. A., 2014, P IEEE C COMP VIS PA; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang H., 2015, INT J COMPUT VISION, P1; WANG H, 2013, INT C COMP VIS, DOI DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang L., 2014, P EUR C COMP VIS; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214; Weinzaepfel Philippe, 2015, P IEEE INT C COMP VI; Xu C., 2012, P IEEE C COMP VIS PA; Yu G., 2015, P IEEE C COMP VIS PA; Yuan J., 2009, P IEEE C COMP VIS PA; Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPR.2009.5206671, 10.1109/CVPRW.2009.5206671]	76	10	10	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	3					287	311		10.1007/s11263-017-1023-9	http://dx.doi.org/10.1007/s11263-017-1023-9			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FE1ER		Green Published, Green Submitted, hybrid			2022-12-18	WOS:000407961700003
J	Robertini, N; Casas, D; De Aguiar, E; Theobalt, C				Robertini, Nadia; Casas, Dan; De Aguiar, Edilson; Theobalt, Christian			Multi-view Performance Capture of Surface Details	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Performance capture; Surface detail; Sums of Gaussian	MOTION CAPTURE; VIDEO; SHAPE	This paper presents a novel approach to recover true fine surface detail of deforming meshes reconstructed from multi-view video. Template-based methods for performance capture usually produce a coarse-to-medium scale detail 4D surface reconstruction which does not contain the real high-frequency geometric detail present in the original video footage. Fine scale deformation is often incorporated in a second pass by using stereo constraints, features, or shading-based refinement. In this paper, we propose an alternative solution to this second stage by formulating dense dynamic surface reconstruction as a global optimization problem of the densely deforming surface. Our main contribution is an implicit representation of a deformable mesh that uses a set of Gaussian functions on the surface to represent the initial coarse mesh, and a set of Gaussians for the images to represent the original captured multi-view images. We effectively find the fine scale deformations for all mesh vertices, which maximize photo-temporal-consistency, by densely optimizing our model-to-image consistency energy on all vertex positions. Our formulation yields a smooth closed form energy with implicit occlusion handling and analytic derivatives. Furthermore, it does not require error-prone correspondence finding or discrete sampling of surface displacement values. We demonstrate our approach on a variety of datasets of human subjects wearing loose clothing and performing different motions. We qualitatively and quantitatively demonstrate that our technique successfully reproduces finer detail than the input baseline geometry.	[Robertini, Nadia; Casas, Dan; Theobalt, Christian] Max Planck Inst Informat, Saarbrucken, Germany; [De Aguiar, Edilson] CEUNES UFES, Sao Mateus, Brazil; [Robertini, Nadia] Intel Visual Comp Insitute Intel VCI, Saarbrucken, Germany	Max Planck Society; Universidade Federal do Espirito Santo	Robertini, N (corresponding author), Max Planck Inst Informat, Saarbrucken, Germany.; Robertini, N (corresponding author), Intel Visual Comp Insitute Intel VCI, Saarbrucken, Germany.	nroberti@mpi-inf.mpg.de; dcasas@mpi-inf.mpg.de; edilson.aguiar@ufes.br; theobalt@mpi-inf.mpg.de	Casas, Dan/AAF-9801-2019; Casas, Dan/P-2111-2017	Casas, Dan/0000-0002-3664-089X; Theobalt, Christian/0000-0001-6104-6625	Max Planck Society; ERC Starting Grant project CapReal [335545]	Max Planck Society(Max Planck SocietyFoundation CELLEX); ERC Starting Grant project CapReal	Open access funding provided by Max Planck Society. This research was funded by the ERC Starting Grant project CapReal (335545).	Ahmed Naveed, 2008, IEEE C COMP VIS PATT, P1; Allain B., 2015, CVPR 2015; Bradley D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360698; Brox T., 2004, HIGH ACCURACY OPTICA; Budd C, 2013, INT J COMPUT VISION, V102, P256, DOI 10.1007/s11263-012-0553-4; Cagniart C, 2010, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2010.5539814; CARRANZA J., 2003, ACM T GRAPHIC, P22; Cortelazzo G.M., 2008, 3DPVT; de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697; DEAGUIAR E, 2007, CVPR; Gall J., 2009, IEEE C COMP VIS PATT; Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755; Hernandez C, 2007, IEEE I CONF COMP VIS, P873; Ilic S, 2006, IEEE T PATTERN ANAL, V28, P328, DOI 10.1109/TPAMI.2006.37; Kolev K, 2009, INT J COMPUT VISION, V84, P80, DOI 10.1007/s11263-009-0233-1; Liu YB, 2013, IEEE T PATTERN ANAL, V35, P2720, DOI 10.1109/TPAMI.2013.47; Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951; Plankers R, 2003, IEEE T PATTERN ANAL, V25, P1182, DOI 10.1109/TPAMI.2003.1227995; Popa T, 2009, COMPUT GRAPH FORUM, V28, P427, DOI 10.1111/j.1467-8659.2009.01382.x; Robertini N., 2014, P INT C 3D VIS 3DV 3; Sand P, 2003, ACM T GRAPHIC, V22, P578, DOI 10.1145/882262.882310; Savoye Y., 2013, P 10 EUR C VIS MED P, P8; Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68; Stoll C, 2011, IEEE I CONF COMP VIS, P951, DOI 10.1109/ICCV.2011.6126338; Theobalt C, 2010, GEOM COMPUT, V5, P127, DOI 10.1007/978-3-642-12392-4_6; Tung T, 2009, IEEE I CONF COMP VIS, P1709, DOI 10.1109/ICCV.2009.5459384; Vlasic D., 2008, ACM TOG; Vlasic D., 2009, ACM TOG; Vlasic D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618520; Waschbusch M, 2005, VISUAL COMPUT, V21, P629, DOI 10.1007/s00371-005-0346-7; Wu CL, 2011, IEEE I CONF COMP VIS, P1108, DOI 10.1109/ICCV.2011.6126358; Wu CL, 2011, IEEE T VIS COMPUT GR, V17, P1082, DOI [10.1109/TVCG.2010.224, 10.1109/TPDS.2010.224]; Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766	34	10	10	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2017	124	1					96	113		10.1007/s11263-016-0979-1	http://dx.doi.org/10.1007/s11263-016-0979-1			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FA2JD	32025094	Green Published, hybrid			2022-12-18	WOS:000405265900006
J	Jia, K; Chan, TH; Zeng, ZN; Gao, SH; Wang, G; Zhang, TZ; Ma, Y				Jia, Kui; Chan, Tsung-Han; Zeng, Zinan; Gao, Shenghua; Wang, Gang; Zhang, Tianzhu; Ma, Yi			ROML: A Robust Feature Correspondence Approach for Matching Objects in A Set of Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object matching; Feature correspondence; Low-rank; Sparsity	SHAPE; ALGORITHM	Feature-based object matching is a fundamental problem for many applications in computer vision, such as object recognition, 3D reconstruction, tracking, and motion segmentation. In this work, we consider simultaneously matching object instances in a set of images, where both inlier and outlier features are extracted. The task is to identify the inlier features and establish their consistent correspondences across the image set. This is a challenging combinatorial problem, and the problem complexity grows exponentially with the image number. To this end, we propose a novel framework, termed Robust Object Matching using Low-rank constraint (ROML), to address this problem. ROML optimizes simultaneously a partial permutation matrix (PPM) for each image, and feature correspondences are established by the obtained PPMs. Two of our key contributions are summarized as follows. (1) We formulate the problem as rank and sparsity minimization for PPM optimization, and treat simultaneous optimization of multiple PPMs as a regularized consensus problem in the context of distributed optimization. (2) We use the alternating direction method of multipliers method to solve the thus formulated ROML problem, in which a subproblem associated with a single PPM optimization appears to be a difficult integer quadratic program (IQP). We prove that under wildly applicable conditions, this IQP is equivalent to a linear sum assignment problem, which can be efficiently solved to an exact solution. Extensive experiments on rigid/non-rigid object matching, matching instances of a common object category, and common object localization show the efficacy of our proposed method.	[Jia, Kui] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, E11 Ave Univ, Taipa, Macau Sar, Peoples R China; [Chan, Tsung-Han] MediaTek Inc, 1,Dusing 1st Rd,Hsinchu Sci Pk, Hsinchu 30078, Taiwan; [Zeng, Zinan] Adv Digital Sci Ctr, 1 Fusionopolis Way, Singapore, Singapore; [Gao, Shenghua; Ma, Yi] ShanghaiTech Univ, Sch Informat Sci & Technol, 8 Bldg,319 Yueyang Rd, Shanghai 200031, Peoples R China; [Wang, Gang] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore; [Zhang, Tianzhu] Chinese Acad Sci, Inst Automat, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China	University of Macau; Mediatek Incorporated; ShanghaiTech University; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Chinese Academy of Sciences; Institute of Automation, CAS	Jia, K (corresponding author), Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, E11 Ave Univ, Taipa, Macau Sar, Peoples R China.	kuijia@umac.mo; thchan@ieee.org; zeng.zinan@gmail.com; gaoshh@shanghaitech.edu.cn; wanggang@ntu.edu.sg; tzzhang@nlpr.ia.ac.cn; mayi@shanghaitech.edu.cn	Zhang, Tianzhu/AGY-9389-2022	Zhang, Tianzhu/0000-0003-0764-6106	National Natural Science Foundation of China [61202158]; Singapore's Agency for Science, Technology and Research (A*STAR)	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Singapore's Agency for Science, Technology and Research (A*STAR)(Agency for Science Technology & Research (A*STAR))	This work is partially supported by the National Natural Science Foundation of China (Grant No. 61202158), and the research grant for the Human Sixth Sense Programme at the Advanced Digital Sciences Center from Singapore's Agency for Science, Technology and Research (A*STAR).	Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226; [Anonymous], 2007, PASCAL VISUAL OBJECT; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Berg AC, 2005, PROC CVPR IEEE, P26; Berg AC, 2001, PROC CVPR IEEE, P607; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Burkard R., 2009, ASSIGNMENT PROBLEMS; Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Chum O., 2007, P IEEE C COMP VIS PA, P1; Collins RT, 2012, PROC CVPR IEEE, P1744, DOI 10.1109/CVPR.2012.6247870; Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228; Cour T., 2006, NIPS; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deselaers T, 2012, INT J COMPUT VISION, V100, P275, DOI 10.1007/s11263-012-0538-3; Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110; Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42; Everingham M., 2006, PASCAL VISUAL OBJECT; Feng J, 2011, IEEE I CONF COMP VIS, P1028, DOI 10.1109/ICCV.2011.6126348; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hao Jiang, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2474, DOI 10.1109/CVPRW.2009.5206776; Kim G., 2008, CVPR; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Lee J, 2011, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2011.5995387; Lee YJ, 2009, INT J COMPUT VISION, V85, P143, DOI 10.1007/s11263-009-0252-y; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Leordeanu M., 2009, NIPS, V2, P7; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Li HS, 2011, IEEE I CONF COMP VIS, P33, DOI 10.1109/ICCV.2011.6126222; Lin Z., 2010, ARXIV PREPRINT ARXIV; Liu D, 2007, IEEE I CONF COMP VIS, P191; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Lucas B.D., 1981, IJCAI 81 P 7 INT JOI, P674, DOI DOI 10.1109/HPDC.2004.1323531; Maciel J, 2003, IEEE T PATTERN ANAL, V25, P187, DOI 10.1109/TPAMI.2003.1177151; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Oliveira R, 2006, LECT NOTES COMPUT SC, V3953, P490, DOI 10.1007/11744078_38; Oliveira R, 2005, PROC CVPR IEEE, P1016; Pachauri D., 2013, ADV NEURAL INFORM PR, V26, P1860; Poore AB, 1997, COMPUT OPTIM APPL, V8, P129, DOI 10.1023/A:1008669120497; Robertson AJ, 2001, COMPUT OPTIM APPL, V19, P145, DOI 10.1023/A:1011285402433; Russell B. C., 2006, P IEEE C COMP VIS PA, V2, P1605; SCOTT GL, 1991, P ROY SOC B-BIOL SCI, V244, P21, DOI 10.1098/rspb.1991.0045; Serre T, 2005, PROC CVPR IEEE, P994; SHAPIRO LS, 1992, IMAGE VISION COMPUT, V10, P283, DOI 10.1016/0262-8856(92)90043-3; Siva P, 2013, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2013.416; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torki M, 2010, PROC CVPR IEEE, P3058, DOI 10.1109/CVPR.2010.5540059; Torresani L, 2008, LECT NOTES COMPUT SC, V5303, P596, DOI 10.1007/978-3-540-88688-4_44; Tuytelaars T, 2010, INT J COMPUT VISION, V88, P284, DOI 10.1007/s11263-009-0271-8; Zass R., 2008, P 2008 IEEE C COMP V, P1, DOI DOI 10.1109/CVPR.2008.4587500; Zeng ZA, 2012, LECT NOTES COMPUT SC, V7576, P325, DOI 10.1007/978-3-642-33715-4_24; Zeng ZN, 2013, PROC CVPR IEEE, P708, DOI 10.1109/CVPR.2013.97; Zhang Y., 2010, ALTERNATING DIRECTIO; Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667; Zhu JY, 2012, PROC CVPR IEEE, P3218, DOI 10.1109/CVPR.2012.6248057	67	10	14	2	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2016	117	2					173	197		10.1007/s11263-015-0858-1	http://dx.doi.org/10.1007/s11263-015-0858-1			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DH6UB		Green Submitted			2022-12-18	WOS:000372926500005
J	Kitamura, Y; Li, YZ; Ito, W; Ishikawa, H				Kitamura, Yoshiro; Li, Yuanzhong; Ito, Wataru; Ishikawa, Hiroshi			Data-Dependent Higher-Order Clique Selection for Artery-Vein Segmentation by Energy Minimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Segmentation; Higher-order energy; Artery-vein segmentation; Surgery simulation	SEPARATION	We propose a novel segmentation method based on energy minimization of higher-order potentials. We introduce higher-order terms into the energy to incorporate prior knowledge on the shape of the segments. The terms encourage certain sets of pixels to be entirely in one segment or the other. The sets can for instance be smooth curves in order to help delineate pulmonary vessels, which are known to run in almost straight lines. The higher-order terms can be converted to submodular first-order terms by adding auxiliary variables, which can then be globally minimized using graph cuts. We also determine the weight of these terms, or the degree of the aforementioned encouragement, in a principled way by learning from training data with the ground truth. We demonstrate the effectiveness of the method in a real-world application in fully-automatic pulmonary artery-vein segmentation in CT images.	[Kitamura, Yoshiro; Li, Yuanzhong; Ito, Wataru] Fujifilm Corp, Imaging Technol Ctr, Tokyo, Japan; [Kitamura, Yoshiro; Ishikawa, Hiroshi] Waseda Univ, Dept Comp Sci & Engn, Tokyo, Japan; [Kitamura, Yoshiro; Ishikawa, Hiroshi] JST CREST, Saitama, Japan	Fujifilm Corporation; Waseda University; Japan Science & Technology Agency (JST)	Kitamura, Y (corresponding author), Fujifilm Corp, Imaging Technol Ctr, Tokyo, Japan.; Kitamura, Y (corresponding author), Waseda Univ, Dept Comp Sci & Engn, Tokyo, Japan.; Kitamura, Y (corresponding author), JST CREST, Saitama, Japan.	yoshiro.kitamura@fujifilm.com			Japan Society for the Promotion of Science (JSPS) [26108003]; Japan Science and Technology Agency (JST)	Japan Society for the Promotion of Science (JSPS)(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science); Japan Science and Technology Agency (JST)(Japan Science & Technology Agency (JST))	We appreciate the advice regarding clinical knowledge and subjective tests given by the Department of Radiology, NTT East Sapporo Hospital. This research was done in cooperation with the Department of Thoracic Surgery, Tokyo Medical University Hospital. In the course of this work, Hiroshi Ishikawa was partially supported by the Grant-in-Aid for Scientific Research on Innovative Areas #26108003 from the Japan Society for the Promotion of Science (JSPS) as well as the CREST grant from the Japan Science and Technology Agency (JST).	Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Buelow T., 2005, P SPIE MED IM, P5746; Ebrahimdoost Y., 2011, P DSP; El-Zehiry NY, 2010, PROC CVPR IEEE, P3257, DOI 10.1109/CVPR.2010.5540057; Ferlay J, 2010, INT J CANCER, V127, P2893, DOI 10.1002/ijc.25516; Frangi AF, 1998, LECT NOTES COMPUT SC, V1496, P130, DOI 10.1007/BFb0056195; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Gao ZY, 2012, IEEE T BIO-MED ENG, V59, P3016, DOI 10.1109/TBME.2012.2212894; HAMMER PL, 1984, MATH PROGRAM, V28, P121, DOI 10.1007/BF02612354; Ikeda N, 2013, ANN THORAC CARDIOVAS, V19, P1, DOI 10.5761/atcs.ra.12.02174; Inoue T., 2013, P SPIE MED IMAGING; Ishikawa H, 2011, IEEE T PATTERN ANAL, V33, P1234, DOI 10.1109/TPAMI.2010.91; IVANESCU PL, 1965, OPER RES, V13, P388, DOI 10.1287/opre.13.3.388; Kadoury S, 2013, LECT NOTES COMPUT SC, V8149, P719, DOI 10.1007/978-3-642-40811-3_90; Kitamura Y., 2013, 5 INT WORKSH PULM IM; Kitamura Y, 2012, 2012 9TH IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING (ISBI), P234, DOI 10.1109/ISBI.2012.6235527; Kohli P., 2005, P ICCV; Kohli P, 2009, IEEE T PATTERN ANAL, V31, P1645, DOI 10.1109/TPAMI.2008.217; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Lei T., 2010, STAT MED IMAGING; Lei TH, 2001, IEEE T MED IMAGING, V20, P689, DOI 10.1109/42.938238; Li S, 2009, MARKOV RANDOM FIELD; Lo P, 2012, IEEE T MED IMAGING, V31, P2093, DOI 10.1109/TMI.2012.2209674; Mekada Y., 2006, P 7 AS PAC C CONTR M, P232; Nieuwenhuis C., 2014, P CVPR; Nowozin S, 2009, PROC CVPR IEEE, P818, DOI 10.1109/CVPRW.2009.5206567; Olsson C, 2013, IEEE I CONF COMP VIS, P2936, DOI 10.1109/ICCV.2013.365; Oswald Martin Ralf, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8692, P32, DOI 10.1007/978-3-319-10593-2_3; Park S, 2013, MED PHYS, V40, DOI 10.1118/1.4811203; Rother C., 2007, P IEEE C COMP VIS PA; Russell C., 2007, P ICCV; Saha PK, 2010, IEEE T MED IMAGING, V29, P840, DOI 10.1109/TMI.2009.2038224; Saji H, 2013, INTERACT CARDIOV TH, V17, P227, DOI 10.1093/icvts/ivt120; Scharstein D., 2007, P CVPR; Schoenemann T, 2012, INT J COMPUT VISION, V99, P53, DOI 10.1007/s11263-012-0518-7; SEBBE R, 2003, P PRORISC WORKSH CIR; Shekhovtsov Alexander, 2012, Pattern Recognition. Proceedings Joint 34th DAGM and 36th OAGM Symposium, P41, DOI 10.1007/978-3-642-32717-9_5; Strandmark Petter, 2011, Energy Minimization Methods in Computer Vision and Pattern Recognition. Proceedings 8th International Conference, EMMCVPR 2011, P205, DOI 10.1007/978-3-642-23094-3_15; Stuhmer J, 2013, IEEE I CONF COMP VIS, P2336, DOI 10.1109/ICCV.2013.290; Sun DQ, 2008, LECT NOTES COMPUT SC, V5304, P83; van Bemmel CM, 2003, IEEE T MED IMAGING, V22, P1224, DOI 10.1109/TMI.2003.817756; Wang C., 2009, P SPIE MI, V72594T; Wang CH, 2013, COMPUT VIS IMAGE UND, V117, P1610, DOI 10.1016/j.cviu.2013.07.004; Yamaguchi T, 2002, CARS 2002: COMPUTER ASSISTED RADIOLOGY AND SURGERY, PROCEEDINGS, P782	44	10	12	1	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2016	117	2					142	158		10.1007/s11263-015-0856-3	http://dx.doi.org/10.1007/s11263-015-0856-3			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DH6UB					2022-12-18	WOS:000372926500003
J	Ozcanli, OC; Dong, Y; Mundy, JL; Webb, H; Hammoud, R; Tom, V				Ozcanli, Ozge C.; Dong, Yi; Mundy, Joseph L.; Webb, Helen; Hammoud, Riad; Tom, Victor			Automatic Geolocation Correction of Satellite Imagery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Georegistration; Satellite imagery; 3-D modeling; RPC camera model; Bias correction		Modern satellites tag their images with geolocation information using GPS and star tracking systems. Depending on the quality of the geopositioning equipment, errors may range from a few meters to tens of meters on the ground. At the current state of art, there is no established method to automatically correct these errors limiting the large-scale joint utilization of cross-platform satellite images. In this paper, an automatic geolocation correction framework that corrects images from multiple satellites simultaneously is presented. As a result of the proposed correction process, all the images are effectively registered to the same absolute geodetic coordinate frame. The usability and the quality of the correction framework are demonstrated through a 3-D surface reconstruction application. The 3-D surface models given by original satellite geopositioning metadata, and the corrected metadata, are compared. The quality difference is measured through an entropy-based metric applied to the orthographic height maps given by the 3-D surface models. Measuring the absolute accuracy of the framework is harder due to lack of publicly available high-precision ground surveys. However, the geolocation of images of exemplar satellites from different parts of the globe are corrected, and the road networks given by OpenStreetMap are projected onto the images using original and corrected metadata to demonstrate the improved quality of alignment.	[Ozcanli, Ozge C.; Dong, Yi; Mundy, Joseph L.] Vis Syst Inc, Providence, RI USA; [Webb, Helen; Hammoud, Riad; Tom, Victor] BAE Syst, Burlington, MA USA	Bae Systems	Ozcanli, OC (corresponding author), Vis Syst Inc, Providence, RI USA.	ozge@visionsystemsinc.com			Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory (AFRL) [FA8650-12-C-7211]	Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory (AFRL)	Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory (AFRL), contract FA8650-12-C-7211. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.	d'Angelo P, 2012, INT ARCH PHOTOGRAMM, V39-B1, P209; Dial G, 2005, P ASPRS 2005 ANN C; Fraser CS, 2003, PHOTOGRAMM ENG REM S, V69, P53, DOI 10.14358/PERS.69.1.53; Grodecki J, 2003, PHOTOGRAMM ENG REM S, V69, P59, DOI 10.14358/PERS.69.1.59; Mikhail E.M., 2001, INTRO MODERN PHOTOGR, V19; Oh Jaehong, 2010, ASPRS ANN C; Pollard Thomas, 2007, P COMP VIS PATT REC; Pollard TB, 2010, PHOTOGRAMM ENG REM S, V76, P817, DOI 10.14358/PERS.76.7.817; Pritt Mark D, 2011, APPL IM PATT REC WOR; Tom V., 1983, P SPIE APPL DIG IM P, P432	10	10	10	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2016	116	3			SI		263	277		10.1007/s11263-015-0852-7	http://dx.doi.org/10.1007/s11263-015-0852-7			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7SY					2022-12-18	WOS:000369421900006
J	Kuang, ZH; Wong, KYK				Kuang, Zhanghui; Wong, Kwan-Yee K.			Relatively-Paired Space Analysis: Learning a Latent Common Space From Relatively-Paired Observations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-modality analysis; Multi-viewanalysis; Absolutely-paired observations; Relatively-paired observations; Structural learning; Cutting plane algorithm	FACE RECOGNITION; STYLE	Discovering a latent common space between different modalities plays an important role in cross-modality pattern recognition. Existing techniques often require absolutely-paired observations as training data, and are incapable of capturing more general semantic relationships between cross-modality observations. This greatly limits their applications. In this paper, we propose a general framework for learning a latent common space from relatively-paired observations (i.e., two observations from different modalities are more-likely-paired than another two). Relative-pairing information is encoded using relative proximities of observations in the latent common space. By building a discriminative model and maximizing a distance margin, a projection function that maps observations into the latent common space is learned for each modality. Cross-modality pattern recognition can then be carried out in the latent common space. To speed up the learning procedure for large scale training data, the problem is reformulated into learning a structural model, which is efficiently solved by the cutting plane algorithm. To evaluate the performance of the proposed framework, it has been applied to feature fusion, cross-pose face recognition, text-image retrieval and attribute-image retrieval. Experimental results demonstrate that the proposed framework outperforms other state-of-the-art approaches.	[Kuang, Zhanghui; Wong, Kwan-Yee K.] Univ Hong Kong, Dept Comp Sci, Pokfulam, Hong Kong, Peoples R China	University of Hong Kong	Kuang, ZH (corresponding author), Univ Hong Kong, Dept Comp Sci, Pokfulam, Hong Kong, Peoples R China.	kuangzhh@gmail.com; kykwong@cs.hku.hk	Wong, Kenneth Kwan Yee/C-1577-2009	Wong, Kenneth Kwan Yee/0000-0001-8560-9007				Andrea F., 2007, ICCV, P1; Bach F.R., 2005, PROBABILISTIC INTERP; Blanz V, 2005, PROC CVPR IEEE, P454; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928; Chai XJ, 2007, IEEE T IMAGE PROCESS, V16, P1716, DOI 10.1109/TIP.2007.899195; Chunhua Shen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2601, DOI 10.1109/CVPR.2011.5995447; De la Torre F, 2001, PROC CVPR IEEE, P643; Ek CH, 2008, LECT NOTES COMPUT SC, V5237, P62, DOI 10.1007/978-3-540-85853-9_6; Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4; Gross R, 2004, IEEE T PATTERN ANAL, V26, P449, DOI 10.1109/TPAMI.2004.1265861; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; Joachims T, 2006, PROC 22 ACM SIGKDD I, P217, DOI DOI 10.1145/1150402.1150429; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Kan MN, 2012, LECT NOTES COMPUT SC, V7572, P808, DOI 10.1007/978-3-642-33718-5_58; Knutsson H., 1997, SCIA COMP VIS LAB, V1; Kumar N., 2009, ICCV; LAMPERT CH, 2010, ECCV, V6312, P566; Lin DH, 2006, LECT NOTES COMPUT SC, V3954, P13; Lin DH, 2005, IEEE I CONF COMP VIS, P1699; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Liu XM, 2005, PROC CVPR IEEE, P502; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Navaratnam R, 2007, IEEE I CONF COMP VIS, P1160; Parameswaran S., 2010, NIPS, P1; Parikh D., 2011, ICCV; Prince SJD, 2008, IEEE T PATTERN ANAL, V30, P970, DOI 10.1109/TPAMI.2008.48; Quadrianto N., 2011, P 28 INT C MACHINE L; Rakotomamonjy A., 2004, TECHNICAL REPORT; Rasiwasia N, 2010, ACM MM, DOI DOI 10.1145/1873951.1873987; Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2; Rupnik J., 2010, SIKDD; Russell S. J, 2002, ADV NEURAL INFORM PR, P12, DOI DOI 10.5555/2968618.2968683; Saenko K., 2010, ECCV, P1; Salakhutdinov R, 2005, NEURAL INF PROCESS S, P513; Shalev-Shwartz Singer Y., 2007, ICML; Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923; Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350; Shen C., 2009, ADV NEURAL INFORM PR, P1651; Shon A., 2006, P ADV NEURAL INFORM, P1233; STEWART GW, 1993, SIAM REV, V35, P551, DOI 10.1137/1035134; Sun TK, 2008, IEEE DATA MINING, P1043, DOI 10.1109/ICDM.2008.28; Taskar Ben, 2004, THESIS; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; Wang B., 2009, CIKM; Weinberger KQ, 2006, NIPS; Wong K. K, 2013, BMVC; Wu W., 2010, MSRTR201086; Zhang JC, 2011, PATTERN RECOGN, V44, P1162, DOI 10.1016/j.patcog.2010.12.011; Zhang W, 2011, PROC CVPR IEEE, P513, DOI 10.1109/CVPR.2011.5995324; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138; Zhou H, 2012, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2012.6247788	55	10	10	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2015	113	3			SI		176	192		10.1007/s11263-014-0783-8	http://dx.doi.org/10.1007/s11263-014-0783-8			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CK3BZ					2022-12-18	WOS:000356091900003
J	McAuley, JJ; Ramisa, A; Caetano, TS				McAuley, Julian J.; Ramisa, Arnau; Caetano, Tiberio S.			Optimization of Robust Loss Functions for Weakly-Labeled Image Taxonomies	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image labeling; Image tagging; Image taxonomies; Structured learning		The recently proposed ImageNet dataset consists of several million images, each annotated with a single object category. These annotations may be imperfect, in the sense that many images contain multiple objects belonging to the label vocabulary. In other words, we have a multi-label problem but the annotations include only a single label (which is not necessarily the most prominent). Such a setting motivates the use of a robust evaluation measure, which allows for a limited number of labels to be predicted and, so long as one of the predicted labels is correct, the overall prediction should be considered correct. This is indeed the type of evaluation measure used to assess algorithm performance in a recent competition on ImageNet data. Optimizing such types of performance measures presents several hurdles even with existing structured output learning methods. Indeed, many of the current state-of-the-art methods optimize the prediction of only a single output label, ignoring this 'structure' altogether. In this paper, we show how to directly optimize continuous surrogates of such performance measures using structured output learning techniques with latent variables. We use the output of existing binary classifiers as input features in a new learning stage which optimizes the structured loss corresponding to the robust performance measure. We present empirical evidence that this allows us to 'boost' the performance of binary classification on a variety of weakly-supervised labeling problems defined on image taxonomies.	[McAuley, Julian J.] Stanford Univ, InfoLab, Stanford, CA 94305 USA; [Ramisa, Arnau] UPC, CSIC, Inst Robot & Informat Ind, Barcelona 08028, Spain; [Caetano, Tiberio S.] NICTA, Machine Learning Grp, Alexandria, NSW 1435, Australia	Stanford University; Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i Informatica Industrial (IRII); Universitat Politecnica de Catalunya; Australian National University	McAuley, JJ (corresponding author), Stanford Univ, InfoLab, Gates Bldg 4A, Stanford, CA 94305 USA.	jmcauley@cs.stanford.edu			Australian Government; Australian Research Council through the ICT Centre of Excellence program; QUAERO project; OSEO; French State agency for innovation; MICINN under the project MIPRCV Consolider Ingenio [CSD2007-00018]	Australian Government(Australian GovernmentCGIAR); Australian Research Council through the ICT Centre of Excellence program(Australian Research Council); QUAERO project; OSEO; French State agency for innovation; MICINN under the project MIPRCV Consolider Ingenio	Part of this work was carried out while both Arnau Ramisa and Tiberio Caetano were at INRIA Grenoble, Rhone-Alpes, and while Julian McAuley was at NICTA and the ANU. NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program. The work was partially funded by the QUAERO project supported by OSEO, the French State agency for innovation and MICINN under the project MIPRCV Consolider Ingenio CSD2007-00018.	Bart E, 2008, PROC CVPR IEEE, P2166; Berg Alex, 2010, IMAGENET LARGE SCALE; Binder A, 2011, INT J COMPUT VISION, P1; Blaschko M., 2010, ADV NEURAL INFORM PR; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Bucak S. S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2801, DOI 10.1109/CVPR.2011.5995734; Cai L., 2004, C INF KNOWL MAN; Csurka G., 2004, IEEE C COMP VIS PATT; Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6; Deselaers T., 2011, IEEE C COMP VIS PATT; Dimitrovski I, 2010, LECT NOTES COMPUT SC, V6388, P152, DOI 10.1007/978-3-642-17711-8_16; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Griffin Gregory, 2007, CALTECH 256 OBJECT C; Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120; Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Huiskes M. J., 2010, INT C MULT INF RETR; Huiskes M. J., 2008, INT C MULT INF RETR; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jia D, 2011, PROC CVPR IEEE, P785, DOI 10.1109/CVPR.2011.5995516; Kawanabe M., 2011, IEEE WORKSH APPL COM; Kim BS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.1016/j.imavis.2013.10.005; Lampert C. H., 2009, LEARNING DETECT UNSE; Lavrenko V., 2003, ADV NEURAL INFORM PR; Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477; Marszalek M., 2008, EUR C COMP VIS; McAuley J., 2011, ENERGY MINIMIZATION; Mensink T., 2011, IEEE C COMP VIS PATT; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Moran S, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.1; Nowak S., 2010, CLEF NOTEBOOKS PAPER; Nowak S., 2011, WORKING NOTES CLEF; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Russakovsky O., 2010, ECCV WORKSH PARTS AT; Sanchez J., 2011, IEEE C COMP VIS PATT; Setia L., 2007, ACM SIGIR WORKSH MUL; Shi Q., 2009, ARTIFICIAL INTELLIGE; Sivic J., 2008, IEEE C COMP VIS PATT; Teo C. H., 2007, KNOWLEDGE DISCOVERY; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; Verbeek J., 2010, INT C MULT INF RETR; Yu C.-N., 2008, KNOWLEDGE DISCOVERY; Yu C.-N. J., 2009, P 26 ANN INT C MACHI, P1169, DOI [10.1145/1553374.1553523, DOI 10.1145/1553374.1553523]; Yuille, 2002, ADV NEURAL INFORM PR, V14	46	10	11	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2013	104	3			SI		343	361		10.1007/s11263-012-0561-4	http://dx.doi.org/10.1007/s11263-012-0561-4			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	190VT		Green Submitted			2022-12-18	WOS:000322371100007
J	Jordt, A; Koch, R				Jordt, Andreas; Koch, Reinhard			Direct Model-Based Tracking of 3D Object Deformations in Depth and Color Video	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deformation; Tracking; Range video	MOTION; REGISTRATION; SHAPE	The tracking of deformable objects using video data is a demanding research topic due to the inherent ambiguity problems, which can only be solved using additional assumptions about the deformation. Image feature points, commonly used to approach the deformation problem, only provide sparse information about the scene at hand. In this paper a tracking approach for deformable objects in color and depth video is introduced that does not rely on feature points or optical flow data but employs all the input image information available to find a suitable deformation for the data at hand. A versatile NURBS based deformation space is defined for arbitrary complex triangle meshes, decoupling the object surface complexity from the complexity of the deformation. An efficient optimization scheme is introduced that is able to calculate results in real-time (25 Hz). Extensive synthetic and real data tests of the algorithm and its features show the reliability of this approach.	[Jordt, Andreas; Koch, Reinhard] Univ Kiel, Multimedia Informat Proc Grp, D-24098 Kiel, Germany	University of Kiel	Jordt, A (corresponding author), Univ Kiel, Multimedia Informat Proc Grp, Hermann Rodewald Str 3, D-24098 Kiel, Germany.	jordt@mip.informatik.uni-kiel.de; rk@mip.informatik.uni-kiel.de		Koch, Reinhard/0000-0003-4398-1569	EU Interreg A4 Project "Intelligent Robots for Handling flexible Objects" (IRFO) [33-1.2-09]	EU Interreg A4 Project "Intelligent Robots for Handling flexible Objects" (IRFO)	This work was supported by the EU Interreg A4 Project "Intelligent Robots for Handling flexible Objects" (IRFO), 33-1.2-09.	Alizadeh F., 2001, MATH PROGRAMMING B, V95, P3; Anand Abhishek, 2012, BRIT MACH VIS C; Auger A, 2010, GECCO-2010 COMPANION PUBLICATION: PROCEEDINGS OF THE 12TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P1625; Bardinet E, 1998, COMPUT VIS IMAGE UND, V71, P39, DOI 10.1006/cviu.1997.0595; Bartczak B, 2009, LECT NOTES COMPUT SC, V5876, P228, DOI 10.1007/978-3-642-10520-3_21; Bascle B, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P323, DOI 10.1109/ICCV.1998.710738; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Cagniar C., 2009, 12 INT C COMP VIS WO; Cai Q, 2010, LECT NOTES COMPUT SC, V6313, P229; Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153; COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675; COSTEIRA J, 1994, CMUCSTR94220; De Aguiar E., 2007, P IEEE C COMP VIS PA, P1; de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697; Del Bue A, 2007, IMAGE VISION COMPUT, V25, P297, DOI 10.1016/j.imavis.2005.10.004; Del Bue A, 2006, INT J COMPUT VISION, V66, P193, DOI 10.1007/s11263-005-3958-5; DELINGETTE H, 1991, P SOC PHOTO-OPT INS, V1570, P21, DOI 10.1117/12.49972; Fayad J., 2009, BRIT MACH VIS C BMVC; Fayad J, 2010, LECT NOTES COMPUT SC, V6314, P297, DOI 10.1007/978-3-642-15561-1_22; Hansen N, 2006, STUD FUZZ SOFT COMP, V192, P75; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hilsmann A., 2009, VIS MOD VIS WORKSH 2; HORN BKP, 1991, CVGIP-IMAG UNDERSTAN, V53, P1, DOI 10.1016/1049-9660(91)90001-6; Jaklic A., 2000, COMPUTATIONAL IMAGIN, V20; Jordt A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.114; KOCH R, 1993, IEEE T PATTERN ANAL, V15, P556, DOI 10.1109/34.216725; McInerney T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P518, DOI 10.1109/ICCV.1993.378169; Munoz E, 2009, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2009.5459366; Netravali A., 1985, AT T BELL LAB TECHNI, V64, P2; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Ostermeier A, 1999, GECCO-99: PROCEEDINGS OF THE GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P902; Piegl L., 1997, NURBS BOOK; Pilet J, 2008, INT J COMPUT VISION, V76, P109, DOI 10.1007/s11263-006-0017-9; Rosenhahn B, 2007, MACH VISION APPL, V18, P25, DOI 10.1007/s00138-006-0046-y; Russell C, 2011, PROC CVPR IEEE; Salzmann M, 2007, IEEE I CONF COMP VIS, P1578; Schiller I., 2008, INT ARCH PHOTOGRAMM, V37, P297; Shen SH, 2010, IEEE T IMAGE PROCESS, V19, P782, DOI 10.1109/TIP.2009.2038831; Shen Shuhan, 2008, ICPR, P1; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Taylor J, 2010, PROC CVPR IEEE, P2761, DOI 10.1109/CVPR.2010.5540002; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L, 2004, ADV NEUR IN, V16, P1555; Torresani L, 2001, PROC CVPR IEEE, P493; Vedula S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P722, DOI 10.1109/ICCV.1999.790293; YAMAMOTO M, 1993, IEEE T PATTERN ANAL, V15, P82, DOI 10.1109/34.184776; Young Min Kim, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1542, DOI 10.1109/ICCVW.2009.5457430; ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149; Zhu JK, 2008, LECT NOTES COMPUT SC, V5304, P766	49	10	10	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					239	255		10.1007/s11263-012-0572-1	http://dx.doi.org/10.1007/s11263-012-0572-1			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO					2022-12-18	WOS:000315501800014
J	Pradeep, V; Lim, J				Pradeep, Vivek; Lim, Jongwoo			Egomotion Estimation Using Assorted Features	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual odometry; SLAM; Structure from motion; Tracking	POSE ESTIMATION; COMPUTATION; POINTS	We propose a novel minimal solver for recovering camera motion across two views of a calibrated stereo rig. The algorithm can handle any assorted combination of point and line features across the four images and facilitates a visual odometry pipeline that is enhanced by well-localized and reliably-tracked line features while retaining the well-known advantages of point features. The mathematical framework of our method is based on trifocal tensor geometry and a quaternion representation of rotation matrices. A simple polynomial system is developed from which camera motion parameters may be extracted more robustly in the presence of severe noise, as compared to the conventionally employed direct linear/subspace solutions. This is demonstrated with extensive experiments and comparisons against the 3-point and line-sfm algorithms.	[Lim, Jongwoo] Honda Res Inst, Mountain View, CA 94043 USA; [Pradeep, Vivek] Microsoft Corp, Appl Sci Grp, Redmond, WA 98052 USA	Honda Motor Company; Microsoft	Lim, J (corresponding author), Honda Res Inst, Mountain View, CA 94043 USA.	vpradeep@microsoft.com; jongwoo.lim@gmail.com						Ansar A, 2003, IEEE T PATTERN ANAL, V25, P578, DOI 10.1109/TPAMI.2003.1195992; BARTOLI A, 2003, ICCV; Bujnak M., 2008, 2008 IEEE C COMPUTER, P1; Chandraker M., 2009, ICCV; Christy S, 1999, COMPUT VIS IMAGE UND, V73, P137, DOI 10.1006/cviu.1998.0717; Comport AI, 2007, IEEE INT CONF ROBOT, P40, DOI 10.1109/ROBOT.2007.363762; Dornaika F, 1999, REAL-TIME IMAGING, V5, P215, DOI 10.1006/rtim.1997.0117; FISCHLER MA, 1997, IJCV, V22, P125; Haralick R., 1991, IEEE COMP SOC C COMP; HARTLEY R, 1998, ECCV, P20; Hartley RI, 1997, INT J COMPUT VISION, V22, P125, DOI 10.1023/A:1007936012022; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HEYDEN A, 1995, THESIS LUND U; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Kukelova Z, 2008, LECT NOTES COMPUT SC, V5304, P302, DOI 10.1007/978-3-540-88690-7_23; Kukelova Zuzana, 2008, BMVC; Li H., 2006, ICPR, V2; Liu Yuncai, 1988, 9th International Conference on Pattern Recognition (IEEE Cat. No.88CH2614-6), P213, DOI 10.1109/ICPR.1988.28209; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Neira J, 1999, IEEE T ROBOTIC AUTOM, V15, P76, DOI 10.1109/70.744604; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Nister D, 2004, PROC CVPR IEEE, P652; Oliensis J., 2000, IEEE COMP SOC C COMP, V2; Pollefeys M., 2007, IJCV; Pradeep V, 2010, PROC CVPR IEEE, P1514, DOI 10.1109/CVPR.2010.5539792; Rosten E, 2005, IEEE I CONF COMP VIS, P1508; Seitz S., 1999, IEEE COMP SOC C COMP, V2; Shashua A, 2000, LECT NOTES COMPUT SC, V1842, P710; Stewenius H, 2006, ISPRS J PHOTOGRAMM, V60, P284, DOI 10.1016/j.isprsjprs.2006.03.005; Torr PHS, 1997, IMAGE VISION COMPUT, V15, P591, DOI 10.1016/S0262-8856(97)00010-3; Triggs B., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P278, DOI 10.1109/ICCV.1999.791231; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561; Zhu Z., 2007, IEEE C COMP VIS PATT, P1; [No title captured]	35	10	11	1	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2012	98	2					202	216		10.1007/s11263-011-0504-5	http://dx.doi.org/10.1007/s11263-011-0504-5			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	934NH					2022-12-18	WOS:000303450500005
J	Munsell, BC; Temlyakov, A; Styner, M; Wang, S				Munsell, Brent C.; Temlyakov, Andrew; Styner, Martin; Wang, Song			Pre-organizing Shape Instances for Landmark-Based Shape Correspondence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape correspondence; Statistical shape modeling; Statistical shape analysis; Shape tree	RECOGNITION; MODELS; SEGMENTATION	The major challenge in constructing a statistical shape model for a structure is shape correspondence, which identifies a set of corresponded landmarks across a population of shape instances to accurately estimate the underlying shape variation. Both global or pairwise shape-correspondence methods have been developed to automatically identify the corresponded landmarks. For global methods, landmarks are found by optimizing a comprehensive objective function that considers the entire population of shape instances. While global methods can produce very accurate shape correspondence, they tend to be very inefficient when the population size is large. For pairwise methods, all shape instances are corresponded to a given template independently. Therefore, pairwise methods are usually very efficient. However, if the population exhibits a large amount of shape variation, pairwise methods may produce very poor shape correspondence. In this paper, we develop a new method that attempts to address the limitations of global and pairwise methods. In particular, we first construct a shape tree to globally organize the population of shape instances by identifying similar shape instance pairs. We then perform pairwise shape correspondence between such similar shape instances with high accuracy. Finally, we combine these pairwise correspondences to achieve a unified correspondence for the entire population of shape instances. We evaluate the proposed method by comparing its performance to five available shape correspondence methods, and show that the proposed method achieves the accuracy of a global method with the efficiency of a pairwise method.	[Munsell, Brent C.; Temlyakov, Andrew; Wang, Song] Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA; [Styner, Martin] Univ N Carolina, Dept Psychiat, Chapel Hill, NC 27599 USA; [Styner, Martin] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA	University of South Carolina; University of South Carolina System; University of South Carolina Columbia; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina; University of North Carolina Chapel Hill	Munsell, BC (corresponding author), Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.	munsell@cec.sc.edu; temlyaka@cec.sc.edu; martin_styner@ieee.org; songwang@cec.sc.edu	Styner, Martin/AAS-9949-2020	Styner, Martin/0000-0002-8747-5118; Wang, Song/0000-0003-4152-5295	AFOSR [FA9550-07-1-0250]; Army Research Laboratory (ARL) [W911NF-10-2-0060];  [NSF-1017199];  [NSF-0951754]; Direct For Computer & Info Scie & Enginr [1017199] Funding Source: National Science Foundation	AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Army Research Laboratory (ARL)(United States Department of DefenseUS Army Research Laboratory (ARL)); ; ; Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This work was funded, in part, by AFOSR FA9550-07-1-0250, NSF-1017199, NSF-0951754, and Army Research Laboratory (ARL) under Cooperative Agreement Number W911NF-10-2-0060. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either express or implied, of AFOSR, NSF, ARL or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein. We would like to thank T. Thodberg for providing the Matlab version of T-MDL, and A. Ericsson for providing Matlab versions of E-MDL, E-MDL+CUR and EUC. A preliminary version of this work was published in Munsell et al. (2009).	Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545; Baker S, 2004, IEEE T PATTERN ANAL, V26, P1380, DOI 10.1109/TPAMI.2004.77; Basri R., 1995, Proceedings of the Workshop on Physics-Based Modeling in Computer Vision (Cat. No.95TB8038), P135, DOI 10.1109/PBMCV.1995.514678; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Bookstein F L, 1997, Med Image Anal, V1, P225; Bookstein F. L., 1991, MORPHOMETRIC TOOLS L; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; BRUCKSTEIN AM, 1992, INT J COMPUT VISION, V7, P271, DOI 10.1007/BF00126396; Catmull Edwin, 1974, COMPUT AIDED GEOM D, P317, DOI 10.1016/B978-0-12-079050-0.50020-5; Cormen T.H., 1990, INTRO ALGORITHMS 2 V; Dalal P, 2010, LECT NOTES COMPUT SC, V6361, P349; Davies R., 2002, THESIS U MANCHESTER; Davies R, 2008, STAT MODELS SHAPE OP; Davies RH, 2002, IEEE T MED IMAGING, V21, P525, DOI 10.1109/TMI.2002.1009388; Di Ianni M., 1996, Control and Cybernetics, V25, P159; Dryden IL, 1998, STAT ANAL SHAPE; Duta N, 1998, IEEE T MED IMAGING, V17, P1049, DOI 10.1109/42.746716; Gonzalez-Jimenez D, 2007, IEEE T INF FOREN SEC, V2, P413, DOI 10.1109/TIFS.2007.903543; Gower J. C., 2004, PROCRUSTES PROBLEMS; Heimann T, 2005, LECT NOTES COMPUT SC, V3565, P566; Heimann T, 2009, MED IMAGE ANAL, V13, P543, DOI 10.1016/j.media.2009.05.004; HILL A, 1994, BMVC94 - PROCEEDINGS OF THE 5TH BRITISH MACHINE VISION CONFERENCE, VOLS 1 AND 2, P429; Karlsson J, 2006, INT C PATT RECOG, P568; LANITIS A, 1995, IMAGE VISION COMPUT, V13, P393, DOI 10.1016/0262-8856(95)99726-H; Latecki LJ, 2007, PATTERN RECOGN, V40, P3069, DOI 10.1016/j.patcog.2007.03.004; Lekadir K, 2007, IEEE T MED IMAGING, V26, P212, DOI 10.1109/TMI.2006.889726; Leventon ME, 2000, PROC CVPR IEEE, P316, DOI 10.1109/CVPR.2000.855835; Longbin Chen, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563078; Marsland S, 2008, IMAGE VISION COMPUT, V26, P333, DOI 10.1016/j.imavis.2006.12.009; Meier D, 2002, IEEE T MED IMAGING, V21, P31, DOI 10.1109/42.981232; Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37; Munsell BC, 2008, IEEE T PATTERN ANAL, V30, P2023, DOI 10.1109/TPAMI.2007.70841; Munsell BC, 2009, PROC CVPR IEEE, P840, DOI 10.1109/CVPRW.2009.5206611; Ozcan E, 1997, PATTERN RECOGN LETT, V18, P987, DOI 10.1016/S0167-8655(97)00123-2; Rueckert D., 2001, P MICCAI, V2208, P77; Saber E, 2005, PATTERN RECOGN, V38, P1560, DOI 10.1016/j.patcog.2005.03.027; Seshadri K., 2009, P 3 IEEE INT C BIOM, P1, DOI DOI 10.1109/BTAS.2009.5339057; Shenton ME, 2002, PSYCHIAT RES-NEUROIM, V115, P15, DOI 10.1016/S0925-4927(02)00025-2; Styner MA, 2003, LECT NOTES COMPUT SC, V2732, P63; Tanase M., 2005, 13th Annual ACM International Conference on Multimedia, P543, DOI 10.1145/1101149.1101272; Thodberg HH, 2003, LECT NOTES COMPUT SC, V2732, P51; van Ginneken B, 2002, IEEE T MED IMAGING, V21, P924, DOI 10.1109/TMI.2002.803121; Veltkamp R., 1999, UUCS199927; Wang S, 2004, PROC CVPR IEEE, P143; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]	52	10	11	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2012	97	2					210	228		10.1007/s11263-011-0477-4	http://dx.doi.org/10.1007/s11263-011-0477-4			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	897RV		Green Submitted			2022-12-18	WOS:000300674200005
J	Sun, ZH; Hoogs, A				Sun, Zhaohui; Hoogs, Anthony			Image Comparison by Compound Disjoint Information with Applications to Perceptual Visual Quality Assessment, Image Registration and Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Disjoint information; Image quality assessment; Image registration; Mutual information; Similarity measures; Video tracking	MUTUAL-INFORMATION; COLOR; MAXIMIZATION; MULTIPLE; VISION	In this paper, we study (normalized) disjoint information as a metric for image comparison and its applications to perceptual image quality assessment, image registration, and video tracking. Disjoint information is the joint entropy of random variables excluding the mutual information. This measure of statistical dependence and information redundancy satisfies more rigorous metric conditions than mutual information, including self-similarity, minimality, symmetry and triangle inequality. It is applicable to two or more random variables, and can be computed by vector histogramming, vector Parzen window density approximation, and upper bound approximation involving fewer variables. We show such a theoretic advantage does have implications in practice. In the domain of digital image and video, multiple visual features are extracted and (normalized) compound disjoint information is derived from a set of marginal densities of the image distributions, thus enriching the vocabulary of content representation. The proposed metric matching functions are applied to several domain applications to demonstrate their efficacy.	[Sun, Zhaohui] GE Global Res, Visualizat & Comp Vis Lab, Niskayuna, NY 12309 USA; [Hoogs, Anthony] Kitware Inc, Clifton Pk, NY 12065 USA	General Electric	Sun, ZH (corresponding author), GE Global Res, Visualizat & Comp Vis Lab, 1 Res Circle, Niskayuna, NY 12309 USA.	sunzh@research.ge.com; anthony.hoogs@kitware.com		Sun, Zhaohui/0000-0001-9011-6426				Aczel J., 1975, MEASURES INFORM THEI; [Anonymous], 2015, FINAL REPORT VIDEO Q; Bennett CH, 1998, IEEE T INFORM THEORY, V44, P1407, DOI 10.1109/18.681318; Bohm C, 2001, ACM COMPUT SURV, V33, P322, DOI 10.1145/502807.502809; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; DALY S, 1993, DIGITAL IMAGES HUMAN, P179; Fano R. M., 1961, TRANSMISSION INFORM; HAN TS, 1980, INFORM CONTROL, V46, P26, DOI 10.1016/S0019-9958(80)90478-7; HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Huang J, 1999, INT J COMPUT VISION, V35, P245, DOI 10.1023/A:1008108327226; HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073; Ibanez L., 2004, ITK SOFTWARE GUIDE I; KELLY DH, 1979, J OPT SOC AM, V69, P1340, DOI 10.1364/JOSA.69.001340; Kunt M., 1998, SIGNAL PROCESSING, V70; LI K, 2007, INT C MED IM COMP CO; Li M, 2004, IEEE T INFORM THEORY, V50, P3250, DOI 10.1109/TIT.2004.838101; LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577; LOWE DG, 1995, NEURAL COMPUT, V7, P72, DOI 10.1162/neco.1995.7.1.72; Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250; Mattes D, 2003, IEEE T MED IMAGING, V22, P120, DOI 10.1109/TMI.2003.809072; McGill WJ., 1954, PSYCHOMETRIKA, V19, P97, DOI 10.1007/BF02289159; McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7; Mundy J. L., 2004, Proceedings. 33rd Applied Imagery Pattern Recognition Workshop, P10; Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867; RAJSKI C, 1961, INFORM CONTROL, V4, P371, DOI 10.1016/S0019-9958(61)80055-7; Rohaly AM, 2000, P SOC PHOTO-OPT INS, V4067, P742, DOI 10.1117/12.386632; Rubner Y, 2001, COMPUT VIS IMAGE UND, V84, P25, DOI 10.1006/cviu.2001.0934; Santini S, 1999, IEEE T PATTERN ANAL, V21, P871, DOI 10.1109/34.790428; Schiele B, 2000, INT J COMPUT VISION, V36, P31, DOI 10.1023/A:1008120406972; Schwartzkopf WC, 2005, IEEE T MED IMAGING, V24, P1593, DOI 10.1109/TMI.2005.859207; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Sheikh H. R., 2006, LIVE IMAGE QUALITY A; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959; Studholme C, 1999, PATTERN RECOGN, V32, P71, DOI 10.1016/S0031-3203(98)00091-0; SUN Z, 2003, IEEE INT C COMP VIS, V1, P440; Sun ZH, 2006, IEEE T IMAGE PROCESS, V15, P678, DOI 10.1109/TIP.2005.863023; Sun ZH, 2005, PROC SPIE, V5747, P1274, DOI 10.1117/12.597108; Sun Zhaohui, 2006, 2006 IEEE COMP SOC C, V1, P857; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487; TEO PC, 1994, IEEE IMAGE PROC, P982, DOI 10.1109/ICIP.1994.413502; Thevenaz P, 2000, IEEE T IMAGE PROCESS, V9, P2083, DOI 10.1109/83.887976; TVERSKY A, 1977, PSYCHOL REV, V84, P327, DOI 10.1037/h0026750; Viola P, 1997, INT J COMPUT VISION, V24, P137, DOI 10.1023/A:1007958904918; Wandell B.A., 1995, FDN VISION; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wells W M 3rd, 1996, Med Image Anal, V1, P35; Winkler S, 1999, SIGNAL PROCESS, V78, P231, DOI 10.1016/S0165-1684(99)00062-6; Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7; Yianilos P.N., 1991, 9108290271 NEC RES I; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Zhang J, 2004, PROC CVPR IEEE, P848; ZHANG J, 2005, INT C INF PROC MED I, P725; ZUREK WH, 1989, NATURE, V341, P119, DOI 10.1038/341119a0	57	10	10	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2010	88	3					461	488		10.1007/s11263-010-0316-z	http://dx.doi.org/10.1007/s11263-010-0316-z			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	580EL					2022-12-18	WOS:000276429900007
J	Peng, T; Jermyn, IH; Prinet, V; Zerubia, J				Peng, Ting; Jermyn, Ian H.; Prinet, Veronique; Zerubia, Josiane			Extended Phase Field Higher-Order Active Contour Models for Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Active contour; Phase field; Shape prior; Parameter analysis; Remote sensing; Road network extraction	ROAD EXTRACTION; AERIAL IMAGES	This paper addresses the segmentation from an image of entities that have the form of a 'network', i.e. the region in the image corresponding to the entity is composed of branches joining together at junctions, e.g. road or vascular networks. We present new phase field higher-order active contour (HOAC) prior models for network regions, and apply them to the segmentation of road networks from very high resolution satellite images. This is a hard problem for two reasons. First, the images are complex, with much 'noise' in the road region due to cars, road markings, etc., while the background is very varied, containing many features that are locally similar to roads. Second, network regions are complex to model, because they may have arbitrary topology. In particular, we address a limitation of a previous model in which network branch width was constrained to be similar to maximum network branch radius of curvature, thereby providing a poor model of networks with straight narrow branches or highly curved, wide branches. We solve this problem by introducing first an additional nonlinear nonlocal HOAC term, and then an additional linear nonlocal HOAC term to improve the computational speed. Both terms allow separate control of branch width and branch curvature, and furnish better prolongation for the same width, but the linear term has several advantages: it is more efficient, and it is able to model multiple widths simultaneously. To cope with the difficulty of parameter selection for these models, we perform a stability analysis of a long bar with a given width, and hence show how to choose the parameters of the energy functions. After adding a likelihood energy, we use both models to extract the road network quasi-automatically from pieces of a QuickBird image, and compare the results to other models in the literature. The state-of-the-art results obtained demonstrate the superiority of our new models, the importance of strong prior knowledge in general, and of the new terms in particular.	[Peng, Ting; Prinet, Veronique] CASIA, LIAMA, Beijing 100190, Peoples R China; [Peng, Ting; Prinet, Veronique] CASIA, NLPR, Beijing 100190, Peoples R China; [Peng, Ting; Jermyn, Ian H.; Zerubia, Josiane] INRIA, Project Team Ariana, F-06902 Sophia Antipolis, France	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Inria	Peng, T (corresponding author), CASIA, LIAMA, 95 Zhongguancun E Rd, Beijing 100190, Peoples R China.	Ting.Peng@loria.fr; Ian.Jermyn@sophia.inria.fr; prinet@nlpr.ia.ac.cn; Josiane.Zerubia@sophia.inria.fr		Zerubia, Josiane/0000-0002-7444-0856	European Union [FP6-507752]; INRIA; Ministry of Science and Technology of China; MAE/Thales Alenia Space/LIAMA	European Union(European Commission); INRIA; Ministry of Science and Technology of China(Ministry of Science and Technology, China); MAE/Thales Alenia Space/LIAMA(Thales Group)	This work was partially supported by European Union Network of Excellence MUSCLE (FP6-507752) and by INRIA Associated Team 'SHAPES'. This work was also supported by the 863 program of the Ministry of Science and Technology of China. The work of the first author was supported by an MAE/Thales Alenia Space/LIAMA grant during her PhD.	Amo M, 2006, IEEE T IMAGE PROCESS, V15, P1192, DOI 10.1109/TIP.2005.864232; Bertozzi AL, 2007, IEEE T IMAGE PROCESS, V16, P285, DOI 10.1109/TIP.2006.887728; Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985; Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915; Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5; Dobrosotskaya JA, 2008, IEEE T IMAGE PROCESS, V17, P657, DOI 10.1109/TIP.2008.919367; Geman D, 1996, IEEE T PATTERN ANAL, V18, P1, DOI 10.1109/34.476006; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Heipke C., 1997, International Archives of Photogrammetry and Remote Sensing, V32, P151; Hu J, 2007, IEEE T GEOSCI REMOTE, V45, P4144, DOI 10.1109/TGRS.2007.906107; Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Lacoste C, 2005, IEEE T PATTERN ANAL, V27, P1568, DOI 10.1109/TPAMI.2005.206; Leventon M. E., 2000, P IEEE C COMP VIS PA; Mena JB, 2003, PATTERN RECOGN LETT, V24, P3037, DOI 10.1016/S0167-8655(03)00164-8; Merlet N, 1996, IEEE T PATTERN ANAL, V18, P426, DOI 10.1109/34.491623; PENG T, 2008, P 21 ISPRS C COMM A; PENG T, 2008, P EUR C COMP VIS MAR; PETERI R, 2003, P INT C IM PROC BARC; Riklin-Raviv T, 2007, INT J COMPUT VISION, V72, P309, DOI 10.1007/s11263-006-9042-y; ROCHERY M, 2005, P IEEE INT C COMP VI; Rochery M, 2006, INT J COMPUT VISION, V69, P27, DOI 10.1007/s11263-006-6851-y; ROUSSON M, 2007, INT J COMPUT VISION, V76, P1573; Srivastava A, 2005, IEEE T PATTERN ANAL, V27, P590, DOI 10.1109/TPAMI.2005.86; Thom R., 2018, STRUCTURAL STABILITY; WANG R, 2003, P INT SOC PHOT REM S; YU Z, 2004, P INT C IM PROC SING	27	10	10	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	88	1					111	128		10.1007/s11263-009-0304-3	http://dx.doi.org/10.1007/s11263-009-0304-3			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	571LS		Green Submitted, Green Accepted			2022-12-18	WOS:000275753900006
J	Okatani, T; Deguchi, K				Okatani, Takayuki; Deguchi, Koichiro			Easy Calibration of a Multi-projector Display System	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-projector display; Projector camera; Camera calibration; Multi-view geometry		In this paper, we present a method for the geometric calibration of a multi-projector display system. The method is such that in order to calibrate the system, the user is only required to place the projectors and capture a single image of the images projected from them onto a planar screen using a hand-held camera. The problem to be solved is divided into the image registration for stitching different projector images into a single seamless image and the image rectification for making the image have the correct rectangular shape. The proposed method is characterized by simultaneously solving both of them from only a single image, which makes the calibration procedures easy. The method assumes an uncalibrated camera and partially calibrated projectors in which only focal lengths are unknown among the internal parameters. In the paper, we first prove the uniqueness of solutions to the problem, which was unclear in the previous studies, and then present a stable numerical algorithm for actually finding the solution. We present several experimental results for synthetic data, in which we show the relation between the calibration accuracy and several factors, and also present experimental results for real data, in which we demonstrate that the proposed method can calibrate a real system with sufficient accuracy for a number of layouts of the projectors.	[Okatani, Takayuki; Deguchi, Koichiro] Tohoku Univ, Grad Sch Informat Sci, Aoba Ku, Sendai, Miyagi 9808579, Japan	Tohoku University	Okatani, T (corresponding author), Tohoku Univ, Grad Sch Informat Sci, Aoba Ku, 6-6-01 Aramaki Aza, Sendai, Miyagi 9808579, Japan.	okatani@fractal.is.tohoku.ac.jp	Okatani, Takayuki/AAE-3339-2019					Bouguet J.Y., 2015, CAMERA CALIBRATION T; Brown M, 2005, IEEE T VIS COMPUT GR, V11, P193, DOI 10.1109/TVCG.2005.27; Chen H, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P339, DOI 10.1109/VISUAL.2002.1183793; Chen YQ, 2000, IEEE VISUAL, P125, DOI 10.1109/VISUAL.2000.885685; HARTLEY R, 2000, MULTIVIEW GEOMETRY C; HEYDEN A, 1998, P 3 AS C COMP VIS HO, V2, P169; Okatani T, 2005, IEEE T PATTERN ANAL, V27, P1845, DOI 10.1109/TPAMI.2005.235; Raij A, 2004, INT C PATT RECOG, P14, DOI 10.1109/ICPR.2004.1333994; Raskar R., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P161, DOI 10.1109/VISUAL.1999.809883; Raskar R, 2003, ACM T GRAPHIC, V22, P809, DOI 10.1145/882262.882349; Raskar R, 2001, PROC CVPR IEEE, P504; Raskar R., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P109, DOI 10.1109/VR.2000.840488; REHG JM, 2002, P 7 INT C CONTR AUT, P926; Steele RM, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P429; Sukthankar R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P247, DOI 10.1109/ICCV.2001.937525; Triggs B., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P89, DOI 10.1007/BFb0055661	16	10	16	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2009	85	1					1	18		10.1007/s11263-009-0242-0	http://dx.doi.org/10.1007/s11263-009-0242-0			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	470UY					2022-12-18	WOS:000268008000001
J	Roussos, A; Maragos, P				Roussos, Anastasios; Maragos, Petros			Reversible Interpolation of Vectorial Images by an Anisotropic Diffusion-Projection PDE	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Partial differential equations (PDEs); Nonlinear anisotropic diffusion; Diffusion-projection PDE; Image interpolation; Vector-valued images; Reversibility; Sampling; Anti-aliasing filter	REGULARIZATION	In this paper, a nonlinear model for the interpolation of vector-valued images is proposed. This model is based on an anisotropic diffusion PDE and performs an interpolation that is reversible. The interpolation solution is restricted to the subspace of functions that can recover the discrete input image, after an appropriate smoothing and sampling. The proposed nonlinear diffusion flow lies on this subspace while its strength and anisotropy adapt to the local variations and geometry of image structures. The derived method effectively reconstructs the real image structures and yields a satisfactory interpolation result. Compared to classic and other existing PDE-based interpolation methods, our proposed method seems to increase the accuracy of the result and to reduce the undesirable artifacts, such as blurring, ringing, block effects and edge distortion. We present extensive experimental results that demonstrate the potential of the method as applied to graylevel and color images.	[Roussos, Anastasios; Maragos, Petros] Natl Tech Univ Athens, Sch ECE, GR-15773 Athens, Greece	National Technical University of Athens	Roussos, A (corresponding author), Natl Tech Univ Athens, Sch ECE, GR-15773 Athens, Greece.	troussos@cs.ntua.gr; maragos@cs.ntua.gr		Roussos, Anastasios/0000-0001-6015-3357	Future and Emerging Technologies (FET); Sixth Framework Programme for Research of the European Commission [021324]	Future and Emerging Technologies (FET)(European Commission); Sixth Framework Programme for Research of the European Commission(European Commission)	The authors acknowledge the financial support of the Future and Emerging Technologies (FET) programme 'ASPI' within the Sixth Framework Programme for Research of the European Commission, under FET-Open contract No. 021324.	Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684; Belahmidi A, 2004, IEEE IMAGE PROC, P649; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Caselles V, 1998, IEEE T IMAGE PROCESS, V7, P376, DOI 10.1109/83.661188; Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844; Dudgeon D. E., 1984, MULTIDIMENSIONAL DIG; Guichard F., 1998, P EUR SIGN PROC C, V3; Malgouyres F, 2001, SIAM J NUMER ANAL, V39, P1, DOI 10.1137/S0036142999362286; Meijering E, 2002, P IEEE, V90, P319, DOI 10.1109/5.993400; NAYLOR AW, 1982, LINEAR OPERATOR THEO; OPPENHEIM AV, 1984, SIGNALS SYSTEMS; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Roussos A, 2007, LECT NOTES COMPUT SC, V4485, P104; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tschumperle D, 2005, IEEE T PATTERN ANAL, V27, P506, DOI 10.1109/TPAMI.2005.87; TSCHUMPERLE D, 2002, THESIS U NICE SOPHIA; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Weickert J, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P315, DOI 10.1007/3-540-31272-2_19; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1	19	10	15	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2009	84	2					130	145		10.1007/s11263-008-0132-x	http://dx.doi.org/10.1007/s11263-008-0132-x			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	451NO					2022-12-18	WOS:000266477100002
J	Kim, W; Park, J; Lee, K				Kim, Wonsik; Park, Joonyoung; Lee, Kyoung Mu			Stereo Matching Using Population-Based MCMC	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo; Population-based MCMC; Energy minimization	ENERGY MINIMIZATION; MONTE-CARLO	In this paper, we propose a new stereo matching method using the population-based Markov Chain Monte Carlo (Pop-MCMC), which belongs to the sampling-based methods. Since the previous MCMC methods produce only one sample at a time, only local moves are available. In contrast, the proposed Pop-MCMC uses multiple chains in parallel and produces multiple samples at a time. It thereby enables global moves by exchanging information between samples, which in turn, leads to faster mixing rate. In the view of optimization, it means that we can reach a lower energy state rapidly. In order to apply Pop-MCMC to the stereo matching problem, we design two effective 2-D mutation and crossover moves among multiple chains to explore a high dimensional state space efficiently. The experimental results on real stereo images demonstrate that the proposed algorithm gives much faster convergence rate than conventional sampling-based methods including SA (Simulated Annealing) and SWC (Swendsen-Wang Cuts). And it also gives consistently lower energy solutions than BP (Belief Propagation) in our experiments. In addition, we also analyze the effect of each move in Pop-MCMC and examine the effect of parameters such as temperature and the number of the chains.	[Kim, Wonsik; Lee, Kyoung Mu] Seoul Natl Univ, ARSI, Dept EECS, Seoul 151742, South Korea; [Kim, Wonsik; Park, Joonyoung] LG Elect Inc, DM Res Lab, Seoul 137724, South Korea	Seoul National University (SNU); LG Electronics	Lee, K (corresponding author), Seoul Natl Univ, ARSI, Dept EECS, Seoul 151742, South Korea.	ultra16@snu.ac.kr; kzoome@lge.com; kyoungmu@snu.ac.kr	Kim, Wonsik/ABR-7682-2022; Lee, Kyoung Mu/AAC-4063-2020	Lee, Kyoung Mu/0000-0001-7210-1036	Ministry of Information and Communication; Defense Acquisition Program Administration; Agency for Defense Development	Ministry of Information and Communication; Defense Acquisition Program Administration; Agency for Defense Development(Agency of Defense Development (ADD), Republic of Korea)	This work was supported in part by the ITRC program by Ministry of Information and Communication and in part by Defense Acquisition Program Administration and Agency for Defense Development, through the Image Information Research Center, Korea.	Barbu A, 2005, IEEE T PATTERN ANAL, V27, P1239, DOI 10.1109/TPAMI.2005.161; Barbu A, 2004, PROC CVPR IEEE, P731; Birchfield S., 1998, IEEE T PATTERN ANAL; Bleyer M, 2005, PROC SPIE, V5665, P288, DOI 10.1117/12.586502; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; GEYER CJ, 1991, COMPUTING SCIENCE AND STATISTICS, P156; Hong L, 2004, PROC CVPR IEEE, P74; Jasra A, 2007, STAT COMPUT, V17, P263, DOI 10.1007/s11222-007-9028-9; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Klaus A, 2006, INT C PATT RECOG, P15; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668; KOLMOGOROV V, 2002, P EUR C COMP VIS, P82; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Komodakis N., 2007, P INT C COMP VIS; KOMODAKIS N, 2007, C COMP VIS PATT REC; Liang FM, 2000, STAT SINICA, V10, P317; Liang FM, 2001, J AM STAT ASSOC, V96, P653, DOI 10.1198/016214501753168325; OHTA Y, 1985, IEEE T PATTERN ANAL, V2, P449; PARK J, 2007, P AS C COMP VIS; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; SPEARS WM, 1992, FDN GENETIC ALGORITH, V2; Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509; SWENDSEN RH, 1987, PHYS REV LETT, V58, P86, DOI 10.1103/PhysRevLett.58.86; Szeliski R., 2006, P EUR C COMP VIS; Tao H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P532, DOI 10.1109/ICCV.2001.937562; Veksler O., 2005, C COMP VIS PATT REC; Yedidia JS, 2000, ADV NEURAL INFORM PR, V13, P689	31	10	10	1	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2009	83	2					195	209		10.1007/s11263-008-0189-6	http://dx.doi.org/10.1007/s11263-008-0189-6			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	423UE		Green Submitted			2022-12-18	WOS:000264520400006
J	Mio, W; Bowers, JC; Liu, XW				Mio, Washington; Bowers, John C.; Liu, Xiuwen			Shape of Elastic Strings in Euclidean Space	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape analysis; Shape space; Shape geodesics; Elastic shapes; Shape manifold	RECOGNITION; METRICS; CURVES	We construct a 1-parameter family of geodesic shape metrics on a space of closed parametric curves in Euclidean space of any dimension. The curves are modeled on homogeneous elastic strings whose elasticity properties are described in terms of their tension and rigidity coefficients. As we change the elasticity properties, we obtain the various elastic models. The metrics are invariant under reparametrizations of the curves and induce metrics on shape space. Analysis of the geometry of the space of elastic strings and path spaces of elastic curves enables us to develop a computational model and algorithms for the estimation of geodesics and geodesic distances based on energy minimization. We also investigate a curve registration procedure that is employed in the estimation of shape distances and can be used as a general method for matching the geometric features of a family of curves. Several examples of geodesics are given and experiments are carried out to demonstrate the discriminative quality of the elastic metrics.	[Mio, Washington] Florida State Univ, Dept Math, Tallahassee, FL 32306 USA; [Bowers, John C.; Liu, Xiuwen] Florida State Univ, Dept Comp Sci, Tallahassee, FL 32306 USA	State University System of Florida; Florida State University; State University System of Florida; Florida State University	Mio, W (corresponding author), Florida State Univ, Dept Math, Tallahassee, FL 32306 USA.	mio@math.fsu.edu			National Science Foundation [CCF-0514743, DMS-0713012]	National Science Foundation(National Science Foundation (NSF))	This work was supported in part by the National Science Foundation, grants CCF-0514743 and DMS-0713012. In our experiments, we used data from the LEMS shape database compiled by B. Kimia, the leaf database from a project at Linkoping University and the Swedish Museum of Natural History, segmented leaf contours made publicly available by Ling and Jacobs, knot data from The KnotPlot Site developed by R. Scharein, and shape models provided courtesy of MPII by the AIM@ Shape Shape Repository.	Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; COHEN I, 1992, LECT NOTES COMPUTER, V588; do Carmo M., 1994, RIEMANNIAN GEOMETRY; GEIGER D, 1995, IEEE T PATTERN ANAL, V17, P294, DOI 10.1109/34.368194; Grenander U., 1993, GEN PATTERN THEORY; Joshi S., 2007, IEEE C COMP VIS PATT; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333; KLASSEN E, 2006, EUR C COMP VIS ECCV; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; MICHOR P, 2007, ARXIV07064299V1; Michor PW, 2007, APPL COMPUT HARMON A, V23, P74, DOI 10.1016/j.acha.2006.07.004; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Mio W, 2004, Q APPL MATH, V62, P359, DOI 10.1090/qam/2054604; Mio W, 2007, IEEE 11 INT C COMP V, P1, DOI DOI 10.1109/ICCV.2007.4409164; Mio W, 2007, INT J COMPUT VISION, V73, P307, DOI [10.1007/s11263-006-9968-0, 10.1007/s11263-006-996S-0]; MUMFORD D, 2002, P INT C MATH BEIJ CH; Palais R. S., 1963, TOPOLOGY, V2, P299; Schmidt FR, 2006, LECT NOTES COMPUT SC, V4174, P142; Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951; Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924; Shah J., 2006, WORKSH MATH FDN COMP; Soderkvist O, 2001, THESIS LINKOPING U; Sundaramoorthi G, 2007, INT J COMPUT VISION, V73, P345, DOI 10.1007/s11263-006-0635-2; Tagare HD, 2002, J MATH IMAGING VIS, V16, P57, DOI 10.1023/A:1013938519103; Tagare HD, 1999, IEEE T MED IMAGING, V18, P570, DOI 10.1109/42.790457; Tu ZW, 2004, LECT NOTES COMPUT SC, V3023, P195; Younes L, 1999, IMAGE VISION COMPUT, V17, P381, DOI 10.1016/S0262-8856(98)00125-5; Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685; Zheng XQ, 2005, LECT NOTES COMPUT SC, V3757, P473, DOI 10.1007/11585978_31	30	10	10	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2009	82	1					96	112		10.1007/s11263-008-0190-0	http://dx.doi.org/10.1007/s11263-008-0190-0			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	402AH					2022-12-18	WOS:000262986100006
J	Wohler, C; d'Angelo, P				Woehler, Christian; d'Angelo, Pablo			Stereo Image Analysis of Non-Lambertian Surfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo vision; Specular reflectance; Shape from shading; Shape from polarisation; Photometric stereo	RECONSTRUCTION; LIGHT; SHAPE	Stereo image analysis is based on establishing correspondences between a pair of images by determining similarity measures for potentially corresponding image parts. Such similarity criteria are only strictly valid for surfaces with Lambertian (diffuse) reflectance characteristics. Specular reflections are viewpoint dependent and may thus cause large intensity differences at corresponding image points. In the presence of specular reflections, traditional stereo approaches are often unable to establish correspondences at all, or the inferred disparity values tend to be inaccurate, or the established correspondences do not belong to the same physical surface point. The stereo image analysis framework for non-Lambertian surfaces presented in this contribution combines geometric cues with photometric and polarimetric information into an iterative scheme that allows to establish stereo correspondences in accordance with the specular reflectance behaviour and at the same time to determine the surface gradient field based on the known photometric and polarimetric reflectance properties. The described approach yields a dense 3D reconstruction of the surface which is consistent with all observed geometric and photopolarimetric data. Initially, a sparse 3D point cloud of the surface is computed by traditional blockmatching stereo. Subsequently, a dense 3D profile of the surface is determined in the coordinate system of camera 1 based on the shape from photopolarimetric reflectance and depth technique. A synthetic image of the surface is rendered in the coordinate system of camera 2 using the illumination direction and reflectance properties of the surface material. Point correspondences between the rendered image and the observed image of camera 2 are established with the blockmatching technique. This procedure yields an increased number of 3D points of higher accuracy, compared to the initial 3D point cloud. The improved 3D point cloud is used to compute a refined dense 3D surface profile. These steps are iterated until convergence of the 3D reconstruction. An experimental evaluation of our method is provided for areas of several square centimetres of forged and cast iron objects with rough surfaces displaying both diffuse and significant specular reflectance components, where traditional stereo image analysis largely fails. A comparison to independently measured ground truth data reveals that the root-mean-square error of the 3D reconstruction results is typically of the order 30-100 mu m at a lateral pixel resolution of 86 mu m. For two example surfaces, the number of stereo correspondences established by the specular stereo algorithm is several orders of magnitude higher than the initial number of 3D points. For one example surface, the number of stereo correspondences decreases by a factor of about two, but the 3D point cloud obtained with the specular stereo method is less noisy, contains a negligible number of outliers, and shows significantly more surface detail than the initial 3D point cloud. For poorly known reflectance parameters we observe a graceful degradation of the accuracy of 3D reconstruction.	[Woehler, Christian] Daimler Grp Res, Environm Percept, D-89013 Ulm, Germany; [d'Angelo, Pablo] German Aerosp Ctr DLR Oberpfaffenhofen, D-82234 Wessling, Germany	Daimler AG; Helmholtz Association; German Aerospace Centre (DLR)	Wohler, C (corresponding author), Daimler Grp Res, Environm Percept, POB 2360, D-89013 Ulm, Germany.	christian.woehler@daimler.com; pablo.angelo@dlr.de						Atkinson GA, 2005, IEEE I CONF COMP VIS, P309; Batlle J, 1998, PATTERN RECOGN, V31, P963, DOI 10.1016/S0031-3203(97)00074-5; Bhat DN, 1998, INT J COMPUT VISION, V26, P91, DOI 10.1023/A:1007940725322; Bischof H, 2004, PROC 9 COMPUTER VISI, P21; Bouguet J.-Y., 2007, CAMERA CALIBRATION T; Bruhn A, 2005, IEEE I CONF COMP VIS, P749; Cox IJ, 1996, COMPUT VIS IMAGE UND, V63, P542, DOI 10.1006/cviu.1996.0040; CRYER JE, 1995, PATTERN RECOGN, V28, P1033, DOI 10.1016/0031-3203(94)00183-M; d'Angelo P, 2005, LECT NOTES COMPUT SC, V3540, P689; d'Angelo P, 2008, ISPRS J PHOTOGRAMM, V63, P297, DOI 10.1016/j.isprsjprs.2007.09.005; DANGELO P, 2005, ISPRS WORKSH BENCHM; DANGELO P, 2006, ISPRS C PHOT COMP VI; Davis J, 2005, IEEE T PATTERN ANAL, V27, P296, DOI 10.1109/TPAMI.2005.37; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FRANKE U, 2000, P IEEE C INT VEH DET; HIRSCHMULLER H, 2001, P INT C COMP VIS PAT; HIRSCHMULLER H., 2006, P IEEE C COMP VIS PA, V2, P2386, DOI DOI 10.1109/CVPR.2006.294; Horn B., 1986, ROBOT VISION, P1; HORN BKP, 1989, 1105A MIT; Jin HL, 2003, PROC CVPR IEEE, P171; KLETTE R, 1999, HDB COMPUTER VISION, V2, P532; Kruger L, 2004, PROC SPIE, V5457, P126, DOI 10.1117/12.545396; Lim J, 2005, IEEE I CONF COMP VIS, P1635; Lohse V, 2006, PLANET SPACE SCI, V54, P661, DOI 10.1016/j.pss.2006.03.002; LOWITZSCH S, 2005, P 10 INT FALL WORKSH; Miyazaki D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P982; Nayar S. K., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P583, DOI 10.1109/CVPR.1993.341071; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; Rahmann S, 1999, P SOC PHOTO-OPT INS, V3826, P22, DOI 10.1117/12.364333; Roy S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P492, DOI 10.1109/ICCV.1998.710763; Samaras D, 2000, PROC CVPR IEEE, P480, DOI 10.1109/CVPR.2000.855858; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Shi J, 1994, P IEEE C COMP VIS PA; SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103; Wang L, 2007, IEEE T PATTERN ANAL, V29, P1616, DOI [10.1109/TPAMI.2007.1171, 10.1109/TPAM1.2007.1171]; WOLFF LB, 1994, J OPT SOC AM A, V11, P3069, DOI 10.1364/JOSAA.11.003069; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zickler T, 2002, LECT NOTES COMPUT SC, V2352, P869	38	10	11	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2009	81	2					172	190		10.1007/s11263-008-0157-1	http://dx.doi.org/10.1007/s11263-008-0157-1			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	393VI					2022-12-18	WOS:000262401600006
J	Fink, M; Ullman, S				Fink, Michael; Ullman, Shimon			From Aardvark to Zorro: A benchmark for mammal image classification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						database; dataset; benchmark; mammals; svm; multiclass; annotation; natural images; animals; machine learning; object recognition		Current object recognition systems aim at recognizing numerous object classes under limited supervision conditions. This paper provides a benchmark for evaluating progress on this fundamental task. Several methods have recently proposed to utilize the commonalities between object classes in order to improve generalization accuracy. Such methods can be termed interclass transfer techniques. However, it is currently difficult to asses which of the proposed methods maximally utilizes the shared structure of related classes. In order to facilitate the development, as well as the assessment of methods for dealing with multiple related classes, a new dataset including images of several hundred mammal classes, is provided, together with preliminary results of its use. The images in this dataset are organized into five levels of variability, and their labels include information on the objects' identity, location and pose. From this dataset, a classification benchmark has been derived, requiring fine distinctions between 72 mammal classes. It is then demonstrated that a recognition method which is highly successful on the Caltech101, attains limited accuracy on the current benchmark (36.5%). Since this method does not utilize the shared structure between classes, the question remains as to whether interclass transfer methods can increase the accuracy to the level of human performance (90%). We suggest that a labeled benchmark of the type provided, containing a large number of related classes is crucial for the development and evaluation of classification methods which make efficient use of interclass transfer.	[Fink, Michael] Hebrew Univ Jerusalem, Interdisciplinary Ctr Neural Computat, IL-91904 Jerusalem, Israel; [Ullman, Shimon] Weizmann Inst Sci, Fac Math & Comp Sci, IL-76100 Rehovot, Israel	Hebrew University of Jerusalem; Weizmann Institute of Science	Fink, M (corresponding author), Hebrew Univ Jerusalem, Interdisciplinary Ctr Neural Computat, IL-91904 Jerusalem, Israel.	fink@huji.ac.il						BELONGIE S, 2001, ICCV; Ben-David S., 2003, COLT; Changizi MA, 2005, P ROY SOC B-BIOL SCI, V272, P267, DOI 10.1098/rspb.2004.2942; Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; FEIFEI L, 2004, CVPR WORKSH GEN BAS; Fink M., 2004, P NIPS 04 17 INT C N, P8; FINK M, 2006, ICML; FINK M, 2007, COGNITIVE SCI; Grauman K., 2005, CVPR; Grauman K., 2005, ICCV; KREMPP S, 2002, SEQUENTIAL LEARNING; LAZEBNIK S, 2003, ICCV; LEVI K, 2004, LCVPR04 WORKSH LEARN; Li FF, 2002, P NATL ACAD SCI USA, V99, P9596, DOI 10.1073/pnas.092277599; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Miller E.G., 2000, CVPR; Peters J, 2017, ADAPT COMPUT MACH LE; Ponce J., 2006, CATEGORY LEVEL OBJEC; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Schiele B, 2000, INT J COMPUT VISION, V36, P31, DOI 10.1023/A:1008120406972; SCHNEIDERMAN H, 2000, CVPR; Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0; Thrun S., 1997, LEARNING LEARN; TORRALBA A, 2004, CVPR; Torresani L., 2010, CVPR; Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4	28	10	10	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2008	77	1-3					143	156		10.1007/s11263-007-0066-8	http://dx.doi.org/10.1007/s11263-007-0066-8			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	267RE					2022-12-18	WOS:000253526100009
J	Burgeth, B; Weickert, J				Burgeth, B; Weickert, J			An explanation for the logarithmic connection between linear and morphological system theory	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	4th International Conference on Scale Space Methods in Computer Vision	JUN 10-12, 2003	ISLE SKYE, SCOTLAND	British Machine Vis Assoc, Kings Coll London, IT Univ Copenhagen		linear system theory; morphology; convex analysis; max-plus algebra; min-plus algebra; slope transform; Cramer transform	SCALE-SPACES; MULTISPECTRAL IMAGES; EQUATIONS; VECTOR	Dorst/van den Boomgaard and Maragos introduced the slope transform as the morphological equivalent of the Fourier transform. Generalising the conjugacy operation from convex analysis it formed the basis of a morphological system theory that bears an almost logarithmic relation to linear system theory; a connection that has not been fully understood so far. Our article provides an explanation by disclosing that morphology in essence is linear system theory in specific algebras. While linear system theory uses the standard plus-prod algebra, morphological system theory is based on the max-plus algebra and the min-plus algebra. We identify the nonlinear operations of erosion and dilation as linear convolutions in the latter algebras. The logarithmic Laplace transform makes a natural appearance as it corresponds to the conjugacy operation in the max-plus algebra. Its conjugate is given by the so-called Cramer transform. Originating from stochastics, the Cramer transform maps Gaussians to quadratic functions and relates standard convolution to erosion. This fundamental transform relies on the logarithm and constitutes the direct link between linear and morphological system theory. Many numerical examples are presented that illustrate the convexifying and smoothing properties of the Cramer transform.	Univ Saarland, Fac Math & Comp Sci, Math Image Anal Grp, D-66041 Saarbrucken, Germany	Saarland University	Burgeth, B (corresponding author), Univ Saarland, Fac Math & Comp Sci, Math Image Anal Grp, Bldg 27, D-66041 Saarbrucken, Germany.	burgeth@mia.uni-saarland.de; weickert@mia.uni-saarland.de						Akian M., 1994, 11 INT C AN OPT SYST, P302; ALVAREZ L, 1993, ARCH RATION MECH AN, V123, P199, DOI 10.1007/BF00375127; [Anonymous], 1992, SYNCHRONIZATION LINE; AZENCOTT R, 1980, LECT NOTES MATH, V774; BAUER H, 1990, MASS INTEGRATIONSTHE; Borwein J. M., 1999, CONVEX ANAL NONLINEA; Burgeth B, 2003, LECT NOTES COMPUT SC, V2695, P325; Castleman K.R., 1996, DIGITAL IMAGE PROCES; DEUSCHEL JD, 1989, LARDE DEVIATIONS; DORST L, 1994, SIGNAL PROCESS, V38, P79, DOI 10.1016/0165-1684(94)90058-2; Ellis R, 1985, GRUNDLEHREN MATH WIS, V271; Florack L, 2001, J MATH IMAGING VIS, V15, P39, DOI 10.1023/A:1011221614364; Florack L, 1999, INT J COMPUT VISION, V31, P247, DOI 10.1023/A:1008026217765; Florack L, 2001, INT J COMPUT VISION, V42, P39, DOI 10.1023/A:1011185417206; Gonzalez R.C., 2006, DIGITAL IMAGE PROCES; Hamming R. W., 1998, DIGITAL FILTERS; HEIJMANS HJA, 2001, J VISUAL COMMUNICATI, V13, P269; Heijmans HJAM, 2001, LECT NOTES COMPUT SC, V2106, P215; HEIMMANS HJAM, 1994, MORPHOLOGICAL IMAGE; Hiriart-Urruty J. B., 2001, FUNDAMENTALS CONVEX; IIJIMA T, 1959, TECH GROUP AUT AUT C; Iijima T., 1973, PATTERN RECOGNITION; Jackway PT, 1996, IEEE T PATTERN ANAL, V18, P38, DOI 10.1109/34.476009; Jackway PT, 1996, IEEE T IMAGE PROCESS, V5, P913, DOI 10.1109/83.503908; Maragos P, 1996, IEEE T IMAGE PROCESS, V5, P922, DOI 10.1109/83.503909; MARAGOS P, 1990, P IEEE, V78, P690, DOI 10.1109/5.54808; MARAGOS P, 1994, SIGNAL PROCESS, V38, P57, DOI 10.1016/0165-1684(94)90057-4; Oberhettinger F., 1973, TABLES LAPLACE TRANS; Oppenheim A.V., 1999, DISCRETE TIME SIGNAL; Rockafellar R. T., 1970, CONVEX ANAL; Schavemaker JGM, 2000, PATTERN RECOGN, V33, P997, DOI 10.1016/S0031-3203(99)00160-0; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; Serra J, 1988, IMAGE ANAL MATH MORP; Soille P., 1999, MORPHOLOGICAL IMAGE, DOI 10.1007/978-3-662-03939-7; SPORRING J, 1997, GUASSIAN SCALE SPACE, V8; van den Boomgaard R, 1992, NIEUW ARCH WISKD, V10, P219; Weickert J, 1999, J MATH IMAGING VIS, V10, P237, DOI 10.1023/A:1008344623873; Welk M, 2003, LECT NOTES COMPUT SC, V2695, P770; Witkin A. P, 1983, P 8 INT JOINT C ART, V2, P945; [No title captured]	40	10	10	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2005	64	2-3					157	169		10.1007/s11263-005-1841-z	http://dx.doi.org/10.1007/s11263-005-1841-z			13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	961YC		Green Submitted			2022-12-18	WOS:000231696700006
J	Lazebnik, S; Ponce, J				Lazebnik, S; Ponce, J			The local projective shape of smooth surfaces and their outlines	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						projective differential geometry; oriented projective geometry; differential invariants; local shape; frontier points	SOLID SHAPE; VISION; CURVES; MOTION	This article examines projectively-invariant local geometric properties of smooth curves and surfaces. Oriented projective differential geometry is proposed as a general framework for establishing such invariants and characterizing the local projective shape of surfaces and their outlines. It is applied to two problems: (1) the projective generalization of Koenderink's famous characterization of convexities, concavities, and inflections of the apparent contours of solids bounded by smooth surfaces, and (2) the image-based construction of rim meshes, which provide a combinatorial description of the arrangement induced on the surface of an object by the contour generators associated with multiple cameras observing it.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Lazebnik, S (corresponding author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.	slazebni@uiuc.edu; jponce@uiuc.edu						Arbogast E., 1991, J PATTERN RECOGNITIO, V5; BAUMGART B, 1974, AIM249 STANF U DEP C; Belhumeur PN, 1997, PROC CVPR IEEE, P1060, DOI 10.1109/CVPR.1997.609461; Berger Marcel, 1987, GEOMETRY 1; BLASCHKE W, 1967, DIFFERENTIAL GEOMETR; Bol G, 1950, PROJEKTIVE DIFFERENT; BOYER E, 1996, LECT NOTES COMPUTER, V1065, P109; BRADY M, 1985, COMPUT VISION GRAPH, V32, P1, DOI 10.1016/0734-189X(85)90001-5; Calabi E, 1998, INT J COMPUT VISION, V26, P107, DOI 10.1023/A:1007992709392; Chum O., 2003, P BRIT MACH VIS C, P73; CIPOLLA R, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P269, DOI 10.1109/ICCV.1995.466775; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; CROSS G, 2000, NATO ADV RES WORKSH, P25; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; Durand F, 2002, ACM T GRAPHIC, V21, P176, DOI 10.1145/508357.508362; FAUGERAS O, 1993, INT J COMPUT VISION, V10, P125, DOI 10.1007/BF01420734; Faugeras O, 1994, LECT NOTES COMPUTER, V825, P11; Gansner ER, 2000, SOFTWARE PRACT EXPER, V30, P1203, DOI 10.1002/1097-024X(200009)30:11<1203::AID-SPE338>3.0.CO;2-N; Hartley RI, 1998, INT J COMPUT VISION, V26, P41, DOI 10.1023/A:1007984508483; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HELMHOLTZ HV, 1962, PHYSL OPTICS; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1976, BIOL CYBERN, V21, P29, DOI 10.1007/BF00326670; KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321; KOENDERINK JJ, 1979, BIOL CYBERN, V32, P211, DOI 10.1007/BF00337644; Lane E P, 1932, PROJECTIVE DIFFERENT; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; LAVEAU S, 1996, P EUR C COMP VIS ECC, P147; Lazebnik S, 2001, PROC CVPR IEEE, P156; LAZEBNIK S, 2003, IN PRESS P INT C COM; LAZEBNIK S, 2002, CVRTR200201 U ILL UR; Levillain, 1992, TH ORIE M THODE D TE; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; MOONS T, 1995, INT J COMPUT VISION, V14, P25, DOI 10.1007/BF01421487; MUNDY J, 1994, LECT NTOES COMPUTER, V825; Mundy J., 1992, GEOMETRIC INVARIANCE; PORRILL J, 1991, IMAGE VISION COMPUT, V9, P45, DOI 10.1016/0262-8856(91)90048-T; RIEGER JH, 1986, OPT LETT, V11, P123, DOI 10.1364/OL.11.000123; SALDEN A, 1993, GEOMETRIC METHODS CO, V2, P60; Stolfi J., 1991, ORIENTED PROJECTIVE; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; Weiss I., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P291, DOI 10.1109/CVPR.1988.196251; WERNER T, 1998, PATTERN RECOGN, P245; WERNER T, 2001, ICCV, P548; WERNER T, 2001, P BRIT MACH VIS C, P441	46	10	10	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2005	63	1					65	83		10.1007/s11263-005-4947-4	http://dx.doi.org/10.1007/s11263-005-4947-4			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	907WS					2022-12-18	WOS:000227749400004
J	Condell, J; Scotney, B; Morrow, P				Condell, J; Scotney, B; Morrow, P			Adaptive grid refinement procedures for efficient optical flow computation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						adaptive grids; Delaunay algorithm; inverse finite elements; motion estimation; optical flow; triangular meshes	FINITE-ELEMENT-METHOD; MOTION ESTIMATION; SEGMENTATION; FRAMEWORK; ALGORITHM; FIELDS; MODEL	Two approaches are described that improve the efficiency of optical flow computation without incurring loss of accuracy. The first approach segments images into regions of moving objects. The method is based on a previously defined Galerkin finite element method on a triangular mesh combined with a multiresolution segmentation approach for object flow computation. Images are automatically segmented into subdomains of moving objects by an algorithm that employs a hierarchy of mesh coarseness for the flow computation, and these subdomains are reconstructed over a finer mesh on which to recompute flow more accurately. The second approach uses an adaptive mesh in which the resolution increases where motion is found to occur. Optical flow is computed over a reasonably coarse mesh, and this is used to construct an optimal adaptive mesh in a way that is different from the gradient methods reported in the literature. The finite element mesh facilitates a reduction in computational effort by enabling processing to focus on particular objects of interest in a scene (i.e. those areas where motion is detected). The proposed methods were tested on real and synthetic image sequences, and promising results are reported.	Univ Ulster, Magee Coll, Fac Engn, Sch Comp & Intelligent Syst, Londonderry BT48 7JL, North Ireland; Univ Ulster, Fac Engn, Sch Comp & Informat Sci, Coleraine BT52 1SA, Londonderry, North Ireland	Ulster University; Ulster University	Condell, J (corresponding author), Univ Ulster, Magee Coll, Fac Engn, Sch Comp & Intelligent Syst, Northland Rd, Londonderry BT48 7JL, North Ireland.	j.condell@ulster.ac.uk; bw.scotney@ulster.ac.uk; pj.morrow@ulster.ac.uk						AISBETT J, 1989, IEEE T PATTERN ANAL, V11, P512, DOI 10.1109/34.24783; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; AXELSSON O, 2001, SIAM CLASSICS APPL M, V35; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BARTOLINI F, 1997, IEEE P 13 INT C DIG, V1, P503; BATTITI R, 1991, INT J COMPUT VISION, V6, P133, DOI 10.1007/BF00128153; Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141; Benayoun S, 1998, INT J COMPUT VISION, V26, P25, DOI 10.1023/A:1007932523504; Bergen J. R., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P27, DOI 10.1109/ICCV.1990.139486; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; Celasun I, 2001, SIGNAL PROCESS-IMAGE, V16, P949, DOI 10.1016/S0923-5965(00)00055-2; Choi JG, 1996, SIGNAL PROCESS, V54, P109, DOI 10.1016/S0165-1684(96)00100-4; Cohen I, 1999, INT J COMPUT VISION, V33, P29, DOI 10.1023/A:1008161130332; CONDELL JV, 2001, LNCS, V2124, P333; CONDELL JV, 2001, P IR MACH VIS IM PRO, P53; Devlaminck V, 1995, P SOC PHOTO-OPT INS, V2588, P312, DOI 10.1117/12.222682; ENKELMANN W, 1988, COMPUT VISION GRAPH, V43, P150, DOI 10.1016/0734-189X(88)90059-X; FENNEMA CL, 1979, COMPUT VISION GRAPH, V9, P301, DOI 10.1016/0146-664X(79)90097-2; Fleet D. J., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P379, DOI 10.1109/CVPR.1989.37875; GARCIA MA, 1999, P IRISH MACH VIS IM, P241; GIACHETTI A, 1996, P 4 EUR C COMP VIS E, P151; GOH WB, 1994, CVGIP-IMAG UNDERSTAN, V59, P307, DOI 10.1006/ciun.1994.1021; GRAHAM JV, 2000, P 4 IR MACH VIS IM P, P59; GRAHAM JV, 2000, P 3 IMA C IM DIG IM; Hata N, 1999, LECT NOTES COMPUT SC, V1679, P928; HEEGER DJ, 1987, J OPT SOC AM A, V4, P1455, DOI 10.1364/JOSAA.4.001455; HORN BKP, ARTIFICIAL INTELLIGE, V17, P185; HWANG SH, 1993, PATTERN RECOGN, V26, P939, DOI 10.1016/0031-3203(93)90059-6; ILLGNER K, 1997, TIME VARYING IMAGE P, V4, P238; KIRCHNER H, 1992, PATTERN RECOGN LETT, V13, P131, DOI 10.1016/0167-8655(92)90044-Z; Lucas B.D., 1981, ITERATIVE IMAGE REGI, P674; Memin E, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P933, DOI 10.1109/ICCV.1998.710828; Moulin P, 1997, IEEE T IMAGE PROCESS, V6, P1606, DOI 10.1109/83.650115; MOULIN P, 1993, 1993 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS : PROCEEDINGS, VOLS 1-4 ( ISCAS 93 ), P1, DOI 10.1109/ISCAS.1993.393640; NAGEL HH, 1983, COMPUT VISION GRAPH, V21, P85, DOI 10.1016/S0734-189X(83)80030-9; ODOBEZ JM, 1994, P 7 EUR C SIGN PROC, P411; Ong EP, 1999, INT J COMPUT VISION, V31, P51, DOI 10.1023/A:1008046826441; SCHNORR C, 1991, INT J COMPUT VISION, V6, P25, DOI 10.1007/BF00127124; SCOTNEY BW, 1991, P 13 WORLD C COMP AP, P505; Szeliski R, 1996, IEEE T PATTERN ANAL, V18, P1199, DOI 10.1109/34.546257; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P129, DOI 10.1109/TPAMI.1986.4767767; Wang GY, 1996, T NONFERR METAL SOC, V6, P6; WEBER J, 1995, INT J COMPUT VISION, V14, P67, DOI 10.1007/BF01421489; Weickert J, 2001, INT J COMPUT VISION, V45, P245, DOI 10.1023/A:1013614317973; Wu S. F., 1993, Journal of Visual Communication and Image Representation, V4, P25, DOI 10.1006/jvci.1993.1003; Xie K, 1996, OPT ENG, V35, P145, DOI 10.1117/1.600885; Yaacobson FS, 1998, COMMUN NUMER METH EN, V14, P621, DOI 10.1002/(SICI)1099-0887(199807)14:7<621::AID-CNM174>3.0.CO;2-U; Yang YY, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P868, DOI 10.1109/ICIP.2001.958258	48	10	10	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2005	61	1					31	54		10.1023/B:VISI.0000042933.07192.26	http://dx.doi.org/10.1023/B:VISI.0000042933.07192.26			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	866XN					2022-12-18	WOS:000224806800002
J	Ronda, JI; Valdes, A; Jaureguizar, F				Ronda, JI; Valdes, A; Jaureguizar, F			Camera autocalibration and horopter curves	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						camera autocalibration; horopter; projective geometry; nonlinear optimization	SELF-CALIBRATION	We describe a new algorithm for the obtainment of the affine and Euclidean calibration of a camera under general motion. The algorithm exploits the relationships of the horopter curves associated to each pair of cameras with the plane at infinity and the absolute conic. Using these properties we define cost functions whose minimization by means of general purpose techniques provides the required calibration. The experiments show the good convergence properties, computational efficiency and robust performance of the new techniques.	Univ Politecn Madrid, Grp Tratamiento Imagenes, E-28040 Madrid, Spain; Univ Complutense Madrid, Dept Geometria & Topol, E-28040 Madrid, Spain	Universidad Politecnica de Madrid; Complutense University of Madrid	Ronda, JI (corresponding author), Univ Politecn Madrid, Grp Tratamiento Imagenes, E-28040 Madrid, Spain.	jir@gti.ssr.upm.es; Antonio_Valdes@mat.ucm.es; fjn@gti.ssr.upm.es	Ronda, José Ignacio/T-7334-2018; Jaureguizar, Fernando/T-7959-2018	Ronda, José Ignacio/0000-0003-1430-1835; Jaureguizar, Fernando/0000-0001-6449-5151; Valdes, Antonio/0000-0001-5930-8307				ARMSTRONG M, 1996, EUR C COMP VIS, P3; BAYBANK SJ, 1993, RECONSTRUCTION PROJE; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Fusiello A, 2000, IMAGE VISION COMPUT, V18, P555, DOI 10.1016/S0262-8856(99)00065-7; HARTLEY RI, 1994, IEEE T PAMI, V16; HARTLEY RI, 1999, P INT C COMP VIS; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hartman HJ, 1998, INSTR SCI, V26, P1, DOI 10.1023/A:1003023628307; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; MAYBANK SJ, 1990, PHILOS T ROY SOC A, V332, P1, DOI 10.1098/rsta.1990.0099; Nister D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P116, DOI 10.1109/ICCV.2001.937612; Pollefeys M, 1999, IEEE T PATTERN ANAL, V21, P707, DOI 10.1109/34.784285; PRESS WH, 2002, NUMERICAL RECIPES CP; Quan L, 1999, IEEE T PATTERN ANAL, V21, P774, DOI 10.1109/34.784291; SCHAFFALITZKY F, 2000, P IND C COMP VIS GRA; Semple J. G., 1998, ALGEBRAIC PROJECTIVE; SEO Y, 2000, INT C COMP VIS BARC; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388	18	10	10	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY-JUN	2004	57	3					219	232		10.1023/B:VISI.0000013095.05991.55	http://dx.doi.org/10.1023/B:VISI.0000013095.05991.55			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	766DR		Green Submitted			2022-12-18	WOS:000188330600004
J	Smeulders, AWM; Huang, TS; Gevers, T				Smeulders, AWM; Huang, TS; Gevers, T			Special issue on content-based image retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Univ Amsterdam, ISIS, NL-1012 WX Amsterdam, Netherlands; Univ Illinois, Beckman Inst, Chicago, IL 60680 USA	University of Amsterdam; University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Smeulders, AWM (corresponding author), Univ Amsterdam, ISIS, NL-1012 WX Amsterdam, Netherlands.	smeulders@science.uva.nl; huang@ifp.uiuc.edu; gevers@science.uva.nl						Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972	1	10	10	0	4	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN-MAR	2004	56	1-2			SI		5	6		10.1023/B:VISI.0000004865.97704.b9	http://dx.doi.org/10.1023/B:VISI.0000004865.97704.b9			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	745MJ					2022-12-18	WOS:000186692200001
J	Kang, SB; Jones, M				Kang, SB; Jones, M			Appearance-based structure from motion using linear classes of 3-D models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure from motion; appearance prediction; parameter reestimation through image synthesis; linear class models; constrained 3-D deformation; face modeling from images	RECONSTRUCTION; SEQUENCES; SHAPE	In this paper, we address the problem of recovering 3-D models from sequences of partly calibrated images with unknown correspondence. To that end, we integrate tracking, structure from motion with geometric constraints (specifically in the form of linear class models) in a single framework. The key to making the proposed approach work is the use of appearance-based model matching and refinement which updates the estimated correspondences on each iteration of the algorithm. Another key feature is the matching of a 3-D model directly with the input images without the conventional 2-step approach of stereo data recovery and 3-D model fitting. Initialization of the linear class model to one of the input images (the reference image) is currently partly manual. This synthesis and refine approach, or appearance-based constrained structure from motion (AbCSfm), is especially useful in recovering shapes of objects whose general structure is known but which may have little discernable texture in significant parts of their surfaces. We applied the proposed approach to 3-D face modeling from multiple images to create new 3-D faces for DECface, a synthetic talking head developed at Cambridge Research Laboratory, Digital Equipment Corporation. The DECface model comprises a collection of 3-D triangular and rectangular facets, with nodes as vertices. In recovering the DECface model, we assume that the sequence of images is taken with a camera with unknown focal length and pose. The geometric constraints used are of the form of linear combination of prototypes of 3-D faces of real people. Results of this approach show its good convergence properties and its robustness against cluttered backgrounds.	Microsoft Corp, Microsoft Res, Redmond, WA 98052 USA; MERL Cambridge Syst, Cambridge, MA 02139 USA	Microsoft	Kang, SB (corresponding author), Microsoft Corp, Microsoft Res, 1 Microsoft Way, Redmond, WA 98052 USA.	sbkang@microsoft.com; mjones@merl.com						AKIMOTO T, 1993, IEEE COMPUT GRAPH, V13, P16, DOI 10.1109/38.232096; [Anonymous], 1985, PERCEPTUAL ORG VISUA; Atick JJ, 1996, NEURAL COMPUT, V8, P1321, DOI 10.1162/neco.1996.8.6.1321; Ayache N, 1991, ARTIFICIAL VISION MO; Azarbayejani A., 1993, C COMP VIS PATT REC, P294; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191; DeCarlo D, 1996, PROC CVPR IEEE, P231, DOI 10.1109/CVPR.1996.517079; FROMKIN V, 1964, LANG SPEECH, V7, P215, DOI 10.1177/002383096400700402; Fua P, 2000, INT J COMPUT VISION, V38, P153, DOI 10.1023/A:1008105802790; Fua P., 1993, Machine Vision and Applications, V6, P35, DOI 10.1007/BF01212430; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; Gourret JP, 1996, COMPUT GRAPH, V20, P863, DOI 10.1016/S0097-8493(96)00057-X; HEYDEN A, 1996, ICPR, P339; HORANYI G, 1996, CATALYSIS, V12, P254; Jebara T, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P128, DOI 10.1109/ICCV.1998.710710; Jebara TS, 1997, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.1997.609312; KANG CY, 1994, IMAGE VISION COMPUT, V12, P5, DOI 10.1016/0262-8856(94)90051-5; La Cascia M, 1998, PROC CVPR IEEE, P508, DOI 10.1109/CVPR.1998.698653; LANITIS A, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P368, DOI 10.1109/ICCV.1995.466919; LEE Y, 1995, SIGGRAPH 95 C P, P55; Lengagne R, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P301, DOI 10.1109/AFGR.1996.557281; LEVERGOOD TM, 1993, 938 DIG EQ CORP CAMB; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; PARKE FI, 1990, ACM SIGGRAPH COURSE, V26; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P715, DOI 10.1109/34.85660; Poggio T., 1992, 1347 AI MIT; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; SCHODL A, 1998, WORKSH PERC US INT P, P43; Shabana A. A, 1989, DYNAMICS MULTIBODY S; SHELTON CR, 1998, THESIS MIT; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Stein GP, 1997, PROC CVPR IEEE, P400, DOI 10.1109/CVPR.1997.609356; Sturm P, 1997, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.1997.609467; SZELISKI R, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P194, DOI 10.1109/CVPR.1994.323829; Szeliski R., 1994, Journal of Visual Communication and Image Representation, V5, P10, DOI 10.1006/jvci.1994.1002; SZELISKI R, 1991, CVPR, P625; SZELISKI R, 1995, INT S COMP VIS COR G, P241; VETTER T, 1998, ECCV98, P499; VETTER T, 1995, 1531 AI MIT; Waters, 2005, COMPUT GRAPH, V21, P17, DOI [10.1145/37402.37405, DOI 10.1145/37402.37405]; Waters K., 1995, Multimedia Tools and Applications, V1, P349, DOI 10.1007/BF01215883; WATERS K, 1996, WORKSH COMP VIS MAN; Watt A, 1990, FUNDAMENTALS 3 DIMEN; Xu A., 2018, KINETIC THEORY, DOI [10.1007/978-94-015-8668-9, DOI 10.1007/978]; ZHENG JY, 1994, IEEE T PATTERN ANAL, V16, P163, DOI 10.1109/34.273734	46	10	10	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2002	49	1					5	22		10.1023/A:1019849812326	http://dx.doi.org/10.1023/A:1019849812326			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	585UJ					2022-12-18	WOS:000177542700001
J	Cheng, YQ; Wang, XG; Collins, RT; Riseman, EM; Hanson, AR				Cheng, YQ; Wang, XG; Collins, RT; Riseman, EM; Hanson, AR			Three-dimensional reconstruction of points and lines with unknown correspondence across images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						feature correspondence matching; point/line affinity measure; weighted bipartite graph matching; maximum network flow	ALGORITHM; MOTION; RECOVERY; SETS	Three-dimensional reconstruction from a set of images is an important and difficult problem in computer vision. In this paper, we address the problem of determining image feature correspondences while simultaneously reconstructing the corresponding 3D features, given the camera poses of disparate monocular views. First, two new affinity measures are presented that capture the degree to which candidate features from different images consistently represent the projection of the same 3D point or 3D line. An affinity measure for point features in two different views is defined with respect to their distance from a hypothetical projected 3D pseudo-intersection point. Similarly, an affinity measure for 2D image line segments across three views is defined with respect to a 3D pseudo-intersection line. These affinity measures provide a foundation for determining unknown correspondences using weighted bipartite graphs representing candidate point and line matches across different images. As a result of this graph representation, a standard graph-theoretic algorithm can provide an optimal, simultaneous matching and triangulation of points across two views, and lines across three views. Experimental results on synthetic and real data demonstrate the effectiveness of the approach.	Carnegie Mellon Univ, Inst Robot, NSH, Pittsburgh, PA 15213 USA; Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA	Carnegie Mellon University; University of Massachusetts System; University of Massachusetts Amherst	Cheng, YQ (corresponding author), Carnegie Mellon Univ, Inst Robot, NSH, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	riseman@cs.umass.edu						AGGARWAL JK, 1981, P IEEE, V69, P562, DOI 10.1109/PROC.1981.12025; Aloimonos J., 1986, Proceedings AAAI-86: Fifth National Conference on Artificial Intelligence, P681; *AM ART INT, 1991, KBV SYST US GUID; *ARPA, 1994, ARP IM UND WORKSH MO; Ayache N, 1991, ARTIFICIAL VISION MO; Basu A., 1987, Proceedings of VLSI and Computers. First International Conference on Computer Technology, Systems and Applications. COMPEURO 87 (Cat. No.87CH2417-4), P815; Bedekar AS, 1996, PROC CVPR IEEE, P61, DOI 10.1109/CVPR.1996.517054; Blostein S. D., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P325; BOLDT M, 1989, IEEE T SYST MAN CYB, V19, P1581, DOI 10.1109/21.44073; BURNS JB, 1986, IEEE T PATTERN ANAL, V8, P425, DOI 10.1109/TPAMI.1986.4767808; CHANG SF, 1989, FAST IMPLEMENTATION; CHENG Y, 1994, ARPA IM UND WORKSH M, P993; CHENG YQ, 1996, TR9691 CMPSCI U MASS; CHENG YQ, 1996, P SPIE VCIP; CHERIYAN J, 1989, SIAM J COMPUT, V18, P1057, DOI 10.1137/0218072; Chou T., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P534; Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097; Collins RT, 1996, IMAGE UNDERSTANDING WORKSHOP, 1996 PROCEEDINGS, VOLS I AND II, P305; Collins RT, 1998, COMPUT VIS IMAGE UND, V72, P143, DOI 10.1006/cviu.1998.0729; COLLINS RT, 1996, P ICCV IEEE, P888; COLLINS RT, 1993, P DARPA IM UND WORKS, P197; COLLINS RT, 1995, SPIE P, V7617, P244; DERICHE R, 1992, P 2 EUR C COMP VIS; EDMONDS J, 1972, J ACM, V19, P248, DOI 10.1145/321694.321699; EDMONDS J, 1965, J RES NBS B, V69; Ford L. R. J., 1962, FLOWS NETWORKS; GABOW HN, 1990, PROCEEDINGS OF THE FIRST ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P434; GANAPATHY S, 1984, P INT C ROBOTICS, P130; Gold S, 1996, PROC CVPR IEEE, P239, DOI 10.1109/CVPR.1996.517080; GOLDBERG AV, 1988, J ACM, V35, P921, DOI 10.1145/48014.61051; GOLDGOF DB, 1992, PATTERN RECOGN, V25, P271, DOI 10.1016/0031-3203(92)90110-5; GRIFFIN PM, 1989, PATTERN RECOGN LETT, V9, P361, DOI 10.1016/0167-8655(89)90065-2; GRUEN AW, 1988, PHOTOGRAMM ENG REM S, V54, P633; HAO J, 1993, SERIES DIMACS, V12, P453; HOPCROFT JE, 1973, J ACM, V23, P225; ITO E, 1988, IM UND WORKSH, P921; JAYNES C, 1994, P ARPA IM UND WORKSH, P359; JOHNSON DS, 1993, SERIES DIMACS, V12; JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710; KAHN P, 1990, IEEE T PATTERN ANAL, V12, P1098, DOI 10.1109/34.61710; KIM WY, 1991, IEEE T PATTERN ANAL, V13, P224, DOI 10.1109/34.75511; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; KUMAR R, 1992, J ROBOTIC SYST, V9, P753, DOI 10.1002/rob.4620090605; KUMAR R, 1992, P DARP IM UND WORKSH, P727; KUMAR R, 1992, TR9204 CMPSCI U MASS; KUMAR R, 1994, P ARPA IM UND MONT C, P947; LEE H, 1990, INT J IMAGING SYSTEM, V2, P55; LEE H, 1986, P 8 INT C PATT REC, P303; Lee Jong Eun, 1993, Korean Biochemical Journal, V26, P1; LESSARD R, 1989, NETWORKS, V19, P459, DOI 10.1002/net.3230190406; *M MAR SRI INT, 1993, RCDE US GUID; Micali S., 1980, 21st Annual Symposium on Foundations of Computer Science, P17; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P715, DOI 10.1109/34.85660; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P730, DOI 10.1109/34.85661; Roy S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P492, DOI 10.1109/ICCV.1998.710763; SCLAROFF S, 1993, P 4 INT C COMP VIS, P308; SCOTT GL, 1991, P ROY SOC B-BIOL SCI, V244, P21, DOI 10.1098/rspb.1991.0045; SHAPIRO LS, 1992, IMAGE VISION COMPUT, V10, P283, DOI 10.1016/0262-8856(92)90043-3; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; Wang XG, 1996, PROC CVPR IEEE, P252, DOI 10.1109/CVPR.1996.517082; WU MS, 1995, PATTERN RECOGN LETT, V16, P23, DOI 10.1016/0167-8655(94)00077-G; WU Z, 1993, IEEE PAMI, V13, P1101; Zhang Z., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P38, DOI 10.1109/ICPR.1990.118061; Zhang Z., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P577, DOI 10.1109/ICCV.1990.139598; [No title captured]; [No title captured]; [No title captured]	67	10	17	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2001	45	2					129	156		10.1023/A:1012424014764	http://dx.doi.org/10.1023/A:1012424014764			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	487KF					2022-12-18	WOS:000171873700003
J	Alexander, DC; Buxton, BF				Alexander, DC; Buxton, BF			Statistical modeling of colour data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						colour; statistics; statistical modeling; segmentation; tracking; active region models; snakes; image processing; computer vision; physics-based vision	ACTIVE-REGION MODELS; REFLECTANCE; TEXTURE; DIFFUSE; IMAGES	In this paper we investigate how best to model naturally arising distributions of colour camera data. It has become standard to model single mode distributions of colour data by ignoring the intensity component and constructing a Gaussian model of the chromaticity. This approach is appealing, because the intensity of data can change arbitrarily due to shadowing and shading, whereas the chromaticity is more robust to these effects. However, it is unclear how best to construct such a model, since there are many domains in which the chromaticity can be represented. Furthermore, the applicability of this kind of model is questionable in all but the most basic lighting environments. We begin with a review of the reflection processes that give rise to distributions of colour data. Several candidate models are then presented; some are from the existing literature and some are novel. Properties of the different models are compared analytically and the models are empirically compared within a region tracking application over two separate sets of data. Results show that chromaticity based models perform well in constrained environments where the physical model upon which they are based applies. It is further found that models based on spherical representations of the chromaticity data provide better performance than those based on more common planar representations, such as the chromaticity plane or the normalised colour space. In less constrained environments, however, such as daylight, chromaticity based models do not perform well, because of the effects of additional illumination components, which violate the physical model upon which they are based.	UCL, Dept Comp Sci, London WC1E 6BT, England	University of London; University College London	Alexander, DC (corresponding author), UCL, Dept Comp Sci, Gower St, London WC1E 6BT, England.			Alexander, Daniel/0000-0003-2439-350X				Alexander DC, 1997, PROC CVPR IEEE, P319, DOI 10.1109/CVPR.1997.609342; ALEXANDER DC, 1997, P BRIT MACH VIS C; ALEXANDER DC, 1997, THESIS U LONDON; ALEXANDER DC, 1999, P IEEE C COMP VIS PA, V1, P244; Bajcsy R, 1996, INT J COMPUT VISION, V17, P241, DOI 10.1007/BF00128233; Barni M, 1997, IMAGE VISION COMPUT, V15, P549, DOI 10.1016/S0262-8856(97)01138-4; BEYER HA, 1992, THESIS ETH ZURICH; BINGHAM C, 1974, ANN STAT, V2, P1201, DOI 10.1214/aos/1176342874; Campbell NW, 1997, PATTERN RECOGN, V30, P555, DOI 10.1016/S0031-3203(96)00112-4; CRISMAN J, 1992, ACTIVE VISION, P107; Duda R.O., 1973, J ROYAL STAT SOC SER; Egan J.P., 1975, SIGNAL DETECTION THE; FERRI F, 1992, PATTERN RECOGN LETT, V13, P561, DOI 10.1016/0167-8655(92)90091-D; Figueiredo MAT, 1997, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.1997.609406; FINLAYSON GD, 1998, P 5 EUR C COMP VIS, P475; Fisher NI., 1987, STAT ANAL SPHERICAL, DOI [10.1017/CBO9780511623059, DOI 10.1017/CBO9780511623059]; Foley James D., 1990, COMPUTER GRAPHICS; FUNT BV, 1995, IEEE T PATTERN ANAL, V17, P522, DOI 10.1109/34.391390; Gevers T, 1999, PATTERN RECOGN, V32, P453, DOI 10.1016/S0031-3203(98)00036-3; HEALEY G, 1992, IEEE T SYST MAN CYB, V22, P64, DOI 10.1109/21.141311; HEALEY G, 1988, INT C COMP VIS, P460; HORN BKP, 1979, APPL OPTICS, V18, P1770, DOI 10.1364/AO.18.001770; IVINS J, 1994, BMVC94 - PROCEEDINGS OF THE 5TH BRITISH MACHINE VISION CONFERENCE, VOLS 1 AND 2, P377; Ivins J, 1998, COMPUT VIS IMAGE UND, V72, P54, DOI 10.1006/cviu.1997.0653; IVINS J, 1995, IMAGE VISION COMPUT, V13, P431, DOI 10.1016/0262-8856(95)99730-O; IVINS J, 1996, THESIS U SHEFFIELD; Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198; KENT JT, 1987, APPL STAT-J ROY ST C, V36, P139, DOI 10.2307/2347545; KLINKER GJ, 1993, PHYSICAL APPROACH CO; Koenderink JJ, 1996, J OPT SOC AM A, V13, P452, DOI 10.1364/JOSAA.13.000452; LEE HC, 1990, IEEE T PATTERN ANAL, V12, P402, DOI 10.1109/34.50626; Mardia K.V., 2000, DIRECTIONAL STAT; MATAS J, 1994, P BRIT MACH VIS C, P469; MATAS J, 1993, P BRIT MACH VIS C, P539; MCNEIL BJ, 1984, MED DECIS MAKING, V4, P137, DOI 10.1177/0272989X8400400203; METZ CE, 1978, SEMIN NUCL MED, V8, P283, DOI 10.1016/S0001-2998(78)80014-2; Nayar SK, 1996, INT J COMPUT VISION, V17, P219, DOI 10.1007/BF00128232; OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684; RONFARD R, 1994, INT J COMPUT VISION, V13, P229, DOI 10.1007/BF01427153; SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409; SWETS JA, 1979, INVEST RADIOL, V14, P109, DOI 10.1097/00004424-197903000-00002; THORPE C, 1988, IEEE T PATTERN ANAL, V10, P362, DOI 10.1109/34.3900; TOMINAGA S, 1991, IEEE T PATTERN ANAL, V13, P658, DOI 10.1109/34.85656; TOMINAGA S, 1989, J OPT SOC AM A, V6, P576, DOI 10.1364/JOSAA.6.000576; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; van Ginneken B, 1998, APPL OPTICS, V37, P130, DOI 10.1364/AO.37.000130; Wyszecki G., 1967, COLOR SCI CONCEPTS M; ZHANG J, 1990, IEEE T PATTERN ANAL, V12, P1009, DOI 10.1109/34.58873; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343	51	10	10	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2001	44	2					87	109		10.1023/A:1011870913626	http://dx.doi.org/10.1023/A:1011870913626			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	492BX					2022-12-18	WOS:000172148300001
J	Heyden, A				Heyden, A			Reduced multilinear constraints: Theory and experiments	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						reconstruction; fundamental matrices; multilinear constraints; linear reconstruction techniques; trifocal tensor; quadfocal tensor	CALIBRATION	This paper deals with the problem of reconstructing the locations of n points in space from m different images without camera calibration. We will show how these reconstruction problems for different n and m can be put into a similar theoretical framework. This will be done using a special choice of coordinates, both in the object and in the images, called reduced affine coordinates. This choice of coordinates simplifies the analysis of the multilinear geometry and gives simpler forms of the multilinear tensors. In particular, we will investigate the cases, which can be solved by linear methods, i.e., greater than or equal to 8 paints in 2 images, greater than or equal to 7 points in 3 images and greater than or equal to 6 points in 4 images. A new concept, the reduced fundamental matrix, is introduced, which gives bilinear expressions in the image coordinates. It has six nonzero elements, which depend on just four parameters and can be used to make reconstruction from 2 images. We also introduce the concept of the reduced trifocal tensor, which gives trilinear expressions in the image coordinates in 3 images. It has 15 nonzero elements and depends on nine parameters and can be used to make reconstruction from 3 images. Finally, the reduced quadfocal tensor is introduced, which describes the relations between points in 4 images and gives quadlinear expressions in the image coordinates. This tensor has 36 nonzero elements which depend on 14 independent parameters and can be used to make reconstruction from 4 images. These tensors give the possibility to calculate linear solutions from greater than or equal to 8 points in 2 images, greater than or equal to 7 points in 3 images and also from greater than or equal to 6 points in 4 images. Furthermore, a canonical form of the camera matrices in a sequence is presented and it is shown that the quadlinear constraints can be calculated from the trilinear ones, and that in general the trilinear constraints can be calculated from the bilinear ones.	Lund Univ, Dept Math, S-22100 Lund, Sweden	Lund University	Heyden, A (corresponding author), Lund Univ, Dept Math, Box 118, S-22100 Lund, Sweden.	heyden@maths.lth.se						BOUFAMA B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1030, DOI 10.1109/ICCV.1995.466821; CARLSSON S, 1995, IEEE WORKSH REPR VIS; FAUGERAS O, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P951, DOI 10.1109/ICCV.1995.466832; FAUGERAS O, 1995, IEEE WORKSH REPRESEN; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Hartley R.I., 1994, P ARPA IM UND WORKSH; HARTLEY RI, 1994, IEEE T PATTERN ANAL, V16, P1036, DOI 10.1109/34.329005; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P882, DOI 10.1109/ICCV.1995.466843; HARTLEY RI, 1992, LECT NOTES COMPUT SC, V588, P579; HEYDEN A, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1058, DOI 10.1109/ICCV.1995.466817; HEYDEN A, 1994, P S IM AN SSAB HALMS, P49; HEYDEN A, 1995, IEEE WORKSH REPR VIS; HEYDEN A, 1996, LECT NOTES COMPUTER, V1065, P671; HEYDEN A, 1995, THESIS LUND U SWEDEN; HEYDEN A, 1995, THEORY APPL IMAGE PR, V2, P57; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LUONG QT, 1994, LECT NOTES COMPUTER, V800, P589; LUONG QT, 1992, THESIS U PARIS SUD C; MAYBANK S, 1993, THEORY RECONSTRUCTIO; MOHR R, 1991, PATTERN RECOGN LETT, V12, P39, DOI 10.1016/0167-8655(91)90026-I; QAN L, 1994, LECT NOTES COMPUTER, V801, P459; SHAHSUA A, 1995, P 5 INT C COMP VIS M, P920; SHASHUA A, 1995, IEEE T PATTERN ANAL, V17, P779, DOI 10.1109/34.400567; SHASHUA A, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P483, DOI 10.1109/CVPR.1994.323870; SHASHUA A, 1994, LECT NOTES COMPUTER, V800, P479; SPARR A, 1994, CODENLUTFD2TFMA94700; SPARR G, 1994, LECT NOTES COMPUTER, V801, P471; SPARR G, 1992, THEORY APPL IMAGE PR, V1, P87; SPETSAKIS M, 1990, P DARPA IM UND WORKS, P271; TRIGGS B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P338, DOI 10.1109/ICCV.1995.466920	33	10	10	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1998	30	1					5	26		10.1023/A:1008020228557	http://dx.doi.org/10.1023/A:1008020228557			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	152CC					2022-12-18	WOS:000077757100001
J	Drewniok, C; Rohr, K				Drewniok, C; Rohr, K			Model-based detection and localization of circular landmarks in aerial images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						camera orientation; image registration; aerial images; model-based recognition; detection; high precision localization; circular landmarks; model fitting	MACHINE VISION; EDGE-DETECTION; REGISTRATION; FIDUCIALS; ALGORITHM; DESIGN	The photogrammetric exploitation of aerial images essentially requires the accurate reconstruction of the imaging geometry. This especially includes the determination of the orientation of the camera. Usually, the orientation parameters are determined by spatial resection, knowing the exact coordinates of control points on the ground and in the image. The reliability and accuracy of this registration task strongly depend on the selection of suitable landmarks as well as on the precision obtained for landmark localization. In this contribution, we consider the problem of automatic landmark extraction for the purpose of aerial image registration. We suggest to use manhole covers as a specific type of circular landmarks which frequently occur in urban environments and we introduce a model-based approach for localizing these features with high subpixel precision. Our approach is based on a parametric intensity model. Localization of the landmarks is done by directly fitting this model to the observed image intensities. Since we have an explicit description of the landmark it is possible to verify the result by exploiting the estimated parameters. We also address the problem of landmark detection which can greatly be supported by template matching. The template used is a prototype model which is generated from representative examples during a training phase. The training scheme also provides initial values for the fitting procedure as well as thresholds for the final verification step. The full approach has been tested on synthetic as well as on real image data.			Drewniok, C (corresponding author), UNIV HAMBURG,FACHBEREICH INFORMAT,VOGT KOLLN STR 30,D-22527 HAMBURG,GERMANY.							AMIR I, 1990, COMPUT VISION GRAPH, V49, P398, DOI 10.1016/0734-189X(90)90112-9; BOSE CB, 1990, IEEE T PATTERN ANAL, V12, P1196, DOI 10.1109/34.62609; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CHAUDHURI BB, 1993, PATTERN RECOGN LETT, V14, P1, DOI 10.1016/0167-8655(93)90126-X; CHIORBOLI G, 1993, IEEE T PATTERN ANAL, V15, P1330, DOI 10.1109/34.250850; DAVIES ER, 1987, PATTERN RECOGN LETT, V6, P323, DOI 10.1016/0167-8655(87)90015-8; DREWNIOK C, 1994, INT J REMOTE SENS, V15, P3743, DOI 10.1080/01431169408954356; DREWNIOK C, 1996, THESIS U HAMBURG; Forstner<spacing Wolfgang, 1987, ISPRS INT C FAST PRO, P2; FORSTNER W, 1993, COMPUTER ROBOT VISIO, P289; HEEGER D, 1991, OBVIUS OBJECT BASED; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; NALWA VS, 1986, IEEE T PATTERN ANAL, V8, P699, DOI 10.1109/TPAMI.1986.4767852; OGORMAN L, 1990, 10 INT C PATT REC AT, P249; PRESS W, 1989, NUMERICAL RECEIPES P; ROHR K, 1992, INT J COMPUT VISION, V9, P213, DOI 10.1007/BF00133702; STARINK JPP, 1993, PATTERN RECOGN LETT, V14, P895, DOI 10.1016/0167-8655(93)90154-6; TICHEM M, 1994, IEEE T PATTERN ANAL, V16, P791, DOI 10.1109/34.308473	18	10	10	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP-OCT	1997	24	3					187	217		10.1023/A:1007919223573	http://dx.doi.org/10.1023/A:1007919223573			31	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XY026					2022-12-18	WOS:A1997XY02600001
J	Xiong, YL; Shafer, SA				Xiong, YL; Shafer, SA			Moment and hypergeometric filters for high precision computation of focus, stereo and optical flow	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						focus; stereo; optical flow; image matching; window effects; foreshortening; Gabor filter; moment filter; hypergeometric filter; computer vision	COMPLEX ARGUMENTS; LARGE MAGNITUDES; GABOR FILTERS	Many low level visual computation problems such as focus, stereo, optical flow, etc., can be formulated as problems of extracting one or more parameters of a non-stationary transformation between two images. Finite-width windows are widely used in various algorithms to extract spatially local information from images. While the choice of window width has a very profound impact on the quality of algorithmic results, there has been no quantitative way to measure or eliminate the negative effects of finite-width windows. To address this problem and the foreshortening problem caused by non-stationarity, we introduce two novel sets of filters: ''moment'' filters and ''hypergeometric'' filters. The recursive properties of these filters allow the effects of finite-width windows and foreshortening to be explicitly analyzed and eliminated. We apply the moment filter approach to the focus and stereo problems, in which one parameter is extracted at every pixel location. We apply the hypergeometric approach to the optical flow problem, in which two parameters are extracted. We demonstrate that algorithms based on moment filters and hypergeometric filters achieve much higher precision than other state-of-art techniques.			Xiong, YL (corresponding author), CARNEGIE MELLON UNIV,INST ROBOT,PITTSBURGH,PA 15213, USA.							Abramowitz M., 1972, HDB MATH FUNCTIONS F; ADELSON EH, 1984, J OPT SOC AM A, P284; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BOVE VM, 1989, P OSA TOP M IM UND M; BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384; Fleet D. J., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P379, DOI 10.1109/CVPR.1989.37875; Fleet D. J., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P52, DOI 10.1109/WVM.1991.212788; FLEET DJ, 1991, CVGIP-IMAG UNDERSTAN, V53, P198, DOI 10.1016/1049-9660(91)90027-M; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; HEEGER DJ, 1988, INT J COMPUT VISION, P279; JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S; KANADE T, 1990, DARPA IM UND WORKSH, P383; Krumm J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P284, DOI 10.1109/CVPR.1992.223262; KRUMM J, 1993, CMURITR9314; LANGLEY K, 1990, 1ST P EUR C COMP VIS, P315; Malik J., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P267, DOI 10.1109/CVPR.1993.340979; NARDIN M, 1992, J COMPUT APPL MATH, V39, P193, DOI 10.1016/0377-0427(92)90129-L; NARDIN M, 1992, ACM T MATH SOFTWARE, V18, P345, DOI 10.1145/131766.131774; Nikiforov A.F., 1988, SPECIAL FUNCTIONS MA; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; Press WH, 1988, NUMERICAL RECIPES C; SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114; SMITH RC, 1986, INT J ROBOTICS RES, V5; Spanier J., 1987, ATLAS FUNCTIONS; Subbarao M., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P149, DOI 10.1109/CCV.1988.589986; WENG J, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P200; WILLSON RG, 1992, CMUCS92118 SCH COMP; Xiong Y., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P68, DOI 10.1109/CVPR.1993.340977; XIONG Y, 1995, INT C COMP VIS, P771; XIONG Y, 1994, CMURITR9428; XIONG Y, 1994, P IEEE C COMP VIS PA, P668; [No title captured]	32	10	11	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	1997	22	1					25	59		10.1023/A:1007927810205	http://dx.doi.org/10.1023/A:1007927810205			35	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WT108					2022-12-18	WOS:A1997WT10800002
J	Giachetti, A; Torre, V				Giachetti, A; Torre, V			The use of optical flow for the analysis of non-rigid motions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							NONRIGID MOTION; FIELD; COMPUTATION; PARAMETERS; RECOVERY	This paper analyses the 2D motion field on the image plane produced by the 3D motion of a plane undergoing simple deformations. When the deformation can be represented by a planar linear vector field, the projected vector field, i.e., the 2D motion field of the deformation, is at most quadratic. This 2D motion field has one singular point, with eigenvalues identical to those of the singular point describing the deformation. As a consequence, the nature of the singular point of the deformation is a projective invariant. When the plane moves and experiences a linear deformation at the same time, the associated 2D motion field is at most quadratic with at most 3 singular points. In the case of a normal rototranslation, i.e., when the angular velocity is normal to the plane, and of a linear deformation, the 2D motion field has one singular point and substantial information on the rigid motion and on the deformation can be recovered from it. Experiments with image sequences of planes moving and undergoing linear deformations show that the proposed analysis can provide accurate results. In addition, experiments with deformable objects, such as water, oil, textiles and rubber show that the proposed approach can provide information on more general 3D deformations.			Giachetti, A (corresponding author), UNIV GENOA,DIPARTIMENTO FIS,VIA DODECANESO 33,I-16146 GENOA,ITALY.		Giachetti, Andrea/AAD-8247-2020	giachetti, andrea/0000-0002-7523-6806				AICARDI F, 1990, BIOL CYBERN, V64, P141; AMARTUR SC, 1993, NUCL MAGNETIC RESONA, V12, P59; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BERGHOLM F, 1991, CVGIP-IMAG UNDERSTAN, V53, P171, DOI 10.1016/1049-9660(91)90025-K; CAMPANI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P90, DOI 10.1016/1049-9660(92)90088-K; DEMICHELI E, 1993, IEEE T PATTERN ANAL, V15, P434, DOI 10.1109/34.211464; FENNEMA CL, 1979, COMPUT VISION GRAPH, V9, P301, DOI 10.1016/0146-664X(79)90097-2; FORD RM, 1994, CVGIP-GRAPH MODEL IM, V56, P75, DOI 10.1006/cgip.1994.1007; FRANCOIS E, 1990, IMAGE VISION COMPUT, V8, P279, DOI 10.1016/0262-8856(90)80004-D; GALLIONE A, 1993, SCIENCE, V261, P348; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; HELMHOLTZ H, 1858, CRELLES J, V55, P25, DOI DOI 10.1515/CRLL.1858.55.25; HILDRETH EC, 1984, PROC R SOC SER B-BIO, V221, P189, DOI 10.1098/rspb.1984.0030; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; JASINSCHI R, 1989, J OPT SOC AM A, V6, P1085; KOENDERINK JJ, 1986, J OPT SOC AM A, V3, P242, DOI 10.1364/JOSAA.3.000242; LICHTENBERG AJ, 1992, REGULAR CHAOTIC DYNA; LONGUETHIGGINS HC, 1984, PROC R SOC SER B-BIO, V223, P165, DOI 10.1098/rspb.1984.0088; NAGEL HH, 1983, COMPUT VISION GRAPH, V21, P85, DOI 10.1016/S0734-189X(83)80030-9; OTTE M, 1994, P 3 EUR C COMP VIS S, V1, P51; PENNA MA, 1992, CVGIP-IMAG UNDERSTAN, V56, P366, DOI 10.1016/1049-9660(92)90048-8; RAO AR, 1992, IEEE T PATTERN ANAL, V14, P693, DOI 10.1109/34.142908; SHU CF, 1993, CVGIP-IMAG UNDERSTAN, V58, P383, DOI 10.1006/ciun.1993.1049; SHULMAN D, 1988, PROC R SOC SER B-BIO, V233, P217, DOI 10.1098/rspb.1988.0020; SOMMERFELD A, 1974, MECH DEFORMABLE BODI; SUBBARAO M, 1989, IEEE T PATTERN ANAL, V3, P266; ULLMAN S, 1984, PERCEPTION, V13, P255, DOI 10.1068/p130255; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; VERRI A, 1989, J OPT SOC AM A, V6, P698, DOI 10.1364/JOSAA.6.000698; W Hirsch M., 1989, DIFF EQUAT+; WOHN K, 1990, COMPUT VISION GRAPH, V49, P127, DOI 10.1016/0734-189X(90)90134-H; YAKAMOTO M, 1993, IEEE T PAMI, V15, P82	32	10	10	0	3	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1996	18	3					255	279		10.1007/BF00123144	http://dx.doi.org/10.1007/BF00123144			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VB108					2022-12-18	WOS:A1996VB10800004
J	Forsyth, DA				Forsyth, DA			Recognizing algebraic surfaces from their outlines	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						recognition; computer vision; algebraic surfaces; invariant theory; outlines	RECOGNITION	The outline in a single picture of a generic algebraic surface of degree three or greater completely determines the projective geometry of the surface. The result holds for a generic perspective view of a generic algebraic surface, where the camera calibration parameters and the focal point are unknown. Known camera calibration appears not to reduce the projective ambiguity. The result is constructive.			Forsyth, DA (corresponding author), UNIV CALIF BERKELEY,DIV COMP SCI,BERKELEY,CA 94720, USA.							BASSET AB, 1910, TREATISE GEOMETRY SU; BINFORD TO, 1989, UNCERTAINTY A1 3; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; CIPOLLA R, 1992, INT J COMPUTER VISIO, V8, P1; DALMEIDA J, 1992, DUKE MATH J, V65, P229, DOI 10.1215/S0012-7094-92-06509-4; DHOME M, 1990, P 1 EUR C COMP VIS; FORSYTH DA, 1994, P CVPR 94; FORSYTH DA, 1991, IEEE T PATTERN ANAL, V13, P10; FORSYTH DA, 1992, P 2 EUR C COMP VIS; FREEMAN H, 1978, IEEE T COMPUT, V27, P819; GIBLIN P, 1986, P ICCV 1 LOND; GOMEZMONT X, 1989, LECT RIEMANN SURFACE; GRIFFITHS P, 1986, METHODS ALGEBRAIC GE; HARTSHORNE R, 1977, ALGEBRAIC GEOMETRY; KAPUR D, 1992, SYMBOLIC NUMERICAL C; KOENDERINK J, 1979, BIOL CYBERNETICS, V32; Koenderink J., 1990, SOLID SHAPE; Koenderink J. J., 1984, PERCEPTION, V13; LAMDAN Y, 1988, JUN P CVPR C ANN ARB, P335; MALIK J, 1987, IJCV, V1; Marr D., 1982, VISION COMPUTATIONAL; Mundy J., 1992, GEOMETRIC INVARIANCE; OHM J, 1980, STUDIES ALGEBRAIC GE; OLVER PJ, 1986, SPRINGER VERLAG GRAD, V107; PETITJEAN S, 1992, INT J COMPUT VISION, V9, P231, DOI 10.1007/BF00133703; PLANTINGA H, 1987, 736 CS TR U WISC; PONCE J, 1989, IEEE T PATTERN ANAL, V11, P951, DOI 10.1109/34.35498; PONCE J, 1992, GEOMETRIC INVARIANCE; PONCE J, 1989, P DARPA IU WORKSH; PONCE J, 1990, P AAAI C BOST JUL; PONCE J, 1991, P IEEE WORKSH DIR AU; PONCE J, 1990, P 5 IEEE INT S INT C; RIEGER J, 1992, INT J COMPUTER VISIO, V7; ROTHWELL C, 1991, P BRIT MACH VIS C; ROTHWELL CA, 1992, GEOMETRIC INVARIANCE; ROTHWELL CA, 1993, P IEEE INT C COMPUTE, P573; SALMON G, MODERN HIGHER ALGEBR; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; TAUBIN G, 1992, GEOMETRIC INVARIANCE; TERZOPOULOS D, 1988, ARTIF INTELL, V36, P91, DOI 10.1016/0004-3702(88)90080-X; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; ULUPINAR F, 1991, P CVPR MAUII; ULUPINAR F, 1990, P ICCV OS; Wayner P. C., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P473, DOI 10.1109/CVPR.1991.139738; WEISS I, 1988, P IMAGE UNDERSTANDIN, P1125; ZERROUG M, IN PRESS INT J COMP; ZISSERMAN A, 1992, GEOMETRIC INVARIANCE	47	10	10	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1996	18	1					21	40		10.1007/BF00126138	http://dx.doi.org/10.1007/BF00126138			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	UJ409					2022-12-18	WOS:A1996UJ40900002
J	DRON, L				DRON, L			THE MULTISCALE VETO MODEL - A 2-STAGE ANALOG NETWORK FOR EDGE-DETECTION AND IMAGE-RECONSTRUCTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							ZERO CROSSINGS; LIGHTNESS; SIGNALS	This article presents the theory behind a model for a two-stage analog network for edge detection and image reconstruction to be implemented in analog VLSI. Edges are detected in the first stage using the multiscale veto rule, which states that an edge exists only if it can pass a threshold test for each of a set of smoothing filters of decreasing bandwidth. The image is reconstructed in die second stage from the brightness values at the pixels between which edges occur. The effect of die multiscale veto rule is that noise is removed with the efficiency of the narrowest-band smoothing filter, while edges are well-localized to feature boundaries without having to identify maxima in the magnitude of the gradient. Unlike previous analog models for edge detection and reconstruction, there are no problems of local minima, and for any given set of parameters, there is a unique solution. The reconstructed images appear natural and are very similar visually to the originals.			DRON, L (corresponding author), MIT,ARTIFICIAL INTELLIGENCE LAB,545 TECHNOL SQ,CAMBRIDGE,MA 02139, USA.							BATALI J, 1981, MIT AI869 ART INT LA; BLAKE A, 1985, COMPUT VISION GRAPH, V32, P314, DOI 10.1016/0734-189X(85)90054-4; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CURTIS SR, 1987, J OPT SOC AM A, V4, P221, DOI 10.1364/JOSAA.4.000221; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gonzalez R. C., 1987, DIGITAL IMAGE PROCES; HARRIS JG, 1989, ANALOG VLSI IMPLEMEN, P27; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; HORN BKP, 1971, MIT AI285 ART INT LA; KEAST CL, 1992, THESIS MIT; KNIGHT JTF, 1983, THESIS MIT; Korn G. A., 1968, MATH HDB SCI ENG; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; Lee H.-S., 1991, 1991 Symposium on VLSI Circuits. Digest of Technical Papers (IEEE Cat. No.91CH3018-9), P109; LOGAN BF, 1977, AT&T TECH J, V56, P487, DOI 10.1002/j.1538-7305.1977.tb00522.x; LUMSDAINE A, 1991, MIT AI1280 ART INT L; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; MUMFORD D, 1985, P IEEE C COMPUT VIS; Rabbani M., 1991, DIGITAL IMAGE COMPRE; RICHARDSON T, 1990, THESIS MIT; TORRE V, 1984, MIT AI768 ART INT LA; Witkin AP, 1983, 8 INT JOINT C ART IN, P1019; WYATT JL, 1992, INT J COMPUT VISION, V8, P217, DOI 10.1007/BF00055153; WYATT JL, 1993, INT J COMPUT VIS; YU PC, 1992, IEEE J SOLID-ST CIRC, V27, P545, DOI 10.1109/4.126542	26	10	10	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1993	11	1					45	61		10.1007/BF01420592	http://dx.doi.org/10.1007/BF01420592			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LW796		Green Submitted			2022-12-18	WOS:A1993LW79600003
J	CIPOLLA, R; ZISSERMAN, A				CIPOLLA, R; ZISSERMAN, A			QUALITATIVE SURFACE SHAPE FROM DEFORMATION OF IMAGE CURVES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								We describe the constraints placed on the differential geometry of a surface by observing a surface curve from a sequence of positions. In particular two new qualitative results are presented. First, it is shown that sign of normal curvature along the surface curve is determined by tracking image curve inflections. This result requires only approximate knowledge of the direction of projected viewer translation. It is a generalization to surface curves of Weinshall's [28] result for surface texture. Second, it is shown that surface orientation at a transverse crossing of surface curves is determined without knowledge of viewer translation. Results are demonstrated on real image sequences.	UNIV OXFORD,DEPT ENGN SCI,OXFORD OX1 3PJ,ENGLAND	University of Oxford			Arandjelović, Ognjen/V-5255-2019	Arandjelović, Ognjen/0000-0002-9314-194X					0	10	10	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	1992	8	1					53	69						17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JF613					2022-12-18	WOS:A1992JF61300003
J	HUNG, YP; COOPER, DB; CERNUSCHIFRIAS, B				HUNG, YP; COOPER, DB; CERNUSCHIFRIAS, B			ASYMPTOTIC BAYESIAN SURFACE ESTIMATION USING AN IMAGE SEQUENCE	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							RANDOM-FIELDS; STEREO; RELAXATION	A new approach is introduced to estimating object surfaces in three-dimensional space from a sequence of images. A 3D surface of interest here is modeled as a function known up to the values of a few parameters. Surface estimation is then treated as the general problem of maximum-likelihood parameter estimation based on two or more functionally related data sets. In our case, these data sets constitute a sequence of images taken at different locations and orientations. Experiments are run to illustrate the various advantages of using as many images as possible in the estimation and of distributing camera positions from first to last over as large a baseline as possible. In order to extract all the usable information from the sequence of images, all the images should be available simultaneously for the parameter estimation. We introduce the use of asymptotic Bayesian approximations in order to summarize the useful information in a sequence of images, thereby drastically reducing both the storage and the amount of processing required. This leads to a sequential Bayesian estimator for the surface parameters, where the information extracted from previous images is summarized in a quadratic form. The attractiveness of our approach is that now all the usual tools of statistical signal analysis, for example, statistical decision theory for object recognition, can be brought to bear; the information extraction appears to be robust and computationally reasonable; the concepts are geometric and simple; and essentially optimal accuracy should result. Experimental results are shown for extending this approach in two ways. One is to model a highly variable surface as a collection of small patches jointly constituting a stochastic process (e.g., a Markov random field) and to reconstruct this surface using maximum a posteriori probability (MAP) estimation. The other is to cluster together those patches constituting the same primitive object through the use of MAP segmentation. This provides a simultaneous estimation and segmentation of a surface into primitive constituent surfaces.	ACAD SINICA, INST INFORMAT SCI, TAIPEI 115, TAIWAN; BROWN UNIV, DIV ENGN, ENGN MAN MACHINE SYST LAB, PROVIDENCE, RI 02912 USA; UNIV BUENOS AIRES, FAC INGN, BUENOS AIRES, ARGENTINA	Academia Sinica - Taiwan; Brown University; University of Buenos Aires			Cernuschi-Frias, Bruno/G-9177-2012	Cernuschi-Frias, Bruno/0000-0001-5335-9402				AMBLARD FG, 1986, OCT P SPIE CAMBR S O; AYACHE N, 1987, INT S ROBOTICS RES, P4; BARNARD ST, 1982, ACM COMPUT SURV, V14, P553, DOI DOI 10.1145/356893.356896; BESL PJ, 1986, COMPUT VISION GRAPH, V33, P33, DOI 10.1016/0734-189X(86)90220-3; BOLLE RM, 1986, IEEE T PATTERN ANAL, V8, P619, DOI 10.1109/TPAMI.1986.4767836; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; BOULT TE, 1988, 1988 P IEEE C COMP V, P177; BROIDA TJ, 1986, MAY P IEEE WORKSH MO, P95; CERNUSCHIFRIAS B, 1989, IEEE T PATTERN ANAL, V11, P1028, DOI 10.1109/34.42835; CERNUSCHIFRIAS B, 1985, JUN P IEEE COMP SOC, P167; COHEN FS, 1987, IEEE T PATTERN ANAL, V9, P195, DOI 10.1109/TPAMI.1987.4767895; COHEN FS, 1983, 3RD P INT C ROB VIS, V449, P17; Cooper D. B., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P74, DOI 10.1109/CCV.1988.589973; Dereniak E.L., 1984, OPTICAL RAD DETECTOR; DERIN H, 1987, IEEE T PATTERN ANAL, V9, P39, DOI 10.1109/TPAMI.1987.4767871; DUDA D, 1973, PATTERN CLASSIFICATI, P381; FAUGERAS OD, 1986, APR P IEEE C ROB AUT, P1433; FAUGERAS OD, 1987, 1ST P INT C COMP VIS, P25; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GRIMSON WEL, 1981, IMAGES SURFACES; HOFF W, 1987, 1ST P INT C COMP VIS, P284; Horn B., 1986, ROBOT VISION, P1; HUNG YP, 1989, LEMS63 BROWN U DIV E; HUNG YP, 1990, IN PRESS FEB P SPIE; KASS M, 1987, INT J COMPUT VISION, V1, P357, DOI 10.1007/BF00133572; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MATTHIES L, 1988, 1988 P IEEE C COMP V, P366; MEDIONI G, 1985, COMPUT VISION GRAPH, V31, P2, DOI 10.1016/S0734-189X(85)80073-6; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; SUBRAHMONIA J, 1989, LEMS64 BROWN U DIV E; Szeliski R., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P207, DOI 10.1109/CCV.1988.589992; XU G, 1985, 9TH P INT JOINT C AR, P892	32	10	11	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1991	6	2					105	132		10.1007/BF00128152	http://dx.doi.org/10.1007/BF00128152			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW779					2022-12-18	WOS:A1991FW77900002
J	PONCE, J				PONCE, J			STRAIGHT HOMOGENEOUS GENERALIZED CYLINDERS - DIFFERENTIAL GEOMETRY AND UNIQUENESS RESULTS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									STANFORD UNIV,ROBOT LAB,STANFORD,CA 94305	Stanford University								AGIN GJ, 1972, AIM273 STANF AI LAB; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BINFORD TO, 1987, P WORKSHOP UNCERTAIN; BINFORD TO, 1971, DEC P IEEE C SYST CO; Bolles R. C., 1982, INT J ROBOT RES, V1, P57; BRADY JM, 1985, 2ND P INT S ROB RES; BRADY M, 1983, HUMAN MACHINE VISION; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; Do Carmo M.P., 2016, DIFFERENTIAL GEOMETR, Vsecond; FAUGERAS OD, 1983, 8TH P INT JOINT C AR, P996; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; HOLLERBACH JM, 1975, MIT AI TR346 TECHN R; HORAUD R, 1987, IEEE T PATTERN ANAL, V9, P401, DOI 10.1109/TPAMI.1987.4767922; HORAUD R, 1987, 1ST P INT C COMP VIS; HUTTENLOCHER DP, 1987, 1ST P INT C COMP VIS; Koenderink J. J., 1984, PERCEPTION, V13; KRIEGMAN DJ, 1988, APR P IEEE C ROB AUT; LOWE D, 1987, INT J COMPUT VISION, V1; MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020; MARR D, 1977, PROC R SOC SER B-BIO, V197, P441, DOI 10.1098/rspb.1977.0080; NEVATIA R, 1977, ARTIF INTELL, V8, P77, DOI 10.1016/0004-3702(77)90006-6; NEVATIA R, 1982, MACHINE PERCEPTION; PONCE J, 1989, IEEE T PATTERN ANAL, V11, P951, DOI 10.1109/34.35498; PONCE J, 1987, INT J COMPUT VISION, V1; PONCE J, 1988, APR P DARPA IM UND W; PONCE J, 1987, DEC P IEEE WORKSH CO; PONCE J, 1988, JUN P IEEE C COMP VI; RAO K, 1987, FEB P DARPA IM UND W; RAO K, 1988, JUN P IEEE C COMP VI; ROBERTS KR, 1985, DEC P DARPA IM UND W; RUBINSTEIN Z, 1969, COURSE ORDINARY DIFF; Rudin W., 1976, PRINCIPLES MATH ANAL, V3; Shafer S. A., 1985, SHADOWS SILHOUETTES; SHANI U, 1984, COMPUT VISION GRAPH, V27, P129, DOI 10.1016/S0734-189X(84)80039-0	34	10	10	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	1990	4	1					79	100		10.1007/BF00137444	http://dx.doi.org/10.1007/BF00137444			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CU085					2022-12-18	WOS:A1990CU08500004
J	Xu, CF; Liang, DK; Xu, YC; Bai, S; Zhan, W; Bai, X; Tomizuka, M				Xu, Chenfeng; Liang, Dingkang; Xu, Yongchao; Bai, Song; Zhan, Wei; Bai, Xiang; Tomizuka, Masayoshi			AutoScale: Learning to Scale for Crowd Counting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Crowd counting; Density map; Long-tailed distribution; Learn to scale; Person localization; Dynamic cross-entropy		Recent works on crowd counting mainly leverage Convolutional Neural Networks (CNNs) to count by regressing density maps, and have achieved great progress. In the density map, each person is represented by a Gaussian blob, and the final count is obtained from the integration of the whole map. However, it is difficult to accurately predict the density map on dense regions. A major issue is that the density map on dense regions usually accumulates density values from a number of nearby Gaussian blobs, yielding different large density values on a small set of pixels. This makes the density map present variant patterns with significant pattern shifts and brings a long-tailed distribution of pixel-wise density values. In this paper, we aim to address such issue in the density map. Specifically, we propose a simple and effective Learning to Scale (L2S) module, which automatically scales dense regions into reasonable closeness levels (reflecting image-plane distance between neighboring people). L2S directly normalizes the closeness in different patches such that it dynamically separates the overlapped blobs, decomposes the accumulated values in the ground-truth density map, and thus alleviates the pattern shifts and long-tailed distribution of density values. This helps the model to better learn the density map. We also explore the effectiveness of L2S in localizing people by finding the local minima of the quantized distance (w.rt. person location map), which has a similar issue as density map regression. To the best of our knowledge, such localization method is also novel in localization-based crowd counting. We further introduce a customized dynamic cross-entropy loss, significantly improving the localization-based model optimization. Extensive experiments demonstrate that the proposed framework termed AutoScale improves upon some state-of-the-art methods in both regression and localization benchmarks on three crowded datasets and achieves very competitive performance on two sparse datasets. An implementation of our method is available at https://github.com/dk-liang/AutoScale.git.	[Xu, Chenfeng; Zhan, Wei; Tomizuka, Masayoshi] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Liang, Dingkang; Bai, Xiang] Huazhong Univ Sci & Technol, Wuhan, Peoples R China; [Xu, Yongchao] Wuhan Univ, Wuhan, Peoples R China; [Bai, Song] ByteDance, Oxford, England; [Bai, Song] Univ Oxford, Oxford, England	University of California System; University of California Berkeley; Huazhong University of Science & Technology; Wuhan University; University of Oxford	Xu, YC (corresponding author), Wuhan Univ, Wuhan, Peoples R China.	xuchenfeng@berkeley.edu; dkliang@hust.edu.cn; yongchao.xu@whu.edu.cn; songbai.site@gmail.com; wzhan@berkeley.edu; xbai@hust.edu.cn; tomizuka@berkeley.edu			National Key Research and Development Program of China [2018AAA0100400]; National Natural Science Foundation of China [61936003, 62176186]; Young Elite Scientists Sponsorship Program by CAST; National Program for Support of Top-NotchYoung Professionals; Program for HUST Academic Frontier Youth Team	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Young Elite Scientists Sponsorship Program by CAST; National Program for Support of Top-NotchYoung Professionals; Program for HUST Academic Frontier Youth Team	The work of Yongchao Xu was supported by the National Key Research and Development Program of China (2018AAA0100400), the NationalNatural Science Foundation of China (61936003 and 62176186), and in part by the Young Elite Scientists Sponsorship Program by CAST. The work of Xiang Bai was supported by theNational Program for Support of Top-NotchYoung Professionals and in part by the Program for HUST Academic Frontier Youth Team.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arteta C, 2016, LECT NOTES COMPUT SC, V9911, P483, DOI 10.1007/978-3-319-46478-7_30; Arteta C, 2014, LECT NOTES COMPUT SC, V8691, P504, DOI 10.1007/978-3-319-10578-9_33; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465; Baxes G.A., 1994, DIGITAL IMAGE PROCES; Brostow G.J., 2006, P IEEE INT C COMP VI, P594, DOI DOI 10.1109/CVPR.2006.320; Cao KD, 2019, ADV NEUR IN, V32; Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21; Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625; Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432; Dong Q, 2017, IEEE I CONF COMP VIS, P1869, DOI 10.1109/ICCV.2017.205; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Gao Junyu, 2019, ARXIV PREPRINT ARXIV; Ge Weina, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2913, DOI 10.1109/CVPRW.2009.5206621; Geng CX, 2021, IEEE T PATTERN ANAL, V43, P3614, DOI 10.1109/TPAMI.2020.2981604; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Guerrero-Gomez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48; Ha David, 2016, ARXIV160909106; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hossain MA, 2019, IEEE WINT CONF APPL, P1280, DOI 10.1109/WACV.2019.00141; Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166; Hu Y., 2020, P EUR C COMP VIS SPR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33; Idrees H, 2015, IEEE T PATTERN ANAL, V37, P1986, DOI 10.1109/TPAMI.2015.2396051; Jaderberg M, 2015, ADV NEUR IN, V28; Jiang S., 2019, IEEE T CIRC SYST VID; Jiang X., 2019, IEEE T NEUR NET LEAR; Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476; Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629; Kang D., 2018, P BMVC, P89; Kang D, 2019, IEEE T CIRC SYST VID, V29, P1408, DOI 10.1109/TCSVT.2018.2837153; Kingma D.P, P 3 INT C LEARNING R; Laradji IH, 2018, LECT NOTES COMPUT SC, V11206, P560, DOI 10.1007/978-3-030-01216-8_34; Li B, 2015, IEEE I CONF COMP VIS, P4175, DOI 10.1109/ICCV.2015.475; Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120; Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545; Liu L., 2020, WEIGHING COUNTS SEQU; LIU LB, 2018, CROWD COUNTING USING, P849; Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186; Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334; Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524; Liu X., 2020, P EUR C COMP VIS SPR; Liu XL, 2019, IEEE T PATTERN ANAL, V41, P1862, DOI 10.1109/TPAMI.2019.2899857; Liu XL, 2018, PROC CVPR IEEE, P7661, DOI 10.1109/CVPR.2018.00799; Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663; Luo A, 2020, AAAI CONF ARTIF INTE, V34, P11693; Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624; Miao YQ, 2020, AAAI CONF ARTIF INTE, V34, P11765; Najibi M, 2019, IEEE I CONF COMP VIS, P9744, DOI 10.1109/ICCV.2019.00984; Oh M. H., 2020, P AAAI C ART INT; Olmschenk G., 2019, ARXIV PREPRINT ARXIV; Onoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38; Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100; Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17; Recasens A, 2018, LECT NOTES COMPUT SC, V11213, P52, DOI 10.1007/978-3-030-01240-3_4; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ribera J., 2019, P IEEE INT C COMP VI; Sajid U, 2020, IEEE T CIRC SYST VID, V30, P3499, DOI 10.1109/TCSVT.2020.2978717; Salakhutdinov R, 2011, PROC CVPR IEEE, P1481, DOI 10.1109/CVPR.2011.5995720; Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830; Sam DB, 2018, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2018.00381; Shi ML, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON INDUSTRIAL ARTIFICIAL INTELLIGENCE (IAI 2019); Shi ZL, 2019, IEEE I CONF COMP VIS, P4199, DOI 10.1109/ICCV.2019.00430; Shi ZL, 2018, PROC CVPR IEEE, P5382, DOI 10.1109/CVPR.2018.00564; Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS); Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969; Sindagi VA, 2019, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2019.00131; Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109; Sindagi VA, 2020, IEEE T IMAGE PROCESS, V29, P323, DOI 10.1109/TIP.2019.2928634; Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206; Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007; Singh B, 2018, ADV NEUR IN, V31; Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Tian Yukun, 2019, IEEE T IMAGE PROCESS, V2, P3; Tsong-Yi Chen, 2010, Proceedings of the 2010 Fourth International Conference on Genetic and Evolutionary Computing (ICGEC 2010), P699, DOI 10.1109/ICGEC.2010.178; Van Horn Grant, 2017, ARXIV170901450; Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8; Wan J, 2019, IEEE I CONF COMP VIS, P1130, DOI 10.1109/ICCV.2019.00122; Wan J, 2019, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2019.00416; Wang M, 2011, PROC CVPR IEEE; Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839; Wang Qi, 2020, ARXIV200103360; Wang YC, 2017, ADV NEUR IN, V30; Xiong HP, 2019, IEEE I CONF COMP VIS, P8361, DOI 10.1109/ICCV.2019.00845; Xu C., 2019, P IEEE INT C COMP VI; Yan ZY, 2019, IEEE I CONF COMP VIS, P952, DOI 10.1109/ICCV.2019.00104; Yang YF, 2020, PROC CVPR IEEE, P4373, DOI 10.1109/CVPR42600.2020.00443; Zhang AR, 2019, IEEE I CONF COMP VIS, P6787, DOI 10.1109/ICCV.2019.00689; Zhang A, 2019, IEEE I CONF COMP VIS, P5713, DOI 10.1109/ICCV.2019.00581; Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684; Zhang L, 2021, IEEE T PATTERN ANAL, V43, P982, DOI 10.1109/TPAMI.2019.2943860; ZHANG Q, 2020, P AAAI C ART INT; Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849; Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578; Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70; Zhao MM, 2019, PROC CVPR IEEE, P12728, DOI 10.1109/CVPR.2019.01302; Zhao X, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P67, DOI 10.1109/AVSS.2009.45; Zhao ZY, 2016, LECT NOTES COMPUT SC, V9912, P712, DOI 10.1007/978-3-319-46484-8_43; Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557; Zhu XX, 2016, INT J COMPUT VISION, V119, P76, DOI 10.1007/s11263-015-0812-2; Zhu XX, 2014, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2014.122	114	9	9	11	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2022	130	2					405	434		10.1007/s11263-021-01542-z	http://dx.doi.org/10.1007/s11263-021-01542-z		JAN 2022	30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZB5ZS					2022-12-18	WOS:000738560100002
J	Ye, HJ; Hu, HX; Zhan, DC				Ye, Han-Jia; Hu, Hexiang; Zhan, De-Chuan			Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image recognition; Meta learning; Generalized few-shot learning; Few-shot learning; Recognition with heterogeneous visual domain		Object recognition in the real-world requires handling long-tailed or even open-ended data. An ideal visual system needs to recognize the populated head visual concepts reliably and meanwhile efficiently learn about emerging new tail categories with a few training instances. Class-balanced many-shot learning and few-shot learning tackle one side of this problem, by either learning strong classifiers for head or learning to learn few-shot classifiers for the tail. In this paper, we investigate the problem of generalized few-shot learning (GFSL)-a model during the deployment is required to learn about tail categories with few shots and simultaneously classify the head classes. We propose the ClAssifier SynThesis LEarning (Castle), a learning framework that learns how to synthesize calibrated few-shot classifiers in addition to the multi-class classifiers of head classes with a shared neural dictionary, shedding light upon the inductive GFSL. Furthermore, we propose an adaptive version of Castle (aCastle) that adapts the head classifiers conditioned on the incoming tail training examples, yielding a framework that allows effective backward knowledge transfer. As a consequence, aCastle can handle GFSL with classes from heterogeneous domains effectively. Castle and aCastle demonstrate superior performances than existing GFSL algorithms and strong baselines on MiniImageNet as well as TieredImageNet datasets. More interestingly, they outperform previous state-of-the-art methods when evaluated with standard few-shot learning criteria.	[Ye, Han-Jia; Zhan, De-Chuan] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China; [Hu, Hexiang] Univ Southern Calif, Los Angeles, CA 90007 USA	Nanjing University; University of Southern California	Ye, HJ (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.	yehj@lamda.nju.edu.cn; hexiangh@usc.edu; zhandc@lamda.nju.edu.cn	Hu, Hexiang/GNW-4536-2022		NSFC-NRF [61861146001, 61773198, 61751306, 61632004, 62006112]; NSF [IIS-1513966/ 1632803/1833137, CCF-1139148]; DARPA [FA8750-18-2-0117]; DARPA-D3M [UCB-00009528]; ARO [W911NF-12-1-0241, W911NF-15-1-0484]; Google	NSFC-NRF; NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA-D3M; ARO; Google(Google Incorporated)	Thanks to Fei Sha for valuable discussions. This research (61773198, 61751306, 61632004, 62006112), NSFC-NRF Joint Research Project under Grant 61861146001, NSF Awards IIS-1513966/ 1632803/1833137, CCF-1139148, DARPA Award#: FA8750-18-2-0117, DARPA-D3M - Award UCB-00009528, Google Research Awards, gifts from Facebook and Netflix, and ARO# W911NF-12-1-0241 and W911NF-15-1-0484.	Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; [Anonymous], 2018, ARXIV180302999 CORR; [Anonymous], 2017, ARXIV170709835 CORR; Ba L.J., 2016, ARXIV160706450 CORR; Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Chen W.-Y., 2019, EUR J INORG CHEM; Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949; Das D, 2020, IEEE T IMAGE PROCESS, V29, P3336, DOI 10.1109/TIP.2019.2959254; den Oord A., 2018, P 6 INT C LEARN REPR; Dong N, 2018, JOINT EUROPEAN C MAC, P573; Finn C, 2017, PR MACH LEARN RES, V70; Gao H., 2018, ADV NEURAL INFORM PR, V31, P983; Ghiasi G., 2018, ADV NEURAL INFORM PR, P10750; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Guo CA, 2017, PR MACH LEARN RES, V70; Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328; Hinton G.E., 2015, ARXIV150302531 CORR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kang B., 2020, P 8 INT C LEARN REPR; Kang BY, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P177; Khosla Aditya, 2011, P C COMP VIS PATT RE; Koch Gregory, 2015, P ICML DEEP LEARN WO, V2; Krause J., 2013, 4 INT IEEE WORKSH 3D; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Larochelle H., 2018, FEW SHOT LEARNING ME; Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091; Lee Y, 2018, PR MACH LEARN RES, V80; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li HY, 2019, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2019.00009; Li X., 2019, ADV NEURAL INFORM PR, V32, P10276; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Lifchitz Y, 2019, PROC CVPR IEEE, P9250, DOI 10.1109/CVPR.2019.00948; Liu Y., 2020, CVPR, P12245; Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264; Lopez-Paz D., 2017, ADV NEURAL INF PROCE, P6467; Ma, 2019, ADV NEURAL INFORM PR, V32, P1565; Maji S., 2013, ARXIV13065151 CORR; Oreshkin Boris N, 2018, ADV NEURAL INFORM PR; Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Ravi Sachin, 2017, P 5 INT C LEARN REPR; Ren M., 2018, P 6 INT C LEARN REPR; Ren M., 2019, ADV NEURAL INFORM PR, P5276; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Rusu AA, 2019, INT C LEARN REPR; Schonfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844; Simonyan K., 2014, P 3 INT C LEARN REPR; Snell J., 2017, ADV NEURAL INFORM PR, P4080; Storkey, 2019, P 7 INT C LEARN REPR; Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049; Torr P H S, 2019, P 7 INT C LEARN REPR; Triantafillou E., 2017, ADV NEURAL INFORM PR, V30, P2255; vanderMaaten L, 2019, ARXIV191104623 CORR; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Vinyals Oriol, 2016, ARXIV160604080, P3630; Vuorio R., 2019, P NEURIPS, P1; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wang T., 2018, ARXIV181110959 CORR; Wang YC, 2017, ADV NEUR IN, V30; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328; Xu, 2020, P 8 INT C LEARN REPR; Ye Han-Jia, 2020, ARXIV200101385, P2; Ye HJ., 2020, FEW SHOT LEARNING VI; Yoon S. W., 2019, P INT C MACH LEARN, P7115; Zhou B., 2020, P IEEE C COMP VIS PA, P9719	72	9	10	16	54	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1930	1953		10.1007/s11263-020-01381-4	http://dx.doi.org/10.1007/s11263-020-01381-4		APR 2021	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU		Green Submitted			2022-12-18	WOS:000641238700003
J	Fu, XY; Qi, Q; Zha, ZJ; Ding, XH; Wu, F; Paisley, J				Fu, Xueyang; Qi, Qi; Zha, Zheng-Jun; Ding, Xinghao; Wu, Feng; Paisley, John			Successive Graph Convolutional Network for Image De-raining	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image de-raining; Graph convolutional networks; Deep learning; Image processing	REMOVAL; VISION	Deep convolutional neural networks (CNNs) have shown their advantages in the single image de-raining task. However, most existing CNNs-based methods utilize only local spatial information without considering long-range contextual information. In this paper, we propose a graph convolutional networks (GCNs)-based model to solve the above problem. We specifically design two graphs to extract representations from new dimensions. The first graph models the global spatial relationship between pixels in the feature, while the second graph models the interrelationship across the channels. By integrating conventional CNNs and our GCNs into a single framework, the proposed method is able to explore comprehensive feature representations from three aspects, i.e., local spatial patterns, global spatial coherence and channel correlation. To better exploit the explored rich feature representations, we further introduce a simple yet effective recurrent operations to perform the de-raining process in a successive manner. Benefiting from the rich information exploration and exploitation, our method achieves state-of-the-art results on both synthetic and real-world data sets.	[Fu, Xueyang; Zha, Zheng-Jun; Wu, Feng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Peoples R China; [Qi, Qi; Ding, Xinghao] Xiamen Univ, Sch Informat, Xiamen, Peoples R China; [Paisley, John] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA; [Paisley, John] Columbia Univ, Data Sci Inst, New York, NY USA	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Xiamen University; Columbia University; Columbia University	Zha, ZJ (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Peoples R China.	zhazj@ustc.edu.cn	Fu, Xueyang/AAA-4940-2021; Ding, Xinghao/ABD-7846-2021	Fu, Xueyang/0000-0001-8036-4071; Ding, Xinghao/0000-0003-2288-5287	National Key R&D Program of China [2020AAA0105702]; National Natural Science Foundation of China (NSFC) [U19B2038, 61620106009, 61901433]; USTC Research Funds of the Double First-Class Initiative [YD2100002003]	National Key R&D Program of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); USTC Research Funds of the Double First-Class Initiative	This work was supported by the National Key R&D Program of China under Grant 2020AAA0105702, the National Natural Science Foundation of China (NSFC) under Grants U19B2038, 61620106009 and 61901433, the USTC Research Funds of the Double First-Class Initiative under Grant YD2100002003.	Abadi Martin, 2016, arXiv; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2; Bossu J, 2011, INT J COMPUT VISION, V93, P348, DOI 10.1007/s11263-011-0421-7; Cao J, 2020, INT J COMPUT VISION, V128, P1485, DOI 10.1007/s11263-019-01229-6; Chang Y, 2017, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2017.191; CHEN C, 2019, IEEE T PATTERN ANAL; Chen J, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2013.2290595; Chen JW, 2018, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR.2018.00333; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247; Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Chen YP, 2018, ADV NEUR IN, V31; Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Fan QN, 2018, LECT NOTES COMPUT SC, V11217, P455, DOI 10.1007/978-3-030-01261-8_27; Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481; Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186; Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802; Garg K, 2005, IEEE I CONF COMP VIS, P1067; GARG K, 2004, PROC CVPR IEEE, P528, DOI DOI 10.1109/CVPR.2004.1315077; Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167; Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759; Jiang TX, 2019, IEEE T IMAGE PROCESS, V28, P2089, DOI 10.1109/TIP.2018.2880512; Jiang TX, 2017, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2017.301; Jin XL, 2021, IET IMAGE PROCESS, V15, P1273, DOI 10.1049/ipr2.12103; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057; Kim JH, 2015, IEEE T IMAGE PROCESS, V24, P2658, DOI 10.1109/TIP.2015.2428933; Kim JH, 2013, IEEE IMAGE PROC, P914, DOI 10.1109/ICIP.2013.6738189; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837; Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636; Li JW, 2018, INT J COMPUT VISION, V126, P855, DOI 10.1007/s11263-018-1075-5; Li LRH, 2019, INT J COMPUT VISION, V127, P1025, DOI 10.1007/s11263-018-01146-0; Li MH, 2018, PROC CVPR IEEE, P6644, DOI 10.1109/CVPR.2018.00695; Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173; Li RT, 2019, IEEE I CONF COMP VIS, P7303, DOI 10.1109/ICCV.2019.00740; Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077; Li X, 2018, LECT NOTES COMPUT SC, V11206, P287, DOI [10.1007/978-3-030-01216-8_18, 10.1007/978-3-030-01267-0_22]; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; Liu JY, 2018, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2018.00341; Liu JY, 2019, IEEE T IMAGE PROCESS, V28, P699, DOI 10.1109/TIP.2018.2869722; Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388; Mordan T, 2019, INT J COMPUT VISION, V127, P1659, DOI 10.1007/s11263-018-1109-z; Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723; Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556; Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263; Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406; REN W, 2019, PROC CVPR IEEE; Ren WH, 2017, PROC CVPR IEEE, P2838, DOI 10.1109/CVPR.2017.303; Romano Y, 2015, SIAM J IMAGING SCI, V8, P1187, DOI 10.1137/140990978; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Santhaseelan V, 2015, INT J COMPUT VISION, V112, P71, DOI 10.1007/s11263-014-0759-8; Shanavas N, 2021, INT J MACH LEARN CYB, V12, P1067, DOI 10.1007/s13042-020-01221-4; Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288; Shi XJ, 2015, ADV NEUR IN, V28; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Wang CS, 2020, IEEE ACCESS, V8, P9488, DOI 10.1109/ACCESS.2020.2964271; Wang GQ, 2019, IEEE I CONF COMP VIS, P5643, DOI 10.1109/ICCV.2019.00574; Wang HX, 2018, INT J COMPUT VISION, V126, P1288, DOI 10.1007/s11263-018-1105-3; Wang H, 2020, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR42600.2020.00317; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255; Wang YL, 2017, IEEE T IMAGE PROCESS, V26, P3936, DOI 10.1109/TIP.2017.2708502; Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400; Wei W, 2017, IEEE I CONF COMP VIS, P2535, DOI 10.1109/ICCV.2017.275; Wojna Z, 2019, INT J COMPUT VISION, V127, P1694, DOI 10.1007/s11263-019-01170-8; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Yang WH, 2021, IEEE T PATTERN ANAL, V43, P4059, DOI 10.1109/TPAMI.2020.2995190; Yang WH, 2019, IEEE T IMAGE PROCESS, V28, P2948, DOI 10.1109/TIP.2019.2892685; Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183; Yasarla R, 2020, IEEE T IMAGE PROCESS, V29, P4544, DOI 10.1109/TIP.2020.2973802; Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860; Yim C, 2011, IEEE T IMAGE PROCESS, V20, P88, DOI 10.1109/TIP.2010.2061859; Yu F., 2016, P ICLR 2016; Zhang HY, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3277958; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhang H, 2019, INT J COMPUT VISION, V127, P845, DOI 10.1007/s11263-019-01175-3; Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079; Zhang H, 2017, IEEE T PATTERN ANAL, V39, P1690, DOI 10.1109/TPAMI.2016.2613924; Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145; Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z; Zheng X, 2013, 2013 2ND INTERNATIONAL CONFERENCE ON ELECTRIC POWER EQUIPMENT - SWITCHING TECHNOLOGY (ICEPE-ST); Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276	102	9	10	3	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1691	1711		10.1007/s11263-020-01428-6	http://dx.doi.org/10.1007/s11263-020-01428-6		MAR 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC					2022-12-18	WOS:000624439800003
J	Feng, W; Yin, F; Zhang, XY; He, WH; Liu, CL				Feng, Wei; Yin, Fei; Zhang, Xu-Yao; He, Wenhao; Liu, Cheng-Lin			Residual Dual Scale Scene Text Spotting by Fusing Bottom-Up and Top-Down Processing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene text spotting; Arbitrary shapes; Bottom-up; Top-down; Residual dual scale		Existing methods for arbitrary shaped text spotting can be divided into two categories: bottom-up methods detect and recognize local areas of text, and then group them into text lines or words; top-down methods detect text regions of interest, then apply polygon fitting and text recognition to the detected regions. In this paper, we analyze the advantages and disadvantages of these two methods, and propose a novel text spotter by fusing bottom-up and top-down processing. To detect text of arbitrary shapes, we employ a bottom-up detector to describe text with a series of rotated squares, and design a top-down detector to represent the region of interest with a minimum enclosing rotated rectangle. Then the text boundary is determined by fusing the outputs of two detectors. To connect arbitrary shaped text detection and recognition, we propose a differentiable operator named RoISlide, which can extract features for arbitrary text regions from whole image feature maps. Based on the extracted features through RoISlide, a CNN and CTC based text recognizer is introduced to make the framework free from character-level annotations. To improve the robustness against scale variance, we further propose a residual dual scale spotting mechanism, where two spotters work on different feature levels, and the high-level spotter is based on residuals of the low-level spotter. Our method has achieved state-of-the-art performance on four English datasets and one Chinese dataset, including both arbitrary shaped and oriented texts. We also provide abundant ablation experiments to analyze how the key components affect the performance.	[Feng, Wei; Yin, Fei; Zhang, Xu-Yao; Liu, Cheng-Lin] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Feng, Wei; Yin, Fei; Zhang, Xu-Yao; Liu, Cheng-Lin] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China; [He, Wenhao] Tencent Map Big Data Lab, Beijing 100193, Peoples R China; [Liu, Cheng-Lin] CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing 100190, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Liu, CL (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Liu, CL (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; Liu, CL (corresponding author), CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing 100190, Peoples R China.	wei.feng@nlpr.ia.ac.cn; fyin@nlpr.ia.ac.cn; xyz@nlpr.ia.ac.cn; angelicohe@ten-cent.com; liucl@nlpr.ia.ac.cn		Feng, Wei/0000-0003-3711-6333; Liu, Cheng-Lin/0000-0002-6743-4175	Major Project for New Generation AI [2018AAA0100400]; National Natural Science Foundation of China [61733007, 61721004]; Key Research Program of Frontier Sciences of CAS [ZDBS-LY-7004]; Youth Innovation Promotion Association of CAS [2019141]	Major Project for New Generation AI; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Research Program of Frontier Sciences of CAS; Youth Innovation Promotion Association of CAS	This work was supported by the Major Project for New Generation AI (Grant No. 2018AAA0100400), the National Natural Science Foundation of China (Grant Nos. 61733007, 61721004), the Key Research Program of Frontier Sciences of CAS under Grant ZDBS-LY-7004, and the Youth Innovation Promotion Association of CAS under Grant 2019141.	Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], EYE MOV READ WIK FRE; Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102; Busta M, 2017, IEEE I CONF COMP VIS, P2223, DOI 10.1109/ICCV.2017.242; Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157; Cheng Zhanzhan, 2017, ARXIV171104226; Feng W., 2019, P IEEE INT C COMP VI; Gomez L, 2017, PATTERN RECOGN, V70, P60, DOI 10.1016/j.patcog.2017.04.027; Graves A., 2006, P 23 INT C MACH LEAR, P369; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331; He P, 2016, AAAI CONF ARTIF INTE, P3501; He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527; He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588; He WH, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107026; He WH, 2018, IEEE T IMAGE PROCESS, V27, P5406, DOI 10.1109/TIP.2018.2855399; He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529; Huang L., 2015, ARXIV150904874; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li H, 2017, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2017.560; Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086; Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Liao MH, 2017, AAAI CONF ARTIF INTE, P4161; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595; Liu YL, 2017, PROC CVPR IEEE, P3454, DOI 10.1109/CVPR.2017.368; Liu Yuliang, 2017, ARXIV171202170; Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2; Lyu P., 2018, P EUR C COMP VIS; Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788; Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127; Neumann L, 2016, IEEE T PATTERN ANAL, V38, P1872, DOI 10.1109/TPAMI.2015.2496234; Park J, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303963; Patel Y., 2018, ARXIV180109919; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939; Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tian SX, 2015, IEEE I CONF COMP VIS, P4651, DOI 10.1109/ICCV.2015.528; Veit Andreas, 2016, ARXIV160107140; Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402; Wang T, 2012, INT C PATT RECOG, P3304; Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154; Wang X, 2019, L@S '19: PROCEEDINGS OF THE SIXTH (2019) ACM CONFERENCE ON LEARNING @ SCALE, DOI 10.1145/3330430.3333614; Yang XP, 2017, DESTECH TRANS COMP, P328; Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765; Yin F., 2017, ARXIV170901727; Yu J, 2016, 2016 IEEE 16TH INTERNATIONAL CONFERENCE ON NANOTECHNOLOGY (IEEE-NANO), P511, DOI 10.1109/NANO.2016.7751526; Zhou X, 2017, IEEE IMAGE PROC, P555; Zhu YY, 2016, FRONT COMPUT SCI-CHI, V10, P19, DOI 10.1007/s11704-015-4488-0	62	9	9	2	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					619	637		10.1007/s11263-020-01388-x	http://dx.doi.org/10.1007/s11263-020-01388-x		OCT 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC					2022-12-18	WOS:000583470700001
J	Zhao, J; Li, JS; Liu, HZ; Yan, SC; Feng, JS				Zhao, Jian; Li, Jianshu; Liu, Hengzhu; Yan, Shuicheng; Feng, Jiashi			Fine-Grained Multi-human Parsing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-human parsing; Benchmark dataset; Nested adversarial learning; Generative Adversarial Networks		Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification, e-commerce, media editing, video surveillance, autonomous driving and virtual reality, etc. To perform well, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as themulti-human parsingtask. In this paper, we first present a new large-scale database "Multi-human Parsing (MHP v2.0)" for algorithm development and evaluation to advance the research on understanding humans in crowded scenes. MHP v2.0 contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels and 16 dense pose key point labels, involving 2-26 persons per image captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, including MHP v1.0, PASCAL-Person-Part and Buffy. NAN serves as a strong baseline to shed light on generic instance-level semantic part prediction and drive the future research on multi-human parsing. With the above innovations and contributions, we have organized the CVPR 2018 Workshop on Visual Understanding of Humans in Crowd Scene (VUHCS 2018) and the Fine-Grained Multi-human Parsing and Pose Estimation Challenge. These contributions together significantly benefit the community. Code and pre-trained models are available at.	[Zhao, Jian; Li, Jianshu; Yan, Shuicheng; Feng, Jiashi] Natl Univ Singapore, Singapore, Singapore; [Zhao, Jian; Liu, Hengzhu] Natl Univ Def Technol, Changsha, Peoples R China; [Yan, Shuicheng] Qihoo 360 AI Inst, Beijing, Peoples R China	National University of Singapore; National University of Defense Technology - China	Zhao, J (corresponding author), Natl Univ Singapore, Singapore, Singapore.; Zhao, J (corresponding author), Natl Univ Def Technol, Changsha, Peoples R China.	zhaojian90@u.nus.edu; jianshu@u.nus.edu; hengzhuliu@nudt.edu.cn; eleyans@nus.edu.sg; elefjia@nus.edu.sg	Yan, Shuicheng/HCI-1431-2022; Feng, Jiashi/AGX-6209-2022	Zhao, Jian/0000-0002-3508-756X				Abadi M, 2015, P 12 USENIX S OPERAT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Chu X, 2015, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2015.383; Collins Robert T, 2000, SYSTEM VIDEO SURVEIL, P1; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; De Brabandere Bert, 2017, ARXIV170802551; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Everingham M., 2012, PASCAL VISUAL OBJECT; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468; Gan C, 2016, AAAI CONF ARTIF INTE, P3487; Girshick R., 2015, ICCV; Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715; Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Jiang H., 2016, ARXIV160403880; KLARE BF, 2015, PROC CVPR IEEE, P1931, DOI DOI 10.1109/CVPR.2015.7298803; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34; Li J, 2017, ARXIV170507206; Li Q., 2017, ARXIV170903612; Liang X., 2015, ARXIV150902636; Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163; Lin  J., 2016, SIGGRAPH ASIA 2016 V, P11, DOI DOI 10.1145/2992138.2992144; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Si, 2017, CVPRW, P1; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Ng AY, 2002, ADV NEUR IN, V14, P849; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471; Turban E, 2002, ELECT COMMERCE MANAG; Vineet Vibhav, 2011, BMVC, V2, P12; Wu Z, 2016, CORR, P1; Xia FT, 2016, LECT NOTES COMPUT SC, V9909, P648, DOI 10.1007/978-3-319-46454-1_39; Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47; Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101; Zhang N, 2015, PROC CVPR IEEE, P4804, DOI 10.1109/CVPR.2015.7299113; Zhang ZP, 2018, INT J COMPUT VISION, V126, P550, DOI 10.1007/s11263-017-1055-1; Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509; Zhao P, 2017, PROCEEDINGS OF 2017 VI INTERNATIONAL CONFERENCE ON NETWORK, COMMUNICATION AND COMPUTING (ICNCC 2017), P7, DOI 10.1145/3171592.3171603; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460	48	9	9	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2185	2203		10.1007/s11263-019-01181-5	http://dx.doi.org/10.1007/s11263-019-01181-5			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG					2022-12-18	WOS:000559365500011
J	Debavelaere, V; Durrleman, S; Allassonniere, S				Debavelaere, Vianney; Durrleman, Stanley; Allassonniere, Stephanie		Alzheimers Dis Neuroimaging Initia	Learning the Clustering of Longitudinal Shape Data Sets into a Mixture of Independent or Branching Trajectories	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Longitudinal data analysis; Mixture model; Branching population; Stochastic optimization; Statistical model; Riemannian manifold	PROGRESSION; MANIFOLDS; MODELS	Given repeated observations of several subjects over time, i.e. a longitudinal data set, this paper introduces a new model to learn a classification of the shapes progression in an unsupervised setting: we automatically cluster a longitudinal data set in different classes without labels. Our method learns for each cluster an average shape trajectory (or representative curve) and its variance in space and time. Representative trajectories are built as the combination of pieces of curves. This mixture model is flexible enough to handle independent trajectories for each cluster as well as fork and merge scenarios. The estimation of such non linear mixture models in high dimension is known to be difficult because of the trapping states effect that hampers the optimisation of cluster assignments during training. We address this issue by using a tempered version of the stochastic EM algorithm. Finally, we apply our algorithm on different data sets. First, synthetic data are used to show that a tempered scheme achieves better convergence. We then apply our method to different real data sets: 1D RECIST score used to monitor tumors growth, 3D facial expressions and meshes of the hippocampus. In particular, we show how the method can be used to test different scenarios of hippocampus atrophy in ageing by using an heteregenous population of normal ageing individuals and mild cognitive impaired subjects.	[Debavelaere, Vianney] Ecole Polytech, Ctr Math Appl, Palaiseau, France; [Durrleman, Stanley] Inst Cerveau & Moelle Epiniere, ARAMIS Lab, 47 Blvd Hop, Paris, France; [Allassonniere, Stephanie] Univ Paris 05, Ctr Rech Cordeliers, Paris, France	Institut Polytechnique de Paris; UDICE-French Research Universities; Sorbonne Universite; Institut National de la Sante et de la Recherche Medicale (Inserm); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	Debavelaere, V (corresponding author), Ecole Polytech, Ctr Math Appl, Palaiseau, France.	vianney.debavelaere@polytechnique.edu		Debavelaere, Vianney/0000-0002-7225-0397	European Research Council [678304]	European Research Council(European Research Council (ERC)European Commission)	Thiswork has been partly funded by the European Research Council with grant 678304.	Abdelkader MF, 2011, COMPUT VIS IMAGE UND, V115, P439, DOI 10.1016/j.cviu.2010.10.006; Allassonniere A., 2019, NEW CLASS ALGORITHMS; Allassonniere S, 2015, SIAM J IMAGING SCI, V8, P1367, DOI 10.1137/140971762; Allassonniere S., 2017, ADV NEURAL INFORM PR, P1152; Allassonniere S, 2010, BERNOULLI, V16, P641, DOI 10.3150/09-BEJ229; Allassonniere S, 2010, ESAIM-PROBAB STAT, V14, P382, DOI 10.1051/ps/2009001; Bone A, 2018, PROC CVPR IEEE, P9271, DOI 10.1109/CVPR.2018.00966; Chakraborty R, 2017, IEEE I CONF COMP VIS, P172, DOI [10.1109/iccv.2017.28, 10.1109/ICCV.2017.28]; Charon N, 2013, SIAM J IMAGING SCI, V6, P2547, DOI 10.1137/130918885; Debavelaere V., 2019, CLUSTERING LONGITUDI; Delyon B, 1999, ANN STAT, V27, P94; Donohue MC, 2014, ALZHEIMERS DEMENT, V10, pS400, DOI 10.1016/j.jalz.2013.10.003; Durrleman S, 2013, INT J COMPUT VISION, V101, P161, DOI 10.1007/s11263-012-0556-1; Fletcher PT, 2013, INT J COMPUT VISION, V105, P171, DOI 10.1007/s11263-012-0591-y; Hong Yi, 2015, Inf Process Med Imaging, V24, P139, DOI 10.1007/978-3-319-19992-4_11; Jedynak BM, 2012, NEUROIMAGE, V63, P1478, DOI 10.1016/j.neuroimage.2012.07.059; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kim HJ, 2017, PROC CVPR IEEE, P5777, DOI 10.1109/CVPR.2017.612; Lorenzen P, 2005, LECT NOTES COMPUT SC, V3750, P411, DOI 10.1007/11566489_51; Lorenzi M, 2011, LECT NOTES COMPUT SC, V6801, P463, DOI 10.1007/978-3-642-22092-0_38; Louis M, 2017, LECT NOTES COMPUT SC, V10589, P29, DOI 10.1007/978-3-319-68445-1_4; Miller MI, 2006, J MATH IMAGING VIS, V24, P209, DOI 10.1007/s10851-005-3624-0; Muralidharan P, 2012, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2012.6247780; Schiratti J-B, 2015, ADV NEURAL INFORM PR, P2404; Schiratti JB, 2017, J MACH LEARN RES, V18; Singh N, 2016, INT J COMPUT VISION, V117, P70, DOI 10.1007/s11263-015-0849-2; Srivastava A, 2005, IEEE T PATTERN ANAL, V27, P590, DOI 10.1109/TPAMI.2005.86; Su JY, 2014, ANN APPL STAT, V8, P530, DOI 10.1214/13-AOAS701; Therasse P, 2000, J NATL CANCER I, V92, P205, DOI 10.1093/jnci/92.3.205; Vaillant M, 2005, LECT NOTES COMPUT SC, V3565, P381; Vercauteren T, 2009, NEUROIMAGE, V45, pS61, DOI 10.1016/j.neuroimage.2008.10.040; Yin L., 2008, IEEE INT C AUT FAC G, V126	32	9	9	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2794	2809		10.1007/s11263-020-01337-8	http://dx.doi.org/10.1007/s11263-020-01337-8		JUN 2020	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ		Green Submitted			2022-12-18	WOS:000537398300002
J	Hu, SX; Lee, GH				Hu, Sixing; Lee, Gim Hee			Image-Based Geo-Localization Using Satellite Imagery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Geo-localization; Markov localization; Cross-view localization; Convolutional Neural Network; NetVLAD		The problem of localization on a geo-referenced satellite map given a query ground view image is useful yet remains challenging due to the drastic change in viewpoint. To this end, in this paper we work on the extension of our earlier work on the Cross-View Matching Network (CVM-Net) (Hu et al. in IEEE conference on computer vision and pattern recognition (CVPR), 2018) for the ground-to-aerial image matching task since the traditional image descriptors fail due to the drastic viewpoint change. In particular, we show more extensive experimental results and analyses of the network architecture on our CVM-Net. Furthermore, we propose a Markov localization framework that enforces the temporal consistency between image frames to enhance the geo-localization results in the case where a video stream of ground view images is available. Experimental results show that our proposed Markov localization framework can continuously localize the vehicle within a small error on our Singapore dataset.	[Hu, Sixing; Lee, Gim Hee] Natl Univ Singapore, Comp 1,13 Comp Dr, Singapore, Singapore	National University of Singapore	Hu, SX (corresponding author), Natl Univ Singapore, Comp 1,13 Comp Dr, Singapore, Singapore.	hu.sixing@u.nus.edu; gimhee.lee@nus.edu.sg						Abadi Martin, 2016, arXiv; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arandjelovic Relja, 2016, IEEE C COMP VIS PATT; Bansal M., 2011, SER MM 11, P1125, DOI [10.1145/2072298.2071954, DOI 10.1145/2072298.2071954]; Bansal M, 2012, LECT NOTES COMPUT SC, V7583, P175, DOI 10.1007/978-3-642-33863-2_18; Bay H., 2006, EUR C COMP VIS ECCV, P404, DOI [10.1007/11744023_32, DOI 10.1007/11744023_32]; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Hays James, 2008, CVPR, DOI DOI 10.1109/CVPR.2008.4587784; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hermans Alexander, 2017, ARXIV170307737; Hu SX, 2018, PROC CVPR IEEE, P7258, DOI 10.1109/CVPR.2018.00758; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Kim D. K., 2017, IEEE INT C ROB AUT; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2013, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2013.120; Liu PC, 2018, IEEE INT C INT ROBOT, P1464, DOI 10.1109/IROS.2018.8594322; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; McManus C., 2014, SHADY DEALINGS ROBUS; Middelberg S, 2014, LECT NOTES COMPUT SC, V8690, P268, DOI 10.1007/978-3-319-10605-2_18; Nister D, 2006, IEEE COMP SOC C COMP, V2, P2161, DOI DOI 10.1109/CVPR.2006.264; Noda Masafumi, 2010, Computer Vision - ACCV 2010 Workshops. ACCV 2010 International Workshops. Revised Selected Papers, P163, DOI 10.1007/978-3-642-22819-3_17; Sattler T, 2016, PROC CVPR IEEE, P1582, DOI 10.1109/CVPR.2016.175; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Senlet T., 2011, FRAMEWORK GLOBAL VEH; Senlet T., 2012, SATELLITE IMAGE BASE; Shan Q., 2014, 2014 2 INT C 3D VIS, VVolume 1, P525; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Stumm E., 2016, IEEE C COMP VIS PATT; Thrun S, 2001, ARTIF INTELL, V128, P99, DOI 10.1016/S0004-3702(01)00069-8; Thrun S., 2002, P 18 C UNC ART INT; Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649; Viquez KDC, 2017, IEEE INT CONF COMP V, P90, DOI 10.1109/ICCVW.2017.19; Viswanathan A, 2014, IEEE INT C INT ROBOT, P192, DOI 10.1109/IROS.2014.6942560; Vo N., 2017, REVISITING IM2GPS DE; Vo NN, 2016, LECT NOTES COMPUT SC, V9905, P494, DOI 10.1007/978-3-319-46448-0_30; Wang J., 2017, DEEP METRIC LEARNING; Workman S., 2015, WIDE AREA IMAGE GEOL; Workman S, 2015, IEEE COMPUT SOC CONF; ZAGORUYKO S, 2015, 2015 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2015.7299064; Zamir AR, 2014, IEEE T PATTERN ANAL, V36, P1546, DOI 10.1109/TPAMI.2014.2299799; Zhai M, 2017, PROC CVPR IEEE, P4132, DOI 10.1109/CVPR.2017.440; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	50	9	11	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1205	1219		10.1007/s11263-019-01186-0	http://dx.doi.org/10.1007/s11263-019-01186-0			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Green Submitted			2022-12-18	WOS:000531431500009
J	Shen, ZY; Lai, WS; Xu, TF; Kautz, J; Yang, MH				Shen, Ziyi; Lai, Wei-Sheng; Xu, Tingfa; Kautz, Jan; Yang, Ming-Hsuan			Exploiting Semantics for Face Image Deblurring	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face image deblurring; Semantic face parsing; Deep convolutional neural networks		In this paper, we propose an effective and efficient face deblurring algorithm by exploiting semantic cues via deep convolutional neural networks. As the human faces are highly structured and share unified facial components (e.g., eyes and mouths), such semantic information provides a strong prior for restoration. We incorporate face semantic labels as input priors and propose an adaptive structural loss to regularize facial local structures within an end-to-end deep convolutional neural network. Specifically, we first use a coarse deblurring network to reduce the motion blur on the input face image. We then adopt a parsing network to extract the semantic features from the coarse deblurred image. Finally, the fine deblurring network utilizes the semantic information to restore a clear face image. We train the network with perceptual and adversarial losses to generate photo-realistic results. The proposed method restores sharp images with more accurate facial features and details. Quantitative and qualitative evaluations demonstrate that the proposed face deblurring algorithm performs favorably against the state-of-the-art methods in terms of restoration quality, face recognition and execution speed.	[Shen, Ziyi] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Lai, Wei-Sheng; Yang, Ming-Hsuan] Univ Calif Merced, Sch Engn, Merced, CA USA; [Shen, Ziyi; Xu, Tingfa] Beijing Inst Technol, Sch Opt & Photon, Beijing, Peoples R China; [Kautz, Jan] Nvidia, Santa Clara, CA USA; [Yang, Ming-Hsuan] Google, Mountain View, CA 94043 USA; [Yang, Ming-Hsuan] Yonsei Univ, Seoul, South Korea	University of California System; University of California Merced; Beijing Institute of Technology; Nvidia Corporation; Google Incorporated; Yonsei University	Xu, TF (corresponding author), Beijing Inst Technol, Sch Opt & Photon, Beijing, Peoples R China.	ziyishen@bit.edu.cn; wlai24@ucmerced.edu; ciom_xtf1@bit.edu.cn; jkautz@nvidia.com; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304	National Natural Science Foundation of China [61527802]; National Nature Science Foundation of China [61371132, 61471043]; NSF CAREER [1149783]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported by the Major Science Instrument Program of the National Natural Science Foundation of China under Grant 61527802, the General Program of National Nature Science Foundation of China under Grants 61371132 and 61471043, NSF CAREER (No. 1149783) and gifts from Adobe and Nvidia.l;	Amos Brandon, 2016, CMUCS16118; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anwar S, 2015, IEEE I CONF COMP VIS, P495, DOI 10.1109/ICCV.2015.64; Boracchi G, 2012, IEEE T IMAGE PROCESS, V21, P3502, DOI 10.1109/TIP.2012.2192126; Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Chrysos GG, 2019, INT J COMPUT VISION, V127, P801, DOI 10.1007/s11263-018-1138-7; Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Genenko YA, 2012, IEEE INT FERRO; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; HaCohen Y, 2013, IEEE I CONF COMP VIS, P2384, DOI 10.1109/ICCV.2013.296; Hao JD, 2017, IEEE INT CONF MULTI; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hradi M., 2015, BRIT MACH VIS C; Hu Z, 2014, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2014.432; Jin MG, 2018, IEEE COMPUT SOC CONF, P858, DOI 10.1109/CVPRW.2018.00118; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kae A, 2013, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2013.263; Kim J, 2016, IEEE CONF COMPUT; Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618; Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188; Lai WS, 2015, PROC CVPR IEEE, P64, DOI 10.1109/CVPR.2015.7298601; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815; Li C, 2017, IEEE INT SYMP PHYS; Li LRH, 2018, PROC CVPR IEEE, P6616, DOI 10.1109/CVPR.2018.00692; Libin Sun, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8692, P231, DOI 10.1007/978-3-319-10593-2_16; Liu Y, 2018, ASIAPAC SIGN INFO PR, P1467, DOI 10.23919/APSIPA.2018.8659790; Mao XJ, 2016, ADV NEUR IN, V29; Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51; Mu N, 2017, IEEE INT C ELECTR TA; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Nimisha TM, 2017, IEEE I CONF COMP VIS, P4762, DOI 10.1109/ICCV.2017.509; Pan JS, 2017, IEEE I CONF COMP VIS, P1077, DOI 10.1109/ICCV.2017.122; Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244; Pan JS, 2016, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2016.306; Pan JS, 2014, LECT NOTES COMPUT SC, V8695, P47, DOI 10.1007/978-3-319-10584-0_4; Parkhi Omkar M., 2015, BRIT MACH VIS C; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10; Ren WQ, 2016, IEEE T IMAGE PROCESS, V25, P3426, DOI 10.1109/TIP.2016.2571062; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418; Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550; Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130; Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677; Sun LB, 2013, IEEE INT CONF COMPUT; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7; Xie XF, 2015, IEEE INT VAC ELECT C; Xu L., 2014, INT C NEUR INF PROC, V27, P1790; Xu L., 2010, ECCV, P157, DOI DOI 10.1007/978-3-642-15549-9_12; Xu L., 2013, IEEE C COMP VIS PATT; Xu XY, 2018, IEEE T IMAGE PROCESS, V27, P194, DOI 10.1109/TIP.2017.2753658; YAN Y, 2017, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2017.738; Zhang J., 2017, IEEE C COMP VIS PATT, P3817, DOI DOI 10.1109/CVPR.2017.742; Zhong L, 2013, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2013.85	70	9	10	2	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2020	128	7					1829	1846		10.1007/s11263-019-01288-9	http://dx.doi.org/10.1007/s11263-019-01288-9		MAR 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MC3NG		Green Submitted			2022-12-18	WOS:000522596400001
J	Zhang, S; Huang, JB; Lim, J; Gong, YH; Wang, JJ; Ahuja, N; Yang, MH				Zhang, Shun; Huang, Jia-Bin; Lim, Jongwoo; Gong, Yihong; Wang, Jinjun; Ahuja, Narendra; Yang, Ming-Hsuan			Tracking Persons-of-Interest via Unsupervised Representation Adaptation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face tracking; Transfer learning; Convolutional neural networks; Triplet loss	MULTITARGET	Multi-face tracking in unconstrained videos is a challenging problem as faces of one person often can appear drastically different in multiple shots due to significant variations in scale, pose, expression, illumination, and make-up. Existing multi-target tracking methods often use low-level features which are not sufficiently discriminative for identifying faces with such large appearance variations. In this paper, we tackle this problem by learning discriminative, video-specific face representations using convolutional neural networks (CNNs). Unlike existing CNN-based approaches which are only trained on large-scale face image datasets offline, we automatically generate a large number of training samples using the contextual constraints for a given video, and further adapt the pre-trained face CNN to the characters in the specific videos using discovered training samples. The embedding feature space is fine-tuned so that the Euclidean distance in the space corresponds to the semantic face similarity. To this end, we devise a symmetric triplet loss function which optimizes the network more effectively than the conventional triplet loss. With the learned discriminative features, we apply an EM clustering algorithm to link tracklets across multiple shots to generate the final trajectories. We extensively evaluate the proposed algorithm on two sets of TV sitcoms and YouTube music videos, analyze the contribution of each component, and demonstrate significant performance improvement over existing techniques.	[Zhang, Shun] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Peoples R China; [Huang, Jia-Bin] Virginia Tech, Dept Elect & Comp Engn, Blacksburg, VA 24060 USA; [Lim, Jongwoo] Hanyang Univ, Dept Comp Sci, Seoul 133791, South Korea; [Gong, Yihong; Wang, Jinjun] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China; [Ahuja, Narendra] Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA; [Yang, Ming-Hsuan] Univ Calif Merced, Sch Engn, Merced, CA 95344 USA	Northwestern Polytechnical University; Virginia Polytechnic Institute & State University; Hanyang University; Xi'an Jiaotong University; University of Illinois System; University of Illinois Urbana-Champaign; University of California System; University of California Merced	Yang, MH (corresponding author), Univ Calif Merced, Sch Engn, Merced, CA 95344 USA.	szhang@nwpu.edu.cn; jbhuang@vt.edu; jlim@hanyang.ac.kr; ygong@mail.xjtu.edu.cn; jinjun@mail.xjtu.edu.cn; n-ahuja@illinois.edu; mhyang@ucmerced.edu	Wang, Jin/GYA-2019-2022; Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304	National Basic Research Program of China (973 Program) [2015CB351705]; National Key Research and Development Program of China [2017YFA0700805]; NSFC [61703344]; Office of Naval Research [N0014-16-1-2314]; Ministry of Science and ICT of Korea [NRF-2017R1A2B4011928, NRF-2017M3C4A7069369]; NSF CRII [1755785]; NSF CAREER [1149783]	National Basic Research Program of China (973 Program)(National Basic Research Program of China); National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Office of Naval Research(Office of Naval Research); Ministry of Science and ICT of Korea; NSF CRII(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	The work is supported by National Basic Research Program of China (973 Program, 2015CB351705), National Key Research and Development Program of China (2017YFA0700805), NSFC (61703344), Office of Naval Research (N0014-16-1-2314), Ministry of Science and ICT of Korea (NRF-2017R1A2B4011928 and Next-Generation Information Computing Development program NRF-2017M3C4A7069369), NSF CRII (1755785), NSF CAREER (1149783) and gifts from Adobe, Panasonic, NEC, and NVIDIA.	Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Andriluka M, 2008, PROC CVPR IEEE, P1873, DOI 10.1109/CVPR.2008.4587583; Anguelov D, 2007, PROC CVPR IEEE, P673; Ayazoglu M., 2012, CVPR; Bauml M, 2013, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2013.462; Ben Shitrit H., 2011, ICCV; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bertinetto Luca, 2016, NIPS; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Breitenstein MD, 2009, IEEE I CONF COMP VIS, P1515, DOI 10.1109/ICCV.2009.5459278; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Cinbis R., 2011, ICCV; Collins RT, 2012, PROC CVPR IEEE, P1744, DOI 10.1109/CVPR.2012.6247870; Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dicle C., 2013, ICCV; Donahue J, 2014, PR MACH LEARN RES, V32; Du M, 2016, IEEE T PATTERN ANAL, V38, P1762, DOI 10.1109/TPAMI.2015.2497689; El Khoury E., 2010, ICMR; Everingham M., 2006, BMVC, DOI DOI 10.5244/C.20.92; Fernando B, 2015, PATTERN RECOGN LETT, V65, P60, DOI 10.1016/j.patrec.2015.07.009; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Fulkerson B., 2008, ECCV; Ganin Y., 2014, ARXIV14097495; Ganin Y, 2016, J MACH LEARN RES, V17; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grabner H, 2006, IEEE C COMP VIS PATT, P260; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Huang C., 2006, ECCVW; Huang C, 2008, LECT NOTES COMPUT SC, V5303, P788, DOI 10.1007/978-3-540-88688-4_58; Jia Y., 2014, P 22 ACM INT C MULT, P675; Jiang Hao, 2007, IEEE C COMP VIS PATT, P2; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kaucic R., 2005, CVPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuo C.-H., 2010, CVPR; Kuo CH, 2011, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2011.5995384; Leibe B., 2007, ICCV; LI Y, 2007, CVPR; Lin D., 2010, ECCV; Lin Z., 2015, ARXIV151003009; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47; Oh SJ, 2015, IEEE I CONF COMP VIS, P3862, DOI 10.1109/ICCV.2015.440; Parkhi Omkar M., 2015, BRIT MACH VIS C; Paul G., 2014, ICASSP; Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260; Perera AG Amitha, 2006, CVPR; Pernici F, 2012, LECT NOTES COMPUT SC, V7585, P597, DOI 10.1007/978-3-642-33885-4_61; Ramanan D., 2007, ICCV; Rao YM, 2019, INT J COMPUT VISION, V127, P701, DOI 10.1007/s11263-018-1135-x; Rao YM, 2017, IEEE I CONF COMP VIS, P3801, DOI 10.1109/ICCV.2017.408; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Roth M., 2012, ICPR; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shu Rui, 2018, P 6 INT C LEARN REPR, P2; Sivic J., 2009, CVPR; Stauffer C., 2003, CVPR; Sun BC, 2016, AAAI CONF ARTIF INTE, P2058; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Taigman Y., 2016, ARXIV161102200; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tang Z., 2015, IJCAI; Tapaswi M., 2014, ICVGIP; Tapaswi M, 2012, CVPR; Tzeng E., 2014, ARXIV PREPRINT ARXIV; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Varghese J., 2016, DETECTING VIDEO SHOT, P131; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wang B., 2014, CVPR; Wang R., 2015, ACM MM; Wojke N, 2017, IEEE IMAGE PROC, P3645; Wu B., 2013, CVPR; Wu B., 2013, ICCV; Xiao S., 2014, ECCV; Xing J., 2009, CVPR; Yang B., 2012, ECCV; Yang SM, 2016, CHIN CONT DECIS CONF, P3443, DOI 10.1109/CCDC.2016.7531578; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yoon JS, 2017, IEEE I CONF COMP VIS, P2186, DOI 10.1109/ICCV.2017.238; Zhang N, 2015, PROC CVPR IEEE, P4804, DOI 10.1109/CVPR.2015.7299113; Zhang S, 2015, PATTERN RECOGN, V48, P580, DOI 10.1016/j.patcog.2014.08.013; Zhang ZP, 2016, LECT NOTES COMPUT SC, V9907, P236, DOI 10.1007/978-3-319-46487-9_15; Zhao X., 2012, ECCV; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133	101	9	9	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2020	128	1					96	120		10.1007/s11263-019-01212-1	http://dx.doi.org/10.1007/s11263-019-01212-1			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KI6WA		Green Submitted			2022-12-18	WOS:000511490100005
J	de Souza, CR; Gaidon, A; Cabon, Y; Murray, N; Lopez, AM				de Souza, Cesar Roberto; Gaidon, Adrien; Cabon, Yohann; Murray, Naila; Manuel Lopez, Antonio			Generating Human Action Videos by Coupling 3D Game Engines and Probabilistic Graphical Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Procedural generation; Human action recognition; Synthetic data; Physics		Deep video action recognition models have been highly successful in recent years but require large quantities of manually-annotated data, which are expensive and laborious to obtain. In this work, we investigate the generation of synthetic training data for video action recognition, as synthetic data have been successfully used to supervise models for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation, physics models and other components of modern game engines. With this model we generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for "Procedural Human Action Videos". PHAV contains a total of 39,982 videos, with more than 1000 examples for each of 35 action categories. Our video generation approach is not limited to existing motion capture sequences: 14 of these 35 categories are procedurally-defined synthetic actions. In addition, each video is represented with 6 different data modalities, including RGB, optical flow and pixel-level semantic labels. These modalities are generated almost simultaneously using the Multiple Render Targets feature of modern GPUs. In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers action classes from multiple datasets) representation learning architecture that is able to simultaneously learn from synthetic and real video datasets, even when their action categories differ. Our experiments on the UCF-101 and HMDB-51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance. Our approach also significantly outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.	[de Souza, Cesar Roberto; Cabon, Yohann; Murray, Naila] NAVER LABS Europe, 6 Chemin Maupertuis, F-38240 Meylan, France; [Gaidon, Adrien] Toyota Res Inst, 4440 El Camino Real, Los Altos, CA 94022 USA; [Manuel Lopez, Antonio] Univ Autonoma Barcelona, Ctr Visio Comp, Edifici O, Barcelona, Spain	Toyota Motor Corporation; Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	de Souza, CR (corresponding author), NAVER LABS Europe, 6 Chemin Maupertuis, F-38240 Meylan, France.	cesar.desouza@naverlabs.com; adrien.gaidon@tri.global; yohann.cabon@naverlabs.com; naila.murray@naverlabs.com; antonio@cvc.uab.es	López, Antonio M/L-5303-2014	López, Antonio M/0000-0002-6979-5783	MINECO/AEI/FEDER, UE [TIN2017-88709-R]; ICREA under the ICREA Academia Program	MINECO/AEI/FEDER, UE(Spanish Government); ICREA under the ICREA Academia Program	Antonio M. Lopez acknowledges the financial support by the Spanish TIN2017-88709-R (MINECO/AEI/FEDER, UE), and by ICREA under the ICREA Academia Program. As CVC/UAB researcher, Antonio also acknowledges the Generalitat de Catalunya CERCA Program and its ACCIO agency.	Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aubry M, 2015, IEEE I CONF COMP VIS, P2875, DOI 10.1109/ICCV.2015.329; Bishop CM, 2006, PATTERN RECOGNITION; Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Carnegie Mellon Graphics Lab, 2016, CARN MELL U MOT CAPT; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Carter MP, 1997, COMPUTER GRAPHICS PR; Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI [10.1109/ICCV.2015.104, 10.1109/ICCV.2015.312]; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; De Souza C. R., 2016, ECCV; De Souza C. R, 2014, ACCORD NET FRAMEWORK; de Souza Cesar Roberto, 2017, CVPR, DOI DOI 10.1109/CVPR.2017.278; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Egges A., 2008, MOT GAM 1 INT WORKSH, V5277; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470; Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65; Galvane Q., 2015, SIGGRAPH; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; Guay M., 2015, SKETCH BASED INTERFA; Guay M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766893; Haeusler R., 2013, GERM C PATT REC; Haltakov V., 2013, GERM C PATT REC; Handa A., 2015, ARXIV150500171 CORR; Handa A, 2016, PROC CVPR IEEE, P4077, DOI 10.1109/CVPR.2016.442; Hao ZK, 2018, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2018.00819; Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006; Ioffe Sergey, 2015, ICML, V37; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Jiang Y., 2013, THUMOS CHALLENGE ACT; Kaneva B., 2011, ICCV; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kuehne H., 2011, P INT C COMP VIS, DOI DOI 10.1109/ICCV.2011.6126543; Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616; Langer MS, 2000, PERCEPTION, V29, P649, DOI 10.1068/p3060; Lerer A, 2016, PR MACH LEARN RES, V48; Li Y, 2018, AAAI CONF ARTIF INTE, P338; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Asensio JML, 2014, EXPERT SYST APPL, V41, P7281, DOI 10.1016/j.eswa.2014.05.004; Marin J, 2010, PROC CVPR IEEE, P137, DOI 10.1109/CVPR.2010.5540218; Marwah Tanya, 2017, ICCV; Massa F, 2016, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2016.648; Matikainen P., 2011, ICCV; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Meister S., 2011, CEMT; Miller G, 1994, SIGGRAPH; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Molnar S, 1991, TECHNICAL REPORT; Nian FD, 2017, COMPUT VIS IMAGE UND, V163, P126, DOI 10.1016/j.cviu.2017.06.012; Onkarappa N, 2015, MULTIMED TOOLS APPL, V74, P3121, DOI 10.1007/s11042-013-1771-7; Papon J, 2015, IEEE I CONF COMP VIS, P774, DOI 10.1109/ICCV.2015.95; Peng XJ, 2014, LECT NOTES COMPUT SC, V8691, P660, DOI 10.1007/978-3-319-10578-9_43; Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151; PERLIN K, 1995, IEEE T VIS COMPUT GR, V1, P5, DOI 10.1109/2945.468392; Perlin K., 2008, MOTION IN GAMES; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Ritschel T., 2009, P 2009 S INT 3D GRAP; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308; Selan J, 2012, SIGGRAPH; Shafaei A., 2016, BMVC; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Singh V. K., 2016, ECCV WORKSH; Soomro K., 2012, CRCVTR1201; Sousa T., 2011, SIGGRAPH; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Steiner B, 2011, THESIS; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Taylor Geoffrey R., 2007, IEEE COMPUTER SOC C, DOI [DOI 10.1109/CVPR.2007.383518, 10.1109/CVPR.2007.383518]; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; van Welbergen H., 2009, P EUR; Vazquez D., 2011, NIPS WORKSH; Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163; Vedantam Ramakrishna, 2015, ICCV; Veeravasarapu V., 2016, ARXIV160509582 CORR; Veeravasarapu V., 2015, ARXIV151201030 CORR; Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang L., 2017, ARXIV170502953 CORR; Wang Lijun, 2015, CVPR; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291; Wu Wayne, 2018, ECCV; Xiong W, 2018, PROC CVPR IEEE, P2364, DOI 10.1109/CVPR.2018.00251; Xu JL, 2014, IEEE T INTELL TRANSP, V15, P2121, DOI 10.1109/TITS.2014.2310138; Yan XC, 2018, LECT NOTES COMPUT SC, V11209, P276, DOI 10.1007/978-3-030-01228-1_17; Yan Y., 2017, ACM MM; Yang CY, 2018, LECT NOTES COMPUT SC, V11214, P204, DOI 10.1007/978-3-030-01249-6_13; Zach C., 2007, P 29 DAGM C PATT REC; Zhao Y., 2018, CVPR; Zheng YJ, 2009, IEEE T PATTERN ANAL, V31, P2243, DOI 10.1109/TPAMI.2008.263; Zitnick CL, 2016, IEEE T PATTERN ANAL, V38, P627, DOI 10.1109/TPAMI.2014.2366143; Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316	105	9	9	1	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1505	1536		10.1007/s11263-019-01222-z	http://dx.doi.org/10.1007/s11263-019-01222-z		OCT 2019	32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW		Green Submitted			2022-12-18	WOS:000492170700001
J	Feng, ZL; Liang, WX; Tao, DC; Sun, L; Zeng, AX; Song, ML				Feng, Zunlei; Liang, Weixin; Tao, Daocheng; Sun, Li; Zeng, Anxiang; Song, Mingli			CU-Net: Component Unmixing Network for Textile Fiber Identification	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Textile fiber identification; Multi-label classification; Self-interchange; Restraining loss; Class imbalance; Small sample size	CLASSIFICATION; IMAGE	Image-based nondestructive textile fiber identification is a challenging computer vision problem, that is practically useful in fashion, decoration, and design. Although deep learning now outperforms humans in many scenarios such as face and object recognition, image-based fiber identification is still an open problem for deep learning given imbalanced sample and small sample size samples. In this paper, we propose the Component Unmixing Network (CU-Net) for nondestructive textile fiber identification. CU-Net learns effective representations given imbalanced sample and small sample size samples to achieve high-performance textile fiber identification. CU-Net comprises a Deep Feature Extraction Module (DFE-Module) and a Component Unmixing Module (CU-Module). Initially, mixed deep features are extracted by DFE-Module from the input textile patches. Then, CU-Module is employed to extract unmixed representations of different fibers from the mixed deep features. In CU-Module, we introduce a self-interchange and a restraining loss to reduce the mixture between representations of different fibers. Furthermore, we extend CU-Net to the proportion analysis task with very good effect. Extensive experiments demonstrate that: (1) self-interchange and the restraining loss effectively unmix different fiber representations and improve fiber identification accuracy; and (2) CU-Net achieves more accurate fiber identification than the current state-of-the-art multi-label classification methods.	[Feng, Zunlei; Liang, Weixin; Sun, Li; Song, Mingli] Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China; [Tao, Daocheng] Univ Sydney, Sydney, NSW 2006, Australia; [Zeng, Anxiang] Alibaba Grp, Hangzhou 859000, Zhejiang, Peoples R China; [Feng, Zunlei] Zhejiang Univ, Coll Software Technol, Ningbo 330200, Zhejiang, Peoples R China; [Song, Mingli] Zhejiang Univ, Binhai Ind Technol Res Inst, Tianjin 300000, Peoples R China	Zhejiang University; University of Sydney; Alibaba Group; Zhejiang University; Zhejiang University	Song, ML (corresponding author), Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China.; Song, ML (corresponding author), Zhejiang Univ, Binhai Ind Technol Res Inst, Tianjin 300000, Peoples R China.	brooksong@zju.edu.cn			National Key Research and Development Program [2016YFB1200203]; National Natural Science Foundation of China [61572428, U1509206]; Key Research and Development Program of Zhejiang Province [2018C01004]; Program of International Science and Technology Cooperation [2013DFG12840]; Project of Science and Technology Research and Development Program of China RailwayCorporation [P2018X002]; Fundamental Research Funds for the Central Universities	National Key Research and Development Program; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Research and Development Program of Zhejiang Province; Program of International Science and Technology Cooperation; Project of Science and Technology Research and Development Program of China RailwayCorporation; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work is supported by National Key Research and Development Program (2016YFB1200203), National Natural Science Foundation of China (61572428, U1509206), Key Research and Development Program of Zhejiang Province (2018C01004), and the Program of International Science and Technology Cooperation (2013DFG12840), Project of Science and Technology Research and Development Program of China RailwayCorporation (P2018X002), Fundamental Research Funds for the Central Universities.	Antonucci A., 2013, P 23 INT JOINT C ART, P1220; Bucak SS, 2009, IEEE I CONF COMP VIS, P2098, DOI 10.1109/ICCV.2009.5459460; Fei Wu, 2015, IEEE Transactions on Big Data, V1, P109, DOI 10.1109/TBDATA.2015.2497270; Gajdosik J, 2006, PROBABILIST ENG MECH, V21, P317, DOI 10.1016/j.probengmech.2005.11.006; Geng X., 2016, CHINA TEXTILE LEADER; Goutsu Y, 2018, INT J COMPUT VISION, V126, P495, DOI 10.1007/s11263-017-1053-3; Hensman P., 2015, IMPACT IMBALANCED TR; Hsu D., 2009, P 22 INT C NEURAL IN, V22, P772; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Izadinia H, 2015, MMCOMMONS'15: PROCEEDINGS OF THE 2015 WORKSHOP ON COMMUNITY-ORGANIZED MULTIMODAL MINING: OPPORTUNITIES FOR NOVEL SOLUTIONS, P13, DOI 10.1145/2814815.2814821; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kapoor A., 2012, ADV NEURAL INFORM PR, P2645; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Langeron Y, 2007, ENG APPL ARTIF INTEL, V20, P415, DOI 10.1016/j.engappai.2006.07.001; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Liu WW, 2015, AAAI CONF ARTIF INTE, P2800; Nam Jinseok, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P437, DOI 10.1007/978-3-662-44851-9_28; Read J., 2010, SCALABLE MULTI LABEL; Ren Z., 2015, ARXIV PREPRINT ARXIV; Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3; Shu X, 2015, NEUROCOMPUTING, V168, P356, DOI 10.1016/j.neucom.2015.05.090; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Song Yale, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163081; Sun XD, 2016, SPECTROSC LETT, V49, P96, DOI 10.1080/00387010.2015.1089446; Tang L, 2009, CHANDOS ASIAN STUD, P211; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Verma Y, 2016, INT J COMPUT VISION, P1; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Wei Y., 2014, ARXIV PREPRINT ARXIV; Wu BY, 2015, IEEE I CONF COMP VIS, P4157, DOI 10.1109/ICCV.2015.473; Xinyu Wu, 2020, ISH Journal of Hydraulic Engineering, V26, P343, DOI 10.1080/09715010.2018.1481772; Xu C, 2015, AAAI CONF ARTIF INTE, P1924; Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8; Yang YB, 2018, IEEE ACCESS, V6, P15576, DOI [10.1109/ACCESS.2018.2810115, 10.1109/TVCG.2018.2865192]; Yeh CK, 2017, AAAI CONF ARTIF INTE, P2838; Yuan HF, 2010, SPECTROSC SPECT ANAL, V30, P1229, DOI 10.3964/j.issn.1000-0593(2010)05-1229-05; Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019; Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0; Zhao FP, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4062; Zhou T., 2012, J MACH LEARN RES, P1444; Zhou TY, 2012, MACH LEARN, V88, P69, DOI 10.1007/s10994-011-5276-1	42	9	10	1	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1443	1454		10.1007/s11263-019-01199-9	http://dx.doi.org/10.1007/s11263-019-01199-9			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL					2022-12-18	WOS:000485320300004
J	Tran, L; Kossaifi, J; Panagakis, Y; Pantic, M				Tran, Linh; Kossaifi, Jean; Panagakis, Yannis; Pantic, Maja			Disentangling Geometry and Appearance with Regularised Geometry-Aware Generative Adversarial Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generative adversarial network; Image generation; Active shape model; Disentanglement; Representation learning; Face analysis; Deep learning; Generative models; GAN	MODELS	Deep generative models have significantly advanced image generation, enabling generation of visually pleasing images with realistic texture. Apart from the texture, it is the shape geometry of objects that strongly dictates their appearance. However, currently available generative models do not incorporate geometric information into the image generation process. This often yields visual objects of degenerated quality. In this work, we propose a regularized Geometry-Aware Generative Adversarial Network (GAGAN) which disentangles appearance and shape in the latent space. This regularized GAGAN enables the generation of images with both realistic texture and shape. Specifically, we condition the generator on a statistical shape prior. The prior is enforced through mapping the generated images onto a canonical coordinate frame using a differentiable geometric transformation. In addition to incorporating geometric information, this constrains the search space and increases the model's robustness. We show that our approach is versatile, able to generalise across domains (faces, sketches, hands and cats) and sample sizes (from as little as approximate to 200-30,000 to more than 200,000). We demonstrate superior performance through extensive quantitative and qualitative experiments in a variety of tasks and settings. Finally, we leverage our model to automatically and accurately detect errors or drifting in facial landmarks detection and tracking in-the-wild.	[Tran, Linh; Kossaifi, Jean; Panagakis, Yannis; Pantic, Maja] Imperial Coll London, London, England; [Panagakis, Yannis] Middlesex Univ London, London, England; [Pantic, Maja] Samsung AI, Cambridge, England	Imperial College London; Middlesex University	Tran, L (corresponding author), Imperial Coll London, London, England.	linh.tran@imperial.ac.uk; jean.kossaifi@imperial.ac.uk; i.panagakis@imperial.ac.uk; m.pantic@imperial.ac.uk	Kossaifi, Jean/AAW-8519-2021; Panagakis, Yannis/AAZ-8090-2020	Kossaifi, Jean/0000-0002-4445-3429; Panagakis, Ioannis/0000-0003-0153-5210	European Community [688835, 645094]	European Community(European Commission)	The work of Linh Tran, Yannis Panagakis and Maja Pantic has been funded by the European Community Horizon 2020 under Grant Agreement Nos. 688835, 645094 (DE-ENIGMA).	Antonakos E, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2431445; Arjovsky M., 2017, ARXIV170107875; Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602; Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Davies R, 2008, STAT MODELS SHAPE OP; Dinh Laurent, 2017, 5 INT C LEARN REPR I; Edwards GJ, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P300, DOI 10.1109/AFGR.1998.670965; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Grzejszczak T, 2016, MULTIMED TOOLS APPL, V75, P16363, DOI 10.1007/s11042-015-2934-5; Gulrajani I, 2017, P NIPS 2017; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kawulok M, 2014, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2014-170; Kingma D.P, P 3 INT C LEARNING R; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kossaifi J., 2017, IEEE CVPR; Kossaifi J, 2017, IMAGE VISION COMPUT, V65, P23, DOI 10.1016/j.imavis.2017.02.001; Kossaifi J, 2017, IEEE T IMAGE PROCESS, V26, P1040, DOI 10.1109/TIP.2016.2642828; Kossaifi J, 2014, IEEE IMAGE PROC, P1420, DOI 10.1109/ICIP.2014.7025284; Kossatfi J, 2015, IEEE IMAGE PROC, P1135, DOI 10.1109/ICIP.2015.7350977; Larsen A. B. L., 2016, INT C MACH LEARN; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Liu Z, 2015, 20TH INTERNATIONAL CONFERENCE ON COMPOSITE MATERIALS; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma B, 2015, IEEE I CONF COMP VIS, P4400, DOI 10.1109/ICCV.2015.500; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Mirza M., 2014, ARXIV; Nalepa J, 2014, COMM COM INF SC, V424, P364; Odena A., 2016, SEMISUPERVISED LEARN; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rupprecht T, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1772, DOI 10.1145/2976749.2989041; Sagonas C., 2015, P IEEE INT C COMP VI; Sagonas C., 2013, CVPR WORKSH; Sagonas C., 2016, INT J COMPUT VISION; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Salimans T, 2016, ADV NEUR IN, V29; Salimans Tim, 2017, ARXIV170105517; Tipping M.E., 2003, ADV NEURAL INFORM PR, V15, P1303; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; Tzimiropoulos G, 2017, INT J COMPUT VISION, V122, P17, DOI 10.1007/s11263-016-0950-1; Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239; Tzimiropoulos G, 2012, IEEE T PATTERN ANAL, V34, P2454, DOI 10.1109/TPAMI.2012.40; van den Oord Aaron, 2016, ARXIV160605328; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang CY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2901; Xie J., 2012, ADV NEURAL INFORM PR, P341, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605; Yang H, 2014, IEEE SIGNAL PROC LET, V21, P1321, DOI 10.1109/LSP.2014.2333544; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614; Zhou F, 2013, IEEE I CONF COMP VIS, P1025, DOI 10.1109/ICCV.2013.131; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	64	9	9	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		824	844		10.1007/s11263-019-01155-7	http://dx.doi.org/10.1007/s11263-019-01155-7			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD		hybrid, Green Published			2022-12-18	WOS:000468525900016
J	Xiao, GB; Wang, HZ; Yan, Y; Suter, D				Xiao, Guobao; Wang, Hanzi; Yan, Yan; Suter, David			Superpixel-Guided Two-View Deterministic Geometric Model Fitting	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Model fitting; Superpixel; Deterministic algorithm; Multiple-structure data	ROBUST; TRANSFORM; CONSENSUS; PRIORS	Geometric model fitting is a fundamental research topic in computer vision and it aims to fit and segment multiple-structure data. In this paper, we propose a novel superpixel-guided two-view geometric model fitting method (called SDF), which can obtain reliable and consistent results for real images. Specifically, SDF includes three main parts: a deterministic sampling algorithm, a model hypothesis updating strategy and a novel model selection algorithm. The proposed deterministic sampling algorithm generates a set of initial model hypotheses according to the prior information of superpixels. Then the proposed updating strategy further improves the quality of model hypotheses. After that, by analyzing the properties of the updated model hypotheses, the proposed model selection algorithm extends the conventional fit-and-remove framework to estimate model instances in multiple-structure data. The three parts are tightly coupled to boost the performance of SDF in both speed and accuracy, and SDF has the deterministic nature. Experimental results show that the proposed SDF has significant advantages over several state-of-the-art fitting methods when it is applied to real images with single-structure and multiple-structure data.	[Xiao, Guobao; Wang, Hanzi; Yan, Yan] Xiamen Univ, Fujian Key Lab Sensing & Comp Smart City, Sch Informat Sci & Engn, Xiamen, Peoples R China; [Suter, David] Edith Cowan Univ, Sch Sci, Joondalup, Australia	Xiamen University; Edith Cowan University	Wang, HZ (corresponding author), Xiamen Univ, Fujian Key Lab Sensing & Comp Smart City, Sch Informat Sci & Engn, Xiamen, Peoples R China.	hanzi.wang@xmu.edu.cn	Wang, Han/GPW-9809-2022	Suter, David/0000-0001-6306-3023	National Natural Science Foundation of China [U1605252, 61702431, 61472334, 61571379]; China Postdoctoral Science Foundation [2017M620272]; Fujian Province Education-Science Project for Middle-aged and Young Teachers [JAT170024]; ARC [DP130102524]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); Fujian Province Education-Science Project for Middle-aged and Young Teachers; ARC(Australian Research Council)	This work was supported by the National Natural Science Foundation of China under Grants U1605252, 61702431, 61472334 and 61571379, by the China Postdoctoral Science Foundation 2017M620272 and by the Fujian Province Education-Science Project for Middle-aged and Young Teachers JAT170024. This work was carried out when David Suter was with The University of Adelaide. This work was partially supported by ARC Grant DP130102524.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Brahmachari AS, 2013, IEEE T PATTERN ANAL, V35, P755, DOI 10.1109/TPAMI.2012.227; Chin TJ, 2012, IEEE T PATTERN ANAL, V34, P625, DOI 10.1109/TPAMI.2011.169; Chin TJ, 2016, IEEE T PATTERN ANAL, V1, P1; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Enqvist O, 2015, INT J COMPUT VISION, V112, P115, DOI 10.1007/s11263-014-0760-2; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fragoso V, 2013, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2013.307; Fragoso V, 2013, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2013.357; Fredriksson J, 2015, PROC CVPR IEEE, P2684, DOI 10.1109/CVPR.2015.7298884; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; Isack H, 2014, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2014.150; Kanazawa Y., 2004, BRIT MACH VIS C, P247; Lai TT, 2017, COMPUT VIS IMAGE UND, V154, P152, DOI 10.1016/j.cviu.2016.10.003; Lee KH, 2013, IEEE I CONF COMP VIS, P41, DOI 10.1109/ICCV.2013.12; Li HD, 2009, IEEE I CONF COMP VIS, P1074, DOI 10.1109/ICCV.2009.5459398; Litman R, 2015, PROC CVPR IEEE, P5243, DOI 10.1109/CVPR.2015.7299161; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Magri L, 2016, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2016.361; Magri L, 2014, PROC CVPR IEEE, P3954, DOI 10.1109/CVPR.2014.505; Mittal S, 2012, IEEE T PATTERN ANAL, V34, P2351, DOI 10.1109/TPAMI.2012.52; Poling B, 2014, INT J COMPUT VISION, V108, P165, DOI 10.1007/s11263-013-0694-0; Tran QH, 2014, INT J COMPUT VISION, V106, P93, DOI 10.1007/s11263-013-0643-y; Serradell E, 2010, LECT NOTES COMPUT SC, V6313, P58; Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892; Tennakoon RB, 2016, IEEE T PATTERN ANAL, V38, P350, DOI 10.1109/TPAMI.2015.2448103; Tordoff BJ, 2005, IEEE T PATTERN ANAL, V27, P1523, DOI 10.1109/TPAMI.2005.199; Pham TT, 2014, IEEE T IMAGE PROCESS, V23, P4601, DOI 10.1109/TIP.2014.2346025; Wand MP, 1994, KERNEL SMOOTHING; Wang HZ, 2015, IEEE I CONF COMP VIS, P2902, DOI 10.1109/ICCV.2015.332; Wang HZ, 2012, IEEE T PATTERN ANAL, V34, P1177, DOI 10.1109/TPAMI.2011.216; Wong HS, 2011, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2011.6126350; Woodford OJ, 2014, INT J COMPUT VISION, V106, P332, DOI 10.1007/s11263-013-0623-2; Xiao GB, 2016, LECT NOTES COMPUT SC, V9910, P517, DOI 10.1007/978-3-319-46466-4_31; Xiao GB, 2016, PATTERN RECOGN, V60, P748, DOI 10.1016/j.patcog.2016.06.026	36	9	9	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2019	127	4					323	339		10.1007/s11263-018-1100-8	http://dx.doi.org/10.1007/s11263-018-1100-8			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HN4DR		Green Submitted			2022-12-18	WOS:000460135000001
J	Zou, CH; Guo, RQ; Li, ZZ; Hoiem, D				Zou, Chuhang; Guo, Ruiqi; Li, Zhizhong; Hoiem, Derek			Complete 3D Scene Parsing from an RGBD Image	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual scene understanding; 3D parsing; Single image reconstruction		One major goal of vision is to infer physical models of objects, surfaces, and their layout from sensors. In this paper, we aim to interpret indoor scenes from one RGBD image. Our representation encodes the layout of orthogonal walls and the extent of objects, modeled with CAD-like 3D shapes. We parse both the visible and occluded portions of the scene and all observable objects, producing a complete 3D parse. Such a scene interpretation is useful for robotics and visual reasoning, but difficult to produce due to the well-known challenge of segmentation, the high degree of occlusion, and the diversity of objects in indoor scenes. We take a data-driven approach, generating sets of potential object regions, matching to regions in training images, and transferring and aligning associated 3D models while encouraging fit to observations and spatial consistency. We use support inference to aid interpretation and propose a retrieval scheme that uses convolutional neural networks to classify regions and retrieve objects with similar shapes. We demonstrate the performance of our method on our newly annotated NYUd v2 dataset (Silberman et al., in: Computer vision-ECCV, 2012, pp 746-760, 2012) with detailed 3D shapes.	[Zou, Chuhang; Li, Zhizhong; Hoiem, Derek] Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA; [Guo, Ruiqi] Google Res, 111 8th Ave, New York, NY 10011 USA	University of Illinois System; University of Illinois Urbana-Champaign; Google Incorporated	Zou, CH (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA.	czou4@illinois.edu; guorq@google.com; zli115@illinois.edu; dhoiem@illinois.edu			ONR MURI Grant [N000141612007, N000141010934]	ONR MURI Grant	This research is supported in part by ONR MURI Grant N000141010934 and ONR MURI Grant N000141612007. We thank David Forsyth for insightful comments and discussion and Saurabh Singh, Kevin Shih and Tanmay Gupta for their comments on an earlier version of the manuscript.	Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Banica D., 2013, CORR; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Chang Angel X., 2015, ARXIV151203012CSGR P; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dasgupta S, 2016, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2016.73; Delage E., 2006, COMP VIS PATT REC 20, V2, P2418, DOI DOI 10.1109/CVPR.2006.23; Deng Z, 2015, IEEE I CONF COMP VIS, P1733, DOI 10.1109/ICCV.2015.202; Dollar P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231; Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42; Flint A., 2011, ICCV; Furukawa Y, 2009, IEEE I CONF COMP VIS, P80, DOI 10.1109/ICCV.2009.5459145; Gong Yunchao, 2013, IJCV; Guo R., 2015, ARXIV150402437; Guo R., 2012, ECCV; Guo R., 2013, ICCV; Gupta S, 2015, PROC CVPR IEEE, P4731, DOI 10.1109/CVPR.2015.7299105; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79; Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20; Hedau V, 2010, ECCV; Hedau V., 2009, INT C COMP VIS; Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5; Karsch K, 2012, LECT NOTES COMPUT SC, V7576, P775, DOI 10.1007/978-3-642-33715-4_56; Kingma D.P, P 3 INT C LEARNING R; Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872; Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071; Lim JJ, 2014, LECT NOTES COMPUT SC, V8694, P478, DOI 10.1007/978-3-319-10599-4_31; Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372; Lin DH, 2013, IEEE I CONF COMP VIS, P1417, DOI 10.1109/ICCV.2013.179; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mallya A, 2015, IEEE I CONF COMP VIS, P936, DOI 10.1109/ICCV.2015.113; Manen S, 2013, IEEE I CONF COMP VIS, P2536, DOI 10.1109/ICCV.2013.315; Ren XF, 2012, PROC CVPR IEEE, P2759, DOI 10.1109/CVPR.2012.6247999; Roberts Lawrence G, 1963, THESIS, P2; Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863; Satkin Scott, 2013, ICCV; Schwing AG, 2012, LECT NOTES COMPUT SC, V7577, P299, DOI 10.1007/978-3-642-33783-3_22; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41; Tighe Joseph, 2010, ECCV, DOI [10.1007/978-3-642-15555-0_26, DOI 10.1007/978-3-642-15555-0_26]; Urtasun R., 2013, RECONSTRUCTION MEETS; Walk S, 2010, LECT NOTES COMPUT SC, V6316, P182, DOI 10.1007/978-3-642-15567-3_14; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xiang Y, 2016, LECT NOTES COMPUT SC, V9912, P160, DOI 10.1007/978-3-319-46484-8_10; Xiao J., 2012, ADV NEURAL INFORM PR, P755; Xiao JX, 2014, INT J COMPUT VISION, V110, P243, DOI 10.1007/978-3-642-33718-5_48; Yamaguchi Kota, 2013, ICCV; Yih Wen-tau, 2011, P 15 C COMP NAT LANG, P247; Zhang J, 2013, IEEE I CONF COMP VIS, P1273, DOI 10.1109/ICCV.2013.161; Zhang Y., 2016, ICCV; Zhang YD, 2014, LECT NOTES COMPUT SC, V8694, P668, DOI 10.1007/978-3-319-10599-4_43; Zhao YB, 2013, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2013.401	56	9	9	2	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2019	127	2					143	162		10.1007/s11263-018-1133-z	http://dx.doi.org/10.1007/s11263-018-1133-z			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HL5LJ		Green Submitted			2022-12-18	WOS:000458768000002
J	Nguyen, RMH; Brown, MS				Nguyen, Rang M. H.; Brown, Michael S.			RAW Image Reconstruction Using a Self-contained sRGB-JPEG Image with Small Memory Overhead	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Radiometric calibration; In-camera image processing; Raw image reconstruction	CAMERA RESPONSE; INTERPOLATION; VISION	Most camera images are saved as 8-bit standard RGB (sRGB) compressed JPEGs. Even when JPEG compression is set to its highest quality, the encoded sRGB image has been significantly processed in terms of color and tone manipulation. This makes sRGB-JPEG images undesirable for many computer vision tasks that assume a direct relationship between pixel values and incoming light. For such applications, the RAW image format is preferred, as RAW represents a minimally processed, sensor-specific RGB image that is linear with respect to scene radiance. The drawback with RAW images, however, is that they require large amounts of storage and are not well-supported by many imaging applications. To address this issue, we present a method to encode the necessary data within an sRGB-JPEG image to reconstruct a high-quality RAW image. Our approach requires no calibration of the camera's colorimetric properties and can reconstruct the original RAW to within 0.5% error with a small memory overhead for the additional data (e.g., 128 KB). More importantly, our output is a fully self-contained 100% compliant sRGB-JPEG file that can be used as-is, not affecting any existing image workflow-the RAW image data can be extracted when needed, or ignored otherwise. We detail our approach and show its effectiveness against competing strategies.	[Nguyen, Rang M. H.] Natl Univ Singapore, Sch Comp, Singapore, Singapore; [Brown, Michael S.] York Univ, Lassonde Sch Engn, Toronto, ON, Canada	National University of Singapore; York University - Canada	Brown, MS (corresponding author), York Univ, Lassonde Sch Engn, Toronto, ON, Canada.	rangnhm@gmail.com; mbrown@eecs.yorku.ca			Google Faculty Research Award; Canadian NSERC Discovery Grant Award; Canada First Research Excellence Fund for the Vision: Science to Applications (VISTA) programme	Google Faculty Research Award(Google Incorporated); Canadian NSERC Discovery Grant Award; Canada First Research Excellence Fund for the Vision: Science to Applications (VISTA) programme	This work was supported in part by a Google Faculty Research Award, a Canadian NSERC Discovery Grant Award, and the Canada First Research Excellence Fund for the Vision: Science to Applications (VISTA) programme.	Brower B., 2011, INFORM TECHNOLOGY DI; CHAKRABARTI A, 2009, BRIT MACH VIS C BMVC; Chakrabarti A, 2014, IEEE T PATTERN ANAL, V36, P2185, DOI 10.1109/TPAMI.2014.2318713; Cheng DL, 2014, J OPT SOC AM A, V31, P1049, DOI 10.1364/JOSAA.31.001049; Coffin D, 1997, DCRAW DECODING RAW D; Dai S., 2007, P IEEE C COMP VIS PA, P1, DOI [10.1109/CVPR.2007.383028, DOI 10.1109/CVPR.2007.383028]; Dang-Nguyen D.-T., 2015, ACM MULTIMEDIA SYSTE; Debevec P. E., 2008, ACM SIGGRAPH 2008 TU; Fattal R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239502; Fergus R, 2009, INT C NEUR INF PROC; Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127; Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075; Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; Grossberg MD, 2003, IEEE T PATTERN ANAL, V25, P1455, DOI 10.1109/TPAMI.2003.1240119; Hamilton E., 1992, JPEG FILE INTERCHANG; HOU HS, 1978, IEEE T ACOUST SPEECH, V26, P508; Kasson J. M., 1993, IS T SPIES S EL IM S; Khan GN, 2011, IEEE IC COMP COM NET; Kim SJ, 2012, IEEE T PATTERN ANAL, V34, P2289, DOI 10.1109/TPAMI.2012.58; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Lin H., 2011, IEEE INT C COMP VIS; Lin H. T., 2012, EUR C COMP VIS ECCV; Manduchi R, 1998, IEEE INTERNATIONAL C; Mann S., 1995, P IS T; Meagher D., 1980, IPLTR80111; Mitsunaga T., 1999, IEEE C COMP VIS PATT; Nguyen R. M. H., 2016, IEEE C COMP VIS PATT; Nguyen R. M. H., 2014, IEEE C COMP VIS PATT; Queau Y, 2017, LECT NOTES COMPUT SC, V10302, P656, DOI 10.1007/978-3-319-58771-4_52; REINSCH CH, 1967, NUMER MATH, V10, P177, DOI 10.1007/BF02162161; Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659; Tai YW, 2013, IEEE T PATTERN ANAL, V35, P2498, DOI 10.1109/TPAMI.2013.40; Thevenaz P, 2000, BIOMED EN S, P393; Xiong Y., 2012, IEEE C COMP VIS PATT; Zhang L., 2005, IEEE C COMP VIS PATT	36	9	9	2	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2018	126	6					637	650		10.1007/s11263-017-1056-0	http://dx.doi.org/10.1007/s11263-017-1056-0			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GC3EO	31258243	Green Published, hybrid			2022-12-18	WOS:000429667300004
J	Ma, SG; Zhang, JM; Sclaroff, S; Ikizler-Cinbis, N; Sigal, L				Ma, Shugao; Zhang, Jianming; Sclaroff, Stan; Ikizler-Cinbis, Nazli; Sigal, Leonid			Space-Time Tree Ensemble for Action Recognition and Localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; Action localization; Space-time tree structure		Human actions are, inherently, structured patterns of body movements. We explore ensembles of hierarchical spatio-temporal trees, discovered directly from training data, to model these structures for action recognition and spatial localization. Discovery of frequent and discriminative tree structures is challenging due to the exponential search space, particularly if one allows partial matching. We address this by first building a concise action word vocabulary via discriminative clustering of the hierarchical space-time segments, which is a two-level video representation that captures both static and non-static relevant space-time segments of the video. Using this vocabulary we then utilize tree mining with subsequent tree clustering and ranking to select a compact set of discriminative tree patterns. Our experiments show that these tree patterns, alone, or in combination with shorter patterns (action words and pairwise patterns) achieve promising performance on three challenging datasets: UCF Sports, HighFive and Hollywood3D. Moreover, we perform cross-dataset validation, using trees learned on HighFive to recognize the same actions in Hollywood3D, and using trees learned on UCF-Sports to recognize and localize the similar actions in JHMDB. The results demonstrate the potential for cross-dataset generalization of the trees our approach discovers.	[Ma, Shugao; Sclaroff, Stan] Boston Univ, Comp Sci, Boston, MA 02215 USA; [Zhang, Jianming] Adobe Res, San Jose, CA USA; [Ikizler-Cinbis, Nazli] Hacettepe Univ, Comp Engn, Ankara, Turkey; [Sigal, Leonid] Disney Res, Pittsburgh, PA USA	Boston University; Adobe Systems Inc.; Hacettepe University	Ma, SG (corresponding author), Boston Univ, Comp Sci, Boston, MA 02215 USA.	shugaoma@bu.edu; jianmzha@adobe.com; sclaroff@bu.edu; nazli@cs.hacettepe.edu.tr; lsigal@disneyresearch.com	Ikizler-Cinbis, Nazli/E-8961-2013		Google Faculty Research Award; US NSF [0855065, 0910908, 1029430]	Google Faculty Research Award(Google Incorporated); US NSF(National Science Foundation (NSF))	This work was supported in part through a Google Faculty Research Award and by US NSF grants 0855065, 0910908, and 1029430.	[Anonymous], 2014, EUR C COMP VIS; Arbelaez P., 2009, CVPR; Ben Aoun N, 2014, J VIS COMMUN IMAGE R, V25, P329, DOI 10.1016/j.jvcir.2013.11.003; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; Brendel W., 2011, ICCV; Chearon G., 2015, ICCV; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Felzenszwalb PF, 2011, IEEE T PATTERN ANAL, V33, P721, DOI 10.1109/TPAMI.2010.135; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Gaidon A, 2014, INT J COMPUT VISION, V107, P219, DOI 10.1007/s11263-013-0677-1; Gilbert A., 2014, ACCV; Gilbert A, 2011, IEEE T PATTERN ANAL, V33, P883, DOI 10.1109/TPAMI.2010.144; Girshick R., 2014, CVPR, DOI 10.1109/CVPR.2014.81; Gkioxari G., 2015, CVPR; Gkioxari G., 2015, INT C COMP VIS; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Hadfield S., 2014, ECCV; Hadfield S., 2013, CVPR; Hoai M., 2013, CVPR; Ikizler N, 2008, INT J COMPUT VISION, V80, P337, DOI 10.1007/s11263-008-0142-8; Ikizler-Cinbis N., 2010, EUR C COMP VIS; Iosifidis A., 2014, ICIP; Jhuang Hueihan, 2013, ICCV; Kantorov V., 2014, CVPR; Karpathy A., 2014, CVPR; Kuehne H., 2011, ICCV; Lan T., 2011, ICCV; Laptev I., 2008, CVPR; Leordeanu M., 2012, ECCV; Ma S., 2013, ICCV; Ma S., 2015, CVPR; Marszalek M., 2009, CVPR; Matikainen P., 2010, ECCV; Mikolajczyk K, 2011, COMPUT VIS IMAGE UND, V115, P426, DOI 10.1016/j.cviu.2010.11.002; Ng J. Y., 2015, CVPR; Nijssen S., 2005, ICCS; Oneata D., 2013, ICCV; Patron-Perez A., 2010, BMVC; Patron-Perez A, 2012, IEEE T PATTERN ANAL, V34, P2441, DOI 10.1109/TPAMI.2012.24; Perronnin F., 2010, ECCV; Ramanan D., 2003, NIPS; Raptis M., 2013, CVPR; Raptis M., 2012, INT C COMP VIS PATT; Rodriguez M. D., 2008, CVPR; Ronfard R., 2007, ICCV; Sadanand S., 2012, P 2012 IEEE C COMP V; Simonyan Karen, 2014, NIPS; Tian Y., 2013, CVPR; Todorovic Sinisa, 2012, ECCV; Tran D., 2012, NIPS; Tran D., 2011, CVPR; Wang H., 2013, ICCV; Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang L., 2014, ECCV; Wang L., 2013, ICCV; WANG Y, 2007, CVPR; Wang Y., 2008, NIPS; Wang Y., 2012, JMLR, V13; Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214; Weinzaepfel P., 2015, ICCV; Wu B., 2014, CVPR; Wu Z., 2015, P 23 ACM INT C MULT; Xie Y., 2011, CVPR; Yang X., 2014, ECCV; Zhang H., 2014, CVPR	67	9	9	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		314	332		10.1007/s11263-016-0980-8	http://dx.doi.org/10.1007/s11263-016-0980-8			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA					2022-12-18	WOS:000425619100010
J	Tonioni, A; Salti, S; Tombari, F; Spezialetti, R; Di Stefano, L				Tonioni, Alessio; Salti, Samuele; Tombari, Federico; Spezialetti, Riccardo; Di Stefano, Luigi			Learning to Detect Good 3D Keypoints	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D Keypoint Detection; 3D Descriptors; Machine Learning; Surface Matching	OBJECT RECOGNITION; SURFACE	The established approach to 3D keypoint detection consists in defining effective handcrafted saliency functions based on geometric cues with the aim of maximizing keypoint repeatability. Differently, the idea behind our work is to learn a descriptor-specific keypoint detector so as to optimize the end-to-end performance of the feature matching pipeline. Accordingly, we cast 3D keypoint detection as a classification problem between surface patches that can or cannot be matched correctly by a given 3D descriptor, i.e. those either good or not in respect to that descriptor. We propose a machine learning framework that allows for defining examples of good surface patches from the training data and leverages Random Forest classifiers to realize both fixed-scale and adaptive-scale 3D keypoint detectors. Through extensive experiments on standard datasets, we show how feature matching performance improves significantly by deploying 3D descriptors together with companion detectors learned by our methodology with respect to the adoption of established state-of-the-art 3D detectors based on hand-crafted saliency functions.	[Tonioni, Alessio; Spezialetti, Riccardo; Di Stefano, Luigi] Univ Bologna, DISI, Bologna, Italy; [Salti, Samuele] Fleetmat Res, Florence, Italy; [Tombari, Federico] Tech Univ Munich, CAMP, Munich, Germany	University of Bologna; Technical University of Munich	Tombari, F (corresponding author), Tech Univ Munich, CAMP, Munich, Germany.	alessio.tonioni2@unibo.it; samuele.salti@fleetmatics.com; tombari@in.tum.de; riccardo.spezialetti@unibo.it; luigi.distefano@unibo.it		Salti, Samuele/0000-0001-5609-426X; Tombari, Federico/0000-0001-5598-5212				Aldoma A., 2014, P INT C INT ROB SYST; Aldoma A, 2012, LECT NOTES COMPUT SC, V7574, P511, DOI 10.1007/978-3-642-33712-3_37; Aldoma A, 2012, IEEE ROBOT AUTOM MAG, V19, P80, DOI 10.1109/MRA.2012.2206675; ALEXANDRE L, 2012, IROS WORKSH COL DEPT; Bariya P, 2010, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2010.5539774; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Behley J., 2012, INT C ROB AUT; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x; Creusot C, 2013, INT J COMPUT VISION, V102, P146, DOI 10.1007/s11263-012-0605-9; Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1561/0600000035, 10.1501/0000000035]; Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4; Guo Y., 2013, 8 INT C COMP GRAPH T; Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y; Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y; Hartmann W, 2014, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2014.9; Holzer S., 2012, 2012 IEEE EUR C COMP; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542; Li YL, 2015, NEUROCOMPUTING, V149, P736, DOI 10.1016/j.neucom.2014.08.003; Lin X., 2016, ARXIV160500129; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z; Ovsjanikov M., 2011, EUR S GEOM PROC, V30; Proenca PF, 2013, LECT NOTES COMPUT SC, V8033, P385, DOI 10.1007/978-3-642-41914-0_38; Rodola E, 2013, INT J COMPUT VISION, V102, P129, DOI 10.1007/s11263-012-0568-x; Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Rusu RB, 2009, IEEE INT CONF ROBOT, P1848; Salti S, 2015, IEEE I CONF COMP VIS, P2318, DOI 10.1109/ICCV.2015.267; Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187; Strecha C, 2009, LECT NOTES COMPUT SC, V5748, P151, DOI 10.1007/978-3-642-03798-6_16; Sukno F., 2012, INT S VIS COMP ISVC; Sun J., 2009, EUR S GEOM PROC, V28; Taati B., 2007, P 11 IEEE INT C COMP, V1421, P18; Teran L, 2014, LECT NOTES COMPUT SC, V8689, P159, DOI 10.1007/978-3-319-10590-1_11; Tombari F, 2013, INT J COMPUT VISION, V102, P198, DOI 10.1007/s11263-012-0545-4; Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017; Verdie Y., 2015, P COMP VIS PATT REC; Wohlkinger W., 2012, INT C ROB AUT ICRA; Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637; Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748	44	9	14	2	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2018	126	1					1	20		10.1007/s11263-017-1037-3	http://dx.doi.org/10.1007/s11263-017-1037-3			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FS6MC					2022-12-18	WOS:000419910500001
J	Jiao, JB; Yang, QX; He, SF; Gu, SH; Zhang, L; Lau, RWH				Jiao, Jianbo; Yang, Qingxiong; He, Shengfeng; Gu, Shuhang; Zhang, Lei; Lau, Rynson W. H.			Joint Image Denoising and Disparity Estimation via Stereo Structure PCA and Noise-Tolerant Cost	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo matching; Image denoising; Disparity estimation; Non-local means	PATCHES	Stereo cameras are now commonly available on cars and mobile phones. However, the captured images may suffer from low image quality under noisy conditions, producing inaccurate disparity. In this paper, we aim at jointly restoring a clean image pair and estimating the corresponding disparity. To this end, we propose a new joint framework that iteratively optimizes these two different tasks in a multi-scale fashion. First, structure information between the stereo pair is utilized to denoise the images using a non-local means strategy. Second, a new noise-tolerant cost function is proposed for noisy stereo matching. These two terms are integrated into a multi-scale framework in which cross-scale information is leveraged to further improve both denoising and stereo matching. Extensive experiments on datasets captured from indoor, outdoor, and low-light conditions show that the proposed method achieves superior performance than the state-of-the-art image denoising and disparity estimation methods. While it outperforms multi-image denoising methods by about 2 dB on average, it achieves a 50% error reduction over radiometric-change-robust stereo matching on the challenging KITTI dataset.	[Jiao, Jianbo; Lau, Rynson W. H.] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China; [Yang, Qingxiong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Anhui, Peoples R China; [He, Shengfeng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Guangdong, Peoples R China; [Gu, Shuhang; Zhang, Lei] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China	City University of Hong Kong; Chinese Academy of Sciences; University of Science & Technology of China, CAS; South China University of Technology; Hong Kong Polytechnic University	Yang, QX (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Anhui, Peoples R China.	Jambol.Jiao@my.cityu.edu.hk; liiton.research@gmail.com; shengfeng_he@yahoo.com; cssgu@comp.polyu.edu.hk; cslzhang@comp.polyu.edu.hk; Rynson.Lau@cityu.edu.hk	He, Shengfeng/E-5682-2016; Yang, Qingxiong/K-1729-2015	He, Shengfeng/0000-0002-3802-4644; Zhang, Lei/0000-0002-2078-4215; Jiao, Jianbo/0000-0003-0833-5115; Yang, Qingxiong/0000-0002-4378-2335	Hong Kong PhD Fellowship Scheme (HKPFS) from the RGC of Hong Kong; SRG grant from City University of Hong Kong [7004416]; GRF grant from the RGC of Hong Kong [PolyU 52124/15E]	Hong Kong PhD Fellowship Scheme (HKPFS) from the RGC of Hong Kong; SRG grant from City University of Hong Kong; GRF grant from the RGC of Hong Kong	We would thank the anonymous reviewers for their constructive suggestions and insightful comments. This work is partially supported by the Hong Kong PhD Fellowship Scheme (HKPFS) from the RGC of Hong Kong, a SRG grant from City University of Hong Kong (No. 7004416), and a GRF grant from the RGC of Hong Kong (PolyU 52124/15E).	Alter F, 2006, LECT NOTES COMPUT SC, V3954, P267; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Buades A, 2008, INT J COMPUT VISION, V76, P123, DOI 10.1007/s11263-007-0052-1; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Chan SH, 2014, IEEE T IMAGE PROCESS, V23, P3711, DOI 10.1109/TIP.2014.2327813; Chen F, 2015, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2015.76; Cho TS, 2010, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2010.5540214; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; De-Maeztu L, 2011, PATTERN RECOGN LETT, V32, P1643, DOI 10.1016/j.patrec.2011.06.027; Deledalle CA, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.25; Dong WS, 2015, IEEE I CONF COMP VIS, P442, DOI 10.1109/ICCV.2015.58; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; Fu Y., 2016, IJCV, V122, P1; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; Heo Y.S., 2007, IEEE C COMP VIS PATT, P1; Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hirschmuller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221; Hirschmuller H, 2002, INT J COMPUT VISION, V47, P229, DOI 10.1023/A:1014554110407; Honda Hiroto, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P82, DOI 10.1109/CVPRW.2015.7301300; Jung IL, 2013, IEEE IMAGE PROC, P2082, DOI 10.1109/ICIP.2013.6738429; Kim YH, 2016, PATTERN RECOGN LETT, V78, P41, DOI 10.1016/j.patrec.2016.04.015; Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2833, DOI 10.1109/CVPR.2011.5995309; Levin A, 2012, LECT NOTES COMPUT SC, V7576, P73, DOI 10.1007/978-3-642-33715-4_6; Liu C, 2010, LECT NOTES COMPUT SC, V6313, P706; Lu X, 2015, IEEE T IMAGE PROCESS, V24, P5469, DOI 10.1109/TIP.2015.2473098; Luo EM, 2013, IEEE IMAGE PROC, P543, DOI 10.1109/ICIP.2013.6738112; Luo EM, 2015, IEEE T IMAGE PROCESS, V24, P2167, DOI 10.1109/TIP.2015.2414873; Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725; Menz MD, 2003, NAT NEUROSCI, V6, P59, DOI 10.1038/nn986; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Mosseri I, 2013, IEEE INT CONF COMPUT; Muresan DD, 2003, IEEE IMAGE PROC, P101; Nir T, 2005, CIS200503 ISR I TECH; Park BG, 2006, LECT NOTES COMPUT SC, V4179, P990; ROMENY BMT, 1993, PERCEPTION VISUAL IN, P73; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scharstein D., 2007, MIDDLEBURY STEREO DA; Shao L, 2014, IEEE T CYBERNETICS, V44, P1001, DOI 10.1109/TCYB.2013.2278548; Shen XY, 2015, IEEE T PATTERN ANAL, V37, P2518, DOI 10.1109/TPAMI.2015.2417569; Tan X, 2014, LECT NOTES COMPUT SC, V8693, P17, DOI 10.1007/978-3-319-10602-1_2; Vemulapalli R., 2015, ARXIV151104067; Vu DT, 2014, IEEE T IMAGE PROCESS, V23, P3428, DOI 10.1109/TIP.2014.2329389; Xu JT, 2016, INT J COMPUT VISION, V119, P179, DOI 10.1007/s11263-016-0886-5; Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36; Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157; Yue HJ, 2014, PROC CVPR IEEE, P2933, DOI 10.1109/CVPR.2014.375; Yue HJ, 2015, IEEE T IMAGE PROCESS, V24, P1967, DOI 10.1109/TIP.2015.2412373; Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345; Zbontar J., 2016, J MACH LEARN RES, V17, P2; Zhang K, 2014, PROC CVPR IEEE, P1590, DOI 10.1109/CVPR.2014.206; Zhang L, 2010, PATTERN RECOGN, V43, P1531, DOI 10.1016/j.patcog.2009.09.023; Zhang L, 2009, PROC CVPR IEEE, P1542, DOI 10.1109/CVPRW.2009.5206836; Zontak M, 2013, PROC CVPR IEEE, P1195, DOI 10.1109/CVPR.2013.158; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	57	9	9	2	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	2					204	222		10.1007/s11263-017-1015-9	http://dx.doi.org/10.1007/s11263-017-1015-9			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FC3PK					2022-12-18	WOS:000406751100006
J	Li, WX; Vasconcelos, N				Li, Wei-Xin; Vasconcelos, Nuno			Complex Activity Recognition Via Attribute Dynamics	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Complex activity; Attribute; Dynamical model; Variational inference; Fisher score		The problem of modeling the dynamic structure of human activities is considered. Video is mapped to a semantic feature space, which encodes activity attribute probabilities over time. The binary dynamic system (BDS) model is proposed to jointly learn the distribution and dynamics of activities in this space. This is a non-linear dynamic system that combines binary observation variables and a hidden Gauss-Markov state process, extending both binary principal component analysis and the classical linear dynamic systems. A BDS learning algorithm, inspired by the popular dynamic texture, and a dissimilarity measure between BDSs, which generalizes the Binet-Cauchy kernel, are introduced. To enable the recognition of highly non-stationary activities, the BDS is embedded in a bag of words. An algorithm is introduced for learning a BDS codebook, enabling the use of the BDS as a visual word for attribute dynamics (WAD). Short-term video segments are then quantized with a WAD codebook, allowing the representation of video as a bag-of-words for attribute dynamics. Video sequences are finally encoded as vectors of locally aggregated descriptors, which summarize the first moments of video snippets on the BDS manifold. Experiments show that this representation achieves state-of-the-art performance on the tasks of complex activity recognition and event identification.	[Li, Wei-Xin; Vasconcelos, Nuno] Univ Calif San Diego, ECE Dept, 9500 Gilman Dr, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Li, WX (corresponding author), Univ Calif San Diego, ECE Dept, 9500 Gilman Dr, La Jolla, CA 92093 USA.	wel017@ucsd.edu; nuno@ece.ucsd.edu	Li, Weixin/AAM-2664-2021	Li, Weixin/0000-0002-0938-8107; Vasconcelos, Nuno/0000-0002-9024-4302	National Science Foundation [IIS-1208522]	National Science Foundation(National Science Foundation (NSF))	The authors acknowledge valuable discussions on the manuscript and references to variational inference by professor Lawrence K. Saul. This work was partially supported by National Science Foundation Grant IIS-1208522.	Afsari B., 2012, CVPR; Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Attias H., 1999, NIPS; Baccouche M., 2011, 2 INT WORKSH HUM BEH; Baccouche Moez, 2010, ICANN; Bhattacharya S., 2014, CVPR; Bhattacharya S., 2013, ACM INT C MULT; Blei D.M., 2006, ICML, V25-29, P113; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Bregler C., 1997, CVPR; Campbell L., 1995, ICCV; Chan A., 2005, CVPR; Chan A. B., 2007, CVPR; Chan AB, 2008, IEEE T PATTERN ANAL, V30, P909, DOI 10.1109/TPAMI.2007.70738; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chaudhry R., 2009, CVPR; Chomat O., 1999, CVPR; Cinbis R. G., 2012, CVPR; Collins Michael, 2002, NIPS; Deng L., 2014, FOUND TRENDS SIGNAL, V7, P197, DOI DOI 10.1561/2000000039; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132; Fathi Alireza, 2008, CVPR; Gaidon A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3201, DOI 10.1109/CVPR.2011.5995646; Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65; Ghahramani Z., 2000, NIPS; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Graves A., 2008, ADV NEURAL INFORM PR, P545, DOI DOI 10.1007/978-1-4471-4072-6; Haasdonk B, 2005, IEEE T PATTERN ANAL, V27, P482, DOI 10.1109/TPAMI.2005.78; Hajimirsadeghi H, 2015, PROC CVPR IEEE, P2596, DOI 10.1109/CVPR.2015.7298875; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ikizler N, 2008, INT J COMPUT VISION, V80, P337, DOI 10.1007/s11263-008-0142-8; Jaakkola T., 1999, NIPS; Jain A., 2013, CVPR; Jain M., 2015, CVPR; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31; Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353; Jones S., 2014, CVPR; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kellokumpu V., 2008, BMVC; Kovashka A., 2010, CVPR; Krapac J., 2011, ICCV; Kuehne H., 2011, P INT C COMP VIS, DOI DOI 10.1109/ICCV.2011.6126543; Kullback S, 1997, INFORM THEORY STAT; Lai K.-T., 2014, ECCV; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lan Z., 2014, TEMPORAL EXTENSION S; Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Laxton B., 2007, CVPR; Li B, 2011, CVPR; Li W., 2012, NIPS; Li W., 2013, ICCV; Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461; Matikainen P, 2010, LECT NOTES COMPUT SC, V6311, P508, DOI 10.1007/978-3-642-15549-9_37; McCallum A., 2006, P 12 ACM SIGKDD INT, DOI DOI 10.1145/1150402.1150450; MOORE D, 1999, ICCV; Moreno P. J., 2004, NIPS; Ni B., 2015, CVPR; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Niyogi S., 1994, CVPR; Over P., 2011, P TRECVID 2011; Palatucci Mark, 2009, ADV NEURAL INFORM PR, P1410; Peng X., 2014, BAG VISUAL WORDS FUS; Peng XJ, 2014, LECT NOTES COMPUT SC, V8691, P660, DOI 10.1007/978-3-319-10578-9_43; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Pinhanez C., 1998, CVPR; Quattoni A, 2007, PROC CVPR IEEE, P1553; Rasiwasia N., 2008, CVPR; Rasiwasia N., 2009, CVPR; Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138; Rasiwasia N, 2012, IEEE T PATTERN ANAL, V34, P902, DOI 10.1109/TPAMI.2011.175; Ravichandran A, 2013, IEEE T PATTERN ANAL, V35, P342, DOI 10.1109/TPAMI.2012.83; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saul LK, 2000, NEURAL COMPUT, V12, P1313, DOI 10.1162/089976600300015385; Schein A. I., 2003, AISTATS; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shao L., 2015, INT J COMPUTER VISIO; Shao L, 2014, IEEE T CYBERNETICS, V44, P817, DOI 10.1109/TCYB.2013.2273174; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Snoek C. G. M., 2005, ACM INT C MULT; Sun  C., 2013, ICCV; Sun C., 2014, CVPR; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tamrakar A., 2012, CVPR; Tang K., 2012, CVPR; Todorovic Sinisa, 2012, ECCV; Vahdat A., 2013, ICCV; Vasconcelos N., 2004, ECCV; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Vishwanathan SVN, 2007, INT J COMPUT VISION, V73, P95, DOI 10.1007/s11263-006-9352-0; Vrigkas M, 2015, FRONT ROBOT AI, DOI 10.3389/frobt.2015.00028; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang Heng, 2009, BMVC, P1; Wang Lijun, 2015, CVPR; Winn J, 2005, J MACH LEARN RES, V6, P661; Xu ZW, 2014, PROC CVPR IEEE, P97, DOI 10.1109/CVPR.2014.20; Yacoob Y., 1998, ICCV; Ye G., 2012, CVPR; Yu M., 2015, IEEE T PATTERN ANAL, VPP; Yu Q., 2012, ACM INT C MULT	119	9	10	1	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		334	370		10.1007/s11263-016-0918-1	http://dx.doi.org/10.1007/s11263-016-0918-1			37	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS		Green Submitted			2022-12-18	WOS:000398162200009
J	Fabbri, R; Kimia, BB				Fabbri, Ricardo; Kimia, Benjamin B.			Multiview Differential Geometry of Curves	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Multiview stereo; Torsion; Non-rigid space curves	EPIPOLAR GEOMETRY; SUBSPACE METHODS; SURFACE SHAPE; MOTION; STEREO; CAMERA; RECONSTRUCTION; DEFORMATION; CONSTRAINTS; CALIBRATION	The field of multiple view geometry has seen tremendous progress in reconstruction and calibration due to methods for extracting reliable point features and key developments in projective geometry. Point features, however, are not available in certain applications and result in unstructured point cloud reconstructions. General image curves provide a complementary feature when keypoints are scarce, and result in 3D curve geometry, but face challenges not addressed by the usual projective geometry of points and algebraic curves. We address these challenges by laying the theoretical foundations of a framework based on the differential geometry of general curves, including stationary curves, occluding contours, and non-rigid curves, aiming at stereo correspondence, camera estimation (including calibration, pose, and multiview epipolar geometry), and 3D reconstruction given measured image curves. By gathering previous results into a cohesive theory, novel results were made possible, yielding three contributions. First, we derive the differential geometry of an image curve (tangent, curvature, curvature derivative) from that of the underlying space curve (tangent, curvature, curvature derivative, torsion). Second, we derive the differential geometry of a space curve from that of two corresponding image curves. Third, the differential motion of an image curve is derived from camera motion and the differential geometry and motion of the space curve. The availability of such a theory enables novel curve-based multiview reconstruction and camera estimation systems to augment existing point-based approaches. This theory has been used to reconstruct a "3D curve sketch", to determine camera pose from local curve geometry, and tracking; other developments are underway.	[Fabbri, Ricardo] Univ Estado Rio De Janeiro, Polytech Inst, BR-28625570 Nova Friburgo, RJ, Brazil; [Kimia, Benjamin B.] Brown Univ, Div Engn, Providence, RI 02912 USA	Universidade do Estado do Rio de Janeiro; Brown University	Fabbri, R (corresponding author), Univ Estado Rio De Janeiro, Polytech Inst, BR-28625570 Nova Friburgo, RJ, Brazil.	rfabbri@gmail.com			FAPERJ/Brazil [E25/2014/204167]; UERJ/Brazil Prociencia; NSF [1116140, 1319914]	FAPERJ/Brazil(Fundacao Carlos Chagas Filho de Amparo a Pesquisa do Estado do Rio De Janeiro (FAPERJ)); UERJ/Brazil Prociencia; NSF(National Science Foundation (NSF))	We gratefully acknowledge the support of FAPERJ/Brazil E25/2014/204167, UERJ/Brazil Prociencia 2014-2017, and NSF awards 1116140 and 1319914.	Agarwal S., 2009, P IEEE INT C COMP VI; Arnold R. D., 1980, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V238, P281; Astrom K, 1996, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.1996.517168; Astrom K, 1999, INT J COMPUT VISION, V33, P51, DOI 10.1023/A:1008113231241; Astrom K, 1998, INT J COMPUT VISION, V28, P85, DOI 10.1023/A:1008006815607; Astrom K, 1999, IEEE T PATTERN ANAL, V21, P114, DOI 10.1109/34.748821; Ayache N., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P422; Baatz G, 2012, LECT NOTES COMPUT SC, V7573, P517, DOI 10.1007/978-3-642-33709-3_37; Baumela L., 2000, P 15 INT C PATT REC, V3, P848; Berthilsson R, 2001, INT J COMPUT VISION, V41, P171, DOI 10.1023/A:1011104020586; Brodsky T, 2002, INT J COMPUT VISION, V48, P91, DOI 10.1023/A:1016094806773; Brodsky T, 2000, INT J COMPUT VISION, V37, P231, DOI 10.1023/A:1008132107950; Brooks MJ, 1997, J OPT SOC AM A, V14, P2670, DOI 10.1364/JOSAA.14.002670; Calakli F, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P25, DOI 10.1109/3DIMPVT.2012.54; Carceroni R., 2001, THESIS; Carceroni R., 1999, P IEEE COMP SOC C CO, P23; CIPOLLA R, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P269, DOI 10.1109/ICCV.1995.466775; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; CIPOLLA R, 1992, INT J COMPUT VISION, V8, P53, DOI 10.1007/BF00126400; Cipolla R., 1999, VISUAL MOTION CURVES; Cipolla R., 1991, THESIS; Cole F., 2009, P SIGGRAPH ACM T GRA; Diskin Y, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.2.023003; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; Dornaika F, 2006, LECT NOTES COMPUT SC, V4153, P76; Esteban CH, 2004, COMPUT VIS IMAGE UND, V96, P367, DOI 10.1016/j.cviu.2004.03.016; Fabbri R., 2010, THESIS; Fabbri R, 2010, P IEEE C COMP VIS PA; Fabbri R, 2012, LECT NOTES COMPUT SC, V7575, P231, DOI 10.1007/978-3-642-33765-9_17; Fathi H., 2015, ADV ENG INFORM; FAUGERAS O, 1993, INT J COMPUT VISION, V10, P125, DOI 10.1007/BF01420734; Faugeras O., 1992, DISAMBIGUATING STERE, P310; Faugeras O., 1990, P MOT 3D CURV ITS RE, P105; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Furukawa Y, 2006, IEEE T PATTERN ANAL, V28, P302, DOI 10.1109/TPAMI.2006.41; Furukawa Y., 2007, P IEEE COMP SOC C CO; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; GIBLIN PJ, 1995, IMAGE VISION COMPUT, V13, P33, DOI 10.1016/0262-8856(95)91466-Q; Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933; GRIMSON WEL, 1981, PHILOS T ROY SOC B, V292, P217, DOI 10.1098/rstb.1981.0031; Guo Y., 2014, CVPR 14; Harris C, 1988, P 4 ALV VIS C, P147, DOI DOI 10.5244/C.2.23; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P882, DOI 10.1109/ICCV.1995.466843; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; Heinly J., 2015, P COMP VIS PATT REC; Hernandez C, 2007, IEEE T PATTERN ANAL, V29, P343, DOI 10.1109/TPAMI.2007.42; Heyden A, 2006, INT C PATT RECOG, P159; ICCV2007, 2007, 11 IEEE INT C COMP V; Jain V., 2009, THESIS; Jain V., 2007, P IEEE INT C IM PROC, V6, P321; Jain V, 2007, COMPUT VIS IMAGE UND, V108, P230, DOI 10.1016/j.cviu.2006.11.024; Kahl F, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P761, DOI 10.1109/ICCV.1998.710803; Kahl F., 2001, P IEEE INT C COMP VI, V2; Kaminski JY, 2004, INT J COMPUT VISION, V56, P195, DOI 10.1023/B:VISI.0000011204.89453.4d; KANATANI K, 1993, INT J COMPUT VISION, V11, P267, DOI 10.1007/BF01469345; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Koenderink J, 2013, I-PERCEPTION, V4, P299, DOI 10.1068/i0600sas; Kowdle A., 2012, ECCV, P211; Kuang YB, 2014, INT C PATT RECOG, P2419, DOI 10.1109/ICPR.2014.419; Kuang YB, 2013, IEEE I CONF COMP VIS, P529, DOI 10.1109/ICCV.2013.71; Kunsberg B., 2014, NEUROMATHEMATICS VIS, P107; Lebeda K., 2014, P ACCV; Li G, 2003, IEEE WORKSH VAR GEOM; Li G, 2006, INT J COMPUT VISION, V69, P59, DOI 10.1007/s11263-006-6853-9; Li H, 2007, P IEEE COMPUTER SOC, P1; Lin WY, 2010, INT J COMPUT VISION, V86, P87, DOI 10.1007/s11263-009-0260-y; Litvinov V., 2012, 2012 INT C 3D IM IC3, P1; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma Y., 2004, INVITATION 3 D VISIO; Mattingly WA, 2015, J VISUAL LANG COMPUT, V29, P54, DOI 10.1016/j.jvlc.2015.02.006; Maybank S., 1992, THEORY RECONSTRUCTIO; Mendonca PRS, 2001, IEEE T PATTERN ANAL, V23, P604, DOI 10.1109/34.927461; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Moreels P, 2007, INT J COMPUT VISION, V73, P263, DOI 10.1007/s11263-006-9967-1; Morevec H.P., 1977, INT JOINT C ART INT, V2, P584; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; Papadopoulo T., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P696; Papadopoulo T., 1996, RR2779; Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a; Ponce J, 1998, INT J COMPUT VISION, V28, P223, DOI 10.1023/A:1008053620575; PORRILL J, 1991, IMAGE VISION COMPUT, V9, P45, DOI 10.1016/0262-8856(91)90048-T; Potsch K., 2011, 16 COMP VIS WINT WOR, P99; Rao D, 2012, IEEE INT C INT ROBOT, P4198, DOI 10.1109/IROS.2012.6385764; Restrepo MI, 2014, ISPRS J PHOTOGRAMM, V98, P1, DOI 10.1016/j.isprsjprs.2014.09.010; Reyes L, 2005, IMAGE VISION COMPUT, V23, P693, DOI 10.1016/j.imavis.2005.03.008; Robert L., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P57, DOI 10.1109/CVPR.1991.139661; Schmid C, 2000, INT J COMPUT VISION, V40, P199, DOI 10.1023/A:1008135310502; Schneevoigt T, 2014, LECT NOTES COMPUT SC, V8753, P629, DOI 10.1007/978-3-319-11752-2_52; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Shashua A., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P479; SHERMAN D, 1990, IEEE T PATTERN ANAL, V12, P1102, DOI 10.1109/34.61711; Shinozuka Y., 2014, P 2014 VIRT REAL INT; Simoes F., 2012, 2012 14th Symposium on Virtual and Augmented Reality (SVR), P74, DOI 10.1109/SVR.2012.5; Sinha SN, 2004, PROC CVPR IEEE, P195; SPETSAKIS M, 1991, INT J COMPUT VISION, V6, P245, DOI 10.1007/BF00115698; Stewenius H., 2007, IEEE C COMP VIS PATT, P1; Tamrakar A., 2007, ICCV2007; Tamrakar A, 2008, THESIS; TENEY D, 2012, SAMPLING BASED MULTI, P160, DOI DOI 10.1109/3DIMPVT.2012.28; Tian TY, 1996, PROC CVPR IEEE, P315, DOI 10.1109/CVPR.1996.517091; Ting-Yen Chen, 2014, Image and Video Technology - PSIVT 2013 Workshops. GCCV 2013, GPID 2013, PAESNPR 2013, and QACIVA 2013. Revised Selected Papers: LNCS 8334, P12, DOI 10.1007/978-3-642-53926-8_2; Triggs B., 1999, P IEEE INT C COMP VI; Usumezbas A., 2016, P EUR C COMP V UNPUB; Valgaerts L, 2012, INT J COMPUT VISION, V96, P212, DOI 10.1007/s11263-011-0466-7; van den Hengel A., 2000, THESIS; Van den Hengel A, 2007, LECT NOTES COMPUT SC, V3417, P190; Vieville T, 1996, COMPUT VIS IMAGE UND, V64, P128, DOI 10.1006/cviu.1996.0049; VIEVILLE T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P750, DOI 10.1109/ICCV.1995.466863; Wang RZ, 2014, PROC CVPR IEEE, P4018, DOI 10.1109/CVPR.2014.513; WAXMAN AM, 1985, INT J ROBOT RES, V4, P72, DOI 10.1177/027836498500400306; Wong K.-Y. K., 2001, VIS FORM 2001, P787; Wong KYK, 2004, IEEE T IMAGE PROCESS, V13, P379, DOI 10.1109/TIP.2003.821113; Yi Ma S.S., 1998, LECT NOTES COMPUTER, V1407, P337; Zhang L., 2013, THESIS; Zhuang X., 1984, First Conference on Artificial Intelligence Applications (Cat. No. 84CH2107-1), P366; Zucker SW, 2014, P IEEE, V102, P812, DOI 10.1109/JPROC.2014.2314723	120	9	9	0	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2016	120	3					324	346		10.1007/s11263-016-0912-7	http://dx.doi.org/10.1007/s11263-016-0912-7			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DV5OK		Green Accepted			2022-12-18	WOS:000382977200005
J	Ni, BB; Paramathayalan, VR; Li, T; Moulin, P				Ni, Bingbing; Paramathayalan, Vignesh R.; Li, Teng; Moulin, Pierre			Multiple Granularity Modeling: A Coarse-to-Fine Framework for Fine-grained Action Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multiple granularity; Fine-grained action detection; Multiple object tracking; Nonparametric label transfer	OBJECT TRACKING; PLAYERS	Detecting fine-grained human action from video sequence is challenging. In this work, we propose to decompose this difficult analytic problem into two sequential tasks with increasing granularity. Firstly, we infer the coarse interaction status, i.e., which object is being manipulated and where the interaction occurs. To address the issue of frequent mutual occlusions during manipulation, we propose an interaction tracking framework in which hand (object) position and interaction status are jointly tracked by explicitly modeling the occlusion context. Secondly, for a given query sequence, the inferred interaction status is utilized to efficiently identify a small set of candidate matching sequences from the annotated training set. Frame-level action labels are then transferred to the query sequence by setting up the matching between the query and candidate sequences. Comprehensive experiments on two challenging fine-grained activity datasets show that: (1) the proposed interaction tracking approach achieves high tracking accuracy for multiple mutually occluded objects (hands) during manipulation action; and (2) the proposed multiple granularity analysis framework achieves superior action detection performance improvement over state-of-the-art methods.	[Ni, Bingbing] Shanghai Jiao Tong Univ, 800 Dong Chuan Rd, Shanghai, Peoples R China; [Paramathayalan, Vignesh R.] Adv Digital Sci Ctr, Singapore, Singapore; [Li, Teng] Anhui Univ, Coll Elect Engn & Automat, Hefei, Peoples R China; [Moulin, Pierre] Univ Illinois, Urbana, IL USA	Shanghai Jiao Tong University; Anhui University; University of Illinois System; University of Illinois Urbana-Champaign	Ni, BB (corresponding author), Shanghai Jiao Tong Univ, 800 Dong Chuan Rd, Shanghai, Peoples R China.	nibingbing@sjtu.edu.cn; vignesh.r@adsc.com.sg; liteng@ahu.edu.cn; moulin@ifp.uiuc.edu			Human Sixth Sense Programme at the Advanced Digital Sciences Center from Singapore's Agency for Science, Technology and Research (A*STAR)	Human Sixth Sense Programme at the Advanced Digital Sciences Center from Singapore's Agency for Science, Technology and Research (A*STAR)(Agency for Science Technology & Research (A*STAR))	This study is supported by the research grant for the Human Sixth Sense Programme at the Advanced Digital Sciences Center from Singapore's Agency for Science, Technology and Research (A*STAR).	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Bischof H., 2006, BMVC, P47; Blackman SS, 2004, IEEE AERO EL SYS MAG, V19, P5, DOI 10.1109/MAES.2004.1263228; Brand M, 1997, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.1997.609450; Chang C, 2005, PROC CVPR IEEE, P566; Chen HT, 2001, PROC CVPR IEEE, P210; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Delaney Martin, 2008, Proj Inf Perspect, P1; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Escorcia V, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P508, DOI 10.1109/ICCVW.2013.72; Fathi A, 2011, IEEE I CONF COMP VIS, P407, DOI 10.1109/ICCV.2011.6126269; Gupta A., 2007, IEEE C COMP VIS PATT, DOI 10.1109/CVPR.2007.383331; Han M, 2004, PROC CVPR IEEE, P864; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Trinh H, 2012, PROC CVPR IEEE, P1902, DOI 10.1109/CVPR.2012.6247890; Iwase S, 2004, INT C PATT RECOG, P751, DOI 10.1109/ICPR.2004.1333881; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231; Kjellstrom H, 2008, LECT NOTES COMPUT SC, V5303, P336, DOI 10.1007/978-3-540-88688-4_25; Klaser Alexander, 2008, BMVC; Koppula H. S., 2012, INT J ROBOTICS RES; Kuehne H, 2014, PROC CVPR IEEE, P780, DOI 10.1109/CVPR.2014.105; Kyriazis N., 2014, IEEE C COMP VIS PATT; Lan T., 2010, ADV NEURAL INFORM PR; Laptev I., 2003, IEEE INT C COMP VIS; Lei J., 2012, ACM C UB COMP, P208; Li C, 2013, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2013.458; Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131; Liu JC, 2013, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2013.239; Lv F., 2007, IEEE C COMP VIS PATT; MacCormick J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P572, DOI 10.1109/ICCV.1999.791275; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Marszalek M., 2009, IEEE C COMP VIS PATT; Moore D. J., 1999, IEEE INT C COMP VIS; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Ni BB, 2014, PROC CVPR IEEE, P756, DOI 10.1109/CVPR.2014.102; Ni BB, 2015, INT J COMPUT VISION, V111, P229, DOI 10.1007/s11263-014-0742-4; Niebles J-C, 2006, BRIT MACH VIS C; Packer B., 2012, IEEE C COMP VIS PATT; PATRONPEREZ A, 2010, BMVC, V1, P2; Patterson DJ, 2005, NINTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P44, DOI 10.1109/ISWC.2005.22; Prest A, 2013, IEEE T PATTERN ANAL, V35, P835, DOI 10.1109/TPAMI.2012.175; Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342; Raptis M, 2012, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2012.6247807; Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361; Shimada Atsushi, 2013, Advances in Depth Image Analysis and Applications. International Workshop, WDIA 2012. Selected and Invited Papers: LNCS 7854, P168, DOI 10.1007/978-3-642-40303-3_18; Vahdat A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1729, DOI 10.1109/ICCVW.2011.6130458; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214; Wesierski D, 2013, IEEE I CONF COMP VIS, P2920, DOI 10.1109/ICCV.2013.363; Wu J., 2007, IEEE INT C COMP VIS; Xu M, 2004, IEEE IMAGE PROC, P2909; Yang CJ, 2005, IEEE I CONF COMP VIS, P212; Yang M, 2009, IEEE T PATTERN ANAL, V31, P1195, DOI 10.1109/TPAMI.2008.146; Yang Y., 2013, IEEE C COMP VIS PATT; Yao B., 2011, INT C MACH LEARN; Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPR.2009.5206671, 10.1109/CVPRW.2009.5206671]; Zhao T., 2004, COMPUTER VISION PATT, V2, P406; Zhou Y, 2014, LECT NOTES COMPUT SC, V8692, P481, DOI 10.1007/978-3-319-10593-2_32	64	9	9	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2016	120	1					28	43		10.1007/s11263-016-0891-8	http://dx.doi.org/10.1007/s11263-016-0891-8			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DU3DX					2022-12-18	WOS:000382092100003
J	Tran, D; Torresani, L				Du Tran; Torresani, Lorenzo			EXMOVES: Mid-level Features for Efficient Action Recognition and Video Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; Action similarity labeling; Video representation; Mid-level features		In this paper we present EXMOVES-learned exemplar-based features for efficient recognition and analysis of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input video sequences. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers several advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of actions even in massive video databases. Third, we show the generality of our approach by training our mid-level descriptors from different low-level features and testing them on two distinct video analysis tasks: human activity recognition as well as action similarity labeling. Experiments on large-scale benchmarks demonstrate the accuracy and efficiency of our proposed method on both these tasks.	[Du Tran; Torresani, Lorenzo] Dartmouth Coll, Dept Comp Sci, 6211 Sudikoff Lab, Hanover, NH 03755 USA	Dartmouth College	Torresani, L (corresponding author), Dartmouth Coll, Dept Comp Sci, 6211 Sudikoff Lab, Hanover, NH 03755 USA.	dutran@cs.dartmouth.edu; lorenzo@cs.dartmouth.edu	Tran, Du/AAB-5973-2021	Tran, Du/0000-0001-9673-7194	NSF CAREER award [IIS-0952943]; NSF award [CNS-1205521]	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF award(National Science Foundation (NSF))	Thanks to Alessandro Bergamo for assistance with the experiments. We are grateful to Tomasz Malisiewicz for clarifications about Exemplar SVM and to Jason Corso for providing the Action Bank exemplars. This research was funded in part by NSF CAREER award IIS-0952943 and NSF award CNS-1205521.	Blank M, 2005, IEEE I CONF COMP VIS, P1395; Chapelle O., 2008, P AM STAT ASS; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; Derpanis KG, 2010, PROC CVPR IEEE, P1990, DOI 10.1109/CVPR.2010.5539874; Dollar P., 2005, P IEEE INT WORKSH VI, P65, DOI [DOI 10.1109/VSPETS.2005.1570899, 10.1109/VSPETS.2005.1570899]; Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Fathi Alireza, 2008, CVPR; Ferrari Vittorio, 2007, NIPS; Hu Y., 2009, INT C COMP VIS; Jain A, 2013, PROC CVPR IEEE, P2571, DOI 10.1109/CVPR.2013.332; Jia D, 2011, PROC CVPR IEEE, P785, DOI 10.1109/CVPR.2011.5995516; jia Li L., 2010, NIPS, DOI [10.1184/R1/6475985.v1, DOI 10.1184/R1/6475985.V1]; Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353; Ke Y., 2010, INT J COMPUTER VISIO; Ke Y., 2005, INT C COMP VIS; Klaser Alexander, 2008, BMVC; Kliper-Gross O, 2012, IEEE T PATTERN ANAL, V34, P615, DOI 10.1109/TPAMI.2011.209; Kuehne H., 2011, P INT C COMP VIS, DOI DOI 10.1109/ICCV.2011.6126543; Lampert C., 2009, IEEE T PATTERN ANAL; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Laptev I, 2007, IEEE I CONF COMP VIS, P2165; Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496; Lowe D.G., 2004, IJCV, V60, DOI [10.1023/B:VISI.0000029664.99615.94, DOI 10.1023/B:VISI.0000029664.99615.94]; Malisiewicz T., 2011, INT C COMP VIS; Marszalek M., 2009, IEEE C COMP VIS PATT; Niebles JC, 2007, PROC CVPR IEEE, P1235; Platt JC, 2000, ADV NEUR IN, P61; Ryoo M. S., 2010, UT INTERACTION DATAS; Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806; Scovanner P., 2007, ACM MM, P357; Soomro K., 2013, CRCVTR1201 U CENTR F; Torresani L., 2010, EUR C COMP VIS; Tran D, 2008, LECT NOTES COMPUT SC, V5302, P548, DOI 10.1007/978-3-540-88682-2_42; Veeraraghavan A., 2006, IEEE C COMP VIS PATT; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wang G., 2009, INT C COMP VIS; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang L., 2007, IEEE C COMP VIS PATT; Wang L, 2011, PROC CVPR IEEE; Weinberger Kilian Q, 2006, ADV NEURAL INFORM PR, P1473, DOI DOI 10.1007/978-3-319-13168-9_; Yu G., 2012, EUR C COMP VIS; Yu G, 2011, PROC CVPR IEEE, P865, DOI 10.1109/CVPR.2011.5995488; Yuan J., 2011, IEEE T PATTERN ANAL; Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPR.2009.5206671, 10.1109/CVPRW.2009.5206671]	50	9	10	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	3			SI		239	253		10.1007/s11263-016-0905-6	http://dx.doi.org/10.1007/s11263-016-0905-6			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FE					2022-12-18	WOS:000380270000003
J	Pizarro, D; Khan, R; Bartoli, A				Pizarro, Daniel; Khan, Rahat; Bartoli, Adrien			Schwarps: Locally Projective ImageWarps Based on 2D Schwarzian Derivatives	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Schwarzian penalizer; Bending energy; Projective differential invariants; Image warps	RECOGNITION; INVARIANTS; CURVES; SHAPE; DECOMPOSITION; MAPS	Image warps -or just warps-capture the geometric deformation existing between two images of a deforming surface. The current approach to enforce awarp's smoothness is to penalize its second order partial derivatives (Bookstein in IEEE Trans Pattern Anal Mach Intell 11: 567-585, 1989; Rueckert et al. in IEEE Trans Med Imaging 18: 712-721, 1999). Because this favors locally affine warps, this fails to capture the local projective component of the image deformation. This may have a negative impact on applications such as image registration and deformable 3D reconstruction. We propose a novel penalty designed to smooth the warp while capturing the deformation's local projective structure. Our penalty is based on equivalents to the Schwarzian derivatives, which are projective differential invariants exactly preserved by homographies. We propose a methodology to derive a set of partial differential equations with only homographies as solutions. We call this system the Schwarzian equations and we explicitly derive them for 2D functions using differential properties of homographies. We name as Schwarp a warp which is estimated by penalizing the residual of Schwarzian equations. Experimental evaluation shows that Schwarps outperform existingwarps in modeling and extrapolation power, and lead to better results in three deformable reconstruction methods, namely, shape reconstruction in shape-fromtemplate, camera calibration in Shape-from-Template and Non-Rigid Structure-from-Motion.	[Pizarro, Daniel; Khan, Rahat; Bartoli, Adrien] CNRS UdA, UMR 6284, ISIT, Clermont Ferrand, France		Khan, R (corresponding author), CNRS UdA, UMR 6284, ISIT, Clermont Ferrand, France.	rahatkhanr@gmail.com		Pizarro, Daniel/0000-0003-0622-4884	EU [307483 FLEXABLE]	EU(European Commission)	This research has received funding from the EUs FP7 ERC research Grant 307483 FLEXABLE.	Bartoli A., 2013, COMPUTER VISION PATT; Bartoli A., 2012, COMPUTER VISION PATT; Bartoli A, 2010, INT J COMPUT VISION, V88, P85, DOI 10.1007/s11263-009-0303-4; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Bregler Christoph, 2000, COMPUTER VISION PATT; BRUCKSTEIN AM, 1995, ANN MATH ARTIF INTEL, V13, P227, DOI 10.1007/BF01530829; Calabi E, 1998, INT J COMPUT VISION, V26, P107, DOI 10.1023/A:1007992709392; Carlsson S, 1996, INT J COMPUT VISION, V17, P193, DOI 10.1007/BF00058751; Carlsson S, 1996, INT J COMPUT VISION, V19, P211, DOI 10.1007/BF00055145; Cayley A., 1880, T CAMBRIDGE PHILOS S, V13; Coddington EA., 2012, INTRO ORDINARY DIFFE; Hartley R., 2003, MULTIPLE VIEW GEOMET; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Khan R., 2014, ECCV; Kummer E., 1836, J REINE ANGEW MATH, V15, P39, DOI [DOI 10.1515/CRLL.1836.15.39, 10.1515/crll.1836.15.39]; Lazebnik S, 2005, INT J COMPUT VISION, V63, P65, DOI 10.1007/s11263-005-4947-4; LEI G, 1990, IEEE T ROBOTIC AUTOM, V6, P432, DOI 10.1109/70.59368; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Malgouyres R, 2009, BRIT MACH VIS C; Matsumoto K., 1993, MEM FS KYUSHU U A, V47, P283; Molzon R, 1996, T AM MATH SOC, V348, P3015, DOI 10.1090/S0002-9947-96-01590-5; MOONS T, 1995, INT J COMPUT VISION, V14, P25, DOI 10.1007/BF01421487; Mundy J., 1992, GEOMETRIC INVARIANCE; Oda T, 1975, SCHWARZIAN DERIVATIV, V226, P82; Olver PJ, 2007, J MATH ANAL APPL, V333, P450, DOI 10.1016/j.jmaa.2006.12.029; OSGOOD B, 1992, DUKE MATH J, V67, P57, DOI 10.1215/S0012-7094-92-06704-4; Ovsienko V., 1989, VESTN MOSK U MAT M+, V44, P8; Ovsienko V., 2009, NOT AM MATH SOC, V56, P34; Ovsienko V., 2005, PROJECTIVE DIFFERENT; Perriollat M, 2011, INT J COMPUT VISION, V95, P124, DOI 10.1007/s11263-010-0352-8; Pilet J, 2008, INT J COMPUT VISION, V76, P109, DOI 10.1007/s11263-006-0017-9; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Salzmann M, 2007, IEEE T PATTERN ANAL, V29, P1481, DOI 10.1109/TPAMI.2007.1080; Sato J, 1998, INT J COMPUT VISION, V28, P117, DOI 10.1023/A:1008011016516; SINGER D, 1978, SIAM J APPL MATH, V35, P260, DOI 10.1137/0135020; Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009; Tanner C., 2005, THESIS; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; VANGOOL L, 1995, IMAGE VISION COMPUT, V13, P259, DOI 10.1016/0262-8856(95)99715-D; Varol A., 2009, INT C COMP VIS; WEISS I, 1993, INT J COMPUT VISION, V10, P207, DOI 10.1007/BF01539536	43	9	9	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	2					93	109		10.1007/s11263-016-0882-9	http://dx.doi.org/10.1007/s11263-016-0882-9			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FA					2022-12-18	WOS:000380269600001
J	Chabiron, O; Malgouyres, F; Tourneret, JY; Dobigeon, N				Chabiron, Olivier; Malgouyres, Francois; Tourneret, Jean-Yves; Dobigeon, Nicolas			Toward Fast Transform Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dictionary learning; Matrix factorization; Fast transform; Sparse representation; Global optimization; Gauss-Seidel	CONVERGENCE; IMAGE; DECOMPOSITION; ALGORITHM; DICTIONARIES; MINIMIZATION; SIGNAL	This paper introduces a new dictionary learning strategy based on atoms obtained by translating the composition of convolutions with -sparse kernels of known support. The dictionary update step associated with this strategy is a non-convex optimization problem. We propose a practical formulation of this problem and introduce a Gauss-Seidel type algorithm referred to as alternative least square algorithm for its resolution. The search space of the proposed algorithm is of dimension , which is typically smaller than the size of the target atom and much smaller than the size of the image. Moreover, the complexity of this algorithm is linear with respect to the image size, allowing larger atoms to be learned (as opposed to small patches). The conducted experiments show that we are able to accurately approximate atoms such as wavelets, curvelets, sinc functions or cosines for large values of K. The proposed experiments also indicate that the algorithm generally converges to a global minimum for large values of and .	[Chabiron, Olivier; Tourneret, Jean-Yves; Dobigeon, Nicolas] ENSEEIHT, CNRS, UMR 5505, IRIT, Toulouse, France; [Malgouyres, Francois] Univ Toulouse, CNRS, IMT, UMR 5219, Toulouse, France	Universite de Toulouse; Universite Federale Toulouse Midi-Pyrenees (ComUE); Universite Toulouse III - Paul Sabatier; Institut National Polytechnique de Toulouse; Universite Toulouse 1 Capitole; Universite de Toulouse - Jean Jaures; Centre National de la Recherche Scientifique (CNRS); Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Mathematical Sciences (INSMI); Universite de Toulouse; Universite Toulouse 1 Capitole; Universite Toulouse III - Paul Sabatier; Universite de Toulouse - Jean Jaures; Institut National des Sciences Appliquees de Toulouse; Universite Federale Toulouse Midi-Pyrenees (ComUE)	Chabiron, O (corresponding author), ENSEEIHT, CNRS, UMR 5505, IRIT, Toulouse, France.	olivier.chabiron@n7.fr	Dobigeon, Nicolas/O-6479-2018	Dobigeon, Nicolas/0000-0001-8127-350X	 [ANR-11-LABX-0040-CIMI];  [ANR-11-IDEX-0002-02]	; 	The authors would like to thank Jose Bioucas-Dias, Jalal Fadili, Remi Gribonval and Julien Mairal for their fruitful remarks on this work. Olivier Chabiron is supported by ANR-11-LABX-0040-CIMI within the program ANR-11-IDEX-0002-02.	Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; ALDROUBI A, 1992, SIGNAL PROCESS, V28, P127, DOI 10.1016/0165-1684(92)90030-Z; Attouch H, 2013, MATH PROGRAM, V137, P91, DOI 10.1007/s10107-011-0484-9; Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1; Bertsekas D., 2003, CONVEX ANAL OPTIMIZA; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Cai JF, 2014, APPL COMPUT HARMON A, V37, P89, DOI 10.1016/j.acha.2013.10.001; Champagnat F, 1996, IEEE T SIGNAL PROCES, V44, P2988, DOI 10.1109/78.553473; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Chouzenoux E., 2013, 00945918 HAL; COHEN A, 1996, SUBBAND WAVELET TRAN; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995; Delsarte P., 1992, IEEE T SIGNAL PROCES, V42, P2955; Dobigeon N, 2010, IEEE T SIGNAL PROCES, V58, P2675, DOI 10.1109/TSP.2010.2041594; Duarte-Carvajalino JM, 2009, IEEE T IMAGE PROCESS, V18, P1395, DOI 10.1109/TIP.2009.2022459; Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1; Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624; Fadili JM, 2010, COMPUT SCI ENG, V12, P44, DOI 10.1109/MCSE.2010.14; Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7; Jenatton R., 2010, PROXIMAL METHODS SPA; Jenatton R, 2011, J MACH LEARN RES, V12, P2297; Kail G, 2012, IEEE T SIGNAL PROCES, V60, P2727, DOI 10.1109/TSP.2012.2190066; Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826; Lu W.-S., 2000, Signal Processing X Theories and Applications. Proceedings of EUSIPCO 2000. Tenth European Signal Processing Conference, P351; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; MACQ B, 1993, IEEE T SIGNAL PROCES, V41, P3568, DOI 10.1109/78.258099; Mailhe B., 2008, EUR SIGN PROC C; Mairal J, 2008, MULTISCALE MODEL SIM, V7, P214, DOI 10.1137/070697653; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Mairal J, 2010, J MACH LEARN RES, V11, P19; Malgouyres F, 2009, INT J COMPUT VISION, V83, P294, DOI 10.1007/s11263-009-0227-z; MULLER ME, 1959, COMMUN ACM, V2, P19, DOI 10.1145/377939.377946; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Ophir B, 2011, IEEE J-STSP, V5, P1014, DOI 10.1109/JSTSP.2011.2155032; Painter T, 2000, P IEEE, V88, P451, DOI 10.1109/5.842996; Peyre G, 2010, SIAM J IMAGING SCI, V3, P646, DOI 10.1137/090770783; PRINCEN JP, 1986, IEEE T ACOUST SPEECH, V34, P1153, DOI 10.1109/TASSP.1986.1164954; Quinsac C., 2011, P 3 INT WORKSH COMP; Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009; Rigamonti R., 2013, IEEE C COMP VIS PATT; Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551; Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477; Sallee Phil, 2002, ADV NEURAL INFORM PR, P1327; Starck JL, 2007, IEEE T IMAGE PROCESS, V16, P297, DOI 10.1109/TIP.2006.887733; Sylvain L, 2005, INT CONF ACOUST SPEE, P293; Thiagarajan JJ, 2011, 2011 IEEE DIGITAL SIGNAL PROCESSING WORKSHOP AND IEEE SIGNAL PROCESSING EDUCATION WORKSHOP (DSP/SPE), P271, DOI 10.1109/DSP-SPE.2011.5739224; Tsiligkaridis T, 2013, IEEE T SIGNAL PROCES, V61, P1743, DOI 10.1109/TSP.2013.2240157; Uhl A, 1996, IMAGE VISION COMPUT, V14, P365, DOI 10.1016/0262-8856(96)89801-5; Whittaker Edmund Taylor, 1915, P R SOC EDINB B, V35, P181, DOI DOI 10.1017/S0370164600017806; Wiesel A, 2012, IEEE T SIGNAL PROCES, V60, P6182, DOI 10.1109/TSP.2012.2218241	52	9	10	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2015	114	2-3			SI		195	216		10.1007/s11263-014-0771-z	http://dx.doi.org/10.1007/s11263-014-0771-z			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CP7MJ		Green Submitted			2022-12-18	WOS:000360071900006
J	Gonzales, C; Dubuisson, S				Gonzales, Christophe; Dubuisson, Severine			Combinatorial Resampling Particle Filter: An Effective and Efficient Method for Articulated Object Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Particle filter; Resampling; Dynamic Bayesian networks; Articulated object tracking	SWARM OPTIMIZATION; BELIEF PROPAGATION; VISUAL TRACKING; HUMAN MOTION	Particle filter (PF) is a method dedicated to posterior density estimations using weighted samples whose elements are called particles. In particular, this approach can be applied to object tracking in video sequences in complex situations and, in this paper, we focus on articulated object tracking, i.e., objects that can be decomposed as a set of subparts. One of PF's crucial step is a resampling step in which particles are resampled to avoid degeneracy problems. In this paper, we propose to exploit mathematical properties of articulated objects to swap conditionally independent subparts of the particles in order to generate new particle sets. We then introduce a new resampling method called Combinatorial Resampling that resamples over the particle set resulting from all the "admissible" swappings, the so-called combinatorial set. In essence, combinatorial resampling (CR) is quite similar to the combination of a crossover operator and a usual resampling, but there exists a fundamental difference between CR and the use of crossover operators: we prove that CR is sound, i.e., in a Bayesian framework, it is guaranteed to represent without any bias the posterior densities of the states over time. By construction, the particle sets produced by CR better represent the density to estimate over the whole state space than the original set and, therefore, CR produces higher quality samples. Unfortunately, the combinatorial set is generally of an exponential size and, therefore, to be scalable, we show how it can be implicitly constructed and resampled from, thus resulting in both an efficient and effective resampling scheme. Finally, through experimentations both on challenging synthetic and real video sequences, we also show that our resampling method outperforms all classical resampling methods both in terms of the quality of its results and in terms of computation times.	[Gonzales, Christophe] Univ Paris 06, Sorbonne Univ, CNRS, LIP6,UMR 7606, F-75005 Paris, France; [Dubuisson, Severine] Univ Paris 06, Sorbonne Univ, CNRS, ISIR,UMR 7222, F-75005 Paris, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	Dubuisson, S (corresponding author), Univ Paris 06, Sorbonne Univ, CNRS, ISIR,UMR 7222, F-75005 Paris, France.	christophe.gonzales@lip6.fr; severine.dubuisson@isir.upmc.fr						Andriluka M, 2008, PROC CVPR IEEE, P1873, DOI 10.1109/CVPR.2008.4587583; Artner NM, 2011, PATTERN RECOGN, V44, P800, DOI 10.1016/j.patcog.2010.10.025; Balan A. O., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P349; Bernier O, 2009, COMPUT VIS IMAGE UND, V113, P29, DOI 10.1016/j.cviu.2008.07.001; Besada-Portas E, 2009, LECT NOTES ARTIF INT, V5781, P131, DOI 10.1007/978-3-642-04180-8_26; Bhattacharyya A., 1943, BULL CALCUTTA MATH S, V35, P99; Bray M, 2007, COMPUT VIS IMAGE UND, V106, P116, DOI 10.1016/j.cviu.2005.09.013; Bray M, 2007, IMAGE VISION COMPUT, V25, P352, DOI 10.1016/j.imavis.2005.10.009; Bray M., 2004, IEEE WORKSH ART NONR, P1; Brubaker M. A., 2007, CVPR; Brubaker MA, 2010, INT J COMPUT VISION, V87, P140, DOI 10.1007/s11263-009-0274-5; Chang IC, 2010, PATTERN RECOGN, V43, P3621, DOI 10.1016/j.patcog.2010.05.003; Chang WY, 2008, IEEE T IMAGE PROCESS, V17, P1154, DOI 10.1109/TIP.2008.924283; Chen Z., 2003, BAYESIAN FILTERING K; Covell MM, 2000, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2000.854875; Darby J, 2008, INT C PATT RECOG, P3125; DARBY J, 2009, BMVC, P1; Das S, 2011, SWARM EVOL COMPUT, V1, P71, DOI 10.1016/j.swevo.2011.05.005; De Campos T, 2006, THESIS SAINT ANNES C; de Chaumont F, 2010, IEEE IMAGE PROC, P4637, DOI 10.1109/ICIP.2010.5649509; Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c; Douc R, 2005, ISPA 2005: Proceedings of the 4th International Symposium on Image and Signal Processing and Analysis, P64, DOI 10.1109/ISPA.2005.195385; Doucet A., 2001, SEQUENTIAL MONTE CAR; Doucet A., 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.; Dubuisson S., 2012, P UAI 12, P237; Duffner S., 2009, BRIT MACH VIS C, P1; Gonzalez J., 2011, 14 INT C ART INT STA; Gross R., 2001, CMURITR0118 ROB I; Guo F, 2008, J ELECTRON MATER, V37, P1, DOI 10.1007/s11664-007-0302-6; Han T.X., 2006, IEEE COMPUTER SOC C, V1, P214; HAUBERG S., 2010, AS C COMP VIS, P758; Hauberg S, 2011, INT J COMPUT VISION, V94, P317, DOI 10.1007/s11263-011-0433-3; Hauberg S, 2010, LECT NOTES COMPUT SC, V6311, P425, DOI 10.1007/978-3-642-15549-9_31; Hofmann M, 2011, COMPUT VIS IMAGE UND, V115, P1559, DOI 10.1016/j.cviu.2011.08.002; Ihler AT, 2004, IPSN '04: THIRD INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P225; Isard M, 2003, PROC CVPR IEEE, P613; John M., 2000, LECT NOTES COMPUTER, P3, DOI DOI 10.1007/3-540-45053-X1; John V, 2010, IMAGE VISION COMPUT, V28, P1530, DOI 10.1016/j.imavis.2010.03.008; Kanazawa K., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P346; Kitagawa G., 1996, J OF COMPUTATIONAL A, V5, P1, DOI DOI 10.2307/1390750; Kjellstrom H, 2010, PROC CVPR IEEE, P747, DOI 10.1109/CVPR.2010.5540140; Krzeszowski T, 2010, LECT NOTES COMPUT SC, V6374, P147, DOI 10.1007/978-3-642-15910-7_17; Kwok NM, 2006, J INTELL ROBOT SYST, V46, P365, DOI 10.1007/s10846-006-9066-0; Kwon J, 2013, IEEE T PATTERN ANAL, V35, P1011, DOI 10.1109/TPAMI.2012.161; Lanz O, 2006, IEEE T PATTERN ANAL, V28, P1436, DOI 10.1109/TPAMI.2006.177; Li R, 2010, INT J COMPUT VISION, V87, P170, DOI 10.1007/s11263-009-0283-4; Li ST, 2011, IEEE T SYST MAN CY B, V41, P1003, DOI 10.1109/TSMCB.2010.2103055; Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847; MacCormick J., 2000, THESIS OXFORD U OXFO; Massey B, 2008, INT CONF ACOUST SPEE, P3457, DOI 10.1109/ICASSP.2008.4518395; Murphy KP., 2002, THESIS UC BERKELEY; Oikonomidis I, 2012, PROC CVPR IEEE, P1862, DOI 10.1109/CVPR.2012.6247885; Oikonomidis I, 2011, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2011.6126483; Pantrigo JJ, 2008, PATTERN RECOGN LETT, V29, P1160, DOI 10.1016/j.patrec.2007.12.012; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Raskin L, 2011, COMPUT VIS IMAGE UND, V115, P503, DOI 10.1016/j.cviu.2010.12.002; Rose C., 2008, AAAI, P1396; Sigal L., 2003, NIPS, P1539; SMITH K, 2004, BRIT MACH VIS C, P25; Subakan O, 2007, IEEE I CONF COMP VIS, P708; Sudderth E., 2004, P IEEE C COMP VIS PA, P189; Sudderth EB, 2010, COMMUN ACM, V53, P95, DOI 10.1145/1831407.1831431; Taylor GW, 2010, PROC CVPR IEEE, P631, DOI 10.1109/CVPR.2010.5540157; Ukita N, 2012, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2012.6248049; Vondrak M., 2008, P IEEE C COMP VIS PA, P1; Wang QC, 2006, LECT NOTES ARTIF INT, V4114, P1216; Widynski N, 2012, COMPUT VIS IMAGE UND, V116, P1076, DOI 10.1016/j.cviu.2012.07.002; Nguyen XS, 2013, LECT NOTES COMPUT SC, V8047, P319, DOI 10.1007/978-3-642-40261-6_38; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085	71	9	10	0	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2015	112	3					255	284		10.1007/s11263-014-0763-z	http://dx.doi.org/10.1007/s11263-014-0763-z			30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CG3EA					2022-12-18	WOS:000353159600001
J	Wetzstein, G; Heidrich, W; Raskar, R				Wetzstein, Gordon; Heidrich, Wolfgang; Raskar, Ramesh			Computational Schlieren Photography with Light Field Probes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computational photography; Light transport; Fluid imaging; Shape from x	SURFACE; TRANSPARENT	We introduce a new approach to capturing refraction in transparent media, which we call light field background oriented Schlieren photography. By optically coding the locations and directions of light rays emerging from a light field probe, we can capture changes of the refractive index field between the probe and a camera or an observer. Our prototype capture setup consists of inexpensive off-the-shelf hardware, including inkjet-printed transparencies, lenslet arrays, and a conventional camera. By carefully encoding the color and intensity variations of 4D light field probes, we show how to code both spatial and angular information of refractive phenomena. Such coding schemes are demonstrated to allow for a new, single image approach to reconstructing transparent surfaces, such as thin solids or surfaces of fluids. The captured visual information is used to reconstruct refractive surface normals and a sparse set of control points independently from a single photograph.	[Wetzstein, Gordon; Raskar, Ramesh] MIT Media Lab, Cambridge, MA 02139 USA; [Heidrich, Wolfgang] Univ British Columbia, Vancouver, BC V5Z 1M9, Canada	Massachusetts Institute of Technology (MIT); University of British Columbia	Wetzstein, G (corresponding author), MIT Media Lab, Cambridge, MA 02139 USA.	gordonw@media.mit.edu		Wetzstein, Gordon/0000-0002-9243-6885	NSERC; Alfred P. Sloan Research Fellowship; DARPA	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Alfred P. Sloan Research Fellowship(Alfred P. Sloan Foundation); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	Gordon Wetzstein was supported by an NSERC Postdoctoral Fellowship. Ramesh Raskar was supported by an Alfred P. Sloan Research Fellowship and a DARPA Young Faculty Award.	Agarwal S, 2004, LECT NOTES COMPUT SC, V3022, P483; Agrawal A., 2006, P ECCV, P483; Arpa A., 2012, P IEEE PROCAMS; Atcheson B., 2010, P VMV; Atcheson B, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409085; Ben-Ezra M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1025; Bonfort T, 2006, LECT NOTES COMPUT SC, V3852, P872; Born M., 1999, PRINCIPLES OPTICS, Vseventh, DOI DOI 10.1017/CBO9781139644181; Dalziel SB, 2000, EXP FLUIDS, V28, P322, DOI 10.1007/s003480050391; Gao C., 2006, 2006 IEEE COMP SOC C, V2, P2316; Hargather MJ, 2010, EXP FLUIDS, V48, P59, DOI 10.1007/s00348-009-0709-3; HERNANDEZ C, 2007, P ICCV; Horovitz I, 2004, IMAGE VISION COMPUT, V22, P681, DOI 10.1016/j.imavis.2004.01.005; HOWES WL, 1984, APPL OPTICS, V23, P2449, DOI 10.1364/AO.23.002449; Hullin MB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360686; Huynh C. P., 2010, P CVPR; Ihrke I, 2010, COMPUT GRAPH FORUM, V29, P2400, DOI 10.1111/j.1467-8659.2010.01753.x; Ihrke L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451,1239510; Ihrke N, 2005, IEEE I CONF COMP VIS, P1055; Kutulakos K.N., 2005, P ICCV; Levoy M, 2009, J MICROSC-OXFORD, V235, P144, DOI 10.1111/j.1365-2818.2009.03195.x; Merzkirch W., 1987, FLOW VISUALIZATION, V2nd; Miyazaki D, 2005, PROC CVPR IEEE, P910; Morris N. J. W., 2005, P ICCV; MORRIS NJW, 2007, P ICCV; Murase H, 1990, P 3 INT C COMP VIS O, P313; Murphy D B, 2001, FUNDAMENTALS LIGHT M; Nehab D., 2005, ACM T GRAPHIC, P24; Ng HS, 2010, IEEE T PATTERN ANAL, V32, P2085, DOI 10.1109/TPAMI.2009.183; Okano F, 1999, OPT ENG, V38, P1072, DOI 10.1117/1.602152; Savarese S, 2002, LECT NOTES COMPUT SC, V2351, P759; Schardin H., 1942, ERG EXAKT NATURWISS, V20, P303, DOI [DOI 10.1007/BFB0111981, 10.1007/BFb0111981]; Settles G. S., 2001, EXP FLUID MECH, DOI 10.1007/978-3-642-56640-0; Settles GS, 2010, P 14 INT S FLOW VIS, P267; Shimizu M., 2006, P CVR, P1; Tarini M, 2005, GRAPH MODELS, V67, P233, DOI 10.1016/j.gmod.2004.11.002; Trifonov B., 2006, PROC EUROGRAPHICS C, P51; Wetzstein G., 2011, INT C COMP VIS ICCV; Wetzstein G., 2011, 2011 IEEE INT C COMP, P1; Wetzstein G, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185576; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; ZHANG X, 1994, EXP FLUIDS, V17, P225; Zongker DE, 1999, COMP GRAPH, P205, DOI 10.1145/311535.311558	43	9	9	0	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2014	110	2			SI		113	127		10.1007/s11263-013-0652-x	http://dx.doi.org/10.1007/s11263-013-0652-x			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AY5UE		Green Submitted			2022-12-18	WOS:000347636400003
J	Badrinarayanan, V; Budvytis, I; Cipolla, R				Badrinarayanan, Vijay; Budvytis, Ignas; Cipolla, Roberto			Mixture of Trees Probabilistic Graphical Model for Video Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video Segmentation; Semi-supervised learning; Mixture of trees probabilistic graphical model; Structured variational inference		We present a novel mixture of trees probabilistic graphical model for semi-supervised video segmentation. Each component in this mixture represents a tree structured temporal linkage between super-pixels from the first to the last frame of a video sequence. We provide a variational inference scheme for this model to estimate super-pixel labels, their corresponding confidences, as well as the confidences in the temporal linkages. Our algorithm performs inference over full video volume which helps to avoid erroneous label propagation caused by using short time-window processing. In addition, our proposed inference scheme is very efficient both in terms of computational speed and use of RAM and so can be applied in real-time video segmentation scenarios. We bring out the pros and cons of our approach using extensive quantitative comparisons on challenging binary and multi-class video segmentation datasets.	[Badrinarayanan, Vijay; Budvytis, Ignas; Cipolla, Roberto] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	Badrinarayanan, V (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.	vb292@cam.ac.uk; ib255@cam.ac.uk; rc10001@cam.ac.uk	Arandjelović, Ognjen/V-5255-2019	Arandjelović, Ognjen/0000-0002-9314-194X; Cipolla, Roberto/0000-0002-8999-2151				Achanta R., 2010, TECHNICAL REPORT; Badrinarayanan V., 2010, CVPR; Badrinarayanan V, 2013, IEEE T PATTERN ANAL, V35, P2751, DOI 10.1109/TPAMI.2013.54; Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376; Bishop CM, 2006, PATTERN RECOGNITION; Boykov Y., 1999, ICCV; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Budvytis I., 2010, BMVC; Budvytis I., 2012, BMVC; Budvytis I., 2011, CVPR; Chen A. Y. C., 2010, Proceedings of 2010 Western New York Image Processing Workshop (WNYIPW), P14, DOI 10.1109/WNYIPW.2010.5649773; Cheng H.-T., 2012, CVPR; CHEUNG V, 2005, CVPR; Chockalingam P., 2009, ICCV; Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Criminisi A., 2013, DECISION FORESTCOM; Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910; Fathi A., 2011, BMVC; Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893; Kannan A., 2006, NIPS, V19; Kohli P, 2005, IEEE I CONF COMP VIS, P922; Lee K.C., 2003, CVPR; Lezama J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3369, DOI 10.1109/CVPR.2011.6044588; Nagaraja N.S., 2012, LNCS; Saul L. K., 1996, NIPS; Settles B., 2012, TECHNICAL REPORT; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Tsai David, 2010, BMVC, DOI [10.5244/C.24.56, DOI 10.5244/C.24.56]; Turner R. E., 2008, WORKSH INF EST PROB; Vazquez-Reina A., 2010, ECCV; Vijayanarasimhan S., 2012, ECCV; Wang TH, 2012, IEEE T MULTIMEDIA, V14, P389, DOI 10.1109/TMM.2011.2177078; Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45	37	9	9	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2014	110	1			SI		14	29		10.1007/s11263-013-0673-5	http://dx.doi.org/10.1007/s11263-013-0673-5			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AQ0BZ					2022-12-18	WOS:000342448000003
J	Zeng, W; Shi, R; Wang, YL; Yau, ST; Gu, XF				Zeng, Wei; Shi, Rui; Wang, Yalin; Yau, Shing-Tung; Gu, Xianfeng		Alzheimer's Dis Neuroimaging Initi	Teichmuller Shape Descriptor and Its Application to Alzheimer's Disease Study	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Teichmuller space; Conformal welding; Shape analysis	SURFACE CONFORMAL PARAMETERIZATION; BRAIN ATROPHY; MRI; SEGMENTATION; MORPHOMETRY	We propose a novel method to apply Teichmuller space theory to study the signature of a family of nonintersecting closed 3D curves on a general genus zero closed surface. Our algorithm provides an efficient method to encode both global surface and local contour shape information. The signature-Teichmuller shape descriptor-is computed by surface Ricci flow method, which is equivalent to solving an elliptic partial differential equation on surfaces and is numerically stable. We propose to apply the new signature to analyze abnormalities in brain cortical morphometry. Experimental results with 3D MRI data from Alzheimer's disease neuroimaging initiative (ADNI) dataset [152 healthy control subjects versus 169 Alzheimer's disease (AD) patients] demonstrate the effectiveness of our method and illustrate its potential as a novel surface-based cortical morphometry measurement in AD research.	[Zeng, Wei] Florida Int Univ, Miami, FL 33199 USA; [Shi, Rui; Gu, Xianfeng] SUNY Stony Brook, Stony Brook, NY 11794 USA; [Wang, Yalin] Arizona State Univ, Tempe, AZ USA; [Yau, Shing-Tung] Harvard Univ, Cambridge, MA 02138 USA	State University System of Florida; Florida International University; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Arizona State University; Arizona State University-Tempe; Harvard University	Zeng, W (corresponding author), Florida Int Univ, Miami, FL 33199 USA.	wzeng@cs.fiu.edu; rshi@cs.sunysb.edu; ylwang@asu.edu; yau@math.harvard.edu; gu@cs.sunysb.edu	Zeng, Wei/J-6474-2014	Gu, Xianfeng/0000-0001-8226-5851; Wang, Yalin/0000-0002-6241-735X	NIH [R01EB007530 0A1, P30 AG010129, K01 AG030514]; NSF [IIS0916286, CCF0916235, CCF0830550, III0713145, CCF-0448399, DMS0528363, DMS-0626223, CCF-0830550, IIS-0916286, CCF-1081424]; ONR [N000140910228]; NSFC [61202146]; SDC [BS2012DX014]; Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health) [U01 AG024904]; National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering; Canadian Institutes of Health Research; Dana Foundation	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); NSFC(National Natural Science Foundation of China (NSFC)); SDC; Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS)); National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute on Aging (NIA)NIH National Institute of Biomedical Imaging & Bioengineering (NIBIB)); Canadian Institutes of Health Research(Canadian Institutes of Health Research (CIHR)); Dana Foundation	This work was supported by NIH R01EB007530 0A1, NSF IIS0916286, NSF CCF0916235, NSF CCF0830550, NSF III0713145, and ONR N000140910228, NSFC 61202146, and SDC BS2012DX014. Data collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: Abbott; Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Amorfix Life Sciences Ltd.; AstraZeneca; Bayer Healthcare; BioClinica, Inc.; Biogen Idec Inc.; Bristol-Myers Squibb Company; Eisai Inc.; Elan Pharmaceuticals Inc.; EliLilly andCompany; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; GE Healthcare; Innogenetics, N. V.; JanssenAlzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Medpace, Inc.; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Servier; Synarc Inc.; and Takeda Pharmaceutical Company. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (http://www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the AD Cooperative Study at the University of California, San Diego. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of California, Los Angeles. This research was also supported by NIH grants P30 AG010129, K01 AG030514, and the Dana Foundation. This work has been supported by NSF CCF-0448399, NSF DMS0528363, NSF DMS-0626223, NSF CCF-0830550, NSF IIS-0916286, NSF CCF-1081424, and ONR N000140910228. Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni. loni. ucla. edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. Acomplete listing ofADNIinvestigators can be found at: http://adni.loni.ucla.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.	[Anonymous], 2000, QUASICONFORMAL TEICH; Ashburner J, 1998, HUM BRAIN MAPP, V6, P348, DOI 10.1002/(SICI)1097-0193(1998)6:5/6<348::AID-HBM4>3.0.CO;2-P; Chincarini A, 2011, NEUROIMAGE, V58, P469, DOI 10.1016/j.neuroimage.2011.05.083; Chow B., 2006, HAMILTONS RICCI FLOW; Chung MK, 2005, NEUROIMAGE, V25, P1256, DOI 10.1016/j.neuroimage.2004.12.052; Chung MK, 2008, IEEE T MED IMAGING, V27, P1143, DOI 10.1109/TMI.2008.918338; Cuingnet R, 2011, NEUROIMAGE, V56, P766, DOI 10.1016/j.neuroimage.2010.06.013; Dale AM, 1999, NEUROIMAGE, V9, P179, DOI 10.1006/nimg.1998.0395; Davies R. H., 2003, INT C INF PROC MED I; Desikan RS, 2006, NEUROIMAGE, V31, P968, DOI 10.1016/j.neuroimage.2006.01.021; Farkas H.M., 1991, GRADUATE TEXTS MATH; Fischl B, 1999, NEUROIMAGE, V9, P195, DOI 10.1006/nimg.1998.0396; Fox NC, 1999, NEUROLOGY, V52, P1687, DOI 10.1212/WNL.52.8.1687; Frisoni GB, 2010, NAT REV NEUROL, V6, P67, DOI 10.1038/nrneurol.2009.215; Gerig G, 2001, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P171, DOI 10.1109/MMBIA.2001.991731; Gorczowski K., 2007, IEEE C COMP VIS PATT, V2007, P1; Gu XF, 2004, IEEE T MED IMAGING, V23, P949, DOI 10.1109/TMI.2004.831226; Guo XJ, 2010, NEUROSCI LETT, V468, P146, DOI 10.1016/j.neulet.2009.10.086; Haker S, 2000, IEEE T MED IMAGING, V19, P665, DOI 10.1109/42.875181; Hamilton R.S., 1988, MATH GEN RELATIVITY, V71, P237, DOI DOI 10.1090/CONM/071/954419; HENRICI P, 1988, APPL COMPUTATIONAL C, V3; Hua X, 2010, NEUROIMAGE, V51, P63, DOI 10.1016/j.neuroimage.2010.01.104; Hurdal MK, 2004, NEUROIMAGE, V23, pS119, DOI 10.1016/j.neuroimage.2004.07.018; Jack CR, 2008, J MAGN RESON IMAGING, V27, P685, DOI 10.1002/jmri.21049; Jack CR, 2004, NEUROLOGY, V62, P591, DOI 10.1212/01.WNL.0000110315.26026.EF; Jin M, 2008, IEEE T VIS COMPUT GR, V14, P1030, DOI 10.1109/TVCG.2008.57; Lai RJ, 2010, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2010.5540023; Liu XW, 2010, INT J COMPUT VISION, V89, P69, DOI 10.1007/s11263-010-0323-0; Lui L. M., 2010, 11 EUR C COMP VIS EC; Mueller SG, 2005, NEUROIMAG CLIN N AM, V15, P869, DOI 10.1016/j.nic.2005.09.008; Pizer SM, 1999, IEEE T MED IMAGING, V18, P851, DOI 10.1109/42.811263; Qiu AQ, 2008, NEUROIMAGE, V42, P1430, DOI 10.1016/j.neuroimage.2008.04.257; Schoen R., 1994, C P LECT NOTES GEOME; SCHWARTZ EL, 1989, IEEE T PATTERN ANAL, V11, P1005, DOI 10.1109/34.35506; Sharon E, 2006, INT J COMPUT VISION, V70, P55, DOI 10.1007/s11263-006-6121-z; SHEN L, 2007, IEEE 7 INT C BIOINF; Shi YG, 2011, LECT NOTES COMPUT SC, V6801, P233, DOI 10.1007/978-3-642-22092-0_20; Thompson P, 1996, IEEE T MED IMAGING, V15, P402, DOI 10.1109/42.511745; Thompson PM, 2003, J NEUROSCI, V23, P994; Thurston W., 1980, GEOMETRY TOPOLOGY 3; Timsari B, 2000, PROC SPIE, V3979, P698, DOI 10.1117/12.387731; Tosun D, 2006, I S BIOMED IMAGING, P1172; Trouve A, 2005, FOUND COMPUT MATH, V5, P173, DOI 10.1007/s10208-004-0128-z; Wang YL, 2007, IEEE T MED IMAGING, V26, P853, DOI 10.1109/TMI.2007.895464; Wang YL, 2006, LECT NOTES COMPUT SC, V4191, P946; Wang YL, 2012, IEEE T MED IMAGING, V31, P251, DOI 10.1109/TMI.2011.2168233; Wang YL, 2011, NEUROIMAGE, V56, P1993, DOI 10.1016/j.neuroimage.2011.03.040; Wang YL, 2009, PROC CVPR IEEE, P202, DOI 10.1109/CVPRW.2009.5206578; Wang YL, 2008, LECT NOTES COMPUT SC, V5241, P585, DOI 10.1007/978-3-540-85988-8_70; Winkler AM, 2010, NEUROIMAGE, V53, P1135, DOI 10.1016/j.neuroimage.2009.12.028; Zeng W, 2008, METHODS APPL ANAL, V15, P539; Zeng W, 2010, IEEE T PATTERN ANAL, V32, P662, DOI 10.1109/TPAMI.2009.201; [No title captured]	53	9	10	1	31	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2013	105	2			SI		155	170		10.1007/s11263-012-0586-8	http://dx.doi.org/10.1007/s11263-012-0586-8			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	207YP		Green Submitted			2022-12-18	WOS:000323641100005
J	Lu, Z; Tai, YW; Deng, FB; Ben-Ezra, M; Brown, MS				Lu, Zheng; Tai, Yu-Wing; Deng, Fanbo; Ben-Ezra, Moshe; Brown, Michael S.			A 3D Imaging Framework Based on High-Resolution Photometric-Stereo and Low-Resolution Depth	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D Reconstruction; High resolution; Photometric stereo; Focal stack	SHAPE	This paper introduces a 3D imaging framework that combines high-resolution photometric stereo and low-resolution depth. Our approach targets imaging scenarios based on either macro-lens photography combined with focal stacking or a large-format camera that are able to image objects with more than 600 samples per mm. These imaging techniques allow photometric stereo algorithms to obtain surface normals at resolutions that far surpass corresponding depth values obtained with traditional approaches such as structured-light, passive stereo, or depth-from-focus. Our work offers two contributions for 3D imaging based on these scenarios. The first is a multi-resolution, patched-based surface reconstruction scheme that can robustly handle the significant resolution difference between our surface normals and depth samples. The second is a method to improve the initial normal estimation by using all the available focal information for images obtained using a focal stacking technique.	[Lu, Zheng; Deng, Fanbo; Brown, Michael S.] Natl Univ Singapore, Singapore 127110, Singapore; [Tai, Yu-Wing] Korea Adv Inst Sci & Technol, Dept Comp Sci, Taejon 305701, South Korea; [Ben-Ezra, Moshe] Microsoft Res Asia, Beijing 10080, Peoples R China	National University of Singapore; Korea Advanced Institute of Science & Technology (KAIST); Microsoft; Microsoft Research Asia	Lu, Z (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	luzheng@cs.utexas.edu; yuwing@cs.kaist.ac.kr; dfanbo@comp.nus.edu.sg; mosheb@microsoft.com; brown@comp.nus.edu.sg	Tai, Yu Wing/C-2047-2011; Lu, Zheng/B-1537-2009	Tai, Yu Wing/0000-0002-3148-0380; Lu, Zheng/0000-0003-4098-2486; LU, Zheng/0000-0002-7799-1776				Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718; AGRAWAL A, 2006, EUR C COMP VIS ECCV; Agrawal A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531401; [Anonymous], 1998, ANAGRAMM DIGITAL REP; Banerjee S., 1992, 11 IAPR INT C PATT R; Bernardini F, 2002, IEEE COMPUT GRAPH, V22, P59, DOI 10.1109/38.974519; Chen C. Y., 2003, P COMP AN IM PATT CA; Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236; Darrell T., 1988, COMPUTER VISION PATT; Fua P., 1994, EUR C COMP VIS ECCV; Hausler G., 1972, OPT COMMUN, V6, P38; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Higo T., 2009, COMPUTER VISION PATT; Horn B.K.P., 1989, SHAPE SHADING; IKEUCHI K, 1987, INT J ROBOT RES, V6, P15, DOI 10.1177/027836498700600102; Lange H., 1999, P 3DIM; Lu Z., 2010, COMPUTER VISION PATT; Malik AS, 2008, PATTERN RECOGN, V41, P2200, DOI 10.1016/j.patcog.2007.12.014; Nayar SK, 1997, INT J COMPUT VISION, V21, P163, DOI 10.1023/A:1007937815113; NAYAR SK, 1994, IEEE T PATTERN ANAL, V16, P824, DOI 10.1109/34.308479; Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226; Reid J., 2009, ACM T MATH SOFTWARE, V36, P1; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; SCHARSTEIN D, 2003, COMPUTER VISION PATT; Seitz Steven M, 2006, COMPUTER VISION PATT; TERZOPOULOS D, 1988, IEEE T PATTERN ANAL, V10, P417, DOI 10.1109/34.3908; Vlasic Daniel, 2009, ACM T GRAPHIC, P1; WHOLER C, 2009, 3D COMPUTER VISION E; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu T. P., 2006, COMPUTER VISION PATT; Wu TP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409072; Xiong Y., 1993, COMPUTER VISION PATT	32	9	11	2	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2013	102	1-3					18	32		10.1007/s11263-012-0589-5	http://dx.doi.org/10.1007/s11263-012-0589-5			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	097VO					2022-12-18	WOS:000315501800003
J	Lehment, N; Kaiser, M; Rigoll, G				Lehment, Nicolas; Kaiser, Moritz; Rigoll, Gerhard			Using Segmented 3D Point Clouds for Accurate Likelihood Approximation in Human Pose Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human pose tracking; Depth image; Observation likelihood approximation; Stochastic tracking; Parallel computing	MOTION CAPTURE	The observation likelihood approximation is a central problem in stochastic human pose tracking. In this article we present a new approach to quantify the correspondence between hypothetical and observed human poses in depth images. Our approach is based on segmented point clouds, enabling accurate approximations even under conditions of self-occlusion and in the absence of color or texture cues. The segmentation step extracts small regions of high saliency such as hands or arms and ensures that the information contained in these regions is not marginalized by larger, less salient regions such as the chest. To enable the rapid, parallel evaluation of many poses, a fast ellipsoid body model is used which handles occlusion and intersection detection in an integrated manner. The proposed approximation function is evaluated on both synthetic and real camera data. In addition, we compare our approximation function against the corresponding function used by a state-of-the-art pose tracker. The approach is suitable for parallelization on GPUs or multicore CPUs.	[Lehment, Nicolas; Kaiser, Moritz; Rigoll, Gerhard] Tech Univ Munich, Inst Human Machine Commun, D-80333 Munich, Germany	Technical University of Munich	Lehment, N (corresponding author), Tech Univ Munich, Inst Human Machine Commun, Arcisstr 21, D-80333 Munich, Germany.	lehment@tum.de; moritz.kaiser@tum.de; rigoll@tum.de	Kaiser, M. Shamim/F-5750-2012	Kaiser, M. Shamim/0000-0002-4604-5461				Azad Pedram, 2008, 2008 8th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2008), P700, DOI 10.1109/ICHR.2008.4755975; Baak A, 2011, IEEE I CONF COMP VIS, P1092, DOI 10.1109/ICCV.2011.6126356; Bernier O., 2008, COMPUTER VISION IMAG, V113, P29; Cayton L., 2011, P 1 INT WORKSH ACC D, P243; Darby J., 2008, BRIT MACH VIS C; Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c; Fontmarty M, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P3397; Fontmarty M, 2009, IEEE IMAGE PROC, P4101, DOI 10.1109/ICIP.2009.5413473; Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755; Ganapathi V, 2010, PROC CVPR IEEE, P755, DOI 10.1109/CVPR.2010.5540141; Girshick R, 2011, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2011.6126270; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Lehment NH, 2010, IEEE IMAGE PROC, P2465, DOI 10.1109/ICIP.2010.5651643; Lichtenauer J, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P767, DOI 10.1109/AFGR.2004.1301627; Lorentz H., 1915, KONENKLIJKE AKAD VAN, V18, P134; Markley FL, 2007, J GUID CONTROL DYNAM, V30, P1193, DOI 10.2514/1.28949; Mikic I, 2003, INT J COMPUT VISION, V53, P199, DOI 10.1023/A:1023012723347; OLEARY DP, 1990, SIAM J MATRIX ANAL A, V11, P466, DOI 10.1137/0611032; Poppe R, 2007, COMPUT VIS IMAGE UND, V108, P4, DOI 10.1016/j.cviu.2006.10.016; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187; Sudderth E. B., 2002, IEEE COMP SOC C COMP, V1, P605; Wilhelms J., 2001, EFFICIENT SPHERICAL; Youding Zhu, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P267	24	9	10	0	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2013	101	3			SI		482	497		10.1007/s11263-012-0557-0	http://dx.doi.org/10.1007/s11263-012-0557-0			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	086WF		Green Submitted			2022-12-18	WOS:000314719000006
J	Lenzen, F; Scherzer, O				Lenzen, Frank; Scherzer, Otmar			Partial Differential Equations for Zooming, Deinterlacing and Dejittering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Non-convex semigroups; Partial differential equations; Dejittering; Deinterlacing; Zooming	IMAGE INTERPOLATION; OPTICAL-FLOW; REGULARIZATION; PDE	In this paper, for imaging applications, we introduce partial differential equations (PDEs), which allow for correcting displacement errors, for dejittering, and for deinterlacing, respectively, in multi-channel data. These equations are derived via semi-groups for non-convex energy functionals. As a particular example, for gray valued data, we find the mean curvature equation and the corresponding non-convex energy functional. As a further application for correction of displacement errors we study image interpolation, in particular zooming, of digital color images. For actual image zooming, the solutions of the proposed PDEs are projected onto a space of functions satisfying interpolation constraints. A comparison of the test results with standard and state-of-the-art interpolation algorithms shows the competitiveness of this approach.	[Lenzen, Frank] Heidelberg Univ, D-69115 Heidelberg, Germany; [Scherzer, Otmar] Univ Vienna, Computat Sci Ctr, A-1090 Vienna, Austria; [Scherzer, Otmar] Austrian Acad Sci, Johann Radon Inst Computat & Appl Math RICAM, A-4040 Linz, Austria	Ruprecht Karls University Heidelberg; University of Vienna; Austrian Academy of Sciences	Lenzen, F (corresponding author), Heidelberg Univ, Speyerer Str 6, D-69115 Heidelberg, Germany.	Frank.Lenzen@iwr.uni-heidelberg.de; otmar.scherzer@univie.ac.at	Scherzer, Otmar/AAA-4132-2019	Scherzer, Otmar/0000-0001-9378-7452	Austrian Science Fund (FWF) within the national research networks Industrial Geometry [9203-N12]	Austrian Science Fund (FWF) within the national research networks Industrial Geometry(Austrian Science Fund (FWF))	This work has been supported by the Austrian Science Fund (FWF) within the national research networks Industrial Geometry, project 9203-N12.	[Anonymous], 1973, N HOLLAND MATH STUDI; Aubert G, 1999, SIAM J APPL MATH, V60, P156, DOI 10.1137/S0036139998340170; Ballester C, 2007, IEEE T IMAGE PROCESS, V16, P2476, DOI 10.1109/TIP.2007.903844; Belahmidi A, 2004, IEEE IMAGE PROC, P649; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Burger Wilhelm, 2005, DIGITALE BILDVERARBE; CHAMBOLLE A, 1994, IEEE IMAGE PROC, P16, DOI 10.1109/ICIP.1994.413266; Chan RH, 2008, SIAM J IMAGING SCI, V1, P273, DOI 10.1137/070711499; Chan TF, 2003, SIAM J APPL MATH, V63, P564; DACOROGNA B, 1982, LECT NOTES COMPUTER, V922; DACOROGNA B, 1989, APPL MATH SCI, V78; De Haan G, 1998, P IEEE, V86, P1839, DOI 10.1109/5.705528; deHaan G, 1997, IEEE T CONSUM ELECTR, V43, P819, DOI 10.1109/30.628721; Deriche R., 1995, P 2 AS C COMP VIS SI, V2, P71; ELBAU P, 2008, REPORTS FSP S092 IND, V75; Ghodstinat M, 2009, LECT NOTES COMPUT SC, V5604, P91, DOI 10.1007/978-3-642-03061-1_5; Grasmair M, 2005, LECT NOTES COMPUT SC, V3459, P303; Guichard F., 1998, P EUR SIGN PROC C, V3; GUICHARD F, 2007, TEXTITCONTRAST UNPUB; JAHNE B, 2002, DIGITALE BILDVERARBE; Kang SH, 2006, IMAGE VISION COMPUT, V24, P143, DOI 10.1016/j.imavis.2005.09.022; Keller S, 2005, LECT NOTES COMPUT SC, V3459, P408; Keller SH, 2008, IEEE T IMAGE PROCESS, V17, P2015, DOI 10.1109/TIP.2008.2003394; Lenzen F, 2009, LECT NOTES COMPUT SC, V5567, P413, DOI 10.1007/978-3-642-02256-2_35; Malgouyres F, 2001, SIAM J NUMER ANAL, V39, P1, DOI 10.1137/S0036142999362286; Nashed MZ., 1976, GEN INVERSES APPL; NESI P, 1993, IMAGE VISION COMPUT, V11, P419, DOI 10.1016/0262-8856(93)90046-J; Nikolova M, 2009, LECT NOTES COMPUT SC, V5567, P439, DOI 10.1007/978-3-642-02256-2_37; Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y; Roussos A, 2007, LECT NOTES COMPUT SC, V4485, P104; Roussos A, 2009, INT J COMPUT VISION, V84, P130, DOI 10.1007/s11263-008-0132-x; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scherzer O, 2000, J MATH IMAGING VIS, V12, P43, DOI 10.1023/A:1008344608808; Shen JH, 2004, SIAM J APPL MATH, V64, P1691, DOI 10.1137/S0036139902418699; Szu H, 2004, PROC SPIE, V5439, P183, DOI 10.1117/12.563654; Thevenaz P, 2000, BIOMED EN S, P393; Tschumperle D, 2005, IEEE T PATTERN ANAL, V27, P506, DOI 10.1109/TPAMI.2005.87; TSCHUMPERLE D, 2004, INT C COMP VIS GRAPH, P301; Tschumperle D, 2006, INT J COMPUT VISION, V68, P65, DOI 10.1007/s11263-006-5631-z; Wang Z, 2007, J BIOMED OPT, V12, DOI 10.1117/1.2779352; Weickert J, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P315, DOI 10.1007/3-540-31272-2_19	41	9	9	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2011	92	2					162	176		10.1007/s11263-010-0326-x	http://dx.doi.org/10.1007/s11263-010-0326-x			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	729DR					2022-12-18	WOS:000287929300003
J	Espuny, F; Gil, JIB				Espuny, Ferran; Gil, Jose I. Burgos			Generic Self-calibration of Central Cameras from Two Rotational Flows	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generic camera; Self-calibration; Optical flow using splines; Camera rotation; Closed-form solutions		We address the self-calibration of a smooth generic central camera from only two dense rotational flows produced by rotations of the camera about two unknown linearly independent axes passing through the camera centre. We give a closed-form theoretical solution to this problem, and we prove that it can be solved exactly up to a linear orthogonal transformation ambiguity. Using the theoretical results, we propose an algorithm for the self-calibration of a generic central camera from two rotational flows. In order to solve the self-calibration problem using real images, we also study the computation of dense optical flows from image sequences acquired by the rotation of a smooth generic central camera. We propose a method for the computation of dense smooth generic flows from rotational camera motions using splines. The proposed methods are validated using both simulated and real image sequences.	[Espuny, Ferran] Univ Barcelona, Dept Algebra & Geometria, E-08007 Barcelona, Spain; [Gil, Jose I. Burgos] UAB, CRM, Fac Sci, Barcelona 08193, Spain	University of Barcelona; Autonomous University of Barcelona; Centre de Recerca Matematica (CRM)	Espuny, F (corresponding author), Univ Barcelona, Dept Algebra & Geometria, Gran Via Corts Catalanes 585, E-08007 Barcelona, Spain.	fespuny@ub.edu; jiburgosgil@gmail.com	Espuny Pujol, Ferran/AAY-9267-2021; Gil, Jose Ignacio Burgos/AAA-6035-2019	Espuny Pujol, Ferran/0000-0001-9085-7400; Gil, Jose Ignacio Burgos/0000-0003-2640-2190	 [MTM2006-14234-C02-01];  [MTM2009-14163-C02-01]	; 	This research was financially supported by the Spanish projects MTM2006-14234-C02-01 and MTM2009-14163-C02-01.	Barreto JP, 2006, COMPUT VIS IMAGE UND, V103, P208, DOI 10.1016/j.cviu.2006.06.003; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Chan TF, 1999, SIAM J NUMER ANAL, V36, P354, DOI 10.1137/S0036142997327075; Dunn A, 2007, P 11 INT C COMP VIS, P1; Espuny F., 2007, VISAPP 2007. Second International Conference on Computer Vision Theory and Applications, P26; Espuny F., 2008, P OMNIVIS; GEYER G, 2000, P ECCV, V2, P445; Grossberg MD, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P108, DOI 10.1109/ICCV.2001.937611; GROSSMANN E, 2006, P CVPR, V1, P1222; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; LHUILLIER M, 2006, P ICPR, V1, P67; NISTER D, 2005, P ICCV, V1, P120; Pless R, 2003, PROC CVPR IEEE, P587, DOI 10.1109/cvpr.2003.1211520; RAMALINGAM S, 2004, P WORKSH OMN VIS CAM, P175; Ramalingam S., 2005, P OMNIVIS; Ramalingam S, 2010, COMPUT VIS IMAGE UND, V114, P210, DOI 10.1016/j.cviu.2009.07.007; STURM P, 2004, P ECCV, V2, P1; YING X, 2004, P EUR C COMP VIS ECC, V1, P442	18	9	12	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2011	91	2					131	145		10.1007/s11263-010-0335-9	http://dx.doi.org/10.1007/s11263-010-0335-9			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	708KJ					2022-12-18	WOS:000286360400001
J	Alvarez, JM; Gevers, T; Lopez, AM				Alvarez, Jose M.; Gevers, Theo; Lopez, Antonio M.			Learning Photometric Invariance for Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Color models; Learning; Photometric invariance; Combining classifiers; Diversified ensembles	FACE SEGMENTATION; COLOR MODELS; SELECTION	Color is a powerful visual cue in many computer vision applications such as image segmentation and object recognition. However, most of the existing color models depend on the imaging conditions that negatively affect the performance of the task at hand. Often, a reflection model (e.g., Lambertian or dichromatic reflectance) is used to derive color invariant models. However, this approach may be too restricted to model real-world scenes in which different reflectance mechanisms can hold simultaneously. Therefore, in this paper, we aim to derive color invariance by learning from color models to obtain diversified color invariant ensembles. First, a photometrical orthogonal and non-redundant color model set is computed composed of both color variants and invariants. Then, the proposed method combines these color models to arrive at a diversified color ensemble yielding a proper balance between invariance (repeatability) and discriminative power (distinctiveness). To achieve this, our fusion method uses a multi-view approach to minimize the estimation error. In this way, the proposed method is robust to data uncertainty and produces properly diversified color invariant ensembles. Further, the proposed method is extended to deal with temporal data by predicting the evolution of observations over time. Experiments are conducted on three different image datasets to validate the proposed method. Both the theoretical and experimental results show that the method is robust against severe variations in imaging conditions. The method is not restricted to a certain reflection model or parameter tuning, and outperforms state-of-the-art detection techniques in the field of object, skin and road recognition. Considering sequential data, the proposed method (extended to deal with future observations) outperforms the other methods.	[Alvarez, Jose M.; Lopez, Antonio M.] Univ Autonoma Barcelona, Comp Vis Ctr, E-08193 Barcelona, Spain; [Alvarez, Jose M.; Lopez, Antonio M.] Univ Autonoma Barcelona, Comp Sci Dpt, E-08193 Barcelona, Spain; [Gevers, Theo] Univ Amsterdam, Intelligent Syst Lab Amsterdam, NL-1098 SJ Amsterdam, Netherlands	Autonomous University of Barcelona; Centre de Visio per Computador (CVC); Autonomous University of Barcelona; University of Amsterdam	Alvarez, JM (corresponding author), Univ Autonoma Barcelona, Comp Vis Ctr, Edifici O,Campus UAB, E-08193 Barcelona, Spain.	jalvarez@cvc.uab.es; th.gevers@uva.nl; antonio@cvc.uab.es	Tamblyn, Robyn/L-6010-2016; López, Antonio M/L-5303-2014	López, Antonio M/0000-0002-6979-5783	Spanish Ministry of Education and Science [TRA2007-62526/AUT, CSD2007-00018]	Spanish Ministry of Education and Science(Spanish Government)	This work was supported by the Spanish Ministry of Education and Science under project TRA2007-62526/AUT and research programme Consolider Ingenio 2010: MIPRCV (CSD2007-00018).	ALVAREZ JM, P 2008 IEEE INT VEH; [Anonymous], 1998, VALUE RISK NEW SCI R; Best P., 1998, IMPLEMENTING VALUE R; Boyd S, 2004, CONVEX OPTIMIZATION; BROWN G, 2005, DIVERSITY CREATION M; Chai D, 1999, IEEE T CIRC SYST VID, V9, P551, DOI 10.1109/76.767122; Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18; Finlayson GD, 2004, LECT NOTES COMPUT SC, V3023, P582; FLECK MM, 1996, P EUR C COMP VIS ECC, V3, P593; Geusebroek JM, 2005, INT J COMPUT VISION, V61, P103, DOI 10.1023/B:VISI.0000042993.50813.60; HARTIGAN JA, 1985, ANN STAT, V13, P70, DOI 10.1214/aos/1176346577; Hong T., 2006, INT TRANSP SYST C 20, P939; IKONOMAKIS N, 2000, J INTELLIGENT ROBOTI, V28; JACOBS RA, 1995, NEURAL COMPUT, V7, P867, DOI 10.1162/neco.1995.7.5.867; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198; KENDER J, 2005, CMURITR0540; Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881; KOVAC J, 2003, INT C COMP TOOL EURO; Kuncheva L I, 2004, COMBINING PATTERN CL; Markowitz H, 1959, PORTFOLIO SELECTION; MELVILLE P, 2005, INFORM FUSION, V6, P1553; Michaud R., 2008, J INVEST MANAG, V6, P8; Michaud R.O., 1998, EFFICIENT ASSET MANA; RASMUSSEN M, 2003, QUANTITATIVE PORTFOL; RIDLER TW, 1978, IEEE T SYST MAN CYB, V8, P630, DOI 10.1109/tsmc.1978.4310039; ROTARU C, 2008, J REAL-TIME IMAGE PR, P1164; SCHERER B, 2002, PORTFOLIO CONSTRUCTI, pCH4; SHARPE WF, 1994, J PORTFOLIO MANAGE, V21, P49, DOI 10.3905/jpm.1994.409501; Sigal L, 2004, IEEE T PATTERN ANAL, V26, P862, DOI 10.1109/TPAMI.2004.35; Sobottka K, 1998, SIGNAL PROCESS-IMAGE, V12, P263, DOI 10.1016/S0923-5965(97)00042-8; Sotelo MA, 2004, AUTON ROBOT, V16, P95, DOI 10.1023/B:AURO.0000008673.96984.28; Stokman H, 2007, IEEE T PATTERN ANAL, V29, P371, DOI 10.1109/TPAMI.2007.58; Tax DMJ, 2002, J MACH LEARN RES, V2, P155, DOI 10.1162/15324430260185583; TSE YK, 1991, JPN WORLD ECON, V3, P285; USMEN NMH, 2003, J INVESTMENT MANAGEM, V1, P1; van de Sande K.E.A., 2008, P IEEE C COMP VIS PA, P453; Weber M., 1999, CALTECH FRONTAL FACE; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; Wyszecki G., 2000, COLOR SCI CONCEPTS M, V2nd	40	9	9	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2010	90	1					45	61		10.1007/s11263-010-0336-8	http://dx.doi.org/10.1007/s11263-010-0336-8			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	616GA					2022-12-18	WOS:000279195800003
J	Fundana, K; Overgaard, NC; Heyden, A				Fundana, Ketut; Overgaard, Niels C.; Heyden, Anders			Variational segmentation of image sequences using region-based active contours and deformable shape priors	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						variational segmentation; level set; gradient descent; image sequences; region-based active contours; shape priors	LEVEL SET REPRESENTATIONS; KERNEL DENSITY-ESTIMATION; TRACKING; DRIVEN; MODEL	In this paper we address the problem of segmentation in image sequences using region-based active contours and level set methods. We propose a novel method for variational segmentation of image sequences containing nonrigid, moving objects. The method is based on the classical Chan-Vese model augmented with a novel frame-to-frame interaction term, which allow us to update the segmentation result from one image frame to the next using the previous segmentation result as a shape prior. The interaction term is constructed to be pose-invariant and to allow moderate deformations in shape. It is expected to handle the appearance of occlusions which otherwise can make segmentation fail. The performance of the model is illustrated with experiments on synthetic and real image sequences.	[Fundana, Ketut; Overgaard, Niels C.; Heyden, Anders] Malmo Univ, Sch Technol, Appl Math Grp, Malmo, Sweden	Malmo University	Fundana, K (corresponding author), Malmo Univ, Sch Technol, Appl Math Grp, Malmo, Sweden.	ketut.fundana@mah.se; nco@mah.se; heyden@mah.se			VISION-TRAIN RTN [CT-2004-005439]; EU [IST 2001 37540]	VISION-TRAIN RTN; EU(European Commission)	This research was funded by the VISION-TRAIN RTN CT-2004-005439 Marie Curie Action within the EC's Sixth Framework Programme. The human walking sequences were downloaded from EU funded CAVIAR project (IST 2001 37540) website and the traffic sequence from KOGS/IAKS Universitat Karlsruhe.	Bresson X, 2006, INT J COMPUT VISION, V68, P145, DOI 10.1007/s11263-006-6658-x; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chan T, 2005, PROC CVPR IEEE, P1164; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985; Cremers D, 2006, INT J COMPUT VISION, V66, P67, DOI 10.1007/s11263-005-3676-z; CREMERS D, 2003, 2 IEEE WORKSH VARIAT; Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5; Cremers D, 2006, IEEE T PATTERN ANAL, V28, P1262, DOI 10.1109/TPAMI.2006.161; Delfour M. C., 2001, SHAPE OPTIMIZATION O, P37, DOI [10.1201/9780203904169.ch3, DOI 10.1201/9780203904169.CH3]; Fundana K, 2007, LECT NOTES COMPUT SC, V4522, P31; Gentile C, 2004, IEEE T IMAGE PROCESS, V13, P166, DOI 10.1109/tip.2003.817232; KASS M, 1988, INT J COMPUT VISION, P321, DOI DOI 10.1007/BF00133570; Leventon ME, 2000, PROC CVPR IEEE, P316, DOI 10.1109/CVPR.2000.855835; Lucas B. D., 1981, IJCAI, P121, DOI DOI 10.5555/1623264.1623280; MOELICH M, 2003, 0314 UCLA DEP MATH; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Osher S, 2003, LEVEL SET METHODS DY; Paragios N, 2005, COMPUT VIS IMAGE UND, V97, P259, DOI 10.1016/j.cviu.2003.04.001; Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758; PARAGIOS N, 2003, LNCS, V2351, P775; Riklin-Raviv T, 2007, INT J COMPUT VISION, V72, P309, DOI 10.1007/s11263-006-9042-y; Rousson M, 2005, LECT NOTES COMPUT SC, V3750, P757, DOI 10.1007/11566489_93; Rousson M, 2002, LECT NOTES COMPUT SC, V2351, P78; Rousson M, 2008, INT J COMPUT VISION, V76, P231, DOI 10.1007/s11263-007-0054-z; Solem JE, 2005, LECT NOTES COMPUT SC, V3459, P419; Thiruvenkadam SR, 2007, LECT NOTES COMPUT SC, V4485, P191; Tsai A, 2003, IEEE T MED IMAGING, V22, P137, DOI 10.1109/TMI.2002.808355	28	9	10	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2008	80	3					289	299		10.1007/s11263-008-0160-6	http://dx.doi.org/10.1007/s11263-008-0160-6			11	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	350RJ					2022-12-18	WOS:000259370500001
J	Ragheb, H; Hancock, ER				Ragheb, Hossein; Hancock, Edwin R.			A light scattering model for layered dielectrics with rough surface boundaries	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Beckmann theory; layered surface; skin reflectance; surface roughness; subsurface scattering; Fresnel coefficient; Snell's law	REFLECTANCE; RADIANCE	A new model for the scattering of light from layered dielectrics with rough surface boundaries is introduced. The model contains a surface scattering component together with a subsurface scattering component. The former component corresponds to the roughness on the upper surface boundary and is modeled using the modified Beckmann model. The latter component accounts for both refraction due to Fresnel transmission through the layer and rough scattering at the lower layer boundary. One interesting consequence of the model is that the peak radiance is deflected away from the specular direction, a behavior that is also evident in BRDF data from human skin. By allowing independent roughness parameters for each surface boundary and controlling the contributions from the two scattering components in the outgoing radiance using a balance parameter, we can achieve excellent fits of the model to the measured BRDF data. We experiment with BRDF data from skin surface samples (human volunteers) and show that the new model outperforms alternative variants of the Beckmann model and the Lafortune et al. reflectance model. As an application in computer graphics, we also show that realistic images of 3D surfaces can be generated using the new model, by setting the values of its physical parameters.	[Ragheb, Hossein] Kingston Univ London, Digital Imaging Res Ctr, Kingston upon Thames KT1 2EE, Surrey, England; [Ragheb, Hossein] Bu Ali Sina Univ, Dept Comp Engn, Hamadan, Iran; [Hancock, Edwin R.] Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England	Kingston University; Bu Ali Sina University; University of York - UK	Ragheb, H (corresponding author), Kingston Univ London, Digital Imaging Res Ctr, Kingston upon Thames KT1 2EE, Surrey, England.	ragheb@basu.ac.ir; erh@cs.york.ac.uk	Hancock, Edwin R/C-6071-2008; Hancock, Edwin/N-7548-2019	Hancock, Edwin R/0000-0003-4496-2028; Hancock, Edwin/0000-0003-4496-2028				*AIM SHAPE, 2004, INRIA SHAP REP; Angelopoulou E, 2001, PROC SPIE, V4299, P243, DOI 10.1117/12.429495; BECKMANN P, 1967, PROGR OPTICS, V6, P55; Beckmann Petr, 1987, SCATTERING ELECTROMA, P4; Caron J, 2002, OPT COMMUN, V207, P17, DOI 10.1016/S0030-4018(02)01415-3; Dana KJ, 1997, PROC CVPR IEEE, P151, DOI 10.1109/CVPR.1997.609313; DICKENS MP, 2007, ICCV WORKSH PHOT AN, P35; Donner C., 2006, RENDERING TECHN, V2006, P409, DOI DOI 10.2312/EGWR/EGSR06/409-417; Hanrahan P., 1993, Computer Graphics Proceedings, P165, DOI 10.1145/166117.166139; Harvey JE, 1999, APPL OPTICS, V38, P6469, DOI 10.1364/AO.38.006469; HE XD, 1991, COMP GRAPH, V25, P175, DOI 10.1145/127719.122738; Horn B., 1986, ROBOT VISION, P1; Igarashi T., 2005, APPEARANCE HUMAN SKI; Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319; LAFORTUNE EPF, 1997, P SIGGRAPH, P117; Lu R, 2000, APPL OPTICS, V39, P5785, DOI 10.1364/AO.39.005785; Lu R, 1998, APPL OPTICS, V37, P5974, DOI 10.1364/AO.37.005974; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; NIETOVESPERINAS M, 1981, OPT ACTA, V28, P1651, DOI 10.1080/713820508; ODONNELL KA, 1987, J OPT SOC AM A, V4, P1194, DOI 10.1364/JOSAA.4.001194; Ogilvy J., 1991, THEORY WAVE SCATTERI; OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Ragheb H, 2006, COMPUT VIS IMAGE UND, V102, P145, DOI 10.1016/j.cviu.2005.11.004; Ragheb H, 2007, PATTERN RECOGN, V40, P2004, DOI 10.1016/j.patcog.2006.10.007; Stam J, 1999, COMP GRAPH, P101, DOI 10.1145/311535.311546; STAM J, 2001, P 12 EUR WORKSH REND, P39; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Vernold CL, 1998, P SOC PHOTO-OPT INS, V3426, P51, DOI 10.1117/12.328477; WOLFF LB, 1994, J OPT SOC AM A, V11, P2956, DOI 10.1364/JOSAA.11.002956; 1999, CORNELL U PROGRAM CO; 2004, UCSD MAGAZINE    MAY, V1	33	9	9	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2008	79	2					179	207		10.1007/s11263-007-0113-5	http://dx.doi.org/10.1007/s11263-007-0113-5			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	302ZV					2022-12-18	WOS:000256009700005
J	Town, C				Town, Christopher			Multi-sensory and multi-modal fusion for sentient computing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	3rd Workshop on Object Tracking and Classification Beyond the Visible Spectrum	JUN 22, 2006	New York, NY	IEEE		multi-sensory fusion; multi-modal fusion; sentient computing; object tracking; Bayesian networks		This paper presents an approach to multi-sensory and multi-modal fusion in which computer vision information obtained from calibrated cameras is integrated with a large-scale sentient computing system known as "SPIRIT"'. The SPIRIT system employs an ultrasonic location infrastructure to track people and devices in an office building and model their state. Vision techniques include background and object appearance modelling, face detection, segmentation, and tracking modules. Integration is achieved at the system level through the metaphor of shared perceptions, in the sense that the different modalities are guided by and provide updates to a shared world model. This model incorporates aspects of both the static (e.g. positions of office walls and doors) and the dynamic (e.g. location and appearance of devices and people) environment. Fusion and inference are performed by Bayesian networks that model the probabilistic dependencies and reliabilities of different sources of information over time. It is shown that the fusion process significantly enhances the capabilities and robustness of both sensory modalities, thus enabling the system to maintain a richer and more accurate world model.	Univ Cambridge, Comp Lab, Cambridge CB3 0FD, England	University of Cambridge	Town, C (corresponding author), Univ Cambridge, Comp Lab, 15 JJ Thomson Ave, Cambridge CB3 0FD, England.	cpt23@cam.ac.uk						Addlesee M, 2001, COMPUTER, V34, P50, DOI 10.1109/2.940013; Bouguet J.Yt., MATLAB CALIBRATION T; CATTIN P, 2001, BIOMETRIC SYSTEM USI; CERNEY M, 2005, GESTURE RECOGNITION; CHOUDHURY T, 2002, P INT C PATT REC; CROWLEY J, 2002, P UBICOMP; DELATORRE F, 2001, P INT C COMP VIS; DELATORRE F, 2003, COMPUTER VISION IMAG; Dey AK, 2001, PERS UBIQUIT COMPUT, V5, P4, DOI 10.1007/s007790170019; Erickson T, 2002, COMMUN ACM, V45, P102, DOI 10.1145/503124.503154; FRITSCH J, 2003, ROBOTICS AUTONOMOUS, V43; Garcia C, 1999, IEEE T MULTIMEDIA, V1, P264, DOI 10.1109/6046.784465; Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716; GENCO A, 2005, LNCS, V3479; HANBURY A, 2003, 8 COMP VIS WINT WORK; HARLE R, 2005, P MOBISYS 2005; HARLE R, 2004, THESIS U CAMBRIDGE E; HARTER A, 1994, IEEE NETWORK, V8, P62, DOI 10.1109/65.260080; HAZAS M, 2004, IEEE COMPUT, P95; Hopper A, 2000, PHILOS T ROY SOC A, V358, P2349, DOI 10.1098/rsta.2000.0652; IPINA DL, 2002, PERS UBIQUIT COMPUT, V6, P206; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; MANSLEY K, 2004, P UB SPRING; MCKENNA SJ, 1998, P ACCV 98, P615; Nummiaro K, 2003, IMAGE VISION COMPUT, V21, P99, DOI 10.1016/S0262-8856(02)00129-4; PEREZ P, 2004, IEEE T PATTERN ANAL; Priyantha N. B., 2001, SERIES TITLE MOBICOM, P1; SHERRAH J, 2001, P INT C COMP VIS; Sinclair D, 2000, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2000.855845	29	9	15	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2007	71	2					235	253		10.1007/s11263-006-7834-8	http://dx.doi.org/10.1007/s11263-006-7834-8			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	098DN					2022-12-18	WOS:000241501300008
J	Griffin, LD; Lillholm, M				Griffin, Lewis D.; Lillholm, Martin			Hypotheses for image features, icons and textons	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	5th International Conference on Scale-Space and PDE Methods in Computer Vision	APR 07-09, 2005	Hofgeismar, GERMANY	German Pattern Recognit Soc			SMOOTHEST REFLECTANCE FUNCTIONS; GAUSSIAN DERIVATIVE MODEL; SPATIAL-TEMPORAL VISION; COLOR CATEGORIES; TEXTURE; STATISTICS; REPRESENTATION; ORIENTATION; APPEARANCE; NEURONS	We review ideas about the relationship between qualitative description of local image structure and quantitative description based on responses to a family of linear filters. We propose a sequence of three linking hypotheses. The first, the Feature Hypothesis, is that qualitative descriptions arise from a category system on filter-response space. The second, the Icon Hypothesis, is that the partitioning into categories of filter response space is determined by a system of iconic images, one associated with each point of the space. The third, the Texton Hypothesis, is that the correct images to play the role of icons are those that are the most likely explanations of a vector of filter responses. We present results in support of these three hypotheses, including new results on 2-D 1 st order structure.	UCL, London, England; Univ Copenhagen, Copenhagen, Denmark	University of London; University College London; University of Copenhagen	Griffin, LD (corresponding author), UCL, London, England.		Griffin, Lewis/C-2118-2008					Barlow H B, 1972, Perception, V1, P371, DOI 10.1068/p010371; BARLOW HB, 1953, J PHYSIOL-LONDON, V119, P69, DOI 10.1113/jphysiol.1953.sp004829; Berlin B, 1969, BASIC COLOR TERMS TH; BIMLER D, 2004, COMMUNICATION; Buchsbaum G, 2002, VISION RES, V42, P559, DOI 10.1016/S0042-6989(01)00303-0; Cen F, 2004, LECT NOTES COMPUT SC, V3117, P304; Davidoff J, 1999, NATURE, V398, P203, DOI 10.1038/18335; DEBNATH L, 1995, INTEGRAL TRANSFORMS; DEBNATH L, 1964, MATH VESNIK, V1, P285; DEVALOIS RL, 1966, J OPT SOC AM, V56, P966, DOI 10.1364/JOSA.56.000966; Dowman M, 2002, LECT NOTES ARTIF INT, V2557, P259; ELLISON TM, 2001, SIMILARITY CATEGORIZ, P29, DOI DOI 10.1093/ACPROF:OSO/9780198506287.003.0003; FLORACK LMJ, 1992, LECT NOTES COMPUT SC, V588, P19; Gardenfors P, 2004, CONCEPTUAL SPACES GE; Georgeson MA, 1997, VISION RES, V37, P127, DOI 10.1016/S0042-6989(96)00078-8; Geusebroek JM, 2003, PATTERN RECOGN LETT, V24, P1653, DOI 10.1016/S0167-8655(02)00322-7; Gibson J., 1979, ECOLOGICAL APPROACH; Griffin L, 1997, COMP IMAG VIS, V8, P165; Griffin LD, 2004, VISION RES, V44, P407, DOI 10.1016/j.visres.2003.09.025; Griffin LD, 2002, PERCEPTION, V31, P377; Griffin LD, 2001, COLOR RES APPL, V26, P151, DOI 10.1002/1520-6378(200104)26:2<151::AID-COL1006>3.0.CO;2-G; GRIFFIN LD, 1995, IMAGE VISION COMPUT, V13, P543, DOI 10.1016/0262-8856(95)91145-4; GRIFFIN LD, 2003, SCALE SPACE 03; GRIFFIN LD, 2005, P SCAL SPAC 2005, P26; GRIFFIN LD, 2005, UNPUB IEEE T PATTERN; GRIFFIN LD, 2005, IN PRESS NETWORK COM; GRIFFIN LD, 1995, THESIS U LONDON; Heiler M, 2005, INT J COMPUT VISION, V63, P5, DOI 10.1007/s11263-005-4944-7; HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455; HURVICH LM, 1957, PSYCHOL REV, V64, P384, DOI 10.1037/h0041403; Jameson D., 1920, OUTLINES THEORY LIGH; JAMESON KA, 2005, IN PRESS COGNITION; Kay P, 2003, P NATL ACAD SCI USA, V100, P9085, DOI 10.1073/pnas.1532837100; Kay P, 2005, CROSS-CULT RES, V39, P39, DOI 10.1177/1069397104267889; KAY P, 1978, LANGUAGE, V54, P610, DOI 10.2307/412789; Kay P, 1999, AM ANTHROPOL, V101, P743, DOI 10.1525/aa.1999.101.4.743; Kimmel R, 2003, INT J COMPUT VISION, V53, P225, DOI 10.1023/A:1023030907417; Koenderink J, 2001, PERCEPTION, V30, P1, DOI 10.1068/p3001ed; Koenderink J. J., 1997, Algebraic Frames for the Perception-Action Cycle. International Workshop, AFPAC'97. Proceedings, P66, DOI 10.1007/BFb0017861; Koenderink J. J., 1992, Journal of Visual Communication and Image Representation, V3, P1, DOI 10.1016/1047-3203(92)90026-P; Koenderink J. J., 1993, J INTELLIGENT SYSTEM, V3, P49; KOENDERINK JAN J., 2003, COLOUR PERCEPTION MI, P1; KOENDERINK JJ, 1992, IEEE T PATTERN ANAL, V14, P597, DOI 10.1109/34.141551; Koenderink JJ, 1998, ADV IMAG ELECT PHYS, V103, P65, DOI 10.1016/S1076-5670(08)70015-6; KOENDERINK JJ, 1990, BIOL CYBERN, V63, P291, DOI 10.1007/BF00203452; KOENDERINK JJ, 1988, BIOL CYBERN, V58, P163, DOI 10.1007/BF00364136; KOENDERINK JJ, 1987, BIOL CYBERN, V55, P367, DOI 10.1007/BF00318371; KOENDERINK JJ, 1996, ADV IMAGE UNDERSTAND, P113; Lawson S, 2002, ELECTRON COMMUN ENG, V14, P112, DOI 10.1049/ecej:20020303; Lee AB, 2003, INT J COMPUT VISION, V54, P83, DOI 10.1023/A:1023705401078; Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638; Lillholm M, 2003, INT J COMPUT VISION, V52, P73, DOI 10.1023/A:1022995822531; Liu XW, 2002, VISION RES, V42, P2617, DOI 10.1016/S0042-6989(02)00297-3; LOGOTHETIS NK, 1995, CURR BIOL, V5, P552, DOI 10.1016/S0960-9822(95)00108-4; MAJTHAY A, 1985, FDN CATASTROPHE THEO; Makram-Ebeid S, 2003, LECT NOTES COMPUT SC, V2695, P57; Manmatha R, 1998, P SOC PHOTO-OPT INS, V3299, P540, DOI 10.1117/12.320145; MARR D, 1980, P ROYAL SOC B, V20, P187; Marr D., 1982, VISION; Martens JB, 1997, IEEE T IMAGE PROCESS, V6, P1103, DOI 10.1109/83.605408; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; NAKAMURA K, 1994, J NEUROPHYSIOL, V71, P1206, DOI 10.1152/jn.1994.71.3.1206; NEWTON I, 1706, ENUMERATIO LINEARUM; PEDERSEN KS, 2003, STAT NATURAL IMAGE C; RICHARDS W, 1979, SENS PROCESS, V3, P207; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; Rivero-Moreno CJ, 2003, LECT NOTES COMPUT SC, V2756, P762; Roberson D, 2005, CROSS-CULT RES, V39, P56, DOI 10.1177/1069397104267890; ROMENY BMT, 1994, IMAGE VISION COMPUT, V12, P317, DOI 10.1016/0262-8856(94)90056-6; ROMENY BMT, 2003, FRONT END VISION MUT; Sigala N, 2002, NATURE, V415, P318, DOI 10.1038/415318a; STEELS L, 2005, IN PRESS BEHAV BRAIN; Tagliati E, 2001, LECT NOTES COMPUT SC, V2106, P51; Thom R., 1972, STRUCTURAL STABILITY; van den Boomgaard R, 2003, LECT NOTES COMPUT SC, V2695, P237; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303; VANTRIGT C, 1990, J OPT SOC AM A, V7, P2208, DOI 10.1364/JOSAA.7.002208; VANTRIGT C, 1990, J OPT SOC AM A, V7, P1891, DOI 10.1364/JOSAA.7.001891; Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4; VARMA M, 2002, ECCV 02 COP; Vogels R, 2001, J COGNITIVE NEUROSCI, V13, P444, DOI 10.1162/08989290152001871; WILSON M, 1981, NEUROPSYCHOLOGIA, V19, P29, DOI 10.1016/0028-3932(81)90041-5; WU SW, 1993, OPT ENG, V32, P1489, DOI 10.1117/12.139507; Yendrikhovskij SN, 2001, J IMAGING SCI TECHN, V45, P409; YOUNG R A, 1987, Spatial Vision, V2, P273, DOI 10.1163/156856887X00222; Young RA, 2001, SPATIAL VISION, V14, P261, DOI 10.1163/156856801753253582; Young RA, 2001, SPATIAL VISION, V14, P321, DOI 10.1163/156856801753253591; Zhilkin P, 2000, MAGN RESON IMAGING, V18, P1143, DOI 10.1016/S0730-725X(00)00209-5; Zhu SC, 2005, INT J COMPUT VISION, V62, P121, DOI 10.1007/s11263-005-4638-1; 2001, SCAL SPAC 01 VANC CA; 2005, SCAP SPAC 05 HOFG GE; 2003, SCAL SPAC 03 ISL SKY; 1999, SCAL SPAC 99 CORF GR	94	9	9	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2006	70	3					213	230		10.1007/s11263-006-6355-9	http://dx.doi.org/10.1007/s11263-006-6355-9			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	091VL					2022-12-18	WOS:000241056300003
J	Sato, J				Sato, J			Recovering multiple view geometry from mutual projections of multiple cameras	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	6th Asian Conference on Computer Vision	JAN 28-30, 2004	Cheju Isl, SOUTH KOREA			multifocal tensors; mutual projections; epipolar geometry; multiple cameras; camera calibration		In this paper, we analyze the computation of epipolar geometry in some special cases where multiple cameras are projected each other in their images. In such cases, epipoles can be obtained directly from images as the projection of cameras. As the result, the epipolar geometry can be computed from less image correspondences with higher stability. In this paper, we analyze the number of corresponding points required for computing bifocal, trifocal and quadrifocal tensors linearly in the case where cameras are projected mutually. We next show a practical linear method for computing multifocal tensors by using the mutual projection of cameras. The degenerate configurations of points and cameras is also studied, and it is shown that some degenerate configurations in general cases are no longer degenerate under the mutual projection of cameras.	Nagoya Inst Technol, Dept Comp Sci & Engn, JST, Nagoya, Aichi 4668555, Japan	Japan Science & Technology Agency (JST); Nagoya Institute of Technology	Sato, J (corresponding author), Nagoya Inst Technol, Dept Comp Sci & Engn, JST, Nagoya, Aichi 4668555, Japan.	junsato@nitech.ac.jp						FAUGERAS O, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P951, DOI 10.1109/ICCV.1995.466832; Faugeras O, 2004, GEOMETRY MULTIPLE IM; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hartley RI, 1997, INT J COMPUT VISION, V22, P125, DOI 10.1023/A:1007936012022; Hartley RI, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P469, DOI 10.1109/ICCV.1998.710760; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Heyden A, 2000, MATH METHOD APPL SCI, V23, P169, DOI 10.1002/(SICI)1099-1476(20000125)23:2<169::AID-MMA110>3.0.CO;2-Y; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; MAYBANK S, 1993, THEORY RECONSTRUCTIO; Shashua A., 1994, P 3 EUR C COMP VIS, P479; Torr P.H.S., 1995, THESIS U OXFORD; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561	15	9	10	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2006	66	2					123	140		10.1007/s11263-005-3954-9	http://dx.doi.org/10.1007/s11263-005-3954-9			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	022CY					2022-12-18	WOS:000236033500003
J	Batur, AU; Hayes, MH				Batur, AU; Hayes, MH			Segmented linear subspaces for illumination-robust face recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						face recognition; illumination; object recognition; illumination modelling; segmented linear subspace	THEORETICAL-ANALYSIS; SYSTEMS; IMAGES	All images of a convex Lambertian surface captured with a fixed pose under varying illumination are known to lie in a convex cone in the image space that is called the illumination cone. Since this cone model is too complex to be built in practice, researchers have attempted to approximate it with simpler models. In this paper, we propose a segmented linear subspace model to approximate the cone. Our idea of segmentation is based on the fact that the success of low dimensional linear subspace approximations of the illumination cone increases if the directions of the surface normals get close to each other. Hence, we propose to cluster the image pixels according to their surface normal directions and to approximate the cone with a linear subspace for each of these clusters separately. We perform statistical performance evaluation experiments to compare our system to other popular systems and demonstrate that the performance increase we obtain is statistically significant.	Georgia Inst Technol, Sch Elect & Comp Engn, Ctr Signal & Image Proc, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Batur, AU (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Ctr Signal & Image Proc, Atlanta, GA 30332 USA.							Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229; Basri R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P383, DOI 10.1109/ICCV.2001.937651; Batur AU, 2001, PROC CVPR IEEE, P296; Belhumeur P. N., 1998, INT J COMPUT VISION, V28, P1; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Beveridge JR, 2001, PROC CVPR IEEE, P535; Bichsel M., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P620, DOI 10.1109/ICIP.1995.537711; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; HALLINAN PW, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P995, DOI 10.1109/CVPR.1994.323941; Lee KC, 2001, PROC CVPR IEEE, P519; Micheals RJ, 2001, PROC CVPR IEEE, P50; Nayar SK, 1996, IEEE INT CONF ROBOT, P1326, DOI 10.1109/ROBOT.1996.506890; Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448; Ramamoorthi R, 2002, IEEE T PATTERN ANAL, V24, P1322, DOI 10.1109/TPAMI.2002.1039204; Shashua A, 1997, INT J COMPUT VISION, V21, P99, DOI 10.1023/A:1007975506780; Sim T, 2001, CMURITR0102; Zhao L, 1999, PATTERN RECOGN, V32, P547, DOI 10.1016/S0031-3203(98)00119-8	18	9	10	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2004	57	1					49	66		10.1023/B:VISI.0000013090.39095.d5	http://dx.doi.org/10.1023/B:VISI.0000013090.39095.d5			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	766DP					2022-12-18	WOS:000188330400003
J	Gevers, T; Stokman, HMG				Gevers, T; Stokman, HMG			Robust photometric invariant region detection in multispectral images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						photometric invariance; region detection; clustering; multispectral images; noise robustness; camera calibration; polar angle representation	COMPONENT ANALYSIS; COLOR; SEGMENTATION; SYSTEM; CAMERA	Our aim is to detect photometric invariant regions in multispectral images robust against sensor noise. Therefore, different polar angle representations of a spectrum are examined for invariance using the dichromatic reflection model. These invariant representations take advantage of white balancing. Based on the camera sensitivity, a theoretical expression is obtained of the certainty associated with the polar angular representations under the influence of noise. The expression is employed by the segmentation technique to ensure robustness against sensor noise.	Univ Amsterdam, Fac Sci, Inst Informat, ISIS, NL-1012 WX Amsterdam, Netherlands	University of Amsterdam	Gevers, T (corresponding author), Univ Amsterdam, Fac Sci, Inst Informat, ISIS, NL-1012 WX Amsterdam, Netherlands.							ANDOUTSOS D, 1999, COMPUTER VISION IMAG, V75, P46; Bajcsy R., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P785, DOI 10.1109/ICPR.1990.118217; Baronti S, 1998, APPL OPTICS, V37, P1299, DOI 10.1364/AO.37.001299; BUITUONG P, 1975, COMMUN ACM, V18, P311; BURNS PD, 1997, COLOR RES APPL, V22, P123; CELENK M, 1990, COMPUT VISION GRAPH, V52, P145, DOI 10.1016/0734-189X(90)90052-W; DUBES R, 1976, PATTERN RECOGN, V8, P247, DOI 10.1016/0031-3203(76)90045-5; FINLAYSON GD, 1994, J OPT SOC AM A, V11, P1553, DOI 10.1364/JOSAA.11.001553; Gevers T, 1999, PATTERN RECOGN, V32, P453, DOI 10.1016/S0031-3203(98)00036-3; Gevers T, 2000, IEEE T IMAGE PROCESS, V9, P102, DOI 10.1109/83.817602; HANEISHI H, 1997, P IS T 50 ANN C, P369; HAUTAKASARI M, 1999, J OPT SOC AM A, V16, P1806; HEALEY G, 1992, IEEE T SYST MAN CYB, V22, P64, DOI 10.1109/21.141311; KAWATA S, 1987, J OPT SOC AM A, V4, P2101, DOI 10.1364/JOSAA.4.002101; KENDER JR, 1976, SATURATION HUE NORMA; KLINKER GJ, 1990, INT J COMPUT VISION, V4, P7, DOI 10.1007/BF00137441; LEVKOWITZ H, 1993, CVGIP-GRAPH MODEL IM, V55, P271, DOI 10.1006/cgip.1993.1019; LIU JQ, 1994, IEEE T PATTERN ANAL, V16, P689, DOI 10.1109/34.297949; MULLIKIN JC, 1994, P SOC PHOTO-OPT INS, V2173, P73, DOI 10.1117/12.175165; OHTA Y, 1980, COMPUT VISION GRAPH, V13, P222, DOI 10.1016/0146-664X(80)90047-7; Shafarenko L, 1998, IEEE T IMAGE PROCESS, V7, P1354, DOI 10.1109/83.709666; SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409; Taylor JR., 1982, INTRO ERROR ANAL, V2nd edition; Tominaga S, 1996, J OPT SOC AM A, V13, P2163, DOI 10.1364/JOSAA.13.002163; Tominaga S, 1999, J ELECTRON IMAGING, V8, P332, DOI 10.1117/1.482702; WOLFF L, 1992, PHYSICS BASED VISION, V2	26	9	10	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2003	53	2					135	151		10.1023/A:1023095923133	http://dx.doi.org/10.1023/A:1023095923133			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	674WZ					2022-12-18	WOS:000182661300002
J	Brodsky, T; Fermuller, C				Brodsky, T; Fermuller, C			Self-calibration from image derivatives	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						camera self-calibration; motion estimation; normal flow; depth distortion; minimization of depth variability	MOTION; ALGORITHM; ERRORS	This study investigates the problem of estimating camera calibration parameters from image motion fields induced by a rigidly moving camera with unknown parameters, where the image formation is modeled with a linear pinhole-camera model. The equations obtained show the flow to be separated into a component due to the translation and the calibration parameters and a component due to the rotation and the calibration parameters. A set of parameters encoding the latter component is linearly related to the flow, and from these parameters the calibration can be determined. However, as for discrete motion, in general it is not possible to decouple image measurements obtained from only two frames into translational and rotational components. Geometrically, the ambiguity takes the form of a part of the rotational component being parallel to the translational component, and thus the scene can be reconstructed only up to a projective transformation. In general, for full calibration at least four successive image frames are necessary, with the 3D rotation changing between the measurements. The geometric analysis gives rise to a direct self-calibration method that avoids computation of optical flow or point correspondences and uses only normal flow measurements. New constraints on the smoothness of the surfaces in view are formulated to relate structure and motion directly to image derivatives, and on the basis of these constraints the transformation of the viewing geometry between consecutive images is estimated. The calibration parameters are then estimated from the rotational components of several flow fields. As the proposed technique neither requires a special set up nor needs exact correspondence it is potentially useful for the calibration of active vision systems which have to acquire knowledge about their intrinsic parameters while they perform other tasks, or as a tool for analyzing image sequences in large video databases.	Philips Res, Briarcliff Manor, NY 10510 USA; Univ Maryland, Ctr Automat Res, Comp Vis Lab, College Pk, MD 20742 USA	Philips; Philips Research; University System of Maryland; University of Maryland College Park	Brodsky, T (corresponding author), Philips Res, 345 Scarborough Rd, Briarcliff Manor, NY 10510 USA.	tbr@philabs.research.philips.com; fer@cfar.umd.edu						ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; ARMSTRONG M, 1996, P 4 EUR C COMP VIS, V1, P3; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; BERGEN JR, 1992, P EUR C COMP VIS, P237; BLACK M, 1994, P EUR C COMP VIS STO, P138; BORISENKO AI, 1986, VECTOR TENSOR ANAL A; Brodsky T, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P83, DOI 10.1109/ICCV.1998.710704; BRODSKY T, 1998, P EUR C COMP VIS, P342; Cheong L, 1998, COMPUT VIS IMAGE UND, V71, P356, DOI 10.1006/cviu.1997.0649; Dron L., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P501, DOI 10.1109/CVPR.1993.341083; Faugeras O., 1992, 3 DIMENSIONAL COMPUT; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; FERMULLER C, 1995, SCIENCE, V270, P1973, DOI 10.1126/science.270.5244.1973; FERMULLER C, 1993, ACTIVE PERCEPTION, pCH3; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; HARTLEY RI, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P908; HARTLEY RI, 1994, P 3 EUR C COMP VIS S, V1, P471; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; LENZ RK, 1988, IEEE T PATTERN ANAL, V10, P713, DOI 10.1109/34.6781; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; MENDELSOHN J, 1997, P INT C COMP AN IM P, P255; Nagel HH, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1006, DOI 10.1109/ICCV.1998.710839; NAGEL HH, 1995, INT J COMPUT VISION, V15, P271, DOI 10.1007/BF01451744; NEGAHDARIPOUR S, 1987, IEEE T PATTERN ANAL, V9, P168, DOI 10.1109/TPAMI.1987.4767884; POLLEFEYS M, 1996, P INT C PATT REC VIE, VA, P349; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; Strang G., 1988, LINEAR ALGEBRA APPL, V3rd; Tsai R.Y., 1986, P IEEE C COMP VIS PA, P364; Vieville T, 1996, COMPUT VIS IMAGE UND, V64, P128, DOI 10.1006/cviu.1996.0049	30	9	9	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2002	48	2					91	114		10.1023/A:1016094806773	http://dx.doi.org/10.1023/A:1016094806773			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	567AZ					2022-12-18	WOS:000176461600002
J	Mokhtarian, F; Khalili, N; Yuen, P				Mokhtarian, F; Khalili, N; Yuen, P			Estimation of error in curvature computation on multi-scale free-form surfaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						free-form surfaces; multi-scale description; local parametrisation; semigeodesic coordinates; Gaussian and mean curvatures; estimation error	MODEL-BASED RECOGNITION; 3D OBJECT RECOGNITION; SHAPE REPRESENTATION; RANGE IMAGES; 3-D; RECOVERY	A novel technique for multi-scale curvature computation on a free-form 3-D surface is presented. This is achieved by convolving local parametrisations of the surface with 2-D Gaussian filters iteratively. In our technique, semigeodesic coordinates are constructed at each vertex of the mesh. Smoothing results are shown for 3-D surfaces with different shapes indicating that surface noise is eliminated and surface details are removed gradually. A number of evolution properties of 3-D surfaces are described. Next, the surface Gaussian and mean curvature values are estimated accurately at multiple scales which are then mapped to colours and displayed directly on the surface. The performance of the technique when selecting different directions as an arbitrary direction for the geodesic at each vertex are also presented. The results indicate that the error observed for the estimation of Gaussian and mean curvatures is quite low after only one iteration. Furthermore, as the surface is smoothed iteratively, the error is further reduced. The results also show that the estimation error of Gaussian curvature is less than that of mean curvature. Our experiments demonstrate that estimation of smoothed surface curvatures are very accurate and not affected by the arbitrary direction of the first geodesic line when constructing semigeodesic coordinates. Our technique is independent of the underlying triangulation and is also more efficient than volumetric diffusion techniques since 2-D rather than 3-D convolutions are employed. Finally, the method presented here is a generalisation of the Curvature Scale Space method for 2-D contours. The CSS method has outperformed comparable techniques within the MPEG-7 evaluation framework. As a result, it has been selected for inclusion in the MPEG-7 package of standards.	Univ Surrey, Sch Elect Engn Informat Technol & Math, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England	University of Surrey	Mokhtarian, F (corresponding author), Univ Surrey, Sch Elect Engn Informat Technol & Math, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.	F.Mokhtarian@ee.surrey.ac.uk; N.Khalili@ee.surrey.ac.uk; P.Yuen@ee.surrey.ac.uk	Yuen, P. C./C-1392-2008					BANKS S, 1990, SIGNAL PROCESSING IM; BERKMANN J, 1994, IEEE T PATTERN ANAL, V16, P1114, DOI 10.1109/34.334391; Besl P J., 1990, MACHINE VISION 3 DIM, P25; BESL PJ, 1985, COMPUT SURV, V17, P75, DOI 10.1145/4078.4081; BESL PJ, 1986, COMPUT VISION GRAPH, V33, P33, DOI 10.1016/0734-189X(86)90220-3; BRECHBUHLER C, 1995, COMPUT VIS IMAGE UND, V61, P154, DOI 10.1006/cviu.1995.1013; CHEN TW, 1994, IEEE T PATTERN ANAL, V16, P719, DOI 10.1109/34.297953; CHERN SS, 1954, COMMENTARIES MATH HE, V28; CHIN RT, 1986, COMPUT SURV, V18, P67, DOI 10.1145/6462.6464; DELINGETTE H, 1992, IMAGE VISION COMPUT, V10, P132, DOI 10.1016/0262-8856(92)90065-B; Delingette H, 1999, INT J COMPUT VISION, V32, P111, DOI 10.1023/A:1008157432188; Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576; DICKINSON SJ, 1992, IEEE T PATTERN ANAL, V14, P174, DOI 10.1109/34.121788; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; Flynn P. J., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P110, DOI 10.1109/CVPR.1989.37837; FLYNN PJ, 1991, IEEE T PATTERN ANAL, V13, P1066, DOI 10.1109/34.99239; Goetz A., 1970, INTRO DIFFERENTIAL G; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; Hilton A., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P117, DOI 10.1007/BFb0015528; Hilton A, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P381, DOI 10.1109/ICIP.1996.560840; HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073; JAIN AK, 1988, IEEE T PATTERN ANAL, V10, P783, DOI 10.1109/34.9102; KANG SB, 1993, IEEE T PATTERN ANAL, V15, P707, DOI 10.1109/34.221171; KEREN D, 1994, IEEE T PATTERN ANAL, V16, P38, DOI 10.1109/34.273718; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1979, BIOL CYBERN, V32, P211, DOI 10.1007/BF00337644; KOENDERINK JJ, 1986, BIOL CYBERN, V53, P383, DOI 10.1007/BF00318204; Kreyszig E, 1959, DIFFERENTIAL GEOMETR; LIANG P, 1994, IEEE T PATTERN ANAL, V16, P249, DOI 10.1109/34.276124; MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591; Mokhtarian F, 1997, COMPUT VIS IMAGE UND, V68, P1, DOI 10.1006/cviu.1997.0544; Mokhtarian F, 2001, IMAGE VISION COMPUT, V19, P271, DOI 10.1016/S0262-8856(00)00076-7; MOKHTARIAN F, 2000, P BRIT MACH VIS C BR, P446; MOKHTARIAN F, 1998, P BRIT MACH VIS C, P730; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; PENTLAND AP, 1986, ARTIF INTELL, V28, P293, DOI 10.1016/0004-3702(86)90052-4; PILU M, 1996, P EUR C COMP VIS CAM, P71; Ping Liang, 1990, Computer Vision, Graphics, and Image Processing, V52, P78, DOI 10.1016/0734-189X(90)90124-E; PONCE J, 1993, 3 DIMENSIONAL OBJECT, P17; RAJA NS, 1994, CVGIP-IMAG UNDERSTAN, V60, P44, DOI 10.1006/ciun.1994.1030; Romeny B.M., 1994, GEOMETRY DRIVEN DIFF; Samet H., 1990, DESIGN ANAL SPATIAL, V85; Schroeder W., 1996, VISUALIZATION TOOLKI; SEIBERT M, 1992, IEEE T PATTERN ANAL, V14, P107, DOI 10.1109/34.121784; Sethian J., 1996, LEVEL SET METHODS; SINHA SS, 1994, HDB PATTERN RECOGNIT, P185; SOLINA F, 1990, IEEE T PATTERN ANAL, V12, P131, DOI 10.1109/34.44401; SOROKA BI, 1976, P 3 IJCPR, P734; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; STODDART AJ, 1998, P EUR C COMP VIS, pR2; SUETENS P, 1992, COMPUT SURV, V24, P5, DOI 10.1145/128762.128763; SWETS DL, 1996, THESIS MICHIGAN STAT; TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P852, DOI 10.1109/ICCV.1995.466848; YUEN P, 1999, SCAND C IM AN GREENL, P303	54	9	9	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2002	48	2					131	149		10.1023/A:1016098907682	http://dx.doi.org/10.1023/A:1016098907682			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	567AZ					2022-12-18	WOS:000176461600004
J	Demirdjian, D; Darrell, T				Demirdjian, D; Darrell, T			Using multiple-hypothesis disparity maps and image velocity for 3-D motion estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion estimation; stereo; robust estimation; disparity space; multi-hypothesis tracking		In this paper we explore a multiple hypothesis approach to estimating rigid motion from a moving stereo rig. More precisely, we introduce the use of Gaussian mixtures to model correspondence uncertainties for disparity and image velocity estimation. We show some properties of the disparity space and show how rigid transformations can be represented. An algorithm derived from standard random sampling-based robust estimators, that efficiently estimates rigid transformations from multi-hypothesis disparity maps and velocity fields is given.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Demirdjian, D (corresponding author), MIT, AI Lab, Cambridge, MA 02139 USA.	demirdji@ai.mit.edu; trevor@ai.mit.edu						BERGEN JR, 1992, EUR C COMP VIS, P237; Bregler C, 1997, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.1997.609382; Demirdjian D, 2000, COMPUT VIS IMAGE UND, V78, P53, DOI 10.1006/cviu.1999.0827; Demirdjian D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P213, DOI 10.1109/ICCV.2001.937520; Devernay F, 1996, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.1996.517084; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Harville M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P206, DOI 10.1109/ICCV.1999.791219; HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127; IRANI M, 1994, INT J COMPUT VISION, V12, P5, DOI 10.1007/BF01420982; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; Lhuillier M, 2000, INT C PATT RECOG, P968, DOI 10.1109/ICPR.2000.905620; ONeill M, 1996, IMAGE VISION COMPUT, V14, P225, DOI 10.1016/0262-8856(95)01061-0; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; STEIN GP, 1998, P COMP VIS PATT REC; ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4	16	9	10	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					219	228		10.1023/A:1014502126337	http://dx.doi.org/10.1023/A:1014502126337			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN					2022-12-18	WOS:000174354700016
J	Ng, KC; Trivedi, M; Ishiguro, H				Ng, KC; Trivedi, M; Ishiguro, H			Generalized multiple baseline stereo and direct virtual view synthesis using range-space search, match, and render	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						range-space approach; generalized multiple baseline stereo; direct virtual view synthesis; wide-baseline stereo; volumetric matching template; matching curve characteristics; error region; volume growing; virtual walkthrough; image-based rendering; omni-directional video		A new "range-space" approach is described for synergistic resolution of both stereovision and reflectance (visual) modeling problems simultaneously. This synergistic approach can be applied to arbitrary camera arrangements with different intrinsic and extrinsic parameters, image types, image resolutions, and image number. These images are analyzed in a step-wise manner to extract 3-D range measurements and also to render a customized perspective view. The entire process is fully automatic. An extensive and detailed experimental validation phase supports the basic feasibility and generality of the Range-Space Approach.	STMicroelect, AST La Jolla Lab, San Diego, CA 92121 USA; Univ Calif San Diego, Comp Vis & Robot Res Lab, La Jolla, CA 92093 USA; Wakayama Univ, Dept Comp & Commun Sci, Wakayama, Japan	STMicroelectronics; University of California System; University of California San Diego; Wakayama University	Ng, KC (corresponding author), STMicroelect, AST La Jolla Lab, 4690 Execut Dr, San Diego, CA 92121 USA.							Aliaga DG, 2001, COMP GRAPH, P443, DOI 10.1145/383259.383311; Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309; Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932; Heigl Benno, 1999, MUSTERERKENNUNG 1999, P3; KANG SB, 1999, P SOC PHOTO-OPT INS, V3641, P2; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; LHUILLIER M, 1998, P BRIT MACH VIS C SO, V2, P700; MAIMONE MW, 1995, IROS '95 - 1995 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS: HUMAN ROBOT INTERACTION AND COOPERATIVE ROBOTS, PROCEEDINGS, VOL 1, P519, DOI 10.1109/IROS.1995.525846; MARAPANE S, 1994, IEEE T PATTERN ANAL, V16; Ng KC, 1999, SECOND IEEE WORKSHOP ON VISUAL SURVEILLANCE (VS'99), PROCEEDINGS, P67, DOI 10.1109/VS.1999.780270; NG KC, 1999, C VID P IEEE VIS SAN; NG KC, 1998, P SPIE, V3528; NG KC, 2001, IEEE WORKSH STER MUL; NG KC, 2000, THESIS U CALIFORNIA; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; PAVLIDIS T, 1990, IEEE T PATTERN ANAL, V12, P225, DOI 10.1109/34.49050; RANDER P, 1996, P INT C MULT FUS INT, P305; Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526; WANG ZF, 1995, P 1995 IEEE INT C SY, V5, P3884; XUAN J, 1995, P IEEE INT C IM PROC, V3, P544; ZHANG Z, 1998, P 3 AS C COMP VIS, P279	21	9	11	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					131	147		10.1023/A:1014589723611	http://dx.doi.org/10.1023/A:1014589723611			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN					2022-12-18	WOS:000174354700010
J	Chang, NL; Zakhor, A				Chang, NL; Zakhor, A			Constructing a multivalued representation for view synthesis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						multivalued representation; arbitrary view synthesis; dense depth estimation; layered depth images (LDI); multiframe stereo algorithm; low-contrast region processing; segmentation and tracking; virtual flythroughs and flyarounds	SEGMENTATION; MOTION; OBJECTS; SCENE	A fundamental problem in computer vision and graphics is that of arbitrary view synthesis for static 3-D scenes, whereby a user-specified viewpoint of the given scene may be created directly from a representation. We propose a novel compact representation for this purpose called the multivalued representation (MVR). Starting with an image sequence captured by a moving camera undergoing either unknown planar translation or orbital motion, a MVR is derived for each preselected reference frame, and may then be used to synthesize arbitrary views of the scene. The representation itself is comprised of multiple depth and intensity levels in which the k-th level consists of points occluded by exactly k surfaces. To build a MVR with respect to a particular reference frame, dense depth maps are first computed for all the neighboring frames of the reference frame. The depth maps are then combined together into a single map, where points are organized by occlusions rather than by coherent affine motions. This grouping facilitates an automatic process to determine the number of levels and helps to reduce the artifacts caused by occlusions in the scene. An iterative multiframe algorithm is presented for dense depth estimation that both handles low-contrast regions and produces piecewise smooth depth maps. Reconstructed views as well as arbitrary flyarounds of real scenes are presented to demonstrate the effectiveness of the approach.	Hewlett Packard Labs, Imaging Technol Dept, Palo Alto, CA 94304 USA; Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	Hewlett-Packard; University of California System; University of California Berkeley	Chang, NL (corresponding author), Hewlett Packard Labs, Imaging Technol Dept, 1501 Page Mill Rd,MS 4U-6, Palo Alto, CA 94304 USA.	nlachang@hpl.hp.com; avz@eecs.Berkeley.EDU	Zakhor, Avideh/GYA-1602-2022	Zakhor, Avideh/0000-0003-4770-6353				ANANDAN P, 1993, MOTION ANAL IMAGE SE, pCH1; ANANDAN P, 1984, P SOC PHOTO-OPT INS, V521, P184; Baker S, 1998, PROC CVPR IEEE, P434, DOI 10.1109/CVPR.1998.698642; Chang NL, 1997, IEEE T IMAGE PROCESS, V6, P584, DOI 10.1109/83.563323; CHANG NL, 1999, THESIS U CALIFORNIA; CHANG NL, 1999, P ICIP INV PAP 25 28, V2, P505; CHANG NL, 1998, P ICIP 5 8 OCT CHIC, V1, P918; CHANG NL, 1997, MULTIVALUED REPRESEN; CHANG NL, 1994, THESIS U CALIFORNIA; Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153; COX IJ, 1992, P BMVC, P337; DARRELL T, 1995, IEEE T PATTERN ANAL, V17, P474, DOI 10.1109/34.391395; DEBEVEC PE, 1996, THESIS U CALIFORNIA; FALKENHAGEN L, 1994, WORKSH COMP IM PROC, P115; Faugeras O., 1994, 3 DIMENSIONAL COMPUT; Fua P., 1993, Machine Vision and Applications, V6, P35, DOI 10.1007/BF01212430; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; HARALICK RM, 1985, COMPUT VISION GRAPH, V29, P100, DOI 10.1016/S0734-189X(85)90153-7; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; INTILLE SS, 1994, 220 MIT MED LAB PERC; Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394; Kang SB, 1997, INT J COMPUT VISION, V25, P167, DOI 10.1023/A:1007971901577; KOCH R, 1993, P EUROGRAPHICS 6 10, P339; LAVEAU S, 1994, 2205 INRIA; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Lim JS, 1990, 2 DIMENSIONAL SIGNAL; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; MATTISON DR, 1989, REPROD TOXICOL, V3, P3, DOI 10.1016/0890-6238(89)90032-4; MAYBANK S, 1993, THEORY RECONSTRUCTIO; McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398; MCMILLAN L, 1995, 95005 U N CAR; Meier T, 1998, IEEE T CIRC SYST VID, V8, P525, DOI 10.1109/76.718500; Murray R. M., 1994, MATH INTRO ROBOTIC M; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; PAL NR, 1993, PATTERN RECOGN, V26, P1277, DOI 10.1016/0031-3203(93)90135-J; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; Sawhney HS, 1996, IEEE T PATTERN ANAL, V18, P814, DOI 10.1109/34.531801; SEITZ SM, 1996, P SIGGRAPH 96, P21; SHADE J, 1998, P SIGGRAPH ORL FL; Shi JB, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P943, DOI 10.1109/ICIP.1998.723676; SHUM HY, 1995, IEEE T PATTERN ANAL, V17, P854, DOI 10.1109/34.406651; Shum HY, 1998, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.1998.698641; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; Vass J, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P958, DOI 10.1109/ICIP.1998.723681; Wang DM, 1998, IEEE T CIRC SYST VID, V8, P539, DOI 10.1109/76.718501; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Weiss Y, 1996, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.1996.517092; Zhang X, 1995, ELECTRON COMM JPN 3, V78, P1, DOI 10.1002/ecjc.4430780801	51	9	9	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2001	45	2					157	190		10.1023/A:1012476031602	http://dx.doi.org/10.1023/A:1012476031602			34	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	487KF					2022-12-18	WOS:000171873700004
J	Duric, Z; Rosenfeld, A; Duncan, J				Duric, Z; Rosenfeld, A; Duncan, J			The applicability of Green's theorem to computation of rate of approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						time to collision; rate of approach; Green's theorem	OPTICAL-FLOW; MOTION; FIELD; IMAGE	The rate of approach (ROA) of a moving observer toward a scene point, as estimated at a given instant, is proportional to the component of the observer's instantaneous velocity in the direction of the point. In this paper we analyze the applicability of Green's theorem to ROA estimation. We derive a formula which relates three quantities: the average value of the ROA for a surface patch in the scene; a surface integral that depends on the surface slant of the patch; and the contour integral of the normal motion field around the image of the boundary of the patch. We analyze how much larger the ROA on the surface patch can be than the value of the contour integral, for given assumptions about the variability of the distance to points on the surface patch. We illustrate our analysis quantitatively using synthetic data, and we also validate it qualitatively on real image sequences.	George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA; Univ Maryland, Ctr Automat Res, College Pk, MD 20742 USA; Univ Maryland, Dept Mech Engn, College Pk, MD 20742 USA	George Mason University; University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Duric, Z (corresponding author), George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.		Duncan, James H/FVY-7148-2022	Duncan, James H/0000-0003-3740-9881				ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; ALBUS JS, 1991, IEEE T SYST MAN CYB, V21, P473, DOI 10.1109/21.97471; ALOIMONOS Y, 1994, INT J COMPUT VISION, V13, P33, DOI 10.1007/BF01420794; ANCONA N, 1993, P DARPA IM UND WORKS, P673; ATKINSON K. E, 1989, INTRO NUMERICAL ANAL; BRUSS AR, 1983, COMPUT VISION GRAPH, V21, P3, DOI 10.1016/S0734-189X(83)80026-7; Burlina P, 1998, INT J COMPUT VISION, V28, P175, DOI 10.1023/A:1008067101494; Burlina P, 1996, IEEE T PATTERN ANAL, V18, P1029, DOI 10.1109/34.541412; CIPOLLA R, 1992, P 2 EUR C COMP VIS, P187; FRANCOIS E, 1990, IMAGE VISION COMPUT, V8, P279, DOI 10.1016/0262-8856(90)80004-D; HORN BKP, 1981, ARTIF INTELL, V17, P189; IKEUCHI K, 1984, ARTIF INTELL, V22, P49, DOI 10.1016/0004-3702(84)90025-0; KOENDERINK JJ, 1975, OPT ACTA, V22, P773, DOI 10.1080/713819112; LEE DN, 1980, PHILOS T R SOC B, V290, P169, DOI 10.1098/rstb.1980.0089; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Marsden J. E., 1976, VECTOR CALCULUS; MAYBANK S, 1987, IMAGE VISION COMPUT, V5, P111, DOI 10.1016/0262-8856(87)90036-9; Meyer F., 1992, Proceedings. 11th IAPR International Conference on Pattern Recognition. Vol.1. Conference A: Computer Vision and Applications, P78, DOI 10.1109/ICPR.1992.201512; NAKAYAMA K, 1985, VISION RES, V25, P625, DOI 10.1016/0042-6989(85)90171-3; NELSON RC, 1989, IEEE T PATTERN ANAL, V11, P1102, DOI 10.1109/34.42840; POGGIO T, 1991, 1289 AI MIT; SHARMA R, 1992, P IEEE RSJ INT C INT; SUBBARAO M, 1990, COMPUT VISION GRAPH, V50, P329, DOI 10.1016/0734-189X(90)90151-K; TISTARELLI M, 1993, IEEE T PATTERN ANAL, V15, P401, DOI 10.1109/34.206959; Verri A., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P171	25	9	9	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1999	31	1					83	98		10.1023/A:1008098810511	http://dx.doi.org/10.1023/A:1008098810511			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	186GM					2022-12-18	WOS:000079720600004
J	LI, SZ				LI, SZ			INVARIANT SURFACE SEGMENTATION THROUGH ENERGY MINIMIZATION WITH DISCONTINUITIES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							RELAXATION; RECOGNITION; IMAGES; VISION	The computational problems in segmenting range data into surface patches based on the invariant surface properties, i.e., mean curvature H and Gaussian curvature K, are investigated. The goal is to obtain reliable HK surface maps. Two commonly encountered problems are: firstly the noise effect in computing derivative estimates, and secondly the smoothing across discontinuities. Here, the segmentation is formulated as finding minimization solutions of energy functionals involving discontinuities. A two-stage approach to the goal is presented: stage (1) from a range image to curvature images and stage (2) from the curvature images to the HK maps. In both stages, solutions are found through minimizing energy functionals that measure the degree of bias of a solution from two constraints: the closeness of the solution to the data, and the smoothness of the solution controlled by predetermined discontinuities. Propagation across discontinuities is prevented during minimization, which preserves the original surface shapes. Experimental results are given for a variety of test images.	UNIV SURREY,DEPT ELECTR & ELECT ENGN,GUILDFORD GU2 5XH,SURREY,ENGLAND	University of Surrey								BESL PJ, 1986, COMPUT VISION GRAPH, V33, P33, DOI 10.1016/0734-189X(86)90220-3; BESL PJ, 1988, SURFACES RANGE IMAGE; BRADY M, 1985, COMPUT VISION GRAPH, V32, P1, DOI 10.1016/0734-189X(85)90001-5; COURANT R, 1953, METHOD MATH PHYSICS, V1; FAN TJ, 1987, FEB P DARPA IM UND W, P351; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GRIMSON WEL, 1984, IMAGES SURFACES; HARALICK RM, 1983, INT J ROBOT RES, V2, P50, DOI 10.1177/027836498300200105; HILDRETH E, 1983, MEASUREMENT VISUAL M; HOFFMAN R, 1987, IEEE T PATTERN ANAL, V9, P608, DOI 10.1109/TPAMI.1987.4767955; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; KOCH C, 1985, MIT751 AI LAB MEM; LECLERC YG, 1987, IEEE T PATT ANAL MAC, V9; LEE D, 1987, 1ST P INT C COMP VIS, P572; LI SZ, 1989, 2ND P INT C INT AUT, P572; LI SZ, 1990, VSSPTR390 SURR U EE; LI SZ, 1989, APR P INT WORKSH MAC; Lipschutz M., 1969, DIFFERENTIAL GEOMETR; Liu S.-C., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P184, DOI 10.1109/CVPR.1989.37848; MARROQUIN JL, 1985, AI860 MIT TECH REP; MEDIONI G, 1984, OCT P IM UND WORKSH, P291; POGGIO T, 1985, PROC R SOC SER B-BIO, V226, P303, DOI 10.1098/rspb.1985.0097; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519; Serra J, 1982, IMAGE ANAL MATH MORP; Smith G. D., NUMERICAL SOLUTION P; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P129, DOI 10.1109/TPAMI.1986.4767767; TERZOPOULOS D, 1988, IEEE T PATTERN ANAL, V10, P417, DOI 10.1109/34.3908; TERZOPOULOS D, 1983, 8TH P INT JOINT C AR, P1073; Tikhonov A., 1977, SOLUTIONS ILL POSED; TOORE V, 1984, MIT768 AI LAB MEM; WITKIN A, 1987, INT J COMPUT VISION, V1, P133, DOI 10.1007/BF00123162; YUILLE A, 1987, MIT987 AI LAB MEM	35	9	11	2	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1990	5	2					161	194		10.1007/BF00054920	http://dx.doi.org/10.1007/BF00054920			34	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EP651					2022-12-18	WOS:A1990EP65100003
J	Chen, X; Pang, AQ; Yang, W; Ma, YX; Xu, L; Yu, JY				Chen, Xin; Pang, Anqi; Yang, Wei; Ma, Yuexin; Xu, Lan; Yu, Jingyi			SportsCap: Monocular 3D Human Motion Capture and Fine-Grained Understanding in Challenging Sports Videos	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human modeling; 3D motion capture; Motion understanding		Markerless motion capture and understanding of professional non-daily human movements is an important yet unsolved task, which suffers from complex motion patterns and severe self-occlusion, especially for the monocular setting. In this paper, we propose SportsCap-the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. Our approach utilizes the semantic and temporally structured sub-motion prior in the embedding space for motion capture and understanding in a data-driven multi-task manner. To enable robust capture under complex motion patterns, we propose an effective motion embedding module to recover both the implicit motion embedding and explicit 3D motion details via a corresponding mapping function as well as a sub-motion classifier. Based on such hybrid motion information, we introduce a multi-stream spatial-temporal graph convolutional network to predict the fine-grained semantic action attributes, and adopt a semantic attribute mapping block to assemble various correlated action attributes into a high-level action label for the overall detailed understanding of the whole sequence, so as to enable various applications like action assessment or motion scoring. Comprehensive experiments on both public and our proposed datasets show that with a challenging monocular sports video input, our novel approach not only significantly improves the accuracy of 3D human motion capture, but also recovers accurate fine-grained semantic action attribute.	[Chen, Xin; Pang, Anqi; Ma, Yuexin; Xu, Lan; Yu, Jingyi] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai, Peoples R China; [Chen, Xin; Pang, Anqi] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai, Peoples R China; [Chen, Xin; Pang, Anqi] Univ Chinese Acad Sci, Shanghai, Peoples R China; [Yang, Wei] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China	ShanghaiTech University; Chinese Academy of Sciences; Shanghai Institute of Microsystem & Information Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Huazhong University of Science & Technology	Xu, L; Yu, JY (corresponding author), ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai, Peoples R China.	chenxin2@shanghaitech.edu.cn; pangaq@shanghaitech.edu.cn; wyangcs@udel.edu; mayuexin@shanghaitech.edu.cn; xulan1@shanghaitech.edu.cn; yujingyi@shanghaitech.edu.cn		Chen, Xin/0000-0002-9347-1367; Xu, Lan/0000-0002-8807-7787	NSFC programs [61976138, 61977047]; National Key Research and Development Program [2018YFB2100500]; STCSM [2015F0203-000-06]; SHMEC [2019-01-07-00-01-E00003]	NSFC programs(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program; STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); SHMEC	This work was supported by NSFC programs (61976138, 61977047), the National Key Research and Development Program (2018YFB2100500), STCSM (2015F0203-000-06) and SHMEC (2019-01-07-00-01-E00003).	Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Bertasius G., 2018, ARXIV PREPRINT ARXIV; Bertasius G, 2018, PROC CVPR IEEE, P5889, DOI 10.1109/CVPR.2018.00617; Bertasius G, 2017, IEEE I CONF COMP VIS, P2196, DOI 10.1109/ICCV.2017.239; Cao ZR, 2021, IEEE T SYST MAN CY-S, V51, P1523, DOI 10.1109/TSMC.2019.2898428; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen JH, 2019, IEEE COMPUT SOC CONF, P2497, DOI 10.1109/CVPRW.2019.00305; Chen X., 2019, ARXIV PREPRINT ARXIV; Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734; Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fani M, 2017, IEEE COMPUT SOC CONF, P85, DOI 10.1109/CVPRW.2017.17; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; He YN, 2021, PROC CVPR IEEE, P11395, DOI 10.1109/CVPR46437.2021.01124; Heilbron Fabian Caba, 2015, IEEE C COMP VIS PATT; Henaff M, 2015, ARXIV150605163; Hu T., 2019, SEE BETTER LOOKING C; Hussein N, 2019, PROC CVPR IEEE, P254, DOI 10.1109/CVPR.2019.00034; Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394; Kanojia G, 2019, IEEE COMPUT SOC CONF, P2467, DOI 10.1109/CVPRW.2019.00302; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kingma D.P., 2015, INT C LEARN REPR, P1; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Li CL, 2018, AAAI CONF ARTIF INTE, P3482; Li RY, 2018, AAAI CONF ARTIF INTE, P3546; Li YW, 2018, LECT NOTES COMPUT SC, V11210, P520, DOI 10.1007/978-3-030-01231-1_32; Li Z, 2019, IEEE INT CON MULTI, P640, DOI 10.1109/ICME.2019.00116; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Loper M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661273; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; Luyang Zhu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P177, DOI 10.1007/978-3-030-58558-7_11; Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Nibali A, 2017, IEEE COMPUT SOC CONF, P94, DOI 10.1109/CVPRW.2017.18; Nie BX, 2015, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2015.7298734; Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006; Parmar P, 2019, PROC CVPR IEEE, P304, DOI 10.1109/CVPR.2019.00039; Parmar P, 2019, IEEE WINT CONF APPL, P1468, DOI 10.1109/WACV.2019.00161; Parmar P, 2017, IEEE COMPUT SOC CONF, P76, DOI 10.1109/CVPRW.2017.16; Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123; Pirsiavash H, 2014, LECT NOTES COMPUT SC, V8694, P556, DOI 10.1007/978-3-319-10599-4_36; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Pishchulin L, 2014, LECT NOTES COMPUT SC, V8753, P678, DOI 10.1007/978-3-319-11752-2_56; Raaj Y, 2019, PROC CVPR IEEE, P4615, DOI 10.1109/CVPR.2019.00475; Ran LY, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17061341; Rematas K, 2018, PROC CVPR IEEE, P4738, DOI 10.1109/CVPR.2018.00498; Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883; Sha Long, 2020, P IEEE CVF C COMP VI, P13627, DOI DOI 10.1109/CVPR42600.2020; Shao D, 2020, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR42600.2020.00269; Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230; Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7; Soomro K., 2012, ARXIV; Su S, 2017, PROC CVPR IEEE, P1206, DOI 10.1109/CVPR.2017.133; Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Suo X, 2021, PROC CVPR IEEE, P6222, DOI 10.1109/CVPR46437.2021.00616; Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Varol Gul, 2018, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2017.2712608; Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668; Wen YH, 2019, AAAI CONF ARTIF INTE, P8989; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Xu LL, 2020, INNOV EDUC TEACH INT, V57, P724, DOI 10.1080/14703297.2019.1593214; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741; Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280; Zhang XK, 2020, IEEE T NEUR NET LEAR, V31, P3047, DOI 10.1109/TNNLS.2019.2935173; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49; Zhou K, 2016, DESTECH TRANS COMP	74	8	8	10	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2021	129	10					2846	2864		10.1007/s11263-021-01486-4	http://dx.doi.org/10.1007/s11263-021-01486-4		AUG 2021	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH4LO		Green Submitted			2022-12-18	WOS:000681171700003
J	Gupta, P; Thatipelli, A; Aggarwal, A; Maheshwari, S; Trivedi, N; Das, S; Sarvadevabhatla, RK				Gupta, Pranay; Thatipelli, Anirudh; Aggarwal, Aditya; Maheshwari, Shubh; Trivedi, Neel; Das, Sourav; Sarvadevabhatla, Ravi Kiran			Quo Vadis, Skeleton Action Recognition?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human action recognition; Human activity recognition; Skeleton; 3-D human pose; Deep learning		In this paper, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. To study skeleton-action recognition in the wild, we introduce Skeletics-152, a curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset. We extend our study to include out-of-context actions by introducing Skeleton-Mimetics, a dataset derived from the recently introduced Mimetics dataset. We also introduce Metaphorics, a dataset with caption-style annotated YouTube videos of the popular social game Dumb Charades and interpretative dance performances. We benchmark state-of-the-art models on the NTU-120 dataset and provide multi-layered assessment of the results. The results from benchmarking the top performers of NTU-120 on the newly introduced datasets reveal the challenges and domain gap induced by actions in the wild. Overall, our work characterizes the strengths and limitations of existing approaches and datasets. Via the introduced datasets, our work enables new frontiers for human action recognition.	[Gupta, Pranay; Thatipelli, Anirudh; Aggarwal, Aditya; Maheshwari, Shubh; Trivedi, Neel; Das, Sourav; Sarvadevabhatla, Ravi Kiran] IIIT Hyderabad, KCIS, F23,3rd Floor, Hyderabad 50032, India	International Institute of Information Technology Hyderabad	Sarvadevabhatla, RK (corresponding author), IIIT Hyderabad, KCIS, F23,3rd Floor, Hyderabad 50032, India.	ravi.kiran@iiit.ac.in		Sarvadevabhatla, Ravi Kiran/0000-0003-4134-1154	MeitY, Government of India	MeitY, Government of India	We wish to thank the anonymous reviewers for their detailed and constructive feedback. We also wish to thank Kalyan Adithya and Sai Shashank Kalakonda for their efforts in creating the project page. This work is partly supported by MeitY, Government of India.	Angelini F., 2018, ACTIONXPOSE NOVEL 2D; Ayumi V, 2016, IEEE ST CONF RES DEV; Caetano C, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS); Caetano C, 2019, SIBGRAPI, P16, DOI 10.1109/SIBGRAPI.2019.00011; Cai YJ, 2019, AAAI CONF ARTIF INTE, P8118; Cao Y, 2013, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2013.343; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Carreira Joao, 2019, ARXIV190706987; Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026; Cheron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368; Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569; Eweiwi A, 2015, LECT NOTES COMPUT SC, V9007, P428, DOI 10.1007/978-3-319-16814-2_28; Halim AA, 2016, IEEE IMAGE PROC, P3041, DOI 10.1109/ICIP.2016.7532918; Hussein Mohamed E, 2013, 23 INT JOINT C ART I; Jasani B, 2019, ARXIV PREPRINT ARXIV, P1; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Jiaying, 2017, ACM MULT WORKSH; Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207; Kipp M, 2001, EV EUR C SPEECH COMM; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Kundu J. N., 2018, ARXIV181202592 CORR; Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217; Li C, 2018, IEEE INT CONF SENS, P1, DOI 10.1109/TFUZZ.2018.2878200; Li C, 2017, IEEE INT CONF MULTI; Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873; Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022; Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019; Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214; Moon G., 2020, INTEGRAL ACTION POSE; Muller M., 2007, CG20072 U BONN, P6; Peng W, 2020, AAAI CONF ARTIF INTE, V34, P2669; Rogez G, 2020, IEEE T PATTERN ANAL, V42, P1146, DOI 10.1109/TPAMI.2019.2892985; Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349; Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810; Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132; Sigurdsson GA, 2017, IEEE I CONF COMP VIS, P2156, DOI 10.1109/ICCV.2017.235; Song SJ, 2017, AAAI CONF ARTIF INTE, P4263; Song YL, 2020, IEEE T FUZZY SYST, V28, P544, DOI 10.1109/TFUZZ.2019.2910714; Tang C., 2016, INT WORKSH UND HUM A, P101; Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang PC, 2018, COMPUT VIS IMAGE UND, V171, P118, DOI 10.1016/j.cviu.2018.04.007; Weinzaepfel P., 2019, ARXIV PREPRINT ARXIV; Weinzaepfel P, 2021, INT J COMPUT VISION, V129, P1675, DOI 10.1007/s11263-021-01446-y; Werner, 2019, ARXIV PREPRINT ARXIV; Wu C, 2019, IEEE INT CONF COMP V, P1740, DOI 10.1109/ICCVW.2019.00216; Yan Sijie, 2018, AAAI; Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631; Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2017.233; Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885	57	8	8	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2097	2112		10.1007/s11263-021-01470-y	http://dx.doi.org/10.1007/s11263-021-01470-y		MAY 2021	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW		Green Submitted			2022-12-18	WOS:000647351500001
J	Xie, Q; Lai, YK; Wu, J; Wang, ZT; Zhang, YM; Xu, K; Wang, J				Xie, Qian; Lai, Yu-Kun; Wu, Jing; Wang, Zhoutao; Zhang, Yiming; Xu, Kai; Wang, Jun			Vote-Based 3D Object Detection with Context Modeling and SOB-3DNMS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object detection; Point cloud processing; 3D deep learning	NETWORK	Most existing 3D object detection methods recognize objects individually, without giving any consideration on contextual information between these objects. However, objects in indoor scenes are usually related to each other and the scene, forming the contextual information. Based on this observation, we propose a novel 3D object detection network, which is built on the state-of-the-art VoteNet but takes into consideration of the contextual information at multiple levels for detection and recognition of 3D objects. To encode relationships between elements at different levels, we introduce three contextual sub-modules, capturing contextual information at patch, object, and scene levels respectively, and build them into the voting and classification stages of VoteNet. In addition, at the post-processing stage, we also consider the spatial diversity of detected objects and propose an improved 3D NMS (non-maximum suppression) method, namely Survival-Of-the-Best 3DNMS (SOB-3DNMS), to reduce false detections. Experiments demonstrate that our method is an effective way to promote detection accuracy, and has achieved new state-of-the-art detection performance on challenging 3D object detection datasets, i.e., SUN RGBD and ScanNet, when only taking point cloud data as input.	[Xie, Qian; Wang, Zhoutao; Zhang, Yiming; Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Nanjing, Peoples R China; [Lai, Yu-Kun; Wu, Jing] Cardiff Univ, Cardiff, Wales; [Xu, Kai] Natl Univ Def Technol, Changsha, Peoples R China	Nanjing University of Aeronautics & Astronautics; Cardiff University; National University of Defense Technology - China	Wang, J (corresponding author), Nanjing Univ Aeronaut & Astronaut, Nanjing, Peoples R China.	wjun@nuaa.edu.cn	Wang, Jun/AAM-6868-2021; Zhang, Yiming/HGB-7344-2022	Wang, Jun/0000-0001-9223-2615; 	National Key Research and Development Program of China [2020YFB2010702, 2018A-AA0102200]; National Natural Science Foundation of China [61772267]; Aeronautical Science Foundation of China [2019ZE052008]; Natural Science Foundation of Jiangsu Province [BK20190016]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Aeronautical Science Foundation of China; Natural Science Foundation of Jiangsu Province(Natural Science Foundation of Jiangsu Province)	This work is funded by the National Key Research and Development Program of China (2020YFB2010702, 2018A-AA0102200), National Natural Science Foundation of China under Grant 61772267, Aeronautical Science Foundation of China (No. 2019ZE052008), and the Natural Science Foundation of Jiangsu Province under Grant BK20190016.	Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301; Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593; Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246; Carion Nicolas, 2020, EUR C COMP VIS ECCV; Chen JT, 2020, PROC CVPR IEEE, P389, DOI 10.1109/CVPR42600.2020.00047; Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5; Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028; Engelmann F, 2017, IEEE INT CONF COMP V, P716, DOI 10.1109/ICCVW.2017.90; Engelmann Francis, 2020, P IEEE CVF C COMP VI, P9031, DOI DOI 10.1109/CVPR42600.2020.00905; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Guan'an Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P275, DOI 10.1007/978-3-030-58598-3_17; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu SM, 2020, IEEE T VIS COMPUT GR, V26, P2485, DOI 10.1109/TVCG.2018.2889944; Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492; Lahoud J, 2017, IEEE I CONF COMP VIS, P4632, DOI 10.1109/ICCV.2017.495; Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298; Li Junnan, 2020, ARXIV PREPRINT ARXIV; Li Y, 2020, ISPRS J PHOTOGRAMM, V165, P43, DOI 10.1016/j.isprsjprs.2020.05.008; Liu ST, 2019, PROC CVPR IEEE, P6452, DOI [10.1109/CVPR.2019.00662, 10.1109/CVPR.2019.01055]; Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910; Liu Y, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278091; McCormac J, 2018, INT CONF 3D VISION, P32, DOI 10.1109/3DV.2018.00015; Najibi Mahyar, 2020, P IEEE CVF C COMP VI, P11913; Paigwar A, 2019, IEEE COMPUT SOC CONF, P1297, DOI 10.1109/CVPRW.2019.00169; Qi CR, 2020, PROC CVPR IEEE, P4403, DOI 10.1109/CVPR42600.2020.00446; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Qi Charles R, 2017, ARXIV170602413; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; REN Z, 2016, PROC CVPR IEEE, P1525, DOI DOI 10.1109/CVPR.2016.169; Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075; Salscheider N.O., 2020, ARXIV PREPRINT ARXIV; Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054; Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086; Shi Weijing, 2020, P IEEE CVF C COMP VI, P1711, DOI DOI 10.1109/CVPR42600.2020.00178; Shi YF, 2019, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2019.00187; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang T, 2013, PROC CVPR IEEE, P1790, DOI 10.1109/CVPR.2013.234; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xie Qizhe, 2020, P IEEE CVF C COMP VI, V1, P3, DOI 10.1109/cvpr42600.2020.01046; Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484; Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Ye XQ, 2018, LECT NOTES COMPUT SC, V11211, P415, DOI 10.1007/978-3-030-01234-2_25; Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407; Yin Tianwei, 2020, ARXIV PREPRINT ARXIV; Yu R, 2016, ARXIV160902948; Yue KY, 2018, ADV NEUR IN, V31; Zambaldi Vinicius, 2018, ARXIV180601830; Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105; Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064; Zhang WX, 2019, PROC CVPR IEEE, P12428, DOI 10.1109/CVPR.2019.01272; Zhang YD, 2014, LECT NOTES COMPUT SC, V8694, P668, DOI 10.1007/978-3-319-10599-4_43; Zhang Yonghui, 2017, ICCV; Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472	73	8	8	7	27	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1857	1874		10.1007/s11263-021-01456-w	http://dx.doi.org/10.1007/s11263-021-01456-w		APR 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU		Green Accepted			2022-12-18	WOS:000635874300001
J	Yu, AJ; Wu, HX; Huang, HB; Lei, Z; He, R				Yu, Aijing; Wu, Haoxue; Huang, Huaibo; Lei, Zhen; He, Ran			LAMP-HQ: A Large-Scale Multi-pose High-Quality Database and Benchmark for NIR-VIS Face Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Heterogeneous face recognition; Near infrared-visible matching; Database; Variational spectral attention; Spectral conditional attention		Near-infrared-visible (NIR-VIS) heterogeneous face recognition matches NIR to corresponding VIS face images. However, due to the sensing gap, NIR images often lose some identity information so that the NIR-VIS recognition issue is more difficult than conventional VIS face recognition. Recently, NIR-VIS heterogeneous face recognition has attracted considerable attention in the computer vision community because of its convenience and adaptability in practical applications. Various deep learning-based methods have been proposed and substantially increased the recognition performance, but the lack of NIR-VIS training samples leads to the difficulty of the model training process. In this paper, we propose a new Large-Scale Multi-Pose High-Quality NIR-VIS database `LAMP-HQ' containing 56,788 NIR and 16,828 VIS images of 573 subjects with large diversities in pose, illumination, attribute, scene and accessory. We furnish a benchmark along with the protocol for NIR-VIS face recognition via generation on LAMP-HQ, including Pixel2-Pixel, CycleGAN, ADFL, PCFH, and PACH. Furthermore, we propose a novel exemplar-based variational spectral attention network to produce high-fidelity VIS images from NIR data. A spectral conditional attentionmodule is introduced to reduce the domain gap between NIR and VIS data and then improve the performance of NIR-VIS heterogeneous face recognition on various databases including the LAMP-HQ.	[Yu, Aijing; Huang, Huaibo; He, Ran] CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; [Wu, Haoxue; Lei, Zhen] CASIA, Ctr Biometr & Secur Res, Beijing, Peoples R China; [Yu, Aijing; Wu, Haoxue; Huang, Huaibo; Lei, Zhen; He, Ran] CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Yu, Aijing; Wu, Haoxue; Huang, Huaibo; Lei, Zhen; He, Ran] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	He, R (corresponding author), CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.; He, R (corresponding author), CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China.; He, R (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.	aijing.yu@cripac.ia.ac.cn; wuhaoxue2019@ia.ac.cn; huaibo.huang@cripac.ia.ac.cn; zlei@nlpr.ia.ac.cn; rhe@nlpr.ia.ac.cn		Yu, Aijing/0000-0002-4782-9858	National Key Research and Development Program of China [2020AAA0140001]; National Natural Science Foundation of China [61721004, U20A20223, 62006228]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is partially funded by National Key Research and Development Program of China (Grant No. 2020AAA0140001), and National Natural Science Foundation of China (Grant Nos. 61721004, U20A20223, 62006228).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bernhard J, 2015, PSYCHOL MED, P1, DOI [10.1002/9781118960608.obm00033, DOI 10.1002/9781118960608.OBM00033]; Biswas S, 2012, IEEE T PATTERN ANAL, V34, P2019, DOI 10.1109/TPAMI.2011.278; Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116; Chen J, 2009, PROC CVPR IEEE, P156, DOI 10.1109/CVPRW.2009.5206832; Di X., 2020, ARXIV200705156; Di X, 2019, INT CONF BIOMETR; Di X, 2018, INT CONF BIOMETR THE; Dong Yi, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163093; Duan B., 2020, P IEEE CVF C COMP VI, P7930, DOI DOI 10.1109/CVPR42600.2020.00795; Gong DH, 2017, IEEE T IMAGE PROCESS, V26, P2079, DOI 10.1109/TIP.2017.2651380; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goswami D, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Gurton KP, 2014, OPT LETT, V39, P3857, DOI 10.1364/OL.39.003857; He R, 2019, IEEE T PATTERN ANAL, V41, P1761, DOI 10.1109/TPAMI.2018.2842770; He R, 2017, AAAI CONF ARTIF INTE, P2000; Hu SW, 2016, IEEE COMPUT SOC CONF, P187, DOI 10.1109/CVPRW.2016.30; Huang D., 2012, IRIPTR12FR001 BEIJ U; Huang DA, 2013, IEEE I CONF COMP VIS, P2496, DOI 10.1109/ICCV.2013.310; Huang LK, 2012, INT C PATT RECOG, P1683; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Huang XS, 2013, IEEE T IMAGE PROCESS, V22, P353, DOI 10.1109/TIP.2012.2215617; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jin Y, 2017, MULTIDIM SYST SIGN P, V28, P905, DOI 10.1007/s11045-016-0401-8; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Juefei-Xu Felix, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P141, DOI 10.1109/CVPRW.2015.7301308; Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740; Kawaguchi K, 2018, AAAI CONF ARTIF INTE, P3382; Kingma D.P, P 3 INT C LEARNING R; Klare BF, 2013, IEEE T PATTERN ANAL, V35, P1410, DOI 10.1109/TPAMI.2012.229; Klare BF, 2011, IEEE T PATTERN ANAL, V33, P639, DOI 10.1109/TPAMI.2010.180; Lei Z, 2012, IEEE T INF FOREN SEC, V7, P1707, DOI 10.1109/TIFS.2012.2210041; Lei Z, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4563043; Lezama J, 2017, PROC CVPR IEEE, P6807, DOI 10.1109/CVPR.2017.720; Li S., 2009, 2009 INT WORKSHOP IN, P1, DOI [10.1109/IWISA.2009.5073068, DOI 10.1109/IWISA.2009.5073068, 10.1049/cp.2009.0112]; Li SZ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P455; Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59; Li SZ, 2006, LECT NOTES COMPUT SC, V3832, P151; Li ZF, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2807705; Lin DH, 2006, LECT NOTES COMPUT SC, V3954, P13; Liu XX, 2016, INT CONF BIOMETR; Maeng H., 2012, AS C COMP VIS, P708; Mallat K, 2018, 2018 INTERNATIONAL CONFERENCE OF THE BIOMETRICS SPECIAL INTEREST GROUP (BIOSIG); Mudunuri SP, 2019, IEEE T INF FOREN SEC, V14, P886, DOI 10.1109/TIFS.2018.2868173; Ouyang SX, 2016, IMAGE VISION COMPUT, V56, P28, DOI 10.1016/j.imavis.2016.09.001; Peng CL, 2019, PATTERN RECOGN, V90, P161, DOI 10.1016/j.patcog.2019.01.041; Reale C, 2016, IEEE COMPUT SOC CONF, P320, DOI 10.1109/CVPRW.2016.47; Riggan BS, 2016, INT CONF BIOMETR THE; Sarfraz M. S., 2015, P BRIT MACHINE VISIO; Sarfraz MS, 2017, INT J COMPUT VISION, V122, P426, DOI 10.1007/s11263-016-0933-2; Saxena S, 2016, LECT NOTES COMPUT SC, V9915, P483, DOI 10.1007/978-3-319-49409-8_40; Shao M, 2017, IEEE T NEUR NET LEAR, V28, P451, DOI 10.1109/TNNLS.2016.2517014; Shao M, 2014, INT J COMPUT VISION, V109, P74, DOI 10.1007/s11263-014-0696-6; Tang X, 2002, IEEE IMAGE PROC, P257; Tang XO, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P687, DOI 10.1109/ICCV.2003.1238414; Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311; Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9; Wang R, 2009, LECT NOTES COMPUT SC, V5558, P319, DOI 10.1007/978-3-642-01793-3_33; Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930; Wu X, 2019, AAAI CONF ARTIF INTE, P9005; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; Xiao LH, 2013, 2013 IEEE SIXTH INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS); Yu JC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1018; Zhang H, 2019, PR MACH LEARN RES, V97; Zhang H, 2019, INT J COMPUT VISION, V127, P845, DOI 10.1007/s11263-019-01175-3; Zhang MJ, 2019, IEEE T IMAGE PROCESS, V28, P642, DOI 10.1109/TIP.2018.2869688; Zhang W, 2011, PROC CVPR IEEE, P513, DOI 10.1109/CVPR.2011.5995324; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu JY, 2014, IEEE T INF FOREN SEC, V9, P501, DOI 10.1109/TIFS.2014.2299977	72	8	8	0	16	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1467	1483		10.1007/s11263-021-01432-4	http://dx.doi.org/10.1007/s11263-021-01432-4		FEB 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC		Green Submitted			2022-12-18	WOS:000617408800002
J	Zou, CH; Su, JW; Peng, CH; Colburn, A; Shan, Q; Wonka, P; Chu, HK; Hoiem, D				Zou, Chuhang; Su, Jheng-Wei; Peng, Chi-Han; Colburn, Alex; Shan, Qi; Wonka, Peter; Chu, Hung-Kuo; Hoiem, Derek			Manhattan Room Layout Reconstruction from a Single 360 degrees Image: A Comparative Study of State-of-the-Art Methods	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D room layout; Deep learning; Single image 3D; Manhattan world		Recent approaches for predicting layouts from 360 degrees panoramas produce excellent results. These approaches build on a common framework consisting of three steps: a pre-processing step based on edge-based alignment, prediction of layout elements, and a post-processing step by fitting a 3D layout to the layout elements. Until now, it has been difficult to compare the methods due to multiple different design decisions, such as the encoding network (e.g., SegNet or ResNet), type of elements predicted (e.g., corners, wall/floor boundaries, or semantic segmentation), or method of fitting the 3D layout. To address this challenge, we summarize and describe the common framework, the variants, and the impact of the design decisions. For a complete evaluation, we also propose extended annotations for the Matterport3D dataset (Chang et al.: Matterport3d: learning from rgb-d data in indoor environments. , 2017), and introduce two depth-based evaluation metrics.	[Zou, Chuhang; Hoiem, Derek] Univ Illinois, Champaign, IL 60680 USA; [Su, Jheng-Wei; Chu, Hung-Kuo] Natl Tsing Hua Univ, Hsinchu, Taiwan; [Peng, Chi-Han] Natl Chiao Tung Univ, Hsinchu, Taiwan; [Peng, Chi-Han] ShanghaiTech Univ, Shanghai, Peoples R China; [Colburn, Alex] Univ Washington, Seattle, WA 98195 USA; [Shan, Qi] Apple Inc, Cupertino, CA 95014 USA; [Wonka, Peter] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia	University of Illinois System; University of Illinois Urbana-Champaign; National Tsing Hua University; National Yang Ming Chiao Tung University; ShanghaiTech University; University of Washington; University of Washington Seattle; Apple Inc; King Abdullah University of Science & Technology	Zou, CH (corresponding author), Univ Illinois, Champaign, IL 60680 USA.	czou4@illinois.edu; jhengweisu@cloud.nthu.edu.tw; pchihan@asu.edu; alex@colburn.org; qshan@apple.com; pwonka@gmail.com; hkchu@cs.nthu.edu.tw; dhoiem@illinois.edu			ONR MURI [N00014-16-1-2007]; Ministry of Science and Technology of Taiwan [108-2218-E-007-050-, 107-2221-E-007-088-MY3]	ONR MURI(MURIOffice of Naval Research); Ministry of Science and Technology of Taiwan(Ministry of Science and Technology, Taiwan)	This research is supported in part by ONR MURI Grant N00014-16-1-2007, iStaging Corp. fund and the Ministry of Science and Technology of Taiwan (108-2218-E-007-050- and 107-2221-E-007-088-MY3). We thank Shang-Ta Yang for providing the source code of DuLa-Net. We thank Cheng Sun for providing the source code of HorizonNet and help run experiments on our provided dataset.	Armeni Iro, 2017, ARXIV170201105; Cabral R, 2014, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2014.546; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; Coughlan J.M., 1999, P ICCV, V2, P941, DOI DOI 10.1109/ICCV.1999.790349; Dasgupta S, 2016, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2016.73; Del Pero L, 2013, PROC CVPR IEEE, P153, DOI 10.1109/CVPR.2013.27; Del Pero L, 2012, PROC CVPR IEEE, P2719, DOI 10.1109/CVPR.2012.6247994; Delage E., 2006, COMP VIS PATT REC 20, V2, P2418, DOI DOI 10.1109/CVPR.2006.23; Flint A., 2010, EUR C COMP VIS, P394; Fukano K, 2016, INT C PATT RECOG, P1768, DOI 10.1109/ICPR.2016.7899892; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hedau V, 2010, ECCV; Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411; Hoiem D, 2005, IEEE I CONF COMP VIS, P654; Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y; Izadinia H, 2017, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2017.260; Kingma D.P, P 3 INT C LEARNING R; Lee CY, 2017, IEEE I CONF COMP VIS, P4875, DOI 10.1109/ICCV.2017.521; Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872; Lee David Changsoo, 2010, NIPS; Liu C, 2018, LECT NOTES COMPUT SC, V11210, P203, DOI 10.1007/978-3-030-01231-1_13; Liu C, 2016, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2016.25; Liu CX, 2015, PROC CVPR IEEE, P3413, DOI 10.1109/CVPR.2015.7298963; Mallya A, 2015, IEEE I CONF COMP VIS, P936, DOI 10.1109/ICCV.2015.113; Monszpart A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766995; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Pintore G., 2016, 2016 IEEE WINT C APP, P1; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ramalingam S, 2013, IEEE I CONF COMP VIS, P497, DOI 10.1109/ICCV.2013.67; Ramalingam S, 2013, PROC CVPR IEEE, P3065, DOI 10.1109/CVPR.2013.394; Ren Yuzhuo, 2016, P AS C COMP VIS, P3; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schwing AG, 2012, LECT NOTES COMPUT SC, V7577, P299, DOI 10.1007/978-3-642-33783-3_22; Schwing AG, 2012, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2012.6248006; Sun C, 2019, PROC CVPR IEEE, P1047, DOI 10.1109/CVPR.2019.00114; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; Wang Fu-En, 2020, ARXIV200313516; Xu J, 2017, IEEE WINT CONF APPL, P354, DOI 10.1109/WACV.2017.46; Yang H, 2016, PROC CVPR IEEE, P5422, DOI 10.1109/CVPR.2016.585; Yang ST, 2018, SA'18: SIGGRAPH ASIA 2018 POSTERS, DOI 10.1145/3283289.3283304; Yang ST, 2019, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2019.00348; Yang YZ, 2018, IEEE CONF COMPUT; Zhang J, 2013, IEEE I CONF COMP VIS, P1273, DOI 10.1109/ICCV.2013.161; Zhang YD, 2014, LECT NOTES COMPUT SC, V8694, P668, DOI 10.1007/978-3-319-10599-4_43; Zhao YB, 2013, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2013.401; Zou CH, 2019, INT J COMPUT VISION, V127, P143, DOI 10.1007/s11263-018-1133-z; Zou CH, 2018, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2018.00219	48	8	8	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1410	1431		10.1007/s11263-020-01426-8	http://dx.doi.org/10.1007/s11263-020-01426-8		FEB 2021	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC					2022-12-18	WOS:000616466300001
J	Luo, CJ; Lin, QX; Liu, YL; Jin, LW; Shen, CH				Luo, Canjie; Lin, Qingxiang; Liu, Yuliang; Jin, Lianwen; Shen, Chunhua			Separating Content from Style Using Adversarial Learning for Recognizing Text in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Text recognition; Attention mechanism; Generative adversarial network; Separation of content and style	SCENE; RECOGNITION; NETWORK	Scene text recognition is an important task in computer vision. Despite tremendous progress achieved in the past few years, issues such as varying font styles, arbitrary shapes and complex backgrounds etc. have made the problem very challenging. In this work, we propose to improve text recognition from a new perspective by separating the text content from complex backgrounds, thus making the recognition considerably easier and significantly improving recognition accuracy. To this end, we exploit the generative adversarial networks (GANs) for removing backgrounds while retaining the text content . As vanilla GANs are not sufficiently robust to generate sequence-like characters in natural images, we propose an adversarial learning framework for the generation and recognition of multiple characters in an image. The proposed framework consists of an attention-based recognizer and a generative adversarial architecture. Furthermore, to tackle the issue of lacking paired training samples, we design an interactive joint training scheme, which shares attention masks from the recognizer to the discriminator, and enables the discriminator to extract the features of each character for further adversarial training. Benefiting from the character-level adversarial training, our framework requires only unpaired simple data for style supervision. Each target style sample containing only one randomly chosen character can be simply synthesized online during the training. This is significant as the training does not require costly paired samples or character-level annotations. Thus, only the input images and corresponding text labels are needed. In addition to the style normalization of the backgrounds, we refine character patterns to ease the recognition task. A feedback mechanism is proposed to bridge the gap between the discriminator and the recognizer. Therefore, the discriminator can guide the generator according to the confusion of the recognizer, so that the generated patterns are clearer for recognition. Experiments on various benchmarks, including both regular and irregular text, demonstrate that our method significantly reduces the difficulty of recognition. Our framework can be integrated into recent recognition methods to achieve new state-of-the-art recognition accuracy.	[Luo, Canjie; Lin, Qingxiang; Liu, Yuliang; Jin, Lianwen] South China Univ Technol, Guangzhou, Peoples R China; [Liu, Yuliang; Shen, Chunhua] Univ Adelaide, Adelaide, SA, Australia; [Jin, Lianwen] SCUT Zhuhai Inst Modern Ind Innovat, Guangzhou, Peoples R China; [Shen, Chunhua] Monash Univ, Melbourne, Vic, Australia	South China University of Technology; University of Adelaide; Monash University	Jin, LW (corresponding author), South China Univ Technol, Guangzhou, Peoples R China.; Jin, LW (corresponding author), SCUT Zhuhai Inst Modern Ind Innovat, Guangzhou, Peoples R China.	canjie.luo@gmail.com; qingxiang.lin@foxmail.com; liu.yuliang@mail.scut.edu.cn; eelwjin@scut.edu.cn; chunhua.shen@adelaide.edu.au		Jin, Lianwen/0000-0002-5456-0957	NSFC [61936003]; GD-NSF [2017A030312006]; National Key Research and Development Program of China [2016YFB1001405]; Fundamental Research Funds for Central Universities [x2dxD2190570]	NSFC(National Natural Science Foundation of China (NSFC)); GD-NSF; National Key Research and Development Program of China; Fundamental Research Funds for Central Universities(Fundamental Research Funds for the Central Universities)	This research was in part supported in part by NSFC (Grant No. 61936003), GD-NSF (No. 2017A030312006), the National Key Research and Development Program of China (No. 2016YFB1001405), and Fundamental Research Funds for Central Universities (x2dxD2190570).	Arjovsky M, 2017, PR MACH LEARN RES, V70; Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789; Bai F, 2018, PROC CVPR IEEE, P1508, DOI 10.1109/CVPR.2018.00163; Bau D, 2019, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2019.00460; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102; Casey RG, 1996, IEEE T PATTERN ANAL, V18, P690, DOI 10.1109/34.506792; Cheng MM, 2020, IEEE T IMAGE PROCESS, V29, P909, DOI 10.1109/TIP.2019.2936746; Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584; Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fang S., 2019, INT JOINT C ART INT; Fuze Cong, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P916, DOI 10.1109/ICDAR.2019.00151; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gordo A, 2015, PROC CVPR IEEE, P2956, DOI 10.1109/CVPR.2015.7298914; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He P, 2016, AAAI CONF ARTIF INTE, P3501; Hensel M, 2017, ADV NEUR IN, V30; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jaderberg M, 2014, ABS14062227 CORR; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34; Jing YH, 2021, IEEE T CYBERNETICS, V51, P568, DOI 10.1109/TCYB.2019.2904768; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942; Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Kingma D.P, P 3 INT C LEARNING R; Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245; Li H, 2019, AAAI CONF ARTIF INTE, P8610; Liao MH, 2019, AAAI CONF ARTIF INTE, P8714; Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu W, 2018, AAAI CONF ARTIF INTE, P7154; Liu WJ, 2016, 2016 IEEE/CSAA INTERNATIONAL CONFERENCE ON AIRCRAFT UTILITY SYSTEMS (AUS), P7, DOI 10.1109/AUS.2016.7748012; Liu Y, 2018, LECT NOTES COMPUT SC, V11209, P449, DOI 10.1007/978-3-030-01228-1_27; Liu ZC, 2018, AAAI CONF ARTIF INTE, P7194; Lucas SM, 2003, PROC INT CONF DOC, P682; Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127; Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097; Odena A, 2017, PR MACH LEARN RES, V70; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Paszke A., 2017, NEUR INF PROC SYST N; Risnumawan A, 2014, EXPERT SYST APPL, V41, P8027, DOI 10.1016/j.eswa.2014.07.008; Rodriguez-Serrano JA, 2015, INT J COMPUT VISION, V113, P193, DOI 10.1007/s11263-014-0793-6; Salimans T, 2016, ADV NEUR IN, V29; Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452; Su BL, 2015, LECT NOTES COMPUT SC, V9003, P35, DOI 10.1007/978-3-319-16865-4_3; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Phan TQ, 2013, IEEE I CONF COMP VIS, P569, DOI 10.1109/ICCV.2013.76; Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402; Wang T, 2012, INT C PATT RECOG, P3304; WEI MH, 2019, PROGNOST SYST HEALT; Wu L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1500, DOI 10.1145/3343031.3350929; Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474; Yang X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3280; Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515; Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765; Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216; Zhang YP, 2019, PROC CVPR IEEE, P2735, DOI 10.1109/CVPR.2019.00285; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu YY, 2016, FRONT COMPUT SCI-CHI, V10, P19, DOI 10.1007/s11704-015-4488-0	68	8	8	2	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					960	976		10.1007/s11263-020-01411-1	http://dx.doi.org/10.1007/s11263-020-01411-1		JAN 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK		Green Submitted			2022-12-18	WOS:000605122800002
J	Zhang, Z; Wang, CY; Qiu, WC; Qin, WH; Zeng, WJ				Zhang, Zhe; Wang, Chunyu; Qiu, Weichao; Qin, Wenhu; Zeng, Wenjun			AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human pose estimation; Multiple camera fusion; Epipolar geometry	TRACKING	Occlusion is probably the biggest challenge for human pose estimation in the wild. Typical solutions often rely on intrusive sensors such as IMUs to detect occluded joints. To make the task truly unconstrained, we present AdaFuse, an adaptive multiview fusion method, which can enhance the features in occluded views by leveraging those in visible views. The core of AdaFuse is to determine the point-point correspondence between two views which we solve effectively by exploring the sparsity of the heatmap representation. We also learn an adaptive fusion weight for each camera view to reflect its feature quality in order to reduce the chance that good features are undesirably corrupted by "bad" views. The fusion model is trained end-to-end with the pose estimation network, and can be directly applied to new camera configurations without additional adaptation. We extensively evaluate the approach on three public datasets including Human3.6M, Total Capture and CMU Panoptic. It outperforms the state-of-the-arts on all of them. We also create a large scale synthetic dataset Occlusion-Person, which allows us to perform numerical evaluation on the occluded joints, as it provides occlusion labels for every joint in the images. The dataset and code are released at .	[Zhang, Zhe; Qin, Wenhu] Southeast Univ, Nanjing, Peoples R China; [Wang, Chunyu; Zeng, Wenjun] Microsoft Res Asia, Beijing, Peoples R China; [Qiu, Weichao] Johns Hopkins Univ, Baltimore, MD USA	Southeast University - China; Microsoft; Microsoft Research Asia; Johns Hopkins University	Qin, WH (corresponding author), Southeast Univ, Nanjing, Peoples R China.	zhangzhecnjs@gmail.com; chnuwa@microsoft.com; qiuwc@gmail.com; qinwenhu@seu.edu.cn; wezeng@microsoft.com		Zhang, Zhe/0000-0003-3632-988X				Martinez AA, 2017, INT SYMP COMPUT EDUC; Amin S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.45; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; [Anonymous], 2016, NIPS WORKSH BAYES DE; Belagiannis V, 2014, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR.2014.216; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Bridgeman L, 2019, IEEE COMPUT SOC CONF, P2487, DOI [10.1109/CVERW.2019.00304, 10.1109/CVPRW.2019.00304]; Burenius M, 2013, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2013.464; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081; Ci H, 2022, IEEE T PATTERN ANAL, V44, P1429, DOI 10.1109/TPAMI.2020.3019139; Ci H, 2019, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2019.00235; Dong JT, 2019, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2019.00798; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gal Y., 2015, DEEP LEARNING WORKSH, V1, P2; Gal Y., 2016, THESIS, V1, P3; Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1; Gilbert A, 2019, INT J COMPUT VISION, V127, P381, DOI 10.1007/s11263-018-1118-y; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He YH, 2019, PROC CVPR IEEE, P2883, DOI 10.1109/CVPR.2019.00300; Hoffmann DT, 2019, LECT NOTES COMPUT SC, V11824, P609, DOI 10.1007/978-3-030-33676-9_43; Ilg E, 2018, LECT NOTES COMPUT SC, V11211, P677, DOI 10.1007/978-3-030-01234-2_40; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Joo H, 2019, IEEE T PATTERN ANAL, V41, P190, DOI 10.1109/TPAMI.2017.2782743; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225; Lakshminarayanan B, 2017, ADV NEUR IN, V30; Lassner Christoph, 2017, CVPR; Li TH, 2019, IEEE I CONF COMP VIS, P872, DOI 10.1109/ICCV.2019.00096; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu YB, 2011, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2011.5995424; Malleson C, 2017, INT CONF 3D VISION, P449, DOI 10.1109/3DV.2017.00058; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794; Peng X, 2018, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2018.00237; Perez P, 2004, P IEEE, V92, P495, DOI 10.1109/JPROC.2003.823147; Pleiss G, 2017, ADV NEUR IN, V30; Qiu HB, 2019, IEEE I CONF COMP VIS, P4341, DOI 10.1109/ICCV.2019.00444; Qiu WC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1221, DOI 10.1145/3123266.3129396; Rhodin H, 2018, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2018.00880; Roetenberg D., 2009, XSENS MVN FULL 6DOF; Rogez G, 2016, ADV NEUR IN, V29; Rongchang Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13683, DOI 10.1109/CVPR42600.2020.01370; Starner T, 2003, MACH VISION APPL, V14, P59, DOI 10.1007/s00138-002-0096-8; Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Tome D, 2018, INT CONF 3D VISION, P474, DOI 10.1109/3DV.2018.00061; Trumble M., 2017, 2017 BRIT MACH VIS C; Trumble M, 2018, LECT NOTES COMPUT SC, V11214, P800, DOI 10.1007/978-3-030-01249-6_48; Tu H., 2020, EUR C COMP VIS ECCV; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Xiang DL, 2019, PROC CVPR IEEE, P10957, DOI 10.1109/CVPR.2019.01122; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551; Zafar U, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0406-y; Zhang Z, 2020, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR42600.2020.00227; Zhao MM, 2019, IEEE I CONF COMP VIS, P10112, DOI 10.1109/ICCV.2019.01021; Zhao MM, 2018, PROC CVPR IEEE, P7356, DOI 10.1109/CVPR.2018.00768; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51	68	8	10	1	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2021	129	3					703	718		10.1007/s11263-020-01398-9	http://dx.doi.org/10.1007/s11263-020-01398-9		NOV 2020	16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT6QC		Green Submitted			2022-12-18	WOS:000589564400001
J	Zhou, HZ; Ummenhofer, B; Brox, T				Zhou, Huizhong; Ummenhofer, Benjamin; Brox, Thomas			DeepTAM: Deep Tracking and Mapping with Convolutional Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	15th European Conference on Computer Vision (ECCV)	SEP 08-14, 2018	Munich, GERMANY			Camera tracking; Multi view stereo; ConvNets		We present a system for dense keyframe-based camera tracking and depth map estimation that is entirely learned. For tracking, we estimate small pose increments between the current camera image and a synthetic viewpoint. This formulation significantly simplifies the learning problem and alleviates the dataset bias for camera motions. Further, we show that generating a large number of pose hypotheses leads to more accurate predictions. For mapping, we accumulate information in a cost volume centered at the current depth estimate. The mapping network then combines the cost volume and the keyframe image to update the depth prediction, thereby effectively making use of depth measurements and image-based priors. Our approach yields state-of-the-art results with few images and is robust with respect to noisy camera poses. We demonstrate that the performance of our 6 DOF tracking competes with RGB-D tracking algorithms.We compare favorably against strong classic and deep learning powered dense depth algorithms.	[Zhou, Huizhong; Brox, Thomas] Univ Freiburg, Freiburg, Germany; [Ummenhofer, Benjamin] Intel Labs, Munich, Germany	University of Freiburg; Intel Corporation	Zhou, HZ (corresponding author), Univ Freiburg, Freiburg, Germany.	zhouh@cs.uni-freiburg.de; benjamin.ummenhofer@intel.com; brox@cs.uni-freiburg.de		Zhou, Huizhong/0000-0003-4016-2250				Abadi M, 2015, P 12 USENIX S OPERAT; Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097; Dhiman V, 2016, PROC CVPR IEEE, P4331, DOI 10.1109/CVPR.2016.469; Eigen David, 2014, NEURIPS; Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577; Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54; Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; Garg R., 2018, IEEE C COMP VIS PATT; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Gupta S, 2015, INT J COMPUT VISION, V112, P133, DOI 10.1007/s11263-014-0777-6; Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79; Hirschmuller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56; Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kendall Alex, 2017, P IEEE C COMP VIS PA; Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650; Kerl C, 2013, IEEE INT CONF ROBOT, P3748, DOI 10.1109/ICRA.2013.6631104; Kingma D.P., 2015, INT C LEARNING REPRE; Klein G., 2007, IEEE ACM INT S MIX A; Li RH, 2018, IEEE INT CONF ROBOT, P7286; Loshchilov I., 2017, INT C LEARN REPR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Schmid C., 2017, ARXIV170407804CS; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Song SY, 2015, PROC CVPR IEEE, P3734, DOI 10.1109/CVPR.2015.7298997; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695; Ummenhofer B., 2017, IEEE C COMP VIS PATT; Valada A, 2018, IEEE INT CONF ROBOT, P6939; Wang S, 2017, INT CONF ACOUST SPEE, P436, DOI 10.1109/ICASSP.2017.7952193; Weerasekera Chamara Saroj, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2524, DOI 10.1109/ICRA.2017.7989293; Weerasekera CS, 2019, LECT NOTES COMPUT SC, V11365, P609, DOI 10.1007/978-3-030-20873-8_39; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Zhang H., 2018, CVPR; Zhou H., 2018, EUR C COMP VIS ECCV; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	41	8	9	2	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		756	769		10.1007/s11263-019-01221-0	http://dx.doi.org/10.1007/s11263-019-01221-0			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	KU1MV					2022-12-18	WOS:000519475600014
J	Zunino, A; Cavazza, J; Volpi, R; Morerio, P; Cavallo, A; Becchio, C; Murino, V				Zunino, Andrea; Cavazza, Jacopo; Volpi, Riccardo; Morerio, Pietro; Cavallo, Andrea; Becchio, Cristina; Murino, Vittorio			Predicting Intentions from Motion: The Subject-Adversarial Adaptation Approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition and prediction; Human intentions; Grasping; Kinematic analysis; Adversarial domain adaptation	ACTION RECOGNITION; DOMAIN ADAPTATION	This paper aims at investigating the action prediction problem from a pure kinematic perspective. Specifically, we address the problem of recognizing future actions, indeed human intentions, underlying a same initial (and apparently unrelated) motor act. This study is inspired by neuroscientific findings asserting that motor acts at the very onset are embedding information about the intention with which are performed, even when different intentions originate from a same class of movements. To demonstrate this claim in computational and empirical terms, we designed an ad hoc experiment and built a new 3D and 2D dataset where, in both training and testing, we analyze a same class of grasping movements underlying different intentions. We investigate how much the intention discriminants generalize across subjects, discovering that each subject tends to affect the prediction by his/her own bias. Inspired by the domain adaptation problem, we propose to interpret each subject as a domain, leading to a novel subject adversarial paradigm. The proposed approach favorably copes with our new problem, boosting the considered baseline features encoding 2D and 3D information and which do not exploit the subject information.	[Zunino, Andrea; Cavazza, Jacopo; Volpi, Riccardo; Morerio, Pietro; Murino, Vittorio] Ist Italiano Tecnol IIT, Pattern Anal & Comp Vis PAVIS, Genoa, Italy; [Cavallo, Andrea; Becchio, Cristina] Ist Italiano Tecnol IIT, CMON Cognit Mot & Neurosci, Genoa, Italy; [Murino, Vittorio] Univ Verona, Dept Comp Sci, Verona, Italy; [Cavallo, Andrea; Becchio, Cristina] Univ Torino, Dept Psychol, Turin, Italy	Istituto Italiano di Tecnologia - IIT; Istituto Italiano di Tecnologia - IIT; University of Verona; University of Turin	Zunino, A (corresponding author), Ist Italiano Tecnol IIT, Pattern Anal & Comp Vis PAVIS, Genoa, Italy.	andrea.zunino@iit.it		Cavallo, Andrea/0000-0002-3717-3760				[Anonymous], 2008, P IEEE C COMP VIS PA; Ansuini C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120432; Ansuini C, 2015, NEUROSCIENTIST, V21, P126, DOI 10.1177/1073858414533827; Bargal SA, 2018, P IEEE C COMP VIS PA; Becchio C, 2018, PHYS LIFE REV, V24, P67, DOI 10.1016/j.plrev.2017.10.002; Butepage J, 2017, P IEEE C COMP VIS PA; Cavallo A., 2016, SCI REPORTS; Cavazza J, 2017, P IEEE C COMP VIS PA; Cavazza J, 2016, P IEEE INT C PATT RE; Cavazza J, 2019, PATTERN RECOGN, V93, P25, DOI 10.1016/j.patcog.2019.03.031; Cavazza J, 2017, LECT NOTES COMPUT SC, V10484, P211, DOI 10.1007/978-3-319-68560-1_19; Chakraborty A, 2014, P AS C COMP VIS ACCV; Chopra S., 2013, ICML WORKSH CHALL RE; Csurka G, 2017, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-58347-1_1; Davis JW, 2006, IMAGE VISION COMPUT, V24, P455, DOI 10.1016/j.imavis.2006.01.012; Fermuller C, 2018, INT J COMPUT VISION, V126, P358, DOI 10.1007/s11263-017-0992-z; Fernando B., 2013, P IEEE INT C COMP VI; Ganin Y., 2015, P INT C MACH LEARN I; Ganin Y, 2016, J MACH LEARN RES, V17; Ghifary M, 2014, ABS14096041 CORR; Gong B., 2012, P IEEE C COMP VIS PA; Gopalan R, 2011, P IEEE INT C COMP VI; Hoai M., 2012, P IEEE C COMP VIS PA; Huang D, 2013, P IEEE INT C COMP VI; Huang DA, 2014, LECT NOTES COMPUT SC, V8695, P489, DOI 10.1007/978-3-319-10584-0_32; Idrees Haroon, 2016, P IEEE C COMP VIS PA; Jain A., 2016, P IEEE C COMP VIS PA; Jegou H., 2010, P IEEE C COMP VIS PA; Kilner JM, 2011, TRENDS COGN SCI, V15, P352, DOI 10.1016/j.tics.2011.06.005; Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15; Kolahi SS, 2013, IEEE SYMP COMP COMMU; Kong Y, 2016, IEEE T PATTERN ANAL, V38, P1844, DOI 10.1109/TPAMI.2015.2491928; Kong Yu, 2017, P IEEE C COMP VIS PA; Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335; Koul A, 2019, COGNITION, V182, P213, DOI 10.1016/j.cognition.2018.10.006; Koul A, 2018, CEREB CORTEX, V28, P2647, DOI 10.1093/cercor/bhy098; Kuehne H., 2011, P IEEE INT C COMP VI; Lan T., 2014, EUR C COMP VIS; Li Kang, 2014, IEEE Trans Pattern Anal Mach Intell, V36, P1644, DOI 10.1109/TPAMI.2013.2297321; Li K, 2012, LECT NOTES COMPUT SC, V7572, P286, DOI 10.1007/978-3-642-33718-5_21; Li W, 2010, P IEEE C COMP VIS PA; Liu MY, 2016, ADV NEUR IN, V29; Lu Chaochao, 2017, P IEEE C COMP VIS PA; Ma S, 2016, P IEEE C COMPUTER VI; Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002; Morerio P, 2018, P INT C LEARN REPR I; Muller M., 2007, TECHNICAL REPORT; Oztop E, 2005, COGNITIVE BRAIN RES, V22, P129, DOI 10.1016/j.cogbrainres.2004.08.004; Rohrbach Marcus, 2012, P IEEE C COMP VIS PA; Ryoo M. S., 2011, P IEEE INT C COMP VI; Ryoo MS, 2015, P IEEE INT C HUM ROB; Shekhar S., 2013, P IEEE C COMP VIS PA; Soomro K., 2012, COMPUT SCI; Soran B, 2015, P IEEE INT C COMP VI; Soriano M, 2018, P NATL ACAD SCI USA, V115, P10452, DOI 10.1073/pnas.1809825115; Stapel J. C., 2012, PSYCHOL RES; Stein S., 2013, P ACM INT JOINT C PE; Sun B, 2016, P AAAI C ART INT AAA; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Taigman Y., 2017, PROC INT C LEARN REP; Tzeng E., 2015, P IEEE INT C COMP VI; Tzeng E., 2017, P IEEE C COMP VIS PA; Tzeng Eric, 2014, ABS14123474 CORR; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Volpi Riccardo, 2018, P IEEE C COMP VIS PA; Vondrick C, 2016, P IEEE C COMP VIS PA; Walker J, 2014, P IEEE C COMP VIS PA; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Xie C, 2018, P INT JOINT C ART IN; Xu Zhen, 2015, P IEEE INT C COMP VI; Zunino A, 2017, P ACM C MULT; Zunino A, 2018, P IEEE INT C PATT RE; Zunino A, 2017, P IEEE C COMP VIS PA; Zunino A, 2017, LECT NOTES COMPUT SC, V10484, P469, DOI 10.1007/978-3-319-68560-1_42	75	8	8	1	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2020	128	1					220	239		10.1007/s11263-019-01234-9	http://dx.doi.org/10.1007/s11263-019-01234-9			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KI6WA		Green Published, hybrid			2022-12-18	WOS:000511490100009
J	Li, W; Zhu, XT; Gong, SG				Li, Wei; Zhu, Xiatian; Gong, Shaogang			Scalable Person Re-Identification by Harmonious Attention	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Person re-identification; Scalable search; Compact model; Attention learning; Local and global representation learning	GLOBAL FEATURES	Existing person re-identification (re-id) deep learning methods rely heavily on the utilisation of large and computationally expensive convolutional neural networks. They are therefore not scalable to large scale re-id deployment scenarios with the need of processing a large amount of surveillance video data, due to the lengthy inference process with high computing costs. In this work, we address this limitation via jointly learning re-id attention selection. Specifically, we formulate a novel harmonious attention network (HAN) framework to jointly learn soft pixel attention and hard region attention alongside simultaneous deep feature representation learning, particularly enabling more discriminative re-id matching by efficient networks with more scalable model inference and feature matching. Extensive evaluations validate the cost-effectiveness superiority of the proposed HAN approach for person re-id against a wide variety of state-of-the-art methods on four large benchmark datasets: CUHK03, Market-1501, DukeMTMC, and MSMT17.	[Li, Wei; Gong, Shaogang] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England; [Zhu, Xiatian] Vis Semant Ltd, London E1 4NS, England	University of London; Queen Mary University London	Zhu, XT (corresponding author), Vis Semant Ltd, London E1 4NS, England.	w.li@qmul.ac.uk; eddy.zhuxt@gmail.com; s.gong@qmul.ac.uk		Zhu, Xiatian/0000-0002-9284-2955	China Scholarship Council; Royal Society Newton Advanced Fellowship Programme [NA150459]; Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety [98111-571149]; Vision Semantics Ltd	China Scholarship Council(China Scholarship Council); Royal Society Newton Advanced Fellowship Programme; Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety(UK Research & Innovation (UKRI)Innovate UK); Vision Semantics Ltd	This work was partially supported by the China Scholarship Council, Vision Semantics Ltd, Royal Society Newton Advanced Fellowship Programme (NA150459), and Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety (98111-571149).	Abadi M., 2017, TENSORFLOW SYSTEM LA; Ahmed E., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299016; Bolukbasi T., 2017, INT C MACH LEARN; Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225; Chen Dongdong, 2018, P IEEE C COMP VIS PA; Chen WH, 2017, AAAI CONF ARTIF INTE, P3988; Chen Y., 2018, BRIT MACH VIS C; Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304; Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Dong XW, 2018, IEEE CONF COMPUT; Edelman S, 1998, BEHAV BRAIN SCI, V21, P449, DOI 10.1017/S0140525X98001253; Evgeniou A., 2007, ADV NEURAL INF PROCE, V19, P41, DOI DOI 10.2139/SSRN.1031158; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Faraone J, 2018, PROC CVPR IEEE, P4300, DOI 10.1109/CVPR.2018.00452; Figurnov M, 2017, PROC CVPR IEEE, P1790, DOI 10.1109/CVPR.2017.194; Guo YQ, 2017, IEEE INT C INTELL TR; HASSIBI B, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P293, DOI 10.1109/ICNN.1993.298572; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hermans Alexander, 2017, ARXIV170307737; Hinton G., 2015, ARXIV150302531; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G., 2017, P IEEE C COMPUTER VI, P4700, DOI DOI 10.1109/CVPR.2017.243; Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291; Huang Gao, 2018, ICLR; Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535; Huang ZH, 2018, LECT NOTES COMPUT SC, V11220, P317, DOI 10.1007/978-3-030-01270-0_19; Hubara I, 2016, ADV NEUR IN, V29; Iandola F.N., 2016, ARXIV; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117; Kodirov E., 2015, BMVC, V3, P8; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lan X., 2018, P AS C COMP VIS, P284; Lan X, 2018, ADV NEUR IN, V31; Li D, 2017, INT SYM COMPUT INTEL, P338, DOI 10.1109/ISCID.2017.51; Li MJ, 2018, LECT NOTES COMPUT SC, V11213, P186, DOI 10.1007/978-3-030-01240-3_12; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762; Liu JY, 2018, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2018.00341; Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152; Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34; Mittal A., 2016, ADV NEURAL INFORM PR, P2675; NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3; Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146; Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Saquib S. M., 2018, IEEE C COMP VIS PATT; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30; Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092; Sifre Laurent, 2014, THESIS; Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766; Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48; Vossel S, 2014, NEUROSCIENTIST, V20, P150, DOI 10.1177/1073858413494269; Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang F, 2016, IEEE CONF COMPUT; Wang H., 2014, PROC IEEE INT C CONS, DOI 10.5244/C.28.48; Wang HX, 2018, INT J COMPUT VISION, V126, P1288, DOI 10.1007/s11263-018-1105-3; Wang HX, 2016, IEEE IMAGE PROC, P769, DOI 10.1109/ICIP.2016.7532461; Wang Jingya, 2018, IEEE C COMP VIS PATT; WANG XP, 2015, IEEE IJCNN; Wang Y, 2018, PROC CVPR IEEE, P8042, DOI 10.1109/CVPR.2018.00839; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226; Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12; ZHANG H, 2017, IEEE INT SYMP PHYS; Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang T, 2017, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2017.469; Zhang W, 2018, IEEE CONF COMPUT; Zhang Xiangyu, 2018, IEEE C COMP VIS PATT; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783; Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11; Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541; Zhong Zhun, 2017, PROC CVPR IEEE, P1318, DOI DOI 10.1109/CVPR.2017.389; Zhu FQ, 2017, IEEE T IMAGE PROCESS, V26, P4806, DOI 10.1109/TIP.2017.2695101; Zhu X, 2017, ARXIV PREPRINT ARXIV, P1; Zhu XT, 2018, IEEE T IMAGE PROCESS, V27, P2286, DOI 10.1109/TIP.2017.2740564; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	111	8	8	3	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1635	1653		10.1007/s11263-019-01274-1	http://dx.doi.org/10.1007/s11263-019-01274-1		DEC 2019	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN		Green Published, hybrid			2022-12-18	WOS:000504120400001
J	Innamorati, C; Ritschel, T; Weyrich, T; Mitra, NJ				Innamorati, Carlo; Ritschel, Tobias; Weyrich, Tim; Mitra, Niloy J.			Learning on the Edge: Investigating Boundary Filters in CNNs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Convolutional neural networks; Boundary rules; Boundary conditions		Convolutional neural networks (CNNs) handle the case where filters extend beyond the image boundary using several heuristics, such as zero, repeat or mean padding. These schemes are applied in an ad-hoc fashion and, being weakly related to the image content and oblivious of the target task, result in low output quality at the boundary. In this paper, we propose a simple and effective improvement that learns the boundary handling itself. At training-time, the network is provided with a separate set of explicit boundary filters. At testing-time, we use these filters which have learned to extrapolate features at the boundary in an optimal way for the specific task. Our extensive evaluation, over a wide range of architectural changes (variations of layers, feature channels, or both), shows how the explicit filters result in improved boundary handling. Furthermore, we investigate the efficacy of variations of such boundary filters with respect to convergence speed and accuracy. Finally, we demonstrate an improvement of 5-20% across the board of typical CNN applications (colorization, de-Bayering, optical flow, disparity estimation, and super-resolution). Supplementary material and code can be downloaded from the project page: http://geometry.cs.ucl.ac.uk/projects/2019/investigating-edge/.	[Innamorati, Carlo; Ritschel, Tobias; Weyrich, Tim; Mitra, Niloy J.] UCL, London, England	University of London; University College London	Innamorati, C (corresponding author), UCL, London, England.	c.innamorati@cs.ucl.ac.uk		Innamorati, Carlo/0000-0002-8431-4335; Weyrich, Tim/0000-0002-4322-8844	European Union's Horizon 2020 research and innovation programme under the Marie Skodowska-Curie Grant [642841]; ERC Starting Grant SmartGeometry [StG-2013-335373]; UK Engineering and Physical Sciences Research Council [EP/K023578/1]; EPSRC [EP/K023578/1] Funding Source: UKRI	European Union's Horizon 2020 research and innovation programme under the Marie Skodowska-Curie Grant; ERC Starting Grant SmartGeometry; UK Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank our reviewers for their helpful comments. We also thank Paul Guerrero, Aron Monszpart and Tuanfeng Yang Wang for their technical help in setting up and fixing the machines used to carry out the experiments in this work. This work was partially funded by the European Union's Horizon 2020 research and innovation programme under the Marie Skodowska-Curie Grant Agreement No 642841, by the ERC Starting Grant SmartGeometry (StG-2013-335373), and by the UK Engineering and Physical Sciences Research Council (Grant EP/K023578/1).	Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Cohen TS, 2016, PR MACH LEARN RES, V48; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dosovitskiy Alexey, 2015, ICCV; FUKUSHIMA K., 1982, COMPETITION COOPERAT, P267; Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399; Goodfellow I., 2016, DEEP LEARNING; Innamorati C., 2018, P BRIT MACH VIS C BM; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; O'Malley T, 2019, KERAS TUNER; Ren JS., 2015, ADV NEURAL INF PROCE, V1, P901; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Uhrig J., 2017, ARXIV170806500; Worrall D. E., 2017, CVPR; Zhang R., 2016, ECCV	17	8	8	1	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		773	782		10.1007/s11263-019-01223-y	http://dx.doi.org/10.1007/s11263-019-01223-y		OCT 2019	10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		hybrid, Green Published			2022-12-18	WOS:000491405100001
J	Gao, Y; Yuille, AL				Gao, Yuan; Yuille, Alan L.			Estimation of 3D Category-Specific Object Structure: Symmetry, Manhattan and/or Multiple Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Symmetry; Manhattan; Single image; Symmetric rigid structure from motion; Symmetric non-rigid structure from motion	STRUCTURE-FROM-MOTION; SHAPE	Many man-made objects have intrinsic symmetries and often Manhattan structure. By assuming an orthographic or a weak perspective projection model, this paper addresses the estimation of 3D structures and camera projection using symmetry and/or Manhattan structure cues, for the two cases when the input is a single image or multiple images from the same category, e.g. multiple different cars from various viewpoints. More specifically, analysis on the single image case shows that Manhattan alone is sufficient to recover the camera projection and then the 3D structure can be reconstructed uniquely by exploiting symmetry. But Manhattan structure can be hard to observe from a single image due to occlusion. Hence, we extend to the multiple-image case which can also exploit symmetry but does not require Manhattan structure. We propose novel structure from motion methods for both rigid and non-rigid object deformations, which exploit symmetry and use multiple images from the same object category as input. We perform experiments on the Pascal3D+ dataset with either human labeled 2D keypoints or with 2D keypoints localized from a convolutional neural network. The results show that our methods which exploit symmetry significantly outperform the baseline methods.	[Gao, Yuan] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan, Hubei, Peoples R China; [Gao, Yuan] Tencent AI Lab, Shenzhen, Peoples R China; [Yuille, Alan L.] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA; [Yuille, Alan L.] Johns Hopkins Univ, Dept Cognit Sci, Baltimore, MD 21218 USA; [Yuille, Alan L.] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA USA	Huazhong University of Science & Technology; Tencent; Johns Hopkins University; Johns Hopkins University; University of California System; University of California Los Angeles	Gao, Y (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan, Hubei, Peoples R China.; Gao, Y (corresponding author), Tencent AI Lab, Shenzhen, Peoples R China.	ethan.y.gao@gmail.com; alan.l.yuille@gmail.com		Yuille, Alan L./0000-0001-5207-9249	ARO [62250-CS]; ONR [N00014-15-1-2356]; NSF [CCF-1317376]	ARO; ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	We would like to thank Ehsan Jahangiri, Cihang Xie, Weichao Qiu, Xuan Dong, Siyuan Qiao for giving feedbacks on the manuscript. This work was partially supported by ARO 62250-CS, ONR N00014-15-1-2356, and the NSF award CCF-1317376.	Agudo A, 2014, PROC CVPR IEEE, P1558, DOI 10.1109/CVPR.2014.202; Akhter I., 2009, CVPR; Akhter I., 2008, NIPS; Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201; Bishop C.M, 2006, PATTERN RECOGN; Bourdev L., 2010, ECCV; Bregler C., 2000, CVPR; Ceylan D, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2517348; Chen L, 2014, GIGASCIENCE, V3, DOI 10.1186/2047-217X-3-24; Chen X., 2014, P 27 ANN C NEURAL IN, P1736, DOI DOI 10.1109/CVPR.2018.00742; Coughlan JM, 2003, NEURAL COMPUT, V15, P1063, DOI 10.1162/089976603765202668; Coughlan JM, 1999, ICCV; Dai Y., 2012, CVPR; Dai YC, 2014, INT J COMPUT VISION, V107, P101, DOI 10.1007/s11263-013-0684-2; Furukawa Y., 2009, CVPR; Gao Y., 2017, IEEE INT C COMP VIS; Gao Y., 2016, ECCV; Gao Yuan, 2019, CVPR; Gordon G. G., 1990, P SPIE; Gotardo PFU, 2011, IEEE T PATTERN ANAL, V33, P2051, DOI 10.1109/TPAMI.2011.50; Grossmann E, 2005, COMPUT VIS IMAGE UND, V99, P151, DOI 10.1016/j.cviu.2005.01.002; Grossmann E., 2002, BMVC; Grossmann E., 2002, ACCV; Hamsici OC, 2012, LECT NOTES COMPUT SC, V7575, P260, DOI 10.1007/978-3-642-33765-9_19; Hartley R., 2004, ROBOTICA; Hong J. Hyeong, 2015, ICCV; Hong W, 2004, INT J COMPUT VISION, V60, P241, DOI 10.1023/B:VISI.0000036837.76476.10; Kar A., 2015, CVPR; Kontsevich L. L., 1987, Optoelectronics, Instrumentation and Data Processing, P76; KONTSEVICH LL, 1993, J OPT SOC AM A, V10, P1129, DOI 10.1364/JOSAA.10.001129; Ma JY, 2015, PATTERN RECOGN, V48, P772, DOI 10.1016/j.patcog.2014.09.005; Ma JY, 2013, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2013.279; Ma JY, 2013, PATTERN RECOGN, V46, P3519, DOI 10.1016/j.patcog.2013.05.017; Marques M, 2009, COMPUT VIS IMAGE UND, V113, P261, DOI 10.1016/j.cviu.2008.09.004; Morris D. D., 2001, CVPR; MUKHERJEE DP, 1995, PHILOS T R SOC A, V351, P77, DOI 10.1098/rsta.1995.0026; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Olsen SI, 2008, J MATH IMAGING VIS, V31, P233, DOI 10.1007/s10851-007-0060-3; Pavlakos G, 2017, LAW PRACT REASON, P113; Rosen J., 2011, SYMMETRY DISCOVERED; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Tang HB, 2007, MOL PAIN, V3, DOI 10.1186/1744-8069-3-42; Thrun S., 2005, ICCV; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L., 2003, NIPS; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; VETTER T, 1994, SPATIAL VISION, V8, P443, DOI 10.1163/156856894X00107; Vicente S., 2014, CVPR; Xiao J., 2004, ECCV	49	8	8	2	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1501	1526		10.1007/s11263-019-01195-z	http://dx.doi.org/10.1007/s11263-019-01195-z			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL					2022-12-18	WOS:000485320300007
J	Lee, JK; Yoon, KJ				Lee, Jeong-Kyun; Yoon, Kuk-Jin			Joint Estimation of Camera Orientation and Vanishing Points from an Image Sequence in a Non-Manhattan World	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Camera orientation estimation; Vanishing point estimation; Kalman filter; Rotation estimation	LINE; DESCRIPTOR; SLAM	A widely used approach for estimating camera orientation is to use the points at infinity, i.e., the vanishing points (VPs). Enforcement of the orthogonal constraint between the VPs, known as the Manhattan world constraint, enables an estimation of the drift-free camera orientation to be achieved. However, in practical applications, this approach is neither effective (because of noisy parallel line segments) nor performable in non-Manhattan world scenes. To overcome these limitations, we propose a novel method that jointly estimates the VPs and camera orientation based on sequential Bayesian filtering. The proposed method does not require the Manhattan world assumption, and can perform a highly accurate estimation of camera orientation. In order to enhance the robustness of the joint estimation, we propose a keyframe-based feature management technique that removes false positives from parallel line clusters and detects new parallel line sets using geometric properties such as the orthogonality and rotational dependence for a VP, a line, and the camera rotation. In addition, we propose a 3-line camera rotation estimation method that does not require the Manhattan world assumption. The 3-line method is applied to the RANSAC-based outlier rejection technique to eliminate outlier measurements; therefore, the proposed method achieves accurate and robust estimation of the camera orientation and VPs in general scenes with non-orthogonal parallel lines. We demonstrate the superiority of the proposed method by conducting an extensive evaluation using synthetic and real datasets and by comparison with other state-of-the-art methods.	[Lee, Jeong-Kyun] Gwangju Inst Sci & Technol, Gwangju, South Korea; [Yoon, Kuk-Jin] Korea Adv Inst Sci & Technol, Dept Mech Engn, Daejeon, South Korea	Gwangju Institute of Science & Technology (GIST); Korea Advanced Institute of Science & Technology (KAIST)	Yoon, KJ (corresponding author), Korea Adv Inst Sci & Technol, Dept Mech Engn, Daejeon, South Korea.	leejk@gist.ac.kr; kjyoon@kaist.ac.kr	Lee, Jeong-Kyun/AAI-4327-2020; Yoon, Kuk-Jin/F-4329-2018	Lee, Jeong-Kyun/0000-0002-6256-7008; yun, gugjin/0000-0002-1634-2756	Samsung Research Funding Center of Samsung Electronics [SRFCTC1603-05]; Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT [NRF-2017M3C4A7069369]	Samsung Research Funding Center of Samsung Electronics(Samsung); Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of Korea)	This work was supported by Samsung Research Funding Center of Samsung Electronics under Project Number SRFCTC1603-05 andNext-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT (NRF-2017M3C4A7069369).	Antone M. E., 2000, P IEEE C COMP VIS PA; Bazin J.-C., 2012, IEEE RSJ INT C INT R; Bazin JC, 2012, PROC CVPR IEEE, P638, DOI 10.1109/CVPR.2012.6247731; Bazin JC, 2012, INT J ROBOT RES, V31, P63, DOI 10.1177/0278364911421954; Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033; Cipolla R., 1999, P BRIT MACH VIS C BM; Civera J., 2009, IEEE RSJ INT C INT R; Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961; Elloumi W, 2017, J REAL-TIME IMAGE PR, V13, P669, DOI 10.1007/s11554-014-0419-9; Fan B, 2012, PATTERN RECOGN, V45, P794, DOI 10.1016/j.patcog.2011.08.004; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Geiger A., 2012, P IEEE COMP SOC C CO; Gomez-Balderas JE, 2012, J INTELL ROBOT SYST, V65, P361, DOI 10.1007/s10846-011-9579-z; Ho KL, 2006, ROBOT AUTON SYST, V54, P740, DOI 10.1016/j.robot.2006.04.016; Kosecka Jana, 2002, EUR C COMP VIS ECCV; Kroeger T, 2015, PROC CVPR IEEE, P2449, DOI 10.1109/CVPR.2015.7298859; Kurz D., 2013, IEEE ACM INT S MIX A; Lee J. K., 2015, P IEEE C COMP VIS PA; Lezama J., 2014, P IEEE C COMP VIS PA; Lu X., 2017, IEEE WINT C APPL COM; Martin P, 2014, IEEE IMAGE PROC, P3352, DOI 10.1109/ICIP.2014.7025678; Mirzaei FM, 2011, IEEE I CONF COMP VIS, P2454, DOI 10.1109/ICCV.2011.6126530; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Neubert P, 2008, IEEE INT C EMERG, P353, DOI 10.1109/ETFA.2008.4638418; Pflugfelder R., 2005, DIGITAL IMAGE COMPUT; Rondon E, 2010, IEEE INT C INT ROBOT; Rother C, 2002, IMAGE VISION COMPUT, V20, P647, DOI 10.1016/S0262-8856(02)00054-9; Schindler G., 2004, P IEEE COMP SOC C CO; SCHMID C, 1997, PROC CVPR IEEE, P666, DOI DOI 10.1109/CVPR.1997.609397; Simon D., 2006, OPTIMAL STATE ESTIMA; Sinha S.N., 2012, TRENDS TOPICS COMPUT, V6554, P267, DOI DOI 10.1007/978-3-642-35740-4; Sinha SN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409112; Sundararajan K., 2011, THESIS; Tardif J. P, 2009, P IEEE INT C COMP VI; Tretyak E, 2012, INT J COMPUT VISION, V97, P305, DOI 10.1007/s11263-011-0488-1; von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300; Wang ZH, 2009, PATTERN RECOGN, V42, P941, DOI 10.1016/j.patcog.2008.08.035; Wenzel F, 2007, COMM COM INF SC, V4, P330; Williams B, 2009, ROBOT AUTON SYST, V57, P1188, DOI 10.1016/j.robot.2009.06.010; Xu Y., 2013, P IEEE C COMP VIS PA; Zhang L., 2008, P IEEE C COMP VIS PA; Zhang LL, 2016, INT J COMPUT VISION, V117, P111, DOI 10.1007/s11263-015-0854-5; Zhang LL, 2013, J VIS COMMUN IMAGE R, V24, P794, DOI 10.1016/j.jvcir.2013.05.006	43	8	8	4	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2019	127	10					1426	1442		10.1007/s11263-019-01196-y	http://dx.doi.org/10.1007/s11263-019-01196-y			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IW9NL					2022-12-18	WOS:000485320300003
J	Shiri, F; Yu, X; Porikli, F; Hartley, R; Koniusz, P				Shiri, Fatemeh; Yu, Xin; Porikli, Fatih; Hartley, Richard; Koniusz, Piotr			Identity-Preserving Face Recovery from Stylized Portraits	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face synthesis; Image stylization; Face recovery; Destylization; Generative models		Given an artistic portrait, recovering the latent photorealistic face that preserves the subject's identity is challenging because the facial details are often distorted or fully lost in artistic portraits. We develop an Identity-preserving Face Recovery from Portraits method that utilizes a Style Removal network (SRN) and a Discriminative Network (DN). Our SRN, composed of an autoencoder with residual block-embedded skip connections, is designed to transfer feature maps of stylized images to the feature maps of the corresponding photorealistic faces. Owing to the Spatial Transformer Network, SRN automatically compensates for misalignments of stylized portraits to output aligned realistic face images. To ensure the identity preservation, we promote the recovered and ground truth faces to share similar visual features via a distance measure which compares features of recovered and ground truth faces extracted from a pre-trained FaceNet network. DN has multiple convolutional and fully-connected layers, and its role is to enforce recovered faces to be similar to authentic faces. Thus, we can recover high-quality photorealistic faces from unaligned portraits while preserving the identity of the face in an image. By conducting extensive evaluations on a large-scale synthesized dataset and a hand-drawn sketch dataset, we demonstrate that our method achieves superior face recovery and attains state-of-the-art results. In addition, our method can recover photorealistic faces from unseen stylized portraits, artistic paintings, and hand-drawn sketches.	[Shiri, Fatemeh; Yu, Xin; Porikli, Fatih; Hartley, Richard; Koniusz, Piotr] Australian Natl Univ, Canberra, ACT, Australia; [Hartley, Richard; Koniusz, Piotr] Data61 CSIRO, Canberra, ACT, Australia	Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO)	Koniusz, P (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.; Koniusz, P (corresponding author), Data61 CSIRO, Canberra, ACT, Australia.	fatemeh.shiri@anu.edu.au; xin.yu@anu.edu.au; fatih.porikli@anu.edu.au; richard.hartley@anu.edu.au; piotr.koniusz@anu.edu.au		Yu, Xin/0000-0002-0269-5649	Australian Research Council (ARC) [DP150104645]	Australian Research Council (ARC)(Australian Research Council)	This work is supported by the Australian Research Council (ARC) Grant DP150104645.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, ARCHIBALD PRIZE ART; Chen D., 2017, ARXIV170309210; Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623; Denton Emily L, 2015, NEURIPS, V2, P4; Dumoulin Vincent, 2016, ARXIV161007629; Gatys L.A., 2016, TECHNICAL REPORT; Gatys L. A., 2016, ARXIV161107865; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta A, 2017, IEEE I CONF COMP VIS, P4087, DOI 10.1109/ICCV.2017.438; Hinton G, NEURAL NETWORKS MA A; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jayasumana S., 2013, CVPR; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karacan L., 2016, ARXIV161200215; Kazemi H, 2018, 2018 INTERNATIONAL CONFERENCE OF THE BIOMETRICS SPECIAL INTEREST GROUP (BIOSIG), DOI 10.1109/WACVW.2018.00006; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koniusz P, 2018, LECT NOTES COMPUT SC, V11220, P815, DOI 10.1007/978-3-030-01270-0_48; Koniusz P, 2018, PROC CVPR IEEE, P5774, DOI 10.1109/CVPR.2018.00605; Koniusz P, 2017, IEEE T PATTERN ANAL, V39, P313, DOI 10.1109/TPAMI.2016.2545667; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Nejati H., 2011, WACV; Parkhi Omkar M., 2015, BRIT MACH VIS C; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sangkloy P., 2016, ARXIV161200835; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968; Sharma A., 2011, CVPR; Shiri F., 2019, WACV; Shiri F., 2017, INT C DIG IM COMP TE, DOI [10.1109/DICTA.2017.8227432, DOI 10.1109/DICTA.2017.8227432]; Shiri F, 2018, IEEE WINT CONF APPL, P102, DOI 10.1109/WACV.2018.00018; Ulyanov D., 2016, ARXIV160708022; Ulyanov D., 2017, ARXIV170102096; Ulyanov D, 2016, PR MACH LEARN RES, V48; van den Oord A, 2016, PR MACH LEARN RES, V48; Wang NN, 2018, PATTERN RECOGN LETT, V107, P59, DOI 10.1016/j.patrec.2017.06.012; Wang X., 2016, ARXIV161201895; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; WILMOT P, 2017, ARXIV170108893, P3; Yin R., 2016, ARXIV, P1; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Yu XT, 2017, AAAI CONF ARTIF INTE, P2859; Yu X, 2017, PROC CVPR IEEE, P5367, DOI 10.1109/CVPR.2017.570; Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20; Zhang H., 2017, P EUR C COMP VIS ECC; Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407; ZHANG L, 1986, TIP, V20, P2378, DOI DOI 10.1109/TIP.2011.2109730; Zhang W., 2011, CVPR; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhu Jun-Yan, 2017, ICCV	64	8	8	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		863	883		10.1007/s11263-019-01169-1	http://dx.doi.org/10.1007/s11263-019-01169-1			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD		Green Submitted			2022-12-18	WOS:000468525900018
J	Wang, MJ; Shu, ZX; Cheng, SY; Panagakis, Y; Samaras, D; Zafeiriou, S				Wang, Mengjiao; Shu, Zhixin; Cheng, Shiyang; Panagakis, Yannis; Samaras, Dimitris; Zafeiriou, Stefanos			An Adversarial Neuro-Tensorial Approach for Learning Disentangled Representations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Adversarial autoencoder; Disentangled representation; Tensor decomposition	DATABASE	Several factors contribute to the appearance of an object in a visual scene, including pose, illumination, and deformation, among others. Each factor accounts for a source of variability in the data, while the multiplicative interactions of these factors emulate the entangled variability, giving rise to the rich structure of visual object appearance. Disentangling such unobserved factors from visual data is a challenging task, especially when the data have been captured in uncontrolled recording conditions (also referred to as in-the-wild) and label information is not available. In this paper, we propose a pseudo-supervised deep learning method for disentangling multiple latent factors of variation in face images captured in-the-wild. To this end, we propose a deep latent variable model, where the multiplicative interactions of multiple latent factors of variation are explicitly modelled by means of multilinear (tensor) structure. We demonstrate that the proposed approach indeed learns disentangled representations of facial expressions and pose, which can be used in various applications, including face editing, as well as 3D face reconstruction and classification of facial expression, identity and pose.	[Wang, Mengjiao; Cheng, Shiyang; Panagakis, Yannis; Zafeiriou, Stefanos] Imperial Coll London, London, England; [Shu, Zhixin; Samaras, Dimitris] SUNY Stony Brook, Stony Brook, NY 11794 USA; [Panagakis, Yannis] Middlesex Univ, London, England	Imperial College London; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Middlesex University	Wang, MJ (corresponding author), Imperial Coll London, London, England.	m.wang15@imperial.ac.uk	Wang, Mengjiao/AAC-9034-2019; Panagakis, Yannis/AAZ-8090-2020	Wang, Mengjiao/0000-0002-4873-5677; Panagakis, Ioannis/0000-0003-0153-5210	EPSRC DTA from Imperial College London; Partner University Fund; SUNY2020 Infrastructure Transportation Security Center; Google Faculty Award; EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans [EP/S010203/1]; EPSRC [EP/S010203/1] Funding Source: UKRI	EPSRC DTA from Imperial College London(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Partner University Fund; SUNY2020 Infrastructure Transportation Security Center; Google Faculty Award(Google Incorporated); EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Mengjiao Wang was supported by an EPSRC DTA from Imperial College London. This work was partially funded by a gift from Adobe, NSF grants CNS-1718014 and DMS 1737876, the Partner University Fund, and the SUNY2020 Infrastructure Transportation Security Center awarded to Zhixin Shu, as well as by a Google Faculty Award and EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans (EP/S010203/1) awarded to Dr. Zafeiriou. We thank Amazon Web Services for providing computational resources.	Alex M, 2002, LECT NOTES COMPUT SC, V2350, P447; [Anonymous], 2017, CVPR; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Berthelot D., 2017, BEGAN BOUNDARY EQUIL; Bolkart T, 2016, PROC CVPR IEEE, P4911, DOI 10.1109/CVPR.2016.531; Booth J., 2017, ARXIV170105360; Cheung B., 2014, P INT C LEARN REPR W; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Deng J., 2018, CORR; Desjardins G., 2012, ARXIV12105474; Fabrigar L. R., 2011, EXPLORATORY FACTOR A; Feng Y., 2018, EUR C COMP VIS ECCV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Jackson A. S., 2017, INT C COMP VIS; Kemelmacher-Shlizerman I, 2013, IEEE I CONF COMP VIS, P3256, DOI 10.1109/ICCV.2013.404; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kossaifi J., 2016, TENSORLY TENSOR LEAR; Kulkarni TD, 2015, ADV NEUR IN, V28; Liu Ziwei, 2015, P INT C COMP VIS ICC; Makhzani A., 2015, ICLR WORKSH, DOI DOI 10.3389/FPHAR.2020.565644; Mathieu M, 2016, ADV NEUR IN, V29; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; NEUDECKER H, 1979, JOURNAL OF THE AMERI, V64, P953, DOI DOI 10.1080/01621459.1969.10501027; Reed S, 2014, PR MACH LEARN RES, V32, P1431; Roemer F, 2012, THESIS; Rupprecht T, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1772, DOI 10.1145/2976749.2989041; Sagonas C., 2017, P IEEE INT C COMP VI; Shu Z., 2017, P IEEE C COMP VIS PA; Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111; Snape P, 2015, PROC CVPR IEEE, P91, DOI 10.1109/CVPR.2015.7298604; Tang Yichuan, 2013, INT C MACH LEARN, P163; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Tewari Ayush, 2017, IEEE INT C COMP VIS; Tewari Ayush, 2018, IEEE C COMP VIS PATT; Tran L, 2018, NONLINEAR 3D FACE MO; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang CY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2901; Wang M., 2017, P IEEE C COMP VIS PA, P4592; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Worrall DE, 2017, IEEE I CONF COMP VIS, P5737, DOI 10.1109/ICCV.2017.611; Wu X., 2015, ARXIV151102683; Yang F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964955; Zafeiriou S, 2013, IEEE T INF FOREN SEC, V8, P121, DOI 10.1109/TIFS.2012.2224109	48	8	8	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		743	762		10.1007/s11263-019-01163-7	http://dx.doi.org/10.1007/s11263-019-01163-7			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD		Green Submitted, hybrid, Green Published			2022-12-18	WOS:000468525900012
J	Wang, YJ; Luo, BN; Shen, J; Pantic, M				Wang, Yujiang; Luo, Bingnan; Shen, Jie; Pantic, Maja			Face Mask Extraction in Video Sequence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Face mask extraction; Semantic face segmentation; Fully convolutional networks; Convolutional LSTM; Segmentation loss		Inspired by the recent development of deep network-based methods in semantic image segmentation, we introduce an end-to-end trainable model for face mask extraction in video sequence. Comparing to landmark-based sparse face shape representation, our method can produce the segmentation masks of individual facial components, which can better reflect their detailed shape variations. By integrating convolutional LSTM (ConvLSTM) algorithm with fully convolutional networks (FCN), our new ConvLSTM-FCN model works on a per-sequence basis and takes advantage of the temporal correlation in video clips. In addition, we also propose a novel loss function, called segmentation loss, to directly optimise the intersection over union (IoU) performances. In practice, to further increase segmentation accuracy, one primary model and two additional models were trained to focus on the face, eyes, and mouth regions, respectively. Our experiment shows the proposed method has achieved a 16.99% relative improvement (from 54.50 to 63.76% mean IoU) over the baseline FCN model on the 300 Videos in the Wild (300VW) dataset.	[Wang, Yujiang; Luo, Bingnan; Shen, Jie; Pantic, Maja] Imperial Coll London, Dept Comp, 180 Queens Gate, London, England	Imperial College London	Wang, YJ (corresponding author), Imperial Coll London, Dept Comp, 180 Queens Gate, London, England.	yujiang.wang14@imperial.ac.uk; bingnan.luo16@imperial.ac.uk; jie.shen07@imperial.ac.uk; m.pantic@imperial.ac.uk		Luo, Bingnan/0000-0002-9367-4359; Wang, Yujiang/0000-0002-6220-029X	EPSRC [EP/N007743/1 (FACER2VM)]; European Community [688835]; China Scholarship Council [201708060212]; EPSRC [EP/N007743/1, EP/H016988/1, EP/J017787/1] Funding Source: UKRI	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Community(European Commission); China Scholarship Council(China Scholarship Council); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work is funded by the EPSRC project EP/N007743/1 (FACER2VM) and the European Community Horizon 2020 under grant agreement no. 688835 (DE-ENIGMA). The work of Yujiang Wang has also been partially supported by China Scholarship Council (No. 201708060212).	Abadi M, 2015, P 12 USENIX S OPERAT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chollet F., 2015, KERAS; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Chung Joon Son, 2016, P AS C COMP VIS, P251, DOI [DOI 10.1007/978-3-319-54427-4_19, 10.1007/978-3-319-54427-4{_}19, DOI 10.1007/978-3-319-54427-4{_}19]; Drayer B., 2016, ARXIV160803066; García Herrera Arístides Lázaro, 2017, Rev.Med.Electrón., P1; Graves A, 2013, ARXIV13080850; Graves A, 2007, LECT NOTES COMPUT SC, V4668, P549; Guclu U, 2017, ARXIV170303305; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2012, COURSERA LECT E, V6e; HuilingWang Tapani Raiko, 2016, ASIA C COMPUTER VISI, P163; Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43; Kae A, 2013, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2013.263; Kawulok M., 2016, ADV FACE DETECTION F, P189, DOI DOI 10.1007/978-3-319-25958-1_8; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; King DE, 2009, J MACH LEARN RES, V10, P1755; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kundu A, 2016, PROC CVPR IEEE, P3168, DOI 10.1109/CVPR.2016.345; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Lee D., 2016, P AS C COMP VIS, P290; Lee K. C., 2008, AUT FAC GEST REC 200, P1; Liu BY, 2015, PROC CVPR IEEE, P4286, DOI 10.1109/CVPR.2015.7299057; Liu X, 2014, PROC CVPR IEEE, P57, DOI 10.1109/CVPR.2014.15; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Ma B, 2015, IEEE I CONF COMP VIS, P4400, DOI 10.1109/ICCV.2015.500; Nagaraja NS, 2015, IEEE I CONF COMP VIS, P3235, DOI 10.1109/ICCV.2015.370; Paszke A., 2016, ARXIV PREPRINT ARXIV; Petridis S., 2017, ARXIV170904343; Rahman Md Atiqur, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P234, DOI 10.1007/978-3-319-50835-1_22; Roy A, 2016, LECT NOTES COMPUT SC, V9908, P186, DOI 10.1007/978-3-319-46493-0_12; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Saleh FS, 2017, IEEE I CONF COMP VIS, P2125, DOI 10.1109/ICCV.2017.232; Scheffler C, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.53; Shelhamer E, 2016, LECT NOTES COMPUT SC, V9915, P852, DOI 10.1007/978-3-319-49409-8_69; Smith BM, 2013, PROC CVPR IEEE, P3484, DOI 10.1109/CVPR.2013.447; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Tran D, 2016, IEEE COMPUT SOC CONF, P402, DOI 10.1109/CVPRW.2016.57; Tripathi S, 2015, INT SOC DESIGN CONF, P157, DOI 10.1109/ISOCC.2015.7401766; Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423; Visin F., 2015, ARXIV150500393; Wang Y., 2017, ARXIV170900443; Warrell J, 2009, IEEE IMAGE PROC, P2481, DOI 10.1109/ICIP.2009.5413918; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Yacoob Y, 2006, IEEE T PATTERN ANAL, V28, P1164, DOI 10.1109/TPAMI.2006.139; Yu F., 2016, P ICLR 2016; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474; Zhang H, 2014, I C VIRTUAL REALITY, P321, DOI 10.1109/ICVRV.2014.65; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58; Zhou Ning., 2016, 2016 IEEE WINT C APP, P1, DOI DOI 10.1109/WACV.2016.7477595; Zhou Shuchang, 2015, ARXIV151209194; Zhou YS, 2015, LECT NOTES COMPUT SC, V9377, P222, DOI 10.1007/978-3-319-25393-0_25; Zimmermann M., 2016, P AS C COMP VIS, P264	65	8	9	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2019	127	6-7			SI		625	641		10.1007/s11263-018-1130-2	http://dx.doi.org/10.1007/s11263-018-1130-2			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HZ0JD		Green Submitted, hybrid			2022-12-18	WOS:000468525900006
J	Hofmann, M; Seeland, M; Mader, P				Hofmann, Martin; Seeland, Marco; Maeder, Patrick			Efficiently Annotating Object Images with Absolute Size Information Using Mobile Devices	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Size annotation; Size measurement; In-situ size annotation; minimum focus distance; Absolute size; Mobile device	DEPTH; REPRESENTATION; LOCALIZATION; FEATURES; SCALE	The projection of a real world scenery to a planar image sensor inherits the loss of information about the 3D structure as well as the absolute dimensions of the scene. For image analysis and object classification tasks, however, absolute size information can make results more accurate. Today, the creation of size annotated image datasets is effort intensive and typically requires measurement equipment not available to public image contributors. In this paper, we propose an effective annotation method that utilizes the camera within smart mobile devices to capture the missing size information along with the image. The approach builds on the fact that with a camera, calibrated to a specific object distance, lengths can be measured in the object's plane. We use the camera's minimum focus distance as calibration distance and propose an adaptive feature matching process for precise computation of the scale change between two images facilitating measurements on larger object distances. Eventually, the measured object is segmented and its size information is annotated for later analysis. A user study showed that humans are able to retrieve the calibration distance with a low variance. The proposed approach facilitates a measurement accuracy comparable to manual measurement with a ruler and outperforms state-of-the-art methods in terms of accuracy and repeatability. Consequently, the proposed method allows in-situ size annotation of objects in images without the need for additional equipment or an artificial reference object in the scene.	[Hofmann, Martin; Seeland, Marco; Maeder, Patrick] Tech Univ Ilmenau, Ilmenau, Germany	Technische Universitat Ilmenau	Hofmann, M (corresponding author), Tech Univ Ilmenau, Ilmenau, Germany.	martin.hofmann@tu-ilmenau.de; marco.seeland@tu-ilmenau.de; patrick.maeder@tu-ilmenau.de	Mäder, Patrick/A-1848-2018; Seeland, Marco/H-1028-2011	Mäder, Patrick/0000-0001-6871-2707; Seeland, Marco/0000-0001-7204-3972; Hofmann, Martin/0000-0002-4440-3317	Friedrich Naumann Stiftung; German Ministry of Education and Research (BMBF) [01LC1319A, 01LC1319B]; German Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB) [3514 685C19]; Stiftung Naturschutz Thuringen (SNT) [SNT-082-248-03/2014]	Friedrich Naumann Stiftung; German Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF)); German Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB); Stiftung Naturschutz Thuringen (SNT)	We would like to thank all participants of our user experiment for supporting our work. We are funded through a scholarship of the Friedrich Naumann Stiftung; the German Ministry of Education and Research (BMBF) Grants: 01LC1319A and 01LC1319B; the German Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB) Grant: 3514 685C19; and the Stiftung Naturschutz Thuringen (SNT) Grant: SNT-082-248-03/2014.	Aanaes H., 2011, INT J COMPUT VISION, P1; Aanaes H., 2010, GROUND TRUTH DATA SE; Agarwal S., 2009, INT C COMP VIS ICCV; [Anonymous], 2017, ORB SLAM2 IOS; Apple Inc, 2017, ARK; Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bradski G, 2000, DR DOBBS J, V25, P120; Bursuc A, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P595, DOI 10.1145/2671188.2749379; Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754; Criminisi A, 1999, IMAGE VISION COMPUT, V17, P625, DOI 10.1016/S0262-8856(98)00183-8; Criminisi A, 2000, INT J COMPUT VISION, V40, P123, DOI 10.1023/A:1026598000963; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Karlsson N, 2005, IEEE INT CONF ROBOT, P24; Ke Y, 2004, PROC CVPR IEEE, P506; Kim H, 2016, INT CONF 3D VISION, P370, DOI 10.1109/3DV.2016.46; Klein George, 2007, P1; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; Kuhl A, 2006, MONOCULAR 3D SCENE R, P607; Lai K, 2011, IEEE INT CONF ROBOT, P1817; Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Li J, 2008, NEUROCOMPUTING, V71; Lin JY, 2013, IEEE T IMAGE PROCESS, V22, P4545, DOI 10.1109/TIP.2013.2274389; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luhmann T., 2006, CLOSE RANGE PHOTOGRA; McGuinness K, 2010, PATTERN RECOGN, V43, P434, DOI 10.1016/j.patcog.2009.03.008; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Moeller M, 2015, IEEE T IMAGE PROCESS, V24, P5369, DOI 10.1109/TIP.2015.2479469; Moreels P, 2007, INT J COMPUT VISION, V73, P263, DOI 10.1007/s11263-006-9967-1; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Mustafah Y. M., 2012, 2012 International Conference on Computer and Communication Engineering (ICCCE), P659, DOI 10.1109/ICCCE.2012.6271270; NAYAR SK, 1994, IEEE T PATTERN ANAL, V16, P824, DOI 10.1109/34.308479; Nitzan D., 1985, IEEE Journal of Robotics and Automation, VRA-1, P3, DOI 10.1109/JRA.1985.1086994; Peng B, 2013, PATTERN RECOGN, V46, P1020, DOI 10.1016/j.patcog.2012.09.015; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; Piasco N, 2018, PATTERN RECOGN, V74, P90, DOI 10.1016/j.patcog.2017.09.013; Robertson P., P IEEE INT C IND POS, P1; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Schonberger J. L., 2017, C COMP VIS PATT REC; Seeland M, 2017, PLOS ONE, V12; SMITH RC, 1986, INT J ROBOT RES, V5, P56, DOI 10.1177/027836498600500404; SUBBARAO M, 1994, INT J COMPUT VISION, V13, P271, DOI 10.1007/BF02028349; Thrun S. a. o., 2002, EXPLORING ARTIFICIAL, V1, P1; Torralba A, 2004, PROC CVPR IEEE, P762; Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017; Uhrig J, 2016, LECT NOTES COMPUT SC, V9796, P14, DOI 10.1007/978-3-319-45886-1_2; Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993; Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z; Watanabe M, 1998, INT J COMPUT VISION, V27, P203, DOI 10.1023/A:1007905828438; Williams B, 2009, ROBOT AUTON SYST, V57, P1188, DOI 10.1016/j.robot.2009.06.010; Wittich HC, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2201-7	60	8	8	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2019	127	2					207	224		10.1007/s11263-018-1093-3	http://dx.doi.org/10.1007/s11263-018-1093-3			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HL5LJ					2022-12-18	WOS:000458768000005
J	Yang, YZ; Feng, JS; Jojic, N; Yang, JC; Huang, TS				Yang, Yingzhen; Feng, Jiashi; Jojic, Nebojsa; Yang, Jianchao; Huang, Thomas S.			Subspace Learning by l(0)-Induced Sparsity	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						l(0)-Induced sparse subspace clustering (l(0)-SSC); Subspace-sparse representation; Proximal gradient descent; Sparse similarity matrix	REPRESENTATION; MINIMIZATION; ALGORITHM	Subspace clustering methods partition the data that lie in or close to a union of subspaces in accordance with the subspace structure. Such methods with sparsity prior, such as sparse subspace clustering (SSC) (Elhamifar and Vidal in IEEE Trans Pattern Anal Mach Intel) 35(11):2765-2781, 2013) with the sparsity induced by the a l(1)-norm, are demonstrated to be effective in subspace clustering. Most of those methods require certain assumptions, e.g. independence or disjointness, on the subspaces. However, these assumptions are not guaranteed to hold in practice and they limit the application of existing sparse subspace clustering methods. In this paper, we propose l(0)-induced sparse subspace clustering (l(0)-SSC). In contrast to the required assumptions, such as independence or disjointness, on subspaces for most existing sparse subspace clustering methods, we prove that l(0)-SSC guarantees the subspace-sparse representation, a key element in subspace clustering, for arbitrary distinct underlying subspaces almost surely under the mild i.i.d. assumption on the data generation. We also present the "no free lunch" theorem which shows that obtaining the subspace representation under our general assumptions can not be much computationally cheaper than solving the corresponding l(0) sparse representation problem of l(0)-SSC. A novel approximate algorithm named Approximate l(0)-SSC (Al-0-SSC) is developed which employs proximal gradient descent to obtain a sub-optimal solution to the optimization problem of l(0)-SSC with theoretical guarantee. The sub-optimal solution is used to build a sparse similarity matrix upon which spectral clustering is performed for the final clustering results. Extensive experimental results on various data sets demonstrate the superiority of Al-0-SSC compared to other competing clustering methods. Furthermore, we extend l(0)-SSC to semi-supervised learning by performing label propagation on the sparse similarity matrix learnt by Al-0-SSC and demonstrate the effectiveness of the resultant semi-supervised learning method termed l(0)-sparse subspace label propagation (l(0)-SSLP).	[Yang, Yingzhen; Yang, Jianchao] Snap Res, Venice, CA 90291 USA; [Yang, Yingzhen; Huang, Thomas S.] Univ Illinois, Beckman Inst, Urbana, IL 61801 USA; [Feng, Jiashi] Natl Univ Singapore, Dept ECE, Singapore, Singapore; [Jojic, Nebojsa] Microsoft Res, Redmond, WA USA	University of Illinois System; University of Illinois Urbana-Champaign; National University of Singapore; Microsoft	Yang, YZ (corresponding author), Snap Res, Venice, CA 90291 USA.; Yang, YZ (corresponding author), Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.	superyyzg@gmail.com; elefjia@nus.edu.sg; jojic@microsoft.com; jianchao.yang@snap.com; t-huang1@illinois.edu	Feng, Jiashi/AGX-6209-2022; Yang, Yingzhen/AAU-6048-2020	Yang, Yingzhen/0000-0003-0502-6122	IBM gift grant; National University of Singapore [R-263-000-C08-133]; Ministry of Education of Singapore AcRF Tier One Grant [R-263-000-C21-112]	IBM gift grant(International Business Machines (IBM)); National University of Singapore(National University of Singapore); Ministry of Education of Singapore AcRF Tier One Grant(Ministry of Education, Singapore)	This material is based upon work supported by the findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. The work of Yingzhen Yang was supported in part by an IBM gift grant to Beckman Institute, UIUC. The work of Jiashi Feng was partially supported by National University of Singapore startup Grant R-263-000-C08-133 and Ministry of Education of Singapore AcRF Tier One Grant R-263-000-C21-112.	Asuncion A, 2007, UCI MACHINE LEARNING; Bhatia K., 2015, ADV NEURAL INF PROCE, P721; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764; Cheng H, 2013, SIGNAL PROCESS, V93, P1408, DOI 10.1016/j.sigpro.2012.09.011; Dyer EL, 2013, J MACH LEARN RES, V14, P2487; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Elhamifar Ehsan, 2011, ADV NEURAL INF PROCE, V24, P3; Hyder M, 2009, INT CONF ACOUST SPEE, P3365, DOI 10.1109/ICASSP.2009.4960346; Jenatton R., 2010, P 27 INT C MACH LEAR, P487; Karasuyama M., 2013, ADV NEURAL INFORM PR, P1547; Li J., 2017, P 31 AAAI C ART INT, P2189; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Mairal J., 2008, ADV NEURAL INFORM PR, P1033; Mairal J, 2010, J MACH LEARN RES, V11, P19; Mancera L, 2006, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2006.312819; Ng AY, 2002, ADV NEUR IN, V14, P849; Park D., 2014, ADV NEURAL INFORM PR, P2753; Peng X, 2015, AAAI CONF ARTIF INTE, P3827; Plummer D., 1986, MATCHING THEORY; Soltanolkotabi M, 2014, ANN STAT, V42, P669, DOI 10.1214/13-AOS1199; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; Wang Y. X., 2016, ARXIV161200991; [王应德 Wang Yingde], 2013, [高分子通报, Polymer Bulletin], P89; Wang Z., 2015, ADV NEURAL INFORM PR, P2521; Yan S., 2009, P SIAM INT C DAT MIN, P792, DOI DOI 10.1137/1.9781611972795.68; Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757; Yang YZ, 2016, LECT NOTES COMPUT SC, V9906, P731, DOI 10.1007/978-3-319-46475-6_45; Yang YZ, 2014, AAAI CONF ARTIF INTE, P3148; You C, 2016, PROC CVPR IEEE, P3918, DOI 10.1109/CVPR.2016.425; Yu XD, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.79; Zhang CH, 2012, STAT SCI, V27, P576, DOI 10.1214/12-STS399; Zhang TZ, 2013, IEEE I CONF COMP VIS, P281, DOI 10.1109/ICCV.2013.42; Zheng M, 2011, IEEE T IMAGE PROCESS, V20, P1327, DOI 10.1109/TIP.2010.2090535; Zheng X., 2004, P 12 ANN ACM INT C M, P885, DOI [DOI 10.1145/1027527.1027731, 10.1145/1027527.1027731]; Zhou D., 2003, ADV NEURAL INFORM PR; Zhu Xiaojin., 2003, P ICLR, P912	43	8	8	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2018	126	10			SI		1138	1156		10.1007/s11263-018-1092-4	http://dx.doi.org/10.1007/s11263-018-1092-4			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GR8XD					2022-12-18	WOS:000443018400005
J	Gaidon, A; Lopez, A; Perronnin, F				Gaidon, Adrien; Lopez, Antonio; Perronnin, Florent			The Reasonable Effectiveness of Synthetic Visual Data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Gaidon, Adrien] Toyota Res Inst, Ann Arbor, MI USA; [Lopez, Antonio] UAB, Comp Vis Ctr, Barcelona, Spain; [Perronnin, Florent] Naver Labs Europe, Meylan, France	Toyota Motor Corporation; Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	Perronnin, F (corresponding author), Naver Labs Europe, Meylan, France.	adrien.gaidon@tri.global; antonio@cvc.uab.es; florent.perronnin@naverlabs.com	López, Antonio M/L-5303-2014	López, Antonio M/0000-0002-6979-5783				Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chen C., 2015, P INT C COMP VIS; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; de Souza C, 2017, P C COMP VIS PATT RE; Dosovitskiy A., 2017, P C ROB LEARN; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gaidon A., 2016, P C COMP VIS PATT RE; Geiger A., 2012, P IEEE COMP SOC C CO; Handa A., 2016, P C COMP VIS PATT RE; Hestness J, 2017, ARXIV171200409; Johnson-Roberson Matthew, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P746, DOI 10.1109/ICRA.2017.7989092; Koltun V., 2017, P INT C COMP VIS; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li X., 2017, P IEEE INT C INT TRA; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Mayer N., 2016, P C COMP VIS PATT RE; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Savva Manolis, 2017, ARXIV171203931; Shah S., 2017, P FIELD SERV ROB; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97	21	8	8	2	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2018	126	9			SI		899	901		10.1007/s11263-018-1108-0	http://dx.doi.org/10.1007/s11263-018-1108-0			3	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GQ3HQ		Bronze			2022-12-18	WOS:000441553300001
J	Zhang, JM; Ma, SG; Sameki, M; Sclaroff, S; Betke, M; Lin, Z; Shen, XH; Price, B; Mech, R				Zhang, Jianming; Ma, Shugao; Sameki, Mehrnoosh; Sclaroff, Stan; Betke, Margrit; Lin, Zhe; Shen, Xiaohui; Price, Brian; Mech, Radomir			Salient Object Subitizing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Salient object; Subitizing; Deep learning; Convolutional neural network	IMAGE	We study the problem of salient object subitizing, i.e. predicting the existence and the number of salient objects in an image using holistic cues. This task is inspired by the ability of people to quickly and accurately identify the number of items within the subitizing range (1-4). To this end, we present a salient object subitizing image dataset of about 14K everyday images which are annotated using an online crowdsourcing marketplace. We show that using an end-to-end trained convolutional neural network (CNN) model, we achieve prediction accuracy comparable to human performance in identifying images with zero or one salient object. For images with multiple salient objects, our model also provides significantly better than chance performance without requiring any localization process. Moreover, we propose a method to improve the training of the CNN subitizing model by leveraging synthetic images. In experiments, wedemonstrate the accuracy and generalizability of our CNN subitizing model and its applications in salient object detection and image retrieval.	[Ma, Shugao; Sameki, Mehrnoosh; Sclaroff, Stan; Betke, Margrit] Boston Univ, Comp Sci Dept, Boston, MA 02215 USA; [Zhang, Jianming; Lin, Zhe; Shen, Xiaohui; Price, Brian; Mech, Radomir] Adobe Res, San Jose, CA 95110 USA	Boston University; Adobe Systems Inc.	Zhang, JM (corresponding author), Adobe Res, San Jose, CA 95110 USA.	jianmzha@adobe.com; shugaoma@bu.edu; sameki@bu.edu; sclaroff@bu.edu; betke@bu.edu; zlin@adobe.com; xshen@adobe.com; bprice@adobe.com; rmech@adobe.com	; Zhang, Jianming/B-1665-2017	Sclaroff, Stanley/0000-0002-0711-4313; Zhang, Jianming/0000-0002-9954-6294	US NSF [0910908, 1029430]; Direct For Computer & Info Scie & Enginr [1337866] Funding Source: National Science Foundation	US NSF(National Science Foundation (NSF)); Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This research was supported in part by US NSF Grants 0910908 and 1029430, and gifts from Adobe and NVIDIA.	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anoraganingrum D., 1999, INT C IM AN PROC; Arteta C., 2014, EUR C COMP VIS ECCV; ATKINSON J, 1976, PERCEPTION, V5, P335, DOI 10.1068/p050335; Berg T. L., 2009, IEEE C COMP VIS PATT; Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30; Boysen S.T., 2014, DEV NUMERICAL COMPET; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Choi J, 2014, IEEE SIGNAL PROC LET, V21, P957, DOI 10.1109/LSP.2014.2321751; Chua T.-S., 2009, PROCEEDINGS OF THE A; Clements D., 1999, TEACHING CHILDREN MA, V5, P400; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; DAVIS H, 1988, BEHAV BRAIN SCI, V11, P561, DOI 10.1017/S0140525X00053437; Dehaene S., 2011, NUMBER SENSE MIND CR; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Feng J., 2011, IEEE INT C COMP VIS; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gopalakrishnan V., 2009, IEEE C COMP VIS PATT; Gross Hans J, 2012, Commun Integr Biol, V5, P1; Gross HJ, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0004263; Gurari D., 2016, ARXIV160808188; Heo Jae-Pil, 2014, IEEE C COMP VIS PATT; Jaderberg M, 2014, ABS14062227 CORR; Jansen BRJ, 2014, BRIT J DEV PSYCHOL, V32, P178, DOI 10.1111/bjdp.12032; Jevons W.S., 1871, NATURE, V3, P281, DOI [10.1038/003281a0, DOI 10.1038/003281A0]; Jia Y., 2014, P 22 ACM INT C MULT, P675; KAUFMAN EL, 1949, AM J PSYCHOL, V62, P498, DOI 10.2307/1418556; Kazemzadeh Sahar, 2014, P 2014 C EMP METH NA, P787, DOI DOI 10.3115/V1/D14-1086; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820; Lempitsky V., 2010, NIPS, V23, P1324; Li XR, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2906152; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; MANDLER G, 1982, J EXP PSYCHOL GEN, V111, P1, DOI 10.1037/0096-3445.111.1.1; Nath S. K., 2006, MED IMAGE COMPUTING; Pahl M, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00162; Peng Xi., 2015, IEEE INT C COMP VIS; Piazza M, 2004, COGNITIVE NEUROSCIENCES III, THIRD EDITION, P865; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481406; Razavian Ali Sharif, 2014, P IEEE C COMP VIS PA, P806, DOI DOI 10.1109/CVPRW.2014.131; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Scharfenberger C., 2013, IEEE INT C COMP ROB; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758; Shin D., 2016, CONTENT COMPLEXITY S; Siva P, 2013, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2013.416; Stark M., 2010, P BRIT MACH VIS C, P1, DOI [10.5244/C.24.106, DOI 10.5244/C.24.106]; Stoianov I, 2012, NAT NEUROSCI, V15, P194, DOI 10.1038/nn.2996; Subburaman V. B., 2012, IEEE INT C ADV VID S; Sun B., 2014, BMVC, V1, P3; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273; TRICK LM, 1994, PSYCHOL REV, V101, P80, DOI 10.1037/0033-295X.101.1.80; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Vuilleumier PO, 2000, BRAIN, V123, P1263, DOI 10.1093/brain/123.6.1263; Wang P., 2012, IEEE C COMP VIS PATT; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xiong B., 2014, EUR C COMP VIS ECCV; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165; Zhang JM, 2015, PROC CVPR IEEE, P4045, DOI 10.1109/CVPR.2015.7299031; Zhang J, 2016, PROC CVPR IEEE, P3428, DOI 10.1109/CVPR.2016.373; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731; Zou W. Y., 2013, ANN C COGN SCI SOC C	71	8	8	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	2					169	186		10.1007/s11263-017-1011-0	http://dx.doi.org/10.1007/s11263-017-1011-0			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	FC3PK		Green Submitted			2022-12-18	WOS:000406751100004
J	Jeon, HG; Lee, JY; Han, Y; Kim, SJ; Kweon, IS				Jeon, Hae-Gon; Lee, Joon-Young; Han, Yudeog; Kim, Seon Joo; Kweon, In So			Generating Fluttering Patterns with Low Autocorrelation for Coded Exposure Imaging	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Coded exposure; Fluttering pattern; Motion deblurring; Computational photography	BINARY SEQUENCES; MERIT FACTOR; LEGENDRE SEQUENCES; ALGORITHMS	The performance of coded exposure imaging critically depends on finding good binary sequences. Previous coded exposure imaging methods have mostly relied on random searching to find the binary codes, but that approach can easily fail to find good long sequences, due to the exponentially expanding search space. In this paper, we present two algorithms for generating the binary sequences, which are especially well suited for generating short and long binary sequences, respectively. We show that the concept of low autocorrelation binary sequences, which has been successfully exploited in the field of information theory, can be applied to generate shutter fluttering patterns. We also propose a new measure for good binary sequences. Based on the new measure, we introduce two new algorithms for coded exposure imaging - a modified Legendre sequence method and a memetic algorithm. Experiments using both synthetic and real data show that our new algorithms consistently generate better binary sequences for the coded exposure problem, yielding better deblurring and resolution enhancement results compared to previous methods of generating the binary codes.	[Jeon, Hae-Gon; Lee, Joon-Young; Han, Yudeog; Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea; [Lee, Joon-Young] Adobe Res, San Jose, CA USA; [Han, Yudeog] Agcy Def Dev, Daejeon, South Korea; [Kim, Seon Joo] Yonsei Univ, Seoul, South Korea	Korea Advanced Institute of Science & Technology (KAIST); Adobe Systems Inc.; Agency of Defense Development (ADD), Republic of Korea; Yonsei University	Kweon, IS (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	hgjeon@rcv.kaist.ac.kr; jolee@adobe.com; ydhan@add.re.kr; seonjookim@yonsei.ac.kr; iskweon77@kaist.ac.kr	Jeon, Hae-Gon/W-5908-2019; Kweon, In So/C-2023-2011	Jeon, Hae-Gon/0000-0003-1105-1666; 	National Research Foundation of Korea (NRF) - Korea government (MSIP) [2010-0028680, 2016-4014610]; Global PH.D Fellowship Program through the National Research Foundation of Korea (NRF) - Ministry of Education [NRF-20151034617]	National Research Foundation of Korea (NRF) - Korea government (MSIP); Global PH.D Fellowship Program through the National Research Foundation of Korea (NRF) - Ministry of Education	This work was supported by the National Research Foundation of Korea (NRF) Grant funded by the Korea government (MSIP) (Nos. 2010-0028680 and 2016-4014610). Hae-Gon Jeon was partially supported by Global PH.D Fellowship Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF-20151034617).	Agrawal A., 2009, P IEEE C COMP VIS PA; Agrawal A., 2007, P IEEE C COMP VIS PA; Asif Salman, 2015, FLATCAM THIN BARE SE; Baden JM, 2011, IEEE T INFORM THEORY, V57, P8084, DOI 10.1109/TIT.2011.2164778; Borwein P, 2004, IEEE T INFORM THEORY, V50, P3234, DOI 10.1109/TIT.2004.838341; Borwein Peter, 2007, ACM Communications in Computer Algebra, V41, P118, DOI 10.1145/1358183.1358185; Boufounos P., 2007, P IEEE INT C AC SPEE; Chen XS, 2011, IEEE T EVOLUT COMPUT, V15, P591, DOI 10.1109/TEVC.2011.2132725; Cossairt O, 2013, IEEE T IMAGE PROCESS, V22, P447, DOI 10.1109/TIP.2012.2216538; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Franzen Rich, 1999, KODAK LOSSLESS TRUE; Gallardo JE, 2009, APPL SOFT COMPUT, V9, P1252, DOI 10.1016/j.asoc.2009.03.005; GOLAY MJE, 1983, IEEE T INFORM THEORY, V29, P934, DOI 10.1109/TIT.1983.1056744; GOLAY MJE, 1977, IEEE T INFORM THEORY, V23, P43, DOI 10.1109/TIT.1977.1055653; Gorthi SS, 2013, OPT EXPRESS, V21, P5164, DOI 10.1364/OE.21.005164; HOHOLDT T, 1988, IEEE T INFORM THEORY, V34, P161, DOI 10.1109/18.2620; Jedwab J., 2005, P SEQ THEIR APPL; JENSEN JM, 1991, IEEE T INFORM THEORY, V37, P617, DOI 10.1109/18.79917; Jeon H.-G., 2015, P INT C COMP VIS ICC; Jeon H.-G., 2013, P INT C COMP VIS ICC; KRISHNAN D, 2009, ADV NEURAL INFORM PR; LEMPEL A, 1977, IEEE T INFORM THEORY, V23, P38, DOI 10.1109/TIT.1977.1055672; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815; LUCY LB, 1974, ASTRON J, V79, P745, DOI 10.1086/111605; Ma CG, 2015, OPT LETT, V40, P2281, DOI 10.1364/OL.40.002281; Martin D., 2001, P INT C COMP VIS ICC; McCloskey S., 2010, P EUR C COMP VIS ECC; McCloskey S, 2012, IEEE T PATTERN ANAL, V34, P2071, DOI 10.1109/TPAMI.2012.108; Mertens S, 1996, J PHYS A-MATH GEN, V29, pL473, DOI 10.1088/0305-4470/29/18/005; Michalewicz Z., 1996, GENETIC ALGORITHMS D, V3rd; Militzer B., 1998, IEEE Transactions on Evolutionary Computation, V2, P34, DOI 10.1109/4235.728212; Nagahara H., 2010, P EUR C COMP VIS ECC; No JS, 1996, IEEE T INFORM THEORY, V42, P2254, DOI 10.1109/18.556617; Raskar R, 2006, ACM T GRAPHIC, V25, P795, DOI 10.1145/1141911.1141957; RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055; Schechner YY, 2007, IEEE T PATTERN ANAL, V29, P1339, DOI 10.1109/TPAMI.2007.1151; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; Tai Y.-W., 2010, P IEEE C COMP VIS PA; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wiener N., 1964, EXTRAPOLATION INTERP; Xiong TY, 2011, IEEE T INFORM THEORY, V57, P493, DOI 10.1109/TIT.2010.2090271; Zhou CY, 2011, INT J COMPUT VISION, V93, P53, DOI 10.1007/s11263-010-0409-8; Zuo C, 2016, OPT LASER ENG, V80, P24, DOI 10.1016/j.optlaseng.2015.12.012	44	8	11	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2017	123	2					269	286		10.1007/s11263-016-0976-4	http://dx.doi.org/10.1007/s11263-016-0976-4			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EU2OM					2022-12-18	WOS:000400868800007
J	Li, J; Tian, YH; Chen, XW; Huang, TJ				Li, Jia; Tian, Yonghong; Chen, Xiaowu; Huang, Tiejun			Measuring Visual Surprise Jointly from Intrinsic and Extrinsic Contexts for Image Saliency Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image saliency; Visual surprise; Intrinsic context; Extrinsic context; Gaussian mixture model; Markov chain	ATTENTION; PREDICT; MODEL	Detecting conspicuous image content is a challenging task in the field of computer vision. In existing studies, most approaches focus on estimating saliency only with the cues from the input image. However, such "intrinsic" cues are often insufficient to distinguish targets and distractors that may share some common visual attributes. To address this problem, we present an approach to estimate image saliency by measuring the joint visual surprise from intrinsic and extrinsic contexts. In this approach, a hierarchical context model is first built on a database of 31.2 million images, where a Gaussian mixture model (GMM) is trained for each leaf node to encode the prior knowledge on "what is where" in a specific scene. For a testing image that shares similar spatial layout within a scene, the pre-trained GMM can serve as an extrinsic context model to measure the "surprise" of an image patch. Since human attention may quickly shift between different surprising locations, we adopt a Markov chain to model a surprise-driven attention-shifting process so as to infer the salient patches that can best capture human attention. Experiments show that our approach outperforms 19 state-of-the-art methods in fixation prediction.	[Li, Jia; Chen, Xiaowu] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing, Peoples R China; [Li, Jia] Beihang Univ, Int Res Inst Multidisciplinary Sci, Beijing, Peoples R China; [Tian, Yonghong; Huang, Tiejun] Peking Univ, Sch EE & CS, Beijing, Peoples R China; [Tian, Yonghong; Huang, Tiejun] Cooperat Medianet Innovat Ctr, Beijing, Peoples R China	Beihang University; Beihang University; Peking University	Li, J (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing, Peoples R China.; Li, J (corresponding author), Beihang Univ, Int Res Inst Multidisciplinary Sci, Beijing, Peoples R China.; Tian, YH (corresponding author), Peking Univ, Sch EE & CS, Beijing, Peoples R China.; Tian, YH (corresponding author), Cooperat Medianet Innovat Ctr, Beijing, Peoples R China.	jiali@buaa.edu.cn; yhtian@pku.edu.cn	Li, Jia/AAB-6431-2019	Li, Jia/0000-0002-4346-8696	National Basic Research Program of China [2015CB351806]; National Natural Science Foundation of China [61370113, 61532003, 61390515, 61421062]; Fundamental Research Funds for the Central Universities	National Basic Research Program of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	Authors would like to thank anonymous reviewers for their helpful comments on the paper. This work was supported in part by grants from National Basic Research Program of China under Contract 2015CB351806, National Natural Science Foundation of China (61370113, 61532003, 61390515 and 61421062), and Fundamental Research Funds for the Central Universities.	Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706; Bruce N. D. B., 2005, ADV NEURAL INF PROCE, P155; Cerf M., 2009, ADV NEURAL INFORM PR; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Chun MM, 1998, COGNITIVE PSYCHOL, V36, P28, DOI 10.1006/cogp.1998.0681; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98; Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11; Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166; Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27; Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929; Gopalakrishnan V, 2009, PROC CVPR IEEE, P1698, DOI 10.1109/CVPRW.2009.5206767; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Hou WL, 2013, PATTERN RECOGN, V46, P2658, DOI 10.1016/j.patcog.2013.03.008; Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267; Hou X., 2009, ADV NEURAL INFORM PR, P681; Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146; HU Y, 2005, IEEE INT C MULT EXP; Itti L, 2005, PROC CVPR IEEE, P631; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Itti L., 2005, NEUROBIOLOGY ATTENTI; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kanwisher N, 2000, NAT REV NEUROSCI, V1, P91, DOI 10.1038/35039043; Kunar MA, 2006, PERCEPT PSYCHOPHYS, V68, P1204, DOI 10.3758/BF03193721; Li J., 2014, MACHINE LEARNING PER; Li J, 2014, INT J COMPUT VISION, V107, P239, DOI 10.1007/s11263-013-0678-0; Li J, 2012, IEEE T IMAGE PROCESS, V21, P1513, DOI 10.1109/TIP.2011.2179665; Li J, 2011, IEEE T CIRC SYST VID, V21, P623, DOI 10.1109/TCSVT.2011.2129430; Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6; Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147; Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047; Lu SJ, 2014, IEEE T PATTERN ANAL, V36, P195, DOI 10.1109/TPAMI.2013.158; Marat S, 2009, INT J COMPUT VISION, V82, P231, DOI 10.1007/s11263-009-0215-3; Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467; Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151; Navalpakkam V, 2007, NEURON, V53, P605, DOI 10.1016/j.neuron.2007.01.018; Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Russell AF, 2014, VISION RES, V94, P1, DOI 10.1016/j.visres.2013.10.005; Siva P, 2013, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2013.416; Sun XS, 2012, PROC CVPR IEEE, P1552, DOI 10.1109/CVPR.2012.6247846; Torralba Antonio, 2005, P586, DOI 10.1016/B978-012375731-9/50100-2; Vig E, 2012, IEEE T PATTERN ANAL, V34, P1080, DOI 10.1109/TPAMI.2011.198; Vikram TN, 2012, PATTERN RECOGN, V45, P3114, DOI 10.1016/j.patcog.2012.02.009; Wang M, 2011, PROC CVPR IEEE, P417, DOI 10.1109/CVPR.2011.5995743; Wang P, 2012, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2012.6248054; Wang W, 2010, PROC CVPR IEEE, P2368, DOI 10.1109/CVPR.2010.5539927; Wei SK, 2013, IEEE T CYBERNETICS, V43, P2216, DOI 10.1109/TCYB.2013.2245890; Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28; Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26; Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32; Zhao Q, 2012, J VISION, V12, DOI 10.1167/12.6.22; Zhao Q, 2011, J VISION, V11, DOI 10.1167/11.3.9; Zhu GK, 2013, IEEE T CYBERNETICS, V43, P2032, DOI 10.1109/TSMCB.2013.2238927	54	8	8	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2016	120	1					44	60		10.1007/s11263-016-0892-7	http://dx.doi.org/10.1007/s11263-016-0892-7			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DU3DX					2022-12-18	WOS:000382092100004
J	Xu, JT; Yang, QX; Tang, JH; Feng, ZR				Xu, Jintao; Yang, Qingxiong; Tang, Jinhui; Feng, Zuren			Linear Time Illumination Invariant Stereo Matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stereo Matching; Minimum spanning tree; Radiometric variation	BELIEF PROPAGATION	This paper proposes a new similarity measure that is invariant to global and local affine illumination changes. Unlike existing methods, its computational complexity is very low. When used for stereo correspondence estimation, its computational complexity is linear in the number of image pixels and disparity searching range. It also outperforms the current state of the art similarity measures in terms of accuracy on the Middlebury benchmark (with radiometric differences).	[Xu, Jintao; Feng, Zuren] Xi An Jiao Tong Univ, Xian, Peoples R China; [Xu, Jintao; Yang, Qingxiong] City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China; [Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Jiangsu, Peoples R China	Xi'an Jiaotong University; City University of Hong Kong; Nanjing University of Science & Technology	Yang, QX (corresponding author), City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China.	qiyang@cityu.edu.hk; jinhuitang@njust.edu.cn	Yang, Qingxiong/K-1729-2015	Yang, Qingxiong/0000-0002-4378-2335; Tang, Jinhui/0000-0001-9008-222X	Research Grants Council of the Hong Kong Special Administrative Region, China [CityU 21201914]	Research Grants Council of the Hong Kong Special Administrative Region, China(Hong Kong Research Grants Council)	This work was supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CityU 21201914).	Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603; Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190; Cox I., 1995, ICIP; De-Maeztu L, 2011, IEEE I CONF COMP VIS, P1708, DOI 10.1109/ICCV.2011.6126434; Ding Yuanyuan, 2011, CVPR; Egnal Geoffrey, 2000, MUTUAL INFORM STEREO; Goesele Michael, 2007, ICCV; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Heo YS, 2013, IEEE T PATTERN ANAL, V35, P1094, DOI 10.1109/TPAMI.2012.167; Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136; Heo YS, 2009, PROC CVPR IEEE, P445, DOI 10.1109/CVPRW.2009.5206507; HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hirschmuller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221; Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156; Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495; Kim J., 2003, ICCV; Klaus A, 2006, INT C PATT RECOG, P15; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Liang Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3033, DOI 10.1109/CVPR.2011.5995480; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scharstein D., 2002, MIDDLEBURY STEREO DA; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Wang L, 2007, IEEE T PATTERN ANAL, V29, P1616, DOI [10.1109/TPAMI.2007.1171, 10.1109/TPAM1.2007.1171]; Yang QX, 2015, IEEE T PATTERN ANAL, V37, P834, DOI 10.1109/TPAMI.2014.2353642; Yang QX, 2010, PROC CVPR IEEE, P1458, DOI 10.1109/CVPR.2010.5539797; Yang QX, 2009, IEEE T PATTERN ANAL, V31, P492, DOI 10.1109/TPAMI.2008.99; Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70; Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345; Zhu SQ, 2012, LECT NOTES COMPUT SC, V7576, P101, DOI 10.1007/978-3-642-33715-4_8; Zomet A., 2002, WACV	32	8	8	0	20	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	2					179	193		10.1007/s11263-016-0886-5	http://dx.doi.org/10.1007/s11263-016-0886-5			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FA					2022-12-18	WOS:000380269600005
J	Gronat, P; Sivic, J; Obozinski, G; Pajdla, T				Gronat, Petr; Sivic, Josef; Obozinski, Guillaume; Pajdla, Tomas			Learning and Calibrating Per-Location Classifiers for Visual Place Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Place recognition; Exemplar SVM; Geo-localization; Classifier calibration		The aim of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint, imaging conditions and the large size of the image database. The contribution of this work is two-fold. First, we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second, as only one or a few positive training examples are available for each location, we propose two methods to calibrate all the per-location SVM classifiers without the need for additional positive training data. The first method relies on p-values from statistical hypothesis testing and uses only the available negative training data. The second method performs an affine calibration by appropriately normalizing the learnt classifier hyperplane and does not need any additional labelled training data. We test the proposed place recognitionmethod with the bag-of-visual-words and Fisher vector image representations suitable for large scale indexing. Experiments are performed on three datasets: 25,000 and 55,000 geotagged street view images of Pittsburgh, and the 24/7 Tokyo benchmark containing 76,000 images with varying illumination conditions. The results show improved place recognition accuracy of the learnt image representation over direct matching of raw image descriptors.	[Gronat, Petr; Sivic, Josef] Inria, Dept Informat, Willow Project, ENS,CNRS,UMR 8548, F-72012 Paris, France; [Obozinski, Guillaume] Ecole Ponts ParisTech, 6 Ave Blaise Pascal, F-77455 Marne La Vallee, France; [Gronat, Petr; Pajdla, Tomas] Czech Tech Univ, Fac Elect Engn, Dept Cybernet, Prague 12135 2, Czech Republic	Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Ecole des Ponts ParisTech; Czech Technical University Prague	Gronat, P (corresponding author), Inria, Dept Informat, Willow Project, ENS,CNRS,UMR 8548, F-72012 Paris, France.; Gronat, P (corresponding author), Czech Tech Univ, Fac Elect Engn, Dept Cybernet, Prague 12135 2, Czech Republic.	petr.gronat@inria.fr	Pajdla, Tomas/K-7954-2013	Pajdla, Tomas/0000-0001-6325-0072	MSR-INRIA laboratory; EIT-ICT labs; Google; ERC project LEAP; EC [RVO13000]; Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory [FA8650-12-C-7212]	MSR-INRIA laboratory; EIT-ICT labs; Google(Google Incorporated); ERC project LEAP; EC(European CommissionEuropean Commission Joint Research Centre); Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory	This work was supported by the MSR-INRIA laboratory, the EIT-ICT labs, Google, the ERC project LEAP and the EC project RVO13000 - Conceptual development of research organization. Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory, contract FA8650-12-C-7212. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon.	Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148; [Anonymous], 2002, LEARNING KERNELS; Arandjelovic R., 2012, IEEE PAMI; Aubry M., 2014, CVPR; AUBRY M., 2014, ACM T GRAPHICS; Bay H., 2006, ECCV; Cao S, 2013, PROC CVPR IEEE, P700, DOI 10.1109/CVPR.2013.96; Casella G, 2001, STAT INFERENCE, V2nd; Chen D., 2011, CVPR; Chum O., 2007, ICCV; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Cummins M., 2009, P ROB SCI SYST SEATT; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Doersch C., 2013, NIPS; Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597; Gebel M, 2007, STUD CLASS DATA ANAL, P141, DOI 10.1007/978-3-540-70981-7_17; Gharbi M., 2012, TECHNICAL REPORT; Gronat P., 2013, CVPR; Gronat P., PROJECT WEBPAGE LEAR; Hariharan B., 2012, ECCV; Hays J., 2008, CVPR; Irschara A., 2009, CVPR; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Kalogerakis E, 2009, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2009.5459259; Klingner B., 2013, ICCV; Knopp J., 2010, ECCV; Li Y., 2009, ICCV; LI Y, 2012, IN ECCV, P3, DOI DOI 10.4028/WWW.SCIENTIFIC.NET/AMR.577.3; Li Yunpeng, 2010, ECCV; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Malisiewicz T., 2011, ICCV; Muja M., 2014, IEEE T PAMI; Nister D., 2006, CVPR; Philbin J., 2010, IJCV; Philbin J., 2007, CVPR; Platt JC, 2000, ADV NEUR IN, P61; Sattler T., 2012, P BMVC; Sattler Torsten, 2012, ECCV; Scheirer W, 2012, CVPR; Schindler G., 2007, CVPR; Shrivastava A., 2011, SIGGRAPH ASIA; Singh S., 2012, ECCV; Sivic J., 2003, ICCV; Tighe J., 2013, CVPR; Torii A., 2011, IEEE WORKSH MOB VIS; Torii A., 2015, PROJECT WEBPAGE 24 7; Torii A., 2013, CVPR; Torii A., 2015, IEEE C COMP VIS PATT; Turcot P., 2009, WS LAVD ICCV; Zadrozny B., 2002, ACM SIGKDD; Zamir A. R., 2010, ECCV	54	8	10	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2016	118	3					319	336		10.1007/s11263-015-0878-x	http://dx.doi.org/10.1007/s11263-015-0878-x			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DP9AM		Green Submitted			2022-12-18	WOS:000378789100003
J	Huang, CH; Cagniart, C; Boyer, E; Ilic, S				Huang, Chun-Hao; Cagniart, Cedric; Boyer, Edmond; Ilic, Slobodan			A Bayesian Approach to Multi-view 4D Modeling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-view; Deformable surface tracking; Mesh registration; Expectation-Maximization; 3D human motion tracking; Bayesian network		This paper considers the problem of automatically recovering temporally consistent animated 3D models of arbitrary shapes in multi-camera setups. An approach is presented that takes as input a sequence of frame-wise reconstructed surfaces and iteratively deforms a reference surface such that it fits the input observations. This approach addresses several issues in this field that include: large frame-to-frame deformations, noise, missing data, outliers and shapes composed of multiple components with arbitrary geometries. The problem is cast as a geometric registration with two major features. First, surface deformations are modeled using mesh decomposition into elements called patches. This strategy ensures robustness by enabling flexible regularization priors through inter-patch rigidity constraints. Second, registration is formulated as a Bayesian estimation that alternates between probabilistic datal-model association and deformation parameter estimation. This accounts for uncertainties in the acquisition process and allows for noise, outliers and missing geometries in the observed meshes. In the case of marker-less 3D human motion capture, this framework can be specialized further with additional articulated motion constraints. Extensive experiments on various 4D datasets show that complex scenes with multiple objects of arbitrary nature can be processed in a robust way. They also demonstrate that the framework can capture human motion and provides visually convincing as well as quantitatively reliable human poses.	[Huang, Chun-Hao; Cagniart, Cedric; Ilic, Slobodan] Tech Univ Munich, D-80290 Munich, Germany; [Boyer, Edmond] LJK INRIA Grenoble Rhone Alpes, Grenoble, France	Technical University of Munich	Huang, CH (corresponding author), Tech Univ Munich, D-80290 Munich, Germany.	huangc@in.tum.de; cagniart@in.tum.de; edmond.boyer@inrialpes.fr; slobodan.ilic@in.tum.de		Ilic, Slobodan/0000-0002-3413-1936	Deutsche Telekom Laboratories	Deutsche Telekom Laboratories	This work was partially funded by Deutsche Telekom Laboratories and partly conducted in their laboratory.	Ahmed N., 2008, IEEE CVPR; BARAN I., 2007, SIGGRAPH; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bishop C.M, 2006, PATTERN RECOGN; Botsch M., 2008, IEEE T VISUALIZATION; Botsch M., 2005, IMA C MATH SURF; Botsch M, 2007, COMPUT GRAPH FORUM, V26, P339, DOI 10.1111/j.1467-8659.2007.01056.x; Cagniart C., 2010, IEEE CVPR; Cagniart C., 2010, ECCV; CHAI J, 2003, P ACM SIGGRAPH EUR S; Corazza S, 2010, INT J COMPUT VISION, V87, P156, DOI 10.1007/s11263-009-0284-3; De Aguiar E., 2010, SIGGRAPH; de Aguiar E., 2008, ACM SIGGRAPH 2008; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Duveau E., 2012, 3DIMPVT IEEE; FRANCO JB, 2003, BMVC; Furukawa Y., 2008, IEEE CVPR; Gall J., 2009, IEEE CVPR; Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1; Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300; Horaud R.P., 2010, IEEE PAMI; Huang C.-H., 2014, CVPR; Huang C. H., 2013, 3D VISION; James D. L., 2005, SIGGRAPH; Karypis G., 1998, SIAM J SCI COMPUT, V7, P14; Lewis J. P., 2000, SIGGRAPH; Li H., 1993, PAMI; Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x; Liao M., 2009, ICCV; Liu YB, 2011, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2011.5995424; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Meng X. L., 1993, BIOMETRIKA; Myronenko A., 2010, IEEE PAMI; Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x; Peel D, 2000, STAT COMPUT, V10, P339, DOI 10.1023/A:1008981510081; Popa T, 2010, COMPUT GRAPH FORUM, V29, P1633, DOI 10.1111/j.1467-8659.2010.01772.x; Rydfalk M., 1987, TECHNICAL REPORT; Salzmann M., 2007, IEEE PAMI; Seitz Steven M, 2006, CVPR; Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4; SORKINE O, 2004, SGP 04; Sorkine Olga, 2007, EUROGRAPHICS; Starck J., 2007, IEEE COMPUTER GRAPHI; Starck J., 2007, ICCV 2007; Stoll C., 2011, IEEE ICCV; Straka M., 2012, ECCV; Sumner R.W., 2007, ACM SIGGRAPH 2007; Toledo S., 2003, TECHNICAL REPORT; Urtasun R., 2004, ECCV; Varanasi K., 2008, ECCV; Vlasic D., 2008, SIGGRAPH; White R., 2007, SIGGRAPH; Zhou Z., 2014, PLOS ONE, V9	54	8	8	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2016	116	2					115	135		10.1007/s11263-015-0832-y	http://dx.doi.org/10.1007/s11263-015-0832-y			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7TJ					2022-12-18	WOS:000369423000001
J	Hoiem, D; Hays, J; Xiao, JX; Khosla, A				Hoiem, Derek; Hays, James; Xiao, Jianxiong; Khosla, Aditya			Guest Editorial: Scene Understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Hoiem, Derek] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; [Hays, James] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA; [Xiao, Jianxiong] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA	University of Illinois System; University of Illinois Urbana-Champaign; Brown University; Princeton University	Hoiem, D (corresponding author), Univ Illinois, Dept Comp Sci, 201 N Goodwin Ave, Urbana, IL 61801 USA.	dhoiem@illinois.edu; hays@cs.brown.edu; xj@princeton.edu; khosla@mit.edu							0	8	8	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2015	112	2			SI		131	132		10.1007/s11263-015-0807-z	http://dx.doi.org/10.1007/s11263-015-0807-z			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CE0TD		Green Submitted, Bronze			2022-12-18	WOS:000351518500001
J	Kobayashi, T				Kobayashi, Takumi			Low-Rank Bilinear Classification: Efficient Convex Optimization and Extensions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Bilinear classifier; Low-rank matrix; Convex optimization; Multiple kernel learning; Cross modal learning	MATRIX	In pattern classification, it is needed to efficiently treat not only feature vectors but also feature matrices defined as two-way data, while preserving the two-way structure such as spatio-temporal relationships. The classifier for the feature matrix is generally formulated in a bilinear form composed of row and column weights which jointly result in a matrix weight. The rank of the matrix should be low from the viewpoint of generalization performance and computational cost. For that purpose, we propose a low-rank bilinear classifier based on the efficient convex optimization. In the proposed method, the classifier is optimized by minimizing the trace norm of the classifier (matrix) to reduce the rank without any hard constraint on it. We formulate the optimization problem in a tractable convex form and provide the procedure to solve it efficiently with the global optimum. In addition, we propose two novel extensions of the bilinear classifier in terms of multiple kernel learning and cross-modal learning. Through kernelizing the bilinear method, we naturally induce a novel multiple kernel learning. The method integrates both the inter kernels between heterogeneous reproducing kernel Hilbert spaces (RKHSs) and the ordinary kernels within respective RKHSs into a new discriminative kernel in a unified manner using the bilinear model. Besides, for cross-modal learning, we consider to map into the common space themulti-modal features which are subsequently classified in that space. We show that the projection and the classification are jointly represented by the bilinear model, and then propose the method to optimize both of them simultaneously in the bilinear framework. In the experiments on various visual classification tasks, the proposed methods exhibit favorable performances compared to the other methods.	Natl Inst Adv Ind Sci & Technol, Tsukuba, Ibaraki, Japan	National Institute of Advanced Industrial Science & Technology (AIST)	Kobayashi, T (corresponding author), Natl Inst Adv Ind Sci & Technol, Tsukuba, Ibaraki, Japan.	takumi.kobayashi@aist.go.jp	Kobayashi, Takumi/M-4913-2016	Kobayashi, Takumi/0000-0002-9244-317X				Akaho S., 2001, INT M PSYCH SOC IMPS; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Christoudias CM, 2010, LECT NOTES COMPUT SC, V6311, P677, DOI 10.1007/978-3-642-15549-9_49; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dempe S., 2002, FDN BILEVEL PROGRAMM; Duda R.O., 2001, PATTERN CLASSIFICATI; Eriksson A, 2010, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2010.5540139; Everingham M., 2007, PASCAL VISUAL OBJECT, DOI DOI 10.1007/S11263-014-0733-5; Fan RE, 2005, J MACH LEARN RES, V6, P1889; Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169; Graepel T, 1999, IEE CONF PUBL, P304, DOI 10.1049/cp:19991126; GUILLAUMIN M, 2010, PROC CVPR IEEE, P902, DOI DOI 10.1109/CVPR.2010.5540120; Haibin Ling, 2007, 2007 11th IEEE International Conference on Computer Vision; Kan MN, 2012, LECT NOTES COMPUT SC, V7572, P808, DOI 10.1007/978-3-642-33718-5_58; Kim Tae-Kyun, 2007, P IEEE C COMP VIS PA, P1; Kobayashi T, 2008, LECT NOTES COMPUT SC, V5302, P346, DOI 10.1007/978-3-540-88682-2_27; Kobayashi T, 2012, LECT NOTES COMPUT SC, V7573, P474, DOI 10.1007/978-3-642-33709-3_34; Kobayashi T, 2012, PATTERN RECOGN LETT, V33, P1188, DOI 10.1016/j.patrec.2012.01.007; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lazebnik S, 2005, IEEE I CONF COMP VIS, P832, DOI 10.1109/ICCV.2005.10; Lazebnik S., 2004, P BRIT MACH VIS C, V2, P959, DOI DOI 10.5244/C.18.98; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Loeff N, 2008, LECT NOTES COMPUT SC, V5305, P451, DOI 10.1007/978-3-540-88693-8_33; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Martnez A., 1998, 24 COMP VIS CTR; Nilsback M-E., 2006, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2006., DOI 10.1109/CVPR.2006.42]; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Nocedal J., 2006, NUMERICAL OPTIMIZATI; OSHERSON DN, 1991, COGNITIVE SCI, V15, P251, DOI 10.1207/s15516709cog1502_3; Otsu N., 2009, PATTERN RECOGN, V30, P185; Pirsiavash H., 2009, P ADV NEUR INF PROC, P1482; Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Schlkopf B., 2001, LEARNING KERNELS; Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350; Smola A. J., 2000, ADV LARGE MARGIN CLA; Srebro N., 2005, P ADV NEURAL INFORM; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Varma M, 2007, IEEE I CONF COMP VIS, P369; Wolf L, 2007, P 11 IEEE INT C COMP, P1, DOI DOI 10.1109/CVPR.2007.383099; Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097; Ye J., 2005, ADV NEURAL INFORM PR, P1569, DOI DOI 10.5555/2976040.2976237	51	8	9	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2014	110	3			SI		308	327		10.1007/s11263-014-0709-5	http://dx.doi.org/10.1007/s11263-014-0709-5			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AT2HL		Green Accepted			2022-12-18	WOS:000344754500006
J	Antunes, M; Barreto, JP				Antunes, Michel; Barreto, Joao P.			SymStereo: Stereo Matching using Induced Symmetry	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dense stereo matching; Matching cost; Symmetry; Stereo rangefinder; SRF	ENERGY MINIMIZATION	Stereo methods always require a matching function for assessing the likelihood of two pixels being in correspondence. Such functions, commonly referred as matching costs, measure the photo-similarity (or dissimilarity) between image regions centered in putative matches. This article proposes a new family of stereo cost functions that measure symmetry instead of photo-similarity for associating pixels across views. We start by observing that, given two stereo views and an arbitrary virtual plane passing in-between the cameras, it is possible to render image signals that are either symmetric or anti-symmetric with respect to the contour where the virtual plane meets the scene. The fact is investigated in detail and used as cornerstone to develop a new stereo framework that relies in symmetry cues for solving the data association problem. Extensive experiments in dense stereo show that our symmetry-based cost functions compare favorably against the best performing photo-similarity matching costs. In addition, we investigate the possibility of accomplishing Stereo Rangefinding that consists in using passive stereo to exclusively recover depth along a pre-defined scan plane. Thorough experiments provide evidence that stereo from induced symmetry is specially well suited for this purpose.	[Antunes, Michel; Barreto, Joao P.] Univ Coimbra, Inst Syst & Robot, Dept Elect & Comp Engn, P-3030 Coimbra, Portugal	Universidade de Coimbra	Antunes, M (corresponding author), Univ Coimbra, Inst Syst & Robot, Dept Elect & Comp Engn, P-3030 Coimbra, Portugal.	michel@isr.uc.pt; jpbar@isr.uc.pt	Barreto, Joao P/I-2845-2012	Barreto, Joao P/0000-0001-5220-9170; Antunes, Michel/0000-0001-6115-5186	Portuguese Science Foundation (FCT) [SFRH/BD/47488/2008]; Google, Inc through "Faculty Research Award"; FCT [PDCS10:PTDC/EEA-AUT/113818/2009, AMS-HMI12: RECI/EEI-AUT/0181/2012]	Portuguese Science Foundation (FCT)(Portuguese Foundation for Science and Technology); Google, Inc through "Faculty Research Award"; FCT(Portuguese Foundation for Science and TechnologyEuropean Commission)	Michel Antunes acknowledges the Portuguese Science Foundation (FCT) that generously funded his PhD work through grant SFRH/BD/47488/2008. Joao P Barreto thanks Google, Inc for the support through a "Faculty Research Award" and FCT for generous funding through grants PDCS10:PTDC/EEA-AUT/113818/2009 and AMS-HMI12: RECI/EEI-AUT/0181/2012.	[Anonymous], EUR C COMP VIS; Ansar A., 2004, 3D DATA PROCESSING V; Antunes M., 2012, IEEE INT C INT ROB S, P1; Antunes M., 2011, INT C COMP VIS WORKS; Antunes M., 2012, 3DIMPVT 3D DATA PROC, P1; Antunes M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.97; Banks J, 2001, INT J ROBOT RES, V20, P512, DOI 10.1177/02783640122067525; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603; Collins R. T., 1996, IEEE C COMP VIS PATT; Fookes C., 2004, 3D DATA PROCESSING V; Gautama S., 1999, P INT WORKSH REC AN; Gong ML, 2007, INT J COMPUT VISION, V75, P283, DOI 10.1007/s11263-006-0032-x; Hirschmuller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221; Hirschmuller H, 2005, P IEEE COMP SOC C CO, P1; Kovesi P, 1995, TECHNICAL REPORT; Kovesi P, 1997, 10 AUST JT C ART INT; Liu Y., 2010, FDN TRENDS COMPUTER; Ma Y., 2003, INVITATION 3 D VISIO; Mordohai P., 2009, INT C VOMP VIS; Ponce J., 2005, IEEE C COMP VIS PATT; Sarkar I, 2007, IEEE T SYST MAN CY B, V37, P1009, DOI 10.1109/TSMCB.2007.890584; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; SCHARSTEIN D, 2007, IEEE C COMP VIS PATT; Strecha C., 2008, IEEE C COMP VIS PATT; Sun J., 2005, IEEE C COMP VIS PATT; Szeliski R, 2004, IEEE T PATTERN ANAL, V26, P419, DOI 10.1109/TPAMI.2004.1262341; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tombari F., 2008, IEEE C COMP VIS PATT; Wang L., 2006, 3D DATA PROCESSING V; Yang Q., 2007, IEEE C COMP VIS PATT; Yoon Kuk-jin, 2006, IEEE T PATTERN ANAL; Zabih R, 1994, EUR C COMP VIS	35	8	8	0	18	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2014	109	3					187	208		10.1007/s11263-014-0715-7	http://dx.doi.org/10.1007/s11263-014-0715-7			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AN1QL					2022-12-18	WOS:000340358600002
J	Hu, WM; Tian, GD; Li, X; Maybank, S				Hu, Weiming; Tian, Guodong; Li, Xi; Maybank, Stephen			An Improved Hierarchical Dirichlet Process-Hidden Markov Model and Its Application to Trajectory Modeling and Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hierarchical Dirichlet process; Hidden Markov model; Trajectory analysis; Video retrieval	VIDEO RETRIEVAL; PATTERNS; SPACE	In this paper, we propose a hierarchical Bayesian model, an improved hierarchical Dirichlet process-hidden Markov model (iHDP-HMM), for visual document analysis. The iHDP-HMM is capable of clustering visual documents and capturing the temporal correlations between the visual words within a visual document while identifying the number of document clusters and the number of visual topics adaptively. A Bayesian inference mechanism for the iHDP-HMM is developed to carry out likelihood evaluation, topic estimation, and cluster membership prediction. We apply the iHDP-HMM to simultaneously cluster motion trajectories and discover latent topics for trajectory words, based on the proposed method for constructing the trajectory word codebook. Then, an iHDP-HMM-based probabilistic trajectory retrieval framework is developed. The experimental results verify the clustering accuracy of the iHDP-HMM and trajectory retrieval accuracy of the proposed framework.	[Hu, Weiming; Tian, Guodong; Li, Xi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Maybank, Stephen] Univ London Birkbeck Coll, Dept Comp Sci & Informat Syst, London WC1E 7HX, England	Chinese Academy of Sciences; Institute of Automation, CAS; University of London; Birkbeck University London	Li, X (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.	wmhu@nlpr.ia.ac.cn; guodong_tian@126.com; lixichinanlpr@gmail.com; sjmaybank@dcs.bbk.ac.uk	Li, Xi/L-1234-2013	Li, Xi/0000-0003-3023-1662				Alon J, 2003, PROC CVPR IEEE, P375; Atev S, 2010, IEEE T INTELL TRANSP, V11, P647, DOI 10.1109/TITS.2010.2048101; Bashir F., 2004, P 6 ACM SIGMM INT WO, P235; Bashir FI, 2007, IEEE T MULTIMEDIA, V9, P58, DOI 10.1109/TMM.2006.886346; Beal M.J., 2006, P P 22 C ANN C UNC A, P23; Beal MJ, 2002, ADV NEUR IN, V14, P577; BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372; Blei D.M., 2004, P INT C MACH LEARN, P121; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chen L., 2005, P 2005 ACM SIGMOD IN, P491, DOI DOI 10.1145/1066157.1066213; Chen LCL, 2004, PACLIC 18: Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation, P227, DOI 10.1145/1026711.1026749; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; DIMITROVA N, 1995, ACM T INFORM SYST, V13, P408, DOI 10.1145/211430.211433; Dyana A, 2007, LECT NOTES COMPUT SC, V4815, P632; Dyana A, 2010, IEEE T CIRC SYST VID, V20, P1080, DOI 10.1109/TCSVT.2010.2051367; Dyana A., 2009, P 7 INT C ADV PATT R, P113; Fei-Fei L, 2005, PROC CVPR IEEE, P524; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Fox E. B., 2008, 25 INT C MACHINE LEA, P312; Georgescu B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P456; Hsieh JW, 2006, IEEE T CIRC SYST VID, V16, P396, DOI 10.1109/TCSVT.2006.869965; Jian YD, 2010, INT J COMPUT VISION, V88, P489, DOI 10.1007/s11263-010-0317-y; Johnson N, 1996, IMAGE VISION COMPUT, V14, P609, DOI 10.1016/0262-8856(96)01101-8; Jung CR, 2008, IEEE T CIRC SYST VID, V18, P1565, DOI 10.1109/TCSVT.2008.2005600; Keogh E. J., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P285, DOI 10.1145/347090.347153; Kivinen JJ, 2007, IEEE I CONF COMP VIS, P329; Kuettel D, 2010, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2010.5539869; Le T., 2006, P 1 INT C COMM SYST, P1, DOI DOI 10.1109/COMSWA.2006.1665200; Le TL, 2007, LECT NOTES COMPUT SC, V4351, P418; Li LJ, 2007, LECT NOTES ARTIF INT, V4456, P1; Li XH, 2008, 2008 IEEE PHOTONICSGLOBAL@SINGAPORE (IPGC), VOLS 1 AND 2, P341; LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577; Little JJ, 2001, P SOC PHOTO-OPT INS, V4315, P545, DOI 10.1117/12.410966; Liu CL, 2004, IEEE T PATTERN ANAL, V26, P198, DOI 10.1109/TPAMI.2004.1262182; Ma X, 2009, IEEE T CIRC SYST VID, V19, P397, DOI 10.1109/TCSVT.2009.2013510; MacEachern SN, 1998, J COMPUT GRAPH STAT, V7, P223, DOI 10.2307/1390815; Morris B, 2009, PROC CVPR IEEE, P312, DOI 10.1109/CVPRW.2009.5206559; Morris BT, 2008, IEEE T CIRC SYST VID, V18, P1114, DOI 10.1109/TCSVT.2008.927109; Morris BT, 2008, IEEE T INTELL TRANSP, V9, P425, DOI 10.1109/TITS.2008.922970; Naftel A., 2006, P IEEE INT C COMP VI, P47; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4; Piotto N, 2009, IEEE T MULTIMEDIA, V11, P1266, DOI 10.1109/TMM.2009.2030746; Sahouria E., 1997, THESIS U CALIFORNIA; Saleemi I, 2009, IEEE T PATTERN ANAL, V31, P1472, DOI 10.1109/TPAMI.2008.175; Shim C.-B., 2000, INT MULT C P 2000 AC, P209; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Sun J, 2005, IEEE I CONF COMP VIS, P717; Teh Yee W, 2005, ADV NEURAL INFORM PR, P1385, DOI DOI 10.1198/016214506000000302; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Veeraraghavan H, 2009, IEEE T INTELL TRANSP, V10, P628, DOI 10.1109/TITS.2009.2026440; Vlachos M, 2006, VLDB J, V15, P1, DOI 10.1007/s00778-004-0144-2; Vlachos M, 2002, PROC INT CONF DATA, P673, DOI 10.1109/ICDE.2002.994784; Wang G., 2006, COMP VIS PATT REC 20, P1597, DOI [10.1109/CVPR.2006.324, DOI 10.1109/CVPR.2006.324]; Wang X., 2007, IEEE C COMP VIS PATT, P1; Wang XQ, 2007, INT J THERM SCI, V46, P1, DOI 10.1016/j.ijthermalsci.2006.06.010; Wang XG, 2006, LECT NOTES COMPUT SC, V3953, P110, DOI 10.1007/11744078_9; Zhang CL, 2006, ICMLA 2006: 5TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P285; Zhang Z, 2006, INT C PATT RECOG, P1135; Zhu X., 2005, CMUCALD05104 SCH COM	60	8	9	0	29	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2013	105	3					246	268		10.1007/s11263-013-0638-8	http://dx.doi.org/10.1007/s11263-013-0638-8			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	208EN					2022-12-18	WOS:000323659900004
J	Cheong, LF; Gao, Z				Cheong, Loong-Fah; Gao, Zhi			Quasi-Parallax for Nearly Parallel Frontal Eyes A Possible Role of Binocular Overlap During Rapid Locomotion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Quasi-parallax; Binocular vision	STEREO-MOTION; VISION; NAVIGATION; ALGORITHM; OBSTACLE; GEOMETRY	In this paper, we explore how a visual system equipped with a pair of frontally-placed eyes/cameras can rapidly estimate egomotion and depths for the task of locomotion by exploiting the eye topography. We eschew the traditional approach of motion-stereo integration, as finding stereo correspondence is a computationally expensive operation. Instead, we propose a quasi-parallax scheme by pairing appropriate visual rays together, thus obviating the need for stereo correspondence and yet being able to leverage on the redundant information present in the binocular overlap. Our model covers realistic visual systems where the two eyes might deviate from the strictly frontal-parallel configuration, and yet the results show that the advantages of the parallax-based approach are retained. In particular, it offers better disambiguation of translation and rotation over conventional two-frame structure from motion approaches, despite not having views covering diametrically opposing directions like that of spherical eyes or laterally-placed eyes. The rapid processing that such scheme entails seems to offer a more realizable and useful alternative for depth recovery during locomotion.	[Cheong, Loong-Fah] Natl Univ Singapore, Elect & Comp Engn Dept, Singapore 117576, Singapore; [Gao, Zhi] Natl Univ Singapore, Interact & Digital Media Inst, Singapore 119613, Singapore	National University of Singapore; National University of Singapore	Gao, Z (corresponding author), Natl Univ Singapore, Interact & Digital Media Inst, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore.	eleclf@nus.edu.sg; gaozhinus@gmail.com			MDA of Singapore [NRF2007IDM-IDM002-069]	MDA of Singapore	The support of the research grant NRF2007IDM-IDM002-069 from MDA of Singapore is gratefully acknowledged.	Argyros AA, 2004, IEEE ROBOT AUTOM MAG, V11, P21, DOI 10.1109/MRA.2004.1371612; AYACHE N, 1989, IEEE T ROBOTIC AUTOM, V5, P804, DOI 10.1109/70.88101; Balasubramanyam P., 1991, CVPR, P115; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; Carelli R, 2002, IEEE IND ELEC, P2433; Cheong L, 1998, COMPUT VIS IMAGE UND, V71, P356, DOI 10.1006/cviu.1997.0649; Cheong LF, 1999, INT J COMPUT VISION, V32, P195, DOI 10.1023/A:1008105012585; CLARK J, 1994, DATA FUSION SENSORY; Coombs D., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P440, DOI 10.1109/CVPR.1993.341093; Corke P, 2004, IEEE INT CONF ROBOT, P3602, DOI 10.1109/ROBOT.2004.1308811; Davies M.N.O., 1994, P339; Dev A., 1997, Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97 (Cat. No.97CH36108), P558, DOI 10.1109/IROS.1997.655067; DUCHON AP, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS - HUMANS, INFORMATION AND TECHNOLOGY, VOLS 1-3, P2272, DOI 10.1109/ICSMC.1994.400203; FRANCESCHINI N, 1992, PHILOS T R SOC B, V337, P283, DOI 10.1098/rstb.1992.0106; Griffiths S, 2006, IEEE ROBOT AUTOM MAG, V13, P34, DOI 10.1109/MRA.2006.1678137; GROSSO E, 1989, IEEE T SYST MAN CYB, V19, P1465, DOI 10.1109/21.44065; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; HILDRETH EC, 1992, VISION RES, V32, P1177, DOI 10.1016/0042-6989(92)90020-J; Ho PK, 2000, IEEE T PATTERN ANAL, V22, P215, DOI 10.1109/34.825760; Hrabar S, 2005, 2005 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P302; Hu CX, 2009, INT J COMPUT VISION, V84, P21, DOI 10.1007/s11263-009-0226-0; Huguet F, 2007, IEEE I CONF COMP VIS, P1342, DOI 10.1109/iccv.2007.4409000; Humbert J. Sean, 2007, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, P2144, DOI 10.1109/IROS.2007.4399488; HUMBERT JS, 2005, P 16 IFAC WORLD C PR; Kim JH, 2010, IEEE T PATTERN ANAL, V32, P1044, DOI 10.1109/TPAMI.2009.82; KRIEGMAN DJ, 1989, IEEE T ROBOTIC AUTOM, V5, P792, DOI 10.1109/70.88100; Lee A.B., 2000, BROWN RANGE IMAGE DA; LI LX, 1993, IEEE T PATTERN ANAL, V15, P657, DOI 10.1109/34.221167; Lim JU, 2008, P IEEE C COMP VIS PA, P1; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma Y, 2000, INT J COMPUT VISION, V36, P71, DOI 10.1023/A:1008124507881; MacLean W. J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P753, DOI 10.1109/ICCV.1999.790297; Martin GR, 2007, J ORNITHOL, V148, pS547, DOI 10.1007/s10336-007-0213-6; Martin GR, 2009, J VISION, V9, DOI 10.1167/9.11.14; McFadden S.A., 1994, P54; McFadden Sally A., 1993, P47; Muratet L, 2005, ROBOT AUTON SYST, V50, P195, DOI 10.1016/j.robot.2004.09.017; Neumann J., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P3712; Neumann TR, 2001, LECT NOTES ARTIF INT, V2159, P627; Pless R, 2004, IEEE ROBOT AUTOM MAG, V11, P39, DOI 10.1109/MRA.2004.1371607; Pons JP, 2007, INT J COMPUT VISION, V72, P179, DOI 10.1007/s11263-006-8671-5; RIEGER JH, 1985, J OPT SOC AM A, V2, P354, DOI 10.1364/JOSAA.2.000354; RUFFIER F, 2007, P 7 M GERM NEUR SOC; SANTOSVICTOR J, 1995, INT J COMPUT VISION, V14, P159, DOI 10.1007/BF01418981; Serres J, 2008, AUTON ROBOT, V25, P103, DOI 10.1007/s10514-007-9069-0; Serres J., 2007, P 7 M GERM NEUR SOC; SHI YQ, 1994, PATTERN RECOGN, V27, P1577, DOI 10.1016/0031-3203(94)90078-7; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Strecha C., 2002, ECCV, P495; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tomasi C., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P422, DOI 10.1109/CVPR.1993.341096; TSOTSOS JK, 1987, INT J COMPUT VISION, V1, P303, DOI 10.1007/BF00133569; VIEVILLE T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P750, DOI 10.1109/ICCV.1995.466863; WAXMAN AM, 1986, IEEE T PATTERN ANAL, V8, P715, DOI 10.1109/TPAMI.1986.4767853; Weber K, 1997, FROM LIVING EYES TO SEEING MACHINES, P226; Williams O, 2005, PROC CVPR IEEE, P250; Zhang HS, 2008, COMPUT VIS IMAGE UND, V111, P307, DOI 10.1016/j.cviu.2008.01.001	59	8	8	0	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2013	101	1					45	63		10.1007/s11263-012-0544-5	http://dx.doi.org/10.1007/s11263-012-0544-5			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	080XV					2022-12-18	WOS:000314278500003
J	Maybank, SJ; Ieng, S; Benosman, R				Maybank, Stephen J.; Ieng, Siohoi; Benosman, Ryad			A Fisher-Rao Metric for Paracatadioptric Images of Lines	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Central projection; Fisher-Rao metric; Hough transform; Line detection; Paraboloidal mirror; Paracatadioptric system; Sobel operator; Trace transform	TRANSFORM	In a central paracatadioptric imaging system a perspective camera takes an image of a scene reflected in a paraboloidal mirror. A 360A degrees field of view is obtained, but the image is severely distorted. In particular, straight lines in the scene project to circles in the image. These distortions make it difficult to detect projected lines using standard image processing algorithms. The distortions are removed using a Fisher-Rao metric which is defined on the space of projected lines in the paracatadioptric image. The space of projected lines is divided into subsets such that on each subset the Fisher-Rao metric is closely approximated by the Euclidean metric. Each subset is sampled at the vertices of a square grid and values are assigned to the sampled points using an adaptation of the trace transform. The result is a set of digital images to which standard image processing algorithms can be applied. The effectiveness of this approach to line detection is illustrated using two algorithms, both of which are based on the Sobel edge operator. The task of line detection is reduced to the task of finding isolated peaks in a Sobel image. An experimental comparison is made between these two algorithms and third algorithm taken from the literature and based on the Hough transform.	[Maybank, Stephen J.] Univ London Birkbeck Coll, Dept Comp Sci & Informat Syst, London WC1E 7HX, England; [Ieng, Siohoi; Benosman, Ryad] Univ Paris 06, Inst Syst Intelligents & Robot, F-75252 Paris 05, France	University of London; Birkbeck University London; UDICE-French Research Universities; Sorbonne Universite	Maybank, SJ (corresponding author), Univ London Birkbeck Coll, Dept Comp Sci & Informat Syst, Malet St, London WC1E 7HX, England.	sjmaybank@dcs.bbk.ac.uk	ieng, siohoi/AAM-2601-2020	ieng, siohoi/0000-0002-0030-6574				AMARI SI, 1985, LECT NOTES COMPUTER, V28; Baker S, 1999, INT J COMPUT VISION, V35, P175, DOI 10.1023/A:1008128724364; Barreto J, 2001, CVPR, V2, P422; Barreto JP, 2006, COMPUT VIS IMAGE UND, V101, P151, DOI 10.1016/j.cviu.2005.07.002; Bazin J. C., 2007, 7 WORKSH OMN VIS CAM; Bazin JC, 2007, LECT NOTES COMPUT SC, V4478, P25; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Daniilidis K, 2002, THIRD WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P3, DOI 10.1109/OMNVIS.2002.1044483; Duan FQ, 2010, LECT NOTES COMPUT SC, V6444, P525, DOI 10.1007/978-3-642-17534-3_65; Gasparini S, 2011, INT J COMPUT VISION, V94, P361, DOI 10.1007/s11263-011-0435-1; Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135; Geyer C., 2000, EUR C COMP VIS, P445; Gonzalez R.C., 2006, DIGITAL IMAGE PROCES; Kadyrov A, 2001, IEEE T PATTERN ANAL, V23, P811, DOI 10.1109/34.946986; KANATANI K, 1996, STAT COMPUTATION GEO; KANG SB, 2000, IEEE C COMP VIS PATT; Maybank SJ, 2008, NEUROCOMPUTING, V71, P2037, DOI 10.1016/j.neucom.2007.07.040; Maybank SJ, 2005, INT J COMPUT VISION, V63, P191, DOI 10.1007/s11263-005-6877-6; Maybank SJ, 2004, IEEE T PATTERN ANAL, V26, P1579, DOI 10.1109/TPAMI.2004.122; Maybank SJ, 2007, INT J COMPUT VISION, V72, P287, DOI 10.1007/s11263-006-9033-z; Pinciroli C., 2005, P 6 WORKSH OMN VIS C; Pratt WK, 2007, DIGITAL IMAGE PROCES, Vxix; Scaramuzza D., 2011, OMNIDIRECTIONAL CAME; Struik Dirk Jan, 1961, LECT CLASSICAL DIFFE, P7; Torii A, 2007, PATTERN RECOGN LETT, V28, P1186, DOI 10.1016/j.patrec.2007.02.002; VASSEUR P, 2004, P BRIT MACH VIS C LO, P57; Ying XH, 2004, INT C PATT RECOG, P839, DOI 10.1109/ICPR.2004.1333903	27	8	9	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2012	99	2					147	165		10.1007/s11263-012-0523-x	http://dx.doi.org/10.1007/s11263-012-0523-x			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	943RU		Green Accepted			2022-12-18	WOS:000304143700002
J	Cotter, CJ; Clark, A; Peiro, J				Cotter, Colin J.; Clark, Allan; Peiro, Joaquim			A Reparameterisation Based Approach to Geodesic Constrained Solvers for Curve Matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Geodesic shooting; Computational anatomy; Curve registration	SHAPE SPACE; REGISTRATION; METRICS; MODELS	We present a numerical algorithm for a new matching approach for parameterisation independent diffeomorphic registration of curves in the plane, targeted at robust registration between curves that require large deformations. This condition is particularly useful for the geodesic constrained approach in which the matching functional is minimised subject to the constraint that the evolving diffeomorphism satisfies the Hamiltonian equations of motion; this means that each iteration of the nonlinear optimisation algorithm produces a geodesic (up to numerical discretisation). We ensure that the computed solutions correspond to geodesics in the shape space by enforcing the horizontality condition (conjugate momentum is normal to the curve). Explicitly introducing and solving for a reparameterisation variable allows the use of a point-to-point matching condition. The equations are discretised using the variational particle-mesh method. We provide comprehensive numerical convergence tests and benchmark the algorithm in the context of large deformations, to show that it is a viable, efficient and accurate method for obtaining geodesics between curves.	[Cotter, Colin J.; Clark, Allan; Peiro, Joaquim] Univ London Imperial Coll Sci Technol & Med, Dept Aeronaut, London, England	Imperial College London	Cotter, CJ (corresponding author), Univ London Imperial Coll Sci Technol & Med, Dept Aeronaut, London, England.	colin.cotter@imperial.ac.uk	Peiro, Joaquim/F-2516-2010	Peiro, Joaquim/0000-0001-8859-4912	EPSRC DTA; BBSRC [BB/E023444/1] Funding Source: UKRI; Biotechnology and Biological Sciences Research Council [BB/E023444/1] Funding Source: researchfish	EPSRC DTA(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); BBSRC(UK Research & Innovation (UKRI)Biotechnology and Biological Sciences Research Council (BBSRC)); Biotechnology and Biological Sciences Research Council(UK Research & Innovation (UKRI)Biotechnology and Biological Sciences Research Council (BBSRC))	AC acknowledges funding from an EPSRC DTA PhD studentship.	Allassonniere S., 2008, P MATH FDN COMP AN; Allassonniere S, 2010, BERNOULLI, V16, P641, DOI 10.3150/09-BEJ229; [Anonymous], [No title captured]; Bauer M., 2010, SOBOLEV METRICS SHAP; Bauer M, 2012, DIFFER GEOM APPL, V30, P33, DOI 10.1016/j.difgeo.2011.10.002; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Charpiat G, 2005, FOUND COMPUT MATH, V5, P1, DOI 10.1007/s10208-003-0094-x; Chetverikov D, 2002, INT C PATT RECOG, P545, DOI 10.1109/ICPR.2002.1047997; COOTES TF, 2004, 8 EUR C COMP VIS ECC, V3024, P316; Cotter CJ, 2008, J PHYS A-MATH THEOR, V41, DOI 10.1088/1751-8113/41/34/344003; Cotter CJ, 2010, J GEOM MECH, V2, P51, DOI 10.3934/jgm.2010.2.51; Cotter SL, 2009, INVERSE PROBL, V25, DOI 10.1088/0266-5611/25/11/115008; Davies RH, 2002, LECT NOTES COMPUT SC, V2352, P3; Durrleman S, 2009, LECT NOTES COMPUT SC, V5761, P297, DOI 10.1007/978-3-642-04268-3_37; Feldmar J, 1996, INT J COMPUT VISION, V18, P99, DOI 10.1007/BF00054998; Fishbaugh J., 2011, P MED IM COMP COMP A; Fletcher P. T., 2008, P 2008 IEEE C COMPUT, P1; Fletcher P.T., 2004, THESIS U N CAROLINA; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; Frank J, 2004, ATMOS SCI LETT, V5, P89, DOI 10.1002/asl.70; Gambaruto AA, 2009, INT J NUMER METH FL, V59, P1259, DOI 10.1002/fld.1866; Gay-Balmaz F, 2011, J GEOM MECH, V3, P41, DOI 10.3934/jgm.2011.3.41; GLAUNES J, 2006, STAT ANAL SHAPES; Glaunes J, 2008, INT J COMPUT VISION, V80, P317, DOI 10.1007/s11263-008-0141-9; GREENGARD L, 1991, SIAM J SCI STAT COMP, V12, P79, DOI 10.1137/0912004; Grenander U, 1998, Q APPL MATH, V56, P617, DOI 10.1090/qam/1668732; Khesin B., 2008, ERGEBNISSE MATH GREN, V51; Kilian M, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276457, 10.1145/1239451.1239515]; Kurtek S, 2011, LECT NOTES COMPUT SC, V6801, P147, DOI 10.1007/978-3-642-22092-0_13; Kurtek S, 2011, IEEE T MED IMAGING, V30, P849, DOI 10.1109/TMI.2010.2099130; Kurtek S, 2010, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2010.5539778; McLachlan R., 2007, ANZIAM J, V48, P1, DOI [10.21914/anziamj.v48i0.82, DOI 10.21914/ANZIAMJ.V48I0.82]; Michor PW, 2007, APPL COMPUT HARMON A, V23, P74, DOI 10.1016/j.acha.2006.07.004; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Miller MI, 2001, INT J COMPUT VISION, V41, P61, DOI 10.1023/A:1011161132514; Nocedal J., 2006, PRACTICAL METHODS OP; Oliphant TE, 2007, COMPUT SCI ENG, V9, P10, DOI 10.1109/MCSE.2007.58; Sharon E, 2006, INT J COMPUT VISION, V70, P55, DOI 10.1007/s11263-006-6121-z; Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184; Sundaramoorthi G, 2011, SIAM J IMAGING SCI, V4, P109, DOI 10.1137/090781139; Taylor DJ, 2009, PROCEEDINGS OF THE ASME SUMMER BIOENGINEERING CONFERENCE - 2009, PT A AND B, P1071; Trouve A, 2005, SIAM J MATH ANAL, V37, P17, DOI 10.1137/S0036141002404838; Trouve A., 2010, Q APPL MATH IN PRESS; Vaillant M, 2005, LECT NOTES COMPUT SC, V3565, P381; Vialard F. X., 2009, THESIS ENS CACHAN; Vialard FX, 2012, INT J COMPUT VISION, V97, P229, DOI 10.1007/s11263-011-0481-8; Younes L, 2008, REND LINCEI-MAT APPL, V19, P25; Younes L, 2007, Q APPL MATH, V65, P113, DOI 10.1090/S0033-569X-07-01027-5; ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149; [No title captured]	51	8	8	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2012	99	1					103	121		10.1007/s11263-012-0520-0	http://dx.doi.org/10.1007/s11263-012-0520-0			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	935NG					2022-12-18	WOS:000303525200006
J	Luo, T; Li, RJ; Zha, HB				Luo, Tao; Li, Renju; Zha, Hongbin			3D Line Drawing for Archaeological Illustration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-scale 3D lines; Random walks; Minimum description length; Archaeological illustration		Archaeological line drawing is an essential component of an archaeological report, which can not only illustrate heritage objects, but also record their accurate geometric measurements. In this paper, we propose a multi-scale approach to generating 3D line drawings on meshes reconstructed from raw scanning data of objects in grottoes. To reduce redundant lines detected on the rough and noisy surfaces, we first construct the discrete multi-scale representation of a given model based on random walks. The transition probability matrices are defined by studying the local variation around each vertex on the mesh. Furthermore, it is difficult to determine a single optimal scale for the whole mesh, because different scales are localized on the surface. A method for local scale selection is proposed based on the minimum description length (MDL) principle. Finally, we generate the line drawing using ridges or valleys detected with the selected scales. Experimental results show that the multi-scale 3D lines can well depict the shapes of heritage objects. Compared with the traditional manual method, our method is more accurate and convenient. Moreover, the offsets of detected lines are less than those using mesh smoothing methods. As assistance in archaeological mapping, our computer-generated line drawings can decrease the time cost to a large extent.	[Luo, Tao; Li, Renju; Zha, Hongbin] Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China	Peking University	Luo, T (corresponding author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.	luotao@cis.pku.edu.cn; lirenju@cis.pku.edu.cn; zha@cis.pku.edu.cn			NKBPRC 973 [2011CB302202]; NHTRDP 863 [2009AA012105, 2009AA01Z329]	NKBPRC 973(National Basic Research Program of China); NHTRDP 863	We sincerely thank for the detailed comments and advice from the anonymous reviewers. This work was supported in part by NKBPRC 973 Grant No. 2011CB302202, NHTRDP 863 Grant No. 2009AA012105 and NHTRDP 863 Grant No. 2009AA01Z329. Thanks for the cooperation of Longmen Grottoes Academy and School of Archaeology and Museology, Peking University.	DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354; DECARLO D, 2007, P NPAR, P63; Desbrun M, 2000, PROC GRAPH INTERF, P145; Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576; Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074; Ikeuchi K, 2007, INT J COMPUT VISION, V75, P189, DOI 10.1007/s11263-007-0039-y; Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470; Kolomenkin M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409110; Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; LUO T, 2009, P 12 INT C COMP VIS, P907; MA H, 1993, ARCHAEOLOGICAL MAPPI; MA W, 2006, P 12 INT C VIRT SYST; Marroquin JL, 2001, IEEE T PATTERN ANAL, V23, P337, DOI 10.1109/34.917570; NOVATNACK J, 2006, INT S 3D DAT PROC VI; NOVATNACK J, 2007, P INT C COMP VIS; Ohtake Y, 2004, ACM T GRAPHIC, V23, P609, DOI 10.1145/1015706.1015768; Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675; RISSANEN J, 1983, ANN STAT, V11, P416, DOI 10.1214/aos/1176346150; RUSINKIEWICZ S, 2004, INT S 3D DAT PROC VI; SPITZER F, 2001, PRINCIPLES RANDOM WA; Stamos I, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P731, DOI 10.1109/ICCV.2001.937699; Wasserman Jack, 2003, MICHELANGELOS FLOREN; Yagou H, 2002, GEOMETRIC MODELING AND PROCESSING: THEORY AND APPLICATIONS, PROCEEDINGS, P124, DOI 10.1109/GMAP.2002.1027503; Yoshizawa S., 2005, P ACM S SOL PHYS MOD, P227, DOI [DOI 10.1145/1060244.1060270, 10.1145/1060244.1060270]	27	8	8	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2011	94	1					23	35		10.1007/s11263-010-0394-y	http://dx.doi.org/10.1007/s11263-010-0394-y			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)	Computer Science	760JM					2022-12-18	WOS:000290320600003
J	Burgeth, B; Pizarro, L; Breuss, M; Weickert, J				Burgeth, Bernhard; Pizarro, Luis; Breuss, Michael; Weickert, Joachim			Adaptive Continuous-Scale Morphology for Matrix Fields	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Mathematical morphology; PDEs; DT-MRI; Tensor field; Dilation; Erosion	FLUX-CORRECTED TRANSPORT; NUMERICAL-SOLUTION SCHEMES; SHOCK FILTERS; DIFFUSION; ENHANCEMENT; EQUATIONS; ALGORITHMS; EVOLUTION; IMAGES	In this article we consider adaptive, PDE-driven morphological operations for 3D matrix fields arising e.g. in diffusion tensor magnetic resonance imaging (DT-MRI). The anisotropic evolution is steered by a matrix constructed from a structure tensor for matrix valued data. An important novelty is an intrinsically one-dimensional directional variant of the matrix-valued upwind schemes such as the Rouy-Tourin scheme. It enables our method to complete or enhance anisotropic structures effectively. A special advantage of our approach is that upwind schemes are utilised only in their basic one-dimensional version, hence avoiding grid effects and leading to an accurate algorithm. No higher dimensional variants of the schemes themselves are required. Experiments with synthetic and real-world data substantiate the gap-closing and line-completing properties of the proposed method.	[Burgeth, Bernhard; Pizarro, Luis; Breuss, Michael; Weickert, Joachim] Univ Saarland, Fac Math & Comp Sci, D-6600 Saarbrucken, Germany; [Pizarro, Luis] Univ London Imperial Coll Sci Technol & Med, Natl Heart & Lung Inst, London, England; [Pizarro, Luis] Univ London Imperial Coll Sci Technol & Med, Dept Comp, London, England	Saarland University; Imperial College London; Imperial College London	Burgeth, B (corresponding author), Univ Saarland, Fac Math & Comp Sci, D-6600 Saarbrucken, Germany.	burgeth@mia.uni-saarland.de; pizarro@mia.uni-saarland.de; breuss@mia.uni-saarland.de; weickert@mia.uni-saarland.de			German Academic Exchange Service (DAAD) [A/05/21715]	German Academic Exchange Service (DAAD)(Deutscher Akademischer Austausch Dienst (DAAD))	L.P. gratefully acknowledges partial funding by the German Academic Exchange Service (DAAD), grant no. A/05/21715.	ALVAREZ L, 1993, ARCH RATION MECH AN, V123, P199, DOI 10.1007/BF00375127; ALVAREZ L, 1994, SIAM J NUMER ANAL, V31, P590, DOI 10.1137/0731032; AREHART AB, 1993, P 4 INT C COMP VIS B, P215; BASSER PJ, 1994, BIOPHYS J, V66, P259, DOI 10.1016/S0006-3495(94)80775-1; BIGUN J, 1991, IEEE T PATTERN ANAL, V13, P775, DOI 10.1109/34.85668; Bigun J., 2006, VISION DIRECTION; Boris J. P., 1975, J COMPUT PH, V18, P248, DOI DOI 10.1016/0021-9991(75)90002-9; BORIS JP, 1976, J COMPUT PHYS, V20, P397, DOI 10.1016/0021-9991(76)90091-7; BORIS JP, 1973, J COMPUT PHYS, V11, P38, DOI 10.1016/0021-9991(73)90147-2; Breuss M, 2007, LECT NOTES COMPUT SC, V4478, P515; Breuss M, 2006, J MATH IMAGING VIS, V25, P187, DOI 10.1007/s10851-006-9696-7; BROCKETT RW, 1994, IEEE T SIGNAL PROCES, V42, P3377, DOI 10.1109/78.340774; Brox T, 2006, IMAGE VISION COMPUT, V24, P41, DOI 10.1016/j.imavis.2005.09.010; Burgeth B, 2007, COMPUTING, V81, P179, DOI 10.1007/s00607-007-0248-9; Burgeth B, 2007, IMAGE VISION COMPUT, V25, P496, DOI 10.1016/j.imavis.2006.06.002; Burgeth B, 2007, SIGNAL PROCESS, V87, P277, DOI 10.1016/j.sigpro.2005.12.012; Burgeth B, 2009, ADV PATTERN RECOGNIT, P125, DOI 10.1007/978-1-84882-299-3_6; Burgeth B, 2009, LECT NOTES COMPUT SC, V5567, P247, DOI 10.1007/978-3-642-02256-2_21; Burgeth B, 2009, MATH VIS, P305, DOI 10.1007/978-3-540-88378-4_15; Chefd'hotel C, 2002, LECT NOTES COMPUT SC, V2350, P251; DIZENZO S, 1986, COMPUT VISION GRAPH, V33, P116, DOI 10.1016/0734-189X(86)90223-9; Feddern C, 2006, INT J COMPUT VISION, V69, P93, DOI 10.1007/s11263-006-6854-8; Forstner<spacing Wolfgang, 1987, ISPRS INT C FAST PRO, P2; Gilboa G, 2002, LECT NOTES COMPUT SC, V2350, P399; Guichard F, 2003, INT J COMPUT VISION, V52, P153, DOI 10.1023/A:1022904124348; Horn R.A., 2013, MATRIX ANAL, P321; KRAMER HP, 1975, PATTERN RECOGN, V7, P53, DOI 10.1016/0031-3203(75)90013-8; Laidlaw D, 2009, MATH VIS, P1, DOI 10.1007/978-3-540-88378-4; Matheron G., 1975, RANDOM SETS INTEGRAL; Matheron G, 1967, ELEMENTS THEORIE MIL; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; OSHER S, 1991, P SOC PHOTO-OPT INS, V1567, P414, DOI 10.1117/12.50835; OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053; Osher S., 2002, APPL MATH SCI, V153; Pizarro L, 2009, LECT NOTES COMPUT SC, V5720, P250, DOI 10.1007/978-3-642-03613-2_23; Remaki L, 2003, J MATH IMAGING VIS, V18, P129, DOI 10.1023/A:1022160416128; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; SAPIRO G, 1993, PATTERN RECOGN, V26, P1363, DOI 10.1016/0031-3203(93)90142-J; SCHAVEMAKER JGM, 1997, P IEEE WORKSH NONL S; SERRA J, 1967, THESIS U NANCY FRANC; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; Serra J, 1988, IMAGE ANAL MATH MORP; Sethian J. A., 1999, LEVEL SET METHODS FA; Soille P., 2013, MORPHOLOGICAL IMAGE; van den Boomgaard R, 1999, LECT NOTES COMPUT SC, V1682, P199; van den Boomgaard R., 1992, THESIS U AMSTERDAM N; VANVLIET LJ, 1989, COMPUT VISION GRAPH, V45, P167, DOI 10.1016/0734-189X(89)90131-X; Weickert J, 2003, LECT NOTES COMPUT SC, V2781, P1; Weickert J., 2006, VISUALIZATION PROCES; WEICKERT J, 2002, CONT MATH, V313, P251	50	8	10	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2011	92	2					146	161		10.1007/s11263-009-0311-4	http://dx.doi.org/10.1007/s11263-009-0311-4			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	729DR		Green Submitted			2022-12-18	WOS:000287929300002
J	Han, B; Joo, SW; Davis, LS				Han, Bohyung; Joo, Seong-Wook; Davis, Larry S.			Multi-Camera Tracking with Adaptive Resource Allocation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object tracking; Resource allocation; Multi-camera tracking; Sensor fusion; Kernel-based Bayesian filtering; Mixture model	INTEGRATION; FILTER; FUSION	Sensor fusion for object tracking is attractive since the integration of multiple sensors and/or algorithms with different characteristics can improve performance. However, there exist several critical limitations to sensor fusion techniques: (1) the measurement cost increases typically as many times as the number of sensors, (2) it is not straightforward to measure the confidence of each source and give it a proper weight for state estimation, and (3) there is no principled dynamic resource allocation algorithm for better performance and efficiency. We describe a method to fuse information from multiple sensors and estimate the current tracker state by using a mixture of sequential Bayesian filters (e.g., particle filter)-one filter for each sensor, where each filter makes a different level of contribution to estimate the combined posterior in a reliable manner. In this framework, multiple sensors interact to determine an appropriate sensor for each particle dynamically; each particle is allocated to only one of the sensors for measurement and a different number of particles is assigned to each sensor. The level of the contribution of each sensor changes dynamically based on its prior information and relative measurement confidence. We apply this technique to visual tracking with multiple cameras, and demonstrate its effectiveness through tracking results in videos.	[Han, Bohyung] Pohang Univ Sci & Technol POSTECH, Dept Comp Sci & Engn, Pohang, South Korea; [Joo, Seong-Wook] Google Inc, Mountain View, CA USA; [Davis, Larry S.] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA	Pohang University of Science & Technology (POSTECH); Google Incorporated; University System of Maryland; University of Maryland College Park	Han, B (corresponding author), Pohang Univ Sci & Technol POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.	bhhan@postech.ac.kr; swjoo@google.com; lsd@cs.umd.edu			Ministry of Education, Science and Technology [2010-0003496]	Ministry of Education, Science and Technology(Ministry of Education, Science & Technology (MEST), Republic of Korea)	This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (2010-0003496).	ADLERS M, 2000, THESIS LINKOPINGS U; AZOZ Y, 1998, P IEEE C COMP VIS PA; Birchfield S, 1998, PROC CVPR IEEE, P232, DOI 10.1109/CVPR.1998.698614; Black J, 2006, IMAGE VISION COMPUT, V24, P1256, DOI 10.1016/j.imavis.2005.06.002; BLOM HAP, 1988, IEEE T AUTOMAT CONTR, V33, P780, DOI 10.1109/9.1299; Cantarella J., 2004, TSNNLS SOLVER LARGE; Chen R, 2000, J R STAT SOC B, V62, P493, DOI 10.1111/1467-9868.00246; Chen YQ, 2004, P IEEE, V92, P485, DOI 10.1109/JPROC.2003.823146; Dockstader SL, 2001, P IEEE, V89, P1441, DOI 10.1109/5.959340; Doucet A., 2001, SEQUENTIAL MONTE CAR; Ha Z, 2008, COMPUT VIS IMAGE UND, V109, P1, DOI 10.1016/j.cviu.2006.12.001; HAN B, 2007, P 11 INT C COMP VIS; Han B, 2008, IEEE T PATTERN ANAL, V30, P1186, DOI 10.1109/TPAMI.2007.70771; Han B, 2009, IEEE T PATTERN ANAL, V31, P919, DOI 10.1109/TPAMI.2008.134; Hoffmann C, 2009, ROBOT AUTON SYST, V57, P268, DOI 10.1016/j.robot.2008.10.009; ISARD M, 1998, P 5 EUR C COMP VIS, V1, P893; Julier SJ, 1997, P SOC PHOTO-OPT INS, V3068, P182, DOI 10.1117/12.280797; Khan SM, 2006, LECT NOTES COMPUT SC, V3954, P133; Kim K, 2006, LECT NOTES COMPUT SC, V3953, P98; Lawson C. L., 1974, SOLVING LEAST SQUARE; LEICHTER I, 2006, P IEEE C COMP VIS PA, P445; Mazor E, 1998, IEEE T AERO ELEC SYS, V34, P103, DOI 10.1109/7.640267; McCane B, 2002, INT J COMPUT VISION, V49, P79, DOI 10.1023/A:1019833915960; Mittal A, 2003, INT J COMPUT VISION, V51, P189, DOI 10.1023/A:1021849801764; MUSICKI D, 2008, IEEE T AEROSPACE ELE, V44; Musicki D, 2008, SYST CONTROL LETT, V57, P216, DOI 10.1016/j.sysconle.2007.08.008; OKUMA K, 2004, P EUR C COMP VIS PRA; Perez P, 2004, P IEEE, V92, P495, DOI 10.1109/JPROC.2003.823147; Rui Y, 2001, PROC CVPR IEEE, P786; Salmond D. J., 1990, Proceedings of the SPIE, V1305, P434, DOI 10.1117/12.2321784; SHERRAH J, 2001, P 8 INT C COMP VIS V; Siebel NT, 2002, LECT NOTES COMPUT SC, V2353, P373; Spengler M, 2003, MACH VISION APPL, V14, P50, DOI 10.1007/s00138-002-0095-9; Triesch J, 2001, NEURAL COMPUT, V13, P2049, DOI 10.1162/089976601750399308; van der Merwe R., 2000, CUEDFINFENGTR380; Vermaak J, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P741, DOI 10.1109/ICCV.2001.937600; VERMAAK J, 2003, P 9 INT C COMP VIS N, V2; WILLIAMS J, 2003, INFORM FUSION, V2, P1047; Wu Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P26, DOI 10.1109/ICCV.2001.937590; YANG C, 2005, P IEEE INT C COMP VI, V1, P212; ZHONG X, 2006, P BRIT MACH VIS C ED	41	8	9	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2011	91	1					45	58		10.1007/s11263-010-0373-3	http://dx.doi.org/10.1007/s11263-010-0373-3			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	705HB					2022-12-18	WOS:000286118400003
J	Torii, A; Havlena, M; Pajdla, T				Torii, Akihiko; Havlena, Michal; Pajdla, Tomas			Omnidirectional Image Stabilization for Visual Object Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Omnidirectional vision; Structure from motion; Image rectification; Object recognition	MOTION	In this paper, we present a pipeline for camera pose and trajectory estimation, and image stabilization and rectification for dense as well as wide baseline omnidirectional images. The proposed pipeline transforms a set of images taken by a single hand-held camera to a set of stabilized and rectified images augmented by the computed camera 3D trajectory and a reconstruction of feature points facilitating visual object recognition. The paper generalizes previous works on camera trajectory estimation done on perspective images to omnidirectional images and introduces a new technique for omnidirectional image rectification that is suited for recognizing people and cars in images. The performance of the pipeline is demonstrated on real image sequences acquired in urban as well as natural environments.	[Torii, Akihiko; Havlena, Michal; Pajdla, Tomas] Czech Tech Univ, Fac Elec Eng, Dept Cybernet, Ctr Machine Percept, Prague 12135 2, Czech Republic	Czech Technical University Prague	Torii, A (corresponding author), Czech Tech Univ, Fac Elec Eng, Dept Cybernet, Ctr Machine Percept, Karlovo Namesti 13, Prague 12135 2, Czech Republic.	torii@cmp.felk.cvut.cz; havlem1@cmp.felk.cvut.cz; pajdla@cmp.felk.cvut.cz	Pajdla, Tomas/K-7954-2013; Torii, Akihiko/B-9270-2015	Pajdla, Tomas/0000-0001-6325-0072; Torii, Akihiko/0000-0002-0267-2674	EC [FP7-SPA-218814]; Czech Government [MSM 6840770038]	EC(European CommissionEuropean Commission Joint Research Centre); Czech Government	The authors were supported by EC project FP7-SPA-218814 PRoVisG, project CAT, and by Czech Government under the research program MSM 6840770038. We would like to thank Bastian Leibe and Andreas Ess for providing us the results of the multibody pedestrian tracking. Also, we would like to thank Horst Bischof for the fruitful discussions at Computer Vision Winter Workshop 2009.	AKBARZADEH A., 2006, 3DPVT; Bakstein H, 2002, THIRD WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P60, DOI 10.1109/OMNVIS.2002.1044492; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; BROWN M, 2003, ICCV 03 WASH DC US; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; CLIPP B, 2008, WACV 08, V1, P1; Cornelis N., 2006, CVPR, V2, P1339, DOI DOI 10.1109/CVPR.2006.118; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; ESS A, 2008, CVPR 08 ANCH AK US; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Geyer C, 2001, PROC CVPR IEEE, P279; Goedeme T, 2007, INT J COMPUT VISION, V74, P219, DOI 10.1007/s11263-006-0025-9; Hartley R., 2003, MULTIPLE VIEW GEOMET; HAVLENA M, 2009, CVPR 09 MIAM FL US; HAVLENA M, 2008, VISAPP 08 FUNCH PORT; Heller J., 2010, CTUCMP201001; HOIEM D, 2006, CVPR, V2, P2137; KAHL F, 2005, ICCV 05 CHIN BEIJ; Ke Q, 2007, IEEE T PATTERN ANAL, V29, P1834, DOI 10.1109/TPAMI.2007.1083; Knopp J., 2009, CTUCMP200901; Leibe B., 2007, ICCV 07 RIO DE JAN B, P2; LEIBE B, 2007, CVPR 07 MINN MN US; LI H, 2005, OMNIVIS 05 CHIN BEIJ; Lourakis M., 2004, 340 FORTH I COMP SCI; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; MARTINEC D, 2007, CVPR 07 MINN MN US; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; *MICR, 2008, PHOT US YOUR CAM STI; Micusik B, 2006, IEEE T PATTERN ANAL, V28, P1135, DOI 10.1109/TPAMI.2006.151; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Muja M., 2009, P INT C COMP VIS THE; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Nister D, 2004, PROC CVPR IEEE, P560; NISTER D, 2006, SPIE UNMANNED SYSTEM, V8; Obdrzalek S, 2003, LECT NOTES COMPUT SC, V2781, P490; OBDRZALEK S, 2002, BRIT MACH VIS C BMVC, V1, P113; *POINT GREY RES, 2005, LAD 2 SPHER DIG CAM; SCARAMUZZA D, 2008, OMNIVIS 08 MARS FRAN; SCHWEIGHOFER G, 2008, BMVC 08 LEEDS UK; Sivic J, 2006, LECT NOTES COMPUT SC, V4170, P127; SNAVELY N, 2008, CVPR 08 ANCH AK US; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; STEWENIUS H, 2005, THESIS LUND U SWEDEN; STURM J, 2006, SEDUMI SOFTWARE PACK; TARDIF J, 2008, IROS 08 NIC FRANC; TORII A, 2008, VISAPP 08 FUNCH PORT; TORII A, 2008, CVPR 08 ANCH AK US; WILLIAMS B, 2007, ICCV 07 RIO DE JAN B	49	8	8	0	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2011	91	2					157	174		10.1007/s11263-010-0350-x	http://dx.doi.org/10.1007/s11263-010-0350-x			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	708KJ					2022-12-18	WOS:000286360400003
J	Wu, FC; Zhang, Q; Hu, ZY				Wu, F. C.; Zhang, Q.; Hu, Z. Y.			Efficient Suboptimal Solutions to the Optimal Triangulation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Fundamental cone; Optimal triangulation; Suboptimal triangulation; Sampson sequence	OPTIMIZATION; GEOMETRY	Given two images, the optimal triangulation of a measured corresponding point pair is to basically find out the real roots of a 6-degree polynomial. Since for each point pair, this root finding process should be done, the optimal triangulation for the whole image is computationally intensive. In this work, via the 3D cone expression of fundamental matrix, called the fundamental cone, together with the Lagrange's multiplier method, the optimal triangulation problem is reformulated. Under this new formulation, the optimal triangulation for a measured point pair is converted to finding out the closest point on the fundamental cone to the measured point in the joint image space, then 3 efficient suboptimal algorithms, each of them can satisfy strictly the epipolar constraint of the two images, are proposed. In our first suboptimal algorithm, the closest point on the generating cone to the measured point is used as the approximation of the optimal solution, which is to find out the real roots of a 4-degree polynomial; in our second suboptimal algorithm, the closest point on the generating line to the measured point is used as the approximation of the optimal solution, which is to find out the real roots of a 2-degree polynomial. Finally, in our third suboptimal algorithm, the converging point of the Sampson approximation sequence is used as the approximation of the optimal solution. Experiments with simulated data as well as real images show that our proposed 3 suboptimal algorithms can achieve comparable estimation accuracy compared with the original optimal triangulation, but with much less computational load. For example, our second and third suboptimal algorithms take only about a 1/5 runtime of the original optimal solution. Besides, under our new formulation, rather than recompute the two Euclidian transformation matrices for each measured point pair, a fixed Euclidian transformation matrix is used for all image point pairs, which, in addition to its mathematical elegance and computational efficiency, is able to remove the dependency of the resulting polynomial's degree on the parameterization of the epipolar pencil in either the first image or in the second image, a drawback in the original optimal triangulation.	[Wu, F. C.; Zhang, Q.; Hu, Z. Y.] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS	Wu, FC (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, POB 2728, Beijing 100190, Peoples R China.	fcwu@nlpr.ia.ac.cn; qzhang@nlpr.ia.ac.cn; huzy@nlpr.ia.ac.cn			National Natural Science Foundation of China [60835003, 61075038]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We wish to thank the anonymous reviewers for their inspiring comments and suggestions. Also, we gratefully acknowledge the support from the National Natural Science Foundation of China (60835003; 61075038).	Anandan P, 2000, LECT NOTES COMPUT SC, V1842, P907; Hartley R, 2004, PROC CVPR IEEE, P504; HARTLEY R, 2008, C COMP VIS PATT REC; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Kahl F, 2008, IEEE T PATTERN ANAL, V30, P1603, DOI 10.1109/TPAMI.2007.70824; Kahl F, 2008, INT J COMPUT VISION, V79, P271, DOI 10.1007/s11263-007-0117-1; Ke Q, 2007, IEEE T PATTERN ANAL, V29, P1834, DOI 10.1109/TPAMI.2007.1083; Lu FF, 2007, LECT NOTES COMPUT SC, V4844, P279; Olsson C, 2009, PROC CVPR IEEE, P1216, DOI 10.1109/CVPRW.2009.5206864; SAMPSON PD, 1982, COMPUT VISION GRAPH, V18, P97, DOI 10.1016/0146-664X(82)90101-0; Stewenius H, 2005, IEEE I CONF COMP VIS, P686; Torr P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P727, DOI 10.1109/ICCV.1998.710798; Torr PHS, 1997, IMAGE VISION COMPUT, V15, P591, DOI 10.1016/S0262-8856(97)00010-3; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561	15	8	9	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2011	91	1					77	106		10.1007/s11263-010-0378-y	http://dx.doi.org/10.1007/s11263-010-0378-y			30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	705HB					2022-12-18	WOS:000286118400005
J	Bermanis, A; Averbuch, A; Keller, Y				Bermanis, Amit; Averbuch, Amir; Keller, Yosi			3-D Symmetry Detection and Analysis Using the Pseudo-polar Fourier Transform	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D symmetry; Pseudo-polar; Symmetry groups		Symmetry detection and analysis in 3D images is a fundamental task in a gamut of scientific fields such as computer vision, medical imaging and pattern recognition to name a few. In this work, we present a computational approach to 3D symmetry detection and analysis. Our analysis is conducted in the Fourier domain using the pseudo-polar Fourier transform. The pseudo-polar representation enables to efficiently and accurately analyze angular volumetric properties such as rotational symmetries. Our algorithm is based on the analysis of the angular correspondence rate of the given volume and its rotated and rotated-inverted replicas in their pseudo-polar representations. We also derive a novel rigorous analysis of the inherent constraints of 3D symmetries via groups-theory based analysis. Thus, our algorithm starts by detecting the rotational symmetry group of a given volume, and the rigorous analysis results pave the way to detect the rest of the symmetries. The complexity of the algorithm is O(N (3)log (N)), where NxNxN is the volumetric size in each direction. This complexity is independent of the number of the detected symmetries. We experimentally verified our approach by applying it to synthetic as well as real 3D objects.	[Bermanis, Amit; Averbuch, Amir] Tel Aviv Univ, Sch Math Sci, IL-69978 Tel Aviv, Israel; [Keller, Yosi] Bar Ilan Univ, Sch Engn, Ramat Gan, Israel	Tel Aviv University; Bar Ilan University	Averbuch, A (corresponding author), Tel Aviv Univ, Sch Math Sci, IL-69978 Tel Aviv, Israel.	amitberm@post.tau.ac.il; amir@math.tau.ac.il; yosi.keller@gmail.com						Averbuch A, 2003, APPL COMPUT HARMON A, V15, P33, DOI 10.1016/S1063-5203(03)00030-7; Bennett EP, 2006, LECT NOTES COMPUT SC, V3951, P508; Bokeloh M, 2009, COMPUT GRAPH FORUM, V28, P697, DOI 10.1111/j.1467-8659.2009.01410.x; Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103; BRONSTEIN MM, 2009, IJCV; Chen S, 2001, P INT C IM PROC, V3, P756; CHERTOK M, 2009, IEEE T PATT IN PRESS; Cheung W, 2007, I S BIOMED IMAGING, P720, DOI 10.1109/ISBI.2007.356953; Cornelius H, 2007, LECT NOTES COMPUT SC, V4522, P152; Derrode S, 2004, SIGNAL PROCESS, V84, P25, DOI 10.1016/j.sigpro.2003.07.006; Hays J, 2006, LECT NOTES COMPUT SC, V3952, P522; HOLDEN A, 1971, SHAPES SPACE SYMMETR; JEGER BE, 1967, VECTOR GEOMETRY LINE; Kazhdan M, 2002, LECT NOTES COMPUT SC, V2351, P642; Keller Y, 2005, IEEE T PATTERN ANAL, V27, P969, DOI 10.1109/TPAMI.2005.128; Keller Y, 2005, IEEE T IMAGE PROCESS, V14, P12, DOI 10.1109/TIP.2004.838692; Keller Y, 2006, IEEE T SIGNAL PROCES, V54, P4323, DOI 10.1109/TSP.2006.881217; Keller Y, 2006, IEEE T IMAGE PROCESS, V15, P2198, DOI 10.1109/TIP.2006.875227; Kim WY, 1999, IEEE T PATTERN ANAL, V21, P768, DOI 10.1109/34.784290; Kiryati N, 1998, INT J COMPUT VISION, V29, P29, DOI 10.1023/A:1008034529558; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucchese L, 2004, PATTERN RECOGN, V37, P2263, DOI 10.1016/j.patcog.2004.04.012; Martinet A, 2006, ACM T GRAPHIC, V25, P439, DOI 10.1145/1138450.1138462; Miller W., 1972, SYMMETRY GROUPS THEI; Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924; Ni D, 2009, COMPUT MED IMAG GRAP, V33, P559, DOI 10.1016/j.compmedimag.2009.05.006; Ovsjanikov M, 2008, COMPUT GRAPH FORUM, V27, P1341, DOI 10.1111/j.1467-8659.2008.01273.x; Pauly M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360642; Pauly PJ, 2008, RARITAN, V27, P1; Prasad VSN, 2004, IEEE T IMAGE PROCESS, V13, P1559, DOI 10.1109/TIP.2004.837564; Prasad VSN, 2005, IEEE I CONF COMP VIS, P954; Raviv D, 2007, IEEE I CONF COMP VIS, P2746; REISFELD D, 1995, INT J COMPUT VISION, V14, P119, DOI 10.1007/BF01418978; Shen DG, 2001, PATTERN RECOGN, V34, P1417, DOI 10.1016/S0031-3203(00)00079-0; STOY GA, 1994, GROUPS GEOMETRY; Thompson D. A. W, 1917, GROWTH FORM; TRUCCO E, 1998, INTRO TECHNIQUES 3D, P333; Weyl H., 1952, SYMMETRY	38	8	8	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2010	90	2					166	182		10.1007/s11263-010-0356-4	http://dx.doi.org/10.1007/s11263-010-0356-4			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	640XQ					2022-12-18	WOS:000281087900003
J	Fortuna, J; Martinez, AM				Fortuna, Jeff; Martinez, Aleix M.			Rigid Structure from Motion from a Blind Source Separation Perspective	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Bundle adjustment; Blind source separation; Subspace analysis; Bayesian analysis	INDEPENDENT COMPONENT ANALYSIS; ICA	We present an information theoretic approach to define the problem of structure from motion (SfM) as a blind source separation one. Given that for almost all practical joint densities of shape points, the marginal densities are non-Gaussian, we show how higher-order statistics can be used to provide improvements in shape estimates over the methods of factorization via Singular Value Decomposition (SVD), bundle adjustment and Bayesian approaches. Previous techniques have either explicitly or implicitly used only second-order statistics in models of shape or noise. A further advantage of viewing SfM as a blind source problem is that it easily allows for the inclusion of noise and shape models, resulting in Maximum Likelihood (ML) or Maximum a Posteriori (MAP) shape and motion estimates. A key result is that the blind source separation approach has the ability to recover the motion and shape matrices without the need to explicitly know the motion or shape pdf. We demonstrate that it suffices to know whether the pdf is sub- or super-Gaussian (i.e., semi-parametric estimation) and derive a simple formulation to determine this from the data. We provide extensive experimental results on synthetic and real tracked points in order to quantify the improvement obtained from this technique.	[Fortuna, Jeff; Martinez, Aleix M.] Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Martinez, AM (corresponding author), Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA.	fortunaj@ece.osu.edu; aleix@ece.osu.edu	Martinez, Aleix M/A-2380-2008		NSF [IIS 0713055]; NIH [R01 DC 005241]; NATIONAL INSTITUTE ON DEAFNESS AND OTHER COMMUNICATION DISORDERS [R01DC005241] Funding Source: NIH RePORTER	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NATIONAL INSTITUTE ON DEAFNESS AND OTHER COMMUNICATION DISORDERS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute on Deafness & Other Communication Disorders (NIDCD))	This research was supported in part by NSF grant IIS 0713055 and NIH grant R01 DC 005241.	Anandan P, 2002, INT J COMPUT VISION, V49, P101, DOI 10.1023/A:1020137420717; Belouchrani A, 1997, IEEE T SIGNAL PROCES, V45, P434, DOI 10.1109/78.554307; Brand M, 2005, PROC CVPR IEEE, P122; Buchanan AM, 2005, PROC CVPR IEEE, P316; Cam L.L., 1986, STAT SCI, V1, P78, DOI [10.1214/ss/1177013818, DOI 10.1214/SS/1177013818, DOI 10.1214/ss/1177013818]; Chen P, 2004, IEEE T PATTERN ANAL, V26, P1051, DOI 10.1109/TPAMI.2004.52; Cichocki A, 1998, NEUROCOMPUTING, V22, P113, DOI 10.1016/S0925-2312(98)00052-6; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ding L., 2008, P IEEE C COMP VIS PA; Eriksson J, 2004, IEEE SIGNAL PROC LET, V11, P601, DOI 10.1109/LSP.2004.830118; Faugeras O., 2001, 3 DIMENSIONAL COMPUT; FITZGIBBON AW, 1998, P EUR SIGN PROC C EU, P1261; Forsyth D. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P660, DOI 10.1109/ICCV.1999.791288; FORTUNA J, 2006, P 3 INT S 3D DAT PRO, P145; Fujiki J, 2004, LECT NOTES COMPUT SC, V3195, P750; Haykin S., 2000, BLIND SOURCE SEPARAT, V1; Hyvarinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71; Hyvarinen A, 1998, NEUROCOMPUTING, V22, P49, DOI 10.1016/S0925-2312(98)00049-6; Jia HJ, 2009, IEEE T PATTERN ANAL, V31, P841, DOI 10.1109/TPAMI.2008.122; Kahl F, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P572, DOI 10.1109/ICCV.2001.937677; Kahl F, 1999, INT J COMPUT VISION, V33, P163, DOI 10.1023/A:1008192713051; Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719; Lee TW, 1999, IEEE SIGNAL PROC LET, V6, P87, DOI 10.1109/97.752062; Morris DD, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P696, DOI 10.1109/ICCV.1998.710793; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Zhang L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P618, DOI 10.1109/ICCV.2003.1238405	27	8	9	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2010	88	3					404	424		10.1007/s11263-009-0313-2	http://dx.doi.org/10.1007/s11263-009-0313-2			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	580EL		Green Accepted			2022-12-18	WOS:000276429900004
J	Chow, CK; Yuen, SY				Chow, Chi Kin; Yuen, Shiu Yin			Recovering Shape by Shading and Stereo Under Lambertian Shading Model	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape from shading; Shape from stereo; Integration of shape modules; Epipolar geometry; Lambertian shading model	RECONSTRUCTION; IMAGES	A method that integrates shape from shading and stereo is reported for Lambertian objects. A rectification is proposed to convert any lighting direction from oblique to orthographic. A sparse stereo method is reported that directly uses depth information and has no foreshortening problem. The method completely solves three difficult problems in stereo, namely, recovering depth at occlusion; matching at places with similar shading and matching at smooth silhouettes. The method has been tested on both synthetic and real images. It shows superior performance compared with two recent stereo algorithms. It is also a method based on the physics of image formation.	[Chow, Chi Kin; Yuen, Shiu Yin] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China	City University of Hong Kong	Yuen, SY (corresponding author), City Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.	chowchi@cityu.edu.hk; kelviny.ee@cityu.edu.hk	Yuen, Shiu Yin/B-7569-2008	YUEN, Shiu Yin Kelvin/0000-0002-5889-8808	CityU [7002128]	CityU(City University of Hong Kong)	The work described in this article was fully supported by a grant from CityU (7002128).	ALOIMONOS J, 1989, INTEGRATION VISUAL M; Carr JC, 1997, IEEE T MED IMAGING, V16, P96, DOI 10.1109/42.552059; Chow CK, 2007, INTERNATIONAL CONFERENCE ON MACHINE VISION 2007, PROCEEDINGS, P7; Chow CK, 2007, FOURTH CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P289, DOI 10.1109/CRV.2007.43; Courteille F, 2004, INT C PATT RECOG, P277, DOI 10.1109/ICPR.2004.1334160; CRYER JE, 1995, PATTERN RECOGN, V28, P1033, DOI 10.1016/0031-3203(94)00183-M; DEVERNAY F, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P208, DOI 10.1109/CVPR.1994.323831; DUROU JD, 2004, 20042R IRIT; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; Forsyth David A, 2012, COMPUTER VISION MODE; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; Hartt K., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P53, DOI 10.1109/CVPR.1989.37828; Hougen D. R., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P148, DOI 10.1109/ICCV.1993.378225; IKEUCHI K, 1983, 744 AI MIT ART INT L; JIN H, 2004, P IEEE C COMP VIS PA, V1, P36; Jin HL, 2008, INT J COMPUT VISION, V76, P245, DOI 10.1007/s11263-007-0055-y; Jin HL, 2005, INT J COMPUT VISION, V63, P175, DOI 10.1007/s11263-005-6876-7; Kimmel R, 2001, J MATH IMAGING VIS, V14, P237, DOI 10.1023/A:1011234012449; Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431; Kobayashi F, 1998, J ELECTRON IMAGING, V7, P8, DOI 10.1117/1.482621; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82; Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668; Leclerc Y. G., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P552, DOI 10.1109/CVPR.1991.139752; MOSTAFA MGH, 1999, P ICIP 99 C IM P OCT, V3, P130; Prados E, 2005, INT J COMPUT VISION, V65, P97, DOI 10.1007/s11263-005-3844-1; Prados E, 2005, PROC CVPR IEEE, P870; Prados E, 2005, LECT NOTES COMPUT SC, V3752, P320; Prados E, 2006, J MATH IMAGING VIS, V25, P307, DOI 10.1007/s10851-006-6899-x; Samaras D, 2000, PROC CVPR IEEE, P480, DOI 10.1109/CVPR.2000.855858; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; Sethian JA, 2000, P NATL ACAD SCI USA, V97, P5699, DOI 10.1073/pnas.090060097; Sun CM, 2002, INT J COMPUT VISION, V47, P99, DOI 10.1023/A:1014585622703; Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509; Tankus A, 2005, INT J COMPUT VISION, V63, P21, DOI 10.1007/s11263-005-4945-6; Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185; TRUCCO E, 1998, INTRO TECHNIQUES 3 D; VERBEEK PW, 1990, PATTERN RECOGN LETT, V11, P681, DOI 10.1016/0167-8655(90)90102-8; Yuen SY, 2007, PATTERN RECOGN LETT, V28, P806, DOI 10.1016/j.patrec.2006.11.008; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658	41	8	9	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2009	85	1					58	100		10.1007/s11263-009-0240-2	http://dx.doi.org/10.1007/s11263-009-0240-2			43	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	470UY					2022-12-18	WOS:000268008000004
J	Yacoob, Y; Davis, L				Yacoob, Yaser; Davis, Larry S.			Segmentation using Appearance of Mesostructure Roughness	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Texture analysis; Image segmentation; Intrinsic images	TEXTURE; COLOR	This paper introduces mesostructure roughness as an effective cue in image segmentation. Mesostructure roughness corresponds to small-scale bumps on the macrostructure (i.e., geometry) of objects. Specifically, the focus is on the texture that is created by the projection of the mesostructure roughness on the camera plane. Three intrinsic images are derived: reflectance, smooth-surface shading and mesostructure roughness shading (meta-texture images). A constructive approach is proposed for computing a meta-texture image by preserving, equalizing and enhancing the underlying surface-roughness across color, brightness and illumination variations. We evaluate the performance on sample images and illustrate quantitatively that different patches of the same material, in an image, are normalized in their statistics despite variations in color, brightness and illumination. We develop an algorithm for segmentation of an image into patches that share salient mesostructure roughness. Finally, segmentation by line-based boundary-detection is proposed and results are provided and compared to known algorithms.	[Yacoob, Yaser; Davis, Larry S.] Univ Maryland, Comp Vis Lab, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Yacoob, Y (corresponding author), Univ Maryland, Comp Vis Lab, College Pk, MD 20742 USA.	yaser@umiacs.umd.edu			U.S. Office of Navy Research (ONR) [N000140610102]	U.S. Office of Navy Research (ONR)(Office of Naval Research)	This work has been supported in part by the U.S. Office of Navy Research (ONR), grant N000140610102.	ADELSON EH, 2004, J VISION, V4; ARBELAEZ P, 2006, WORKSH PERC ORG COMP; Barrow H., 1978, COMPUT VIS SYST, V2, P2; Chen JQ, 2005, IEEE T IMAGE PROCESS, V14, P1524, DOI 10.1109/TIP.2005.852204; Chen T, 2006, P IEEE C COMP VIS PA, P1825, DOI [10.1109/CVPR.2006.182, DOI 10.1109/CVPR.2006.182]; Cour T, 2005, PROC CVPR IEEE, P1124; Cula OG, 2004, INT J COMPUT VISION, V59, P33, DOI 10.1023/B:VISI.0000020670.05764.55; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; Finlayson GD, 2004, LECT NOTES COMPUT SC, V3023, P582; HORPRASERT T, 1999, INT C COMP VIS FRAME; Koenderink JJ, 1996, J OPT SOC AM A, V13, P452, DOI 10.1364/JOSAA.13.000452; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638; Liu XG, 2001, COMP GRAPH, P97; LUNDBERG AJ, 2001, INT C COMP VIS, V1, P225; Maenpaa T, 2004, PATTERN RECOGN, V37, P1629, DOI 10.1016/j.patcog.2003.11.011; Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684; Ragheb H, 2003, PROC CVPR IEEE, P177; Tappen M.F., 2006, 2006 IEEE COMPUTER S, V2, P1992; Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Varma M, 2002, LECT NOTES COMPUT SC, V2352, P255	24	8	8	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2009	83	3					248	273		10.1007/s11263-009-0224-2	http://dx.doi.org/10.1007/s11263-009-0224-2			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	424EI		Green Submitted			2022-12-18	WOS:000264547900003
J	Wu, FC; Wang, ZH; Hu, ZY				Wu, F. C.; Wang, Z. H.; Hu, Z. Y.			Cayley Transformation and Numerical Stability of Calibration Equation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Camera calibration; The absolute conic; Calibration equation; Cayley transformation; Numerical stability	SELF-CALIBRATION; CAMERA CALIBRATION	The application of Cayley transformation to enhance the numerical stability of camera calibration is investigated. First, a new calibration equation, called the standard calibration equation, is introduced using the Cayley transformation and its analytical solution is obtained. The standard calibration equation is equivalent to the classical calibration equation, but it exhibits remarkable better numerical stability. Second, a one-parameter calibration family, called the Cayley calibration family which is equivalent to the standard calibration equation, is obtained using also the Cayley transformation and it is found that this family is composed of those infinite homographies whose rotation has the same axis with the rotation between the two given views. The condition number of equations in the Cayley calibration family varies with the parameter value, and an algorithm to determine the best parameter value is provided. Third, the generalized Cayley calibration families equivalent to the standard calibration equation are also introduced via generalized Cayley transformations. An example of the generalized Cayley transformations is illustrated, called the S-Cayley calibration family. As in the Cayley calibration family, the numerical stability of equations in a generalized Cayley calibration family also depends on the parameter value. In addition, a more generic calibration family is also proposed and it is proved that the standard calibration equation, the Cayley calibration family and the S-Cayley calibration family are all some special cases of this generic calibration family.	[Wu, F. C.; Wang, Z. H.; Hu, Z. Y.] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS	Wu, FC (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, POB 2728, Beijing 100190, Peoples R China.	fcwu@nlpr.ia.ac.cn; zhwang@nlpr.ia.ac.cn; huzy@nlpr.ia.ac.cn			National Natural Science Foundation of China [60835003, 60575019]; National High-Tech Research and Development Program of China [2006AA01Z116]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National High-Tech Research and Development Program of China(National High Technology Research and Development Program of China)	We wish to thank the anonymous reviewers for their inspiring comments and suggestions on this paper. Also, we gratefully acknowledge the support from the National Natural Science Foundation of China (60835003, 60575019) and the National High-Tech Research and Development Program of China (2006AA01Z116).	BELL S, 1992, CAYLEY TRANSFORM POT; BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855; Chris H., 1988, P 4 ALVEY VISION C, P189; FAIG W, 1975, PHOTOGRAMM ENG REM S, V41, P1479; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Hartley RI, 1997, INT J COMPUT VISION, V22, P5, DOI 10.1023/A:1007957826135; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P133, DOI 10.1109/34.574792; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luong QT, 1997, INT J COMPUT VISION, V22, P261, DOI 10.1023/A:1007982716991; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; Oliensis J, 2007, IEEE T PATTERN ANAL, V29, P2217, DOI 10.1109/TPAMI.2007.1132; Pollefeys M, 1999, IEEE T PATTERN ANAL, V21, P707, DOI 10.1109/34.784285; Pollefeys M., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P31, DOI 10.1007/BFb0015521; PONCE J, 2000, P COMP VIS PATT REC, P780; Press WH, 1988, NUMERICAL RECIPES C; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; Sturm P., 1999, P IEEE C COMP VIS PA, P432, DOI DOI 10.1109/CVPR.1999.786974; Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388; Tsai R.Y., 1986, P IEEE C COMP VIS PA, P364; Wang L., 2007, IEEE INT C COMP VIS; Wu FC, 2005, PATTERN RECOGN, V38, P755, DOI 10.1016/j.patcog.2004.11.005; Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	26	8	11	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2009	82	2					156	184		10.1007/s11263-008-0193-x	http://dx.doi.org/10.1007/s11263-008-0193-x			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	408FP					2022-12-18	WOS:000263421200003
J	Ikeuchi, K; Klinker, G; Ohta, Y; Szeliski, R				Ikeuchi, Katsushi; Klinker, Gudrun; Ohta, Yuichi; Szeliski, Richard			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									Univ Tokyo, Grad Sch Interdisciplinary Informat Studies, Tokyo 1538505, Japan; Tech Univ Munich, Fachbereich Informat, D-85748 Garching, Germany; Univ Tsukuba, Dept Intelligent Interact Technol, Tsukuba, Ibaraki 3058573, Japan; Microsoft Res, Redmond, WA 98052 USA	University of Tokyo; Technical University of Munich; University of Tsukuba; Microsoft	Ikeuchi, K (corresponding author), Univ Tokyo, Grad Sch Interdisciplinary Informat Studies, 4-6-1 Komaba, Tokyo 1538505, Japan.	ki@cvl.iis.u-tokyo.ac.jp; klinker@in.tum.de; ohta@iit.tsukuba.ac.jp; szeliski@microsoft.com	Ohta, Yuichi/R-1471-2019	Ohta, Yuichi/0000-0001-7323-3532				Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394; Kanade T., 1973, THESIS KYOTO U; KLINKER GJ, 1988, INT J COMPUT VISION, V2, P7, DOI 10.1007/BF00836279; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639	4	8	8	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2007	75	1					1	2		10.1007/s11263-007-0047-y	http://dx.doi.org/10.1007/s11263-007-0047-y			2	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	197RM					2022-12-18	WOS:000248574200001
J	Knight, J; Reid, I				Knight, Joss; Reid, Ian			Automated alignment of robotic pan-tilt camera units using vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						alignment; active vision; calibration	HEAD-EYE PLATFORM; CALIBRATION	In this paper we show how to carry out an automatic alignment of a pan-tilt camera platform with its natural coordinate frame, using only images obtained from the cameras during controlled motion of the unit. An active camera in aligned orientation represents the zero position for each axis, and allows axis odometry to be referred to a fixed reference frame; such referral is otherwise only possible using mechanical means, such as end-stops, which cannot take account of the unknown relationship between the camera coordinate frame and its mounting. The algorithms presented involve the calculation of two-view transformations (homographies or epipolar geometry) between pairs of images related by controlled rotation about individual head axes. From these relationships, which can be calculated linearly or optimised iteratively, an invariant line to the motion can be extracted which represents an aligned viewing direction. We present methods for general and degenerate motion (translating or non-translating), and general and degenerate scenes (non-planar and planar, but otherwise unknown), which do not require knowledge of the camera calibration, and are resistant to lens distortion non-linearity. Detailed experimentation in simulation, and in real scenes, demonstrate the speed, accuracy, and robustness of the methods, with the advantages of applicability to a wide range circumstances and no need to involve calibration objects or complex motions. Accuracy of within half a degree can be achieved with a single motion, and we also show how to improve on this by incorporating images from further motions, using a natural extension of the basic algorithm.	Univ Oxford, Dept Engn Sci, Robot Res Grp, Act Vis Lab, Oxford OX1 3PJ, England	University of Oxford	Knight, J (corresponding author), Univ Oxford, Dept Engn Sci, Robot Res Grp, Act Vis Lab, Parks Rd, Oxford OX1 3PJ, England.	joss@robots.ox.ac.uk; ian@robots.ox.ac.uk		Reid, Ian/0000-0001-7790-6423				ARMSTRONG MN, 1996, THESIS U OXFORD; BEARDSLEY PA, 1995, P JOINT EUR CHIN WOR; DAVISON A, 1998, THESIS OXFORD U DEP; DESOUZA GN, 2002, P IEEE INT C ROB AUT, V4, P3336; HARIS CG, 1988, P 4 ALV VIS C MANCH, P147; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HAYMAN E, 2000, P 15 IEEE INT C PATT, V1, P80; Horaud R, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P96, DOI 10.1109/ICCV.1998.710706; Knight J, 2003, PROC CVPR IEEE, P503; KNIGHT J, 2002, THESIS OXFORD U DEP; KNIGHT J, 2000, P IEEE INT C ROB AUT, V4, P3203; KNIGHT R, 1800, PHILOS MAG, V6, P1; Li MX, 1998, IEEE T ROBOTIC AUTOM, V14, P153, DOI 10.1109/70.660862; MA SD, 1996, IEEE T ROBOT AUTOMAT, V12, P114, DOI DOI 10.1109/70.481755; MAYBANK S, 1993, THEORY RECONSTRUCTIO; McLauchlan PF, 1996, IEEE T PATTERN ANAL, V18, P15, DOI 10.1109/34.476007; Murray DW, 1997, PERCEPTION, V26, P1519, DOI 10.1068/p261519; MURRAY DW, 1996, P 7 BRIT MACH VIS C, P635; Reid ID, 1996, IMAGE VISION COMPUT, V14, P635, DOI 10.1016/0262-8856(96)01095-5; SAMPSON PD, 1982, COMPUT VISION GRAPH, V18, P97, DOI 10.1016/0146-664X(82)90101-0; SHARKEY PM, 1993, MECHATRONICS, V3, P517, DOI 10.1016/0957-4158(93)90021-S; Tordoff B, 2000, INT C PATT RECOG, P423, DOI 10.1109/ICPR.2000.905367; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; VIEVILLE T, 1996, P 4 EUR C COMP VIS, P207; Zisserman A., 1995, Proceedings IEEE Workshop on Representation of Visual Scenes (In Conjunction with ICCV'95) (Cat. No.95TB8126), P93, DOI 10.1109/WVRS.1995.476857	25	8	9	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2006	68	3					219	237		10.1007/s11263-005-5032-8	http://dx.doi.org/10.1007/s11263-005-5032-8			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	064NU					2022-12-18	WOS:000239097200001
J	Iwata, H				Iwata, H			Full-surround image display technologies	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						immersive projection display; polyhedral screen; spherical screen; virtual reality		This paper presents two methods for developing full-surround displays: rear-projection-based "Garnet Vision" and specialized spherical screen named "Ensphered Vision." The Garnet Vision is a closed polyhedron screen. A viewer stands inside the polyhedron so that image covers full solid angle around the viewer. Optimum configuration of polyhedron is determined by two criteria: pixel efficiency and space efficiency. The criteria maximizes space utility of the display. Through examination of these criteria, rhombic dodecahedron is chosen. A dodecahedron screen with twelve projectors is built, in which a viewer can stand. Ensphered Vision is an image display system for full-surround spherical screen. Sphere is an ideal shape of a screen that covers human visual field. Distance between eyes and screen should be constant while a viewer rotates the head. We use single projector and a convex mirror in order to display seamless image. The optical system employs two mirrors: a plain mirror and a spherical convex mirror. The spherical convex mirror scatters the light from the projector in the spherical screen. The flat mirror provides the viewer to the sweet-spot where he/she can see the image from the center of the sphere. This optical configuration enables seamless wide-angle image in a very limited space.	Univ Tsukuba, Inst Engn Mech & Syst, Tsukuba, Ibaraki 305, Japan	University of Tsukuba	Iwata, H (corresponding author), Univ Tsukuba, Inst Engn Mech & Syst, Tsukuba, Ibaraki 305, Japan.							COURCHESNE L, 2000, SIGGRAPH 2000 C ABST; CRUZNEIRA C, 1993, P SIGGRAPH 93; Hashimoto W, 1999, T VIRTUAL REALITY SO, V4, P479; IWATA H, 1996, P ICAT 96; IWATA H, 1999, SIGGRAPH 99 C ABSTR; MAX N, 1982, COMPUTER GRAPHICS, V16, P208; PUGH A, 1976, POLYHEDRA; SMITH EL, 1998, SIGGRAPH 98; TEITEL MA, 1990, P SOC PHOTO-OPT INS, V1256, P168, DOI 10.1117/12.19902; Wenninger M.J., 1971, POLYHEDRON MODELS	10	8	8	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	2004	58	3					227	235		10.1023/B:VISI.0000019685.36452.55	http://dx.doi.org/10.1023/B:VISI.0000019685.36452.55			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	836GD					2022-12-18	WOS:000222542900004
J	Shum, HY; Wang, LF; Chai, JX; Tong, X				Shum, HY; Wang, LF; Chai, JX; Tong, X			Rendering by manifold hopping	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						manifold mosaic; concentric mosaic; perceptually smooth navigation; warping; plenoptic functions; image-based rendering; video computing		In this paper, we present a novel image-based rendering technique, which we call manifold hopping. Our technique provides users with perceptually continuous navigation by using only a small number of strategically sampled manifold mosaics or multiperspective panoramas. Manifold hopping has two modes of navigation: moving continuously along any manifold, and discretely between manifolds. An important feature of manifold hopping is that significant data reduction can be achieved without sacrificing output visual fidelity, by carefully adjusting the hopping intervals. A novel view along the manifold is rendered by locally warping a single manifold mosaic using a constant depth assumption, without the need for accurate depth or feature correspondence. The rendering errors caused by manifold hopping can be analyzed in the signed Hough ray space. Experiments with real data demonstrate that we can navigate smoothly in a virtual environment with as little as 88k data compressed from 11 concentric mosaics.	Microsoft Res Asia, Beijing 100080, Peoples R China	Microsoft; Microsoft Research Asia	Shum, HY (corresponding author), Microsoft Res Asia, 49 Zhichun Rd, Beijing 100080, Peoples R China.	hshum@microsoft.com						Avidan S, 1997, PROC CVPR IEEE, P1034, DOI 10.1109/CVPR.1997.609457; CHAI JX, 2000, P ECCV2000 WORKSH SM; CHAI JX, 2000, P SIGGRAPH 2000; Chang CF, 1999, COMP GRAPH, P291, DOI 10.1145/311535.311571; Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153; Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; Gupta R, 1997, IEEE T PATTERN ANAL, V19, P963, DOI 10.1109/34.615446; KANG SB, 1999, SPIE, V3641, P2; Landy M.S., 1991, COMPUTATIONAL MODELS; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Lippman A., 1980, Computer Graphics, V14, P32, DOI 10.1145/965105.807465; Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292; McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398; MCMILLAN L, 1999, TR97013 UNC; OLIVEIRA M, 1999, TR99015 UNC; Peleg S, 1997, PROC CVPR IEEE, P338, DOI 10.1109/CVPR.1997.609346; Rademacher P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P199, DOI 10.1145/280814.280871; Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543; Seitz S. M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P21, DOI 10.1145/237170.237196; Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882; Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573; SHUM HY, 1999, P INT C COMP VIS, P14; WOOD DN, 1997, ANN C SERIES ACM SIG, P243; Zheng J. Y., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P161, DOI 10.1109/ICPR.1990.118082; Zorin D., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P257, DOI 10.1145/218380.218449	26	8	9	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2002	50	2					185	201		10.1023/A:1020398016678	http://dx.doi.org/10.1023/A:1020398016678			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	597DX					2022-12-18	WOS:000178207800006
J	Agrawal, M; Davis, LS				Agrawal, M; Davis, LS			Trinocular stereo using shortest paths and the ordering constraint	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						computer vision; trinocular stereo; dynamic programming; ordering constraint		This paper describes a new algorithm for disparity estimation using trinocular stereo. The three cameras are placed in a right angled configuration. A graph is then constructed whose nodes represent the individual pixels and whose edges are along the epipolar lines. Using the well known uniqueness and ordering constraint for pair by pair matches simultaneously, a path with the least matching cost is found using dynamic programming and the disparity filled along the path. This process is repeated iteratively until the disparity at all the pixels are filled up. To demonstrate the effectiveness of our approach, we present results from real world images and compare it with the traditional line by line stereo using dynamic programming.	Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Agrawal, M (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	mla@umiacs.umd.edu; lsd@umiacs.umd.edu						AGRAWAL M, 2000, P IND C VIS GRAPH IM; Agrawal Motilal, 2001, P IEEE C COMP VIS PA; Birchfield S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P489, DOI 10.1109/ICCV.1999.791261; Birchfield S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1073, DOI 10.1109/ICCV.1998.710850; Boykov Y, 1998, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.1998.698673; Boykov Y., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P377, DOI 10.1109/ICCV.1999.791245; Cox IJ, 1996, COMPUT VIS IMAGE UND, V63, P542, DOI 10.1006/cviu.1996.0040; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; OHTA Y, 1986, ICPR86, P519; Pritchett P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P754, DOI 10.1109/ICCV.1998.710802; Roy S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P492, DOI 10.1109/ICCV.1998.710763; STEWART CV, 1988, P 2 INT C COMP VIS, P134, DOI DOI 10.1109/CCV.1988.589983; [No title captured]	15	8	8	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					43	50		10.1023/A:1017478504047	http://dx.doi.org/10.1023/A:1017478504047			8	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN		Green Submitted			2022-12-18	WOS:000174354700003
J	Shan, Y; Zhang, ZY				Shan, Y; Zhang, ZY			New measurements and corner-guidance for curve matching with probabilistic relaxation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						stereoscopic vision; curve matching; probabilistic relaxation; image features; edge matching; epipolar geometry	LABELING PROCESSES; EPIPOLAR GEOMETRY; STEREO	Reliable curve matching is a difficult yet important problem in many vision-based applications including image-based modeling. We describe in this paper two aspects of our research in this area: a new algorithm for curve matching (including lines) within a probabilistic relaxation framework, and an approach of incorporating previously matched points/corners to guide curve matching. We propose similarity-invariant unary and binary measurements suitable for curves, and introduce an additional measurement to model the uncertainty of the binary measurements. The uncertainty measure is proven to be very important in computing the matching support from neighboring matches. We also show how to use a set of previously matched points/corners to guide the curve matching. The role of the corner guidance is explicitly modeled by a set of unary measurements and a similarity function under the same relaxation framework. Preprocessing techniques contributing to the success of our curve matching techniques are also developed and discussed. Experiments with complex real scenes show that the rate of correct matching is higher than 98%.	Microsoft Corp, Redmond, WA 98052 USA	Microsoft	Shan, Y (corresponding author), Microsoft Corp, 1 Microsoft Way, Redmond, WA 98052 USA.							ALIBHAI S, 2000, P 6 EUR C COMP VIS D, V1, P314; AYACHE N, 1987, INT J COMPUT VISION, V1, P107, DOI 10.1007/BF00123161; Ayache N., 1990, STEREOVISION SENSOR; Belhumeur PN, 1996, INT J COMPUT VISION, V19, P237, DOI 10.1007/BF00055146; Candocia F, 1997, IEEE T IMAGE PROCESS, V6, P1460, DOI 10.1109/83.624977; CHRISTMAS WJ, 1995, IEEE T PATTERN ANAL, V17, P749, DOI 10.1109/34.400565; HUMMEL RA, 1983, IEEE T PATTERN ANAL, V5, P267, DOI 10.1109/TPAMI.1983.4767390; Kittler J., 1989, International Journal of Pattern Recognition and Artificial Intelligence, V3, P29, DOI 10.1142/S021800148900005X; MEDIONI G, 1985, COMPUT VISION GRAPH, V31, P2, DOI 10.1016/S0734-189X(85)80073-6; NASRABADI NM, 1992, IEEE T PATTERN ANAL, V14, P566, DOI 10.1109/34.134060; Pelillo M, 1997, J MATH IMAGING VIS, V7, P309, DOI 10.1023/A:1008255111261; POLLARD S, 1991, 3D MODEL RECOGNITION, P11; POLLARD SB, 1991, 3D MODEL RECOGNITION, P33; Robert L., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P57, DOI 10.1109/CVPR.1991.139661; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519; SCHMID C, 1997, CVPR, P666; Xu A., 2018, KINETIC THEORY, DOI [10.1007/978-94-015-8668-9, DOI 10.1007/978]; ZHANG Z, 1992, 3D DYNAMIC SCENE ANA; ZHANG Z, 2001, LECT NOTES COMPUTER, V2018, P68; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561; ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4	21	8	12	3	10	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2002	46	2					157	171		10.1023/A:1013591914229	http://dx.doi.org/10.1023/A:1013591914229			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	507LN					2022-12-18	WOS:000173030100003
J	Streekstra, GJ; Van Den Boomgaard, R; Smeulders, AWM				Streekstra, GJ; Van Den Boomgaard, R; Smeulders, AWM			Scale dependency of image derivatives for feature measurement in curvilinear structures	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						feature measurement; scale; image derivatives; curvilinear structures; curvature; center line detection; localization; bias removal; diameter measurement	TRACKING	Extraction of image features is a crucial step in many image analysis tasks. In feature extraction methods Gaussian derivative kernels are frequently utilized. Blurring of the image due to convolution with these kernels gives rise to feature measures different from the intended value in the original image. We propose to solve this problem by explicitly modeling the scale dependency of derivatives combined with measurement of derivatives at multiple scales. This approach is illustrated in methods for feature measurement in curvilinear structures. Results in 3D Confocal Images confirm that modelling of scale behavior of derivatives results in improved methods for center line localization in curved line structures and enables curvature and diameter measurement.	Acad Med Ctr, Dept Med Phys, NL-1100 DE Amsterdam, Netherlands; Univ Amsterdam, Fac Sci, NL-1098 SJ Amsterdam, Netherlands	University of Amsterdam; Academic Medical Center Amsterdam; University of Amsterdam	Streekstra, GJ (corresponding author), Acad Med Ctr, Dept Med Phys, POB 22700, NL-1100 DE Amsterdam, Netherlands.	geert@wins.uva.nl; rein@wins.ura.nl; smeulders@science.ura.nl						BEVACQUA G, 1987, COMPUT VISION GRAPH, V40, P219, DOI 10.1016/S0734-189X(87)80116-0; BRAKENHOFF GJ, 1979, J MICROSC-OXFORD, V117, P219, DOI 10.1111/j.1365-2818.1979.tb01178.x; CAPOWSKY JJ, 1989, COMPUTER TECHNIQUES; Cesar RM, 1997, REV SCI INSTRUM, V68, P2177, DOI 10.1063/1.1148112; COLLOREC R, 1988, PATTERN RECOGN LETT, V8, P353, DOI 10.1016/0167-8655(88)90086-4; DERICHE R, 1993, INT J COMPUT VISION, V10, P101, DOI 10.1007/BF01420733; EBERLY D, 1996, COMPUTATIONAL IMAGIN, V7; FLORACK LMJ, 1992, IMAGE VISION COMPUT, V10, P376, DOI 10.1016/0262-8856(92)90024-W; Frangi AF, 1999, IEEE T MED IMAGING, V18, P946, DOI 10.1109/42.811279; Gonzalez R. C., 1992, DIGITAL IMAGING PROC; HARALICK RM, 1983, COMPUT VISION GRAPH, V22, P28, DOI 10.1016/0734-189X(83)90094-4; HOUTSMULLER AB, 1993, CYTOMETRY, V14, P501, DOI 10.1002/cyto.990140509; JONK A, 1997, LINE TRACKER INTERNA; KOENDERINK JJ, 1994, PATTERN RECOGN LETT, V15, P439, DOI 10.1016/0167-8655(94)90134-1; KOLLER TM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P864, DOI 10.1109/ICCV.1995.466846; KUIJPER A, 1999, P 2 INT C SCAL SPAC, P318; Lindeberg T, 1996, PROC CVPR IEEE, P465, DOI 10.1109/CVPR.1996.517113; Lindeberg T., 1992, Journal of Mathematical Imaging and Vision, V1, P65, DOI 10.1007/BF00135225; Lindeberg T., 1994, SCALE SPACE THEORY C; LORENZ C, 1997, P 1 JOINT C COMP VIS, P233; Maintz JBA, 1996, IEEE T PATTERN ANAL, V18, P353, DOI 10.1109/34.491617; NOORDMANS HJ, 1997, THESIS AMSTERDAM; Sato Y, 1998, Med Image Anal, V2, P143, DOI 10.1016/S1361-8415(98)80009-1; Sporring Jon, 1997, GAUSSIAN SCALE SPACE; STAAL JJ, 1999, P 2 INT C SCAL SPAC, P105; Steger C, 1998, IEEE T PATTERN ANAL, V20, P113, DOI 10.1109/34.659930; Steger C., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P251, DOI 10.1109/ICPR.1996.546827; STEGER C, 1998, THESIS MUENCHEN; STREEKSTRA GJ, 1999, P 2 INT C SCAL SPAC, P501; STREEKSTRA GJ, 2000, P 6 EUR C COMP VIS D, P856; VANVLIET LJ, 1993, THESIS DELFT	31	8	8	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		2001	42	3					177	189		10.1023/A:1011191615794	http://dx.doi.org/10.1023/A:1011191615794			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	443FU					2022-12-18	WOS:000169330800003
J	Hornegger, J; Niemann, H				Hornegger, J; Niemann, H			Probabilistic modeling and recognition of 3-D objects	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						statistical object recognition; pose estimation; expectation maximization algorithm; mixture densities; hidden Markov models; marginalization; global optimization; adaptive random search		This paper introduces a uniform statistical framework for both 3-D and 2-D object recognition using intensity images as input data. The theoretical part provides a mathematical tool for stochastic modeling. The algorithmic part introduces methods for automatic model generation, localization, and recognition of objects. 2-D images are used for learning the statistical appearance of 3-D objects; both the depth information and the matching between image and model features are missing for model generation. The implied incomplete data estimation problem is solved by the Expectation Maximization algorithm. This leads to a novel class of algorithms for automatic model generation from projections. The estimation of pose parameters corresponds to a non-linear maximum likelihood estimation problem which is solved by a global optimization procedure. Classification is done by the Bayesian decision rule. This work includes the experimental evaluation of the various facets of the presented approach. An empirical evaluation of learning algorithms and the comparison of different pose estimation algorithms show the feasibility of the proposed probabilistic framework.	Univ Erlangen Nurnberg, Lehrstuhl Mustererkennung Informat 5, D-91058 Erlangen, Germany	University of Erlangen Nuremberg	Hornegger, J (corresponding author), Univ Erlangen Nurnberg, Lehrstuhl Mustererkennung Informat 5, Martensstr 3, D-91058 Erlangen, Germany.	hornegger@informatik.uni-erlangen.de; niemann@informatik.uni-erlangen.de	Hornegger, Joachim/H-2465-2017	Hornegger, Joachim/0000-0002-1834-8844				Altmann S. L., 1986, ROTATIONS QUATERNION; Anderson T.W, 1958, INTRO MULTIVARIATE S; [Anonymous], 1998, STAT METHODS SPEECH; Bishop, 1995, NEURAL NETWORKS PATT; BOENDER CGE, 1982, MATH PROGRAM, V22, P125, DOI 10.1007/BF01581033; CAGLIOTI V, 1994, IEEE T PATTERN ANAL, V16, P524, DOI 10.1109/34.291444; CORANA A, 1987, ACM T MATH SOFTWARE, V13, P262, DOI 10.1145/29380.29864; DUBUISSON B, 1993, PATTERN RECOGN, V26, P155, DOI 10.1016/0031-3203(93)90097-G; ERMAKOV SM, 1983, PROBABILITY THEORY A, V28, P129; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Horn B., 1986, ROBOT VISION, P1; HORNEGGER J, 1994, INT C PATT RECOG, P557, DOI 10.1109/ICPR.1994.577035; Hornegger J, 1997, INT CONF ACOUST SPEE, P3173, DOI 10.1109/ICASSP.1997.595466; HORNEGGER J, 1999, HDB COMPUTER VISION, V2; HORNEGGER J, 1996, STAT MODELING CLASSI; HUANG XD, 1990, INFORMATION TECHNOLO, V7; JAIN AK, 1993, 3 DIMENSINAL OBJECT; KANATANI K, 1993, OXFORD ENG SCI SERIE, V37; Kanatani K., 1996, STAT OPTIMIZATION GE, V18; Kittler J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P666, DOI 10.1109/ICCV.1993.378148; Lawley DN, 1971, FACTOR ANAL STAT MET, V2nd; Li XL, 2000, IEEE T PATTERN ANAL, V22, P371, DOI 10.1109/34.845379; McLachlan GJ., 1996, WILEY SERIES PROBABI; Mockus J., 1989, BAYESIAN APPROACH GL; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; NIEMANN H, 2000, IN PRESS INT J PATTE; Paulus D.W., 1998, APPL PATTERN RECOGNI; PONCE J, 1996, LECT NOTES COMPUTER, V1144; POPE A, 1995, THESIS U BRIT COLUMB; Press W.H., 1988, NUMERICAL RECIPES AR; Ripley BD., 1996; SCHELE B, 1997, THESIS I NATL POLYTE; Schiele B, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P249, DOI 10.1109/ICCV.1998.710726; TIMMER GT, 1984, THESIS ERASMUS U ROT; Trucco E., 1998, INTRO TECHNIQUES 3D; Ullman S., 1996, HIGH LEVEL VISION OB; Wells WM, 1997, INT J COMPUT VISION, V21, P63, DOI 10.1023/A:1007923522710; WELLS WM, 1993, THESIS MIT CAMBRIDGE; WINKLER G, 1995, IMAGE ANAL RANDOM FI, V27; ZHU SZ, 1998, INT J COMPUT VISION, V27, P127	41	8	8	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2000	39	3					229	251		10.1023/A:1026515828914	http://dx.doi.org/10.1023/A:1026515828914			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	382EJ					2022-12-18	WOS:000165809800003
J	Freedman, D; Brandstein, MS				Freedman, D; Brandstein, MS			Contour tracking in clutter: A subset approach	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						contour tracking; low-level vision; visual clutter; subset learning; iterative minimization; Legendre polynomials; morphological filters	SHAPE	A new method for tracking contours of moving objects in clutter is presented. For a given object, a model of its contours is learned from training data in the form of a subset of contour space. Greater complexity is added to the contour model by analyzing rigid and non-rigid transformations of contours separately. In the course of tracking, multiple contours may be observed due to the presence of extraneous edges in the form of clutter; the learned model guides the algorithm in picking out the correct one. The algorithm, which is posed as a solution to a minimization problem, is made efficient by the use of several iterative schemes. Results applying the proposed algorithm to the tracking of a flexing finger and to a conversing individual's lips are presented.	Harvard Univ, Div Engn & Appl Sci, Cambridge, MA 02138 USA	Harvard University	Freedman, D (corresponding author), Harvard Univ, Div Engn & Appl Sci, Cambridge, MA 02138 USA.	freedman@hrl.harvard.edu; msb@hrl.harvard.edu						AMINI AA, 1988, P 2 INT C COMP VIS, P95; AYACHE N, 1992, ACTIVE VISION, P285; BLAKE A, 1995, ARTIF INTELL, V78, P179, DOI 10.1016/0004-3702(95)00032-1; BLAKE A, 1993, INT J COMPUT VISION, V11, P127, DOI 10.1007/BF01469225; BREGLER C, 1994, P IEEE INT C AC SPEE, V2, P669; BROCKETT R, 1994, IEEE DECIS CONTR P, P3247, DOI 10.1109/CDC.1994.411640; COOTES TF, 1994, IMAGE VISION COMPUT, V12, P355, DOI 10.1016/0262-8856(94)90060-4; DALTON B, 1995, P NATO ASI C SPEECHR; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; KASS M, 1987, P 1 INT C COMP VIS; Kaucic R., 1996, P EUR C COMP VIS, P376; LIPSON P, 1990, P 1 EUR C COMP VIS; LUETTIN J, 1996, P IEEE INT C AC SPEE, V2, P817; MAK MW, 1994, SPEECH COMMUN, V14, P279, DOI 10.1016/0167-6393(94)90067-1; SULLIVAN GD, 1992, PHILOS T ROY SOC B, V337, P361, DOI 10.1098/rstb.1992.0114; XU G, 1993, P 4 INT C COMP VIS B; YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169	17	8	8	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2000	38	2					173	186		10.1023/A:1008157803698	http://dx.doi.org/10.1023/A:1008157803698			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	345GP					2022-12-18	WOS:000088808500004
J	Vieville, T; Lingrand, D				Vieville, T; Lingrand, D			Using specific displacements to analyze motion without calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						structure and motion; specific displacements; self-calibration	EUCLIDEAN RECONSTRUCTION; UNCALIBRATED IMAGES; SELF-CALIBRATION; SEQUENCES; VISION; POINT	Considering the field of un-calibrated image sequences and self-calibration, this paper analyzes the use of specific displacements (such as fixed axis rotation, pure translations,...) or specific sets of camera parameters. This allows to induce affine or metric constraints, which can lead to self-calibration and 3D reconstruction. A uniformed formalism for such models already developed in the literature plus some novel models are developed here. A hierarchy of special situations is described, in order to tailor the most appropriate camera model to either the actual robotic device supporting the camera, or to tailor the fact we only have a reduced set of data available. This visual motion perception module leads to the estimation of a minimal 3D parameterization of the retinal displacement for a monocular visual system without calibration, and leads to self-calibration and 3D dynamic analysis. The implementation of these equations is analyzed and experimented.	INRIA, F-06902 Valbonne, France	Inria	Vieville, T (corresponding author), INRIA, BP93, F-06902 Valbonne, France.	vthierry@sophia.inria.fr						BEARDSLEY P, 1994, LECT NOTES COMPUTER, V2, P85; BEARDSLEY PA, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P58, DOI 10.1109/ICCV.1995.466806; BOLLES RC, 1981, INT JOINT C ART INT, P637; CHRISTY S, 1994, 2421 INRIA; ENCISO R, 1996, WORKSH ALCATECH DENM, P21; ENCISO R, 1995, NOTES COMPUTER SCI, V976, P307; FAUGERAS O, 1995, J OPT SOC AM A, V12, P465, DOI 10.1364/JOSAA.12.000465; FAUGERAS OD, 1990, INT J COMPUT VISION, V4, P225, DOI 10.1007/BF00054997; FAUGERAS OD, 1157 INRIA; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Harris C, 1988, P 4 ALV VIS C, P147, DOI DOI 10.5244/C.2.23; HARTLEY R, 1994, LECT NOTES COMPUTER, V800, P471; HARTLEY RI, 1994, IEEE T PATTERN ANAL, V16, P1036, DOI 10.1109/34.329005; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P882, DOI 10.1109/ICCV.1995.466843; HEYDEN A, 1997, CVPR, P438; Huber P., 1981, ROBUST STAT; KANATANI K, 1992, GEOMETRIC COMPUTATIO; LAVEST JM, 1993, IEEE T ROBOTIC AUTOM, V9, P196, DOI 10.1109/70.238283; LINGRAND D, 1996, 13 ICPR, V1, P810; Luong Q. T., 1992, THESIS U PARIS SUD; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; Maybank S. J., 1990, International Journal of Imaging Systems and Technology, V2, P380, DOI 10.1002/ima.1850020412; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; POLLEFEYS M, 1995, P 2 AS C COMP VIS SI, V2, P6; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; SHAPIRO L, 1993, 197493 OUEL U OXF DE; SHASHUA A, 1994, IEEE T PATTERN ANAL, V16, P778, DOI 10.1109/34.308472; SHASHUA A, 1994, P INT C COMP VIS PAT; STEIN GP, 1995, P 5 INT C COMP VIS B; Sturm P, 1997, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.1997.609467; TORR PHS, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1037, DOI 10.1109/ICCV.1995.466820; TORR PHS, 1997, CVPR97, P47; VANGOOL L, 1994, P INT C PATT REC JER; VIEVILLE T, 1995, MACH VISION APPL, V8, P41, DOI 10.1007/BF01213637; Vieville T, 1996, INT J COMPUT VISION, V17, P7, DOI 10.1007/BF00127817; VIEVILLE T, 1994, IMAGE VISION COMPUT, V12, P227, DOI 10.1016/0262-8856(94)90076-0; VIEVILLE T, 1995, USE VIEVILLE ZELLER; VIEVILLE T, 1995, ROBOTICS AUTONOMOUS, V14; VIEVILLE T, 1996, 4 ECCV, V2, P207; WILLSON R, 1994, THESIS CARNEGIEMELLO; ZELLER C, 1994, INT C PATT RECOG, P132, DOI 10.1109/ICPR.1994.576244; ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4; ZISSERMAN A, 1994, LECT NOTES COMPUTER, V825	43	8	8	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1999	31	1					5	29		10.1023/A:1008082308694	http://dx.doi.org/10.1023/A:1008082308694			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	186GM					2022-12-18	WOS:000079720600001
J	Moses, Y; Ullman, S				Moses, Y; Ullman, S			Generalization to novel views: Universal, class-based, and model-based processing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						object recognition; invariance	EDGE-DETECTION; OBJECT RECOGNITION; SPATIAL-FREQUENCY; IMAGES; EXTRACTION; FILTERS; MOTION; SPACE; FACES	A major problem in object recognition is that a novel image of a given object can be different from all previously seen images. Images can vary considerably due to changes in viewing conditions such as viewing position and illumination. In this paper we distinguish between three types of recognition schemes by the level at which generalization to novel images takes place: universal, class, and model-based. The first is applicable equally to all objects, the second to a class of objects, and the third uses known properties of individual objects. We derive theoretical limitations on each of the three generalization levels. For the universal level, previous results have shown that no invariance can be obtained. Here we show that this limitation holds even when the assumptions made on the objects and the recognition functions are relaxed. We also extend the results to changes of illumination direction. For the class level, previous studies presented specific examples of classes of objects for which functions invariant to viewpoint exist. Here, we distinguish between classes that admit such invariance and classes that do not. We demonstrate that there is a tradeoff between the set of objects that can be discriminated by a given recognition function and the set of images from which the recognition function can recognize these objects. Furthermore, we demonstrate that although functions that are invariant to illumination direction do not exist at the universal level, when the objects are restricted to belong to a given class, an invariant function to illumination direction can be defined. A general conclusion of this study is that class-based processing, that has not been used extensively in the past, is often advantageous for dealing with variations due to viewpoint and illuminant changes.	Weizmann Inst Sci, Dept Appl Math & Comp Sci, IL-76100 Rehovot, Israel	Weizmann Institute of Science	Moses, Y (corresponding author), Weizmann Inst Sci, Dept Appl Math & Comp Sci, IL-76100 Rehovot, Israel.							Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229; Basri R, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P541, DOI 10.1109/ICCV.1998.710769; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BIEDERMAN I, 1985, COMPUT VISION GRAPH, V32, P29, DOI 10.1016/0734-189X(85)90002-7; BRUNELLI R, 1991, IJCAI AUSTR, P1278; BURNS JB, 1992, GEOMETRICAL INVARIAN; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CLEMENS DJ, 1990, P DARPA IM UND WORKS, P604; CLEMENS DT, 1991, IEEE T PATTERN ANAL, V13, P1007, DOI 10.1109/34.99235; CRAW I, 1987, PATTERN RECOGN LETT, V5, P183, DOI 10.1016/0167-8655(87)90039-0; DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160; Davis L. S., 1975, COMPUT VISION GRAPH, V4, P248, DOI [DOI 10.1016/0146-664X(75)90012-X, 10.1016/0146-664X(75)90012-X]; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; FAWCETT R, 1994, IMAGE VISION COMPUT, V12, P615, DOI 10.1016/0262-8856(94)90015-9; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; HALLINAN PW, P IEEE C COMP VIS PA, P995; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; JACOBS D, 1992, IEEE C COMP VIS PATT, P439; Kanade T., 1977, COMPUTER RECOGNITION, VVolume 47; Kaya Y., 1972, FRONTIERS PATTERN RE, V1, P265; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; Lamdan Y., 1988, P IEEE INT C COMP VI, P238; LAMDAN Y, 1987, IEEE T ROBOTIC AUTOM, V6, P578; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; MARCELJA S, 1980, J OPT SOC AM, V70, P1297, DOI 10.1364/JOSA.70.001297; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Moses Y, 1996, PERCEPTION, V25, P443, DOI 10.1068/p250443; MOSES Y, 1992, P EUR C COMP VIS, P820; MOSES Y, 1993, THESIS WEIZMANN I SC; NIXON M, 1985, SPIE P, V575, P279; POLLEN DA, 1983, IEEE T SYST MAN CYB, V13, P907, DOI 10.1109/TSMC.1983.6313086; Rothwell C. A., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P573, DOI 10.1109/ICCV.1993.378159; ROTHWELL CA, 1992, ECCV, P757; SHASHUA A, 1992, ADV NEURAL INFORMATI, V4, P68; TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; ULLMAN S, 1989, COGNITION, V32, P193, DOI 10.1016/0010-0277(89)90036-X; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; VIOLA P, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P16, DOI 10.1109/ICCV.1995.466930; WARRINGTON EK, 1978, PERCEPTION, V7, P695, DOI 10.1068/p070695; WEINSHALL D, 1993, INT J COMPUT VISION, V10, P27, DOI 10.1007/BF01440845; WONG KH, 1989, P ICASSP, P1638; YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169; ZISSERMAN A, 1995, ARTIF INTELL, V78, P239, DOI 10.1016/0004-3702(95)00023-2	49	8	8	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1998	29	3					233	253		10.1023/A:1008088813977	http://dx.doi.org/10.1023/A:1008088813977			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	138BU					2022-12-18	WOS:000076952200004
J	McQuirk, IS; Horn, BKP; Lee, HS; Wyatt, JL				McQuirk, IS; Horn, BKP; Lee, HS; Wyatt, JL			Estimating the focus of expansion in analog VLSI	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						focus of expansion; motion vision; passive navigation; analog VLSI	PASSIVE NAVIGATION; IMAGER	In the course of designing an integrated system for locating the focus of expansion (FOE) from a sequence of images taken while a camera is translating, a variety of direct motion vision algorithms based on image brightness gradients have been studied (McQuirk, 1991, 1996b). The location of the FOE is the intersection of the translation vector of the camera with the image plane, and hence gives the direction of camera motion. This paper describes two approaches that appeared promising for analog very large scale integrated (VLSI) circuit implementation. In particular, two algorithms based on these approaches are compared with respect to bias, robustness to noise, and suitability for realization in analog VLSI. From these results, one algorithm was chosen for implementation. This paper also briefly discuss the real-time analog CMOS/CCD VLSI architecture realized in the FOE chip.	MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA; MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA; MIT, Microsyst Technol Labs, Cambridge, MA 02139 USA; MIT, Elect Res Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	McQuirk, IS (corresponding author), Maxim Integrated Prod, Sunnyvale, CA 94086 USA.			/0000-0003-3434-391X				BRUSS AR, 1983, COMPUT VISION GRAPH, V21, P3, DOI 10.1016/S0734-189X(83)80026-7; DRON L, 1993, INT J COMPUT VISION, V11, P45, DOI 10.1007/BF01420592; Fleming W.H., 1965, FUNCTIONS SEVERAL VA; GILBERT B, 1968, IEEE J SOLID-ST CIRC, VSC 3, P365, DOI 10.1109/JSSC.1968.1049925; HAKKARAINEN JM, 1993, IEEE J SOLID-ST CIRC, V28, P799, DOI 10.1109/4.222179; Horn B., 1986, ROBOT VISION, P1; HORN B, 1990, ARTIF INTELL, V2, P530; HORN B, 1981, ARTIF INTELL, V16, P185; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; JAIN R, 1983, IEEE T PATTERN ANAL, V21; KEAST CL, 1993, IEEE J SOLID-ST CIRC, V28, P431, DOI 10.1109/4.210025; MCQUIRK I, 1996, THESIS MIT CAMBRIDGE; MCQUIRK I, 1996, 1577 AI MIT ART INT; MCQUIRK I, 1997, 1997 ISSCC, P24; MCQUIRK IS, 1991, THESIS MIT CAMBRIDGE; NEGAHDARIPOUR S, 1987, IEEE T PATTERN ANAL, V9, P168, DOI 10.1109/TPAMI.1987.4767884; NEGAHDARIPOUR S, 1987, P WORKSH COMP VIS MI; NEGAHDARIPOUR S, 1986, P IEEE INT C ROB AUT; STANDLEY DL, 1991, IEEE J SOLID-ST CIRC, V26, P1853, DOI 10.1109/4.104177; Stein G. P., 1993, THESIS MIT CAMBRIDGE; TANNER JE, 1986, VLSI SIGNAL PROCESSI, V2, P59; Umminger CB, 1995, IEEE J SOLID-ST CIRC, V30, P1382, DOI 10.1109/4.482165; Varga RS., 1999, MATRIX ITERATIVE ANA; Yang W., 1990, 1990 IEEE International Solid-State Circuits Conference. Digest of Technical Papers. (Cat. No.90CH2824-1), P218, DOI 10.1109/ISSCC.1990.110203	24	8	10	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	1998	28	3					261	277		10.1023/A:1008009805554	http://dx.doi.org/10.1023/A:1008009805554			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	118BM					2022-12-18	WOS:000075817800005
J	Sato, J; Cipolla, R				Sato, J; Cipolla, R			Quasi-invariant parameterisations and matching of curves in images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						quasi-invariant parameterizations; semi-local invariants; integral invariants; differential invariants; curve matching; bilateral symmetry	OBJECT RECOGNITION; SKEWED SYMMETRY; SHAPE	In this paper, we investigate quasi-invariance on a smooth manifold, and show that there exist quasi-invariant parameterisations which are not exactly invariant but approximately invariant under group transformations and do not require high order derivatives. The affine quasi-invariant parameterisation is investigated in more detail and exploited for defining general affine semi-local invariants from second order derivatives only. The new invariants are implemented and used for matching curve segments under general affine motions and extracting symmetry axes of objects with 3D bilateral symmetry.	Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	Sato, J (corresponding author), Nagoya Inst Technol, Nagoya, Aichi 466, Japan.	js2@eng.cam.ac.uk; cipolla@eng.cam.ac.uk	Arandjelović, Ognjen/V-5255-2019	Arandjelović, Ognjen/0000-0002-9314-194X; Cipolla, Roberto/0000-0002-8999-2151				ABUMOSTAFA YS, 1984, IEEE T PATTERN ANAL, V6, P698, DOI 10.1109/TPAMI.1984.4767594; BARRETT EB, 1991, CVGIP-IMAG UNDERSTAN, V53, P46, DOI 10.1016/1049-9660(91)90004-9; BINFORD TO, 1993, P DARPA IM UND WORKS, P819; BLAKE A, 1995, INT J ROBOT RES, V14, P425, DOI 10.1177/027836499501400503; BRADY M, 1984, INT J ROBOT RES, V3, P36, DOI 10.1177/027836498400300302; BRUCKSTEIN AM, 1993, CVGIP-IMAG UNDERSTAN, V58, P49, DOI 10.1006/ciun.1993.1031; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CHAM TJ, 1996, P BRIT MACH VIS C ED, V2, P363; CIPOLLA R, 1992, P 2 EUR C COMP VIS, P187; Cyganski D., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P496; FRIEDBERG SA, 1986, COMPUT VISION GRAPH, V34, P138, DOI 10.1016/S0734-189X(86)80055-X; GELFAND I, 1963, CALCULUS ARIATIONS; GIBLIN PJ, 1985, AM MATH MON, V92, P689, DOI 10.2307/2323220; GROSS AD, 1994, INT J COMPUT VISION, V13, P91, DOI 10.1007/BF01420797; Guggenheimer H., 1977, DIFFERENTIAL GEOMETR; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; Jacobson N, 1962, LIE ALGEBRAS; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; Kanade T., 1983, HUMAN MACHINE VISION, P237; Kanatani Kenichi, 1990, GROUP THEORETICAL ME, P4; KOENDERINK JJ, 1976, BIOL CYBERN, V21, P29, DOI 10.1007/BF00326670; LIE S, 1927, GESAMMELTE ABHANDLUN, V6; MOHAN R, 1992, IEEE T PATTERN ANAL, V14, P616, DOI 10.1109/34.141553; MOONS T, 1995, INT J COMPUT VISION, V14, P25, DOI 10.1007/BF01421487; Mundy J., 1992, GEOMETRIC INVARIANCE; Olver, 1986, APPL LIE GROUPS DIFF; Olver P., 1994, GEOMETRY DRIVEN DIFF, V1, P255; Olver P. J., 1995, EQUIVALENCE INVARIAN, DOI DOI 10.1017/CBO9780511609565; PAUWELS EJ, 1995, INT J COMPUT VISION, V14, P49, DOI 10.1007/BF01421488; REISFELD D, 1995, INT J COMPUT VISION, V14, P119, DOI 10.1007/BF01418978; REISS TH, 1993, LNCS, V676; ROTHWELL CA, 1995, INT J COMPUT VISION, V16, P57, DOI 10.1007/BF01428193; SATO J, 1996, P BRIT MACHINE VISIO, V1, P63; SATO J, 1996, P 13 INT C PATT REC, V1, P915; TAUBIN G, 1992, ARTIF INT, P375; VANGOOL L, 1995, COMPUT VIS IMAGE UND, V61, P138, DOI 10.1006/cviu.1995.1010; VANGOOL L, 1995, INT J ROBOT RES, V14, P407, DOI 10.1177/027836499501400502; VANGOOL LJ, 1992, ARTIF INT, P157; WEISS I, 1993, INT J COMPUT VISION, V10, P207, DOI 10.1007/BF01539536; WEISS I, 1988, P IM UND WORKSH, V2, P1125; Zerroug M, 1996, IEEE T PATTERN ANAL, V18, P237, DOI 10.1109/34.485553; ZERROUG M, 1993, P DARPA IM UND WORKS, P725; ZISSERMAN A, 1992, ARTIF INT, P228	44	8	8	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN-JUL	1998	28	2					117	136		10.1023/A:1008011016516	http://dx.doi.org/10.1023/A:1008011016516			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	103BB					2022-12-18	WOS:000074959800002
J	Alter, TD; Jacobs, DW				Alter, TD; Jacobs, DW			Uncertainty propagation in model-based recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						model-based vision; object recognition; alignment; noise; uncertainty; error propagation; linear programming; perspective; scaled-orthographic; bounded error; Gaussian error	OBJECT RECOGNITION; 3-D MODELS; 2-D IMAGES; PERSPECTIVE; TRACKING	Robust recognition systems require a careful understanding of the effects of error in sensed features. In model-based recognition, matches between model features and sensed image features typically are used to compute a model pose and then project the unmatched model features into the image. The error in the image features results in uncertainty in the projected model features. We first show how error propagates when poses are based on three pairs of 3D model and 2D image points. In particular, we show how to simply and efficiently compute the distributed region in the image where an unmatched model point might appear, for both Gaussian and bounded error in the detection of image points, and for both. scaled-orthographic and perspective projection models. Next, we provide geometric and experimental analyses to indicate when this linear approximation will succeed and when it will fail. Then, based on the linear approximation, we show how we can utilize Linear Programming to compute bounded propagated error regions for any number of initial matches. Finally, we use these results to extend, from two-dimensional to three-dimensional objects, robust implementations of alingment, interpretation-tree serach, and transformation clustering.	MIT, AI Lab, Cambridge, MA 02139 USA; NEC Res Inst, Princeton, NJ 08540 USA	Massachusetts Institute of Technology (MIT); NEC Corporation	Alter, TD (corresponding author), MIT, AI Lab, Room 750,545 Technol Sq, Cambridge, MA 02139 USA.							ALTER TD, 1994, IEEE T PATTERN ANAL, V16, P802, DOI 10.1109/34.308475; ALTER TD, 1992, 1410 MIT ART INT LAB; ALTER TD, 1993, P 4 INT C COMP VIS; ALTER TD, 1994, IEEE C COMP VIS PATT, P892; [Anonymous], 1985, PERCEPTUAL ORG VISUA; AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BAIRD HS, 1985, MODEL BASED IMAGE MA; BASRI R, 1994, CS9419 WEIZM I SCI; BEVERIDGE R, 1990, IEEE T COMP VIS PATT, P18; Bolles R. C., 1982, INT J ROBOT RES, V1, P57; BOLLES RC, 1978, P DARPA IU WORKSH, P163; BREUEL T, 1991, IEEE C COMP VIS PATT, P257; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; CASS T, 1996, 4 EUR C COMP VIS; CASS T, 1992, THESIS MIT; CASS T, 1992, 2 EUR C COMP VIS, P834; CLARK CS, 1979, SPIE, V186, P54; CLEMENS DT, 1991, IEEE T PATTERN ANAL, V13, P1007, DOI 10.1109/34.99235; COSTA M, 1960, P 6 ISR C ART INT, P35; ELLIS RE, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P348, DOI 10.1109/ROBOT.1989.100012; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; GENNERY DB, 1992, INT J COMPUT VISION, V7, P243, DOI 10.1007/BF00126395; GRIMSON W, 1992, P EUROPEAN C COMPUTE, P291; Grimson W. E. L., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P334, DOI 10.1109/ICCV.1990.139544; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; GRIMSON WEL, 1992, P IEEE C COMP VIS PA; GRIMSON WEL, 1990, IEEE T PATTERN ANAL, V12; Hel-Or Y., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P77, DOI 10.1109/CVPR.1992.223224; HORAUD R, 1987, IEEE T PATTERN ANAL, V9, P401, DOI 10.1109/TPAMI.1987.4767922; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; HUTTENLOCHER DP, 1988, 1045 MIT ART INT LAB; JACOBS D, 1991, IEEE C COMP VIS PATT, P269; JACOBS DW, 1993, 1416 MIT ART INT LAB; Kumar R., 1989, P WORKSHOP INTERPRET, P52; Lamdan Y., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P335, DOI 10.1109/CVPR.1988.196257; LAMDAN Y, 1990, IEEE T ROBOTIC AUTOM, V6, P578, DOI 10.1109/70.62047; LAMDAN Y, 1991, IEEE C COMP VIS PATT, P22; LOWE DG, 1992, INT J COMPUT VISION, V8, P113, DOI 10.1007/BF00127170; OSHIMA M, 1983, IEEE T PATTERN ANAL, V5, P353, DOI 10.1109/TPAMI.1983.4767405; POELMAN C, 1994, P EUR C COMP VIS; RIGOUTSOS I, 1992, THESIS NEW YORK U; RIGOUTSOS I, 1991, 8 ISR C ART INT COMP; ROBERTS L, 1966, OPTICAL ELECTROOPTIC; Rothwell C., 1992, IEEE C COMP VIS PATT, P109; Sarachik K. B., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P400, DOI 10.1109/CVPR.1993.341099; SEIDEL R, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P211, DOI 10.1145/98524.98570; STRANG G, 1988, LINEAR ALGEBRA ITS A; SUGIMOTO A, 1993, P SOC PHOTO-OPT INS, V1904, P183, DOI 10.1117/12.146690; Therrien C. W., 1989, DECISION ESTIMATION; Thompson D. W., 1987, Proceedings of the 1987 IEEE International Conference on Robotics and Automation (Cat. No.87CH2413-3), P208; ULLMAN S, 1991, IEEE T PATTERN ANAL, V13, P992, DOI 10.1109/34.99234; WAYNER PC, 1991, IEEE C COMP VIS PATT, P473; WELLS W, 1993, 1398 MIT ART INT LAB; WELLS WM, 1991, IEEE COMP SOC COMP V, P486; WENSHALL D, 1993, IEEE C COMP VIS PATT, P220; [No title captured]	56	8	9	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR-APR	1998	27	2					127	159		10.1023/A:1007989016491	http://dx.doi.org/10.1023/A:1007989016491			33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZK692					2022-12-18	WOS:000073351600002
J	Parvin, B; Medioni, G				Parvin, B; Medioni, G			B-rep object description from multiple range views	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							RECOGNIZING 3-D OBJECTS; CONSTRAINED SEARCH; CURVED OBJECTS; EDGE-DETECTION; PRIMAL SKETCH; ROBOT VISION; RECOGNITION; IMAGES; MODEL; OPTIMIZATION	We present a system which computes an integrated description of an object from multiple range images. The object description is in the form of B-rep (boundary representation), which has not been achieved by the computer vision community. To do so, we emphasize the inherent difficulties and ambiguities in the low to mid level vision, and present novel techniques of resolving them. In this system, each view of the object is represented as an attributed graph, where nodes correspond to the surfaces (vertices) and links represent the relationship between surfaces. The main issue in surface extraction is contour closure, which is formulated as a dynamic network. The underlying principle for this network is weak smoothness and geometric cohesion, and is modeled as the interaction between long and short term variables. Long term variables represent the initial boundary grouping computed from the low level surface features, and short term variables represent the competing hypotheses that cooperate with the long term variables. The matching problem involves matching visible surfaces and vertices, and provides the necessary basis for volumetric reconstruction from multiple views. The matching strategy is a two step process, where in each step uses the Hopfield network. At each step, we specify a set of local, adjacency and global constraints, and define an appropriate energy function to be minimized. At the first level of this hierarchy, surface patches are matched and the rigidity transformation is computed. At the second level, the mapping is refined by matching the corresponding vertices, and the transformation is verified. The multiple-view reconstruction consists of two steps. First, we build a composite graph that contains the bounding surfaces and their corresponding attributes, and then intersect these surfaces so that the edges and vertices corresponding to the B-rep description are identified. We present results on objects with planar, as well as quadratically-curved surfaces.	UNIV SO CALIF,INST ROBOT & INTELLIGENT SYST,LOS ANGELES,CA 90089	University of Southern California	Parvin, B (corresponding author), UNIV CALIF BERKELEY,LAWRENCE BERKELEY NATL LAB,INFORMAT & COMP SCI DIV,BERKELEY,CA 94702, USA.							ASADA H, 1986, IEEE T PATTERN ANAL, V8, P2, DOI 10.1109/TPAMI.1986.4767747; Barr A. H., 1981, IEEE COMPUT GRAPH, V1, P1, DOI [DOI 10.1109/MCG.1981.1673788, 10.1109/MCG.1981.1673788]; Bartels RH, 1987, INTRO SPLINES USE CO; BESL PJ, 1988, IEEE T PATTERN ANAL, V10, P167, DOI 10.1109/34.3881; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BOLLES RC, 1986, INT J ROBOT RES, V5, P3, DOI 10.1177/027836498600500301; Boult T. E., 1987, P WORKSH SPAT REAS M, P128; BROOKS RA, 1983, IEEE T PATTERN ANAL, V5, P140, DOI 10.1109/TPAMI.1983.4767366; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CHEN CH, 1989, IEEE T SYST MAN CYB, V19, P1535, DOI 10.1109/21.44070; CHEN JS, 1989, THESIS U SO CALIFORN; CHEN Y, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P2724, DOI 10.1109/ROBOT.1991.132043; CHIN RT, 1986, COMPUT SURV, V18, P67, DOI 10.1145/6462.6464; Duda R.O., 1973, J ROYAL STAT SOC SER; FAN TJ, 1989, IEEE T PATTERN ANAL, V11, P1140, DOI 10.1109/34.42853; FAN TJ, 1987, IEEE T ROBOTIC AUTOM, V3, P527; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FAUX I, 1983, COMP GEOM-THEOR APPL, P107; Ferrie F. P., 1987, Proceedings of the IEEE Computer Society Workshop on Computer Vision (Cat. No.87TH0210-5), P117; FERRIE FP, 1993, IEEE T PATTERN ANAL, V15, P771, DOI 10.1109/34.236252; FLYNN PJ, 1991, IEEE T PATTERN ANAL, V13, P1066, DOI 10.1109/34.99239; FLYNN PJ, 1988, IEEE C COMP VIS PATT, P261; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GRIMSON W, 1987, IEEE T PATTERN ANAL, V9; GRIMSON WEL, 1990, ARTIF INTELL, V44, P121, DOI 10.1016/0004-3702(90)90100-E; GROSSBERG S, 1985, PERCEPT PSYCHOPHYS, V38, P141, DOI 10.3758/BF03198851; HARALICK RM, 1983, INT J ROBOT RES, V2, P50, DOI 10.1177/027836498300200105; HENSEN C, 1989, IEEE T PATTERN ANAL, V11, P1181; HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141; HOPFIELD JJ, 1986, SCIENCE, V233, P625, DOI 10.1126/science.3755256; HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088; HORN B, 1984, P IEEE, P1656; HUMMEL JE, 1990, 12 ANN C COGN SCI SO, P614; IKEUCHI K, 1988, P 2 INT C COMP VIS, P228; JAIN AK, 1988, IEEE T PATTERN ANAL, V10, P783, DOI 10.1109/34.9102; JAIN R, 1988, MACH VISION APPL, P45; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; KONEN W, 1993, NEURAL COMPUT, V5, P719, DOI 10.1162/neco.1993.5.5.719; LOWE DG, 1985, IEEE T PATTERN ANAL, V7, P320, DOI 10.1109/TPAMI.1985.4767660; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; MARROQUIN JL, 1989, BIOL CYBERN, V61, P457, DOI 10.1007/BF02414907; MOHAN R, 1989, JUN P IEEE C COMP VI, P333; MOHAN R, 1989, IEEE T PATTERN ANAL, V11, P1140; NEVATIA R, 1977, ARTIF INTELL, V8, P77, DOI 10.1016/0004-3702(77)90006-6; NOBORIO H, 1988, IEEE T PATTERN ANAL, V10, P769, DOI 10.1109/34.9101; OSHIMA M, 1983, IEEE T PATTERN ANAL, V5, P353, DOI 10.1109/TPAMI.1983.4767405; Parvin B., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P393, DOI 10.1109/CVPR.1991.139722; PARVIN B, 1989, P INT JOINT C NEUR N, P281; PARVIN B, 1991, IEEE C ROBOTICS AUTO, P1808; PARVIN B, 1991, THESIS U SO CALIFORN; PARVIN B, 1992, P IEEE C ROB AUT, V2, P1602; Pentland A. P., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P242; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; Ponce J., 1987, Proceedings of the IEEE Computer Society Workshop on Computer Vision (Cat. No.87TH0210-5), P268; POTMESIL M, 1983, 8TH P INT JOINT C AI, P1089; RAO K, 1987, P AAAI WORKSH SPAT R, P168; REQUICHA AAG, 1985, P IEEE, V73, P30, DOI 10.1109/PROC.1985.13108; SAINTMARC P, 1991, IEEE T PATTERN ANAL, V13, P514, DOI 10.1109/34.87339; SATO K, 1991, P IEEE WORKSH DIR AU, P2; SEKITA I, 1988, PATTERN RECOGN, V21, P9, DOI 10.1016/0031-3203(88)90067-2; Sha'asua A., 1988, INT C COMP VIS, P321; SOLINA F, 1990, IEEE T PATTERN ANAL, V12, P131, DOI 10.1109/34.44401; Soucy M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P348, DOI 10.1109/CVPR.1992.223166; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; Tanaka H. T., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P88, DOI 10.1109/CVPR.1993.340974; VEMORI B, 1986, P IEEE C COMP VIS PA, P435; von der Malsburg C, 1981, 812 M PLANCK I BIOPH; YUILLE A, 1990, J BIOL CYBERNETICS, V61, P115; Zucker SW, 1989, NEURAL COMPUT, V1, P68, DOI 10.1162/neco.1989.1.1.68; [No title captured]	72	8	8	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1996	20	1-2					81	112		10.1007/BF00144118	http://dx.doi.org/10.1007/BF00144118			32	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VQ240					2022-12-18	WOS:A1996VQ24000005
J	Durou, JD; Maitre, H				Durou, JD; Maitre, H			On convergence in the methods of Strat and of Smith for shape from Shading	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								We study the convergence of two iterative Shape from Shading methods: the methods of Strat and of Smith. We try to determine the spectral radius of the Jacobian matrix of each iteration at any possible fixed point. We show that the method of Strat diverges for any image containing at least four pixels forming a square, any reflectance map and any relative weight between the irradiance term and the integrability term. An example is provided, in which divergence occurs after a large number of iterations, even if the reconstructed surface approaches the real surface after only a few iterations. We show then by a similar way that the method of Smith diverges for any image containing at least nine pixels forming a square, any reflectance map and any relative weight between the irradiance term and the smoothing term.			Durou, JD (corresponding author), ECOLE NATL SUPER TELECOMMUN BRETAGNE,DEPT IMAGES,46 RUE BARRAULT,F-75634 PARIS 13,FRANCE.							BROOKS MJ, 1985, AUG P INT JOINT C AR, P932; DUPUIS P, 1993, P INT C COMP VIS, P692; DUROU JD, 1993, THESIS U ORSAY PARIS; DUROU JD, 1988, THESIS DEP IMAGES FR; Horn B.K.P., 1970, THESIS MIT CAMBRIDGE; Horn B.K.P., 1989, 1105 MIT; Horn B.K.P., 1989, SHAPE SHADING; HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; Lattis S, 1910, CR HEBD ACAD SCI, V150, P1106; LEE D, 1985, DEC P DARPA IM UND W, P489; MAITRE H, 1981, COMPUT VISION GRAPH, V16, P95, DOI 10.1016/0146-664X(81)90050-2; MALIK J, 1989, IEEE T PATTERN ANAL, V11, P555, DOI 10.1109/34.24791; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; SMITH GB, 1982, P DARPA IM UND WORK, P132; STRAT TM, 1979, THESIS MIT CAMBRIDGE; SZELISKI R, 1991, CVGIP-IMAG UNDERSTAN, V53, P129, DOI 10.1016/1049-9660(91)90023-I	17	8	8	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1996	17	3					273	289		10.1007/BF00128234	http://dx.doi.org/10.1007/BF00128234			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	UB510		Green Submitted			2022-12-18	WOS:A1996UB51000003
J	KOSECKA, J; CHRISTENSEN, HI; BAJCSY, R				KOSECKA, J; CHRISTENSEN, HI; BAJCSY, R			DISCRETE-EVENT MODELING OF VISUALLY GUIDED BEHAVIORS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							VISION	When visual behaviors are combined to provide a specific functionality needed for a task, the combination is often based on heuristic rules. In this article we show that by adopting the Discrete-Event Systems (DES) formalism for describing the interaction between visual behaviors it is possible to provide systems that have well-defined properties in terms of observability and controllability. The method is in particular suited for describing the coupling between action and perception. An introduction to the use of DES is provided and it is demonstrated how DES are used for modeling behaviors and controlling a mobile robot equipped with a binocular camera head and some additional sensors.	AALBORG UNIV,IMAGE ANAL LAB,DK-9220 AALBORG O,DENMARK	Aalborg University	KOSECKA, J (corresponding author), UNIV PENN,GRASP LAB,3401 WALNUT ST,ROOM 301C,PHILADELPHIA,PA 19104, USA.		Christensen, Henrik I/A-2261-2009					ALOIMONOS J, 1990, P DARPA IMAGE UNDERS, P816; ARKIN RC, 1987, APR P INT C ROB AUT; BAJCSY R, 1992, NOV P WORKSH INT ROB; BALEMI S, 1991, SUPERVISORY CONTROL; BALLARD DH, 1991, ARTIF INTELL, V48, P57, DOI 10.1016/0004-3702(91)90080-4; BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032; Carpenter RHS, 1988, MOVEMENTS EYES; CHRISTENSEN HI, 1993, INT J PATT RECOG ART, P6; CHRISTENSEN HI, 1992, APPLICATION AI X MAC; Clark J. J., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P514, DOI 10.1109/CCV.1988.590032; CONNOLLY CI, 1993, J ROBOTIC SYST, V10, P931, DOI 10.1002/rob.4620100704; Crowley JL, 1994, VISION PROCESS; DICKMANNS ED, 1988, MACH VISION APPL, V1, P223; HORSWILL I, 1993, P ANIMALS ANIMATS, V2; KOSECKA J, 1994, UNPUB 1994 INT C PAT; KOSECKA J, 1993, 4TH P INT C COMP VIS; LOWE D, 1987, PERCEPTUAL ORG VISUA; MALLOT HA, 1991, BIOL CYBERN, V64, P177, DOI 10.1007/BF00201978; Marr D., 1982, VISION; RAMADGE PJ, 1987, SIAM J CONTROL OPTIM, V25, P206, DOI 10.1137/0325013; RAMADGE PJG, 1989, P IEEE, V77, P81, DOI 10.1109/5.21072; REYNOLDS CW, 1988, PHYSICALLY BASED MOD; Robinson D. A., 1987, VISION BRAIN COOPERA, P89; SHAYMAN MA, 1992, SUPERVISORY CONTROL; TARR M, 1993, P INT JOINT C ARTIF, P1991; TISTARELLI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P108, DOI 10.1016/1049-9660(92)90089-L; TSOTSOS JK, 1993, BEHAVIORIST INTELLIG; WONHAM WM, 1987, SIAM J CONTROL OPTIM, V25, P637, DOI 10.1137/0325036	28	8	8	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1995	14	2					179	191		10.1007/BF01418982	http://dx.doi.org/10.1007/BF01418982			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QT789					2022-12-18	WOS:A1995QT78900006
J	STENSTROM, JR; CONNOLLY, CI				STENSTROM, JR; CONNOLLY, CI			CONSTRUCTING OBJECT MODELS FROM MULTIPLE IMAGES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							PROJECTIONS; SCENE	Methods for constructing face-edge-vertex models of objects directly from image data are presented. These methods, applicable to both range and intensity data, produce refined models using multiple images. Each model bounds the actual positions of surfaces on the object. The techniques rely on topologically significant image features to generate bounding volumes for the object surfaces. Potentially occupied volumes obtained from the multiple image viewpoints are intersected to form a cumulative description of potential surface positions. In conjunction with certain coherence and visibility assumptions, a model can be produced with little or no ambiguity. Typically, several well-chosen views will produce a good approximation to the object under consideration. Illustrative examples of building models from multiple range and intensity images are provided.	GE,CTR CORP RES & DEV,SCHENECTADY,NY 12345	General Electric								ALLEN PK, 1986, 1986 P C ROB AUT SAN; [Anonymous], 1985, PERCEPTUAL ORG VISUA; ARKIN RC, 1987, COINS8780 U MASS COI; ASADA H, 1985, AIM758 MIT TECH REPT; BAKER H, 1977, 5TH P INT JOINT C AR, P649; BALASUBRAMANYAM P, 1991, JUN P C COMP VIS PAT, P115; Baumgart B, 1975, PROC 1975 NATL COMPU, V44, P589, DOI DOI 10.1145/1499949.1500071; BOISSONAT JD, 1985, JUN P IEEE C COMP VI, P393; BOISSONNAT JD, 1988, 1988 P C ROB AUT IEE, P1798; BRADY JM, 1985, COMPUT VIS GRAPH PRO, V32; BRICE CR, 1970, ARTIF INTELL, V1, P205, DOI 10.1016/0004-3702(70)90008-1; BURNS JB, 1988, P 1988 DARPA IM UND, P711; BURNS JB, 1992, 9217 U MASS COMP SCI; CANNY JF, 1984, TR720 MIT TECH REPT; CHIEN CH, 1984, 7TH P INT C PATT REC, V2, P817; CONNOLLY CI, 1992, GEOMETER SOLID MODEL; CONNOLLY CI, 1984, MAR P INT C ROB ATL, P25; DOLAN J, 1992, JUN P C COMP VIS PAT; Faugeras O. D., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P796; FAUGERAS OD, 1983, 8TH P INT JOINT C AR, P996; GIBLIN P, 1908, FEB DARPA IM UND WOR, P900; GRIMSON WEL, 1985, 3RD P INT S ROB RES; GRIMSON WEL, 1981, IMAGES SURFACES; GRUPEN RA, 1991, 1991 P INT C ROB AUT; Henle M., 1979, COMBINATORIAL INTRO; HERMAN M, 1986, ARTIF INTELL, V30, P289, DOI 10.1016/0004-3702(86)90002-0; HERMAN M, 1985, 1985 P C ROB AUT ST, P426; HOFFMAN CM, 1987, 87875 CORN U DEP COM; HONG J, 1990, 1990 P INT C ROB AUT, P1568; JANE FT, 1986, JAMES ALL WORLDS AIR; JONES JL, 1990, 1990 P INT C ROB AUT, P683; LIM HS, 1988, P DARPA IM UND WORKS, P809; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LOZANOPEREZ T, 1981, IEEE T SYST MAN CYB, V11, P681, DOI 10.1109/TSMC.1981.4308589; LYONS D, 1985, 1985 IEEE INT C ROB, P588; MARKOWSKY G, 1980, IBM J RES DEV, V24, P582, DOI 10.1147/rd.245.0582; Marr D., 1982, VISION; MEAGHER DJ, 1982, COMPUT GRAPH PROCESS, V19; MILENKOVIC VJ, 1989, GEOMETRIC REASONING, P377; NOYES T, 1986, THESIS RENSS POL I D; OLIENSIS J, 1991, JUN P C COMP VIS PAT, P559; POPPLESTONE RJ, 1975, 4TH P INT JOINT C AR, P664; POTMESIL M, 1933, 8TH P INT JOINT C AR, P1089; SHIRAI Y, 1971, 2ND P INT JOINT C AR, P80; SHNEIER M, 1979, 6TH P IJCAI TOK, P818; STENSTROM JR, 1986, 1986 P C ROB AUT SAN; STENSTROM JR, 1988, THESIS RENSSELAER PO; STENSTROM JR, 1985, 1985 P C ROB AUT ST, P436; TERZOPOULOS D, 1983, 8TH P INT JOINT C AR, P1073; THOMPSON DW, 1987, IEEE J ROBOTIC AUTOM, P208; WESLEY MA, 1981, IBM J RES DEV, V25, P934, DOI 10.1147/rd.256.0934; WHITNEY H, 1955, ANN MATH, V62, P374, DOI 10.2307/1970070; [No title captured]; [No title captured]; [No title captured]	55	8	11	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1992	9	3					185	212		10.1007/BF00133701	http://dx.doi.org/10.1007/BF00133701			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KC414					2022-12-18	WOS:A1992KC41400002
J	ASADA, M; KIMURA, M; TANIGUCHI, Y; SHIRAI, Y				ASADA, M; KIMURA, M; TANIGUCHI, Y; SHIRAI, Y			DYNAMIC INTEGRATION OF HEIGHT MAPS INTO A 3D WORLD REPRESENTATION FROM RANGE IMAGE SEQUENCES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							NAVIGATION	Integration of 2 1/2D sketches obtained at different observation stations into a consistent world (or object) representation is one of the central issues in computer vision and robotics. The resolution and accuracy of 2 1/2D sketches may be different from one view point to another, and inconsistent data between different observations may occur. This article presents an approach to building a spatiotemporal representation of dynamic scenes including moving objects from a sequence of range images taken by a moving observer. A range image is transformed into a height-map representation, which is segmented into the ground plane and objects on it. In order to capture the resolution and accuracy of the range information and the consistency of the height information between different height maps, we define a reliability measure of the height information for each bucket on the height map. Using this reliability, the system finds the correspondences of both static and moving objects between different observations, and successively refines the height information and its reliability with newly acquired data, dealing with inconsistent data. Final representation of the integrated height map consists of the time stamp of the last observation, region labels of static and moving objects and their spatiotemporal properties such as height information, its reliability, and die velocities of both the observer and independently moving objects. We applied the method to road scenes physically simulated by landscape toy models and show the experimental results.	OSAKA UNIV,SUITA,OSAKA 565,JAPAN	Osaka University								ASADA M, 1989, P IEEE RSJ INT WORKS, P54; AYACHE N, 1988, INT J ROBOT RES, V7, P45, DOI 10.1177/027836498800700605; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; DEMENTHON D, 1987, CARTR337 U MARYL CTR; ELFES A, 1989, COMPUTER, V22, P46, DOI 10.1109/2.30720; FAUGERAS OD, 1987, P INT C COMPUT VIS L, P24; GRIMSON WEL, 1981, IMAGES SURFACES; KWEON IS, 1990, P IEEE INT WORKSH IN, P127; MATTHEIS L, 1988, P DARPA IMAGE UNDERS, P199; NAGAO M, 1979, EDGE PRESERVING SMOO, V9, P394; OLIN KM, 1989, P IMAGE UNDERSTANDIN, P134; THORPE C, 1988, IEEE T PATTERN ANAL, V10, P362, DOI 10.1109/34.3900; TURK MA, 1987, IEEE T ROBOTIC AUTOM, P273; WAXMAN AM, 1986, P IEEE INT C ROBOT A, P16000	14	8	8	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	1992	9	1					31	53						23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JW393					2022-12-18	WOS:A1992JW39300003
J	DHOND, UR; AGGARWAL, JK				DHOND, UR; AGGARWAL, JK			A COST-BENEFIT-ANALYSIS OF A 3RD CAMERA FOR STEREO CORRESPONDENCE	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							IMAGES; VISION	This paper looks at the twin issues of the gain in accuracy of stereo correspondence and the accompanying increase in computational cost due to the use of a third camera for stereo analysis. Trinocular stereo algorithms differ from binocular algorithms essentially in the epipolar constraint used in the local matching stage. The current literature does not provide any insight into the relative merits of binocular and trinocular stereo matching with the matching accuracy being verified against the ground truth. Experiments for evaluating the relative performance of binocular and trinocular stereo algorithms were conducted. The stereo images used for the performance evaluation were generated by applying a Lambertian reflectance model to real Digital Elevation Maps (DEMs) available from the U.S. Geological Survey. The matching accuracy of the stereo algorithms was evaluated by comparing the observed stereo disparity against the ground truth derived from the DEMs. It was observed that trinocular local matching reduced the percentage of mismatches having large disparity errors by more than half when compared to binocular matching. On the other hand, trinocular stereopsis increased the computational cost of local matching over binocular by only about one-fourth. We also present a quantization-error analysis of the depth reconstruction process for the nonparallel stereo-imaging geometry used in our experiments.	UNIV TEXAS,DEPT ELECT & COMP ENGN,COMP & VIS RES CTR,AUSTIN,TX 78712	University of Texas System; University of Texas Austin								AYACHE N, 1987, INT J COMPUT VISION, V1, P107, DOI 10.1007/BF00123161; AYACHE N, 1987, 1ST P INT C COMP VIS, P422; BARNARD ST, 1980, IEEE T PATTERN ANAL, V2, P333, DOI 10.1109/TPAMI.1980.4767032; BARNARD ST, 1982, ACM COMPUT SURV, V14, P553, DOI DOI 10.1145/356893.356896; BLOSTEIN SD, 1987, IEEE T PATTERN ANAL, V9, P752, DOI 10.1109/TPAMI.1987.4767982; DHOND UR, 1989, IEEE T SYST MAN CYB, V19, P1489, DOI 10.1109/21.44067; DHOND UR, 1989, TECHNICAL DIGEST SER, V14, P78; GRIMSON WEL, 1985, IEEE T PATTERN ANAL, V7, P17, DOI 10.1109/TPAMI.1985.4767615; GRIMSON WEL, 1981, PHILOS T ROY SOC B, V292, P217, DOI 10.1098/rstb.1981.0031; HANSEN C, 1988, MAY NATO ADV RES WOR; HOFF W, 1989, IEEE T PATTERN ANAL, V11, P121, DOI 10.1109/34.16709; ITO M, 1986, IEEE T PATTERN ANAL, V8, P524, DOI 10.1109/TPAMI.1986.4767817; KIM YC, 1987, IEEE J ROBOT AUTOM, V3, P361; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; MATTHIES L, 1987, IEEE T ROBOTIC AUTOM, V3, P239, DOI 10.1109/JRA.1987.1087097; MEDIONI GG, 1980, COMP GRAPHICS IMG P, V13, P257; MILENKOVIC VJ, 1985, DEC P IM UND WORKSH, P163; PEITIKAINEN M, 1986, JUN IEEE CS C PATT R, P2; POLLARD SB, 1985, PERCEPTION, V14, P449, DOI 10.1068/p140449; Robinson G.S., 1977, COMPUT VISION GRAPH, V6, P492, DOI 10.1016/s0146-664x(77)80024-5; RODRIGUEZ JJ, 1989, TECHNICAL DIGEST SER, V14, P30; RODRIGUEZ JJ, 1988, CVPR, P153; Yachida M., 1986, Eighth International Conference on Pattern Recognition. Proceedings (Cat. No.86CH2342-4), P1041; YACHIDA M, 1985, 3RD P INT S ROB RES, P11	25	8	9	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	APR	1991	6	1					39	58		10.1007/BF00127125	http://dx.doi.org/10.1007/BF00127125			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FL254					2022-12-18	WOS:A1991FL25400003
J	MURRAY, DW; CASTELOW, DA; BUXTON, BF				MURRAY, DW; CASTELOW, DA; BUXTON, BF			FROM IMAGE SEQUENCES TO RECOGNIZED MOVING POLYHEDRAL OBJECTS	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									UNIV OXFORD,DEPT ENGN SCI,OXFORD OX1 3PJ,ENGLAND	University of Oxford								ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; ASADA H, 1984, 1984 P WORKSH COMP V, P8; Ballard D.H., 1982, COMPUTER VISION; BLAKE A, 1986, 7TH P EUR C ART INTE, P518; BOLLES RC, 1983, 8TH P INT JOINT C AR, P1116; BRADY JM, 1987, 3RD P ALVEY VISION C, P259; BURNS JB, 1986, IEEE T PATTERN ANAL, V8, P425, DOI 10.1109/TPAMI.1986.4767808; BURT P, 1980, PERCEPTION, V9, P671, DOI 10.1068/p090671; BUXTON BF, 1984, 6TH P EUR C ART INT, P631; BUXTON BF, 1983, P ROY SOC LOND B BIO, V218, P22; CANNY J, 1983, AITR720 ART INT LAB; CASTELOW DA, 1988, COMPUTING VISUAL MOT; Faugeras O. D., 1983, P INT JOINT C ART IN, V8, P996; FENNEMA CL, 1979, COMPUT VISION GRAPH, V9, P301, DOI 10.1016/0146-664X(79)90097-2; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; GRIMSON WEL, 1985, 3RD P INT S ROB RES, P81; HARALICK RM, 1986, GRAPHICS IMAGE PROCE, V36, P372; HARRIS CG, 1986, IN PRESS IMAGE VISIO; HEEGER DJ, 1987, 1ST P INT C COMP VIS, P181; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; LLOYD SA, 1985, IMAGE VISION COMPUT, V3, P177, DOI 10.1016/0262-8856(85)90005-8; LONGUETHIGGINS HC, 1984, PROC R SOC SER B-BIO, V223, P165, DOI 10.1098/rspb.1984.0088; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; MURRAY DW, 1987, IEEE T PATTERN ANAL, V9, P220, DOI 10.1109/TPAMI.1987.4767896; MURRAY DW, 1988, INT J COMPUT VISION, V2, P147; NEUMANN B, 1980, P PATTERN RECOG C MI; POLLARD SB, 1987, IMAGE VISION COMPUT, V5, P73, DOI 10.1016/0262-8856(87)90030-8; POLLARD SB, 1985, PERCEPTION, V14, P449, DOI 10.1068/p140449; Proffitt D. R., 1984, First Conference on Artificial Intelligence Applications (Cat. No. 84CH2107-1), P232; Ramer Urs, 1975, COMPUTER GRAPHICS IM, V4, P81; ROSENFELD A, 1982, DIGITAL PICTURE PROC, V2; RUFF BDP, 1987, IN PRESS MAR WORKSH; SCHUNCK BG, 1986, COMPUT VISION GRAPH, V35, P20, DOI 10.1016/0734-189X(86)90124-6; SCOTT GL, 1987, IMAGE VISION COMPUT, V5, P67, DOI 10.1016/0262-8856(87)90029-1; Southall JPC, 1911, HDB PHYSL OPTIK; SPOERRI A, 1987, 1 INT C COMP VIS, P209; THOMPSON WB, 1987, 1ST P INT C COMP VIS, P201; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; TSAI RY, 1981, R921 COORD SCI LAB U; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; ULLMAN S, 1983, PHYSICAL BIOL PROCES; VERRI A, 1987, 1ST P INT C COMP VIS, P171; WALLACH H, 1953, J EXP PSYCHOL, V45, P205, DOI 10.1037/h0056880; WAXMAN AM, 1987, INT J COMPUT VISION, V1, P239, DOI 10.1007/BF00127823; WAXMAN AM, 1985, INT J ROBOT RES, V4, P95, DOI 10.1177/027836498500400307; WESTPHAL H, 1986, COMPUT VISION GRAPH, V34, P302, DOI 10.1016/S0734-189X(86)80045-7; YAMUMOTO Y, 1985, P IEEE C COMPUT VISI, P89	48	8	9	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1989	3	3					181	208		10.1007/BF00133031	http://dx.doi.org/10.1007/BF00133031			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AP282					2022-12-18	WOS:A1989AP28200001
J	NALWA, VS				NALWA, VS			REPRESENTING ORIENTED PIECEWISE C2 SURFACES	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									AT&T BELL LABS,HOLMDEL,NJ 07733	AT&T; Nokia Corporation; Nokia Bell Labs								AGIN GJ, 1976, IEEE T COMPUT, V25, P439, DOI 10.1109/TC.1976.1674626; Barr A. H., 1981, IEEE Computer Graphics and Applications, V1, P11, DOI 10.1109/MCG.1981.1673799; BESL PJ, 1985, COMPUT SURV, V17, P75, DOI 10.1145/4078.4081; BINFORD TO, 1971, UNPUB IEEE C SYSTEMS; BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0; Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154; BRADY M, 1985, COMPUT VISION GRAPH, V32, P1, DOI 10.1016/0734-189X(85)90001-5; BROU P, 1984, INT J ROBOT RES, V3, P89, DOI 10.1177/027836498400300406; CHERN S, 1957, AM J MATH, V79, P949, DOI 10.2307/2372445; Do Carmo M.P., 2016, DIFFERENTIAL GEOMETR, Vsecond; FIREY WJ, 1967, MATHEMATIKA, V14, P1, DOI 10.1112/S0025579300007956; GARDNER M, 1965, SCI AM, P222; Hilbert D., 1952, GEOMETRY IMAGINATION; HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073; KOENDERINK JJ, 1986, BIOL CYBERN, V53, P383, DOI 10.1007/BF00318204; Lipschutz M., 1969, DIFFERENTIAL GEOMETR; NALWA VS, 1988, INT J COMPUT VISION, V2, P103, DOI 10.1007/BF00133696; NEVATIA R, 1977, ARTIF INTELL, V8, P77, DOI 10.1016/0004-3702(77)90006-6; PENTLAND AP, 1986, ARTIF INTELL, V28, P293, DOI 10.1016/0004-3702(86)90052-4; Pogorelov A.V., 1973, EXTRINSIC GEOMETRY C; Requicha A. A. G., 1980, Computing Surveys, V12, P437, DOI 10.1145/356827.356833; Spivak M., 1965, CALCULUS MANIFOLDS, DOI DOI 10.1201/9780429501906; Stoker JJ, 1969, DIFFERENTIAL GEOMETR; THORPE JA, 1979, ELEMENTARY TOPICS DI; Tikhonov A., 1977, SOLUTIONS ILL POSED; [No title captured]; [No title captured]	27	8	8	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1989	3	2					131	153		10.1007/BF00126429	http://dx.doi.org/10.1007/BF00126429			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AH264					2022-12-18	WOS:A1989AH26400003
J	Kong, Y; Fu, Y				Kong, Yu; Fu, Yun			Human Action Recognition and Prediction: A Survey	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; Action prediction; Video data; Survey	HUMAN ACTION CATEGORIES; VISUAL VOCABULARIES; OBJECT AFFORDANCES; BIOLOGICAL MOTION; PERCEPTION; CONTEXT; DEPTH; REPRESENTATION; VIDEOS; MODELS	Derived from rapid advances in computer vision and machine learning, video analysis tasks have been moving from inferring the present state to predicting the future state. Vision-based action recognition and prediction from videos are such tasks, where action recognition is to infer human actions (present state) based upon complete action executions, and action prediction to predict human actions (future state) based upon incomplete action executions. These two tasks have become particularly prevalent topics recently because of their explosively emerging real-world applications, such as visual surveillance, autonomous driving vehicle, entertainment, and video retrieval, etc. Many attempts have been devoted in the last a few decades in order to build a robust and effective framework for action recognition and prediction. In this paper, we survey the complete state-of-the-art techniques in action recognition and prediction. Existing models, popular algorithms, technical difficulties, popular action databases, evaluation protocols, and promising future directions are also provided with systematic discussions.	[Kong, Yu] Rochester Inst Technol, B Thomas Golisano Coll Comp & Informat Sci, Rochester, NY 14623 USA; [Fu, Yun] Northeastern Univ, Dept ECE, Boston, MA 02115 USA; [Fu, Yun] Northeastern Univ, Coll CIS, Boston, MA 02115 USA	Rochester Institute of Technology; Northeastern University; Northeastern University	Kong, Y (corresponding author), Rochester Inst Technol, B Thomas Golisano Coll Comp & Informat Sci, Rochester, NY 14623 USA.	yu.kong@rit.edu; yunfu@ece.neu.edu						Abbeel P., 2004, P 21 INT C MACHINE L, P1; Abu-El-Haija S, 2016, YOUTUBE 8M LARGE SCA; Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Alahi A, 2014, PROC CVPR IEEE, P2211, DOI 10.1109/CVPR.2014.283; [Anonymous], 2012, UCF101 DATASET 101 H; Ballan L, 2016, LECT NOTES COMPUT SC, V9905, P697, DOI 10.1007/978-3-319-46448-0_42; Bao W., 2021, ICCV; Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bhattacharyya A., 2021, CVPR; Bishay Mina, 2019, BMVC; Blake R, 2007, ANNU REV PSYCHOL, V58, P47, DOI 10.1146/annurev.psych.57.102904.190152; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; Bojanowski P, 2014, LECT NOTES COMPUT SC, V8693, P628, DOI 10.1007/978-3-319-10602-1_41; Bregonzio M, 2009, PROC CVPR IEEE, P1948, DOI 10.1109/CVPRW.2009.5206779; Buchler U, 2018, LECT NOTES COMPUT SC, V11219, P797, DOI 10.1007/978-3-030-01267-0_47; Cao Kaidi, 2020, CVPR; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124; Chen S., 2021, P IEEE CVF INT C COM, P8178; Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16; Choi Wongun, 2009, 2009 IEEE 12 INT C C, P1282; Chung J., 2021, ICCV; Chunhui L., 2017, ARXIV PREPRINT ARXIV; Ciptadi A, 2014, LECT NOTES COMPUT SC, V8690, P695, DOI 10.1007/978-3-319-10605-2_45; Clarke TJ, 2005, PERCEPTION, V34, P1171, DOI 10.1068/p5203; CUTTING JE, 1977, B PSYCHONOMIC SOC, V9, P353, DOI 10.3758/BF03337021; Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44; Darwin C., 1872, EXPRESS EMOT MAN, DOI DOI 10.1037/10001-000; Dawar N, 2018, IEEE SENS J, V18, P9660, DOI 10.1109/JSEN.2018.2872862; Decety J, 1999, TRENDS COGN SCI, V3, P172, DOI 10.1016/S1364-6613(99)01312-1; Dendorfer Patrick, 2021, ICCV; Diba A, 2017, PROC CVPR IEEE, P1541, DOI 10.1109/CVPR.2017.168; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dragan AD, 2011, IEEE INT CONF ROBOT; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Duchenne O, 2009, IEEE I CONF COMP VIS, P1491, DOI 10.1109/ICCV.2009.5459279; DUONG TV, 2005, PROC CVPR IEEE, P838, DOI DOI 10.1109/CVPR.2005.61; Duta IC, 2017, PROC CVPR IEEE, P3205, DOI 10.1109/CVPR.2017.341; Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726; Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47; Fanti C, 2005, PROC CVPR IEEE, P1166; Feichtenhofer C, 2016, ADV NEUR IN, V29; Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Felzenszwalb P, 2008, PROC CVPR IEEE, P1984; Fernando Basura, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P13219, DOI 10.1109/CVPR46437.2021.01302; Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607; Finn C, 2016, PR MACH LEARN RES, V48; Fouhey DF, 2014, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2014.260; Furnari A, 2021, IEEE T PATTERN ANAL, V43, P4021, DOI 10.1109/TPAMI.2020.2992889; Gan C, 2018, PROC CVPR IEEE, P5589, DOI 10.1109/CVPR.2018.00586; Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392; Gao Jiyang, 2017, ARXIV170501180; Gao Mingfei, 2021, CVPR; Geng CX, 2021, IEEE T PATTERN ANAL, V43, P3614, DOI 10.1109/TPAMI.2020.2981604; Ghadiyaram D, 2019, PROC CVPR IEEE, P12038, DOI 10.1109/CVPR.2019.01232; Girase H., 2021, ICCV; Girdhar Rohit, 2017, PROC CVPR IEEE, P971, DOI DOI 10.1109/CVPR.2017.337; Giuliari F, 2021, INT C PATT RECOG, P10335, DOI 10.1109/ICPR48806.2021.9412190; GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; Gu C., 2017, ARXIV PREPRINT ARXIV; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; Guangyi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P643, DOI 10.1007/978-3-030-58598-3_38; Guo M, 2018, LECT NOTES COMPUT SC, V11220, P282, DOI 10.1007/978-3-030-01270-0_17; Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240; Hadfield S, 2013, PROC CVPR IEEE, P3398, DOI 10.1109/CVPR.2013.436; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Hasan M, 2014, LECT NOTES COMPUT SC, V8691, P705, DOI 10.1007/978-3-319-10578-9_46; HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010; Hongjie Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P102, DOI 10.1007/978-3-030-58580-8_7; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HU JF, 2015, PROC CVPR IEEE, P5344; Hu WM, 2007, IEEE T IMAGE PROCESS, V16, P1168, DOI 10.1109/TIP.2006.891352; Huang DA, 2016, LECT NOTES COMPUT SC, V9908, P137, DOI 10.1007/978-3-319-46493-0_9; Huang DA, 2014, LECT NOTES COMPUT SC, V8695, P489, DOI 10.1007/978-3-319-10584-0_32; Ikizler N., 2007, CVPR; Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jia, 2008, CVPR; Jia CC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P87, DOI 10.1145/2647868.2654928; Jiang Y.-G., 2014, THUMOS CHALLENGE ACT; Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560; Kar A, 2017, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR.2017.604; Karaman S., 2014, ECCV THUMOS WORKSHOP, V1, P5; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kay W., 2017, ARXIV PREPRINT ARXIV; Ke Q., 2021, P IEEE CVF WINT C AP; Ke QH, 2019, PROC CVPR IEEE, P9917, DOI 10.1109/CVPR.2019.01016; Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486; Keestra, 2015, TRANSDISCIPLINARITY, P201; Kim K, 2011, IEEE I CONF COMP VIS, P1164, DOI 10.1109/ICCV.2011.6126365; Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15; Klaser Alexander, 2008, BMVC; Kliper-Gross O, 2012, IEEE T PATTERN ANAL, V34, P615, DOI 10.1109/TPAMI.2011.209; Kolahi SS, 2013, IEEE SYMP COMP COMMU; Kong Y., 2014, ECCV WORKSH; Kong Y, 2018, AAAI CONF ARTIF INTE, P7000; Kong Y, 2020, IEEE T PATTERN ANAL, V42, P539, DOI 10.1109/TPAMI.2018.2882805; Kong Y, 2017, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2017.390; Kong Y, 2017, INT J COMPUT VISION, V123, P350, DOI 10.1007/s11263-016-0982-6; Kong Y, 2016, IEEE T PATTERN ANAL, V38, P1844, DOI 10.1109/TPAMI.2015.2491928; Kong Y, 2015, PROC CVPR IEEE, P1054, DOI 10.1109/CVPR.2015.7298708; Kong Y, 2014, LECT NOTES COMPUT SC, V8693, P596, DOI 10.1007/978-3-319-10602-1_39; Kong Y, 2012, LECT NOTES COMPUT SC, V7572, P300, DOI 10.1007/978-3-642-33718-5_22; Kong Y, 2014, IEEE T PATTERN ANAL, V36, P1775, DOI 10.1109/TPAMI.2014.2303090; Kooij JFP, 2014, LECT NOTES COMPUT SC, V8694, P618, DOI 10.1007/978-3-319-10599-4_40; Koppula H., 2013, INT C MACHINE LEARNI, P792, DOI DOI 10.1177/0278364913478446; Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335; Koppula HS, 2013, IEEE INT C INT ROBOT, P2071, DOI 10.1109/IROS.2013.6696634; Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446; Kretzschmar H, 2014, IEEE INT CONF ROBOT, P4015, DOI 10.1109/ICRA.2014.6907442; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Kurakin A, 2012, EUR SIGNAL PR CONF, P1975; Lai SF, 2018, IEEE T IMAGE PROCESS, V27, P2272, DOI 10.1109/TIP.2017.2751145; Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45; Lan T, 2012, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2012.6247821; Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Laptev I, 2007, IEEE I CONF COMP VIS, P2165; Le QV, 2011, PROC CVPR IEEE; Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79; Lee N, 2016, IEEE WINT CONF APPL; Li, 2021, P IEEE CVF C COMP VI, P4751; Li JC, 2019, IEEE INT C INT ROBOT, P6150; Li K, 2014, IEEE T PATTERN ANAL, V36, P1644, DOI 10.1109/TPAMI.2013.2297321; Li K, 2012, LECT NOTES COMPUT SC, V7572, P286, DOI 10.1007/978-3-642-33718-5_21; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Liang JW, 2019, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR.2019.00587; Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399; Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1; Lin YY, 2014, PROC CVPR IEEE, P2617, DOI 10.1109/CVPR.2014.335; Liu JG, 2009, PROC CVPR IEEE, P461, DOI 10.1109/CVPRW.2009.5206845; Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873; Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50; Liu L., 2013, P 23 INT JOINT C ART; Liu X, 2021, PROC CVPR IEEE, P14887, DOI 10.1109/CVPR46437.2021.01465; Liu Y., 2020, ARXIV PREPRINT ARXIV; Liu Y, 2019, PROC CVPR IEEE, P3599, DOI 10.1109/CVPR.2019.00372; Lovegrove S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.93; Lu CW, 2014, PROC CVPR IEEE, P772, DOI 10.1109/CVPR.2014.104; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Luo G, 2014, IEEE T PATTERN ANAL, V36, P2466, DOI 10.1109/TPAMI.2014.2329301; Luo JJ, 2013, IEEE I CONF COMP VIS, P1809, DOI 10.1109/ICCV.2013.227; Luo ZX, 2018, LECT NOTES COMPUT SC, V11213, P170, DOI 10.1007/978-3-030-01240-3_11; MA SG, 2016, PROC CVPR IEEE, P1942, DOI DOI 10.1109/CVPR.2016.214; Ma WC, 2017, PROC CVPR IEEE, P4636, DOI 10.1109/CVPR.2017.493; Mainprice J, 2016, IEEE T ROBOT, V32, P897, DOI 10.1109/TRO.2016.2581216; Mangalam Karttikeya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P759, DOI 10.1007/978-3-030-58536-5_45; Mangalam K., 2020, ARXIV PREPRINT ARXIV; Marchetti F, 2020, PROC CVPR IEEE, P7141, DOI 10.1109/CVPR42600.2020.00717; Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557; Mass J., 1971, MOTION PERCEPTION; Mehrasa N, 2019, PROC CVPR IEEE, P3160, DOI 10.1109/CVPR.2019.00328; Messing R, 2009, IEEE I CONF COMP VIS, P104, DOI 10.1109/ICCV.2009.5459154; Hoai M, 2012, PROC CVPR IEEE, P2863, DOI 10.1109/CVPR.2012.6248012; Mishra A, 2018, IEEE WINT CONF APPL, P372, DOI 10.1109/WACV.2018.00047; Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32; Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443; Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464; Morency L., 2007, CVPR; Morris BT, 2011, IEEE T PATTERN ANAL, V33, P2287, DOI 10.1109/TPAMI.2011.64; Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877; Narayanan S, 2021, PROC CVPR IEEE, P15794, DOI 10.1109/CVPR46437.2021.01554; NI BB, 2011, 2011 IEEE INT C; Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4; Niebles JC, 2007, PROC CVPR IEEE, P1235; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Ofli F, 2013, IEEE WORK APP COMP, P53, DOI 10.1109/WACV.2013.6474999; Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684; Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98; Oza P, 2019, PROC CVPR IEEE, P2302, DOI 10.1109/CVPR.2019.00241; Patron-Perez A, 2012, IEEE T PATTERN ANAL, V34, P2441, DOI 10.1109/TPAMI.2012.24; Pei MT, 2011, IEEE I CONF COMP VIS, P487, DOI 10.1109/ICCV.2011.6126279; Perera Pramuditha, 2020, CVPR; Perrett T, 2021, PROC CVPR IEEE, P475, DOI 10.1109/CVPR46437.2021.00054; Perronnin F, 2007, PROC CVPR IEEE, P2272; Plotz T., 2011, IJCAI; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Purushwalkam S., 2016, ARXIV PREPRINT ARXIV; Qadir O, 2011, IEEE C EVOL COMPUTAT, P208; Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590; Rajko S., 2007, CVPR; Ramanathan V, 2013, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2013.320; Ramezani M, 2016, ARTIF INTELL REV, V46, P485, DOI 10.1007/s10462-016-9473-y; Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342; Raptis M, 2010, LECT NOTES COMPUT SC, V6311, P577, DOI 10.1007/978-3-642-15549-9_42; Rasouli M., 2021, CVPR; Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4; Ricoeur Paul, 1992, ONESELF ANOTHER; Rizzolatti G, 2004, ANNU REV NEUROSCI, V27, P169, DOI 10.1146/annurev.neuro.27.070203.144230; Rizzolatti G, 2010, NAT REV NEUROSCI, V11, P264, DOI 10.1038/nrn2805; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Rohit G., 2021, ICCV; Roitberg A, 2020, IEEE INT VEH SYM, P1048; Ryoo MS, 2015, ACMIEEE INT CONF HUM, P295, DOI 10.1145/2696454.2696462; Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349; Ryoo MS, 2011, INT J COMPUT VISION, V93, P183, DOI 10.1007/s11263-010-0355-5; Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361; Ryoo M. S., 2010, UT INTERACTION DATAS; Ryoo MS, 2006, 2006 IEEE COMPUTER S, V2, P1709, DOI DOI 10.1109/CVPR.2006.242; Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144; Satkin S, 2010, LECT NOTES COMPUT SC, V6311, P536, DOI 10.1007/978-3-642-15549-9_39; Scheirer WJ, 2014, IEEE T PATTERN ANAL, V36, P2317, DOI 10.1109/TPAMI.2014.2321392; Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Scovanner P., 2007, ACM MM, P357; Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312; Shi QF, 2011, INT J COMPUT VISION, V93, P22, DOI 10.1007/s11263-010-0384-0; Shotton J, 2013, IEEE T PATTERN ANAL, V35, P2821, DOI 10.1109/TPAMI.2012.241; Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Shu Y, 2018, IEEE INT CON MULTI; Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132; Simonyan K, 2014, ADV NEUR IN, V27; Singh Sanchit, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P48, DOI 10.1109/AVSS.2010.63; Sminchisescu C, 2005, IEEE I CONF COMP VIS, P1808; Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370; Song L, 2019, PROC CVPR IEEE, P11979, DOI 10.1109/CVPR.2019.01226; Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328; Su H, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2772; Sumi S, 2000, SWISS J PSYCHOL, V59, P126, DOI 10.1024//1421-0185.59.2.126; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721; Sun L, 2014, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2014.336; Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591; Suris D, 2021, PROC CVPR IEEE, P12602, DOI 10.1109/CVPR46437.2021.01242; Tang K., 2012, ADV NEURAL INFORM PR; Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808; Tang YS, 2019, PROC CVPR IEEE, P1207, DOI 10.1109/CVPR.2019.00130; Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11; Tran D, 2008, LECT NOTES COMPUT SC, V5302, P548, DOI 10.1007/978-3-540-88682-2_42; Troje NF, 2005, PERCEPT PSYCHOPHYS, V67, P667, DOI 10.3758/BF03193523; Troje NF, 2002, J VISION, V2, P371, DOI 10.1167/2.5.2; Turek MW, 2010, LECT NOTES COMPUT SC, V6312, P664, DOI 10.1007/978-3-642-15552-9_48; Vahdat A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1729, DOI 10.1109/ICCVW.2011.6130458; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Walker J, 2014, PROC CVPR IEEE, P3302, DOI 10.1109/CVPR.2014.416; Wang, 2012, CVPR; Wang Chien-Yao, 2021, YOU ONLY LEARN ONE R, P7; Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang Heng, 2009, BMVC, P1; Wang J, 2012, LECT NOTES COMPUT SC, V7573, P872, DOI 10.1007/978-3-642-33709-3_62; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang KZ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P97, DOI 10.1145/2647868.2654912; Wang L., 2014, ACTION RECOGNITION D; Wang LA, 2007, PROC CVPR IEEE, P2518; Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang S. B., 2006, PROC IEEE COMPUT SOC, P1521, DOI DOI 10.1109/CVPR.2006.132; Wang XL, 2017, IEEE I CONF COMP VIS, P1338, DOI 10.1109/ICCV.2017.149; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Wang Y., 2008, NIPS; Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214; Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013; Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48; Wolf C, 2014, COMPUT VIS IMAGE UND, V127, P14, DOI 10.1016/j.cviu.2014.06.014; Wong S., 2007, CVPR; Wongun Choi, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3273, DOI 10.1109/CVPR.2011.5995707; Wu BX, 2014, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2014.334; Wu J., 2015, ADV NEURAL INF PROCE, V28, P1, DOI DOI 10.1007/978-3-319-26532-2_15; Wu XX, 2011, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2011.5995624; Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222; Wulfmeier M., 2016, ARXIV PREPRINT ARXIV; Xia L, 2013, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2013.365; Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058; Xu HJ, 2019, IEEE T PATTERN ANAL, V41, P2319, DOI 10.1109/TPAMI.2019.2921539; Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617; Xu MZ, 2019, IEEE I CONF COMP VIS, P5531, DOI 10.1109/ICCV.2019.00563; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang HT, 2018, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2018.00157; Yang S, 2015, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2015.7298769; Yang WF, 2021, PROC CVPR IEEE, P53, DOI 10.1109/CVPR46437.2021.00012; Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108; Yang XT, 2019, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2019.00035; Yang Y, 2019, PATTERN RECOGN, V85, P60, DOI 10.1016/j.patcog.2018.07.030; Yang Y, 2012, LECT NOTES COMPUT SC, V7574, P722, DOI 10.1007/978-3-642-33712-3_52; Yao BP, 2012, LECT NOTES COMPUT SC, V7575, P173, DOI 10.1007/978-3-642-33765-9_13; Yao BP, 2012, IEEE T PATTERN ANAL, V34, P1691, DOI 10.1109/TPAMI.2012.67; Yeffet L, 2009, IEEE I CONF COMP VIS, P492, DOI 10.1109/ICCV.2009.5459201; Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293; Yilmaz A, 2005, PROC CVPR IEEE, P984; Yu G, 2015, LECT NOTES COMPUT SC, V9007, P50, DOI 10.1007/978-3-319-16814-2_4; Yu T.H., 2010, BMVC; Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562; YUAN CF, 2013, PROC CVPR IEEE, P423, DOI DOI 10.1109/CVPR.2013.61; Yuan CF, 2016, INT J COMPUT VISION, V118, P151, DOI 10.1007/s11263-015-0867-0; Yuan CF, 2013, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2013.99; Yuan CF, 2014, IEEE T IMAGE PROCESS, V23, P658, DOI 10.1109/TIP.2013.2291319; Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPR.2009.5206671, 10.1109/CVPRW.2009.5206671]; Yuan Li, 2021, ICCV; Yuan Y, 2021, ARXIV PREPRINT ARXIV; Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719; Zhai XH, 2013, MULTIMEDIA SYST, V19, P395, DOI 10.1007/s00530-012-0297-6; Zhang H, 2017, IEEE T PATTERN ANAL, V39, P1690, DOI 10.1109/TPAMI.2016.2613924; Zhang YP, 2012, IEEE VTS VEH TECHNOL; Zhao H., 2017, ARXIV170509684; Zhao H, 2019, IEEE I CONF COMP VIS, P8667, DOI 10.1109/ICCV.2019.00876; Zhao Hengshuang, 2021, ICCV; Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49; Zhou BL, 2011, PROC CVPR IEEE; Zhu LC, 2018, LECT NOTES COMPUT SC, V11211, P782, DOI 10.1007/978-3-030-01234-2_46; Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885; ZIEBART BD, 2008, AAAI, P322; Ziebart BD, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3931, DOI 10.1109/IROS.2009.5354147	325	7	7	39	65	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2022	130	5					1366	1401		10.1007/s11263-022-01594-9	http://dx.doi.org/10.1007/s11263-022-01594-9		MAR 2022	36	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	0V4WA		Green Submitted			2022-12-18	WOS:000776215100001
J	Damen, D; Doughty, H; Farinella, GM; Furnari, A; Kazakos, E; Ma, J; Moltisanti, D; Munro, J; Perrett, T; Price, W; Wray, M				Damen, Dima; Doughty, Hazel; Farinella, Giovanni Maria; Furnari, Antonino; Kazakos, Evangelos; Ma, Jian; Moltisanti, Davide; Munro, Jonathan; Perrett, Toby; Price, Will; Wray, Michael			Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video dataset; Egocentric vision; First-person vision; Action understanding; Multi-benchmark large-scale dataset; Annotation quality		This paper introduces the pipeline to extend the largest dataset in egocentric vision, EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (Damen in Scaling egocentric vision: ECCV, 2018), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection enables new challenges such as action detection and evaluating the "test of time"-i.e. whether models trained on data collected in 2018 can generalise to new footage collected two years later. The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.	[Damen, Dima; Doughty, Hazel; Kazakos, Evangelos; Ma, Jian; Moltisanti, Davide; Munro, Jonathan; Perrett, Toby; Price, Will; Wray, Michael] Univ Bristol, Bristol, Avon, England; [Farinella, Giovanni Maria; Furnari, Antonino] Univ Catania, Catania, Italy; [Doughty, Hazel] Univ Amsterdam, Amsterdam, Netherlands; [Moltisanti, Davide] Nanyang Technol Univ, Singapore, Singapore	University of Bristol; University of Catania; University of Amsterdam; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Damen, D (corresponding author), Univ Bristol, Bristol, Avon, England.	Dima.Damen@bristol.ac.uk; hazel.doughty@uva.nl; gfarinella@dmi.unict.it; furnari@dmi.unict.it; evangelos.kazakos@bristol.ac.uk; jian.ma@bristol.ac.uk; davide.moltisanti@ntu.edu.sg; jonathan.munro@bristol.ac.uk; toby.Perrett@bristol.ac.uk; will.price@bristol.ac.uk; michael.wray@bristol.ac.uk		Damen, Dima/0000-0001-8804-6238; Doughty, Hazel/0000-0002-3670-3897; Moltisanti, Davide/0000-0003-4265-8882	Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Program (DTP); EPSRC [EP/T004991/1]; Piano della Ricerca 2016-2018 linea di Intervento 2 of DMI; MISE - PON I&C 2014-2020, ENIGMA project [CUP: B61B19000520008]; MIUR AIM - Attrazione e Mobilita Internazionale Linea 1 [AIM1893589, CUP E64118002540007]	Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Program (DTP)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Piano della Ricerca 2016-2018 linea di Intervento 2 of DMI; MISE - PON I&C 2014-2020, ENIGMA project; MIUR AIM - Attrazione e Mobilita Internazionale Linea 1	Research at Bristol is supported by Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Program (DTP), EPSRC Fellowship UMPIRE (EP/T004991/1). Research at Catania is sponsored by Piano della Ricerca 2016-2018 linea di Intervento 2 of DMI, by MISE - PON I&C 2014-2020, ENIGMA project (CUP: B61B19000520008) and by MIUR AIM - Attrazione e Mobilita Internazionale Linea 1 - AIM1893589 - CUP E64118002540007. We thank David Fouhey and Dandan Shan from University of Michigan for providing the ego-trained hand-object detection model prior to its public release. We also thank Sanja Fidler from University of Toronto for contributing to the first edition of EPIC-KITCHENS. We appreciate the efforts of all voluntary participants to collect and narrate this dataset.	Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Bhattacharyya Apratim, 2019, ICLR, P7; Bojanowski P, 2014, LECT NOTES COMPUT SC, V8693, P628, DOI 10.1007/978-3-319-10602-1_41; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Cao Y, 2017, BMVC; Caputo Barbara, 2014, INT C CROSS LANGUAGE, P192; Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Carreira Joao, 2019, ARXIV190706987; Chang CY, 2019, PROC CVPR IEEE, P3541, DOI 10.1109/CVPR.2019.00366; Chen David, 2011, P 49 ANN M ASS COMP, P190; Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218; Cheron G, 2018, ADV NEUR IN, V31; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; DAMEN D, 2012, BMVC; Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44; Dandan Shan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9866, DOI 10.1109/CVPR42600.2020.00989; De Geest R, 2016, LECT NOTES COMPUT SC, V9909, P269, DOI 10.1007/978-3-319-46454-1_17; De la Torre Fernando, 2008, GUIDE CARNEGIE MELLO; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding L, 2018, PROC CVPR IEEE, P6508, DOI 10.1109/CVPR.2018.00681; Fathi A, 2012, LECT NOTES COMPUT SC, V7572, P314, DOI 10.1007/978-3-642-33718-5_23; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Furnari A, 2021, IEEE T PATTERN ANAL, V43, P4021, DOI 10.1109/TPAMI.2020.2992889; Furnari Antonino, 2018, P EUR C COMP VIS ECC; Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10; Geiger A., 2012, P IEEE COMP SOC C CO; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gorban A., 2015, THUMOS CHALLENGE ACT; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Gygli M, 2020, INT J COMPUT VISION, V128, P1061, DOI 10.1007/s11263-019-01255-4; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502; HECKMAN JJ, 1979, ECONOMETRICA, V47, P153, DOI 10.2307/1912352; Heilbron FC, 2018, LECT NOTES COMPUT SC, V11215, P212, DOI 10.1007/978-3-030-01252-6_13; HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698; Honnibal Matthew, 2017, SPACY 2 NATURAL LANG; Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI 10.1109/WACV45572.2020.9093358; Huang XY, 2018, IEEE COMPUT SOC CONF, P1067, DOI 10.1109/CVPRW.2018.00141; Jamal Arshad, 2018, BMVC; James Melville, 2020, Arxiv, DOI arXiv:1802.03426; Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418; Jiang Y.-G., 2014, THUMOS CHALLENGE ACT; JIN XX, 2016, ECCV; Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kay W., 2017, ARXIV PREPRINT ARXIV; Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559; King DB, 2015, ACS SYM SER, V1214, P1; Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Kuehne H, 2014, PROC CVPR IEEE, P780, DOI 10.1109/CVPR.2014.105; Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113; Li J, 2019, IEEE I CONF COMP VIS, P6252, DOI 10.1109/ICCV.2019.00634; Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632; Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718; Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139; Liu Z., 2020, P CVPR, P12406; Lovegrove S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.93; Ma WC, 2017, PROC CVPR IEEE, P4636, DOI 10.1109/CVPR.2017.493; Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498; Mahdisoltani Farzaneh, 2018, ARXIV180409235; Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557; Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036; Mettes P, 2016, LECT NOTES COMPUT SC, V9909, P437, DOI 10.1007/978-3-319-46454-1_27; Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272; Mikolov T., 2013, ARXIV; Moltisanti D, 2019, PROC CVPR IEEE, P9907, DOI 10.1109/CVPR.2019.01015; Moltisanti D, 2017, IEEE I CONF COMP VIS, P2905, DOI 10.1109/ICCV.2017.314; Monfort M, 2020, MOMENTS TIME DATASET; Munro J, 2020, PROC CVPR IEEE, P119, DOI 10.1109/CVPR42600.2020.00020; Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Oberdiek P, 2020, IEEE COMPUT SOC CONF, P1331, DOI 10.1109/CVPRW50498.2020.00172; Pan BX, 2020, AAAI CONF ARTIF INTE, V34, P11815; Paszke A, 2019, ADV NEUR IN, V32; Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149; Peng Xingchao, 2017, VISDA VISUAL DOMAIN; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706; Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560; Pirsiavash H, 2012, PROC CVPR IEEE, P2847, DOI 10.1109/CVPR.2012.6248010; Planamente M, 2021, ARXIV PREPRINT ARXIV; Plizzari C, 2021, ARXIV PREPRINT ARXIV; Qi F, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P429, DOI 10.1145/3240508.3240633; Rasiwasia N, 2014, JMLR WORKSH CONF PRO, V33, P823; Richard A, 2018, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2018.00627; Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Roth J, 2019, AVA ACTIVE SPEAKER A; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Sigurdsson G.A., 2018, ARXIV180409626; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381; Soomro K., 2012, ARXIV; Stein S, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P729, DOI 10.1145/2493432.2493482; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; TORRALBA A, 2011, PROC CVPR IEEE, P1521, DOI DOI 10.1109/CVPR.2011.5995347; Ueberla JP, 1997, INT CONF ACOUST SPEE, P807, DOI 10.1109/ICASSP.1997.596052; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Weinzaepfel Philippe, 2016, ARXIV160505197; Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054; Wulfmeier M, 2018, IEEE INT CONF ROBOT, P4489; Xu JL, 2016, PROC CVPR IEEE, P5356, DOI 10.1109/CVPR.2016.578; Xu TT, 2016, IMAGE VISION COMPUT, V55, P127, DOI 10.1016/j.imavis.2016.01.001; Yang L, 2021, ARXIV PREPRINT ARXIV; Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y; Yogamani S, 2019, IEEE I CONF COMP VIS, P9307, DOI 10.1109/ICCV.2019.00940; Yu Fisher, 2018, BDD100K DIVERSE DRIV, P6; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zhai M, 2017, PROC CVPR IEEE, P4132, DOI 10.1109/CVPR.2017.440; Zhai Xiaohua, 2019, ARXIV191004867; Zhao H, 2019, IEEE I CONF COMP VIS, P8667, DOI 10.1109/ICCV.2019.00876; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49; Zhou B, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aaw6661; Zhou LW, 2019, PROC CVPR IEEE, P6571, DOI 10.1109/CVPR.2019.00674; Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590	129	7	7	2	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					33	55		10.1007/s11263-021-01531-2	http://dx.doi.org/10.1007/s11263-021-01531-2		OCT 2021	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC		hybrid, Green Submitted, Green Published			2022-12-18	WOS:000709268800001
J	Bellitto, G; Salanitri, FP; Palazzo, S; Rundo, F; Giordano, D; Spampinato, C				Bellitto, G.; Proietto Salanitri, F.; Palazzo, S.; Rundo, F.; Giordano, D.; Spampinato, C.			Hierarchical Domain-Adapted Feature Learning for Video Saliency Prediction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video saliency Prediction; Conspicuity networks; Conspicuity maps; Domain adaptation; Gradient reversal layer; Domain specific learning	VISUAL-ATTENTION; MODEL	In this work, we propose a 3D fully convolutional architecture for video saliency prediction that employs hierarchical supervision on intermediate maps (referred to as conspicuity maps) generated using features extracted at different abstraction levels. We provide the base hierarchical learning mechanism with two techniques for domain adaptation and domain-specific learning. For the former, we encourage the model to unsupervisedly learn hierarchical general features using gradient reversal at multiple scales, to enhance generalization capabilities on datasets for which no annotations are provided during training. As for domain specialization, we employ domain-specific operations (namely, priors, smoothing and batch normalization) by specializing the learned features on individual datasets in order to maximize performance. The results of our experiments show that the proposed model yields state-of-the-art accuracy on supervised saliency prediction. When the base hierarchical model is empowered with domain-specific modules, performance improves, outperforming state-of-the-art models on three out of five metrics on the DHF1K benchmark and reaching the second-best results on the other two. When, instead, we test it in an unsupervised domain adaptation setting, by enabling hierarchical gradient reversal layers, we obtain performance comparable to supervised state-of-the-art. Source code, trained models and example outputs are publicly available at https:// github.com/ perceivelab/ hd2s.	[Bellitto, G.; Proietto Salanitri, F.; Palazzo, S.; Giordano, D.; Spampinato, C.] Univ Catania, PeRCeiVe Lab, Catania, Italy; [Rundo, F.] STMicrolectronics, ADG Cent R&D, Catania, Italy	University of Catania; STMicroelectronics	Palazzo, S (corresponding author), Univ Catania, PeRCeiVe Lab, Catania, Italy.	simone.palazzo@unict.it		Proietto Salanitri, Federica/0000-0002-6122-4249; Palazzo, Simone/0000-0002-2441-0982	Universita degli Studi di Catania within the CRUI-CARE Agreement	Universita degli Studi di Catania within the CRUI-CARE Agreement	Open access funding provided by Universita degli Studi di Catania within the CRUI-CARE Agreement.	[Anonymous], 2014, COMPUTER VISION SPOR, DOI DOI 10.1007/978-3-319-09396-3_9; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665; Bazzani Loris, 2016, ARXIV160308199; Borji Ali, 2015, ARXIV150503581; Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601; Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753; Che ZH, 2020, IEEE T IMAGE PROCESS, V29, P2287, DOI 10.1109/TIP.2019.2945857; Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672; Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Droste Richard, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P419, DOI 10.1007/978-3-030-58558-7_25; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785; Ganin Y, 2016, J MACH LEARN RES, V17; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guraya FFE, 2010, PROCEEDINGS OF THE NINTH INTERNATIONAL SYMPOSIUM ON DISTRIBUTED COMPUTING AND APPLICATIONS TO BUSINESS, ENGINEERING AND SCIENCE (DCABES 2010), P508, DOI 10.1109/DCABES.2010.160; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688; Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jia S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103887; Jiang L., 2017, ARXIV PREPRINT ARXIV; Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37; JIANG M, 2015, PROC CVPR IEEE, P1072, DOI DOI 10.1109/CVPR.2015.7298710; Judd T., 2012, MIT TECHNICAL REPORT; Kan MN, 2015, IEEE I CONF COMP VIS, P3846, DOI 10.1109/ICCV.2015.438; Kay W., 2017, ARXIV PREPRINT ARXIV; King DB, 2015, ACS SYM SER, V1214, P1; Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004; Kummerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513; Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112; Li JN, 2018, ADV NEUR IN, V31; Li S, 2007, INT CONF ACOUST SPEE, P1073; Li Yanghao, 2016, ARXIV160304779; Lim MK, 2014, INT C PATT RECOG, P3957, DOI 10.1109/ICPR.2014.678; Linardos P, 2019, BRIT MACH VIS C; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Long MS, 2015, PR MACH LEARN RES, V37, P97; Lu L., 2017, ICAC, P1; Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557; Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154; Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248; Nguyen T.V., 2013, ACM MM, P987; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71; Pan Junting, 2017, ABS170101081 CORR; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shao J, 2005, INT CONF ACOUST SPEE, P233; Shokri M, 2020, J VIS COMMUN IMAGE R, V68, DOI 10.1016/j.jvcir.2020.102769; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Sun MJ, 2019, IEEE T CYBERNETICS, V49, P2900, DOI 10.1109/TCYB.2018.2832053; Tang YX, 2016, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2016.233; Tong YB, 2011, COGN COMPUT, V3, P241, DOI 10.1007/s12559-010-9094-8; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Wang HY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1519, DOI 10.1145/3240508.3240677; Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417; Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941; Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514; Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612; Wu XY, 2020, AAAI CONF ARTIF INTE, V34, P12410; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Yang C, 2018, INT CONF GEOINFORM; Zhang J, 2018, PROC CVPR IEEE, P9029, DOI 10.1109/CVPR.2018.00941; ZHANG PP, 2017, IEEE I CONF COMP VIS, P202, DOI DOI 10.1109/ICCV.2017.31; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223	73	7	7	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3216	3232		10.1007/s11263-021-01519-y	http://dx.doi.org/10.1007/s11263-021-01519-y		OCT 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN		Green Submitted, hybrid			2022-12-18	WOS:000703802800001
J	Wang, GQ; Sun, CM; Sowmya, A				Wang, Guoqing; Sun, Changming; Sowmya, Arcot			Context-Enhanced Representation Learning for Single Image Deraining	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image deraining; Encoder-decoder network; Imbalanced distribution; Context-enhanced representation learning	REMOVAL; RAIN	Perception of content and structure in images with rainstreaks or raindrops is challenging, and it often calls for robust deraining algorithms to remove the diversified rainy effects. Much progress has been made on the design of advanced encoder-decoder single image deraining networks. However, most of the existing networks are built in a blind manner and often produce over/under-deraining artefacts. In this paper, we point out, for the first time, that the unsatisfactory results are caused by the highly imbalanced distribution between rainy effects and varied background scenes. Ignoring this phenomenon results in the representation learned by the encoder being biased towards rainy regions, while paying less attention to the valuable contextual regions. To resolve this, a context-enhanced representation learning and deraining network is proposed with a novel two-branch encoder design. Specifically, one branch takes the rainy image directly as input for learning a mixed representation depicting the variation of both rainy regions and contextual regions, and another branch is guided by a carefully learned soft attention mask to learn an embedding only depicting the contextual regions. By combining the embeddings from these two branches with a carefully designed co-occurrence modelling module, and then improving the semantic property of the co-occurrence features via a bi-directional attention layer, the underlying imbalanced learning problem is resolved. Extensive experiments are carried out for removing rainstreaks and raindrops from both synthetic and real rainy images, and the proposed model is demonstrated to produce significantly better results than state-of-the-art models. In addition, comprehensive ablation studies are also performed to analyze the contributions of different designs. Code and pre-trained models will be publicly available at .	[Wang, Guoqing; Sun, Changming] CSIRO Data61, POB 76, Epping, NSW 1710, Australia; [Wang, Guoqing; Sun, Changming; Sowmya, Arcot] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia	Commonwealth Scientific & Industrial Research Organisation (CSIRO); University of New South Wales Sydney	Sun, CM (corresponding author), CSIRO Data61, POB 76, Epping, NSW 1710, Australia.; Sun, CM (corresponding author), Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.	changming.sun@csiro.au	Sun, Changming/A-3276-2008	Sun, Changming/0000-0001-5943-1989				Abadi M., USENIX S OP SYST DES; Abel G., ADV NEURAL INFORM PR; [Anonymous], IEEE T PATTERN ANAL; Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bossu J, 2011, INT J COMPUT VISION, V93, P348, DOI 10.1007/s11263-011-0421-7; Chen X., ADV NEURAL INFORM PR; Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Choo J, P IEEE C COMP VIS PA; Chuang Y.Y., P IEEE C COMP VIS PA; Dabov K, 2007, IEEE IMAGE PROC, P313, DOI 10.1109/icip.2007.4378954; Efros A.A., P IEEE C COMP VIS PA; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186; Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802; Gang S., P IEEE C COMP VIS PA; Garg K, 2004, PROC CVPR IEEE, P528; Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6; Ge Y., ADV NEURAL INFORM PR; He K., P IEEE C COMP VIS PA; Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759; Huang G., P IEEE C COMP VIS PA; Huang X., P EUR C COMP VIS ECC; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057; Kingma D.P., P INT C LEARN REPR I; Koltun V, P INT C LEARN REPR I; Li G., P ACM INT C MULT ACM; Li S., P IEEE C COMP VIS PA; Li X., P EUR C COMP VIS ECC; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu N., JOINT IAPR INT WORKS; Locatello F., ADV NEURAL INFORM PR; Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388; Mao X., ADV NEURAL INFORM PR; Mao X., P IEEE INT C COMP VI; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Max W, P INT C LEARN REPR I; Patel V.M., P IEEE C COMP VIS PA; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263; Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406; Ren W., P IEEE C COMP VIS PA; Ronneberger O., P INT C MED IM COMP; Santhaseelan V, 2015, INT J COMPUT VISION, V112, P71, DOI 10.1007/s11263-014-0759-8; Simonyan K., P INT C LEARN REPR I; Sun S.H., P IEEE C IM PROC ICI; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; Tschannen M., ADV NEURAL INFORM PR; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang GQ, 2019, IEEE I CONF COMP VIS, P5643, DOI 10.1109/ICCV.2019.00574; Wang T., P IEEE C COMP VIS PA; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400; Yang F.E., P IEEE C COMP VIS PA; Yasarla R, 2020, IEEE T IMAGE PROCESS, V29, P4544, DOI 10.1109/TIP.2020.2973802; You S, 2016, IEEE T PATTERN ANAL, V38, P1721, DOI 10.1109/TPAMI.2015.2491937; Zhang H, P IEEE C COMP VIS PA; Zhang K., P IEEE C COMP VIS PA; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhou BL, 2019, IEEE T PATTERN ANAL, V41, P2131, DOI 10.1109/TPAMI.2018.2858759	64	7	8	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2021	129	5					1650	1674		10.1007/s11263-020-01425-9	http://dx.doi.org/10.1007/s11263-020-01425-9		MAR 2021	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RY0YC					2022-12-18	WOS:000624439800004
J	Jiang, HZ; Li, YX; Zhao, HJ; Li, XD; Yang, X				Jiang, Hongzhi; Li, Yuxi; Zhao, Huijie; Li, Xudong; Yang, Xu			Parallel Single-Pixel Imaging: A General Method for Direct-Global Separation and 3D Shape Reconstruction Under Strong Global Illumination	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Light transport; Computational photography; Single-pixel imaging; Direct illumination; Global illumination; 3D shape reconstruction	TRANSLUCENT OBJECTS; INTERREFLECTIONS	We present parallel single-pixel imaging (PSI), a photography technique that captures light transport coefficients and enables the separation of direct and global illumination, to achieve 3D shape reconstruction under strong global illumination. PSI is achieved by extending single-pixel imaging (SI) to modern digital cameras. Each pixel on an imaging sensor is considered an independent unit that can obtain an image using the SI technique. The obtained images characterize the light transport behavior between pixels on the projector and the camera. However, the required number of SI illumination patterns generally becomes unacceptably large in practical situations. We introduce local region extension (LRE) method to accelerate the data acquisition of PSI. LRE perceives that the visible region of each camera pixel accounts for a local region. Thus, the number of detected unknowns is determined by local region area, which is extremely beneficial in terms of data acquisition efficiency. PSI possesses several properties and advantages. For instance, PSI captures the complete light transport coefficients between the projector-camera pair, without making specific assumptions on measured objects and without requiring special hardware and restrictions on the arrangement of the projector-camera pair. The perfect reconstruction property of LRE can be proven mathematically. The acquisition and reconstruction stages are straightforward and easy to implement in the existing projector-camera systems. These properties and advantages make PSI a general and sound theoretical model to decompose direct and global illuminations and perform 3D shape reconstruction under global illumination.	[Jiang, Hongzhi; Li, Yuxi; Zhao, Huijie; Li, Xudong; Yang, Xu] Beihang Univ BUAA, Sch Instrumentat & Optoelect Engn, Minist Educ, Key Lab Precis Optomechatron Technol, 37 Xueyuan Rd, Beijing 100191, Peoples R China	Beihang University	Zhao, HJ (corresponding author), Beihang Univ BUAA, Sch Instrumentat & Optoelect Engn, Minist Educ, Key Lab Precis Optomechatron Technol, 37 Xueyuan Rd, Beijing 100191, Peoples R China.	jhz1862@buaa.edu.cn; yuxili@buaa.edu.cn; buaa_zhaohj@163.com; xdli@buaa.edu.cn; xuyang17@buaa.edu.cn		Yuxi, Li/0000-0002-1922-5811	National Natural Science Foundation of China (NSFC) [61875007, 61735003]; National Key R&D Program of China [2020YFB2010701, 6141B061106]	National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China	This work was supported by National Natural Science Foundation of China (NSFC) (61875007, 61735003), National Key R&D Program of China (2020YFB2010701), and foundation (6141B061106).	Bian LH, 2016, SCI REP-UK, V6, DOI 10.1038/srep24752; Chan WL, 2008, APPL PHYS LETT, V93, DOI 10.1063/1.2989126; Chen CH, 2007, IEEE C EVOL COMPUTAT, P1; Chen HM, 2014, SMART SCI, V2, P132, DOI 10.1080/23080477.2014.11665616; Chen HJ, 2015, PROC CVPR IEEE, P2358, DOI 10.1109/CVPR.2015.7298849; Chen TC, 2009, INVERSE PROBL SCI EN, V17, P213, DOI 10.1080/17415970802082815; Chiba N., 2017, IEEE INT C MECH AUT; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; Devaux F, 2016, OPTICA, V3, P698, DOI 10.1364/OPTICA.3.000698; Edgar MP, 2019, NAT PHOTONICS, V13, P13, DOI 10.1038/s41566-018-0300-7; Ferri F, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.253603; Garg G., 2006, EUROGRAPHICS S RENDE, P251; Gungor A, 2018, IEEE IMAGE PROC, P1827, DOI 10.1109/ICIP.2018.8451675; Gupta M, 2013, INT J COMPUT VISION, V102, P33, DOI 10.1007/s11263-012-0554-3; Gupta M, 2012, PROC CVPR IEEE, P813, DOI 10.1109/CVPR.2012.6247753; Hahn J, 2014, DIGIT SIGNAL PROCESS, V26, P113, DOI 10.1016/j.dsp.2013.12.001; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; Jiang H., 2017, C 3D MEASN INT MAN; Jiang HZ, 2019, OPT EXPRESS, V27, P33564, DOI 10.1364/OE.27.033564; Jiang HZ, 2017, OPT EXPRESS, V25, P15118, DOI 10.1364/OE.25.015118; Lutzke P, 2011, OPT ENG, V50, DOI 10.1117/1.3582858; Masselus V, 2003, ACM T GRAPHIC, V22, P613, DOI 10.1145/882262.882315; Meng WW, 2019, OPT EXPRESS, V27, P31490, DOI 10.1364/OE.27.031490; Nayar SK, 2006, ACM T GRAPHIC, V25, P935, DOI 10.1145/1141911.1141977; NAYAR SK, 1991, INT J COMPUT VISION, V6, P173, DOI 10.1007/BF00115695; O'Toole M, 2014, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR.2014.421; O'Toole M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185535; O'Toole M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866165; Park J, 2009, IEEE T CONSUM ELECTR, V55, P987, DOI 10.1109/TCE.2009.5278053; Peers P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477929; Pharr M., 2017, PHYS BASED RENDERING, V3rd, P348; Phillips DB, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1601782; Radwell N, 2014, OPTICA, V1, P285, DOI 10.1364/OPTICA.1.000285; Ren PR, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766899; Ryczkowski P, 2016, NAT PHOTONICS, V10, P167, DOI [10.1038/NPHOTON.2015.274, 10.1038/nphoton.2015.274]; Schechner YY, 2007, IEEE T PATTERN ANAL, V29, P1339, DOI 10.1109/TPAMI.2007.1151; Sen P, 2005, ACM T GRAPHIC, V24, P745, DOI 10.1145/1073204.1073257; Sen P, 2009, COMPUT GRAPH FORUM, V28, P609, DOI 10.1111/j.1467-8659.2009.01401.x; Sun B, 2013, SCIENCE, V340, P844, DOI 10.1126/science.1234454; Sun BQ, 2012, OPT EXPRESS, V20, P16892, DOI 10.1364/OE.20.016892; Sun MJ, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12010; Tarjan R., 1972, SIAM Journal on Computing, V1, P146, DOI 10.1137/0201010; Wang YW, 2016, IEEE PHOTONIC TECH L, V28, P288, DOI 10.1109/LPT.2015.2494878; Watts CM, 2014, NAT PHOTONICS, V8, P605, DOI [10.1038/NPHOTON.2014.139, 10.1038/nphoton.2014.139]; Xu Y, 2019, OPT EXPRESS, V27, P18421, DOI 10.1364/OE.27.018421; Zhang Z, 1999, P 7 IEEE INT C COMP, V22, P1330, DOI DOI 10.1109/34.888718; Zhang ZB, 2017, OPT EXPRESS, V25, P19619, DOI 10.1364/OE.25.019619; Zhang ZB, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7225; Zhao HJ, 2018, OPT EXPRESS, V26, P7117, DOI 10.1364/OE.26.007117	49	7	7	11	31	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1060	1086		10.1007/s11263-020-01413-z	http://dx.doi.org/10.1007/s11263-020-01413-z		JAN 2021	27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK		hybrid			2022-12-18	WOS:000605541100002
J	Yu, Q; Song, JF; Song, YZ; Xiang, T; Hospedales, TM				Yu, Qian; Song, Jifei; Song, Yi-Zhe; Xiang, Tao; Hospedales, Timothy M.			Fine-Grained Instance-Level Sketch-Based Image Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Fine-grained; Sketch understanding; Image retrieval; Cross-modality; Deep learning		The problem of fine-grained sketch-based image retrieval (FG-SBIR) is defined and investigated in this paper. In FG-SBIR, free-hand human sketch images are used as queries to retrieve photo images containing the same object instances. It is thus a cross-domain (sketch to photo) instance-level retrieval task. It is an extremely challenging problem because (i) visual comparisons and matching need to be executed under large domain gap, i.e., from black and white line drawing sketches to colour photos; (ii) it requires to capture the fine-grained (dis)similarities of sketches and photo images while free-hand sketches drawn by different people present different levels of deformation and expressive interpretation; and (iii) annotated cross-domain fine-grained SBIR datasets are scarce, challenging many state-of-the-art machine learning techniques, particularly those based on deep learning. In this paper, for the first time, we address all these challenges, providing a step towards the capabilities that would underpin a commercial sketch-based object instance retrieval application. Specifically, a new large-scale FG-SBIR database is introduced which is carefully designed to reflect the real-world application scenarios. A deep cross-domain matching model is then formulated to solve the intrinsic drawing style variability, large domain gap issues, and capture instance-level discriminative features. It distinguishes itself by a carefully designed attention module. Extensive experiments on the new dataset demonstrate the effectiveness of the proposed model and validate the need for a rigorous definition of the FG-SBIR problem and collecting suitable datasets.	[Yu, Qian] Beihang Univ, Beijing, Peoples R China; [Yu, Qian; Song, Jifei; Song, Yi-Zhe; Xiang, Tao; Hospedales, Timothy M.] Univ Surrey, SketchX, CVSSP, Surrey, England; [Hospedales, Timothy M.] Univ Edinburgh, Edinburgh, Midlothian, Scotland	Beihang University; University of Surrey; University of Edinburgh	Yu, Q (corresponding author), Beihang Univ, Beijing, Peoples R China.; Yu, Q (corresponding author), Univ Surrey, SketchX, CVSSP, Surrey, England.	qianyu@buaa.edu.cn; j.song@qmul.ac.uk; y.song@surrey.ac.uk; t.xiang@surrey.ac.uk; t.hospedales@ed.ac.uk		Song, Yi-Zhe/0000-0001-5908-3275				Bui T., 2016, ARXIV161105301; Bui T, 2018, COMPUT GRAPH-UK, V71, P77, DOI 10.1016/j.cag.2017.12.006; Cao Y., 2010, INT C MULT, P1605; Cao Y, 2011, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2011.5995460; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Collomosse J, 2017, IEEE I CONF COMP VIS, P2679, DOI 10.1109/ICCV.2017.290; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540; Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266; Eitz M, 2010, COMPUT GRAPH-UK, V34, P482, DOI 10.1016/j.cag.2010.07.002; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fukui Akira, 2016, ARXIV160601847; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; Gygli M, 2013, IEEE I CONF COMP VIS, P1633, DOI 10.1109/ICCV.2013.205; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu R, 2013, COMPUT VIS IMAGE UND, V117, P790, DOI 10.1016/j.cviu.2013.02.005; Hu R, 2010, IEEE IMAGE PROC, P1025, DOI 10.1109/ICIP.2010.5649331; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; James S., 2014, P INT C MULT RETR; Jiang Y. G., 2013, AAAI C ART INT; Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990; Krizhevsky A., 2011, P EUR S ART NEUR NET, P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Landay JA, 2001, COMPUTER, V34, P56, DOI 10.1109/2.910894; Li K, 2017, IEEE T IMAGE PROCESS, V26, P5908, DOI 10.1109/TIP.2017.2745106; Li Y, 2015, COMPUT VIS IMAGE UND, V137, P1, DOI 10.1016/j.cviu.2015.02.003; Li Yi, 2014, BMVC; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Lin YL, 2013, IEEE I CONF COMP VIS, P3495, DOI 10.1109/ICCV.2013.434; Liu Y, 2017, LECT NOTES COMPUT SC, V10132, P277, DOI 10.1007/978-3-319-51811-4_23; Lu Jiasen, 2016, ABS161201887 CORR; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Marr D., 1982, VISION; Mnih V., 2014, NEURAL INFORM PROCES, DOI DOI 10.48550/ARXIV.1406.6247; Moulin C, 2014, PATTERN RECOGN, V47, P260, DOI 10.1016/j.patcog.2013.06.003; Nam H., 2016, DUAL ATTENTION NETWO; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374; Philbin J, 2008, PROC CVPR IEEE, P2285; Prosser B. J., 2010, PROC BRIT MACH VIS C, P6, DOI DOI 10.5244/C.24.21; Qin J, 2017, PROC CVPR IEEE, P6728, DOI 10.1109/CVPR.2017.712; Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566; Radenovic F, 2018, LECT NOTES COMPUT SC, V11209, P774, DOI 10.1007/978-3-030-01228-1_46; Ren XF, 2008, LECT NOTES COMPUT SC, V5304, P533; Rui Hu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3661, DOI 10.1109/ICIP.2011.6116513; Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954; Sermanet P, 2014, ARXIV PREPRINT ARXIV; Song JF, 2017, IEEE I CONF COMP VIS, P5552, DOI 10.1109/ICCV.2017.592; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang C., 2010, P 19 INT C WORLD WID; Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144; Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32; Yu Q, 2017, INT J COMPUT VISION, V122, P411, DOI 10.1007/s11263-016-0932-3; [于谦 Yu Qian], 2015, [高分子通报, Polymer Bulletin], P1; Yu Q, 2016, IEEE CONF COMPUT; Zhang JY, 2018, LECT NOTES COMPUT SC, V11206, P304, DOI 10.1007/978-3-030-01216-8_19; Zhu JY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601145; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	69	7	8	4	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					484	500		10.1007/s11263-020-01382-3	http://dx.doi.org/10.1007/s11263-020-01382-3		SEP 2020	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF					2022-12-18	WOS:000574133000002
J	Feng, ZH; Kittler, J; Awais, M; Wu, XJ				Feng, Zhen-Hua; Kittler, Josef; Awais, Muhammad; Wu, Xiao-Jun			Rectified Wing Loss for Efficient and Robust Facial Landmark Localisation with Convolutional Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Facial landmark localisation; Deep convolutional neural networks; Rectified Wing Loss; Pose-based data balancing; Coarse-to-fine networks	FACE ALIGNMENT; REGRESSION; CASCADE	Efficient and robust facial landmark localisation is crucial for the deployment of real-time face analysis systems. This paper presents a new loss function, namely Rectified Wing (RWing) loss, for regression-based facial landmark localisation with Convolutional Neural Networks (CNNs). We first systemically analyse different loss functions, including L2, L1 and smooth L1. The analysis suggests that the training of a network should pay more attention to small-medium errors. Motivated by this finding, we design a piece-wise loss that amplifies the impact of the samples with small-medium errors. Besides, we rectify the loss function for very small errors to mitigate the impact of inaccuracy of manual annotation. The use of our RWing loss boosts the performance significantly for regression-based CNNs in facial landmarking, especially for lightweight network architectures. To address the problem of under-representation of samples with large pose variations, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation strategies. Last, the proposed approach is extended to create a coarse-to-fine framework for robust and efficient landmark localisation. Moreover, the proposed coarse-to-fine framework is able to deal with the small sample size problem effectively. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits of our RWing loss and prove the superiority of the proposed method over the state-of-the-art approaches.	[Feng, Zhen-Hua; Kittler, Josef; Awais, Muhammad] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England; [Wu, Xiao-Jun] Jiangnan Univ, Sch Internet Things Engn, Wuxi 214122, Jiangsu, Peoples R China	University of Surrey; Jiangnan University	Feng, ZH (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.	z.feng@surrey.ac.uk; j.kittler@surrey.ac.uk; m.a.rana@surrey.ac.uk; wu_xiaojun@jiangnan.edu.cn	Awais, Muhammad/HCI-3725-2022; Feng, Zhenhua/T-3139-2019	Feng, Zhenhua/0000-0002-4485-4249	EPSRC [EP/R018456/1, EP/N007743/1] Funding Source: UKRI	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))		Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bansal Ankan, 2017, 2017 IEEE International Joint Conference on Biometrics (IJCB), P464, DOI 10.1109/BTAS.2017.8272731; Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602; Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600; Bhagavatula C, 2017, IEEE I CONF COMP VIS, P4000, DOI 10.1109/ICCV.2017.429; Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Cootes T. F., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P227, DOI 10.1109/AFGR.2000.840639; Cootes TF, 2002, IMAGE VISION COMPUT, V20, P657, DOI 10.1016/S0262-8856(02)00055-0; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cristinacce D., 2006, P BRIT MACH VIS C, V3, P929; Cui Z, 2019, IEEE T PATTERN ANAL, V41, P1271, DOI 10.1109/TPAMI.2018.2828424; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Deng JK, 2019, INT J COMPUT VISION, V127, P599, DOI 10.1007/s11263-018-1134-y; Deng JK, 2019, IEEE T IMAGE PROCESS, V28, P3636, DOI 10.1109/TIP.2019.2899267; Deng JK, 2016, IMAGE VISION COMPUT, V47, P19, DOI 10.1016/j.imavis.2015.11.005; Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094; Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047; Dong Y, 2015, COMPUT STAND INTER, V42, P105, DOI 10.1016/j.csi.2015.06.004; ech J, 2016, COMPUTING, V47, P60; FAN HQ, 2016, COMPUTING, V47, P27, DOI DOI 10.1016/J.IMAVIS.2015.11.004; Feng ZH, 2019, IEEE SIGNAL PROC LET, V26, P450, DOI 10.1109/LSP.2019.2895291; Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238; Feng ZH, 2018, IEEE INT CONF AUTOMA, P780, DOI 10.1109/FG.2018.00123; Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392; Feng ZH, 2015, IEEE T IMAGE PROCESS, V24, P3425, DOI 10.1109/TIP.2015.2446944; Feng ZH, 2015, IEEE SIGNAL PROC LET, V22, P76, DOI 10.1109/LSP.2014.2347011; Gao XB, 2010, IEEE T SYST MAN CY C, V40, P145, DOI 10.1109/TSMCC.2009.2035631; Ghiasi G, 2014, PROC CVPR IEEE, P1899, DOI 10.1109/CVPR.2014.306; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Guler RA, 2017, PROC CVPR IEEE, P2614, DOI 10.1109/CVPR.2017.280; Hara K, 2014, PROCEEDINGS OF 2014 INTERNATIONAL SYMPOSIUM ON ELECTRICAL INSULATING MATERIALS (ISEIM 2014), P526, DOI 10.1109/ISEIM.2014.6870835; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Howard A.G., 2017, MOBILENETS EFFICIENT; HUANG Z, 2015, ARXIV151104901; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Jourabloo A., 2017, IEEE INT C COMP VIS; Jourabloo A, 2017, INT J COMPUT VISION, V124, P187, DOI 10.1007/s11263-017-1012-z; Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; Kingma D.P, P 3 INT C LEARNING R; Kittler J, 2016, LECT NOTES COMPUT SC, V9756, P185, DOI 10.1007/978-3-319-41778-3_19; Koestinger M., 2011, ICCV WORKSH, DOI [10.1109/ICCVW.2011.6130513, DOI 10.1109/ICCVW.2011.6130513]; Koppen P, 2018, PATTERN RECOGN, V74, P617, DOI 10.1016/j.patcog.2017.09.006; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551; LIANG Z, 2015, ARXIV150703409; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442; Liu ZW, 2019, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2019.00358; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Luo P, 2012, PROC CVPR IEEE, P2480, DOI 10.1109/CVPR.2012.6247963; Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393; Martinez B, 2016, IMAGE VISION COMPUT, V47, P36, DOI 10.1016/j.imavis.2015.09.003; Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523; Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35; Messer K., 1999, 2 INT C AUDIO VIDEO, P965; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Parkhi Omkar M., 2015, BRIT MACH VIS C; Phillips J.J., 2005, PROC CVPR IEEE, V1, P947, DOI DOI 10.1109/CVPR.2005.268; Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233; Rashid M, 2017, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2017.174; Ren SQ, 2016, IEEE T IMAGE PROCESS, V25, P1233, DOI 10.1109/TIP.2016.2518867; Romdhani S., 1999, BMVC99. Proceedings of the 10th British Machine Vision Conference, P483; Roth J., 2016, IEEE C COMP VIS PATT; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Sun KY, 2017, I C VIRTUAL REALITY, P160, DOI 10.1109/ICVRV.2017.00040; Sun X, 2015, PROC CVPR IEEE, P824, DOI 10.1109/CVPR.2015.7298683; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453; Uia M, 2016, COMPUTING, V47, P45; Walecki R, 2016, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2016.530; Wang NN, 2018, NEUROCOMPUTING, V275, P50, DOI 10.1016/j.neucom.2017.05.013; Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508; Wu WY, 2017, IEEE COMPUT SOC CONF, P2096, DOI 10.1109/CVPRW.2017.261; Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227; Wu Y, 2018, COMPUTING, V73, P1; Wu Y, 2019, INT J COMPUT VISION, V127, P115, DOI 10.1007/s11263-018-1097-z; Wu Yue, 2018, IEEE Trans Pattern Anal Mach Intell, V40, P3067, DOI 10.1109/TPAMI.2017.2787130; Wu Y, 2017, PROC CVPR IEEE, P5719, DOI 10.1109/CVPR.2017.606; Xiao ST, 2017, IEEE I CONF COMP VIS, P1642, DOI 10.1109/ICCV.2017.181; Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4; Xing JL, 2018, IEEE T PATTERN ANAL, V40, P987, DOI 10.1109/TPAMI.2017.2697958; Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Xu X, 2017, IEEE INT CONF AUTOMA, P642, DOI 10.1109/FG.2017.81; Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253; Yu X, 2016, LECT NOTES COMPUT SC, V9909, P52, DOI 10.1007/978-3-319-46454-1_4; Zeiler Matthew D, 2012, ARXIV12125701; Zeng JJ, 2018, IEEE T IMAGE PROCESS, V27, P2096, DOI 10.1109/TIP.2017.2784571; Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52; Zhang J, 2016, PROC CVPR IEEE, P3428, DOI 10.1109/CVPR.2016.373; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhu ML, 2019, PROC CVPR IEEE, P3481, DOI 10.1109/CVPR.2019.00360; Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	106	7	7	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2126	2145		10.1007/s11263-019-01275-0	http://dx.doi.org/10.1007/s11263-019-01275-0			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG		hybrid, Green Submitted			2022-12-18	WOS:000559365500008
J	Steger, C; Ulrich, M				Steger, Carsten; Ulrich, Markus			A Camera Model for Line-Scan Cameras with Telecentric Lenses	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Line-scan cameras; Telecentric lenses; Camera models; Camera model degeneracies; Camera calibration	CALIBRATION; GEOMETRY	We propose a camera model for line-scan cameras with telecentric lenses. The camera model assumes a linear relative motion with constant velocity between the camera and the object. It allows to model lens distortions, while supporting arbitrary positions of the line sensor with respect to the optical axis. We comprehensively examine the degeneracies of the camera model and propose methods to handle them. Furthermore, we examine the relation of the proposed camera model to affine cameras. In addition, we propose an algorithm to calibrate telecentric line-scan cameras using a planar calibration object. We perform an extensive evaluation of the proposed camera model that establishes the validity and accuracy of the proposed model. We also show that even for lenses with very small lens distortions, the distortions are statistically highly significant. Therefore, they cannot be omitted in real-world applications.	[Steger, Carsten; Ulrich, Markus] MVTec Software GmbH, Arnulfstr 205, D-80634 Munich, Germany		Steger, C (corresponding author), MVTec Software GmbH, Arnulfstr 205, D-80634 Munich, Germany.	steger@mvtec.com; ulrich@mvtec.com	Ulrich, Markus/F-4826-2014	Ulrich, Markus/0000-0001-8457-5554; Steger, Carsten/0000-0003-3426-1703				Beyerer J., 2016, MACHINE VISION AUTOM; Blahusch G., 1999, QCAV99. 5th International Conference on Quality Control by Artificial Vision. Proceedings, P31; BROWN DC, 1971, PHOTOGRAMM ENG, V37, P855; Brown Duane C, 1966, PHOTOGRAMMETRIC ENG, P2, DOI DOI 10.1234/12345678; Chen TE, 2003, PHOTOGRAMM ENG REM S, V69, P71, DOI 10.14358/PERS.69.1.71; de Oliveira O, 2013, REAL ANAL EXCH, V39, P207; Donne S, 2017, IEEE IMAGE PROC, P36; Drareni J, 2011, INT J COMPUT VISION, V91, P146, DOI 10.1007/s11263-010-0349-3; Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658; Fitzgibbon AW, 2001, PROC CVPR IEEE, P125; Grun A., 1978, INT C S COMM 5 INT S; Gupta R, 1997, IEEE T PATTERN ANAL, V19, P963, DOI 10.1109/34.615446; Haala N., 1998, ISPRS COMM 3 S OBJ R, P23; HORAUD R, 1993, IEEE T ROBOTIC AUTOM, V9, P71, DOI 10.1109/70.210796; Hui B., 2012, OPTICAL ENG, V51; Hui B., 2012, OPTICAL ENG, V51; Hui BW, 2013, IEEE T INSTRUM MEAS, V62, P2567, DOI 10.1109/TIM.2013.2256815; Johnson FC, 2003, INFORM RES, V8; Lanser S, 1997, THESIS; Lanser S, 1995, MUSTERERKENNUNG, P481, DOI 10.1007/978-3-642-79980-8\_57; Lee C, 2000, PHOTOGRAMM ENG REM S, V66, P385; Lenhardt K, 2017, HDB MACHINE COMPUTER, V2, P179; LENZ R, 1990, ISPRS J PHOTOGRAMM, V45, P90, DOI 10.1016/0924-2716(90)90095-S; Lenz R, 1987, MUSTERERKENNUNG, P212; Lenz R., 1988, VIEDEOMETRIE CCD SEN; Lilienblum E, 2013, LECT NOTES COMPUT SC, V8142, P81, DOI 10.1007/978-3-642-40602-7_9; Luna CA, 2010, IEEE T INSTRUM MEAS, V59, P2185, DOI 10.1109/TIM.2009.2031344; Luster SD, 2012, MACHINE VISION HANDBOOK, VOLS 1-3, P259, DOI 10.1007/978-1-84996-169-1_6; Mallon J, 2007, PATTERN RECOGN LETT, V28, P921, DOI 10.1016/j.patrec.2006.12.008; MVTec Software GmbH, 2005, HALCON VERS 7 1; Niu MH, 2018, IEEE ACCESS, V6, P23711, DOI 10.1109/ACCESS.2018.2817629; Poli D, 2007, PHOTOGRAMM ENG REM S, V73, P187, DOI 10.14358/PERS.73.2.187; Ramalingam S, 2006, LECT NOTES COMPUT SC, V3851, P704; Ramalingam S, 2017, IEEE T PATTERN ANAL, V39, P1309, DOI 10.1109/TPAMI.2016.2592904; SHAPIRO LS, 1995, INT J COMPUT VISION, V16, P147, DOI 10.1007/BF01539553; Song KC, 2018, OPT LASER ENG, V110, P296, DOI 10.1016/j.optlaseng.2018.06.014; Steger C, 1998, IEEE T PATTERN ANAL, V20, P113, DOI 10.1109/34.659930; Steger C., 2000, INT ARCH PHOTOGRA B3, V33, P141; Steger C., 1998, UNBIASED EXTRACTION; Steger C., 2008, MACHINE VISION ALGOR; Steger C, 2018, MACHINE VISION ALGOR, V2; Steger C, 2018, J MATH IMAGING VIS, V60, P246, DOI 10.1007/s10851-017-0756-y; Steger C, 2017, INT J COMPUT VISION, V123, P121, DOI 10.1007/s11263-016-0964-8; Steger C, 2013, COMPUT VIS IMAGE UND, V117, P97, DOI 10.1016/j.cviu.2012.08.007; Steger C, 2012, ISPRS J PHOTOGRAMM, V74, P202, DOI 10.1016/j.isprsjprs.2012.09.012; Sturm P, 2010, FOUND TRENDS COMPUT, V6, P1, DOI 10.1561/0600000023; Sun B., 2016, SENSORS, V16; Sun B, 2017, APPL OPTICS, V56, P7905, DOI 10.1364/AO.56.007905; Sun B, 2016, APPL OPTICS, V55, P6836, DOI 10.1364/AO.55.006836; Ulrich M, 2019, MACH VISION APPL, V30, P1013, DOI 10.1007/s00138-019-01032-w; Yao M, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013028; Ye JW, 2014, VISUAL COMPUT, V30, P93, DOI 10.1007/s00371-013-0786-4; Zhang JB, 2018, IEEE INT CON AUTO SC, P678, DOI 10.1109/COASE.2018.8560438	53	7	7	6	25	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2021	129	1								10.1007/s11263-020-01358-3	http://dx.doi.org/10.1007/s11263-020-01358-3		AUG 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PU0KV		Green Published, hybrid			2022-12-18	WOS:000559206700001
J	Dutta, A; Akata, Z				Dutta, Anjan; Akata, Zeynep			Semantically Tied Paired Cycle Consistency for Any-Shot Sketch-Based Image Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article								Low-shot sketch-based image retrieval is an emerging task in computer vision, allowing to retrieve natural images relevant to hand-drawn sketch queries that are rarely seen during the training phase. Related prior works either require aligned sketch-image pairs that are costly to obtain or inefficient memory fusion layer for mapping the visual information to a semantic space. In this paper, we address any-shot,i.e. zero-shot and few-shot, sketch-based image retrieval (SBIR) tasks, where we introduce the few-shot setting for SBIR. For solving these tasks, we propose a semantically aligned paired cycle-consistent generative adversarial network (SEM-PCYC) for any-shot SBIR, where each branch of the generative adversarial network maps the visual information from sketch and image to a common semantic space via adversarial training. Each of these branches maintains cycle consistency that only requires supervision at the category level, and avoids the need of aligned sketch-image pairs. A classification criteria on the generators' outputs ensures the visual to semantic space mapping to be class-specific. Furthermore, we propose to combine textual and hierarchical side information via an auto-encoder that selects discriminating side information within a same end-to-end model. Our results demonstrate a significant boost in any-shot SBIR performance over the state-of-the-art on the extended version of the challenging Sketchy, TU-Berlin and QuickDraw datasets.	[Dutta, Anjan] Univ Exeter, Innovat Ctr, Dept Comp Sci, Streatham Campus, Exeter EX4 4RN, Devon, England; [Akata, Zeynep] Univ Tubingen, Tubingen AI Ctr, Cluster Excellence Machine Learning, D-72076 Tubingen, Germany	University of Exeter; Eberhard Karls University of Tubingen	Dutta, A (corresponding author), Univ Exeter, Innovat Ctr, Dept Comp Sci, Streatham Campus, Exeter EX4 4RN, Devon, England.	a.dutta@exeter.ac.uk; zeynep.akata@uni-tuebingen.de			European Union [665919]; ERC [853489]; DFG [390727645, 2064/1];  [RTI2018-102285-A-I00]	European Union(European Commission); ERC(European Research Council (ERC)European Commission); DFG(German Research Foundation (DFG)); 	This work has received funding from the European Union under Marie Skodowska-Curie Grant Agreement No. 665919, from the ERC under the Horizon 2020 program (Grant Agreement No. 853489), the Spanish Ministry project RTI2018-102285-A-I00 and DFG-EXC-Nummer 2064/1-Projektnummer 390727645. The TITAN Xp and TITAN V used for this research were donated by the NVIDIA Corporation.	Akata Z, 2016, PROC CVPR IEEE, P59, DOI 10.1109/CVPR.2016.14; Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Al-Halah Z, 2016, PROC CVPR IEEE, P5975, DOI 10.1109/CVPR.2016.643; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chen JX, 2018, LECT NOTES COMPUT SC, V11217, P624, DOI 10.1007/978-3-030-01261-8_37; Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dey S, 2019, PROC CVPR IEEE, P2174, DOI 10.1109/CVPR.2019.00228; Ding ZM, 2017, PROC CVPR IEEE, P6005, DOI 10.1109/CVPR.2017.636; Dutta A, 2019, PROC CVPR IEEE, P5084, DOI 10.1109/CVPR.2019.00523; Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540; Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2; Finn C, 2017, PR MACH LEARN RES, V70; Frome Andrea, 2013, NEURIPS; FU ZY, 2015, PROC CVPR IEEE, P2635, DOI DOI 10.1109/CVPR.2015.7298879; Garcia V., 2018, ICLR; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Guo YC, 2018, AAAI CONF ARTIF INTE, P6870; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hu GS, 2017, IEEE I CONF COMP VIS, P3764, DOI 10.1109/ICCV.2017.404; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu R, 2013, COMPUT VIS IMAGE UND, V117, P790, DOI 10.1016/j.cviu.2013.02.005; Jayaraman D, 2014, ADV NEUR IN, V27; Jiang J, 1997, INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, 1997 DIGEST OF TECHNICAL PAPERS, P94; Joulin A., 2017, ICLR, P1; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Koch G., 2015, ICML DEEP LEARNING W; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lin D., 1998, P INT C MACH LEARN; Liu L, 2017, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR.2017.247; Liu Q, 2019, IEEE I CONF COMP VIS, P3661, DOI 10.1109/ICCV.2019.00376; Liu S, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/290182; Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653; Mensink T, 2014, PROC CVPR IEEE, P2441, DOI 10.1109/CVPR.2014.313; Mikolov T., 2013, ARXIV; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; Pang KY, 2019, PROC CVPR IEEE, P677, DOI 10.1109/CVPR.2019.00077; Pang Kaiyue, 2017, BMVC; Paszke A., 2017, 31 C NEURAL INFORM P, P1, DOI DOI 10.1017/CB09781107707221.009; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Preissler M, 2018, 2018 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P304; Qi YG, 2016, IEEE IMAGE PROC, P2460, DOI 10.1109/ICIP.2016.7532801; Qiao RZ, 2016, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2016.247; Ravi S., 2017, INT C LEARN REPR, P12; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Romera-Paredes Bernardino, 2015, ICML; Saavedra J. M., 2015, PROC BRIT MACH VIS C, P1; Saavedra JM, 2014, IEEE IMAGE PROC, P2998, DOI 10.1109/ICIP.2014.7025606; Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954; Schonfeld E., 2018, CVPR, P8247; Shen YM, 2018, PROC CVPR IEEE, P3598, DOI 10.1109/CVPR.2018.00379; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Song JF, 2017, IEEE I CONF COMP VIS, P5552, DOI 10.1109/ICCV.2017.592; Song Jifei, 2017, BMVC; Su W, 2015, P 2015 INT C THEOR I, P349, DOI DOI 10.1145/2808194.2809481; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797; Wang M, 2015, PROC VLDB ENDOW, V8, P998, DOI 10.14778/2794367.2794370; Wang SY, 2017, AAAI CONF ARTIF INTE, P2725; Wang WL, 2018, AAAI CONF ARTIF INTE, P4211; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Yang Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1286, DOI 10.1145/2964284.2964319; Yang ZH, 2016, J INEQUAL APPL, DOI 10.1186/s13660-016-0988-1; Yelamarthi SK, 2018, LECT NOTES COMPUT SC, V11208, P316, DOI 10.1007/978-3-030-01225-0_19; Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93; Yu Q, 2017, INT J COMPUT VISION, V122, P411, DOI 10.1007/s11263-016-0932-3; [于谦 Yu Qian], 2015, [高分子通报, Polymer Bulletin], P1; Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12; Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474; ZHANG ZM, 2016, PROC CVPR IEEE, P6034, DOI DOI 10.1109/CVPR.2016.649; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	87	7	7	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2684	2703		10.1007/s11263-020-01350-x	http://dx.doi.org/10.1007/s11263-020-01350-x		JUL 2020	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Green Published, Green Submitted, hybrid			2022-12-18	WOS:000555671900001
J	Chin, TJ; Cai, ZP; Neumann, F				Chin, Tat-Jun; Cai, Zhipeng; Neumann, Frank			Robust Fitting in Computer Vision: Easy or Hard?	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	15th European Conference on Computer Vision (ECCV)	SEP 08-14, 2018	Munich, GERMANY			Robust fitting; Consensus maximisation; Inlier set maximisation; Computational hardness	ALGORITHMS; COMPLEXITY	Robust model fitting plays a vital role in computer vision, and research into algorithms for robust fitting continues to be active. Arguably the most popular paradigm for robust fitting in computer vision is consensus maximisation, which strives to find the model parameters that maximise the number of inliers. Despite the significant developments in algorithms for consensus maximisation, there has been a lack of fundamental analysis of the problem in the computer vision literature. In particular, whether consensus maximisation is "tractable" remains a question that has not been rigorously dealt with, thus making it difficult to assess and compare the performance of proposed algorithms, relative to what is theoretically achievable. To shed light on these issues, we present several computational hardness results for consensus maximisation. Our results underline the fundamental intractability of the problem, and resolve several ambiguities existing in the literature.	[Chin, Tat-Jun; Cai, Zhipeng; Neumann, Frank] Univ Adelaide, Adelaide, SA, Australia	University of Adelaide	Chin, TJ (corresponding author), Univ Adelaide, Adelaide, SA, Australia.	tat-jun.chin@adelaide.edu.au	Cai, Zhipeng/AAA-7659-2021	Neumann, Frank/0000-0002-2721-3618				AMALDI E, 1995, THEOR COMPUT SCI, V147, P181, DOI 10.1016/0304-3975(94)00254-G; [Anonymous], 2014, IEEE COMP SOC C COMP; [Anonymous], EUR C COMP VIS ECCV; [Anonymous], 52 TU; [Anonymous], 2009, IEEE INT C COMP VIS; [Anonymous], 2011, IEEE COMP SOC C COMP; [Anonymous], 2009, BRIT MACH VIS C BMVC; [Anonymous], 2016, IEEE COMP SOC C COMP; [Anonymous], 2018, EUR C COMP VIS ECCV; [Anonymous], EUR C COMP VIS ECCV; [Anonymous], ENERGY MINIMIZATION; [Anonymous], 2015, IEEE COMP SOC C COMP; [Anonymous], 2017, IEEE COMP SOC C COMP; [Anonymous], IEEE COMP VIS PATT R; [Anonymous], 2004, EMERGING TOPICS COMP; Aronov B, 2008, SIAM J COMPUT, V38, P899, DOI 10.1137/060669474; Bazin JC, 2013, IEEE T PATTERN ANAL, V35, P1565, DOI 10.1109/TPAMI.2012.264; Ben-David S, 2002, J COMPUT SYST SCI, V64, P22, DOI 10.1006/jcss.2001.1797; Bustos AP, 2015, IEEE I CONF COMP VIS, P2165, DOI 10.1109/ICCV.2015.250; Cai ZP, 2018, LECT NOTES COMPUT SC, V11216, P699, DOI 10.1007/978-3-030-01258-8_42; CHENEY E. W, 1982, INTRO APPROXIMATION, V2nd; Cygan M., 2015, PARAMETERIZED ALGORI, DOI [10.1007/978-3-319-21275-3, DOI 10.1007/978-3-319-21275-3]; Downey R. G., 1999, PARAMETRIZED COMPLEX; Enqvist O, 2015, INT J COMPUT VISION, V112, P115, DOI 10.1007/s11263-014-0760-2; Erickson J, 2006, DISCRETE COMPUT GEOM, V36, P593, DOI 10.1007/s00454-006-1267-6; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fukuda K, 1997, COMP GEOM-THEOR APPL, V8, P1, DOI 10.1016/0925-7721(95)00049-6; Garey M., 1990, COMPUTERS INTRACTABI; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Giannopoulos P, 2009, LECT NOTES COMPUT SC, V5917, P198, DOI 10.1007/978-3-642-11269-0_16; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; Hartley R., 2003, MULTIPLE VIEW GEOMET; Johnson D. S., 1978, Theoretical Computer Science, V6, P93, DOI 10.1016/0304-3975(78)90006-3; JOHNSON DS, 1974, J COMPUT SYST SCI, V9, P256, DOI 10.1016/S0022-0000(74)80044-9; Kobayashi KW, 2017, COMP SEMICOND INTEGR; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; MATOUSEK J, 1995, DISCRETE COMPUT GEOM, V14, P365, DOI 10.1007/BF02570713; Tran QH, 2014, INT J COMPUT VISION, V106, P93, DOI 10.1007/s11263-013-0643-y; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Vedaldi Andrea, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249	41	7	7	2	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		575	587		10.1007/s11263-019-01207-y	http://dx.doi.org/10.1007/s11263-019-01207-y			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	KU1MV		Green Submitted			2022-12-18	WOS:000519475600002
J	Harwath, D; Recasens, A; Suris, D; Chuang, G; Torralba, A; Glass, J				Harwath, David; Recasens, Adria; Suris, Didac; Chuang, Galen; Torralba, Antonio; Glass, James			Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	15th European Conference on Computer Vision (ECCV)	SEP 08-14, 2018	Munich, GERMANY			Vision and language; Sound; Speech; Multimodal learning; Language acquisition; Visual object discovery; Unsupervised learning; Self-supervised learning		In this paper, we explore neural network models that learn to associate segments of spoken audio captions with the semantically relevant portions of natural images that they refer to. We demonstrate that these audio-visual associative localizations emerge from network-internal representations learned as a by-product of training to perform an image-audio retrieval task. Our models operate directly on the image pixels and speech waveform, and do not rely on any conventional supervision in the form of labels, segmentations, or alignments between the modalities during training. We perform analysis using the Places 205 and ADE20k datasets demonstrating that our models implicitly learn semantically coupled object and word detectors.	[Harwath, David; Recasens, Adria; Suris, Didac; Chuang, Galen; Torralba, Antonio; Glass, James] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Harwath, D (corresponding author), MIT, Cambridge, MA 02139 USA.	dharwath@csail.mit.edu; recasens@csail.mit.edu; didacsuris@gmail.com; galen.chuang@gmail.com; torralba@csail.mit.edu; glass@mit.edu						Alishahi A., 2017, P ACL C NAT LANG LEA; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2015, P IEEE INT C COMP VI; [Anonymous], 2017, P IEEE C COMP VIS PA; Arandjelovic Relja, 2017, P IEEE INT C COMP VI; Aytar Y., 2016, P NEUR INF PROC SYST; Bergamo A., 2014, ARXIV14093964 CORR; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Chen ZQ, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511284; Chlamtac I, 1996, MILCOM 96, CONFERENCE PROCEEDINGS, VOLS 1-3, P108, DOI 10.1109/MILCOM.1996.568594; Chrupala G., 2017, P ANN M ASS COMP LIN; Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231; de Vries H., 2017, P IEEE C COMP VIS PA; Doersch C., 2015, ARXIV150505192 CORR; Drexler J., 2017, P GROUND LANG UND WO; Faghri Fartash, 2018, P BRIT MACH VIS C BM; Fang H., 2015, P IEEE C COMP VIS PA; Fellbaum Christiane, 1998, WORDNET ELECT DATABA; Gao H., 2015, P NEUR INF PROC SYST; Gelderloos L., 2016, ARXIV161003342; Gemmeke J. F., 2017, P INT C AC SPEECH SI; Girshick R., 2013, P IEEE C COMP VIS PA; Guerin J., 2017, ARXIV170701700 CORR; Harwath D., 2017, P ANN M ASS COMP LIN; Harwath D., 2016, P ADV NEUR INF PROC, P1866; Harwath D., 2018, P IEEE EUR C COMP VI; He K., 2015, ILSVRC COCO 2015 COM, P1; Hihn J, 2016, AEROSP CONF PROC; Ioffe S., 2015, P INT C MACH LEARN I; Jansen A., 2018, P INT C AC SPEECH SI; JANSEN A, 2010, P ANN C INT SPEECH C; Jansen A., 2011, P IEEE WORKSH AUT SP; Kamper H., 2015, P INT C AC SPEECH SI; Kamper H., 2017, P ANN C INT SPEECH C; Kamper H, 2016, IEEE-ACM T AUDIO SPE, V24, P669, DOI 10.1109/TASLP.2016.2517567; Karpathy Andrey, 2015, P IEEE C COMP VIS PA; Kwak S., 2015, P IEEE C COMP VIS PA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee C., 2012, P ANN M ASS COMP LIN; Lewis Paul, 2016, ETHNOLOGUE LANGUAGES, V19th; Lin T.Y., 2014, P EUR C COMP VIS 201; Malinowski M., 2015, P IEEE INT C COMP VI; Malinowski M., 2014, P NEUR INF PROC SYST; Ondel  Lucas, 2016, 5 WORKSH SPOK LANG T; Owens A., 2016, P IEEE EUR C COMP VI; Owens A., 2016, P IEEE C COMP VIS PA; Park AS, 2008, IEEE T AUDIO SPEECH, V16, P186, DOI 10.1109/TASL.2007.909282; Redmon J., 2016, P 2016 IEEE C COMP V; Reed S. E., 2016, ARXIV160505396 CORR; Ren M., 2015, P NEUR INF PROC SYST; Renshaw D., 2015, P ANN C INT SPEECH C; Roy D, 2003, IEEE T MULTIMEDIA, V5, P197, DOI 10.1109/TMM.2003.811618; Roy DK, 2002, COGNITIVE SCI, V26, P113, DOI 10.1207/s15516709cog2601_4; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Russell B. C., 2006, P IEEE C COMP VIS PA; Shih K. J., 2015, P IEEE C COMP VIS PA; SPELKE ES, 1990, COGNITIVE SCI, V14, P29, DOI 10.1207/s15516709cog1401_3; Sutskever I., 2012, NIPS, DOI DOI 10.1145/3065386; Thiolliere R., 2015, P ANN C INT SPEECH C; Thomee B., 2015, ARXIV150301817 CORR; Vinyals Oriol, 2015, P IEEE C COMP VIS PA; Weber M., 2010, P IEEE C COMP VIS PA; Xu Kelvin, 2015, P INT C MACH LEARN I; Zhang Y., 2012, P INT C AC SPEECH SI; Zhou B., 2015, P INT C LEARN REPR I; Zhou B., 2014, P NEUR INF PROC SYST	67	7	7	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		620	641		10.1007/s11263-019-01205-0	http://dx.doi.org/10.1007/s11263-019-01205-0			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	KU1MV		Green Submitted			2022-12-18	WOS:000519475600006
J	Veit, A; Belongie, S				Veit, Andreas; Belongie, Serge			Convolutional Networks with Adaptive Inference Graphs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	15th European Conference on Computer Vision (ECCV)	SEP 08-14, 2018	Munich, GERMANY			Convolutional neural networks; Gumbel-Softmax; Residual networks		Do convolutional networks really need a fixed feed-forward structure? What if, after identifying the high-level concept of an image, a network could move directly to a layer that can distinguish fine-grained differences? Currently, a network would first need to execute sometimes hundreds of intermediate layers that specialize in unrelated aspects. Ideally, the more a network already knows about an image, the better it should be at deciding which layer to compute next. In this work, we propose convolutional networks with adaptive inference graphs (ConvNet-AIG) that adaptively define their network topology conditioned on the input image. Following a high-level structure similar to residual networks (ResNets), ConvNet-AIG decides for each input image on the fly which layers are needed. In experiments on ImageNet we show that ConvNet-AIG learns distinct inference graphs for different categories. Both ConvNet-AIG with 50 and 101 layers outperform their ResNet counterpart, while using 20%and 38% less computations respectively. By grouping parameters into layers for related classes and only executing relevant layers, ConvNet-AIG improves both efficiency and overall classification quality. Lastly, we also study the effect of adaptive inference graphs on the susceptibility towards adversarial examples. We observe that ConvNet-AIG shows a higher robustness than ResNets, complementing other known defense mechanisms.	[Veit, Andreas] Google Res, New York, NY USA; [Belongie, Serge] Cornell Univ, Dept Comp Sci, New York, NY 10021 USA; [Belongie, Serge] Cornell Univ, Cornell Tech, New York, NY 10021 USA	Google Incorporated; Cornell University; Cornell University	Veit, A (corresponding author), Google Res, New York, NY USA.	aveit@google.com; sjb344@cornell.edu		Belongie, Serge/0000-0002-0388-5217				Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Bengio Emmanuel, 2015, CORR; Bengio Yoshua, 2013, ARXIV13083432; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Figurnov M, 2017, PROC CVPR IEEE, P1790, DOI 10.1109/CVPR.2017.194; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gumbel E.J., 1954, STAT THEORY EXTREME, V33; Guo Chuan, 2017, ARXIV171100117; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang G., 2017, ABS170309844 CORR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Jang E., 2016, ARXIV; Jie H., 2017, P IEEE C COMP VIS PA, P99; Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230; Maddison Chris J, 2016, ARXIV161100712; Misra I., 2017, P IEEE C COMP VIS PA, P1792; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Teerapittayanon S, 2016, INT C PATT RECOG, P2464, DOI 10.1109/ICPR.2016.7900006; Veit A, 2016, ADV NEUR IN, V29; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb	34	7	7	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2020	128	3			SI		730	741		10.1007/s11263-019-01190-4	http://dx.doi.org/10.1007/s11263-019-01190-4			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	KU1MV		Green Submitted			2022-12-18	WOS:000519475600012
J	Zhang, YQ; Bai, YH; Ding, ML; Ghanem, B				Zhang, Yongqiang; Bai, Yancheng; Ding, Mingli; Ghanem, Bernard			Multi-task Generative Adversarial Network for Detecting Small Objects in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Small object detection; Small face detection; Super-resolution network; Multi-task generative adversarial network; COCO; WIDER FACE		Object detection results have been rapidly improved over a short period of time with the development of deep convolutional neural networks. Although impressive results have been achieved on large/medium sized objects, the performance on small objects is far from satisfactory and one of remaining open challenges is detecting small object in unconstrained conditions (e.g. COCO and WIDER FACE benchmarks). The reason is that small objects usually lack sufficient detailed appearance information, which can distinguish them from the backgrounds or similar objects. To deal with the small object detection problem, in this paper, we propose an end-to-end multi-task generative adversarial network (MTGAN), which is a general framework. In the MTGAN, the generator is a super-resolution network, which can up-sample small blurred images into fine-scale ones and recover detailed information for more accurate detection. The discriminator is a multi-task network, which describes each inputted image patch with a real/fake score, object category scores, and bounding box regression offsets. Furthermore, to make the generator recover more details for easier detection, the classification and regression losses in the discriminator are back-propagated into the generator during training process. Extensive experiments on the challenging COCO and WIDER FACE datasets demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a blurred small one, and show that the detection performance, especially for small sized objects, improves over state-of-the-art methods by a large margin.	[Zhang, Yongqiang; Ding, Mingli] Harbin Inst Technol, Sch Instrumentat Sci & Engn, Harbin, Peoples R China; [Bai, Yancheng] Chinese Acad Sci, Inst Software, Beijing, Peoples R China; [Ghanem, Bernard] King Abdullah Univ Sci & Technol, Visual Comp Ctr, Thuwal, Saudi Arabia	Harbin Institute of Technology; Chinese Academy of Sciences; Institute of Software, CAS; King Abdullah University of Science & Technology	Ding, ML (corresponding author), Harbin Inst Technol, Sch Instrumentat Sci & Engn, Harbin, Peoples R China.	zhangyongqiang@hit.edu.cn; yancheng.bai.1987@gmail.com; dingml@hit.edu.cn; Bernardghanem@gmail.com		Ding, Mingli/0000-0002-7510-7043; Zhang, Yongqiang/0000-0002-0437-7337	Natural Science Foundation of China [61603372]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The majority of this work was done when Yongqiang Zhang was a visiting Ph.D. student at King Abdullah University of Science and Technology (KAUST), and the others are continued at Harbin Institute of Technology (HIT). This work was supported by Natural Science Foundation of China, Grant No. 61603372.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.596; Bai Y., 2017, ARXIV PREPRINT ARXIV; Bai YC, 2018, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2018.00010; Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22; Cheng B., 2018, ARXIV181004002; Cheng BW, 2018, LECT NOTES COMPUT SC, V11219, P473, DOI 10.1007/978-3-030-01267-0_28; Chi C., 2018, ABS180902693 CORR; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Denton Emily L, 2015, NEURIPS, V2, P4; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Fu C. -Y., 2017, ARXIV170106659; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hradis M., 2015, P BMVC, V10, P2, DOI DOI 10.5244/C.29.6; Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166; Huang JY, 2017, IEEE ICC; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jain V., 2010, UMCS2010009; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li J., 2018, ARXIV181010220; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shen ZQ, 2019, AAAI CONF ARTIF INTE, P4886, DOI 10.1609/aaai.v33i01.33014886; Shiri F, 2019, INT J COMPUT VISION, V127, P863, DOI 10.1007/s11263-019-01169-1; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Shrivastava Abhinav, 2016, ARXIV161206851; Song YB, 2019, INT J COMPUT VISION, V127, P785, DOI 10.1007/s11263-019-01148-6; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tang X, 2018, LECT NOTES COMPUT SC, V11213, P812, DOI 10.1007/978-3-030-01240-3_49; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wan S., 2016, ARXIV160802236; Wang Jianfeng, 2017, ARXIV171107246; Wang Yitong, 2017, ARXIV170905256; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wen YD, 2019, INT J COMPUT VISION, V127, P668, DOI 10.1007/s11263-018-01142-4; Yan J, 2013, SCI WORLD J, DOI 10.1155/2013/458106; Yan JJ, 2014, PROC CVPR IEEE, P2497, DOI 10.1109/CVPR.2014.320; Zhang Changzheng, 2018, ARXIV180202142; Zhang H, 2019, INT J COMPUT VISION, V127, P845, DOI 10.1007/s11263-019-01175-3; Zhang J., 2017, ARXIV171200721; Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30; Zhang SM, 2019, CHAOS SOLITON FRACT, V127, P1, DOI 10.1016/j.chaos.2019.06.021; Zhang Y., 2017, ICMV 2016, V10341; Zhang Y, 2019, IEEE T CIRCUITS-I, V66, P4592, DOI 10.1109/TCSI.2019.2936946; Zhang YQ, 2019, PATTERN RECOGN LETT, V128, P407, DOI 10.1016/j.patrec.2019.10.005; Zhang YQ, 2019, PATTERN RECOGN, V94, P74, DOI 10.1016/j.patcog.2019.05.023; Zhang YQ, 2018, PATTERN RECOGN, V84, P68, DOI 10.1016/j.patcog.2018.07.005; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhu C., 2017, DEEP LEARNING BIOMET; Zhu CC, 2018, PROC CVPR IEEE, P5127, DOI 10.1109/CVPR.2018.00538; Zhu CC, 2017, ADV COMPUT VIS PATT, P57, DOI 10.1007/978-3-319-61657-5_3; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	74	7	7	5	29	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2020	128	6					1810	1828		10.1007/s11263-020-01301-6	http://dx.doi.org/10.1007/s11263-020-01301-6		FEB 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LQ3MN		Green Submitted			2022-12-18	WOS:000516304300001
J	Jiang, B; Tang, J; Luo, B				Jiang, Bo; Tang, Jin; Luo, Bin			Efficient Feature Matching via Nonnegative Orthogonal Relaxation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Feature matching; Nonnegative orthogonal constraint; Multiplicative update; Integer Quadratic Programming	GRAPH; ALGORITHM	Feature matching problem that incorporates pair-wise constraints can be formulated as an Integer Quadratic Programming (IQP) problem with one-to-one matching constraint. Since it is NP-hard, relaxation models are required. One main challenge for optimizing IQP matching is how to incorporate the discrete one-to-one matching constraint in IQP matching optimization. In this paper, we present a new feature matching relaxation model, called Nonnegative Orthogonal Relaxation (NOR), that aims to optimize IQP matching problem in nonnegative orthogonal domain. One important benefit of the proposed NOR model is that it can naturally incorporate the discrete one-to-one matching constraint in its optimization and can return a desired sparse (approximate discrete) solution for the problem. An efficient and effective update algorithm has been developed to solve the proposed NOR model. Promising experimental results on several benchmark datasets demonstrate the effectiveness and efficiency of the proposed NOR method.	[Jiang, Bo; Tang, Jin; Luo, Bin] Anhui Univ, Sch Comp Sci & Technol, 111 Jiulong Rd, Hefei, Anhui, Peoples R China	Anhui University	Luo, B (corresponding author), Anhui Univ, Sch Comp Sci & Technol, 111 Jiulong Rd, Hefei, Anhui, Peoples R China.	zeyiabc@163.com; ahu_lb@163.com			NSFC Key Projects of International (Regional) Cooperation and Exchanges [61860206004]; National Natural Science Foundation of China [61602001, 61872005, 61671018]; Natural Science Foundation of Anhui Province [1708085QF139]; Natural Science Foundation of Anhui Higher Education Institutions of China [KJ2016A020]	NSFC Key Projects of International (Regional) Cooperation and Exchanges; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Anhui Province(Natural Science Foundation of Anhui Province); Natural Science Foundation of Anhui Higher Education Institutions of China(National Natural Science Foundation of China (NSFC))	This work is supported in part by NSFC Key Projects of International (Regional) Cooperation and Exchanges under Grant (61860206004); National Natural Science Foundation of China (61602001, 61872005, 61671018); Natural Science Foundation of Anhui Province (1708085QF139); Natural Science Foundation of Anhui Higher Education Institutions of China (KJ2016A020).	Adamczewski K, 2015, IEEE I CONF COMP VIS, P109, DOI 10.1109/ICCV.2015.21; Albarelli A, 2012, INT J COMPUT VISION, V97, P36, DOI 10.1007/s11263-011-0432-4; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Caelli T, 2004, IEEE T PATTERN ANAL, V26, P515, DOI 10.1109/TPAMI.2004.1265866; Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; Collins T, 2014, LECT NOTES COMPUT SC, V8695, P138, DOI 10.1007/978-3-319-10584-0_10; Cour Timothee, 2006, ADV NEURAL INFORM PR, DOI DOI 10.7551/MITPRESS/7503.003.0044; Egozi A, 2013, IEEE T PATTERN ANAL, V35, P18, DOI 10.1109/TPAMI.2012.51; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; Hamid R, 2013, PROC CVPR IEEE, P2914, DOI 10.1109/CVPR.2013.375; Jiang B, 2017, AAAI CONF ARTIF INTE, P4089; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Leordeanu M., 2009, NIPS, P1114; Leordeanu M, 2012, INT J COMPUT VISION, V96, P28, DOI 10.1007/s11263-011-0442-2; Lin LA, 2009, PROC CVPR IEEE, P1351, DOI 10.1109/CVPRW.2009.5206585; Lin WY, 2017, IEEE T IMAGE PROCESS, V26, P2438, DOI 10.1109/TIP.2017.2683063; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17; Liu ZY, 2014, INT J COMPUT VISION, V109, P169, DOI 10.1007/s11263-014-0707-7; Liu ZY, 2014, IEEE T PATTERN ANAL, V36, P1258, DOI 10.1109/TPAMI.2013.223; Liu ZY, 2012, IEEE T PATTERN ANAL, V34, P1451, DOI 10.1109/TPAMI.2012.45; Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478; Maciel J, 2003, IEEE T PATTERN ANAL, V25, P187, DOI 10.1109/TPAMI.2003.1177151; Peng LG, 2006, SECOND INTERNATIONAL CONFERENCE ON DOCUMENT IMAGE ANALYSIS FOR LIBRARIES, PROCEEDINGS, P126; SCOTT GL, 1991, P ROY SOC B-BIOL SCI, V244, P21, DOI 10.1098/rspb.1991.0045; SHAPIRO LS, 1992, IMAGE VISION COMPUT, V10, P283, DOI 10.1016/0262-8856(92)90043-3; SINKHORN R, 1967, PAC J MATH, V21, P343, DOI 10.2140/pjm.1967.21.343; Tian Y, 2012, LECT NOTES COMPUT SC, V7574, P821, DOI 10.1007/978-3-642-33712-3_59; Torresani L, 2008, LECT NOTES COMPUT SC, V5303, P596, DOI 10.1007/978-3-540-88688-4_44; UMEYAMA S, 1988, IEEE T PATTERN ANAL, V10, P695, DOI 10.1109/34.6778; van Wyk BJ, 2004, IEEE T PATTERN ANAL, V26, P1526, DOI 10.1109/TPAMI.2004.95; Wang T, 2018, IEEE T PATTERN ANAL, V40, P1494, DOI 10.1109/TPAMI.2017.2716350; Wang T, 2018, IEEE T PATTERN ANAL, V40, P2853, DOI 10.1109/TPAMI.2017.2767591; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zass R, 2008, PROC CVPR IEEE, P1221; Zhang Z, 2016, PROC CVPR IEEE, P1202, DOI 10.1109/CVPR.2016.135; Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667; Zhou Q, 2018, AAAI CONF ARTIF INTE, P7599	40	7	7	3	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2019	127	9					1345	1360		10.1007/s11263-019-01185-1	http://dx.doi.org/10.1007/s11263-019-01185-1			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IL9YV					2022-12-18	WOS:000477642300009
J	St-Charles, PL; Bilodeau, GA; Bergevin, R				St-Charles, Pierre-Luc; Bilodeau, Guillaume-Alexandre; Bergevin, Robert			Online Mutual Foreground Segmentation for Multispectral Stereo Videos	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video object segmentation; Cosegmentation; Multispectral imagery; Energy minimization; Video signal processing	IMAGE REGISTRATION; FUSION	The segmentation of video sequences into foreground and background regions is a low-level process commonly used in video content analysis and smart surveillance applications. Using a multispectral camera setup can improve this process by providing more diverse data to help identify objects despite adverse imaging conditions. The registration of several data sources is however not trivial if the appearance of objects produced by each sensor differs substantially. This problem is further complicated when parallax effects cannot be ignored when using close-range stereo pairs. In this work, we present a new method to simultaneously tackle multispectral segmentation and stereo registration. Using an iterative procedure, we estimate the labeling result for one problem using the provisional result of the other. Our approach is based on the alternating minimization of two energy functions that are linked through the use of dynamic priors. We rely on the integration of shape and appearance cues to find proper multispectral correspondences, and to properly segment objects in low contrast regions. We also formulate our model as a frame processing pipeline using higher order terms to improve the temporal coherence of our results. Our method is evaluated under different configurations on multiple multispectral datasets, and our implementation is available online.	[St-Charles, Pierre-Luc; Bilodeau, Guillaume-Alexandre] Polytech Montreal, 2900 Boul Edouard Montpetit, Montreal, PQ, Canada; [Bergevin, Robert] Univ Laval, 2325 Rue Univ, Quebec City, PQ, Canada	Universite de Montreal; Polytechnique Montreal; Laval University	St-Charles, PL (corresponding author), Polytech Montreal, 2900 Boul Edouard Montpetit, Montreal, PQ, Canada.	pierre-luc.st-charles@polymtl.ca; guillaume-alexandre.bilodeau@polymtl.ca; robert.bergevin@gel.ulaval.ca		St-Charles, Pierre-Luc/0000-0001-7761-2839; Bergevin, Robert/0000-0002-1115-7471	NSERC; FRQ-NT team Grant [2014-PR-172083]; REPARTI (Regroupement pour l'etude des environnements partages intelligents repartis) FRQNT strategic cluster	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); FRQ-NT team Grant; REPARTI (Regroupement pour l'etude des environnements partages intelligents repartis) FRQNT strategic cluster	This work was supported by NSERC, by FRQ-NT team Grant No. 2014-PR-172083, and by REPARTI (Regroupement pour l'etude des environnements partages intelligents repartis) FRQNT strategic cluster. We gratefully acknowledge the support of NVIDIA Corporation with the donation of a Titan X GPU used for this research. We also thank Chris Holmberg Bahnsen who provided us with the full calibration data needed to rectify the stereo pairs of the VAP trimodal segmentation dataset.	Andres B., 2012, ABS12060111 CORR; [Anonymous], 2017, P IEEE INT C COMP VI; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Bienkowski L., 2012, QUANTITATIV INFRARED, V254; Bilodeau GA, 2011, IMAGE VISION COMPUT, V29, P41, DOI 10.1016/j.imavis.2010.08.002; Bilodeau GA, 2014, INFRARED PHYS TECHN, V64, P79, DOI 10.1016/j.infrared.2014.02.005; Bleyer M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3081, DOI 10.1109/CVPR.2011.5995581; Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Caelles S., 2017, P IEEE C COMP VIS PA; Coiras E, 2000, OPT ENG, V39, P282, DOI 10.1117/1.602363; Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010; Djelouah A, 2015, IEEE T PATTERN ANAL, V37, P1890, DOI 10.1109/TPAMI.2014.2385704; Fix A, 2014, PROC CVPR IEEE, P1138, DOI 10.1109/CVPR.2014.149; Fix A, 2011, IEEE I CONF COMP VIS, P1020, DOI 10.1109/ICCV.2011.6126347; Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706; Jain S. D., 2017, P IEEE C COMP VIS PA; Jeong S., 2017, IEEE T PATTERN ANAL; Ju R, 2015, IEEE I CONF COMP VIS, P1724, DOI 10.1109/ICCV.2015.201; Kappes JH, 2013, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2013.175; Kim S, 2015, PROC CVPR IEEE, P2103, DOI 10.1109/CVPR.2015.7298822; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668; Kroeger T, 2016, LECT NOTES COMPUT SC, V9908, P471, DOI 10.1007/978-3-319-46493-0_29; Lempitsky V, 2010, IEEE T PATTERN ANAL, V32, P1392, DOI 10.1109/TPAMI.2009.143; Li CL, 2017, IEEE T CIRC SYST VID, V27, P725, DOI 10.1109/TCSVT.2016.2556586; Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664; Mouats T, 2013, 2013 16TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1981; Palmero C, 2016, INT J COMPUT VISION, V118, P217, DOI 10.1007/s11263-016-0901-x; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Pinggera P, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.26; Pistarelli Marcelo D., 2013, Computer Analysis of Images and Patterns. 15th International Conference, CAIP 2013. Proceedings: LNCS 8048, P217, DOI 10.1007/978-3-642-40246-3_27; Riklin-Raviv T, 2008, INT J COMPUT VISION, V79, P231, DOI 10.1007/s11263-007-0115-3; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rother C., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91; Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3; Shechtman E, 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383198; St-Charles P. L., 2017, P IEEE C COMP VIS WO; St-Charles PL, 2016, IEEE T IMAGE PROCESS, V25, P4768, DOI 10.1109/TIP.2016.2598691; Tippetts B, 2016, J REAL-TIME IMAGE PR, V11, P5, DOI 10.1007/s11554-012-0313-2; Torabi A, 2012, COMPUT VIS IMAGE UND, V116, P210, DOI 10.1016/j.cviu.2011.10.006; TRON R, 2007, P IEEE C COMP VIS PA; Nguyen T, 2016, IEEE ANTENNAS PROP, P63, DOI 10.1109/APS.2016.7695740; Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530; Woodford O, 2009, IEEE T PATTERN ANAL, V31, P2115, DOI 10.1109/TPAMI.2009.131; Zhang C, 2016, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2016.437; Zhao J, 2014, MULTIMED TOOLS APPL, V73, P61, DOI 10.1007/s11042-012-1299-2; Zhu HY, 2016, J VIS COMMUN IMAGE R, V34, P12, DOI 10.1016/j.jvcir.2015.10.012; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	53	7	7	1	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2019	127	8					1044	1062		10.1007/s11263-018-01141-5	http://dx.doi.org/10.1007/s11263-018-01141-5			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IH5UM		Green Submitted			2022-12-18	WOS:000474559000005
J	Peng, X; Feris, RS; Wang, XY; Metaxas, DN				Peng, Xi; Feris, Rogerio S.; Wang, Xiaoyu; Metaxas, Dimitris N.			RED-Net: A Recurrent Encoder-Decoder Network for Video-Based Face Alignment	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Recurrent learning; Encoder-Decoder network; Face alignment		We propose a novel method for real-time face alignment in videos based on a recurrent encoder-decoder network model. Our proposed model predicts 2D facial point heat maps regularized by both detection and regression loss, while uniquely exploiting recurrent learning at both spatial and temporal dimensions. At the spatial level, we add a feedback loop connection between the combined output response map and the input, in order to enable iterative coarse-to-fine face alignment using a single network model, instead of relying on traditional cascaded model ensembles. At the temporal level, we first decouple the features in the bottleneck of the network into temporal-variant factors, such as pose and expression, and temporal-invariant factors, such as identity information. Temporal recurrent learning is then applied to the decoupled temporal-variant features. We show that such feature disentangling yields better generalization and significantly more accurate results at test time. We perform a comprehensive experimental analysis, showing the importance of each component of our proposed model, as well as superior results over the state of the art and several variations of our method in standard datasets.	[Peng, Xi; Metaxas, Dimitris N.] Rutgers State Univ, Piscataway, NJ 08854 USA; [Feris, Rogerio S.] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA; [Wang, Xiaoyu] Intellifusion, Redmond, WA USA	Rutgers State University New Brunswick; International Business Machines (IBM)	Peng, X (corresponding author), Rutgers State Univ, Piscataway, NJ 08854 USA.	xipeng.cs@rutgers.edu; rsferis@us.ibm.com; fanghuaxue@gmail.com.com; dnm@cs.rutgers.edu		Wang, Xiaoyu/0000-0002-6431-8822				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2004, TECHNICAL REPORT; Asthana A., 2003, P IEEE C COMP VIS PA, P3444; Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240; Badrinarayanan V., 2015, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Belhumeur P. N., 2011, P IEEE C COMP VIS PA; Black M., 1995, P IEEE C COMP VIS PA; Bulat A., 2016, HUMAN POSE ESTIMATIO, P717; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chrysos GG, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P954, DOI 10.1109/ICCVW.2015.126; Cootes Timothy F, 1992, BMVC, DOI DOI 10.1007/978-1-4471-3201-1_28; DeCarlo D, 2000, INT J COMPUT VISION, V38, P99, DOI 10.1023/A:1008122917811; Gao XB, 2010, IEEE T SYST MAN CY C, V40, P145, DOI 10.1109/TSMCC.2009.2035631; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hong S., 2015, ARXIV PREPRINT ARXIV, P1495; Jia Y., 2014, P 22 ACM INT C MULT, P675; Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kendall A, 2015, P BRIT MACH VIS C 20; Koestinger M., 2011, WORKSH BENCH FAC IM; Lai H., 2015, ARXIV151009083V2; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Learned-Miller G.B.H.E., 2014, UMCS2014003; Long J. L., 2014, ADV NEURAL INFORM PR, V27, P1601; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu L, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3249; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mikolov Tomas, 2014, P 3 INT C LEARN REPR; Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37; Nair V, 2010, P 27 INT C MACHINE L, P807; Oliver N, 1997, PROC CVPR IEEE, P123, DOI 10.1109/CVPR.1997.609309; Patras I, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P97, DOI 10.1109/AFGR.2004.1301515; Peng X., 2017, IEEE T IND ELECTRON, P1; Peng X., 2016, P BRIT MACH VIS C; Peng X, 2017, IEEE I CONF COMP VIS, P1632, DOI 10.1109/ICCV.2017.180; Peng X, 2016, LECT NOTES COMPUT SC, V9905, P38, DOI 10.1007/978-3-319-46448-0_3; Peng X, 2015, IEEE I CONF COMP VIS, P3880, DOI 10.1109/ICCV.2015.442; Peng X, 2015, COMPUT VIS IMAGE UND, V136, P92, DOI 10.1016/j.cviu.2015.03.008; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shen J., 2015, P IEEE INT C COMP VI; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989; Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Wang XY, 2015, IEEE T PATTERN ANAL, V37, P2071, DOI 10.1109/TPAMI.2015.2389830; Wu Yue, 2016, P IEEE C COMP VIS PA; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yang Jimei, 2015, NIPS; Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu X., 2016, PROCEEDINGS OF THE I	61	7	7	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2018	126	10			SI		1103	1119		10.1007/s11263-018-1095-1	http://dx.doi.org/10.1007/s11263-018-1095-1			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GR8XD		Green Submitted			2022-12-18	WOS:000443018400003
J	Gurari, D; He, K; Xiong, B; Zhang, JM; Sameki, M; Jain, SD; Sclaroff, S; Betke, M; Grauman, K				Gurari, Danna; He, Kun; Xiong, Bo; Zhang, Jianming; Sameki, Mehrnoosh; Jain, Suyog Dutt; Sclaroff, Stan; Betke, Margrit; Grauman, Kristen			Predicting Foreground Object Ambiguity and Efficiently Crowdsourcing the Segmentation(s)	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Salient object detection; Segmentation; Crowdsourcing	ALGORITHM; TRUTH	We propose the ambiguity problem for the foreground object segmentation task and motivate the importance of estimating and accounting for this ambiguity when designing vision systems. Specifically, we distinguish between images which lead multiple annotators to segment different foreground objects (ambiguous) versus minor inter-annotator differences of the same object. Taking images from eight widely used datasets, we crowdsource labeling the images as "ambiguous" or "not ambiguous" to segment in order to construct a new dataset we call STATIC. Using STATIC, we develop a system that automatically predicts which images are ambiguous. Experiments demonstrate the advantage of our prediction system over existing saliency-based methods on images from vision benchmarks and images taken by blind people who are trying to recognize objects in their environment. Finally, we introduce a crowdsourcing system to achieve cost savings for collecting the diversity of all valid "ground truth" foreground object segmentations by collecting extra segmentations only when ambiguity is expected. Experiments show our system eliminates up to 47% of human effort compared to existing crowdsourcing methods with no loss in capturing the diversity of ground truths.	[Gurari, Danna] Univ Texas Austin, Sch Informat, 1616 Guadalupe St, Austin, TX 78701 USA; [Xiong, Bo; Jain, Suyog Dutt; Grauman, Kristen] Univ Texas Austin, Dept Comp Sci, 2317 Speedway,Stop D9500, Austin, TX 78712 USA; [He, Kun; Sameki, Mehrnoosh; Sclaroff, Stan; Betke, Margrit] Boston Univ, Dept Comp Sci, 111 Cummington Mall, Boston, MA 02215 USA; [Zhang, Jianming] Adobe Res, 345 Pk Ave, San Jose, CA 95110 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin; Boston University; Adobe Systems Inc.	Gurari, D (corresponding author), Univ Texas Austin, Sch Informat, 1616 Guadalupe St, Austin, TX 78701 USA.	danna.gurari@ischool.utexas.edu; hekun@cs.bu.edu; bxiong@cs.utexas.edu; jianmzha@adobe.com; sameki@cs.bu.edu; suyog@cs.utexas.edu; sclaroff@cs.bu.edu; betke@cs.bu.edu; grauman@cs.utexas.edu		Sclaroff, Stanley/0000-0002-0711-4313	Office of Naval Research [ONR YIP N00014-12-1-0754]; National Science Foundation [IIS-1421943]; Div Of Information & Intelligent Systems [1421943] Funding Source: National Science Foundation	Office of Naval Research(Office of Naval Research); National Science Foundation(National Science Foundation (NSF)); Div Of Information & Intelligent Systems(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	The authors gratefully acknowledge funding from the Office of Naval Research (ONR YIP N00014-12-1-0754) and National Science Foundation (IIS-1421943) and thank the anonymous crowd workers for participating in our experiments.	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Alpert S, 2007, PROC CVPR IEEE, P359; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Berg AC, 2012, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR.2012.6248100; Berg Tamara L., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPR.2009.5204174; Biancardi AM, 2010, INT J COMPUT ASS RAD, V5, P295, DOI 10.1007/s11548-009-0401-3; Bigham Jeffrey P, 2010, P 23 NUAL ACM S US I, P333, DOI [10.1145/1866029.1866080?casa_token=eqdciLsaAKsAAAAA:v_iSvCJKVqaa-xY5ls_4fwveOme0IVWxS0hy40kPYpp, DOI 10.1145/1866029.1866080]; Borenstein E, 2008, IEEE T PATTERN ANAL, V30, P2109, DOI 10.1109/TPAMI.2007.70840; Borji A, 2013, VISION RES, V91, P62, DOI 10.1016/j.visres.2013.07.016; Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30; Brady Erin, 2013, P SIGCHI C HUMAN FAC, P2117; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Cholleti SR, 2009, INT J ARTIF INTELL T, V18, P633, DOI 10.1142/S0218213009000330; Dalal N., 2005, IEEE C COMP COMP VIS; Dingding Liu, 2011, Machine Learning and Data Mining in Pattern Recognition. Proceedings 7th International Conference, MLDM 2011, P484, DOI 10.1007/978-3-642-23199-5_36; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Feng J, 2011, IEEE I CONF COMP VIS, P1028, DOI 10.1109/ICCV.2011.6126348; GARCIAPEREZ MA, 1989, PERCEPT PSYCHOPHYS, V46, P397, DOI 10.3758/BF03204995; Gilbert E, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2759, DOI 10.1145/2556288.2557081; Gurari D., 2016, AAAI C HUM COMP CROW, P59; Jain SD, 2013, IEEE I CONF COMP VIS, P1313, DOI 10.1109/ICCV.2013.166; Jas M., 2015, IEEE C COMP VIS PATT; Jayant C, 2011, ASSETS 11: PROCEEDINGS OF THE 13TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P203; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kohlberger T, 2012, LECT NOTES COMPUT SC, V7510, P528, DOI 10.1007/978-3-642-33415-3_65; Kovashka A., 2014, INT J COMPUT VISION, V115, P185; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Leopold D., 2004, BINOCULAR RIVALRY PE, P231; Li S., 2017, IEEE T PATTERN ANAL; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Meger D, 2008, ROBOT AUTON SYST, V56, P503, DOI 10.1016/j.robot.2008.03.008; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Shaw Aaron D., 2011, P ACM 2011 C COMPUTE, P275, DOI [DOI 10.1145/1958824.1958865, 10.1145/1958824.1958865]; Sheshadri Aashish, 2013, P 1 AAAI C HUMAN COM, P156, DOI 10.1.1.644.2813; Shi Y, 2008, IEEE T IMAGE PROCESS, V17, P645, DOI 10.1109/TIP.2008.920737; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Torralba A, 2003, INT C COMP VIS ICCV; Vazquez M, 2014, ACM T COMPUT-HUM INT, V21, DOI 10.1145/2651380; Vijayanarasimhan S, 2011, INT J COMPUT VISION, V91, P24, DOI 10.1007/s11263-010-0372-4; Warfield SK, 2004, IEEE T MED IMAGING, V23, P903, DOI 10.1109/TMI.2004.828354; Welinder P., 2010, 2010 IEEE COMPUTER S, P25, DOI [10.1109/CVPRW.2010.5543189, DOI 10.1109/CVPRW.2010.5543189]; Whitehill J, 2009, ADV NEURAL INFORM PR, P2035; Zhang JM, 2015, PROC CVPR IEEE, P4045, DOI 10.1109/CVPR.2015.7299031; Zhong Y., 2013, P 15 INT ACM SIGACCE, P20	56	7	7	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2018	126	7					714	730		10.1007/s11263-018-1065-7	http://dx.doi.org/10.1007/s11263-018-1065-7			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GH0DS		Green Submitted			2022-12-18	WOS:000433072800003
J	Georgakis, C; Panagakis, Y; Pantic, M				Georgakis, Christos; Panagakis, Yannis; Pantic, Maja			Dynamic Behavior Analysis via Structured Rank Minimization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dynamic behavior analysis; Structured rank minimization; Linear time-invariant systems; Hankel matrix; Low-rank; Sparsity	HANKEL MATRIX; CONVERGENCE; CONVERSATIONS	Human behavior and affect is inherently a dynamic phenomenon involving temporal evolution of patterns manifested through a multiplicity of non-verbal behavioral cues including facial expressions, body postures and gestures, and vocal outbursts. A natural assumption for human behavior modeling is that a continuous-time characterization of behavior is the output of a linear time-invariant system when behavioral cues act as the input (e.g., continuous rather than discrete annotations of dimensional affect). Here we study the learning of such dynamical system under real-world conditions, namely in the presence of noisy behavioral cues descriptors and possibly unreliable annotations by employing structured rank minimization. To this end, a novel structured rank minimization method and its scalable variant are proposed. The generalizability of the proposed framework is demonstrated by conducting experiments on 3 distinct dynamic behavior analysis tasks, namely (i) conflict intensity prediction, (ii) prediction of valence and arousal, and (iii) tracklet matching. The attained results outperform those achieved by other state-of-the-art methods for these tasks and, hence, evidence the robustness and effectiveness of the proposed approach.	[Georgakis, Christos; Panagakis, Yannis; Pantic, Maja] Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England; [Panagakis, Yannis] Middlesex Univ, Dept Comp Sci, London, England; [Pantic, Maja] Univ Twente, Fac Elect Engn Math & Comp Sci EEMCS, Enschede, Netherlands	Imperial College London; Middlesex University; University of Twente	Georgakis, C (corresponding author), Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England.	christos.georgakis@imperial.ac.uk; i.panagakis@imperial.ac.uk; m.pantic@imperial.ac.uk	Panagakis, Yannis/AAZ-8090-2020	Georgakis, Christos/0000-0003-3648-2040; Panagakis, Ioannis/0000-0003-0153-5210	EPSRC project [EP/N007743/1]; European Community Horizon [H] [645094]; EPSRC [EP/J017787/1, EP/N007743/1, EP/H016988/1] Funding Source: UKRI; Engineering and Physical Sciences Research Council [EP/N007743/1, EP/J017787/1, EP/H016988/1] Funding Source: researchfish	EPSRC project(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Community Horizon [H]; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work is funded by the EPSRC project EP/N007743/1 (FACER2VM). The work of C. Georgakis and Y. Panagakis is also partially supported by the European Community Horizon 2020 [H2020/2014-2020] under Grant Agreement No. 645094 (SEWA).	Ayazoglu M, 2013, IEEE I CONF COMP VIS, P3575, DOI 10.1109/ICCV.2013.444; Ayazoglu M, 2012, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2012.6247865; Ayazoglu M, 2011, IEEE I CONF COMP VIS, P2462, DOI 10.1109/ICCV.2011.6126531; B?nziger T., 2010, BLUEPRINT AFFECTIVE, P271, DOI DOI 10.1037/A0025827; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Bertsekas D. P., 2014, CONSTRAINED OPTIMIZA; Bhattacharya S, 2014, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2014.287; Binlong Li, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3193, DOI 10.1109/CVPR.2011.5995672; Bousmalis K., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P746, DOI 10.1109/FG.2011.5771341; Bousmalis K., 2009, ACII WORKSHOPS, P1; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chen CH, 2016, MATH PROGRAM, V155, P57, DOI 10.1007/s10107-014-0826-5; Chrysos GG, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P954, DOI 10.1109/ICCVW.2015.126; Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X; Cowie R., 2010, P LREC INT WORKSH EM, P42; Dai Y, 2014, DEGRADATION ANAL POW, P1; Deniz O, 2008, LECT NOTES COMPUT SC, V5359, P602, DOI 10.1007/978-3-540-89646-3_59; Dicle C, 2013, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2013.286; Ding T, 2007, IEEE DECIS CONTR P, P1941; Ding T, 2007, IEEE I CONF COMP VIS, P817; Ding T, 2008, IEEE DECIS CONTR P, P3446, DOI 10.1109/CDC.2008.4739090; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730; Fazel M, 2013, SIAM J MATRIX ANAL A, V34, P946, DOI 10.1137/110853996; Georgakis C, 2012, IEEE IMAGE PROC, P741, DOI 10.1109/ICIP.2012.6466966; Gunes H, 2013, IMAGE VISION COMPUT, V31, P120, DOI 10.1016/j.imavis.2012.06.016; Gunes H, 2011, COMPUTER ANALYSIS OF HUMAN BEHAVIOR, P255, DOI 10.1007/978-0-85729-994-9_10; Gunes H, 2010, LECT NOTES ARTIF INT, V6356, P371, DOI 10.1007/978-3-642-15892-6_39; Huber P. J., 2011, ROBUST STAT; Ji P, 2014, LECT NOTES COMPUT SC, V8694, P204, DOI 10.1007/978-3-319-10599-4_14; Kaltwang S, 2016, IEEE T PATTERN ANAL, V38, P1748, DOI 10.1109/TPAMI.2015.2501824; Kaltwang S, 2015, PROC CVPR IEEE, P296, DOI 10.1109/CVPR.2015.7298626; Kaltwang S, 2012, LECT NOTES COMPUT SC, V7432, P368, DOI 10.1007/978-3-642-33191-6_36; Kawato S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P40, DOI 10.1109/AFGR.2000.840610; Kim S., 2012, INTERSPEECH, P1167; Kim S, 2012, INT CONF ACOUST SPEE, P5089, DOI 10.1109/ICASSP.2012.6289065; Lane Richard D., 2002, COGNITIVE NEUROSCIEN; Li GY, 2015, SIAM J OPTIMIZ, V25, P2434, DOI 10.1137/140998135; Liavas AP, 2015, IEEE T SIGNAL PROCES, V63, P5450, DOI 10.1109/TSP.2015.2454476; Littlewort G, 2006, IMAGE VISION COMPUT, V24, P615, DOI 10.1016/j.imavis.2005.09.011; Liu GC, 2012, NEURAL COMPUT, V24, P3371, DOI 10.1162/NECO_a_00369; LOCKERD A, 2002, CHI 02 HUM FACT COMP, P574, DOI DOI 10.1145/506443.506490; Magnusson S, 2016, IEEE T CONTROL NETW, V3, P296, DOI 10.1109/TCNS.2015.2476198; Markovsky I, 2014, SIGNAL PROCESS, V96, P406, DOI 10.1016/j.sigpro.2013.09.021; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P5, DOI 10.1109/T-AFFC.2011.20; Morency LP, 2010, AUTON AGENT MULTI-AG, V20, P70, DOI 10.1007/s10458-009-9092-y; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Nicolaou MA, 2011, IEEE T AFFECT COMPUT, V2, P92, DOI 10.1109/T-AFFC.2011.9; Nicolaou MA, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853852; Nicolaou MA, 2012, IMAGE VISION COMPUT, V30, P186, DOI 10.1016/j.imavis.2011.12.005; Nie F., 2013, KNOWL INF SYST, V42, P525, DOI DOI 10.1007/S10115-013-0713-Z; Nie F., 2012, PROC 26 AAAI C ARTIF, P655; Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4; Orozco J, 2013, IMAGE VISION COMPUT, V31, P322, DOI 10.1016/j.imavis.2013.02.001; Panagakis Y, 2016, IEEE T PATTERN ANAL, V38, P1665, DOI 10.1109/TPAMI.2015.2497700; Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976; Pantic M., 2014, SOCIAL SIGNAL PROCES, P84; Pantic M., 2007, MACHINE ANAL FACIAL; Pantic M., 2011, VISUAL ANAL HUMANS, P511, DOI DOI 10.1007/978-0-85729-997-0_26; Papamakarios G., 2014, BRIT MACH VIS C BMVC; Park H, 1999, BIT, V39, P757, DOI 10.1023/A:1022347425533; Pavlovic V., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P94, DOI 10.1109/ICCV.1999.791203; Rakicevic N., 2016, INT C PATT REC ICPR; RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714; Sagonas C, 2014, PROC CVPR IEEE, P1789, DOI 10.1109/CVPR.2014.231; Scherer K.R., 2010, BLUEPRINT AFFECTIVE; SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420; Sun Dennis L., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6201, DOI 10.1109/ICASSP.2014.6854796; Surana A, 2013, IEEE DECIS CONTR P, P6489, DOI 10.1109/CDC.2013.6760916; Suykens J, 2013, ORGANOMETALLICS, V11, P4283; Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79; Valstar M., 2013, P 3 ACM INT WORKSHOP, P3, DOI DOI 10.1145/2512530.2512533; Van Overschee P., 2012, THEORY IMPLEMENTATIO; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003; Vrigkas M, 2015, FRONT ROBOT AI, DOI 10.3389/frobt.2015.00028; Wang F., 2015, ARXIV150503063; Wang F., 2014, ARXIV14108625; Wang Y., 2016, ARXIV151106324; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238; Xu YY, 2012, FRONT MATH CHINA, V7, P365, DOI 10.1007/s11464-012-0194-5; Yu A. W., 2014, P ADV NEUR INF PROC, P1350; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	86	7	7	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2018	126	2-4			SI		333	357		10.1007/s11263-016-0985-3	http://dx.doi.org/10.1007/s11263-016-0985-3			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FW8XA	31983807	Green Published, Green Accepted, hybrid			2022-12-18	WOS:000425619100011
J	Thomas, D; Sugimoto, A				Thomas, Diego; Sugimoto, Akihiro			Parametric Surface Representation with Bump Image for Dense 3D Modeling Using an RBG-D Camera	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D modeling; Consumer depth camera; Bump image; Real-time; Parametric surface		When constructing a dense 3D model of an indoor static scene from a sequence of RGB-D images, the choice of the 3D representation (e.g. 3D mesh, cloud of points or implicit function) is of crucial importance. In the last few years, the volumetric truncated signed distance function (TSDF) and its extensions have become popular in the community and largely used for the task of dense 3D modelling using RGB-D sensors. However, as this representation is voxel based, it offers few possibilities for manipulating and/or editing the constructed 3D model, which limits its applicability. In particular, the amount of data required to maintain the volumetric TSDF rapidly becomes huge which limits possibilities for portability. Moreover, simplifications (such as mesh extraction and surface simplification) significantly reduce the accuracy of the 3D model (especially in the color space), and editing the 3D model is difficult. We propose a novel compact, flexible and accurate 3D surface representation based on parametric surface patches augmented by geometric and color texture images. Simple parametric shapes such as planes are roughly fitted to the input depth images, and the deviations of the 3D measurements to the fitted parametric surfaces are fused into a geometric texture image (called the Bump image). A confidence and color texture image are also built. Our 3D scene representation is accurate yet memory efficient. Moreover, updating or editing the 3D model becomes trivial since it is reduced to manipulating 2D images. Our experimental results demonstrate the advantages of our proposed 3D representation through a concrete indoor scene reconstruction application.	[Thomas, Diego; Sugimoto, Akihiro] Natl Inst Informat, Tokyo, Japan; [Thomas, Diego] Kyushu Univ, Fukuoka, Japan	Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan; Kyushu University	Thomas, D (corresponding author), Natl Inst Informat, Tokyo, Japan.; Thomas, D (corresponding author), Kyushu Univ, Fukuoka, Japan.	diegot.thomas@gmail.com; sugimoto@nii.ac.jp		Thomas, Diego/0000-0002-8525-7133	Ministry of Education, Culture, Sports, Science and Technology of Japan	Ministry of Education, Culture, Sports, Science and Technology of Japan(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT))	This work is in part supported by Grant-in-Aid for Scientific Research of the Ministry of Education, Culture, Sports, Science and Technology of Japan.	Anasosalu P. K., 2013, P CDC4CV; [Anonymous], 2012, P INT C ROB SCI SYST; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chen J., 2013, ACM T GRAPHIC, V32; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; Henry P., 2013, P 3DV 13; Henry P, 2012, INT J ROBOT RES, V31, P647, DOI 10.1177/0278364911434148; Hernandez M, 2012, EUR SIGNAL PR CONF, P1995; Kahler O., 2015, IEEE T VISUALIZATION; Kazhdan M., 2006, P EUR S GEOM; Lengyel E., 2001, TERATHON SOFTWARE 3D; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Neibner M., 2013, ACM T GRAPHIC, V32; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Nguyen CV, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P524, DOI 10.1109/3DIMPVT.2012.84; Pfister Hanspeter, 2000, ACM T GRAPHICS; Roth H., 2012, P BMVC; Segal A., 2009, ROBOTICS SCI SYSTEMS; Steinbrucker F., 2013, P INT C COMP VIS ICC; Thomas D., 2013, P ICCV; Thomas D., 2014, P ECCV WORKSH 14 CDC; Weise Thibaut, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1630, DOI 10.1109/ICCVW.2009.5457479; Zeng M, 2013, GRAPH MODELS, V75, P126, DOI 10.1016/j.gmod.2012.09.002; Zhou Q.-Y., 2013, P ICCV; Zhou QY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461919	27	7	8	0	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2017	123	2					206	225		10.1007/s11263-016-0969-3	http://dx.doi.org/10.1007/s11263-016-0969-3			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EU2OM					2022-12-18	WOS:000400868800004
J	Zhao, F; Huang, YZ; Wang, L; Xiang, T; Tan, TN				Zhao, Fang; Huang, Yongzhen; Wang, Liang; Xiang, Tao; Tan, Tieniu			Learning Relevance Restricted Boltzmann Machine for Unstructured Group Activity and Event Understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Representation learning; Video analysis; Restricted Boltzmann Machine; Sparse Bayesian learning		Analyzing unstructured group activities and events in uncontrolled web videos is a challenging task due to (1) the semantic gap between class labels and low-level visual features, (2) the demanding computational cost given high-dimensional low-level feature vectors and (3) the lack of labeled training data. These difficulties can be overcome by learning a meaningful and compact mid-level video representation. To this end, in this paper a novel supervised probabilistic graphical model termed Relevance Restricted Boltzmann Machine (ReRBM) is developed to learn a low-dimensional latent semantic representation for complex activities and events. Our model is a variant of the Restricted Boltzmann Machine (RBM) with a number of critical extensions: (1) sparse Bayesian learning is incorporated into the RBM to learn features which are relevant to video classes, i.e., discriminative; (2) binary stochastic hidden units in the RBM are replaced by rectified linear units in order to better explain complex video contents and make variational inference tractable for the proposed model; and (3) an efficient variational EM algorithm is formulated for model parameter estimation and inference. We conduct extensive experiments on two recent challenging benchmarks: the Unstructured Social Activity Attribute dataset and the Event Video dataset. The experimental results demonstrate that the relevant features learned by our model provide better semantic and discriminative description for videos than a number of alternative supervised latent variable models, and achieves state of the art performance in terms of classification accuracy and retrieval precision, particularly when only a few labeled training samples are available.	[Zhao, Fang; Huang, Yongzhen; Wang, Liang; Tan, Tieniu] Chinese Acad Sci, Inst Automat, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; [Xiang, Tao] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London, England; [Huang, Yongzhen; Wang, Liang] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; University of London; Queen Mary University London; Chinese Academy of Sciences	Huang, YZ (corresponding author), Chinese Acad Sci, Inst Automat, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.; Huang, YZ (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.	fang.zhao@nlpr.ia.ac.cn; yzhuang@nlpr.ia.ac.cn; wangliang@nlpr.ia.ac.cn; t.xiang@qmul.ac.uk; tnt@nlpr.ia.ac.cn		Wang, Yunlong/0000-0002-3535-308X	National Basic Research Program of China [2012CB316300]; National Natural Science Foundation of China [61525306, 61573354, 61135002, 61420106015]; Strategic Priority Research Program of the CAS [XDB02070100]	National Basic Research Program of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Strategic Priority Research Program of the CAS	This work is jointly supported by National Basic Research Program of China (2012CB316300), National Natural Science Foundation of China (61525306, 61573354, 61135002, 61420106015), and Strategic Priority Research Program of the CAS (XDB02070100).	Bengio, 2012, ARXIV12065538 CORR; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; BOHNING D, 1992, ANN I STAT MATH, V44, P197, DOI 10.1007/bf00048682; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Desjardins G., 2012, ARXIV12034416 CORR; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Fu YW, 2012, LECT NOTES COMPUT SC, V7575, P530, DOI 10.1007/978-3-642-33765-9_38; Gopalan R, 2013, PROC CVPR IEEE, P2738, DOI 10.1109/CVPR.2013.353; Harva M, 2007, SIGNAL PROCESS, V87, P509, DOI 10.1016/j.sigpro.2006.06.006; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Larochelle H, 2012, J MACH LEARN RES, V13, P643; Logan B., 2000, INT S MUS INF RETR, V270, P1; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mittelman R, 2013, PROC CVPR IEEE, P476, DOI 10.1109/CVPR.2013.68; Nair V., 2010, ICML, P807; Neal R. M., 2012, BAYESIAN LEARNING NE; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Ranzato M, 2010, PROC CVPR IEEE, P2551, DOI 10.1109/CVPR.2010.5539962; Rasiwasia N, 2013, IEEE T PATTERN ANAL, V35, P2665, DOI 10.1109/TPAMI.2013.69; Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4; Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Salakhutdinov R, 2009, ADV NEURAL INFORM PR; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188; Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Tsochantaridis I., 2004, P 21 INT C MACH LEAR, P104, DOI DOI 10.1145/1015330.1015341; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang Y, 2009, IEEE T PATTERN ANAL, V31, P1762, DOI 10.1109/TPAMI.2009.43; Wei XY, 2011, IEEE T CIRC SYST VID, V21, P62, DOI 10.1109/TCSVT.2011.2105597; Yang Y, 2012, LECT NOTES COMPUT SC, V7574, P722, DOI 10.1007/978-3-642-33712-3_52; Zhao F., 2013, ADV NEURAL INFORM PR, P2580; Zhu J, 2012, J MACH LEARN RES, V13, P2237	48	7	8	1	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2016	119	3			SI		329	345		10.1007/s11263-016-0896-3	http://dx.doi.org/10.1007/s11263-016-0896-3			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DS0FE					2022-12-18	WOS:000380270000008
J	Yuan, CF; Wu, BX; Li, X; Hu, WM; Maybank, S; Wang, FS				Yuan, Chunfeng; Wu, Baoxin; Li, Xi; Hu, Weiming; Maybank, Stephen; Wang, Fangshi			Fusing Features and Local Features with Context-Aware Kernels for Action Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action recognition; Spatio-temporal interest points; 3D R transform; Hypergraph; Context-aware kernel	FACE REPRESENTATION; 2-DIMENSIONAL PCA; PATTERNS	The performance of action recognition in video sequences depends significantly on the representation of actions and the similarity measurement between the representations. In this paper, we combine two kinds of features extracted from the spatio-temporal interest points with context-aware kernels for action recognition. For the action representation, local cuboid features extracted around interest points are very popular using a Bag of Visual Words (BOVW) model. Such representations, however, ignore potentially valuable information about the global spatio-temporal distribution of interest points. We propose a new global feature to capture the detailed geometrical distribution of interest points. It is calculated by using the 3D transform which is defined as an extended 3D discrete Radon transform, followed by the application of a two-directional two-dimensional principal component analysis. For the similarity measurement, we model a video set as an optimized probabilistic hypergraph and propose a context-aware kernel to measure high order relationships among videos. The context-aware kernel is more robust to the noise and outliers in the data than the traditional context-free kernel which just considers the pairwise relationships between videos. The hyperedges of the hypergraph are constructed based on a learnt Mahalanobis distance metric. Any disturbing information from other classes is excluded from each hyperedge. Finally, a multiple kernel learning algorithm is designed by integrating the norm regularization into a linear SVM classifier to fuse the feature and the BOVW representation for action recognition. Experimental results on several datasets demonstrate the effectiveness of the proposed approach for action recognition.	[Yuan, Chunfeng; Wu, Baoxin; Hu, Weiming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Li, Xi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China; [Maybank, Stephen] Birkbeck Coll, Dept Comp Sci & Informat Syst, London, England; [Wang, Fangshi] Beijing Jiaotong Univ, Sch Software Engn, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Zhejiang University; University of London; Birkbeck University London; Beijing Jiaotong University	Hu, WM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.	cfyuan@nlpr.ia.ac.cn; bxwu@nlpr.ia.ac.cn; xilizju@zju.edu.cn; wmhu@nlpr.ia.ac.cn; sjmaybank@dcs.bbk.ac.uk; fshwang@bjtu.edu.cn	Li, Xi/L-1234-2013	Li, Xi/0000-0003-3023-1662	973 basic research program of China [2014CB349303]; Natural Science Foundation of China [61472421, 61472420, 61303086, 61202327]; CAS Center for Excellence in Brain Science and Intelligence Technology; Guangdong Natural Science Foundation [S2012020011081]	973 basic research program of China(National Basic Research Program of China); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CAS Center for Excellence in Brain Science and Intelligence Technology; Guangdong Natural Science Foundation(National Natural Science Foundation of Guangdong Province)	This work is partly supported by the 973 basic research program of China (Grant No. 2014CB349303), the Natural Science Foundation of China (Grant Nos. 61472421, 61472420, 61303086, 61202327), the Project Supported by CAS Center for Excellence in Brain Science and Intelligence Technology, and the Project Supported by Guangdong Natural Science Foundation (Grant No. S2012020011081).	[Anonymous], 2008, P 1 ACM INT C MULT I; ARMIJO L, 1966, PAC J MATH, V16, P1, DOI 10.2140/pjm.1966.16.1; Averbuch A, 2003, APPL COMPUT HARMON A, V15, P33, DOI 10.1016/S1063-5203(03)00030-7; Bregonzio M., 2011, BMVC, P1; Bregonzio M, 2009, PROC CVPR IEEE, P1948, DOI 10.1109/CVPRW.2009.5206779; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Daras P., 2004, 3DPVT, P953; Ellis C, 2013, INT J COMPUT VISION, V101, P420, DOI 10.1007/s11263-012-0550-7; Gaidon A, 2014, INT J COMPUT VISION, V107, P219, DOI 10.1007/s11263-013-0677-1; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Hong CQ, 2014, IEEE SYS MAN CYBERN, P2853, DOI 10.1109/SMC.2014.6974362; Huang YC, 2010, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2010.5540012; Ikizler-Cinbis N, 2010, LECT NOTES COMPUT SC, V6311, P494, DOI 10.1007/978-3-642-15549-9_36; Kloft M, 2011, J MACH LEARN RES, V12, P953; Kovashka A, 2010, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2010.5539881; Kulkarni K, 2015, INT J COMPUT VISION, V112, P90, DOI 10.1007/s11263-014-0758-9; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496; Li X, 2014, IEEE T KNOWL DATA EN, V26, P2588, DOI 10.1109/TKDE.2013.126; Liang Z, 2012, PATTERN RECOGN, V45, P3886, DOI 10.1016/j.patcog.2012.04.017; Liu J., 2008, EURASIP J ADV SIG PR, V2008, P1, DOI DOI 10.1074/JBC.M802695200; Marzalek M., 2009, COMPUTER VISION PATT, P2929; Mikolajczyk K, 2008, PROC CVPR IEEE, P2229; Ni BB, 2015, INT J COMPUT VISION, V111, P229, DOI 10.1007/s11263-014-0742-4; Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4; Oikonomopoulos A, 2011, IEEE T IMAGE PROCESS, V20, P1126, DOI 10.1109/TIP.2010.2076821; Oshin O., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P111, DOI 10.1109/FG.2011.5771382; Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Savarese S, 2008, 2008 IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING, P119; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Shi QF, 2011, INT J COMPUT VISION, V93, P22, DOI 10.1007/s11263-010-0384-0; Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721; Sun XH, 2009, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2009.5204255; Tabbone S, 2006, COMPUT VIS IMAGE UND, V102, P42, DOI 10.1016/j.cviu.2005.06.005; Varma M., 2009, P 26 ANN INT C MACH, P1065; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang Heng, 2009, BMVC, P1; Wang HX, 2015, IEEE T CYBERNETICS, V45, P465, DOI 10.1109/TCYB.2014.2327960; Wang L, 2009, WORLD NONGR CONN WIN, P1; Wang L, 2007, IEEE T IMAGE PROCESS, V16, P1646, DOI 10.1109/TIP.2007.896661; Wang Y, 2007, PROCEEDINGS OF THE SIXTH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS, P3; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Weng CQ, 2015, IEEE T MULTIMEDIA, V17, P626, DOI 10.1109/TMM.2015.2414720; Wu BX, 2014, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2014.334; Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097; Yeffet L, 2009, IEEE I CONF COMP VIS, P492, DOI 10.1109/ICCV.2009.5459201; Yu J, 2012, IEEE T IMAGE PROCESS, V21, P3262, DOI 10.1109/TIP.2012.2190083; Yuan CF, 2013, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2013.99; Yuan JS, 2011, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2011.5995476; Yuan JS, 2012, IEEE T SYST MAN CY B, V42, P334, DOI 10.1109/TSMCB.2011.2172605; Yuan J, 2007, PROC CVPR IEEE, P1930, DOI 10.1109/CVPR.2007.383222; Zhang DQ, 2005, NEUROCOMPUTING, V69, P224, DOI 10.1016/j.neucom.2005.06.004; Zhang LM, 2014, IEEE T CYBERNETICS, V44, P1408, DOI 10.1109/TCYB.2013.2285219; Zhou D., 2006, ADV NEURAL INF PROCE, V19, P1601; Zhu F, 2014, INT J COMPUT VISION, V109, P42, DOI 10.1007/s11263-014-0703-y	60	7	7	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2016	118	2			SI		151	171		10.1007/s11263-015-0867-0	http://dx.doi.org/10.1007/s11263-015-0867-0			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DO0OE		Green Accepted			2022-12-18	WOS:000377477400004
J	Nasihatkon, B; Hartley, R; Trumpf, J				Nasihatkon, Behrooz; Hartley, Richard; Trumpf, Jochen			A Generalized Projective Reconstruction Theorem and Depth Constraints for Projective Factorization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multiple view geometry; Projective reconstruction; Projective reconstruction theorem; Projective factorization; Projective depths; Constraints on projective depths	MOTION; MATRICES; NORM	This paper presents a generalized version of the classic projective reconstruction theorem which helps to choose or assess depth constraints for projective depth estimation algorithms. The theorem shows that projective reconstruction is possible under a much weaker constraint than requiring all estimated projective depths to be nonzero. This result enables us to present classes of depth constraints under which any reconstruction of cameras and points projecting into given image points is projectively equivalent to the true camera-point configuration. It also completely specifies the possible wrong configurations allowed by other constraints. We demonstrate the application of the theorem by analysing several constraints used in the literature, as well as presenting new constraints with desirable properties. We mention some of the implications of our results on iterative depth estimation algorithms and projective reconstruction via rank minimization. Our theory is verified by running experiments on both synthetic and real data.	[Nasihatkon, Behrooz; Hartley, Richard; Trumpf, Jochen] Australian Natl Univ, Canberra, ACT, Australia; [Nasihatkon, Behrooz; Hartley, Richard] NICTA, Canberra, ACT, Australia; [Nasihatkon, Behrooz] Chalmers Univ Technol, S-41296 Gothenburg, Sweden	Australian National University; Australian National University; Chalmers University of Technology	Nasihatkon, B (corresponding author), Chalmers Univ Technol, S-41296 Gothenburg, Sweden.	behrooz.nasihatkon@chalmers.se	Nasihatkon, Behrooz/AAT-9724-2020	Hartley, Richard/0000-0002-5005-0191	Australian Government; Australian Research Council through ICT Centre of Excellence program	Australian Government(Australian GovernmentCGIAR); Australian Research Council through ICT Centre of Excellence program(Australian Research Council)	NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.	Angst R, 2011, IEEE I CONF COMP VIS, P2502, DOI 10.1109/ICCV.2011.6126536; BUCHANAN T, 1988, COMPUT VISION GRAPH, V42, P130, DOI 10.1016/0734-189X(88)90146-6; Dai YC, 2013, IEEE T PATTERN ANAL, V35, P2238, DOI 10.1109/TPAMI.2013.20; Dai YC, 2010, LECT NOTES COMPUT SC, V6314, P396; HARTLEY R, 2003, P AUSTR JAP ADV WORK; Hartley R., 2004, ROBOTICA; Hartley R, 2007, INT J COMPUT VISION, V71, P5, DOI 10.1007/s11263-005-4796-1; Heyden A, 1999, IMAGE VISION COMPUT, V17, P981, DOI 10.1016/S0262-8856(99)00002-5; Lin Z., 2010, UILUENG092215; Luenberger D.G, 2016, LINEAR NONLINEAR PRO, DOI 10.1007/978-3-319-18842-3; Mahamud S, 2001, PROC CVPR IEEE, P1018; Oliensis J, 2007, IEEE T PATTERN ANAL, V29, P2217, DOI 10.1109/TPAMI.2007.1132; Semple J. G., 1952, OXFORD CLASSIC TEXTS; SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591; SINKHORN R, 1967, AM MATH MON, V74, P402, DOI 10.2307/2314570; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; Triggs B, 1996, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.1996.517170; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; Ueshiba T., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P296, DOI 10.1007/BFb0055674; Yang JF, 2013, MATH COMPUT, V82, P301; Zangwill WI, 1969, PRENTICE HALL INT SE	21	7	8	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2015	115	2					87	114		10.1007/s11263-015-0803-3	http://dx.doi.org/10.1007/s11263-015-0803-3			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CS7SL					2022-12-18	WOS:000362285700002
J	Becker, F; Lenzen, F; Kappes, JH; Schnorr, C				Becker, Florian; Lenzen, Frank; Kappes, Joerg H.; Schnoerr, Christoph			Variational Recursive Joint Estimation of Dense Scene Structure and Camera Motion from Monocular High Speed Traffic Sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from motion; Variational approach; Recursive formulation; Dense depth map	FLOW	We present an approach to jointly estimating camera motion and dense structure of a static scene in terms of depth maps from monocular image sequences in driver-assistance scenarios. At each instant of time, only two consecutive frames are processed as input data of a joint estimator that fully exploits second-order information of the corresponding optimization problem and effectively copes with the non-convexity due to both the imaging geometry and the manifold of motion parameters. Additionally, carefully designed Gaussian approximations enable probabilistic inference based on locally varying confidence and globally varying sensitivity due to the epipolar geometry, with respect to the high-dimensional depth map estimation. Embedding the resulting joint estimator in an online recursive framework achieves a pronounced spatio-temporal filtering effect and robustness. We evaluate hundreds of images taken from a car moving at speed up to 100 km/h and being part of a publicly available benchmark data set. The results compare favorably with two alternative settings: stereo based scene reconstruction and camera motion estimation in batch mode using multiple frames. They, however, require a calibrated camera pair or storage for more than two frames, which is less attractive from a technical viewpoint than the proposed monocular and recursive approach. In addition to real data, a synthetic sequence is considered which provides reliable ground truth.	[Becker, Florian; Lenzen, Frank] Heidelberg Univ, Heidelberg Collaboratory Image Proc, D-69115 Heidelberg, Germany; [Kappes, Joerg H.; Schnoerr, Christoph] Heidelberg Univ, Image & Pattern Anal Grp, D-69115 Heidelberg, Germany	Ruprecht Karls University Heidelberg; Ruprecht Karls University Heidelberg	Becker, F (corresponding author), Heidelberg Univ, Heidelberg Collaboratory Image Proc, Speyerer Str 6, D-69115 Heidelberg, Germany.	becker@math.uni-heidelberg.de; frank.lenzen@iwr.uni-heidelberg.de; kappes@math.uni-heidelberg.de; schnoerr@math.uni-heidelberg.de						Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Bagnato L, 2011, J MATH IMAGING VIS, V41, P182, DOI 10.1007/s10851-011-0267-1; Bain A, 2009, STOCH MOD APPL PROBA, V60, P1, DOI 10.1007/978-0-387-76896-0_1; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Becker F, 2011, IEEE I CONF COMP VIS, P1692, DOI 10.1109/ICCV.2011.6126432; Bonnans J.F., 2006, NUMERICAL OPTIMIZATI; Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; Comport A, 2007, IEEE INT C ROB AUT I; Enzweiler M, 2009, IEEE T PATTERN ANAL, V31, P2179, DOI 10.1109/TPAMI.2008.260; Fleet D., 2006, OPTICAL FLOW ESTIMAT; Geiger A., 2010, AS C COMP VIS QUEENS; Geiger A., 2012, COMPUTER VISION PATT; Geronimo D, 2010, IEEE T PATTERN ANAL, V32, P1239, DOI 10.1109/TPAMI.2009.122; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Graber G., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P708, DOI 10.1109/ICCVW.2011.6130318; Hadsell R, 2009, J FIELD ROBOT, V26, P120, DOI 10.1002/rob.20276; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Helmke U, 2007, INT J COMPUT VISION, V74, P117, DOI 10.1007/s11263-006-0005-0; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Irani M, 2002, IEEE T PATTERN ANAL, V24, P1528, DOI 10.1109/TPAMI.2002.1046174; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Klein G., 2007, IEEE ACM INT S MIX A; Konolige K, 2008, IEEE T ROBOT, V24, P1066, DOI 10.1109/TRO.2008.2004832; LEE DC, 2009, IEEE COMP SOC C COMP; Lenzen F., 2013, P 4 INT C SCAL SPAC; Lin W., 2011, INVESTIGATION DIETAR, P1; Liu BY, 2010, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2010.5539823; Meister S., 2012, SPIE OPTICAL ENG, V51, P6; Mester R., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P712, DOI 10.1109/ICCVW.2011.6130319; Mouragnon E, 2009, IMAGE VISION COMPUT, V27, P1178, DOI 10.1016/j.imavis.2008.11.006; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Newcombe RA, 2010, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2010.5539794; NISTER D, 2004, PROC CVPR IEEE, P652, DOI DOI 10.1109/CVPR.2004.1315094; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Rabe C, 2010, LECT NOTES COMPUT SC, V6314, P582, DOI 10.1007/978-3-642-15561-1_42; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Saxena A, 2008, INT J COMPUT VISION, V76, P53, DOI 10.1007/s11263-007-0071-y; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Sheikh Y., 2007, IEEE COMP SOC C COMP; Stuhmer J., 2010, TRENDS TOPICS COMPUT; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; TIERNEY L, 1986, J AM STAT ASSOC, V81, P82, DOI 10.2307/2287970; Triggs B., 2000, BUNDLE ADJUSTMENT MO, V1883; Valgaerts L, 2012, INT J COMPUT VISION, V96, P212, DOI 10.1007/s11263-011-0466-7; Valgaerts L, 2010, LECT NOTES COMPUT SC, V6314, P568, DOI 10.1007/978-3-642-15561-1_41; Vaudrey T., 2008, 23 INT C IM VIS COMP, P1; Wedel A, 2008, LECT NOTES COMPUT SC, V5302, P739, DOI 10.1007/978-3-540-88682-2_56; Weishaupt A., 2010, EUSIPCO; Wendel A, 2012, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2012.6247833; Wojek C, 2010, LECT NOTES COMPUT SC, V6314, P467, DOI 10.1007/978-3-642-15561-1_34; Yamaguchi K, 2012, LECT NOTES COMPUT SC, V7576, P45, DOI 10.1007/978-3-642-33715-4_4; ZEFRAN M, 1999, INT J ROBOTICS RES, V18	57	7	8	1	30	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2013	105	3					269	297		10.1007/s11263-013-0639-7	http://dx.doi.org/10.1007/s11263-013-0639-7			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	208EN					2022-12-18	WOS:000323659900005
J	Chaudhry, R; Hager, G; Vidal, R				Chaudhry, Rizwan; Hager, Gregory; Vidal, Rene			Dynamic Template Tracking and Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Dynamic templates; Dynamic textures; Human actions; Tracking; Linear dynamical systems; Recognition	CLASSIFICATION	In this paper we address the problem of tracking non-rigid objects whose local appearance and motion changes as a function of time. This class of objects includes dynamic textures such as steam, fire, smoke, water, etc., as well as articulated objects such as humans performing various actions. We model the temporal evolution of the object's appearance/motion using a linear dynamical system. We learn such models from sample videos and use them as dynamic templates for tracking objects in novel videos. We pose the problem of tracking a dynamic non-rigid object in the current frame as a maximum a-posteriori estimate of the location of the object and the latent state of the dynamical system, given the current image features and the best estimate of the state in the previous frame. The advantage of our approach is that we can specify a-priori the type of texture to be tracked in the scene by using previously trained models for the dynamics of these textures. Our framework naturally generalizes common tracking methods such as SSD and kernel-based tracking from static templates to dynamic templates. We test our algorithm on synthetic as well as real examples of dynamic textures and show that our simple dynamics-based trackers perform at par if not better than the state-of-the-art. Since our approach is general and applicable to any image feature, we also apply it to the problem of human action tracking and build action-specific optical flow trackers that perform better than the state-of-the-art when tracking a human performing a particular action. Finally, since our approach is generative, we can use a-priori trained trackers for different texture or action classes to simultaneously track and recognize the texture or action in the video.	[Chaudhry, Rizwan; Vidal, Rene] Johns Hopkins Univ, Ctr Imaging Sci, Baltimore, MD 21218 USA; [Hager, Gregory] Johns Hopkins Univ, Baltimore, MD USA	Johns Hopkins University; Johns Hopkins University	Chaudhry, R (corresponding author), Johns Hopkins Univ, Ctr Imaging Sci, Baltimore, MD 21218 USA.	rizwanch@cis.jhu.edu; hager@cs.jhu.edu; rvidal@cis.jhu.edu	Vidal, Rene/A-3367-2010; Hager, Gregory D/A-3222-2010		European Research Council;  [NSF 0941463];  [NSF 0941362];  [ONR N00014-09-10084]	European Research Council(European Research Council (ERC)European Commission); ; ; 	The authors would like to thank Diego Rother for useful discussions. This work was supported in part by the European Research Council grant Video World, and by the Grants, NSF 0941463, NSF 0941362 and ONR N00014-09-10084.	Ali S, 2010, IEEE T PATTERN ANAL, V32, P288, DOI 10.1109/TPAMI.2008.284; Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737; Bischof H., 2006, BMVC, P47; Bissacco A, 2001, PROC CVPR IEEE, P52; Bissacco A, 2007, IEEE T PATTERN ANAL, V29, P1958, DOI 10.1109/TPAMI.2007.1101; Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436; Chan AB, 2007, PROC CVPR IEEE, P208; Chaudhry R., 2009, 0901 J HOPK U DEP CO; Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821; Collins R., 2005, P IEEE INT WORKSHOP, V2, P35; Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; De Cock K, 2002, SYST CONTROL LETT, V46, P265, DOI 10.1016/S0167-6911(02)00135-4; Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132; Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726; Fan ZM, 2007, IEEE T PATTERN ANAL, V29, P1268, DOI 10.1109/TPAMI.2007.1034; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Gill P., 1987, PRACTICAL OPTIMIZATI; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Hager GD, 2004, PROC CVPR IEEE, P790; Ikizler N, 2009, IMAGE VISION COMPUT, V27, P1515, DOI 10.1016/j.imavis.2009.02.002; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Jepson AD, 2001, PROC CVPR IEEE, P415; Kim M, 2009, IEEE T PATTERN ANAL, V31, P1847, DOI 10.1109/TPAMI.2009.37; Leibe B, 2008, IEEE T PATTERN ANAL, V30, P1683, DOI 10.1109/TPAMI.2008.170; Lim H., 2006, IEEE INT C COMP VIS; Lin WC, 2007, IEEE T PATTERN ANAL, V29, P777, DOI 10.1109/TPAMI.2007.1053; Lin Z, 2009, IEEE I CONF COMP VIS, P444; Nejhum SMS, 2008, PROC CVPR IEEE, P1811; Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4; North B, 2000, IEEE T PATTERN ANAL, V22, P1016, DOI 10.1109/34.877523; Pavlovic V., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P94, DOI 10.1109/ICCV.1999.791203; Peteri R., 2010, VISION APPL SPECIAL; Peteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009; Ravichandran A, 2008, LECT NOTES COMPUT SC, V5303, P514, DOI 10.1007/978-3-540-88688-4_38; Ravichandran A, 2011, IEEE T PATTERN ANAL, V33, P158, DOI 10.1109/TPAMI.2010.61; Ravichandran A, 2009, PROC CVPR IEEE, P1651, DOI 10.1109/CVPRW.2009.5206847; Saisan P, 2001, PROC CVPR IEEE, P58; Thurau C., 2008, CVPR, P1; Vidal R, 2005, PROC CVPR IEEE, P516; Xie YL, 2011, PROC CVPR IEEE, P25, DOI 10.1109/CVPR.2011.5995648; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355	43	7	9	0	38	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2013	105	1					19	48		10.1007/s11263-013-0625-0	http://dx.doi.org/10.1007/s11263-013-0625-0			30	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	194HL		Green Submitted			2022-12-18	WOS:000322621800002
J	Patz, T; Kirby, RM; Preusser, T				Paetz, Torben; Kirby, Robert M.; Preusser, Tobias			Ambrosio-Tortorelli Segmentation of Stochastic Images: Model Extensions, Theoretical Investigations and Numerical Methods	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image processing; Ambrosio-Tortorelli model; Segmentation; Uncertainty; Stochastic images; Stochastic partial differential equations; Polynomial chaos; Generalized spectral decomposition; Adaptive grid; Edge linking	FINITE-ELEMENT-METHOD; POLYNOMIAL CHAOS; APPROXIMATION; FLOW	We discuss an extension of the Ambrosio-Tortorelli approximation of the Mumford-Shah functional for the segmentation of images with uncertain gray values resulting from measurement errors and noise. Our approach yields a reliable precision estimate for the segmentation result, and it allows us to quantify the robustness of edges in noisy images and under gray value uncertainty. We develop an ansatz space for such images by identifying gray values with random variables. The use of these stochastic images in the minimization of energies of Ambrosio-Tortorelli type leads to stochastic partial differential equations for a stochastic smoothed version of the original image and a stochastic phase field for the edge set. For the discretization of these equations we utilize the generalized polynomial chaos expansion and the generalized spectral decomposition (GSD) method. In contrast to the simple classical sampling technique, this approach allows for an efficient determination of the stochastic properties of the output image and edge set by computations on an optimally small set of random variables. Also, we use an adaptive grid approach for the spatial dimensions to further improve the performance, and we extend an edge linking method for the classical Ambrosio-Tortorelli model for use with our stochastic model. The performance of the method is demonstrated on artificial data and a data set from a digital camera as well as real medical ultrasound data. A comparison of the intrusive GSD discretization with a stochastic collocation and a Monte Carlo sampling is shown.	[Paetz, Torben; Preusser, Tobias] Jacobs Univ, Sch Sci & Engn, Bremen, Germany; [Paetz, Torben; Preusser, Tobias] Fraunhofer MEVIS Inst Med Image Comp, Bremen, Germany; [Kirby, Robert M.] Univ Utah, Sch Comp & Sci Comp, Salt Lake City, UT USA; [Kirby, Robert M.] Univ Utah, Imaging Inst, Salt Lake City, UT USA	Jacobs University; Utah System of Higher Education; University of Utah; Utah System of Higher Education; University of Utah	Patz, T (corresponding author), Jacobs Univ, Sch Sci & Engn, Bremen, Germany.	t.paetz@jacobs-university.de; kirby@cs.utah.edu; t.preusser@jacobs-university.de			Deutsche Forschungsgemeinschaft [PR 1038/5-1]; NSF [IIS-0914564]	Deutsche Forschungsgemeinschaft(German Research Foundation (DFG)); NSF(National Science Foundation (NSF))	We acknowledge PD Dr. Christoph S. Garbe from the University of Heidelberg, Germany for providing the pedestrian data set and D. Ojdanic from Fraunhofer MEVIS, Bremen, Germany for providing the ultrasound data set. The first and the third author were supported by grant PR 1038/5-1 from the Deutsche Forschungsgemeinschaft. The second author would like to acknowledge funding support under NSF IIS-0914564.	AMBROSIO L, 1990, COMMUN PUR APPL MATH, V43, P999, DOI 10.1002/cpa.3160430805; AMBROSIO L, 1992, B UNIONE MAT ITAL, V6B, P105; ASKEY R, 1985, MEM AM MATH SOC, V319; Babuska I, 2005, COMPUT METHOD APPL M, V194, P1251, DOI 10.1016/j.cma.2004.02.026; Bobrowski A., 2005, FUNCTIONAL ANAL PROB; Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43; CAMERON RH, 1947, ANN MATH, V48, P385, DOI 10.2307/1969178; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Debusschere BJ, 2004, SIAM J SCI COMPUT, V26, P698, DOI 10.1137/S1064827503427741; DEGIORGI E, 1989, ARCH RATION MECH AN, V108, P195, DOI 10.1007/BF01052971; Desceliers C, 2006, INT J NUMER METH ENG, V66, P978, DOI 10.1002/nme.1576; DIBENEDETTO E., 2002, REAL ANAL; Eiermann M, 2007, COMPUT VIS SCI, V10, P3, DOI 10.1007/s00791-006-0047-4; Erdem E, 2007, LECT NOTES COMPUT SC, V4485, P545; Ernst OG, 2012, ESAIM-MATH MODEL NUM, V46, P317, DOI 10.1051/m2an/2011045; Ghanem R.G., 1991, STOCHASTIC FINITE EL; Griethe H., 2006, SIMVIS, P143; Harbrecht H, 2012, APPL NUMER MATH, V62, P428, DOI 10.1016/j.apnum.2011.10.001; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Lin G, 2006, J COMPUT PHYS, V217, P260, DOI 10.1016/j.jcp.2006.02.009; Loeve M., 1977, GRADUATE TEXTS MATH; Malladi R., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P3; McCane B, 2001, COMPUT VIS IMAGE UND, V84, P126, DOI 10.1006/cviu.2001.0930; McWhirter J., 2006, BRAIN SYSTEMS NEW DI; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Nestares O, 2000, PROC CVPR IEEE, P523, DOI 10.1109/CVPR.2000.855864; Nouy A, 2007, COMPUT METHOD APPL M, V196, P4521, DOI 10.1016/j.cma.2007.05.016; Ohlberger M, 1999, IEEE T VIS COMPUT GR, V5, P74, DOI 10.1109/2945.764874; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Osher S., 2002, APPL MATH SCI, V44, P685; P. G. CIARLET, 1978, FINITE ELEMENT METHO; Patz T, 2010, LECT NOTES COMPUT SC, V6315, P254, DOI 10.1007/978-3-642-15555-0_19; Preusser T, 2000, J VIS COMMUN IMAGE R, V11, P183, DOI 10.1006/jvci.1999.0444; Preusser T, 2008, INT J COMPUT VISION, V80, P375, DOI 10.1007/s11263-008-0145-5; Sethian J. A., 1999, LEVEL SET METHODS FA; Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444; Stefanou G, 2009, INT J NUMER METH ENG, V79, P127, DOI 10.1002/nme.2546; WEBER J, 1995, INT J COMPUT VISION, V14, P67, DOI 10.1007/BF01421489; Xiu DB, 2002, SIAM J SCI COMPUT, V24, P619, DOI 10.1137/S1064827501387826	41	7	7	0	14	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2013	103	2			SI		190	212		10.1007/s11263-012-0578-8	http://dx.doi.org/10.1007/s11263-012-0578-8			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	150JC					2022-12-18	WOS:000319385400003
J	Nock, R; Piro, P; Nielsen, F; Ali, WBH; Barlaud, M				Nock, Richard; Piro, Paolo; Nielsen, Frank; Ali, Wafa Bel Haj; Barlaud, Michel			Boosting k-NN for Categorization of Natural Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Boosting; k nearest neighbors; Image categorization; Scene classification	CLASSIFICATION; KERNEL; RISK	The k-nearest neighbors (k-NN) classification rule has proven extremely successful in countless many computer vision applications. For example, image categorization often relies on uniform voting among the nearest prototypes in the space of descriptors. In spite of its good generalization properties and its natural extension to multi-class problems, the classic k-NN rule suffers from high variance when dealing with sparse prototype datasets in high dimensions. A few techniques have been proposed in order to improve k-NN classification, which rely on either deforming the nearest neighborhood relationship by learning a distance function or modifying the input space by means of subspace selection. From the computational standpoint, many methods have been proposed for speeding up nearest neighbor retrieval, both for multidimensional vector spaces and nonvector spaces induced by computationally expensive distance measures. In this paper, we propose a novel boosting approach for generalizing the k-NN rule, by providing a new k-NN boosting algorithm, called UNN (Universal Nearest Neighbors), for the induction of leveraged k-NN. We emphasize that UNN is a formal boosting algorithm in the original boosting terminology. Our approach consists in redefining the voting rule as a strong classifier that linearly combines predictions from the k closest prototypes. Therefore, the k nearest neighbors examples act as weak classifiers and their weights, called leveraging coefficients, are learned by UNN so as to minimize a surrogate risk, which upper bounds the empirical misclassification rate over training data. These leveraging coefficients allows us to distinguish the most relevant prototypes for a given class. Indeed, UNN does not affect the k-nearest neighborhood relationship, but rather acts on top of k-NN search. We carried out experiments comparing UNN to k-NN, support vector machines (SVM) and AdaBoost on categorization of natural scenes, using state-of-the art image descriptors (Gist and Bag-of-Features) on real images from Oliva and Torralba (Int. J. Comput. Vis. 42(3):145-175, 2001), Fei-Fei and Perona (IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pp. 524-531, 2005), and Xiao et al. (IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3485-3492, 2010). Results display the ability of UNN to compete with or beat the other contenders, while achieving comparatively small training and testing times.	[Piro, Paolo] Ist Italiano Tecnol, Via Morego 30, I-16163 Genoa, Italy; [Nock, Richard] Univ Antilles Guyane, CEREGMIA, Campus De Schoelcher, Martinique, France; [Nielsen, Frank] Sony Comp Sci Labs Inc, Tokyo, Japan; [Nielsen, Frank] Ecole Polytech, LIX Dept, Palaiseau, France; [Ali, Wafa Bel Haj; Barlaud, Michel] Univ Nice Sophia Antipolis, CNRS, F-06903 Sophia Antipolis, France	Istituto Italiano di Tecnologia - IIT; Sony Corporation; Institut Polytechnique de Paris; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Cote d'Azur	Piro, P (corresponding author), Ist Italiano Tecnol, Via Morego 30, I-16163 Genoa, Italy.	rnock@martinique.univ-ag.fr; paolo.piro@iit.it; nielsen@lix.polytechnique.fr; wafa@i3s.unice.fr; barlaud@i3s.unice.fr		Nielsen, Frank/0000-0001-5728-0726				Amores J, 2006, PATTERN RECOGN LETT, V27, P201, DOI 10.1016/j.patrec.2005.08.019; Athitsos V, 2008, IEEE T PATTERN ANAL, V30, P89, DOI 10.1109/TPAMI.2007.1140; Bartlett PL, 2007, J MACH LEARN RES, V8, P2347; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bel Haj ali W., 2012, INT C COMP VIS THEOR; Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009; Brighton H, 2002, DATA MIN KNOWL DISC, V6, P153, DOI 10.1023/A:1014043630878; Cucala L, 2009, J AM STAT ASSOC, V104, P263, DOI 10.1198/jasa.2009.0125; Dudani S. A., 1976, IEEE Transactions on Systems, Man and Cybernetics, VSMC-6, P325, DOI 10.1109/TSMC.1976.5408784; Fei-Fei L, 2005, PROC CVPR IEEE, P524; FUKUNAGA K, 1984, IEEE T PATTERN ANAL, V6, P314, DOI 10.1109/TPAMI.1984.4767523; Garcia-Pedrajas N, 2009, EXPERT SYST APPL, V36, P10570, DOI 10.1016/j.eswa.2009.02.065; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Gouesbet G., 2006, 10 INT C LIQUID ATOM; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; Gupta L., 2007, EURASIP J APPL SIG P, V2007, P123; HART PE, 1968, IEEE T INFORM THEORY, V14, P515, DOI 10.1109/TIT.1968.1054155; Hastie T, 1996, IEEE T PATTERN ANAL, V18, P607, DOI 10.1109/34.506411; Holmes CC, 2003, BIOMETRIKA, V90, P99, DOI 10.1093/biomet/90.1.99; Hsu C.-W., 2003, PRACTICAL GUIDE SUPP; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Kakade S., 2009, TECH REP; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Masip D, 2006, PATTERN RECOGN, V39, P164, DOI 10.1016/j.patcog.2005.06.004; Nguyen X, 2009, ANN STAT, V37, P876, DOI 10.1214/08-AOS595; Nock R, 2001, PATTERN RECOGN LETT, V22, P407, DOI 10.1016/S0167-8655(00)00133-1; NOCK R, 2009, ADV NEURAL INFORM PR, V21, P1201; Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Paredes R, 2006, IEEE T PATTERN ANAL, V28, P1100, DOI 10.1109/TPAMI.2006.145; Payne A, 2005, PATTERN RECOGN, V38, P1533, DOI 10.1016/j.patcog.2004.12.014; Piro P, 2012, NEUROCOMPUTING, V80, P3, DOI 10.1016/j.neucom.2011.07.026; Quattoni A., 2009, IEEE COMP SOC C COMP; Ruiz FE., 2009, INFORM THEORY COMPUT; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Serrano N, 2004, PATTERN RECOGN, V37, P1773, DOI 10.1016/j.patcog.2004.03.003; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487; Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yu K, 2002, NEURAL PROCESS LETT, V15, P147, DOI 10.1023/A:1015244902967; Yuan M, 2010, J MACH LEARN RES, V11, P111; Zhang H., 2006, IEEE INT C COMP VIS, V2, P2126; Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019; Zhu J, 2009, STAT INTERFACE, V2, P349; Zuo WM, 2008, PATTERN ANAL APPL, V11, P247, DOI 10.1007/s10044-007-0100-z	49	7	7	1	33	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2012	100	3					294	314		10.1007/s11263-012-0539-2	http://dx.doi.org/10.1007/s11263-012-0539-2			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	008KV		Green Submitted			2022-12-18	WOS:000308956700005
J	Damen, D; Hogg, D				Damen, Dima; Hogg, David			Explaining Activities as Consistent Groups of Events	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Activity analysis; Event recognition; Global explanations	RECOGNITION; TRACKING; REPRESENTATION	We propose a method for disambiguating uncertain detections of events by seeking global explanations for activities. Given a noisy visual input, and exploiting our knowledge of the activity and its constraints, one can provide a consistent set of events explaining all the detections. The paper presents a complete framework that starts with a general way to formalise the set of global explanations for a given activity using attribute multiset grammars (AMG). An AMG combines the event hierarchy with the necessary features for recognition and algebraic constraints defining allowable combinations of events and features. Parsing a set of detections by such a grammar finds a consistent set of events that satisfies the activity's constraints. Each parse tree has a posterior probability in a Bayesian sense. To find the best parse tree, the grammar and a finite set of detections are mapped into a Bayesian network. The set of possible labellings of the Bayesian network corresponds to the set of all parse trees for a given set of detections. We compare greedy, multiple-hypotheses trees, reversible jump MCMC, and integer programming for finding the Maximum a Posteriori (MAP) solution over the space of explanations. The framework is tested for two applications; the activity in a bicycle rack and around a building entrance.	[Damen, Dima] Univ Bristol, Dept Comp Sci, Bristol, Avon, England; [Hogg, David] Univ Leeds, Sch Comp, Leeds, W Yorkshire, England	University of Bristol; University of Leeds	Damen, D (corresponding author), Univ Bristol, Dept Comp Sci, Bristol, Avon, England.	damen@cs.bris.ac.uk; d.c.hogg@leeds.ac.uk		Hogg, David/0000-0002-6125-9564; Damen, Dima/0000-0001-8804-6238	EPSRC [EP/D061334/1]; Engineering and Physical Sciences Research Council [EP/D061334/1] Funding Source: researchfish	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We would like to thank Les Proll for assisting with formulating and solving the integer programming optimisation problem. This work was supported in part by the EPSRC (Project LAVID, EP/D061334/1).	Abney SP, 1997, COMPUT LINGUIST, V23, P597; Aho A.V., 1986, COMPILERS PRINCIPLES; Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Blevins J., 2001, NONTRANSFORMATIONAL; Damen D., 2009, P COMP VIS PATT REC; Damen D., 2009, THESIS U LEEDS UK; Damen D., 2008, P EUR COMP VIS C ECC; Damen D., 2009, P BRIT MACH VIS C BM; de la Higuera C, 2005, PATTERN RECOGN, V38, P1332, DOI 10.1016/j.patcog.2005.01.003; Fan Q., 2009, P COMP VIS PATT REC; Felzenszwalb P., 2000, P COMP VIS PATT REC; FICO D. O., 2007, XPRESS MP SOLV VERS; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gollin E., 1991, THESIS BROWN U; Gong S., 2003, P INT C COMP VIS ICC; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Gupta A., 2009, P COMP VIS PATT REC; Hamid R., 2007, P INT C COMP VIS ICC; Han F, 2005, IEEE I CONF COMP VIS, P1778; Hongeng S, 2004, COMPUT VIS IMAGE UND, V96, P129, DOI 10.1016/j.cviu.2004.02.005; Huang T, 1998, ARTIF INTELL, V103, P77, DOI 10.1016/S0004-3702(98)00067-8; Intille SS, 2001, COMPUT VIS IMAGE UND, V81, P414, DOI 10.1006/cviu.2000.0896; Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686; Joo S.-W., 2006, COMP VIS PATT REC WO; Joo SW, 2006, IEEE IMAGE PROC, P2897, DOI 10.1109/ICIP.2006.313035; KASTENS U, 1980, ACTA INFORM, V13, P229, DOI 10.1007/BF00288644; Kitani K. M., 2005, WORKSH VIS SURV PERF; Knuth D., 1968, MATH SYSTEMS THEORY, V2; Lin L, 2009, PATTERN RECOGN LETT, V30, P180, DOI 10.1016/j.patrec.2008.02.023; Magee D., 2002, Proceedings of the Statistical Methods in Video Processing Workshop, P7; MOREFIELD CL, 1977, IEEE T AUTOMAT CONTR, V22, P302, DOI 10.1109/TAC.1977.1101500; Nevatia R., 2003, P IEEE WORKSH EV MIN; Nguyen N., 2006, P BRIT MACH VIS C BM; Nilsson N.J., 1971, PROBLEM SOLVING METH; Oh S, 2004, IEEE DECIS CONTR P, P735, DOI 10.1109/CDC.2004.1428740; REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177; Riberio P., 2005, INT WORKSH HUM ACT R; Rota M., 2000, IEEE INT WORKSH VIS; Rother C., 2004, ACM T GRAPH SIGGRAPH; Shi Y., 2004, P COMP VIS PATT REC; Siskind JM, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P149; Smith K., 2007, THESIS EPFL; Smith P., 2005, P INT C COMP VIS ICC; Tran S., 2008, P EUR C COMP VIS ECC; Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239; Wang LA, 2011, INT J COMPUT VISION, V93, P162, DOI 10.1007/s11263-010-0393-z; Williams H.P., 1999, MODEL BUILDING MATH, V4th; Wu B., 2005, P INT C COMP VIS ICC; Wu Y, 2004, INT J COMPUT VISION, V58, P55, DOI 10.1023/B:VISI.0000016147.97880.cd; Young R, 1998, INT C PATT RECOG, P1718, DOI 10.1109/ICPR.1998.712055; Yu Q., 2007, P COMP VIS PATT REC; ZHAO T, 2004, P COMP VIS PATT REC; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	54	7	7	0	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2012	98	1					83	102		10.1007/s11263-011-0497-0	http://dx.doi.org/10.1007/s11263-011-0497-0			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	919PY		Green Accepted			2022-12-18	WOS:000302341900005
J	Gong, HF; Zhu, SC				Gong, Haifeng; Zhu, Song-Chun			Intrackability: Characterizing Video Statistics and Pursuing Video Representations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Object tracking; Video representation; Entropy; Inferential uncertainty	MULTIPLE OBJECT TRACKING; PERCEPTUAL SCALE-SPACE; PUZZLING FINDINGS; SELECTION	Videos of natural environments contain a wide variety of motion patterns of varying complexities which are represented by many different models in the vision literature. In many situations, a tracking algorithm is formulated as maximizing a posterior probability. In this paper, we propose to measure the video complexity by the entropy of the posterior probability, called the intrackability, to characterize the video statistics and pursue optimal video representations. Based on the definition of intrackability, our study is aimed at three objectives. Firstly, we characterize video clips of natural scenes by intrackability. We calculate the intrackabilities of image points to measure the local inferential uncertainty, and collect the histogram of the intrackabilities over the video in space and time as the global video statistics. We find that a PCA scatter-plot based on the first two principle components of intrackability histograms can reflect the major variations, i.e., image scaling and object density, in natural video clips. Secondly, we show that different video representations, including deformable contours, tracking kernels with various appearance features, dense motion fields, and dynamic texture models, are connected by the change of intrackability and thus develop a simple criterion for model transition and for pursuing the optimal video representation. Thirdly, we derive the connections between the intrackability measure and other criteria in the literature such as the Shi-Tomasi texturedness measure, conditional number, and Harris-Stephens R score, and compare with the Shi-Tomasi measure in tracking experiments.	[Zhu, Song-Chun] Univ Calif Los Angeles, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Gong, HF (corresponding author), 3330 Walnut St,Levine Hall L471, Philadelphia, PA 19104 USA.	haifeng.gong@gmail.com; sczhu@stat.ucla.edu			NSFC [60832004, 60875005]	NSFC(National Natural Science Foundation of China (NSFC))	The authors would like to thank Dr. Yingnian Wu at UCLA for discussions and thank Youdong Zhao at Lotus Hill Institute for assistance. We also thanks Jeffrey Byrne, Qihui Zhu, Weiyu Zhang and Katerina Fragkiadaki for proofreading. This work is supported by NSFC 60832004 and 60875005.	Ali S., 2007, CVPR; Ali S., 2008, ECCV; Badrinarayanan V., 2007, ICIP; Black MJ, 2000, INT J COMPUT VISION, V38, P231, DOI 10.1023/A:1008195307933; Collins R. T, 2003, CVPR; Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; Cong Y, 2009, PROC CVPR IEEE, P1093, DOI 10.1109/CVPRW.2009.5206648; Dreschler L., 1981, P INT JOINT C ART IN, P692; Fan Z., 2006, CVPR; Fitzgibbon A, 2001, ICCV; Han T. X., 2005, ICCV; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Kwon J, 2009, PROC CVPR IEEE, P991, DOI 10.1109/CVPRW.2009.5206501; Li Z., 2007, SPIE MIPPR; Li Z., 2007, EMMCVPR; LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346; MacCormick J, 2000, INT J COMPUT VISION, V39, P57, DOI 10.1023/A:1008122218374; MARR D, 1979, J OPT SOC AM, V69, P914, DOI 10.1364/JOSA.69.000914; Nickels K, 2002, IMAGE VISION COMPUT, V20, P47, DOI 10.1016/S0262-8856(01)00076-2; Pan P., 2009, IEEE WORKSH VS PETS; Pylyshyn ZW, 2006, SPATIAL VISION, V19, P485, DOI 10.1163/156856806779194017; Pylyshyn ZW, 2006, VIS COGN, V14, P175, DOI 10.1080/13506280544000200; Pylyshyn ZW, 2004, VIS COGN, V11, P801, DOI 10.1080/13506280344000518; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Sato K, 2004, COMPUT VIS IMAGE UND, V96, P100, DOI 10.1016/j.cviu.2004.02.003; Segvic S., 2006, ECCV; Serby D, 2004, INT C PATT RECOG, P184, DOI 10.1109/ICPR.2004.1334091; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Soatto S., 2001, ICCV; Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444; Szummer M., 1996, ICIP; Tommasini T., 1998, CVPR; Veenman CJ, 2001, IEEE T PATTERN ANAL, V23, P54, DOI 10.1109/34.899946; Wang Y, 2008, INT J COMPUT VISION, V80, P143, DOI 10.1007/s11263-008-0138-4; Wang YZ, 2005, IEEE I CONF COMP VIS, P58; Wang YZ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P213, DOI 10.1109/ICCV.2003.1238343; Witkin A, 1983, INT JOINT C AI KAUFM; Wu YN, 2008, Q APPL MATH, V66, P81; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Zhou XS, 2005, IEEE T PATTERN ANAL, V27, P115, DOI 10.1109/TPAMI.2005.3	44	7	8	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2012	97	3					255	275		10.1007/s11263-011-0486-3	http://dx.doi.org/10.1007/s11263-011-0486-3			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	907VP					2022-12-18	WOS:000301447600002
J	Bhavsar, AV; Rajagopalan, AN				Bhavsar, Arnav V.; Rajagopalan, A. N.			Towards Unrestrained Depth Inference with Coherent Occlusion Filling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Defocus blur; Multi-view stereo; Depth from defocus; Belief propagation; Inpainting	DEFOCUS BLUR; STEREO; SHAPE; IMAGE; FOCUS	Traditional depth estimation methods typically exploit the effect of either the variations in internal parameters such as aperture and focus (as in depth from defocus), or variations in extrinsic parameters such as position and orientation of the camera (as in stereo). When operating off-the-shelf (OTS) cameras in a general setting, these parameters influence the depth of field (DOF) and field of view (FOV). While DOF mandates one to deal with defocus blur, a larger FOV necessitates camera motion during image acquisition. As a result, for unfettered operation of an OTS camera, it becomes inevitable to account for pixel motion as well as optical defocus blur in the captured images. We propose a depth estimation framework using calibrated images captured under general camera motion and lens parameter variations. Our formulation seeks to generalize the constrained areas of stereo and shape from defocus (SFD)/focus (SFF) by handling, in tandem, various effects such as focus variation, zoom, parallax and stereo occlusions, all under one roof. One of the associated challenges in such an unrestrained scenario is the problem of removing user-defined foreground occluders in the reference depth map and image (termed inpainting of depth and image). Inpainting is achieved by exploiting the cue from motion parallax to discover (in other images) the correspondence/color information missing in the reference image. Moreover, considering the fact that the observations could be differently blurred, it is important to ensure that the degree of defocus in the missing regions (in the reference image) is coherent with the local neighbours (defocus inpainting).	[Bhavsar, Arnav V.; Rajagopalan, A. N.] Indian Inst Technol, Dept Elect Engn, Image Proc & Comp Vis Lab, Madras 600036, Tamil Nadu, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras	Bhavsar, AV (corresponding author), Indian Inst Technol, Dept Elect Engn, Image Proc & Comp Vis Lab, Madras 600036, Tamil Nadu, India.	arnav.bhavsar@gmail.com; raju@ee.iitm.ac.in	Bhavsar, Arnav/HDN-7002-2022	Ambasamudram, Rajagopalan/0000-0002-0006-6961; Bhavsar, Arnav/0000-0003-2849-4375				AHUJA N, 1993, IEEE T PATTERN ANAL, V15, P1007, DOI 10.1109/34.254059; Bhavsar A. V., 2009, BRIT MACH VIS C BMVC; Bhavsar A. V., 2010, BRIT MACH VIS C BMVC; Bhavsar AV, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P359, DOI 10.1109/ICVGIP.2008.41; Chan TF, 2006, J MATH IMAGING VIS, V26, P85, DOI 10.1007/s10851-006-6865-7; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Criminisi A, 2003, PROC CVPR IEEE, P721; Deschenes F, 2004, IMAGE VISION COMPUT, V22, P35, DOI 10.1016/j.imavis.2003.08.003; Dou Q., 2008, IEEE C COMP VIS PATT, P1; Drouin MA, 2005, PROC CVPR IEEE, P351; ENS J, 1993, IEEE T PATTERN ANAL, V15, P97, DOI 10.1109/34.192482; Favaro P, 2005, IEEE T PATTERN ANAL, V27, P406, DOI 10.1109/TPAMI.2005.43; Favaro P, 2008, IEEE T PATTERN ANAL, V30, P518, DOI 10.1109/TPAMI.2007.1175; Felzenszwalb PR, 2004, PROC CVPR IEEE, P261; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Frese C., 2006, 2006 IEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (IEEE Cat. No. 06TH8908), P243, DOI 10.1109/MFI.2006.265618; Gu JW, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618490; Hartley R., 2003, MULTIPLE VIEW GEOMET; KANG SB, 2001, MSRTR200180; Kim J., 2007, IEEE INT C IM PROC I, V5, pV; KROTKOV E, 1993, INT J COMPUT VISION, V11, P187, DOI 10.1007/BF01469228; Li S., 1995, MARKOV RANDOM FIELD, P1; Myles Z, 1998, IEEE T PATTERN ANAL, V20, P652, DOI 10.1109/34.683782; NAYAR SK, 1994, IEEE T PATTERN ANAL, V16, P824, DOI 10.1109/34.308479; Paramanand C., 2010, IEEE WORKSH 3 DIM IN; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; Rajagopalan A.N., 1999, DEPTH DEFOCUS REAL A; Rajagopalan AN, 2004, IEEE T PATTERN ANAL, V26, P1521, DOI 10.1109/TPAMI.2004.102; Sahay R., 2009, BRIT MACH VIS C BMVC; Sahay RR, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P243; Sahay RR, 2010, J OPT SOC AM A, V27, P1203, DOI 10.1364/JOSAA.27.001203; Sahay RR, 2009, LECT NOTES COMPUT SC, V5748, P362, DOI 10.1007/978-3-642-03798-6_37; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Seitz S., 2009, P INT C COMP VIS ICC; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Sorel M., 2007, THESIS CHARLES U; Strecha C, 2004, PROC CVPR IEEE, P552; Subbarao M, 1997, P SOC PHOTO-OPT INS, V3204, P11, DOI 10.1117/12.294452; Wang L., 2008, 2008 IEEE C COMPUTER, P1; Watanabe M., 1995, EUROPEAN C COMP VIS, P439; Wohler C, 2009, ISPRS J PHOTOGRAMM, V64, P529, DOI 10.1016/j.isprsjprs.2009.03.004; Wrotniak J. A., 2003, DEPTH FIELD TABLES O; Wrotniak J. A., 2006, DEPTH FIELD TABLES O; Yang QX, 2009, IEEE T PATTERN ANAL, V31, P492, DOI 10.1109/TPAMI.2008.99; Zhou CY, 2007, 2007 IEEE C COMP VIS, P1	45	7	7	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2012	97	2					167	190		10.1007/s11263-011-0476-5	http://dx.doi.org/10.1007/s11263-011-0476-5			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	897RV					2022-12-18	WOS:000300674200003
J	Bouchafa, S; Zavidovique, B				Bouchafa, Samia; Zavidovique, Bertrand			c-Velocity: A Flow-Cumulating Uncalibrated Approach for 3D Plane Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image motion analysis; Pattern recognition; Image scene analysis; Egomotion; Optical flow	OPTICAL-FLOW; MOTION; FOCUS	This paper deals with plane detection from a monocular image sequence without camera calibration or a priori knowledge about the egomotion. Within a framework of driver assistance applications, it is assumed that the 3D scene is a set of 3D planes. In this paper, the vision process considers obstacles, roads and buildings as planar structures. These planes are detected by exploiting iso-velocity curves after optical flow estimation. A Hough Transform-like frame called c-velocity was designed. This paper explains how this c-velocity, defined by analogy to the v-disparity in stereovision, can represent planes, regardless of their orientation and how this representation facilitates plane extraction. Under a translational camera motion, planar surfaces are transformed into specific parabolas of the c-velocity space. The error and robustness analysis of the proposed technique confirms that this cumulative approach is very efficient for making the detection more robust and coping with optical flow imprecision. Moreover, the results suggest that the concept could be generalized to the detection of other parameterized surfaces than planes.	[Bouchafa, Samia; Zavidovique, Bertrand] Univ Paris 11, Inst Elect Fondamentale, F-91405 Orsay, France	UDICE-French Research Universities; Universite Paris Saclay	Bouchafa, S (corresponding author), Univ Paris 11, Inst Elect Fondamentale, Bat 425, F-91405 Orsay, France.	samia.bouchafa@u-psud.fr		Bouchafa, Samia/0000-0002-2860-8128				Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Bouchafa S, 2006, IMAGE VISION COMPUT, V24, P70, DOI 10.1016/j.imavis.2005.09.013; Bouchafa S., 2010, P 10 INT C PATT REC, P177; Bouchafa S., 2008, IEEE INT WORKSH IM P, P1; DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242; FERMULLER C, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P245, DOI 10.1109/ICCV.1995.466779; Hanes DA, 2008, BIOL CYBERN, V98, P273, DOI 10.1007/s00422-008-0224-2; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1064; HILDRETH EC, 1992, VISION RES, V32, P1177, DOI 10.1016/0042-6989(92)90020-J; Irani M, 1997, IEEE T PATTERN ANAL, V19, P268, DOI 10.1109/34.584105; Labayrade R, 2002, IV'2002: IEEE INTELLIGENT VEHICLE SYMPOSIUM, PROCEEDINGS, P646; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Lucas B. D., 1981, IJCAI, P121, DOI DOI 10.5555/1623264.1623280; Luong QT, 1997, INT J COMPUT VISION, V22, P261, DOI 10.1023/A:1007982716991; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; MACLEAN WJ, 1994, BRIT MACH VIS C, P13; NEGAHDARIPOUR S, 1989, COMPUT VISION GRAPH, V46, P303, DOI 10.1016/0734-189X(89)90035-2; Roberts R, 2009, PROC CVPR IEEE, P57, DOI 10.1109/CVPRW.2009.5206538; Sazbon D, 2004, MACH VISION APPL, V15, P229, DOI 10.1007/s00138-004-0152-7; Stein GP, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P362, DOI 10.1109/IVS.2000.898370; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781	21	7	8	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2012	97	2					148	166		10.1007/s11263-011-0475-6	http://dx.doi.org/10.1007/s11263-011-0475-6			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	897RV					2022-12-18	WOS:000300674200002
J	Shet, V; Singh, M; Bahlmann, C; Ramesh, V; Neumann, J; Davis, L				Shet, Vinay; Singh, Maneesh; Bahlmann, Claus; Ramesh, Visvanathan; Neumann, Jan; Davis, Larry			Predicate Logic Based Image Grammars for Complex Pattern Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Stochastic image grammars; Logical reasoning; Human detection; Object detection and classification; Bilattice; Back propagation; Aerial image analysis		Predicate logic based reasoning approaches provide a means of formally specifying domain knowledge and manipulating symbolic information to explicitly reason about different concepts of interest. Extension of traditional binary predicate logics with the bilattice formalism permits the handling of uncertainty in reasoning, thereby facilitating their application to computer vision problems. In this paper, we propose using first order predicate logics, extended with a bilattice based uncertainty handling formalism, as a means of formally encoding pattern grammars, to parse a set of image features, and detect the presence of different patterns of interest. Detections from low level feature detectors are treated as logical facts and, in conjunction with logical rules, used to drive the reasoning. Positive and negative information from different sources, as well as uncertainties from detections, are integrated within the bilattice framework. We show that this approach can also generate proofs or justifications (in the form of parse trees) for each hypothesis it proposes thus permitting direct analysis of the final solution in linguistic form. Automated logical rule weight learning is an important aspect of the application of such systems in the computer vision domain. We propose a rule weight optimization method which casts the instantiated inference tree as a knowledge-based neural network, interprets rule uncertainties as link weights in the network, and applies a constrained, back-propagation algorithm to converge upon a set of rule weights that give optimal performance within the bilattice framework. Finally, we evaluate the proposed predicate logic based pattern grammar formulation via application to the problems of (a) detecting the presence of humans under partial occlusions and (b) detecting large complex man made structures as viewed in satellite imagery. We also evaluate the optimization approach on real as well as simulated data and show favorable results.	[Shet, Vinay; Singh, Maneesh; Bahlmann, Claus; Ramesh, Visvanathan] Siemens Corp Res, Princeton, NJ 08540 USA; [Neumann, Jan] Streamsage Comcast, Washington, DC 20005 USA; [Davis, Larry] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA	Siemens AG; University System of Maryland; University of Maryland College Park	Shet, V (corresponding author), Siemens Corp Res, 755 Coll Rd E, Princeton, NJ 08540 USA.	vinay.shet@siemens.com; maneesh.singh@siemens.com; claus.bahlmann@siemens.com; visvanathan.ramesh@siemens.com; jan_neumann@cable.comcast.com; lsd@umiacs.umd.edu		Singh, Maneesh/0000-0002-7414-1813	US Government [NBCHC080029]	US Government	Application and experimental validation of the reasoning framework on aerial images has been funded by US Government contract # NBCHC080029. Aerial images provided by DigiGlobe.	[Anonymous], 2003, CAV DAT; Arieli O, 2005, LECT NOTES COMPUT SC, V3571, P563; Arieli O, 2006, LECT NOTES ARTIF INT, V3885, P22; Binford TO, 2003, IEEE T PATTERN ANAL, V25, P837, DOI 10.1109/TPAMI.2003.1206513; Csurka G., 2004, ECCV SLCV WORKSH; Cussens J., 1999, P 15 C UNC ART INT; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Felzenszwalb PF, 2001, PROC CVPR IEEE, P1056; Fern A., 2005, INT JOINT C ART INT; Fidler S., 2007, CVPR; FITTING M, 1990, PROCEEDINGS OF THE TWENTIETH INTERNATIONAL SYMPOSIUM ON MULTIPLE-VALUED LOGIC, P238, DOI 10.1109/ISMVL.1990.122627; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman Nir, 1999, P 16 INT JOINT C ART; Gavrila D, 1999, IEEE INT C COMP VIS; Gavrila DM, 2000, LNCS, P37, DOI DOI 10.1007/3-540-45053-X; Geman S., 2003, MATH FDN SPEECH LANG, P1; Ginsberg M., 1988, COMPUTATIONAL INTELL; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Jin Y., 2006, CVPR, V2, P2145; JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0; KERSTING K, 2001, P 11 INT C IND LOG P; Kokkinos I., 2009, P IEEE C COMP VIS PA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LECUN Y, 1998, EFFICIENT BACKPROP N; Leibe B, 2005, PROC CVPR IEEE, P878; Leung T., 2001, INT J COMPUT VISION, V43; Liang Lin, 2007, 2007 11th IEEE International Conference on Computer Vision, P1; Lin Z, 2007, IEEE I CONF COMP VIS, P2301; MAHONEY JJ, 1993, ADV NEURAL INFORM PR, V5, P107; Mann W. B., 1995, THESIS STANFORD U; Papageorgiou C., 1998, P INT VEH, P241; POGGIO T, 1990, SCIENCE, V247, P978, DOI 10.1126/science.247.4945.978; PONCE J, 1989, IEEE T PATTERN ANAL, V11, P951, DOI 10.1109/34.35498; Ramesh V., THESIS U WASHINGTON; Renninger LW, 2004, VISION RES, V44, P2301, DOI 10.1016/j.visres.2004.04.006; Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318; Sato T., 1997, P 15 INT JOINT C ART; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Schweizer B., 1963, PUBL MATH DEBRECEN; Shet V., 2009, 1 INT WORKSH STOCH I; Shet V.D., 2007, CVPR; Shet VD, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P224; Shet VD, 2006, LECT NOTES COMPUT SC, V3954, P119; Sochman J, 2005, PROC CVPR IEEE, P150; Taskar B., 2002, UAI; Todorovic S., 2008, CVPR08; Towell G. G., 1990, AAAI-90 Proceedings. Eighth National Conference on Artificial Intelligence, P861; Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Varma M., 2005, INT J COMPUTER VISIO; Viola P., 2001, ROBUST REAL TIME OBJ; Viola Paul, 2001, PROC CVPR IEEE; Wang W, 2006, IEEE T IMAGE PROCESS, V15, P3033, DOI 10.1109/TIP.2006.877496; WU, 2005, IEEE INT C COMP VIS; Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7; Zhu L., 2008, COMPUTER VISION ECCV; Zhu Qiang, 2006, CVPR, DOI DOI 10.1109/CVPR.2006.119; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	58	7	8	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2011	93	2					141	161		10.1007/s11263-010-0343-9	http://dx.doi.org/10.1007/s11263-010-0343-9			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	740PD					2022-12-18	WOS:000288806000003
J	Gur, Y; Pasternak, O; Sochen, N				Gur, Yaniv; Pasternak, Ofer; Sochen, Nir			Fast GL(n)-Invariant Framework for Tensors Regularization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Beltrami framework; Differential geometry; PDEs; Tensor-valued image; Diffusion tensor imaging	GEOMETRY; TISSUES	We propose a novel framework for regularization of symmetric positive-definite (SPD) tensors (e. g., diffusion tensors). This framework is based on a local differential geometric approach. The manifold of symmetric positive-definite (SPD) matrices, P-n, is parameterized via the Iwasawa coordinate system. In this framework distances on P-n are measured in terms of a natural GL(n)-invariant metric. Via the mathematical concept of fiber bundles, we describe the tensor-valued image as a section where the metric over the section is induced by the metric over P-n. Then, a functional over the sections accompanied by a suitable data fitting term is defined. The variation of this functional with respect to the Iwasawa coordinates leads to a set of 1/2n(n + 1) coupled equations of motion. By means of the gradient descent method, these equations of motion define a Beltrami flow over P-n. It turns out that the local coordinate approach via the Iwasawa coordinate system results in very simple numerics that leads to fast convergence of the algorithm. Regularization results as well as results of fibers tractography for DTI are presented.	[Gur, Yaniv; Sochen, Nir] Tel Aviv Univ, Dept Appl Math, IL-69978 Tel Aviv, Israel; [Pasternak, Ofer] Tel Aviv Univ, Sch Comp Sci, IL-69978 Tel Aviv, Israel	Tel Aviv University; Tel Aviv University	Gur, Y (corresponding author), Tel Aviv Univ, Dept Appl Math, IL-69978 Tel Aviv, Israel.	yanivg@post.tau.ac.il; oferpas@post.tau.ac.il; sochen@post.tau.ac.il						BASSER PJ, 1994, BIOPHYS J, V66, P259, DOI 10.1016/S0006-3495(94)80775-1; Basser PJ, 1996, J MAGN RESON SER B, V111, P209, DOI [10.1006/jmrb.1996.0086, 10.1016/j.jmr.2011.09.022]; Batchelor PG, 2005, MAGN RESON MED, V53, P221, DOI 10.1002/mrm.20334; Blomgren P, 1998, IEEE T IMAGE PROCESS, V7, P304, DOI 10.1109/83.661180; ChefD'Hotel C, 2004, J MATH IMAGING VIS, V20, P147, DOI 10.1023/B:JMIV.0000011324.14508.fb; Coulon O., 2001, Information Processing in Medical Imaging. 17th International Conference, IPMI 2001. Proceedings (Lecture Notes in Computer Science Vol.2082), P92; Feddern C, 2006, INT J COMPUT VISION, V69, P93, DOI 10.1007/s11263-006-6854-8; Fillard P, 2007, IEEE T MED IMAGING, V26, P1472, DOI 10.1109/TMI.2007.899173; Fletcher PT, 2007, SIGNAL PROCESS, V87, P250, DOI 10.1016/j.sigpro.2005.12.018; GUR Y, 2007, LECT NOTES COMPUTER, V4485, P580; JORGENSON J, 2005, LECT NOTES MATH; JOST J, 2001, RIEMANNIAN GEOMETRY; Kimmel R, 1997, LECT NOTES COMPUT SC, V1252, P236; LANG S, 1999, FUNDEMENTALS DIFFERE; Moakher M, 2005, SIAM J MATRIX ANAL A, V26, P735, DOI 10.1137/S0895479803436937; Mori S., 2005, MRI ATLAS HUMAN WHIT, V1st; Pajevic S, 1999, MAGNET RESON MED, V42, P526, DOI 10.1002/(SICI)1522-2594(199909)42:3<526::AID-MRM15>3.0.CO;2-J; Pasternak O, 2008, MAGN RESON IMAGING, V26, P1133, DOI 10.1016/j.mri.2008.01.006; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Pierpaoli C, 1996, RADIOLOGY, V201, P637, DOI 10.1148/radiology.201.3.8939209; Rosenfeld, 1997, GEOMETRY LIE GROUPS; SHAFRIR D, 2005, P 3 IEEE WORKSH VAR; Sochen N, 1998, IEEE T IMAGE PROCESS, V7, P310, DOI 10.1109/83.661181; Terras A, 1988, HARMONIC ANAL SYMMET, VII; Tschumperle D, 2002, INT J COMPUT VISION, V50, P237, DOI 10.1023/A:1020870207168; Wang ZZ, 2004, IEEE T MED IMAGING, V23, P930, DOI 10.1109/TMI.2004.831218; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; WEICKERT J, 2005, VISUALIZATION PROCES; Zerai M, 2007, LECT NOTES COMPUT SC, V4485, P592	30	7	7	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2009	85	3					211	222		10.1007/s11263-008-0196-7	http://dx.doi.org/10.1007/s11263-008-0196-7			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	502CO					2022-12-18	WOS:000270432200002
J	Hu, CX; Cheong, LF				Hu, Chuanxin; Cheong, Loong Fah			Linear Quasi-Parallax SfM Using Laterally-Placed Eyes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Ego-motion estimation based on optical flow; Quasi-parallax terms; Lateral camera pairs; Compound eyes	NEURONS; MOTION; HONEYBEE; SYSTEM	A large class of visual systems in the biological world often has multiple eyes in simultaneous motion and yet has little or no overlap in the visual fields between the eyes. These systems include the lateral eyes found in many vertebrates and the compound eyes in insects. Instead of computing feature correspondences between the eyes, which might not even be possible due to the lack of overlap in the visual fields, we exploit the organizational possibility offered by the eye topography. In particular, we leverage on the pair of visual rays that are parallel to each other but opposite in direction, and compute what we call the quasi-parallax for translation recovery. Besides resulting in parsimonious visual processing, the quasi-parallax term also enhances the information pick-up for the translation, as it is almost rotation-free. The rotation is subsequently recovered from a pencil of visual rays using the individual epipolar constraints of each camera. As a result of using these different and appropriate aspects of visual rays for motion recovery, our method is numerically more effective in disambiguating the translation and rotation. In comparison to the gold standard solution obtained by the bundle adjustment (BA) technique, our method has a better Fisher information matrix for a lateral eye pair, as well as a superior experimental performance under the case of narrow field of view. For other eye configurations, the two methods achieve comparable performances, with our linear method slightly edging the nonlinear BA method when there exists imperfection in the calibration.	[Hu, Chuanxin] Microsoft Res Asia, Search Technol Ctr, Beijing, Peoples R China; [Cheong, Loong Fah] Natl Univ Singapore, Singapore 117548, Singapore	Microsoft; Microsoft Research Asia; National University of Singapore	Hu, CX (corresponding author), Microsoft Res Asia, Search Technol Ctr, 4F Beijing Sigma Ctr,49 Zhichun Rd, Beijing, Peoples R China.	chhu@microsoft.com; eleclf@nus.edu.sg						Anandan P, 2002, INT J COMPUT VISION, V49, P101, DOI 10.1023/A:1020137420717; Baker P, 2004, IEEE ROBOT AUTOM MAG, V11, P31, DOI 10.1109/MRA.2004.1371606; Blanke H, 1997, J COMP PHYSIOL A, V181, P383, DOI 10.1007/s003590050123; Chahl JS, 1997, APPL OPTICS, V36, P8275, DOI 10.1364/AO.36.008275; CLARK J, 1994, DATA FUSION SENSORY; Coombs D., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P440, DOI 10.1109/CVPR.1993.341093; Davies M.N.O., 1994, P339; Franz MO, 1998, AUTON ROBOT, V5, P111, DOI 10.1023/A:1008821210922; Franz MO, 1998, BIOL CYBERN, V79, P191, DOI 10.1007/s004220050470; Haag J, 2001, J NEUROSCI, V21, P5685, DOI 10.1523/JNEUROSCI.21-15-05685.2001; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; Hornsey R, 2004, P SOC PHOTO-OPT INS, V5301, P13, DOI 10.1117/12.526811; Huber SA, 1998, FROM ANIM ANIMAT, P77; IBBOTSON MR, 1991, J COMP PHYSIOL A, V168, P91, DOI 10.1007/BF00217107; Jeong KH, 2006, SCIENCE, V312, P557, DOI 10.1126/science.1123053; Kern R, 1998, J COMP PHYSIOL A, V182, P239, DOI 10.1007/s003590050174; Kim JY, 2005, OPT LETT, V30, P5, DOI 10.1364/OL.30.000005; Lee A.B., 2000, BROWN RANGE IMAGE DA; Leedan Y, 2000, INT J COMPUT VISION, V37, P127, DOI 10.1023/A:1008185619375; LEONARD CS, 1988, J NEUROPHYSIOL, V60, P2073, DOI 10.1152/jn.1988.60.6.2073; Lim J., 2008, CVPR; LIM J, 2007, OMNIVIS, P1; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Neumann J., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P3712; Pless R, 2004, IEEE ROBOT AUTOM MAG, V11, P39, DOI 10.1109/MRA.2004.1371607; SRINIVASAN MV, 1991, VISUAL NEUROSCI, V6, P519, DOI 10.1017/S095252380000136X; Srinivasan MV, 1996, J EXP BIOL, V199, P237; Srinivasan MV, 2001, BIOL BULL, V200, P216, DOI 10.2307/1543319; Stewenius H, 2004, LECT NOTES COMPUT SC, V3023, P252; Sturm P, 2005, PROC CVPR IEEE, P206; THOMAS I, 1994, LINEAR STRUCTURE MOT; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; TSAO AT, 1997, CVPR; Wylie DRW, 1999, J NEUROPHYSIOL, V81, P267, DOI 10.1152/jn.1999.81.1.267; Xiang T, 2003, INT J COMPUT VISION, V51, P111, DOI 10.1023/A:1021627622971; ZHANG ZY, 1995, IEEE T PATTERN ANAL, V17, P1222, DOI 10.1109/34.476516	38	7	7	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2009	84	1					21	39		10.1007/s11263-009-0226-0	http://dx.doi.org/10.1007/s11263-009-0226-0			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	445GH					2022-12-18	WOS:000266037900002
J	Gorelick, L; Basri, R				Gorelick, Lena; Basri, Ronen			Shape Based Detection and Top-Down Delineation Using Image Segments	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape-based object detection; Class-based top-down segmentation		We introduce a segmentation-based detection and top-down figure-ground delineation algorithm. Unlike common methods which use appearance for detection, our method relies primarily on the shape of objects as is reflected by their bottom-up segmentation. Our algorithm receives as input an image, along with its bottom-up hierarchical segmentation. The shape of each segment is then described both by its significant boundary sections and by regional, dense orientation information derived from the segment's shape using the Poisson equation. Our method then examines multiple, overlapping segmentation hypotheses, using their shape and color, in an attempt to find a "coherent whole," i.e., a collection of segments that consistently vote for an object at a single location in the image. Once an object is detected, we propose a novel pixel-level top-down figure-ground segmentation by "competitive coverage" process to accurately delineate the boundaries of the object. In this process, given a particular detection hypothesis, we let the voting segments compete for interpreting (covering) each of the semantic parts of an object. Incorporating competition in the process allows us to resolve ambiguities that arise when two different regions are matched to the same object part and to discard nearby false regions that participated in the voting process. We provide quantitative and qualitative experimental results on challenging datasets. These experiments demonstrate that our method can accurately detect and segment objects with complex shapes, obtaining results comparable to those of existing state of the art methods. Moreover, our method allows us to simultaneously detect multiple instances of class objects in images and to cope with challenging types of occlusions such as occlusions by a bar of varying size or by another object of the same class, that are difficult to handle with other existing class-specific top-down segmentation methods.	[Gorelick, Lena] Weizmann Inst Sci, Dept Comp Sci & Appl Math, IL-76100 Rehovot, Israel; [Basri, Ronen] Toyota Technol Inst, Chicago, IL 60637 USA	Weizmann Institute of Science; Toyota Technological Institute - Chicago	Gorelick, L (corresponding author), Weizmann Inst Sci, Dept Comp Sci & Appl Math, IL-76100 Rehovot, Israel.	lena.gorelick@weizmann.ac.il			US-Israel Binational Science Foundation [2002/254]; European Commission Project [IST-2000-26001]; Weizmann Institute	US-Israel Binational Science Foundation(US-Israel Binational Science Foundation); European Commission Project(European CommissionEuropean Commission Joint Research Centre); Weizmann Institute	The authors thank Achi Brandt for his help particularly in formalizing a multilevel solution to the Poisson equation. Research was supported in part by the US-Israel Binational Science Foundation grant number 2002/254 and by the European Commission Project IST-2000-26001 VIBES. The vision group at the Weizmann Institute is supported in part by the Moross Foundation.	AGARWAL S, 2002, EUR C COMP VIS, V2, P113; BORENSTEIN E, 2006, P IEEE C COMP VIS PA; BORENSTEIN E, 2004, WORKSH PERC ORG COMP; BURL MC, 1998, LECT NOTES COMPUTER, V1407; CAO L, 2007, INT C COMP VIS 2007; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Ferrari V., 2007, P IEEE C COMP VIS PA; Gorelick L, 2006, IEEE T PATTERN ANAL, V28, P1991, DOI 10.1109/TPAMI.2006.253; Gustafson K., 1998, CONTEMP MATH, V218, P432; Kumar MP, 2005, PROC CVPR IEEE, P18; KUMAR MP, 2004, BRIT MACH VIS C 2004; Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3; LEVIN A, 2006, EUR C COMP VIS 2006; Mori G., 2004, IEEE C COMP VIS PATT; OPELT A, 2006, EUR C COMP VIS MAY 2; PANTOFARU C, 2008, COMBINING REGIONS PA, P23; REN X, 2005, ADV NEURAL INFORM PR, V18; REN X, 2005, ICCV, V1, P824; Russell B. C., 2006, P IEEE C COMP VIS PA; Sharon E, 2006, NATURE, V442, P810, DOI 10.1038/nature04977; Shotton J, 2005, IEEE I CONF COMP VIS, P503; TODOROVIC S, 2007, INT C COMP VIS 2007; TROTTENBERG U., 2000, MULTIGRID; Ullman S., 2001, INT WORKSH VIS FORM; Vidal-Naquet M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P281; WANG L, 2007, AS C COMP VIS 2007; Weber M, 2000, PROC CVPR IEEE, P101, DOI 10.1109/CVPR.2000.854754; WINN J, 2005, INT C COMP VIS BEIJ	28	7	7	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2009	83	3					211	232		10.1007/s11263-009-0216-2	http://dx.doi.org/10.1007/s11263-009-0216-2			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	424EI					2022-12-18	WOS:000264547900001
J	Adluru, N; Latecki, LJ				Adluru, Nagesh; Latecki, Longin Jan			Contour Grouping Based on Contour-Skeleton Duality	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Contour grouping; Skeletons; Shape models; Rao-Blackwellized particle filters; SLAM	SHAPE; SEGMENTATION; TRACKING; FILTERS	In this paper we present a method for grouping relevant object contours in edge maps by taking advantage of contour-skeleton duality. Regularizing contours and skeletons simultaneously allows us to combine both low level perceptual constraints as well as higher level model constraints in a very effective way. The models are represented using paths in symmetry sets. Skeletons are treated as trajectories of an imaginary virtual robot in a discrete space of "symmetric points" obtained from pairs of edge segments. Boundaries are then defined as the maps obtained by grouping the associated pairs of edge segments along the trajectories. Casting the grouping problem in this manner makes it similar to the problem of Simultaneous Localization and Mapping (SLAM). Hence we adapt the state-of-the-art probabilistic framework namely Rao-Blackwellized particle filtering that has been successfully applied to SLAM. We use the framework to maximize the joint posterior over skeletons and contours.	[Adluru, Nagesh; Latecki, Longin Jan] Temple Univ, Philadelphia, PA 19122 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple University	Adluru, N (corresponding author), Temple Univ, Philadelphia, PA 19122 USA.	nagesh@temple.edu		Latecki, Longin Jan/0000-0002-5102-8244				ADLURU N, 2008, IAPR INT C PATT REC; ADLURU N, 2007, ICCV 07; [Anonymous], 1985, PERCEPTUAL ORG VISUA; Bai X, 2008, IEEE T PATTERN ANAL, V30, P1282, DOI 10.1109/TPAMI.2007.70769; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Black MJ, 2000, INT J COMPUT VISION, V38, P231, DOI 10.1023/A:1008195307933; BLAKE A, 1997, ACTIVE CONTOURS; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; Blum Harry, 1967, TRANSFORMATION EXTRA, V43, P2; BORENSTEIN E, 2002, ECCV, P109; BORGEFORS G, 1988, IEEE T PATTERN ANAL, V10, P849, DOI 10.1109/34.9107; BRUCE JW, 1985, P ROY SOC EDINB A, V101, P163, DOI 10.1017/S0308210500026263; CARPENTER J, 1999, BUILDING ROBUST SIMU; Carpin S, 2005, ROBOT AUTON SYST, V53, P1, DOI 10.1016/j.robot.2005.07.001; Chang HJ, 2007, IEEE T ROBOT, V23, P281, DOI 10.1109/TRO.2007.892230; Choi HI, 1997, PAC J MATH, V181, P57, DOI 10.2140/pjm.1997.181.57; COX IJ, 1993, INT J COMPUT VISION, V11, P5, DOI 10.1007/BF01420590; Doucet A., 2001, SEQUENTIAL MONTE CAR; DOUCET A, 1998, SEQUENTIAL SIMULATIO; ELIAZAR A, 2004, IEEE INT C ROB AUT I; Eliazar A., 2003, INT JOINT C ART INT; Feldman J, 2001, PERCEPT PSYCHOPHYS, V63, P1171, DOI 10.3758/BF03194532; Feldman J, 2003, INT J COMPUT VISION, V55, P5, DOI 10.1023/A:1024454423670; Feldman J, 1999, ACTA PSYCHOL, V102, P137, DOI 10.1016/S0001-6918(98)00054-7; Feldman J, 2006, P NATL ACAD SCI USA, V103, P18014, DOI 10.1073/pnas.0608811103; Ferrari V, 2006, LECT NOTES COMPUT SC, V3953, P14, DOI 10.1007/11744078_2; Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144; Fox D, 2003, INT J ROBOT RES, V22, P985, DOI 10.1177/0278364903022012001; FOX D, 1999, EFFICIENT MULTIROBOT; GALUN M, 2007, ICCV 07; Giblin P, 2000, MATHEMATICS OF SURFACES IX, P306; Giblin PJ, 2003, IEEE T PATTERN ANAL, V25, P895, DOI 10.1109/TPAMI.2003.1206518; Giblin PJ, 2003, INT J COMPUT VISION, V54, P143, DOI 10.1023/A:1023761518825; Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486; HANDSCHIN JE, 1970, AUTOMATICA, V6, P555, DOI 10.1016/0005-1098(70)90010-5; HANDSCHIN JE, 1969, INT J CONTROLS, V9, P353; Hoiem D., 2007, INT C COMP VIS ICCV; Howard A., 2005, P ROB SCI SYST CAMBR; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Kimia B, 2002, LECT NOTES COMPUT SC, V2525, P219; KONG A, 1994, J AM STAT ASSOC, V89, P278, DOI 10.2307/2291224; Kovesi P. D., 2008, MATLAB OCTAVE FUNCTI; Kuijper A, 2004, INT C PATT RECOG, P190, DOI 10.1109/ICPR.2004.1334500; KUIJPER A, 2005, IMAGE PROCESSING 200, P497; KUIJPER A, 2006, ECCV06, P213; Kuijper A, 2006, INT C PATT RECOG, P179; Kuijper A, 2006, J MATH IMAGING VIS, V26, P127, DOI 10.1007/s10851-006-8372-2; Kumar MP, 2005, PROC CVPR IEEE, P18; LATECKI LJ, 2008, NEURAL INFORM PROCES; LATECKI LJ, 2006, ACM SIGKDD INT C KNO; Leibe B, 2005, PROC CVPR IEEE, P878; LEIBE B, 2003, BMVC03; Leyton M., 1992, SYMMETRY CAUSALITY M; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; Liu JS, 1998, J AM STAT ASSOC, V93, P1022; Liu TL, 1998, INT C PATT RECOG, P994, DOI 10.1109/ICPR.1998.711856; Lockwood E. H., 2007, BOOK CURVES; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Meila M., 2005, P 22 INT C MACH LEAR, P577; MOHAN R, 1992, IEEE T PATTERN ANAL, V14, P616, DOI 10.1109/34.141553; MORAVEC HP, 1988, AI MAG, V9, P61; Opelt A., 2006, P EUR C COMP VIS; Perez P, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P524, DOI 10.1109/ICCV.2001.937670; PODOLAK J., 2006, ACM T GRAPHICS P SIG, V25; RAVIV D, 2007, ICCV; REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177; REN X, 2005, P ICCV; Ren XF, 2008, INT J COMPUT VISION, V77, P47, DOI 10.1007/s11263-007-0092-6; Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924; Shotton J, 2005, IEEE I CONF COMP VIS, P503; Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703; Siddiqi K., 2007, MEDIAL REPRESENTATIO; Stachniss C., 2005, ROBOTICS SCI SYSTEMS, V2, P65, DOI DOI 10.15607/RSS.2005.I.009; STAHL JS, 2006, P CVPR; Stein A, 2007, IEEE I CONF COMP VIS, P110; TAMRAKAR A, 2007, ICCV 07; Tanizaki H, 1997, COMPUT STAT DATA AN, V25, P417, DOI 10.1016/S0167-9473(97)00016-9; Thrun S., 2005, PROBABILISTIC ROBOTI; TRINH N, 2007, ICCV 07; Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x; Tu ZW, 2004, LECT NOTES COMPUT SC, V3023, P195; VEKSLER O, 2008, ECCV08; WANG H, 2006, POCV06; WANG H, 2008, ECCV08; Wang LM, 2007, LECT NOTES COMPUT SC, V4843, P189; Wertheimer M., 1958, READINGS PERCEPTION; Wertheimer M, 1923, PSYCHOL FORSCH, V4, P301, DOI 10.1007/BF00410640; Witkin A. P., 1983, HUMAN MACHINE VISION; YANG X, 2008, ECCV08; Zaritskii V. S., 1975, AUTOMAT REM CONTR+, P95; ZHU Q, 2008, ECCV08; ZHU Q, 2007, ICCV 07; ZHU SC, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P465	95	7	7	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2009	83	1					12	29		10.1007/s11263-009-0208-2	http://dx.doi.org/10.1007/s11263-009-0208-2			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	413JY		Green Submitted			2022-12-18	WOS:000263790600002
J	Damon, J				Damon, James			Tree structure for contractible regions in R-3	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						blum medial axis; contractible region; irreducible medial components; essential and inessential fin curves; Y-network; top level and second level (directed) trees	DISTANCE; CURVES; AXIS	For contractible regions Omega in R-3 with generic smooth boundary, we determine the global structure of the Blum medial axis M. We give an algorithm for decomposing M into "irreducible components" which are attached to each other along "fin curves". The attaching cannot be described by a tree structure as in the 2D case. However, a simplified but topologically equivalent medial structure M with the same irreducible components can be described by a two level tree structure. The top level describes the simplified form of the attaching, and the second level tree structure for each irreducible component specifies how to construct the component by attaching smooth medial sheets to the network of Y-branch curves. The conditions for these structures are complete in the sense that any region whose Blum medial axis satisfies the conditions is contractible.	Univ N Carolina, Dept Math, Chapel Hill, NC 27599 USA	University of North Carolina; University of North Carolina Chapel Hill	Damon, J (corresponding author), Univ N Carolina, Dept Math, Chapel Hill, NC 27599 USA.							Bille P, 2005, THEOR COMPUT SCI, V337, P217, DOI 10.1016/j.tcs.2004.12.030; BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0; Bogaevsky IA, 2002, PHYSICA D, V173, P1, DOI 10.1016/S0167-2789(02)00652-8; BOGAEVSKY IA, 1990, ST PETERSBURG LENING, V1, P807; Damon J, 2005, INT J COMPUT VISION, V63, P45, DOI 10.1007/s11263-005-4946-5; Damon J, 2003, ANN I FOURIER, V53, P1941, DOI 10.5802/aif.1997; DAMON J, IN PRESS COMM ANAL G; DAMON J, IN PRESS GEOMETRY TO; GAGE M, 1986, J DIFFER GEOM, V23, P69; GAGE ME, 1984, INVENT MATH, V76, P357, DOI 10.1007/BF01388602; Giblin P, 2002, LECT NOTES COMPUT SC, V2351, P718; Giblin P, 2000, MATHEMATICS OF SURFACES IX, P306; Goodman S. E, 2005, BEGINNING TOPOLOGY; GRAYSON MA, 1987, J DIFFER GEOM, V26, P285; KIMIA B., 1990, 3 DIMENSIONAL COMPUT; MATHER JN, 1983, P SYMP PURE MATH, V40, P199; Munkres J. R., 2000, TOPOLOGY, V2nd; PIZER S, IN PRESS MEDIAL REPR; Pizer SM, 2003, INT J COMPUT VISION, V55, P155, DOI 10.1023/A:1026135101267; Pizer SM, 2003, INT J COMPUT VISION, V55, P85, DOI 10.1023/A:1026313132218; Sethian J., 1996, LEVEL SET METHODS; Siddiqi K, 2002, INT J COMPUT VISION, V48, P215, DOI 10.1023/A:1016376116653; SZEKELY G, 1994, P 2 INT WORKSH VIS F, P532; VALIENTE G, 2002, ALGORITHMS TREES GRA; YOMDIN Y, 1981, COMPOS MATH, V43, P225	25	7	7	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2007	74	2					103	116		10.1007/s11263-006-0004-1	http://dx.doi.org/10.1007/s11263-006-0004-1			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	167ZH					2022-12-18	WOS:000246490900001
J	Mercier, B; Meneveaux, D; Fournier, A				Mercier, Bruno; Meneveaux, Daniel; Fournier, Alain			A framework for automatically recovering object shape, reflectance and light sources from calibrated images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						shape from silhouette; marching cubes; multiple light sources detection; reflectance properties recovery; reshading	DIRECTION; MODEL	In this paper, we present a complete framework for recovering an object shape, estimating its reflectance properties and light sources from a set of images. The whole process is performed automatically. We use the shape from silhouette approach proposed by R. Szeliski (1993) combined with image pixels for reconstructing a triangular mesh according to the marching Cubes algorithm. A classification process identifies regions of the object having the same appearance. For each region, a single point or directional light source is detected. Therefore, we use specular lobes, lambertian regions of the surface or specular highlights seen on images. An identification method jointly (i) decides what light Sources are actually significant and (ii) estimates diffuse and specular coefficients for a surface represented by the modified Phong model (Lewis, 1994). In order to validate our algorithm efficiency, we present a case study with various objects, light sources and surface properties. As shown in the results, our system proves accurate even for real objects images obtained with an inexpensive acquisition system.	Univ Poitiers, SIC Lab, F-86962 Futuroscope, France; Univ British Columbia, Dept Comp Sci, Imager Lab, Vancouver, BC V6T 1Z4, Canada	Universite de Poitiers; University of British Columbia	Mercier, B (corresponding author), Univ Poitiers, SIC Lab, Bat SP2MI,Bd M & P Curie, F-86962 Futuroscope, France.	mercier@sic.univ-poitiers.fr; daniel@sic.univ-poitiers.fr						Boivin S, 2001, COMP GRAPH, P107, DOI 10.1145/383259.383270; BROOKS MJ, 1985, SHAPE SOURCE SHADING; CALLET P, 2004, ICCVG 2004, P469; CHEN Q, 1999, COMPUTER VISION PATT, P29; CHIEN CH, 1986, COMPUT VISION GRAPH, V36, P100, DOI 10.1016/S0734-189X(86)80031-7; Cook R., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; Debevec P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P189, DOI 10.1145/280814.280864; Esteban CH, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P46, DOI 10.1109/IM.2003.1240231; FAUGERAS OD, 1998, LNCS, V1406, P379; GUILLOU E, 2000, THESIS U RENNES 1; Hasenfratz Jean-Marc, 2003, VISION VIDEO GRAPHIC, P49; HAWKINS T, 2004, EGSR RENDERING TECHN, P309; HE XD, 1991, ACM SIGGRAPH COMPUTE, V25, P175, DOI DOI 10.1145/127719; HOUGEN DR, 1994, CVPR, P991; HOUGEN DR, 1993, P INT C COMP VIS, P148; KAY G, 1994, CVGIP-IMAG UNDERSTAN, V59, P183, DOI 10.1006/ciun.1994.1012; KUTULAKOS KN, 1998, TR692; Lafortune E. P. F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P117, DOI 10.1145/258734.258801; LEE CH, 1985, ARTIF INTELL, V26, P125, DOI 10.1016/0004-3702(85)90026-8; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849; LEWIS R, 1994, COMPUTER GRAPHICS FO, V13; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; MARTIN WN, 1983, IEEE T PATTERN ANAL, V5, P150, DOI 10.1109/TPAMI.1983.4767367; Mercier B., 2005, J WSCG, V13, P112; MERCIER B, 2004, ICCVG 2004, P66; NILLIUS P, 2001, CVPR, P1076; Oren M., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P239, DOI 10.1145/192161.192213; PENTLAND AP, 1982, J OPT SOC AM, V72, P448, DOI 10.1364/JOSA.72.000448; Powell MW, 2001, IEEE T PATTERN ANAL, V23, P1022, DOI 10.1109/34.955114; Sato I., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P306, DOI 10.1109/CVPR.1999.786956; Sato Y, 1996, GRAPH MODEL IM PROC, V58, P437, DOI 10.1006/gmip.1996.0036; SATO Y, 1997, COMPUTER GRAPHICS, V31, P379; Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462; SINGH H, 1994, CSTR3218 U MAR CTR A; Slabaugh G, 2001, SPRING EUROGRAP, P81; SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; VEGA OE, 1994, CVGIP-IMAG UNDERSTAN, V60, P285, DOI 10.1006/ciun.1994.1058; WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078; YANG Y, 1991, IEEE P COMP VIS PATT, P534; Yu YZ, 1999, COMP GRAPH, P215; ZHANG Y, 2000, CVPR00, P269; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658; ZHOU W, 2002, EUR C COMP VIS, P206	46	7	7	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2007	73	1					77	93		10.1007/s11263-006-9273-y	http://dx.doi.org/10.1007/s11263-006-9273-y			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	141YV		Green Submitted			2022-12-18	WOS:000244616100004
J	Avidan, S; Moses, Y; Moses, Y				Avidan, Shai; Moses, Yael; Moses, Yoram			Centralized and distributed multi-view correspondence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						correspondence; wide baseline stereo; multi-view; distributed vision; random graphs; error correction; fault tolerance; distributed multi-camera system		A probabilistic algorithm is presented for finding correspondences across multiple images in systems with large numbers of cameras and considerable overlap. The algorithm employs the theory of random graphs to provide an efficient probabilistic algorithm that performs Wide-baseline Stereo (WBS) comparisons on a small number of image pairs, and then propagates correspondence information among the cameras. A concrete mathematical analysis of its performance is given. The algorithm is extended to handle false-positive and false-negative failures of the WBS computations. We characterize the delectability of the existence of such failures, and propose an efficient method for this detection. Based on this, we propose a heuristic method for discarding false matches, and demonstrate its effectiveness in reducing errors. Since in many multi-camera applications cameras are attached to processors that handle local processing and communication, it is natural to consider distributed solutions that make use of the local processors and do not use a central computer. Our algorithm is especially suited to run in a distributed setting. If the local processors are sufficiently powerful, this allows an order of magnitude increase in computational efficiency. More importantly, a distributed implementation provides strong robustness guarantees, and eliminates the existence of a single point of failure that is inherent when the application is coordinated by a central computer. We show how to efficiently overcome processor crashes and communication failures with a minimal reduction in the quality of the algorithm's results.	Efi Arazi Sch Comp Sci, Interdisciplinary Ctr, Herzliyya, Israel; Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Reichman University; Technion Israel Institute of Technology	Avidan, S (corresponding author), Efi Arazi Sch Comp Sci, Interdisciplinary Ctr, Herzliyya, Israel.	avidan@idc.ac.il; yael@idc.ac.il; moses@ee.technion.ac.il						AVIDAN S, 2004, P 8 EUR C COMP VIS E, P428; Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95; BAUMBERG A, 2000, P IEEE COMP VIS PATT, P1; BOLLOBAS B, 1985, RANDOM GRAPHS ANN DI; Bollobas B., 1985, RANDOM GRAPHS, DOI 10.1017/CBO9780511814068.008; Brumitt B, 2000, IEEE PERS COMMUN, V7, P41, DOI 10.1109/98.878536; Cai Q, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P356, DOI 10.1109/ICCV.1998.710743; CHETVERIKOV D, 2002, TEXTURE, P25; Collins R. T., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P528, DOI 10.1109/CVPR.1999.786988; Collins Robert T, 2000, SYSTEM VIDEO SURVEIL, P1; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Ferrari V, 2003, PROC CVPR IEEE, P718; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Kang JM, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P181, DOI 10.1109/MOTION.2002.1182232; KARUPPIAH D, 2001, ICVS, P201; Levi N, 2003, PROC CVPR IEEE, P518; Lopez-De-Ipina D, 2002, PERS UBIQUIT COMPUT, V6, P206, DOI 10.1007/s007790200020; Lynch N. A., 1996, DISTRIBUTED ALGORITH; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mittal A, 2003, INT J COMPUT VISION, V51, P189, DOI 10.1023/A:1021849801764; Mullender S., 1993, DISTRIBUTED SYSTEMS, Vsecond; NARAYANAN PJ, 1995, P IEEE WORKSH REPR V, P69; Pritchett P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P754, DOI 10.1109/ICCV.1998.710802; Rahimi M., 2004, P 2 INT C EMB NETW S, P311; Saito H., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P516, DOI 10.1109/IM.1999.805384; Schaffalitzky F, 2002, LECT NOTES COMPUT SC, V2350, P414; Schaffalitzky F, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P636, DOI 10.1109/ICCV.2001.937686; TANENBAUM AS, 2001, DISTRIBUTED SYSTEMS; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Tuytelaars T., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P412; WREN CR, 2003, P INT C UB COMP SEAT, P205	32	7	8	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2007	71	1					49	69		10.1007/s11263-005-4888-y	http://dx.doi.org/10.1007/s11263-005-4888-y			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	094GT					2022-12-18	WOS:000241228600002
J	Janssen, B; Kanters, F; Duits, R; Florack, L; Romeny, BTH				Janssen, Bart; Kanters, Frans; Duits, Remco; Florack, Luc; Romeny, Bart Ter Haar			A linear image reconstruction framework based on Sobolev type inner products	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	5th International Conference on Scale-Space and PDE Methods in Computer Vision	APR 07-09, 2005	Hofgeismar, GERMANY	German Pattern Recognit Soc		reconstruction; scale space; sampling; deep structure		Exploration of information content of features that are present in images has led to the development of several reconstruction algorithms. These algorithms aim for a reconstruction from the features that is visually close to the image from which the features are extracted. Degrees of freedom that are not fixed by the constraints are disambiguated with the help of a so-called prior (i.e. a user defined model). We propose a linear reconstruction framework that generalizes a previously proposed scheme. The algorithm greatly reduces the complexity of the reconstruction process compared to non-linear methods. As an example we propose a specific prior and apply it to the reconstruction from singular points. The reconstruction is visually more attractive and has a smaller L-2-error than the reconstructions obtained by previously proposed linear methods.	Eindhoven Univ Technol, NL-5600 MB Eindhoven, Netherlands	Eindhoven University of Technology	Janssen, B (corresponding author), Eindhoven Univ Technol, Dolech 2,POB 513, NL-5600 MB Eindhoven, Netherlands.	B.J.Janssen@tue.nl; F.M.W.Kanters@tue.nl; R.Duits@tue.nl; L.M.J.Florack@tue.nl; B.M.terHaarRomeny@tue.nl	Romenij, Bart M. ter Haar/A-5323-2013					DAMON J, 1995, J DIFFER EQUATIONS, V115, P368, DOI 10.1006/jdeq.1995.1019; Duits R, 2004, J MATH IMAGING VIS, V20, P267, DOI 10.1023/B:JMIV.0000024043.96722.aa; DUITS R, 2005, 1 INT WORKSH DEEP ST; Florack L, 2004, IEEE IMAGE PROC, P271; Florack L, 2000, J MATH IMAGING VIS, V12, P65, DOI 10.1023/A:1008304909717; Gilmore R., 1981, CATASTROPHE THEORY; Johansen P, 2000, J MATH IMAGING VIS, V13, P193, DOI 10.1023/A:1011241531216; JOHANSEN P, 1986, P 8 INT C PATT REC P, P215; KANTERS, 2004, SCALESPACEVIZ; Kanters F, 2003, LECT NOTES COMPUT SC, V2695, P464; Kybic J, 2002, IEEE T SIGNAL PROCES, V50, P1965, DOI 10.1109/TSP.2002.800391; Lillholm M, 2003, INT J COMPUT VISION, V52, P73, DOI 10.1023/A:1022995822531; Nielsen M, 2001, LECT NOTES COMPUT SC, V2106, P39; PAPOULIS A, 1977, IEEE T CIRCUITS SYST, V24, P652, DOI 10.1109/TCS.1977.1084284; Platel B, 2004, IEEE IMAGE PROC, P389; *REMC DUITS, 2005, THESIS TU EINDHOVEN; SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969; Tikhonov A.N., 1977, SOLUTION ILL POSED P; UNSER M, 1994, IEEE T SIGNAL PROCES, V42, P2915, DOI 10.1109/78.330352; Wolfram S., 1991, MATH SYSTEM DOING MA; Yosida K, 1980, FUNCTIONAL ANAL, V6th, DOI 10.1007/978-3-662-25762-3	21	7	7	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2006	70	3					231	240		10.1007/s11263-006-6703-9	http://dx.doi.org/10.1007/s11263-006-6703-9			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	091VL					2022-12-18	WOS:000241056300004
J	Xiang, T; Gong, SG				Xiang, Tao; Gong, Shaogang			Model selection for unsupervised learning of visual context	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						learning for vision; visual context; model selection; dynamic scene modelling; clustering; Bayesian methods; mixture models	MIXTURE; RECOGNITION	This study addresses the problem of choosing the most suitable probabilistic model selection criterion for unsupervised learning of visual context of a dynamic scene using mixture models. A rectified Bayesian Information Criterion (BICr) and a Completed Likelihood Akaike's Information Criterion (CL-AIC) are formulated to estimate the optimal model order (complexity) for a given visual scene. Both criteria are designed to overcome poor model selection by existing popular criteria when the data sample size varies from small to large and the true mixture distribution kernel functions differ from the assumed ones. Extensive experiments on learning visual context for dynamic scene modelling are carried out to demonstrate the effectiveness of BICr and CL-AIC, compared to that of existing popular model selection criteria including BIC, AIC and Integrated Completed Likelihood (ICL). Our study suggests that for learning visual context using a mixture model, BICr is the most appropriate criterion given sparse data, while CL-AIC should be chosen given moderate or large data sample sizes.	Queen Mary Univ London, Dept Comp Sci, London E1 4NS, England	University of London; Queen Mary University London	Xiang, T (corresponding author), Queen Mary Univ London, Dept Comp Sci, London E1 4NS, England.	txiang@dcs.qmul.ac.uk; sgg@dcs.qmul.ac.uk			Engineering and Physical Sciences Research Council [GR/S63687/01] Funding Source: researchfish	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))		Akaike H., 1973, 2 INT S INFORM THEOR, P267, DOI DOI 10.1007/978-1-4612-1694-0_15; Bernardo J. M., 1994, BAYESIAN THEORY; Biernacki C, 2000, IEEE T PATTERN ANAL, V22, P719, DOI 10.1109/34.865189; Bishop, 1995, NEURAL NETWORKS PATT; Brand M, 2000, IEEE T PATTERN ANAL, V22, P844, DOI 10.1109/34.868685; Brand M, 1997, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.1997.609450; Celeux G, 1996, J CLASSIF, V13, P195, DOI 10.1007/BF01246098; Chapelle O, 2002, MACH LEARN, V48, P9, DOI 10.1023/A:1013943418833; Cherkassky V, 2003, NEURAL COMPUT, V15, P1691, DOI 10.1162/089976603321891864; Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X; DEMPSTER A, 1979, J ROYAL STAT SOC B, V41, P276; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138; FITZGERALD WJ, 1996, NUMERICAL BAYESIAN M; GATH I, 1989, IEEE T PATTERN ANAL, V11, P773, DOI 10.1109/34.192473; Gong SG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P742, DOI 10.1109/ICCV.2003.1238423; Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; HOETING J, 1995, STAT SCI, V14, P382; Hongeng S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P84, DOI 10.1109/ICCV.2001.937608; HURIVICH C, 1976, BIOMETRIKA, V76, P297; HURVICH CM, 1990, BIOMETRIKA, V77, P709, DOI 10.1093/biomet/77.4.709; Johnson N, 1998, PROC CVPR IEEE, P866, DOI 10.1109/CVPR.1998.698706; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; KULLBACK S, 1968, INFORM THEORY STAT; Lange T, 2004, NEURAL COMPUT, V16, P1299, DOI 10.1162/089976604773717621; McKenna SJ, 2000, COMPUT VIS IMAGE UND, V80, P42, DOI 10.1006/cviu.2000.0870; McKenna SJ, 2004, INT C PATT RECOG, P138, DOI 10.1109/ICPR.2004.1333723; MCLACHLAN G, 1997, FINITE MIXTURE MODEL; Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684; Raftery AE, 1995, SOCIOL METHODOL, V25, P185, DOI 10.2307/271066; Rissanen Jorma, 1989, STOCHASTIC COMPLEXIT; Roberts SJ, 1997, PATTERN RECOGN, V30, P261, DOI 10.1016/S0031-3203(96)00079-9; Roberts SJ, 1998, IEEE T PATTERN ANAL, V20, P1133, DOI 10.1109/34.730550; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; SHIBATA R, 1976, BIOMETRIKA, V63, P117; Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677; Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; Triggs B., 1998, P 5 EUROPEAN C COMPU; Wada T, 2000, IEEE T PATTERN ANAL, V22, P873, DOI 10.1109/34.868687; XIANG T, 2002, BRIT MACH VIS C, P233; ZALEWSKI L, 2004, 0043 U LOND VIS LAB	43	7	7	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2006	69	2					181	201		10.1007/s11263-005-5024-8	http://dx.doi.org/10.1007/s11263-005-5024-8			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	065LW					2022-12-18	WOS:000239162400002
J	Fagerstrom, D				Fagerstrom, D			Temporal scale spaces	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	4th International Conference on Scale Space Methods in Computer Vision	JUN 10-12, 2003	ISLE SKYE, SCOTLAND	British Machine Vis Assoc, Kings Coll London, IT Univ Copenhagen		scale-space; temporal measurement; stable density functions; fractional diffusion		In this paper we discuss how to define a scale space suitable for temporal measurements. We argue that such a temporal scale space should possess the properties of: temporal causality, linearity, continuity, positivity, recursitivity as well as translational and scaling covariance. It is shown that these requirements imply a one parameter family of convolution kernels. Furthermore it is shown that these measurements can be realized in a time recursive way, with the current data as input and the temporal scale space as state, i.e. there is no need for storing earlier input. This family of measurement processes contains the diffusion equation on the half line (that represents the temporal scale) with the input signal as boundary condition on the temporal axis. The diffusion equation is unique among the measurement processes in the sense that it is preserves positivity (in the scale domain) and is locally generated. A numerical scheme is developed and relations to other approaches are discussed.	Royal Inst Technol, Dept Numer Anal & Comp Sci, Computat Vis & Act Percept Lab, SE-10044 Stockholm, Sweden	Royal Institute of Technology	Fagerstrom, D (corresponding author), Royal Inst Technol, Dept Numer Anal & Comp Sci, Computat Vis & Act Percept Lab, SE-10044 Stockholm, Sweden.	danielf@nada.kth.se						Aczel J., 1989, FUNCTIONAL EQUATIONS; Feller V, 1984, INTRO PROBABILITY TH; Hille E., 1957, FUNCTIONAL ANAL SEMI; KOENDERINK JJ, 1988, BIOL CYBERN, V58, P163, DOI 10.1007/BF00364136; Koenderink JJ, 1992, ARTIFICIAL BIOL VISI, P1; LFORACK LMJ, 1997, SERIES MATH IMAGING; Lindeberg T., 1996, LNCS, V1064, P229; PAUWELS EJ, 1995, IEEE T PATTERN ANAL, V17, P691, DOI 10.1109/34.391411; Podlubny I., 1999, MATH SCI ENG; RICHTMYER RD, 1967, DIFFERENCE METHODS I; Salden AH, 1998, J MATH IMAGING VIS, V9, P103, DOI 10.1023/A:1008300826001; Samko S. G., 1992, FRACTIONAL INTEGRALS; Saul'yev V.K., 1964, INTEGRATION EQUATION; Treves F., 1967, TOPOLOGICAL VECTOR S; Weickert J, 1997, COMP IMAG VIS, V8, P45	15	7	7	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2005	64	2-3					97	106		10.1007/s11263-005-1837-8	http://dx.doi.org/10.1007/s11263-005-1837-8			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	961YC					2022-12-18	WOS:000231696700002
J	Van Trigt, C				Van Trigt, C			Illuminant-dependence of von Kries type quotients	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						von Kries hypothesis; colour constancy; discounting the illuminant; chromatic adaptation; narrow sensor; reflectance; spectral distribution function	SMOOTHEST REFLECTANCE FUNCTIONS; COLOR CONSTANCY; CHROMATIC ADAPTATION; VISUAL-SYSTEM; APPEARANCE; SENSITIVITY; IMAGE	A von Kries quotient is defined as the cone signal of a reflectance under some illuminant divided by the same cone signal of the illuminant. A von Kries type quotient is a similar ratio, the cone sensitivity being replaced with some linear combination of the CIE color matching functions P(lambda). We study the illuminant-(in) dependent behavior of von Kries type quotients by means of an expansion consisting of one illuminant- independent term and a series of illuminant- dependent ones. It is proved that the series rapidly decreases and that the dominating first term is small if P(lambda) is a narrow function of wavelength and the reflectance and spectral distribution functions are sufficiently broad-band, defined in the text. Von Kries type quotients have a favorable illuminant- independent behavior if and only if the reflectance and spectral distribution functions are smooth functions of wavelength with chromaticity coordinates in a restricted neighborhood of the achromatic point belonging to the equal-energy spectrum, dependent on the narrowness of P(lambda), comprising the object color solid only if P(lambda) were a delta-function.			Van Trigt, C (corresponding author), Saturnus 8, NL-5591 PB Heeze, Netherlands.	c.h.p.vantrigt@hetnet.nl						Abramowitz M, 1968, HDB MATH FUNCTIONS; Bauml KH, 1999, J OPT SOC AM A, V16, P1521, DOI 10.1364/JOSAA.16.001521; BAUML KH, 1995, J OPT SOC AM A, V12, P261, DOI 10.1364/JOSAA.12.000261; Brainard DH, 1997, J OPT SOC AM A, V14, P2091, DOI 10.1364/JOSAA.14.002091; BRAINARD DH, 1992, J OPT SOC AM A, V9, P1433, DOI 10.1364/JOSAA.9.001433; BREENE RG, 1961, SHIFT SHAPE SPECTRAL; BRILL MH, 1986, COLOR RES APPL, V11, P196, DOI 10.1002/col.5080110306; CHICHILNISKY EJ, 1995, VISION RES, V35, P239, DOI 10.1016/0042-6989(94)00122-3; CIE, 1974, CIE PUBL, Vsecond; DEBRUIJN NG, 1961, ASYMPTOTIC METHODS A; FAIRCHILD MD, 1992, VISION RES, V32, P2077, DOI 10.1016/0042-6989(92)90069-U; FINLAYSON GD, 1994, J OPT SOC AM A, V11, P1553, DOI 10.1364/JOSAA.11.001553; FORSYTH DA, 1990, INT J COMPUT VISION, V5, P5, DOI 10.1007/BF00056770; Goldberg R.R., 1970, FOURIER TRANSFORMS; JUDD DB, 1964, J OPT SOC AM, V54, P1031, DOI 10.1364/JOSA.54.001031; Lythgoe J.N., 1979, ECOLOGY VISION; POLYA G, 1964, AUFGABEN LEHRSATZE A, V1, P5; Schrodinger E, 1920, ANN PHYS-BERLIN, V62, P603; SMITH VC, 1975, VISION RES, V15, P161, DOI 10.1016/0042-6989(75)90203-5; SPROSON WN, 1983, COLOR SCI TELEVISION; Szego G, 1967, ORTHOGONAL POLYNOMIA; THORNTON WA, 1986, COLOR RES APPL, V11, P160, DOI 10.1002/col.5080110213; van Trigt C, 1999, COLOR RES APPL, V24, P197, DOI 10.1002/(SICI)1520-6378(199906)24:3<197::AID-COL6>3.0.CO;2-S; vanTrigt C, 1997, J OPT SOC AM A, V14, P741, DOI 10.1364/JOSAA.14.000741; VANTRIGT C, 1990, J OPT SOC AM A, V7, P2208, DOI 10.1364/JOSAA.7.002208; VANTRIGT C, 1990, J OPT SOC AM A, V7, P1891, DOI 10.1364/JOSAA.7.001891; VANTRIGT C, 1994, J OPT SOC AM A, V11, P1003, DOI 10.1364/JOSAA.11.001003; VANTRIGT C, 1999, Patent No. 5905543; VOS JJ, 1971, VISION RES, V11, P799, DOI 10.1016/0042-6989(71)90003-4; WERNER JS, 1982, VISION RES, V22, P929, DOI 10.1016/0042-6989(82)90029-3; WEST G, 1982, J MATH BIOL, V15, P249, DOI 10.1007/BF00275077; Whittaker E., 1962, COURSE MODERN ANAL; WORTHEY JA, 1985, J OPT SOC AM A, V2, P1014, DOI 10.1364/JOSAA.2.001014; Wyszecki G., 2000, COLOR SCI CONCEPTS M, V2nd; YULE JAC, 1967, PRINCIPLES COLOR REP; [No title captured]	36	7	7	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2005	61	1					5	30		10.1023/B:VISI.0000042932.05887.4e	http://dx.doi.org/10.1023/B:VISI.0000042932.05887.4e			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	866XN					2022-12-18	WOS:000224806800001
J	Sim, R; Dudek, G				Sim, R; Dudek, G			Learning generative models of scene features	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	Conference on Computer Vision and Pattern Recognition	DEC 08-14, 2001	KAUAI, HI	IEEE Comp Soc		robot pose estimation; appearance-based modeling; probabilistic localization; generative modeling; feature extraction; feature representation; visual attention	LOCALIZATION; OBJECT	We present a method for learning a set of generative models which are suitable for representing selected image-domain features of a scene as a function of changes in the camera viewpoint. Such models are important for robotic tasks, such as probabilistic position estimation (i.e. localization), as well as visualization. Our approach entails the automatic selection of the features, as well as the synthesis of models of their visual behavior. The model we propose is capable of generating maximum-likelihood views, as well as a measure of the likelihood of a particular view from a particular camera position. Training the models involves regularizing observations of the features from known camera locations. The uncertainty of the model is evaluated using cross validation, which allows for a priori evaluation of features and their attributes. The features themselves are initially selected as salient points by a measure of visual attention, and are tracked across multiple views. While the motivation for this work is for robot localization, the results have implications for image interpolation, image-based scene reconstruction and object recognition. This paper presents a formulation of the problem and illustrative experimental results.	McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2A7, Canada	McGill University	Sim, R (corresponding author), McGill Univ, Ctr Intelligent Machines, 3480 Univ St, Montreal, PQ H3A 2A7, Canada.	simra@cim.mcgill.ca; dudek@cim.mcgill.ca						Belhumeur PN, 1996, PROC CVPR IEEE, P270, DOI 10.1109/CVPR.1996.517085; Choset H, 2001, IEEE T ROBOTIC AUTOM, V17, P125, DOI 10.1109/70.928558; COOTES T, 1994, P BRIT MACH VIS C; DELLAERT F, 1999, IEEE COMP SOC C COMP, P2588; DUDEK G, 2000, INT C ROB AUT, P466; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Fox D, 1998, ROBOT AUTON SYST, V25, P195, DOI 10.1016/S0921-8890(98)00049-9; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Haykin S, 1998, NEURAL NETWORKS COMP, V2nd; Horn B.K.P., 1989, SHAPE SHADING; Kohavi R., 1995, IJCAI, P1137; Leonard J. J., 1991, Proceedings IROS '91. IEEE/RSJ International Workshop on Intelligent Robots and Systems '91. Intelligence for Mechanical Systems (Cat. No.91TH0375-6), P1442, DOI 10.1109/IROS.1991.174711; Lhuillier M., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P139, DOI 10.1109/CVPR.1999.784621; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; NASTAR C, 1996, P 4 EUR C COMP VIS E; NAYAR SK, 1994, IEEE INT CONF ROBOT, P3237, DOI 10.1109/ROBOT.1994.351072; PICARD RR, 1984, J AM STAT ASSOC, V79, P575, DOI 10.2307/2288403; POGGIO T, 1990, NATURE, V343, P263, DOI 10.1038/343263a0; POURRAZ F, 1999, P IEEE COMP SOC C PA; Rekleitis I. M., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P3164, DOI 10.1109/ROBOT.2000.845150; Schmid C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P485, DOI 10.1109/CVPR.1999.784725; Se S, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P226, DOI 10.1109/IRDS.2002.1041393; Sim R, 2001, IMAGE VISION COMPUT, V19, P733, DOI 10.1016/S0262-8856(00)00109-8; SIM R, 2003, P 18 INT JOINT C ART, P6; Stein GP, 2000, IEEE T PATTERN ANAL, V22, P992, DOI 10.1109/34.877522; Thrun S, 1998, MACH LEARN, V31, P29, DOI 10.1023/A:1007436523611; Tikhonov A.N., 1977, SOLUTION ILL POSED P; TSOTSOS JK, 1990, BEHAV BRAIN SCI, V13, P423, DOI 10.1017/S0140525X00079577; Vetter T, 1997, IEEE T PATTERN ANAL, V19, P733, DOI 10.1109/34.598230; Wahba G, 1979, SMOOTHING TECHNIQUES, P233; Yamauchi B, 1998, IEEE INT CONF ROBOT, P3715, DOI 10.1109/ROBOT.1998.681416	31	7	7	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2004	60	1					45	61		10.1023/B:VISI.0000027789.58057.14	http://dx.doi.org/10.1023/B:VISI.0000027789.58057.14			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	820LW					2022-12-18	WOS:000221391600006
J	Barreto, JP; Araujo, H				Barreto, JP; Araujo, H			A general framework for the selection of world coordinate systems in perspective and catadioptric imaging applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						sensor modeling; catadioptric; onmidirectional vision; visual servoing; active tracking		An imaging system with a single effective viewpoint is called a central projecyion system. The conventional perspective camera is an example of central projection system. A catadioptric realization of omnidirectional vision combines reflective surfaces with lenses. Catadioptric systems with an unique projection center are also examples of central projection systems. Whenever an image is acquired, points in 3D space are mapped into points in the 2D image plane. The image formation process represents a transformation from R-2 to R-2, and mathematical models can be used to describe it. This paper discusses the definition of world coordinate systems that simplify the modeling of general central projection imaging. We show that an adequate choice of the world coordinate reference system can be highly advantageous. Such a choice does not imply that new information will be available in the images. Instead the geometric transformations will be represented in a common and more compact framework, while simultaneously enabling newer insights. The first part of the paper focuses on static imaging systems that include both perspective cameras and catadioptric systems. A systematic approach to select the world reference frame is presented. In particular we derive coordinate systems that satisfy two differential constraints (the "compactness" and the "decoupling" constraints). These coordinate systems have several advantages for the representation of the transformations between the 3D world and the image plane. The second part of the paper applies the derived mathematical framework to active tracking of moving targets. In applications of visual control of motion the relationship between motion in the scene and image motion must be established. In the case of active tracking of moving targets these relationships become more complex due to camera motion. Suitable world coordinate reference systems are defined for three distinct situations: perspective camera with planar translation motion, perspective camera with pan and tilt rotation motion, and catadioptric imaging system rotating around an axis going through the effective viewpoint and the camera center. Position and velocity equations relating image motion, camera motion and target 3D motion are derived and discussed. Control laws to perform active tracking of moving targets using visual information are established.	Univ Coimbra, Dept Elect & Comp Engn, Inst Syst & Robot, Coimbra, Portugal	Universidade de Coimbra	Barreto, JP (corresponding author), Univ Coimbra, Dept Elect & Comp Engn, Inst Syst & Robot, Coimbra, Portugal.		Araujo, Helder/B-3554-2008; Barreto, Joao P/I-2845-2012	Araujo, Helder/0000-0002-9544-424X; Barreto, Joao P/0000-0001-5220-9170				Baker S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P35, DOI 10.1109/ICCV.1998.710698; BARRETO JP, 2000, SIRS2000 8 INT S INT, P233; BARRETO JP, 1999, ROBUST VISION VISION; BARRETO JP, 2001, P IEEE INT C COMP VI; BARRETT JW, 1999, ADV THEOR MATH PHYS, V3, P1; BATISTA J, 1997, ICRA 97 IEEE INT C R; BATISTA J, 1995, P M2VIP 95 2 INT C M; BOGNER S, 1995, P IEEE SMC C, P3100; CHROUST S, 2000, AMC00 IEEE RSJ INT W, P19; Corke P., 1996, VISUAL CONTROL ROBOT; COURANT R, 1998, DIFFERENTIAL INTEGRA; Geyer Christopher, 2000, LNCS, P445, DOI DOI 10.1007/3-540-45053-X_29; Gluckman J, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P999, DOI 10.1109/ICCV.1998.710838; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hecht E., 1974, OPTICS; Krautgartner P, 1998, IEEE INT CONF ROBOT, P2315, DOI 10.1109/ROBOT.1998.680668; LAVEAU S, 1996, P EUR C COMP VIS; Malis E, 1999, IEEE T ROBOTIC AUTOM, V15, P238, DOI 10.1109/70.760345; Murray R., 1994, MATH INTRO ROBOTICS; NALWA V, 1996, TRUE OMNIDIRECTIONAL; SHARKEY PM, 1993, MECHATRONICS, V3, P517, DOI 10.1016/0957-4158(93)90021-S; Stolfi J., 1991, ORIENTED PROJECTIVE; SVOBODA T, 1998, P 5 EUR C COMP VIS, P218; YAGI Y, 1990, P INT C ROB SYST; YAMAZAWA K, 1993, P INT C ROB SYST; YAMAZAWA K, 1994, P IEEE INT C ROB AUT, P1062	26	7	7	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2004	57	1					23	47		10.1023/B:VISI.0000013089.51664.b2	http://dx.doi.org/10.1023/B:VISI.0000013089.51664.b2			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	766DP		Green Submitted			2022-12-18	WOS:000188330400002
J	Laptev, I; Lindeberg, T				Laptev, I; Lindeberg, T			A distance measure and a feature likelihood map concept for scale-invariant model matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	3rd International Conference on Scale-Space and Morphology held in conjunction with the 8th International Conference on Computer Vision	JUL 07-08, 2001	VANCOUVER, CANADA	IEEE Tech Comm Pattern Anal & Machine Intelligence		matching; scale-space; image features; tracking; recognition; multi-scale representations	OBJECT RECOGNITION; SHAPE CUES; REPRESENTATION; SEGMENTATION; TRACKING	This paper presents two approaches for evaluating multi-scale feature-based object models. Within the first approach, a scale-invariant distance measure is proposed for comparing two image representations in terms of multi-scale features. Based on this measure, the maximisation of the likelihood of parameterised feature models allows for simultaneous model selection and parameter estimation. The idea of the second approach is to avoid an explicit feature extraction step and to evaluate models using a function defined directly from the image data. For this purpose, we propose the concept of a feature likelihood map, which is a function normalised to the interval [0, 1], and that approximates the likelihood of image features at all points in scale-space. To illustrate the applicability of both methods, we consider the area of hand gesture analysis and show how the proposed evaluation schemes can be integrated within a particle filtering approach for performing simultaneous tracking and recognition of hand models under variations in the position, orientation, size and posture of the hand. The experiments demonstrate the feasibility of the approach, and that real time performance can be obtained by pyramid implementations of the proposed concepts.	Royal Inst Technol, Computat Vis & Act Percept Lab, Dept Numer Anal & Comp Sci, SE-10044 Stockholm, Sweden	Royal Institute of Technology	Laptev, I (corresponding author), Royal Inst Technol, Computat Vis & Act Percept Lab, Dept Numer Anal & Comp Sci, SE-10044 Stockholm, Sweden.	laptev@nada.kth.se; tony@nada.kth.se	Lindeberg, Tony/G-3580-2011	Lindeberg, Tony/0000-0002-9081-2170				Almansa A, 2000, IEEE T IMAGE PROCESS, V9, P2027, DOI 10.1109/83.887971; Bevensee R.M., 1993, MAXIMUM ENTROPY SOLU; BIGUN J, 1991, IEEE T PATTERN ANAL, V13, P775, DOI 10.1109/34.85668; BILLMEYER F, 1982, PRINCIPLES COLOUR TE; Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436; Bretzner L, 1999, LECT NOTES COMPUT SC, V1682, P117; BRETZNER L, 1999, J VISUAL COMMUNICATI, V11, P115; Bretzner L., 2002, P FAC GEST WASH DC U, P63; BURBECK CA, 1995, VISION RES, V35, P1917, DOI 10.1016/0042-6989(94)00286-U; Chomat O, 2000, LECT NOTES COMPUT SC, V1842, P117; Cipolla R., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P374, DOI 10.1109/ICCV.1993.378190; Cipolla R., 1998, COMPUTER VISION HUMA; CROWLEY JL, 1987, IEEE T PATTERN ANAL, V9, P113, DOI 10.1109/TPAMI.1987.4767876; Cui Y., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P617, DOI 10.1109/ICPR.1996.547020; DEUTSCHER J, 2000, P COMP VIS PATT REC; Florack LMJ, 1997, IMAGE STRUCTURE; Forsyth DA, 1999, INT J COMPUT VISION, V32, P63, DOI 10.1023/A:1008145029462; FREEMAN WT, 1995, P INT C FAC GEST REC; Garding J, 1996, INT J COMPUT VISION, V17, P163, DOI 10.1007/BF00058750; GAUCH JM, 1993, IEEE T PATTERN ANAL, V15, P635, DOI 10.1109/34.216734; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GRIFFIN LD, 1992, IMAGE VISION COMPUT, V10, P389, DOI 10.1016/0262-8856(92)90025-X; Hall D, 2000, LECT NOTES COMPUT SC, V1842, P164; Heap T, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P344, DOI 10.1109/ICCV.1998.710741; ISARD M, 1998, LECT NOTES COMPUTER, V1406, P893, DOI DOI 10.1007/BFB0055711; ISARD M, 1996, LECT NOTES COMPUTER, V1064; KOENDERINK JJ, 1992, IEEE T PATTERN ANAL, V14, P597, DOI 10.1109/34.141551; Laptev I, 2001, LECT NOTES COMPUT SC, V2106, P98; Laptev I, 2001, LECT NOTES COMPUT SC, V2106, P63; LIFSHITZ LM, 1990, IEEE T PATTERN ANAL, V12, P529, DOI 10.1109/34.56189; LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346; Lindeberg T, 1997, IMAGE VISION COMPUT, V15, P415, DOI 10.1016/S0262-8856(97)01144-X; Lindeberg T., 1997, Algebraic Frames for the Perception-Action Cycle. International Workshop, AFPAC'97. Proceedings, P94, DOI 10.1007/BFb0017862; Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Lindeberg T., 1994, SCALE SPACE THEORY C; LINDEBERG T, 2002, IMAGE VISION COMPUTI; LINDEBERG T, 2002, UNPUB SCALE SELECTIO; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; MACCORMICK J, 2000, LECT NOTES COMPUTER, V1843; MIKOLAJCZYK K, 2002, LECT NOTES COMPUTER, V2350; MIKOLAJCZYK K, 2001, P 8 INT C COMP VIS V; Olsen OF, 1997, COMP IMAG VIS, V8, P191; Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226; Pizer S. M., 1994, Journal of Mathematical Imaging and Vision, V4, P303, DOI 10.1007/BF01254105; RAO AR, 1991, CVGIP-GRAPH MODEL IM, V53, P157, DOI 10.1016/1049-9652(91)90059-S; REHG JM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P612, DOI 10.1109/ICCV.1995.466882; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Schiele B, 2000, INT J COMPUT VISION, V36, P31, DOI 10.1023/A:1008120406972; Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215; Shokoufandeh A, 1999, IMAGE VISION COMPUT, V17, P445, DOI 10.1016/S0262-8856(98)00124-3; Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703; SIDENBLADH H, 2001, P 8 INT C COMP VIS V; Sullivan J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1068, DOI 10.1109/ICCV.1999.790391; Triesch J, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P170, DOI 10.1109/AFGR.1996.557260; Vincken KL, 1997, IEEE T PATTERN ANAL, V19, P109, DOI 10.1109/34.574787; VONHARDENBERG C, 2001, ACM WORKSH PERC US I; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729	59	7	7	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY-JUN	2003	52	2-3					97	120		10.1023/A:1022947906601	http://dx.doi.org/10.1023/A:1022947906601			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	659EL		Green Submitted			2022-12-18	WOS:000181764500003
J	Leclerc, YG; Luong, QT; Fua, P				Leclerc, YG; Luong, QT; Fua, P			Self-consistency and MDL: A paradigm for evaluating point-correspondence algorithms, and its application to detecting changes in surface elevation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						consistency; stereo; minimal description length; correspondences	STEREO	The self-consistency methodology is a new paradigm for evaluating certain vision algorithms without relying extensively on ground truth. We demonstrate its effectiveness in the case of point-correspondence algorithms and use our approach to predict their accuracy. For point-correspondence algorithms, our methodology consists in applying independently the algorithm to subsets of images obtained by varying the camera geometry while keeping 3-D object geometry constant. Matches that should correspond to the same surface element in 3-D are collected to create statistics that are then used as a measure of the accuracy and reliability of the algorithm. These statistics can then be used to predict the accuracy and reliability of the algorithm applied to new images of new scenes. An effective representation for these statistics is a scatter diagram along two dimensions: A normalized distance and a matching score. The normalized distance make the statistics invariant to camera geometry, while the matching score allows us to predict the accuracy of individual matches. We introduce a new matching score based on Minimum Description Length (MDL) theory, which is shown to be a better predictor of the quality of a match than the traditional Sum of Squared Distance (SSD) score. We demonstrate the potential of our methodology in two different application areas. First, we compare different point-correspondence algorithms, matching scores, and window sizes. Second, we detect changes in terrain elevation between 3-D terrain models reconstructed from two sets of images taken at a different time. We finish by discussing the application of self-consistency to other vision problems.	SRI Int, Ctr Artificial Intelligence, Menlo Pk, CA 94025 USA; Ecole Polytech Fed Lausanne, Lausanne, Switzerland	SRI International; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Leclerc, YG (corresponding author), SRI Int, Ctr Artificial Intelligence, Menlo Pk, CA 94025 USA.	leclerc@ai.sri.com; luong@ai.sri.com; pascal.fua@epfl.ch	Fua, Pascal/H-3928-2011	Fua, Pascal/0000-0002-6702-9970				ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; Ayache N., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P422; Ayache N, 1991, ARTIFICIAL VISION MO; BEJANIN M, 1994, WACV94, P160; BOULT T, 1998, DARPA98, P305; Cho K, 1997, IEEE T PATTERN ANAL, V19, P1185, DOI 10.1109/34.632979; CSURKA G, 1996, CVGIP IU; FAUGERAS O, 1992, INT WORKSH ROB COMP, P1; Forstner W., 1982, INT ARCH PHOTOGRAMME, P176; FORSTNER W, 1994, PERFORMANCE VERSUS M; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; Fua P., 1991, P 12 INT JOINT C ART, P1292; Horn B.K.P., 1989, SHAPE SHADING; Huertas A, 2000, IMAGE VISION COMPUT, V18, P583, DOI 10.1016/S0262-8856(99)00063-3; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; LECLERC Y, 1999, P ON DAY WORKSH PERF; LECLERC YG, 1989, INT J COMPUT VISION, V3, P73, DOI 10.1007/BF00054839; LECLERC YG, 1992, INT J COMPUT VISION, V9, P113, DOI 10.1007/BF00129683; LILLESTRAND RL, 1972, IEEE T COMPUT, VC 21, P654, DOI 10.1109/T-C.1972.223570; MATTHIES L, 1992, INT J COMPUT VISION, V8, P71, DOI 10.1007/BF00126401; MOHR R, 1993, CVPR, P543; QUAM L, 1971, THESIS STANFORD U; RISSANEN J, 1989, ENCY STAT SCI, V3, P73; Rosin PL, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P274, DOI 10.1109/ICCV.1998.710730; Sarkar S, 1998, COMPUT VIS IMAGE UND, V71, P110, DOI 10.1006/cviu.1997.0637; Sugihara K., 1986, MACHINE INTERPRETATI; Szeliski R., 1994, Journal of Visual Communication and Image Representation, V5, P10, DOI 10.1006/jvci.1994.1002; SZELISKI R, 1999, ICCV99 CORF GREEC; SZELISKI R, 1999, P VIS ALG THEOR PRAC; Torr PHS, 1997, MACH VISION APPL, V9, P321, DOI 10.1007/s001380050051; YACHIDA M, 1986, ICPR, P1041; YI SK, 1994, MACH VISION APPL, V7, P93, DOI 10.1007/BF01215805	32	7	8	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2003	51	1					63	83		10.1023/A:1020940807324	http://dx.doi.org/10.1023/A:1020940807324			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	610FU		Green Submitted			2022-12-18	WOS:000178950700003
J	Shah, M				Shah, M			Guest introduction: The changing shape of computer vision in the twenty-first century	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material							EDGE		Univ Cent Florida, Sch Elect Engn & Comp Sci, Comp Vis Lab, Orlando, FL 32816 USA	State University System of Florida; University of Central Florida	Shah, M (corresponding author), Univ Cent Florida, Sch Elect Engn & Comp Sci, Comp Vis Lab, Orlando, FL 32816 USA.	shah@cs.ucf.edu		Shah, Mubarak/0000-0001-6172-5572				BERGEN JR, 1992, LECT NOTES COMPUT SC, V588, P237; Black M.J., 1997, MOTION BASED RECOGNI, P245; CANNATA R, 2000, APPL IM PATT REC WOR; Canny J., 1986, IEEE T PATTERN ANAL, V8, P769, DOI DOI 10.1109/TPAMI.1986.4767851; CEDRAS C, 1995, IMAGE VISION COMPUT, V13, P129, DOI 10.1016/0262-8856(95)93154-K; Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153; ESSA I, 1997, MOTION BASED RECOGNI, P271; FAUGERAS OD, 1994, P EUR C COMP VIS STO, P485; FLICKNER M, 1997, INTELLIGENT MULTIMED, P7; GRIMSON WEL, 1996, T MED IMAGING; Hampapur A, 1997, P SOC PHOTO-OPT INS, V3022, P188, DOI 10.1117/12.263407; HORN BKP, 1973, 285 MIT AI; JAVED O, 2002, EUR C COMP VIS COP D; KHAN S, 2001, IEEE COMP VIS PATT R; KOLLER D, 1991, CVPR 91, P90; Mann S, 1997, IEEE T IMAGE PROCESS, V6, P1281, DOI 10.1109/83.623191; Manning R. A., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P388, DOI 10.1109/CVPR.1999.786968; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Marr D., 1982, VISION; OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639; RANGARAJAN K, 1991, CVGIP-IMAG UNDERSTAN, V54, P56, DOI 10.1016/1049-9660(91)90075-Z; ROSENFELD A, 1972, IEEE T COMPUT, VC 21, P677, DOI 10.1109/T-C.1972.223573; SEITZ SM, 1996, P SIGGRAPH 96, P21; TSOTSOS JK, 1980, IEEE T PATTERN ANAL, V2, P563, DOI 10.1109/TPAMI.1980.6447704; WEXLER Y, 2000, P CVPR; WITKIN AP, 1986, PIXELS PREDICATES, P149	27	7	8	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2002	50	2					103	110		10.1023/A:1020323930790	http://dx.doi.org/10.1023/A:1020323930790			8	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	597DX					2022-12-18	WOS:000178207800001
J	Seitz, SM; Kalai, A; Shum, HY				Seitz, SM; Kalai, A; Shum, HY			Omnivergent stereo	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						stereo; panorama; mosaic; vergence; multiperspective imaging	MOSAICS	The notion of a virtual camera for optimal 3D reconstruction is introduced. Instead of planar perspective images that collect many rays at a fixed viewpoint, omnivergent cameras collect a small number of rays at many different viewpoints. The resulting 2D manifold of rays is arranged into two multiple-perspective images for stereo reconstruction. We call such images omnivergent images, and the process of reconstructing the scene from such images omnivergent stereo. This procedure is shown to produce 3D scene models with minimal reconstruction error, due to the fact that for any point in the 3D scene, two rays with maximum vergence angle can be found in the omnivergent images. Furthermore, omnivergent images are shown to have horizontal epipolar lines, enabling the application of traditional stereo matching algorithms, without modification. Three types of omnivergent virtual cameras are presented: spherical omnivergent cameras, center-strip cameras and dual-strip cameras.	Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA; MIT, Dept Math, Cambridge, MA 02139 USA; Microsoft Res, Vis Technol Grp, Beijing, Peoples R China	University of Washington; University of Washington Seattle; Massachusetts Institute of Technology (MIT); Microsoft	Seitz, SM (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.	seitz@cs.washington.edu; akalai@mit.edu; hshum@microsoft.com						Baker S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P35, DOI 10.1109/ICCV.1998.710698; BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525; Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; Huang HC, 1998, GRAPH MODEL IM PROC, V60, P196, DOI 10.1006/gmip.1998.0467; ISHIGURO H, 1992, IEEE T PATTERN ANAL, V14, P257, DOI 10.1109/34.121792; Kang SB, 1996, PROC CVPR IEEE, P364, DOI 10.1109/CVPR.1996.517098; KATAYAMA A, 1995, P SPIE A, V2409, P21; Kumar R., 1995, Proceedings IEEE Workshop on Representation of Visual Scenes (In Conjunction with ICCV'95) (Cat. No.95TB8126), P10, DOI 10.1109/WVRS.1995.476847; LEVOY M, 1996, P SIGGRAPH 96; MANN S, 1994, P 1 INT C IM PROC; McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398; Nayar SK, 2000, PROC CVPR IEEE, P388; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; PAJDLA T, 2001, P COMP VIS WINT WORK, P223; Peleg S, 1997, PROC CVPR IEEE, P338, DOI 10.1109/CVPR.1997.609346; Peleg S, 2000, PROC CVPR IEEE, P208, DOI 10.1109/CVPR.2000.855821; Peleg S., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P395, DOI 10.1109/CVPR.1999.786969; Seitz SM, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P26, DOI 10.1109/ICCV.2001.937495; SHUM HY, 1999, P 7 INT C COMP VIS, P22; Szeliski R, 1996, IEEE COMPUT GRAPH, V16, P22, DOI 10.1109/38.486677; Wood D. N., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P243, DOI 10.1145/258734.258859	22	7	7	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	2002	48	3					159	172		10.1023/A:1016342731674	http://dx.doi.org/10.1023/A:1016342731674			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	573BJ					2022-12-18	WOS:000176809200001
J	Etienne-Cummings, R				Etienne-Cummings, R			Biologically inspired visual motion detection in VLSI	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						visual motion detection; motion chips; motion sensors; hardware optical-flow measurement; biologically inspired vision chips; neuromorphic vision	ANALOG VLSI; SILICON RETINA; MODEL; PERCEPTION; IMPLEMENTATION; VELOCITY; SYSTEMS; SENSOR; CHIPS; CELLS	Visual motion detection is a fundament component of vision, and plays a vital role in scene analysis and understanding for behaving organisms. In computer vision, motion detection requires considerable resources to obtain real-time results. Very Large Scale Integration (VLSI) technology offers a convenient substrate upon which both photosensitive elements and motion extracting circuits can be implemented, thus allowing real-time motion detection. This paper presents two approaches for implementing real-time visual motion detection in VLSI. The two approaches mimic the two primary methods found in biological organisms. Insect motion detection employs local correlation and is implemented very close to the photoreceptors. In contrast, primate motion detection is performed in cortex, using spatiotemporally oriented neural filters. The analysis, construction and results of the hardware models of insect and primate visual motion detection are presented.	Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA	Johns Hopkins University	Etienne-Cummings, R (corresponding author), Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.		Etienne-Cummings, Ralph/A-3227-2010					ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; ANDREOU A, 1990, P 1990 IEEE S SYST M, P707; ANDREOU AG, 1991, IEEE T NEURAL NETWOR, V2, P205, DOI 10.1109/72.80331; BARLOW H, 1982, SENSES PHYSL RETINA; BENSON RG, 1992, ADV NEUR IN, V4, P756; Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110; BURNS J, 2001, DIG TECH PAP ISSCC 2, V44, P268; DELBRUCK T, 1993, IEEE T NEURAL NETWOR, V4, P529, DOI 10.1109/72.217194; Etienne-Cummings R, 1999, IEEE T CIRCUITS-II, V46, P1121, DOI 10.1109/82.793703; Etienne-Cummings R., 1993, Proceedings. 1993 Computer Architectures for Machine Perception. (Cat. No. 93TH0608-0), P241, DOI 10.1109/CAMP.1993.622478; Etienne-Cummings R, 1999, ADV NEUR IN, V11, P685; EtienneCummings R, 1997, ISSCC DIG TECH PAP I, V40, P38; EtienneCummings R, 1997, IEEE T CIRCUITS-I, V44, P55, DOI 10.1109/81.558442; ETIENNECUMMINGS R, 1994, THESIS U PENNSYLVANI; ETIENNECUMMINGS R, 1992, P INT JOINT C NEUR N, V4, P426; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; GOTTARDI M, 1995, SENSOR ACTUAT A-PHYS, V46, P251, DOI 10.1016/0924-4247(94)00900-3; GRZYWACZ NM, 1990, PROC R SOC SER B-BIO, V239, P129, DOI 10.1098/rspb.1990.0012; Harrison RR, 1998, ADV NEUR IN, V10, P880; HEEGER D, 1996, P NAT ACAD SCI, V92, P623; HEEGER DJ, 1987, J OPT SOC AM A, V4, P1455, DOI 10.1364/JOSAA.4.001455; Higgins CM, 1999, IEEE T CIRCUITS-II, V46, P677, DOI 10.1109/82.769776; HORIUCHI T, 1992, INT J COMPUT VISION, V8, P203, DOI 10.1007/BF00055152; Horn B., 1986, ROBOT VISION, P1; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; KOCH C, 1991, SPIE, V1473, P2; KRAMER J, 1995, P IEEE INT S CIRC SY, P413; LANDY MS, 1991, VISION RES, V31, P859, DOI 10.1016/0042-6989(91)90153-V; Liu SC, 2000, IEEE T CIRCUITS-II, V47, P1458, DOI 10.1109/82.899640; Liu SC, 1998, ADV NEUR IN, V10, P712; Marr D., 1982, VISION; MARTIN GR, 1985, FORM FUNCTION BIRDS, V3; MAUNSELL JHR, 1983, J NEUROPHYSIOL, V49, P1127, DOI 10.1152/jn.1983.49.5.1127; MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356; MILLER K, 1999, IEEE INT S CIRCUITS, V5, P116; Mizuno M, 1997, ISSCC DIG TECH PAP I, V40, P256, DOI 10.1109/ISSCC.1997.585377; MUELLER P, 1995, P COMP INFO SCI S; NAKAYAMA K, 1985, VISION RES, V25, P625, DOI 10.1016/0042-6989(85)90171-3; REICHARDT W, 1979, BIOL CYBERN, V35, P81, DOI 10.1007/BF00337434; Reichardt W, 1961, SENSORY COMMUNICATIO; Sarpeshkar R, 1996, P IEEE, V84, P969, DOI 10.1109/5.503298; SIMONCELLI E, 1991, INVESTIGATIVE OPTHAL, V32, P983; SIMONCELLI EP, 1993, THESIS MIT MA; SPILLMANN L, 1990, VISUAL PERCEPTION; TANNER JE, 1986, VLSI SIGNAL PROCESSI, V2, P59; VANDERSPIEGEL J, 1992, IEEE J SOLID-ST CIRC, V27, P82, DOI 10.1109/4.109559; VANDERSPIEGEL J, 1995, FUZZY LOGIC NEURAL N, pCH31; VANSANTEN JPH, 1984, J OPT SOC AM A, V1, P451, DOI 10.1364/JOSAA.1.000451; Yakovleff AJS, 1998, ANALOG INTEGR CIRC S, V15, P183, DOI 10.1023/A:1008203907863; Yamada K, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P650, DOI 10.1109/IVS.2000.898422	50	7	7	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP-OCT	2001	44	3					175	198		10.1023/A:1012272131141	http://dx.doi.org/10.1023/A:1012272131141			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	496NM					2022-12-18	WOS:000172402900002
J	Dubuc, B; Zucker, SW				Dubuc, B; Zucker, SW			Complexity, confusion, and perceptual grouping. Part I: The curve-like representation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						perceptual organization; segmentation; tangent maps; image curves texture; texture flow; complexity	COMPLETION; VISION; FIELDS	Intermediate-level vision is central to form perception, and we outline an approach to intermediate-level segmentation based on complexity analysis. We focus on the problem of edge detection, and how edge elements might be grouped together. This is typical because, once the local structure is established, the transition to global structure must be effected and context is critical. To illustrate, consider an edge element inferred from an unknown image. Is this local edge part of a long curve, or part of a texture ? If the former, which is the next element along the curve ? If the latter, is the texture like a hair pattern, in which nearby elements are oriented similarly, or like a spaghetti pattern, in which they are not ? Are there other natural possibilities ? Such questions raise issues of dimensionality, since curves are 1-D and textures are 2-D, and also of complexity. Working toward a measure of representational complexity for vision, in this first of a pair of papers we develop a foundation based on geometric measure theory. The main result concerns the distribution of tangents in space and in orientation, which serves as a formal basis for the concrete measure of representational complexity developed in the companion paper.	Espace Courbe, Montreal, PQ H4C 3C5, Canada; Yale Univ, Ctr Computat Vis & Control, Dept Comp Sci, New Haven, CT 06520 USA; Yale Univ, Ctr Computat Vis & Control, Dept Elect Engn, New Haven, CT 06520 USA	Yale University; Yale University	Dubuc, B (corresponding author), Espace Courbe, 642 de Courcelle,Suite 303, Montreal, PQ H4C 3C5, Canada.	benoit@espacecourbe.com; zucker-steven@cs.yale.edu		Dubuc, Benoit/0000-0002-9481-3154				Besicovitch AS, 1928, MATH ANN, V98, P422, DOI 10.1007/BF01451603; BINFORD TO, 1981, ARTIF INTELL, V17, P205, DOI 10.1016/0004-3702(81)90025-4; Boyer K.L., 2000, PERCEPTUAL ORG ARTIF; Burkill J. C., 1970, 2 COURSE MATH ANAL; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; CESARI L, 1965, SURFACE AREA; COX IJ, 1993, INT J COMPUT VISION, V11, P5, DOI 10.1007/BF01420590; DAVID C, 1990, INT J COMPUT VISION, V5, P219, DOI 10.1007/BF00126500; Dubuc B, 2001, INT J COMPUT VISION, V42, P55, DOI 10.1023/A:1011189501276; Edgar G. A., 1990, UNDERGRADUATE TEXTS; Falconer K.J., 2014, FRACTAL GEOMETRY MAT, V3rd ed.; FALCONER KJ, 1987, CAMBRIDGE TRACTS MAT, V85; FEDERER H, 1985, GEOMETRIC MEASURE TH; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Gemelli A, 1931, Z PSYCHOL PHYSIOL SI, V123, P308; Guy G, 1996, INT J COMPUT VISION, V20, P113, DOI 10.1007/BF00144119; GUZMAN A, 1968, FAL AFIPS P JOINT CO, V33, P291; HEITGER F, 1993, P 4 INT C COMP VIS, P32; Hilbert D., 1990, GEOMETRY IMAGINATION; HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455; HUGGINS P, 2000, FOLDS CUT SCENE; IVERSON LA, 1995, IEEE T PATTERN ANAL, V17, P982, DOI 10.1109/34.464562; IVERSON LA, 1993, THESIS MCGILL U MONT; Kanizsa G., 1979, ORG VISION; KASS M, 1987, COMPUT VISION GRAPH, V37, P362, DOI 10.1016/0734-189X(87)90043-0; Koenderink J., 1990, SOLID SHAPE; LEUNG T, 1998, P 5 EUR C COMP VIS, P544; MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923; MALIK J, 2000, PERCEPTUAL ORG ARTIF; Mandelbrot, 1982, FRACTAL GEOMETRY NAT, P394; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; MENDESFRANCE M, 1991, FRACTAL GEOMETRY ANA, P325; MOREL JM, 1995, PROGR NONLINEAR DIFF, V14; Morgan F., 1987, GEOMETRIC MEASURE TH; MUMFORD D, 1992, ALGEBRAIC GEOMETRY A; Munkres J.R., 1975, TOPOLOGY 1 COURSE; NITZBERG M, 1993, LECT NOTEDS COMPUTER, V662; Peitgen H. O., 2004, NEW FRONTIERS SCI, DOI [10.1007/b97624, DOI 10.1007/B97624]; POINCARE H, 1926, DERNIERES PENSEES; ROGERS CA, 1970, HOUSDORFF MEASURES; SAUND E, 1993, CVGIP-IMAG UNDERSTAN, V58, P327, DOI 10.1006/ciun.1993.1045; Simmons G.F., 1963, INTRO TOPOLOGY MODER; Simon H. A, 1968, SCI ARTIFICIAL; SMITH KT, 1971, PRIMER MODERN ANAL; Tricot C., 1995, CURVES FRACTAL DIMEN, DOI [10.1007/978-1-4612-4170-6, DOI 10.1007/978-1-4612-4170-6]; Tricot C, 1991, FRACTAL GEOMETRY ANA, P367; ULLMAN S, 1990, COLD SH Q B, V55, P889; Waltz D., 1975, PSYCHOL COMPUTER VIS, P19; Williams LR, 1996, COMPUT VIS IMAGE UND, V64, P1, DOI 10.1006/cviu.1996.0043; Williams LR, 1997, NEURAL COMPUT, V9, P837, DOI 10.1162/neco.1997.9.4.837; YUILLE A, 1998, ADV NEURAL INFORMATI, P641; Zucker SW, 1989, NEURAL COMPUT, V1, P68, DOI 10.1162/neco.1989.1.1.68; ZUCKER SW, 1977, IEEE T COMPUT, V26, P394, DOI 10.1109/TC.1977.1674848; ZUCKER SW, 1993, CURR SCI INDIA, V64, P407; ZUCKER SW, 1985, COMPUT VISION GRAPH, V32, P74, DOI 10.1016/0734-189X(85)90003-9	56	7	7	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		2001	42	1-2					55	82		10.1023/A:1011189501276	http://dx.doi.org/10.1023/A:1011189501276			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	437VX					2022-12-18	WOS:000169015200005
J	Cheong, LF; Ng, KO				Cheong, LF; Ng, KO			Geometry of distorted visual space and Cremona transformation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						motion analysis; structure from motion; stereopsis; error analysis; shape representation; cremona transformation	RECOVERING 3-D MOTION; NOISY FLOW FIELD; INHERENT AMBIGUITIES; CURVED SURFACES; PERCEPTION; STEREOPSIS; ALGORITHM; SHAPE	An important issue concerning the design of any vision system is the choice of a proper space representation. In order to search for clues to a suitable representation, we look at the distortion of space arising from errors in motion or stereo estimates. Understanding this space distortion has important epistemological implications for the problem of space representation because it tells us what can be and what cannot be computed. This paper is therefore an enquiry into the nature of space representation through the study of the space distortion, though it is not a psychophysical or physiological study but rather a computational one. We show that the distortion transformation is a quadratic Cremona transformation, which is bijective almost everywhere except on the set of fundamental elements. We identify the fundamental elements of both the direct and the inverse transformations, and study the behaviour of the space distortion by analyzing the transformation of space elements (lines, planes) that pass through these fundamental elements.	Natl Univ Singapore, Dept Elect Engn, Singapore 119260, Singapore; Natl Univ Singapore, Dept Math, Singapore 119260, Singapore	National University of Singapore; National University of Singapore	Cheong, LF (corresponding author), Natl Univ Singapore, Dept Elect Engn, 10 Kent Ridge Crescent, Singapore 119260, Singapore.							ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; ALOIMONOS J, 1988, INT J COMPUT VISION, V2, P171, DOI 10.1007/BF00133699; ALOIMONOS Y, 1994, INT J COMPUT VISION, V13, P33, DOI 10.1007/BF01420794; BARATOFF G, 1997, CARTR861 U MAR CTR A; BARRON JL, 1987, P 6 NAT C ART INT SE, P700; BLACK M, 1994, P EUR C COMP VIS STO, P138; BOBER M, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P947, DOI 10.1109/CVPR.1994.323931; Cheong L, 1998, COMPUT VIS IMAGE UND, V71, P356, DOI 10.1006/cviu.1997.0649; Cox D., 1992, IDEALS VARIETIES ALG, V3, DOI [10.1007/978-1-4757-2181-2, DOI 10.1007/978-1-4757-2181-2]; DANIILIDIS K, 1993, P IEEE C COMP VIS PA, P188; Daniilidis Kostas, 1997, VISUAL NAVIGATION BI; Dutta R., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P106, DOI 10.1109/ICCV.1990.139504; FERMULLER C, 1995, INT J COMPUT VISION, V14, P147, DOI 10.1007/BF01418980; Fermuller C, 1997, BIOL CYBERN, V77, P323, DOI 10.1007/s004220050393; Fleet DJ, 1992, MEASUREMENT IMAGE VE; FOLEY JM, 1972, PERCEPT PSYCHOPHYS, V11, P423, DOI 10.3758/BF03206284; FOLEY JM, 1980, PSYCHOL REV, V87, P411, DOI 10.1037/0033-295X.87.5.411; GARDING J, 1995, VISION RES, V35, P703, DOI 10.1016/0042-6989(94)00162-F; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; HEEL J, 1990, 1190 MIT AI; HORN BKP, 1987, INT J COMPUT VISION, V1, P259, DOI 10.1007/BF00127824; Hudson H. P., 1996, CREMONA TRANSFORMATI; JOHNSTON EB, 1991, VISION RES, V31, P1351, DOI 10.1016/0042-6989(91)90056-B; Krames J., 1940, MONATSHEFTE MATH PHY, V49, P327, DOI DOI 10.1007/BF01707311; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Marr D., 1982, VISION; MAYBANK S, 1993, THEORY RECONSTRUCTIO; NEGAHDARIPOUR S, 1989, INT J COMPUT VISION, V3, P293, DOI 10.1007/BF00132601; Negahdaripour S., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P607; OGLE KN, 1964, RES BIONCULAR VISION; SCHUNCK BG, 1989, IEEE T PATTERN ANAL, V11, P1010, DOI 10.1109/34.42834; Semple J., 1949, INTRO ALGEBRAIC GEOM; Shafarevich I. R., 1994, BASIC ALGEBRAIC GEOM, V1; SINCLAIR D, 1994, INT J COMPUT VISION, V13, P57, DOI 10.1007/BF01420795; SPETSAKIS ME, 1988, P 2 INT C COMP VIS, P449; THOMAS JI, 1993, P DARPA IM UND WORKS, P691; TITTLE JS, 1995, J EXP PSYCHOL HUMAN, V21, P663, DOI 10.1037/0096-1523.21.3.663; TODD JT, 1989, PSYCHOL REV, V96, P643, DOI 10.1037/0033-295X.96.4.643; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; VERRI A, 1989, IEEE T PATTERN ANAL, V11, P490, DOI 10.1109/34.24781; VIEVILLE T, 1989, TRADITIONAL NONTRADI; Weng J., 1991, MOTION STRUCTURE IMA; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903	44	7	7	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1999	32	3					195	212		10.1023/A:1008105012585	http://dx.doi.org/10.1023/A:1008105012585			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	229HK					2022-12-18	WOS:000082187500002
J	Fischl, B; Cohen, MA; Schwartz, EL				Fischl, B; Cohen, MA; Schwartz, EL			Rapid anisotropic diffusion using space-variant vision	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						anisotropic diffusion; complex log map; space-variant vision; image enhancement; polar exponential grid	EXPONENTIAL CHIRP TRANSFORM; EDGE-DETECTION; NONLINEAR DIFFUSION; IMAGE-ENHANCEMENT; NEURAL DYNAMICS; BRIGHTNESS PERCEPTION; SHOCK FILTERS; SEGMENTATION; RESOLUTION; FRAMEWORK	Many computer and robot vision applications require multi-scale image analysis. Classically, this has been accomplished through the use of a linear scale-space, which is constructed by convolution of visual input with Gaussian kernels of varying size (scale). This has been shown to be equivalent to the solution of a Linear diffusion equation on an infinite domain, as the Gaussian is the Green's function of such a system (Koenderink, 1984). Recently, much work has been focused on the use of a variable conductance function resulting in anisotropic diffusion described by a nonlinear partial differential equation (PDE). The use of anisotropic diffusion with a conductance coefficient which is a decreasing function of the gradient magnitude has been shown to enhance edges, while decreasing some types of noise (Perona and Malik, 1987). Unfortunately, the solution of the anisotropic diffusion equation requires the numerical integration of a nonlinear PDE which is a costly process when carried out on a uniform mesh such as a typical image. In this paper we show that the complex log transformation, variants of which are universally used in mammalian retino-cortical systems, allows the nonlinear diffusion equation to be integrated at exponentially enhanced rates due to the nonuniform mesh spacing inherent in the log domain. The enhanced integration rates, coupled with the intrinsic compression of the complex log transformation, yields a speed increase of between two and three orders of magnitude, providing a means of performing rapid image enhancement using anisotropic diffusion.	Boston Univ, Dept Cognit & Neural Syst, Boston, MA 02215 USA	Boston University	Fischl, B (corresponding author), Boston Univ, Dept Cognit & Neural Syst, Boston, MA 02215 USA.	fischl@cns.bu.edu; mike@cns.bu.edu; eric@thing4.bu.edu						ALVAREZ L, 1992, SIAM J NUMER ANAL, V29, P845, DOI 10.1137/0729052; ALVAREZ L, 1994, SIAM J NUMER ANAL, V31, P590, DOI 10.1137/0731032; [Anonymous], 1941, AM J OPHTHALMOL, DOI [10.1016/S0002-9394(41)91363-6, DOI 10.1016/S0002-9394(41)91363-6]; BONMASSAR G, 1994, INT C PATT RECOG, P204, DOI 10.1109/ICPR.1994.577160; BONMASSAR G, 1996, IN PRESS REAL TIME I; BONMASSAR G, 1995, UNPUB IEEE T PATTERN; BONMASSAR G, 1996, IEEE COMP SOC C COMP, P492; CAMPBELL FW, 1966, J PHYSIOL-LONDON, V186, P558, DOI 10.1113/jphysiol.1966.sp008056; CATTE F, 1992, SIAM J NUMER ANAL, V29, P182, DOI 10.1137/0729012; Churchill R.V., 1984, COMPLEX VARIABLES AP; COHEN MA, 1984, PERCEPT PSYCHOPHYS, V36, P428, DOI 10.3758/BF03207497; COTTET GH, 1993, MATH COMPUT, V61, P659, DOI 10.1090/S0025-5718-1993-1195422-2; Dang T., 1994, Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation, P65, DOI 10.1109/IAI.1994.336683; DANIEL PM, 1961, J PHYSIOL-LONDON, V159, P203, DOI 10.1113/jphysiol.1961.sp006803; ELFALLAH AI, 1994, P SOC PHOTO-OPT INS, V2182, P49, DOI 10.1117/12.171091; ENGQUIST B, 1989, MATH COMPUT, V52, P509, DOI 10.1090/S0025-5718-1989-0955750-9; Fischl B, 1997, IEEE T PATTERN ANAL, V19, P342, DOI 10.1109/34.588012; Fischl B, 1997, NEURAL NETWORKS, V10, P815, DOI 10.1016/S0893-6080(96)00125-6; FISCHL B, 1996, INT C PATT REC VIENN; FISCHL B, 1997, 1 INT C SCAL SPAC TH; GERRITS HJM, 1970, EXP BRAIN RES, V11, P411; GROSSBERG S, 1988, PERCEPT PSYCHOPHYS, V43, P241, DOI 10.3758/BF03207869; GROSSBERG S, 1985, PSYCHOL REV, V92, P173, DOI 10.1037/0033-295X.92.2.173; HABERMAN R, 1987, ELEMENTARY APPL PART; HUMMEL A, 1986, READINGS COMPUTER VI; ILLNER R, 1993, MATH METHOD APPL SCI, V16, P545, DOI 10.1002/mma.1670160803; KACUR J, 1995, APPL NUMER MATH, V17, P47, DOI 10.1016/0168-9274(95)00008-I; LEE TS, 1995, VISION RES, V35, P2643, DOI 10.1016/0042-6989(95)00032-U; LI XP, 1994, PATTERN RECOGN, V27, P1029, DOI 10.1016/0031-3203(94)90142-2; MALLADI R, 1995, P NATL ACAD SCI USA, V92, P7046, DOI 10.1073/pnas.92.15.7046; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; MERIGAN WH, 1990, VISION RES, V30, P985, DOI 10.1016/0042-6989(90)90107-V; MESSNER RA, 1985, COMPUT VISION GRAPH, V31, P50, DOI 10.1016/S0734-189X(85)80075-X; NITZBERG M, 1992, IEEE T PATTERN ANAL, V14, P826, DOI 10.1109/34.149593; NORDSTROM KN, 1990, IMAGE VISION COMPUT, V8, P318, DOI 10.1016/0262-8856(90)80008-H; ORAM MW, 1992, J NEUROPHYSIOL, V68, P70, DOI 10.1152/jn.1992.68.1.70; OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053; PAUWELS EJ, 1993, P SOC PHOTO-OPT INS, V2094, P836, DOI 10.1117/12.158000; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Perona P., 1987, Proceedings of the IEEE Computer Society Workshop on Computer Vision (Cat. No.87TH0210-5), P16; Perona P., 1994, GEOMETRY DRIVEN DIFF, P73, DOI [10.1007/978-94-017-1699-4-3, DOI 10.1007/978-94-017-1699-4]; PRICE CB, 1990, IEE PROC-I, V137, P136, DOI 10.1049/ip-i-2.1990.0020; ROJER AS, 1992, SPIE PROGRAM COMPUTE, V9; ROJER AS, 1990, 10TH INT C PATT REC, V2, P278; SANDINI G, 1989, INT S ROB RES; SANDINI G, 1989, P OSA OPT M IM UND M; SCHENKAR P, 1981, P IEE INT C AC SPEEC, P1144; SCHWARTZ EL, 1980, VISION RES, V20, P645, DOI 10.1016/0042-6989(80)90090-5; SCHWARTZ EL, 1977, BIOL CYBERN, V25, P181, DOI 10.1007/BF01885636; SCHWARTZ EL, 1994, PRIMARY VISUAL CORTE, V10, pCH9; Shah J, 1996, PROC CVPR IEEE, P136, DOI 10.1109/CVPR.1996.517065; THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63; VOGELS R, 1991, EXP BRAIN RES, V84, P1; WASSLE H, 1990, VISION RES, V30, P1897, DOI 10.1016/0042-6989(90)90166-I; WEIMAN C, 1988, SPIE C DIG OPT SHAP; Wertheim T., 1894, Z PSYCHOL, V7, P172; WHITAKER RT, 1993, CVGIP-IMAG UNDERSTAN, V57, P111, DOI 10.1006/ciun.1993.1007; WHITAKER RT, 1991, COMPUTER VISION GRAP, V57, P99; WILLIAMS DR, 1987, J OPT SOC AM A, V4, P1514, DOI 10.1364/JOSAA.4.001514; Witkin A.P., 1983, INT JOINT C ART INT, P1019; Yamamoto H, 1996, COMPUT VIS IMAGE UND, V63, P50, DOI 10.1006/cviu.1996.0004	63	7	7	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	1998	28	3					199	212		10.1023/A:1008043919667	http://dx.doi.org/10.1023/A:1008043919667			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	118BM					2022-12-18	WOS:000075817800001
J	Astrom, K; Heyden, A				Astrom, K; Heyden, A			Continuous time matching constraints for image streams	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						multilinear constraint; uncalibrated; calibrated camera; structure; motion; optical flow		Corresponding image points of a rigid object in a discrete sequence of images fulfil the so-called multilinear constraint. In this paper the continuous time analogue of this constraint, for a continuous stream of images, is introduced and studied. The constraint links the Taylor series expansion of the motion of the image points with the Taylor series expansion of the relative motion and orientation between the object and the camera. The analysis is done both for calibrated and uncalibrated cameras. Two simplifications are also presented for the uncalibrated camera case. One simplification is made using an affine reduction and the so-called kinetic depths. The second simplification is based upon a projective reduction with respect to the image of a planar configuration. The analysis shows that the constraint involving second-order derivatives are needed to determine camera motion. Experiments with real and simulated data are also presented.	Lund Univ, Dept Math, Lund, Sweden	Lund University	Astrom, K (corresponding author), Lund Univ, Dept Math, Lund, Sweden.	kalle@maths.lth.se; heyden@maths.lth.se	Astrom, Kalle/AAT-9538-2020; Åström, Kalle/C-2836-2009	Astrom, Kalle/0000-0002-8689-7810; Åström, Kalle/0000-0002-8689-7810				Astrom K, 1996, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.1996.517168; ASTROM K, 1996, P 4 EUR C COMP VIS C, P97; ASTROM K, 1993, 1 IFAC INT WORKSH IN, P181; BLOM J, 1992, THESIS U UTRECHT UTR; FAUGERAS O, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P951, DOI 10.1109/ICCV.1995.466832; FAUGERAS OD, 1992, P 2 EUR C COMP VIS S, P563; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; Heyden A., 1995, Proceedings IEEE Workshop on Representation of Visual Scenes (In Conjunction with ICCV'95) (Cat. No.95TB8126), P45, DOI 10.1109/WVRS.1995.476851; Heyden A, 1997, IMAGE VISION COMPUT, V15, P749, DOI 10.1016/S0262-8856(97)00005-X; HEYDEN A, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1058, DOI 10.1109/ICCV.1995.466817; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LUONG QT, 1994, P 3 EUR C COMP VIS S, P589; Shashua A, 1996, IEEE T PATTERN ANAL, V18, P873, DOI 10.1109/34.537342; SPARR G, 1994, P 4 EUR C COMP VIS C, P471; Stefanovic P, 1973, ITC J, V3, P417; TRIGGS B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P338, DOI 10.1109/ICCV.1995.466920; VIEVILLE T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P750, DOI 10.1109/ICCV.1995.466863	17	7	7	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1998	28	1					85	96		10.1023/A:1008006815607	http://dx.doi.org/10.1023/A:1008006815607			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZX971					2022-12-18	WOS:000074573800005
J	Fua, P				Fua, P			Fast, accurate and consistent modeling of drainage and surrounding terrain	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						constrained optimization; delineation; deformable models; rivers	DIGITAL ELEVATION MODELS; SURFACES; NETWORKS	We propose an automated approach to modeling drainage channels-and, more generally, linear features that lie on the terrain-from multiple images. It produces models of the features and of the surrounding terrain that are accurate and consistent and requires only minimal human intervention. We take advantage of geometric constraints and photommetric knowledge. First, rivers flow downhill and lie at the bottom of valleys whose floors tend to be either V- or U-shaped. Second, the drainage pattern appears in gray-level images as a network of linear features that can be visually detected. Many approaches have explored individual facets of this problem. Ours unifies these elements in a common framework. We accurately model terrain and features as 3-dimensional objects from several information sources that may be in error and inconsistent with one another. This approach allows us to generate models that are faithful to sensor data, internally consistent and consistent with physical constraints. We have proposed generic models that have been applied to the specific task at hand. We show that the constraints can be expressed in a computationally effective way and, therefore, enforced while initializing the models and then fitting them to the data. Furthermore, these techniques are general enough to work on other features that are constrained by predictable forces.	LIG, Comp Graph Lab, CH-1015 Lausanne, Switzerland; SRI Int, Menlo Pk, CA 94025 USA	SRI International	Fua, P (corresponding author), LIG, Comp Graph Lab, CH-1015 Lausanne, Switzerland.	fua@lig.di.sri.com		Fua, Pascal/0000-0002-6702-9970				BAND LE, 1986, WATER RESOUR RES, V22, P15, DOI 10.1029/WR022i001p00015; BRECHBUHLER C, 1995, COMPUT VIS IMAGE UND, V61, P154, DOI 10.1006/cviu.1995.1013; COHEN L, 1996, C COMP VIS PATT REC, P666; FAIRFIELD J, 1991, WATER RESOUR RES, V27, P709, DOI 10.1029/90WR02658; FISCHLER MA, 1981, COMPUT VISION GRAPH, V15, P201, DOI 10.1016/0146-664X(81)90056-3; FISCHLER MA, 1983, COMPUTER VISION PATT, P351; Fletcher R, 1987, PRACTICAL METHODS OP, V1; Fua P., 1990, Machine Vision and Applications, V3, P45, DOI 10.1007/BF01211451; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; Fua P, 1996, COMPUT VIS IMAGE UND, V64, P111, DOI 10.1006/cviu.1996.0048; FUA P, 1996, 18 ISPRS C VIENN AUS; FUA P, 1996, EUR C COMP VIS CAMBR, P495; Gill P. E., 1981, PRACTICAL OPTIMIZATI; GILLULY J, 1968, PRINCIPLES GEOLOGY; GULCH E, 1988, INT ARCH PHOTOGRAMME, V27, P254; Hannah M., 1988, INT SOC PHOTOGRAMM R, V27, P280; HEIPKE C, 1992, INT SOC PHOTOGRAMMET, P832; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KOENDERINK JJ, 1993, SPIE, V2031; KOENDERINK JJ, 1991, PATERN RECOGNITION L, V15, P439; KOH E, 1994, EURO C COMP VIS STOC; LENGAGNE R, 1996, INT C IM PROC LAUS S; McInerney T., 1993, INT C COMP VIS, P518; MERLET N, 1995, IEEE T PATERN ANAL M, V18; METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727; Mortensen E.N., 1995, SIGGRAPH, P191; MUNDY JL, 1992, ARPA IM UND WORKSH S, P215; OCALLAGHAN JF, 1984, COMPUT VISION GRAPH, V28, P323, DOI [10.1016/S0734-189X(84)80011-0, 10.1016/0734-189X(89)90053-4]; PAIGE CC, 1982, ACM T MA SOFTW, V8; ROSEN, 1961, SIAM J APPL MATH, V8, P181; SANDER PT, 1990, IEEE T PATTERN ANAL, V12, P833, DOI 10.1109/34.57680; TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840; WROBEL BP, 1991, PHOTOGRAMM REC, V13, P765, DOI 10.1111/j.1477-9730.1991.tb00738.x	33	7	7	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB-MAR	1998	26	3					215	234		10.1023/A:1007905112118	http://dx.doi.org/10.1023/A:1007905112118			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	ZD490		Green Submitted			2022-12-18	WOS:000072691200003
J	Cipolla, R; Fletcher, G; Giblin, P				Cipolla, R; Fletcher, G; Giblin, P			Following cusps	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							SURFACE; SHAPE; CONTOURS; PROFILES	It is known that the deformation of the apparent contours of a surface under perspective projection and viewer motion enable the recovery of the geometry of the surface, for example by utilising the epipolar parametrization. These methods break down with apparent contours that are singular i.e., with cusps. In this paper we study this situation and show how, nevertheless, the surface geometry (including the Gauss curvature and mean curvature of the surface) can be recovered by following the cusps. Indeed the formulae are much simpler in this case and require lower spatio-temporal derivatives than in the general case of nonsingular apparent contours. We also show that following cusps does not by itself provide us with information on viewer motion.	UNIV LIVERPOOL, DEPT PURE MATH, LIVERPOOL L69 3BX, MERSEYSIDE, ENGLAND	University of Liverpool	Cipolla, R (corresponding author), UNIV CAMBRIDGE, DEPT ENGN, TRUMPINGTON ST, CAMBRIDGE CB2 1PZ, ENGLAND.		Arandjelović, Ognjen/V-5255-2019	Arandjelović, Ognjen/0000-0002-9314-194X; Cipolla, Roberto/0000-0002-8999-2151				Bruce J.W., 1992, CURVES SINGULARITIES; CIPOLLA R, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P269, DOI 10.1109/ICCV.1995.466775; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; FLETCHER GJ, 1996, P 4 EUR C COMP VIS C, P107; GIBLIN P, 1987, 1ST P INT C COMP VIS, P136; GIBLIN PJ, 1988, IMAGE VISION COMPUT, V6, P225, DOI 10.1016/0262-8856(88)90012-1; GIBLIN PJ, 1994, J OPT SOC AM A, V11, P1976, DOI 10.1364/JOSAA.11.001976; GIBLIN PJ, 1995, IMAGE VISION COMPUT, V13, P33, DOI 10.1016/0262-8856(95)91466-Q; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1982, PERCEPTION, V11, P129, DOI 10.1068/p110129; KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321; ONeill B., 1966, ELEMENTARY DIFFERENT; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787	13	7	7	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1997	23	2					115	129		10.1023/A:1007920028712	http://dx.doi.org/10.1023/A:1007920028712			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XK384					2022-12-18	WOS:A1997XK38400001
J	Li, SZ				Li, SZ			Parameter estimation for optimal object recognition: Theory and application	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							STATISTICAL-ANALYSIS; RANDOM-FIELDS; RELAXATION; REGULARIZATION; RESTORATION; IMAGES	Object recognition systems involve parameters such as thresholds, bounds and weights. These parameters have to be tuned before the system can perform successfully. A common practice is to choose such parameters manually on an ad hoc basis, which is a disadvantage. This paper presents a novel theory of parameter estimation for optimization-based object recognition where the optimal solution is defined as the global minimum of an energy function. The theory is based on supervised learning from examples. Correctness and instability are established as criteria for evaluating the estimated parameters. A correct estimate enables the labeling implied in each exemplary configuration to be encoded in a unique global energy minimum. The instability is the ease with which the minimum is replaced by a non-exemplary configuration after a perturbation. The optimal estimate minimizes the instability. Algorithms are presented for computing correct and minimal-instability estimates. The theory is applied to the parameter estimation for MRF-based recognition and promising results are obtained.			Li, SZ (corresponding author), NANYANG TECHNOL UNIV, SCH ELECT & ELECT ENGN, SINGAPORE 639798, SINGAPORE.							BADDELEY AJ, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL II, P136, DOI 10.1109/ICPR.1992.201739; BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; BHANU B, 1984, IEEE T PATTERN ANAL, V6, P137, DOI 10.1109/TPAMI.1984.4767499; Boser B. E., 1992, P 5 ANN ACM WORKSH C; COHEN FS, 1987, IEEE T PATTERN ANAL, V9, P195, DOI 10.1109/TPAMI.1987.4767895; Cooper P. R., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P287, DOI 10.1109/ICCV.1990.139532; CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341; DAVIS LS, 1979, IEEE T PATTERN ANAL, V1, P60, DOI 10.1109/TPAMI.1979.4766876; DERIN H, 1987, IEEE T PATTERN ANAL, V9, P39, DOI 10.1109/TPAMI.1987.4767871; Duda R.O., 1973, J ROYAL STAT SOC SER; FAN TJ, 1989, IEEE T PATTERN ANAL, V11, P1140, DOI 10.1109/34.42853; FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602; GEIGER D, 1987, P INT JOINT C AI; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GHAHRAMAN DE, 1980, IEEE T SYST MAN CYB, V10, P181, DOI 10.1109/TSMC.1980.4308468; Grimson W. E. L., 1990, OBJECT RECOGNITION C; Huang T.S., 1993, P 4 INT C COMP VIS B, P121; JACOBUS CJ, 1980, IEEE T PATTERN ANAL, V2, P495, DOI 10.1109/TPAMI.1980.6447696; KIM IY, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL I, P441, DOI 10.1109/ICPR.1992.201595; Li S., 1995, MARKOV RANDOM FIELD, P1; LI SZ, 1994, P IEEE COMP SOC C CO; LI SZ, 1993, P 1 AS C COMP VIS OS; MARROQUIN JL, 1985, THESIS MIT AI LAB; Modestino J. W., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P458, DOI 10.1109/CVPR.1989.37888; NADABAR SG, 1992, JUN P IEEE C COMP VI, P528; Nasrabadi N. M., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P325, DOI 10.1109/ICCV.1990.139542; OSHIMA M, 1983, IEEE T PATTERN ANAL, V5, P353, DOI 10.1109/TPAMI.1983.4767405; PELILLO M, 1994, IEEE T PATTERN ANAL, V16, P933, DOI 10.1109/34.310691; POGGIO T, 1990, NATURE, V343, P263, DOI 10.1038/343263a0; POPE A, 1993, P IEEE INT C COMP VI, P296; QIAN W, 1989, J APPL STAT, V16, P267; Rosenblatt F., 1961, PRINCIPLES NEURODYNA, DOI 10.21236/AD0256582; SHAHRARAY B, 1989, IEEE T PATTERN ANAL, V11, P600, DOI 10.1109/34.24794; SHAPIRO LG, 1981, IEEE T PATTERN ANAL, V3, P504, DOI 10.1109/TPAMI.1981.4767144; THOMPSON AM, 1991, IEEE T PATTERN ANAL, V13, P326, DOI 10.1109/34.88568; Vapnik V., 1982, ESTIMATION DEPENDENC; WAHBA G, 1980, APPROXIMATION THEORY, V2; Well W. M.  III, 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P486, DOI 10.1109/CVPR.1991.139740; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P806, DOI 10.1109/34.149592; WONG AKC, 1985, IEEE T PATTERN ANAL, V7, P599, DOI 10.1109/TPAMI.1985.4767707; ZHANG J, 1988, THESIS RENSSELAER PO	42	7	8	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	1997	21	3					207	222		10.1023/A:1007947800092	http://dx.doi.org/10.1023/A:1007947800092			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WN532					2022-12-18	WOS:A1997WN53200003
J	Petitjean, S				Petitjean, S			The enumerative geometry of projective algebraic surfaces and the complexity of aspect graphs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							OBJECTS	The aspect graph is a popular viewer-centered representation that enumerates all the topologically distinct views of an object. Building the aspect graph requires partitioning viewpoint space in view-equivalent cells by a certain number of visual event surfaces. If the object is piecewise-smooth algebraic, then all visual event surfaces are either made of lines having specified contacts with the object or made of lines supporting the points of contacts of planes having specified contacts with the object. In this paper, we present a general framework for studying the enumerative properties of line and plane systems. The context is that of enumerative geometry and intersection theory. In particular, we give exact results for the degrees of all visual event surfaces coming up in the construction of aspect graphs of piecewise-smooth algebraic bodies. We conclude by giving a bound on the number of topologically distinct views of such objects.	INRIA LORRAINE,F-54506 VANDOEUVRE NANCY,FRANCE		Petitjean, S (corresponding author), CTR RECH & INFORMAT NANCY,CNRS,BATIMENT LORIA,BP 239,F-54506 VANDOEUVRE NANCY,FRANCE.							CHEVALLEY C, 1945, T AM MATH SOC, V57, P1, DOI 10.2307/1990167; COLLEY SJ, 1987, ADV MATH, V66, P149, DOI 10.1016/0001-8708(87)90033-8; COLLEY SJ, 1983, THESIS MIT; COLLEY SJ, 1986, P 1984 VANC C ALG GE, V6, P47; FULTON W, 1984, ERGEBNISSE MATH; GROTHENDIECK A, 1958, P SEM C CHAVALLEY; HARTSHORNE R, 1977, ALGEBRAIC GEOMETRY; Hodge W.V.D., 1953, METHODS ALGEBRAIC GE, V2; HODGE WVD, 1953, METHODS ALGEBRAIC GE, V1; Kleiman S.L, 1977, REAL COMPLEX SINGULA, P297; KLEIMAN SL, 1981, ACTA MATH-DJURSHOLM, V147, P13, DOI 10.1007/BF02392866; KLEIMAN SL, 1987, P SYMP PURE MATH, V46, P321; KLEIMAN SL, 1969, PUBLICATIONS MATH IH, V36; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; KULIKOV VS, 1983, FUNCT ANAL APPL+, V17, P176; LAKSOV D, 1993, REPORT ALGEBRAIC GEO; LEBARZ P, 1987, ADV MATH, V64, P87, DOI 10.1016/0001-8708(87)90006-5; MCCRORY C, 1980, PROFILES SURFACES; Namba Makoto, 1984, GEOMETRY PROJECTIVE; PETITJEAN S, 1994, P ISSAC 94 INT S SYM, P9; PETITJEAN S, 1994, IN PRESS P MEGA 94 I; PETITJEAN S, 1995, THESIS I NAT POLYTEC; RAN Z, 1985, ACTA MATH-DJURSHOLM, V155, P81, DOI 10.1007/BF02392538; RIEGER J, 1993, FBIHHM22893 U HAMB; RIEGER JH, 1987, IMAGE VISION COMPUT, V5, P91, DOI 10.1016/0262-8856(87)90033-3; RIEGER JH, 1993, J SYMB COMPUT, V16, P259, DOI 10.1006/jsco.1993.1044; RONGA F., 1992, STEWART PLATFORMS CO; Salmon G., 1915, TREATISE ANAL GEOMET, VII; Schubert H, 1879, KALKUL ABZAHLENDEN G; SHCHERBAK OP, 1982, T TBILISI I MAT, V13, P232; SHCHERBAK OP, 1982, T TBILISI I MAT, V13, P280; TARI F, 1991, J LOND MATH SOC, V44, P155; TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273; WHITNEY H, 1955, ANN MATH, V62, P374, DOI 10.2307/1970070; XAMBODESCAMPS S, 1993, MA2IR93009 U POL CAT; [No title captured]; [No title captured]; [No title captured]	38	7	7	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	1996	19	3					261	287		10.1007/BF00055147	http://dx.doi.org/10.1007/BF00055147			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	VG016					2022-12-18	WOS:A1996VG01600003
J	SAWHNEY, HS; HANSON, AR				SAWHNEY, HS; HANSON, AR			TRACKABILITY AS A CUE FOR POTENTIAL OBSTACLE IDENTIFICATION AND 3-D DESCRIPTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							MOTION	In many man-made environments, obstacles in the path of a mobile robot can be characterized as shallow, that is, they have relatively small extent in depth compared to the distance from the camera. We present a framework for segmenting shallow structures from their background over a sequence of images. Shallowness is first quantified as affine describability. This is embedded in a tracking system within which hypothesized model structures undergo a cycle of prediction and model-matching. Structures emerge either as shallow or nonshallow based on their affine trackability. Two major contributions of this work are (i) aggregate object tracking based on 3-D motion and structure constraints in constrast with traditional primitive feature tracking based on image motion heuristics, and (ii) use of temporal behavior for object segmentation and 3-D reconstruction.	UNIV MASSACHUSETTS,DEPT COMP SCI,AMHERST,MA 01003	University of Massachusetts System; University of Massachusetts Amherst								ADIV G, 1989, IEEE T PATTERN ANAL, V11, P477, DOI 10.1109/34.24780; ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; Bar-Shalom Y., 1988, TRACKING DATA ASS; BOLDT M, 1989, IEEE T SYST MAN CYB, V19, P1581, DOI 10.1109/21.44073; BROLIO J, 1989, COMPUTER, V22, P22, DOI 10.1109/2.42029; Crowley J. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P658, DOI 10.1109/CCV.1988.590047; DERICHE R, 1990, 1ST P EUR C COMP VIS, P259; FAUGERAS OD, 1987, 1ST P INT C COMP VIS, P25; FAUGERAS OD, 1985, 3RD P INT S ROB RES; GELB A, 1986, APPLIED OPTIMAL ESTI; HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443; HUTTENLOCHER DP, 1989, P C COMPUT VIS PATT, P1114; Kanade T., 1983, HUMAN MACHINE VISION, P237; Kumar R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P209, DOI 10.1109/CVPR.1992.223273; LAMDAN Y, 1988, JUN P CVPR C ANN ARB, P335; Mahalanobis P.C., 1936, P NAT I SCI INDIA, V2, P49; MEHRA RK, 1970, IEEE T AUTOMAT CONTR, VAC15, P175, DOI 10.1109/TAC.1970.1099422; Nelson R. C., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P188, DOI 10.1109/CCV.1988.589990; THOMPSON DW, 1987, IEEE J ROBOTIC AUTOM, P208; TSAI RY, 1984, IMAGE UNDERSTANDING, P135; WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779; Williams L. R., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P441, DOI 10.1109/CCV.1988.590021; [No title captured]	24	7	9	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	1993	11	3					237	265		10.1007/BF01469344	http://dx.doi.org/10.1007/BF01469344			29	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MD681					2022-12-18	WOS:A1993MD68100002
J	VISTNES, R				VISTNES, R			TEXTURE MODELS AND IMAGE MEASURES FOR TEXTURE-DISCRIMINATION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									STANFORD UNIV, DEPT COMP SCI, ROBOT LAB, STANFORD, CA 94305 USA	Stanford University								AHUJA N, 1983, PATTERN MODELS; ALOIMONOS J, 1988, INT J COMPUT VISION, V2, P171, DOI 10.1007/BF00133699; Anderson T. W, 1984, INTRO MULTIVARIATE S; BECK J, 1982, HUMAN MACHINE VISION, P1; BINFORD TO, 1983, PHYSICAL BIOL PROCES; Blostein D., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P444; Brodatz P., 1966, TEXTURES PHOTOGRAPHI; BRUNO A, 1986, P INT C PATTERN RECO, P817; CAELLI T, 1985, Spatial Vision, V1, P19, DOI 10.1163/156856885X00044; CONNERS RW, 1980, IEEE T PATTERN ANAL, V2, P204, DOI 10.1109/TPAMI.1980.4767008; CONNERS RW, 1981, IMAGE MODELING, P29; Cornsweet T., 1970, VISUAL PERCEPTION; DAVIS LS, 1982, COMPUT VISION GRAPH, V19, P95, DOI 10.1016/0146-664X(82)90102-2; DAVIS LS, 1979, COMPUT VISION GRAPH, V11, P111, DOI 10.1016/0146-664X(79)90061-3; DAVIS LS, 1981, IEEE T PATTERN ANAL, V3, P214, DOI 10.1109/TPAMI.1981.4767084; DAVIS LS, 1979, IEEE T PATTERN ANAL, V1, P251, DOI 10.1109/TPAMI.1979.4766921; DAVIS LS, 1980, COMPUT VISION GRAPH, V12, P25, DOI 10.1016/0146-664X(80)90002-7; GAGALOWICZ A, 1985, COMPUT VISION GRAPH, V30, P289, DOI 10.1016/0734-189X(85)90162-8; HARALICK RM, 1980, COMPUT VISION GRAPH, V12, P60, DOI 10.1016/0146-664X(80)90004-0; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; HARWOOD D, 1985, COMPUT VISION GRAPH, V32, P404, DOI 10.1016/0734-189X(85)90060-X; HEALEY G, 1987, P DARPA IMAGE UNDERS, P599; HEALEY G, 1988, P IEEE C COMPUT VISI; JULESZ B, 1986, BIOL CYBERN, V54, P245, DOI 10.1007/BF00318420; JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0; JULESZ B, 1983, BELL SYST TECH J, V62, P1619, DOI 10.1002/j.1538-7305.1983.tb03502.x; JULESZ B, 1982, RECOGNITION PATTERN, P33; KANATANI K, 1989, ARTIF INTELL, V38, P1, DOI 10.1016/0004-3702(89)90066-0; KASHYAP RL, 1986, IEEE T PATTERN ANAL, V8, P472, DOI 10.1109/TPAMI.1986.4767811; KASHYAP RL, 1986, HDB PATTERN RECOGNIT, P281; KJELL B, 1984, 559 U WISC COMP SCI; Laws K. I., 1980, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V238, P376; LAWS KI, 1984, SRI334 AI CTR INT; LAWS KI, 1980, USC IMAGE P; Leclerc Y., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P34; LEU JG, 1985, COMPUT VISION GRAPH, V31, P67, DOI 10.1016/S0734-189X(85)80076-1; LIEDTKE CE, 1983, IMAGE SEQUENCE PROCE; MALESON JT, 1977, OCT P DARPA IM UND W, P19; MARR D, 1975, COLD SPRING HARB SYM, V40, P647; MATSUYAMA T, 1982, COMPUT VISION GRAPH, V18, P259, DOI 10.1016/0146-664X(82)90035-1; MUERLE JL, 1970, PICTURE PROCESSING P, P371; Peleg S, 1984, IEEE Trans Pattern Anal Mach Intell, V6, P518, DOI 10.1109/TPAMI.1984.4767557; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P661, DOI 10.1109/TPAMI.1984.4767591; PIETIKAINEN M, 1983, IEEE T SYST MAN CYB, V13, P421, DOI 10.1109/TSMC.1983.6313175; Raafat H. M., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P469; Rearick T. C., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P312; SACHS L, 1984, APPLIED STATISTICS H; TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999; TOMITA FY, 1979, 6TH INT JONT C ART I, P884; TREISMAN A, 1985, COMPUT VISION GRAPH, V31, P156, DOI 10.1016/S0734-189X(85)80004-9; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; TSUJI S, 1973, COMPUTER GRAPHICS IM, V2, P216; VANGOOL L, 1985, COMPUT VISION GRAPH, V29, P336, DOI 10.1016/0734-189X(85)90130-6; VILNROTTER FM, 1986, IEEE T PATTERN ANAL, V8, P76, DOI 10.1109/TPAMI.1986.4767754; VISTNES R, 1988, THESIS STANFORD U; Voorhees H., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P250; WERMSER D, 1982, 6TH P INT C PATT REC, P178; WESZKA JS, 1976, IEEE T SYST MAN CYB, V6, P269, DOI 10.1109/TSMC.1976.5408777; WITKIN AP, 1981, ARTIF INTELL, V17, P17, DOI 10.1016/0004-3702(81)90019-9; ZUCKER SW, 1976, PERCEPTION, V5, P419, DOI 10.1068/p050419	62	7	7	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	1989	3	4					313	336		10.1007/BF00132602	http://dx.doi.org/10.1007/BF00132602			24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CC907					2022-12-18	WOS:A1989CC90700003
J	SUBBARAO, M				SUBBARAO, M			INTERPRETATION OF IMAGE FLOW - RIGID CURVED SURFACES IN MOTION	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article									SUNY STONY BROOK,DEPT ELECT ENGN,STONY BROOK,NY 11794	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook								ADIV G, 1985, IEEE T PATTERN ANAL, V7, P384, DOI 10.1109/TPAMI.1985.4767678; ADIV G, 1985, DEC P DARP IM UND WO, P399; HAY JC, 1966, PSYCHOL REV, V73, P550, DOI 10.1037/h0023863; HILDRETH E, 1983, MEASUREMENT VISUAL M; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; KANATANI K, 1986, CAR TR214 U MAR CTR; KANATANI K, 1985, 9TH P INT JOINT C AR, P886; KOENDERINK JJ, 1975, OPT ACTA, V22, P773, DOI 10.1080/713819112; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; LONGUETHIGGINS HC, 1984, NATURE, V223, P165; MAYBANK SJ, 1985, P ROY SOC LOND A MAT, V401, P317, DOI 10.1098/rspa.1985.0101; ONEILL B, 1966, ELEMENTARY DIFFERENT, P204; PISKUNOV N, 1974, DIFFERENTIAL INTEGRA, V1, P264; SUBBARAO M, 1986, COMPUT VISION GRAPH, V36, P208, DOI 10.1016/0734-189X(86)90076-9; SUBBARAO M, 1986, THESIS U MARYLAND; TSAI RY, 1981, R922 U ILL TECH REP; TSAI RY, 1984, PAMI, V6, P13; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; WAXMAN AM, 1985, INT J ROBOT RES, V4, P72, DOI 10.1177/027836498500400306; WAXMAN AM, 1985, INT J ROBOT RES, V4, P95, DOI 10.1177/027836498500400307; WAXMAN AM, 1987, 1ST P INT C COMP VIS, P12; WOHN K, 1984, THESIS U MARYLAND	23	7	7	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1988	2	1					77	96		10.1007/BF00836282	http://dx.doi.org/10.1007/BF00836282			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AC189					2022-12-18	WOS:A1988AC18900004
J	Yan, QS; Gong, D; Shi, JQF; van den Hengel, A; Shen, CH; Reid, I; Zhang, YN				Yan, Qingsen; Gong, Dong; Shi, Javen Qinfeng; van den Hengel, Anton; Shen, Chunhua; Reid, Ian; Zhang, Yanning			Dual-Attention-Guided Network for Ghost-Free High Dynamic Range Imaging	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						High dynamic range imaging; De-ghosting; Attention mechanism; Deep learning	IMAGES; VIDEO	Ghosting artifacts caused by moving objects and misalignments are a key challenge in constructing high dynamic range (HDR) images. Current methods first register the input low dynamic range (LDR) images using optical flow before merging them. This process is error-prone, and often causes ghosting in the resulting merged image. We propose a novel dual-attention-guided end-to-end deep neural network, called DAHDRNet, which produces high-quality ghost-free HDR images. Unlike previous methods that directly stack the LDR images or features for merging, we use dual-attention modules to guide the merging according to the reference image. DAHDRNet thus exploits both spatial attention and feature channel attention to achieve ghost-free merging. The spatial attention modules automatically suppress undesired components caused by misalignments and saturation, and enhance the fine details in the non-reference images. The channel attention modules adaptively rescale channel-wise features by considering the inter-dependencies between channels. The dual-attention approach is applied recurrently to further improve feature representation, and thus alignment. A dilated residual dense block is devised to make full use of the hierarchical features and increase the receptive field when hallucinating missing details. We employ a hybrid loss function, which consists of a perceptual loss, a total variation loss, and a content loss to recover photo-realistic images. Although DAHDRNet is not flow-based, it can be applied to flow-based registration to reduce artifacts caused by optical-flow estimation errors. Experiments on different datasets show that the proposed DAHDRNet achieves state-of-the-art quantitative and qualitative results.	[Yan, Qingsen; Gong, Dong; Shi, Javen Qinfeng; van den Hengel, Anton; Shen, Chunhua; Reid, Ian] Univ Adelaide, Australian Inst Machine Learning, Adelaide, SA, Australia; [Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian, Peoples R China	University of Adelaide; Northwestern Polytechnical University	Gong, D (corresponding author), Univ Adelaide, Australian Inst Machine Learning, Adelaide, SA, Australia.	edgong01@gmail.com		Gong, Dong/0000-0002-2668-9630	Centre for Augmented Reasoning at the Australian Institute for Machine Learning; ARC [DP140102270, DP160100703]; NSFC [61871328, 61971273, 61901384]	Centre for Augmented Reasoning at the Australian Institute for Machine Learning; ARC(Australian Research Council); NSFC(National Natural Science Foundation of China (NSFC))	This work was partially supported by the Centre for Augmented Reasoning at the Australian Institute for Machine Learning, ARC (DP140102270, DP160100703), and NSFC (61871328, 61971273, 61901384).	Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325; Bogoni L, 2000, INT C PATT RECOG, P7, DOI 10.1109/ICPR.2000.903475; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218; Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816; Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834; Fan HQ, 2018, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2018.00118; Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592; Glorot X., 2010, PROC MACH LEARN RES, P249; Gong D., 2018, ARXIV PREPRINT ARXIV; Gong D, 2017, PROC CVPR IEEE, P3806, DOI 10.1109/CVPR.2017.405; Granados M, 2010, PROC CVPR IEEE, P215, DOI 10.1109/CVPR.2010.5540208; Grosch Thorsten, VISION MODELING VISU, P277; Hafner D, 2014, INT C PATT RECOG, P2065, DOI 10.1109/ICPR.2014.360; Heo YS, 2011, LECT NOTES COMPUT SC, V6495, P486, DOI 10.1007/978-3-642-19282-1_39; Hu J, 2013, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2013.154; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jacobs K, 2008, IEEE COMPUT GRAPH, V28, P84, DOI 10.1109/MCG.2008.23; Kalantari NK, 2019, COMPUT GRAPH FORUM, V38, P193, DOI 10.1111/cgf.13630; Kalantari Nima Khademi, 2017, ACM T GRAPHICS P SIG, V36, DOI DOI 10.1145/3072959.3073609; Kang SB, 2003, ACM T GRAPHIC, V22, P319, DOI 10.1145/882262.882270; Kingma D.P, P 3 INT C LEARNING R; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee C, 2014, IEEE SIGNAL PROC LET, V21, P1045, DOI 10.1109/LSP.2014.2323404; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Malik J., 2008, P 24 ANN C COMPUTER, P31, DOI DOI 10.1145/1401132.1401174; MANN S, 1995, IS&T'S 48TH ANNUAL CONFERENCE - IMAGING ON THE INFORMATION SUPERHIGHWAY, FINAL PROGRAM AND PROCEEDINGS, P442; Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935; Martel JNP, 2020, IEEE T PATTERN ANAL, V42, P1642, DOI 10.1109/TPAMI.2020.2986944; Metzler CA, 2020, PROC CVPR IEEE, P1372, DOI 10.1109/CVPR42600.2020.00145; Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857; Oh TH, 2015, IEEE T PATTERN ANAL, V37, P1219, DOI 10.1109/TPAMI.2014.2361338; Pece F., 2010, Proceedings 2010 Conference on Visual Media Production (CVMP 2010). 7th European Conference on Visual Media Production, P1, DOI 10.1109/CVMP.2010.8; Prabhakar K. Ram, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P497, DOI 10.1007/978-3-030-58589-1_30; Raman S, 2011, VISUAL COMPUT, V27, P1099, DOI 10.1007/s00371-011-0653-0; Reinhard E., 2010, HIGH DYNAMIC RANGE I, V2nd, P145; Santos M. S., 2020, ARXIV PREPRINT ARXIV; Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222; Srikantha A, 2012, SIGNAL PROCESS-IMAGE, V27, P650, DOI 10.1016/j.image.2012.02.001; Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366; Sun QL, 2020, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR42600.2020.00146; Szpak ZL, 2015, PROC CVPR IEEE, P2132, DOI 10.1109/CVPR.2015.7298825; Szpak ZL, 2014, COMPUT VIS IMAGE UND, V125, P200, DOI 10.1016/j.cviu.2014.04.008; Tumblin J, 2005, PROC CVPR IEEE, P103; Tursun OT, 2016, COMPUT GRAPH FORUM, V35, P139, DOI 10.1111/cgf.12818; Tursun OT, 2015, COMPUT GRAPH FORUM, V34, P683, DOI 10.1111/cgf.12593; Wu SZ, 2018, LECT NOTES COMPUT SC, V11206, P120, DOI 10.1007/978-3-030-01216-8_8; Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346; Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185; Yan QS, 2019, IEEE WINT CONF APPL, P41, DOI 10.1109/WACV.2019.00012; Yan QS, 2017, NEUROCOMPUTING, V269, P160, DOI 10.1016/j.neucom.2017.03.083; Yang J., 2018, ECCV, P654; Yang X, 2018, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2018.00193; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhang C, 2020, IEEE INT CON MULTI; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang W, 2012, IEEE T IMAGE PROCESS, V21, P2318, DOI 10.1109/TIP.2011.2170079; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498; Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865; Zimmer H, 2011, COMPUT GRAPH FORUM, V30, P405, DOI 10.1111/j.1467-8659.2011.01870.x	61	6	6	6	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2022	130	1					76	94		10.1007/s11263-021-01535-y	http://dx.doi.org/10.1007/s11263-021-01535-y		OCT 2021	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	YC9GC					2022-12-18	WOS:000712244700001
J	Zhang, YH; Wang, LJ; Wang, D; Qi, JQ; Lu, HCA				Zhang, Yunhua; Wang, Lijun; Wang, Dong; Qi, Jinqing; Lu, Huchuan			Learning Regression and Verification Networks for Robust Long-term Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Long-term visual tracking; Regression network; Classification network		This paper proposes a new visual tracking algorithm, which leverages the merits of both template matching approaches and classification models for long-term object detection and tracking. To this end, a regression network is learned offline to detect a set of target candidates through target template matching. To cope with target appearance variations in long-term scenarios, a target-aware feature fusion mechanism is also developed, giving rise to more effective template matching. Meanwhile, a verification network is trained online to better capture target appearance and identify the target from potential candidates. During online update, contaminated training samples can be filtered out through a monitoring module, alleviating model degeneration caused by error accumulation. The regression and verification networks operate in a cascaded manner, which allows tracking to be performed in a coarse-to-fine manner and enforces the discriminative power. To further address the target reappearance issues in long-term tracking, a learning-based switching scheme is proposed, which learns to switch the tracking mode between local and global search based on the tracking results. Extensive evaluations on long-term tracking in the wild have been conducted. We achieve state-of-the-art performance on the OxUvA long-term tracking dataset. Our submission based on the proposed method has also won the 1st place of the long-term tracking challenge in VOT-2018 competition.	[Zhang, Yunhua; Wang, Lijun; Wang, Dong; Qi, Jinqing; Lu, Huchuan] Dalian Univ Technol, Dalian, Peoples R China; [Lu, Huchuan] Dalian Univ Technol, Key Lab Intelligent Control & Optimizat Ind Equip, Minist Educ, Dalian, Peoples R China; [Lu, Huchuan] Dalian Univ Technol, Ningbo Inst, Ningbo, Zhejiang, Peoples R China; [Zhang, Yunhua] Univ Amsterdam, Amsterdam, Netherlands	Dalian University of Technology; Dalian University of Technology; Dalian University of Technology; University of Amsterdam	Wang, LJ (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.	y.zhang9@uva.nl; ljwang@dlut.edu.cn; wdice@dlut.edu.cn; jinqing@dlut.edu.cn; lhchuan@dlut.edu.cn						Abadi M., TENSORFLOW LARGE SCA; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29; Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490; Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84; Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585; Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI 10.1109/ICCV.2017.129; Han B, 2017, PROC CVPR IEEE, P521, DOI 10.1109/CVPR.2017.63; He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508; Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675; Howard A.G, 2017, ARXIV170404861; Jung I., 2018, ECCV; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kristan, 2018, ARXIV PREPRINT ARXIV; Kristan M., 2017, ARXIV PREPRINT ARXIV; Kristan Matej, 2018, P EUROPEAN C COMPUTE; Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu W., 2016, DESTECH TRANS COMP; Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177; Matas J., 2017, ICCV WORKSH; NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465; Nebehay G, 2015, PROC CVPR IEEE, P2784, DOI 10.1109/CVPR.2015.7298895; Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sun C, 2018, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2018.00058; Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934; Supancic J, 2017, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2017.43; Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158; Valmadre J., 2018, ECCV; Wang LJ, 2016, PROC CVPR IEEE, P1373, DOI 10.1109/CVPR.2016.153; Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Yan Bin, 2019, ICCV; Zhong Z., 2017, ARXIV PREPRINT ARXIV; Zhu G, 2016, PROC CVPR IEEE, P943, DOI 10.1109/CVPR.2016.108; Zhu Z., 2018, ECCV; Zhu Zheng, 2018, CVPR	41	6	6	4	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2536	2547		10.1007/s11263-021-01487-3	http://dx.doi.org/10.1007/s11263-021-01487-3		JUN 2021	12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN					2022-12-18	WOS:000663288700001
J	Mao, W; Liu, MM; Salzmann, M; Li, HD				Mao, Wei; Liu, Miaomiao; Salzmann, Mathieu; Li, Hongdong			Multi-level Motion Attention for Human Motion Prediction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human motion prediction; Motion attention; Deep learning		Human motion prediction aims to forecast future human poses given a historical motion. Whether based on recurrent or feed-forward neural networks, existing learning based methods fail to model the observation that human motion tends to repeat itself, even for complex sports actions and cooking activities. Here, we introduce an attention based feed-forward network that explicitly leverages this observation. In particular, instead of modeling frame-wise attention via pose similarity, we propose to extract motion attention to capture the similarity between the current motion context and the historical motion sub-sequences. In this context, we study the use of different types of attention, computed at joint, body part, and full pose levels. Aggregating the relevant past motions and processing the result with a graph convolutional network allows us to effectively exploit motion patterns from the long-term history to predict the future poses. Our experiments on Human3.6M, AMASS and 3DPW validate the benefits of our approach for both periodical and non-periodical actions. Thanks to our attention model, it yields state-of-the-art results on all three datasets. Our code is available at https://github.com/wei-mao-2019/HisRepItself.	[Mao, Wei; Liu, Miaomiao; Li, Hongdong] Australian Natl Univ, Canberra, ACT, Australia; [Salzmann, Mathieu] EPFL CVLab, Lausanne, Switzerland; [Salzmann, Mathieu] ClearSpace, Lausanne, Switzerland	Australian National University	Mao, W (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.	wei.mao@anu.edu.au; miaomiao.liu@anu.edu.au; mathieu.salzmann@epfl.ch; hongdong.li@anu.edu.au		Mao, Wei/0000-0002-8876-8983	Australia Research Council DECRA Fellowship [DE180100628]; ARC [DP200102274]	Australia Research Council DECRA Fellowship; ARC(Australian Research Council)	This research was supported in part by the Australia Research Council DECRA Fellowship (DE180100628) and ARC Discovery Grant (DP200102274). The authors would like to thank NVIDIA for the donated GPU (Titan V).	Akhter Ijaz, 2009, P NIPS; [Anonymous], 2018, CVPR, DOI DOI 10.1109/CVPR.2018.00939; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865; Butepage J, 2017, PROC CVPR IEEE, P1591, DOI 10.1109/CVPR.2017.173; Cai Y., 2020, ECCV; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494; Gong HF, 2011, IEEE I CONF COMP VIS, P619, DOI 10.1109/ICCV.2011.6126296; Gopalakrishnan A, 2019, PROC CVPR IEEE, P12108, DOI 10.1109/CVPR.2019.01239; Gui LY, 2018, LECT NOTES COMPUT SC, V11208, P823, DOI 10.1007/978-3-030-01225-0_48; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; JAIN A, 2016, PROC CVPR IEEE, P5308, DOI DOI 10.1109/CVPR.2016.573; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Koppula HS, 2013, IEEE INT C INT ROBOT, P2071, DOI 10.1109/IROS.2013.6696634; Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605; Levine S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185524; Li C, 2018, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2018.00548; Li X, 2018, PROC CVPR IEEE, P3032, DOI 10.1109/CVPR.2018.00320; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554; Mao W., 2020, ECCV; Mao W, 2019, IEEE I CONF COMP VIS, P9488, DOI 10.1109/ICCV.2019.00958; Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497; Pavllo D, 2020, INT J COMPUT VISION, V128, P855, DOI 10.1007/s11263-019-01245-6; Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883; Ruiz AH, 2019, IEEE I CONF COMP VIS, P7133, DOI 10.1109/ICCV.2019.00723; Sidenbladh H, 2002, LECT NOTES COMPUT SC, V2350, P784; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; Tang YY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P935; Vaswani A, 2017, ADV NEUR IN, V30; von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	34	6	6	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2021	129	9					2513	2535		10.1007/s11263-021-01483-7	http://dx.doi.org/10.1007/s11263-021-01483-7		JUN 2021	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TS1RN		Green Submitted			2022-12-18	WOS:000662108800001
J	Gomes, TL; Martins, R; Ferreira, J; Azevedo, R; Torres, G; Nascimento, ER				Gomes, Thiago L.; Martins, Renato; Ferreira, Joao; Azevedo, Rafael; Torres, Guilherme; Nascimento, Erickson R.			A Shape-Aware Retargeting Approach to Transfer Human Motion and Appearance in Monocular Videos	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Motion retargeting; Human image synthesis; Human motion; Video-to-video translation; Image manipulation		Transferring human motion and appearance between videos of human actors remains one of the key challenges in Computer Vision. Despite the advances from recent image-to-image translation approaches, there are several transferring contexts where most end-to-end learning-based retargeting methods still perform poorly. Transferring human appearance from one actor to another is only ensured when a strict setup has been complied, which is generally built considering their training regime's specificities. In this work, we propose a shape-aware approach based on a hybrid image-based rendering technique that exhibits competitive visual retargeting quality compared to state-of-the-art neural rendering approaches. The formulation leverages the user body shape into the retargeting while considering physical constraints of the motion in 3D and the 2D image domain. We also present a new video retargeting benchmark dataset composed of different videos with annotated human motions to evaluate the task of synthesizing people's videos, which can be used as a common base to improve tracking the progress in the field. The dataset and its evaluation protocols are designed to evaluate retargeting methods in more general and challenging conditions. Our method is validated in several experiments, comprising publicly available videos of actors with different shapes, motion types, and camera setups. The dataset and retargeting code are publicly available to the community at: https://www.verlab.dcc.ufmg.br/retargeting-motion.	[Gomes, Thiago L.; Ferreira, Joao; Azevedo, Rafael; Torres, Guilherme; Nascimento, Erickson R.] Univ Fed Minas Gerais, Dept Comp Sci, Belo Horizonte, MG, Brazil; [Martins, Renato] INRIA, Sophia Antipolis, France	Universidade Federal de Minas Gerais; Inria	Gomes, TL (corresponding author), Univ Fed Minas Gerais, Dept Comp Sci, Belo Horizonte, MG, Brazil.	thiagoluange@dcc.ufmg.br; renato.martins@inria.fr; joaoferreira@dcc.ufmg.br; rafaelvieiraz@ufmg.br; torres@dcc.ufmg.br; erickson@dcc.ufmg.br	Nascimento, Erickson R./G-5374-2014	Nascimento, Erickson R./0000-0003-2973-2232; Torres, Guilherme/0000-0002-6280-5627; Martins, Renato/0000-0003-0053-0004; Azevedo, Rafael/0000-0001-7728-0221; Gomes, Thiago/0000-0002-7679-8210; Moreira Ferreira, Joao Pedro/0000-0002-8093-9880	CAPES; CNPq; FAPEMIG	CAPES(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); CNPq(Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPQ)); FAPEMIG(Fundacao de Amparo a Pesquisa do Estado de Minas Gerais (FAPEMIG))	The authors thank CAPES, CNPq, and FAPEMIG for funding thiswork. We also thank NVIDIA for the donation of a Titan XP GPU used in this research.	Aberman K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322999; Aberman Kfir, 2018, ARXIV180806847; Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870; Bau D, 2019, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2019.00460; Black M. J., 2007, NIPS; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603; Choi KJ, 2000, J VISUAL COMP ANIMAT, V11, P223, DOI 10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5; De Boor C., 1978, PRACTICAL GUIDE SPLI; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923; Gelly S., 2019, ARXIV181201717; Gleicher M., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P33, DOI 10.1145/280814.280820; Gomes TL, 2020, IEEE WINT CONF APPL, P3355, DOI 10.1109/WACV45572.2020.9093395; Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47; Hassan M, 2019, IEEE I CONF COMP VIS, P2282, DOI 10.1109/ICCV.2019.00237; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98; Lassner Christoph, 2017, CVPR; Levi Z, 2015, IEEE T VIS COMPUT GR, V21, P264, DOI 10.1109/TVCG.2014.2359463; Li H, 2020, LEUKEMIA, V34, P1503, DOI [10.1038/s41375-020-0848-3, 10.1007/s11263-020-01364-5]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Ma LQ, 2017, ADV NEUR IN, V30; Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554; Marra F, 2020, IEEE ACCESS, V8, P133488, DOI 10.1109/ACCESS.2020.3009877; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Mir A, 2020, PROC CVPR IEEE, P7021, DOI 10.1109/CVPR42600.2020.00705; Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8; Peng XB, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275014; Shysheya A, 2019, PROC CVPR IEEE, P2382, DOI 10.1109/CVPR.2019.00249; Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494; Sun Yang-Tian, 2020, ARXIV200313510; Tatarchenko M., 2015, ABS151106702 CORR, V1, P2; Tewari A, 2020, COMPUT GRAPH FORUM, V39, P701, DOI 10.1111/cgf.14022; Villegas R, 2018, PROC CVPR IEEE, P8639, DOI 10.1109/CVPR.2018.00901; Wang C, 2019, AAAI CONF ARTIF INTE, P5232; Wang Suchen, 2020, CVPR; Wang TC, 2018, ADV NEUR IN, V31; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Xu R, 2019, PROC CVPR IEEE, P3718, DOI 10.1109/CVPR.2019.00384; Yang Jimei, 2015, NIPS; Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhao B., 2017, MULTIVIEW IMAGE GENE	54	6	6	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2057	2075		10.1007/s11263-021-01471-x	http://dx.doi.org/10.1007/s11263-021-01471-x		APR 2021	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW		Green Submitted			2022-12-18	WOS:000645481000001
J	Shekar, AK; Gou, L; Ren, L; Wendt, A				Shekar, Arvind Kumar; Gou, Liang; Ren, Liu; Wendt, Axel			Label-Free Robustness Estimation of Object Detection CNNs for Autonomous Driving Applications	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Robustness; Label-free; CNN-testing; Autonomous-driving		The advent of Convolutional Neural Networks (CNNs) has led to its increased application in several domains. One noteworthy application is the perception system for autonomous driving that rely on the predictions from CNNs. On one hand, predicting the learned objects with maximum accuracy is of importance. On the other hand, it is still a challenge to evaluate the reliability of CNN-based perception systems without ground truth information. Such evaluations are of significance for autonomous driving applications. One way to estimate reliability is by evaluating robustness of the detections in the presence of artificial perturbations. However, several existing works on perturbation-based robustness quantification rely on the ground truth labels. Acquiring the ground truth labels is a tedious, expensive and error-prone process. In this work we propose a novel label-free robustness metric for quantifying the robustness of CNN object detectors. We quantify the robustness of the detections to a specific type of input perturbation based on the prediction confidences. In short, we check the sensitivity of the predicted confidences under increased levels of artificial perturbation. Thereby, we avoid the need for ground truth annotations. We perform extensive evaluations on our traffic light detector from autonomous driving applications and on public object detection networks and datasets. The evaluations show that our label-free metric is comparable to the ground truth aided robustness scoring.	[Shekar, Arvind Kumar; Wendt, Axel] Robert Bosch GmbH, Stuttgart, Germany; [Gou, Liang; Ren, Liu] Robert Bosch Res & Technol Ctr, Palo Alto, CA USA	Bosch; Bosch	Shekar, AK (corresponding author), Robert Bosch GmbH, Stuttgart, Germany.	arvindkumar.shekar@de.bosch.com; liang.gou@us.bosch.com; liu.ren@us.bosch.com; axel.wendt@de.bosch.com		Shekar, Arvind Kumar/0000-0002-5853-5310				Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Brach K., 2020, ARXIV200703293; Carlini N., 2019, CORR; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dodge S, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX); Dolatshah M, 2018, PROC VLDB ENDOW, V12, P376, DOI 10.14778/3297753.3297758; Feng D, 2018, IEEE INT C INTELL TR, P3266, DOI 10.1109/ITSC.2018.8569814; Fregin A, 2018, IEEE INT CONF ROBOT, P3376; Gal Y., 2016, P 33 INT C MACH LEAR, P1050, DOI DOI 10.5555/3045390.3045502; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gopinath D, 2018, LECT NOTES COMPUT SC, V11138, P3, DOI 10.1007/978-3-030-01090-4_1; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Hein M, 2017, NIPS 17; Hendrycks D., 2019, ICLR, P1; Hosseini H, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P101, DOI 10.1109/ICMLA.2017.0-172; Huang XW, 2017, LECT NOTES COMPUT SC, V10426, P3, DOI 10.1007/978-3-319-63387-9_1; Huang ZY, 2018, ACM S THEORY COMPUT, P17, DOI 10.1145/3188745.3188858; Johanson M, 2014, IEEE INT CONF BIG DA, P736, DOI 10.1109/BigData.2014.7004298; Kapishnikov A., 2019, ARXIV190602825; Katz G., 2017, ARXIV170902802; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lakshminarayanan B, 2017, ADV NEUR IN, V30; Li JY, 2014, IEEE-ACM T AUDIO SPE, V22, P745, DOI 10.1109/TASLP.2014.2304637; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Maglogiannis I. G, 2007, HCI INFORM RETRIEVAL, V160; Maier-Hein L, 2014, LECT NOTES COMPUT SC, V8674, P349, DOI 10.1007/978-3-319-10470-6_44; Mangal R, 2019, 2019 IEEE/ACM 41ST INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING: NEW IDEAS AND EMERGING RESULTS (ICSE-NIER 2019), P93, DOI 10.1109/ICSE-NIER.2019.00032; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Schmidt L, 2018, ADV NEUR IN, V31; Shahrokni A, 2013, INFORM SOFTWARE TECH, V55, P1, DOI 10.1016/j.infsof.2012.06.002; Shekar AK, 2017, LECT NOTES ARTIF INT, V10534, P239, DOI 10.1007/978-3-319-71249-9_15; Strong A. I, 2016, SCIENCE, V5, P6; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Temel D, 2018, 2018 17TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P137, DOI 10.1109/ICMLA.2018.00028; Tramer F., 2020, 37 INT C MACH LEARN; Vijayanarasimhan S, 2009, PROC CVPR IEEE, P2262, DOI 10.1109/CVPRW.2009.5206705; von Ahn Luis, 2004, P SIGCHI C HUM FACT, DOI [10.1145/985692.985733, DOI 10.1145/985692.985733]; Yu F., 2019, ARXIV190504270; Zhang J. M., 2020, ARXIV190610742 CORR; Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865	47	6	6	1	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1185	1201		10.1007/s11263-020-01423-x	http://dx.doi.org/10.1007/s11263-020-01423-x		JAN 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK					2022-12-18	WOS:000607058700001
J	Geng, ZL; Cao, C; Tulyakov, S				Geng, Zhenglin; Cao, Chen; Tulyakov, Sergey			Towards Photo-Realistic Facial Expression Manipulation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Generative adversarial network; Graphics	DATABASE	We present a method for photo-realistic face manipulation. Given a single RGB face image with an arbitrary expression, our method can synthesize another arbitrary expression of the same person. To achieve this, we first fit a 3D face model and disentangle the face into its texture and shape. We then train separate networks in each of these spaces. In texture space, we use a conditional generative network to change the appearance, and carefully design the input format and loss functions to achieve the best results. In shape space, we use a fully connected network to predict an accurate face shape. When available, the shape branch uses depth data for supervision. Both networks are conditioned on expression coefficients rather than discrete labels, allowing us to generate an unlimited number of expressions. Furthermore, we adopt spatially adaptive denormalization on our texture space representation to improve the quality of the synthesized results. We show the superiority of this disentangling approach through both quantitative and qualitative studies. The proposed method does not require paired data, and is trained using an in-the-wild dataset of videos consisting of talking people. To achieve this, we present a simple yet efficient method to select appropriate key frames from these videos. In a user study, our method is preferred in 83.2% of cases when compared to state-of-the-art alternative approaches.	[Geng, Zhenglin; Tulyakov, Sergey] Snap Inc, Santa Monica, CA USA; [Cao, Chen] Facebook Real Labs, Pittsburgh, PA 15213 USA	Facebook Inc	Cao, C (corresponding author), Facebook Real Labs, Pittsburgh, PA 15213 USA.	zju.caochen@gmail.com						Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Bodnar C., 2018, ABS180500676 CORR; Bouaziz S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461976; Cao C, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275093; Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; Chen YC, 2018, PROC CVPR IEEE, P3541, DOI 10.1109/CVPR.2018.00373; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Chung JS, 2018, INTERSPEECH, P1086; Friesen E, 1978, TECHNIQUE MEASUREMEN; Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125; Geng JH, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275043; Geng ZL, 2019, PROC CVPR IEEE, P9813, DOI 10.1109/CVPR.2019.01005; Hensel M, 2017, ADV NEUR IN, V30; Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karras T., 2017, PROGR GROWING GANS I; Kazemi V., 2014, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2014.241; KIM HYEON-ZOO, 2018, [ASSOCIATION CULTURELLE FRANC0-COREENNE, 프랑스 문화 연구], V37, P1, DOI 10.18022/acfco.2018.37.1.001; Kingma D.P, P 3 INT C LEARNING R; Lample G, 2017, ADV NEUR IN, V30; Levy B., 2009, ACM SIGGRAPH ASIA CO; Li H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778769; Liu Ming-Yu, 2017, NIPS; Liu Rosanne, 2018, ABS180703247 CORR; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Ma DS, 2015, BEHAV RES METHODS, V47, P1122, DOI 10.3758/s13428-014-0532-5; Mao X., 2016, MULTICLASS GENERATIV; Nagano K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275075; Olszewski K, 2017, IEEE I CONF COMP VIS, P5439, DOI 10.1109/ICCV.2017.580; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269; Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50; Siarohin A., 2018, ARXIV181208861; Tewari A., 2018, P IEEE C COMP VIS PA; Thies Justus, 2016, CVPR, DOI DOI 10.1109/CVPR.2016.262; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Tulyakov S, 2018, IEEE T PATTERN ANAL, V40, P2250, DOI 10.1109/TPAMI.2017.2750687; Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645; Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209; Wang TC, 2018, ADV NEUR IN, V31; Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41; Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457; Zhou H, 2019, AAAI CONF ARTIF INTE, P9299; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	49	6	6	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2744	2761		10.1007/s11263-020-01361-8	http://dx.doi.org/10.1007/s11263-020-01361-8		AUG 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY					2022-12-18	WOS:000565043900002
J	Bone, A; Colliot, O; Durrleman, S				Bone, Alexandre; Colliot, Olivier; Durrleman, Stanley		Alzheimer's Dis Neuroimaging Initi	Learning the spatiotemporal variability in longitudinal shape data sets	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Longitudinal data; Statistical shape analysis; Large deformation diffeomorphic metric mapping; Medical imaging; Disease progression modeling	RIEMANNIAN-MANIFOLDS; TRAJECTORIES; MORPHOMETRY; FRAMEWORK; VERSION; MODEL	In this paper, we propose a generative statistical model to learn the spatiotemporal variability in longitudinal shape data sets, which contain repeated observations of a set of objects or individuals over time. From all the short-term sequences of individual data, the method estimates a long-term normative scenario of shape changes and a tubular coordinate system around this trajectory. Each individual data sequence is therefore (i) mapped onto a specific portion of the trajectory accounting for differences in pace of progression across individuals, and (ii) shifted in the shape space to account for intrinsic shape differences across individuals that are independent of the progression of the observed process. The parameters of the model are estimated using a stochastic approximation of the expectation-maximization algorithm. The proposed approach is validated on a simulated data set, illustrated on the analysis of facial expression in video sequences, and applied to the modeling of the progressive atrophy of the hippocampus in Alzheimer's disease patients. These experiments show that one can use the method to reconstruct data at the precision of the noise, to highlight significant factors that may modulate the progression, and to simulate entirely synthetic longitudinal data sets reproducing the variability of the observed process.	[Bone, Alexandre; Colliot, Olivier; Durrleman, Stanley] ICM, Inst Cerveau, F-75013 Paris, France; [Bone, Alexandre; Colliot, Olivier; Durrleman, Stanley] INSERM, U1127, F-75013 Paris, France; [Bone, Alexandre; Colliot, Olivier; Durrleman, Stanley] CNRS, UMR 7225, F-75013 Paris, France; [Bone, Alexandre; Colliot, Olivier; Durrleman, Stanley] Sorbonne Univ, F-75013 Paris, France; [Bone, Alexandre; Colliot, Olivier; Durrleman, Stanley] Inria, Aramis Project Team, F-75013 Paris, France	UDICE-French Research Universities; Sorbonne Universite; Institut National de la Sante et de la Recherche Medicale (Inserm); Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Biology (INSB); UDICE-French Research Universities; Universite Paris Cite; UDICE-French Research Universities; Sorbonne Universite; Inria	Bone, A (corresponding author), ICM, Inst Cerveau, F-75013 Paris, France.; Bone, A (corresponding author), INSERM, U1127, F-75013 Paris, France.; Bone, A (corresponding author), CNRS, UMR 7225, F-75013 Paris, France.; Bone, A (corresponding author), Sorbonne Univ, F-75013 Paris, France.; Bone, A (corresponding author), Inria, Aramis Project Team, F-75013 Paris, France.	alexandre.bone@icm-institute.org; olivier.colliot@icm-institute.org; stanley.durrleman@icm-institute.org		Bone, Alexandre/0000-0003-4442-3876	European Research Council (ERC) [678304]; European Union's Horizon 2020 research and innovation programme [666992, 826421]; program "Investissements d' d'avenir" [ANR-10-IAIHU-06, ANR-19-P3IA-0001]; Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health) [U01 AG024904]; DOD ADNI (Department of Defense) [W81XWH-12-2-0012]; National Institute on Aging; National Institute of Biomedical Imaging and Bioengineering; AbbVie; Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research and Development, LLC.; Johnson & Johnson Pharmaceutical Research and Development LLC.; Lumosity; Lundbeck; Merck Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; Transition Therapeutics; Canadian Institutes of Health Research	European Research Council (ERC)(European Research Council (ERC)European Commission); European Union's Horizon 2020 research and innovation programme; program "Investissements d' d'avenir"(French National Research Agency (ANR)); Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS)); DOD ADNI (Department of Defense); National Institute on Aging(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute on Aging (NIA)); National Institute of Biomedical Imaging and Bioengineering(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Biomedical Imaging & Bioengineering (NIBIB)); AbbVie(AbbVie); Alzheimer's Association(Alzheimer's Association); Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen(Biogen); Bristol-Myers Squibb Company(Bristol-Myers Squibb); CereSpir, Inc.; Cogstate(CogState Limited); Eisai Inc.(Eisai Co Ltd); Elan Pharmaceuticals, Inc.; Eli Lilly and Company(Eli Lilly); EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare(General ElectricGE Healthcare); IXICO Ltd.; Janssen Alzheimer Immunotherapy Research and Development, LLC.; Johnson & Johnson Pharmaceutical Research and Development LLC.(Johnson & JohnsonJohnson & Johnson USA); Lumosity; Lundbeck(Lundbeck Corporation); Merck Co., Inc.(Merck & Company); Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation(Novartis); Pfizer Inc.(Pfizer); Piramal Imaging; Servier(Servier); Takeda Pharmaceutical Company(Takeda Pharmaceutical Company Ltd); Transition Therapeutics; Canadian Institutes of Health Research(Canadian Institutes of Health Research (CIHR))	The research leading to this publication has been funded in part by the European Research Council (ERC) under grant agreement No 678304 (LEASP), European Union's Horizon 2020 research and innovation programme under Grant Agreement No. 666992 (EuroPOND) and No 826421 (TVB-Cloud), and the program "Investissements d' d'avenir" ANR-10-IAIHU-06 (IHU ICM) and ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). The facial expression data set at the basis of Section 5.2 was built and shared by the Binghamton University. The authors warmly thank Pr. Lijun Yin for granting data access, and Peng Liu for his help in downloading the data set. Regarding Section 5.3, data collection and sharing was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense Award No. W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research and Development, LLC.; Johnson & Johnson Pharmaceutical Research and Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer's Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.	Abdi H, 2003, ENCY MEASUREMENT STA, V792, P795; Ahrens J, 2005, VISUALIZATION HDB, V717, P863; Allassonniere S, 2015, SIAM J IMAGING SCI, V8, P1367, DOI 10.1137/140971762; Atchade YF, 2006, METHODOL COMPUT APPL, V8, P235, DOI 10.1007/s11009-006-8550-0; Banerjee M, 2016, PROC CVPR IEEE, P4424, DOI 10.1109/CVPR.2016.479; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Ben Amor B, 2014, IEEE T CYBERNETICS, V44, P2443, DOI 10.1109/TCYB.2014.2308091; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Bilgel M, 2016, NEUROIMAGE, V134, P658, DOI 10.1016/j.neuroimage.2016.04.001; Bone A, 2019, LECT NOTES COMPUT SC, V11492, P195, DOI 10.1007/978-3-030-20351-1_15; Bone A, 2018, PROC CVPR IEEE, P9271, DOI 10.1109/CVPR.2018.00966; Chakraborty R, 2017, I S BIOMED IMAGING, P999, DOI 10.1109/ISBI.2017.7950684; Charlier B., 2017, EFFICIENT KERNEL PRO; Charon N., 2020, RIEMANNIAN GEOMETRIC, P441; Christensen GE, 1996, IEEE T IMAGE PROCESS, V5, P1435, DOI 10.1109/83.536892; Cury C, 2019, NEUROIMAGE, V188, P282, DOI 10.1016/j.neuroimage.2018.11.063; Debavelaere V, 2019, LECT NOTES COMPUT SC, V11767, P66, DOI 10.1007/978-3-030-32251-9_8; Delyon B, 1999, ANN STAT, V27, P94; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Durrleman S, 2014, NEUROIMAGE, V101, P35, DOI 10.1016/j.neuroimage.2014.06.043; Durrleman S, 2013, INT J COMPUT VISION, V103, P22, DOI 10.1007/s11263-012-0592-x; Durrleman S, 2013, INT J COMPUT VISION, V101, P161, DOI 10.1007/s11263-012-0556-1; Fischl B, 2002, NEURON, V33, P341, DOI 10.1016/S0896-6273(02)00569-X; Fischl B, 2000, P NATL ACAD SCI USA, V97, P11050, DOI 10.1073/pnas.200033797; Fishbaugh J, 2014, I S BIOMED IMAGING, P385, DOI 10.1109/ISBI.2014.6867889; Fletcher PT, 2013, INT J COMPUT VISION, V105, P171, DOI 10.1007/s11263-012-0591-y; Gori P, 2017, MED IMAGE ANAL, V35, P458, DOI 10.1016/j.media.2016.08.011; Hinkle J, 2012, LECT NOTES COMPUT SC, V7574, P1, DOI 10.1007/978-3-642-33712-3_1; Hirsch M., 2012, DIFFERENTIAL TOPOLOG; Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223; Joshi SC, 2000, IEEE T IMAGE PROCESS, V9, P1357, DOI 10.1109/83.855431; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kim H. J., 2017, P IEEE C COMP VIS PA; Koval I, 2017, INT C MED IM COMP CO, P451, DOI DOI 10.1007/978-3-319-66182-7_52; Kuhn E., 2004, ESAIM PROBABILITY 13, V8, P115, DOI DOI 10.1051/PS:2004007; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Lorenzi HA, 2010, PLOS NEGLECT TROP D, V4, DOI 10.1371/journal.pntd.0000716; Lorenzi M, 2011, LECT NOTES COMPUT SC, V6801, P463, DOI 10.1007/978-3-642-22092-0_38; Louis M, 2018, SIAM J NUMER ANAL, V56, P2563, DOI 10.1137/17M1130617; Louis M, 2017, LECT NOTES COMPUT SC, V10589, P29, DOI 10.1007/978-3-319-68445-1_4; MANASSE FK, 1963, J MATH PHYS, V4, P735, DOI 10.1063/1.1724316; Marin JM, 2012, STAT COMPUT, V22, P1167, DOI 10.1007/s11222-011-9288-2; Marinescu RV, 2017, LECT NOTES COMPUT SC, V10265, P134, DOI 10.1007/978-3-319-59050-9_11; Miller MI, 2006, J MATH IMAGING VIS, V24, P209, DOI 10.1007/s10851-005-3624-0; Muralidharan P, 2012, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2012.6247780; Musuvathy S, 2011, IEEE ENG MED BIO, P595, DOI 10.1109/IEMBS.2011.6090132; Nader C. A., 2019, ARXIV190210952; Niethammer M, 2011, LECT NOTES COMPUT SC, V6892, P655, DOI 10.1007/978-3-642-23629-7_80; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Schiratti J-B, 2015, ADV NEURAL INFORM PR, P2404; Schiratti JB, 2017, J MACH LEARN RES, V18; Singh N, 2016, INT J COMPUT VISION, V117, P70, DOI 10.1007/s11263-015-0849-2; Stern Y, 2006, ALZ DIS ASSOC DIS, V20, P112, DOI 10.1097/01.wad.0000213815.20177.19; Su JY, 2014, PROC CVPR IEEE, P620, DOI 10.1109/CVPR.2014.86; Su JY, 2014, ANN APPL STAT, V8, P530, DOI 10.1214/13-AOAS701; Sun Y, 2008, LECT NOTES COMPUT SC, V5303, P58, DOI 10.1007/978-3-540-88688-4_5; Vaillant M., 2005, INFORM PROCESSING ME, P1, DOI DOI 10.1007/11505730_; Woolrich MW, 2009, NEUROIMAGE, V45, pS173, DOI 10.1016/j.neuroimage.2008.10.055; Yin L., 2008, AUTOMATIC FACE GESTU, V08, P1, DOI DOI 10.1109/AFGR.2008.4813324; Younes L., 2010, APPL MATH SCI; Younes L, 2007, Q APPL MATH, V65, P113, DOI 10.1090/S0033-569X-07-01027-5; Zhang Miaomiao, 2015, Inf Process Med Imaging, V24, P249, DOI 10.1007/978-3-319-19992-4_19; Zhang Miaomiao, 2013, Inf Process Med Imaging, V23, P37, DOI 10.1007/978-3-642-38868-2_4	65	6	6	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2020	128	12					2873	2896		10.1007/s11263-020-01343-w	http://dx.doi.org/10.1007/s11263-020-01343-w		JUL 2020	24	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NV4KZ		Green Submitted			2022-12-18	WOS:000545051500001
J	Moschoglou, S; Ploumpis, S; Nicolaou, MA; Papaioannou, A; Zafeiriou, S				Moschoglou, Stylianos; Ploumpis, Stylianos; Nicolaou, Mihalis A.; Papaioannou, Athanasios; Zafeiriou, Stefanos			3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D; Face; GAN; Generation; Translation; Representation		Over the past few years, Generative Adversarial Networks (GANs) have garnered increased interest among researchers in Computer Vision, with applications including, but not limited to, image generation, translation, imputation, and super-resolution. Nevertheless, no GAN-based method has been proposed in the literature that can successfully represent, generate or translate 3D facial shapes (meshes). This can be primarily attributed to two facts, namely that (a) publicly available 3D face databases are scarce as well as limited in terms of sample size and variability (e.g., few subjects, little diversity in race and gender), and (b) mesh convolutions for deep networks present several challenges that are not entirely tackled in the literature, leading to operator approximations and model instability, often failing to preserve high-frequency components of the distribution. As a result, linear methods such as Principal Component Analysis (PCA) have been mainly utilized towards 3D shape analysis, despite being unable to capture non-linearities and high frequency details of the 3D face-such as eyelid and lip variations. In this work, we present 3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D facial surfaces, while retaining the high frequency details of 3D face shapes. We conduct an extensive series of both qualitative and quantitative experiments, where the merits of 3DFaceGAN are clearly demonstrated against other, state-of-the-art methods in tasks such as 3D shape representation, generation, and translation.	[Moschoglou, Stylianos; Ploumpis, Stylianos; Papaioannou, Athanasios; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, London, England; [Moschoglou, Stylianos; Ploumpis, Stylianos; Papaioannou, Athanasios; Zafeiriou, Stefanos] Facesoftio, London, England; [Nicolaou, Mihalis A.] Cyprus Inst, Computat Based Sci & Technol Res Ctr, Nicosia, Cyprus	Imperial College London	Moschoglou, S (corresponding author), Imperial Coll London, Dept Comp, London, England.; Moschoglou, S (corresponding author), Facesoftio, London, England.	s.moschoglou@imperial.ac.uk; s.ploumpis@imperial.ac.uk; m.nicolaou@cyi.ac.cy; a.papaioannou11@imperial.ac.uk; s.zafeiriou@imperial.ac.uk			EPSRC DTA studentship from Imperial College London; EPSRC [EP/N007743/1, EP/S010203/1]	EPSRC DTA studentship from Imperial College London(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Stylianos Moschoglou is supported by an EPSRC DTA studentship from Imperial College London, Stylianos Ploumpis by the EPSRC Project EP/N007743/1 (FACER2VM), and Stefanos Zafeiriou by the EPSRC Project EP/S010203/1 (DEFORM).	[Anonymous], 2016, P INT C LEARN REPR I; [Anonymous], IEEE C COMP VIS PATT; [Anonymous], P INT C LEARN REPR I; [Anonymous], 2017, P IEEE C COMP VIS PA; Berthelot D., 2017, BEGAN BOUNDARY EQUIL; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598; Booth J, 2014, IEEE IMAGE PROC, P4672, DOI 10.1109/ICIP.2014.7025947; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Brunton A, 2014, COMPUT VIS IMAGE UND, V128, P1, DOI 10.1016/j.cviu.2014.05.005; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Davies R, 2008, STAT MODELS SHAPE OP; DESMET M, 2010, P AS C COMP VIS, P276; Dosovitskiy Alexey, 2016, NEURIPS; Dou P., 2017, P IEEE C COMP VIS PA, P21; Fan H., 2017, P CVPR HON HAW 21 26, V2, P6; Feng Y., 2018, P EUR C COMP VIS ECC, P534; Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jolliffe I, 2011, INT ENCY STAT SCI, DOI 10.1007/978-3-642-04898-2_455; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Karras Tero, 2018, ICLR; Nguyen K, 2018, PATTERN RECOGN, V78, P23, DOI 10.1016/j.patcog.2018.01.002; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D., P INT C LEARN REPR I, DOI DOI 10.1145/1830483.1830503; Kingma D.P., 2015, INT C LEARN REPR, P1; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lei T, 2017, PR MACH LEARN RES, V70; Litany O, 2018, PROC CVPR IEEE, P1886, DOI 10.1109/CVPR.2018.00202; Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603; Liu F., 2019, ARXIV190204943; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Lucic M., 2018, P ADV NEURAL INFORM; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Maier-Hein L, 2012, IEEE T PATTERN ANAL, V34, P1520, DOI 10.1109/TPAMI.2011.248; Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616; Mirza M., 2014, ARXIV PREPRINT ARXIV; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Qi C. R., 2017, IEEE P COMPUT VIS PA, V1, P4, DOI DOI 10.1109/CVPR.2017.16; Radford A., 2016, P INT C LEARN REPR I; Ranjan A., 2018, P EUR C COMP VIS ECC, P704; Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589; Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107; Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270; Tran AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163; Wang T.-C., 2018, IEEE C COMP VIS PATT, P5, DOI DOI 10.1038/s41438-017-0012-z; Wang Weiyue, 2017, P IEEE INT C COMP VI, P2298; Yang C., 2017, IEEE C COMP VIS PATT, V1, P3; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	56	6	7	2	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2020	128	10-11			SI		2534	2551		10.1007/s11263-020-01329-8	http://dx.doi.org/10.1007/s11263-020-01329-8		MAY 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	NS4KY		Green Submitted, hybrid			2022-12-18	WOS:000530783200002
J	Gupta, S; Tolani, V; Davidson, J; Levine, S; Sukthankar, R; Malik, J				Gupta, Saurabh; Tolani, Varun; Davidson, James; Levine, Sergey; Sukthankar, Rahul; Malik, Jitendra			Cognitive Mapping and Planning for Visual Navigation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Visual navigation; Spatial representations; Learning for navigation	PERCEPTION; WORLD	We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: (a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and (b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as "going to a chair". We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.	[Gupta, Saurabh; Tolani, Varun; Levine, Sergey; Malik, Jitendra] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Gupta, Saurabh; Davidson, James; Levine, Sergey; Sukthankar, Rahul; Malik, Jitendra] Google, Mountain View, CA 94043 USA	University of California System; University of California Berkeley; Google Incorporated	Gupta, S (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Gupta, S (corresponding author), Google, Mountain View, CA 94043 USA.	sgupta@eecs.berkeley.edu; vtolani@berkeley.edu; jcdavidson@google.com; svlevine@eecs.berkeley.edu; sukthankar@google.com; malik@eecs.berkeley.edu						Abadi M, 2015, P 12 USENIX S OPERAT; Abel D, 2016, ICML WORKSH ABSTR RE; Ammirato P., 2017, ICRA; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Anderson Peter, 2018, EVALUATION EMBODIED, V1; Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170; Aydemir A, 2013, IEEE T ROBOT, V29, P986, DOI 10.1109/TRO.2013.2256686; Bhatti S., 2016, ARXIV161200380; Blundell C., 2016, ARXIV160604460; Bruce J., 2018, P 2 C ROBOT LEARNING; Canny, 1988, COMPLEXITY ROBOT MOT; Chang Angel, 2017, INT C 3D VIS 3DV; Chaplot D. S., 2018, INT C LEARN REPR; Daftry S., 2016, ISER; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Das Abhishek, 2018, CVPR; Davison A. J., 1998, ECCV; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duan Y., 2016, RL2 FAST REINFORCEME; ELFES A, 1987, IEEE T ROBOTIC AUTOM, V3, P249, DOI 10.1109/JRA.1987.1087096; ELFES A, 1989, COMPUTER, V22, P46, DOI 10.1109/2.30720; Fraundorfer F., 2012, INT C INT ROB SYST I; Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8; Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024; Gordon Daniel, 2018, IEEE C COMP VIS PATT; Gu SX, 2016, PR MACH LEARN RES, V48; Gupta S., 2016, COMPUTER VISION PATT; Gupta S., 2013, P IEEE C COMP VIS PA; Haarnoja T., 2016, NIPS; Hadsell R, 2009, J FIELD ROBOT, V26, P120, DOI 10.1002/rob.20276; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Heess N., 2015, NEURAL INF PROCESS S; Henriques Joao F, 2018, CVPR; Henry P., 2010, ISER; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Izadi Shahram, 2011, UIST, DOI [10.1145/2047196.2047270, DOI 10.1145/2047196.2047270]; Jacobson D. H., 1970, DIFFERENTIAL DYNAMIC; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kahn G., 2017, ICRA; Kavraki L. E., 1996, PROBABILISTIC ROADMA, P2; Khan A., 2018, INT C LEARN REPR ICL; KHATIB O, 1986, INT J ROBOT RES, V5, P90, DOI 10.1177/027836498600500106; Kim H., 2003, NIPS; Kingma D.P, P 3 INT C LEARNING R; Kohl Nate, 2004, ICRA; Koltun, 2018, ICLR, P1; Konolige K., 2010, IJRR; Koppula Hema S, 2011, NIPS; Kuipers B., 1991, Robotics and Autonomous Systems, V8, P47, DOI 10.1016/0921-8890(91)90014-C; LaValle S. M., 2000, RAPIDLY EXPLORING RA; Levine S, 2016, J MACH LEARN RES, V17; Li W., 2004, ICINCO; Mirowski Piotr, 2017, INT C LEARN REPR; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; NISTER D, 2004, PROC CVPR IEEE, P652, DOI DOI 10.1109/CVPR.2004.1315094; Oh J, 2016, PR MACH LEARN RES, V48; Parisotto E., 2018, ICLR; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Ross St<prime>ephane, 2011, AISTATS; Sadeghi F, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII; Savva Manolis, 2017, ARXIV171203931; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Tamar A, 2016, ADV NEURAL INFORM PR, P2146; Thrun S., 2005, PROBABILISTIC ROBOTI; TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626; Toussaint M., 2003, NIPS; Wierstra D, 2010, LOG J IGPL, V18, P620, DOI 10.1093/jigpal/jzp049; Wu Yi, 2018, BUILDING GENERALIZAB; Xia F, 2018, PROC CVPR IEEE, P9068, DOI 10.1109/CVPR.2018.00945; Yamauchi B, 1997, 1997 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION - CIRA '97, PROCEEDINGS, P146, DOI 10.1109/CIRA.1997.613851; Zamir AR, 2016, LECT NOTES COMPUT SC, V9907, P535, DOI 10.1007/978-3-319-46487-9_33; Zhang M., 2016, ICRA	79	6	7	1	23	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1311	1330		10.1007/s11263-019-01236-7	http://dx.doi.org/10.1007/s11263-019-01236-7			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL9ON		Green Submitted			2022-12-18	WOS:000531884600001
J	Li, Q; Sun, ZN; He, R; Tan, TN				Li, Qi; Sun, Zhenan; He, Ran; Tan, Tieniu			A General Framework for Deep Supervised Discrete Hashing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Supervised discrete hashing; l(2) loss; Hinge loss; Alternating minimization method		With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning, deep hashing methods have shown superior performance over the traditional hashing methods. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a general deep supervised discrete hashing framework based on the assumption that the learned binary codes should be ideal for classification. Both the similarity information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithms. Besides, both the pairwise similarity information and the triplet ranking information are exploited in this paper. In addition, two different loss functions are presented: l(2) loss and hinge loss, which are carefully designed for the classification term under the one stream framework. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our approach outperforms current state-of-the-art methods on benchmark datasets.	[Li, Qi; Sun, Zhenan; He, Ran; Tan, Tieniu] CASIA, Natl Lab Pattern Recognit, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; [Li, Qi] Chinese Acad Sci, Artificial Intelligence Res, Qingdao, Peoples R China; [Sun, Zhenan; He, Ran; Tan, Tieniu] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China; [Li, Qi; Sun, Zhenan; He, Ran; Tan, Tieniu] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Sun, ZN (corresponding author), CASIA, Natl Lab Pattern Recognit, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.; Sun, ZN (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.; Sun, ZN (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.	qli@nlpr.ia.ac.cn; Znsun@nlpr.ia.ac.cn; rhe@nlpr.ia.ac.cn; tnt@nlpr.ia.ac.cn		li, qi/0000-0002-7905-2860	Natural Science Foundation of China [U1836217, 61702513, 61721004, 61427811]; Shandong Provincial Key Research and Development Program (Major Scientific and Technological Innovation Project) [2019JZZY010119]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shandong Provincial Key Research and Development Program (Major Scientific and Technological Innovation Project)	This work was partially supported by the Natural Science Foundation of China (Grant Nos. U1836217, 61702513, 61721004, and 61427811). This work was also partially supported by CAS-AIR and Shandong Provincial Key Research and Development Program (Major Scientific and Technological Innovation Project) (No. 2019JZZY010119).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Cao Y, 2016, AAAI CONF ARTIF INTE, P3457; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Cui Y, 2018, PATTERN RECOGN, V78, P79, DOI 10.1016/j.patcog.2018.01.007; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gorisse D, 2012, IEEE T PATTERN ANAL, V34, P402, DOI 10.1109/TPAMI.2011.193; Gui J, 2018, IEEE T NEUR NET LEAR, V29, P608, DOI 10.1109/TNNLS.2016.2636870; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427; Ji Jianqiu, 2012, ADV NEURAL INFORM PR, P108; Kang WC, 2016, AAAI CONF ARTIF INTE, P1230; Koutaki G., 2016, ARXIV161110017; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466; Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; Li K, 2017, IEEE T PATTERN ANAL, V39, P1825, DOI 10.1109/TPAMI.2016.2610969; Li Q, 2017, ADV NEUR IN, V30; Li Q, 2016, PATTERN RECOGN, V59, P142, DOI 10.1016/j.patcog.2016.02.006; Li W., 2016, INT JOINT C ARTIFICI, P1711; Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253; Lin K, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301269; Liong VE, 2020, IEEE T PATTERN ANAL, V42, P580, DOI 10.1109/TPAMI.2018.2882816; Liong VE, 2017, IEEE I CONF COMP VIS, P4097, DOI 10.1109/ICCV.2017.439; Liu B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P755, DOI 10.1145/3240508.3240543; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu WW, 2011, SYST BIODIVERS, V9, P247, DOI 10.1080/14772000.2011.605812; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mu YD, 2010, AAAI CONF ARTIF INTE, P539; Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842; Raginsky M., 2009, ADV NEURAL INFORM PR, P1509, DOI [10.5555/2984093.2984263, DOI 10.5555/2984093.2984263]; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Song JK, 2018, AAAI CONF ARTIF INTE, P394; Su SP, 2018, ADV NEUR IN, V31; Tomar VS, 2013, INTERSPEECH, P1775; Wang J, 2010, MICROOPTICS AND NANOOPTICS FABRICATION, P127; Wang Jianfeng, 2013, P 21 ACM INT C MULTI, P133; Wang J, 2017, IEEE INFOCOM SER, DOI 10.1007/s12083-017-0556-6; Wang JD, 2019, IEEE T PATTERN ANAL, V41, P1308, DOI 10.1109/TPAMI.2018.2835468; Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154; Wang X., 2016, P AS C COMP VIS, P70; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; Yang EK, 2019, PROC CVPR IEEE, P2941, DOI 10.1109/CVPR.2019.00306; Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812; Yao T., 2016, P INT JOINT C ART IN, P3931; Yelamarthi SK, 2018, LECT NOTES COMPUT SC, V11208, P316, DOI 10.1007/978-3-030-01225-0_19; Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12; Zhang HW, 2018, IEEE T INF FOREN SEC, V13, P2409, DOI 10.1109/TIFS.2018.2800901; Zhang J, 2019, IEEE T CIRC SYST VID, V29, P212, DOI 10.1109/TCSVT.2017.2771332; Zhang J, 2018, AAAI CONF ARTIF INTE, P539; Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang ZM, 2016, PROC CVPR IEEE, P1487, DOI 10.1109/CVPR.2016.165; Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763; Zhao X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3511; Zhu H, 2016, AAAI CONF ARTIF INTE, P2415	66	6	6	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2204	2222		10.1007/s11263-020-01327-w	http://dx.doi.org/10.1007/s11263-020-01327-w		APR 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG					2022-12-18	WOS:000558380300001
J	Hackel, T; Usvyatsov, M; Galliani, S; Wegner, JD; Schindler, K				Hackel, Timo; Usvyatsov, Mikhail; Galliani, Silvano; Wegner, Jan D.; Schindler, Konrad			Inference, Learning and Attention Mechanisms that Exploit and Preserve Sparsity in CNNs	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep learning; Sparse convolutions; Voxel grids		Convolutional neural networks (CNNs) are a powerful tool for pattern recognition and computer vision, but they do not scale well to higher-dimensional inputs, because of the associated memory demands for storing and manipulating high-dimensional tensors. This work starts from the observation that higher-dimensional data, like for example 3D voxel volumes, are sparsely populated. CNNs naturally lend themselves to densely sampled data, and sophisticated, massively parallel implementations are available. On the contrary, existing frameworks by and large lack the ability to efficiently process sparse data. Here, we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights of a CNN, and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework, when processing data with a high degree of sparsity. Our scheme provides (i) an efficient GPU implementation of a convolution layer based on direct, sparse convolution, as well as sparse implementations of the ReLU and max-pooling layers; (ii) a filter step within the convolution layer, which we call attention, that prevents fill-in, i.e., the tendency of convolution to rapidly decrease sparsity, and guarantees an upper bound on the computational resources; and (iii) an adaptation of back-propagation that makes it possible to combine our approach with standard learning frameworks, while still benefitting from sparsity in the data as well as the model.	[Hackel, Timo; Usvyatsov, Mikhail; Galliani, Silvano; Wegner, Jan D.; Schindler, Konrad] Swiss Fed Inst Technol, Photogrammetry & Remote Sensing, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Usvyatsov, M (corresponding author), Swiss Fed Inst Technol, Photogrammetry & Remote Sensing, Zurich, Switzerland.	timo.hackel@geod.baug.ethz.ch; mikhail.usvyatsov@geod.baug.ethz.ch; silvano.galliani@geod.baug.ethz.ch; jan.wegner@geod.baug.ethz.ch; konrad.schindler@geod.baug.ethz.ch						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Alabi T., 2012, J EXPT ALGORITHMICS, V17, P4, DOI DOI 10.1145/2133803.2345676; Boulch A, 2019, EUROGRAPHICS 3DOR; Brock A., 2017, ARXIV160804236; Charles R., 2017, ADV NEURAL INFORM PR; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chetlur S., 2014, ARXIV; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Dai A., 2017, CVPR; Denil Misha, 2013, NIPS, DOI DOI 10.5555/2999792.2999852; Dentinel Zarembaw, 2014, NEURIPS, P1269; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Engelcke M., 2016, ICRA; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Graham Benjamin, 2017, ARXIV170601307; Graham Benjamin, 2014, ARXIV14096070; Hackel T., 2018, GCPR; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; Hane Christian, 2017, ARXIV170400710; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang J, 2016, INT C PATT RECOG, P2670, DOI 10.1109/ICPR.2016.7900038; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jampani V., 2016, P IEEE C COMP VIS PA; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lai K., 2014, INT C ROB AUT ICRA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li YY, 2016, ADV NEUR IN, V29; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; NISSEN MJ, 1987, COGNITIVE PSYCHOL, V19, P1, DOI 10.1016/0010-0285(87)90002-8; Parashar A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P27, DOI 10.1145/3079856.3080254; Park Jongsoo, 2017, INT C LEARN REPR; Prokhorov D, 2010, IEEE T NEURAL NETWOR, V21, P858, DOI 10.1109/TNN.2010.2044802; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Riegler G., 2017, ARXIV170401047; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Robertson EM, 2007, J NEUROSCI, V27, P10073, DOI 10.1523/JNEUROSCI.2747-07.2007; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012; Wang P., 2017, INT J ROBUST NONLINE, V4, P1; Wang Peng-Shuai, 2018, ACM T GRAPH SIGGRAPH, V37, P6; Wen W., 2016, ADV NEURAL INFORM PR, P2074; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	50	6	6	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2020	128	4			SI		1047	1059		10.1007/s11263-020-01302-5	http://dx.doi.org/10.1007/s11263-020-01302-5		MAR 2020	13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LC5TN		Green Accepted, Green Submitted			2022-12-18	WOS:000518246900002
J	Tabernik, D; Kristan, M; Leonardis, A				Tabernik, Domen; Kristan, Matej; Leonardis, Ales			Spatially-Adaptive Filter Units for Compact and Efficient Deep Neural Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Compact ConvNets; Efficient ConvNets; Displacement units; Adjustable receptive fields		Convolutional neural networks excel in a number of computer vision tasks. One of their most crucial architectural elements is the effective receptive field size, which has to be manually set to accommodate a specific task. Standard solutions involve large kernels, down/up-sampling and dilated convolutions. These require testing a variety of dilation and down/up-sampling factors and result in non-compact networks and large number of parameters. We address this issue by proposing a new convolution filter composed of displaced aggregation units (DAU). DAUs learn spatial displacements and adapt the receptive field sizes of individual convolution filters to a given problem, thus reducing the need for hand-crafted modifications. DAUs provide a seamless substitution of convolutional filters in existing state-of-the-art architectures, which we demonstrate on AlexNet, ResNet50, ResNet101, DeepLab and SRN-DeblurNet. The benefits of this design are demonstrated on a variety of computer vision tasks and datasets, such as image classification (ILSVRC 2012), semantic segmentation (PASCAL VOC 2011, Cityscape) and blind image de-blurring (GOPRO). Results show that DAUs efficiently allocate parameters resulting in up to 4x more compact networks in terms of the number of parameters at similar or better performance.	[Tabernik, Domen; Kristan, Matej; Leonardis, Ales] Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana, Slovenia; [Leonardis, Ales] Univ Birmingham, Sch Comp Sci, Birmingham, W Midlands, England	University of Ljubljana; University of Birmingham	Tabernik, D (corresponding author), Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana, Slovenia.	domen.tabernik@fri.uni-lj.si; matej.kristan@fri.uni-lj.si; a.leonardis@cs.bham.ac.uk			Slovenian Research Agency ARRS [GOSTOP C3330-16-529000, DIVID J2-9433, ViAMaRo L2-6765, P2-0214]; MURI Project - MoD/Dstl; EPSRC [EP/N019415/1]	Slovenian Research Agency ARRS(Slovenian Research Agency - Slovenia); MURI Project - MoD/Dstl(MURI); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors would like to thank Hector Basevi for his valuable comments and suggestion on improving the paper. This work was supported in part by the following research projects and programs: Project GOSTOP C3330-16-529000, DIVID J2-9433 and ViAMaRo L2-6765, Program P2-0214 financed by Slovenian Research Agency ARRS, and MURI Project financed by MoD/Dstl and EPSRC through EP/N019415/1 Grant. We thank Vitjan Zavrtanik for his contribution in porting the DAUs to the TensorFlow framework.	Amidror I., 2013, MASTERING DISCRETE F; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2014, ICLR; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Chang JL, 2018, ADV NEUR IN, V31; Chen L.-C., 2017, RETHINKING ATROUS CO; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Everingham M., 2012, PASCAL VISUAL OBJECT; Glorot X., 2010, PROC MACH LEARN RES, P249; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI 10.1109/TPAMI.2018.2844175; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Holschneider M., 1990, WAVELETS, P286, DOI [10.1007/978-3-642-75988-8_28, 10.1007/978-3-642-75988-828, DOI 10.1007/978-3-642-75988-8_28]; Jacobsen JH, 2016, PROC CVPR IEEE, P2610, DOI 10.1109/CVPR.2016.286; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luan S., 2017, IEEE T IMAGE PROCESS, V26, P1, DOI [10.1109/TIP.2017.2779545, DOI 10.1109/TIP.2017.2779545]; Luo P, 2017, IEEE I CONF COMP VIS, P2737, DOI 10.1109/ICCV.2017.296; Luo WJ, 2016, ADV NEUR IN, V29; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Navab Nassir, 2015, MED IMAGE COMPUTING; Redmon J., 2016, YOLO9000 BETTER FAST, DOI [10.1109/IJCNN.2015.7280696, DOI 10.1109/IJCNN.2015.7280696]; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Santerne A, 2016, ASTRON ASTROPHYS, V587, DOI 10.1051/0004-6361/201527329; Tabernik D, 2018, PROC CVPR IEEE, P9388, DOI 10.1109/CVPR.2018.00978; Tabernik D, 2016, INT C PATT RECOG, P3470, DOI 10.1109/ICPR.2016.7900171; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75	40	6	6	3	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2020	128	8-9			SI		2049	2067		10.1007/s11263-019-01282-1	http://dx.doi.org/10.1007/s11263-019-01282-1		JAN 2020	19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	MZ8HG		Green Submitted, Green Accepted			2022-12-18	WOS:000505388600001
J	Logothetis, F; Mecca, R; Sgallari, F; Cipolla, R				Logothetis, Fotios; Mecca, Roberto; Sgallari, Fiorella; Cipolla, Roberto			A Differential Approach to Shape from Polarisation: A Level-Set Characterisation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape from polarisation; Linear PDEs; Photometric stereo; Quasi-linear PDEs; Specular reflection; Diffuse reflection; Variational method	PHOTOMETRIC STEREO; RECONSTRUCTION; REFLECTION; DIFFUSE	Despite the longtime research aimed at retrieving geometrical information of an object from polarimetric imaging, physical limitations in the polarisation phenomena constrain current approaches to provide ambiguous depth estimation. As an additional constraint, polarimetric imaging formulation differs when light is reflected off the object specularly or diffusively. This introduces another source of ambiguity that current formulations cannot overcome. With the aim of deriving a formulation capable of dealing with as many heterogeneous effects as possible, we propose a differential formulation of the Shape from Polarisation problem that depends only on polarimetric images. This allows the direct geometrical characterisation of the level-set of the object keeping consistent mathematical formulation for diffuse and specular reflection. We show via synthetic and real-world experiments that diffuse and specular reflection can be easily distinguished in order to extract meaningful geometrical features from just polarimetric imaging. The inherent ambiguity of the Shape from Polarization problem becomes evident through the impossibility of reconstructing the whole surface with this differential approach. To overcome this limitation, we consider shading information elegantly embedding this new formulation into a two-light calibrated photometric stereo approach..	[Logothetis, Fotios; Cipolla, Roberto] Univ Cambridge, Dept Engn, Cambridge, England; [Mecca, Roberto] Ist Italiano Tecnol, Lab Computat & Stat Learning, Genoa, Italy; [Sgallari, Fiorella] Univ Bologna, Dept Math, Bologna, Italy	University of Cambridge; Istituto Italiano di Tecnologia - IIT; University of Bologna	Logothetis, F (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	fl302@cam.ac.uk; roberto.mecca@iit.it; fiorella.sgallari@unibo.it; rc10001@cam.ac.uk		Cipolla, Roberto/0000-0002-8999-2151				Arbelaez P, 2009, PROC CVPR IEEE, P2294, DOI 10.1109/CVPRW.2009.5206707; Atkinson G, 2006, P IEEE C COMP VIS PA, V1, P495; Atkinson GA, 2005, IEEE I CONF COMP VIS, P309; Atkinson GA, 2007, IEEE T PATTERN ANAL, V29, P2001, DOI 10.1109/TPAMI.2007.1099; Atkinson GA, 2007, LECT NOTES COMPUT SC, V4673, P466; Atkinson GA, 2006, IEEE T IMAGE PROCESS, V15, P1653, DOI 10.1109/TIP.2006.871114; Blinn J. F, 1977, C COMP GRAPH INT TEC; BRUCKSTEIN AM, 1988, COMPUT VISION GRAPH, V44, P139, DOI 10.1016/S0734-189X(88)80002-1; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Chandraker M, 2013, IEEE T PATTERN ANAL, V35, P2941, DOI 10.1109/TPAMI.2012.217; Chen CH, 2007, IEEE C EVOL COMPUTAT, P1; DAVIS PA, 1984, J GEOPHYS RES, V89, P9449, DOI 10.1029/JB089iB11p09449; Dollar P., 2013, ICCV; Drbohlav O, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P581, DOI 10.1109/ICCV.2001.937570; Huynh CP, 2013, INT J COMPUT VISION, V101, P64, DOI 10.1007/s11263-012-0546-3; Kadambi A, 2015, IEEE I CONF COMP VIS, P3370, DOI 10.1109/ICCV.2015.385; Koshikawa K., 1979, P 6 INT JOINT C ART, V1, P493; MartinezHerrero R, 2009, SPRINGER SER OPT SCI, V147, P1; Mecca R., 2017, BRIT MACH VIS C BMVC; Mecca R, 2016, SIAM J IMAGING SCI, V9, P1858, DOI 10.1137/16M1068177; Mecca R, 2014, SIAM J IMAGING SCI, V7, P2732, DOI 10.1137/140968100; Mecca R, 2013, SIAM J IMAGING SCI, V6, P616, DOI 10.1137/110857258; Miyazaki D, 2005, PROC CVPR IEEE, P910; Miyazaki D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1381; Miyazaki D, 2004, IEEE T PATTERN ANAL, V26, P73, DOI 10.1109/TPAMI.2004.1261080; Miyazaki D, 2007, IEEE T PATTERN ANAL, V29, P2018, DOI 10.1109/TPAMI.2007.1117; Miyazaki D, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P104, DOI 10.1109/3DIMPVT.2012.14; Morel O, 2006, IEEE IMAGE PROC, P2181, DOI 10.1109/ICIP.2006.312877; Nayar S. K., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P583, DOI 10.1109/CVPR.1993.341071; Papadhimitri T., 2013, C COMP VIS PATT REC; Prados E., 2005, C COMP VIS PATT REC; Rahmann S, 2001, PROC CVPR IEEE, P149; Saito M., 1999, C COMP VIS PATT REC, P386; Smith W, 2016, COMPUT VIS IMAGE UND, V145, P128, DOI 10.1016/j.cviu.2015.11.019; Smith WAP, 2016, LECT NOTES COMPUT SC, V9912, P109, DOI 10.1007/978-3-319-46484-8_7; Taamazyan Vage, 2016, ARXIV160502066; Tankus A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P862; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Tozza S., 2017, INT C COMP VIS ICCV; Ngo TT, 2015, PROC CVPR IEEE, P2310, DOI 10.1109/CVPR.2015.7298844; Umeyama S, 2004, IEEE T PATTERN ANAL, V26, P639, DOI 10.1109/TPAMI.2004.1273960; Wang F, 2016, IEEE IMAGE PROC, P1983, DOI 10.1109/ICIP.2016.7532705; WOLFF LB, 1991, IEEE T PATTERN ANAL, V13, P635, DOI 10.1109/34.85655; WOLFF LB, 1994, J OPT SOC AM A, V11, P2935, DOI 10.1364/JOSAA.11.002935; WOLFF LB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P1000, DOI 10.1109/CVPR.1994.323942; Yu Y., 2017, INT C COMP VIS WORKS; Zhu Q, 2006, P 2006 IEEE COMP SOC, V2, P1839	47	6	6	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1680	1693		10.1007/s11263-018-1127-x	http://dx.doi.org/10.1007/s11263-018-1127-x			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY		Green Published, hybrid			2022-12-18	WOS:000492425300006
J	Ngo, TT; Nagahara, H; Nishino, K; Taniguchi, R; Yagi, Y				Thanh-Trung Ngo; Nagahara, Hajime; Nishino, Ko; Taniguchi, Rin-ichiro; Yagi, Yasushi			Reflectance and Shape Estimation with a Light Field Camera Under Natural Illumination	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Light field camera; Natural illumination; Reflectance; Shape from shading	PHOTOMETRIC STEREO; RECONSTRUCTION	Reflectance and shape are two important components in visually perceiving the real world. Inferring the reflectance and shape of an object through cameras is a fundamental research topic in the field of computer vision. While three-dimensional shape recovery is pervasive with varieties of approaches and practical applications, reflectance recovery has only emerged recently. Reflectance recovery is a challenging task that is usually conducted in controlled environments, such as a laboratory environment with a special apparatus. However, it is desirable that the reflectance be recovered in the field with a handy camera so that reflectance can be jointly recovered with the shape. To that end, we present a solution that simultaneously recovers the reflectance and shape (i.e., dense depth and normal maps) of an object under natural illumination with commercially available handy cameras. We employ a light field camera to capture one light field image of the object, and a 360-degree camera to capture the illumination. The proposed method provides positive results in both simulation and real-world experiments.	[Thanh-Trung Ngo; Nagahara, Hajime; Yagi, Yasushi] Osaka Univ, Osaka, Japan; [Nishino, Ko] Kyoto Univ, Kyoto, Japan; [Taniguchi, Rin-ichiro] Kyshu Univ, Fukuoka, Fukuoka, Japan	Osaka University; Kyoto University; Kyushu University	Ngo, TT (corresponding author), Osaka Univ, Osaka, Japan.	trung@am.sanken.osaka-u.ac.jp; nagahara@ids.osaka-u.ac.jp; kon@i.kyoto-u.ac.jp; rin@kyudai.jp; yagi@am.sanken.osaka-u.ac.jp			Japan Society for the Promotion of Science [JP16H01675]	Japan Society for the Promotion of Science(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	We thank anonymous reviewers for their suggestions on how to improve the quality of the manuscript and for an interesting discussion about a future work. We thank Glenn Pennycook, MSc, from Edanz Group (www.edanzediting.com/ac) for editing a draft of this manuscript. Funding was provided by Japan Society for the Promotion of Science (Grant No. JP16H01675).	Abrams A, 2012, LECT NOTES COMPUT SC, V7573, P357, DOI 10.1007/978-3-642-33709-3_26; Ackermann J, 2012, PROC CVPR IEEE, P262, DOI 10.1109/CVPR.2012.6247684; Alldrin N., 2008, IEEE C COMP VIS PATT, P1; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Ben-Ezra M., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587766.17; Delaunoy A, 2014, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2014.193; Dror RO, 2001, P SOC PHOTO-OPT INS, V4299, P231, DOI 10.1117/12.429494; Godard C, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P19, DOI 10.1109/3DV.2015.10; Goldman DB, 2005, IEEE I CONF COMP VIS, P341; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Hertzmann A, 2005, IEEE T PATTERN ANAL, V27, P1254, DOI 10.1109/TPAMI.2005.158; Higo T, 2010, PROC CVPR IEEE, P1157, DOI 10.1109/CVPR.2010.5540084; Hold-Geoffroy Y., 2015, P IEEE INT C COMP PH, P1, DOI DOI 10.1109/ICCPHOT.2015.7168379; Huang R, 2011, IEEE IMAGE PROC, P13, DOI 10.1109/ICIP.2011.6115701; Hui Z, 2017, IEEE T PATTERN ANAL, V39, P2060, DOI 10.1109/TPAMI.2016.2623613; Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762; Johnson M. K., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2553, DOI 10.1109/CVPR.2011.5995510; Jung J, 2015, PROC CVPR IEEE, P4521, DOI 10.1109/CVPR.2015.7299082; Kolmogorov V, 2014, IMAGE PROCESS ON LIN, V4, P220, DOI 10.5201/ipol.2014.97; Lombardi S, 2016, IEEE T PATTERN ANAL, V38, P129, DOI 10.1109/TPAMI.2015.2430318; Marschner SR, 2000, APPL OPTICS, V39, P2592, DOI 10.1364/AO.39.002592; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; Mukaigawa Y, 2011, LECT NOTES COMPUT SC, V6492, P336, DOI 10.1007/978-3-642-19315-6_26; Ngo T., 2017, 28 BRIT MACH VIS C; Nishino K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P599, DOI 10.1109/ICCV.2001.937573; Nishino K, 2011, J OPT SOC AM A, V28, P8, DOI 10.1364/JOSAA.28.000008; Oren M., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P239, DOI 10.1145/192161.192213; Oxholm G, 2016, IEEE T PATTERN ANAL, V38, P376, DOI 10.1109/TPAMI.2015.2450734; Oxholm G, 2014, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2014.277; Oxholm G, 2012, LECT NOTES COMPUT SC, V7572, P528, DOI 10.1007/978-3-642-33718-5_38; Patow G, 2003, COMPUT GRAPH FORUM, V22, P663, DOI 10.1111/j.1467-8659.2003.00716.x; Pharr M., 2010, PHYS BASED RENDERING; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; Ricoh Company, 2016, THET S CAM; Rusinkiewicz S. M., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P11; Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462; Smith WAP, 2016, LECT NOTES COMPUT SC, V9912, P109, DOI 10.1007/978-3-319-46484-8_7; Szeliski R., 2010, COMPUTER VISION ALGO, DOI DOI 10.1007/978-3-030-34372-9; Tao M. W., 2014, ECCV WORKSH LIGHT FI, P1; Tao MW, 2015, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2015.7298804; Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Ngo TT, 2015, PROC CVPR IEEE, P2310, DOI 10.1109/CVPR.2015.7298844; USC Institute for Creative Technologies, 2008, HIGH RES LIGHT PROB; Van der Jeught S, 2016, OPT LASER ENG, V87, P18, DOI 10.1016/j.optlaseng.2016.01.011; Wang TC, 2016, PROC CVPR IEEE, P5451, DOI 10.1109/CVPR.2016.588; Wanner S, 2012, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2012.6247656; Wolff L. B., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P40; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Xia R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980248; Yu LF, 2013, IEEE INT CONF COMPUT, DOI 10.1109/ICCPhot.2013.6528306; Zhou CY, 2011, IEEE T IMAGE PROCESS, V20, P3322, DOI 10.1109/TIP.2011.2171700; Zhou ZL, 2013, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR.2013.195	54	6	6	2	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2019	127	11-12			SI		1707	1722		10.1007/s11263-019-01149-5	http://dx.doi.org/10.1007/s11263-019-01149-5			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	JG9VY					2022-12-18	WOS:000492425300008
J	Zhao, H; Lu, M; Yao, AB; Chen, YR; Zhang, L				Zhao, Hao; Lu, Ming; Yao, Anbang; Chen, Yurong; Zhang, Li			Learning to Draw Sight Lines	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene understanding; Gaze following; Dataset biases; Convolutional neural network (CNN)	VISUAL-ATTENTION; GAZE; TRACKING; MODEL	In this paper, we are concerned with the task of gaze following. Given a scene (e.g. a girl playing soccer on the field) and a human subject's head position, this task aims to infer where she is looking (e.g. at the soccer ball). An existing method adopts a saliency model conditioned on the head position. However, this methodology is intrinsically troubled with dataset bias issues, which we will reveal in detail. In order to resolve these issues, we argue that the right methodology is to simulate how human beings follow gazes. Specifically, we propose the hypothesis that a human follows gazes by searching for salient objects along the subject's sight line direction. To algorithmically embody this hypothesis, a two-stage method is proposed, which is dubbed as learning to draw sight lines. In the first stage, a fully convolutional network is trained to directly regress the existence strength of sight lines. It may seem counterintuitive at a first glance as these so-called sight lines do not really exist in the form of learnable image gradients. However, with the large-scale dataset GazeFollow, we demonstrate that this highly abstract concept can be grounded into neural network activations. An extensive study is conducted on the design of this sight line grounding network. We show that the best model we visited can already outperform the state-of-the-arts by a large margin, using a naive greedy inference strategy. We attribute these improvements to modern architecture design philosophies. However, no matter how strong the sight line grounding network is, the greedy inference strategy cannot handle a bunch of failure cases caused by dataset bias issues. We identify these issues and demonstrate that those grounded sight lines, which is a unique ingredient of our method, is the key to overcome them. Specifically, an algorithm termed as RASP is introduced as a second stage. RASP has five intriguing features: (1) it explicitly embodies the aforementioned hypothesis; (2) it involves no hyper-parameters, thus guaranteeing its robustness; (3) if needed, it can be implemented as an integrated layer for end-to-end inference; (4) it improves the performances of all sight line grounding networks we inspected; (5) further analyses confirm that RASP works by alleviating those spotted dataset biases. Strong results are achieved on the GazeFollow benchmark. Combining RASP and the best sight line grounding network can bring mean distance, minimum distance and mean angle difference 45.85%, 42.60%, and 49.23% closer towards human performance when compared with state-of-the-arts. We also contribute a video gaze following benchmark called GazeShift, on which we further demonstrate the importance of RASP in video applications. Codes and models will be released, encouraging further research on the important task of gaze following. Along with our implementation, we contribute a well-engineered toolbox for joint subject tracking and gaze following.	[Zhao, Hao; Lu, Ming; Zhang, Li] Tsinghua Univ, Beijing, Peoples R China; [Yao, Anbang; Chen, Yurong] Intel Labs China, Beijing, Peoples R China	Tsinghua University; Intel Corporation	Zhao, H (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	zhao-h13@mails.tsinghua.edu.cn; lu-m13@mails.tsinghua.edu.cn; anbang.yao@intel.com; yurong.chen@intel.com; chinazhangli@mail.tsinghua.edu.cn			National Natural Science Foundation of China [61132007, 61172125, U1533132]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We thank anonymous reviewers for suggestions on literature review and experimental design. This work was jointly supported by National Natural Science Foundation of China (Grant Nos. 61132007, 61172125 and U1533132).	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Agrawal Aishwarya, 2017, ARXIV171200377; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, P CVPR; [Anonymous], 2017, P IEEE C COMP VIS PA; [Anonymous], 2017, CVPR; [Anonymous], 2006, NIPS; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833; Breitenstein MD, 2008, PROC CVPR IEEE, P3613; Brooks R, 2005, DEVELOPMENTAL SCI, V8, P535, DOI 10.1111/j.1467-7687.2005.00445.x; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Chong E., 2018, ECCV, P383; Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan LF, 2018, PROC CVPR IEEE, P6460, DOI 10.1109/CVPR.2018.00676; Flom R, 2004, INFANT BEHAV DEV, V27, P181, DOI 10.1016/j.infbeh.2003.09.007; Fouhey DF, 2014, INT J COMPUT VISION, V110, P259, DOI 10.1007/s11263-014-0710-z; Funes Mora K.A., 2014, P S EYE TRACK RES AP, P255; Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hoiem D, 2005, IEEE I CONF COMP VIS, P654; Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Judd T, 2009, 2009 IEEE 12 INT C C; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Y, 2013, IEEE I CONF COMP VIS, P3216, DOI 10.1109/ICCV.2013.399; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Li YQ, 2015, INT CONF ASIC; Liang Q, 2016, ADV BIO SCI RES, V3, P316; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu F, 2014, IEEE T PATTERN ANAL, V36, P2033, DOI 10.1109/TPAMI.2014.2313123; Lu F, 2011, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2011.6126237; Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754; Lu M, 2017, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2017.270; Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3; Lv CG, 2009, ADV INTEL SYS RES, P681; Mallya A, 2015, IEEE I CONF COMP VIS, P936, DOI 10.1109/ICCV.2015.113; Marin-Jimenez MJ, 2014, INT J COMPUT VISION, V106, P282, DOI 10.1007/s11263-013-0655-7; Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154; Mathe S, 2012, LECT NOTES COMPUT SC, V7573, P842, DOI 10.1007/978-3-642-33709-3_60; Morimoto CH, 2005, COMPUT VIS IMAGE UND, V98, P4, DOI 10.1016/j.cviu.2004.07.010; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Ping Wei, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6801, DOI 10.1109/CVPR.2018.00711; Recasens A., 2017, IEEE INT C COMP VIS, V4; Recasens Adria, 2015, NIPS; Rehg JM, 2013, PROC CVPR IEEE, P3414, DOI 10.1109/CVPR.2013.438; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schneider T, 2014, INT C PATT RECOG, P1167, DOI 10.1109/ICPR.2014.210; Schwing AG, 2012, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2012.6248006; Senju A, 2008, CURR BIOL, V18, P668, DOI 10.1016/j.cub.2008.03.059; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Sugano Y, 2008, LECT NOTES COMPUT SC, V5304, P656, DOI 10.1007/978-3-540-88690-7_49; Sugano Y, 2014, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2014.235; Wood E, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P131, DOI 10.1145/2857491.2857492; Wu Z, 2016, CORR, P1; Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153; Yao A., 2018, US Patent App, Patent No. [15/573,631, 15573631]; Yu Fisher, 2016, MULTISCALE CONTEXT A; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32; Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30; Zhang XC, 2015, PROC CVPR IEEE, P4511, DOI 10.1109/CVPR.2015.7299081; Zhao YB, 2013, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2013.401; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	76	6	6	2	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2020	128	5					1076	1100		10.1007/s11263-019-01263-4	http://dx.doi.org/10.1007/s11263-019-01263-4		NOV 2019	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	LL3BW					2022-12-18	WOS:000495960600001
J	Schuster, R; Wasenmuller, O; Unger, C; Kuschk, G; Stricker, D				Schuster, Rene; Wasenmueller, Oliver; Unger, Christian; Kuschk, Georg; Stricker, Didier			SceneFlowFields plus plus : Multi-frame Matching, Visibility Prediction, and Robust Interpolation for Scene Flow Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Scene flow; Matching; Occlusions; Interpolation		State-of-the-art scene flow algorithms pursue the conflicting targets of accuracy, run time, and robustness. With the successful concept of pixel-wise matching and sparse-to-dense interpolation, we shift the operating point in this field of conflicts towards universality and speed. Avoiding strong assumptions on the domain or the problem yields a more robust algorithm. This algorithm is fast because we avoid explicit regularization during matching, which allows an efficient computation. Using image information from multiple time steps and explicit visibility prediction based on previous results, we achieve competitive performances on different data sets. Our contributions and results are evaluated in comparative experiments. Overall, we present an accurate scene flow algorithm that is faster and more generic than any individual benchmark leader.	[Schuster, Rene; Wasenmueller, Oliver; Stricker, Didier] DFKI German Res Ctr Artificial Intelligence, Kaiserslautern, Germany; [Unger, Christian; Kuschk, Georg] BMW Grp, Munich, Germany	German Research Center for Artificial Intelligence (DFKI); BMW AG	Schuster, R (corresponding author), DFKI German Res Ctr Artificial Intelligence, Kaiserslautern, Germany.	rene.schuster@dfki.de; oliver.wasenmuller@dfki.de; christian.unger@bmw.de; g.kuschk@gmx.de; didier.stricker@dfki.de	Wasenmüller, Oliver/AAF-6850-2021					Bailer C., 2015, INT C COMP VIS ICCV; Bailer C, 2019, IEEE T PATTERN ANAL, V41, P1879, DOI 10.1109/TPAMI.2018.2859970; Basha T, 2013, INT J COMPUT VISION, V101, P6, DOI 10.1007/s11263-012-0542-7; Behl A., 2017, C COMP VIS PATT REC; Brox T., 2004, EUR C COMP VIS ECCV; Butler D. J., 2012, EUR C COMP VIS ECCV; Chen Q, 2016, C COMP VIS PATT REC; Dollar Piotr, 2013, INT C COMP VIS ICCV; Gaidon A., 2016, C COMP VIS PATT REC; Geiger A., 2012, C COMP VIS PATT REC; He K., 2012, C COMP VIS PATT REC; Herbst E., 2013, INT C ROB AUT ICRA; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hu Yinlin, 2016, C COMP VIS PATT REC; Hu Yinlin, 2017, C COMP VIS PATT REC; Huguet F., 2007, INT C COMP VIS ICCV; Jaimez M., 2015, INT C ROB AUT ICRA; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Lv F., 2018, P EUR C COMP VIS ECC, DOI [10.1109/icce53296.2022.9730769, DOI 10.1109/ICCE53296.2022.9730769]; Lv Z., 2016, EUR C COMP VIS ECCV; Ma W. C., 2019, C COMP VIS PATT REC; Martin D., 2001, INT C COMP VIS ICCV; Mayer N., 2016, C COMP VIS PATT REC; Menze M., 2015, C COMP VIS PATT REC; Menze M, 2018, ISPRS J PHOTOGRAMM, V140, P60, DOI 10.1016/j.isprsjprs.2017.09.013; Missbach C, 2014, ELIFE, V3, DOI 10.7554/eLife.02115; Neoral M., 2017, COMP VIS WINT WORKSH; Oep A., 2016, INT C ROB AUT ICRA; Ren Z., 2017, INT C 3D VIS 3DV; Revaud J., 2015, C COMP VIS PATT REC; Ros G., 2015, WINT C APPL COMP VIS; Saxena R., 2019, INT VEH S 4; Sbeiti M, 2013, INT CONF COMPUT NETW; Schuster R., 2018, WINT C APPL COMP VIS WINT C APPL COMP VIS; Schuster R., 2018, COMM VEH TECHN S CVT; Schuster Rene, 2018, INT C IM PROC ICIP; Taniai T., 2017, C COMP VIS PATT REC; Vedula S., 1999, INT C COMP VIS ICCV; Vogel C, 2015, INT J COMPUT VISION, V115, P1, DOI 10.1007/s11263-015-0806-0; Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083; Wannenwetsch Anne S., 2017, INT C COMP VIS ICCV; Wedel A., 2008, EUR C COMP VIS ECCV; Xu P, 2016, MACH VISION APPL, V27, P331, DOI 10.1007/s00138-014-0649-7; Yamaguchi K., 2014, EUR C COMP VIS ECCV; Yoshida T., 2017, INT C IM PROC ICIP; Zweig Shay, 2017, C COMP VIS PATT REC	46	6	6	2	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					527	546		10.1007/s11263-019-01258-1	http://dx.doi.org/10.1007/s11263-019-01258-1		NOV 2019	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KJ1GX		Green Submitted			2022-12-18	WOS:000495044700001
J	Hu, WM; Shi, XC; Zhou, ZW; Xing, JL; Ling, HB; Maybank, S				Hu, Weiming; Shi, Xinchu; Zhou, Zongwei; Xing, Junliang; Ling, Haibin; Maybank, Stephen			Dual L-1-Normalized Context Aware Tensor Power Iteration and Its Applications to Multi-object Tracking and Multi-graph Matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-dimensional assignment; Context; hyper-context aware tensor power iteration; Multi-object tracking; Multi-graph matching	ASSIGNMENT ALGORITHM; MULTITARGET TRACKING; MODEL	The multi-dimensional assignment problem is universal for data association analysis such as data association-based visual multi-object tracking and multi-graph matching. In this paper, multi-dimensional assignment is formulated as a rank-1 tensor approximation problem. A dual L-1-normalized context/hyper-context aware tensor power iteration optimization method is proposed. The method is applied to multi-object tracking and multi-graph matching. In the optimization method, tensor power iteration with the dual unit norm enables the capture of information across multiple sample sets. Interactions between sample associations are modeled as contexts or hyper-contexts which are combined with the global affinity into a unified optimization. The optimization is flexible for accommodating various types of contextual models. In multi-object tracking, the global affinity is defined according to the appearance similarity between objects detected in different frames. Interactions between objects are modeled as motion contexts which are encoded into the global association optimization. The tracking method integrates high order motion information and high order appearance variation. The multi-graph matching method carries out matching over graph vertices and structure matching over graph edges simultaneously. The matching consistency across multi-graphs is based on the high-order tensor optimization. Various types of vertex affinities and edge/hyper-edge affinities are flexibly integrated. Experiments on several public datasets, such as the MOT16 challenge benchmark, validate the effectiveness of the proposed methods.	[Hu, Weiming; Shi, Xinchu; Zhou, Zongwei; Xing, Junliang] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Hu, Weiming; Shi, Xinchu; Zhou, Zongwei; Xing, Junliang] CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing, Peoples R China; [Hu, Weiming; Shi, Xinchu; Zhou, Zongwei; Xing, Junliang] Univ Chinese Acad Sci, Beijing 100190, Peoples R China; [Ling, Haibin] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA; [Maybank, Stephen] Birkbeck Coll, Dept Comp Sci & Informat Syst, Malet St, London WC1E 7HX, England	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; University of London; Birkbeck University London	Hu, WM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.; Hu, WM (corresponding author), CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing, Peoples R China.; Hu, WM (corresponding author), Univ Chinese Acad Sci, Beijing 100190, Peoples R China.	wmhu@nlpr.ia.ac.cn; xcshi@nlpr.ia.ac.cn; zwzhou@nlpr.ia.ac.cn; jlxing@nlpr.ia.ac.cn; hling@cs.stonybrook.edu; sjmaybank@dcs.bbk.ac.uk	Xing, Junliang/HGE-9630-2022	Xing, Junliang/0000-0001-6801-0510	NSFC [U1636218]; Natural Science Foundation of China [61751212, 61721004, 61772225]; Beijing Natural Science Foundation [L172051]; Key Research Program of Frontier Sciences, CAS [QYZDJ-SSW-JSC040]; CAS External cooperation key project; National Natural Science Foundation of Guangdong [2018B030311046]	NSFC(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Key Research Program of Frontier Sciences, CAS; CAS External cooperation key project; National Natural Science Foundation of Guangdong(National Natural Science Foundation of Guangdong Province)	This work is supported by the NSFC-general technology collaborative Fund for basic research (Grant No. U1636218), the Natural Science Foundation of China (Grant Nos. 61751212, 61721004, 61772225), Beijing Natural Science Foundation (Grant No. L172051), the Key Research Program of Frontier Sciences, CAS, Grant No. QYZDJ-SSW-JSC040, the CAS External cooperation key project, and the National Natural Science Foundation of Guangdong (No. 2018B030311046).	Ali S., 2007, P IEEE C COMP VIS PA, DOI [10.1109/CVPR.2007.382977, DOI 10.1109/CVPR.2007.382977]; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Benfold B, 2011, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2011.6126516; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003; Bochinski E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS); Breitenstein M., 2010, IEEE T PAMI, V33, P1820, DOI DOI 10.1109/TPAMI.2010.232; Brendel W., 2011, P IEEE C COMP VIS PA, P1820; Buschmann M, 2016, J HIGH ENERGY PHYS, DOI 10.1007/JHEP09(2016)036; Cho MS, 2013, IEEE I CONF COMP VIS, P25, DOI 10.1109/ICCV.2013.11; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347; Collins RT, 2012, PROC CVPR IEEE, P1744, DOI 10.1109/CVPR.2012.6247870; Cour T., 2007, P ADV NEURAL INFORM, P313; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995; Deb S, 1997, IEEE T AERO ELEC SYS, V33, P523, DOI 10.1109/7.575891; Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110; Fang K, 2018, IEEE WINT CONF APPL, P466, DOI 10.1109/WACV.2018.00057; Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282; Huang C, 2008, LECT NOTES COMPUT SC, V5303, P788, DOI 10.1007/978-3-540-88688-4_58; Khan Z, 2005, IEEE T PATTERN ANAL, V27, P1805, DOI 10.1109/TPAMI.2005.223; Lee B, 2016, LECT NOTES COMPUT SC, V9914, P68, DOI 10.1007/978-3-319-48881-3_6; Lee J, 2011, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2011.5995387; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Leordeanu M, 2011, IEEE I CONF COMP VIS, P2274, DOI 10.1109/ICCV.2011.6126507; Leordeanu Marius, 2009, ADV NEURAL INFORM PR; Liu ZY, 2014, INT J COMPUT VISION, V109, P169, DOI 10.1007/s11263-014-0707-7; Luber M, 2010, IEEE INT CONF ROBOT, P464, DOI 10.1109/ROBOT.2010.5509779; Pachauri D., 2013, ADV NEURAL INFORM PR, V26, P1860; Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260; Pellegrini S, 2010, LECT NOTES COMPUT SC, V6311, P452, DOI 10.1007/978-3-642-15549-9_33; Ngoc QN, 2015, PROC CVPR IEEE, P5270, DOI 10.1109/CVPR.2015.7299164; Regalia PA, 2000, INT CONF ACOUST SPEE, P2709, DOI 10.1109/ICASSP.2000.861047; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Scovanner P, 2009, IEEE I CONF COMP VIS, P381, DOI 10.1109/ICCV.2009.5459224; Shafique K, 2008, PROC CVPR IEEE, P1825; Shi XC, 2016, PROC CVPR IEEE, P5062, DOI 10.1109/CVPR.2016.547; Shi XC, 2014, PROC CVPR IEEE, P3518, DOI 10.1109/CVPR.2014.450; Sole-Ribalta A, 2013, INT J PATTERN RECOGN, V27, DOI 10.1142/S0218001413500018; Sole-Ribalta A, 2011, COMPUT VIS IMAGE UND, V115, P929, DOI 10.1016/j.cviu.2010.12.007; Tang S., 2016, P IEEE C COMP VIS PA, P3539; Wojke N, 2017, IEEE IMAGE PROC, P3645; Yamaguchi Kota, 2011, IEEE C COMP VIS PATT, P2, DOI DOI 10.1109/CVPR.2011.5995468; Yan JC, 2013, IEEE I CONF COMP VIS, P1649, DOI 10.1109/ICCV.2013.207; Yan JC, 2016, IEEE T PATTERN ANAL, V38, P1228, DOI 10.1109/TPAMI.2015.2477832; Yan JC, 2015, IEEE T IMAGE PROCESS, V24, P994, DOI 10.1109/TIP.2014.2387386; Yan JC, 2014, LECT NOTES COMPUT SC, V8689, P407, DOI 10.1007/978-3-319-10590-1_27; Yang B, 2012, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR.2012.6247892; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zass R, 2008, PROC CVPR IEEE, P1221; Zeng Y, 2010, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2010.5540189; Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1774, DOI 10.1109/TPAMI.2015.2501802	65	6	6	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2020	128	2					360	392		10.1007/s11263-019-01231-y	http://dx.doi.org/10.1007/s11263-019-01231-y		OCT 2019	33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	KJ1GX		hybrid, Green Accepted			2022-12-18	WOS:000491042100001
J	Runia, TFH; Snoek, CGM; Smeulders, AWM				Runia, Tom F. H.; Snoek, Cees G. M.; Smeulders, Arnold W. M.			Repetition Estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video analysis; Motion; Periodicity; Repetition counting; Wavelet transform; Motion segmentation	PERIODIC MOTION DETECTION; SEGMENTATION; RECOGNITION	Visual repetition is ubiquitous in our world. It appears in human activity (sports, cooking), animal behavior (a bee's waggle dance), natural phenomena (leaves in the wind) and in urban environments (flashing lights). Estimating visual repetition from realistic video is challenging as periodic motion is rarely perfectly static and stationary. To better deal with realistic video, we elevate the static and stationary assumptions often made by existing work. Our spatiotemporal filtering approach, established on the theory of periodic motion, effectively handles a wide variety of appearances and requires no learning. Starting from motion in 3D we derive three periodic motion types by decomposition of the motion field into its fundamental components. In addition, three temporal motion continuities emerge from the field's temporal dynamics. For the 2D perception of 3D motion we consider the viewpoint relative to the motion; what follows are 18 cases of recurrent motion perception. To estimate repetition under all circumstances, our theory implies constructing a mixture of differential motion maps: F, del F, del<bold>F and </bold>del xF. We temporally convolve the motion maps with wavelet filters to estimate repetitive dynamics. Our method is able to spatially segment repetitive motion directly from the temporal filter responses densely computed over the motion maps. For experimental verification of our claims, we use our novel dataset for repetition estimation, better-reflecting reality with non-static and non-stationary repetitive motion. On the task of repetition counting, we obtain favorable results compared to a deep learning alternative.	[Runia, Tom F. H.; Snoek, Cees G. M.; Smeulders, Arnold W. M.] Univ Amsterdam, QUVA Deep Vis Lab, Sci Pk 904, NL-1098 XH Amsterdam, Netherlands	University of Amsterdam	Runia, TFH (corresponding author), Univ Amsterdam, QUVA Deep Vis Lab, Sci Pk 904, NL-1098 XH Amsterdam, Netherlands.	runia@uva.nl						Abraham R., 2012, MANIFOLDS TENSOR ANA, V75; Adelson E. H., 2001, HUMAN VISION ELECT I, V4299; Albu AB, 2008, PATTERN RECOGN, V41, P6, DOI 10.1016/j.patcog.2007.03.013; Azy O, 2008, INT C PATT RECOG, P5; Belongie S, 2006, SPATIAL COHERENCE VI; Briassouli A, 2007, IEEE T PATTERN ANAL, V29, P1244, DOI 10.1109/TPAMI.2007.1042.; Burghouts GJ, 2006, IEEE T IMAGE PROCESS, V15, P1572, DOI 10.1109/TIP.2005.864234; Chetverikov D., 2006, PROC BRIT MACH VIS C, P167; Cutler R, 2000, IEEE T PATTERN ANAL, V22, P781, DOI 10.1109/34.868681; Davis J, 2000, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2000.855878; Goldenberg R, 2005, PATTERN RECOGN, V38, P1033, DOI 10.1016/j.patcog.2004.11.024; GROSSMANN A, 1984, SIAM J MATH ANAL, V15, P723, DOI 10.1137/0515056; Henriques J., 2012, P EUR C COMP VIS; Huang S, 2016, IEEE IPCCC; Ilg E., 2017, P IEEE C COMP VIS PA; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Klaser A., 2008, P BMVC 2008 19 BRIT, P275; KOENDERINK JJ, 1975, OPT ACTA, V22, P773, DOI 10.1080/713819112; KOENDERINK JJ, 1988, BIOL CYBERN, V58, P159, DOI 10.1007/BF00364135; Laptev I, 2005, IEEE I CONF COMP VIS, P816; Levy O., 2015, P IEEE INT C COMP VI; Li X, 2018, PROC CVPR IEEE, P3032, DOI 10.1109/CVPR.2018.00320; Lindeberg T, 2017, J IMAGING SCI, V11, p438 ; Liu F, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P376, DOI 10.1109/ICCV.1998.710746; Lu CM, 2004, IEEE T PATTERN ANAL, V26, P258, DOI 10.1109/TPAMI.2004.1262196; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223; Pogalin E., 2008, P IEEE C COMP VIS PA; Polana R, 1997, INT J COMPUT VISION, V23, P261, DOI 10.1023/A:1007975200487; Ran Y, 2007, INT J COMPUT VISION, V71, P143, DOI 10.1007/s11263-006-8575-4; Revaud J., 2015, P IEEE C COMP VIS PA; Runia T.F.H., 2018, P IEEE C COMP VIS PA; Sarel B., 2005, P IEEE INT C COMP VI; Thangali A., 2005, P IEEE WORKSH APPL C; Tokmakov P., 2017, P IEEE C COMP VIS PA; Torrence C, 1998, B AM METEOROL SOC, V79, P61, DOI 10.1175/1520-0477(1998)079<0061:APGTWA>2.0.CO;2; Tralie CJ, 2018, SIAM J IMAGING SCI, V11, P1049, DOI 10.1137/17M1150736; TSAI PS, 1994, PATTERN RECOGN, V27, P1591, DOI 10.1016/0031-3203(94)90079-5; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zhou TH, 2006, INT C COMMUN CIRCUIT, P2009	41	6	6	3	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2019	127	9					1361	1383		10.1007/s11263-019-01194-0	http://dx.doi.org/10.1007/s11263-019-01194-0			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	IL9YV		Green Published, hybrid			2022-12-18	WOS:000477642300010
J	Mettes, P; Snoek, CGM				Mettes, Pascal; Snoek, Cees G. M.			Pointly-Supervised Action Localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Action localization; Point supervision; Spatio-temporal proposals		This paper strives for spatio-temporal localization of human actions in videos. In the literature, the consensus is to achieve localization by training on bounding box annotations provided for each frame of each training video. As annotating boxes in video is expensive, cumbersome and error-prone, we propose to bypass box-supervision. Instead, we introduce action localization based on point-supervision. We start from unsupervised spatio-temporal proposals, which provide a set of candidate regions in videos. While normally used exclusively for inference, we show spatio-temporal proposals can also be leveraged during training when guided by a sparse set of point annotations. We introduce an overlap measure between points and spatio-temporal proposals and incorporate them all into a new objective of a multiple instance learning optimization. During inference, we introduce pseudo-points, visual cues from videos, that automatically guide the selection of spatio-temporal proposals. We outline five spatial and one temporal pseudo-point, as well as a measure to best leverage pseudo-points at test time. Experimental evaluation on three action localization datasets shows our pointly-supervised approach (1) is as effective as traditional box-supervision at a fraction of the annotation cost, (2) is robust to sparse and noisy point annotations, (3) benefits from pseudo-points during inference, and (4) outperforms recent weakly-supervised alternatives. This leads us to conclude that points provide a viable alternative to boxes for action localization.	[Mettes, Pascal; Snoek, Cees G. M.] Univ Amsterdam, Amsterdam, Netherlands	University of Amsterdam	Mettes, P (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	P.S.M.Mettes@uva.nl; cgmsnoek@uva.nl			Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) [D17PC00343]	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC)	This work is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00343. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government.	Andrews S., 2002, NIPS, V2, P561; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Chen W., 2015, INT C COMP VIS; Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Dollar P., 2005, P IEEE INT WORKSH VI, P65, DOI [DOI 10.1109/VSPETS.2005.1570899, 10.1109/VSPETS.2005.1570899]; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Fernando B, 2017, IEEE COMPUT SOC CONF, P1604, DOI 10.1109/CVPRW.2017.205; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; Hoiem Derek, 2012, ECCV, P340, DOI [10.1007/978-3-642-33712-3_25, DOI 10.1007/978-3-642-33712-3_25]; Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620; Jain M., 2014, P IEEE C COMP VIS PA; Jain M, 2017, INT J COMPUT VISION, V124, P287, DOI 10.1007/s11263-017-1023-9; Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Jain S., 2016, P AAAI C HUM COMP CR, V4, P89; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Kalogeiton V., 2017, INT C COMP VIS; Lan T, 2011, IEEE I CONF COMP VIS, P2003, DOI 10.1109/ICCV.2011.6126472; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Manen S, 2017, IEEE I CONF COMP VIS, P290, DOI 10.1109/ICCV.2017.40; MarianPuscas M., 2015, INT C COMP VIS; Marszaek M., 2009, P IEEE C COMP VIS PA; Mettes P., 2017, BRIT MACH VIS C; Mettes P., 2017, INT C COMP VIS; Mettes Pascal, 2016, ECCV; Mikolov T., 2013, WORKSH 26 AAAI C ART; Oneata D, 2014, LECT NOTES COMPUT SC, V8691, P737, DOI 10.1007/978-3-319-10578-9_48; Papadopoulos D. P., 2017, INT C COMP VIS; Papadopoulos D. P., 2016, P IEEE C COMP VIS PA; Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Russakovsky O, 2015, PROC CVPR IEEE, P2121, DOI 10.1109/CVPR.2015.7298824; Saha S., 2016, BRIT MACH VIS C; Saha S., 2017, INT C COMP VIS; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Sharma S., 2015, ADV NEURAL INFORM PR; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Singh G., 2017, INT C COMP VIS; Siva P, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.65; Soomro K., 2012, ARXIV; Soomro K., 2015, INT C COMP VIS; Soomro K., 2017, INT C COMP VIS; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Su H., 2012, AAAI WORKSHOPS; Tian Y., 2013, P IEEE C COMP VIS PA; Tseng PH, 2009, J VISION, V9, DOI 10.1167/9.7.4; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; van Gemert J. C., 2015, BRIT MACH VIS C; Vondrick C, 2013, INT J COMPUT VISION, V101, P184, DOI 10.1007/s11263-012-0564-1; WANG H, 2013, INT C COMP VIS, DOI DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang Heng, 2009, BMVC, P1; Wang L, 2014, LECT NOTES COMPUT SC, V8692, P640, DOI 10.1007/978-3-319-10593-2_42; Weinzaepfel P., 2015, INT C COMP VIS; Yang Z., 2017, BRIT MACH VIS C; Yu G., 2015, P IEEE C COMP VIS PA	63	6	6	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2019	127	3					263	281		10.1007/s11263-018-1120-4	http://dx.doi.org/10.1007/s11263-018-1120-4			19	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HM5CT		Green Published, hybrid, Green Submitted			2022-12-18	WOS:000459493200003
J	Loing, V; Marlet, R; Aubry, M				Loing, Vianney; Marlet, Renaud; Aubry, Mathieu			Virtual Training for a Real Application: Accurate Object-Robot Relative Localization Without Calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Relative localization; Pose estimation; Convolutional Neural Networks; Synthetic data; Virtual training; Robotics	RECOGNITION; GENERATION	Localizing an object accurately with respect to a robot is a key step for autonomous robotic manipulation. In this work, we propose to tackle this task knowing only 3D models of the robot and object in the particular case where the scene is viewed from uncalibrated cameras-a situation which would be typical in an uncontrolled environment, e.g., on a construction site. We demonstrate that this localization can be performed very accurately, with millimetric errors, without using a single real image for training, a strong advantage since acquiring representative training data is a long and expensive process. Our approach relies on a classification Convolutional Neural Network trained using hundreds of thousands of synthetically rendered scenes with randomized parameters. To evaluate our approach quantitatively and make it comparable to alternative approaches, we build a new rich dataset of real robot images with accurately localized blocks.	[Loing, Vianney] UPE, Ecole Ponts ParisTech, Navier UMR 8205, Champs Sur Marne, France; [Marlet, Renaud; Aubry, Mathieu] UPE, Ecole Ponts ParisTech, LIGM UMR 8049, Champs Sur Marne, France	Ecole des Ponts ParisTech; Universite Gustave-Eiffel; Ecole des Ponts ParisTech; Universite Gustave-Eiffel	Loing, V (corresponding author), UPE, Ecole Ponts ParisTech, Navier UMR 8205, Champs Sur Marne, France.	vianney.loing@enpc.fr						[Anonymous], 2012, ADV NEURAL INFORM PR; [Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Collet A, 2011, INT J ROBOT RES, V30, P1284, DOI 10.1177/0278364911401765; Collet A, 2010, IEEE INT CONF ROBOT, P2050, DOI 10.1109/ROBOT.2010.5509615; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Feng C., 2014, INT S AUT ROB CONSTR; Fidler S., 2012, ADV NEURAL INFORM PR; Garrido-Jurado S, 2016, PATTERN RECOGN, V51, P481, DOI 10.1016/j.patcog.2015.09.023; Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005; Glasner D, 2011, IEEE I CONF COMP VIS, P1275, DOI 10.1109/ICCV.2011.6126379; He K., 2017, ARXIV170306870, P2980, DOI [10.1109/ICCV.2017.322, DOI 10.1109/ICCV.2017.322]; Hodan T, 2016, LECT NOTES COMPUT SC, V9915, P606, DOI 10.1007/978-3-319-49409-8_52; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Levine S, 2018, INT J ROBOT RES, V37, P421, DOI 10.1177/0278364917710318; Levine S, 2016, J MACH LEARN RES, V17; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; Massa F, 2016, 27 BRIT MACH VIS C B; Massa F, 2016, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2016.648; Mundy JL, 2006, LECT NOTES COMPUT SC, V4170, P3; Peng X., 2017, ARXIV170105524; Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151; Pepik B, 2015, LECT NOTES COMPUT SC, V9358, P517, DOI 10.1007/978-3-319-24947-6_43; Pepik B, 2012, PROC CVPR IEEE, P3362, DOI 10.1109/CVPR.2012.6248075; Pinto L, 2016, IEEE INT CONF ROBOT, P3406, DOI 10.1109/ICRA.2016.7487517; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Roberts Lawrence G, 1963, THESIS, P2; ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352; Sadeghi F, 2018, ROB SCI SYST RSS C; Saenko K, 2014, 25 BRIT MACH VIS C B; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sermanet P., 2014, P 2 INT C LEARN REPR; Shafaei A., 2016, 27 BRIT MACH VIS C B; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Tobin J., 2017, 30 INT C INT ROB SYS; Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758; Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Xiao J, 2012, ADV NEURAL INFORM PR, P746	43	6	6	0	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2018	126	9			SI		1045	1060		10.1007/s11263-018-1102-6	http://dx.doi.org/10.1007/s11263-018-1102-6			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	GQ3HQ		Green Submitted			2022-12-18	WOS:000441553300010
J	Goutsu, Y; Takano, W; Nakamura, Y				Goutsu, Yusuke; Takano, Wataru; Nakamura, Yoshihiko			Classification of Multi-class Daily Human Motion using Discriminative Body Parts and Sentence Descriptions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Hidden Markov model; Fisher vector; Multiple kernel learning; Motion classification; Multi-class; Sentence description	PARTIAL LEAST-SQUARES; ACTION RECOGNITION; POSE; IMITATION; LATENCY	In this paper, we propose a motion model that focuses on the discriminative parts of the human body related to target motions to classify human motions into specific categories, and apply this model to multi-class daily motion classifications. We extend this model to a motion recognition system which generates multiple sentences associated with human motions. The motion model is evaluated with the following four datasets acquired by a Kinect sensor or multiple infrared cameras in a motion capture studio: UCF-kinect; UT-kinect; HDM05-mocap; and YNL-mocap. We also evaluate the sentences generated from the dataset of motion and language pairs. The experimental results indicate that the motion model improves classification accuracy and our approach is better than other state-of-the-art methods for specific datasets, including human-object interactions with variations in the duration of motions, such as daily human motions. We achieve a classification rate of 81.1% for multi-class daily motion classifications in a non cross-subject setting. Additionally, the sentences generated by the motion recognition system are semantically and syntactically appropriate for the description of the target motion, which may lead to human-robot interaction using natural language.	[Goutsu, Yusuke] AIST, Comp Vis Res Grp, Cent 1,1-1-1 Umezono, Tsukuba, Ibaraki, Japan; [Takano, Wataru] Osaka Univ, Ctr Math Modeling & Data Sci, 1-3 Machikaneyamacho, Toyonaka, Osaka, Japan; [Nakamura, Yoshihiko] Univ Tokyo, Mechanoinformat, Bunkyo Ku, 7-3-1 Hongo, Tokyo, Japan	National Institute of Advanced Industrial Science & Technology (AIST); Osaka University; University of Tokyo	Goutsu, Y (corresponding author), AIST, Comp Vis Res Grp, Cent 1,1-1-1 Umezono, Tsukuba, Ibaraki, Japan.	yusuke.goutsu@aist.go.jp	Goutsu, Yusuke/M-4361-2016	Goutsu, Yusuke/0000-0002-9712-6720	Japan Society for the Promotion of Science [26700021]; Strategic Information and Communications R&D Promotion Program of the Ministry of Internal Affairs and Communications [142103011]	Japan Society for the Promotion of Science(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science); Strategic Information and Communications R&D Promotion Program of the Ministry of Internal Affairs and Communications	This research was partially supported by a Grant-in-Aid for Young Scientists (A) (No. 26700021) from the Japan Society for the Promotion of Science, and by the Strategic Information and Communications R&D Promotion Program (No. 142103011) of the Ministry of Internal Affairs and Communications.	Chaaraoui AA, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P91, DOI 10.1109/ICCVW.2013.19; Barker M, 2003, J CHEMOMETR, V17, P166, DOI 10.1002/cem.785; Billard AG, 2006, ROBOT AUTON SYST, V54, P370, DOI 10.1016/j.robot.2006.01.007; Chaudhry R, 2013, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW.2013.153; Devanne M, 2013, LECT NOTES COMPUT SC, V8158, P456, DOI 10.1007/978-3-642-41190-8_49; Donald M., 1991, ORIGINS MODERN MIND; Dong J.L., 1999, P 5 ACM SIGKDD INT C, P43, DOI [10.1145/312129.312191, DOI 10.1145/312129.312191]; Ellis C, 2013, INT J COMPUT VISION, V101, P420, DOI 10.1007/s11263-012-0550-7; Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772; Eweiwi A, 2015, LECT NOTES COMPUT SC, V9007, P428, DOI 10.1007/978-3-319-16814-2_28; Goutsu Y, 2015, IEEE INT CONF ROBOT, P3024, DOI 10.1109/ICRA.2015.7139614; Goutsu Y, 2013, IEEE INT C INT ROBOT, P151, DOI 10.1109/IROS.2013.6696346; He Y., 2016, EUR C COMP VIS WORKS; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Inamura T, 2004, INT J ROBOT RES, V23, P363, DOI 10.1177/0278364904042199; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Kanda H, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P1852; Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608; Kulic D, 2009, IEEE T ROBOT, V25, P1158, DOI 10.1109/TRO.2009.2026508; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019; Lo Presti L, 2015, IMAGE VISION COMPUT, V44, P29, DOI 10.1016/j.imavis.2015.09.007; Mangu L, 2000, COMPUT SPEECH LANG, V14, P373, DOI 10.1006/csla.2000.0152; Muller M., 2007, CG20072 U BONN, P6; Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007; Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98; Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497; Raman N, 2015, NEUROCOMPUTING, V154, P149, DOI 10.1016/j.neucom.2014.12.009; Rizzolatti G, 2001, NAT REV NEUROSCI, V2, P661, DOI 10.1038/35090060; Rosipal R, 2002, J MACH LEARN RES, V2, P97, DOI 10.1162/15324430260185556; Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381; Slama R, 2015, PATTERN RECOGN, V48, P556, DOI 10.1016/j.patcog.2014.08.011; Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531; Sugita Y, 2005, ADAPT BEHAV, V13, P33, DOI 10.1177/105971230501300102; Sun C, 2013, IEEE I CONF COMP VIS, P913, DOI 10.1109/ICCV.2013.453; Takano W, 2015, INT J ROBOT RES, V34, P1314, DOI 10.1177/0278364915587923; Vieira A. W., 2012, PROGR PATTERN RECOGN, P252, DOI DOI 10.1007/978-3-642-33275-3; Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123; Wang J, 2012, LECT NOTES COMPUT SC, V7573, P872, DOI 10.1007/978-3-642-33709-3_62; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wei P, 2013, IEEE I CONF COMP VIS, P3136, DOI 10.1109/ICCV.2013.389; Xia L., 2012, IEEE COMP SOC C COMP, V2012, P20, DOI DOI 10.1109/CVPRW.2012.6239233; Yang X., 2012, P 20 ACM INT C MULTI, P1057, DOI [10.1145/2393347.2396382, DOI 10.1145/2393347.2396382]; Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512; Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342	45	6	6	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2018	126	5					495	514		10.1007/s11263-017-1053-3	http://dx.doi.org/10.1007/s11263-017-1053-3			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FZ0UP		hybrid			2022-12-18	WOS:000427289200003
J	Han, K; Wong, KYK; Liu, MM				Han, Kai; Wong, Kwan-Yee K.; Liu, Miaomiao			Dense Reconstruction of Transparent Objects by Altering Incident Light Paths Through Refraction	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Reconstruction; Transparent object; Refraction; Light path	SPECULAR 3D SHAPE; STRUCTURED LIGHT	This paper addresses the problem of reconstructing the surface shape of transparent objects. The difficulty of this problem originates from the viewpoint dependent appearance of a transparent object, which quickly makes reconstruction methods tailored for diffuse surfaces fail disgracefully. In this paper, we introduce a fixed viewpoint approach to dense surface reconstruction of transparent objects based on refraction of light. We present a simple setup that allows us to alter the incident light paths before light rays enter the object by immersing the object partially in a liquid, and develop a method for recovering the object surface through reconstructing and triangulating such incident light paths. Our proposed approach does not need to model the complex interactions of light as it travels through the object, neither does it assume any parametric form for the object shape nor the exact number of refractions and reflections taken place along the light paths. It can therefore handle transparent objects with a relatively complex shape and structure, with unknown and inhomogeneous refractive index. We also show that for thin transparent objects, our proposed acquisition setup can be further simplified by adopting a single refraction approximation. Experimental results on both synthetic and real data demonstrate the feasibility and accuracy of our proposed approach.	[Han, Kai; Wong, Kwan-Yee K.] Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China; [Liu, Miaomiao] Australian Natl Univ, Data61, CSIRO, Canberra, ACT, Australia; [Liu, Miaomiao] Australian Natl Univ, CECS, Canberra, ACT, Australia	University of Hong Kong; Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University	Han, K (corresponding author), Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.	khan@cs.hku.hk	Han, Kai/AAB-7809-2021; Wong, Kenneth Kwan Yee/C-1577-2009	Han, Kai/0000-0002-7995-9999; Wong, Kenneth Kwan Yee/0000-0001-8560-9007; Liu, Miaomiao/0000-0001-6485-3510	Research Grant Council of the Hong Kong (SAR), China [HKU 718113E]	Research Grant Council of the Hong Kong (SAR), China(Hong Kong Research Grants Council)	This project is supported by a Grant from the Research Grant Council of the Hong Kong (SAR), China, under the Project HKU 718113E.	Balzer J, 2010, MEASUREMENT, V43, P1305, DOI 10.1016/j.measurement.2010.07.013; Batlle J, 1998, PATTERN RECOGN, V31, P963, DOI 10.1016/S0031-3203(97)00074-5; Ben-Ezra M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1025; Bouguet J. Y., 2008, CAMERA CALIBRATION T; Chari V, 2013, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2013.189; Ding YY, 2011, IEEE I CONF COMP VIS, P2478, DOI 10.1109/ICCV.2011.6126533; Eren G, 2009, OPT EXPRESS, V17, P11457, DOI 10.1364/OE.17.011457; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Han K, 2015, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2015.7299026; Hata S., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P684, DOI 10.1109/ICPR.1996.547652; Hullin M. B., 2008, ACM T GRAPHICS SIGGR, V87, P10; Ihrke I., 2008, EUROGRAPHICS STAR P, P87; Ihrke I, 2010, COMPUT GRAPH FORUM, V29, P2400, DOI 10.1111/j.1467-8659.2010.01753.x; Ihrke N, 2005, IEEE I CONF COMP VIS, P1055; Inokuchi S., 1984, P INT C PATT REC, P806; Ji Y, 2013, PROC CVPR IEEE, P2507, DOI 10.1109/CVPR.2013.324; Kutulakos KN, 2008, INT J COMPUT VISION, V76, P13, DOI 10.1007/s11263-007-0049-9; Kutulakos KN, 2005, IEEE I CONF COMP VIS, P1448; Liu D, 2014, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2014.90; Ma CG, 2014, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2014.420; Miyazaki D, 2005, PROC CVPR IEEE, P910; Morris NJW, 2007, IEEE I CONF COMP VIS, P425; Morris NJW, 2011, IEEE T PATTERN ANAL, V33, P1518, DOI 10.1109/TPAMI.2011.24; MURASE H, 1992, IEEE T PATTERN ANAL, V14, P1045, DOI 10.1109/34.159906; Murase H., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P313, DOI 10.1109/ICCV.1990.139539; Narasimhan SG, 2005, IEEE I CONF COMP VIS, P420; O'Toole M, 2014, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR.2014.421; Qian YM, 2016, PROC CVPR IEEE, P4369, DOI 10.1109/CVPR.2016.473; Reshetouski Ilya, 2013, Time-Of-Flight and Depth Imaging. Sensors, Algorithms and Applications. Dagstuhl 2012 Seminar on Time-of-Flight Imaging and GCPR 2013 Workshop on Imaging New Modalities: LNCS 8200, P77, DOI 10.1007/978-3-642-44964-2_5; Sai-Kit Yeung, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2513, DOI 10.1109/CVPR.2011.5995472; Shan Q, 2012, PROC CVPR IEEE, P286, DOI 10.1109/CVPR.2012.6247687; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Trifonov B., 2006, PROC EUROGRAPHICS C, P51; Tsai CY, 2015, IEEE IMAGE PROC, P606, DOI 10.1109/ICIP.2015.7350870; Wetzstein G, 2011, IEEE I CONF COMP VIS, P1180, DOI 10.1109/ICCV.2011.6126367; Wust C., 1991, Machine Vision and Applications, V4, P193, DOI 10.1007/BF01230201; Xie WY, 2014, PROC CVPR IEEE, P2203, DOI 10.1109/CVPR.2014.282; Zuo XX, 2015, IEEE I CONF COMP VIS, P2237, DOI 10.1109/ICCV.2015.258	38	6	7	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2018	126	5					460	475		10.1007/s11263-017-1045-3	http://dx.doi.org/10.1007/s11263-017-1045-3			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FZ0UP		Green Submitted			2022-12-18	WOS:000427289200001
J	Wigness, M; Draper, BA; Beveridge, JR				Wigness, Maggie; Draper, Bruce A.; Beveridge, J. Ross			Efficient Label Collection for Image Datasets via Hierarchical Clustering	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Efficient label collection; Hierarchical clustering; Image classification; Visual concept discovery	CLASSIFICATION; SCENE; SHAPE	Raw visual data used to train classifiers is abundant and easy to gather, but lacks semantic labels that describe visual concepts of interest. These labels are necessary for supervised learning and can require significant human effort to collect. We discuss four labeling objectives that play an important role in the design of frameworks aimed at collecting label information for large training sets while maintaining low human effort: discovery, efficiency, exploitation and accuracy. We introduce a framework that explicitly models and balances these four labeling objectives with the use of (1) hierarchical clustering, (2) a novel interestingness measure that defines structural change within the hierarchy, and (3) an iterative group-based labeling process that exploits relationships between labeled and unlabeled data. Results on benchmark data show that our framework collects labeled training data more efficiently than existing labeling techniques and trains higher performing visual classifiers. Further, we show that our resulting framework is fast and significantly reduces human interaction time when labeling real-world multi-concept imagery depicting outdoor environments.	[Wigness, Maggie] US Army, Res Lab, Adelphi, MD 20783 USA; [Draper, Bruce A.; Beveridge, J. Ross] Colorado State Univ, Ft Collins, CO 80523 USA	United States Department of Defense; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL); Colorado State University	Wigness, M (corresponding author), US Army, Res Lab, Adelphi, MD 20783 USA.	maggie.b.wigness.civ@mail.mil; draper@cs.colostate.edu; ross@cs.colostate.edu						Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Chaaraoui AA, 2012, EXPERT SYST APPL, V39, P10873, DOI 10.1016/j.eswa.2012.03.005; [Anonymous], THESIS; Biswas A, 2012, PROC CVPR IEEE, P2152, DOI 10.1109/CVPR.2012.6247922; Chang JC, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3180, DOI 10.1145/2858036.2858411; Chatterjee A., 2012, VISION BASED AUTONOM, V455; Chen JJ, 2014, SCI WORLD J, DOI 10.1155/2014/416530; Chilton L.B., 2013, P SIGCHI C HUMAN FAC, P1999, DOI [10.1145/2470654.2466265, DOI 10.1145/2470654.2466265]; Dai DX, 2012, LECT NOTES COMPUT SC, V7574, P483, DOI 10.1007/978-3-642-33712-3_35; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P3099, DOI 10.1145/2556288.2557011; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894; Galleguillos C, 2014, INT J COMPUT VISION, V108, P115, DOI 10.1007/s11263-013-0679-z; Gilbert A, 2011, IEEE I CONF COMP VIS, P2166, DOI 10.1109/ICCV.2011.6126493; Griffin Gregory, 2007, CALTECH 256 OBJECT C; Holub A, 2008, PROC CVPR IEEE, P885; Jain P, 2009, PROC CVPR IEEE, P762, DOI 10.1109/CVPRW.2009.5206651; Joshi Ajay J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2372, DOI 10.1109/CVPRW.2009.5206627; Kapoor A, 2007, IEEE I CONF COMP VIS, P134; Krishna R., 2016, P CHI C HUM FACT COM; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee YJ, 2012, IEEE T PATTERN ANAL, V34, P346, DOI 10.1109/TPAMI.2011.122; Lee YJ, 2011, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2011.5995523; Lennon C., 2013, ARLTR6653; Li X., 2013, P COMP VIS PATT REC; Liu D, 2007, IEEE I CONF COMP VIS, P191; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Settles B., 2010, 1648 U WISC MAD DEP, DOI DOI 10.1016/J.MATLET.2010.11.072; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Sorokin A., 2008, COMP VIS PATT REC WO; Sun C, 2015, IEEE I CONF COMP VIS, P2596, DOI 10.1109/ICCV.2015.298; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tamuz Omer, 2011, ARXIV11051033; Tuytelaars T, 2010, INT J COMPUT VISION, V88, P284, DOI 10.1007/s11263-009-0271-8; Vijayanarasimhan S, 2014, INT J COMPUT VISION, V108, P97, DOI 10.1007/s11263-014-0721-9; Vijayanarasimhan S, 2010, PROC CVPR IEEE, P3035, DOI 10.1109/CVPR.2010.5540055; WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967; Wigness M., 2016, P INT C INT ROB SYST; Wigness M., 2014, P WINT C APPL COMP V; Wigness M., 2015, P COMP VIS PATT REC; Xiong C., 2012, P EUR C DAT MIN, V1, P3; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	53	6	6	1	13	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2018	126	1					59	85		10.1007/s11263-017-1039-1	http://dx.doi.org/10.1007/s11263-017-1039-1			27	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FS6MC					2022-12-18	WOS:000419910500004
J	Alnajar, F; Gevers, T; Valenti, R; Ghebreab, S				Alnajar, Fares; Gevers, Theo; Valenti, Roberto; Ghebreab, Sennay			Auto-Calibrated Gaze Estimation Using Human Gaze Patterns	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Eye gaze estimation; Calibration free; Auto-calibration	STATISTICS; APPEARANCE; TRACKING	We present a novel method to auto-calibrate gaze estimators based on gaze patterns obtained from other viewers. Our method is based on the observation that the gaze patterns of humans are indicative of where a new viewer will look at. When a new viewer is looking at a stimulus, we first estimate a topology of gaze points (initial gaze points). Next, these points are transformed so that they match the gaze patterns of other humans to find the correct gaze points. In a flexible uncalibrated setup with a web camera and no chin rest, the proposed method is tested on ten subjects and ten images. The method estimates the gaze points after looking at a stimulus for a fewseconds with an average error below4.5 degrees. Although the reported performance is lower than what could be achieved with dedicated hardware or calibrated setup, the proposed method still provides sufficient accuracy to trace the viewer attention. This is promising considering the fact that auto-calibration is done in a flexible setup, without the use of a chin rest, and based only on a fewseconds of gaze initialization data. To the best of our knowledge, this is the first work to use human gaze patterns in order to auto-calibrate gaze estimators.	[Alnajar, Fares; Gevers, Theo; Valenti, Roberto] Univ Amsterdam, Informat Inst, Fac Sci, Amsterdam, Netherlands; [Ghebreab, Sennay] Amsterdam Univ Coll, Amsterdam, Netherlands	University of Amsterdam; University of Amsterdam	Alnajar, F (corresponding author), Univ Amsterdam, Informat Inst, Fac Sci, Amsterdam, Netherlands.	F.Alnajar@uva.nl			Dutch national program COMMIT	Dutch national program COMMIT	This research is supported by the Dutch national program COMMIT.	Alnajar F, 2013, IEEE I CONF COMP VIS, P137, DOI 10.1109/ICCV.2013.24; Chen JX, 2011, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.2011.5995675; Draelos M., 2015, INT C IM PROC; Geusebroek JM, 2005, INT J COMPUT VISION, V62, P7, DOI 10.1007/s11263-005-4632-7; Guestrin ED, 2006, IEEE T BIO-MED ENG, V53, P1124, DOI 10.1109/TBME.2005.863952; Guestrin ED, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P267, DOI 10.1145/1344471.1344531; Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30; Hansen DW, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P132, DOI 10.1109/ACV.2002.1182170; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Lu F, 2011, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2011.6126237; Majaranta P., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P15, DOI 10.1145/507072.507076; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Russell B., 2005, AIM2005025 MIT AI CS; Scholte HS, 2009, J VISION, V9, DOI 10.1167/9.4.29; Smith K, 2008, IEEE T PATTERN ANAL, V30, P1212, DOI 10.1109/TPAMI.2007.70773; Sugano Y, 2014, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2014.235; Sugano Y, 2013, IEEE T PATTERN ANAL, V35, P329, DOI 10.1109/TPAMI.2012.101; Sugano Y, 2010, PROC CVPR IEEE, P2667, DOI 10.1109/CVPR.2010.5539984; TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999; Tan KH, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P191, DOI 10.1109/ACV.2002.1182180; Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Valenti R, 2012, IEEE T PATTERN ANAL, V34, P1785, DOI 10.1109/TPAMI.2011.251; Villanueva A, 2006, IMAGE VISION COMPUT, V24, P663, DOI 10.1016/j.imavis.2005.06.001; WEDEL M., 2008, REV MARKETING RES; Xiong C., 2015, MULTIMED TOOLS APPL, V75, P1; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	28	6	8	1	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2017	124	2					223	236		10.1007/s11263-017-1014-x	http://dx.doi.org/10.1007/s11263-017-1014-x			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	FC3PK		hybrid, Green Published			2022-12-18	WOS:000406751100007
J	Lin, GS; Liu, FY; Shen, CH; Wu, JX; Shen, HT				Lin, Guosheng; Liu, Fayao; Shen, Chunhua; Wu, Jianxin; Shen, Heng Tao			Structured Learning of Binary Codes with Column Generation for Optimizing Ranking Measures	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Binary code; Hashing; Nearest neighbor search; Ranking; Structured learning		Hashing methods aim to learn a set of hash functions which map the original features to compact binary codes with similarity preserving in the Hamming space. Hashing has proven a valuable tool for large-scale information retrieval. We propose a column generation based binary code learning framework for data-dependent hash function learning. Given a set of triplets that encode the pairwise similarity comparison information, our column generation based method learns hash functions that preserve the relative comparison relations within the large-margin learning framework. Our method iteratively learns the best hash functions during the column generation procedure. Existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest-multivariate performance measures such as the AUC and NDCG. Our column generation based method can be further generalized from the triplet loss to a general structured learning based framework that allows one to directly optimize multivariate performance measures. For optimizing general ranking measures, the resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. We use a combination of column generation and cutting-plane techniques to solve the optimization problem. To speed-up the training we further explore stage-wise training and propose to optimize a simplified NDCG loss for efficient inference. We demonstrate the generality of our method by applying it to ranking prediction and image retrieval, and show that it outperforms several state-of-the-art hashing methods.	[Lin, Guosheng; Liu, Fayao; Shen, Chunhua] Univ Adelaide, Adelaide, SA, Australia; [Wu, Jianxin] Nanjing Univ, Nanjing, Jiangsu, Peoples R China; [Shen, Heng Tao] Univ Elect Sci & Technol, Chengdu, Peoples R China	University of Adelaide; Nanjing University; University of Electronic Science & Technology of China	Shen, CH (corresponding author), Univ Adelaide, Adelaide, SA, Australia.	guosheng.lin@adelaide.edu.au; fayaoliu@gmail.com; chunhua.shen@adelaide.edu.au; wujx2001@nju.edu.cn; shenhengtao@hotmail.com	Shen, Heng Tao/ABD-5331-2021; Lin, Guosheng/N-9110-2019; Lin, Guosheng/Q-4024-2017	Lin, Guosheng/0000-0002-0329-7458; Lin, Guosheng/0000-0002-0329-7458	ARC [FT120100969]; National Nature Science Foundation of China [61632007]	ARC(Australian Research Council); National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC))	C. Shen's participation was supported by an ARC Future Fellowship (FT120100969). H. T. Shen's participation was supported by National Nature Science Foundation of China (No. 61632007).	[Anonymous], P INT C MACH LEARN; Boyd S, 2004, CONVEX OPTIMIZATION; Cakir F., 2015, P INT C COMP VIS; Chakrabarti S., 2008, P ACM KNOWL DISC DAT; Dean T., 2013, P INT C COMP VIS PAT; Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092; Gionis A., 1999, P INT C VER LARG BAS; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Heo J., 2012, P INT C COMP VIS PAT; Jarvelin K., 2000, P ACM C SIGIR; Joachims T., 2005, P INT C MACH LEARN; Joachims T., 2006, P ACM KNOWL DISC DAT; KELLEY JE, 1960, J SOC IND APPL MATH, V8, P703, DOI 10.1137/0108053; Kulis B., 2009, NIPS; Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219; Li X., 2013, P INT C MACH LEARN; Lim D., 2014, P INT C MACH LEARN; Lin G., 2014, P EUR C COMP VIS; Lin G., 2014, P INT C COMP VIS PAT; Lin G., 2013, P INT C COMP VIS; Liu W., 2012, P INT C COMP VIS PAT; McFee B., 2010, P INT C MACH LEARN; Norouzi M., 2012, ADV NEURAL INFORM PR, P1; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Schultz M., 2004, P ADV NEUR INF PROC; Shalit U, 2012, J MACH LEARN RES, V13, P429; Shen CH, 2014, IEEE T PATTERN ANAL, V36, P2089, DOI 10.1109/TPAMI.2014.2315792; Shen CH, 2012, J MACH LEARN RES, V13, P1007; Shen CH, 2010, IEEE T PATTERN ANAL, V32, P2216, DOI 10.1109/TPAMI.2010.47; Shen F., 2013, P INT C COMP VIS PAT; Shen F., 2015, P INT C COMP VIS PAT; Sowndarrajan P. T., 2021, MOSEK OPT TOOLB MATL, V2021, P1; Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103; Torralba A., 2008, IEEE T PATTERN ANAL; Wang J., 2014, ARXIV14082927 CORR; Wang Q., 2015, P INT JOINT C ART IN, V34, P2393; Weiss Y., 2008, P ADV NEUR INF PROC; Weiss Y., 2012, P EUR C COMP VIS; Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3; Yue Y., 2007, P ACM C SIGIR; Zhang D., 2010, P ACM C SIGIR WORKSH; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236	42	6	6	0	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2017	123	2					287	308		10.1007/s11263-016-0984-4	http://dx.doi.org/10.1007/s11263-016-0984-4			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EU2OM					2022-12-18	WOS:000400868800008
J	Palmero, C; Esquirol, J; Bayo, V; Cos, MA; Ahmadmonfared, P; Salabert, J; Sanchez, D; Escalera, S				Palmero, Cristina; Esquirol, Jordi; Bayo, Vanessa; Angel Cos, Miquel; Ahmadmonfared, Pouya; Salabert, Joan; Sanchez, David; Escalera, Sergio			Automatic Sleep System Recommendation by Multi-modal RBG-Depth-Pressure Anthropometric Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sleep system recommendation; RGB-Depth data; Pressure imaging; Anthropometric landmark extraction; Multi-part human body segmentation	WEIGHT; ALIGNMENT; POSTURE; QUALITY; KINECT; MODEL	This paper presents a novel system for automatic sleep system recommendation using RGB, depth and pressure information. It consists of a validated clinical knowledge-based model that, along with a set of prescription variables extracted automatically, obtains a personalized bed design recommendation. The automatic process starts by performing multi-part human body RGB-D segmentation combining GrabCut, 3D Shape Context descriptor and Thin Plate Splines, to then extract a set of anthropometric landmark points by applying orthogonal plates to the segmented human body. The extracted variables are introduced to the computerized clinical model to calculate body circumferences, weight, morphotype and Body Mass Index categorization. Furthermore, pressure image analysis is performed to extract pressure values and at-risk points, which are also introduced to the model to eventually obtain the final prescription of mattress, topper, and pillow. We validate the complete system in a set of 200 subjects, showing accurate category classification and high correlation results with respect to manual measures.	[Palmero, Cristina; Ahmadmonfared, Pouya] Comp Vis Ctr, Campus UAB,Edifici O, Barcelona 08193, Spain; [Esquirol, Jordi; Bayo, Vanessa; Angel Cos, Miquel] Escola Univ Gimbernat, SURF, Avinguda Generalitat 202-206, Barcelona 08174, Spain; [Salabert, Joan; Sanchez, David] Dormity Com, Via Augusta 85-87, Barcelona 08174, Spain; [Escalera, Sergio] UB, Dept Matemat Aplicada & Anal, Gran Via Corts Catalanes 585, Barcelona 08007, Spain	Autonomous University of Barcelona; Centre de Visio per Computador (CVC); University of Barcelona	Palmero, C (corresponding author), Comp Vis Ctr, Campus UAB,Edifici O, Barcelona 08193, Spain.	c.palmero.cantarino@gmail.com; jordi.esquirol@eug.es; vanesa.bayo@eug.es; mcos@car.edu; pouyaam@gmail.com; joansalabert@dormity.com; davidsanchez@dormity.com; sergio@maia.ub.es	Esquirol-Caussa, Jordi/A-6163-2016; Escalera, Sergio/L-2998-2015; Bayo-Tallon, Vanessa/AAC-8522-2020	Esquirol-Caussa, Jordi/0000-0001-8802-8400; Escalera, Sergio/0000-0003-0617-8873; Bayo-Tallon, Vanessa/0000-0001-9611-4720	Spanish Project [TIN2013-43478-P]; Dormity.com<SUP>(R)</SUP>	Spanish Project(Spanish Government); Dormity.com<SUP>(R)</SUP>	This work has been partially supported by Spanish Project TIN2013-43478-P and Dormity.com<SUP>(R)</SUP>	Allen B., 2004, EXPLORING SPACE HUMA; Bain D, 2003, J Wound Care, V12, P231; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Ben Azouz Z, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P750; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; Canda A.S., 2012, VARIABLES ANTROPOMET; Carmona-Benjumea A, 2001, REV INSHT, V14, P22; Cippitelli E, 2015, SENSORS-BASEL, V15, P1417, DOI 10.3390/s150101417; Clarkson S., 2014, P COMP VIS ECCV 2014, P372; de Sanidad y Consumo M., 2008, ESTUDIO ANTROPOMETRI; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Determann RM, 2007, CRIT CARE NURSE, V27, P48; DeVocht JW, 2006, APPL ERGON, V37, P297, DOI 10.1016/j.apergo.2005.07.002; Espitia-Contreras A, 2014, 2014 IEEE VIRTUAL REALITY (VR), P71, DOI 10.1109/VR.2014.6802056; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Foubert N., 2010, THESIS; Gordon Susan J, 2010, J Pain Res, V3, P137; Gordon SJ, 2011, PHYSIOTHER CAN, V63, P183, DOI 10.3138/ptc.2010-13; Gordon SJ, 2009, MANUAL THER, V14, P671, DOI 10.1016/j.math.2009.02.006; Gupta S, 2010, INT J COMPUT VISION, V90, P331, DOI 10.1007/s11263-010-0360-8; Harada T, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P968, DOI 10.1109/ROBOT.1999.772434; Harada T, 2001, IEEE INT CONF ROBOT, P3201, DOI 10.1109/ROBOT.2001.933111; Huang SH, 2014, J MANUF SYST, V33, P699, DOI 10.1016/j.jmsy.2014.02.005; Joint F. Organization W. H., 1985, EN PROT REQ REP JOIN; JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710; Kim HS, 2013, ACTA OPHTHALMOL, V91, pE502, DOI 10.1111/aos.12151; Lazzaro EC, 2014, J GLAUCOMA, V23, P282, DOI 10.1097/01.ijg.0000435848.90957.fe; Leilnahari K, 2011, BIOMED ENG ONLINE, V10, DOI 10.1186/1475-925X-10-103; Liu JJ, 2013, INT CONF PERVAS COMP, P207, DOI 10.1109/PerCom.2013.6526734; Liu JJ, 2014, PERVASIVE MOB COMPUT, V10, P34, DOI 10.1016/j.pmcj.2013.10.008; Lopez-Torres M, 2008, APPL ERGON, V39, P123, DOI 10.1016/j.apergo.2006.11.002; Lorenz MW, 2007, J NEUROL NEUROSUR PS, V78, P1331, DOI 10.1136/jnnp.2007.117150; Lorenzo-Navarro J, 2013, SENSORS-BASEL, V13, P8222, DOI 10.3390/s130708222; Madadi M, 2015, PATTERN RECOGN LETT, V56, P14, DOI 10.1016/j.patrec.2015.01.012; Martinez M, 2014, IEEE IMAGE PROC, P5791, DOI 10.1109/ICIP.2014.7026171; Matsuo J, 2011, J TISSUE VIABILITY, V20, P55, DOI 10.1016/j.jtv.2010.12.002; Meng-Chieh Yu, 2012, Proceedings of the International Conference on Health Informatics. HEALTHINF 2012, P12; Metsis V, 2014, PERS UBIQUIT COMPUT, V18, P19, DOI 10.1007/s00779-012-0623-1; Miller S, 2013, OSTOMY WOUND MANAG, V59, P44; Mogelmose A, 2013, IEEE COMPUT SOC CONF, P301, DOI 10.1109/CVPRW.2013.52; Nechala P, 1999, PLAST RECONSTR SURG, V103, P1819, DOI 10.1097/00006534-199906000-00002; Nguyen TV, 2014, J COMPUT SCI TECH-CH, V29, P777, DOI 10.1007/s11390-014-1467-0; Reyes M, 2013, COMPUT IND, V64, P1316, DOI 10.1016/j.compind.2013.04.009; Romero Collazos J., 2008, ANALISIS FORMA PROPO; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rusu R. B., 2013, SEMANTIC 3D OBJECT M, V85; Saenz Z. L., 2011, WORK READING MASS, V41, P1281; Suzuki K, 2003, COMPUT VIS IMAGE UND, V89, P1, DOI 10.1016/S1077-3142(02)00030-9; Uhm T, 2015, MEASUREMENT, V61, P169, DOI 10.1016/j.measurement.2014.10.044; Verhaert V., 2011, WORK READING MASS, V41, P2268; Verhaert V., 2011, P 1 INT S DIG HUM MO, V2202; Verhaert V, 2011, ERGONOMICS, V54, P169, DOI 10.1080/00140139.2010.538725; Wang Q., 2014, ARXIV14100745; Weimin Huang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4336, DOI 10.1109/ICPR.2010.1054; Wong MHY, 2013, SINGAP MED J, V54, P146, DOI 10.11622/smedj.2013050; Yang CH, 2014, IEEE PHOTON CONF, P1, DOI 10.1109/IPCon.2014.6994959; Yousefi R., 2011, P SO BIOM ENG C SBEC; Zuberi Najeeb A, 2004, Sleep Breath, V8, P201, DOI 10.1007/s11325-004-0201-5	59	6	7	3	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2017	122	2			SI		212	227		10.1007/s11263-016-0919-0	http://dx.doi.org/10.1007/s11263-016-0919-0			16	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EQ5ZS					2022-12-18	WOS:000398162200003
J	Krajsek, K; Menzel, MI; Scharr, H				Krajsek, Kai; Menzel, Marion I.; Scharr, Hanno			A Riemannian Bayesian Framework for Estimating Diffusion Tensor Images	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Riemannian manifold; Diffusion tensor images; Diffusion tensor imaging (DTI); Estimation theory	MAGNETIC-RESONANCE IMAGES; LOG-EUCLIDEAN METRICS; MATRIX-VALUED IMAGES; TO-NOISE RATIO; RICIAN DISTRIBUTION; MRI DATA; SIGNAL; MANIFOLDS; FIELDS; STATISTICS	Diffusion tensor magnetic resonance imaging (DT-MRI) is a non-invasive imaging technique allowing to estimate the molecular self-diffusion tensors of water within surrounding tissue. Due to the low signal-to-noise ratio of magnetic resonance images, reconstructed tensor images usually require some sort of regularization in a post-processing step. Previous approaches are either suboptimal with respect to the reconstruction or regularization step. This paper presents a Bayesian approach for simultaneous reconstruction and regularization of DT-MR images that allows to resolve the disadvantages of previous approaches. To this end, estimation theoretical concepts are generalized to tensor valued images that are considered as Riemannian manifolds. Doing so allows us to derive a maximum a posteriori estimator of the tensor image that considers both the statistical characteristics of the Rician noise occurring in MR images as well as the nonlinear structure of tensor valued images. Experiments on synthetic data as well as real DT-MRI data validate the advantage of considering both statistical as well as geometrical characteristics of DT-MRI.	[Krajsek, Kai; Scharr, Hanno] Forschungszentrum Julich, IBG Plant Sci 2, D-52425 Julich, Germany; [Menzel, Marion I.] GE Global Res Diagnost & Biomed Technol, D-85748 Garching, Germany	Helmholtz Association; Research Center Julich	Krajsek, K (corresponding author), Forschungszentrum Julich, IBG Plant Sci 2, D-52425 Julich, Germany.	k.krajsek@fz-juelich.de	Scharr, Hanno/D-9051-2015; Menzel, Marion Irene/H-8952-2016; Krajsek, Kai/S-1057-2016	Scharr, Hanno/0000-0002-8555-6416; Menzel, Marion Irene/0000-0003-0087-9134; Krajsek, Kai/0000-0003-3417-161X				Alexander AL, 2007, NEUROTHERAPEUTICS, V4, P316, DOI 10.1016/j.nurt.2007.05.011; Andersen AH, 1996, MAGNET RESON MED, V36, P331, DOI 10.1002/mrm.1910360222; Andersson JLR, 2008, NEUROIMAGE, V42, P1340, DOI 10.1016/j.neuroimage.2008.05.053; Arsigny V, 2005, LECT NOTES COMPUT SC, V3749, P115; Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965; Atkinson C., 1981, SANKHYA A, V43, P345; Basser PJ, 1996, J MAGN RESON SER B, V111, P209, DOI [10.1006/jmrb.1996.0086, 10.1016/j.jmr.2011.09.022]; Batchelor PG, 2005, MAGN RESON MED, V53, P221, DOI 10.1002/mrm.20334; BERNSTEIN MA, 1989, MED PHYS, V16, P813, DOI 10.1118/1.596304; Burgeth B, 2007, LECT NOTES COMPUT SC, V4485, P556; Burgeth B, 2009, MATH VIS, P305, DOI 10.1007/978-3-540-88378-4_15; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Castano-Moraga CA, 2007, SIGNAL PROCESS, V87, P263, DOI 10.1016/j.sigpro.2006.02.049; ChefD'Hotel C, 2004, J MATH IMAGING VIS, V20, P147, DOI 10.1023/B:JMIV.0000011324.14508.fb; Chen B, 2005, MAGN RESON MED, V54, P393, DOI 10.1002/mrm.20582; COHEN RL, 1985, ANN MATH, V122, P237, DOI 10.2307/1971304; Constantinides CD, 1997, MAGNET RESON MED, V38, P852, DOI 10.1002/mrm.1910380524; Coulon O., 2001, Information Processing in Medical Imaging. 17th International Conference, IPMI 2001. Proceedings (Lecture Notes in Computer Science Vol.2082), P92; Courant R., 1953, METHODS MATH PHYS, V1st English edition; Cox R., 2006, INT SOC MAGN RESON M, P349; Dunham W., 1990, JOURNEY GENIUS GREAT, P133; EDELSTEIN WA, 1983, J COMPUT ASSIST TOMO, V7, P391, DOI 10.1097/00004728-198306000-00001; EDELSTEIN WA, 1984, MED PHYS, V11, P180, DOI 10.1118/1.595484; Edlow BL, 2016, BMC NEUROL, V16, DOI 10.1186/s12883-015-0525-8; Feddern C, 2006, INT J COMPUT VISION, V69, P93, DOI 10.1007/s11263-006-6854-8; Fillard P, 2005, LECT NOTES COMPUT SC, V3753, P112, DOI 10.1007/11577812_10; Fillard P, 2007, IEEE T MED IMAGING, V26, P1472, DOI 10.1109/TMI.2007.899173; Fletcher PT, 2007, SIGNAL PROCESS, V87, P250, DOI 10.1016/j.sigpro.2005.12.018; Fletcher PT, 2004, LECT NOTES COMPUT SC, V3117, P87; Florack L., 2014, VISUALIZATION PROCES, P189, DOI DOI 10.1007/978-3-642-54301-2_8; Forstner W., 1999, 1999 STUTTG U DEP GE; Golub Gene H., 2013, MATRIX COMPUTATION, V3; GUDBJARTSSON H, 1995, MAGNET RESON MED, V34, P910, DOI 10.1002/mrm.1910340618; Gur Y, 2005, LECT NOTES COMPUT SC, V3752, P13; Gur Y., 2012, COMPUTATIONAL IMAGIN, V41, P83; Gur Y., 2007, P IEEE COMP SOC WORK, P1; Gur Y, 2009, INT J COMPUT VISION, V85, P211, DOI 10.1007/s11263-008-0196-7; Hampel FR., 2011, WILEY SERIES PROBABI; Hartmann S., 2003, TECHN MECH, V23, P283; Hasan KM, 2001, J MAGN RESON, V152, P41, DOI 10.1006/jmre.2001.2400; Helgason S., 1978, DIFFERENTIAL GEOMETR, V80; HENKELMAN RM, 1985, MED PHYS, V12, P232, DOI 10.1118/1.595711; HOULT DI, 1976, J MAGN RESON, V24, P71, DOI 10.1016/0022-2364(76)90233-X; Huber P. J, 1981, ROBUST STAT; Jeong HK, 2008, MAGN RESON MED, V60, P1408, DOI 10.1002/mrm.21734; Jermyn IH, 2005, ANN STAT, V33, P583, DOI 10.1214/009053604000001273; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; Kay S. M, 1993, FUNDAMENTALS STAT PR, V1; Koay CG, 2006, J MAGN RESON, V179, P317, DOI 10.1016/j.jmr.2006.01.016; Krajsek K, 2008, LECT NOTES COMPUT SC, V5305, P326, DOI 10.1007/978-3-540-88693-8_24; Krajsek K, 2006, LECT NOTES COMPUT SC, V4174, P91; Krajsek K, 2012, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2012.6247779; Krajsek K, 2009, IEEE I CONF COMP VIS, P2327, DOI 10.1109/ICCV.2009.5459431; Landman B., 2007, P IEEE 11 INT C COMP, P1; Landman B., 2007, MMBIA08, P1; Le Bihan D, 2001, J MAGN RESON IMAGING, V13, P534, DOI 10.1002/jmri.1076; LEBIHAN D, 1986, RADIOLOGY, V161, P401, DOI 10.1148/radiology.161.2.3763909; Lenglet C, 2005, LECT NOTES COMPUT SC, V3565, P591; Lenglet C, 2006, J MATH IMAGING VIS, V25, P423, DOI 10.1007/s10851-006-6897-z; LIBOVE JM, 1980, J PHYS E SCI INSTRUM, V13, P38, DOI 10.1088/0022-3735/13/1/013; Macovski A, 1996, MAGNET RESON MED, V36, P494, DOI 10.1002/mrm.1910360327; Martin-Fernandez M, 2003, LECT NOTES COMPUT SC, V2809, P506; MCGIBNEY G, 1993, MED PHYS, V20, P1077, DOI 10.1118/1.597004; Melonakos J, 2007, LECT NOTES COMPUT SC, V4791, P36; Menzel M. I., 2002, THESIS; Moakher M, 2005, SIAM J MATRIX ANAL A, V26, P735, DOI 10.1137/S0895479803436937; MORMAN KN, 1986, J APPL MECH-T ASME, V53, P726, DOI 10.1115/1.3171840; Muller MJ, 2007, NEUROBIOL AGING, V28, P398, DOI 10.1016/j.neurobiolaging.2006.01.009; Nowak RD, 1999, IEEE T IMAGE PROCESS, V8, P1408, DOI 10.1109/83.791966; ORTENDAHL DA, 1984, MAGNET RESON MED, V1, P316, DOI 10.1002/mrm.1910010304; ORTENDAHL DA, 1983, IEEE T NUCL SCI, V30, P692, DOI 10.1109/TNS.1983.4332357; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Pennec X, 1999, PROCEEDINGS OF THE IEEE-EURASIP WORKSHOP ON NONLINEAR SIGNAL AND IMAGE PROCESSING (NSIP'99), P194; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Rao C. R, 1945, B CALCUTTA MATH SOC, V37, P81, DOI DOI 10.1007/978-1-4612-0919-5_16; Rice S O, 1944, BELL SYSTEM TECHNICA, V24; ROEMER PB, 1990, MAGNET RESON MED, V16, P192, DOI 10.1002/mrm.1910160203; Scharr H, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P840; Scharr H., 2000, THESIS; Scheenen TWJ, 2007, PLANT PHYSIOL, V144, P1157, DOI [10.1104/pp.106.089250, 10.1104/PP.106.089250]; Sijbers J, 1998, IEEE T MED IMAGING, V17, P357, DOI 10.1109/42.712125; Sijbers J, 2004, MAGNET RESON MED, V51, P586, DOI 10.1002/mrm.10728; Sijbers J, 2007, PHYS MED BIOL, V52, P1335, DOI 10.1088/0031-9155/52/5/009; Skovgaard L. T, 1981, 813 DAN MED RES COUN; STEJSKAL EO, 1965, J CHEM PHYS, V42, P288, DOI 10.1063/1.1695690; TARANTOLA A, 1982, J GEOPHYS-Z GEOPHYS, V50, P159; Tarantola A., 2005, INVERSE PROBLEM THEO, p72; Tschumperle D, 2001, PROC CVPR IEEE, P948; Tschumperle D, 2002, INT J COMPUT VISION, V50, P237, DOI 10.1023/A:1020870207168; WANG Y, 1994, P IEEE INT C IM PROC, V1, P866; Weickert J, 1999, INT J COMPUT VISION, V31, P111, DOI 10.1023/A:1008009714131; Weickert J., 1999, HDB COMPUTER VISION, V2, P423; WEICKERT J, 2002, CONT MATH, V313, P251; Weickert  J., 1996, THESIS; Westin CR, 2003, LECT NOTES COMPUT SC, V2809, P564; Wood JC, 1999, MAGNET RESON MED, V41, P631, DOI 10.1002/(SICI)1522-2594(199903)41:3<631::AID-MRM29>3.0.CO;2-Q; Zerai M., 2007, CORR; Zerai M, 2007, LECT NOTES COMPUT SC, V4485, P592; [No title captured]	100	6	6	0	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2016	120	3					272	299		10.1007/s11263-016-0909-2	http://dx.doi.org/10.1007/s11263-016-0909-2			28	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DV5OK					2022-12-18	WOS:000382977200003
J	Zhang, C; Shen, CH; Shen, TZ				Zhang, Chao; Shen, Chunhua; Shen, Tingzhi			Unsupervised Feature Learning for Dense Correspondences Across Scenes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Unsupervised feature learning; Scene alignment; Dense scene correspondence; Loopy belief propagation	BELIEF PROPAGATION; SPARSE	We propose a fast, accurate matching method for estimating dense pixel correspondences across scenes. It is a challenging problem to estimate dense pixel correspondences between images depicting different scenes or instances of the same object category. While most such matching methods rely on hand-crafted features such as SIFT, we learn features from a large amount of unlabeled image patches using unsupervised learning. Pixel-layer features are obtained by encoding over the dictionary, followed by spatial pooling to obtain patch-layer features. The learned features are then seamlessly embedded into a multi-layer matching framework. We experimentally demonstrate that the learned features, together with our matching model, outperform state-of-the-art methods such as the SIFT flow (Liu et al. in IEEE Trans Pattern Anal Mach Intell 33(5):978-994, 2011), coherency sensitive hashing (Korman and Avidan in: Proceedings of the IEEE international conference on computer vision (ICCV), 2011) and the recent deformable spatial pyramid matching (Kim et al. in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2013) methods both in terms of accuracy and computation efficiency. Furthermore, we evaluate the performance of a few different dictionary learning and feature encoding methods in the proposed pixel correspondence estimation framework, and analyze the impact of dictionary learning and feature encoding with respect to the final matching performance.	[Zhang, Chao; Shen, Tingzhi] Beijing Inst Technol, Beijing 100081, Peoples R China; [Zhang, Chao; Shen, Chunhua] Univ Adelaide, Adelaide, SA 5005, Australia; [Shen, Chunhua] Australian Ctr Robot Vis, Brisbane, Qld, Australia	Beijing Institute of Technology; University of Adelaide; Australian Centre for Robotic Vision	Shen, CH (corresponding author), Univ Adelaide, Adelaide, SA 5005, Australia.; Shen, CH (corresponding author), Australian Ctr Robot Vis, Brisbane, Qld, Australia.	chhshen@gmail.com			ARC [FT120100969]	ARC(Australian Research Council)	This work is in part supported by ARC Grant FT120100969. C. Zhang's contribution was made when she was a visiting student at the University of Adelaide.	Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; Barnes C., 2010, P EUR C COMP VIS; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Berg AC, 2005, PROC CVPR IEEE, P26; Bo LF, 2013, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2013.91; Boureau Y.L., 2010, P 27 INT C MACH LEAR, P111; Coates A., 2011, ADV NEURAL INFORM PR, P2528, DOI DOI 10.1016/J.PSYCHRES.2009.03.008; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Coates Adam, 2011, P 28 INT C MACH LEAR, P921; Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445; Everingham M., 2014, INT J COMPUTER VISIO; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; Hassner T, 2012, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2012.6247842; Heikkila M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014; Huang G. B., 2012, P ADV NEUR INF PROC; Ihler AT, 2005, J MACH LEARN RES, V6, P905; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Korman S, 2011, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2011.6126421; Le Q., 2012, P INT C MACH LEARN; Le Q.V., 2011, NEURIPS, P1017; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Leordeanu M., 2005, P IEEE INT C COMP VI, V2; Leordeanu M., 2013, P IEEE INT C COMP VI; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536; Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Mairal J, 2010, J MACH LEARN RES, V11, P19; Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253; TOLA E, 2008, P IEEE C COMP VIS PA; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Tuytelaars T., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P412; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Zhou G., 2012, 15 INT C ARTIFICIAL, P1453	34	6	6	0	29	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2016	116	1					90	107		10.1007/s11263-015-0829-6	http://dx.doi.org/10.1007/s11263-015-0829-6			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	DC7TE		Green Submitted			2022-12-18	WOS:000369422500005
J	Yang, XF; Li, YH; Reutens, D; Jiang, TZ				Yang, Xianfeng; Li, Yonghui; Reutens, David; Jiang, Tianzi			Diffeomorphic Metric Landmark Mapping Using Stationary Velocity Field Parameterization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computational anatomy; Diffeomorphic metric mapping; Stationary parameterization; Landmark matching; Metric approximation	COMPUTATIONAL ANATOMY; IMAGE REGISTRATION; SUBGROUPS; STATISTICS; FLOWS	Large deformation diffeomorphic metric mapping (LDDMM) has been shown as an effective computational paradigm to measure anatomical variability. However, its time-varying vector field parameterization of diffeomorphism flow leads to computationally expensive implementation, as well as some theoretical issues in metric based shape analysis, e.g. high order metric approximation via Baker-Campbell-Hausdorff (BCH) formula. To address these problems, we study the role of stationary vector field parameterization in context of LDDMM. Under this setting registration is formulated as finding the Lie group exponential path with minimal energy in Riemannian manifold of diffeomorphisms bringing two shapes together. Accurate derivation of Euler-Lagrange equation shows that optimal vector field for landmark matching is associated with singular momenta at landmark trajectories in whole time domain, and a new momentum optimization scheme is proposed to solve the variational problem. Length of group exponential path is also proposed as an alternative shape metric to geodesic distance, and pair-wise metrics among a population are computed through an approximation method via BCH formula which only needs registrations to a template. The proposed methods have been tested on both synthesized data and real database. Compared to non-stationary parameterization, this method can achieve comparable registration accuracy in significantly reduced time. Second order metric approximation by this method also improves significantly over first order, which can not be achieved by non-stationary parameterization. Correlation between the two shape metrics is also investigated, and their statistical power in clinical study compared.	[Yang, Xianfeng; Li, Yonghui; Jiang, Tianzi] Univ Queensland, Queensland Brain Inst, Brisbane, Qld 4072, Australia; [Yang, Xianfeng; Reutens, David; Jiang, Tianzi] Univ Queensland, Ctr Adv Imaging, Brisbane, Qld 4072, Australia; [Jiang, Tianzi] Chinese Acad Sci, Brainnetome Ctr, Inst Automat, Beijing 100190, Peoples R China; [Jiang, Tianzi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Jiang, Tianzi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China	University of Queensland; University of Queensland; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS	Jiang, TZ (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.	jiangtz@nlpr.ia.ac.cn	li, yong/HDN-3885-2022; Reutens, David/C-7226-2018; Jiang, Tianzi/I-4256-2012	Jiang, Tianzi/0000-0001-9531-291X	National Key Basic Research and Development Program (973) [2011CB707800]; Strategic Priority Research Program of the Chinese Academy of Sciences [XDB02030300]; National Natural Science Foundation of China [91132301]	National Key Basic Research and Development Program (973)(National Basic Research Program of China); Strategic Priority Research Program of the Chinese Academy of Sciences(Chinese Academy of Sciences); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was partially supported by the National Key Basic Research and Development Program (973) (Grant No. 2011CB707800), the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB02030300), and the National Natural Science Foundation of China (Grant No. 91132301). We also thank Dr. Anqi Qiu for thoughtful discussions on LDDMM.	[Anonymous], 1999, NUMERICAL OPTIMIZATI; Arsigny V, 2006, LECT NOTES COMPUT SC, V4190, P924; Ashburner J, 2007, NEUROIMAGE, V38, P95, DOI 10.1016/j.neuroimage.2007.07.007; Ashburner J, 2011, NEUROIMAGE, V55, P954, DOI 10.1016/j.neuroimage.2010.12.049; Avants BB, 2008, MED IMAGE ANAL, V12, P26, DOI 10.1016/j.media.2007.06.004; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Beg MF, 2007, IEEE T MED IMAGING, V26, P1179, DOI 10.1109/TMI.2007.898813; Cox T.F., 2001, MULTIDIMENSIONAL SCA, V2nd; Davis BC, 2010, INT J COMPUT VISION, V90, P255, DOI 10.1007/s11263-010-0367-1; Du J, 2011, LECT NOTES COMPUT SC, V6801, P448, DOI 10.1007/978-3-642-22092-0_37; Dupuis P, 1998, Q APPL MATH, V56, P587, DOI 10.1090/qam/1632326; Glaunes J, 2008, INT J COMPUT VISION, V80, P317, DOI 10.1007/s11263-008-0141-9; GRABOWSKI J, 1988, FUND MATH, V131, P103, DOI 10.4064/fm-131-2-103-121; Grenander U, 1998, Q APPL MATH, V56, P617, DOI 10.1090/qam/1668732; Grenander U., 1993, GEN PATTERN THEORY M; Hernandez M, 2009, INT J COMPUT VISION, V85, P291, DOI 10.1007/s11263-009-0219-z; Holm DD, 2004, NEUROIMAGE, V23, pS170, DOI 10.1016/j.neuroimage.2004.07.017; Joshi SC, 2000, IEEE T IMAGE PROCESS, V9, P1357, DOI 10.1109/83.855431; Lorenzi M, 2013, INT J COMPUT VISION, V105, P111, DOI 10.1007/s11263-012-0598-4; Marcus DS, 2007, J COGNITIVE NEUROSCI, V19, P1498, DOI 10.1162/jocn.2007.19.9.1498; Miller MI, 2006, J MATH IMAGING VIS, V24, P209, DOI 10.1007/s10851-005-3624-0; Miller MI, 2002, ANNU REV BIOMED ENG, V4, P375, DOI 10.1146/annurev.bioeng.4.092101.125733; Miller MI, 2009, HUM BRAIN MAPP, V30, P2132, DOI 10.1002/hbm.20655; Pennec X, 2009, LECT NOTES COMPUT SC, V5416, P347; Qiu AQ, 2008, NEUROIMAGE, V42, P1430, DOI 10.1016/j.neuroimage.2008.04.257; Qiu AQ, 2007, LECT NOTES COMPUT SC, V4791, P186; Raffelt D, 2011, NEUROIMAGE, V56, P1171, DOI 10.1016/j.neuroimage.2011.02.014; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Thompson P, 1996, IEEE T MED IMAGING, V15, P402, DOI 10.1109/42.511745; Trouve A, 1998, INT J COMPUT VISION, V28, P213, DOI 10.1023/A:1008001603737; Trouve A., 1995, TECHNICAL REPORT; Vaillant M, 2005, LECT NOTES COMPUT SC, V3565, P381; Vaillant M, 2004, NEUROIMAGE, V23, pS161, DOI 10.1016/j.neuroimage.2004.07.023; Vercauteren T, 2008, LECT NOTES COMPUT SC, V5241, P754, DOI 10.1007/978-3-540-85988-8_90; Vercauteren T, 2009, NEUROIMAGE, V45, pS61, DOI 10.1016/j.neuroimage.2008.10.040; Vialard FX, 2012, INT J COMPUT VISION, V97, P229, DOI 10.1007/s11263-011-0481-8; Wang L, 2007, IEEE T MED IMAGING, V26, P462, DOI 10.1109/TMI.2006.887380; WOJTYNSKI W, 1994, STUD MATH, V111, P163, DOI 10.4064/sm-111-2-163-185; Yang CJ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P464; Yang XF, 2011, LECT NOTES COMPUT SC, V6801, P257, DOI 10.1007/978-3-642-22092-0_22; Yang XF, 2011, NEUROIMAGE, V56, P149, DOI 10.1016/j.neuroimage.2011.01.069; Younes L, 2010, SHAPES DIFFEOMORPHIS, V171, P1; Younes L, 2007, Q APPL MATH, V65, P113, DOI 10.1090/S0033-569X-07-01027-5	44	6	6	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2015	115	2					69	86		10.1007/s11263-015-0802-4	http://dx.doi.org/10.1007/s11263-015-0802-4			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CS7SL					2022-12-18	WOS:000362285700001
J	Demetz, O; Hafner, D; Weickert, J				Demetz, Oliver; Hafner, David; Weickert, Joachim			Morphologically Invariant Matching of Structures with the Complete Rank Transform	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Computer vision; Optical flow; Invariance; Illumination change; TGV	OPTICAL-FLOW ESTIMATION; COMPUTATION; ACCURACY; PATTERN	Invariances are one of the key concepts to render computer vision algorithms robust against severe illumination changes. However, there is no free lunch: With any invariance comes an unavoidable loss of information. The goal of our paper is to introduce two novel descriptors which minimise this loss: the complete rank transform and the complete census transform. They are invariant under monotonically increasing intensity rescalings, while containing a maximum possible amount of information. To analyse our descriptors, we embed them as constancy assumptions into a variational framework for optic flow computation. As a suitable regularisation term, we choose total generalised variation that favours piecewise affine solutions. Our experiments focus on the KITTI benchmark where robustness w.r.t. illumination changes is one of the main issues. The results demonstrate that our descriptors yield state-of-the-art accuracy.	[Demetz, Oliver; Hafner, David; Weickert, Joachim] Univ Saarland, Math Image Anal Grp, D-66123 Saarbrucken, Germany	Saarland University	Demetz, O (corresponding author), Univ Saarland, Math Image Anal Grp, D-66123 Saarbrucken, Germany.	demetz@mia.uni-saarland.de			Cluster of Excellence Multimodal Computing and Interaction within the Excellence Initiative of the German Federal Government; Deutsche Forschungsgemeinschaft	Cluster of Excellence Multimodal Computing and Interaction within the Excellence Initiative of the German Federal Government; Deutsche Forschungsgemeinschaft(German Research Foundation (DFG))	Our research is partly funded by the Cluster of Excellence Multimodal Computing and Interaction within the Excellence Initiative of the German Federal Government, and by the Deutsche Forschungsgemeinschaft through a Gottfried Wilhelm Leibniz Prize for Joachim Weickert. This is gratefully acknowledged.	ALVAREZ L, 1993, ARCH RATION MECH AN, V123, P199, DOI 10.1007/BF00375127; [Anonymous], P VIS MOD VIS WORKSH; Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2; Bhat DN, 1998, IEEE T PATTERN ANAL, V20, P415, DOI 10.1109/34.677275; Braux-Zin J, 2013, IEEE I CONF COMP VIS, P185, DOI 10.1109/ICCV.2013.30; Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Bruhn A, 2005, IEEE I CONF COMP VIS, P749; Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222; Chan CH, 2012, IEEE T INF FOREN SEC, V7, P602, DOI 10.1109/TIFS.2011.2175920; CHARBONNIER P, 1994, IEEE IMAGE PROC, P168; Chen J, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.122; Demetz O, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.50; Demetz O, 2014, LECT NOTES COMPUT SC, V8689, P455, DOI 10.1007/978-3-319-10590-1_30; Feng Tang, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2631, DOI 10.1109/CVPRW.2009.5206550; Froba B, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P91, DOI 10.1109/AFGR.2004.1301514; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Gelfand I. M., 2000, CALCULUS VARIATIONS; Gennert M., 1987, 975 MIT ART INT LAB; Grewenig S., 2013, 327 SAARL U DEP MATH; Hafner D, 2013, LNCS, P210, DOI DOI 10.1007/978-3-642-38267-3; Hermann Simon, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P556, DOI 10.1007/978-3-642-37484-5_45; Hewer A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.129; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Kim TH, 2013, IEEE I CONF COMP VIS, P3344, DOI 10.1109/ICCV.2013.415; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mei X, 2011, PROC CVPR IEEE, P1257; Mileva Y., 2007, LECT NOTES COMPUTER; Mittal A., 2006, 2006 IEEE COMP SOC C, V1, P849; Mohamed MA, 2014, IEEE T CIRC SYST VID, V24, P1499, DOI 10.1109/TCSVT.2014.2308628; Muller T., 2011, LECT NOTES COMPUTER; Otte M., 1994, LECT NOTES COMPUTER, V800, DOI [10.1007/3-540-57956-7_5, DOI 10.1007/3-540-57956-7]; Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y; Pietikainen M, 2011, COMPUT IMAGING VIS, V40, P1; Press W.H., 2007, NUMERICAL RECIPES; Puxbaum P., 2010, ADV VISUAL COMPUTI 1, V6453; Ranftl R, 2014, LECT NOTES COMPUT SC, V8689, P439, DOI 10.1007/978-3-319-10590-1_29; Ranftl R, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P401, DOI 10.1109/IVS.2012.6232171; Rashwan HA, 2013, LECT NOTES COMPUT SC, V8142, P354, DOI 10.1007/978-3-642-40602-7_38; Sloane N. J. A., 1995, ENCY INTEGER SEQUENC; Soatto S, 2009, IEEE I CONF COMP VIS, P2138, DOI 10.1109/ICCV.2009.5459468; Stein F, 2004, LECT NOTES COMPUT SC, V3175, P79; Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x; Tukey J., 1977, EXPLORATORY DATA ANA; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; van de Weijer J, 2004, IEEE IMAGE PROC, P1835; Vogel C, 2013, LECT NOTES COMPUT SC, V8142, P343, DOI 10.1007/978-3-642-40602-7_37; Vogel CR, 1996, SIAM J SCI COMPUT, V17, P227, DOI 10.1137/0917016; Wang ZH, 2011, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2011.6126294; Wedel A., 2008, LECT NOTES COMPUTER, V5604; Wei D., 2014, P IEEE INT C 3D VIS; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Werlberger M, 2010, PROC CVPR IEEE, P2464, DOI 10.1109/CVPR.2010.5539945; Xu L, 2010, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2010.5539820; Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345; Zimmer H, 2011, INT J COMPUT VISION, V93, P368, DOI 10.1007/s11263-011-0422-6; Zimmer H, 2009, LECT NOTES COMPUT SC, V5681, P207, DOI 10.1007/978-3-642-03641-5_16	58	6	6	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2015	113	3			SI		220	232		10.1007/s11263-015-0800-6	http://dx.doi.org/10.1007/s11263-015-0800-6			13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CK3BZ					2022-12-18	WOS:000356091900006
J	Pertuz, S; Garcia, MA; Puig, D				Pertuz, Said; Angel Garcia, Miguel; Puig, Domenec			Efficient Focus Sampling Through Depth-of-Field Calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Depth-of-field; Autofocus; Sampling; Calibration; Defocus model	FOCAL LENGTH; MODEL; BLUR; SYSTEMS	Due to the limited depth-of-field (DOF) of conventional digital cameras, only the objects within a certain distance range from the camera are in focus. Objects outside the DOF are observed with different amounts of defocus depending on their position. Focus sampling consists of capturing different images of the same scene by changing the focus configuration of the camera in order to alternately bring objects at different depths into focus. Focus sampling is an important part of different focus-related applications such as autofocus, focus stacking and depth estimation. This work proposes a calibration procedure for modeling the depth-of-field of conventional cameras in order to perform an efficient focus sampling. The method is simple in terms of repeatability and can be easily implemented in different imaging devices. Experimental tests are presented in order to illustrate the effectiveness of the proposed approach in autofocus. Results demonstrate that a significant reduction in the number of frames required to capture during autofocusing can be achieved by means of the proposed methodology.	[Pertuz, Said] Univ Ind Santander, Escuela Ingn Elect Elect & Telecomunicac, Bucaramanga, Colombia; [Angel Garcia, Miguel] Autonomous Univ Madrid, Dept Elect & Commun Technol, E-28049 Madrid, Spain; [Puig, Domenec] Univ Rovira & Virgili, Dept Comp Sci & Math, E-43007 Tarragona, Spain	Universidad Industrial de Santander; Autonomous University of Madrid; Universitat Rovira i Virgili	Pertuz, S (corresponding author), Univ Ind Santander, Escuela Ingn Elect Elect & Telecomunicac, Bucaramanga, Colombia.	spertuz@uis.edu.co; miguelangel.garcia@uam.es; domenec.puig@urv.cat	Pertuz, Said/I-5226-2019; Garcia, Miguel Angel/C-4304-2014	Pertuz, Said/0000-0001-8498-9917; Garcia, Miguel Angel/0000-0003-2611-6821				Aggarwal M, 2002, INT J COMPUT VISION, V48, P195, DOI 10.1023/A:1016324132583; Aguet F, 2008, IEEE T IMAGE PROCESS, V17, P1144, DOI 10.1109/TIP.2008.924393; Almeida MSC, 2010, IEEE T IMAGE PROCESS, V19, P36, DOI 10.1109/TIP.2009.2031231; Asada N., 2001, P VIS INT, P165; Baba M., 2002, P ICVI, P274; Bac S, 2007, COMPUT GRAPH FORUM, V26, P571, DOI 10.1111/j.1467-8659.2007.01080.x; Bass M., 2010, HANDBOOD OF OPTICS, V1; Born M., 1999, PRINCIPLES OPTICS, Vseventh, DOI DOI 10.1017/CBO9781139644181; Darrell T., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P504, DOI 10.1109/CVPR.1988.196282; de Angelis M, 1999, OPT COMMUN, V160, P5, DOI 10.1016/S0030-4018(98)00636-1; Denton M. B., 2000, FURTHER DEV SCI OPTI; Deschenes F, 2004, IMAGE VISION COMPUT, V22, P35, DOI 10.1016/j.imavis.2003.08.003; Edmund Optics, 2012, GAUG DEPTH FIELD YOU; Elder JH, 1998, IEEE T PATTERN ANAL, V20, P699, DOI 10.1109/34.689301; Favaro P., 2007, P IEEE INT C COMP VI, P1; Fleming D., 2012, DEPTH FIELD CALCULAT; FUJITA H, 1992, IEEE T MED IMAGING, V11, P34, DOI 10.1109/42.126908; Hasinoff SW, 2011, IEEE T PATTERN ANAL, V33, P2203, DOI 10.1109/TPAMI.2011.62; Hasinoff SW, 2009, IEEE I CONF COMP VIS, P333, DOI 10.1109/ICCV.2009.5459269; Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468; HORN BKP, 1990, ROBOT VISION; Hornberg A., 2006, HDB MACHINE VISION; Joshi N., 2008, P IEEE COMP SOC C CO; JUDY P F, 1976, Medical Physics (Woodbury), V3, P233, DOI 10.1118/1.594283; Kayargadde V, 1996, INT J IMAG SYST TECH, V7, P102, DOI 10.1002/(SICI)1098-1098(199622)7:2<102::AID-IMA6>3.0.CO;2-C; LEI F, 1994, APPL OPTICS, V33, P6603, DOI 10.1364/AO.33.006603; Muhammad MS, 2012, IEEE T PATTERN ANAL, V34, P564, DOI 10.1109/TPAMI.2011.144; Nair H. N., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P309, DOI 10.1109/CVPR.1992.223258; PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940; Pertuz S., 2010, P INT C PATT REC; Pertuz S, 2013, PATTERN RECOGN, V46, P1415, DOI 10.1016/j.patcog.2012.11.011; Pertuz S, 2013, IEEE T IMAGE PROCESS, V22, P1242, DOI 10.1109/TIP.2012.2231087; Samei E, 1998, MED PHYS, V25, P102, DOI 10.1118/1.598165; Subbarao M., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P498, DOI 10.1109/CVPR.1988.196281; Szeliski R., 2011, COMPUTER VISION ALGO; Tay CJ, 2005, OPT COMMUN, V248, P339, DOI 10.1016/j.optcom.2004.12.036; Tsai DC, 2013, IEEE T CONSUM ELECTR, V59, P731, DOI 10.1109/TCE.2013.6689683; Tsai DC, 2012, IEEE T IMAGE PROCESS, V21, P459, DOI 10.1109/TIP.2011.2164417; Vaquero D., 2011, 2011 IEEE WORKSH APP, P511, DOI [10.1109/wacv.2011.5711547, DOI 10.1109/WACV.2011.5711547]; Watanabe M, 1998, INT J COMPUT VISION, V27, P203, DOI 10.1023/A:1007905828438; Watanabe M., 1995, TECHNICAL REPORT; WILLSON R, 1994, THESIS CARNEGIE MELL; Wu SX, 2009, GLOB TELECOMM CONF, P5673; Yousefi S, 2011, IEEE T CONSUM ELECTR, V57, P1003, DOI 10.1109/TCE.2011.6018848	45	6	6	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2015	112	3					342	353		10.1007/s11263-014-0770-0	http://dx.doi.org/10.1007/s11263-014-0770-0			12	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	CG3EA					2022-12-18	WOS:000353159600005
J	Holzer, S; Ilic, S; Tan, D; Pollefeys, M; Navab, N				Holzer, Stefan; Ilic, Slobodan; Tan, David; Pollefeys, Marc; Navab, Nassir			Efficient Learning of Linear Predictors for Template Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Template tracking; Linear predictors; Learning; Dimensionality reduction; Discrete cosine transform (DCT)	ONLINE	The research on tracking templates or image patches in a sequence of images has been largely dominated by energy-minimization-based methods. However, since its introduction in Jurie and Dhome (IEEE Trans Pattern Anal Mach Intell, 2002), the learning-based approach called linear predictors has proven to be an efficient and reliable alternative for template tracking, demonstrating superior tracking speed and robustness. But, their time intensive learning procedure prevented their use in applications where online learning is essential. Indeed, Holzer et al. (Adaptive linear predictors for real-time tracking, 2010) presented an iterative method to learn linear predictors; but it starts with a small template that makes it unstable at the beginning. Therefore, we propose three methods for highly efficient learning of full-sized linear predictors-where the first one is based on dimensionality reduction using the discrete cosine transform; the second is based on an efficient reformulation of the learning equations; and, the third is a combination of both. They show different characteristics with respect to learning time and tracking robustness, which makes them suitable for different scenarios.	[Holzer, Stefan; Ilic, Slobodan; Tan, David; Navab, Nassir] Tech Univ Munich, Fak Informat, D-85748 Garching, Germany; [Pollefeys, Marc] Swiss Fed Inst Technol, Inst Visual Comp, Dept Comp Sci, Comp Vis & Geometry Lab CVG, CH-8092 Zurich, Switzerland	Technical University of Munich; Swiss Federal Institutes of Technology Domain; ETH Zurich	Holzer, S (corresponding author), Tech Univ Munich, Fak Informat, I-16,Boltzmannstr 3, D-85748 Garching, Germany.	holzers@in.tum.de; Slobodan.Ilic@in.tum.de; tanda@in.tum.de; marc.pollefeys@inf.ethz.ch; navab@in.tum.de	Pollefeys, Marc/I-7607-2013	Ilic, Slobodan/0000-0002-3413-1936	Willow Garage, Inc., California, USA	Willow Garage, Inc., California, USA	This work was partly funded by Willow Garage, Inc., California, USA.	Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Baker S., 2001, C COMP VIS PATT REC; Ben-Israel A., 2003, GEN INVERSES; Benhimane S, 2007, INT J ROBOT RES, V26, P661, DOI 10.1177/0278364907080252; Dame A., 2010, 2010 9 IEEE INT S MI; DELLAERT F, 1999, ICCV WORKSH FRAM RAT; Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19; Grassl C., 2003, P PATT REC 25 DAGM S; Grassl C., 2004, IMAGE ANAL INTERPRET; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Hinterstoisser S., 2008, C COMP VIS PATT REC; Hinterstoisser S., 2010, IEEE COMP SOC C COMP; Hinterstoisser S., 2012, AS C COMP VIS; Hinterstoisser S., 2009, IEEE COMP SOC C COMP; Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326; Holzer S., 2010, IEEE COMP SOC C COMP; Holzer S., 2012, AS C COMP VIS; Holzer S., 2009, IEEE COMP SOC C COMP; Holzer S., 2012, 12 EUR C COMP VIS EC; Holzer S, 2013, IEEE T PATTERN ANAL, V35, P105, DOI 10.1109/TPAMI.2012.86; Jurie F, 2002, IEEE T PATTERN ANAL, V24, P996, DOI 10.1109/TPAMI.2002.1017625; Jurie F., 2002, BRIT MACH VIS C; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Klein G, 2008, LECT NOTES COMPUT SC, V5303, P802, DOI 10.1007/978-3-540-88688-4_59; Klein George, 2007, P1; La Cascia M, 2000, IEEE T PATTERN ANAL, V22, P322, DOI 10.1109/34.845375; Lieberknecht S, 2009, INT SYM MIX AUGMENT, P145, DOI 10.1109/ISMAR.2009.5336487; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucas Bruce D, 1981, P 7 INT JOINT C ART; Malis E, 2004, IEEE INT CONF ROBOT, P1843, DOI 10.1109/ROBOT.2004.1308092; Matas J., 2006, COMPUTER VISION GRAP; Mayol WW, 2008, MACH VISION APPL, V19, P65, DOI 10.1007/s00138-007-0087-x; Ozuysal M, 2007, C COMP VIS PATT REC; Parisot Pascaline, 2007, 2007 Third International IEEE Conference on Signal-Image Technologies and Internet-Based System (SITIS), P891, DOI 10.1109/SITIS.2007.83; Richa R., 2011, 2011 IEEE RSJ INT C; Shum HY, 2000, INT J COMPUT VISION, V36, P101, DOI 10.1023/A:1008195814169; Vacchetti L, 2004, IEEE T PATTERN ANAL, V26, P1385, DOI 10.1109/TPAMI.2004.92; Zimmermann K, 2009, IEEE T PATTERN ANAL, V31, P677, DOI 10.1109/TPAMI.2008.119	38	6	6	0	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2015	111	1					12	28		10.1007/s11263-014-0729-1	http://dx.doi.org/10.1007/s11263-014-0729-1			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AZ6RC					2022-12-18	WOS:000348345400002
J	Anderson, R; Stenger, B; Cipolla, R				Anderson, Robert; Stenger, Bjoern; Cipolla, Roberto			Using Bounded Diameter Minimum Spanning Trees to Build Dense Active Appearance Models	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Active appearance models; Groupwise registration; Minimum spanning trees	IMAGE REGISTRATION	We present a method for producing dense active appearance models (AAMs), suitable for video-realistic synthesis. To this end we estimate a joint alignment of all training images using a set of pairwise registrations and ensure that these pairwise registrations are only calculated between similar images. This is achieved by defining a graph on the image set whose edge weights correspond to registration errors and computing a bounded diameter minimum spanning tree. Dense optical flow is used to compute pairwise registration and a flow refinement method to align small scale texture is introduced. Further, given the registration of training images, vertices are added to the AAM to minimise the error between the observed flow fields and the flow fields interpolated between the AAM mesh points. We demonstrate a significant improvement in model compactness.	[Anderson, Robert; Cipolla, Roberto] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England; [Stenger, Bjoern] Toshiba Res Europe Ltd, Cambridge CB4 0GZ, England	University of Cambridge; Toshiba Corporation	Anderson, R (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.	ra312@cam.ac.uk	Arandjelović, Ognjen/V-5255-2019	Arandjelović, Ognjen/0000-0002-9314-194X; Cipolla, Roberto/0000-0002-8999-2151				Abboud B, 2004, SIGNAL PROCESS-IMAGE, V19, P723, DOI 10.1016/j.image.2004.05.009; Baker S, 2004, IEEE T PATTERN ANAL, V26, P1380, DOI 10.1109/TPAMI.2004.77; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Brox T, 2009, PROC CVPR IEEE, P41, DOI 10.1109/CVPRW.2009.5206697; Cootes T., 2005, 16 BRIT MACH VIS C, V2, P879; Cootes T. F., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P484, DOI 10.1007/BFb0054760; Cootes TF, 2001, PROC SPIE, V4322, P236, DOI 10.1117/12.431093; Cootes TF, 2010, IEEE T PATTERN ANAL, V32, P1994, DOI 10.1109/TPAMI.2009.193; Cristinacce D., 2008, MLVMA WORKSH ECCV; Deena S., 2010, ICMI MLMI, P1; DEFLORIANI L, 1989, IEEE COMPUT GRAPH, V9, P67, DOI 10.1109/38.19053; Garey M.R., 1979, COMPUTERS INTRACTABI; Gupta P., 2000, C NUMERANTIUM, V144, P161; Hamm J, 2010, MED IMAGE ANAL, V14, P633, DOI 10.1016/j.media.2010.06.001; Hernandez M, 2009, INT J COMPUT VISION, V85, P291, DOI 10.1007/s11263-009-0219-z; Hill DLG, 2001, PHYS MED BIOL, V46, pR1, DOI 10.1088/0031-9155/46/3/201; Julstrom B., 2009, J EXPT ALGORITHMICS, V14; Klaudiny M, 2012, LECT NOTES COMPUT SC, V7575, P743, DOI 10.1007/978-3-642-33765-9_53; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Learned-Miller EG, 2006, IEEE T PATTERN ANAL, V28, P236, DOI 10.1109/TPAMI.2006.34; Liu C., 2009, PIXELS EXPLORING NEW; Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3; Ma B, 2000, IEEE IMAGE PROC, P481, DOI 10.1109/ICIP.2000.901000; Marsland S, 2003, LECT NOTES COMPUT SC, V2879, P771; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Mittrapiyanuruk P, 2004, IEEE INT CONF ROBOT, P5147, DOI 10.1109/ROBOT.2004.1302534; Peng YG, 2010, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2010.5540138; Ramnath K., 2008, CVPR, P1; Sabuncu MR, 2008, IEEE T IMAGE PROCESS, V17, P788, DOI 10.1109/TIP.2008.918951; Saragih J., 2006, P HCSNET WORKSH US V, V56, P51; Saragih J, 2007, IEEE I CONF COMP VIS, P2173; Sidorov K. A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2401, DOI 10.1109/CVPR.2011.5995632; Sidorov KA, 2009, PROC CVPR IEEE, P2208, DOI 10.1109/CVPRW.2009.5206516; Singh A, 2007, SOFT COMPUT, V11, P911, DOI 10.1007/s00500-006-0142-y; Smith BM, 2012, LECT NOTES COMPUT SC, V7574, P43, DOI 10.1007/978-3-642-33712-3_4; Theobald BJ, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P134; Tong Y, 2009, PROC CVPR IEEE, P2097, DOI 10.1109/CVPRW.2009.5206670; Vetter T, 1997, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.1997.609295; Walker K. N., 1999, BMVC99. Proceedings of the 10th British Machine Vision Conference, P463; Wang L, 2011, INTERSPEECH, P3307; Zen H, 2009, SPEECH COMMUN, V51, P1039, DOI 10.1016/j.specom.2009.04.004; Zhao C, 2011, PROC CVPR IEEE, P561, DOI 10.1109/CVPR.2011.5995381; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	43	6	6	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	OCT	2014	110	1			SI		48	57		10.1007/s11263-013-0661-9	http://dx.doi.org/10.1007/s11263-013-0661-9			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AQ0BZ					2022-12-18	WOS:000342448000005
J	Guo, G; Wang, YZ; Jiang, TT; Yuille, AL; Fang, F; Gao, W				Guo, Ge; Wang, Yizhou; Jiang, Tingting; Yuille, Alan L.; Fang, Fang; Gao, Wen			A Shape Reconstructability Measure of Object Part Importance with Applications to Object Detection and Localization	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape part; Part importance; Shape reconstruction; Object recognition and detection	RECOGNITION	We propose a computational model which computes the importance of 2-D object shape parts, and we apply it to detect and localize objects with and without occlusions. The importance of a shape part (a localized contour fragment) is considered from the perspective of its contribution to the perception and recognition of the global shape of the object. Accordingly, the part importance measure is defined based on the ability to estimate/recall the global shapes of objects from the local part, namely the part's "shape reconstructability". More precisely, the shape reconstructability of a part is determined by two factors-part variation and part uniqueness. (i) Part variation measures the precision of the global shape reconstruction, i.e. the consistency of the reconstructed global shape with the true object shape; and (ii) part uniqueness quantifies the ambiguity of matching the part to the object, i.e. taking into account that the part could be matched to the object at several different locations. Taking both these factors into consideration, an information theoretic formulation is proposed to measure part importance by the conditional entropy of the reconstruction of the object shape from the part. Experimental results demonstrate the benefit with the proposed part importance in object detection, including the improvement of detection rate, localization accuracy, and detection efficiency. By comparing with other state-of-the-art object detectors in a challenging but common scenario, object detection with occlusions, we show a considerable improvement using the proposed importance measure, with the detection rate increased over . On a subset of the challenging PASCAL dataset, the Interpolated Average Precision (as used in the PASCAL VOC challenge) is improved by 4-8 %. Moreover, we perform a psychological experiment which provides evidence suggesting that humans use a similar measure for part importance when perceiving and recognizing shapes.	[Guo, Ge; Wang, Yizhou; Jiang, Tingting; Gao, Wen] Peking Univ, Key Lab Machine Percept MoE, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China; [Yuille, Alan L.] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA USA; [Fang, Fang] Peking Univ, Dept Psychol, Peking Tsinghua Ctr Life Sci, PKU IDG McGovern Inst Brain Res, Beijing 100871, Peoples R China; [Fang, Fang] Peking Univ, Key Lab Machine Percept MoE, Peking Tsinghua Ctr Life Sci, PKU IDG McGovern Inst Brain Res, Beijing 100871, Peoples R China	Peking University; University of California System; University of California Los Angeles; Peking University; Peking University	Wang, YZ (corresponding author), Peking Univ, Key Lab Machine Percept MoE, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.	gguo@jdl.ac.cn; Yizhou.Wang@pku.edu.cn; ttjiang@pku.edu.cn; yuille@stat.ucla.edu; ffang@pku.edu.cn; wgao@pku.edu.cn		Yuille, Alan L./0000-0001-5207-9249; Jiang, Tingting/0000-0002-5372-0656	Office of Naval Research [N00014-12-1-0883];  [973-2011CBA00400];  [NSFC-61272027];  [NSFC-61121002];  [NSFC-61231010];  [NSFC-61210005];  [NSFC-61103087];  [NSFC-31230029]	Office of Naval Research(Office of Naval Research); ; ; ; ; ; ; 	We'd like to thank for the support from the following research grants 973-2011CBA00400, NSFC-61272027, NSFC-61121002, NSFC-61231010, NSFC-61210005, NSFC-61103087, NSFC-31230029, and Office of Naval Research N00014-12-1-0883.	Bai X, 2008, IEEE T PATTERN ANAL, V30, P1282, DOI 10.1109/TPAMI.2007.70769; Beck J., 1982, ORG REPRESENTATION P, P167; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BIEDERMAN I, 1991, COGNITIVE PSYCHOL, V23, P393, DOI 10.1016/0010-0285(91)90014-F; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Bouchard G., 2005, IEEE C COMP VIS PATT; Bower G. H., 2011, J EXP PSYCHOL-HUM L, V2, P456; Cai H., 2010, IEEE C COMP VIS PATT; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Crandall D, 2006, EUR C COMP VIS; Dubinskiy A., 2003, P IEEE INT C COMP VI; Duchi J., 2008, INT C MACH LEARN; Epshtein B., 2007, IEEE C COMP VIS PATT; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb P. F., 2010, IEEE C COMP VIS PATT; Felzenszwalb P. F., 2009, COMPUTER VISION PATT; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Ferrari V., 2009, INT J COMPUT VISION, P2; Ferrari V., 2008, IEEE T PATTERN ANAL; FERRARI V, 2006, EUR C COMP VIS ECCV; Freifeld O., 2010, IEEE C COMP VIS PATT; Gopalan R., 2010, EUR C COMP VIS; Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3; HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; Ion A, 2011, COMPUT VIS IMAGE UND, V115, P817, DOI 10.1016/j.cviu.2011.02.006; JURIE F, 2004, IEEE C COMP VIS PATT; Kersten D., 2004, ANN REV PSYCHOL; Kimia BB, 2003, INT J COMPUT VISION, V54, P157, DOI 10.1023/A:1023713602895; Kira K., 1992, 9 INT C MACH LEARN; Lin L., 2012, IEEE C COMP VIS PATT; Liu H, 2010, IEEE C COMP VIS PATT; Lu C., 2009, P IEEE INT C COMP VI; Luo P., 2010, EUR C COMP VIS; Ma T., 2011, IEEE C COMP VIS PATT; Maji S., 2009, IEEE C COMP VIS PATT; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Mikolajczyk K., 2004, EUR C COMP VIS; Ommer B., 2009, INT C COMP VIS; Opelt A, 2008, INT J COMPUT VISION, V80, P16, DOI 10.1007/s11263-008-0139-3; Rachmawati L, 2010, IEEE C EVOL COMPUTAT; Ravishankar S., 2008, EUR C COMP VIS; Renninger LW, 2007, J VISION, V7, DOI 10.1167/7.3.6; Rensink RA, 1998, VISION RES, V38, P2489, DOI 10.1016/S0042-6989(98)00051-0; Riemenschneider H., 2010, EUR C COMP VIS; Sala P., 2010, EUR C COMP VIS; Schneiderman H., 2004, INT J COMPUT VISION, V60, P135; Schnitzspan P., 2010, IEEE C COMP VIS PATT; Sharvit D, 1998, J VIS COMMUN IMAGE R, V9, P366, DOI 10.1006/jvci.1998.0396; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Shotton Jamie, 2008, IEEE T PATTERN ANAL, V1; Siddiqi K, 1996, PERCEPTION, V25, P399, DOI 10.1068/p250399; Sukumar S.R., 2006, P 3 INT S 3D DAT PRO; Toshev A, 2012, INT J COMPUT VISION, V99, P123, DOI 10.1007/s11263-012-0521-z; Ullman S, 2007, TRENDS COGN SCI, V11, P58, DOI 10.1016/j.tics.2006.11.009; Wang Xiaoyu, 2012, IEEE C COMP VIS PATT; Yarlagadda P., 2010, EUR C COMP VIS; Yarlagadda P., 2012, EUR C COMP VIS; Zhu L., 2010, IEEE C COMP VIS PATT; Zhu Q., 2008, EUR C COMP VIS; Zhu S. C., 1998, IEEE T PATTERN ANAL	62	6	7	0	22	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2014	108	3					241	258		10.1007/s11263-014-0705-9	http://dx.doi.org/10.1007/s11263-014-0705-9			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science	AH8NO					2022-12-18	WOS:000336394900005
J	Maji, S; Shakhnarovich, G				Maji, Subhransu; Shakhnarovich, Gregory			Part and Attribute Discovery from Relative Annotations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Relative annotations; Crowdsourcing; Semantic parts; Fine-grained attributes	POSE	Part and attribute based representations are widely used to support high-level search and retrieval applications. However, learning computer vision models for automatically extracting these from images requires significant effort in the form of part and attribute labels and annotations. We propose an annotation framework based on comparisons between pairs of instances within a set, which aims to reduce the overhead in manually specifying the set of part and attribute labels. Our comparisons are based on intuitive properties such as correspondences and differences, which are applicable to a wide range of categories. Moreover, they require few category specific instructions and lead to simple annotation interfaces compared to traditional approaches. On a number of visual categories we show that our framework can use noisy annotations collected via "crowdsourcing" to discover semantic parts useful for detection and parsing, as well as attributes suitable for fine-grained recognition.	[Maji, Subhransu; Shakhnarovich, Gregory] Toyota Technol Inst, 6045 S Kenwood Ave, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago	Maji, S (corresponding author), Toyota Technol Inst, 6045 S Kenwood Ave, Chicago, IL 60637 USA.	smaji@ttic.edu						Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; [Anonymous], 2010, CALIFORNIA I TECHNOL; Berg T., 2010, EUR C COMP VIS; Blei D.M., 2003, P 26 ANN INT ACM SIG, P127, DOI [10.1145/860435.860460, DOI 10.1145/860435.860460]; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bourdev L., 2009, INT C COMP VIS; Bourdev L., 2010, EUR C COMP VIS; Bourdev L. D., 2011, INT C COMP VIS; Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32; Brown P. F., 1990, Computational Linguistics, V16, P79; BUSH V, ATLANTIC MONTHLY; Cimpoi M., 2014, COMPUTER VISION PATT; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Desai C, 2012, LECT NOTES COMPUT SC, V7575, P158, DOI 10.1007/978-3-642-33765-9_12; Duan K, 2012, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2012.6248089; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Farhadi A., 2010, COMPUTER VISION PATT; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468; Frome A., 2007, ADV NEURAL INFORM PR, V19, P417; Girshick RB, 2012, DISCRIMINATIVELY TRA; Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33; Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; Kovashka A, 2012, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2012.6248026; Kumar N., 2008, EUR C COMP VIS; Leibe B., 2004, EUROPEAN C COMPUTER, P17; Maji S., 2013, COMPUTER VISION PATT; Maji S., 2012, 2 INT WORKSH PARTS A; Maji S., 2012, HUM COMP WORKSH AAAI; Maji Subhransu, 2011, UCBEECS201179; Malisiewicz T., 2011, INT C COMP VIS; Malisiewicz T., 2009, ADV NEURAL INF PROCE, V22, P1222; Parikh D., 2011, WORKSH FIN GRAIN VIS; Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999; Tamuz O., 2011, INT C MACH LEARN ICM; von Ahn L., 2006, P SIGCHI C HUMAN FAC, P55, DOI DOI 10.1145/1124772.1124782; Von Ahn Luis, 2004, P SIGCHI C HUM FACT, P319, DOI DOI 10.1145/985692.985733; Weber M., 2000, COMPUTER VISION PATT; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	45	6	6	0	6	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2014	108	1-2			SI		82	96		10.1007/s11263-014-0716-6	http://dx.doi.org/10.1007/s11263-014-0716-6			15	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AG7BT					2022-12-18	WOS:000335573700006
J	Collard, A; Bonnabel, S; Phillips, C; Sepulchre, R				Collard, Anne; Bonnabel, Silvere; Phillips, Christophe; Sepulchre, Rodolphe			Anisotropy Preserving DTI Processing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Diffusion tensor MRI; Interpolation; Spectral decomposition; Anisotropy; Quaternions; Riemannian manifold	POSITIVE SEMIDEFINITE MATRICES; DIFFUSION TENSOR IMAGES; LOG-EUCLIDEAN METRICS; GEOMETRIC MEANS; DEFORMABLE REGISTRATION; RIEMANNIAN FRAMEWORK; DEFINITE MATRICES; MRI; ORIENTATION; STATISTICS	Statistical analysis of diffusion tensor imaging (DTI) data requires a computational framework that is both numerically tractable (to account for the high dimensional nature of the data) and geometric (to account for the nonlinear nature of diffusion tensors). Building upon earlier studies exploiting a Riemannian framework to address these challenges, the present paper proposes a novel metric and an accompanying computational framework for DTI data processing. The proposed approach grounds the signal processing operations in interpolating curves. Well-chosen interpolating curves are shown to provide a computational framework that is at the same time tractable and information relevant for DTI processing. In addition, and in contrast to earlier methods, it provides an interpolation method which preserves anisotropy, a central information carried by diffusion tensor data.	[Collard, Anne; Phillips, Christophe; Sepulchre, Rodolphe] Univ Liege, Dept Elect Engn & Comp Sci, B-4000 Liege, Belgium; [Bonnabel, Silvere] Mines Paris Tech, Robot Lab, Math & Syst, F-75006 Paris, France; [Phillips, Christophe] Univ Liege, Cyclotron Res Ctr, B-4000 Liege, Belgium; [Sepulchre, Rodolphe] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Liege; UDICE-French Research Universities; PSL Research University Paris; MINES ParisTech; University of Liege; University of Cambridge	Collard, A (corresponding author), Univ Liege, Dept Elect Engn & Comp Sci, B-4000 Liege, Belgium.	anne.collard@ulg.ac.be	Phillips, Christophe/M-3025-2017	Phillips, Christophe/0000-0002-4990-425X				Alexander DC, 2000, COMPUT VIS IMAGE UND, V77, P233, DOI 10.1006/cviu.1999.0817; Alexander DC, 2001, IEEE T MED IMAGING, V20, P1131, DOI 10.1109/42.963816; Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996; Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965; Awate SP, 2007, LECT NOTES COMPUT SC, V4791, P294; BASSER PJ, 1994, BIOPHYS J, V66, P259, DOI 10.1016/S0006-3495(94)80775-1; Basser PJ, 1996, J MAGN RESON SER B, V111, P209, DOI [10.1006/jmrb.1996.0086, 10.1016/j.jmr.2011.09.022]; Batchelor PG, 2005, MAGN RESON MED, V53, P221, DOI 10.1002/mrm.20334; Bonnabel S, 2013, LINEAR ALGEBRA APPL, V438, P3202, DOI 10.1016/j.laa.2012.12.009; Bonnabel S, 2009, SIAM J MATRIX ANAL A, V31, P1055, DOI 10.1137/080731347; BURBEA J, 1982, J MULTIVARIATE ANAL, V12, P575, DOI 10.1016/0047-259X(82)90065-3; Castano-Moraga CA, 2006, I S BIOMED IMAGING, P93; Castro FJS, 2007, I S BIOMED IMAGING, P45, DOI 10.1109/ISBI.2007.356784; Cetingul HE, 2012, 2012 9TH IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING (ISBI), P1389, DOI 10.1109/ISBI.2012.6235827; ChefD'Hotel C, 2004, J MATH IMAGING VIS, V20, P147, DOI 10.1023/B:JMIV.0000011324.14508.fb; Cheng J, 2009, LECT NOTES COMPUT SC, V5761, P911, DOI 10.1007/978-3-642-04268-3_112; Chiang MC, 2008, IEEE T MED IMAGING, V27, P442, DOI 10.1109/TMI.2007.907326; Dai YC, 2010, LECT NOTES COMPUT SC, V5995, P335; Dryden IL, 2009, ANN APPL STAT, V3, P1102, DOI 10.1214/09-AOAS249; Fillard P, 2007, IEEE T MED IMAGING, V26, P1472, DOI 10.1109/TMI.2007.899173; Fletcher PT, 2007, SIGNAL PROCESS, V87, P250, DOI 10.1016/j.sigpro.2005.12.018; Fletcher PT, 2009, NEUROIMAGE, V45, pS143, DOI 10.1016/j.neuroimage.2008.10.052; Goh A, 2011, NEUROIMAGE, V56, P1181, DOI 10.1016/j.neuroimage.2011.01.053; Goodlett CB, 2009, NEUROIMAGE, V45, pS133, DOI 10.1016/j.neuroimage.2008.10.060; Gur Y., 2007, P IEEE COMP SOC WORK, P1; Ingalhalikar M, 2010, INT J IMAG SYST TECH, V20, P99, DOI 10.1002/ima.20232; Jensen JH, 2010, NMR BIOMED, V23, P698, DOI 10.1002/nbm.1518; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; Kindlmann G, 2007, LECT NOTES COMPUT SC, V4791, P1; Lenglet C, 2009, NEUROIMAGE, V45, pS111, DOI 10.1016/j.neuroimage.2008.10.054; Lenglet C, 2006, J MATH IMAGING VIS, V25, P423, DOI 10.1007/s10851-006-6897-z; Lepore N, 2006, LECT NOTES COMPUT SC, V4190, P191; Moakher M, 2005, SIAM J MATRIX ANAL A, V26, P735, DOI 10.1137/S0895479803436937; Moakher M, 2011, J MATH IMAGING VIS, V40, P171, DOI 10.1007/s10851-010-0255-x; Ncube S., 2011, SPIE C SERIES, V7962, P7; Nesterov Y., 1994, SIAM, V13; Parker G. J. M., 2003, J MAGN RESON, V18, P245; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Petz D, 2005, SIAM J MATRIX ANAL A, V27, P712, DOI 10.1137/050621906; Sarlette A., 2007, P 17 IFAC S AUT CONT; SKOVGAARD LT, 1984, SCAND J STAT, V11, P211; Smith ST, 2005, IEEE T SIGNAL PROCES, V53, P1610, DOI 10.1109/TSP.2005.845428; Thevenaz P, 2000, IEEE T MED IMAGING, V19, P739, DOI 10.1109/42.875199; Tschumperle D, 2001, PROC CVPR IEEE, P948; Tuch DS, 2004, MAGN RESON MED, V52, P1358, DOI 10.1002/mrm.20279; Weldeselassie Y., 2009, MED IM COMP COMP ASS, P173; Weldeselassie YT, 2007, SPIE, V6512; Yeo B.T.T., 2008, P IEEE INT S BIOM IM; Yeo BTT, 2009, IEEE T MED IMAGING, V28, P1914, DOI 10.1109/TMI.2009.2025654; Zhang H, 2006, MED IMAGE ANAL, V10, P764, DOI 10.1016/j.media.2006.06.004; Zhou D., 2013, INT J COMPUT THEORY, V5, P108; Zhou D., 2010, THESIS U NOTTINGHAM; [No title captured]	55	6	9	1	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2014	107	1					58	74		10.1007/s11263-013-0674-4	http://dx.doi.org/10.1007/s11263-013-0674-4			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	AB2RT		Green Submitted			2022-12-18	WOS:000331640500004
J	Del Bue, A				Del Bue, Alessio			Adaptive Non-rigid Registration and Structure from Motion from Image Trajectories	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						2D-3D non-rigid registration; Structure from Motion; Factorization; GSVD	SHAPE; FACTORIZATION	This paper addresses the problem of registering a known 3D model to a set of 2D deforming image trajectories. The proposed approach can adapt to a scenario where the 3D model to register is not an exact description of the measured image data. This results in finding a 2D-3D registration, given the complexity of having both 2D deforming data and a coarse description of the image observations. The method acts in two distinct phases. First, an affine step computes a factorization for both the 2D image data and the 3D model using a joint subspace decomposition. This initial solution is then upgraded by finding the best projection to the image plane complying with the metric constraints given by a scaled orthographic camera. Both steps are computed efficiently in closed-form with the additional feature of being robust to degenerate motions which may possibly affect the 2D image data (i.e. lack of relevant rigid motion). A further extension of the approach allows to compute the full 3D deformations of the shape given the first initial (rigid) registration. This step results in solving a Non-rigid Structure from Motion (NRSfM) problem using the 3D known shape as a prior. Experimental results show the robustness of the method in registration tasks such as pose estimation and 3D reconstruction when degenerate image motion is present.	IIT, Pattern Anal & Comp Vis PAVIS, I-16163 Genoa, Italy	Istituto Italiano di Tecnologia - IIT	Del Bue, A (corresponding author), IIT, Pattern Anal & Comp Vis PAVIS, Via Morego 30, I-16163 Genoa, Italy.	alessio.delbue@iit.it		Del Bue, Alessio/0000-0002-2262-4872	Fundacao para a Ciencia e a Tecnologia through the POS_Conhecimento Program; "MODI-3D Models from 2D Images" [PTDC/EEA-ACR/72201/2006]	Fundacao para a Ciencia e a Tecnologia through the POS_Conhecimento Program; "MODI-3D Models from 2D Images"	This work was partially supported by Fundacao para a Ciencia e a Tecnologia (ISR/IST pluriannual funding) through the POS_Conhecimento Program (include FEDER funds) and grant PTDC/EEA-ACR/72201/2006, "MODI-3D Models from 2D Images". E. Munoz, J. Xiao and J. Peyras kindly made available sequences used in the experimental section.	Akhter I., 2009, P IEEE C COMP VIS PA; Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Bjorck A., 1996, NUMERICAL METHODS LE; Brand M, 2005, PROC CVPR IEEE, P122; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Del Bue A, 2007, IMAGE VISION COMPUT, V25, P297, DOI 10.1016/j.imavis.2005.10.004; Del Bue A, 2008, P IEEE C COMP VIS PA, P1; Del Bue A, 2006, P IEEE C COMP VIS PA, V1, P1191; Del Bue A, 2012, IEEE T PATTERN ANAL, V34, P1496, DOI 10.1109/TPAMI.2011.238; Del Bue A, 2010, LECT NOTES COMPUT SC, V6313, P87; Forsyth D. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P660, DOI 10.1109/ICCV.1999.791288; Hansen P., 1998, RANK DEFICIENT DISCR; Loog M., 2010, P BRIT MACH VIS C, P701; Marques M, 2009, COMPUT VIS IMAGE UND, V113, P261, DOI 10.1016/j.cviu.2008.09.004; Olsen SI, 2008, J MATH IMAGING VIS, V31, P233, DOI 10.1007/s10851-007-0060-3; Sayd P., 2008, P IEEE C COMP VIS PA, P1; SOLEM J, 2005, ADV NEURAL INFORM PR, V17; Stegmann MB, 2003, IEEE T MED IMAGING, V22, P1319, DOI 10.1109/TMI.2003.817780; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; Xiao J, 2004, PROC CVPR IEEE, P535; Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9; Yezzi AJ, 2003, INT J COMPUT VISION, V53, P153, DOI 10.1023/A:1023048024042	25	6	6	0	21	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2013	103	2			SI		226	239		10.1007/s11263-012-0577-9	http://dx.doi.org/10.1007/s11263-012-0577-9			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	150JC					2022-12-18	WOS:000319385400005
J	Sadek, R; Facciolo, G; Arias, P; Caselles, V				Sadek, Rida; Facciolo, Gabriele; Arias, Pablo; Caselles, Vicent			A Variational Model for Gradient-Based Video Editing	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Video editing; Gradient based; Variational methods	OPTICAL-FLOW; REMOVAL	In this work we present a gradient-based variational model for video editing, addressing the problem of propagating gradient-domain information along the optical flow of the video. The resulting propagation is temporally consistent and blends seamlessly with its spatial surroundings. In addition, the presented model is able to cope with additive illumination changes and handles occlusions/dis-occlusions. The problem of propagation along the optical flow arises in different video editing applications. In this work we consider the application where a user edits a frame by modifying the texture of an object's surface and wishes to propagate this editing throughout the video.	[Sadek, Rida; Arias, Pablo; Caselles, Vicent] Univ Pompeu Fabra, Barcelona 08018, Spain; [Facciolo, Gabriele] ENS Cachan, CMLA, F-94235 Cachan, France	Pompeu Fabra University; UDICE-French Research Universities; Universite Paris Saclay	Sadek, R (corresponding author), Univ Pompeu Fabra, Carrer Tanger 122-134, Barcelona 08018, Spain.	rida.sadek@upf.edu; gabriele.facciolo@cmla.ens-cachan.fr; pablo.arias@upf.edu; vicent.caselles@upf.edu	Facciolo, Gabriele/AGH-7776-2022; Sadek, Rida/I-3365-2015	Sadek, Rida/0000-0003-0476-8714	MICINN [MTM2009-08171]; GRC [2009 SGR 773]; FPI from the Spanish MICINN [BES-2007-14451]; ICREA Academia; Generalitat de Catalunya	MICINN(Ministry of Science and Innovation, Spain (MICINN)Spanish GovernmentEuropean Commission); GRC(Science Foundation Ireland); FPI from the Spanish MICINN; ICREA Academia(ICREA); Generalitat de Catalunya(Generalitat de Catalunya)	We acknowledge partial support by MICINN project, reference MTM2009-08171, and by GRC reference 2009 SGR 773. PA is supported by the FPI Grant BES-2007-14451 from the Spanish MICINN. VC also acknowledges partial support by "ICREA Academia" prize for excellence in research funded by the Generalitat de Catalunya.	Adams R. A., 2003, SOBOLEV SPACES, VSecond; Agrawal A., 2007, ICCV 07 SHORT COURS; Alvarez L, 2007, INT J COMPUT VISION, V75, P371, DOI 10.1007/s11263-007-0041-4; Ambrosio L., 2000, OX MATH M, DOI 10.1017/S0024609301309281; Arias P, 2011, INT J COMPUT VISION, V93, P319, DOI 10.1007/s11263-010-0418-7; Ayvaci A, 2012, INT J COMPUT VISION, V97, P322, DOI 10.1007/s11263-011-0490-7; Baker Simon, 2007, 2007 11th IEEE International Conference on Computer Vision, P1; Bhat P., 2007, P EUROGRAPHICS S REN, P327, DOI 10.2312; Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Bugeau A, 2010, MOD VIS WORKSH, P8; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Facciolo Gabriele, 2011, Energy Minimization Methods in Computer Vision and Pattern Recognition. Proceedings 8th International Conference, EMMCVPR 2011, P59, DOI 10.1007/978-3-642-23094-3_5; Fattal R, 2002, ACM T GRAPHIC, V21, P249; Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18; Georgiev T., 2005, IMAGE RECONSTRUCTION, P61; Horn B.K.P.., 1986, ROBOT VISION ELECT E; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Jia JY, 2006, IEEE T PATTERN ANAL, V28, P832, DOI 10.1109/TPAMI.2006.108; Kokaram AC, 2005, IEE P-VIS IMAGE SIGN, V152, P407, DOI 10.1049/ip-vis:20045152; Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263; Liu C, 2008, PROC CVPR IEEE, P3911; Meinhardt-Llopis E., 2012, PREPRINT; Negahdaripour S, 1998, IEEE T PATTERN ANAL, V20, P961, DOI 10.1109/34.713362; Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y; Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269; Rav-Acha A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360616; Sadek R, 2012, VARIATIONAL MODEL S; Salgado A, 2007, LECT NOTES COMPUT SC, V4739, P709; Sand Peter, 2008, International Journal of Computer Vision, V80, P72, DOI 10.1007/s11263-008-0136-6; Shiratori T., 2006, P IEEE C COMP VIS PA, V1, P411, DOI DOI 10.1109/CVPR.2006.330; Sun DQ, 2012, PROC CVPR IEEE, P1768, DOI 10.1109/CVPR.2012.6247873; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; Volz S, 2011, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2011.6126359; Wang HC, 2005, PROC CVPR IEEE, P1201; Weickert J., 2006, MATH MODELS REGISTRA; Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355; Zhang YJ, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P516; Zhou M. H, 1998, RELATION, V10, P7098	42	6	8	0	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2013	103	1					127	162		10.1007/s11263-012-0597-5	http://dx.doi.org/10.1007/s11263-012-0597-5			36	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	137CV					2022-12-18	WOS:000318413500006
J	Lin, WY; Cheong, LF; Tan, P; Dong, G; Liu, SY				Lin, Wen-Yan; Cheong, Loong-Fah; Tan, Ping; Dong, Guo; Liu, Siying			Simultaneous Camera Pose and Correspondence Estimation with Motion Coherence	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Structure from Motion; Registration	REPRESENTATION	Traditionally, the camera pose recovery problem has been formulated as one of estimating the optimal camera pose given a set of point correspondences. This critically depends on the accuracy of the point correspondences and would have problems in dealing with ambiguous features such as edge contours and high visual clutter. Joint estimation of camera pose and correspondence attempts to improve performance by explicitly acknowledging the chicken and egg nature of the pose and correspondence problem. However, such joint approaches for the two-view problem are still few and even then, they face problems when scenes contain largely edge cues with few corners, due to the fact that epipolar geometry only provides a "soft" point to line constraint. Viewed from the perspective of point set registration, the point matching process can be regarded as the registration of points while preserving their relative positions (i.e. preserving scene coherence). By demanding that the point set should be transformed coherently across views, this framework leverages on higher level perceptual information such as the shape of the contour. While thus potentially allowing registration of non-unique edge points, the registration framework in its traditional form is subject to substantial point localization error and is thus not suitable for estimating camera pose. In this paper, we introduce an algorithm which jointly estimates camera pose and correspondence within a point set registration framework based on motion coherence, with the camera pose helping to localize the edge registration, while the "ambiguous" edge information helps to guide camera pose computation. The algorithm can compute camera pose over large displacements and by utilizing the non-unique edge points can recover camera pose from what were previously regarded as feature-impoverished SfM scenes. Our algorithm is also sufficiently flexible to incorporate high dimensional feature descriptors and works well on traditional SfM scenes with adequate numbers of unique corners.	[Lin, Wen-Yan; Liu, Siying] Inst Infocomm Res, Singapore 138632, Singapore; [Cheong, Loong-Fah; Tan, Ping] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore; [Dong, Guo] DSO Natl Labs, Singapore 118230, Singapore	Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R); National University of Singapore	Lin, WY (corresponding author), Inst Infocomm Res, 1 Fusionopolis Way,21-01 Connexis,South Tower, Singapore 138632, Singapore.	wdlin@i2r.a-star.edu.sg; eleclf@nus.edu.sg; ptan@nus.edu.sg; gdong@dso.org.sg; sliu@i2r.a-star.edu.sg			IDM Project Office, Media Development Authority of Singapore [NRF2007IDM-IDM002-069]; DSO [R263-000-400-592]	IDM Project Office, Media Development Authority of Singapore; DSO	This work is partially supported by project grant NRF2007IDM-IDM002-069 on "Life Spaces" from the IDM Project Office, Media Development Authority of Singapore and DSO grant R263-000-400-592.	Bartoli A, 2005, COMPUT VIS IMAGE UND, V100, P416, DOI 10.1016/j.cviu.2005.06.001; Bartoli A., 2001, P COMP VIS PATT REC; Bay H., 2006, P EUR C COMP VIS; Bellile V.G., 2007, P INT S MIX AUGM REA; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Chris H., 1988, P 4 ALVEY VISION C, P189; Chui H., 2000, P COMP VIS PATT REC; David P., 2002, INT J COMPUT VISION, V2, P424; Dellaert F., 2000, P COMP VIS PATT REC; Engels C., 2006, PHOTOGRAMMETRIC COMP; Enqvist O., 2008, EUR C COMP VIS; Enqvist O., 2009, BRIT C MACH VIS; Enqvist O., 2009, INT C COMP VIS; Faugeras O., 1995, P INT C COMP VIS; Faugeras O. D., 1987, P INT C COMP VIS; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Furukawa Y., 2007, P C COMP VIS PATT RE; Georgel P., 2009, WORKSH MOT VID COMP, P1; Georgel P., 2008, BRIT MACH VIS C; Goshen L, 2008, IEEE T PATTERN ANAL, V30, P1230, DOI 10.1109/TPAMI.2007.70768; Hao Jiang, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2474, DOI 10.1109/CVPRW.2009.5206776; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hou SY, 2008, COMPUT AIDED DESIGN, V40, P94, DOI 10.1016/j.cad.2007.08.007; Kahl F, 2008, INT J COMPUT VISION, V79, P271, DOI 10.1007/s11263-007-0117-1; Klein G, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P113, DOI 10.1109/ISMAR.2003.1240694; Klein George, 2007, P1; Kovesi P, 2011, MATLAB OCTAVE FUNCTI; Lehmann S, 2007, IEEE T PATTERN ANAL, V29, P82, DOI 10.1109/TPAMI.2007.250601; Lin W. Y., 2009, P INT C COMP VIS; Liu C., 2008, P EUR C COMP VIS; Lourakis MIA, 2009, ACM T MATH SOFTWARE, V36, DOI 10.1145/1486525.1486527; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Makadia A, 2007, INT J COMPUT VISION, V75, P311, DOI 10.1007/s11263-007-0035-2; Masson L, 2003, LECT NOTES COMPUT SC, V2749, P661; Meltzer J., 2008, P C COMP VIS PATT RE; Moisan L, 2004, INT J COMPUT VISION, V57, P201, DOI 10.1023/B:VISI.0000013094.38752.54; Moreno-Noguer F., 2008, P EUR C COMP VIS; Mouragnon E., 2006, IEEE COMP SOC C COMP, V1, P363, DOI DOI 10.1109/CVPR.2006.236; Myronenko A, 2007, ADV NEURAL INFORM PR; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; PAPADOPOULO T, 1996, P EUR C COMP VIS; Pressigout M., 2005, INT C IM PROC; Rangarajan A, 1997, LECT NOTES COMPUT SC, V1230, P29; Ricardo O., 2005, 3DIM INT C 3 D DIG I; SCHELLEWALD C, 2005, ENERGY MINIMIZATION; Sheikh Y., 2007, P COMP VIS IM PROC; Szeliski R., 1993, INT J COMPUT VISION, V28, p[28, 141]; Torresani Lorenzo, 2008, EUR C COMP VIS; Triggs B., 1999, VISION ALGORITHMS TH; Vacchetti L, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P48, DOI 10.1109/ISMAR.2004.24; Valgaerts L., 2008, P PATT REC; Wong K.-Y. K., 2001, P INT C COMP VIS; Wong KYK, 2002, IMAGE VISION COMPUT, V20, P441, DOI 10.1016/S0262-8856(02)00015-X; Yu L, 2007, IEEE INT C BIOINFORM, P9, DOI 10.1109/BIBM.2007.19; YUILLE AL, 1989, INT J COMPUT VISION, V3, P155, DOI 10.1007/BF00126430; Zhang Z, 2004, INT J COMPUT VISION, V13, P119; Zhao CS, 1997, PATTERN RECOGN, V30, P1817, DOI 10.1016/S0031-3203(97)00007-1	59	6	6	0	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2012	96	2					145	161		10.1007/s11263-011-0456-9	http://dx.doi.org/10.1007/s11263-011-0456-9			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	876AW		Green Published			2022-12-18	WOS:000299080200001
J	Breuss, M; Weickert, J				Breuss, Michael; Weickert, Joachim			Highly Accurate Schemes for PDE-Based Morphology with General Convex Structuring Elements	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Mathematical morphology; Dilation; Erosion; Partial differential equation; Finite difference method; Flux-corrected transport scheme	SHAPE; VISCOSITY	The two fundamental operations in morphological image processing are dilation and erosion. These processes are defined via structuring elements. It is of practical interest to consider a variety of structuring element shapes. The realisation of dilation/erosion for convex structuring elements by use of partial differential equations (PDEs) allows for digital scalability and subpixel accuracy. However, numerical schemes suffer from blur by dissipative artifacts. In our paper we present a family of so-called flux-corrected transport (FCT) schemes that addresses this problem for arbitrary convex structuring elements. The main characteristics of the FCT-schemes are: (i) They keep edges very sharp during the morphological evolution process, and (ii) they feature a high degree of rotational invariance. We validate the FCT-scheme theoretically by proving consistency and stability. Numerical experiments with diamonds and ellipses as structuring elements show that FCT-schemes are superior to standard schemes in the field of PDE-based morphology.	[Breuss, Michael; Weickert, Joachim] Univ Saarland, Math Image Anal Grp, Fac Math & Comp Sci, D-66041 Saarbrucken, Germany	Saarland University	Breuss, M (corresponding author), Univ Saarland, Math Image Anal Grp, Fac Math & Comp Sci, Campus E1-1, D-66041 Saarbrucken, Germany.	breuss@mia.uni-saarland.de; weickert@mia.uni-saarland.de						ALVAREZ L, 1993, ARCH RATION MECH AN, V123, P199, DOI 10.1007/BF00375127; Arehart A. B., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P215, DOI 10.1109/ICCV.1993.378217; BANON GJF, 2007, P 8 INT S MATH MORPH, V1; BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; BORIS JP, 1976, J COMPUT PHYS, V20, P397, DOI 10.1016/0021-9991(76)90091-7; BORIS JP, 1973, J COMPUT PHYS, V11, P38, DOI 10.1016/0021-9991(73)90147-2; Breuss M, 2007, J COMPUT APPL MATH, V206, P520, DOI 10.1016/j.cam.2006.08.006; Breuss M, 2009, LECT NOTES COMPUT SC, V5567, P758, DOI 10.1007/978-3-642-02256-2_63; BROCKETT RW, 1992, P IEEE INT C AC SPEE, V3, P125; Butt MA, 1996, COMP IMAG VIS, P31; Farin G., 2002, CURVES SURFACES CADG; Gottlieb S, 1998, MATH COMPUT, V67, P73, DOI 10.1090/S0025-5718-98-00913-2; Gottlieb S, 2001, SIAM REV, V43, P89, DOI 10.1137/S003614450036757X; GOUTSIAS J, 2000, COMPUTATIONAL IMAGIN, V18; HAIRER E, 1987, SPRINGER SERIES COMP, V8; KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741; LEVEQUE R. J, 2002, CAMBRIDGE TEXTS APPL, V31, DOI [10.1017/CBO9780511791253, DOI 10.1017/CBO9780511791253]; Matheron G., 1975, RANDOM SETS INTEGRAL; Matheron G, 1967, ELEMENTS THEORIE MIL; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Osher S., 2002, APPL MATH SCI, V153; Pizarro L, 2009, LECT NOTES COMPUT SC, V5720, P250, DOI 10.1007/978-3-642-03613-2_23; RONSE C, 2005, COMPUTATIONAL IMAGIN, V30; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; SAPIRO G, 1993, PATTERN RECOGN, V26, P1363, DOI 10.1016/0031-3203(93)90142-J; SERRA J, 1967, THESIS U NANCY FRANC; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; Serra J, 1988, IMAGE ANAL MATH MORP; Sethian J. A., 1999, LEVEL SET METHODS FA; Siddiqi K, 1997, GRAPH MODEL IM PROC, V59, P278, DOI 10.1006/gmip.1997.0438; Soille P., 2013, MORPHOLOGICAL IMAGE; TADMOR E, 1984, MATH COMPUT, V43, P369, DOI 10.1090/S0025-5718-1984-0758189-X; TALBOT H, 2002, P 6 INT S MATH MORPH; Tuzikov AV, 1998, COMPUT IMAGING VIS, V12, P59; van den Boomgaard R, 1999, LECT NOTES COMPUT SC, V1682, P199; van den Boomgaard R., 1992, THESIS U AMSTERDAM N	36	6	6	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2011	92	2					132	145		10.1007/s11263-010-0366-2	http://dx.doi.org/10.1007/s11263-010-0366-2			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	729DR					2022-12-18	WOS:000287929300001
J	Lhuillier, M				Lhuillier, Maxime			A Generic Error Model and Its Application to Automatic 3D Modeling of Scenes Using a Catadioptric Camera	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image based modeling; Catadioptric camera; Error propagation; Automatic system		Recently, it was suggested that structure-from-motion be solved using generic tools which are exploitable for any kind of camera. The same challenge applies for the automatic reconstruction of 3D models from image sequences, which includes structure-from-motion. This article is a new step in this direction. First, a generic error model is introduced for central cameras. Second, this error model is systematically used in the 3D modeling process. The experiments are carried out in a context which has rarely been addressed until now: the automatic 3D modeling of scenes using a catadioptric camera.	UBP CNRS, UMR 6602, LASMEA, F-63177 Aubiere, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite Clermont Auvergne (UCA)	Lhuillier, M (corresponding author), UBP CNRS, UMR 6602, LASMEA, 24 Ave Landais, F-63177 Aubiere, France.	maxime.lhuillier@univ-bpclermont.fr						AKBARZADEH A, 2006, 3DPTV 06; BARRON DFJ, 1992, INT J COMPUTER VISIO, V12; Bunschoten R, 2003, IEEE T ROBOTIC AUTOM, V19, P351, DOI 10.1109/TRA.2003.808850; CURLESS B, 1996, SIGGRAPH, P30; DANIILIDIS K, 2010, PAGE OMNIDIRECTIONAL; DOUBEK P, 2002, OMNIVIS 02; EVERSSENNE J, 2004, CVMP 04; FAUGERAS O, 2001, GEOMETRY MULTIPLE; FLECK S, 2005, ICRA 05; GROSSBERG M, 2001, ICCV 01; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; KANG SB, 1997, INT J COMPUTER VISIO, V25; Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153; Kolmogorov Vladimir, 2001, ICCV 01; Lhuillier M, 2002, IEEE T PATTERN ANAL, V24, P1140, DOI 10.1109/TPAMI.2002.1023810; Lhuillier M., 2008, CVPR 08; LHUILLIER M, 2006, ICPR 06; LHUILLIER M, 2007, CVPR 07; LHUILLIER M, 2008, COMPUTER VISION IMAG, V109; LI H, 2008, CVPR 08; Micusik B, 2006, IEEE T PATTERN ANAL, V28, P1135, DOI 10.1109/TPAMI.2006.151; Mouragnon E, 2009, IMAGE VISION COMPUT, V27, P1178, DOI 10.1016/j.imavis.2008.11.006; NISTER D, 2007, J MATH IMAGING VISIO, V27; OKUTOMI M, IEEE T PATTERN ANAL, V15; PLESS R, 2003, CVPR 03; POLLEFEYS M, 2004, INT J COMPUTER VISIO, V59; RAMALINGAM SLS, 2006, COMPUTER VISION IMAG, V103; SCHINDLER K, 2003, LNCS, V2781; Schindler K., 2003, DAGM 03; SEITZ S, 2006, CVPR 06; SOUCY M, 1995, IEEE T PATTERN ANAL, V17, P344, DOI 10.1109/34.385982; STRELOW D, 2001, IROS 01; TISSOT N, 2010, TISSOTS INDICATRIX; TONI A, 2008, CVPR 08, P8; [No title captured]	35	6	7	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2011	91	2					175	199		10.1007/s11263-010-0374-2	http://dx.doi.org/10.1007/s11263-010-0374-2			25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	708KJ		Green Submitted			2022-12-18	WOS:000286360400004
J	Scherer-Negenborn, N; Schaefer, R				Scherer-Negenborn, Norbert; Schaefer, Rolf			Model Fitting with Sufficient Random Sample Coverage	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Model fitting; Probabilistic algorithms; RANSAC; Robust regression	ROBUST; MLESAC	It has been observed previously that the number of iterations required to derive good model parameter values used by RANSAC-like model estimators is too optimistic. We present the derivation of an analytical formula that allows the calculation of the sufficient limit of iterations needed to obtain good parameter values with the prescribed probability for any number of model parameters. It explains the values that had been found experimentally for certain numbers of model parameters by others very well. Furthermore, the improvement that our approach of SUfficient Random SAmple Coverage (SURSAC) offers, in comparison to the original RANSAC algorithm as well as to its adaptive modification by Hartley and Zisserman, is demonstrated with synthetic data for the case of a non-linear model function over a wide range of outlier fractions and different ratios of inlier and outlier densities.	[Scherer-Negenborn, Norbert; Schaefer, Rolf] FGAN FOM, Ettlingen, Germany		Scherer-Negenborn, N (corresponding author), FGAN FOM, Ettlingen, Germany.	scherer@fom.fgan.de						Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Chum O., 2004, P ACCV, V2, P812; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Hartley R., 2003, MULTIPLE VIEW GEOMET; HUBER PJ, 1985, ROBUST STAT; Lacey AJ, 2000, P BRIT MACH VIS C, P646; Matas J, 2004, IMAGE VISION COMPUT, V22, P837, DOI 10.1016/j.imavis.2004.02.009; MEER P, 1991, INT J COMPUT VISION, V6, P59, DOI 10.1007/BF00127126; MEER P, 2004, ROBUST TECHNIQUES CO, pCH4; Michaelsen E., 2006, S PHOT COMP VIS; Nister D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P199; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; Stewart CV, 1999, SIAM REV, V41, P513, DOI 10.1137/S0036144598345802; TORDOFF B, 2002, P 7 ECCV, V1, P82; Tordoff BJ, 2005, IEEE T PATTERN ANAL, V27, P1523, DOI 10.1109/TPAMI.2005.199; Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832	18	6	6	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2010	89	1					120	128		10.1007/s11263-010-0329-7	http://dx.doi.org/10.1007/s11263-010-0329-7			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	584RF					2022-12-18	WOS:000276769500008
J	Brooks, R; Arbel, T				Brooks, Rupert; Arbel, Tal			Generalizing Inverse Compositional and ESM Image Alignment	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Image alignment; Image registration; Inverse compositional method; Efficient second order method	MUTUAL-INFORMATION; REGISTRATION; TRACKING; MATRIX	Inverse compositional (IC) image alignment (Baker and Matthews in Int. J. Comput. Vis. 56(3):221-255, 2004) uses the symmetry between the roles of the fixed and moving images for faster processing. However, it requires implementation of compositional optimizer update steps. The IC approach can be viewed as an efficient way of computing the similarity measure derivative relative to the fixed image warp parameters. Since the mapping between the fixed and moving warp parameters is continuous and differentiable, this derivative can be converted into the moving warp space using the chain rule. This avoids the need for compositional update steps. Our generalization also allows the efficient second order method (ESM) (Malis in Proceedings of the 2004 IEEE International Conference on Robotics and Automation (ICRA04), pp. 1843-1848, 2004; Benhimane and Malis in IEEE/RSJ International Conference on Intelligent Robots and Systems, 2004; Malis and Benhimane in Robot. Auton. Syst. 52(1):39-52, 2005) to be applied to general parameterizations of the transformation. Experiments using multiple similarity measures and optimizers show that our generalized IC method equals or exceeds the performance of the original IC approach. The generalized ESM approach is more reliable than the classic approach as it increases the capture radius of the optimization.	[Brooks, Rupert] Natl Res Council Canada, Inst Ind Mat, Boucherville, PQ J4B 6Y4, Canada; [Arbel, Tal] McGill Univ, McGill Ctr Intelligent Machines, Montreal, PQ, Canada	National Research Council Canada; McGill University	Brooks, R (corresponding author), Natl Res Council Canada, Inst Ind Mat, Boucherville, PQ J4B 6Y4, Canada.	rupert.brooks@imi.cnrc-nrc.gc.ca; arbel@cim.mcgill.ca						ANDREOPOULOS A, 2005, P 14 SCAND C IM AN J; ANUTA PE, 1969, SOC PHOTO-OPT INSTRU, V7, P168; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; BAKER S, 2004, CMURITR0464 CARN MEL; BARTOLI A, 2006, P 17 BRIT MACH VIS C, V1, P157; BENHIMANE S, 2004, IEEE RSJ INT C INT R; Brooks R, 2006, INT C PATT RECOG, P1200; BROWN LG, 1992, COMPUT SURV, V24, P325, DOI 10.1145/146370.146374; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Conn A.R., 2000, TRUST REGION METHODS, DOI [10.1137/1.9780898719857, DOI 10.1137/1.9780898719857]; DENNIS JE, 1993, SIAM J NUMER ANAL, V30, P1291, DOI 10.1137/0730067; Dowson N, 2008, IEEE T PATTERN ANAL, V30, P180, DOI 10.1109/TPAMI.2007.70757; Fletcher R, 1987, PRACTICAL METHODS OP, V1; Gill P. E., 1981, PRACTICAL OPTIMIZATI; Harshman RA, 2001, J CHEMOMETR, V15, P689, DOI 10.1002/cem.665; Ibanez L., 2005, ITK SOFTWARE GUIDE I; Keller Y, 2004, IEEE T IMAGE PROCESS, V13, P1042, DOI 10.1109/TIP.2004.823823; Kwan RKS, 1999, IEEE T MED IMAGING, V18, P1085, DOI 10.1109/42.816072; Liu TL, 2004, IEEE T PATTERN ANAL, V26, P397, DOI 10.1109/TPAMI.2004.1262335; Lucas B.D., 1981, ITERATIVE IMAGE REGI, P674; Malis E, 2005, ROBOT AUTON SYST, V52, P39, DOI 10.1016/j.robot.2005.03.014; Malis E, 2004, IEEE INT CONF ROBOT, P1843, DOI 10.1109/ROBOT.2004.1308092; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Moler C, 2003, SIAM REV, V45, P3, DOI 10.1137/S00361445024180; Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867; Sheskin D. J., 2000, HDB PARAMETRIC NONPA; Simard PY, 2000, INT J IMAG SYST TECH, V11, P181, DOI 10.1002/1098-1098(2000)11:3<181::AID-IMA1003>3.0.CO;2-E; Szeliski R., 2004, MSRTR200492; Thevenaz P, 2000, IEEE T IMAGE PROCESS, V9, P2083, DOI 10.1109/83.887976; Toews M, 2005, LECT NOTES COMPUT SC, V3750, P163, DOI 10.1007/11566489_21; van de Kraats EB, 2005, IEEE T MED IMAGING, V24, P1177, DOI 10.1109/TMI.2005.853240; Venkataraman P., 2001, APPL OPTIMIZATION MA; VIOLA P, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P16, DOI 10.1109/ICCV.1995.466930; Weidendorfer J., 2004, P 4 INT C COMP SCI I; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	36	6	6	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	87	3					191	212		10.1007/s11263-009-0263-8	http://dx.doi.org/10.1007/s11263-009-0263-8			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	551KA					2022-12-18	WOS:000274205700001
J	Lakaemper, R; Sobel, M				Lakaemper, Rolf; Sobel, Marc			Using the Particle Filter Approach to Building Partial Correspondences Between Shapes	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Shape matching; Particle filters; Shape recognition; Feature correspondences; Shape alignment	NONRIGID SHAPES	Constructing correspondences between points characterizing one shape with those characterizing another is crucial to understanding what the two shapes have in common. These correspondences are the basis for most alignment processes and shape similarity measures. In this paper we use particle filters to establish perceptually correct correspondences between point sets characterizing shapes. Local shape feature descriptors are used to establish the probability that a point on one shape corresponds to a point on the other shape. Global correspondence structures are calculated using additional constraints on domain knowledge. Domain knowledge is characterized by prior distributions which serve to characterize hypotheses about the global relationships between shapes. These hypotheses are formulated online. This means global constraints are learnt during the particle filtering process, which makes the approach especially interesting for applications where global constraints are hard to define a priori. As an example for such a case, experiments demonstrate the performance of our approach on partial shape matching.	[Lakaemper, Rolf; Sobel, Marc] Temple Univ, Philadelphia, PA 19122 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple University	Lakaemper, R (corresponding author), Temple Univ, Philadelphia, PA 19122 USA.	lakamper@temple.edu; marc.sobel@temple.edu			NSF [IIS-0534929]; Dept. of Energy [DE-FG52-27508]	NSF(National Science Foundation (NSF)); Dept. of Energy	Rolf Lakaemper was partly funded by grants NSF IIS-0534929 and Dept. of Energy DE-FG52-27508.	Alt Helmut, 2006, P 22 EUR WORKSH COMP, P107; Back T., 1997, IEEE Transactions on Evolutionary Computation, V1, P3, DOI 10.1109/4235.585888; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; CHEN L, 2008, COMP VIS PATT REC WO; Crisan D, 2002, IEEE T SIGNAL PROCES, V50, P736, DOI 10.1109/78.984773; DEBRUIJNE M, 2004, ICPR 04, V3, P722; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; DELMORAL P, 2007, BAYESIAN STAT, V8, P1; Doucet A., 2001, SEQUENTIAL MONTE CAR; Goldberg DE, 1989, GENETIC ALGORITHMS S; Gorelick L, 2006, IEEE T PATTERN ANAL, V28, P1991, DOI 10.1109/TPAMI.2006.253; GOWER J, 2005, PROCRUSTES PROBLEMS, V70; HARALICK RM, 1979, IEEE T PATTERN ANAL, V1, P173, DOI 10.1109/TPAMI.1979.4766903; Holland J.H., 1975, ADAPTATION NATURAL A, DOI DOI 10.7551/MITPRESS/1090.001.0001; KITTLER J, 1985, IMAGE VISION COMPUT, V3, P206, DOI 10.1016/0262-8856(85)90009-5; LAKAEMPER R, 2008, IEEE INT C COMP VIS; LAKAEMPER R, 2008, IAPR INT C PATT REC; Latecki L.J., 2007, IEEE INT C DAT MIN I; Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850; Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41; LIU J, 2000, THEORETICAL FRAMEWOR; Liu J. S., 2002, MONTE CARLO STRATEGI; Milios E, 2000, IEEE T IMAGE PROCESS, V9, P141, DOI 10.1109/83.817606; Mokhtarian F, 2002, INT J COMPUT VISION, V48, P131, DOI 10.1023/A:1016098907682; Mokhtarian F., 1996, P BRIT MACH VIS C, P53; MORAL PD, 2006, J ROYAL STAT SOC B, V68, P411, DOI DOI 10.1111/J; PEREZ P, 2001, P INT C COMP VIS VAN, P424; QIAN W, 1992, J STAT COMPUTING SIM, V40; Rathi Y, 2007, IEEE T IMAGE PROCESS, V16, P1370, DOI 10.1109/TIP.2007.894244; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519; SANDHU R, 2008, PARTICLE FILTERING R; Schmidt FR, 2007, IEEE I CONF COMP VIS, P1479; Scott C, 2006, IEEE T IMAGE PROCESS, V15, P1831, DOI 10.1109/TIP.2006.877038; Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951; Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703; Thrun S., 2002, P 17 ANN C UNC AI UA; VELTKAMP RC, 2001, STATE OF THE ART SHA; Zheng YF, 2006, IEEE T PATTERN ANAL, V28, P643, DOI 10.1109/TPAMI.2006.81	38	6	7	0	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2010	88	1					1	23		10.1007/s11263-009-0288-z	http://dx.doi.org/10.1007/s11263-009-0288-z			23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	571LS					2022-12-18	WOS:000275753900001
J	Yao, Y; Abidi, B; Abidi, M				Yao, Yi; Abidi, Besma; Abidi, Mongi			3D Target Scale Estimation and Target Feature Separation for Size Preserving Tracking in PTZ Video	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Size preserving tracking; Structure from motion; Feature segmentation; Surveillance systems	FACTORIZATION METHOD; MOTION; ZOOM; SHAPE	To achieve size preserving tracking, in addition to controlling the camera's pan and tilt motions to keep the object of interest in the camera's field of view (FOV), the camera's focal length is adjusted automatically to compensate for the changes in the target's image size caused by the relative motion between the camera and the target. The estimation accuracy of these changes determines the effectiveness of the resulting zoom control. The existing method of choice for real-time target scale estimation applies structure from motion (SFM) based on the weak perspective projection model. In this paper we propose a target scale estimation algorithm with a linear solution based on the more advanced paraperspective projection model, which improves the accuracy of scale estimation by considering center offset. Another key issue in SFM based algorithms is the separation of target and background features, especially when composite camera (pan/tilt/zoom) and target motions are involved. This paper designs a fast target feature separation/grouping algorithm, the 3D affine shape method. The resulting separation automatically adapts to the target's 3D geometry and motion and is able to accommodate a large amount of off-plane rotation, which most existing separation/grouping algorithms find difficult to achieve. Experimental results illustrate the effectiveness of the proposed scale estimation and feature separation algorithms in tracking translating and rotating objects with a PTZ camera while preserving their sizes. In comparison with the leading size preserving tracking algorithm described by Tordoff and Murray, our algorithm is able to reduce the cumulative tracking error significantly from 17.4% to 3.3%.	[Yao, Yi; Abidi, Besma; Abidi, Mongi] Univ Tennessee, Dept Elect & Comp Engn, Imaging Robot & Intelligent Syst Lab, Knoxville, TN 37996 USA	University of Tennessee System; University of Tennessee Knoxville	Abidi, B (corresponding author), Univ Tennessee, Dept Elect & Comp Engn, Imaging Robot & Intelligent Syst Lab, Knoxville, TN 37996 USA.	besma@utk.edu						Bouguet JY, 2003, PYRAMIDAL IMPLEMENTA; Collins RT, 2003, PROC CVPR IEEE, P234; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; CROSSMANN E, 2000, INT C PATT REC BARC, P1864; Denzler J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P400; Fayman JA, 2001, MACH VISION APPL, V13, P25, DOI 10.1007/PL00013268; FAYMAN JF, 1998, IEEE INT C ROB AUT L, P1051; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Harwood D., 2000, EUR C COMP VIS, P751, DOI DOI 10.1007/3-540-45053-X_48; Hatano K, 2003, SICE 2003 ANNUAL CONFERENCE, VOLS 1-3, P2443; Hoad P., 1995, IEE INT C IM PROC IT, P291; KANG S, 2004, FUSION COLOR SHAPE O; Kehtarnavaz N, 2003, REAL-TIME IMAGING, V9, P197, DOI 10.1016/S1077-2014(03)00037-8; Kim YO, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P29, DOI 10.1109/AVSS.2003.1217898; KRUEGER V, 1999, IEEE INT C COMP VIS, P141; Kruger V, 2000, INT C PATT RECOG, P127, DOI 10.1109/ICPR.2000.905289; Kuo TK, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON CONTROL APPLICATIONS, VOLS 1 & 2, P725, DOI 10.1109/CCA.2002.1038690; LEE D, 2005, INT TECHN C CIRC SYS, V4, P1605; LEE D, 2007, DYNAMIC VIDEO TRACKI; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Liptak BG, 1995, INSTRUMENT ENG HDB P, V3; lssacson E., 1994, ANAL NUMERICAL METHO; MATSUYAMA T, 2006, IEEE C COMP VIS PATT, P728; Mirmehdi M, 1997, SCIA '97 - PROCEEDINGS OF THE 10TH SCANDINAVIAN CONFERENCE ON IMAGE ANALYSIS, VOLS 1 AND 2, P447; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; Shah H, 2004, INT CONF ACOUST SPEE, P721; SHAIK JS, 2000, DETECTION TRACKING R; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Tordoff B, 2004, IEEE T PATTERN ANAL, V26, P98, DOI 10.1109/TPAMI.2004.1261082; Tordoff BJ, 2003, PROC CVPR IEEE, P273; TORDOFF BJ, 2001, BRIT MACH VIS C MANC; TORDOFF BJ, 2000, OUEL222800 OXF U DEP; Torresani L, 2001, PROC CVPR IEEE, P493; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wei J, 2001, IEEE T PATTERN ANAL, V23, P896, DOI 10.1109/34.946992; WOELK F, 2005, IEEE INT VEH S LAS V; YAO Y, 2006, IEEE INT C ADV VID S, P46; YAO Y, 2006, SPIE DEF SEC S ORL F; Yao Y, 2006, IEEE IMAGE PROC, P2817, DOI 10.1109/ICIP.2006.312994	40	6	6	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2009	82	3					244	263		10.1007/s11263-008-0198-5	http://dx.doi.org/10.1007/s11263-008-0198-5			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	411TF					2022-12-18	WOS:000263672800002
J	Yamazaki, S; Narasimhan, SG; Baker, S; Kanade, T				Yamazaki, Shuntaro; Narasimhan, Srinivasa G.; Baker, Simon; Kanade, Takeo			The Theory and Practice of Coplanar Shadowgram Imaging for Acquiring Visual Hulls of Intricate Objects	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multi-view geometry; Shape reconstruction; Shape from silhouette; Imaging system; Calibration; Intricate shape; Shadowgram; Coplanar shadowgram	SILHOUETTE	Acquiring 3D models of intricate objects (like tree branches, bicycles and insects) is a challenging task due to severe self-occlusions, repeated thin structures, and surface discontinuities. In theory, a shape-from-silhouettes (SFS) approach can overcome these difficulties and reconstruct visual hulls that are close to the actual shapes, regardless of the complexity of the object. In practice, however, SFS is highly sensitive to errors in silhouette contours and the calibration of the imaging system, and has therefore not been used for obtaining accurate shapes with a large number of views. In this work, we present a practical approach to SFS using a novel technique called coplanar shadowgram imaging that allows us to use dozens to even hundreds of views for visual hull reconstruction. A point light source is moved around an object and the shadows (silhouettes) cast onto a single background plane are imaged. We characterize this imaging system in terms of image projection, reconstruction ambiguity, epipolar geometry, and shape and source recovery. The coplanarity of the shadowgrams yields unique geometric properties that are not possible in traditional multi-view camera-based imaging systems. These properties allow us to derive a robust and automatic algorithm to recover the visual hull of an object and the 3D positions of the light source simultaneously, regardless of the complexity of the object. We demonstrate the acquisition of several intricate shapes with severe occlusions and thin structures, using 50 to 120 views.	[Yamazaki, Shuntaro] Natl Inst Adv Ind Sci & Technol, Tokyo, Japan; [Narasimhan, Srinivasa G.; Kanade, Takeo] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Baker, Simon] Microsoft Res, Redmond, WA USA	National Institute of Advanced Industrial Science & Technology (AIST); Carnegie Mellon University; Microsoft	Yamazaki, S (corresponding author), Natl Inst Adv Ind Sci & Technol, Tokyo, Japan.	shun-yamazaki@aist.go.jp; srinivas@cs.cmu.edu; sbaker@microsoft.com; tk@cs.cmu.edu						Astrom K, 1999, INT J COMPUT VISION, V33, P51, DOI 10.1023/A:1008113231241; BALAN A, 2007, P INT C COMP VIS 07; Baumgart B.G., 1974, THESIS STANFORD U; BESANT WH, 1890, CONIC SECTIONS TREAT; Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5; CAMPBELL N, 2007, P BRIT MACH VIS C, P530; CHANDRAKER MK, 2005, P COMP VIS PATT REC, V1, P788; Cheung KM, 2005, INT J COMPUT VISION, V62, P221, DOI 10.1007/s11263-005-4881-5; CIPOLLA R, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P269, DOI 10.1109/ICCV.1995.466775; Cross G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P323, DOI 10.1109/ICCV.1999.791237; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Drbohlav O, 2005, IEEE I CONF COMP VIS, P1850; DRBOHLAV O, 2002, P EUR C COMP VIS, P46; Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658; Franco J.-S., 2003, BRIT MACH VIS C BMVC, V1, P329, DOI [DOI 10.5244/C.17.32, 10.5244/C.17.32]; Furukawa Y, 2006, IEEE T PATTERN ANAL, V28, P302, DOI 10.1109/TPAMI.2006.41; Georghiades AS, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P816; Hartley R., 2004, ROBOTICA; HAYAKAWA H, 1994, J OPT SOC AM A, V11, P3079, DOI 10.1364/JOSAA.11.003079; Hernandez C, 2007, IEEE T PATTERN ANAL, V29, P343, DOI 10.1109/TPAMI.2007.42; Hooke R., 1667, MICROGRAPHIA; Kriegman DJ, 2001, J OPT SOC AM A, V18, P1804, DOI 10.1364/JOSAA.18.001804; Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849; Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951; Press WH, 1988, NUMERICAL RECIPES C; SAVARESE S, 2005, INT J COMPUT VISION, V71, P305; SAWHNEY HS, 1994, INT C PATT RECOG, P403, DOI 10.1109/ICPR.1994.576308; Seitz S., 2006, P CVPR 06 IE COMP SO, V1, P519, DOI DOI 10.1109/CVPR.2006.19; Settles G. S., 2001, EXP FLUID MECH, DOI 10.1007/978-3-642-56640-0; Sinha SN, 2004, PROC CVPR IEEE, P195; SMITH AR, 1996, P SIGGRAPH 96, P259; TAN P, 2007, P COMP VIS PATT REC, P1; Wong KYK, 2004, IEEE T IMAGE PROCESS, V13, P379, DOI 10.1109/TIP.2003.821113; YAMAZAKI S, 2007, P INT C COMP VIS 07; Yezzi A, 2003, INT J COMPUT VISION, V53, P31, DOI 10.1023/A:1023079624234	35	6	6	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	2009	81	3					259	280		10.1007/s11263-008-0170-4	http://dx.doi.org/10.1007/s11263-008-0170-4			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	394GK		Green Submitted			2022-12-18	WOS:000262433800003
J	Fuchs, M; Scherzer, O				Fuchs, Matthias; Scherzer, Otmar			Regularized reconstruction of shapes with statistical a priori knowledge	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						statistical shape analysis; variational methods; regularization theory; image segmentation; shape recognition	PRINCIPAL GEODESIC ANALYSIS; ACTIVE CONTOURS; SEGMENTATION; SPACES; MODEL	The reconstruction of geometry or, in particular, the shape of objects is a common issue in image analysis. Starting from a variational formulation of such a problem on a shape manifold we introduce a regularization technique incorporating statistical shape knowledge. The key idea is to consider a Riemannian metric on the shape manifold which reflects the statistics of a given training set. We investigate the properties of the regularization functional and illustrate our technique by applying it to region-based and edge-based segmentation of image data. In contrast to previous works our framework can be considered on arbitrary (finite-dimensional) shape manifolds and allows the use of Riemannian metrics for regularization of a wide class of variational problems in image processing.	[Fuchs, Matthias; Scherzer, Otmar] Univ Innsbruck, Inst Comp Sci, A-6020 Innsbruck, Austria	University of Innsbruck	Fuchs, M (corresponding author), Univ Innsbruck, Inst Comp Sci, Tech Str 21A-2, A-6020 Innsbruck, Austria.	matz.fuchs@uibk.ac.at; otmar.scherzer@uibk.ac.at	Scherzer, Otmar/AAA-4132-2019					BOOTHBY WM, 1975, PURE APPL MATH, V63; Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043; CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; CHEN Y, 2001, 1 IEEE WORKSH VAR LE; Chen YM, 2002, INT J COMPUT VISION, V50, P315, DOI 10.1023/A:1020878408985; Cremers D, 2003, LECT NOTES COMPUT SC, V2695, P388; Cremers D, 2003, IMAGE VISION COMPUT, V21, P77, DOI 10.1016/S0262-8856(02)00128-2; Cremers D, 2003, PATTERN RECOGN, V36, P1929, DOI 10.1016/S0031-3203(03)00056-6; Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915; ENGL HW, 2000, MATH ITS APPL, V375; Fang W, 2007, PATTERN RECOGN, V40, P2163, DOI 10.1016/j.patcog.2006.12.014; Fletcher PT, 2004, LECT NOTES COMPUT SC, V3117, P87; Fletcher PT, 2003, PROC CVPR IEEE, P95; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; Fritscher KD, 2006, INT J COMPUT ASS RAD, V1, P123, DOI 10.1007/s11548-006-0048-2; FUCHS M, 2007, 41 JOINT RES PROGR I; Gastaud M, 2004, IEEE T CIRC SYST VID, V14, P726, DOI 10.1109/TCSVT.2004.826758; Helgason Sigurdur, 1978, PURE APPL MATH, V80; Huckemann S, 2006, ADV APPL PROBAB, V38, P299, DOI 10.1239/aap/1151337073; Joshi S, 2002, IEEE T MED IMAGING, V21, P538, DOI 10.1109/TMI.2002.1009389; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; LE HL, 1993, ANN STAT, V21, P1225, DOI 10.1214/aos/1176349259; LEVENTON M, 2000, IEEE C COMP VIS PATT, V1, P316, DOI DOI 10.1109/CVPR.2000.855835]; Michor PW, 2007, APPL COMPUT HARMON A, V23, P74, DOI 10.1016/j.acha.2006.07.004; Michor PW, 2006, J EUR MATH SOC, V8, P1, DOI 10.4171/JEMS/37; Miller MI, 2006, J MATH IMAGING VIS, V24, P209, DOI 10.1007/s10851-005-3624-0; Mumford D., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P22; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; PETERSON P, 1998, GRADUATE TEXTS MATH, V171; Rousson M, 2002, LECT NOTES COMPUT SC, V2351, P78; Tsai A, 2003, IEEE T MED IMAGING, V22, P137, DOI 10.1109/TMI.2002.808355; Vaillant M, 2004, NEUROIMAGE, V23, pS161, DOI 10.1016/j.neuroimage.2004.07.023	33	6	6	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2008	79	2					119	135		10.1007/s11263-007-0103-7	http://dx.doi.org/10.1007/s11263-007-0103-7			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	302ZV					2022-12-18	WOS:000256009700002
J	Torres-Mendez, LA; Dudek, G				Torres-Mendez, Luz A.; Dudek, Gregory			Inter-image statistics for 3D environment modeling	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						3D environment modeling; sensor fusion; Markov random fields	STEREO; SHAPE	In this article we present a method for automatically recovering complete and dense depth maps of an indoor environment by fusing incomplete data for the 3D environment modeling problem. The geometry of indoor environments is usually extracted by acquiring a huge amount of range data and registering it. By acquiring a small set of intensity images and a very limited amount of range data, the acquisition process is considerably simplified, saving time and energy consumption. In our method, the intensity and partial range data are registered first by using an image-based registration algorithm. Then, the missing geometric structures are inferred using a statistical learning method that integrates and analyzes the statistical relationships between the visual data and the available depth on terms of small patches. Experiments on real-world data on a variety of sampling strategies demonstrate the feasibility of our method.	[Torres-Mendez, Luz A.; Dudek, Gregory] McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2A7, Canada	McGill University	Torres-Mendez, LA (corresponding author), CINVESTAV, Campus Saltillo,Carretera Saltillo Monterrey,Km 1, Coahuila 25900, Mexico.	abril.torres@cinvestav.edu.mx	Torres-Mendez, Luz Abril/A-8234-2013; Dudek, Gregory L/H-3567-2012	Torres-Mendez, Luz Abril/0000-0002-0327-3851; 				Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; COBZAS D, 2003, THESIS U ALBERTA CAN; CONNOLLY CI, 1984, P INT C ROB, P25; CRIMINISI A, 2003, IEEE COMPUTER VISION; DAVIES D, 1986, P 1 INT C MOD UNC AP, P16; Diebel James, 2005, NEURAL INF PROCESS S, P291; EFROS A, 2001, SIGGRAPH, P1033; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; El-Hakim SF, 1998, ISPRS J PHOTOGRAMM, V53, P379, DOI 10.1016/S0924-2716(98)00021-5; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; FITZGIBBON AW, 1998, P EUR SIGN PROC C EU, P1261; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075; FREEMAN WT, 2003, ADV NEURAL INFORM PR, V15; Fua P., 1993, Machine Vision and Applications, V6, P35, DOI 10.1007/BF01212430; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; HAMMERSLEY JM, 1971, UNPUB MARKOV FIELD F; Hoiem D., 2005, P ICCV; Howe CQ, 2002, P NATL ACAD SCI USA, V99, P13184, DOI 10.1073/pnas.162474299; Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668; KUIPERS B, 1978, COGNITIVE SCI, V2, P1291; LEE A, 2001, COMMUNICATION; Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849; LITTLE JJ, 1990, IMAGE VISION COMPUT, V8, P328, DOI 10.1016/0262-8856(90)80009-I; MICHELS J, 2005, P 21 INT C MACH LEAR; Nelson W. L., 1988, IEEE INT C ROB AUT N, P1504; NYLAND L, 1999, P MULT MOD AN WORKSH, P8; POGGIO T, 1988, SCIENCE, V242, P436, DOI 10.1126/science.3175666; POLLEFEYS M, 1998, P SMILE WORKSH POST, P138; Potetz B, 2003, J OPT SOC AM A, V20, P1292, DOI 10.1364/JOSAA.20.001292; PULLI K, 1997, LECT NOTES COMPUTER, V1310, P385; Saxena A., 2007, P INT JOINT C ART IN; SAXENA A, 2006, P ADV NEUR INF PROC; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scharstein D, 2003, PROC CVPR IEEE, P195; Sequeira V, 1999, ISPRS J PHOTOGRAMM, V54, P1, DOI 10.1016/S0924-2716(98)00026-4; Shafer G., 1976, MATH THEORY EVIDENCE, VVolume 1; STAMOS I, 2000, CVPR; STAN Z, 1995, COMPUTER SCI WORKBEN; Sun J, 2005, PROC CVPR IEEE, P399; Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509; SZELISKI R, 2006, ECCV, V2, P19; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torralba A, 2002, IEEE T PATTERN ANAL, V24, P1226, DOI 10.1109/TPAMI.2002.1033214; Torres-Mendez LA, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P231, DOI 10.1109/ACV.2002.1182187; Torres-Mendez LA, 2006, LECT NOTES ARTIF INT, V4293, P715; TORRESMENDEZ L, 2005, THESIS MCGILL U; TORRESMENDEZ LA, 2004, P IEEE RSJ C INT ROB; TORRESMENDEZ LA, 2004, AAAI P JUL 2004, P476; Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009; Whaite P, 1997, IEEE T PATTERN ANAL, V19, P193, DOI 10.1109/34.584097; WINKLER G., 1995, IMAGE ANAL RANDOM FI; ZHANG L, 2005, CVPR, V2, P288; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284	57	6	6	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2008	79	2					137	158		10.1007/s11263-007-0108-2	http://dx.doi.org/10.1007/s11263-007-0108-2			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	302ZV					2022-12-18	WOS:000256009700003
J	Schmid, C; Soatto, S; Tomasi, C				Schmid, Cordelia; Soatto, Stefano; Tomasi, Carlo			Untitled	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Editorial Material									[Schmid, Cordelia] INRIA, LEAR Team, F-38330 Montbonnot St Martin, France; [Soatto, Stefano] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA; [Tomasi, Carlo] Duke Univ, Dept Comp Sci, Durham, NC 27708 USA	Inria; University of California System; University of California Los Angeles; Duke University	Schmid, C (corresponding author), INRIA, LEAR Team, F-38330 Montbonnot St Martin, France.	Cordelia.Schmid@inrialpes.fr; soatto@cs.ucla.edu; tomasi@cs.duke.edu							0	6	6	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2008	76	2					105	105		10.1007/s11263-007-0073-9	http://dx.doi.org/10.1007/s11263-007-0073-9			1	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	255VH					2022-12-18	WOS:000252685400001
J	Roberts, TJ; Mckenna, SJ; Ricketts, IW				Roberts, Timothy J.; Mckenna, Stephen J.; Ricketts, Ian W.			Human pose estimation using partial configurations and probabilistic regions	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						human pose estimation; boundary likelihood; interpart likelihood; part based estimation; colour features	MOTION; TRACKING; RECONSTRUCTION	A method for recovering a part-based description of human pose from single images of people is described. It is able to perform estimation efficiently in the presence significant background clutter, large foreground variation, self-occlusion and occlusion by other objects. This is achieved through two key developments. Firstly, a new formulation is proposed that allows partial configurations, hypotheses with differing numbers of parts, to be made and compared. This permits efficient global sampling in the presence of self and other object occlusions without prior knowledge of body part visibility. Secondly, a highly discriminatory likelihood model is proposed comprising two complementary components. A boundary component improves upon previous appearance distribution divergence methods by incorporating high-level shape and appearance information and hence better discriminates textured, overlapping body parts. An inter-part component uses appearance similarity of body parts to reduce the number of false-positive, multi-part hypotheses, hence increasing estimation efficiency. Results are presented for challenging images with unknown subject and large variations in subject appearance, scale and pose.	Univ Dundee, Sch Comp, Dundee DD1 4HN, Scotland	University of Dundee	Roberts, TJ (corresponding author), Univ Dundee, Sch Comp, Dundee DD1 4HN, Scotland.	troberts@computing.dundee.ac.uk; stephen@computing.dundee.ac.uk; ricketts@computing.dundee.ac.uk	McKenna, Stephen/AAL-8335-2020	McKenna, Stephen/0000-0003-0530-2035; Roberts, Timothy/0000-0002-1464-0151				Baerlocher P, 2001, INT FED INFO PROC, V68, P180; Barron C, 2001, COMPUT VIS IMAGE UND, V81, P269, DOI 10.1006/cviu.2000.0888; Baumberg A. M., 1994, Proceedings of the 1994 IEEE Workshop on Motion of Non-Rigid and Articulated Objects (Cat. No.94TH0671-8), P194, DOI 10.1109/MNRAO.1994.346236; Bowden R, 2000, IMAGE VISION COMPUT, V18, P729, DOI 10.1016/S0262-8856(99)00076-1; Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581; CHAM T, 1999, IEEE INT C COMP VIS, V2, P239; Choo K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P321, DOI 10.1109/ICCV.2001.937643; COMANICIU D, 2002, IEEE C COMP VIS PATT, P673; Deutscher J, 2001, PROC CVPR IEEE, P669; Deutscher J, 2000, PROC CVPR IEEE, P126, DOI 10.1109/CVPR.2000.854758; DEUTSCHER J, 1999, ICCV, P1144; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Felzenszwalb PF, 2000, PROC CVPR IEEE, P66, DOI 10.1109/CVPR.2000.854739; Forsyth DA, 1997, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.1997.609399; Forsyth DA, 1999, INT J COMPUT VISION, V32, P63, DOI 10.1023/A:1008145029462; Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716; Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056; GROSSO M, 1989, P COMP AN HUM FIG, P83; Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683; Haritaoglu I, 1999, SECOND IEEE WORKSHOP ON VISUAL SURVEILLANCE (VS'99), PROCEEDINGS, P6, DOI 10.1109/VS.1999.780263; HOGG D, 1983, IMAGE VISION COMPUT, V1, P5, DOI DOI 10.1016/0262-8856(83)90003-3; Ioffe S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P690, DOI 10.1109/ICCV.2001.937589; Ioffe S, 2001, INT J COMPUT VISION, V43, P45, DOI 10.1023/A:1011179004708; ISARD M, 1996, EUR C COMP VIS, V1, P343; Ju SX, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P38, DOI 10.1109/AFGR.1996.557241; Kakadiaris IA, 1996, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.1996.517057; Karaulova I. A., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P352; Lee MW, 2004, LECT NOTES COMPUT SC, V3022, P126; LEUNG MK, 1995, IEEE T PATTERN ANAL, V17, P359, DOI 10.1109/34.385981; MacCormick J, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P390, DOI 10.1109/ICCV.1998.710748; MACCORMICK J, 1998, EUR C COMP VIS; Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; McKenna SJ, 2000, COMPUT VIS IMAGE UND, V80, P42, DOI 10.1006/cviu.2000.0870; METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727; Mikolajczyk K, 2004, LECT NOTES COMPUT SC, V3021, P69; Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897; MOESLUND TB, 1999, LIA9901 U AALB; MOESLUND TB, 2000, IEEE WORKSH HUM MOD; Mori G, 2002, LECT NOTES COMPUT SC, V2352, P666; MORI G, 2004, RECOVERING HUMAN BOD, P326; MORRI SDD, 1998, IEEE C COMP VIS PATT, P289; ONG EJ, 1999, BMVC P           SEP, P33; OROURKE J, 1980, IEEE T PATTERN ANAL, V2, P522, DOI 10.1109/TPAMI.1980.6447699; Park JS, 2000, PROC SPIE, V3972, P2; PUZICHA J, 1999, IEEE INT C COMP VIS, P1165; Ramanan D, 2003, PROC CVPR IEEE, P467; REHG JM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P612, DOI 10.1109/ICCV.1995.466882; ROBERTS TJ, 2004, EUR C COMP VIS; Ronfard R, 2002, LECT NOTES COMPUT SC, V2353, P700; Rosales R, 2001, PROC CVPR IEEE, P821; Rosales R, 2000, PROC CVPR IEEE, P721, DOI 10.1109/CVPR.2000.854946; RUZON MA, 1999, IEEE C COMP VIS PATT, P160; Schiele B, 2000, INT J COMPUT VISION, V36, P31, DOI 10.1023/A:1008120406972; SHAHROKNI A, 2004, EUR C COMP VIS PRAG, V2, P566; Sidenbladh H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P709, DOI 10.1109/ICCV.2001.937696; SIDENBLADH H, 2000, INT C AUT FAC GEST R, P368; Sminchisescu C, 2003, PROC CVPR IEEE, P69; Sminchisescu C, 2001, PROC CVPR IEEE, P447; Stauffer C, 2001, PROC CVPR IEEE, P221; Taylor CJ, 2000, COMPUT VIS IMAGE UND, V80, P349, DOI 10.1006/cviu.2000.0878; Wachter S, 1999, COMPUT VIS IMAGE UND, V74, P174, DOI 10.1006/cviu.1999.0758; Wilhelms J, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P155, DOI 10.1109/HUMO.2000.897386; Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236; ZHAO JM, 1994, ACM T GRAPHIC, V13, P313, DOI 10.1145/195826.195827	66	6	6	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2007	73	3					285	306		10.1007/s11263-006-9781-9	http://dx.doi.org/10.1007/s11263-006-9781-9			22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	155HB					2022-12-18	WOS:000245567400003
J	Maybank, SJ				Maybank, Stephen J.			Application of the Fisher-Rao metric to ellipse detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						ellipse detection; Fisher-Rao metric; flat metric; geodesic; Hough transform; Kullback-Leibler distance; lattice; multiresolution; Riemannian manifold; volume of a Riemannian manifold; Voronoi's principal lattice	OBJECTS	The parameter space for the ellipses in a two dimensional image is a five dimensional manifold, where each point of the manifold corresponds to an ellipse in the image. The parameter space becomes a Riemannian manifold under a Fisher-Rao metric, which is derived from a Gaussian model for the blurring of ellipses in the image. Two points in the parameter space are close together under the Fisher-Rao metric if the corresponding ellipses are close together in the image. The Fisher-Rao metric is accurately approximated by a simpler metric under the assumption that the blurring is small compared with the sizes of the ellipses under consideration. It is shown that the parameter space for the ellipses in the image has a finite volume under the approximation to the Fisher-Rao metric. As a consequence the parameter space can be replaced, for the purpose of ellipse detection, by a finite set of points sampled from it. An efficient algorithm for sampling the parameter space is described. The algorithm uses the fact that the approximating metric is flat, and therefore locally Euclidean, on each three dimensional family of ellipses with a fixed orientation and a fixed eccentricity. Once the sample points have been obtained, ellipses are detected in a given image by checking each sample point in turn to see if the corresponding ellipse is supported by the nearby image pixel values. The resulting algorithm for ellipse detection is implemented. A multiresolution version of the algorithm is also implemented. The experimental results suggest that ellipses can be reliably detected in a given low resolution image and that the number of false detections can be reduced using the multiresolution algorithm.	Univ London Birkbeck Coll, Sch Comp Sci & Informat Syst, London WC1E 7HX, England	University of London; Birkbeck University London	Maybank, SJ (corresponding author), Univ London Birkbeck Coll, Sch Comp Sci & Informat Syst, Malet St, London WC1E 7HX, England.	sjmaybank@dcs.bbk.ac.uk						Abramowitz M, 1965, HDB MATH FUNCTIONS F; Agrawal M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P782; AGUADO AS, 1995, P 5 INT C IM PROC IT, P375; Amari S.I., 1985, LECT NOTES STATISTIC, P28; [Anonymous], 1992, SHAPE DETECTION COMP; Arias-Castro E, 2005, IEEE T INFORM THEORY, V51, P2402, DOI 10.1109/TIT.2005.850056; CHAVEL I, 1996, CAMBRIDGE TRACTS MAT, V108; Conway J. H., 1988, GRUNDLEHREN MATH WIS, V290; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Desolneux A, 2003, ANN STAT, V31, P1822; Desolneux A, 2000, INT J COMPUT VISION, V40, P7, DOI 10.1023/A:1026593302236; Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658; Gallot S., 1990, RIEMANNIAN GEOMETRY; Hartley R., 2003, MULTIPLE VIEW GEOMET; JI Q, 1999, IEEE INT C IM PROC I, V2, P730; KANATANI K, 1994, IEEE T PATTERN ANAL, V16, P320, DOI 10.1109/34.276132; KANATANI K, 2003, INT J IMAGE GRAPHICE, V4, P35; KANTANI K, 1996, STAT COMPUTATION GEO; Kwolek B, 2004, LECT NOTES COMPUT SC, V2034, P192; KWOLEK B, LECT NOTES COMPUTER, V3024; Lazebnik S., 2004, P BRIT MACH VIS C, V2, P959, DOI DOI 10.5244/C.18.98; LEEDAN Y, 6 INT C COM PVIS ICC, P733; LUTTINGER P, 1994, INFORMATIONSDIENST S, V12, P1; MAIO D, 1998, NATO ASI SER, P568; Maybank SJ, 2005, INT J COMPUT VISION, V63, P191, DOI 10.1007/s11263-005-6877-6; Maybank SJ, 2004, IEEE T PATTERN ANAL, V26, P1579, DOI 10.1109/TPAMI.2004.122; MAYBANK SJ, 2003, P ROY SOC LOND A MAT, V459, P1; MAYBANK SJ, 2003, J MATH IMAGING VISIO, V25; Rosin PL, 1996, GRAPH MODEL IM PROC, V58, P494, DOI 10.1006/gmip.1996.0041; Rucklidge WJ, 1997, INT J COMPUT VISION, V24, P251, DOI 10.1023/A:1007975324482; Scaggiante A., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P582, DOI 10.1109/ICIAP.1999.797659; TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273; Torr PHS, 2004, IEEE T PATTERN ANAL, V26, P648, DOI 10.1109/TPAMI.2004.1273967; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303; WANG C, 2003, GLOBAL CHANGE BIOL, V9, P1; Wolfram S., 2003, MATH BOOK; Xie YH, 2002, INT C PATT RECOG, P957, DOI 10.1109/ICPR.2002.1048464; XU L, 1993, CVGIP-IMAG UNDERSTAN, V57, P131, DOI 10.1006/ciun.1993.1009; Yao J, 2004, INT C PATT RECOG, P859, DOI 10.1109/ICPR.2004.1334394; Younes L, 1998, SIAM J APPL MATH, V58, P565, DOI 10.1137/S0036139995287685; Zhang ZY, 1997, IMAGE VISION COMPUT, V15, P59, DOI 10.1016/S0262-8856(96)01112-2	41	6	7	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2007	72	3					287	307		10.1007/s11263-006-9033-z	http://dx.doi.org/10.1007/s11263-006-9033-z			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	133MW		Green Accepted, Green Submitted			2022-12-18	WOS:000244018000004
J	Pitiot, A; Delingette, H; Thompson, PM				Pitiot, Alain; Delingette, Herve; Thompson, Paul M.			Learning shape correspondence for n-D curves	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						curve matching; shape descriptor; learning approach	DISCRIMINATION	We present a learning method that introduces explicit knowledge into the shape correspondence problem. Given two input curves to be matched, our approach establishes a dense correspondence field between them, where the characteristics of the matching field closely resemble those in an a priori learning set. We build a shape distance matrix from the values of a shape descriptor computed at every point along the curves. This matrix embeds the correspondence problem in a highly expressive and redundant construct and provides the basis for a pattern matching strategy for curve matching. We selected the previously introduced observed transport measure as a shape descriptor, as its properties make it particularly amenable to the matching problem. Synthetic and real examples are presented along with discussions of the robustness and applications of the technique.	Univ Nottingham, Lab Image & Data Anal, Brain & Body Ctr, Nottingham NG7 2RD, England; INRIA, Projet ASCLEPIOS, Sophia Antipolis, France; Univ Calif Los Angeles, Sch Med, Lab Neuroimaging, Los Angeles, CA USA	University of Nottingham; Inria; University of California System; University of California Los Angeles; University of California Los Angeles Medical Center; David Geffen School of Medicine at UCLA	Pitiot, A (corresponding author), Univ Nottingham, Lab Image & Data Anal, Brain & Body Ctr, Nottingham NG7 2RD, England.		Thompson, Paul M/C-4194-2018	Thompson, Paul M/0000-0002-4720-8867				Angenent S, 2003, SIAM J MATH ANAL, V35, P61, DOI 10.1137/S0036141002410927; ARKIN E, 1991, IEEE T PATTERN ANAL, V13, P703; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; ATTNEAVE FRED, 1966, P123; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Bookstein F. L., 1991, MORPHOMETRIC TOOLS L; Chui HL, 2000, PROC CVPR IEEE, P44, DOI 10.1109/CVPR.2000.854733; Cohen I, 1998, PROC CVPR IEEE, P741, DOI 10.1109/CVPR.1998.698686; COHEN L, 1992, P EUR C COMP VIS ECC, P458; COOTES TF, 1998, P EUR C COMP VIS, V2, P484; Davatzikos C, 1996, IEEE T MED IMAGING, V15, P112, DOI 10.1109/42.481446; David S, 2002, J SUSTAIN AGR, V21, P5, DOI 10.1300/J064v21n02_03; Fleute M, 1999, Med Image Anal, V3, P209, DOI 10.1016/S1361-8415(99)80020-6; Gdalyahu Y, 1999, IEEE T PATTERN ANAL, V21, P1312, DOI 10.1109/34.817410; GOLDMEIER E, 1972, PSYCHOL ISSUES, V8, P1; GOSHTASBY A, 1985, IEEE T PATTERN ANAL, V7, P738, DOI 10.1109/TPAMI.1985.4767734; GRENANDER U, 1991, HANDS PATTERN THEROT; HAKE HAROLD W., 1966, P142; Hebb D., 1949, ORG BEHAV; HORN BKP, 1990, INT J COMPUT VISION, V5, P37, DOI 10.1007/BF00056771; Kanai T, 2000, IEEE COMPUT GRAPH, V20, P62, DOI 10.1109/38.824544; Kelemen A, 1999, IEEE T MED IMAGING, V18, P828, DOI 10.1109/42.811260; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; KOENDERINK JJ, 1986, BIOL CYBERN, V53, P383, DOI 10.1007/BF00318204; Koffka K., 1935, PRINCIPLES GESTALT P; Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850; Leventon ME, 2000, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P4, DOI 10.1109/MMBIA.2000.852354; LEYTON M, 1987, COMPUT VISION GRAPH, V38, P327, DOI 10.1016/0734-189X(87)90117-4; Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MARR D, 1970, PROC R SOC SER B-BIO, V176, P161, DOI 10.1098/rspb.1970.0040; MARR D, 1976, P ROYAL SOC LOND, P483; McInerney T, 1996, Med Image Anal, V1, P91, DOI 10.1016/S1361-8415(96)80007-7; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681; PITIOT A, 2003, THESIS ECOLE MINES P; PITIOT A, 2003, P INF PROC MED IM IP; PLATO, 1967, MENO 380 BC, V3; PROKOP RJ, 1992, CVGIP-GRAPH MODEL IM, V54, P438, DOI 10.1016/1049-9652(92)90027-U; Sebastian TB, 2000, IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, PROCEEDINGS, P70, DOI 10.1109/MMBIA.2000.852362; Thome JR, 2004, INT J REFRIG, V27, P294, DOI 10.1016/j.ijrefrig.2003.08.003; TROUVE A, 2000, P 6 EUR C COMP VIS T, P573; ULUPINAR F, 2000, P INT C PATT REC ICP, P147; Van Otterloo P.J, 1992, CONTOUR ORIENTED APP; VELTKAMP PC, 1991, P INT C SHAP MOD APP, P188; Veltkamp R., 1999, UUCS199927; Wang YM, 2000, PROC CVPR IEEE, P644, DOI 10.1109/CVPR.2000.854933; Zusne L., 1970, VISUAL PERCEPTION FO	47	6	6	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2007	71	1					71	88		10.1007/s11263-006-8114-3	http://dx.doi.org/10.1007/s11263-006-8114-3			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	094GT					2022-12-18	WOS:000241228600003
J	Solem, JE; Heyden, A				Solem, Jan Erik; Heyden, Anders			Reconstructing open surfaces from image data	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						variational methods; computer vision; level set method; multiple view stereo; structure from motion		In this paper a method for fitting open surfaces to data obtained from images is presented using a level set representation of the surface. This is done by tracking a curve, representing the boundary, on the implicitly defined surface. This curve is given as the intersection of the level set describing the surface and an auxiliary level set. These two level sets are propagated using the same motion vector field. Special care has to be taken in order for the surfaces not to intersect at other places than at the desired boundary. Methods for accomplishing this are presented and a fast scheme for finding, initial values is proposed. This method gives a piecewise linear approximation of the initial surface boundary using a partition of the convex hull of the recovered 3D data. With the approach described in this paper, open surfaces can be fitted to e.g. point clouds obtained using structure from motion techniques. This paper solves an important practical problem since in many cases the surfaces in the scene are open or can only be viewed from certain directions. Experiments on several data sets support the method.	Malmo Univ Hosp, Sch Technol & Soc, Malmo, Sweden	Lund University; Skane University Hospital	Solem, JE (corresponding author), Malmo Univ Hosp, Sch Technol & Soc, Malmo, Sweden.	jes@ts.mah.se; heyden@ts.mah.se						BERTALMIO M, 1999, LECT NOTES COMPUTER, V1682; Burchard P, 2001, J COMPUT PHYS, V170, P720, DOI 10.1006/jcph.2001.6758; Caselles V, 1997, IEEE T PATTERN ANAL, V19, P394, DOI 10.1109/34.588023; Caselles Vicent, 1997, INT J COMPUTER VISIO; CHENG LT, 2001, 0032 UCLA CAM; DERVIEUX A, 1979, LECT NOTES MATH, V771, P145; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; FAUGERAS O, 2000, P 9 IMA C MATH SURF; Gravouil A, 2002, INT J NUMER METH ENG, V53, P2569, DOI 10.1002/nme.430; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011; Lhuillier M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1313; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Morris DD, 2000, PROC CVPR IEEE, P332, DOI 10.1109/CVPR.2000.855837; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; Osher S., 2002, APPL MATH SCI, V44, P685; Piegl L, 1996, NURBS BOOK; Pons JP, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P894; PONS JP, 2005, INT C COMP VIS PATT; Sethian J. A., 1999, LEVEL SET METHODS FA; Smereka P, 2000, PHYSICA D, V138, P282, DOI 10.1016/S0167-2789(99)00216-X; SOLEM J, 2005, ADV NEURAL INFORM PR, V17; SOLEM JE, 2005, 5 INT C SCAL SPAC PD, P419; SOLEM JE, 2004, 2 INT S 3D DAT PROC; SOLEM JE, 2004, INT C COMP VIS PATT; Zhao HK, 2000, COMPUT VIS IMAGE UND, V80, P295, DOI 10.1006/cviu.2000.0875	26	6	7	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2006	69	3					267	275		10.1007/s11263-006-7068-9	http://dx.doi.org/10.1007/s11263-006-7068-9			9	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	067YM					2022-12-18	WOS:000239338700001
J	Zelnik-Manor, L; Irani, M				Zelnik-Manor, Lihi; Irani, Michal			On single-sequence and multi-sequence factorizations	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						factorization; subspace based methods; multi-body segmentation; temporal synchronization; multisequence matching	MOTION	Subspace based factorization methods are commonly used for a variety of applications, such as 3D reconstruction, multi-body segmentation and optical flow estimation. These are usually applied to a single video sequence. In this paper we present an analysis of the multi-sequence case and place it under a single framework with the single sequence case. In particular, we start by analyzing the characteristics of subspace based spatial and temporal segmentation. We show that in many cases objects moving with different 3D motions will be captured as a single object using multi-body (spatial) factorization approaches. Similarly, frames viewing different shapes might be grouped as displaying the same shape in the temporal factorization framework.(1) We analyze what causes these degeneracies and show that in the case of multiple sequences these can be made useful and provide information for both temporal synchronization of sequences and spatial matching of points across sequences.	CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA; Weizmann Inst Sci, Dept Comp Sci & Appl Math, IL-76100 Rehovot, Israel	California Institute of Technology; Weizmann Institute of Science	Zelnik-Manor, L (corresponding author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.	lihi@vision.caltech.edu; michal.irani@weizmann.ac.il						Boult T. E., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P179, DOI 10.1109/WVM.1991.212809; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; COSTEIRA J, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1071, DOI 10.1109/ICCV.1995.466815; Gear CW, 1998, INT J COMPUT VISION, V29, P133, DOI 10.1023/A:1008026310903; Ichimura N, 2000, PROC CVPR IEEE, P446, DOI 10.1109/CVPR.2000.854877; Irani M, 2002, INT J COMPUT VISION, V48, P173, DOI 10.1023/A:1016372015744; IRANI M, 2000, ECCV, P539; KANATANI K, 2001, INT C COMP VIS VANC, V1, P301; MACHLINE M, 2002, WORKSH VIS MODE DYN; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L, 2001, PROC CVPR IEEE, P493; WOLF L, 2002, WORKSH COP; ZELNIKMANOR L, 2003, IEEE C COMP VIS PATT; ZELNIKMANOR L, 2004, EUR C COMP VIS PRAG, V2, P434	14	6	7	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	MAY	2006	67	3					313	326		10.1007/s11263-006-5157-4	http://dx.doi.org/10.1007/s11263-006-5157-4			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	052JA		Green Accepted, Green Submitted			2022-12-18	WOS:000238228800004
J	Chau, CP; Siu, WC				Chau, CP; Siu, WC			Generalized Hough transform using regions with homogeneous color	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article							CIRCLES; WATERSHEDS; OBJECTS	A novel generalized Hough transform algorithm which makes use of the color similarity between homogeneous segments as the voting criterion is proposed in this paper. The input of the algorithm is some regions with homogeneous color. These regions are obtained by first pre-segmenting the image using the morphological watershed algorithm and then refining the resultant outputs by a region merging algorithm. Region pairs belonging to the object are selected to generate entries of the reference table for the Hough transform. Every R-table entry stores a relative color between the selected region pairs. This is done in order to compute the color similarity and in turn generate votes during the voting process and some relevant information to recover the transformation parameters of the object. Based on the experimental results, our algorithm is robust to change of illumination, occlusion and distortion of the segmentation output. It recognizes objects which were translated, rotated, scaled and even located in a complex environment.	Hong Kong Polytech Univ, Dept Elect & Informat Engn, Ctr Multimedia Signal Proc, Kowloon, Hong Kong, Peoples R China	Hong Kong Polytechnic University	Chau, CP (corresponding author), Hong Kong Polytech Univ, Dept Elect & Informat Engn, Ctr Multimedia Signal Proc, Kowloon, Hong Kong, Peoples R China.	enwcsiu@polyu.edu.hk						Ballard D.H., 1982, COMPUTER VISION; BALLIVETTKATCHENKO D, 1981, J MOL CATAL, V13, P1, DOI 10.1016/0304-5102(81)85027-4; Beucher S, 1994, COMP IMAG VIS, V2, P69; BEUCHER S, MATH MORPHOLOGY IMAG, pCH12; CHAN R, 1991, IEE PROC-E, V138, P335, DOI 10.1049/ip-e.1991.0046; CONKER RS, 1988, COMPUT VISION GRAPH, V43, P115, DOI 10.1016/0734-189X(88)90057-6; Dougherty E. R., 1992, INTRO MORPHOLOGICAL; FOLEY J, 1992, COMPUTER GRAPHICS PR; FORSBERG J, 1995, IEEE ROBOT AUTOM MAG, V2, P18, DOI 10.1109/100.388295; GAO H, 2001, IEEE T CIRCUITS SYST, V11; Haris K, 1998, IEEE T IMAGE PROCESS, V7, P1684, DOI 10.1109/83.730380; Hough P.V., 1962, US Patent, Patent No. [US3069654A, 3069654, 3,069,654]; Leung H, 1996, IEE P-RADAR SON NAV, V143, P246, DOI 10.1049/ip-rsn:19960404; Li HL, 1998, IEEE T CIRCUITS-II, V45, P80, DOI 10.1109/82.659459; Lo RC, 1996, IEE P-VIS IMAGE SIGN, V143, P201, DOI 10.1049/ip-vis:19960448; MEYER F, 1992, P 4 INT C IM PROC IT; NIXON MS, 1993, IEE PROC-I, V140, P86, DOI 10.1049/ip-i-2.1993.0012; PAL NR, 1993, PATTERN RECOGN, V26, P1277, DOI 10.1016/0031-3203(93)90135-J; SCHUNN AB, 1994, PRACTICAL COLOR MEAS; SER PK, 1995, IEE P-VIS IMAGE SIGN, V142, P262, DOI 10.1049/ip-vis:19952199; SER PK, 1995, J VIS COMMUN IMAGE R, V6, P256, DOI 10.1006/jvci.1995.1022; Tsai DM, 1997, IMAGE VISION COMPUT, V15, P877, DOI 10.1016/S0262-8856(97)00033-4; VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344; WU XL, 1993, IEEE T PATTERN ANAL, V15, P808, DOI 10.1109/34.236248; YIP RKK, 1992, PATTERN RECOGN, V25, P1007, DOI 10.1016/0031-3203(92)90064-P; YUEN HK, 1989, IMAGE VISION COMPUT, V7, P31, DOI 10.1016/0262-8856(89)90017-6	26	6	6	0	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	2004	59	2					183	199		10.1023/B:VISI.0000022289.77537.91	http://dx.doi.org/10.1023/B:VISI.0000022289.77537.91			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	842DR					2022-12-18	WOS:000222983800004
J	Ishiguro, H; Sogo, T; Barth, M				Ishiguro, H; Sogo, T; Barth, M			Baseline detection and localization for invisible omnidirectional cameras	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						omnidirectional camera; distributed vision; invisible camera; identification; localization; baseline detection; triangle constraint; constraint propagation		Two key problems for camera networks that observe wide areas with many distributed cameras are self-localization and camera identification. Although there are many methods for localizing the cameras, one of the easiest and most desired methods is to estimate camera positions by having the cameras observe each other; hence the term self-localization. If the cameras have a wide viewing field, e.g. an omnidirectional camera, and can observe each other, baseline distances between pairs of cameras and relative locations can be determined. However, if the projection of a camera is relatively small on the image of other cameras and is not readily visible, the baselines cannot be detected. In this paper, a method is proposed to determine the baselines and relative locations of these "invisible" cameras. The method consists of two processes executed simultaneously: (a) to statistically detect the baselines among the cameras, and (b) to localize the cameras by using information from (a) and propagating triangle constraints. Process (b) works for the localization in the case where the cameras are observed each other, and it does not require complete observation among the cameras. However, if many cameras cannot be observed each other because of the poor image resolution, it dose not work. The baseline detection by process (a) solves the problem. This methodology is described in detail and results are provided for several scenarios.	Wakayama Univ, Dept Comp & Commun Sci, Wakayama, Japan; Kyoto Univ, Dept Social Informat, Kyoto, Japan; Univ Calif Riverside, Dept Elect Engn, Riverside, CA 92521 USA	Wakayama University; Kyoto University; University of California System; University of California Riverside	Ishiguro, H (corresponding author), Wakayama Univ, Dept Comp & Commun Sci, Wakayama, Japan.	ishiguro@sys.wakayama-u.ac.jp	Sogo, Takuya/AAK-7075-2020; Wu, Guoyuan/AAJ-8804-2020	Barth, Matthew/0000-0002-4735-5859				Adelson E. H., 1991, COMPUTATION MODELS V; Aggarwal JK, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P90, DOI 10.1109/NAMW.1997.609859; BOYD JE, 1998, IEEE INT C MULT SYST; COLLINS L, 1999, P AM NUCL SOC ANS 8; HONG J, 1991, P INT C ROB AUT; ISHIGURO H, 1992, IEEE T PATTERN ANAL, V14, P257, DOI 10.1109/34.121792; Ishiguro H., 1998, P INT C INF SYST AN, P433; ISHIGURO H, 2001, P INT JOINT C ART IN, P1375; ISHIGURO H, 1997, P INT JOINT C ART IN, P36; JAIN R, 1995, P INT C MULT COMP SY; Kato K., 1999, Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289), P966, DOI 10.1109/IROS.1999.812805; Medioni G, 2001, IEEE T PATTERN ANAL, V23, P873, DOI 10.1109/34.946990; NAYAR SK, 1997, P IM UND WORKSH, P1431; Rees D. W., 1970, US Patent, Patent No. 3505465; SARACHIK KB, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P984, DOI 10.1109/ROBOT.1989.100109; Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552; YAGI Y, 1990, P IROS; YAMAZAWA K, 1993, P INC C ROB SYST	18	6	6	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	2004	58	3					209	226		10.1023/B:VISI.0000019687.45792.f0	http://dx.doi.org/10.1023/B:VISI.0000019687.45792.f0			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	836GD					2022-12-18	WOS:000222542900003
J	Porr, B; Nurenberg, B; Worgotter, F				Porr, B; Nurenberg, B; Worgotter, F			A VLSI-compatible computer vision algorithm for stereoscopic depth analysis in real-time	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						stereo-disparity; phase-based; causal processing; electronic filter	OPTICAL-FLOW; PHASE; PERFORMANCE; FILTERS	The analysis of the depth coordinates of objects in a visual scene is of vital importance for animals as well as in technological applications like autonomous robot navigation or product quality control. In this article we describe a phase-based algorithm for stereoscopic depth analysis which utilizes IIR-filters.(1) This algorithm is especially well suited to be built into dedicated VLSI-hardware and can therefore, also be used as a fast real-time front end in any more general image processing system. Example movies which demonstrate the real-time capabilities of this algorithm can be found at: http://www.cn.stir.ac.uk/Real-Time-Stereo.	Univ Stirling, Dept Psychol, Ctr Cognit & Computat Neurosci, Stirling FK9 4LA, Scotland; I to I GmbH, Dept 3D Stereo, D-21079 Hamburg, Germany	University of Stirling	Porr, B (corresponding author), Univ Stirling, Dept Psychol, Ctr Cognit & Computat Neurosci, Stirling FK9 4LA, Scotland.							BARNARD ST, 1982, ACM COMPUT SURV, V14, P553, DOI DOI 10.1145/356893.356896; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Cozzi A, 1997, MACH VISION APPL, V9, P334, DOI 10.1007/s001380050052; DHOND UR, 1989, IEEE T SYST MAN CYB, V19, P1489, DOI 10.1109/21.44067; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FLEET D, 1991, COMP VISION GRAPHIC, V53, P14; FLEET DJ, 1995, IEEE T PATTERN ANAL, V17, P61, DOI 10.1109/34.368151; FLEET DJ, 1993, IEEE T PATTERN ANAL, V15, P1253, DOI 10.1109/34.250844; FLEET DJ, 1991, CVGIP-IMAG UNDERSTAN, V53, P198, DOI 10.1016/1049-9660(91)90027-M; FLEET DJ, 1993, P C VIS INT TOR CAN, P116; FLEET DJ, 1994, IEEE INT C SYST MAN, P48; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; Haralick RM, 1992, COMPUTER ROBOT VISIO, V2; HENKEL RD, 1994, 4 U BREM ZENTR KOGN; KANADE T, 1995, P INT ROB SYST C IRO; Kuglin C. D., 1975, Proceedings of the 1975 International Conference on Cybernetics and Society, P163; LANGLEY K, 1991, IMAGE VISION COMPUT, V9, P296, DOI 10.1016/0262-8856(91)90034-M; LANGLEY K, 1992, P 9 ISR S AI COMP VI, P255; SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114	19	6	6	0	1	KLUWER ACADEMIC PUBL	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2002	49	1					39	55		10.1023/A:1019825715052	http://dx.doi.org/10.1023/A:1019825715052			17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	585UJ					2022-12-18	WOS:000177542700003
J	Shao, JL				Shao, JL			Generation of temporally consistent multiple virtual camera views from stereoscopic image sequences	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						stereoscopic image sequences; 3D TV; image matching; multiple views; color regularization; view morphing; view interpolation; cross-frame consistency	MOTION PARALLAX; TRACKING; OBJECTS; SCALE	The emergence of a new generation of 3D auto stereoscopic displays is driving the requirement for multi-baseline images. The dominant form of this display technology requires multiple views of the same scene, captured at a single instance in time along a common baseline in order to project stereoscopic images to the viewer. The direct acquisition of multiple views (typically 8 or 16 for the current generation of such displays) is problematic due to the difficulty of configuring, calibrating and controlling multiple cameras simultaneously. This paper describes a technique that alleviates these problems by generating the required views from binocular images. Considering each stereo pair in isolation leads to inconsistency across image sequences. By incorporating a motion-tracking algorithm this problem is significantly reduced. In this paper we describe a novel approach to stereo matching of image sequences for the purpose of generating multiple virtual camera views. Results of extensive tests on stereo image sequences will be documented indicating that this approach is promising both in terms of the speed of execution and the quality of the results produced.	Dynam Digital Depth Res, Bentley, WA 6983, Australia		Shao, JL (corresponding author), Dynam Digital Depth Res, POB 1169, Bentley, WA 6983, Australia.	jshao@ddd.com						ALVAREZ L, 2000, 3874 INRIA; BARRON L, 1995, P INT IM PROC, V2, P193; BEALL AC, 1995, P SOC PHOTO-OPT INS, V2411, P288, DOI 10.1117/12.207547; BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003; Black MJ, 1996, IEEE T PATTERN ANAL, V18, P972, DOI 10.1109/34.541407; Cai Q, 1999, IEEE T PATTERN ANAL, V21, P1241, DOI 10.1109/34.809119; CHARBONNIER P, 1994, IEEE IMAGE PROC, P168; DELAGNES P, 1995, PATTERN RECOGN, V15, P171; FALKENHAGEN L, 1995, P INT WORKSH STER 3; GEMAN D, 1992, IEEE T PATTERN ANAL, V14, P367, DOI 10.1109/34.120331; GIL S, 1996, P 4 EUR C COMP VIS, V2, P307; Huang HC, 2000, PROC SPIE, V3957, P102, DOI 10.1117/12.384434; Jelinek D, 2001, VIRTUAL AND AUGMENTED ARCHITECTURE (VAA'01), P219; JENKIN MRM, 1994, CVGIP-IMAG UNDERSTAN, V59, P72, DOI 10.1006/ciun.1994.1005; Jones GA, 1997, COMPUT VIS IMAGE UND, V65, P57, DOI 10.1006/cviu.1996.0482; KANG SB, 1999, SPIE, V3641, P2; KOCH R, 1994, ISPRS J PHOTOGRAMM, V49, P23, DOI 10.1016/0924-2716(94)90021-3; KOCH R, 1995, P INT C COMP VIS CAM; KOLLER D, 1993, INT J COMPUT VISION, V10, P257, DOI 10.1007/BF01539538; La Cascia M, 2000, IEEE T PATTERN ANAL, V22, P322, DOI 10.1109/34.845375; LANDY M, 1991, PLENOPTIC FUNCTION; MATTISON DR, 1989, REPROD TOXICOL, V3, P3, DOI 10.1016/0890-6238(89)90032-4; MORAVEC K, 1998, P BRIT MACH VIS C; NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833; NIEM W, 1994, P IM PROC BROADC VID; *NYK R D, 2001, 38 NHK R D; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; ROGERS B, 1979, PERCEPTION, V8, P125, DOI 10.1068/p080125; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Saito H, 1995, IEEE IND ELEC, P1231, DOI 10.1109/IECON.1995.483973; SEITZ SM, 1996, P SIGGRAPH 96, P21; Shao J, 1998, EXPERT SYST APPL, V14, P471, DOI 10.1016/S0957-4174(98)00006-2; VEDULA S, 1999, CMURI9915 ROB I; Veksler O, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P540, DOI 10.1109/ICCV.2001.937563; WAXMAN AM, 1986, IEEE T PATTERN ANAL, V8, P715, DOI 10.1109/TPAMI.1986.4767853; WAXMAN AM, 1986, IEEE T PATTERN ANAL, V8, P406, DOI 10.1109/TPAMI.1986.4767806; WEIK S, 1999, SPIE, V3653; ZHANG Z, 1992, 3D DYNAMIC SCENE ANA; [No title captured]	39	6	6	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR-JUN	2002	47	1-3					171	180		10.1023/A:1014545908590	http://dx.doi.org/10.1023/A:1014545908590			10	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	530JN					2022-12-18	WOS:000174354700013
J	Dubuc, B; Zucker, SW				Dubuc, B; Zucker, SW			Complexity, confusion, and perceptual grouping. Part II: Mapping complexity	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						perceptual organization; segmentation; complexity; curve detection; texture	FRACTAL DIMENSION; DISTANCE TRANSFORMATIONS; IMAGES	Intermediate-level vision is central to form perception, and we outline an approach to intermediate-level segmentation based on complexity analysis. In this second of a pair of papers, we continue the focus on edge-element grouping, and the motivating example of an edge element inferred from an unknown image. Is this local edge part of a long curve, or part of a texture ? If the former, which is the next element along the curve ? If the latter, is the texture like a well-combed hair pattern, in which nearby elements are oriented similarly, or more chaotic, as in a spaghetti pattern ? In the previous paper we showed how these questions raise issues of complexity and dimensionality, and how context in both position and orientation are important. We now propose a measure based on tangential and normal complexities, and illustrate its computation. Tangential complexity is related to extension; normal complexity to density. Taken together they define four canonical classes of tangent distributions: those arising from curves, from texture flows, from turbulent textures, and from isolated "dust". Examples are included.	Espace Courbe, Montreal, PQ H4C 3C5, Canada; Yale Univ, Ctr Computat Vis & Control, Dept Comp Sci, New Haven, CT 06520 USA; Yale Univ, Ctr Computat Vis & Control, Dept Elect Engn, New Haven, CT 06520 USA	Yale University; Yale University	Dubuc, B (corresponding author), Espace Courbe, 642 de Courcelle,Suite 303, Montreal, PQ H4C 3C5, Canada.	benoit@espacecourbe.com; zucker-steven@cs.yale.edu		Dubuc, Benoit/0000-0002-9481-3154				Alter TD, 1996, PROC CVPR IEEE, P13, DOI 10.1109/CVPR.1996.517047; ANTONINI A, 1993, J NEUROSCI, V13, P3549; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0; BORGEFORS G, 1984, COMPUT VISION GRAPH, V27, P321, DOI 10.1016/0734-189X(84)90035-5; BRETON P, 1994, THESIS MCGILL U MONT; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DUBUC B, 1989, PHYS REV A, V39, P1500, DOI 10.1103/PhysRevA.39.1500; Dubuc B, 1996, SIAM J NUMER ANAL, V33, P602, DOI 10.1137/0733032; DUBUC B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P142, DOI 10.1109/ICCV.1995.466794; DUBUC B, 1988, THESIS MCGILL U MONT; DUBUC B, 1995, THESIS MCGILL U MONT; DUPAIN Y, 1986, ARCH RATION MECH AN, P155; FLACONER KJ, 1990, FRACTAL GEOMETRY MAT; Gemelli A, 1931, Z PSYCHOL PHYSIOL SI, V123, P308; HEITGER F, 1993, P 4 INT C COMP VIS, P32; Kanizsa G., 1979, ORG VISION; Koenderink J., 1990, SOLID SHAPE; Kolmogorov A. N., 1968, International Journal of Computer Mathematics, V2, P157, DOI 10.1080/00207166808803030; KOLMOGOROV AN, 1987, SELECTED WORKS, V3; LEYMARIE F, 1992, CVGIP-IMAG UNDERSTAN, V55, P84, DOI 10.1016/1049-9660(92)90008-Q; Lindeberg T., 1993, Proceedings of the 8th Scandinavian Conference on Image Analysis, P857; LINDENBAUM M, 2000, P ECCV, P257; MAHONEY JV, 1987, 980 MIT ART INT LAB; Mandelbrot, 1982, FRACTAL GEOMETRY NAT, P394; MANDELBROT BB, 1985, PHYS SCRIPTA, V32, P257, DOI 10.1088/0031-8949/32/4/001; Matheron G., 1975, RANDOM SETS INTEGRAL; MENDESFRANCE M, 1991, FRACTALS NONINTEGRAL, P222; MENDESFRANCE M, 1991, FRACTAL GEOMETRY ANA, P325; Pagels H. R., 1988, DREAMS REASON COMPUT; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P661, DOI 10.1109/TPAMI.1984.4767591; PENTLAND AP, 1985, IMAGE VISION COMPUT, V3, P153, DOI 10.1016/0262-8856(85)90002-2; SANTALO LA, 1976, ENCY MATH ITS APPL, V2; Serra J, 1982, IMAGE ANAL MATH MORP; SHAASHUA A, 1988, P 2 INT C COMP VIS, P321, DOI DOI 10.1109/CCV.1988.590008; TREISMAN A, 1985, COMPUT VISION GRAPH, V31, P156, DOI 10.1016/S0734-189X(85)80004-9; TRICOT C, 1988, REV PHYS APPL, V23, P111, DOI 10.1051/rphysap:01988002302011100; Tricot C., 1995, CURVES FRACTAL DIMEN, DOI [10.1007/978-1-4612-4170-6, DOI 10.1007/978-1-4612-4170-6]; Turing AM, 1937, P LOND MATH SOC, V42, P230, DOI 10.1112/plms/s2-42.1.230; ULLMAN S, 1990, COLD SH Q B, V55, P889; Valiron G., 1966, THEORIE FONCTIONS; WILLIAMS L, 1998, P ECCV, P432	42	6	6	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.		2001	42	1-2					83	115		10.1023/A:1011141618114	http://dx.doi.org/10.1023/A:1011141618114			33	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	437VX					2022-12-18	WOS:000169015200006
J	Stauder, J				Stauder, J			Point light source estimation from two images and its limits	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						illumination estimation; point light source; estimation error analysis; Cramer-Rao lower bound; observation model; maximum-likelihood estimation	ILLUMINANT DIRECTION; OBJECT RECOGNITION; MACHINE VISION; FACIAL IMAGES; FLOW-FIELD; SHAPE; MOTION; MODEL; SEQUENCES; RECOVERY	In this paper, the performance of parameter estimation of a single static distant point light source from two video images is analyzed in terms of estimation theory. The illumination parameters are the intensity and direction of the light source. In the first part of this paper, estimators from the literature are reviewed. Most recent estimators evaluate as input data two video images as well as the 3D shape and the 3D motion of the visible moving objects. In the second part of the paper, the performance of these recent methods is analyzed. The input data to estimation as well as the inherent input data errors are described by a stochastic observation model. Based on this model, the performance is analyzed regarding the Cramer-Rao theoretical lower bound of estimation error variances. The bound is derived for a variety of cases of scene illumination, object motion and errors in input data. For simplification purpose, the bound is valid only for object motions with the rotation axis lying in the image plane. The analysis shows in which cases which estimation accuracy can be expected with current methods. Finally, a comparison of the bound with one of the recent estimators shows that recent estimators are suboptimal in case of errors in the 3D shape of the objects. In future work, the stochastic observation model presented in this paper can be used to improve illumination estimation.	Thomson Multimedia R&D France, F-35511 Cesson Sevigne, France; Leibniz Univ Hannover, Inst Theoret Nachrichtentech & Informat Verarbeit, D-30167 Hannover, Germany; IRISA, INRIA, Rennes, France	Technicolor SA; Leibniz University Hannover; Inria	Stauder, J (corresponding author), Thomson Multimedia R&D France, 1 Ave Belle Fontaine,BP 19, F-35511 Cesson Sevigne, France.	stauderj@thmulti.com						ARNSPANG J, 1991, PATTERN RECOGN LETT, V12, P203, DOI 10.1016/0167-8655(91)90033-I; BECKER S, 1994, FORMULATING SCENE PR; BEJANIN M, 1994, 2 INT IEEE WORKSH AP, P160; Blohm W, 1997, IEEE T IMAGE PROCESS, V6, P1129, DOI 10.1109/83.605410; BLONDE L, 1996, IEEE MULTI MEDIA, V3; Bronstein I. N., 1996, TEUBNER TASCHENBUCH; Brunelli R, 1997, IMAGE VISION COMPUT, V15, P741, DOI 10.1016/S0262-8856(97)00024-3; CHOJNACKI W, 1994, J OPT SOC AM A, V11, P118, DOI 10.1364/JOSAA.11.000118; DAIMLERBENZ AG, 1992, COST 211 SIM SUBG TU; Deshpande SG, 1998, COMPUT VIS IMAGE UND, V72, P10, DOI 10.1006/cviu.1997.0657; DESHPANDE SG, 1996, THESIS INDIAN I TECH; DESHPANDE SG, 1996, ICIP 96 INT C IM PRO; DREW MS, 1990, IEEE C COMP VIS, P394; EISERT P, 1996, 3D IMAGE ANAL SYNTHE, P61; FORSYTH D, 1989, ALV VIS C, P193; GILGE M, 1990, SPIE, V1224, P355; GRIMSEHL E, 1988, LEHRBUCH PHYSIK, V3; HAMPSON FJ, 1996, INT C IM PROC LAUS, V1, P101; Haralick R.M., 1993, COMPUTER ROBOT VISIO, V2; Hashimoto T., 1992, Systems and Computers in Japan, V23, P1, DOI 10.1002/scj.4690231201; IKEUCHI K, 1999, MIXED REALITY MERGIN; IRANI M, 1994, INT J COMPUT VISION, V12, P5, DOI 10.1007/BF01420982; Iwahori Y., 1991, Systems and Computers in Japan, V22, P99, DOI 10.1002/scj.4690221209; Kanatani K., 1996, STAT OPTIMIZATION GE; KLINKER GJ, 1990, INT J COMPUT VISION, V4, P7, DOI 10.1007/BF00137441; KLINKER GJ, 1988, INT J COMPUT VISION, V2, P7, DOI 10.1007/BF00836279; KOCH R, 1993, IEEE T PATTERN ANAL, V15, P556, DOI 10.1109/34.216725; KOLLER D, 1993, INT J COMPUT VISION, V10, P257, DOI 10.1007/BF01539538; Lambert J. H., 1892, LAMBERTS PHOTOMETRIE; LANGER MS, 1994, J OPT SOC AM A, V11, P467, DOI 10.1364/JOSAA.11.000467; LEE CH, 1989, SHAPE SHADING, P323; LEE KM, 1994, CVGIP-IMAG UNDERSTAN, V59, P202, DOI 10.1006/ciun.1994.1013; LEE KY, 1991, PICT COD S TOK, P275; MANABE Y, 1995, T I ELECT INF COMM D, V2, P86; Melsa J, 1978, DECISION ESTIMATION; Mendel J. M, 1995, LESSONS ESTIMATION T; MEYBERG K, 1993, HOHERE MATH, V1; MOLONEY CR, 1991, IEEE P ICASSP 91 TOR, V4, P2425; Mukawa N., 1992, Systems and Computers in Japan, V23, P66, DOI 10.1002/scj.4690231206; Mukawa N., 1992, Systems and Computers in Japan, V23, P92, DOI 10.1002/scj.4690231008; MURASE H, 1994, IEEE T PATTERN ANAL, V16, P1219, DOI 10.1109/34.387485; MURASE H, 1991, IEEE INT C COMP VIS, P313; MUSMANN HG, 1989, IMAGE COMMUN, V1, P117; Nayar SK, 1996, INT J COMPUT VISION, V17, P219, DOI 10.1007/BF00128232; NAYAR SK, 1991, IEEE T PATTERN ANAL, V13, P611, DOI 10.1109/34.85654; NEGAHDARIPOUR S, 1993, INT C COMP VIS BERL, P2; NICOLAS H, 1995, J VIS COMMUN IMAGE R, V6, P303, DOI 10.1006/jvci.1995.1026; Nicolas H, 1997, P SOC PHOTO-OPT INS, V3024, P1330, DOI 10.1117/12.263213; NOMURA A, 1995, PATTERN RECOGN LETT, V16, P285, DOI 10.1016/0167-8655(94)00094-J; ONO E, 1993, PICT COD S LAUS; Oppenheim A. V., 1998, DISCRETE TIME SIGNAL, V2nd; OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684; OSTERMANN J, 1994, SIGNAL PROCESS-IMAGE, V6, P143, DOI 10.1016/0923-5965(94)90012-4; Papoulis A., 1991, COMMUNICATIONS SIGNA, V3; Pearson D., 1990, Signal Processing: Image Communication, V2, P377, DOI 10.1016/0923-5965(90)90025-D; PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P879, DOI 10.1109/34.93807; PENTLAND AP, 1990, INT J COMPUT VISION, V4, P153, DOI 10.1007/BF00127815; PENTLAND AP, 1982, J OPT SOC AM, V72, P448, DOI 10.1364/JOSA.72.000448; Phillips PJ, 1996, PATTERN RECOGN LETT, V17, P921, DOI 10.1016/0167-8655(96)00046-3; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; POCHEC P, 1991, PATTERN RECOGN LETT, V12, P363, DOI 10.1016/S0167-8655(05)80006-6; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; RASHID HU, 1992, IMAGE VISION COMPUT, V10, P119, DOI 10.1016/0262-8856(92)90006-O; Scales L. E., 1985, INTRO NONLINEAR OPTI; Shafer S. A., 1985, SHADOWS SILHOUETTES; Sorenson H. W, 1980, CONTROL SYSTEMS THEO; STAUDER J, 1995, SIGNAL PROCESS-IMAGE, V7, P355, DOI 10.1016/0923-5965(95)00008-7; STAUDER J, 1999, THESIS U HANNOVER GE; Stauder J, 1999, IEEE T MULTIMEDIA, V1, P136, DOI 10.1109/6046.766735; STAUDIER J, 1998, INT C COMP GRAPH HAN, P506; Stewart AJ, 1997, IEEE T PATTERN ANAL, V19, P1020, DOI 10.1109/34.615450; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; TREVES P, 1994, IEEE IMAGE PROC, P373, DOI 10.1109/ICIP.1994.413338; Voinov V., 1989, UNBIASED ESTIMATORS; Wang KF, 1996, IEICE T FUND ELECTR, VE79A, P1330; Watt A., 1993, 3D COMPUTER GRAPHICS; Wei J, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL III, P614, DOI 10.1109/ICIP.1997.632196; WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074; WOLFF LB, 1991, IEEE T PATTERN ANAL, V13, P635, DOI 10.1109/34.85655; WOLFF LB, 1988, INT J COMPUT VISION, V30, P55; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; YANG Y, 1990, INT C COMP VIS, P534; YI SK, 1995, COMPUT VIS IMAGE UND, V61, P122, DOI 10.1006/cviu.1995.1009; YOUNG GSJ, 1992, IEEE T PATTERN ANAL, V14, P995, DOI 10.1109/34.159903; YU Y, 1998, SIGGRAPH 98, P207; Zhang R, 1996, COMPUT VIS IMAGE UND, V63, P221, DOI 10.1006/cviu.1996.0016; Zheng JY, 1997, IEEE T PATTERN ANAL, V19, P513, DOI 10.1109/34.589212; ZHENG JY, 1996, INT C PATT REC ICPR, P800; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658; Zheng Q, 1993, IEEE T IMAGE PROCESS, V2, P311, DOI 10.1109/83.236535	90	6	6	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2000	36	3					195	220		10.1023/A:1008177019313	http://dx.doi.org/10.1023/A:1008177019313			26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	315LD					2022-12-18	WOS:000087114500002
J	Ponce, J; Genc, Y				Ponce, J; Genc, Y			Epipolar geometry and linear subspace methods: A new approach to weak calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						projective geometry; subspace methods; weak calibration	SELF-CALIBRATION; MOTION; CAMERA; ALGORITHM; IMAGES; MATRIX; SCENE	This paper addresses the problem of estimating the epipolar geometry from point correspondences between two images taken by uncalibrated perspective cameras. It is shown that Jepson's and Heeger's linear subspace technique for infinitesimal motion estimation can be generalized to the finite motion case by choosing an appropriate basis for projective space. This yields a linear method for weak calibration. The proposed algorithm has been implemented and tested on both real and synthetic images, and it is compared to other linear and non-linear approaches to weak calibration.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Ponce, J (corresponding author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.	ponce@cs.uiuc.edu; y-genc@cs.uiuc.edu	Genc, Yakup/AAG-4668-2019	Genc, Yakup/0000-0002-6952-6735				BARNARD ST, 1980, IEEE T PATTERN ANAL, V2, P333, DOI 10.1109/TPAMI.1980.4767032; BOUFAMA B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1030, DOI 10.1109/ICCV.1995.466821; CARLSSON S, 1994, LECT NOTES COMPUTER, V825, P145; CHASLES M, 1855, NOUV ANN MATH, V14; Ciarlet P. G., 1990, HDB NUMERICAL ANAL, V1-9; COUAPEL B, 1994, THESIS U RENNES 1; DEMEY S, 1992, P BRIT MACH VIS C LE, P49; DERICHE R, 1993, INT J COMPUT VISION, V10, P101, DOI 10.1007/BF01420733; DERICHE R, 1994, LECT NOTES COMPUTER, V800, P567; Dongarra J. J., 1979, LINPACK USERS GUIDE; DRESCHLER L, 1982, INT C PATT REC, P542; FAUGERAS O, 1988, J PATTERN RECOGNITIO, V2, P485; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P564; Faugeras Olivier, 1993, 3 DIMENSIONAL COMPUT, P2; FUA P, 1993, MACHINE VISION APPL, V6; GENNERY D, 1980, THESIS STANFORD U ST; GROS P, 1992, 90IMAG RT LIFIAIRIMA; Harris C, 1988, P 4 ALV VIS C, P147, DOI DOI 10.5244/C.2.23; Hartley R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P761, DOI 10.1109/CVPR.1992.223179; Hartley R., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P549, DOI 10.1109/CVPR.1993.341076; HARTLEY R, 1993, P DARPA IM UND WORKS, P737; Hartley R. I., 1993, P DARPA IM UND WORKS, P745; HARTLEY RI, 1992, LECT NOTES COMPUT SC, V588, P579; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1064; HARTLEY RI, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P908; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; HESSE O, 1863, J REINE ANGEW MATH, V62, P188; HORAUD R, 1989, IEEE T PATTERN ANAL, V11, P1168, DOI 10.1109/34.42855; Horn BKP, 1986, COMPUTER VISION; HUANG TS, 1989, IEEE T PATTERN ANAL, V11, P1310, DOI 10.1109/34.41368; JEPSON A, 1991, IEEE WORKSH VIS MOT, P124; Jepson AD, 1992, SPATIAL VISION HUMAN, P39; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; LAVEAU S, 1994, 2205 INRIA SOPH ANT; LAWN J, 1996, LNCS, V1065, P161; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Luong Q.-T., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P489, DOI 10.1109/CVPR.1993.341085; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; LUONG QT, 1994, LECT NOTES COMPUTER, V800, P577; LUONG QT, 1993, 1894 INRIA SOPH ANT; LUONG QT, 1992, THESIS U PARIX 11 OR; Maybank S., 1992, THEORY RECONSTRUCTIO; MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171; MOHR R, 1992, 84IMAG RT LIFIAIRIMA; Moravec H.P., 1977, PROC INT JOINT C ART, P584; More J., 1980, ANL8074; NISHIHARA H, 1984, 780 MIT AI; NISHIMURA E, 1993, 1ST P AS C COMP VIS, P199; NOBLE JA, 1988, IMAGE VISION COMPUT, V6, P121, DOI 10.1016/0262-8856(88)90007-8; OLSEN SI, 1992, LECT NOTES COMPUT SC, V588, P307; Ponce J., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P463; Robert L., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P540, DOI 10.1109/ICCV.1993.378165; ROHR K, 1992, IMAGE VISION COMPUT, V10, P66, DOI 10.1016/0262-8856(92)90001-J; SEITZ S, 1995, WORKSH REPR VIS SC B; SEITZ SM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P330; SHAPIRO LG, 1981, IEEE T PATTERN ANAL, V3, P504, DOI 10.1109/TPAMI.1981.4767144; SHAPIRO LS, 1993, REJECTING OUTLIERS E; SHASHUA A, 1993, FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION : PROCEEDINGS, P583; SINCLAIR D, 1992, P BRIT MED VIS C, P59; Sturm R., 1869, MATH ANN, V1, P533; TODD JA, 1946, PROJECTIVE ANAL GEOM; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TOMASI C, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P913, DOI 10.1109/CVPR.1994.323924; TORR PHS, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1037, DOI 10.1109/ICCV.1995.466820; TORR PHS, 1993, P SOC PHOTO-OPT INS, V2059, P432, DOI 10.1117/12.150246; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; VIEVILLE T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P750, DOI 10.1109/ICCV.1995.466863; WENG JY, 1992, IEEE T PATTERN ANAL, V14, P806, DOI 10.1109/34.149592; Wilkinson J., 1971, LINEAR ALGEBRA, V2; XU G, 1993, IMAGE CORRES SEGMENT; ZHANG ZY, 1994, IMAGE VISION COMPUT, V12, P110, DOI 10.1016/0262-8856(94)90020-5; ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4	73	6	6	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL-AUG	1998	28	3					223	243		10.1023/A:1008053620575	http://dx.doi.org/10.1023/A:1008053620575			21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	118BM					2022-12-18	WOS:000075817800003
J	Heyden, A				Heyden, A			Reconstruction from image sequences by means of relative depths	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	5th International Conference on Computer Vision	JUN 20-23, 1995	MIT, CAMBRIDGE, MA	IEEE Comp Soc Tech Comm Pattern Anal & Machine Intelligence	MIT		POLYHEDRAL IMAGES; COMPUTATIONS	This paper deals with the problem of reconstructing the locations of n points in space from m different images without camera calibration. It shows how these problems can be put into a similar theoretical framework. A new concept, the reduced fundamental matrix, is introduced. It contains just 4 parameters and can be used to predict locations of points in the images and to make reconstruction. We also introduce the concept of reduced fundamental tensor, which describes the relations between points in 3 images. It has 15 components and depends on 9 parameters. Necessary and sufficient conditions for a tensor to be a reduced fundamental tensor are derived. This framework can be generalised to a sequence of images. The dependencies between the different representations are investigated. Furthermore a canonical form of the camera matrices in a sequence are presented.			Heyden, A (corresponding author), LUND UNIV,DEPT MATH,BOX 118,S-22100 LUND,SWEDEN.							CHASLES M, 1855, NOUV ANN MATH, V14; FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321; FAUGERAS OD, 1992, LECTURE NOTES COMPUT, V588, P563; HARTLEY RI, 1994, IEEE T PATTERN ANAL, V16, P1036, DOI 10.1109/34.329005; HEYDEN A, 1995, LUFTD2TFMA957003SE I; HEYDEN A, 1994, P S IM AN SSAB HALMS; LUONG QT, 1994, LECT NOTES COMPUTER, V800, P589; MAYBANK S, 1993, THEORY RECONSTRUCTIO; SHAHSUA A, 1994, LECT NOTES COMPUTER, V800, P479; SPARR G, 1992, IMAGE VISION COMPUT, V10, P683, DOI 10.1016/0262-8856(92)90013-S; SPARR G, 1992, LECT NOTES COMPUT SC, V588, P378, DOI 10.1016/0262-8856(92)90013-S; SPARR G, 1994, LECT NOTES COMPUTER, V801, P471; SPARR G, 1991, P 7 SCAND C IM AN, P274	13	6	6	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1997	24	2					155	161		10.1023/A:1007963021756	http://dx.doi.org/10.1023/A:1007963021756			7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XW658					2022-12-18	WOS:A1997XW65800005
J	Xiong, YL; Shafer, SA				Xiong, YL; Shafer, SA			Hypergeometric filters for optical flow and affine matching	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article; Proceedings Paper	5th International Conference on Computer Vision	JUN 20-23, 1995	MIT, CAMBRIDGE, MA	IEEE Comp Soc Tech Comm Pattern Anal & Machine Intelligence	MIT		COMPLEX ARGUMENTS; LARGE MAGNITUDES; PHASE	This paper proposes new ''hypergeometric'' filters for the problem of image matching under the translational and affine model. This new set of filters has the following advantages: (1) High-precision registration of two images under the translational and affine model. Because the window effects are eliminated, we are able to achieve superb performance in both translational and affine matching. (2) Affine matching without exhaustive search or image warping. Due to the recursiveness of the filters in the spatial domain, We are able to analytically express the relation between filter outputs and the six affine parameters. This analytical relation enables us to directly compute these affine parameters. (3) Generality. The approach we demonstrate here can be applied to a broad class of matching problems as long as the transformation between the two image patches can be mathematically represented in the frequency domain.			Xiong, YL (corresponding author), CARNEGIE MELLON UNIV,INST ROBOT,PITTSBURGH,PA 15213, USA.							Abramowitz M., 1972, HDB MATH FUNCTIONS, V10; ADELSON EH, 1984, J OPT SOC AM A, P284; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Brodatz P., 1966, TEXTURES PHOTOGRAPHI; Fleet D. J., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P379, DOI 10.1109/CVPR.1989.37875; FLEET DJ, 1991, CVGIP-IMAG UNDERSTAN, V53, P198, DOI 10.1016/1049-9660(91)90027-M; JONES DG, 1991, THESIS STANFORD U; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; Manmatha R., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P754, DOI 10.1109/CVPR.1993.341158; NARDIN M, 1992, J COMPUT APPL MATH, V39, P193, DOI 10.1016/0377-0427(92)90129-L; NARDIN M, 1992, ACM T MATH SOFTWARE, V18, P345, DOI 10.1145/131766.131774; ROBERT L, 1994, EUR C COMP VIS, P377; Spanier J., 1987, ATLAS FUNCTIONS; Subbarao M., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P149, DOI 10.1109/CCV.1988.589986; WENG J, 1993, INT J COMPUT VISION, V11, P211, DOI 10.1007/BF01469343; Xiong Y., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P68, DOI 10.1109/CVPR.1993.340977; XIONG Y, 1994, CMURITR9428	18	6	6	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	SEP	1997	24	2					163	177		10.1023/A:1007915105826	http://dx.doi.org/10.1023/A:1007915105826			15	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XW658					2022-12-18	WOS:A1997XW65800006
J	Cai, LD; Mayhew, J				Cai, LD; Mayhew, J			A note on some phase differencing algorithms for disparity estimation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						stereo disparity; Gabor filter; phase difference; singularity; numerical instability; matching residual norm		Disparity can be estimated from the local phase differences between a pair of stereo images after Gabor filterings. In this paper, some phase differencing algorithms are discussed from the point of view of numerical computation. They are all categorised as certain types of Newton iterations, hence are restricted by the preconditions of Newton iterations. Furthermore, a numerical sensitivity analysis is made for phase computation near its singularity points without referring to any specific filtering method. It makes clear that the numerical instability has its roots in the singularities of the phase functions themselves rather than in their derivatives, therefore severe errors cannot be properly reduced by Newton iterations. Hence, the singularity problem must be carefully treated for any disparity estimation using phase differencing techniques. In the case of foveal vision, such as target detection or vergence control, the central parts of scenes are of interest. A shift-trials algorithm is proposed to contain the effects of singularities instead of relying on detection and removal of singularity neighbourhoods through local computation. This algorithm directly tackles the singularity problem by posing the local disparity estimations as a minimisation of the phase differences of the left and right views in terms of a global matching residual norm. By a direct search for solution, no numerical derivative is needed, phase singularity can be tolerated and robust results are thus produced.	UNIV SHEFFIELD,ARTIFICIAL VIS RES UNIT,SHEFFIELD S10 2TN,S YORKSHIRE,ENGLAND	University of Sheffield								BARNARD ST, 1980, IEEE T PATTERN ANAL, V2, P333, DOI 10.1109/TPAMI.1980.4767032; CAI LD, 1992, P BRIT MACHINE VISIO, P237; EPSTEIN B, 1970, LINEAR FUNCTIONAL AN; Fleet D. J., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P52, DOI 10.1109/WVM.1991.212788; FLEET DJ, 1991, CVGIP-IMAG UNDERSTAN, V53, P198, DOI 10.1016/1049-9660(91)90027-M; FLEET DJ, 1992, KLUWER INT SERIES EN, V169; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; Hardy G. H., 1952, INEQUALITIES; JENKIN MRM, 1991, CVGIP-IMAG UNDERSTAN, V53, P14, DOI 10.1016/1049-9660(91)90002-7; JEPSON AD, 1991, IMAGE VISION COMPUT, V9, P338, DOI 10.1016/0262-8856(91)90039-R; JULSEZ B, 1971, FDN CYCLOPEAN PERCEP; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; MAYHEW JEW, 1981, ARTIF INTELL, V17, P349, DOI 10.1016/0004-3702(81)90029-1; Press WH, 1988, NUMERICAL RECIPES C; Rudin W., 1976, PRINCIPLES MATH ANAL, V3; SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114; VITERBI AJ, 1963, P IEEE, V51, P1737, DOI 10.1109/PROC.1963.2686; WESTELIUS CJ, 1992, LITHISYI1363 LINK U	18	6	10	0	0	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	MAR	1997	22	2					111	124		10.1023/A:1007983927952	http://dx.doi.org/10.1023/A:1007983927952			14	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	XD967					2022-12-18	WOS:A1997XD96700001
J	DURIC, Z; ROSENFELD, A; DAVIS, LS				DURIC, Z; ROSENFELD, A; DAVIS, LS			EGOMOTION ANALYSIS BASED ON THE FRENET-SERRET MOTION MODEL	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						CONSTRAINT INTERSECTION; CURVATURE AND TORSION; DOMINANT MOTION; EGOMOTION ANALYSIS; FRENET-SERRET MOTION MODEL; NORMAL OPTICAL FLOW; SCREW MOTION EQUATIONS		In this paper we propose a new model, Frenet-Serret motion, for the motion of an observer in a stationary environment. This model relates the motion parameters of the observer to the curvature and torsion of the path along which the observer moves. Screw-motion equations for Frenet-Serret motion are derived and employed for geometrical analysis of the motion. Normal flow is used to derive constraints on the rotational and translational velocity of the observer and to compute egomotion by intersecting these constraints in the manner proposed in (Duric and Aloimonos 1991) The accuracy of egomotion estimation is analyzed for different combinations of observer motion and feature distance. We explain the advantages of controlling feature distance to analyze egomotion and derive the constraints on depth which make either rotation or translation dominant in the perceived normal flow field. The results of experiments on real image sequences are presented.			DURIC, Z (corresponding author), UNIV MARYLAND,CTR AUTOMAT RES,COMP VIS LAB,COLLEGE PK,MD 20742, USA.							ALBUS JS, 1991, IEEE T SYST MAN CYB, V21, P473, DOI 10.1109/21.97471; ALOIMONOS J, 1984, P INT C PATTERN RECO, P542; BOTTEMA O, THEORETICAL KINEMATI; BRUSS AR, 1983, COMPUT VISION GRAPH, V21, P3, DOI 10.1016/S0734-189X(83)80026-7; DRESCHLER L, 1982, COMPUT VISION GRAPH, V20, P199, DOI 10.1016/0146-664X(82)90081-8; DURIC Z, CARTR482 U MAR CTR A; HORN BKP, 1988, INT J COMPUT VISION, V2, P51, DOI 10.1007/BF00836281; HORN BKP, 1981, ARTIF INTELL, V17, P189; IKEUCHI K, 1984, ARTIF INTELL, V22, P49, DOI 10.1016/0004-3702(84)90025-0; Kreyszig E, 1959, DIFFERENTIAL GEOMETR; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; NEGAHDARIPOUR S, 1987, AI939 MIT ART INT LA; Raviv D., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P217, DOI 10.1109/WVM.1991.212804; SPETSAKIS ME, 1990, INT J COMPUT VISION, V4, P171, DOI 10.1007/BF00054994; Tomasi C., 1991, Proceedings of the IEEE Workshop on Visual Motion (Cat. No.91TH0390-5), P21, DOI 10.1109/WVM.1991.212792; TSAI RY, 1984, IEEE T PATTERN ANAL, V6, P13, DOI 10.1109/TPAMI.1984.4767471; VERRI A, 1987, 1ST P INT C COMP VIS, P171	17	6	7	0	2	KLUWER ACADEMIC PUBL	DORDRECHT	SPUIBOULEVARD 50, PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS	0920-5691			INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	1995	15	1-2					105	122		10.1007/BF01450851	http://dx.doi.org/10.1007/BF01450851			18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QY042					2022-12-18	WOS:A1995QY04200005
J	Zhang, Y; Zhang, Z; Wang, Y; Zhang, Z; Zhang, L; Yan, SC; Wang, M				Zhang, Yan; Zhang, Zhao; Wang, Yang; Zhang, Zheng; Zhang, Li; Yan, Shuicheng; Wang, Meng			Dual-Constrained Deep Semi-Supervised Coupled Factorization Network with Enriched Prior	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Deep semi-supervised coupled factorization network; Representation learning; Dual constraints; Clustering; Enriched prior; Error correction; Fine-tuning of features	NONNEGATIVE MATRIX FACTORIZATION; DIMENSIONALITY REDUCTION; LOCAL COORDINATE; DECOMPOSITION	Nonnegativity based matrix factorization is usually powerful for learning the parts-based "shallow" representation, however it fails to discover deep hidden information within both the basis concept and representation spaces. In this paper, we therefore propose a new dual-constrained deep semi-supervised coupled factorization network ((DSCF)-C-2-Net) for learning hierarchical representations. (DSCF)-C-2-Net is formulated as the joint partial-label and structure-constrained deep factorization network using multi-layers of linear transformations, which coupled updates the basic concepts and new representations in each layer. An error correction mechanism with feature fusion strategy is also integrated between consecutive layers to improve the representation ability of features. To improve the discriminating abilities of both representation and coefficients in feature space, we clearly consider how to enrich the prior knowledge by the coefficients-based label prediction, and incorporate the enriched prior knowledge as the additional label and structure constraints. To be specific, the label constraint enables the intra-class samples to have the same coordinate in the feature space, while the structure constraint forces the coefficients in each layer to be block-diagonal so that the enriched prior knowledge are more accurate. Besides, we integrate the adaptive dual-graph learning to retain the locality structures of both the data manifold and feature manifold in each layer. Finally, a fine-tuning process is performed to refine the structure-constrained matrix and data weight matrix in each layer using the predicted labels for more accurate representations. Extensive simulations on public databases show that our method can obtain state-of-the-art performance.	[Zhang, Yan; Zhang, Li] Soochow Univ, Sch Comp Sci & Technol, Suzhou, Peoples R China; [Zhang, Zhao; Wang, Yang; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei, Peoples R China; [Zhang, Zhao; Wang, Yang; Wang, Meng] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei, Peoples R China; [Zhang, Zhao; Wang, Yang; Wang, Meng] Hefei Univ Technol, Intelligent Interconnected Syst Lab Anhui Prov, Hefei, Peoples R China; [Zhang, Zheng] Harbin Inst Technol, Biocomp Res Ctr, Shenzhen, Peoples R China; [Zhang, Zheng] Pengcheng Lab, Shenzhen, Peoples R China; [Yan, Shuicheng] Sea AI Lab, Singapore, Singapore	Soochow University - China; Hefei University of Technology; Hefei University of Technology; Hefei University of Technology; Harbin Institute of Technology	Zhang, Z (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei, Peoples R China.; Zhang, Z (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei, Peoples R China.; Zhang, Z (corresponding author), Hefei Univ Technol, Intelligent Interconnected Syst Lab Anhui Prov, Hefei, Peoples R China.	cszzhang@gmail.com	Yan, Shuicheng/HCI-1431-2022; zhang, zhang/GQZ-6804-2022; Zhang, Zhao/B-5136-2010	Zhang, Zhao/0000-0002-5703-7969	National Natural Science Foundation of China [62072151, 62020106007, 61806035 andU1936217]; Anhui Provincial Natural Science Fund for Distinguished Young Scholars [2008085J30]; Fundamental Research Funds for the Central Universities of China [JZ2019-HGPA0102]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Anhui Provincial Natural Science Fund for Distinguished Young Scholars; Fundamental Research Funds for the Central Universities of China(Fundamental Research Funds for the Central Universities)	The authors also would like to express our sincere thanks to the anonymous reviewers' constructive comments and suggestions which have made the paper a higher standard. We also sincerely thank Prof. Mingliang Xu and Prof. Yi Yang for their professional discussion on the error correction mechanism with feature fusion strategy, and the fine-tuning process for refining the features. This work is partially supported by the National Natural Science Foundation of China (62072151, 62020106007, 61806035 andU1936217), Anhui Provincial Natural Science Fund for Distinguished Young Scholars (2008085J30) and the Fundamental Research Funds for the Central Universities of China (JZ2019-HGPA0102).	Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bergstra J., 2013, JMLR WORKSHOP C P IC, V28, P115, DOI [10.5555/3042817.3042832, DOI 10.5555/3042817.3042832]; Cai D, 2005, IEEE T KNOWL DATA EN, V17, P1624, DOI 10.1109/TKDE.2005.198; Cai D, 2011, IEEE T KNOWL DATA EN, V23, P902, DOI 10.1109/TKDE.2010.165; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Cichocki A, 2006, ELECTRON LETT, V42, P947, DOI 10.1049/el:20060983; Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Dhillon I. S., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P269, DOI 10.1145/502512.502550; GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027; Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229; Gu QQ, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P359; He X., 2005, P INT C NEUR INF PRO, P507, DOI DOI 10.3233/IDT-120147; He ZY, 2017, IEEE T CYBERNETICS, V47, P354, DOI 10.1109/TCYB.2016.2514714; Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Jolliffe I. T., 1986, PRINCIPAL COMPONENT, P41, DOI DOI 10.1007/978-1-4757-1904-8; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Leibe B, 2003, PROC CVPR IEEE, P409; Li HR, 2017, J VIS COMMUN IMAGE R, V49, P392, DOI 10.1016/j.jvcir.2017.10.005; Li X, 2017, NEUROCOMPUTING, V238, P139, DOI 10.1016/j.neucom.2017.01.045; Li X, 2015, INT CONF COMP SCI ED, P486, DOI 10.1109/ICCSE.2015.7250295; Li Xuelong, 2017, IEEE Trans Cybern, V47, P3840, DOI 10.1109/TCYB.2016.2585355; Li ZC, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2624140; Lin BH, 2020, IEEE T IMAGE PROCESS, V29, P565, DOI 10.1109/TIP.2019.2928627; Liu HF, 2014, IEEE T CYBERNETICS, V44, P1214, DOI 10.1109/TCYB.2013.2287103; Liu HF, 2012, IEEE T PATTERN ANAL, V34, P1299, DOI 10.1109/TPAMI.2011.217; Lovasz L., 1986, MATCHING THEORY; Ma SH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2539; Ma XK, 2019, IEEE T KNOWL DATA EN, V31, P273, DOI 10.1109/TKDE.2018.2832205; Nayar S, 1996, CUCS00696; Pan JJ, 2021, IEEE T PATTERN ANAL, V43, P1546, DOI 10.1109/TPAMI.2019.2956046; Peng Y, 2019, INT CONF ACOUST SPEE, P3162, DOI 10.1109/ICASSP.2019.8682779; Rahiche A, 2021, IEEE T IMAGE PROCESS, V30, P5997, DOI 10.1109/TIP.2021.3088266; Rajabi R, 2015, IEEE GEOSCI REMOTE S, V12, P38, DOI 10.1109/LGRS.2014.2325874; Ren JH, 2020, IEEE T IMAGE PROCESS, V29, P3941, DOI 10.1109/TIP.2020.2965289; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Shang FH, 2012, PATTERN RECOGN, V45, P2237, DOI 10.1016/j.patcog.2011.12.015; Shen YM, 2019, INT J COMPUT VISION, V127, P1614, DOI 10.1007/s11263-019-01166-4; Sugiyama M, 2007, J MACH LEARN RES, V8, P1027; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Trigeorgis G, 2017, IEEE T PATTERN ANAL, V39, P417, DOI 10.1109/TPAMI.2016.2554555; Wang M., 2019, P SIAM INT C DAT MIN; Weyrauch B., 2004, 2004 C COMP VIS PATT, DOI 10.1109/CVPR.2004.315; Xiao H., 2017, FASHION MNIST NOVEL; Xiao T, 2019, LECT NOTES ARTIF INT, V11439, P426, DOI 10.1007/978-3-030-16148-4_33; Xu W., 2004, P 27 ACM SIGIR INT C, P202; Yang J, 2002, PATTERN RECOGN, V35, P1997, DOI 10.1016/S0031-3203(02)00040-7; Yang Y., 2011, P 22 INT JOINT C ART, P1589, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-267; Ye J, 2017, NEURAL PROCESS LETT, V46, P427, DOI 10.1007/s11063-017-9598-2; Ye J, 2014, NEUROCOMPUTING, V138, P120, DOI 10.1016/j.neucom.2014.02.029; Zhang CQ, 2020, INT J COMPUT VISION, V128, P2344, DOI 10.1007/s11263-020-01307-0; Zhang H, 2020, IEEE T NEUR NET LEAR, V31, P4538, DOI 10.1109/TNNLS.2019.2956015; Zhang Y., 2020, P 35 AAAI C ART INT; Zhang Z, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2021.102534; Zhang Z, 2021, IEEE T KNOWL DATA EN, V33, P1523, DOI 10.1109/TKDE.2019.2940576; Zhang Z, 2020, IEEE T KNOWL DATA EN, V32, P952, DOI 10.1109/TKDE.2019.2893956; Zhang Z, 2016, IEEE T IMAGE PROCESS, V25, P2429, DOI 10.1109/TIP.2016.2547180; Zhao RB, 2018, IEEE T SIGNAL PROCES, V66, P129, DOI 10.1109/TSP.2017.2757914	59	5	5	2	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	DEC	2021	129	12					3233	3254		10.1007/s11263-021-01524-1	http://dx.doi.org/10.1007/s11263-021-01524-1		OCT 2021	22	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WP1EN		Green Submitted			2022-12-18	WOS:000705812800001
J	Blum, H; Sarlin, PE; Nieto, J; Siegwart, R; Cadena, C				Blum, Hermann; Sarlin, Paul-Edouard; Nieto, Juan; Siegwart, Roland; Cadena, Cesar			The Fishyscapes Benchmark: Measuring Blind Spots in Semantic Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Semantic segmentation; Anomaly detection; Uncertainty estimation; Out-of-distribution detection; Autonomous driving		Deep learning has enabled impressive progress in the accuracy of semantic segmentation. Yet, the ability to estimate uncertainty and detect failure is key for safety-critical applications like autonomous driving. Existing uncertainty estimates have mostly been evaluated on simple tasks, and it is unclear whether these methods generalize to more complex scenarios. We present Fishyscapes, the first public benchmark for anomaly detection in a real-world task of semantic segmentation for urban driving. It evaluates pixel-wise uncertainty estimates towards the detection of anomalous objects. We adapt state-of-the-art methods to recent semantic segmentation models and compare uncertainty estimation approaches based on softmax confidence, Bayesian learning, density estimation, image resynthesis, as well as supervised anomaly detection methods. Our results show that anomaly detection is far from solved even for ordinary situations, while our benchmark allows measuring advancements beyond the state-of-the-art. Results, data and submission information can be found at https://fishyscapes.com/.	[Blum, Hermann; Siegwart, Roland; Cadena, Cesar] Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland; [Sarlin, Paul-Edouard] Swiss Fed Inst Technol, Comp Vis & Geometry Grp, Zurich, Switzerland; [Nieto, Juan] Microsoft Res, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Federal Institutes of Technology Domain; ETH Zurich	Blum, H (corresponding author), Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland.	blumh@ethz.ch		Blum, Hermann/0000-0002-1713-7877	ETH Zurich	ETH Zurich(ETH Zurich)	Open Access funding provided by ETH Zurich.	Antoran J., 2020, ADV NEUR IN; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bergmann P, 2019, PROC CVPR IEEE, P9584, DOI 10.1109/CVPR.2019.00982; Bevandic P, 2019, LECT NOTES COMPUT SC, V11824, P33, DOI 10.1007/978-3-030-33676-9_3; Bozhinoski D, 2019, J SYST SOFTWARE, V151, P150, DOI 10.1016/j.jss.2019.02.021; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Choi Hyunsun, 2018, WAIC WHY GENERATIVE; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai DX, 2020, INT J COMPUT VISION, V128, P1182, DOI 10.1007/s11263-019-01182-4; DeVries Terrance, 2018, ARXIV180204865; Di Biase G, 2021, PROC CVPR IEEE, P16913, DOI 10.1109/CVPR46437.2021.01664; Dillon J.V., 2017, TENSORFLOW DISTRIBUT; Dinh L, 2017, PR MACH LEARN RES, V70; Dinh Laurent, 2014, ARXIV14108516; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Florence Peter R, 2018, ARXIV180608756; Frey BJ, 1999, NEURAL COMPUT, V11, P193, DOI 10.1162/089976699300016872; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Gal Y., 2016, THESIS, V1, P3; Gal Y., 2016, P 33 INT C MACH LEAR, P1050, DOI DOI 10.5555/3045390.3045502; Geiger A., 2012, P IEEE COMP SOC C CO; Geyer Jakob, 2020, A2D2 AUDI AUTONOMOUS; Golan I, 2018, ADV NEUR IN, V31; Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Guo CA, 2017, PR MACH LEARN RES, V70; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hendrycks D., 2019, BENCHMARK ANOMALY SE; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jiang H, 2018, ADV NEUR IN, V31; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kingma DP, 2018, ADV NEUR IN, V31; Lakshminarayanan B, 2017, ADV NEUR IN, V30; Lee Kimin, 2018, ICLR; Lian SQ, 2018, DES AUT CON, DOI 10.1145/3195970.3196020; Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39; Lis K, 2019, IEEE I CONF COMP VIS, P2152, DOI 10.1109/ICCV.2019.00224; Loquercio A, 2020, IEEE ROBOT AUTOM LET, V5, P3153, DOI 10.1109/LRA.2020.2974682; Malinin A, 2018, ADV NEUR IN, V31; Malkov YA, 2020, IEEE T PATTERN ANAL, V42, P824, DOI 10.1109/TPAMI.2018.2889473; Mandelbaum A., 2017, DISTANCE BASED CONFI; McCormac J, 2018, INT CONF 3D VISION, P32, DOI 10.1109/3DV.2018.00015; Mukhoti Jishnu, 2018, ARXIV181112709; Nalisnick E., 2019, INT C LEARN REPR, P6; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Papernot N, 2018, ARXIV180304765; Pidhorskyi S, 2018, ADV NEUR IN, V31; Pinggera P, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1099, DOI 10.1109/IROS.2016.7759186; Postels J., 2020, QUANTIFYING ALEATORI; Richter C, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII; Rohekar RY, 2019, ADV NEUR IN, V32; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruff L, 2018, PR MACH LEARN RES, V80; Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356; Saito T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0118432; SAKARIDIS C, 2020, IEEE T PATTERN ANAL; Sakaridis C, 2018, LECT NOTES COMPUT SC, V11217, P707, DOI 10.1007/978-3-030-01261-8_42; Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252; Van Amersfoort Joost, 2020, ARXIV PREPRINT ARXIV, P9690; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Zendel O, 2018, LECT NOTES COMPUT SC, V11210, P407, DOI 10.1007/978-3-030-01231-1_25	69	5	5	2	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	NOV	2021	129	11					3119	3135		10.1007/s11263-021-01511-6	http://dx.doi.org/10.1007/s11263-021-01511-6		SEP 2021	17	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	WH8ZM		Green Submitted, Green Published, hybrid			2022-12-18	WOS:000695432000001
J	Zhao, T; Han, JW; Yang, L; Wang, BL; Zhang, DW				Zhao, Tao; Han, Junwei; Yang, Le; Wang, Binglu; Zhang, Dingwen			SODA: Weakly Supervised Temporal Action Localization Based on Astute Background Response and Self-Distillation Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Temporal action localization; Background response; Self-distillation learning	OBJECT DETECTION	Weakly supervised temporal action localization is a practical yet challenging task. Although great efforts have been made in recent years, the existing methods still have limited capacity in dealing with the challenges of over-localization, joint-localization, and under-localization. Based on our investigation, the first two challenges arise from insufficient ability to suppress background response, while the third challenge is due to the lack of discovering action frames. To better address these challenges, we first propose the astute background response strategy. By enforcing the classification target of the background category to be zero, such a strategy can endow the conductive effect between video-level classification and frame-level classification, thus guiding the action category to suppress responses at background frames astutely and helping address the over-localization and joint-localization challenges. For alleviating the under-localization challenge, we introduce the self-distillation learning strategy. It simultaneously learns one master network and multiple auxiliary networks, where the auxiliary networks enhance the master network to discover complete action frames. Experimental results on three benchmarks demonstrate the favorable performance of the proposed method against previous counterparts, and its efficacy to tackle the existing three challenges.	[Zhao, Tao; Han, Junwei; Yang, Le; Wang, Binglu; Zhang, Dingwen] Northwestern Polytech Univ, Sch Automat, Xian, Peoples R China	Northwestern Polytechnical University	Han, JW; Zhang, DW (corresponding author), Northwestern Polytech Univ, Sch Automat, Xian, Peoples R China.	taozhao2011@gmail.com; junweihan2010@gmail.com; nwpuyangle@gmail.com; wbl921129@gmail.com; zhangdingwen2006yyy@gmail.com	Zhang, Dingwen/S-9447-2017	Zhang, Dingwen/0000-0001-8369-8886	National Natural Science Foundation of China [61876140, U1801265]; Key-Area Research and Development Program of Guangdong Province [2019B010110001]; Research Funds for Interdisciplinary subject NWPU	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key-Area Research and Development Program of Guangdong Province; Research Funds for Interdisciplinary subject NWPU	This work was supported by the the National Natural Science Foundation of China under Grants 61876140 and U1801265, Key-Area Research and Development Program of Guangdong Province(2019B010110001), the Research Funds for Interdisciplinary subject NWPU.	[Anonymous], 2016, P ECCV; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Byun H., 2020, P AAAI C ART INT; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chan L, 2021, INT J COMPUT VISION, V129, P361, DOI 10.1007/s11263-020-01373-4; Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124; Choe J, 2020, PROC CVPR IEEE, P3130, DOI 10.1109/CVPR42600.2020.00320; Choe J, 2019, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR.2019.00232; CROWLEY EJ, 2018, P ANN C NEUR INF PRO, P2893; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5; Gao ZN, 2019, IEEE T PATTERN ANAL, V41, P3086, DOI 10.1109/TPAMI.2018.2866114; Gong C., 2018, IJCAI, P2156; Gong C, 2022, IEEE T PATTERN ANAL, V44, P2841, DOI 10.1109/TPAMI.2020.3044997; Gong C, 2017, IEEE T NEUR NET LEAR, V28, P1452, DOI 10.1109/TNNLS.2016.2514360; Gong G., 2020, P IEEE C COMP VIS PA, P9819; Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21; Han JW, 2018, PROC CVPR IEEE, P9080, DOI 10.1109/CVPR.2018.00946; Hattori H, 2018, INT J COMPUT VISION, V126, P1027, DOI 10.1007/s11263-018-1077-3; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374; Hou YN, 2019, IEEE I CONF COMP VIS, P1013, DOI 10.1109/ICCV.2019.00110; Jain M, 2020, PROC CVPR IEEE, P1168, DOI 10.1109/CVPR42600.2020.00125; Jiang Y.-G., 2014, ECCV WORKSH; Kingma DP, 2015, INT C LEARN REPR ICL; Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399; Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1; Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400; Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043; Lu CW, 2019, INT J COMPUT VISION, V127, P993, DOI 10.1007/s11263-018-1129-8; Lu Xiankai, 2020, CVPR; Mettes P, 2016, LECT NOTES COMPUT SC, V9909, P437, DOI 10.1007/978-3-319-46454-1_27; Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17; Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877; Pang Y., 2020, P IEEE CVF C COMP VI, P9413; Paszke A, 2019, ADV NEUR IN, V32; Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35; Peisen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P539, DOI 10.1007/978-3-030-58598-3_32; Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706; Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560; Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590; Ramanathan Vignesh, 2020, P IEEE CVF C COMP VI, P9342; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109; Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Simonyan K, 2014, ADV NEUR IN, V27; Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381; Song L., 2020, INT J COMPUT VISION, V129, P1; Sukmin Yun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13873, DOI 10.1109/CVPR42600.2020.01389; Taojiannan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P299, DOI 10.1007/978-3-030-58452-8_18; Toolkit CPTG, 2019, V10 1 DOC; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Welling M., 2016, SEMISUPERVISED CLASS; Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639; Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403; Xu H., 2020, EUR C COMP VIS; Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617; Xu M., 2020, CVPR, P10156; Xu YL, 2019, AAAI CONF ARTIF INTE, P9070; Yan Y, 2020, INT J COMPUT VISION, V128, P1414, DOI 10.1007/s11263-019-01244-7; Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486; Yang L, 2018, IEEE T IMAGE PROCESS, V27, P4025, DOI 10.1109/TIP.2018.2834221; Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562; Yuan L, 2020, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR42600.2020.00396; Yuan Y., 2019, P INT C LEARN REP; Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719; Zha S., 2020, EUR C COMP VIS; Zhang CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P738, DOI 10.1145/3343031.3351044; Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4; Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P475, DOI 10.1109/TPAMI.2018.2881114; Zhang LF, 2019, IEEE I CONF COMP VIS, P3712, DOI 10.1109/ICCV.2019.00381; Zhang XY, 2020, AAAI CONF ARTIF INTE, V34, P12886; Zhang XY, 2019, AAAI CONF ARTIF INTE, P9227; Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144; Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454; Zhong JX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P35, DOI 10.1145/3240508.3240511; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhu LC, 2022, IEEE T PATTERN ANAL, V44, P273, DOI 10.1109/TPAMI.2020.3007511	91	5	5	3	10	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2021	129	8					2474	2498		10.1007/s11263-021-01473-9	http://dx.doi.org/10.1007/s11263-021-01473-9		MAY 2021	25	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TR3NV					2022-12-18	WOS:000656515600001
J	Peng, P; Yang, KF; Luo, FY; Li, YJ				Peng, Peng; Yang, Kai-Fu; Luo, Fu-Ya; Li, Yong-Jie			Saliency Detection Inspired by Topological Perception Theory	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Topological perception theory; Topological complexity; Topological saliency prior; Salient object detection; Fixation prediction	OBJECT DETECTION; VISUAL-ATTENTION; NETWORK; MODEL	The topological perception theory claims that visual perception of a scene begins from topological properties and then exploits local details. Inspired by this theory, we defined the topological descriptor and topological complexity, and we observed, based on statistics, that the saliencies of the regions with higher topological complexities are generally higher than those of regions with lower topological complexities. We then introduced the topological complexity as a saliency prior and proposed a novel unsupervised topo-prior-guided saliency detection system (TOPS). This system is framed as a topological saliency prior (topo-prior)-guided two-level local cue processing (i.e., pixel- and regional-level cues) with a multi-scale strategy, which includes three main modules: (1) a basic computational model of the topological perception theory for extracting topological features from images, (2) a topo-prior calculation method based on the topological features, and (3) a global-local saliency combination framework guided by the topo-prior. Extensive experiments on widely used salient object detection (SOD) datasets demonstrate that our system outperforms the unsupervised state-of-the-art algorithms. In addition, the topo-prior proposed in this work can be used to boost supervised methods including the deep-learning-based ones for fixation prediction and SOD tasks.	[Peng, Peng; Yang, Kai-Fu; Luo, Fu-Ya; Li, Yong-Jie] Univ Elect Sci & Technol China, Sch Life Sci & Tecnol, MOE Key Lab Neuroinformat, Chengdu 610054, Peoples R China	University of Electronic Science & Technology of China	Li, YJ (corresponding author), Univ Elect Sci & Technol China, Sch Life Sci & Tecnol, MOE Key Lab Neuroinformat, Chengdu 610054, Peoples R China.	pengpanda.uestc@gmail.com; yangkf@uestc.edu.cn; luofuya@std.uestc.edu.cn; liyj@uestc.edu.cn		peng, peng/0000-0001-6570-6117	Key Area R&D Program of Guangdong Province [2018B030338001]; Natural Science Foundations of China [62076055, 61806041]; 111 Project [B12027]	Key Area R&D Program of Guangdong Province; Natural Science Foundations of China(National Natural Science Foundation of China (NSFC)); 111 Project(Ministry of Education, China - 111 Project)	The authors would like to thank Professor Lin Chen for his helpful discussions and suggestions on the modeling of his topological perception theory. This work was supported by the Key Area R&D Program of Guangdong Province (#2018B030338001), the Natural Science Foundations of China (#62076055, #61806041). This work was also supported by the 111 Project (B12027) of China. We also thank LetPub for its linguistic assistance during the preparation of this manuscript.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Adelson E.H., 1984, RCA ENG, V29, P33; Arbelaez P., 2006, COMP VIS PATT REC WO, P182, DOI DOI 10.1109/CVPRW.2006.48; Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9; Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715; Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727; Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89; Borji A, 2011, IEEE INT CONF ROBOT, P1902; Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5; Cai W., 2017, IEEE T IMAGE PROCESS, V27, P1; Chen JZ, 2019, IEEE IMAGE PROC, P1630, DOI 10.1109/ICIP.2019.8802611; Chen L, 2005, VIS COGN, V12, P553, DOI 10.1080/13506280444000256; Chen L, 2003, P NATL ACAD SCI USA, V100, P6884, DOI 10.1073/pnas.0732090100; CHEN L, 1982, SCIENCE, V218, P699, DOI 10.1126/science.7134969; Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15; Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3763, DOI 10.1109/TIP.2020.2965989; Chen SH, 2016, PATTERN RECOGN, V60, P2, DOI 10.1016/j.patcog.2016.05.016; Chen XW, 2017, IEEE I CONF COMP VIS, P1050, DOI 10.1109/ICCV.2017.119; Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414; Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401; Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832; Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174; DEYOE EA, 1988, TRENDS NEUROSCI, V11, P219, DOI 10.1016/0166-2236(88)90130-0; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P2625, DOI 10.1109/TIP.2014.2305100; Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616; Garcia-Diaz A, 2012, IMAGE VISION COMPUT, V30, P51, DOI 10.1016/j.imavis.2011.11.007; Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868; Gu XD, 2013, COMPUT VIS IMAGE UND, V117, P1400, DOI 10.1016/j.cviu.2013.05.004; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; He LX, 2015, P NATL ACAD SCI USA, V112, pE5647, DOI 10.1073/pnas.1512408112; He S., 2019, P IEEE C COMP VIS PA, P10206, DOI [10.1109/cvpr.2019.01045, DOI 10.1109/CVPR.2019.01045]; Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648; Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146; Hou XD, 2007, PROC CVPR IEEE, P2280; Huang XM, 2018, PATTERN RECOGN, V76, P95, DOI 10.1016/j.patcog.2017.10.027; Huang XM, 2017, IEEE T IMAGE PROCESS, V26, P4243, DOI 10.1109/TIP.2017.2710636; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500; Ji QG, 2013, SIGNAL PROCESS-IMAGE, V28, P241, DOI 10.1016/j.image.2012.11.008; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kim J, 2016, IEEE T IMAGE PROCESS, V25, P9, DOI 10.1109/TIP.2015.2495122; Klingner Marvin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P582, DOI 10.1007/978-3-030-58565-5_35; KOCH C, 1985, HUM NEUROBIOL, V4, P219; Koffka K., PRINCIPLES GESTALT P; Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620; Kummerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513; Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887; Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147; Li NY, 2015, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2015.7299158; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474; Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326; Liu N, 2015, PROC CVPR IEEE, P362, DOI 10.1109/CVPR.2015.7298633; Liu Q, 2017, IEEE T IMAGE PROCESS, V26, P4537, DOI 10.1109/TIP.2017.2703081; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; LIVINGSTONE MS, 1987, J NEUROSCI, V7, P3416; Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219; Markham A., 2019, ARXIV PREPRINT ARXIV; Marr David., 1982, Q REV BIOL, V8; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626; Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019; Qin Y, 2018, INT J COMPUT VISION, V126, P751, DOI 10.1007/s11263-017-1062-2; Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606; Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981; Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27; Scharfenberger C, 2013, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2013.131; Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703; Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960; Siva P, 2013, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2013.416; Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256; Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358; Wang B, 2007, P NATL ACAD SCI USA, V104, P21014, DOI 10.1073/pnas.0709664104; Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3; Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404; Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50; Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330; Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417; Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154; Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612; Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612; Wolfe JM, 2011, TRENDS COGN SCI, V15, P77, DOI 10.1016/j.tics.2010.12.001; WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774; Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403; Xiao XL, 2019, IEEE T IMAGE PROCESS, V28, P2126, DOI 10.1109/TIP.2018.2882156; Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276; Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Yang KF, 2016, IEEE T IMAGE PROCESS, V25, P3475, DOI 10.1109/TIP.2016.2572600; Yang KF, 2015, COMM COM INF SC, V546, P94, DOI 10.1007/978-3-662-48558-3_10; Yongzhen Huang, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P180; Zeng Y, 2019, PROC CVPR IEEE, P6067, DOI 10.1109/CVPR.2019.00623; Zhang JM, 2016, IEEE T PATTERN ANAL, V38, P889, DOI 10.1109/TPAMI.2015.2473844; Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26; Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32; Zhang L, 2019, PROC CVPR IEEE, P6017, DOI 10.1109/CVPR.2019.00618; Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081; Zhao Q, 2013, SIGNAL PROCESS, V93, P1401, DOI 10.1016/j.sigpro.2012.06.014; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731; Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320; Zhou LC, 2020, NEURAL NETWORKS, V121, P308, DOI 10.1016/j.neunet.2019.09.009; Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360; Zhuo Y, 2003, SCIENCE, V299, P417, DOI 10.1126/science.1077091; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	115	5	5	1	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2021	129	8					2352	2374		10.1007/s11263-021-01478-4	http://dx.doi.org/10.1007/s11263-021-01478-4		MAY 2021	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TR3NV					2022-12-18	WOS:000653602100001
J	Zhao, SC; Li, B; Xu, PF; Yue, XY; Ding, GG; Keutzer, K				Zhao, Sicheng; Li, Bo; Xu, Pengfei; Yue, Xiangyu; Ding, Guiguang; Keutzer, Kurt			MADAN: Multi-source Adversarial Domain Aggregation Network for Domain Adaptation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Domain adaptation (DA); Multi-source DA; Simulation-to-real; Domain aggregation; Generative adversarial network		Domain adaptation aims to learn a transferable model to bridge the domain shift between one labeled source domain and another sparsely labeled or unlabeled target domain. Since the labeled data may be collected from multiple sources, multi-source domain adaptation (MDA) has attracted increasing attention. Recent MDA methods do not consider the pixel-level alignment between sources and target or the misalignment across different sources. In this paper, we propose a novel MDA framework to address these challenges. Specifically, we design a novel Multi-source Adversarial Domain Aggregation Network (MADAN). First, an adapted domain is generated for each source with dynamic semantic consistency while aligning towards the target at the pixel-level cycle-consistently. Second, sub-domain aggregation discriminator and cross-domain cycle discriminator are proposed to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and the target domain while training the task network. For the segmentation adaptation, we further enforce category-level alignment and incorporate multi-scale image generation, which constitutes MADAN+. We conduct extensive MDA experiments on digit recognition, object classification, and simulation-to-real semantic segmentation tasks. The results demonstrate that the proposed MADAN and MADAN+ models outperform state-of-the-art approaches by a large margin.	[Zhao, Sicheng; Ding, Guiguang] Tsinghua Univ, BNRist, Beijing, Peoples R China; [Li, Bo; Yue, Xiangyu; Keutzer, Kurt] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Xu, Pengfei] Didi Chuxing, Beijing, Peoples R China	Tsinghua University; University of California System; University of California Berkeley	Ding, GG (corresponding author), Tsinghua Univ, BNRist, Beijing, Peoples R China.; Xu, PF (corresponding author), Didi Chuxing, Beijing, Peoples R China.	schzhao@gmail.com; drluodian@gmail.com; xupengfeipf@didiglobal.com; xyyue@berkeley.edu; dinggg@tsinghua.edu.cn; keutzer@berkeley.edu			National Natural Science Foundation of China [61701273, U1936202]; Berkeley DeepDrive	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Berkeley DeepDrive	This work is supported by the National Natural Science Foundation of China (Nos. 61701273, U1936202) and Berkeley DeepDrive.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2007, P 15 ACM INT C MULTI; [Anonymous], 2009, ADV NEURAL INFORM PR; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Becker Carlos J, 2013, ADV NEURAL INFORM PR, V1, P485; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233; Chattopadhyay R, 2012, ACM T KNOWL DISCOV D, V6, DOI 10.1145/2382577.2382582; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen X, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P23, DOI 10.1145/3366423.3380091; Chen YH, 2017, IEEE I CONF COMP VIS, P2011, DOI 10.1109/ICCV.2017.220; Chen YH, 2018, PROC CVPR IEEE, P7892, DOI 10.1109/CVPR.2018.00823; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding ZM, 2018, IEEE T NEUR NET LEAR, V29, P310, DOI 10.1109/TNNLS.2016.2618765; Duan L., 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411; Duan LX, 2012, IEEE T NEUR NET LEAR, V23, P504, DOI 10.1109/TNNLS.2011.2178556; Duan LX, 2012, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2012.6247819; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Yaroslav, 2015, ICML; Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2014, IEEE T PATTERN ANAL, V36, P2288, DOI 10.1109/TPAMI.2013.249; Griffin Gregory, 2007, CALTECH 256 OBJECT C; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hoffman J, 2018, ADV NEUR IN, V31; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615; Hu LQ, 2018, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2018.00162; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang HS, 2018, LECT NOTES COMPUT SC, V11220, P611, DOI 10.1007/978-3-030-01270-0_36; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924; Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503; Keutzer, 2021, WEB C; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2017, PR MACH LEARN RES, V70; Long MS, 2016, ADV NEUR IN, V29; Louizos C., 2015, ARXIV PREPRINT ARXIV; Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149; Redko Ievgen, 2019, AISTATS, P849; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Riemer Matthew, 2019, INT C LEARN REPR, V1; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Sha, 2013, P INT C MACH LEARN; Shen J, 2018, AAAI CONF ARTIF INTE, P4058; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Sun BC, 2016, AAAI CONF ARTIF INTE, P2058; Sun Qian, 2011, ADV NEURAL INFORM PR, P505; Sun SL, 2013, INT CONF MACH LEARN, P24, DOI 10.1109/ICMLC.2013.6890438; Sun SL, 2015, INFORM FUSION, V24, P84, DOI 10.1016/j.inffus.2014.12.003; Sun Y., 2019, UNSUPERVISED DOMAIN, P2; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Van der Maaten L., 2008, J MACH LEARN RES, V9, P2579; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Vu TT, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6163; Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI 10.1109/ICRA.2019.8793495; Wu ZX, 2018, LECT NOTES COMPUT SC, V11209, P535, DOI 10.1007/978-3-030-01228-1_32; Xu JL, 2019, IEEE ACCESS, V7, P156694, DOI 10.1109/ACCESS.2019.2949697; Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417; Xu ZJ, 2012, LECT NOTES COMPUT SC, V7665, P332, DOI 10.1007/978-3-642-34487-9_41; Yu Fisher, 2018, ARXIV180504687; Yue XY, 2019, IEEE I CONF COMP VIS, P2100, DOI 10.1109/ICCV.2019.00219; Yue XY, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P458, DOI 10.1145/3206025.3206080; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zhao H, 2018, ADV NEUR IN, V31; Zhao SC, 2020, AAAI CONF ARTIF INTE, V34, P12975; Zhao SC, 2022, IEEE T NEUR NET LEAR, V33, P473, DOI 10.1109/TNNLS.2020.3028503; Zhao SC, 2019, ADV NEUR IN, V32; Zhao SC, 2019, AAAI CONF ARTIF INTE, P2620; Zhao Sicheng, 2020, ARXIV200212169; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhuo JB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P261, DOI 10.1145/3123266.3123292	99	5	5	11	28	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	AUG	2021	129	8					2399	2424		10.1007/s11263-021-01479-3	http://dx.doi.org/10.1007/s11263-021-01479-3		MAY 2021	26	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	TR3NV		Green Submitted			2022-12-18	WOS:000653602100002
J	Saunders, B; Camgoz, NC; Bowden, R				Saunders, Ben; Camgoz, Necati Cihan; Bowden, Richard			Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Sign language production; 3D Multi-channel sign language; Continuous sequence generation	RECOGNITION	Sign languages are multi-channel visual languages, where signers use a continuous 3D space to communicate. Sign language production (SLP), the automatic translation from spoken to sign languages, must embody both the continuous articulation and full morphology of sign to be truly understandable by the Deaf community. Previous deep learning-based SLP works have produced only a concatenation of isolated signs focusing primarily on the manual features, leading to a robotic and non-expressive production. In this work, we propose a novel Progressive Transformer architecture, the first SLP model to translate from spoken language sentences to continuous 3D multi-channel sign pose sequences in an end-to-end manner. Our transformer network architecture introduces a counter decoding that enables variable length continuous sequence generation by tracking the production progress over time and predicting the end of sequence. We present extensive data augmentation techniques to reduce prediction drift, alongside an adversarial training regime and a mixture density network (MDN) formulation to produce realistic and expressive sign pose sequences. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging PHOENIX14T dataset and setting baselines for future research. We further provide a user evaluation of our SLP model, to understand the Deaf reception of our sign pose productions.	[Saunders, Ben; Camgoz, Necati Cihan; Bowden, Richard] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford, Surrey, England	University of Surrey	Saunders, B (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford, Surrey, England.	b.saunders@surrey.ac.uk	Bowden, Richard/AAF-8283-2019	Bowden, Richard/0000-0003-3285-8020; Saunders, Ben/0000-0001-9413-6280	SNSF Sinergia project 'SMILE' [CRSII2 160811]; European Union [762021]; EPSRC [EP/R03298X/1]	SNSF Sinergia project 'SMILE'; European Union(European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work received funding from the SNSF Sinergia project 'SMILE' (CRSII2 160811), the European Union's Horizon2020 research and innovation programme under grant agreement no. 762021 'Content4All' and the EPSRC project 'ExTOL' (EP/R03298X/1). This work reflects only the authors view and the Commission is not responsible for any use that may be made of the information it contains.	Adaloglou N., 2019, IEEE T MULTIMEDIA; [Anonymous], 2016, AUTOMATED TECHNIQUE; [Anonymous], 2014, P ADV NEUR INF PROC; Bauer B, 2000, INT C PATT RECOG, P463, DOI 10.1109/ICPR.2000.906112; Berndt DJ, 1994, KDD WORKSH, V10, P359; Bishop C.M., 1994, MIXTURE DENSITY NETW; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bowden R., 2020, P BRIT MACH VIS C; Bragg D, 2019, ASSETS'19: THE 21ST INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P16, DOI 10.1145/3308561.3353774; British Deaf Association (BDA), 2020, UK DEAF COMMUNITY; Camgoz Necati Cihan, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P301, DOI 10.1007/978-3-030-66823-5_18; Camgoz N. C., 2020, P IEEE C COMPUTER VI; Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812; Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI [10.1109/ICCV.2017.332, 10.1109/ICCVW.2017.364]; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Cooper H, 2012, J MACH LEARN RES, V13, P2205; Cox S., 2002, ASSETS 2002. Proceedings of the Fifth International ACM SIGCAPH Conference on Assistive Technologies, P205, DOI 10.1145/638249.638287; Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175; Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Ebling S., 2015, P SLPAT 2015 6 WORKS; Elliott R., 2008, Universal Access in the Information Society, V6, P375, DOI 10.1007/s10209-007-0102-z; Forster J, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P1911; Ginosar S, 2019, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2019.00361; Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033; Glorot X., 2010, PROC MACH LEARN RES, P249; Graves A, 2013, ARXIV13080850; Grobel K, 1997, IEEE SYS MAN CYBERN, P162, DOI 10.1109/ICSMC.1997.625742; Gungor, 2019, IEEE INT S INN INT S; Ha David, 2018, ICLR; Ha David, 2018, NEURIPS, DOI [10.5281/zenodo.1207631, DOI 10.5281/ZENODO.1207631]; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton, 2016, ARXIV PREPRINT ARXIV; Holt JA., 1993, AM ANN DEAF, V138, P172, DOI DOI 10.1353/AAD.2012.0684; Hu Y, 2018, IEEE ICC; Huang Cheng-Zhi Anna, 2018, INT C LEARN REPR; Huang Jie, 2018, AAAI, DOI [10.1609/aaai.v32i1.11903, DOI 10.48550/ARXIV.1801.10111]; Huenerfauth, 2010, P NAACL HLT 2010; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kingma D.P, P 3 INT C LEARNING R; Kipp Michael, 2011, Intelligent Virtual Agents. Proceedings 11th International Conference, IVA 2011, P113, DOI 10.1007/978-3-642-23974-8_13; Kipp M., P 13 INT ACM SIGACCE; Ko SK, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9132683; Koller O, 2016, P BRIT MACH VIS C 20; Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077; Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364; Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013; Kreutzer J, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P109; Lale, 2016, P SIGN PROC COMM APP; Lee HY, 2019, ADV NEUR IN, V32; Li C, 2019, PROC CVPR IEEE, P7864, DOI 10.1109/CVPR.2019.00806; Liu T. Y., 2017, P AS C MACH LEARN; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Makansi O, 2019, PROC CVPR IEEE, P7137, DOI 10.1109/CVPR.2019.00731; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mirza M., 2014, ARXIV; Mukherjee S, 2019, INT CONF ACOUST SPEE, P2027, DOI 10.1109/ICASSP.2019.8682158; Orbay A, 2020, IEEE INT CONF AUTOMA, P222, DOI 10.1109/FG47880.2020.00002; Pfau, 2010, NONMANUALS THEIR GRA; Povey D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5874; Press Ofir, 2017, ARXIV170601399; PROKUDIN S, 2018, P EUR C COMP ECCV; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ren XG, 2019, PROC VLDB ENDOW, V12, P1344, DOI 10.14778/3342263.3342272; Salimans T, 2016, ADV NEUR IN, V29; Schuster M, 2000, ADV NEUR IN, V12, P589; Sheard, 2006, TECHNOL DISABIL; Stafylopatis A., 2018, IEEE 31 INT C TOOLS; Starner T., 1997, MOTION BASED RECOGNI, P227; STOKOE WC, 1980, ANNU REV ANTHROPOL, V9, P365, DOI 10.1146/annurev.an.09.100180.002053; Stoll S, 2020, INT J COMPUT VISION, V128, P891, DOI 10.1007/s11263-019-01281-2; Sun M. T., 2017, ADV NEURAL INFORM PR; Sutskever I, 2014, ADV NEUR IN, V27; Sutton-Spence Rachel, 1999, LINGUISTICS BRIT SIG; TAMURA S, 1988, PATTERN RECOGN, V21, P343, DOI 10.1016/0031-3203(88)90048-9; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Valli C., 2000, LINGUISTICS AM SIGN; Varamesh Ali, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13083, DOI 10.1109/CVPR42600.2020.01310; Vaswani A, 2017, ADV NEUR IN, V30; Vogler C., 1999, P IEEE INT C COMP VI; Wang X, 2017, INT CONF ACOUST SPEE, P4895, DOI 10.1109/ICASSP.2017.7953087; Wu D, 2020, INT J ACCOUNT INF MA, V28, P184, DOI [10.1016/j.ijid.2020.03.004, 10.1108/IJAIM-12-2018-0148]; Wu LS, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278049; Xiao QK, 2020, NEURAL NETWORKS, V125, P41, DOI 10.1016/j.neunet.2020.01.030; Yang Z., 2017, ARXIV; Ye Q, 2018, LECT NOTES COMPUT SC, V11214, P817, DOI 10.1007/978-3-030-01249-6_49; Yin K., 2020, ECCV SIGN LANG REC T; Zelinka J, 2020, IEEE WINT CONF APPL, P3384, DOI 10.1109/WACV45572.2020.9093516; Zhang XY, 2018, IEEE T PATTERN ANAL, V40, P849, DOI 10.1109/TPAMI.2017.2695539; Zhang Y, 2016, PROC NIPS WORKSHOP A, V21, P21; Zhang ZY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1441; Zhou, 2013, IEEE INT C AUT FAC G; Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zwitserlood I., 2004, P C WORKSH ASS TECHN	100	5	5	2	11	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2021	129	7					2113	2135		10.1007/s11263-021-01457-9	http://dx.doi.org/10.1007/s11263-021-01457-9		MAY 2021	23	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SQ8CW		hybrid, Green Submitted			2022-12-18	WOS:000648223000002
J	Ma, C; Yang, F; Li, Y; Jia, HZ; Xie, XD; Gao, W				Ma, Cong; Yang, Fan; Li, Yuan; Jia, Huizhu; Xie, Xiaodong; Gao, Wen			Deep Human-Interaction and Association by Graph-Based Learning for Multiple Object Tracking in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Multiple Object Tracking in the Wild; Human Interaction Model; Deep Association Network; Graph Neural Network		Multiple Object Tracking (MOT) in the wild has a wide range of applications in surveillance retrieval and autonomous driving. Tracking-by-Detection has become a mainstream solution in MOT, which is composed of feature extraction and data association. Most of the existing methods focus on extracting targets' individual features and optimizing the association by hand-crafted algorithms. In this paper, we specially consider the interrelation cue between targets and we propose Human-Interaction Model (HIM) to extract interaction features between the tracked target and its surrounding. The interaction model has more discriminative features to distinguish objects, especially in crowded (dense) scene. Meanwhile we propose an efficient end-to-end model, Deep Association Network (DAN), to optimize the association with graph-based learning mechanism. Both HIM and DAN are constructed by three kinds of deep networks, which include Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Graph Neural Network (GNN). The CNNs extract appearance features from bounding box images, the RNNs encoder motion features from historical positions of trajectory. And then the GNNs aim to extract interaction features and optimize graph structure to associate the objects in different frames. In addition, we present a novel end-to-end training strategy for Deep Association Network and Human-Interaction Model. Our experimental results demonstrate performance of our method reaches the state-of-the-art on MOT15, MOT16 and DukeMTMCT datasets.	[Ma, Cong; Yang, Fan; Li, Yuan; Jia, Huizhu; Xie, Xiaodong; Gao, Wen] Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China	Peking University	Li, Y (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China.	Cong-Reeshard.Ma@pku.edu.cn; fyang.eecs@pku.edu.cn; yuanli@pku.edu.cn; hzjia@pku.edu.cn; donxie@pku.edu.cn; wgao@pku.edu.cn	Yang, Fan/AAU-1689-2021					Acuna D, 2018, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2018.00096; Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Battaglia Peter W, 2018, ARXIV180601261; Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Cai, 2020, ARXIV PREPRINT ARXIV; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Chen JH, 2017, IEEE COMPUT SOC CONF, P2143, DOI 10.1109/CVPRW.2017.266; Chen L, 2018, IEEE INT CON MULTI; Chen L, 2017, IEEE IMAGE PROC, P645; Chen X, 2018, TRANSPORT REV, V38, P625, DOI 10.1080/01441647.2017.1396265; Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347; Chu P, 2019, IEEE WINT CONF APPL, P161, DOI 10.1109/WACV.2019.00023; CHU Q, 2017, P IEEE C COMP VIS PA, P4836; Dendorfer P., 2019, ARXIV190604567CS; Dicle C, 2013, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2013.286; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Feng X, 2017, IEEE INFOCOM SER, DOI 10.1109/JSTARS.2017.2686488; Gao X, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P201, DOI 10.1145/3240508.3240548; Garcia V., 2018, ICLR; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Henschel Roberto, 2018, IEEE C COMP VIS PATT; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Keuper M, 2020, IEEE T PATTERN ANAL, V42, P140, DOI 10.1109/TPAMI.2018.2876253; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kipf T, 2018, PR MACH LEARN RES, V80; Lan L, 2018, IEEE T IMAGE PROCESS, V27, P4585, DOI 10.1109/TIP.2018.2843129; Leal-Taix L, 2015, ARXIV PREPRINT ARXIV; Levinkov E, 2017, PROC CVPR IEEE, P1904, DOI 10.1109/CVPR.2017.206; Ma C, 2018, IEEE INT CON MULTI; Ma C, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P253, DOI 10.1145/3323873.3325010; Martin-Martin R, 2019, ARXIV PREPRINT ARXIV; Milan, ARXIV PREPRINT ARXIV; Milan A., 2016, MOT16 BENCHMARK MULT; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Rosenhahn, 2017, CORR; Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144; Sahbani B., 2017, ICSET, P109; Schulter S, 2017, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2017.292; Shen, 2018, ARXIV PREPRINT ARXIV; Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30; Sheng H, 2019, IEEE T CIRC SYST VID, V29, P3660, DOI 10.1109/TCSVT.2018.2881123; Sui YL, 2013, INT SYM CODE GENER, P1; Tahayori H, 2017, IEEE INT CONF FUZZY; Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394; Tesfaye Y. T., 2017, ARXIV170606196; Wang GA, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P482, DOI 10.1145/3343031.3350853; Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang Dongfang, 2020, ARXIV200703578, P24; Yang F, 2019, PATTERN RECOGN, V86, P143, DOI 10.1016/j.patcog.2018.08.015; Yang F, 2021, IEEE J EM SEL TOP P, V9, P4026, DOI 10.1109/JESTPE.2020.2970335; Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155; Yu, 2020, IEEE T MULTIMEDIA; Zhang MY, 2022, IEEE T BIG DATA, V8, P809, DOI 10.1109/TBDATA.2020.2991008; Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463; Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhuang, 2019, IEEE SIGNAL PROC LET	72	5	5	4	17	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1993	2010		10.1007/s11263-021-01460-0	http://dx.doi.org/10.1007/s11263-021-01460-0		APR 2021	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU					2022-12-18	WOS:000641238700006
J	Zhong, XB; Ding, CX; Qu, X; Tao, DC				Zhong, Xubin; Ding, Changxing; Qu, Xian; Tao, Dacheng			Polysemy Deciphering Network for Robust Human-Object Interaction Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Human-object interaction; Verb polysemy; Language priors; Attention model		Human-Object Interaction (HOI) detection is important to human-centric scene understanding tasks. Existing works tend to assume that the same verb has similar visual characteristics in different HOI categories, an approach that ignores the diverse semantic meanings of the verb. To address this issue, in this paper, we propose a novel Polysemy Deciphering Network (PD-Net) that decodes the visual polysemy of verbs for HOI detection in three distinct ways. First, we refine features for HOI detection to be polysemy-aware through the use of two novel modules: namely, Language Prior-guided Channel Attention (LPCA) and Language Prior-based Feature Augmentation (LPFA). LPCA highlights important elements in human and object appearance features for each HOI category to be identified; moreover, LPFA augments human pose and spatial features for HOI detection using language priors, enabling the verb classifiers to receive language hints that reduce intra-class variation for the same verb. Second, we introduce a novel Polysemy-Aware Modal Fusion module, which guides PD-Net to make decisions based on feature types deemed more important according to the language priors. Third, we propose to relieve the verb polysemy problem through sharing verb classifiers for semantically similar HOI categories. Furthermore, to expedite research on the verb polysemy problem, we build a new benchmark dataset named HOI-VerbPolysemy (HOI-VP), which includes common verbs (predicates) that have diverse semantic meanings in the real world. Finally, through deciphering the visual polysemy of verbs, our approach is demonstrated to outperform state-of-the-art methods by significant margins on the HICO-DET, V-COCO, and HOI-VP databases. Code and data in this paper are available at .	[Zhong, Xubin; Ding, Changxing; Qu, Xian] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Peoples R China; [Ding, Changxing] Pazhou Lab, Guangzhou 510330, Peoples R China; [Tao, Dacheng] JD Com, JD Explore Acad, Beijing, Peoples R China	South China University of Technology; Pazhou Lab	Ding, CX (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Peoples R China.; Ding, CX (corresponding author), Pazhou Lab, Guangzhou 510330, Peoples R China.	eexubin@mail.scut.edu.cn; chxding@scut.edu.cn; eequxian.scut@mail.scut.edu.cn; dacheng.tao@jd.com			National Natural Science Foundation of China [62076101, 61702193, U1801262]; Program for Guangdong Introducing Innovative and Entrepreneurial Teams [2017ZT07X183]; Natural Science Fund of Guangdong Province [2018A030313869]; Science and Technology Program of Guangzhou [201804010272]; Guangzhou Key Laboratory of Body Data Science [201605030011]; Fundamental Research Funds for the Central Universities of China [2019JQ01]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Guangdong Introducing Innovative and Entrepreneurial Teams; Natural Science Fund of Guangdong Province(National Natural Science Foundation of Guangdong Province); Science and Technology Program of Guangzhou; Guangzhou Key Laboratory of Body Data Science; Fundamental Research Funds for the Central Universities of China(Fundamental Research Funds for the Central Universities)	This work was supported by the National Natural Science Foundation of China under Grant 62076101, 61702193, and U1801262, the Program for Guangdong Introducing Innovative and Entrepreneurial Teams under Grant 2017ZT07X183, the Natural Science Fund of Guangdong Province under Grant 2018A030313869, the Science and Technology Program of Guangzhou under Grant 201804010272, the Guangzhou Key Laboratory of Body Data Science under Grant 201605030011, and the Fundamental Research Funds for the Central Universities of China under Grant 2019JQ01.	[Anonymous], ARXIV190911059; [Anonymous], ARXIV190407850; [Anonymous], ARXIV170509892; [Anonymous], ARXIV190903918; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Bansal A, 2020, AAAI CONF ARTIF INTE, V34, P10460; Bingjie Xu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P2019, DOI 10.1109/CVPR.2019.00212; Chao YW, 2018, IEEE WINT CONF APPL, P381, DOI 10.1109/WACV.2018.00048; Chao YW, 2015, IEEE I CONF COMP VIS, P1017, DOI 10.1109/ICCV.2015.122; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Chen Xinlei, 2017, ARXIV170202138; Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44; Ding C., 2020, IEEE T PATTERN ANAL; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Gao Chen, 2018, BRIT MACH VIS C; Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680; Girdhar R, 2017, ADV NEUR IN, V30; Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872; Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207; Gupta S., 2015, ARXIV PREPRINT ARXIV; Gupta T, 2019, IEEE I CONF COMP VIS, P9676, DOI 10.1109/ICCV.2019.00977; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang Eric, 2012, P 50 ANN M ASS COMP, V1, P873; Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Ji J., 2020, CVPR, P10236, DOI 10.1109/cvpr42600.2020.01025; Kato K, 2018, LECT NOTES COMPUT SC, V11218, P247, DOI 10.1007/978-3-030-01264-9_15; Kingma D.P, P 3 INT C LEARNING R; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z; Li BL, 2019, IEEE IMAGE PROC, P3601, DOI 10.1109/ICIP.2019.8803448; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Li YL, 2020, PROC CVPR IEEE, P379, DOI 10.1109/CVPR42600.2020.00046; Li YL, 2019, PROC CVPR IEEE, P3580, DOI 10.1109/CVPR.2019.00370; Liao Y, 2020, PROC CVPR IEEE, P479, DOI 10.1109/CVPR42600.2020.00056; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Lin X, 2020, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR42600.2020.00380; Liu NH, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P932, DOI 10.1145/3292500.3330967; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lu JS, 2019, ADV NEUR IN, V32; Ma R, 2020, INT CONF ACOUST SPEE, P8129, DOI 10.1109/ICASSP40776.2020.9053503; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331; Meng LL, 2019, IEEE INT CONF COMP V, P1513, DOI 10.1109/ICCVW.2019.00189; Mikolov T., 2013, ARXIV; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Oomoto K, 2017, INT CONF KNOWL SMART, P28, DOI 10.1109/KST.2017.7886073; Pereira S, 2019, IEEE T MED IMAGING, V38, P2914, DOI 10.1109/TMI.2019.2918096; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Peyre J, 2019, IEEE I CONF COMP VIS, P1981, DOI 10.1109/ICCV.2019.00207; Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shen LY, 2018, IEEE WINT CONF APPL, P1568, DOI 10.1109/WACV.2018.00181; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Ulutan Oytun, 2020, CVPR; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wan B, 2019, IEEE I CONF COMP VIS, P9468, DOI 10.1109/ICCV.2019.00956; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang N, 2021, IEEE T IMAGE PROCESS, V30, P1784, DOI 10.1109/TIP.2020.3048629; Wang TC, 2020, PROC CVPR IEEE, P4115, DOI 10.1109/CVPR42600.2020.00417; Wang TC, 2019, IEEE I CONF COMP VIS, P5693, DOI 10.1109/ICCV.2019.00579; Wang WB, 2019, PROC CVPR IEEE, P8180, DOI 10.1109/CVPR.2019.00838; Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Ye Q, 2016, LECT NOTES COMPUT SC, V9912, P346, DOI 10.1007/978-3-319-46484-8_21; Yong-Lu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10163, DOI 10.1109/CVPR42600.2020.01018; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yu XJ, 2011, NEW ENGL J MED, V364, P1523, DOI 10.1056/NEJMoa1010095; Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611; Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331; Zhao Y, 2020, INT J COMPUT VISION, V128, P74, DOI 10.1007/s11263-019-01211-2; Zheng B, 2015, INT J COMPUT VISION, V112, P221, DOI 10.1007/s11263-014-0795-4; Zhong WB, 2020, INT J AUTOM COMPUT, V17, P1, DOI 10.1007/s11633-019-1190-y; Zhou PH, 2019, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2019.00093; Zhou TF, 2020, PROC CVPR IEEE, P4262, DOI 10.1109/CVPR42600.2020.00432; Zhu YS, 2019, IEEE T IMAGE PROCESS, V28, P113, DOI 10.1109/TIP.2018.2865280; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	82	5	5	4	12	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUN	2021	129	6					1910	1929		10.1007/s11263-021-01458-8	http://dx.doi.org/10.1007/s11263-021-01458-8		APR 2021	20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	SH0NU		Green Submitted			2022-12-18	WOS:000641238700004
J	Pascual-Escudero, B; Nayak, A; Briot, S; Kermorgant, O; Martinet, P; El Din, MS; Chaumette, F				Pascual-Escudero, Beatriz; Nayak, Abhilash; Briot, Sebastien; Kermorgant, Olivier; Martinet, Philippe; El Din, Mohab Safey; Chaumette, Francois			Complete Singularity Analysis for the Perspective-Four-Point Problem	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Pose estimation; Visual servoing; Singularity; PnP		This paper is concerned with pose estimation and visual servoing from four points. We determine the configurations for which the corresponding Jacobian matrix becomes singular, leading to inaccurate and unstable results. Using an adequate representation and algebraic geometry, it is shown that, for any orientation between the camera and the object, there are always two to six singular locations of the camera in the generic case where the points are not coplanar, corresponding to the intersection of four cylinders. The particular case where the four points are coplanar is also characterized. Furthermore, some realistic example configurations are considered to substantiate the theory and to demonstrate failure cases in pose estimation and image-based visual servoing when the camera approaches a singularity.	[Pascual-Escudero, Beatriz; Kermorgant, Olivier] Ecole Cent Nantes, Lab Sci Numer Nantes LS2N, UMR CNRS 6004, Nantes, France; [Nayak, Abhilash; Briot, Sebastien] Ctr Natl Rech Sci, Lab Sci Numer Nantes LS2N, UMR CNRS 6004, Nantes, France; [Martinet, Philippe] Inria Sophia Antipolis, Sophia Antipolis, France; [El Din, Mohab Safey] Sorbonne Univ, CNRS, LIP6, Equipe PolSys, Paris, France; [Chaumette, Francois] Univ Rennes, Inria, CNRS, IRISA, Rennes, France	Nantes Universite; Ecole Centrale de Nantes; Centre National de la Recherche Scientifique (CNRS); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Rennes	Nayak, A (corresponding author), Ctr Natl Rech Sci, Lab Sci Numer Nantes LS2N, UMR CNRS 6004, Nantes, France.	beatriz.pascual.escudero@gmail.com; Abhilash.Nayak@ls2n.fr; Sebastien.Briot@ls2n.fr; Olivier.Kermorgant@ls2n.fr; Philippe.Martinet@inria.fr; Mohab.Safey@lip6.fr; Francois.Chaumette@inria.fr	Francois, Chaumette/AAH-1481-2021; Martinet, Philippe/E-1714-2015	Francois, Chaumette/0000-0002-1238-4385; Pascual-Escudero, Beatriz/0000-0001-5957-0882; Martinet, Philippe/0000-0001-5827-0431	French ANR project SESAME [RFI AtlanSTIC2020];  [ANR-18-CE33-0011]	French ANR project SESAME(French National Research Agency (ANR)); 	This work was supported by the project PROMPT funded by the RFI AtlanSTIC2020, and by the French ANR project SESAME (funding ID: ANR-18-CE33-0011).	Ben-Horin P, 2006, MECH MACH THEORY, V41, P958, DOI 10.1016/j.mechmachtheory.2006.03.008; Bezout E., 1779, THEORIE GENERALE EQU; Briot S., 2013, P 2013 IEEE INT C RO; Briot S, 2017, IEEE T ROBOT, V33, P536, DOI 10.1109/TRO.2016.2637912; Briot S, 2017, IEEE ROBOT AUTOM LET, V2, P412, DOI 10.1109/LRA.2016.2633975; Briot S, 2016, MECH MACH THEORY, V106, P115, DOI 10.1016/j.mechmachtheory.2016.08.013; Briot S, 2015, IEEE T ROBOT, V31, P1337, DOI 10.1109/TRO.2015.2489499; Burschka D, 2003, IEEE INT CONF ROBOT, P3917; Chaumette F., 2008, HDB ROBOTICS; Corke PI, 2010, IEEE INT CONF ROBOT, P5550, DOI 10.1109/ROBOT.2010.5509199; Cox D., 2007, IDEALS VARIETIES ALG; DEMENTHON DF, 1992, LECT NOTES COMPUT SC, V588, P335, DOI 10.1007/BF01450852; Fomena RT, 2011, IEEE T ROBOT, V27, P256, DOI 10.1109/TRO.2011.2104431; Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599; Hamel T, 2002, IEEE T ROBOTIC AUTOM, V18, P187, DOI 10.1109/TRA.2002.999647; Hamel T, 2018, IEEE T AUTOMAT CONTR, V63, P726, DOI 10.1109/TAC.2017.2726179; HORAUD R, 1989, COMPUT VISION GRAPH, V47, P33, DOI 10.1016/0734-189X(89)90052-2; Hutchinson S, 1996, IEEE T ROBOTIC AUTOM, V12, P651, DOI 10.1109/70.538972; Iwatsuki M, 2005, IEEE T ROBOT, V21, P266, DOI 10.1109/TRO.2004.837242; Kalkbrener M, 1997, J SYMB COMPUT, V24, P51, DOI 10.1006/jsco.1997.0113; Kneip L, 2014, IEEE INT CONF ROBOT, P1, DOI 10.1109/ICRA.2014.6906582; Kneip L, 2014, LECT NOTES COMPUT SC, V8689, P127, DOI 10.1007/978-3-319-10590-1_9; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Liu M, 2010, IEEE INT CONF ROBOT, P4062, DOI 10.1109/ROBOT.2010.5509441; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043; Lu CP, 2000, IEEE T PATTERN ANAL, V22, P610, DOI 10.1109/34.862199; Marchand E, 2005, IEEE ROBOT AUTOM MAG, V12, P40, DOI 10.1109/MRA.2005.1577023; Marchand E, 2016, IEEE T VIS COMPUT GR, V22, P2633, DOI 10.1109/TVCG.2015.2513408; Marchand T, 2002, COMPUT GRAPH FORUM, V21, P289, DOI 10.1111/1467-8659.t01-1-00588; Merlet J. P., 2006, PARALLEL ROBOTS, V2nd; PAPANIKOLOPOULOS NP, 1995, J INTELL ROBOT SYST, V13, P279, DOI 10.1007/BF01424011; Persson M., 2018, P EUR C COMP VIS ECC, P318; Rives, 1993, TECH REP; Rosenzveig V., 2014, P 2014 IEEE INT C RO; Tahri O, 2005, IEEE T ROBOT, V21, P1116, DOI 10.1109/TRO.2005.853500; Thompson EH., 1966, PHOTOGRAMM REC, V5, P201, DOI [10.1111/j.1477-9730.1966.tb00870.x, DOI 10.1111/J.1477-9730.1966.TB00870.X]	37	5	5	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					1217	1237		10.1007/s11263-020-01420-0	http://dx.doi.org/10.1007/s11263-020-01420-0		JAN 2021	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK		Green Submitted			2022-12-18	WOS:000608386600001
J	Jensen, SHN; Doest, MEB; Aanaes, H; Del Bue, A				Jensen, Sebastian Hoppe Nesgaard; Doest, Mads Emil Brix; Aanaes, Henrik; Del Bue, Alessio			A Benchmark and Evaluation of Non-Rigid Structure from Motion	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Non-rigid structure from motion; Dataset; Evaluation; Deformation modelling	3D SHAPE; TRACKING; RECONSTRUCTION; FACTORIZATION	Non-rigid structure from motion (nrsfm), is a long standing and central problem in computer vision and its solution is necessary for obtaining 3D information from multiple images when the scene is dynamic. A main issue regarding the further development of this important computer vision topic, is the lack of high quality data sets. We here address this issue by presenting a data set created for this purpose, which is made publicly available, and considerably larger than the previous state of the art. To validate the applicability of this data set, and provide an investigation into the state of the art of nrsfm, including potential directions forward, we here present a benchmark and a scrupulous evaluation using this data set. This benchmark evaluates 18 different methods with available code that reasonably spans the state of the art in sparse nrsfm. This new public data set and evaluation protocol will provide benchmark tools for further development in this challenging field.	[Jensen, Sebastian Hoppe Nesgaard; Doest, Mads Emil Brix; Aanaes, Henrik] DTU Compute, Lyngby, Denmark; [Del Bue, Alessio] Ist Italiano Tecnol IIT, Visual Geometry & Modelling VGM Lab, Pattern Anal & Comp Vis PAVIS, I-08028 Genoa, Italy	Technical University of Denmark; Istituto Italiano di Tecnologia - IIT	Del Bue, A (corresponding author), Ist Italiano Tecnol IIT, Visual Geometry & Modelling VGM Lab, Pattern Anal & Comp Vis PAVIS, I-08028 Genoa, Italy.	snje@dtu.dk; mebd@dtu.dk; aanes@dtu.dk; alessio.delbue@iit.it		Del Bue, Alessio/0000-0002-2262-4872	Istituto Italiano di Tecnologia within the CRUI-CARE Agreement	Istituto Italiano di Tecnologia within the CRUI-CARE Agreement	Open access funding provided by Istituto Italiano di Tecnologia within the CRUI-CARE Agreement.	Aanaes H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9; Aanaes H, 2012, INT J COMPUT VISION, V97, P18, DOI 10.1007/s11263-011-0473-8; Agudo A, 2018, IEEE T PATTERN ANAL, V40, P2137, DOI 10.1109/TPAMI.2017.2752710; Agudo A, 2017, PROC CVPR IEEE, P1513, DOI 10.1109/CVPR.2017.165; Agudo A, 2016, IEEE T PATTERN ANAL, V38, P979, DOI 10.1109/TPAMI.2015.2469293; Akhter I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159523; Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201; Ansari MD, 2017, INT CONF 3D VISION, P78, DOI 10.1109/3DV.2017.00019; Bartoli A., 2008, INT C COMP VIS PATT; Bouguet J.-Y., 2001, INTEL CORP, V5, P4, DOI DOI 10.1109/HPDC.2004.1323531; Brand M, 2001, PROC CVPR IEEE, P315; Brandt S., 2011, WORKSH NONR SHAP AN; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Cha G., 2019, IEEE T PATTERN ANAL; Chhatkuli A., 2014, BMVC; Chhatkuli A, 2018, IEEE T PATTERN ANAL, V40, P2428, DOI 10.1109/TPAMI.2017.2762669; Cho J, 2016, INT J COMPUT VISION, V117, P226, DOI 10.1007/s11263-015-0860-7; Dai YC, 2014, INT J COMPUT VISION, V107, P101, DOI 10.1007/s11263-013-0684-2; Del Bue A, 2007, IMAGE VISION COMPUT, V25, P297, DOI 10.1016/j.imavis.2005.10.004; Del Bue A, 2005, LECT NOTES COMPUT SC, V3723, P97; Del Bue A., 2006, INT C COMP VIS PATT; Del Bue A., 2005, LECT NOTES COMPUTER, V3723, DOI 10.1007/11564386_9; Del Bue A, 2013, INT J COMPUT VISION, V103, P226, DOI 10.1007/s11263-012-0577-9; Del Bue A, 2012, IEEE T PATTERN ANAL, V34, P1496, DOI 10.1109/TPAMI.2011.238; Del Bue A, 2011, IEEE I CONF COMP VIS, P675, DOI 10.1109/ICCV.2011.6126303; Deutsches Institut fur Normung, 2012, TECH REP; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Fayad J, 2010, LECT NOTES COMPUT SC, V6314, P297, DOI 10.1007/978-3-642-15561-1_22; Golyanik V, 2019, PROCEEDINGS OF MVA 2019 16TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA); Gotardo P.F.U., 2011, IEEE INT C COMP VIS; Gotardo PFU, 2011, PROC CVPR IEEE, DOI 10.1109/cvpr.2011.5995560; Gotardo PFU, 2011, IEEE T PATTERN ANAL, V33, P2051, DOI 10.1109/TPAMI.2011.50; Gower J. C., 2004, PROCRUSTES PROBLEMS; GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478; Hamsici OC, 2012, LECT NOTES COMPUT SC, V7575, P260, DOI 10.1007/978-3-642-33765-9_19; Hartley R, 2008, LECT NOTES COMPUT SC, V5302, P276, DOI 10.1007/978-3-540-88682-2_22; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Hong JH, 2017, PROC CVPR IEEE, P5939, DOI 10.1109/CVPR.2017.629; Kahl F, 2002, WORKSH VIS MOD DYN S; Kong C, 2016, PROC CVPR IEEE, P4123, DOI 10.1109/CVPR.2016.447; Kumar S, 2020, IEEE WINT CONF APPL, P51, DOI 10.1109/WACV45572.2020.9093514; Kumar S, 2017, PATTERN RECOGN, V71, P428, DOI 10.1016/j.patcog.2017.05.014; Lee M, 2017, IEEE T PATTERN ANAL, V39, P1388, DOI 10.1109/TPAMI.2016.2596720; Lee M, 2016, PROC CVPR IEEE, P4670, DOI 10.1109/CVPR.2016.505; Li X, 2018, PROC CVPR IEEE, P3032, DOI 10.1109/CVPR.2018.00320; Llado X., 2006, P INT C PATT REC HON; Llado X, 2010, IMAGE VISION COMPUT, V28, P1339, DOI 10.1016/j.imavis.2010.01.014; Menze Moritz, 2015, CVPR; Modrzejewski R, 2019, INT J COMPUT ASS RAD, V14, P1237, DOI 10.1007/s11548-019-02001-4; Olsen SI, 2008, J MATH IMAGING VIS, V31, P233, DOI 10.1007/s10851-007-0060-3; Ornhag M.V., 2020, P IEEE CVF C COMP VI, P8474; Ozyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X; PALADINI M, 2009, INT C COMP VIS PATT; Paladini M, 2012, INT J COMPUT VISION, V96, P252, DOI 10.1007/s11263-011-0468-5; Parashar S, 2018, IEEE T PATTERN ANAL, V40, P2442, DOI 10.1109/TPAMI.2017.2760301; Park S, 2018, IEEE T IMAGE PROCESS, V27, P249, DOI 10.1109/TIP.2017.2757280; Reich C, 1997, P SOC PHOTO-OPT INS, V3100, P236, DOI 10.1117/12.287750; Russell C, 2011, PROC CVPR IEEE; Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38; Salzmann M, 2007, IEEE T PATTERN ANAL, V29, P1481, DOI 10.1109/TPAMI.2007.1080; Seber GAF, 2003, WILEY SERIES PROBABI, V2nd; Simon T, 2017, IEEE T PATTERN ANAL, V39, P2201, DOI 10.1109/TPAMI.2016.2638904; Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0; TAYLOR J, 2010, PROC CVPR IEEE, P2761, DOI DOI 10.1109/CVPR.2010.5540002; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L, 2001, PROC CVPR IEEE, P493; Torresani L, 2004, ADV NEURAL INFORM PR; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; University C.M, 2002, CMU GRAPH LAB MOT CA; Valmadre J, 2012, PROC CVPR IEEE, P1394, DOI 10.1109/CVPR.2012.6247826; Varol A, 2009, IEEE I CONF COMP VIS, P1811, DOI 10.1109/ICCV.2009.5459403; Velleman P. F., 1981, APPL BASICS COMPUTIN; Vicente S, 2012, LECT NOTES COMPUT SC, V7574, P426, DOI 10.1007/978-3-642-33712-3_31; Vidal R, 2006, LECT NOTES COMPUT SC, V3952, P205; Wang G, 2008, PATTERN RECOGN LETT, V29, P72, DOI 10.1016/j.patrec.2007.09.004; Wang GH, 2007, PATTERN RECOGN LETT, V28, P507, DOI 10.1016/j.patrec.2006.09.006; Wang YX, 2015, INT J COMPUT VISION, V111, P315, DOI 10.1007/s11263-014-0746-0; WILLIAMSON DF, 1989, ANN INTERN MED, V110, P916, DOI 10.7326/0003-4819-110-11-916; Xiao J, 2005, IEEE I CONF COMP VIS, P1075; Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9; Zappella L, 2013, COMPUT VIS IMAGE UND, V117, P113, DOI 10.1016/j.cviu.2012.09.004; Zhu YY, 2014, PROC CVPR IEEE, P1542, DOI 10.1109/CVPR.2014.200	82	5	5	1	7	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	APR	2021	129	4					882	899		10.1007/s11263-020-01406-y	http://dx.doi.org/10.1007/s11263-020-01406-y		DEC 2020	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RB3QK		Green Submitted, hybrid			2022-12-18	WOS:000603534600001
J	Zhang, JJ; Wang, L; Zhou, LP; Li, WQ				Zhang, Jianjia; Wang, Lei; Zhou, Luping; Li, Wanqing			Beyond Covariance: SICE and Kernel Based Visual Feature Representation	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Covariance matrix; Structure sparsity; Sparse inverse covariance estimate; Kernel matrix; Visual representation	REGION COVARIANCE; DIFFUSION TENSOR; CLASSIFICATION; RECOGNITION; SELECTION; MATRICES	The past several years have witnessed increasing research interest on covariance-based feature representation. Originally proposed as a region descriptor, it has now been used as a general representation in various recognition tasks, demonstrating promising performance. However, covariance matrix has some inherent shortcomings such as singularity in the case of small sample, limited capability in modeling complicated feature relationship, and a single, fixed form of representation. To achieve better recognition performance, this paper argues that more capable and flexible symmetric positive definite (SPD)-matrix-based representation shall be explored, and this is attempted in this work by exploiting prior knowledge of data and nonlinear representation. Specifically, to better deal with the issues of small number of feature vectors and high feature dimensionality, we propose to exploit the structure sparsity of visual features and exemplify sparse inverse covariance estimate as a new feature representation. Furthermore, to effectively model complicated feature relationship, we propose to directly compute kernel matrix over feature dimensions, leading to a robust, flexible and open framework of SPD-matrix-based representation. Through theoretical analysis and experimental study, the proposed two representations well demonstrate their advantages over the covariance counterpart in skeletal human action recognition, image set classification and object classification tasks.	[Zhang, Jianjia] Sun Yat Sen Univ, Sch Biomed Engn, Shenzhen 518107, Guangdong, Peoples R China; [Zhang, Jianjia] Univ Technol Sydney, Sch Comp Sci, Sydney, NSW 2007, Australia; [Wang, Lei; Li, Wanqing] Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW 2522, Australia; [Zhou, Luping] Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia	Sun Yat Sen University; University of Technology Sydney; University of Wollongong; University of Sydney	Wang, L (corresponding author), Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW 2522, Australia.	seuzjj@gmail.com; leiw@uow.edu.au; luping.zhou@sydney.edu.au; wanqing@uow.edu.au	Li, Wanqing/ABG-2620-2020; Wang, Lei/D-9079-2013	Wang, Lei/0000-0002-0961-0441; Li, Wanqing/0000-0002-4427-2687	Australian Research Council - Australian Government [DP200101289]; Australian Research Council [DE160100241]	Australian Research Council - Australian Government(Australian Research Council); Australian Research Council(Australian Research Council)	The authors would like to thank Chang Tang for helping with preparing the action data sets and RuipingWang for sharing their processed image set data sets used in the experiments. Thanks also go to anonymous reviewers and editors for the constructive comments and the guidance in the revision of this paper. Lei Wang is the recipient of an Australian Research Council Discovery Project (Project Number DP200101289) funded by the Australian Government. Luping Zhou is supported by Australian Research Council DECRA program (DE160100241).	Adamczak R, 2010, J AM MATH SOC, V23, P535; Ali S, 2007, IEEE I CONF COMP VIS, P1703; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965; Aydin R, 2014, IN C IND ENG ENG MAN, P1, DOI 10.1109/IEEM.2014.7058588; Banerjee O, 2008, J MACH LEARN RES, V9, P485; BASSER PJ, 1994, J MAGN RESON SER B, V103, P247, DOI 10.1006/jmrb.1994.1037; Cavazza J., 2017, IEEE C COMP VIS PATT, P33; Cavazza J, 2019, PATTERN RECOGN, V93, P25, DOI 10.1016/j.patcog.2019.03.031; Cavazza J, 2017, LECT NOTES COMPUT SC, V10484, P211, DOI 10.1007/978-3-319-68560-1_19; Cavazza J, 2016, INT C PATT RECOG, P408, DOI 10.1109/ICPR.2016.7899668; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chetty K, 2010, IEEE RAD CONF, P188, DOI 10.1109/RADAR.2010.5494627; Chunfeng Yuan, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P343; Cimpoi M, 2016, INT J COMPUT VISION, V118, P65, DOI 10.1007/s11263-015-0872-3; Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461; Cirujeda Pol, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P657, DOI 10.1109/3DV.2014.10; Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325; Donahue J, 2014, PR MACH LEARN RES, V32; Dryden IL, 2009, ANN APPL STAT, V3, P1102, DOI 10.1214/09-AOAS249; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Durand T, 2017, PROC CVPR IEEE, P5957, DOI 10.1109/CVPR.2017.631; Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fasshauer GE, 2011, DOLOMIT RES NOTES AP, V4, P21; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Gross Ralph, 2001, CMURITR0118; Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132; Harandi MT, 2014, LECT NOTES COMPUT SC, V8690, P17, DOI 10.1007/978-3-319-10605-2_2; Harandi MT, 2012, LECT NOTES COMPUT SC, V7573, P216, DOI 10.1007/978-3-642-33709-3_16; Hastie T, 2009, ELEMENTS STAT LEARNI; Hayat M, 2017, INT J COMPUT VISION, V123, P479, DOI 10.1007/s11263-017-1000-3; Haykin S, 1998, NEURAL NETWORKS COMP, V2nd; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hsu CW, 2003, PRACTICAL GUIDE SUPP; Hu JF, 2018, LECT NOTES COMPUT SC, V11211, P346, DOI 10.1007/978-3-030-01234-2_21; Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172; Huang JZ, 2011, J MACH LEARN RES, V12, P3371; Huang SA, 2010, NEUROIMAGE, V50, P935, DOI 10.1016/j.neuroimage.2009.12.120; Hussein Mohamed E, 2013, 23 INT JOINT C ART I; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Jayasumana S, 2013, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2013.17; Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486; Koller D., 2009, PROBABILISTIC GRAPHI; Koniusz P, 2016, PROC CVPR IEEE, P5395, DOI 10.1109/CVPR.2016.582; Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3; Koniusz Piotr, 2013, HIGHER ORDER OCCURRE; Kulkarni P, 2016, LECT NOTES COMPUT SC, V9912, P329, DOI 10.1007/978-3-319-46484-8_20; Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115; Lehrmann AM, 2013, IEEE I CONF COMP VIS, P1281, DOI 10.1109/ICCV.2013.162; Leibe B, 2003, PROC CVPR IEEE, P409; Li PH, 2017, IEEE I CONF COMP VIS, P2089, DOI 10.1109/ICCV.2017.228; Li PH, 2012, LECT NOTES COMPUT SC, V7574, P469, DOI 10.1007/978-3-642-33712-3_34; Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572; Li WB, 2015, IEEE I CONF COMP VIS, P4444, DOI 10.1109/ICCV.2015.505; Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391; Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Mualler M., 2009, P ACM SIGGRAPH EUR S, P17; Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76; Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668; Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98; Pang YW, 2008, IEEE T CIRC SYST VID, V18, P989, DOI 10.1109/TCSVT.2008.924108; Pang YW, 2008, IEEE T SYST MAN CY B, V38, P1652, DOI 10.1109/TSMCB.2008.927276; Park J., 2007, DIGITAL CORRELATION; Peters J, 2017, ADAPT COMPUT MACH LE; Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790; Poczos B, 2012, PROC CVPR IEEE, P2989, DOI 10.1109/CVPR.2012.6248028; Porikli F, 2006, P IEEE COMP SOC C CO, P728, DOI [10.1109/CVPR.2006.94, DOI 10.1109/CVPR.2006.94]; Quang M. H., 2014, ADV NEURAL INFORM PR, P388; Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261; Romero A., 2013, INT C COMP VIS COMP, P1; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230; Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7; Smith SM, 2011, NEUROIMAGE, V54, P875, DOI 10.1016/j.neuroimage.2010.08.063; Song SJ, 2017, AAAI CONF ARTIF INTE, P4263; Sra S., 2011, ARXIV11101773; Sun HL, 2017, PROC CVPR IEEE, P6240, DOI 10.1109/CVPR.2017.661; Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533; Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Wang H, 2019, IEEE I CONF COMP VIS, P8508, DOI 10.1109/ICCV.2019.00860; Wang L, 2020, IEEE T IMAGE PROCESS, V29, P15, DOI 10.1109/TIP.2019.2925285; Wang L, 2015, IEEE I CONF COMP VIS, P4570, DOI 10.1109/ICCV.2015.519; Wang Q., 2019, ARXIV190406836; Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965; Wang W, 2015, PROC CVPR IEEE, P3395, DOI 10.1109/CVPR.2015.7298816; Wei Y., 2014, ARXIV PREPRINT ARXIV; Wei ZJ, 2016, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2016.326; Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566; Wu YW, 2015, PATTERN ANAL APPL, V18, P45, DOI 10.1007/s10044-014-0430-6; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108; Yun Kiwon, 2012, 2012 IEEE COMP SOC C, P28; Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2017.233; Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24; Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885; Zunino A, 2017, LECT NOTES COMPUT SC, V10484, P469, DOI 10.1007/978-3-319-68560-1_42	108	5	5	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	FEB	2021	129	2					300	320		10.1007/s11263-020-01376-1	http://dx.doi.org/10.1007/s11263-020-01376-1		SEP 2020	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	QH2HF		Green Published			2022-12-18	WOS:000568233000002
J	Vasudevan, AB; Dai, DX; Van Gool, L				Vasudevan, Arun Balajee; Dai, Dengxin; Van Gool, Luc			Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Vision-and-language navigation; Long-range navigation; Spatial memory; Dual attention	LANDMARK	The role of robots in society keeps expanding, bringing with it the necessity of interacting and communicating with humans. In order to keep such interaction intuitive, we provide automatic wayfinding based on verbal navigational instructions. Our first contribution is the creation of a large-scale dataset with verbal navigation instructions. To this end, we have developed an interactive visual navigation environment based on Google Street View; we further design an annotation method to highlight mined anchor landmarks and local directions between them in order to help annotators formulate typical, human references to those. The annotation task was crowdsourced on the AMT platform, to construct a new Talk2Nav dataset with 10, 714 routes. Our second contribution is a new learning method. Inspired by spatial cognition research on the mental conceptualization of navigational instructions, we introduce a soft dual attention mechanism defined over the segmented language instructions to jointly extract two partial instructions-one for matching the next upcoming visual landmark and the other for matching the local directions to the next landmark. On the similar lines, we also introduce spatial memory scheme to encode the local directional transitions. Our work takes advantage of the advance in two lines of research: mental formalization of verbal navigational instructions and training neural network agents for automatic way finding. Extensive experiments show that our method significantly outperforms previous navigation methods. For demo video, dataset and code, please refer to our project page..	[Vasudevan, Arun Balajee; Dai, Dengxin; Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland; [Van Gool, Luc] Katholieke Univ Leuven, Leuven, Belgium	Swiss Federal Institutes of Technology Domain; ETH Zurich; KU Leuven	Vasudevan, AB (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	arunv@vision.ee.ethz.ch; dai@vision.ee.ethz.ch; vangool@vision.ee.ethz.ch			Toyota Motor Europe via the research Project TRACE-Zurich	Toyota Motor Europe via the research Project TRACE-Zurich	This work is funded by Toyota Motor Europe via the research Project TRACE-Zurich.	Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Anderson Peter, 2018, EVALUATION EMBODIED, V1; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Aneja J, 2018, PROC CVPR IEEE, P5561, DOI 10.1109/CVPR.2018.00583; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ba J. L, 2015, ARXIV151102793; BENDER EM, 2011, LINGUISTIC ISSUES LA, V6, P1, DOI DOI 10.33011/LILT.V6I.1239; Boularias A, 2015, IEEE INT CONF ROBOT, P1976, DOI 10.1109/ICRA.2015.7139457; Brahmbhatt S, 2017, PROC CVPR IEEE, P3087, DOI 10.1109/CVPR.2017.329; Chen David L, 2011, AAAI; Chen H, 2019, PROC CVPR IEEE, P12530, DOI 10.1109/CVPR.2019.01282; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32; Das A, 2018, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2018.00008; de Vries Harm, 2018, ARXIV180703367; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deruyttere T, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2088; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fried D, 2018, ADV NEUR IN, V31; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gordon D, 2018, PROC CVPR IEEE, P4089, DOI 10.1109/CVPR.2018.00430; Grabler F, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360699; Graves A., 2014, ARXIV14105401; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Graves Alex, 2016, ARXIV160308983; Gupta S, 2017, PROC CVPR IEEE, P7272, DOI 10.1109/CVPR.2017.769; Gygli M, 2016, PROC CVPR IEEE, P1001, DOI 10.1109/CVPR.2016.114; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hecker S, 2019, ARXIV190310995; Hecker S, 2018, LECT NOTES COMPUT SC, V11211, P449, DOI 10.1007/978-3-030-01234-2_27; Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618; Hermann K. M, 2019, LEARNING FOLLOW DIRE; Hermann Karl Moritz, 2017, ARXIV170606551; Hill F, 2017, UNDERSTANDING GROUND; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Holscher C, 2011, COGNITION, V121, P228, DOI 10.1016/j.cognition.2011.06.005; Hu R, 2019, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2019.01039; Hu RH, 2018, LECT NOTES COMPUT SC, V11211, P55, DOI 10.1007/978-3-030-01234-2_4; Hudson Drew Arad, 2018, INT C LEARN REPR, P2; Ishikawa T, 2012, SPAT COGN COMPUT, V12, P1, DOI 10.1080/13875868.2011.581773; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Ke LYM, 2019, PROC CVPR IEEE, P6734, DOI 10.1109/CVPR.2019.00690; Khosla A, 2014, PROC CVPR IEEE, P3710, DOI 10.1109/CVPR.2014.474; Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010; Klippel A, 2005, J VISUAL LANG COMPUT, V16, P311, DOI 10.1016/j.jvlc.2004.11.004; Klippel A, 2005, SPATIAL INFORM THEOR; Language Tool, 2016, SPELL CHECK API; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ma C. Y, 2019, ARXIV190103035; Ma CY, 2019, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR.2019.00689; Michon P. E, 2001, SPATIAL INFORM THEOR; Mikolov T., 2013, ARXIV; Millonig A, 2007, IEEE T INTELL TRANSP, V8, P43, DOI 10.1109/TITS.2006.889439; Mirowski P, 2018, NIPS; Mirowski P., 2017, PROC INT C LEARN REP, P1; Nguyen K, 2019, PROC CVPR IEEE, P12519, DOI 10.1109/CVPR.2019.01281; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Price B, 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00728; Ren NX, 2017, PROCEEDINGS OF THE ASME 36TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2017, VOL 9; Sadhu SD, 2018, NEW POLYMER NANOCOMPOSITES FOR ENVIRONMENTAL REMEDIATION, P77, DOI 10.1016/B978-0-12-811033-1.00004-4; Sutskever I, 2014, ADV NEUR IN, V27; Thoma J, 2019, PROC CVPR IEEE, P7375, DOI 10.1109/CVPR.2019.00756; Tom A, 2004, APPL COGNITIVE PSYCH, V18, P1213, DOI 10.1002/acp.1045; Tom A, 2003, LECT NOTES COMPUT SC, V2825, P362; Tversky B, 1999, LECT NOTES COMPUT SC, V1661, P51; Vasudevan AB, 2018, PROC CVPR IEEE, P4129, DOI 10.1109/CVPR.2018.00434; Vasudevan AB, 2018, IEEE WINT CONF APPL, P1861, DOI 10.1109/WACV.2018.00206; Vaswani A, 2017, ADV NEUR IN, V30; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wang X, 2019, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2019.00679; Weissenberg J, 2014, P 2 ACM SIGSPATIAL I, P8; Weyand T, 2016, LECT NOTES COMPUT SC, V9912, P37, DOI 10.1007/978-3-319-46484-8_3; Wortsman M, 2019, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2019.00691; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Zang X, 2018, CORR; Zhu XX, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8050739; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	84	5	5	0	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JAN	2021	129	1								10.1007/s11263-020-01374-3	http://dx.doi.org/10.1007/s11263-020-01374-3		AUG 2020	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PU0KV		Green Published, hybrid, Green Submitted, Green Accepted			2022-12-18	WOS:000564493000001
